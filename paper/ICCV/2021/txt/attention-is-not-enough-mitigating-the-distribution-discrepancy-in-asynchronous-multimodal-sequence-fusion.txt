Abstract
Videos flow as the mixture of language, acoustic, and vision modalities. A thorough video understanding needs to fuse time-series data of different modalities for pre-diction. Due to the variable receiving frequency for se-quences from each modality, there usually exists inherent asynchrony across the collected multimodal streams. To-wards an efficient multimodal fusion from asynchronous multimodal streams, we need to model the correlations between elements from different modalities. The recent
Multimodal Transformer (MulT) approach extends the self-attention mechanism of the original Transformer network to learn the crossmodal dependencies between elements.
However, the direct replication of self-attention will suf-fer from the distribution mismatch across different modal-ity features. As a result, the learnt crossmodal dependen-cies can be unreliable. Motivated by this observation, this work proposes the Modality-Invariant Crossmodal Atten-tion (MICA) approach towards learning crossmodal inter-actions over modality-invariant space in which the distribu-tion mismatch between different modalities is well bridged.
To this end, both the marginal distribution and the elements with high-confidence correlations are aligned over the com-mon space of the query and key vectors which are computed from different modalities. Experiments on three standard benchmarks of multimodal video understanding clearly val-idate the superiority of our approach. 1.

Introduction
Videos analysis involves time-series data of language, acoustic, and vision modalities. Towards a thorough video
∗ Corresponding author: F. Lv (email: fengmaolv@126.com). understanding, we need to fuse the data sequences from dif-ferent modalities. In practice, however, the collected mul-timodal streams are usually asynchronous due to the vari-able receiving frequency for sequences of different modal-ities [18]. For example, the sound or the subtitle may not exactly match what the video displays. The inherent asyn-chrony across different modalities raises a challenge on per-forming efficient multimodal fusion which requires to have precise information of the actual relationships between ele-ments from different modality sequences.
To this end, the prior works manually preprocess the vi-sual and acoustic sequences by aligning them to the resolu-tion of textual words [15, 19, 24]. Then, multimodal fusion is performed on the word-aligned time steps. However, the manual alignment process usually requires a huge amount of time and labor effort. The recent Multimodal Trans-former (MulT) approach extends the self-attention mecha-nism of the standard Transformer to learn the correlations between elements from different modalities [18]. Based on the latent crossmodal interaction explored via the cross-modal attention operations, MulT performs multimodal fu-sion directly from the asynchronous multimodal sequences without manual alignment.
However, if we take a further insight into the crossmodal attention mechanism in MulT, we will find that the direct replication of Transformer is suboptimal for asynchronous multimodal sequence fusion. In the standard Transformer model, the self-attention operations explore the correlations between elements by comparing the query and key vectors which are computed from the elements’ features [21, 27].
On the other hand, MulT focuses on exploring the cross-modal correlations between elements. The query and key vectors involved in the crossmodal attention operation are computed from different modalities. Due to the hetero-geneities across different modality features [9, 27], there
will exist a clear distribution mismatch in the common space of queries and keys. Hence, their dot-product cannot reveal reliable crossmodal correlations between elements.
Motivated by the above observation, this work proposes the Modality-Invariant Crossmodal Attention (MICA) ap-proach towards multimodal fusion from asynchronous mul-timodal sequences. The core idea of our approach is to perform crossmodal attention over modality-invariant space in which the distribution discrepancy between different modalities is bridged. To this end, our approach enforces modality-invariance on the common space of the query and key vectors which are computed from different modalities.
Overall, our approach bridges the distribution mismatch in two ways. One is to match the marginal distribution via
Maximum Mean Discrepancy (MMD) which is commonly used in transfer learning or domain adaptation [13, 10]. The other is to match the elements with high-confidence corre-lations via our proposed Propagated Element-level Align-ment (PEA) strategy. To be specific, our approach propa-gates the information of the crossmodal correlations along the network layers, i.e., the elements with high-confidence correlations in a previous layer will also participate in the element-level alignment loss (e.g., the L2 loss) of the sub-sequent layers. The propagation strategy can enforce the consistency across the network layers and guide the Trans-former network to progressively obtain better crossmodal correlations between elements. Compared with the original
MulT model, our approach can overcome the distribution discrepancy between different modalities and build more reliable crossmodal relationships for multimodal fusion from asynchronous multimodal sequences. Experiments on three multimodal video understanding benchmarks clearly demonstrate the effectiveness of our approach.
To sum up, the contributions of this work are three-fold:
• We draw the first attention on the distribution discrep-ancy problem which restrains the attention mechanism to obtain reliable crossmodal correlations for asyn-chronous multimodal sequence fusion.
• We propose to perform crossmodal attention over modality-invariant space where the distribution gap across modalities is bridged. Both the marginal distri-bution mismatch and the element-level mismatch are aligned to reduce the distribution discrepancy.
• Our approach can obtain the state-of-the-art perfor-mance on different benchmarks of multimodal video understanding. 2.