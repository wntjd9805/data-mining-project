Abstract
Model compression techniques are recently gaining ex-plosive attention for obtaining efficient AI models for var-ious real time applications. Channel pruning is one im-portant compression strategy, and widely used in slimming various DNNs. Previous gate-based or importance-based pruning methods aim to remove channels whose “impor-tance” are smallest. However, it remains unclear what cri-teria the channel importance should be measured on, lead-ing to various channel selection heuristics. Some other sampling-based pruning methods deploy sampling strategy to train sub-nets, which often causes the training instabil-ity and the compressed model’s degraded performance. In view of the research gaps, we present a new module named
Gates with Differentiable Polarization (GDP), inspired by principled optimization ideas. GDP can be plugged before convolutional layers without bells and whistles, to control the on-and-off of each channel or whole layer block. Dur-ing the training process, the polarization effect will drive a subset of gates to smoothly decrease to exact zero, while other gates gradually stay away from zero by a large mar-gin. When training terminates, those zero-gated channels can be painlessly removed, while other non-zero gates can be absorbed into the succeeding convolution kernel, caus-ing completely no interruption to training nor damage to the trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show that the proposed GDP algo-rithm achieves the state-of-the-art performance on various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to DeepLabV3Plus-ResNet50 on the chal-lenging Pascal VOC segmentation task, whose test perfor-mance sees no drop (even slightly improved) with over 60%
FLOPs saving. 1.

Introduction
Model compression techniques recently attract many at-tentions from both industry and academic research commu-nities for compressing Deep Neural Networks (DNNs) to satisfy devices with limited computing and storage capabil-ities. One common model compression method is pruning, which includes unstructured pruning (i.e., weights sparsi-[4, 20, 38, 46, 66] and structured pruning (e.g., fication) channel pruning and kernel pruning) [78]. Unstructured pruning removes individual weight, which results in sparse connection and requires sparse matrix operations supports.
The structured pruning eliminates an entire neuron or a block at once, leaving the model’s structure and connections intact and thus having no special requirements for hardware or specific libraries. In this paper, we focus on the latter.
Importance-based pruning methods usually prune chan-nels/blocks whose importance are small, but it remains un-clear what criteria the importance should be measured on, leading to various importance selection heuristics, such as the magnitude of weights [74], first-order or second-order information of weights [11, 56, 59], knockoff features of filters [69] and so on. One recent track of works follow the sparse learning optimization routines, utilizing LASSO or group LASSO [24, 48, 74] to regularize training weights.
However, these methods need to prune from a skewed yet continuous histogram of weights values, and the selection of the pruning threshold [20] is critical yet ad-hoc, taking Fig-ure 1(a) for example. Moreover, removing many small yet non-zero weights will inevitably damage the network, and often need to fine-tune before the next pruning schedule, causing iterative pruning-finetuning loop [19, 42, 48, 55].
Besides the importance-based pruning methods, some sampling-based methods [14, 34, 53, 87, 22, 84] either sam-ple a sub-network or calculate expectations over whole net-work to train, utilizing Gambul-Softmax [31], STE [1] or
Bernoulli distribution [66, 84] strategies. These meth-ods may potentially cause either unstable training due to equiprobable sampling (i.e. sample a completely different sub-net when the probability is nearly uniformly distributed at the beginning of training), or converging at local mini-mum due to deterministic sampling (i.e. sample nearly the same sub-net when the probability becomes deterministic), thus need various ad-hoc manual designs [14].
pling/expectation. That makes it (1) easy to use, with nearly no manual crafting; (2) produce stable and smooth training; (3) incur almost no computational or time overhead; and (4) not rely on any specific mod-ules such as BN or ReLU.
• State-of-the-Art Results: Experiments on CIFAR-10 and ImageNet classification tasks show that GDP out-performs all previous competitors with clear margins.
GDP can obtain slightly improved performance in the
Pascal VOC segmentation task with DeepLabV3Plus-ResNet50, while reducing more than 60% FLOPs. 2.