Abstract
Occluded person re-identiﬁcation (ReID) aims at re-identifying occluded pedestrians from occluded or holistic images taken across multiple cameras. Current state-of-the-art (SOTA) occluded ReID models rely on some aux-iliary modules, including pose estimation, feature pyramid and graph matching modules, to learn multi-scale and/or part-level features to tackle the occlusion challenges. This unfortunately leads to complex ReID models that (i) fail to generalize to challenging occlusions of diverse appear-ance, shape or size, and (ii) become ineffective in handling non-occluded pedestrians. However, real-world ReID ap-plications typically have highly diverse occlusions and in-volve a hybrid of occluded and non-occluded pedestrians.
To address these two issues, we introduce a novel ReID model that learns discriminative single-scale global-level pedestrian features by enforcing a novel exponentially sen-sitive yet bounded distance loss on occlusion-based aug-mented data. We show for the ﬁrst time that learning single-scale global features without using these auxiliary modules is able to outperform the SOTA multi-scale and/or part-level feature-based models. Further, our simple model can achieve new SOTA performance in both occluded and non-occluded ReID, as shown by extensive results on three oc-cluded and two general ReID benchmarks. Additionally, we create a large-scale occluded person ReID dataset with various occlusions in different scenes, which is signiﬁcantly larger and contains more diverse occlusions and pedestrian dressings than existing occluded ReID datasets, providing a more faithful occluded ReID benchmark. The dataset is available at: https: // git. io/ OPReID 1.

Introduction
Person re-identiﬁcation (ReID) aims to search for the same person from a gallery of pedestrian images taken from different cameras, which is a critical task in computer vision
*CY and GP equally contributed to this work. CY and XB are with
State Key Laboratory of Software Development Environment, Jiangxi Re-search Institute, Beihang University. †XB and GP are corresponding au-thors, e-mail: baixiao@buaa.edu.cn, pangguansong@gmail.com
Figure 1 – Overview of three occluded ReID approaches. Exist-ing approaches (A&B) rely on auxiliary modules, such as pose estimation and feature pyramid, to learn multi-scale or part-level features for occluded ReID, while our proposed approach (C) can address the problem with signiﬁcantly simpliﬁed models that learn single-scale global features with single backbone. due to its broad applications in multi-camera tracking, video surveillance, and forensic search. Most existing ReID ap-proaches [7, 16, 32, 36, 43] generally assume that the whole body of the person is visible. However, in real applications, many pedestrian images can be occluded by various obsta-cles such as cars, trees, and crowd. It is challenging for the general ReID approaches to learn effective representations of these occluded images [6, 15, 26, 34, 37, 39], leading to ineffective performance for occluded ReID [22, 47].
A number of occluded ReID methods [8, 12, 14, 22, 33] have been proposed to tackle this problem. The current state-of-the-arts can be roughly divided into two categories, key-points based methods and feature pyramid matching methods. As shown in Figure 1 (A), the key-points based methods [8, 22, 33] often utilize pose estimation models to obtain some extra semantic information such as key-points heat-maps/graphs for identity matching in the training or inference stage. These part-level features or graph match-ing strategies help overcome the occlusion problem. How-ever, the ReID performance is heavily relied on the perfor-mance of pose estimation models. Further, these pose es-timation models may also suffer from occlusions [4, 28].
As shown in Figure 1 (B), the other type of occluded ReID methods [12,14] is built upon the feature pyramid matching framework using a single backbone network during train-ing. At the inference stage, they extract multi-scale fea-tures from both the query and gallery images for person matching. This feature pyramid matching strategy works well for the easy occlusions, in which only the query im-ages contain occluded body parts and the gallery images are holistic, but it fails to deal with the cases that both the query and gallery contain occluded images, especially when the occlusions are of different appearance, shape or size. Fur-ther, without the support of key-points models, the feature pyramid matching strategy can be strongly biased by the occlusion obstacles. For example, when the query image contains similar car obstacles as in the person images in the gallery, as illustrated in Figure 1(B), these models would wrongly yield a high matching score due to the similarity of the occlusion objects rather than the person appearance.
Additionally, these auxiliary modules in both types of meth-ods can lead to over-complicated models, rendering them less effective in handling non-occluded pedestrians. How-ever, real-world ReID applications typically have highly di-verse occlusions and involve a hybrid of occluded and non-occluded pedestrians.
In this work we propose a novel ReID model that learns discriminative single-scale global-level pedestrian repre-sentations for such real-world ReID applications. Al-though it is a largely simpliﬁed model that does not re-quire auxiliary models, it can well generalize to diverse oc-clusions and perform effectively in handling both occluded and holistic pedestrians. Particularly, our method leverages an occlusion-based data augmentation and an exponentially sensitive yet bounded distance loss to learn ﬁne-grained dis-criminative features from non-occluded body parts. In do-ing so, our model is optimized in an end-to-end fashion us-ing a single backbone network (e.g., Resnet-50), as shown in Figure 1 (C). The network is enhanced by disentangled non-local (DNL) operations and our proposed reconstruc-tive pooling layer to better learn the non-occluded features.
During inference, it uses only the single-scale global fea-ture representations from the ﬁnal feature layer, rather than the multi-scale or multiple part-level features as in current models, for person matching.
Further, there are limited publicly available dataset benchmarks for the occluded ReID task. Existing relevant datasets, including P-iLIDS [12], P-ReID [42], O-ReID
[47] and O-Duke [22], are small and have too monotonous occlusions to represent the problem complexities in real-world applications. Even worse, these monotonous occlu-sions may mislead the design and evaluation of ReID mod-els. For example, in the largest dataset O-Duke, most per-sons in the query image set are occluded by the same car
Table 1 – Modules and features used in occluded ReID meth-ods. Modules include Single-Backbone (S-B), Pose-Estimation (P-E), Feature-Pyramid-Matching (FP-M) and Graph-Matching (G-M). Features include Multi-Scale-Feature (MS-F), Part-level-Feature (Part-F) and Global-level-Feature (Global-F).
Module
Method
DSR [12]
FPR [14]
PGFA [22]
PVPM [8]
S-B P-E FP-M G-M MS-F (cid:88) (cid:88) (cid:88) (cid:88)
HOReID [33] (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Ours
Feature
Part-F Global-F (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) at the same body parts such as legs and feet (see Figure 4), which may lead to a “Clever Hans Phenomenon” [17], e.g., ReID models can achieve ‘correct’ ReID based on fea-tures extracted from the occlusion objects rather than the person appearance. To address these issues, we create a large-scale occluded ReID dataset with highly diverse oc-clusions (e.g. carts, signs, storage racks, etc.) at different body parts from diverse scenes inside and outside the super-markets and shopping malls (see Section 4 and Supplemen-tary Materials). It includes 7,918 identities with all seasons’ dressing and 72,442 images collected using 6,000 cameras.
The resulting data is signiﬁcantly more faithful and larger than the existing largest occluded ReID dataset – O-Duke that contains only 1,812 identities with exclusively winter dressing and 35,489 images collected using 8 cameras. The dataset is carefully prepared to avoid the privacy problem.
In summary, our main contributions are as follows.
• We propose to tackle occluded person ReID by learn-ing single-scale global feature representations using a single network backbone, contrasting to current state-of-the-art models that learn multi-scale and/or part-level features, with their performance heavily depen-dent on one or more auxiliary modules, as summarized in Table 1. To this end, we introduce a novel ReID model that minimizes an exponentially sensitive yet bounded distance loss on occlusion-based augmented data to learn such global features. Through this model, we show for the ﬁrst time that learning single-scale global features outperforms the multi-scale or part-level features for occluded ReID (e.g., up to 17%-19% relative improvement in both top-1 accuracy and av-erage precision), providing important implication for exploring simple and effective ReID models.
• We further show that our model can achieve new SOTA performance in both occluded and non-occluded ReID, as shown by extensive results on three occluded and two general ReID datasets. This is an important ca-pability for real-world ReID applications that typically involve a hybrid of occluded and holistic pedestrians.
• We introduce a large-scale occluded person ReID
dataset with both indoor and outdoor occlusions in dif-ferent scenes, which is signiﬁcantly larger and contains substantially more diverse occlusions and pedestrian dressings than existing occluded ReID datasets, pro-viding a more faithful occluded ReID benchmark. 2.