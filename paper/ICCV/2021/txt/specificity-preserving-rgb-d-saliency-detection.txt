Abstract
RGB-D saliency detection has attracted increasing at-tention, due to its effectiveness and the fact that depth cues can now be conveniently captured. Existing works often focus on learning a shared representation through various fusion strategies, with few methods explicitly considering how to preserve modality-specific characteristics.
In this paper, taking a new perspective, we propose a specificity-preserving network (SP-Net) for RGB-D saliency detection, which benefits saliency detection performance by exploring both the shared information and modality-specific proper-ties (e.g., specificity). Specifically, two modality-specific networks and a shared learning network are adopted to generate individual and shared saliency maps. A cross-enhanced integration module (CIM) is proposed to fuse cross-modal features in the shared learning network, which are then propagated to the next layer for integrating cross-level information. Besides, we propose a multi-modal fea-ture aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder, which can provide rich complementary multi-modal information to boost the saliency detection per-formance. Further, a skip connection is used to combine hierarchical features between the encoder and decoder lay-ers. Experiments on six benchmark datasets demonstrate that our SP-Net outperforms other state-of-the-art methods.
Code is available at: https://github.com/taozh2017/SPNet. 1.

Introduction
Saliency detection aims to locate the most visually prominent object(s) in a given scene [46]. It has been widely applied in various vision-related tasks, such as image un-derstanding [75], video/semantic segmentation [55, 58], ac-tion recognition [51], [55], and person re-identification [67].
Although significant progress has been made, it is still challenging to accurately locate salient objects in com-*Corresponding author: Deng-Ping Fan (dengpfan@gmail.com).
Figure 1. Comparison between existing RGB-D saliency detec-tion frameworks and our proposed model. (a) RGB and depth images are fed into two separate network streams and then the fused high-level features are fed into a decoder (e.g., [4,5,25,37]). (b) Depth features are integrated into the RGB network using a tailor-maid subnetwork (e.g., [6, 66, 72]). (c) Our method explic-itly explores the shared information and modality-specific char-acteristics. Then, the features learned from the modality-specific decoders are integrated into the shared decoder to boost saliency detection performance. plex scenes, such as instance cluttered background or low-contrast lighting conditions. Recently, with the large avail-ability of depth sensors in smart devices, depth maps have been introduced to provide geometric and spatial informa-tion to improve the saliency detection performance. Conse-quently, fusing RGB and depth images has gained increas-ing interest in the saliency detection community [39,64,69].
For RGB-D saliency detection, it is critical to effectively fuse RGB and depth images. Some works utilize an early fusion strategy via a simple concatenation for this. For ex-ample, these models [41, 46, 52, 56] directly integrate RGB and depth images to form a four-channel input. However, this type of fusion does not consider the distribution gap be-tween the two modalities, which may lead to inaccurate fea-ture fusion. Besides, various models based on a late fusion strategy use two parallel network streams to generate inde-pendent saliency maps for RGB and depth data, then the two maps are fused to obtain a final prediction map [15, 24, 57].
However, it is difficult to capture the complex interactions between the two modalities with this type of fusion.
Currently, various middle fusion methods utilize two in-dependent networks to learn intermediate features of the two modalities separately, and then the fused features are fed into a subsequent network or decoder (as shown in Fig. 1 (a)). Besides, other methods carry out cross-modal fusion at multiple scales [4, 5, 7, 25, 27, 37]. As a result, the com-plex correlations can be effectively exploited from the two modalities. Moreover, several methods utilize depth infor-mation to enhance RGB features via a tailor-maid subnet-work [6, 66, 72] (as shown in Fig. 1 (b)). For example,
Zhao et al.
[66] introduced a contrast prior into a CNN-based architecture to enhance the depth information, and then the enhanced depth was integrated with RGB features using a fluid pyramid integration module. Zhu et al. [72] utilized an independent subnetwork to extract depth-based features, which were then incorporated into the RGB net-work. It should be noted that the above methods mainly fo-cus on learning shared representations by fusing them and then use a decoder to generate the final saliency map. What is more, there is no decoder with supervision to guide the depth-based feature learning [66, 72], which may prevent optimal depth features from being obtained. From a multi-modal learning perspective, several works [26, 42, 70, 71] have shown that exploring both the shared information and modality-specific characteristics can improve the model performance. However, few RGB-D saliency detection models explicitly exploit modality-specific characteristics.
To this end, we propose a novel specificity-preserving network for RGB-D saliency detection (termed SP-Net), which not only explores the shared information but also exploits modality-specific characteristics to boost saliency detection performance. Specifically, two encoder subnet-works are used to extract multi-scale features for the two modalities, and a cross-enhanced integration module (CIM) is proposed to fuse cross-modal features. Then, we use a
U-Net [53] structure to construct a modality-specific de-coder, in which skip connections between the encoder and decoder layers are used to combine hierarchical features. In this way, we can learn powerful modality-specific features in each independent decoder. Moreover, we also construct a shared decoder to combine hierarchical features from pre-vious CIMs using skip connections. To make full use of the modality-specific features, we propose a multi-modal fea-ture aggregation (MFA) to integrate them into the shared decoder. Finally, we formulate a unified and end-to-end trainable framework to achieve RGB-D saliency detection.
Our main contributions are summarized as follows:
• We propose a novel specificity-preserving network for RGB-D saliency detection (SP-Net), which can explore the shared information as well as preserve modality-specific characteristics.
• We propose a cross-enhanced integration module (CIM) to fuse the cross-modal features and learn shared representations for the two modalities. The out-put of each CIM is then propagated to the next layer to capture cross-level information.
• We propose a simple but effective multi-modal feature aggregation (MFA) module to integrate these learned modality-specific features. It makes full use of the fea-tures learned in the modality-specific decoder to boost the saliency detection performance.
• Extensive experiments on six public datasets demon-strate the superiority of our model over thirty bench-marking methods. Moreover, we carry out an attribute-based evaluation to study the performance of many state-of-the-art RGB-D saliency detection methods under different challenging factors (e.g., number of salient objects, indoor or outdoor environments, and light conditions), which has not been done previously by existing studies. 2.