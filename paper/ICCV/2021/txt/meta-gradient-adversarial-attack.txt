Abstract
In recent years, research on adversarial attacks has be-come a hot spot. Although current literature on the transfer-based adversarial attack has achieved promising results for improving the transferability to unseen black-box models, it still leaves a long way to go. Inspired by the idea of meta-learning, this paper proposes a novel architecture called
Meta Gradient Adversarial Attack (MGAA), which is plug-and-play and can be integrated with any existing gradient-based attack method for improving the cross-model trans-ferability. Specifically, we randomly sample multiple mod-els from a model zoo to compose different tasks and iter-atively simulate a white-box attack and a black-box attack in each task. By narrowing the gap between the gradient directions in white-box and black-box attacks, the transfer-ability of adversarial examples on the black-box setting can be improved. Extensive experiments on the CIFAR10 and
ImageNet datasets show that our architecture outperforms the state-of-the-art methods for both black-box and white-box attack settings. 1.

Introduction
With the rapid development of neural networks in re-cent years, the reliability of neural networks has gradu-ally attracted more and more attention. The neural net-works are exceedingly sensitive to adversarial examples, i.e., the imperceptible perturbation on the input can eas-ily fool the model, leading to unexpected mistakes. For example, when employing face recognition technology for payment, a slight perturbation on the face image may trick the face recognition model into recognizing as someone else. Since attack and defense are two complementary as-pects, the researches on adversarial attacks can ultimately improve the robustness of the model, thereby making the model more reliable.
In recent years, many methods have been proposed to improve the success rate of attacks against white-box mod-els, such as FGSM [15], C&W [2], PGD [31], BIM [21],
DeepFool [32], etc. Based on the access to model param-eters, these methods can make the model misclassify the input images by only adding human-imperceptible pertur-bations, which is named as the white-box attack. How-ever, in reality, a more practical scenario is that the attacker cannot obtain any information of the target model, that is, the black-box attack. Therefore, some methods turn to uti-lize the transferability of adversarial examples to conduct black-box attacks, such as MIM [8], DIM [50], TIM [9], NI-SI [27], etc. Although most of these methods have achieved promising results under the scenario of black-box attacks, the transferability of adversarial examples is still limited due to the discrepancy between the white-box models and unseen black-box models.
Inspired by the philosophy of meta-learning, we propose a novel architecture named Meta Gradient Adversarial At-tack (MGAA), which is plug-and-play and can be incor-porated with any gradient-based adversarial attack method.
The main idea of MGAA is to generate the adversarial ex-amples by iteratively simulating white-box and black-box attacks to improve the transferability. Specifically, as shown in Fig. 1, in each iteration, multiple models are randomly sampled from the model zoo to compose a task, which can be divided into the meta-train and the meta-test step. The meta-train step first uses an ensemble of multiple models to simulate white-box attacks to obtain temporary adversar-ial examples, which are then used as a basis by the meta-test step to obtain the perturbation by simulating a black-box attack scenario. Finally, the perturbation obtained dur-ing the meta-test step is added to the adversarial examples generated in the previous iteration.
In Section 3.3, more theoretical analyses demonstrate that our proposed archi-tecture can gradually narrow the gap of gradient directions between white-box attack and black-box attack settings, thus improving the transferability of generated adversarial examples. Different from vanilla meta-learning methods, which enhance the generalization through model training, our proposed MGAA directly utilizes the gradient informa-Figure 1: Overview of our Meta Gradient Adversarial Attack (MGAA). MGAA consists of multiple (T in the figure) itera-tions. In each iteration, n + 1 models are randomly sampled from the model zoo to compose a meta-task. Each task is divided into two steps: the meta-train step and the meta-test step. In the meta-train step, an ensemble of the n sampled models is utilized to conduct the gradient-based attack to generate perturbations, which can be repeated by K times. The meta-test step uses the adversarial example xi,K obtained from the meta-train step as the basis, and utilizes the last sampled model to generate perturbations for adversarial attacks. Finally, the perturbation generated in the meta-test step xi,mt âˆ’ xi,K is added to xi, the final adversarial example after the i-th task. tion to improve the transferability of the adversarial exam-ples without the need of training an extra model.
Extensive experiments on the CIFAR10 [20] and Ima-geNet [38] dataset demonstrate that our proposed architec-ture significantly improves the success rates of both white-box and black-box attacks. Especially, by integrating TI-DIM [9] method into our proposed architecture, the av-erage attack success rate under the targeted attack setting against 10 white-box models increases by 27.67%, and the attack success rate against 6 black-box models increases by 28.52% on ImageNet, which clearly shows the superiority of our method.
The main contributions of this paper are as follows: 1. We propose the Meta Gradient Adversarial Attack architecture inspired by the philosophy of meta-learning, which iteratively simulates the white-box and the black-box attack to narrow the gap of the gradient directions when generating adversarial examples, thereby further improving the transferability of adversarial examples. 2. The proposed architecture can be combined with any existing gradient-based attack method in a plug-and-play mode. 3. Extensive experiments show that the proposed archi-tecture can significantly improve the attack success rates un-der both white-box and black-box settings. 2.