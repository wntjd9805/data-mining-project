Abstract
Federated Learning (FL) aims to establish a shared model across decentralized clients under the privacy-preserving constraint. Despite certain success, it is still challenging for FL to deal with non-IID (non-independent and identical distribution) client data, which is a general scenario in real-world FL tasks. It has been demonstrated that the performance of FL will be reduced greatly under the non-IID scenario, since the discrepant data distribu-tions will induce optimization inconsistency and feature di-vergence issues. Besides, naively minimizing an aggregate loss function in this scenario may have negative impacts on some clients and thus deteriorate their personal model performance. To address these issues, we propose a Uni-ﬁed Feature learning and Optimization objectives align-ment method (FedUFO) for non-IID FL. In particular, an adversary module is proposed to reduce the divergence on feature representation among different clients, and two con-sensus losses are proposed to reduce the inconsistency on optimization objectives from two perspectives. Extensive experiments demonstrate that our FedUFO can outperform the state-of-the-art approaches, including the competitive one data-sharing method. Besides, FedUFO can enable more reasonable and balanced model performance among different clients. 1.

Introduction
Nowadays, the machine learning based artiﬁcial intelli-gence (AI) technologies often rely heavily on large amounts of training data. However, together with the over-collection and over-utilization of personal private data, the risks of pri-vacy disclosure and abuse are increased. For example, in
ﬁnance, medical treatment and smart city applications, data leakage may lead to huge loss on properties, and even life.
*Ling-Yu Duan is the corresponding author.
Figure 1. A comparison of the traditional FedAVG and the pro-posed FedUFO. By making use of the global, group data informa-tion extracted from global and local models, our FedUFO effec-tively aligns the feature representation and optimization objective on local, group and global levels.
To establish health and sustainable AI ecosystem, McMa-han et al. [20] proposed a new machine learning paradigm
- Federated Learning (FL), which breaks the data barriers across different clients (such as regions and industries) un-der privacy-protection constraint. The main idea of FL is to change the transmission and aggregation from data-level to model-level, so that the advantage of big data can be taken in the AI applications without privacy disclosure. Inspired by this paradigm, data-preserving decentralized learning has been studied in a variety of applications [3, 4, 5, 18].
In real-world FL tasks, data distributions of different clients may vary since the data are usually collected from different sources or scenarios. However, existing FL ap-proaches often simply ignore the distribution divergence, and it has been demonstrated that performance of the global model on non-IID (non-independent and identical distribu-tion) federated data can be much worse than that on the IID ones [22, 9, 16]. The reason is proved to be the weight diver-gence derived from the discrepant data distribution [32, 19], i.e., the local models are optimized to different directions, which are far from the ideal. The distribution divergence of
non-IID data will also aggravate the performance fairness issue [15], which means that the established model yields signiﬁcantly different local performance across clients.
This motivates us to develop a novel non-IID FL method by simultaneously aligning the local optimization objectives across clients. Speciﬁcally, a group consensus loss (CGR loss) and a global consensus loss (CGL loss) are proposed to explicitly reduce optimization objective inconsistencies by considering a group of clients and all clients respectively.
This is achieved by ﬁxing the biased distribution of model predictions by utilizing the extracted group and global data information. Besides, an adversary scheme with a uniﬁed adversarial loss (UAD loss) is proposed to implicitly align the optimization objectives on the feature level, and this en-forces the model learning generalized feature representation across all data distributions. Fig. 1 is a comparison between the proposed FedUFO and traditional FedAVG [20] meth-ods. Compared with FedAVG, which only utilizes the lo-cal data during local training without considering the opti-mization objective differences, our FedUFO can exploit the additional global and group data information for objective alignment, and hence the obtained models are more reliable.
According to the extensive experiments, it has been ver-iﬁed that simply using the proposed UAD loss can out-perform the existing approaches, and performance of our model can be further improved by adding the CGR and
CGL losses. Our FedUFO can also achieve more fair perfor-mance distribution across clients. Moreover, by evaluating
FedUFO on four challenging re-identiﬁcation and classiﬁ-cation tasks, the effectiveness of FedUFO in real-world FL applications is validated.
To summarize, the main contributions of this paper are:
• We design a group consensus loss and a global consen-sus loss to explicitly align the local, group and global optimization objectives to alleviate the varied data dis-tribution issue in non-IID FL.
• We propose an adversary scheme with a uniﬁed ad-versarial loss to learn uniﬁed feature representation that can further help align the optimization objectives across clients implicitly.
• Numerous experiments are conducted in various com-puter vision tasks. The results demonstrate superiority of the proposed method in real-world applications. 2.