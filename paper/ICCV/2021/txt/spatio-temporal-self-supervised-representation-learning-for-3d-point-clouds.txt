Abstract
To date, various 3D scene understanding tasks still lack practical and generalizable pre-trained models, primar-ily due to the intricate nature of 3D scene understanding tasks and their immense variations introduced by camera views, lighting, occlusions, etc. In this paper, we tackle this challenge by introducing a spatio-temporal representation learning (STRL) framework, capable of learning from unla-beled 3D point clouds in a self-supervised fashion. Inspired by how infants learn from visual data in the wild, we explore the rich spatio-temporal cues derived from the 3D data.
Specifically, STRL takes two temporally-correlated frames from a 3D point cloud sequence as the input, transforms it with the spatial data augmentation, and learns the invariant representation self-supervisedly. To corroborate the efficacy of STRL, we conduct extensive experiments on three types (synthetic, indoor, and outdoor) of datasets. Experimental results demonstrate that, compared with supervised learn-ing methods, the learned self-supervised representation fa-cilitates various models to attain comparable or even bet-ter performances while capable of generalizing pre-trained models to downstream tasks, including 3D shape classifica-tion, 3D object detection, and 3D semantic segmentation.
Moreover, the spatio-temporal contextual cues embedded in 3D point clouds significantly improve the learned represen-tations. 1.

Introduction
Point cloud is a quintessential 3D representation for vi-sual analysis and scene understanding. It differs from alter-native 3D representations (e.g., voxel, mesh) as it is ubiq-uitous: Entry-level depth sensors (even on cellphones) di-rectly produce point clouds before triangulating into meshes or converting to voxels, making it mostly applicable to 3D scene understanding tasks such as 3D shape analysis [5], 3D object detection and segmentation [58, 10]. Despite its omnipresence in 3D representation, however, annotating 3D point cloud data is proven to be much more difficult com-* indicates equal contribution.
Figure 1: Overview of our method. By learning the spatio-temporal data invariance from a point cloud sequence, our method self-supevisedly learns an effective representation. pared with labeling conventional 2D image data; this ob-stacle precludes its potentials in 3D visual tasks. As such, properly leveraging the colossal amount of unlabeled 3D point cloud data is a sine qua non for the success of large-scale 3D visual analysis and scene understanding.
Meanwhile, self-supervised learning from unlabeled im-ages [11, 45, 24, 22, 6, 19, 7] and videos [54, 80, 34, 51] becomes a nascent direction in representation learning with great potential in downstream tasks.
In this paper, we fill in the absence by exploiting self-supervised representation learning for 3D point clouds to address a long-standing problem in our community—the supervised training struggles at producing practical and generalizable pre-trained models due to the supervision-starved nature of the 3D data. Specifically, we consider the following three principles in model design and learning:
Simplicity Although self-supervised learning ap-proaches for 3D point clouds exist, they rely exclusively on spatial analysis by reconstructing the 3D point clouds [1, 75, 53, 20]. This static perspective of self-supervised learning is designed explicitly with complex operations, architectures, or losses, making it difficult to train and generalize to diversified downstream tasks. We believe such intricate designs are artificially introduced and unnecessary, and could be diminished or eliminated by complementing the missing temporal contextual cues, akin to how infants may understand this world [18, 57]. 1
Invariance Learning data invariance via data augmen-tation and contrasting has shown promising results on im-ages and videos [22, 6, 19]. A natural question arises: How could we introduce and leverage the invariance in 3D point clouds for self-supervised learning?
Generalizability Prior literature [1, 75, 53, 20] has only verified the self-supervisedly learned representations in shape classification on synthetic datasets [5], which pos-sesses dramatically different characteristics compared with the 3D data of natural indoor [58, 10] or outdoor [16] en-vironments, thus failed to demonstrate sufficient generaliz-ability to higher-level tasks (e.g., 3D object detection).
To adhere to the above principles and tackle the chal-lenges introduced thereby, we devise a spatio-temporal representation learning (STRL) framework to learn from unlabeled 3D point clouds. Of note, STRL is remarkably simple by learning only from the positive pairs, inspired by the BYOL [19]. Specifically, STRL uses two neural net-works, referred to as online and target networks, that inter-act and learn from each other. By augmenting one input, we train the online network to predict the target network rep-resentation of another temporally correlated input, obtained by a separate augmentation process.
To learn the invariant representation [12, 68], we ex-plore the inextricably spatio-temporal contextual cues em-bedded in 3D point clouds. In our approach, the online net-work’s and target network’s inputs are temporally corre-lated, sampled from a point cloud sequence. Specifically, for natural images/videos, we sample two frames with a natural viewpoint change in depth sequences as the input pair. For synthetic data like 3D shape, we augment the original input by rotation, translation, and scaling to emulate the view-point change. The temporal difference between the inputs avails models of capturing the randomness and invariance across different viewpoints. Additional spatial augmenta-tions further facilitate the model to learn 3D spatial struc-tures of point clouds; see examples in Fig. 1 and Sect. 3.
To generalize the learned representation, we adopt sev-eral practical networks as backbone models. By pre-training on large datasets, we verify that the learned representa-tions can be readily adapted to downstream tasks directly or with additional feature fine-tuning. We also demonstrate that the learned representation can be generalized to distant domains, different from the pre-trained domains; e.g., the representation learned from ScanNet [10] can be general-ized to shape classification tasks on ShapeNet [5] and 3D object detection task on SUN RGB-D [58].
We conduct extensive experiments on various domains and test the performance by applying the pre-trained repre-sentation to downstream tasks, including 3D shape classifi-cation, 3D object detection, and 3D semantic segmentation.
Next, we summarize our main findings.
Our method outperforms prior arts. By pre-training with STRL and applying the learned models to downstream tasks, it (i) outperforms the state-of-the-art unsupervised methods on ModelNet40 [71] and reaches 90.9% 3D shape classification accuracy with linear evaluation, (ii) shows significant improvements in semi-supervised learning with limited data, and (iii) boosts the downstream tasks by trans-ferring the pre-trained models, e.g., it improves 3D object detection on SUN RGB-D [58] and KITTI dataset [16], and 3D semantic segmentation on S3DIS [2] via fine-tuning.
Simple learning strategy leads to the satisfying per-formance of learned 3D representation. Through the ablative study in Tables 7 and 8, we observe that STRL can learn the self-supervised representations with simple aug-mentations; it robustly achieves a satisfying accuracy (about 85%) on ModelNet40 linear classification, which echoes re-cent findings [46] that simply predicting the 3D orientation helps learn good representation for 3D point clouds.
The spatio-temporal cues boost the performance of learned representation. Relying on spatial or temporal augmentation alone only yield relatively low performance as shown in Tables 7 and 8. In contrast, we achieve an im-provement of 3% accuracy by learning the invariant repre-sentations combining both spatial and temporal cues.
Pre-training on synthetic 3D shapes is indeed help-ful for real-world applications. Recent study [73] shows the representation learned from ShapeNet is not well-generalized to the downstream tasks. Instead, we report an opposite observation in Table 6, showing the representation pre-trained on ShapeNet can achieve comparable and even better performance while applying to downstream tasks that tackle complex data obtained in the physical world. 2.