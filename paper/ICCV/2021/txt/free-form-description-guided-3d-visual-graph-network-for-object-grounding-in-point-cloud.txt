Abstract 3D object grounding aims to locate the most relevant target object in a raw point cloud scene based on a free-form language description. Understanding complex and di-verse descriptions, and lifting them directly to a point cloud is a new and challenging topic due to the irregular and sparse nature of point clouds. There are three main chal-lenges in 3D object grounding: to ﬁnd the main focus in the complex and diverse description; to understand the point cloud scene; and to locate the target object.
In this pa-per, we address all three challenges. Firstly, we propose a language scene graph module to capture the rich struc-ture and long-distance phrase correlations. Secondly, we introduce a multi-level 3D proposal relation graph module to extract the object-object and object-scene co-occurrence relationships, and strengthen the visual features of the ini-tial proposals. Lastly, we develop a description guided 3D visual graph module to encode global contexts of phrases and proposals by a nodes matching strategy. Extensive experiments on challenging benchmark datasets (ScanRe-fer [3] and Nr3D [42]) show that our algorithm outper-forms existing state-of-the-art. Our code is available at https://github.com/PNXD/FFL-3DOG. 1.

Introduction
Imagine a scenario where an old person with limited mo-bility wakes up in the morning, feeling unwell, and instructs a robot to fetch medicine from a brown table. He/she could say “A brown table is located in the corner of the room, it is to the right of a white cabinet and to the left of black shoes. The front of it is a light blue curtain.” For a human,
ﬁnding the table based on the free-form language is an easy task. However, for assistive robots, parsing the large 3D vi-sual scene, ﬁnding the target object and understanding the global context based on natural language descriptions is a challenging task. These sentences describe the appearance of the target object (table), its spatial location relative to other objects (cabinet, curtain and shoes) and the global
∗Equal contribution
†Corresponding author
Figure 1. Proposed model for object grounding in 3D scenes. A multi-level proposal relation graph Go is formed to strengthen the visual features of the initial proposals, then the 3D visual graph
Gu is constructed under the guidance of the language scene graph
Gl which reﬁnes the initial coarse proposals. The language scene graph Gl predicts the nodes matching with the 3D visual graph
Gu and the matching scores φ1 and φ2 are fused to make the ﬁnal grounding predictions. scene (room), which offer a rich source of information to localize the target object and guide the robot.
With the widespread availability of LiDARs, depth cam-eras and light ﬁeld cameras [13, 12], 3D scene represen-tation in the from of point clouds is becoming increas-ingly available and affordable in many application domains such as robotics, autonomous driving etc. Understanding the free-form descriptions and lifting them to the 3D point cloud scene is a new topic and challenging in the ﬁeld of vision-and-language.
Research on the use of complex descriptions for 3D ob-ject grounding in point clouds is still in its infancy and only a few methods exist in the literature. To advance this line of research, ScanRefer [3] introduced the ﬁrst large-scale
dataset that couples free-form descriptions to objects in 3D scenes. The ScanRefer [3] method consists of two stages: the ﬁrst stage aims to use a 3D object detector VoteNet [24] to generate a set of 3D object proposals based on the input point cloud, the second stage correlates a global language expression with 3D visual features of each proposal, com-putes a conﬁdence score for each fused feature, and then takes the proposal with the highest score as the target object.
Similar to ScanRefer [3], Yuan et al. [42] replaced the 3D object detector with 3D panoptic segmentation backbone to obtain instance-level candidates, and captured the context of each candidate for visual and language feature matching, where the instance with the highest score is regarded as the target object.
The above methods suffer from several issues due to the inherent difﬁculties of both complex language processing and irregular 3D point cloud recognition. Firstly, free-form descriptions are complex and diverse in nature, and contain several sentences where strong context and long-term rela-tion dependencies exist among them. Both ScanRefer [3] and InstanceRefer [42] do not consider that and only learn holistic representations of the description. Secondly, these two-stage solutions heavily rely on 3D object detectors or 3D panoptic segmentation backbones, and the quality of the object proposals obtained in the ﬁrst stage are coarse which severely affect the overall performance. Conventional two-stage methods use the coarse object proposals directly in the following steps and fail to consider the relationships be-tween the surrounding object proposals and global informa-tion in cluttered 3D scenes to reﬁne the visual features of coarse proposals. Thirdly, the relationships between object proposals and phrases have not been fully explored. All existing methods [3, 42] neglect linguistic and visual struc-tures, and only fuse the global input description embedding with 3D visual features for grounding.
To address the aforementioned limitations, we propose a free-form description guided 3D visual graph network for 3D object grounding in point clouds, as shown in Fig-ure 1. Especially, to incorporate the rich structure and lan-guage context, we parse the complex free-form description into three components (noun phrases, pronouns and relation phrases) and construct a language scene graph to compute context-aware phrase presentation through message propa-gation, in which the nodes and edges correspond to noun phrase plus pronouns and relation phrases respectively.
Moreover, through the 3D object detector of VoteNet [24], a set of initial 3D object proposals are extracted from the in-put raw point cloud. We introduce a multi-level 3D relation graph to leverage two co-occurrence relationships (object-object and object-scene), which strengthen the visual fea-tures of the initial proposals to boost the performance of the subsequent operations. Furthermore, we use the language scene graph to guide the pruning of redundant proposals and then reﬁne the selected ones. Built on top of the reﬁned proposal set, we introduce a 3D visual graph to generate a context-aware object representation via message propaga-tion. Concretely, nodes are the selected proposals relevant to the noun phrase, and the edges encode the relationships between object proposals. Finally, the nodes of 3D visual graph are adaptively matched with the nodes of language scene graph, then fused with the matching score in propos-als pruning for the ﬁnal 3D object grounding.
To sum up, our key contributions include: (1) We propose a free-form description guided 3D visual graph for object grounding that directly exploits the raw point cloud and is end-to-end trainable. (2) We propose a lan-guage scene graph module that captures the rich structure and long-distance phrase correlations; (3) We propose a multi-level 3D proposal relation graph module that extracts the object-object and object-scene co-occurrence relation-ships to strengthen the visual features of the initial propos-als; (4) We propose a description 3D visual graph mod-ule that encodes global contexts of phrases and proposals through nodes matching. Experiments were performed on the benchmark ScanRefer [3] and Nr3D [3] datasets and state-of-the-art results [3, 2, 42] were achieved. 2.