Abstract
Egocentric video recognition is a challenging task that requires to identify both the actor’s motion and the active object that the actor interacts with. Recognizing the ac-tive object is particularly hard due to the cluttered back-ground with distracting objects, the frequent ﬁeld of view changes, severe occlusion, etc. To improve the active ob-ject classiﬁcation, most existing methods use object detec-tors or human gaze information, which are computation-ally expensive or require labor-intensive annotations. To avoid these additional costs, we propose an end-to-end
Interactive Prototype Learning (IPL) framework to learn better active object representations by leveraging the mo-tion cues from the actor. First, we introduce a set of verb prototypes to disentangle active object features from dis-tracting object features. Each prototype corresponds to a primary motion pattern of an egocentric action, offer-ing a distinctive supervision signal for active object fea-ture learning. Second, we design two interactive oper-ations to enable the extraction of active object features, i.e., noun-to-verb assignment and verb-to-noun selection.
These operations are parameter-efﬁcient and can learn ju-dicious location-aware features on top of 3D CNN back-bones. We demonstrate that the IPL framework can gen-eralize to different backbones and outperform the state-of-the-art on three large-scale egocentric video datasets, i.e.,
EPIC-KITCHENS-55, EPIC-KITCHENS-100 and EGTEA. 1.

Introduction
Egocentric videos have become popular on social me-dia and have attracted increasingly more attention in com-puter vision since the introduction of datasets, such as
EGTEA [21], Charades-Ego [32], EPIC-KITCHENS [6, 5, 7]. Unlike third-person videos where actions usually hap-pen at a distance, egocentric videos focus on person and ob-ject interactions at a closer look. Understanding egocentric videos requires to identify both the motion from the actor and the object that the actor interacts with. Recent egocen-Figure 1. The motivation of Interactive Prototype Learning (IPL) framework. The noun classiﬁcation is difﬁcult as the active ob-ject can be surrounded by a considerable number of distracting objects. Our framework aims to collaboratively learn judicious motion-relevant spatio-temporal features for more accurate noun (active object) classiﬁcation. tric video datasets [5, 7] are usually constructed by decom-posing an action into a combination of a verb and a noun, where action recognition can be achieved by classifying the associated verb and noun. For instance, “cut potato” is di-vided into a verb “cut” and a noun “potato”. Such a formu-lation helps to distinguish the subtle semantic differences among actions.
Egocentric videos focus on domain-speciﬁc ﬁne-grained actions, while the existing third-person datasets [18] are like more generic and collected from various domains, sports and daily activities. In egocentric videos, the back-ground scene is often similar among different actions. For instance, “cutting carrot” and “peeling potato” can both happen in the same kitchen scene. Hence, the usefulness of scene context information is limited in egocentric videos, making the recognition task more challenging.
Besides the aforementioned challenges, noun classiﬁca-tion is particularly difﬁcult as the active object [9, 11] in-volved in the action can be surrounded by a considerable number of distracting objects, e.g., the bowls and pans sur-rounding the active object “potato” in Fig. 1. Indeed, noun
classiﬁcation tends to have much lower accuracy than verb in egocentric video datatsets [6, 40], and is the bottleneck of the whole action recognition system.
Previous methods either use off-the-shelf object detec-tors [40, 42] or human gaze provided by the datasets [21] as additional cues to improve noun recognition. However, running object detectors on high-resolution video frames is computationally expensive, and human annotations are not
In this paper, we propose to improve always available. active object recognition in egocentric videos by leveraging the information learned from the actor’s motion. The active objects often locate in areas where the actors perform the action. Moreover, the actor’s motion carries the intent of the actor and is often the dominant signal in the egocentric videos, which can serve as a reliable supervision to improve active object recognition.
We devise an end-to-end Interactive Prototype Learning (IPL) framework for joint verb and noun classiﬁcation (Fig. 1).
IPL learns verb prototypes using the supervi-sion from verb labels, and each verb prototype encodes the motion pattern of a verb class. The learned verb proto-types are used to guide noun classiﬁcation through disentan-gling active object features from distracting object features.
This is achieved by two interactive operations, i.e., noun-to-verb assignment and verb-to-noun selection. The two operations collaboratively extract judicious location-aware spatio-temporal features for noun classiﬁcation. The noun-to-verb assignment aims to aggregate the features based on their similarities to the verb prototypes. In the verb-to-noun selection, we choose the most action-relevant features for the noun classiﬁcation.
Some components of IPL share the same spirit as
NetVLAD [1], but there are a few key differences. First, our prototypes are learned with direct supervision from verb annotations. Each prototype corresponds to each verb class, whereas the semantic meaning of the NetVLAD clusters is unclear. Second, our prototypes are shared by verb and noun classiﬁcation in a multi-task setting, where the harder task (i.e., noun classiﬁcation) can beneﬁt from the infor-mation learned from verb classiﬁcation. Third, instead of concatenating the features from all clusters like NetVLAD, we propose a verb-to-noun selection mechanism to identify discriminative features from active objects.
With extensive experiments and detailed ablation stud-ies, we demonstrate that IPL outperforms the state of the art on three large-scale egocentric video dataset, and is able to generalize to different video backbones [3, 38]. To summa-rize, we made the following major contributions:
• Propose to leverage the information learned from rec-ognizing the actor’s motion to improve active object classiﬁcation, which is currently the bottleneck of ego-centric video recognition.
• Design the IPL framework which allows better infor-mation ﬂow between the verb and noun classiﬁcation task by sharing the same set of feature prototypes. superior
• IPL shows results on three egocentric i.e., EPIC-KITCHENS-100 [7], EPIC-datasets,
KITCHENS-55 [6] and EGTEA [21], without the ad-ditional cost of object detection and human gaze anno-tations. 2.