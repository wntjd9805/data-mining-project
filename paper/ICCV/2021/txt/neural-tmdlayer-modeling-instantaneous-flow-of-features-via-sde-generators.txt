Abstract
We study how stochastic differential equation (SDE) based ideas can inspire new modiﬁcations to existing al-gorithms for a set of problems in computer vision. Loosely speaking, our formulation is related to both explicit and im-plicit strategies for data augmentation and group equivari-ance, but is derived from new results in the SDE literature on estimating inﬁnitesimal generators of a class of stochas-tic processes. If and when there is nominal agreement be-tween the needs of an application/task and the inherent properties and behavior of the types of processes that we can efﬁciently handle, we obtain a very simple and efﬁcient plug-in layer that can be incorporated within any existing network architecture, with minimal modiﬁcation and only a few additional parameters. We show promising experiments on a number of vision tasks including few shot learning, point cloud transformers and deep variational segmentation obtaining efﬁciency or performance improvements. 1.

Introduction
Consider a deep neural network model with parameters
W which we train using the following update rule,
W ← W − η∇W EzR (W, z) (1) where z is a random variable representing data and R(·) rep-resents the loss function. Now, consider a slightly general form of the same update formula,
W ← W − η∇W EzR (W, T z) . (2)
The only change here is the introduction of T which can be assumed to be some data transformation matrix. If T = I, we see that Stochastic Gradient Descent (SGD) is a special case of (2) under the assumption that we approximate the expectation in (2) with ﬁnite iid samples (or a mini-batch).
Let us unpack the data transformation notation a bit to check what it offers. If a set of transformations T are chosen beforehand, and applied to the data samples before train-ing commences, T z simply represents data samples derived via data augmentation. On the other hand, T z may not necessarily be explicitly instantiated as above. For exam-ple, spherical CNN [16] shows that when point cloud type data are embedded on the sphere with spherical convolu-tional operators, then it is possible to learn representations of data that are equivariant to the group action of rotations with no explicit data augmentation procedure. In particular, these approaches register each data point on a standard tem-plate (like the sphere) on which efﬁcient convolutions can be deﬁned based on differential geometric constructions – in other words, utilizing the properties of the transformations
T of interest and how they relate the data points, such a treatment enables the updates to implicitly take into account the loss on T z. Conceptually, many results [16, 48, 42] on equivariance show that by considering the entire orbit of each sample (a 3D point cloud) during training, for special types of T , it is possible to avoid explicit data augmentation.
We can take a more expanded view of the above idea.
Repeated application of a transformation T on data point z produces a discrete sequence {z(t)}∞ t=0 where z(0) = z, z(t) = T t−1z. In general, the transformation matrix at the t-th step, denoted by T (t), need not even be generated
Indeed, in practice T (t) is selected from a ﬁxed matrix. from a set of appropriate transformations such as rotation, blur and so on, with some ordering, which could even be stochastic. At a high level, approaches such as [16, 12] can be seen as a special case of (2). Making this argument pre-cise needs adding an appropriate number of auxiliary vari-ables and by averaging over all possible realizable T ’s – the speciﬁc steps are not particularly relevant since apart from helping set up the intuition we just described, algorithms for equivariance to speciﬁc group actions do not directly inform our development. For the sake of convenience, we will pri-marily focus on the continuous time system since under the same initial conditions, the trajectories of both (continuous and discrete) systems coincide at all integers t.
What does z(t) actually represent? There are two in-terpretations of z(t): (i) it formalizes on-the-ﬂy or instan-jectories for the same initial conditions. Our main insight is that recent results in the SDE literature show that (un-der some technical conditions), the dynamics z(t) can be completely characterized by (functions of) the inﬁnitesimal generator L of the process z(t) which can be efﬁciently es-timated using ﬁnite data. We exploit this result via a sim-ple modiﬁcation to the estimation procedure – one that can be directly used within any backpropagation based train-ing scheme. Speciﬁcally, we exploit the result from [2] where the authors call the generator Target Measure Dif-fusion map (TMDmap). This leads to our TMDlayer that can be conveniently dropped into a network, and be used as a plug-and-play module with just a few additional parame-ters. When utilized within standard deep learning pipelines, our layer allows incorporating much richer domain infor-mation if available, or as a regularizer or an augmentation scheme, or as a substitute to an existing layer. We ﬁnd this is beneﬁcial to the overall performance of the model.
Our contributions. Models such as a Neural ODE
[10] and Neural SDE [34] usually parameterize the dynam-ical system as a stand-alone model, and show how gradients can be efﬁciently backpropagated through this module. We take a different line of approach: we propose a stochastic process inspired layer which, in its most rudimentary form, can be thought of as an augmentation scheme that can work with existing layers in deep neural networks. But differ-ent from explicit data augmentation (rotation, ﬂipping) that happens in the input image space, our layer can be utilized in the feature space and is fully adaptive to the input. But it is more than another augmentation scheme. Our layer al-lows modeling the time varying/stochastic property of the data/features, and controls them by a proper parameteri-zation which is highly parameter efﬁcient. We show that this stochasticity is not only mathematically interesting, but can be exploited in applications including point cloud trans-formers, object segmentation and few-shot recognition. 1.1.