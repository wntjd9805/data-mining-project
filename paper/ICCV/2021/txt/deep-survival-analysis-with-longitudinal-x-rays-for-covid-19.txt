Abstract
Time-to-event analysis is an important statistical tool for allocating clinical resources such as ICU beds. However, classical techniques like the Cox model cannot directly incor-porate images due to their high dimensionality. We propose a deep learning approach that naturally incorporates multi-ple, time-dependent imaging studies as well as non-imaging data into time-to-event analysis. Our techniques are bench-marked on a clinical dataset of 1,894 COVID-19 patients, and show that image sequences significantly improve predic-tions. For example, classical time-to-event methods produce a concordance error of around 30-40% for predicting hos-pital admission, while our error is 25% without images and 20% with multiple X-rays included. Ablation studies suggest that our models are not learning spurious features such as scanner artifacts and that models which use multiple images tend to perform better than those which only use one. While our focus and evaluation is on COVID-19, the methods we develop are broadly applicable. 1.

Introduction
COVID-19 is the most significant global health emer-gency in recent memory, with hundreds of thousands dead and widespread economic disruption. There is growing evi-dence that imaging is useful for the diagnosis and manage-ment of COVID-19 [1, 2]. Clinicians use radiology imaging to assess structural information which cannot be assessed with laboratory tests or physical examination. In COVID-19, chest imaging adds a high-dimensional assessment of the degree of pulmonary involvement of the disease. It allows clinicians to rule out other conditions which might contribute to the patient’s presentation such as lobular pneumonia and pneumothorax and to assess the patient for comorbidities such as heart failure, emphysema, and coronary artery dis-ease. Some researchers have already found that imaging features predict mortality in COVID-19 [3].
In this paper we address the challenge of predicting the time course of COVID-19 patient outcomes; for example, the probability that a specific patient will need an ICU bed in the next few days following hospital admission. Classical statistical techniques for time-to-event analysis (sometimes referred to as survival analysis) are widely used, but struggle with incorporating images due to their high dimensionality.
We begin with an overview of time-to-event analysis and a discussion of the challenges that images and COVID-19 present. Our deep learning approach is presented in section 3, followed by a review of related work in section 4. We de-scribe our clinical dataset and some implementation details, including the baseline, in section 5. Experimental results are given in section 6, with additional data and analysis in the supplemental material. 2. Time-to-event analysis
Time-to-event analysis techniques [4] predict the proba-bility of an outcome event occurring before a specific time, while accounting for right-censored (incompletely observed) data. Right-censoring happens when the event under study may not be observed within the relevant time period. In the clinical setting, these methods can predict a patient’s prob-ability of undergoing an event in a particular time interval as a function of their features. In our dataset for instance, when predicting if a hospitalized COVID-19 patient will be admitted to the ICU, right-censoring happens when, as of today, the patient has not been admitted.
Time-to-event analysis focuses on three interrelated quan-tities: (1) the hazard function, the rate of an event at time t given that the event did not occur before time t, which is not affected by right-censoring [5]; (2) the cumulative hazard function, the integral of the hazard function between 0 and time t; and (3) the survival function, a decreasing function that provides the probability that a patient will not experience the event of interest beyond any specified time t and is expressed as the exponential of the negative of the cumulative hazard function.
While the hazard function is not a probability, h(t) can be viewed as the probability of the event occurring in a small interval around t given that the event did not occur before t. For clinical purposes, once we have estimated the hazard
function we can then compute the probability of an event occurring during a specific time interval, e.g. ICU admission in the 72 hours after hospitalization. 2.1. Cox proportional hazards model
We model the hazard function using the most popular model, the Cox model [6], defined as h(t|x) = h0(t) exp [r(x(t))]. (1)
Here t is the time, x(t) is the set of features, h0(t) is the baseline hazard, the hazard of the specific event under study shared by all patients at time t, and r(x(t)) is the risk function, which describes the relationship between a patient’s features x(t) and the hazard of experiencing an event. Note that h0(t) only depends on time and not on features.
The Cox model has several advantages. First, it has no distributional assumptions, so its results will closely approx-imate the results for the correct model [7]. Second, even though the baseline hazard is assumed to be “unknown” and left unspecified, under minimal assumptions, the hazard, the cumulative hazard and the survival functions can be directly determined. These can then be used for predicting the proba-bility of an event occurring before the observing time t1 [7].
The Cox model assumes that time-fixed features (features that do not change over time) have a linear multiplicative effect on the hazard function and that the hazard ratio is constant over time. This is known as the proportional haz-ards (PH) assumption [6]. In our task, this means that, for example, patients with a low income have a higher (or lower) hazard of dying compared with patients with a high income and this ratio is constant over time. Note that this assumption is only needed for time-fixed features and successful strate-gies can be easily implemented to detect and overcome its violation. Examples include the graphical approach of log-log plots for detection, as well as adding into the Cox model an interaction between the non-proportional time-fixed fea-ture and time for overcoming its violation [9, 10]. 2.2. Images present challenges
The Cox model is a mainstay of time-to-event analysis, and has been extended to deal with complex scenarios [11, 12,13,14,15,16]. However, there are two features of our task that require us to go beyond the state of the art. First, images pose a significant challenge due to their high dimensionality.
Second, the time course of COVID-19 involves multiple interrelated events that cannot be predicted independently.
While there is compelling evidence that imaging studies are helpful in the diagnosis and management of COVID-19
[1,2,3], images present significant challenges. The amount of 1Another statistical technique used to predict probabilities of binary events is logistic regression [8]. Although widely used, logistic regression ignores the information about the time to an event and the censoring time, while time-to-event techniques fully exploit this important information. data in a single imaging study is orders of magnitude larger than the data available from other sources; a single medical image can easily be hundreds of megabytes.2 However, the Cox proportional hazards model cannot directly handle images as features due to their high dimensionality. As [17] reports, such inputs lead to degenerate behavior. It would of course be possible to learn a feature from an imaging study, for example a rating of disease severity on a 3-point scale.
Such an approach would severely and unnecessarily limit what can be learned from the images, which is a particularly poor choice for a novel disease.
As mentioned, the COVID-19 disease process involves competing and interrelated events. A straightforward appli-cation of time-to-event analysis would predict these events independently. This could easily lead to incoherent and self-contradictory predictions (for example, predicting that ICU discharge will almost certainly happen before admission to the ICU). 3. Our approach
Our main goal is to predict the probability of experiencing death, ICU admission, ICU discharge, hospital admission, and hospital discharge before the observing time t, as a func-tion of patient’s features. To do so, we assume nonlinear proportional hazards [13] for the time-fixed features.3 This assumption relaxes the more strict assumption of linear pro-portional hazards of the classical Cox model. In our analyses this means that, for instance, the hazard of dying among older patients at baseline increases non-linearly compared with younger patients at baseline and this ratio is constant over time [7]. This assumption has been already used in a variety of state of the art deep learning time-to-event tech-niques [11, 13, 18]. We also made the common assumption of non-informative censoring [19] which states that after conditioning on observable features, censoring and the risk for an event of interest are independent, i.e., the censoring mechanism does not depend on the unobserved features.
To estimate the hazard function introduced in Eq. (1) we compute two components, the baseline hazard h0(t) that only depends on the time t and the risk function r(x(t)) that only depends on features x(t). Once the hazard function is estimated, the cumulative hazard function and the survival function can be easily derived [4]; these are then used to pre-dict the patient’s probability of undergoing an event before time t. In other words, for each patient’s set of features x(t), we can predict the probability of an event happening before the observed time t. The baseline hazard does not depend on the features x(t) and therefore, it can be easily computed by 2While CT is becoming increasingly available, early in the pandemic imaging was primarily chest X-ray (CXR). 3This assumption is only needed for time-fixed features. Time-dependent features already depend on time, making the hazard also de-pending on time.
using classical estimators. We used the one presented in Eq. 4.34 of [19]. The risk function, however, depends on time-fixed and time-dependent image and non-image features. To estimate the risk function r(x(t)) while taking into account these challenging types of features, we developed novel deep learning techniques. 3.1. Architecture
To incorporate time-dependent imaging studies, time-dependent non-image data, and time-fixed variables, our proposed architecture has three components. First, we use a convolutional LSTM (ConvLSTM) [20] and an RNN-LSTM to extract time-dependent image features and time-dependent non-image features respectively. Then, we concatenate (⊕) the features extracted from the networks with time-fixed variables mapped to its corresponding embedding space, and passed the concatenated vector through a set of fully con-nected layers (FC Layers) to predict the risk function (Risk).
The architecture is shown in figure 1. 3.2. Loss function
Computed on the hazard function, the Cox’s partial like-lihood loss has been successfully used in recent state-of-the-art deep learning techniques [13, 18]. This likelihood function, however, is unsuitable, since it only applies to con-tinuous time data where no two events occur at the same time. In the case of discrete time data, ties may occur and all possible orders of these tied events should be considered.
Therefore, we adopt Efron’s approximation for handling ties, which is a computationally efficient estimation on the original Cox’s partial likelihood when ties are present [21].
Specifically, the loss function is as follows: (cid:104)
L = ∑
Ri − i log( ∑ j:Tj≥ti di−1
∑ w=0 where Ri = ∑ j∈Ki er(x j((t)) − (cid:105)
Ui)
, w di r(x j(t)),Ui = ∑ j∈Ki er(x j(t)). (2)
Here i denotes a unique time point and Tj is the followup time for patient j. We define the event indicator C j, where
C j = 1 if the patient j experiences an event at followup time
Tj, and C j = 0 if censored. The risk estimate r(x j(t)) is the output of our architectures for patient j. Ki is the set of patients whose followup time Tj = ti and C j = 1, and di = |Ki|. It is worth noticing that by construction, this loss function does not contain the baseline hazard h0(t), making its computation easier. 3.3. Evaluation and inference
We use concordance error [22] to compare performances of different models. This calculates the proportion of falsely ordered event pairs divided by the total number of possible evaluation pairs.
Figure 1: Our proposed architecture that handles time-fixed data, longitudinal non-image data and longitudinal images.
Similar to the loss function, during inference time, our model estimates the baseline hazard ∆H0(ti) and the cumula-tive baseline hazard H0(t) function using the Efron estimator for handling ties:
∆H0(ti) =
Di−1
∑ l=0
H0(t) = ∑ ti≤t 1
∑ j:Tj≥ti er(x j) − l
∆H0(ti)
Di ∑ j∈Ki er(x j)
, (3) (4) 3.4. Mini-batch SGD and stratified sampling
In the Cox partial likelihood function, we see that the formulation involves the risk predictions of all the patients whose followup time is longer than Ti. It is computationally costly and almost infeasible to optimize this loss when mod-els use time-dependent images. Instead, we use mini-batch stochastic gradient descent: for each iteration, we sample a subset of patients and compute the loss on the subset. To closely mimic the data distribution of the original patient group and to keep the loss function in a stable range (the range of loss function correlates to the number of events in the patient group), we use stratified sampling to maintain the same ratio of non-censored and censored patients in each mini-batch. 4.