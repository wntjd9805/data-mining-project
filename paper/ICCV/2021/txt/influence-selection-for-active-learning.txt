Abstract
The existing active learning methods select the samples by evaluating the sample’s uncertainty or its effect on the diversity of labeled datasets based on different task-specific or model-specific criteria. In this paper, we propose the In-fluence Selection for Active Learning(ISAL) which selects the unlabeled samples that can provide the most positive in-fluence on model performance. To obtain the influence of the unlabeled sample in the active learning scenario, we design the Untrained Unlabeled sample Influence Calcu-lation(UUIC) to estimate the unlabeled sample’s expected gradient with which we calculate its influence. To prove the effectiveness of UUIC, we provide both theoretical and experimental analyses. Since the UUIC just depends on the model gradients, which can be obtained easily from any neural network, our active learning algorithm is task-agnostic and model-agnostic. ISAL achieves state-of-the-art performance in different active learning settings for dif-ferent tasks with different datasets. Compared with previous methods, our method decreases the annotation cost at least by 12%, 13% and 16% on CIFAR10, VOC2012 and COCO, respectively. 1.

Introduction
Active learning is a kind of sampling algorithm that aims to reduce the annotation cost by helping the model to achieve better performance with fewer labeled training samples. In those areas with a limited annotation budget or the areas that need large amounts of labeled samples, active learning plays an important and irreplaceable role. How-ever, unlike the rapid progress of weakly supervised learn-ing and semi-supervised learning, the development of active
*This work was done during their internship at SenseTime Research.
†Equal contribution.
‡Corresponding author
Figure 1: Using UUIC to calculate the influence of unla-beled samples. These two samples will be annotated as
’Bird’ if they are selected. In UUIC, we calculate the in-fluence of the sample by calculating −∇θl(R, ˆθ)T H −1
Gzi.
ˆθ
The more negative the influence value is, the more positive influence on model performance the sample provides. Base on the result from UUIC, our ISAL algorithm selects the sample z1 for annotation. learning is limited. Especially in the computer vision area, most of the existing active learning algorithms are restricted to the image classification problem.
Given a pool of unlabeled images, different active learning algorithms evaluate the importance of each im-age with different criteria, which can be divided into uncertainty-based methods and diversity-based methods.
The uncertainty-based methods [19, 14, 34, 8, 40] use different criteria to evaluate the uncertainty of an image and select the images that the trained model is less con-fident about. However, the neural network shows over-confidence [13] toward the unfamiliar samples, indicating that using the uncertainty to estimate the samples’ impor-tance may not be accurate, deteriorating the performance of the active learning algorithm.
The diversity-based methods [24, 39, 10, 31] aim to se-lect a subset from the whole unlabeled dataset with the
largest diversity. These methods do not consider the model state. Besides, some of them need to measure the distance between each labeled image and each unlabeled image, meaning that their computation complexity are quadratic with respect to the size of the dataset. This disadvantage will become more apparent on the large-scale dataset.
In this paper,
In addition to image classification, object detection is also an important area that has large amounts of applica-tions. The annotation for the datasets [20, 35, 6] of object detection is extremely time-consuming. Thus, active learn-ing for object detection is well demanded. However, the re-search in active learning for object detection [30, 15, 4, 11] is rare and most of the proposed methods are designed for specific architecture, e.g., Faster R-CNN [27] or SSD [21]. instead of designing a task or even architecture-specific algorithms, we propose an algorithm that can be generally applied to different tasks and archi-tectures. There are already some successful attempts like the diversity-based coreset [31] and the uncertainty-based learning loss [40] algorithm, which proves that the general algorithm for active learning is possible. Unlike these two algorithms that select samples by measuring the feature dis-tance or the expected loss which are assumed to be corre-lated with the potential influence on the model, our method estimates the samples’ influence directly.
Our method,
Influence Selection for Active Learn-ing(ISAL), selects samples with the most positive influ-ence, i.e. the model performance will be enhanced most by adding this sample with full annotation into the labeled dataset. The influence measurement was first proposed by
Cook [3] for robust statistics. However, the scenario for the influence estimation in our work is entirely different.
In our case, the samples are unlabeled and untrained. We design the Untrained Unlabeled sample Influence Calcula-tion(UUIC) to calculate the influence of the unlabeled and untrained sample by estimating its expected gradient. Fig-ure. 1 shows how UUIC evaluates unlabeled samples and helps ISAL select samples. Since UUIC just needs to use the model gradients, which can be easily obtained in a neu-ral network no matter what task is and how complex the model structure is, our proposed ISAL is task-agnostic and model-agnostic.
ISAL achieves state-of-the-art performance among all comparing active learning algorithms for both the image classification and object detection task in the commonly used active learning setting with different representative datasets. Our method saves 12%, 13%, 16% annota-tion than the best comparing methods in CIFAR10 [17],
VOC2012 [6] and COCO [20], respectively. In addition, the existing methods for object detection perform better than random sampling only when the trained model’s perfor-mance is far lower than the ones trained on the full dataset, indicating that some selected samples may not be the best choice. Thus, we apply ISAL to a large-scale active learning setting for object detection. ISAL decreases the annotation cost at least by 8% than all comparing methods when the detector reaches 94.4% performance of the model trained on the full COCO dataset.
The contribution of this paper is summarized as follows: 1. We propose Influence Selection for Active Learn-ing(ISAL), a task-agnostic and model-agnostic active learning algorithm, which selects samples based on the calculated influence. 2. We design the Untrained Unlabeled sample Influence
Calculation(UUIC), a method to calculate the influ-ence of the unlabeled and untrained sample by estimat-ing its expected gradient. To validate UUIC’s effec-tiveness, we provide both theoretical and experimental analyses. 3. ISAL achieves state-of-the-art performance in differ-ent experiment settings for both image classification and object detection. 2.