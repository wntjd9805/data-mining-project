Abstract
Gaze following, i.e., detecting the gaze target of a hu-man subject, in 2D images has become an active topic in computer vision. However, it usually suffers from the out of frame issue due to the limited field-of-view (FoV) of 2D im-ages. In this paper, we introduce a novel task, gaze follow-ing in 360-degree images which provide an omnidirectional
FoV and can alleviate the out of frame issue. We collect the first dataset, “GazeFollow360”1, for this task, contain-ing around 10,000 360-degree images with complex gaze behaviors under various scenes. Existing 2D gaze follow-ing methods suffer from performance degradation in 360-degree images since they may use the assumption that a gaze target is in the 2D gaze sight line. However, this as-sumption is no longer true for long-distance gaze behav-iors in 360-degree images, due to the distortion brought by sphere-to-plane projection. To address this challenge, we propose a 3D sight line guided dual-pathway framework, to detect the gaze target within a local region (here) and from a distant region (there), parallelly. Specifically, the lo-cal region is obtained as a 2D cone-shaped field along the 2D projection of the sight line starting at the human sub-ject’s head position, and the distant region is obtained by searching along the sight line in 3D sphere space. Finally, the location of the gaze target is determined by fusing the estimations from both the local region and the distant re-gion. Experimental results show that our method achieves significant improvements over previous 2D gaze following methods on our GazeFollow360 dataset. 1.

Introduction
Gaze behavior is an essential part of human behavior, which is significant in studying human social behavior, human-object interaction [23, 22, 10, 40, 27, 32, 11, 9, 44, 2, 13]. Gaze following [36], has been an active topic in the computer vision community, whose purpose is to predict the
*Corresponding author: Guangtao Zhai, Wei Shen 1The dataset is at https://michaelliyunhao.github.io/here-or-there location where each human subject in a scene is looking at, given a 2D image containing one or more human subjects.
Rapid developments have been witnessed for gaze fol-lowing methods [7, 26, 47, 8, 14], but they are restricted in 2D images or 2D videos, which easily suffer from the situ-ation that gaze targets are out of frame, due to the limited field-of-view (FoV), as shown in Fig. 1(left). It is hard to perceive a whole surrounding scene in a 2D image. Unlike 2D images, 360-degree images capture the entire viewing sphere surrounding the optical center of a camera, which alleviates this issue. In addition, 360-degree images have gradually been utilized in various scenes. For instance, au-tonomous driving systems take 360-degree images as the input, and thus gaze following in 360-degree images can be used for human behavior understanding, such as human mo-tion prediction, which can help detect the human attention to avoid traffic crash. Together with the fact that the prices of 360-degree cameras (e.g., Ricoh Theta S, Samsung Gear 360) have been reduced, it becomes promising to conduct gaze following research in 360-degree images.
In light of these facts, in this paper, we propose a new task: gaze following in 360-degree images. Compared with gaze following in 2D images, two challenges are encoun-tered in this task: (1) Current gaze following approaches are deep learning based which are data driven, but there is no public available large dataset for gaze following in 360-degree images. (2) Previous 2D gaze following methods are built upon the assumption that a gaze target should be in the 2D sight line of the human subject in the 2D image plane coordinate, as shown in Fig. 1(middle), while this as-sumption is no longer true for long-distance gaze behaviors in 360-degree images, due to sphere-to-plane projection, as shown in Fig. 1(right).
To deal with the first challenge, we establish the first large scale dataset “GazeFollow360” for gaze following in 360-degree images by collecting 360-degree images from real world scenes, including various indoor and outdoor scenes. Our dataset contains around 10,000 high quality human-gazing target annotation pairs. Each gaze target lo-Figure 1. Left: The gaze target of the human subject in the 2D image is out-of-frame, due to the limited field-of-view of the 2D image.
Middle: In a 2D image, the gaze target of a human subject is in his 2D sight line, since perspective projection preserves straight lines.
Right: In a 360-degree image, this property still holds for short-distance gaze behaviors, while it is no longer true for long-distance gaze behaviors, due to the large distortion brought by sphere-to-plane projection. Thus our method copes with these two conditions parallelly by proposing a dual-pathway network. cation is annotated by around 4 human labelers, and the fi-nal annotated location is the average. The dataset covers a wide range of potential application scenarios such as class-rooms and sitting rooms, which can encourage development on gaze following in 360-degree images.
In addition, to address the second challenge, we pro-pose a sight line guided dual-pathway framework for gaze following in 360-degree images. The second challenge is caused by the mismatch between the 2D sight line of a human subject and the gaze target to be looked at in 2D images, due to sphere-to-plane projection. The mismatch occurs when the human subject performs a long-distance gaze behavior, since the distortion brought by sphere-to-plane projection is large at this situation. While for a short-distance gaze behavior, the gaze target locates within a local region around the human subject’s head. This local region on the sphere can be approximated by a plane, thus the dis-tortion can be negligible and the assumption that the gaze target is in the 2D sight line still holds.
Based on these observations, we model the gaze sight line in 3D sphere space rather than in 2D image plane coor-dinate which avoids sphere-to-plane projection and reflects the propagation of sight line in real world more naturally.
Guided by the predicted 3D gaze sight line, we propose a dual-pathway framework detects the gaze target within a local region (here) and from a distant region (there), par-allelly. Concretely, given a human subject’s head image, the direction of the sight line (gaze direction) is first esti-mated. Then, the local region is obtained as a 2D cone-shaped field along the gaze direction starting at the human subject’s head position, and the distant region is obtained by searching along the gaze direction in 3D sphere space. Af-terwards, gaze target estimation becomes attention guided saliency detection in both the local region and the distant region. The attention value at a position in the local region is inversely proportional to its angular difference to the sight line and that in the distant region is inversely proportional to its distance to the interaction between the sight line and the 3D sphere. Finally, the location of the gaze target is deter-mined by fusing the estimations from both the local region and the distant region.
Our framework is inspired by the human perception pro-cess. To infer the gaze target of a human subject, humans used to first estimate a rough gaze direction of the human subject, then infer the possible regions of the gaze target, and finally confirm the location of the gaze target within the possible regions according to image content, such as object saliency.
The contributions of our paper are three-fold: (1) To our best knowledge, this is the first work that studies gaze fol-lowing in 360-degree images. (2) We establish “GazeFol-low360”, the first large-scale dataset for gaze following in 360-degree images which contains 10,058 4K high resolu-tion images with annotations of heads and gaze targets. (3)
We propose a sight line guided dual-pathway framework to address the mismatch between the sight line of a human subject and the gaze target in 360-degree images. 2.