Abstract
The performance of a computer vision model depends on the size and quality of its training data. Recent stud-ies have unveiled previously-unknown composition biases in common image datasets which then lead to skewed model outputs, and have proposed methods to mitigate these bi-ases. However, most existing works assume that human-generated annotations can be considered gold-standard and unbiased. In this paper, we reveal that this assumption can be problematic, and that special care should be taken to pre-vent models from learning such annotation biases. We focus on facial expression recognition and compare the label bi-ases between lab-controlled and in-the-wild datasets. We demonstrate that many expression datasets contain signifi-cant annotation biases between genders, especially when it comes to the happy and angry expressions, and that tradi-tional methods cannot fully mitigate such biases in trained models. To remove expression annotation bias, we propose an AU-Calibrated Facial Expression Recognition (AUC-FER) framework that utilizes facial action units (AUs) and incorporates the triplet loss into the objective function. Ex-perimental results suggest that the proposed method is more effective in removing expression annotation bias than exist-ing techniques. 1.

Introduction
Computer vision models rely heavily on large sets of training images. Unfortunately, most datasets are “biased” in one way or another [58]. Traditional (i.e., lab-controlled) datasets are often too small and not diverse enough to train a robust model. Recently, many large-scale image datasets have been created through web-scraping and crowdsourced annotations [16, 95]. While this practice helps researchers collect millions of diverse “in-the-wild” images rapidly at low cost, it also introduces an undesired problem of dataset bias [79, 78, 64]. To mitigate the problem of biases effec-tively, we need to know (1) what causes biases (source), (2) which specific problems, datasets, or models suffer from bi-ases, and (3) which methods are effective in each situation.
Machine learning models, unless explicitly modified, have been shown to be capable of learning bias from data [23] and, consequently, to produce biased outcomes against cer-tain groups of people, undermining fairness and social trust of AI systems [83, 30, 94, 8, 9, 18, 26, 48].
We consider the scenario of supervised learning. Let
X = {Xi}N i=1 denote the collection of input images, and
Y = {Yi}N i=1 be the set of labels. A dataset is unbiased if the joint distribution P (X, Y ) matches reality. In particular, this requires the annotated labels, Y |X, to be unbiased.
For tabulated data, label bias is a classical focus in the fairness literature, where machine learning models are ap-plied to some historically discriminatory data whose labels are unfair to certain racial or gender groups [65], such as recidivism prediction [12], loan approval, and employment decisions [89, 63]. For crowdsourced image annotations, however, it is often assumed that the annotations are not sys-tematically biased. Each annotator may have their personal biases, and there may be labeling mistakes, but given the di-versity and large size of the data, they are generally assumed to be just another component of random noises [6, 96].
In reality, however, it is unlikely that people’s biases are all idiosyncratic. In fact, annotators may possess systematic cultural or societal biases, and if not specifically trained, they may incorporate such biases into their annotations. As a result, models trained on such data will become unfair. In this paper, we investigate the presence of systematic anno-tation bias in large in-the-wild datasets. We focus on the task of facial expression recognition. Fairness in expression recognition has not received wide attention [87, 70], yet it has a profound impact: more and more companies nowa-days conduct video job interviews in which algorithms are used to evaluate applicants’ facial expressions, voice, and word selection to predict their skills, behaviors, and per-sonality traits [90, 67, 28]; in addition, automated emotion analysis is already ubiquitous and used in consumer anal-ysis, content recommendation, clinical psychology, lie de-tection, pain assessment, and many other human computer
interfaces (e.g. “smile” shutters) [76, 68].
AUs and body keypoints).
In the context of facial expression recognition, studies in psychology have shown that human observers are more likely to perceive women’s faces as happier than men’s faces even when their smiles have the same intensity [75], and it is believed that raters hold cultural stereotypes and that these stereotypes influence the judgment of emotions
[31, 46]. We hypothesize that such bias is present in many in-the-wild expression datasets whose labels are annotated by non-experts. In particular, we seek to answer the ques-tion: “Are annotators equally likely to assign different ex-pression labels between males and females?” As we will show, for subjective tasks such as facial expression recog-nition, image annotations can be systematically biased, and special efforts need to be taken to address such bias.
We note that, currently, most debiasing techniques in the deep learning literature focus on biases that come from the images themselves (i.e., the bias in the distribution P (X)).
This is often known as “dataset bias” [79, 78] or “sample selection bias” [64]. It happens when the dataset is biased in its composition of images. As a result, models trained on one dataset do not generalize well to the real world due to the domain shift between the source and target. The trained model can also have undesirable accuracy differ-ences across different groups or classes [9]. Additionally, the data may contain spurious or undesirable correlations.
When such undesirable correlation involves protected at-tributes (e.g., gender, race, or age), the model is consid-ered “unfair.” Numerous methods have been proposed to decorrelate these attributes and ensure that models trained on such data do not discriminate people based on their pro-tected attributes [71, 2, 34, 53, 62, 11, 60, 38].
However, debiasing P (X) does not solve all problems since the joint distribution P (X, Y ) will still be biased if the annotated labels, P (Y |X), are biased. As we will demon-strate in this paper, existing techniques that are designed to mitigate data composition bias fail to fully mitigate the bias that comes from annotations. On the other hand, classical methods that address label bias are intrusive in that they of-ten involve changing the labels prior to training [55, 39].
In this paper, we address annotation bias that arises in facial expression recognition tasks. We propose an AU-Calibrated
Facial Expression Recognition (AUC-FER) framework that uses the facial action units (AUs) and incorporates the triplet loss into the objective function to learn an embedding space in which the expressions are classified similarly for peo-ple with similar AUs. We demonstrate that the presented method more effectively mitigates annotation biases than existing methods. We note that although our framework is designed for facial expression recognition, it can be adapted to many other applications that require subjective human la-beling such as activity recognition or image captioning but some fair or objective measures are available (such as the
The contribution of this paper is threefold:
• We compare the existence of annotation bias between lab-controlled datasets and in-the-wild datasets for fa-cial expression recognition and observe that in-the-wild datasets often contain significant systematic bias in their annotations. To the best of our knowledge, this is the first work to demonstrate the effect of systematic annotation bias associated with image data.
• We further demonstrate that such systematic annota-tion bias will be learned by trained models and thus cannot be ignored as is often assumed in the literature.
• We propose a novel AU-Calibrated Facial Expression
Recognition (AUC-FER) framework that utilizes facial action units to remove expression annotation bias. Ex-periments suggest that it outperforms existing debias-ing techniques for removing annotation bias. 2.