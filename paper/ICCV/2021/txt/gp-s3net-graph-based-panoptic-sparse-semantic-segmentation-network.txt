Abstract
Panoptic segmentation as an integrated task of both static environmental understanding and dynamic object identiﬁcation, has recently begun to receive broad research interest. In this paper, we propose a new computationally efﬁcient LiDAR based panoptic segmentation framework, called GP-S3Net. GP-S3Net is a proposal-free approach in which no object proposals are needed to identify the ob-jects in contrast to conventional two-stage panoptic sys-tems, where a detection network is incorporated for cap-turing instance information. Our new design consists of a novel instance-level network to process the semantic results by constructing a graph convolutional network to identify objects (foreground), which later on are fused with the back-ground classes. Through the ﬁne-grained clusters of the foreground objects from the semantic segmentation back-bone, over-segmentation priors are generated and subse-quently processed by 3D sparse convolution to embed each cluster. Each cluster is treated as a node in the graph and its corresponding embedding is used as its node feature. Then a GCNN predicts whether edges exist between each cluster pair. We utilize the instance label to generate ground truth edge labels for each constructed graph in order to supervise the learning. Extensive experiments demonstrate that GP-S3Net outperforms the current state-of-the-art approaches, by a signiﬁcant margin across available datasets such as, nuScenes and SemanticPOSS, ranking 1st on the competi-tive public SemanticKITTI leaderboard upon publication. mantic/instance segmentation and more recently panoptic segmentation. In recent years, deep learning has been used as the main solution to object detection and segmentation.
With more granular data available, specially from high res-olution LiDARs, the task of semantic/instance segmenta-tion has become a more vital part of any perception mod-ule due to generating all the fundamental perceptual infor-mation needed for robotic applications such as free space, parking area and vegetation, in addition to all dynamic ob-jects in 3D coordinates.
DS-Net
TORNADO-Net
Ours
Ground Truth 1.

Introduction
One of the main tasks in realizing autonomy in robotic applications is scene understanding using the available data collected from sensors such as camera, Light Detection
And Ranging (LiDAR) and RAdio Detection and Ranging (RADAR). Scene understanding can be divided into differ-ent tasks such as scene classiﬁcation, object detection, se-∗Indicates equal contribution.
†Emails:
{ryan.razani, ran.cheng1, thomas.enxu.li, ehsan.taghavi, yuan.ren3, liu.bingbing}@huawei.com
Figure 1: Comparison of our proposed method with DS-Net
[15] and TORNADONet [11] on SemanticKITTI bench-mark [1]. DS-Net and TORNADONet suffer from over-segmentation problem, while ours shows successful panop-tic predictions.
Normally, the task of semantic segmentation and in-stance segmentation are treated separately, using two dif-ferent DNN models.
In contrast, panoptic segmentation combines semantic and instance segmentation tasks as ex-plained in [19] by deﬁning two different categories of model predictions, namely, “things” and “stuff”. “Things” is re-ferred to all countable objects such as cars, pedestrians, bikes, etc. and “stuff” is referred to uncountable seman-tics (background) such as road, vegetation, parking. Then, the panoptic task is deﬁned by predicting the things and stuff together in one uniﬁed solution. Although the panop-tic segmentation deﬁnition seems plausible, the question of
“Is there a way to integrate independent semantic segmen-tation and target recognition into a system so that the in-stance level segmentation of objects can effectively use the results of semantic segmentation?” has not been addressed adequately in the literature.
In this paper we present a novel approach to solve the panoptic segmentation problem solely based on Li-DAR that can achieve state-of-the-art panoptic and se-mantic segmentation results on various benchmarks such as SemanticKITTI [1], nuScenes [2], and SemanticPOSS
[24]. Our method uses graph network to generate instances directly from over-segmented clusters obtained by HDB-SCAN [3] that takes in predicted foreground points from the semantic segmentation backbone.
Instead of learning the offset vector of each cluster and obtaining instances by re-clustering, a connectivity probability of each edge is learned in the graph. Over-segmented clusters are aggre-gated together by these connected edges to form instances.
Thus, conventional clustering problem is transformed into a supervised edge classiﬁcation problem. By doing so, we introduce the following contributions:
• A ﬂexible panoptic segmentation framework to beneﬁt from the best available semantic segmentation models and their output.
• A novel clustering and graph convolutional neural net-work that generates instance-level results (things and their IDs).
• A seamless fusion of semantic and instance level re-sults to generate panoptic segmentation predictions.
• A thorough experimental results of three major out-door datasets and an ablation study to show the effec-tiveness of the proposed solution. 2.