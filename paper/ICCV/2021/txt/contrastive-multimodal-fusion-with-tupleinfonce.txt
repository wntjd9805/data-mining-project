Abstract
This paper proposes a method for representation learning of multimodal data using contrastive losses. A traditional approach is to contrast different modalities to learn the infor-mation shared among them. However, that approach could fail to learn the complementary synergies between modal-ities that might be useful for downstream tasks. Another approach is to concatenate all the modalities into a tuple and then contrast positive and negative tuple correspondences.
However, that approach could consider only the stronger modalities while ignoring the weaker ones. To address these issues, we propose a novel contrastive learning objective,
TupleInfoNCE. It contrasts tuples based not only on posi-tive and negative correspondences, but also by composing new negative tuples using modalities describing different scenes. Training with these additional negatives encourages the learning model to examine the correspondences among modalities in the same tuple, ensuring that weak modalities are not ignored. We provide a theoretical justiﬁcation based on mutual-information for why this approach works, and we propose a sample optimization algorithm to generate posi-tive and negative samples to maximize training efﬁcacy. We
ﬁnd that TupleInfoNCE signiﬁcantly outperforms previous state of the arts on three different downstream tasks. 1.

Introduction
Human perception of the world is naturally multimodal.
What we see, hear, and feel all contain different kinds of information. Various modalities complement and disam-biguate each other, forming a representation of the world.
Our goal is to train machines to fuse such multimodal inputs to produce such representations in a self-supervised manner without manual annotations.
An increasingly popular self-supervised representation learning paradigm is contrastive learning, which learns fea-ture representations via optimizing a contrastive loss and solving an instance discrimination task [21, 10, 4]. Recently
Tuple Augmentation Hyper-parameter Updates
Tuple 
Augmentation
Positive Samples
Multimodality Inputs
Anchor Samples
Tuple
Disturbing
Negative Samples
Multimodal 
Feature Encoder
Shared  Weights
Multimodal 
Feature Encoder
Shared  Weights
Multimodal 
Feature Encoder
TupleInfoNCE
Tuple Disturbing Hyper-parameter Updates
Unsupervised
Feature Evaluation
Figure 1. Overview of sample-optimized TupleInfoNCE. several works have explored contrastive learning for mul-timodal representation learning [28, 1, 18]. Among them, the majority [28, 1] learn a crossmodal embedding space – they contrast different modalities to capture the information shared across modalities. However, they do not examine the fused representation of multiple modalities directly, fail-ing to fully leverage multimodal synergies. To cope with this issue, [18] proposes an RGB-D representation learn-ing framework to directly contrast pairs of point-pixel pairs.
However, it is restricted to two modalities only.
Instead of contrasting different data modalities, we pro-pose to contrast multimodal input tuples, where each tuple element corresponds to one modality. We learn representa-tions so that tuples describing the same scene (set of multi-modal observations) are brought together while tuples from different scenes are pushed apart. This is more general than crossmodal contrastive learning. It not only supports extract-ing the shared information across modalities, but also allows modalities to disambiguate each other and to keep their spe-ciﬁc information, producing better-fused representations.
However, contrasting tuples is not as straightforward as contrasting single elements, especially if we want the learned representation to encode the information from each element in the tuple and to fully explore the synergies among them.
The core challenge is: “which tuple samples to contrast?”
Previously researchers [34, 18] have observed that always contrasting tuples containing corresponding elements from
the same scene can converge to a lazy suboptimum where the network relies only on the strongest modality for scene discrimination. Therefore to avoid weak modalities being ignored and to facilitate modality fusion, we need to contrast from more challenging negative samples. Moreover, we need to optimize the positive samples as well so that the contrastive learning can keep the shared information between positive and anchor samples while abstracting away nuisance factors. Strong variations between the positive and anchor samples usually result in smaller shared information but a greater degree of invariance against nuisance variables. Thus a proper tradeoff is needed.
To handle the above challenges, we propose a novel con-trastive learning objective named TupleInfoNCE (Figure 1).
Unlike the popular InfoNCE loss [21], TupleInfoNCE is designed explicitly to facilitate multimodal fusion. TupleIn-foNCE leverages positive samples generated via augmenting anchors and it exploits challenging negative samples whose elements are not necessarily in correspondence. These neg-ative samples encourage a learning model to examine the correspondences among elements in an input tuple, ensuring that weak modalities and the modality synergy are not ig-nored. To generate such negative samples we present a tuple disturbing strategy with a theoretical basis for why it helps.
TupleInfoNCE also introduces optimizable hyper-parameters to control both the negative sample and the pos-itive sample distributions. This allows optimizing samples through a hyper-parameter optimization process. We deﬁne reward functions regarding these hyper-parameters and mea-sure the quality of learned representations via unsupervised feature evaluation. We put unsupervised feature evaluation in an optimization loop that updates these hyper-parameters to ﬁnd a sample-optimized TupleInfoNCE (Figure 1).
We evaluate TupleInfoNCE on a wide range of multi-modal fusion tasks including multimodal semantic segmen-tation on NYUv2 [26], multimodal object detection on SUN
RGB-D [27] and multimodal sentiment analysis on CMU-MOSI [35] and CMU-MOSEI [36]. We demonstrate signiﬁ-cant improvements over previous state-of-the-art multimodal self-supervised representation learning methods (+4.7 mIoU on NYUv2, +1.2 mAP@0.25 on SUN RGB-D, +1.0% acc7 on MOSI, and +0.5% acc7 on MOSEI). 2.