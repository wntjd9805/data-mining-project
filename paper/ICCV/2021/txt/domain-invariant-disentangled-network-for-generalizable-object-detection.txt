Abstract
We address the problem of domain generalizable object detection, which aims to learn a domain-invariant detec-tor from multiple “seen” domains so that it can generalize well to other “unseen” domains. The generalization abil-ity is crucial in practical scenarios especially when it is difficult to collect data. Compared to image classification, domain generalization in object detection has seldom been explored with more challenges brought by domain gaps on both image and instance levels.
In this paper, we pro-pose a novel generalizable object detection model, termed
Domain-Invariant Disentangled Network (DIDN). In con-trast to directly aligning multiple sources, we integrate a disentangled network into Faster R-CNN. By disentangling representations on both image and instance levels, DIDN is able to learn domain-invariant representations that are suitable for generalized object detection. Furthermore, we design a cross-level representation reconstruction to com-plement this two-level disentanglement so that informative object representations could be preserved. Extensive exper-iments are conducted on five benchmark datasets and the results demonstrate that our model achieves state-of-the-art performances on domain generalization for object de-tection. 1.

Introduction
Object detection is a fundamental yet challenging prob-lem in computer vision. It aims to identify and localize all object instances of certain categories in an image. In the past few years, we have witnessed significant breakthroughs of supervised object detection [9, 2, 30, 24, 10, 3] on various benchmark datasets [17, 33, 8, 42]. Nonetheless, perform-ing object detection in practice remains challenging due to the complexity and diversity of natural scenes. Learning a general object detector requires collecting a large amount data, which is highly expensive in real word scenarios with various domains. An alternative is to transfer the learned
*This work was performed while Chuang Lin worked as an intern at
ByteDance.
Figure 1. An illustration of our approach for domain generaliz-able object detection. If directly extending prior domain adapta-tion methods [12, 46, 40, 32, 47, 1] to the unseen domain, it will fail to generalize to the unseen domain since there is no data avail-able in training. Our method first extracts the domain-independent object content to avoid fully matching all source domains. Fur-ther, we learn a shared feature space for domain generalization with preserved informative object representation. knowledge from labeled source domains to another differ-ent but related target domain. However, because of the pres-ence of dataset bias or domain shift [35], i.e. the joint prob-ability distributions of observed data and labels are different in different domains, direct transfer may not perform well.
Unsupervised domain adaptation (UDA) is one of the most popular attempts to remedy this problem and consid-erable efforts have been made [12, 46, 40, 32, 47, 1, 15].
Given the well labeled source data and the known target data without labels, the idea of UDA is to align the data distribution between the source and target domains so that the trained model on the source can well generalize to the target [45]. However, these methods still require pre-collecting target data and retraining the model for different target domains. Therefore, it is difficult to extend domain adaptation methods to the scenarios where target data is un-available.
In this paper, we focus on domain generalizable object
detection, a more general problem which does not rely on target data and aims at learning a universal object detector directly from multiple source domains. Hopefully, the de-tector could perform well on any previously “unseen” target domains. The main challenge of generalizable object detec-tion still lies in the notorious domain shift across multiple domains. On one hand, the domain shift is not only mani-fested on the image level (e.g, weather, time, scene layouts, etc.), but also on the instance level (e.g, object appearance, size, etc.). The resulting model needs to learn invariant rep-resentations on both levels. On the other hand, prior meth-ods on domain adaptation align feature space by directly matching the distributions between the source domains and the known target domain, which is available during training.
However, when target data is not observable, only matching multiple source domains would be insufficient for gener-alized object detection, because it might not learn a well aligned feature space for an unseen target domain, as illus-trated in Figure 1.
To address the above challenges, in this paper we pro-pose a novel framework for generalizable object detection.
Inspired by the recent disentangled works [28] for image translation, we propose a Domain-Invariant Disentangled
Network (DIDN) to learn a universal object detector. The network consists of three components:
Image-level Dis-entanglement, Instance-level Disentanglement and Cross-level Reconstruction.
Image-level and instance-level dis-entanglements aim to explicitly disentangle representation spaces to domain-independent and domain-exclusive parts.
By integrating them into Faster R-CNN framework, the two-level disentanglement enables DIDN to extract gener-alized features suitable for object detection. We believe the consistent representation of objects in the two levels is more helpful to preserve the informative features for object de-tection. We further enforce a cross-level reconstruction to complement the detection model, since the two-level disen-tanglements are independent of each other.
In summary, the contributions of this paper are threefold: 1) We propose to generalize object detection from mul-tiple sources to a previously unseen domain. To the best of our knowledge, this is the first work to explore domain generalization for object detection. 2) We develop a novel end-to-end learning framework termed DIDN, to learn domain-invariant representation on both image level and instance level for generalizable object detection. 3) We conduct extensive experiments on multiple bench-mark datasets. DIDN outperforms the best baseline in terms of mAP by 2.2%, 2.1%, and 3.1% on Cityscapes, Foggy
Cityscapes and BDD100k, respectively. 2.