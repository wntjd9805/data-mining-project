Abstract
RGB-D based 6D pose estimation has recently achieved remarkable progress, but still suffers from two major limita-tions: (1) ineffective representation of depth data and (2) in-sufficient integration of different modalities. This paper pro-poses a novel deep learning approach, namely Graph Con-volutional Network with Point Refinement (PR-GCN), to si-multaneously address the issues above in a unified way. It first introduces the Point Refinement Network (PRN) to pol-ish 3D point clouds, recovering missing parts with noise re-moved. Subsequently, the Multi-Modal Fusion Graph Con-volutional Network (MMF-GCN) is presented to strengthen
RGB-D combination, which captures geometry-aware inter-modality correlation through local information propagation in the graph convolutional network. Extensive experiments are conducted on three widely used benchmarks, and state-of-the-art performance is reached. Besides, it is also shown that the proposed PRN and MMF-GCN modules are well generalized to other frameworks. 1.

Introduction 6D pose estimation aims to predict the orientation and lo-cation of an object in the 3D space from a canonical frame.
It has received extensive attention in computer vision, since it is one of the fundamental steps for a wide range of appli-cations, such as robotics grasping [6, 35, 47] and augmented reality [22, 23]. Traditional methods [10, 11] attempt to ac-complish this task based on RGB images only. They adopt handcraft features (e.g. SIFT [21] and SURF [1]) to estab-lish correspondence between input and canonical images.
Inspired by the great success in detection/recognition, deep neural networks are recently explored to address this issue, including the single-stage regression methods [15] and key-point based methods [13, 32, 36, 35, 26, 25, 20]. Despite the remarkable promotion in accuracy, RGB-based deep mod-*indicates the corresponding author.
Figure 1. Example object: (a) / (b) are RGB / depth images; (c)/(d) are generated incomplete noisy point cloud and ground-truth. els heavily rely on textures; thus sensitive to illumination variations, severe occlusions, and cluttered backgrounds.
Along with the emergence and innovation of depth sen-sors, 6D pose estimation on RGB-D data has become popu-lar, expecting to deliver performance gain by adding geom-etry information. Early works [11, 24, 45] estimate object poses from RGB images and refine them according to depth maps. Later studies [38, 17] dedicate to integrating RGB and depth clues in a more sophisticated way. Particularly,
[41, 31, 30, 38, 9] represent depth images as 3D point clouds and the models are more efficient in computation and stor-age than those on original depth maps. By jointly making use of both modalities, RGB-D based solutions report better scores, with the superiority in the presence of the difficulties aforementioned as well as the low-texture case.
However, current RGB-D pose estimation suffers from
two major limitations: ineffective representation of depth data and insufficient combination of two modalities. For the former, as captured in cluttered scenes, depth informa-tion is usually noisy and incomplete (see Fig. 1). Inferring poses from such data, either in 2D depth maps or 3D point clouds, is not robust, leading to accuracy deterioration. For the latter, RGB and depth clues are fused by concatenating separately learned single-modal features [38] or by applying a simple point-wise encoder [9], where inter-modality cor-relations are not considered or roughly modeled in a global manner, leaving much room for improvement.
In this paper, we propose a novel deep learning approach, namely Graph Convolutional Network with Point Refine-ment (PR-GCN), to simultaneously address the two limita-tions in a unified way. As in Fig. 2, given the RGB image and 3D point cloud (generated from depth map) of an ob-ject, we first introduce a Point Refinement Network (PRN) to polish the point cloud. Endowed with an encoder-decoder structure and trained with a regularized multi-resolution re-gression loss, PRN recovers the missing parts of the raw in-put with noise removed. Subsequently, we integrate RGB-D clues by a Mutli-Modal Fusion Graph Convolutional Net-work (MMF-GCN). It constructs a k-Nearest Neighbor (k-NN) graph and extracts geometry-aware inter-modality cor-relation through local information propagation in the Graph
Convolutional Network (GCN). An additional k-NN graph and GCN are employed to encode local geometry attributes of the refined point cloud as a complement to the original data. The features from the two GCNs are then combined and fed into several fully-connected layers for final 6D pose prediction. We extensively evaluate PR-GCN on three pub-lic benchmarks, Linemod [11], Occlusion Linemod [2], and
YCB-Video [40], and achieve the state-of-the-art perfor-mance. We also show that the proposed PRN and MMF-GCN modules are well generalized to other frameworks.
The contributions: 1) We propose the PR-GCN approach to 6D pose estimation by enhancing depth representation and multi-modal combination. 2) We present the PRN mod-ule with a regularized multi-resolution regression loss for point-cloud refinement. To the best of our knowledge, it is the first that applies 3D point generation to this task. 3) We develop the MMF-GCN module to capture local geometry-aware inter-modality correlation for RGB-D fusion. 2.