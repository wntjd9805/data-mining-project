Abstract
Localizing objects and estimating their extent in 3D is an important step towards high-level 3D scene understand-ing, which has many applications in Augmented Reality and Robotics. We present ODAM, a system for 3D Object
Detection, Association, and Mapping using posed RGB videos. The proposed system relies on a deep learning front-end to detect 3D objects from a given RGB frame and asso-ciate them to a global object-based map using a graph neu-ral network (GNN). Based on these frame-to-model asso-ciations, our back-end optimizes object bounding volumes, represented as super-quadrics, under multi-view geometry constraints and the object scale prior. We validate the pro-posed system on ScanNet where we show a signiﬁcant im-provement over existing RGB-only methods. 1.

Introduction
Endowing machine perception with the capability of in-ferring 3D object-based maps brings AI systems one step closer to semantic understanding of the world. This task re-quires building a consistent 3D object-based map of a scene.
We focus on the space between the category-level seman-tic reconstructions [29] and object-based maps with render-able dense object models [27, 45] and represent objects by the 3D bounding volumes from posed RGB frames. As an analogy to the use of 2D bounding boxes (BBs) in images, a 3D bounding volume presents a valuable abstraction of location and space, enabling for example, object-level plan-ning for robots [13, 15], learning scene-level priors over ob-jects [55], or anchoring information on object instances. A robust way of inferring bounding volumes and associated views of individual objects in a scene is a stepping stone to-ward reconstructing, embedding and describing the objects with advanced state-of-the-art methods such as NeRF [32], and GRAF [47], which commonly assume a set of associ-ated frames observing an object or part of a scene that can
Figure 1: ODAM overview. Given a posed RGB video,
ODAM estimates oriented 3D bounding volumes of objects represented by super-quadrics in a scene. be obtained from the proposed reconstruction system.
Nevertheless, this task of localizing objects and estimat-ing their extents in 3D using RGB-only videos presents a number of challenges. First, despite the impressive success of deep learning methods for 2D object detectors [7, 16, 43], recent efforts that formulate 3D object mapping as a single-view 3D detection problem [5, 24, 33] suffer from accu-racy due to the depth-scale ambiguity in the perspective projection (as demonstrated empirically in Sec. 4.2). Sec-ond, unlike estimation of 3D points from multiple 2D ob-servations that has been studied extensively in SfM and
SLAM [6, 14, 22, 34, 53], there has been little work and consensus on how to leverage multi-view constraints for 3D bounding volume location and extent [35, 60]. Speciﬁcally, the representation for 3D volume and how to formulate a suitable energy function remain open questions. Third, the crucial problem that needs to be solved prior to multi-view optimization is the associations of detections of individual 3D object instances from different viewpoints, where un-like SfM or SLAM incorrect association noticeably biases
the 3D object localization. However, this problem is under-explored for cluttered indoor environments, where speciﬁc problems such as having multiple objects with near iden-tical visual appearance and heavy occlusion (e.g., multiple chairs closely arranged in a room as can be seen in Fig. 6) are commonplace. Depth ambiguity and partial observa-tions complicate the data association problem.
We propose ODAM, a novel framework that incorpo-rates a deep learning front-end and multi-view optimization back-end to address 3D object mapping from posed RGB videos. The advantage of using RGB-only over RGB-D is signiﬁcantly less power consumption. We assume the poses of the images are known; these are readily available with modern mobile/AR devices. The front-end ﬁrst detects ob-jects of interest and predicts each object’s 2D attributes (2D
BB, object class), as well as its 3D BB parameterized by 6 Degree-of-Freedom (DoF) rigid pose and 3 DoF scale given a single RGB frame as shown in Fig. 2. The primary use of the 3D attributes for each detection is to facilitate data association between a new frame and the current global 3D map. Concretely we develop a graph neural network (GNN) which takes as inputs the 2D and 3D attributes of the cur-rent frames detections and matches them to existing object instances in the map. The front-end of can run 6 fps on av-erage on a modern GPU on cluttered scenes such as those in ScanNet [10].
The back-end of ODAM is a multi-view optimization that optimizes each object’s oriented bounding volume rep-resented by a super-quadric surface given multiple associ-ated 2D bounding box (BB) observations. Previous object-level SLAM frameworks have adopted either cuboids [60] or ellipsoids [18, 35] as their object representation, but they are often not a good model for the extent of a generic object as depicted in Fig. 3. Super-quadric – a uniﬁed representa-tion for shape primitives including cuboids, ellipsoids, and cylinders – permits blending between cuboids and ellipsoids (and cylinders) and can therefore provide a tight bound-ing volume for the multi-view optimization. While super-quadric has been used to ﬁt point cloud data [39, 40, 49] or recently parse object shapes from a single image using a deep network [38], we present the ﬁrst approach to op-timize super-quadrics given multiple 2D BB observations to the best of our knowledge. Besides the representation, we realize that the 2D BBs given by the object detector are not error free due to occlusions in cluttered indoor environ-ments. We incorporate category-conditioned priors in the optimization objective to improve the robustness.
Contribution. Our contributions are threefold: (1) we present ODAM, a novel online 3D object-based mapping system that integrates an deep-learning front-end running at 6 fps, and a geometry-based back-end. ODAM is the current best performing 3D detection and mapping RGB-only systems for complex indoor scenes in ScanNet [10]; (2) we present a novel method for associating single-view detections to the object-level. Our association employs a novel attention-based GNN taking as inputs the 2D and 3D attributes of the detections; (3) we identify the limita-tions of common 3D bounding volume representations used in multi-view optimization and introduce a super-quadric-based optimization under object-scale priors which shows clear improvements over previous methods. 2.