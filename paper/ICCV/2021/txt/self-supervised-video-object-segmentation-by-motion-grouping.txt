Abstract 1.

Introduction
Animals have evolved highly functional visual systems to understand motion, assisting perception even under com-plex environments. In this paper, we work towards develop-ing a computer vision system able to segment objects by ex-ploiting motion cues, i.e. motion segmentation. To achieve this, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background, which can be trained in a self-supervised man-ner, i.e. without using any manual annotations. Despite us-ing only optical flow, and no appearance information, as input, our approach achieves superior results compared to previous state-of-the-art self-supervised methods on public benchmarks (DAVIS2016, SegTrackv2, FBMS59), while be-ing an order of magnitude faster. On a challenging cam-ouflage dataset (MoCA), we significantly outperform other self-supervised approaches, and are competitive with the top supervised approach, highlighting the importance of motion cues and the potential bias towards appearance in existing video segmentation models.
When looking around the world, we effortlessly perceive a complex scene as a set of distinct objects. This phe-nomenon is referred to as perceptual grouping – the pro-cess of organizing the incoming visual information – and is usually considered a fundamental cognitive ability that enables understanding and interacting with the world effi-ciently. How do we accomplish such a remarkable percep-tual achievement, given that the visual input is, in a sense, just a spatial distribution of variously colored individual points/pixels? In 1923, Wertheimer [80] first introduced the
Gestalt principles with the goal of formulating the underly-ing causes by which sensory data is organized into groups, or Gestalten. The principles are much like heuristics with
“a bag of tricks” [58] that the visual system may exploit for grouping, for example, proximity, similarity, closure, con-tinuation, common fate, etc.
In computer vision, perceptual grouping is often closely related to the problem of segmentation, i.e. extracting the objects with arbitrary shape (pixel-wise labels) from clut-tered scenes. In the recent literature of semantic or instance
segmentation, tremendous progress has been made by train-ing deep neural networks on image or video datasets. While it is exciting to see machines with the ability to detect, seg-ment, and classify objects in images or video frames, train-ing such segmentation models through supervised learning requires massive human annotation, and consequently lim-its their scalability. Even more importantly, the assumption that objects can be well-identified by their appearance alone in static frames is often an oversimplification – objects are not always visually distinguishable from their background environment. For instance when trying to discover cam-ouflaged animals/objects from the background (Figure 1), extra cues, such as motion or sound, are usually required.
Among the numerous cues, motion is usually simple to obtain as it can be generated from unlabeled videos. In this paper, we aim to exploit such cues for object segmentation in a self-supervised manner, i.e. zero human annotation is required for training. At a high level, we aim to exploit the common fate principle, with the basic assumption being that elements tend to be perceived as a group if they move in the same direction at the same rate (have similar opti-cal flow). Specifically, we tackle the problem by training a generative model that decomposes the optical flow into foreground (object) and background layers, describing each as a homogeneous field, with discontinuities occurring only between layers. We adopt a variant of the Transformer [72], with the self-attention being replaced by slot attention [44], where iterative grouping and binding have been built into the architecture. With some critical architectural changes, we show that pixels undergoing similar motion are grouped together and assigned to the same layer.
To summarize, we make the following contributions: first, we introduce a simple architecture for video ob-ject segmentation by exploiting motions, using only opti-cal flow as input. Second, we propose a self-supervised proxy task that is used to train the architecture without any manual supervision. To validate these contributions, we conduct thorough ablation studies on the components that are key to the success of our architecture, such as a consistency loss on optical flow computed from vari-ous frame gaps. We evaluate the proposed architecture on public benchmarks (DAVIS2016 [55], SegTrackv2 [40], and FBMS59 [52]), outperforming previous state-of-the-art self-supervised models. Moreover, we also evaluate on a camouflage dataset (MoCA [39]), demonstrating a significant performance improvement over the other self-supervised approaches, with comparable performance to the best supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appear-ance in existing video segmentation models. 2.