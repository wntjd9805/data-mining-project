Abstract
In this paper, we present a novel approach to synthesize realistic images based on their semantic layouts. It hypoth-esizes that for objects with similar appearance, they share similar representation. Our method establishes dependen-cies between regions according to their appearance corre-lation, yielding both spatially variant and associated rep-resentations. Conditioning on these features, we propose a dynamic weighted network constructed by spatially con-ditional computation (with both convolution and normaliza-tion). More than preserving semantic distinctions, the given dynamic network strengthens semantic relevance, beneﬁt-ing global structure and detail synthesis. We demonstrate that our method gives the compelling generation perfor-mance qualitatively and quantitatively with extensive exper-iments on benchmarks. 1.

Introduction
Semantic image synthesis transforms the abstract seman-tic layout to realistic images, an inverse task of semantic segmentation (Figure 1). It is widely used in image manip-ulations and content creation. Recent methods on this task are built upon generative adversarial networks (GAN) [7], modeling image distribution conditioning on segmentation masks.
Despite its substantial achievement [14, 31, 24, 21, 29, 4, 26, 16, 30], this line of research is still challenging due to the high complexity of characterizing object distributions.
Recent advance [24, 21] on GAN-based image synthesis concentrates on how to exploit spatial semantic variance in the input for better preserving layout and independent semantics, leading to further generative performance im-provement. They both use different parametric operators to handle different objects.
Speciﬁcally, SPADE [24] proposes a spatial semantics-dependent denormalization operation for the common nor-malization, as the feature statistics are highly correlated with semantics. CC-FPSE [21] extends this idea to convo-lution using dynamic weights, generating spatially-variant
Figure 1. Semantic image synthesis results of our method on face and scene datasets. convolutions from the semantic layouts. Relationships be-tween objects are implicitly modeled by the weight-sharing convolutional kernels (SPADE) or hierarchical structures brought by stacked convolutions. We believe when per-forming semantic-aware operations, enhancing object rela-tionship could be further beneﬁcial to ﬁnal synthesis, since context and long-range dependency have proven effective in several vision tasks [6, 32, 43, 41, 34].
To explicitly build connection between objects and stuff in image synthesis, we propose a semantic encoding and stylization method, named semantic-composition genera-tive adversarial network (SC-GAN). We ﬁrst generate the semantic-aware and appearance-correlated representations from mapping the discrete semantic layout to their corre-sponding images.
Our idea is inspired by the following observation. Dif-ferent semantics are with labels in scene or face parsing datasets. Some of them are highly correlated in appearance, e.g., the left and right eyes in CelebAMask-HQ [19]. We abstract objects in images or feature maps into vectors by encoding the semantic layout, which we call semantic vec-tors. This intermediate representation is to characterize the relationship between different semantics based on their ap-pearance similarity.
With these semantic vectors, we create semantic-aware and appearance-consistent local operations in a semantic-stylization fashion. Consistent with the proposed seman-tic vectors, these dynamic operators are also variant in se-mantics and correlated in appearance. Our proposed op-erators are extended from existing conditional computation
[38] to stylize the input conditioned on the semantic vec-tors. Speciﬁcally, we exploit semantic vectors to combine a shared group of learnable weights, parameterizing the con-volutions and normalizations used in semantic stylization.
Note the learning of semantic vectors and render candi-dates is non-trivial. Intuitive designs that encode the seman-tic layout to semantic vectors for later dynamic computa-tions make semantic vectors stationary, since these seman-tic vectors are not directly regularized by the appearance information in the image. In this paper, we make the learn-ing of semantic encoding and stylization relatively indepen-dent. Semantic encoding is trained by estimating the corre-sponding natural images from the input semantic layouts by maximum likelihood. Semantic stylization is trained in an adversarial manner.
Our method is validated on several image synthesis benchmark datasets quantitatively and qualitatively. Also, the decoupled design of semantics encoding and stylization makes our method applicable to other tasks, e.g., unpaired image translation [46]. Our contribution is threefold.
• We propose a new generator design for GAN-based se-mantic image synthesis. We present spatially-variant and appearance-correlated operations (both convolu-tion and normalization) via spatially conditional com-putation, exploiting both local and global characteris-tics for semantic-aware processing.
• Our proposed generator with a compact capacity out-performs other popular ones on face and scene synthe-sis datasets in visual and numerical comparisons.
• The decoupled design in our method ﬁnds other appli-cations, e.g. unpaired image-to-image translation. 2.