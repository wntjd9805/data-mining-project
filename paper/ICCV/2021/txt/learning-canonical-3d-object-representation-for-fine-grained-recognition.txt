Abstract
We propose a novel framework for fine-grained object recognition that learns to recover object variation in 3D space from a single image, trained on an image collection without using any ground-truth 3D annotation. We accom-plish this by representing an object as a composition of 3D shape and its appearance, while eliminating the effect of camera viewpoint, in a canonical configuration. Unlike conventional methods modeling spatial variation in 2D im-ages only, our method is capable of reconfiguring the ap-pearance feature in a canonical 3D space, thus enabling the subsequent object classifier to be invariant under 3D geo-metric variation. Our representation also allows us to go beyond existing methods, by incorporating 3D shape vari-ation as an additional cue for object recognition. To learn the model without ground-truth 3D annotation, we deploy a differentiable renderer in an analysis-by-synthesis frame-work. By incorporating 3D shape and appearance jointly in a deep representation, our method learns the discrimi-native representation of the object and achieves competitive performance on fine-grained image recognition and vehi-cle re-identification. We also demonstrate that the perfor-mance of 3D shape reconstruction is improved by learning fine-grained shape deformation in a boosting manner. 1.

Introduction
Object recognition [34, 23, 17, 76] is one of the most fundamental and essential tasks in computer vision fields, which has achieved steady progress by the advent of deep convolutional neural networks. However, it still remains a challenging problem, especially when an object undergoes severe geometric deformations, e.g., by object scale, pose and part variations, which frequently occur across different instances, or by camera viewpoint changes [16, 25, 8, 27].
This work was supported by the National Research Foundation of
Korea (NRF) grant funded by the Korea government (MSIP) (NRF-2021R1A2C2006703), and the Yonsei University Research Fund of 2021 (2021-22-0001).
∗Corresponding author
Figure 1. Intuition of our method: Given (a) 2D image, we re-cover object variation in 3D space by estimating (b) 3D shape de-formation, (c) camera viewpoint change and (d) appearance vari-ation. It allows for using 3D shape and appearance in a canonical space, while eliminating camera viewpoint variation, enabling us to deal with 3D object variations and facilitating the subsequent object classifier.
To overcome these challenges, recent works [25, 39, 53, 7, 8] seek to handle such geometric variations based on an assumption that the object variation can be decomposed into appearance and 2D spatial variation. They first estimate an appearance flow from an input and then warp the input into a canonical configuration so as to remove the spatial variation, from which the appearance feature is extracted to facilitate the subsequent classifier’s task. The appear-ance flow is generally estimated by modeling 2D transfor-mation [25, 39, 53], e.g., affine transformation, or by learn-ing offset of sampling locations in the convolutional opera-tors [7, 8]. These methods, however, do not account for the fact that the object variation, given an image, is due to the variations in appearance, 3D shape and camera viewpoint as in Fig. 1. While the effect of camera viewpoint should be eliminated for achieving geometric invariance, 3D shape variation can be used as an additional cue to extract a shape feature that is able to supplement an appearance feature, but none of the existing methods utilize this.
However, estimating 3D object information from a sin-gle image is challenging, since collecting the ground-truth 3D shape is notoriously difficult and time-consuming [69], thus limiting the supervised learning for this task. To over-come this, some methods implicitly consider the 3D object
structure by learning a discriminative feature representation for fine-grained recognition. Formally, they use an extra module to localize the discriminative object parts [33, 20], by using explicit part detectors [1, 32] or implicit attention mechanisms [17, 75]. However, these methods can only lo-calize a few semantic parts without understanding the holis-tic object structure, and can be limited if the network fails to consistently localize object parts across multiple instances.
In this paper, we present a method that estimates 3D ob-ject information in a canonical configuration, including 3D object shape and appearance, with camera viewpoint. It en-ables the subsequent classifier to directly work on the 3D object information, from which both appearance and shape features are simultaneously extracted. It allows for handling subtle intra-class variations by means of both appearance and 3D shape features, which is not available at the existing approaches. To this end, we deploy a differentiable ren-derer [29, 43], to infer 3D shape, without ground-truth 3D annotation, in an analysis-by-synthesis framework, as in re-cent 3D shape reconstruction methods [28, 24, 56, 49]. In particular, we incorporate this framework into an encoder-decoder architecture that disentangles the object variation to 3D shape, appearance, and camera viewpoint. To this end, an image is embedded into a low-dimensional latent code that is fed into separate decoders to estimate the afore-mentioned factors independently. We also exploit multiple hypothesis camera prediction to avoid local minima during training as in [24, 35, 19].
Unlike conventional methods [25, 39, 53] that model 2D spatial variation only, our method is capable of reconfigur-ing an appearance feature in a canonical space, where each semantic part of an object is mapped to the same location in a canonical space. Moreover, our method enables dense se-mantic alignment [73] into a canonical configuration, where positional encoding [42, 2] can further improve recognition performance, while the conventional methods [1, 32, 17, 75] are limited by highlighting only a few salient parts of an ob-ject. To improve recognition ability between subtle object variations, we further introduce a shape encoder to utilize 3D shape deformation as an additional cue. By incorporat-ing 3D shape and appearance jointly in a deep representa-tion, our method consistently boosts the discriminative rep-resentation learning on fine-grained image recognition and vehicle re-identification tasks. In addition, our joint learn-ing framework enables us to improve 3D shape reconstruc-tion capability by discriminating shape variations between different fine-grained categories. 2.