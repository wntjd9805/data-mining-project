Abstract
This paper presents a novel task together with a new benchmark for detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. Con-ventional work in temporal video segmentation and ac-tion detection focuses on localizing pre-deﬁned action cat-egories and thus does not scale to generic videos. Cogni-tive Science has known since last century that humans con-sistently segment videos into meaningful temporal chunks.
This segmentation happens naturally, without pre-deﬁned event categories and without being explicitly asked to do so.
Here, we repeat these cognitive experiments on mainstream
CV datasets; with our novel annotation guideline which ad-dresses the complexities of taxonomy-free event boundary annotation, we introduce the task of Generic Event Bound-ary Detection (GEBD) and the new benchmark Kinetics-GEBD. We view GEBD as an important stepping stone to-wards understanding the video as a whole, and believe it has been previously neglected due to a lack of proper task deﬁnition and annotations. Through experiment and human study we demonstrate the value of the annotations. Fur-ther, we benchmark supervised and un-supervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD.
We release our annotations and baseline codes at CVPR’21
LOVEU Challenge: https://sites.google.com/ view/loveucvpr21. 1.

Introduction
Cognitive science tells us [49] that humans perceive video in terms of “events” (goal-directed sequences of ac-tions, like “washing a car” or “cooking a meal”), and fur-ther, people segment events naturally and spontaneously while perceiving video, breaking down longer events into a series of shorter temporal units. However, mainstream
SOTA video models [47, 48, 13, 7, 28, 12] still commonly process short clips (e.g. 1s long), followed by some kind of pooling operation to generate video-level predictions.
Recent years have seen signiﬁcant progress in temporal
Figure 1: Examples of generic event boundaries: 1) A long jump is segmented at a shot cut, then between actions of
Run, Jump and Stand up (dominant subject in red circle). 2) color / brightness change 3) new subject appears. action detection [8, 9, 14], segmentation [24, 3, 21, 11] and parsing [35, 38] in videos. Despite this, we have not seen major developments in modeling long-form video. The cog-nitive science suggests that one underlying deﬁcit is event segmentation: unlike our SOTA models, humans naturally divide video into meaningful units and can reason about these units. In contrast to our current methods build upon limited sets of predeﬁned action classes, humans perceive a broad and diverse set of segment boundaries without any predeﬁned target classes.
To enable machines to develop such ability, we pro-pose a new task called Generic Event Boundary Detection (GEBD) which aims at localizing the moments where hu-mans naturally perceive event boundaries. As Fig. 1 shows, our event boundaries could happen at the moments where
THUMOS’14
ActivityNet v1.3
Charades
HACS Segments
AVA
EPIC-Kitchens
EPIC-Kitchens-100
TAPOS Instances
Kinetics-GEBD (raw)
Kinetics-GEBD (clean)
#videos 2700 27801 67000 50000 214 432 700 16294 55351 54691
#segments 18K 23K 10K 139K 197K 39K 89K 48K 1771K 1561K
#boundaries 36K 46K 20K 278K 394K 79K 179K 33K 1498K 1290K video domain sports in-the-wild household in-the-wild movie kitchen kitchen sports in-the-wild in-the-wild boundary cause action action action action action action action action generic generic
#boundary classes 20 203 157 200 80 2747, open-vocab 4053, open-vocab open-vocab taxonomy-free taxonomy-free
#Annotations per video 1 1 1 1 1 1 1 1 4.93 4.94
Table 1: Comparing our Kinetics-GEBD with other video boundary datasets. Our Kinetics-GEBD has the largest number of temporal boundaries (e.g. 32x of ActivityNet, 8x of EPIC-Kitchens-100), spans a broad spectrum of video domains in the wild in contrast to sports or kitchen centric, is open-vocabulary rather than building on a pre-deﬁned taxonomy, contains boundaries caused by not only action change but also generic event change, and has almost 5 annotations per video to capture human perception differences and therefore ensure diversity. Note that for ActivityNet and TAPOS, since ground truths of test set are withheld, we do not include #segments and #boundaries of their test sets. the action changes (e.g. Run to Jump), the subject changes (e.g. a new person appears), the environment changes (e.g. suddenly become bright), for example.
To annotate ground truths of such taxonomy-free event boundaries, the common strategies used by the existing tem-poral tasks with pre-deﬁned taxonomy do not work: 1. Existing tasks require us to manually deﬁne each tar-get class carefully i.e. its semantic differentiators com-pared to other classes. But it is impractical to enu-merate and manually deﬁne all candidate generic event boundary classes. 2. The existing tasks typically focus on shot and action boundaries, neglecting other generic event boundaries as the examples shown in Fig. 1 like change of subject.
In this paper, we propose to follow cognitive experiments
[49] in annotating event boundaries on computer vision datasets. We choose the popular Kinetics [20] dataset as our video source and construct a new event segmentation benchmark Kinetics-GEBD. The marked boundaries are relatively consistent across different annotators; the main challenge raising ambiguity is the level of detail. For exam-ple, one annotator might mark boundaries at the beginning and end of a dance sequence, while another might annotate every dance move. We develop several novel principles in design annotation guideline to ensure consistent level of de-tail across different annotators while explicitly capturing the human perception differences with a multi-review protocol.
Our new GEBD task and benchmark will be valuable in: 1. Immediately supporting applications like video edit-ing, summarization, keyframe selection, highlight de-tection. Event boundaries divide a video into natural, meaningful units and can rule out unnatural cuts in the middle of a unit, for example. 2. Spurring progress in long-form video; GEBD is a ﬁrst step towards segmenting video into meaningful units and enabling further reasoning based on these units.
In summary, our contributions are four-fold:
• A new task and benchmark, Kinetics-GEBD, for de-tecting generic event boundaries without the need of a predeﬁned target event taxonomy.
• We propose novel annotation task design principles that are effective yet easy for annotators to follow. We disambiguate what shall be annotated as event bound-aries while preserving diversity across individuals in the annotation.
• We benchmark a number of supervised and un-supervised methods on the TAPOS [38] dataset and our
Kinetics-GEBD.
• We demonstrate the value of our event boundaries on downstream applications including video-level classi-ﬁcation and video summarization. 2.