Abstract
This paper studies the task of estimating the 3D human poses of multiple persons from multiple calibrated camera views. Following the top-down paradigm, we decompose the task into two stages, i.e. person localization and pose es-timation. Both stages are processed in coarse-to-ﬁne man-ners. And we propose three task-speciﬁc graph neural net-works for effective message passing. For 3D person lo-calization, we ﬁrst use Multi-view Matching Graph Mod-ule (MMG) to learn the cross-view association and recover coarse human proposals. The Center Reﬁnement Graph
Module (CRG) further reﬁnes the results via ﬂexible point-based prediction. For 3D pose estimation, the Pose Regres-sion Graph Module (PRG) learns both the multi-view ge-ometry and structural relations between human joints. Our approach achieves state-of-the-art performance on CMU
Panoptic and Shelf datasets with signiﬁcantly lower com-putation complexity. 1.

Introduction
The task of estimating 3D human poses of multiple per-sons from multiple views is a long-standing problem. It has attracted increasing attention for its wide range of applica-tions, e.g. sports broadcasting [6] and retail analysis [35].
Recent research on 3D multi-person pose estimation us-ing multi-view images generally follows two streams: 2D-to-3D lifting-based approaches and direct 3D estimation ap-proaches. As shown in Figrue 1(a), 2D-to-3D lifting ap-proaches [3, 4] ﬁrst estimate 2D joints in each view through monocular pose estimator, then associate 2D poses across views, and ﬁnally lift the matched 2D single-view poses to 3D via triangulation [2] or Pictorial Structure Models (PSM) [11]. Such approaches are generally efﬁcient and are the de-facto standard when seeking real-time perfor-mance [31]. However, the 3D reconstruction accuracy is
*Corresponding author.
Figure 1. Overview of mainstream multi-view 3D pose estimation frameworks. (a) 2D-to-3D lifting-based approaches (b) Direct 3D pose estimation approaches. (c) Our approach applies graph-based matching algorithm to detect human centers, and applies a graph-based pose reﬁnement model to effectively utilize both geometric cues and human structural prior to achieve better performance. limited by the 2D pose estimation, which is not robust to occlusion. As shown in Figure 1(b), direct 3D ap-proaches [35] construct the discretized 3D volumetric rep-resentations [28, 29] by gathering multi-view features and directly operate in the 3D space. Such approaches avoid making incorrect decisions in 2D camera views. However, their computation cost increases cubically with the size of the space. They also suffer the quantization errors caused by space discretization [35].
As shown in Figure 1(c), we combine the virtues of both approaches by adopting 2D-to-3D lifting for efﬁcient 3D human center detection in the ﬁrst stage, and direct 3D es-timation approach for accurate single-person 3D pose es-timation in the second stage. To strike a balance between accuracy and efﬁciency, both stages are processed in coarse-to-ﬁne manners with task-speciﬁc graph neural networks.
For coarse-level 3D human center detection in the ﬁrst stage, we generate coarse human center predictions via multi-view matching. Previous methods perform associa-tion across views by multi-view geometric constraints [18] and appearance similarity [11]. However, their match-ing criteria are hand-crafted and not learnable, which may suffer from tedious hyper-parameter tuning and inaccurate matching results. To solve this problem, we propose the
Multi-view Matching Graph Module (MMG) to learn from data to match people across views by considering both the visual and geometric cues. It also captures the relationship among multiple views to make more reliable predictions.
For ﬁne-level 3D human center detection in the ﬁrst stage, we propose a graph-based point predictor, i.e. Cen-ter Reﬁnement Graph Module (CRG), to reﬁne the coarse human center locations. Previous works [1, 6, 29, 28, 35] mostly discretize the space into voxels and operate on a regular grid. CRG instead adopts implicit ﬁeld represen-tations [21, 32, 33] and directly operates on the continuous 3D space to predict whether a point is a human center or not.
It gives us the ﬂexibility to balance between accuracy and speed, by sampling with arbitrary step sizes. Additionally, we propose to use graph models to learn to fuse multi-view features, which are not well-exploited in literature.
For coarse-level single-person pose estimation, we sim-ply use an off-the-shelf pose estimator to generate initial 3D poses based on the detected human proposals. For ﬁne-level single-person pose estimation, we propose the Pose Regres-sion Graph Module (PRG) to reﬁne the initial 3D poses, by exploiting both the spatial relations between body joints and the geometric relations across multiple views.
The three graph modules can alleviate the aforemen-tioned weakness caused by inaccurate 2D detection or space discretization and improve the pose estimation accuracy.
Our main contributions can be summarized as follows:
• To the best of our knowledge, this is the ﬁrst attempt of using task-speciﬁc graph neural networks for multi-view 3D pose estimation. We propose a novel coarse-to-ﬁne framework that signiﬁcantly outperforms the previous approaches both in accuracy and efﬁciency.
• We propose Multi-view Matching Graph Module (MMG) to signiﬁcantly improve the performance of multi-view human association via learnable matching.
• We propose Center Reﬁnement Graph Module (CRG) for point-based human center reﬁnement, which effec-tively aggregates multi-view features via graph neu-ral networks, and adaptively samples points to achieve more efﬁcient and accurate localization.
• We propose a powerful graph-based model, termed
Pose Regression Graph (PRG) for 3D human pose re-ﬁnement. It accounts for both the human body struc-tural information and the multi-view geometry to gen-erate more accurate 3D human poses. 2.