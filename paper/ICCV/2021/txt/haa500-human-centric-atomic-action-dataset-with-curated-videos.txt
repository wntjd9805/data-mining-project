Abstract
We contribute HAA5001, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambi-guities in action classification, HAA500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g.,
“Baseball Pitching” vs “Free Throw in Basketball”. Thus
HAA500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as “Throw”. HAA500 has been carefully curated to capture the precise movement of hu-man figures with little class-irrelevant motions or spatio-temporal label noises.
The advantages of HAA500 are fourfold: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20–60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic ac-tion classes. Our extensive experiments including cross-data validation using datasets collected in the wild demon-strate the clear benefits of human-centric and atomic char-acteristics of HAA500, which enable training even a base-line deep learning model to improve prediction by attend-ing to atomic human poses. We detail the HAA500 dataset statistics and collection methodology and compare quanti-tatively with existing action recognition datasets. 1.

Introduction
Observe the coarse annotation provided by commonly-used action recognition datasets such as [21, 25, 42], where the same action label was assigned to a given complex video action sequence (e.g., Play Soccer, Play Baseball) typically lasting 10 seconds or 300 frames, thus introducing a lot of ambiguities during training as two or more action categories may contain the same atomic action (e.g., Run is one of the atomic actions for both Play Soccer and Play Baseball). 1HAA500 project page: https://www.cse.ust.hk/haa.
This work was supported by Kuaishou Technology and the Research
Grant Council of the Hong Kong SAR under grant no. 16201818.
Recently, atomic action datasets [5, 16, 17, 36, 39] have been introduced in an attempt to resolve the aforementioned issue. Google’s AVA actions dataset [17] provides dense an-notations of 80 atomic visual actions in 430 fifteen-minute video clips where actions are localized in space and time.
AVA spoken activity dataset [36] contains temporally la-beled face tracks in videos, where each face instance is labeled as speaking or not, and whether the speech is au-dible. Something-Something dataset [16] contains clips of humans performing pre-defined basic actions with daily ob-jects.
However, some of their actions are still coarse which can be further split into atomic classes with significantly different motion gestures. E.g., AVA [17] and Something-Something [16] contain Play Musical Instrument and Throw
Something as a class, respectively, where the former should be further divided into sub-classes such as Play Piano and
Play Cello, and the latter into Soccer Throw In and Pitch
Baseball, etc., because each of these atomic actions has sig-nificantly different gestures. Encompassing different visual postures into a single class poses a deep neural network al-most insurmountable challenge to properly learn the perti-nent atomic action, which probably explains the prevailing low performance employing even the most state-of-the-art architecture, ACAR-Net (mAP: 38.30%) [33], in AVA [17], despite only having 80 classes.
The other problem with existing action recognition video datasets is that their training examples contain actions ir-relevant to the target action. Video datasets typically have fixed clip lengths, allowing unrelated video frames to be easily included during the data collection stage. Kinetics 400 dataset [21], with a fixed 10-second clip length, con-tains a lot of irrelevant actions, e.g., showing the audience before the main violin playing, or a person takes a long run before kicking the ball. Another problem is having too lim-ited or too broad field-of-view, where a video only exhibits a part of a human interacting with an object [16], or a single video contains multiple human figures with different actions present [17, 21, 48].
Recently, FineGym [39] has been introduced to solve the aforementioned limitations by proposing fine-grained ac-tion annotations, e.g., Balance Beam-Dismount-Salto For-ward Tucked. But due to the expensive data collection pro-Sports/Athletics
Playing Musical Instruments
Run (Dribble)
Throw In
Shoot
Save
Daily Actions
Grand Piano
Cello
Gong
Recorder r e c c o
S l l a b e s a
B
Run
Pitch
Swing
Catch Flyball
Applaud
Waist Bow
Fist Bump
Salute
Figure 1. HAA500 is a fine-grained atomic action dataset, with fine-level action annotations (e.g., Soccer-Dribble, Soccer-Throw In) compared to the traditional composite action annotations (e.g., Soccer, Baseball). HAA500 is comparable to existing coarse-grained atomic action datasets, where we have distinctions (e.g., Soccer-Throw In, Baseball-Pitch) within an atomic action (e.g., Throw Something) when the action difference is visible. The figure above displays sample videos from three different areas of HAA500. Observe that each video contains one or a few dominant human figures performing the pertinent action. cess, they only contain 4 events with atomic action anno-tations (Balance Beam, Floor Exercise, Uneven Bars, and
Vault-Women), and their clips were extracted from profes-sional gymnasium videos in athletic or competitive events.
In this paper, we contribute Human-centric Atomic Ac-tion dataset (HAA500) which has been constructed with carefully curated videos with a high average of 69.7% de-tectable joints, where a dominant human figure is present to perform the labeled action. The curated videos have been annotated with fine-grained labels to avoid ambiguity, and with dense per-frame action labeling and no unrelated frames being included in the collection as well as annota-tion. HAA500 contains a wide variety of atomic actions, ranging from athletic atomic action (Figure Skating - Ina
Bauer) to daily atomic action (Eating a Burger). HAA500 is also highly scalable, where adding a class takes only 20– 60 minutes. The clips are class-balanced and contain clear visual signals with little occlusion. As opposed to “in-the-wild” atomic action datasets, our “cultivated” clean, class-balanced dataset provides an effective alternative to advance research in atomic visual actions recognition and thus video understanding. Our extensive cross-data experiments vali-date that precise annotation of fine-grained classes leads to preferable properties against datasets with orders of magni-tude larger in size.
Figure 1 shows example atomic actions collected. 2.