Abstract
We present DocFormer - a multi-modal transformer based architecture for the task of Visual Document Un-derstanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction.
DocFormer uses text, vision and spatial features and com-bines them using a novel multi-modal self-attention layer.
DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is eval-uated on 4 different datasets each with strong baselines.
DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters). 1.

Introduction
The task of Visual Document Understanding (VDU) aims at understanding digital documents either born as
PDF’s or as images. VDU focuses on varied document re-lated tasks like entity grouping, sequence labeling, docu-ment classification. While modern OCR engines [33] have become good at predicting text from documents, VDU of-ten requires understanding both the structure and layout of documents. The use of text or even text and spatial features alone is not sufficient for this purpose. For the best results, one needs to exploit the text, spatial features and the image.
One way to exploit all these features is using transformer models [4, 14, 51]. Transformers have recently been used for VDU [25, 54, 55]. These models differ in how the unsu-pervised pre-training is done, the way self-attention is mod-ified for the VDU domain or how they fuse modalities (text and/or image and spatial). There have been text only [14], text plus spatial features only [25, 54] approaches for VDU.
However, the holy-grail is to fuse all three modalities (text,
Figure 1: Snippet of a Document: Various VDU tasks on this document may include labeling each text token into fixed classes or grouping tokens into a semantic class and finding relationships
Key and “1/29/74” between tokens e.g. (“DATE PREPARED”
Value) or classifying the document into different categories. (cid:1)
Note a document could have “other” text e.g. “C-5” which the model should ignore or classify as “other” depending on the task. (cid:1) visual and spatial features). This is desirable since there is some information in text that visual features miss out (lan-guage semantics), and there is some information in visual features that text misses out (text font and visual layout for example).
Multi-modal training in general is difficult since one has to map a piece of text to an arbitrary span of visual con-tent. For example in Figure 1, “ITEM 1” needs to be mapped to the visual region. Said a different way, text de-scribes semantic high-level concept(s) e.g. the word “per-son” whereas visual features map to the pixels (of a person) in the image. It is not easy to enforce feature correlation across modalities from text ←−→ image. We term this issue as cross-modality feature correlation and reference it later to show how DocFormer presents an approach to address this.
DocFormer follows the now common, pre-training and fine-tuning strategy. DocFormer incorporates a novel multi-modal self-attention with shared spatial embeddings in an encoder only transformer architecture. In addition, we pro-pose three pre-training tasks of which two are novel un-supervised multi-modal tasks: learning-to-reconstruct and multi-modal masked language modeling task. Details are provided in Section 3. To the best of our knowledge, this is the first approach for doing VDU which does not use bulky pre-trained object-detection networks for visual feature ex-traction. DocFormer instead uses plain ResNet50 [21] fea-tures along with shared spatial (between text and image) embeddings which not only saves memory but also makes it easy for DocFormer to correlate text, visual features via spatial features. DocFormer is trained end-to-end with the visual branch trained from scratch. We now highlight the contributions of our paper:
• A novel multi-modal attention layer capable of fusing text, vision and spatial features in a document.
• Three unsupervised pre-training tasks which encour-age multi-modal feature collaboration. Two of these learning-are novel unsupervised multi-modal tasks: to-reconstruct task and a multi-modal masked lan-guage modeling task.
• DocFormer is end-to-end trainable and it does not rely on a pre-trained object detection network for visual features simplifying its architecture. On four varied downstream VDU tasks, DocFormer achieves state of the art results. On some tasks it out-performs large variants of other transformer almost 4x its size (in the number of parameters). In addition, DocFormer does not use custom OCR unlike some of the recent papers
[55, 25]. 2.