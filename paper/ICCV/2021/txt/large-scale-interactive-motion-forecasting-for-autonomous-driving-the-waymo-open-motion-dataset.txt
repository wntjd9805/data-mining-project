Abstract
As autonomous driving systems mature, motion forecast-ing has received increasing attention as a critical require-ment for planning. Of particular importance are interactive situations such as merges, unprotected turns, etc., where predicting individual object motion is not sufﬁcient. Joint predictions of multiple objects are required for effective route planning. There has been a critical need for high-quality motion data that is rich in both interactions and an-notation to develop motion planning models. In this work, we introduce the most diverse interactive motion dataset to our knowledge, and provide speciﬁc labels for interact-ing objects suitable for developing joint prediction models.
With over 100,000 scenes, each 20 seconds long at 10 Hz, our new dataset contains more than 570 hours of unique data over 1750 km of roadways. It was collected by mining for interesting interactions between vehicles, pedestrians, and cyclists across six cities within the United States. We use a high-accuracy 3D auto-labeling system to generate high quality 3D bounding boxes for each road agent, and provide corresponding high deﬁnition 3D maps for each scene. Furthermore, we introduce a new set of metrics that provides a comprehensive evaluation of both single agent and joint agent interaction motion forecasting models. Fi-nally, we provide strong baseline models for individual-agent prediction and joint-prediction. We hope that this new large-scale interactive motion dataset will provide new op-portunities for advancing motion forecasting models. 1.

Introduction
Motion forecasting has received increasing attention as a critical requirement for planning in autonomous driving systems [8, 14, 40, 36, 28, 34]. Due to the complexity of scenes that autonomous systems need to safely handle, pre-dicting object motion in the scene is a difﬁcult task, suitable for machine learning models. Building effective motion (a) A vehicle waits for a pedestrian to fully cross the crosswalk before commencing a turn. (b) A vehicle accelerates onto the street only after the incoming vehicle turns.
Figure 1: Examples of interactions between agents in a scene in the WAYMO OPEN MOTION DATASET. Each ex-ample highlights how predicting the joint behavior of agents aids in predicting likely future scenarios. Solid and dashed lines indicate the road graph and associated lanes. Each nu-meral indicates a unique agent in the scene. forecasting models requires large amounts of high quality real world data. Creating a dataset for motion forecasting is complicated by the fact that the distribution of real world data is highly imbalanced [4, 18, 32, 38]; in the common case, vehicles drive straight at a constant velocity. In or-der to develop effective models, a dataset must contain and measure performance on a wide range of behaviors and tra-jectory shapes for different object types that an autonomous system will encounter in operation.
We argue that critical situations (e.g., merges, lane changes, and unprotected turns) require the joint prediction of a set of multiple interacting objects, not just a single ob-ject. An example of a pedestrian and vehicle interacting is illustrated in Figure 1a where a vehicle waits for a pedes-trian to fully cross the street before turning. In Figure 1b,
the orange vehicle accelerates into the street only after en-suring the incoming blue vehicle’s intention is to deceler-ate and turn off of the street. Most existing datasets have focused on single agent representation, but there has been considerably less work on interaction modeling at a large scale, which motivates this work.
The goal of this work is to provide a large scale, diverse dataset with speciﬁc annotations for interacting objects to promote the development of models to jointly predict inter-active behaviors. In addition, we aim to supply object be-haviors over a wide range of road geometries, and thus pro-vide a large set of annotated interactions over a diverse set of locations. To generate such a set, we develop criteria for mining interactive behavior over a large corpus of driving data. We explicitly annotate groups of interacting objects in both training and validation/test data to enable development of models that jointly predict the motion of multiple agents as well as individual prediction models.
We aim to provide high quality object tracking data to re-duce uncertainty due to perception noise. The cost of hand labeling a dataset of the required size is prohibitive. Instead we use a state-of-the-art automatic labeling system [26] to provide high quality detection and tracking data of objects in the scenes. In contrast with many datasets which provide tracking from on-board autonomous systems, the off-board automatic labeling system provides higher accuracy as it is not constrained to run in real time. These high quality tracks allow us to focus on understanding the complexity of object behavior, rather than on dealing with perception noise.
Evaluation of interactive prediction models requires met-rics formulated for joint predictions as motivated by recent work [33, 6, 34, 28]. In Section 4, we discuss existing work on generalizing metrics to the joint prediction case. We also propose a novel mean Average Precision (mAP) metric to capture the performance of models across different object types, prediction time scales, and trajectory shape buckets (e.g., u-turns, left turns). This method is inspired by metrics used in the object detection literature and overcomes limi-tations in currently adopted metrics. We discuss how this metric attempts to address issues with existing metrics.
We name our large-scale interactive motion dataset:
WAYMO OPEN MOTION DATASET. It will be made pub-licly available to the research community, and we hope it will provide new directions and opportunities in developing motion forecasting models. We summarize the contribu-tions of our work as follows:
• We release a large-scale dataset for motion forecast-ing research with speciﬁcally labeled interactive be-haviors. The data is derived from high quality percep-tion output across a large array of diverse scenes with rich annotations from multiple cities.
• We provide novel metrics for motion prediction anal-ysis along with challenging benchmarks for both the
Lyft
NuSc
Argo
Inter
Ours
# unique tracks
Avg track length
Time horizon
# segments
Segment duration
Total time
Unique roadways
Sampling rate
# cities covered
# object types
Boxes 3D maps
Ofﬂine perception
Interactions
Trafﬁc signal states 53.4 m § 1.8 s § 5 s 170k 25 s 1118 h 10 km 10 Hz 1 3 2D (cid:51) 4.3 k
-6 s 1k 20 s 5.5 h
-2 Hz 2 1 † 3D 11.7 m ‡ 2.48 s ‡ 3 s 324k 5 s 320 h 290 km 10 Hz 2 1 ‡
None (cid:51) 40 k 19.8 s ∗ 3 s
--16.5 h ∗
-10 Hz 6 ∗ 1 2D (cid:51) (cid:51) 7.64 m 7.04 s †† 8 s 104k 20 s 574 h 1750 km†† 10 Hz 6 3 3D (cid:51) (cid:51) (cid:51) (cid:51)
Table 1: Comparison of popular behavior prediction and motion forecasting datasets. Speciﬁcally, we compare
Lyft Level 5 [19], NuScenes [4], Argoverse [9], Interactions
[39], and our dataset across multiple dimensions. # object types measures the number of types of objects to predict the motion trajectory. Dashed line ”-” indicates that data is not available or not applicable. § Lyft Level 5 number of unique tracks and average track length are determined through pri-vate correspondence. † nuScenes [4] provides annotations for 23 objects types (stationary vehicles are removed), but only the vehicle is predicted. ‡ Argoverse [9] provides an-notations for 15 object types (Appendix B) but only vehi-cle is predicted. The number of unique tracks is determined through private correspondence. The average track length is estimated from data. ∗ Interactions [39] gathered data from 4 countries including 6 cities (the last statistic is collected through personal communication) and the entire dataset is not divided into segments. The average track length is esti-mated from data. †† Our average track length is computed on the 20s segments of the training split. Our total unique roadway distance is calculated by hashing our autonomous vehicle poses as UTM coordinates into 25 meter voxels and counting the number of non-zero voxels. marginal and joint prediction cases. 2.