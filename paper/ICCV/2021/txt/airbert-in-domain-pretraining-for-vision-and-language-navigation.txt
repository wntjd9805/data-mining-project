Abstract
Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments us-ing natural language instructions. Given the scarcity of domain-specific training data and the high diversity of im-age and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent meth-ods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB1, a large-scale and diverse in-domain VLN dataset. We first col-lect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal or-der inside PI pairs. We use BnB to pretrain our Airbert2 model that can be adapted to discriminative and genera-tive settings and show that it outperforms state of the art for
Room-to-Room (R2R) navigation and Remote Referring Ex-pression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a chal-lenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses. 1.

Introduction
In vision-and-language navigation (VLN), an agent is asked to navigate in home environments following natural language instructions [3, 5]. This task is attractive to many real-world applications such as domestic robotics and per-sonal assistants. However, given the high diversity of VLN data across environments and the difficulty of the manual collection and annotation of VLN training data at scale, the 1Bed and Breakfast 2Airbert is an Old Irish word meaning practice, here referring to model pretraining on pretext tasks similar to VLN.
Figure 1: VLN tasks are evaluated on unseen environments at test time. Top: None of the training houses contain a Christmas theme making this test environment particularly challenging. Bottom:
We build a large-scale, visually diverse, and in-domain dataset by creating path-instruction pairs close to a VLN-like setup and show the benefits of self-supervised pretraining. performance of current methods remains limited, especially for previously unseen environments [48].
Our work is motivated by significant improvements in vision and language pretraining [2, 9, 23, 24, 25, 38], where deep transformer models [42] are trained via self-supervised proxy tasks [10] using large-scale, automatically harvested image-text datasets [32, 36].
Such pretraining enables learning transferable multi-modal representations achieving state-of-the-art performance in various vision and language tasks. Similarly, with the goal of learning an embodied agent that generalizes, recent works [13, 16, 22, 28] have explored different pretraining approaches for VLN tasks.
In [13, 16], annotated path-instruction pairs are aug-mented with a speaker model that generates instructions for random unseen paths. However, as these paths orig-inate from a small set of 61 houses used during training, they are limited in visual diversity. The limited pretraining
environments do not equip agents with visual understand-ing abilities that enable generalization to unseen houses, see
Fig. 1. To address this problem, VLN-BERT [28] proposes to pretrain the agent on generic image-caption datasets that are abundant and cover diverse visio-linguistic knowledge.
However, these image-caption pairs are quite different from the dynamic visual stream (path) and navigable instructions observed by a VLN agent. Such out-of-domain pretraining, although promising, only brings limited gains to the navi-gation performance. Besides the above limitations, existing pretraining methods do not place much emphasis on tem-poral reasoning abilities in their proxy tasks such as one-step action prediction [13] and path-instruction pairing [28], while such reasoning is important to a sequential decision making task like VLN. As a result, even if performance in downstream tasks is improved, the pretrained models may still be brittle. For example, a simple corruption of instruc-tions by swapping noun phrases within the instruction, or replacing them with other nouns, leads to significant confu-sion as models are unable to pick the correct original pair.
In this paper, we explore a different data source and proxy tasks to address the above limitations in pretrain-ing a generic VLN agent. Though navigation instructions are rarely found on the Internet, image-caption pairs from home environments are abundant in online marketplaces (e.g. Airbnb), which include images and descriptions of rental listings. We collect BnB, a new large-scale dataset with 1.4M indoor images and 0.7M captions. First, we show that in-domain image-caption pairs bring additional benefits for downstream VLN tasks when applied with generic web data [28].
In order to further reduce the domain gap be-tween the BnB pretraining and the VLN task, we present an approach to transform static image-caption pairs into visual paths and navigation-like instructions (Fig. 1 bottom), lead-ing to large additional performance gains. We also propose a shuffling loss that improves the modelâ€™s temporal reason-ing abilities by learning a temporal alignment between a path and the corresponding instruction.
Our pretrained model, Airbert, is a generic transformer backbone that can be readily integrated in both discrimina-tive VLN tasks such as path-instruction compatibility pre-diction [28] and generative VLN tasks [15] in R2R navi-gation [5] and REVERIE remote referring expression [34].
We achieve state-of-the-art performance on these VLN tasks with our pretrained model. Beyond the standard eval-uation, our in-domain pretraining opens an exciting new di-rection of one/few-shot VLN where the agent is trained on examples only from one/few environment(s) and expected to generalize to other unseen environments.
In summary, the contributions of this work are three-fold. (1) We collect a new large-scale in-domain dataset,
BnB, to promote pretraining for vision-and-language nav-igation tasks. (2) We curate the dataset in different ways to reduce the distribution shift between pretraining and
VLN and also propose the shuffling loss to improve tem-poral reasoning abilities. (3) Our pretrained Airbert can be plugged into generative or discriminative architectures and achieves state-of-the-art performance on R2R and
REVERIE datasets. Moreover, our model generalizes well under a challenging one/few-shot VLN evaluation, truly highlighting the capabilities of our learning paradigm. 2.