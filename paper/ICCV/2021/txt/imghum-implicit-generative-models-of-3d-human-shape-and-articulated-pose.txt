Abstract
We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh.
We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed im-plicit generative model of human pose, shape, and seman-tics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expres-sions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Addi-tionally, our model has attached spatial semantics making it straightforward to establish correspondences between dif-ferent shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems. 1.

Introduction
Mathematical models of the human body have been proven effective in a broad variety of tasks.
In the last decades models of varying degrees of realism have been successfully deployed e.g. for 3D human motion analysis
[46], 3D human pose and shape reconstruction [24, 52], per-sonal avatar creation [3, 54], medical diagnosis and treat-ment [16], or image synthesis and video editing [53, 21].
Modern statistical body models are typically learnt from large collections of 3D scans of real people, which are used to capture the body shape variations among the human pop-ulation. Dynamic scans, when available, can be used to fur-ther model how different poses affect the deformation of the muscles and the soft-tissue of the human body.
The recently released GHUM model [49] follows this methodology by describing the human body, its shape vari-ation, articulated pose including fingers, and facial ex-pressions as a moderate resolution mesh based on a low-dimensional, partly interpretable parameterization. In the
* The first two authors contributed equally.
Figure 1. imGHUM is the first parametric full human body model imGHUM represented as an implicit signed distance function. successfully models broad variations in pose, shape, and facial ex-pressions. The level sets of imGHUM are shown in blue-scale. deep learning literature GHUM and similar models [27, 23] are typically used as fixed function layers. This means that the model is parameterized with the output of a neural net-work or some other non-linear function, and the resulting mesh is used to compute the final function value. While this approach works well for several tasks, including, more recently, 3D reconstruction, the question of how to best rep-resent complex 3D deformable and articulated structures is open. Recent work dealing with the 3D visual reconstruc-tion of general objects aimed to represent the output not as meshes but as implicit functions [28, 32, 7, 29]. Such ap-proaches thus describe surfaces by the zero-level-set (deci-sion boundary) of a function over points in 3D-space. This has clear benefits as the output is neither constrained by a template mesh topology, nor is it discretized and thus of fixed spatial resolution.
In this work, we investigate the possibility to learn a data-driven statistical body model as an implicit function.
Given the maturity of state of the art explicit human models, it is crucial that an equivalent implicit representation main-tains their key, attractive properties – representing compa-rable variation in shape and pose and similar level of detail.
This is challenging since recently-proposed implicit func-tion networks tend to produce overly smooth shapes and fail for articulated humans [8]. We propose a novel net-work architecture and a learning paradigm that enable, for the first time, constructing detailed generative models of hu-man pose, shape, and semantics, represented as Signed Dis-tance Functions (SDFs) (see fig. 1). Our multi-part archi-tecture focuses on difficult to model body components like 1
g e n erativ e p o s e g e n erativ e s h a p e g e n erativ e h a n d s g e n. e x p re ssio n interp olatio n sig n e d dista n c e s s e m a ntic s c o ntin u o u s re p.
✓
✗
✗
✓
✓
✓
✗
✗
✗
✓
✓
✗
✗
✗
✓
✓
✗
✗
✗
✓
✓
✗
✓
✓
✓
✗
✗
✓
✗
✓
✓
✗
✗
✗
✓
✗
✓
✓
✓
✓
GHUM [49]
IF-Net [8]
IGR [14]
NASA [11] imGHUM
Table 1. Comparison of different approaches to model human bod-ies. GHUM is meshed-based and thus discretized. IGR only al-lows for shape interpolation. NASA lacks generative capabilities for shape, hands, and facial expressions and only returns occu-pancy values. Only imGHUM combines all favorable properties. hands and faces. Moreover, imGHUM models its neighbor-hood through distance values, enabling e.g. collision tests.
Our model is not bound to a specific resolution and thus can be easily queried at arbitrary locations. Being template-free further paves the way to our ultimate goal to fairly represent diversity of mankind, including disabilities which may not be always well covered by a generic template of standard topology. Finally, in contrast to recent implicit function net-works, our model additionally carries on the explicit seman-tics of mesh-based models. Specifically, our implicit func-tion also returns correspondences to a canonical represen-tation near and on its zero-level-set, enabling e.g. texturing or body part labeling. This holistic approach is novel and significantly more difficult to produce, as can be noted in prior work which could only demonstrate individual prop-erties, c.f . tab. 1. Our contribution – and the key to success – stems from the novel combination of adequate, genera-tive latent representations, network architectures with fine grained encoding, implicit losses with attached semantics, and the consistent aggregation of multi-part components.
Besides extensive evaluation of 3D deformable and artic-ulated modeling capabilities, we also demonstrate surface completion using imGHUM and give an outlook to mod-eling varying topologies. Our models are available for re-search [1]. 1.1.