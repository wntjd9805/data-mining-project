Abstract
Image and video enhancement such as color constancy, low light enhancement, and tone mapping on smartphones is challenging, because high-quality images should be achieved efﬁciently with a limited resource budget. Unlike prior works that either used very deep CNNs or large Trans-former models, we propose a structure-aware lightweight
Transformer, termed STAR, for real-time image enhance-ment. STAR is formulated to capture long-range dependen-cies between image patches, which naturally and implicitly captures the structural relationships of different regions in an image. STAR is a general architecture that can be eas-ily adapted to different image enhancement tasks. Exten-sive experiments show that STAR can effectively boost the quality and efﬁciency of many tasks such as illumination enhancement, auto white balance, and photo retouching, which are indispensable components for image processing on smartphones. For example, STAR reduces model com-plexity and improves image quality compared to the recent state-of-the-art [19] on the MIT-Adobe FiveK dataset [7] (i.e., 1.8dB PSNR improvements with 25% parameters and 13% ﬂoat operations.) 1.

Introduction
Recent years have witnessed signiﬁcant progress on a va-riety of image and video enhancement tasks with learning-based methods, such as super-resolution, denoising, de-mosaicking, low light enhancement, color constancy (i.e., white balance), tone mapping. Yet, there are still two key challenges for deploying these methods on edge devices.
First, these methods must process high-resolution images efﬁciently within a very limited computation budget, mak-ing trade-offs between model ﬂexibility and computation efﬁciency. Second, they need to incorporate structural and global information of input images in order to achieve high-quality stable results, especially for tasks such as color con-Figure 1. Comparison of curve estimation results between CNN and STAR for low light enhancement. ”CNN” denotes the original backbone model used by [19] that predicts per-pixel curves. For fair comparison, we also train the same CNN model with down-sampling (denoted as CNN-D), so that both STAR and CNN-D predict per-token curves. Both the enhanced results and model complexity (number of parameters, FLOPS) are shown. We also visualize the predicted curves by plotting the mean of α (See more details in Section 4.1). stancy, low light enhancement, and tone mapping. Even for tasks with local support, such as denoising and demosaick-ing, structure-aware region-based processing can often pro-duce better results [33].
To address these two challenges, prior work can be sum-marized into three categories. The ﬁrst approach is to use stacked, very deep CNNs [49, 9, 17, 45, 19]. In order to maintain high-frequency details, the spatial resolution is of-ten kept unchanged, and thus these methods have a large computational cost and memory footprint. The second ap-proach is to estimate one set of global adjustment func-tion [52, 35, 24, 31], but they lack the ﬂexibility to handle the complexity of real-world scenarios (e.g., mixed illumi-nation for white balance, HDR scenes for tone mapping).
Finally, the third approach is to explicitly use a segmenta-tion network to divide images into semantically-meaningful regions and process each region separately [33, 51]. How-ever, this approach also has limitations such as requiring per-pixel annotated datasets.
In this paper, we proposed STAR (Structure-aware
Transformer), a general lightweight backbone for various
real-time image post-processing tasks. STAR is formulated to capture long-range dependencies among image patches, which naturally and implicitly captures the structure rela-tionships within an image. STAR is a general architec-ture that can be easily adapted for a variety of learning-based image enhancement tasks. Rather than stacked con-volution layers, STAR is based on the Transformer module, which mainly consists of multi-head self-attention and full-connected layers only. Transformer [44] is widely used in natural language processing for its high training efﬁciency towards long-distance dependencies and tremendous model capacity. Models building upon Transformer achieved sur-prising performance that even surpasses human recognition ability in speciﬁc language tasks [11].
Specialized in image enhancement tasks, we design the
STAR network, which can be free of stacked convolution and thus more efﬁcient in extracting structural information.
In STAR, patches of the image are tokenized into token embeddings, much like word embeddings in NLP. Rather than computing pixel-wise dependencies directly, STAR ex-plicitly learns token-wise dependencies for image patches.
Fig.1 shows an example. As shown in Fig.1, STAR delivers high efﬁciency to enhance an image. In addition to high efﬁ-ciency (0.07 GFLOPS), as shown, STAR can also implicitly learn the semantic structure and thus deliver more semantic-meaningful results than CNNs [19]. Instead of having one module for general information, as suggested in [27, 48], we employ a specialized two-branch design naming long-short Range Transformer to ensure STAR can focus on cap-turing global contexts thus reducing computations.
We further validate STAR on several recent image enhancement methods, including illumination enhance-ment [19], white balance [2] and photo retouching with 3D lookup table [52]. Experimental results show that STAR can often effectively boost the performance of these tasks with signiﬁcantly reduced model complexity, which offers great advantages for real-time processing on edge devices.
For instance, on low light enhancement using DCE-Net [19]
, compared to CNN backbone, STAR-based methods can achieve 1.8dB PSNR improvement, while only requiring 25% parameters and 13% ﬂoat operations (FLOPS). 2.