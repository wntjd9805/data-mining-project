Abstract
Efficient processing of high-resolution video streams is safety-critical for many robotics applications such as au-tonomous driving.
Image downsampling is a commonly adopted technique to ensure the latency constraint is met.
However, this naive approach greatly restricts an object de-tector’s capability to identify small objects. In this paper, we propose an attentional approach that elastically mag-nifies certain regions while maintaining a small input can-vas. The magnified regions are those that are believed to have a high probability of containing an object, whose sig-nal can come from a dataset-wide prior or frame-level prior computed from recent object predictions. The magnification is implemented by a KDE-based mapping to transform the bounding boxes into warping parameters, which are then fed into an image sampler with anti-cropping regulariza-tion. The detector is then fed with the warped image and we apply a differentiable backward mapping to get bounding box outputs in the original space. Our regional magnifica-tion allows algorithms to make better use of high-resolution input without incurring the cost of high-resolution process-ing. On the autonomous driving datasets Argoverse-HD and BDD100K, we show our proposed method boosts the detection AP over standard Faster R-CNN, with and with-out finetuning. Additionally, building on top of the previous state-of-the-art in streaming detection, our method sets a new record for streaming AP on Argoverse-HD (from 17.8 to 23.0 on a GTX 1080 Ti GPU), suggesting that it has achieved a superior accuracy-latency tradeoff. 1.

Introduction
Safety-critical robotic agents such as self-driving cars make use of an enormous suite of high-resolution percep-tual sensors, with the goal of minimizing blind spots, max-imizing perception range, and ensuring redundancy [5, 4, 37]. We argue that “over-sensed” perception platforms pro-vide unique challenges for vision algorithms since those visual sensors must rapidly consume sensor streams while continuously reporting back the state of the world. While
* denotes equal contribution.
Figure 1: Standard image downsampling (top right) limits the capability of the object detector to find small objects. In this paper, we propose an attentional warping method (bot-tom right) that enlarges salient objects in the image while maintaining a small input resolution. Challenges arise when warping also alters the output labels (e.g., bounding boxes). numerous techniques exist to make a particular model run fast, such as quantization [40], model compression [8], and inference optimization [30], at the end of the day, simple approaches that subsample sensor data (both spatially by frame downsampling and temporally by frame dropping) are still most effective for meeting latency constraints [19].
However, subsampling clearly throws away information, negating the goals of high-resolution sensing in the first place! This status quo calls for novel vision algorithms.
To address this challenge, we take inspiration from the human visual system; biological vision makes fundamental use of attentional processing. While current sensing stacks make use of regular grid sampling, the human vision system in the periphery has a much lower resolution than in the cen-ter (fovea), due to the pooling of information from retinal receptors by retinal ganglion cells. Such variable resolution is commonly known as foveal vision [18].
In this paper, we propose FOVEAted image magnifica-tion (FOVEA) for object detection, which retains high reso-lution for objects of interest while maintaining a small can-vas size. We exploit the sparsity in detection datasets – objects of interest usually only cover a portion of the im-age. The key idea is to resample such that background pix-els can make room for objects of interest. The input images are downsampled and warped such that salient areas in the warped image have higher resolutions. While image warp-ing has been explored for image classification [16, 31] and regression [31], major challenges remain for object detec-tion. First, processing the images in the warped space will produce bounding box outputs in the warped space. We make use of differentiable backward maps to unwarp the bounding box coordinates. Second, it’s much more chal-lenging to identify regions for magnification. Empirically, we find that end-to-end trained saliency networks that work well for image classification fail for object detection. How-ever, unlike gaze estimation and fine-grained image classi-fication, the tasks evaluated on by [31], we have an explicit signal for saliency with object detection – bounding box an-notations or outputs. Specifically, we use dataset-wide pri-ors and object locations from the previous frame (for video streams). We train a function that maps bounding box loca-tions to warping parameters. Third, object detection has a much lower tolerance to cropping than image classification, since objects appear not only in the center but also near the edges of the image. We find that previous image warping methods are very susceptible to this issue, so we introduce an anti-cropping modification to the warping formulation.
We validate our approach on two self-driving datasets for 2D object detection: Argoverse-HD [19] and BDD100K
[42]. First, we show that even without learning, our hand-coded bounding-box-guided magnification improves the av-erage precision (AP) for off-the-shelf Faster R-CNN [33], suggesting that considerable sparsity exists in the input space for those datasets. Next, we finetune the detector with differentiable image warping and a backward label mapping, which further boosts AP. In both cases, the im-provement for small objects is most significant. Finally, to show that such accuracy improvement is worth the latency cost, we evaluate our algorithm under the streaming percep-tion framework [19], and we achieve state-of-the-art perfor-mance in terms of streaming AP. 2.