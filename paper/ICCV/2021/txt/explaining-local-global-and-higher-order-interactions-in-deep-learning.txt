Abstract
We present a simple yet highly generalizable method for explaining interacting parts within a neural network’s rea-soning process. First, we design an algorithm based on cross derivatives for computing statistical interaction ef-fects between individual features, which is generalized to both 2-way and higher-order (3-way or more) interactions.
We present results side by side with a weight-based attri-bution technique, corroborating that cross derivatives are a superior metric for both 2-way and higher-order interaction detection. Moreover, we extend the use of cross derivatives as an explanatory device in neural networks to the computer vision setting by expanding Grad-CAM, a popular gradient-based explanatory tool for CNNs, to the higher order. While
Grad-CAM can only explain the importance of individual objects in images, our method, which we call Taylor-CAM, can explain a neural network’s relational reasoning across multiple objects. We show the success of our explanations both qualitatively and quantitatively, including with a user study. We will release all code as a tool package to facilitate explainable deep learning. 1.

Introduction
The universe is made up of myriad interacting parts. To truly understand complex systems and processes, it is not enough to view their functions as an amalgamation of in-dependent contributors. Rather, they are a complex web of inter-operating influences. For much of the past, explain-able deep learning has concerned itself with identifying important features, feature vectors, and isolated concepts.
However, in the real world, humans intuitively understand that decisions are consequences of complex relations, not merely extrapolated from rankings of singular phenomena.
For example, upon seeing a yield sign, it is natural to look to see if there are also passing cars. If not, the yield sign may be safely dismissed and one could keep driving
If there is a passing car, the law is to without stopping. yield to the other car. If an intelligent agent made the de-cision to stop upon approaching a yield sign and a passing
Figure 1: An automated driver decides whether to “stop” or “go.” Here, the decision cannot be explained by individ-ual factors alone, but by the interaction between the yield sign and the passing car. Taylor-CAM identifies interac-tions by considering how changing one object affects the significance of another, such as how changing a passing car into an empty road would change the meaning of the yield sign from “stop” to “go.” car, explaining their actions with precision would require an explanation of this interaction. As far as individual factors go, perhaps a nearby pedestrian is also present, but with-out an interactional interpretation, one would not be able to distinguish the independence of the yield sign and passing car from the pedestrian, and one would not be privy to the knowledge of the salient interaction. Furthermore, a naive observer might think that yield signs always indicate “stop” without realizing that the agent’s response to the yield sign would depend on the presence of a passing car.
Similarly, explaining an agent’s strategies in any task
— be it computer vision, natural language processing, biomedicine, reinforcement learning, or future forecasting
— is imprecise without an interactional approach. How-ever, interactional strategies are not always summarizable by heatmaps [6, 24, 25, 39, 40] or ordered rankings [10, 21, 29]; and they often require an understanding of many
dependencies — complex dependencies, such as those be-tween higher-level concepts (e.g. vector representations in deep neural networks [3, 22, 23, 38]) — not just single-dimensional features as typically explored in the statistical interaction effects literature [9, 13, 31, 32]. In light of all of this, we propose a number of contributions towards ex-plaining interactions in deep learning:
T-NID, an algorithm for statistical interaction effects that outperforms recent state-of-the-art baselines with both pairwise and higher-order interactions. Interaction effects are a fundamental notion in statistics [36]. We make this computation tractable by translating local interaction ef-fects into global interaction effects via representative sam-ples and employing a simple subsampling heuristic.
Taylor-CAM, an explanatory tool that extends Grad-CAM [24], which assigns importances to feature vectors based on input gradients, by generalizing it to the 2-way and higher-order setting using the same formalism of inter-action effects as for T-NID. This method is demonstrated on multi-object detection and relational reasoning in visual question-answering (VQA).
Visualizations of Taylor-CAM’s explanations that en-able a human cohort to reverse engineer questions in rela-tional VQA without knowing the answers and interpret rela-tional reasoning better than with existing explanatory tools like Grad-CAM and GLIDER [31] from just a convolutional neural network’s (CNN) feature maps. 2.