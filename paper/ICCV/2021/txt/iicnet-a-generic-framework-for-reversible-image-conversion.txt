Abstract
Reversible image conversion (RIC) aims to build a re-versible transformation between speciﬁc visual content (e.g., short videos) and an embedding image, where the original content can be restored from the embedding when necessary.
This work develops Invertible Image Conversion Net (IIC-Net) as a generic solution to various RIC tasks due to its strong capacity and task-independent design. Unlike pre-vious encoder-decoder based methods, IICNet maintains a highly invertible structure based on invertible neural net-*Joint ﬁrst authors works (INNs) to better preserve the information during con-version. We use a relation module and a channel squeeze layer to improve the INN nonlinearity to extract cross-image relations and the network ﬂexibility, respectively. Exper-imental results demonstrate that IICNet outperforms the speciﬁcally-designed methods on existing RIC tasks and can generalize well to various newly-explored tasks. With our generic IICNet, we no longer need to hand-engineer task-speciﬁc embedding networks for rapidly occurring vi-sual content. Our source codes are available at: https:
//github.com/felixcheng97/IICNet.
1.

Introduction
Visual media can be classiﬁed into different types, in-cluding live photos [3], binocular images or videos [14], and dual-view images or videos [1]. Usually, speciﬁc de-vices or platforms are required to view the visual media con-tent. For example, binocular content may only be applicable in 3D devices, so we may need to generate corresponding monocular content to make them compatible with common devices [14]. Instead of simply dropping parts of the orig-inal content, a better choice is to build a reversible trans-formation, where the embedding is compatible with com-mon devices, and the original content can be restored when necessary. Also, the single embedding image can help save the storage cost and transmission bandwidth. As a result, many researchers are motivated to study several reversible image conversion (RIC) tasks [14, 33, 40] to establish a re-versible transformation between visual content and an em-bedding image. Some examples are shown in Figure 1.
RIC tasks are challenging since we often need to em-bed much richer information implicitly in one single im-age, which may lead to unavoidable information loss. Previ-ous works [14, 33, 40] usually employ an encoder-decoder based framework, which learns the informative bottleneck representation but has limited ability to capture the lost in-formation [29, 34]. For example, Zhu et al. [40] embed a video preview into a single image and restore the origi-nal content with cascaded encoders and decoders, in which they sacriﬁce the quality of the embedding image to em-bed more information, but their restored frames are still not highly accurate due to the information loss problem. Hence, one key objective in RIC tasks is to mitigate such informa-tion loss. Another concern is that although RIC tasks share the same embedding-restoration procedure for high-quality embedding and restored images, previous methods usually have task-speciﬁc designs (e.g., optical ﬂow in [40]), mak-ing them challenging to generalize to other types of visual content. Hence, with the rapid growth of media formats plus the increasing interest in the RIC tasks, it is desirable to de-velop a generic framework for solving all types of RIC tasks.
Considering these aspects, we propose Invertible Image
Conversion Net (IICNet) as a generic framework for RIC tasks. To alleviate the information loss problem, we utilize invertible neural networks (INNs) [12, 13] as a strictly in-vertible embedding module. A channel squeeze layer [35] is used and integrated into INNs for ﬂexible reduction of di-mensions, with only very minor deviations introduced to the invertible architecture. Furthermore, we introduce a relation module to strengthen the limited nonlinear representation capability of INNs [12] to better capture cross-image rela-tions, in which independent cross-image convolution layers are used, with residual connections for better maintaining a highly reversible structure.
With the strong embedding capacity and the generic module design, IICNet does not rely on any task-speciﬁc technique, making it capable of dealing with different con-tent types. We also allow lower-resolution embedding for higher compression rates.
Figure 1(a) gives a concrete example for illustration.
Given a sequence of video frames, our IICNet can embed the spatial-temporal information of the sequence into one lower-resolution image that is visually similar to the downsampled middle reference frame. There are some promising applica-tions. First, we may embed a short video clip or live photo in one image. Second, we can embed a high-resolution high-FPS video into a low-resolution low-FPS video. In this way, we can allow ﬂexible adoptions for different devices and save storage. Other potential applications are shown in Fig-ure 1(b-e), including mononizing binocular images, embed-ding dual-view images or multi-layer images, and even the general image hiding steganography task.
This paper presents the ﬁrst generic framework IICNet for different RIC tasks, supported by extensive experiments on ﬁve tasks, including two newly-explored tasks: (1) em-bedding a dual-view image into a single-view one; (2) the re-versible conversion between multi-layer images and a single image. Both quantitative and qualitative results show that our method outperforms the existing methods on the studied tasks. Ablation studies are conducted for the network mod-ules and loss functions. More information and demo results are included in the supplementary materials. 2.