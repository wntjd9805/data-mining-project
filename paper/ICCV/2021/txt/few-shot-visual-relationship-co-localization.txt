Abstract
In this paper, given a small bag of images, each contain-ing a common but latent predicate, we are interested in lo-calizing visual subject-object pairs connected via the com-mon predicate in each of the images. We refer to this novel problem as visual relationship co-localization or VRC as an abbreviation. VRC is a challenging task, even more so than the well-studied object co-localization task. This becomes further challenging when using just a few images, the model has to learn to co-localize visual subject-object pairs con-nected via unseen predicates. To solve VRC, we propose an optimization framework to select a common visual relation-ship in each image of the bag. The goal of the optimization framework is to find the optimal solution by learning visual relationship similarity across images in a few-shot setting.
To obtain robust visual relationship representation, we uti-lize a simple yet effective technique that learns relationship embedding as a translation vector from visual subject to vi-sual object in a shared space. Further, to learn visual rela-tionship similarity, we utilize a proven meta-learning tech-nique commonly used for few-shot classification tasks. Fi-nally, to tackle the combinatorial complexity challenge aris-ing from an exponential number of feasible solutions, we use a greedy approximation inference algorithm that selects approximately the best solution.
We extensively evaluate our proposed framework on variations of bag sizes obtained from two challenging pub-lic datasets, namely VrR-VG and VG-150, and achieve im-pressive visual co-localization performance. 1.

Introduction
Figure 1: Given a bag of four images as shown in the first row, can you find the visual subjects and objects connected via a common predicate? Our proposed model in this pa-per automatically does that. In this illustration, the “biting” predicate is present in all four images in the first row. Our proposed model localizes those visual subjects and objects in each image that are connected via “biting” as shown in the third row. Note that the category name “biting” is not provided to our approach. Here, green and yellow bound-ing boxes indicate the localized visual subject and objects respectively using our approach.[Best viewed in color].
Localizing visual relationship (<subject, predicate, object>) in images is a core task towards holistic scene in-terpretation [15, 37]. Often the success of such localization tasks heavily relies on the availability of large-scale anno-tated datasets. Can we localize visual relationships in im-ages by looking into just a few examples? In this paper, to-wards addressing this problem, we introduce an important and unexplored task of Visual Relationship Co-localization (or VRC in short). VRC has the following problem set-ting: given a bag of b images, each containing a common latent predicate, our goal is to automatically localize those visual subject-object pairs that are connected via the com-mon predicate in each of the b images. Note that, during both the training and testing phases, the only assumption is that each image in a bag contains a common predicate.
However, its category, e.g. biting, is latent.
Consider Figure 1 to better understand our goal. Given a bag of four images, each containing a latent common pred-icate, e.g. “biting” in this illustration, we aim to localize visual subject-object pairs, such as (dog, frisbee), (man, hot dog), and so on, with respect to the common predicate in each of the images. VRC is significantly more challenging than well-explored object co-localization [11, 26, 30] due to the following: (i) Common objects often share a similar vi-sual appearance. However, common relationships can visu-ally be very different, for example, visual relationships such as “dog biting frisbee” and “man biting hot dog” are very different in visual space. (ii) Relationship co-localization requires both visual as well as semantic interpretation of the scene. Further, VRC is also distinctly different from visual relationship detection (VRD) that aims to estimate the max-imum likelihood for (<subject, predicate, object>) tuples from a predefined fixed set of visual relationships common across the train and test sets. It should be noted that test predicates are not provided even during the training phase of VRC. Therefore, the model addressing VRC has to inter-pret the semantics of unseen visual relationships during test time.
Visual relationship co-localization (VRC) has many po-tential applications, examples include automatic image an-notation, bringing interpretability in image search engines, visual relationship discovery. In this work, we pose VRC as a labeling problem. To this end, every possible visual subject-object pair in each image is a potential label for common visual subject-object pair. To get the optimal label-ing, we define an objective function parametrized by model parameters whose minima corresponds to visual subject-object pairs that are connected via a common latent pred-icate in all the images. To generalize well on unseen pred-icates, we follow the meta-learning paradigm to train the model. Just as a good meta-learning model learns on vari-ous learning tasks, we train our model on a variety of bags having different common latent predicates in each of them so that the model generalizes to new bags. We use a greedy approximation algorithm during inference that breaks down the problem into small sub-problems and combines the so-lutions of sub-problems greedily.
To evaluate the performance of the proposed model for
VRC, we use two public datasets, namely VrR-VG [18] and VG-150 [34]. Our method achieves impressive perfor-mance for this challenging task. This is attributed to our principled formulation of the problem by defining a suitable objective function and our meta-learning-based approach to optimize it. Further, we present several ablation stud-ies to validate the effectiveness of different components of our proposed framework. On bag size = 4, we achieve 76.12% co-localization accuracy on unseen predicates of
VrR-VG [18] dataset.
The contributions of this paper are two folds: (i) We introduce a novel task – VRC (Visual Relationship Co-Localization). VRC has several potential applications and is an important step towards holistic scene interpretation. (ii) Inspired by the recent success of the meta-learning paradigm in solving few-shot learning tasks, we propose a novel framework for performing few-shot visual relation-ship co-localization. Our framework learns robust repre-sentation for latent visual predicates and is efficacious in performing visual relationship co-localization with only a few examples. 2.