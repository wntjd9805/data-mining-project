Abstract
Recent methods for visual question answering rely on large-scale annotated datasets. Manual annotation of ques-tions and answers for videos, however, is tedious, expen-sive and prevents scalability. In this work, we propose to avoid manual annotation and generate a large-scale train-ing dataset for video question answering making use of au-tomatic cross-modal supervision. We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video nar-rations. Given narrated videos, we then automatically gen-erate the HowToVQA69M dataset with 69M video-question-answer triplets. To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer. We intro-duce the zero-shot VideoQA task and show excellent results, in particular for rare answers. Furthermore, we demon-strate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and
How2QA. Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language bi-ases and high-quality redundant manual annotations. 1.

Introduction
Answering questions about videos requires a detailed un-derstanding of the visual content and its association with the natural language. Indeed, given the large diversity of ques-tions, methods for Video Question Answering (VideoQA) should reason about scenes, objects and human actions as well as their complex temporal interactions.
Current approaches to VideoQA rely on deep fully-supervised models trained on manually annotated datasets with question and answer pairs [22, 31, 34, 35, 40, 42, 48].
Collecting and annotating VideoQA datasets, however, is cumbersome, time consuming, expensive and therefore not scalable. As a result, current VideoQA datasets are rel-atively small (see Figure 2). This limitation hinders the 3Czech Institute of Informatics, Robotics and Cybernetics at the Czech
Technical University in Prague.
Figure 1: Given videos with transcribed narration, we lever-age language models and cross-modal supervision to obtain large-scale VideoQA data. Above are two examples from our dataset. progress in the field as state-of-the-art VideoQA models of-ten require a large amount of training data.
In this work, we address the scale issue with a new approach for automatically generating a VideoQA dataset, see Figure 1 for examples. The idea is to leverage cross-modal supervision together with text-only tools for question generation and to automatically annotate VideoQA from a large amount of readily-available narrated videos.
In-spired by the recent progress in language generation us-ing transformer-based language models [10], we leverage transformers trained on a question-answering text corpus to generate a diverse set of non-scripted questions and corre-sponding open-vocabulary answers from text. By applying these transformers to speech transcripts of narrated videos from the large-scale HowTo100M dataset [58], we create
HowToVQA69M, an open-ended VideoQA dataset with 69 million video-question-answer triplets and a diverse set of more than 16M unique answers (see Figure 3). As shown in
Figure 2, our HowToVQA69M is two orders of magnitude larger compared to prior VideoQA datasets.
Given the limited diversity of existing datasets, cur-rent methods typically reduce video question answering to a classification problem, where frequent answers are as-signed to unique classes. Typically, up to 5K unique pos-sible answers are considered. Such an approach, however, does not scale to the open vocabulary of 16M different an-swers in our dataset. To address this problem and to en-able video question answering with highly diverse questions
and answers, we introduce a training procedure based on contrastive learning between a video-question multi-modal transformer and an answer transformer that can handle free-form answers. This bypasses the need to define a discrete set of answer classes.
The goal of our work is to advance truly open-ended and generic solutions to VideoQA. To evaluate generalization, we propose a new zero-shot VideoQA task where we pro-hibit any manual supervision of visual data during train-ing. Our VideoQA model, trained on HowToVQA69M, demonstrates excellent zero-shot results on multiple exist-ing datasets, especially for rare answers. Moreover, when finetuned on target datasets, our model significantly outper-forms the state of the art on MSRVTT-QA [84], MSVD-QA [84] ActivityNet-QA [91], and How2QA [46].
Initial experiments showed that existing benchmarks for open-ended VideoQA [84, 91] contain a language bias [28], i.e., their questions can often be answered without looking at the video. To better evaluate the impact of visual informa-tion in VideoQA, we introduce a new open-ended VideoQA dataset (iVQA) with manually collected questions and an-swers, where we exclude questions that could be answered without watching the video. Moreover, to account for mul-tiple possible answers, iVQA contains five independently collected answers for each question.
In summary, our work proposes the following three con-tributions: (i) We introduce an approach to automatically generate a large-scale VideoQA dataset, HowToVQA69M. Re-lying on cross-modal supervision, we use transform-ers trained on an existing text-only question-answering corpus and generate video-question-answer triplets from videos and transcribed narrations. (ii) We train a VideoQA model on HowToVQA69M with contrastive learning between a multi-modal video-question transformer and an answer transformer. We show the efficiency of our model in the new zero-shot
VideoQA task and outperform the state of the art in four existing VideoQA benchmarks: MSRVTT-QA,
MSVD-QA, ActivityNet-QA and How2QA. (iii) Finally, we introduce a new manually annotated open-ended VideoQA benchmark iVQA that excludes non-visual questions and contains multiple possible an-swers for each question.
Code, datasets and trained models are available at [1]. 2.