Abstract
A myriad of recent breakthroughs in hand-crafted neu-ral architectures for visual recognition have highlighted the urgent need to explore hybrid architectures consisting of diversified building blocks. Meanwhile, neural architecture search methods are surging with an expectation to reduce human efforts. However, whether NAS methods can effi-ciently and effectively handle diversified search spaces with disparate candidates (e.g. CNNs and transformers) is still an open question. In this work, we present Block-wisely
Self-supervised Neural Architecture Search (BossNAS), an unsupervised NAS method that addresses the problem of in-accurate architecture rating caused by large weight-sharing space and biased supervision in previous methods. More specifically, we factorize the search space into blocks and utilize a novel self-supervised training scheme, named en-semble bootstrapping, to train each block separately before searching them as a whole towards the population center.
Additionally, we present HyTra search space, a fabric-like hybrid CNN-transformer search space with searchable down-sampling positions. On this challenging search space, our searched model, BossNet-T, achieves up to 82.5% accuracy on ImageNet, surpassing EfficientNet by 2.4% with compara-ble compute time. Moreover, our method achieves superior architecture rating accuracy with 0.78 and 0.76 Spearman correlation on the canonical MBConv search space with Im-ageNet and on NATS-Bench size search space with CIFAR-100, respectively, surpassing state-of-the-art NAS methods.1 1.

Introduction
The development of neural network architectures has brought about significant progress in a wide range of visual recognition tasks over the past several years. Representative
*Corresponding Author. 1Code: https://github.com/changlin31/BossNAS. (a) NAS with large weight-sharing space (39). (b) Block-wise NAS with biased supervision. (c) Block-wisely self-supervised NAS.
Figure 1: Comparision of three NAS schemes. Red arrows represent the supervision during training and searching. examples of such models include ResNet [25], SENet [31],
MobileNet [30] and EfficientNet [64]. Recently, the newly emerging attention-based architectures are coming to the forefront in the vision field, challenging the dominance of convolutional neural networks (CNNs). This exciting break-through in vision transformers led by ViT [20] and DETR
[8], are achieving competitive performance on various vi-sion tasks, such as image classification [20, 66, 79, 14, 9], object detection [8, 90, 62], semantic segmentation [88], and others [26, 50, 33]. As suggested by prior works [20, 62, 3], hybrids of CNNs and transformers can outperform both pure transformers and pure CNNs.
Despite the large advances brought about by network design, manually finding well-optimized hybrid architec-tures can be challenging, especially as the number of design choices increases. Neural Architecture Search (NAS) is a popular approach to reducing the human effort in network architecture design by automatically searching for optimal
architectures in a predefined search space. Representative success in performing NAS on manually designed build-ing blocks include MobileNetV3 [29], EfficientNet [64], etc. These works are searched by multi-trial NAS methods
[63, 92, 2, 89, 12, 47], which are computationally prohibitive (costing thousands of GPU days). Recent weight-sharing
NAS methods [6, 53, 4, 43] encode the entire search space as a weight-sharing supernet to avoid repetitive training of candidate networks, thus largely reducing the search cost.
However, as shown in Fig. 1a, architecture search spaces with layer-level granularity grow exponentially with increased network depth, which has been identified (in
[37, 39]) as the main culprit of inaccurate architecture rat-ing2 in weight-sharing NAS methods. To reduce the size of the large weight-sharing space, previous works [37, 46] factorize the search space into blocks and use a pretrained teacher model to provide block-wise supervision (Fig. 1b).
Despite their high ranking correlation and high efficiency, we find (in Sec. 5) their results to be highly correlated with the teacher architecture. As illustrated in Fig. 1b, when training by a teacher with blue nodes, candidate architectures with more blue nodes tend to get higher ranks in these methods.
This limits its application on diversified search spaces with disparate candidates, such as CNNs and transformers.
On the other hand, unsupervised NAS [41] has recently emerged as an interesting research topic. Without access to any human-annotated labels, unsupervised NAS methods (optimized with pretext tasks [41] or random labels [87]) have been proven capable of achieving comparable perfor-mance to supervised NAS methods. Accordingly, we pro-pose to use an unsupervised learning method as an alternative to supervised distillation in the aforementioned block-wise
NAS scheme (Fig. 1c), aiming to address the problem of architectural bias caused by the use of the teacher model.
In this work, we propose a novel unsupervised NAS method, Block-wisely Self-supervised Neural Architecture
Search (BossNAS), which aims to address the problem of inaccurate predictive architecture ranking caused by a large weight-sharing space while avoiding possible architectural bias caused by the use of the teacher model. As opposed to the block-wise solutions discussed above, which utilize distillation as intermediate supervision, we propose a self-supervised representation learning scheme named ensemble bootstrapping to optimize each block of our supernet. To be more specific, each sampled sub-networks are trained to predict the probability ensemble of all the sampled ones in the target network, between different augmented views of the same image. In the searching stage, an unsupervised eval-uation metric, is proposed to ensure fairness by searching towards the architecture population center. More specifi-cally, the probability ensemble of all the architectures in the population is used as the evaluation target to measure the 2In this work, architecture rating accuracy refers to the correlation of the predicted architecture ranking and the ground truth architecture ranking. performance of the sampled models.
Additionally, we design a fabric-like hybrid CNN-transformer search space (HyTra) with searchable down-sampling positions and use it as a case study for hybrid architectures to evaluate our method. In each layer of HyTra search space, CNN building blocks and transformer build-ing blocks of different resolutions are in parallel and can be chosen flexibly. This diversified search space covers pure transformers with fixed content length and normal CNNs with progressively reduced spatial scales.
We prove that our NAS method can generalize well on three different search spaces and three datasets. On HyTra search space, our searched models outperforms the ones searched by our supervised NAS counterpart [37], prov-ing that our method successfully avoids possible architec-ture bias brought by supervised distillation. Our method achieves superior architecture rating accuracy with 0.78 and 0.76 Spearman correlation on the canonical MBConv search space with ImageNet and on NATS-Bench size search space
SS [17] with CIFAR-100, respectively, surpassing state-of-the-art NAS methods, proving that our method successfully suppressed the problem of inaccurate architecture rating caused by large weight-sharing space.
Our searched models on HyTra search space achieves 82.5% accuracy on ImageNet, surpassing EfficientNet [64] by 2.4%, with comparable compute time3. By providing strong results through BossNet-T, we hope that this diver-sified HyTra search space with disparate candidates and high-performance architectures can serve as a new arena for future NAS works. We also hope that our BossNAS can serve as a widely used tool for hybrid architecture design. 2.