Abstract
In this paper we present SurfaceNet, an approach for estimating spatially-varying bidirectional reﬂectance distri-bution function (SVBRDF) material properties from a sin-gle image. We pose the problem as an image translation task and propose a novel patch-based generative adver-sarial network (GAN) that is able to produce high-quality, high-resolution surface reﬂectance maps. The employment of the GAN paradigm has a twofold objective: 1) allowing the model to recover ﬁner details than standard translation models; 2) reducing the domain shift between synthetic and real data distributions in an unsupervised way.
An extensive evaluation, carried out on a public benchmark of synthetic and real images under different illumination conditions, shows that SurfaceNet largely outperforms ex-isting SVBRDF reconstruction methods, both quantitatively and qualitatively. Furthermore, SurfaceNet exhibits a re-markable ability in generating high-quality maps from real samples without any supervision at training time.
Source code available at https://github.com/ perceivelab/surfacenet. 1.

Introduction
Computer-generated imagery (CGI) and 3D graphics play an important role in a variety of applications, including visual effects, architectural modeling, simulators, cultural heritage, video games, virtual or augmented reality and au-tomotive design. The increasing computational power of both professional and consumer hardware has led to a grow-ing interest in high-quality CGI, whose basic requirement for creating realistic images is the deﬁnition and implemen-tation of robust digital models that describe how real-world materials interact with light [12, 13]. This task is easily carried out by humans, who are able to intuitively identify materials’ physical properties by analyzing how light is re-ﬂected, transmitted and absorbed before reaching the ob-server’s eyes. Artiﬁcially emulating this process would re-quire a physically-accurate simulation of how generic ma-terials interact with light, but the complexity of such task and the needed level of surface details make this approach computationally unfeasible.
In practice, most approximations for rendering reﬂec-tions over surfaces simplify the task by deﬁning a model that describes how light interacts with pixel-level elements of a material depicted in a digital image: the properties of the material are modeled by a spatially-varying bidirec-tional reﬂectance distribution function (SVBRDF) that is parameterized by a set of properties encoding color, planar deformation and reﬂectivity. However, even measuring this approximation is a major challenge in computer graphics.
Following the success of deep learning methods in com-puter vision, the estimation of material reﬂectance proper-ties has been increasingly posed as a learning task [8, 15, 9, 10, 16]. Following this trend, we propose SurfaceNet, a fully-convolutional network for SVBRDF estimation from a single input image. Unlike methods that estimate material properties from multiple input images [9, 15], our approach better suits non-professional application scenarios, where it is unfeasible to obtain reliable acquisitions of a surface with a sufﬁciently steady view point.
More speciﬁcally, we pose SVBRDF estimation as an image-to-image translation task and introduce a deep gener-ative adversarial architecture, consisting of a generator that employs a fully-convolutional multi-head encoder-decoder network that predicts a set of SVBRDF maps, and a patch-based discriminator that is trained to distinguish between estimated maps and ground-truth ones. We propose to use a generative adversarial loss to compensate for the blurriness typically introduced by L1 or L2 losses. This is an alterna-tive approach to recent methods that perform neural render-ing [8, 15] to produce output images from the estimated re-ﬂectance maps as a supervisory signal. Moreover, the GAN framework also allows us to employ real-world images, for which no reﬂectance maps are available, during the train-ing procedure, alongside synthetic images. As a result, our generator learns to extract features that can be shared be-tween synthetic and real images, enforcing an implicit do-main adaptation mechanism and reducing the distribution
gap between input images from the two modalities. Finally, the combined use of skip connections within the generator and of a patch-based discriminator allows the model to fo-cus on and recover detailed features of small patches. We argue that these characteristics are particularly appropriate for SVBRDF estimation of real-world materials, where the capability to work at high-resolution on surface details is essential and where pattern structure is generally local.
We evaluate our method on a wide variety of materials from publicly-available SVBRDF libraries and real-world pictures of surfaces; in our experiments on single-image in-puts, our method largely outperforms previous works, both qualitatively and quantitatively.
To summarize, the contributions introduced by the pro-posed method are the following:
• We present a deep network for single-image SVBRDF estimation that, leveraging the properties of GANs in generating high-frequency details and learning data distributions in an unsupervised way, is able to predict high-quality reﬂectance maps from real-world pho-tographs;
• Our patch-based discrimination strategy allows our model to recover ﬁne, local, features in the output maps, even at high resolution (2048×2048);
• Experimental results on multiple datasets under dif-ferent illumination conditions show that our model largely outperforms, both quantitatively and qualita-tively, existing single-image estimation methods, set-ting new state-of-the-art performance on the task. 2.