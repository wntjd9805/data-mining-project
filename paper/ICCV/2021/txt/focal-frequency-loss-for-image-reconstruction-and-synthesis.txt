Abstract
Image reconstruction and synthesis have witnessed re-markable progress thanks to the development of generative models. Nonetheless, gaps could still exist between the real and generated images, especially in the frequency domain.
In this study, we show that narrowing gaps in the frequency domain can ameliorate image reconstruction and synthe-sis quality further. We propose a novel focal frequency loss, which allows a model to adaptively focus on frequency com-ponents that are hard to synthesize by down-weighting the easy ones. This objective function is complementary to ex-isting spatial losses, offering great impedance against the loss of important frequency information due to the inher-ent bias of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve popular models, such as VAE, pix2pix, and SPADE, in both percep-tual quality and quantitative performance. We further show its potential on StyleGAN2.1, 2 1.

Introduction
We have seen remarkable progress in image reconstruc-tion and synthesis along with the development of generative models [19, 34, 15, 32, 56], and the progress continues with the emergence of various powerful deep learning-based ap-proaches [31, 43, 44, 55]. Despite their immense success, one could still observe the gaps between the real and gener-ated images in certain cases.
These gaps are sometimes manifested in the form of ar-tifacts that are discernible. For instance, upsampling lay-ers using transposed convolutions tend to produce checker-board artifacts [42]. The gaps, in some other cases, may only be revealed through the frequency spectrum analysis.
Recent studies [59, 72, 22] in media forensics have shown some notable periodic patterns in the frequency spectra of manipulated images, which may be consistent with artifacts in the spatial domain. In Figure 1, we show some paired ex-amples of real images and the fake ones generated by typi-cal generative models for image reconstruction and synthe-sis. It is observed that the frequency domain gap between 1 GitHub: https://github.com/EndlessSora/focal-frequency-loss. 2 Project page: https://www.mmlab-ntu.com/project/fﬂ/index.html.
Figure 1. Frequency domain gaps between the real and the gener-ated images by typical generative models in image reconstruction and synthesis. Vanilla AE [19] loses important frequencies, lead-ing to blurry images (Row 1 and 2). VAE [34] biases to a lim-ited spectrum region (Row 3), losing high-frequency information (outer regions and corners). Unnatural periodic patterns can be spotted on the spectra of images generated by GAN (pix2pix) [23] (Row 4), consistent with the observable checkerboard artifacts (zoom in for view). In some cases, a frequency spectrum region shift occurs to GAN-generated images (Row 5). the real and fake images may be a common issue for these methods, albeit in slightly different forms.
The observed gaps in the frequency domain may be im-puted to some inherent bias of neural networks when ap-plied to reconstruction and synthesis tasks. Fourier anal-ysis highlights a phenomenon called spectral bias [48, 40, 54], a learning bias of neural networks towards low-frequency functions. Thus, generative models tend to es-chew frequency components that are hard to synthesize, i.e., hard frequencies, and converge to an inferior point. F-Principle [67] shows that the priority of ﬁtting certain fre-quencies in a network is also different throughout the train-ing, usually from low to high. Consequently, it is difﬁcult for a model to maintain important frequency information as it tends to generate frequencies with a higher priority.
In this paper, we carefully study the frequency domain gap between real and fake images and explore ways to ame-liorate reconstruction and synthesis quality by narrowing this gap. Existing methods [34, 23, 43] usually adopt pixel losses in the spatial domain, while spatial domain losses hardly help a network ﬁnd hard frequencies and synthesize them, in that every pixel shares the same signiﬁcance for a certain frequency. In comparison, we transform both the real and generated samples to their frequency representa-tions using the standard discrete Fourier transform (DFT).
The images are decomposed into sines and cosines, exhibit-ing periodic properties. Each coordinate value on the fre-quency spectrum depends on all the image pixels in the spa-tial domain, representing a speciﬁc spatial frequency. Ex-plicitly minimizing the distance of coordinate values on the real and fake spectra can help networks easily locate difﬁ-cult regions on the spectrum, i.e., hard frequencies.
To tackle these hard frequencies, inspired by hard exam-ple mining [11, 51] and focal loss [36], we propose a simple yet effective frequency-level objective function, named fo-cal frequency loss. We map each spectrum coordinate value to a Euclidean vector in a two-dimensional space, with both the amplitude and phase information of the spatial fre-quency put under consideration. The proposed loss function is deﬁned by the scaled Euclidean distance of these vectors by down-weighting easy frequencies using a dynamic spec-trum weight matrix. Intuitively, the matrix is updated on the
ﬂy according to a non-uniform distribution on the current loss of each frequency during training. The model will then rapidly focus on hard frequencies and progressively reﬁne the generated frequencies to improve image quality.
The main contribution of this work is a novel focal fre-quency loss that directly optimizes generative models in the frequency domain. We carefully motivate how a loss can be built on a space where frequencies of an image can be well represented and distinguished, facilitating optimization in the frequency dimension. We further explain the way that enables a model to focus on hard frequencies, which may be pivotal for quality improvement. Extensive experiments demonstrate the effectiveness of the proposed loss on repre-sentative baselines [19, 34, 23, 43], and the loss is comple-mentary to existing spatial domain losses such as perceptual loss [27]. We further show the potential of focal frequency loss to improve state-of-the-art StyleGAN2 [31]. 2.