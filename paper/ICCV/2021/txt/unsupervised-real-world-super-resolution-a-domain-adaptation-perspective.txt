Abstract
Most existing convolution neural network (CNN) based super-resolution (SR) methods generate their paired train-ing dataset by artiﬁcially synthesizing low-resolution (LR) images from the high-resolution (HR) ones. However, this dataset preparation strategy harms the application of these
CNNs in real-world scenarios due to the inherent domain gap between the training and testing data. A popular at-tempts towards the challenge is unpaired generative ad-versarial networks, which generate “real” LR counterparts from real HR images using image-to-image translation and then perform super-resolution from “real” LR→SR. De-spite great progress, it is still difﬁcult to synthesize perfect
“real” LR images for super-resolution. In this paper, we
ﬁrstly consider the real-world SR problem from the tradi-tional domain adaptation perspective. We propose a novel unpaired SR training framework based on feature distri-bution alignment, with which we can obtain degradation-indistinguishable feature maps and then map them to HR images. In order to generate better SR images for target
LR domain, we introduce several regularization losses to force the aligned feature to locate around the target domain.
Our experiments indicate that our SR network obtains the state-of-the-art performance over both blind and unpaired
SR methods on diverse datasets. 1.

Introduction
Single image super-resolution (SISR), aiming to increase the resolution of an image from a single low-resolution (LR) counterpart, has attracted a lot of attentions in com-puter vision community. In recent years, convolution neu-ral networks (CNNs) have been applied to SISR task[6] and achieved the state-of-the-art performance[5] over tradi-tional arts. In order to construct training image pairs, some
SR studies leverage determined operations such as bicubic interpolation, to down-sample the original high-resolution
* Authors contributed equally. This work was performed while
Haochen Zhang worked as an intern at ByteDance.
† Corresponding author. (HR) images. Obviously, such predeﬁned techniques limit the model generalization capability as several blur and noise of unknown types exist in real LR images. As a result, the notorious domain gap, between the training LR images and the real-world testing images, harms the inference perfor-mance of these well-trained CNNs in real-world scenarios.
Blind SR is a straightforward attempt to process real LR images, which aims to restore HR images from LR counter-parts with unknown degradation parameters. For example,
[1, 10, 20] considered arbitrary blur kernels during the train-ing process. Although these blind models have achieved satisfactory performance for a large range of predeﬁned degradations, the performance still will drop drastically in real-world scenerios, as the distribution of real LR images is different from those degraded by manually designed op-erations anyway. Recently, inspired by the success of gen-erative adversarial network (GAN) [9] in image style trans-lation [48], many studies use CycleGAN[48] framework to train SR networks in an unpaired manner. They usually sup-pose that only two unpaired datasets are available: a real
LR dataset with no predeﬁned degradations and a real HR one. The main idea is directly generating real LR counter-parts from real HR images using image-to-image transla-tion. Then a SR network is trained to map the degraded LR outputs to the corresponding HR images in a paired man-ner. Although these methods have shown their advantages in real-world SR by directly simulating the real LR image distribution, it is still difﬁcult to train an ideal degraded LR image generator to perfectly mimic real images and real-world SR remains a challenging problem so far.
In contrast to existing SISR efforts generating plausible
“real” LR by image-to-image transferring, in this paper we reconsider the unpaired real-world SR from a feature-level domain adaptation perspective. Speciﬁcally, the source do-main includes the real HR dataset and its synthetic LR counterparts while the real LR dataset is regarded as the target domain inputs without labels. As opposed to high-level vision tasks which try to learn a domain-invariant image representation, our goal is to obtain degradation-indistinguishable feature maps and then map these feature maps to HR images.
Inspired by several adversarial-based domain adaptation approaches in learning domain-invariant features[8, 26, 31], we propose a novel framework for unpaired SR training based on feature alignment, shown as in Figure 1 (a). Such feature alignment technique could hopefully make the fea-tures from source and target LR images indistinguishable, so that we can obtain the target HR images during the infer-ence stage by using the decoder network trained with only source HR supervision. However, different from high-level domain adaptation where the aligned image representations are of low resolution, the shared feature space in SR task is extremely large due to relatively fewer down-sampling in
CNNs. Therefore as described in Figure 1 (b), we also in-troduce extra constraints to further help align features and preserve more details compared to the traditional domain adaptation task. The whole domain adaptation based frame-work for unpaired SR training is illustrated in Figure 2.
To our best knowledge, this is the ﬁrst work formulating unpaired SR training as a feature-level domain adaptation problem and our main contributions are summarized as be-low:
• We propose a novel unpaired SR training framework based on feature distribution alignment.
• We introduce several losses to not only align feature space better but also preserve image details for the downstream SR task.
• Extensive experiments on three challenging datasets show that our proposed method has advantages over the existing unpaired SR training solutions. 2.