Abstract
Recent strategies achieved ensembling “for free” by fit-ting concurrently diverse subnetworks inside a single base network. The main idea during training is that each sub-network learns to classify only one of the multiple inputs simultaneously provided. However, the question of how to best mix these multiple inputs has not been studied so far.
In this paper, we introduce MixMo, a new generalized framework for learning multi-input multi-output deep sub-networks. Our key motivation is to replace the subopti-mal summing operation hidden in previous approaches by a more appropriate mixing mechanism. For that purpose, we draw inspiration from successful mixed sample data aug-mentations. We show that binary mixing in features - par-ticularly with rectangular patches from CutMix - enhances results by making subnetworks stronger and more diverse.
We improve state of the art for image classification on
CIFAR-100 and Tiny ImageNet datasets. Our easy to im-plement models notably outperform data augmented deep ensembles, without the inference and memory overheads.
As we operate in features and simply better leverage the expressiveness of large networks, we open a new line of re-search complementary to previous works. 1.

Introduction
Convolutional Neural Networks (CNNs) have shown ex-ceptional performance in computer vision tasks, notably classification [42]. However, among other limitations, ob-taining reliable predictions remains challenging [34, 58].
For additional robustness in real-world scenarios or to win
Kaggle competitions, CNNs usually pair up with two prac-tical strategies: data augmentation and ensembling.
Data augmentation reduces overfitting and improves
*Equal contribution.
†Correspondence to alexandre.rame@lip6.fr
Figure 1: MixMo overview. We embed M = 2 inputs into a shared space with convolutional layers (c1, c2), mix them, pass the embedding through further layers and out-put 2 predictions via dense layers (d1, d2). The key point of our MixMo is the mixing block. Mixing with patches performs better than basic summing: 85.40% vs. 83.06% (MIMO [30]) on CIFAR-100 with WRN-28-10. generalization, notably by diversifying training samples
[51]. Traditional approaches are label-preserving. In con-trast, recent mixed sample data augmentation (MSDA) create artificial samples by mixing multiple inputs and their labels proportionally to a ratio λ. The seminal work Mixup
[86] linearly interpolates pixels while Manifold Mixup [76] interpolates latent features in the network. Binary masking
MSDAs [21, 29, 41] such as CutMix [83] have since di-versified mixed samples by pasting patches from one image onto another in place of interpolation.
Aggregating predictions from a diverse set of neural net-works (i.e. with different failure cases) strongly improves generalization [14, 28, 43], notably uncertainty estimation
[2, 27, 58]. An ensemble of several small networks usually performs better than one large network empirically [9, 50].
Yet, unfortunately, ensembling is costly in time and memory both at training and inference: this often limits applicability.
In this paper, we propose MixMo, a new generalized multi-input multi-output framework: we train a base net-work with M ≥ 2 inputs and outputs. This way, we fit
M independent subnetworks [23, 30, 66] defined by an in-put/output pair and a subset of network weights. This is possible as large networks only leverage a subset of their weights [19]. Rather than pruning (ie, eliminating) inactive filters [44, 47], we seek to fully use the available neurons and over parameterization through multiple subnetworks.
The challenge is to prevent homogenization and enforce diversity among subnetworks with no structural differences.
Thus, we consider M (input, label) pairs at the same time in training: {(xi, yi)}0≤i<M . M images are treated simul-taneously, as shown on Fig. 1 with M = 2. The M inputs are encoded by M separate convolutional layers {ci}0≤i<M into a shared latent space before being mixed. The represen-tation is then fed to the core network, which finally branches out into M dense layers {di}0≤i<M . Diverse subnetworks naturally emerge as di learns to classify yi from input xi. At inference, the same image is repeated M times: we obtain ensembling “for free” by averaging M predictions.
The key divergent point between MixMo variants lies in the multi-input mixing block that seeks features indepen-dence. Should the merging be a basic summation or a con-catenation, we would recover MIMO [30] or respectively
Aggregated Learning [66] - which both featured this multi-input multi-output strategy.
Our main intuition is simple: we see summing as a bal-anced and restrictive form of Mixup [86] where λ = 1
M .
By analogy, we draw from the considerable MSDA litera-ture to design a more appropriate mixing block. In particu-lar, we leverage binary masking methods to ensure subnet-works diversity. Our framework allows us to create a new
Cut-MixMo variant inspired by CutMix [83], and illustrated in Fig. 1: a patch of features from the first input is pasted into the features from the second input.
This asymmetrical mixing also raises new questions re-garding information flow in the network’s features. We tackle the imbalance between the multiple classification training tasks via a new weighting scheme. Conversely,
MixMo’s double nature as a new mixing augmentation in features yields important insights on traditional MSDA.
In summary, our contributions are threefold: 1. We propose a general framework, MixMo, connecting two successful fields: mixing samples data augmenta-tions & multi-input multi-output ensembling. 2. We identify the appropriate mixing block to best tackle the diversity/individual accuracy trade-off in subnet-works: our easy to implement Cut-MixMo benefits from the synergy between CutMix and ensembling.
Figure 2: Main results. CIFAR-100 with WRN-28-w. Our
Cut-MixMo variant (patch mixing and M = 2) surpasses
CutMix and deep ensembles (with half the parameters) by leveraging over-parameterization in wide networks.
We demonstrate excellent accuracy and uncertainty esti-mation with MixMo on CIFAR-10/100 and Tiny ImageNet.
Specifically, Cut-MixMo with M = 2 reaches state of the art on these standard datasets: as exhibited by Fig. 2, it out-performs CutMix, MIMO and deep ensembles, at (almost) the same inference cost as a single network. 2.