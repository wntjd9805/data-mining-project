Abstract
Motion is an important cue for video prediction and of-ten utilized by separating video content into static and dy-namic components. Most of the previous work utilizing mo-tion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explic-itly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without his-tory already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the state-of-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challeng-ing real-world autonomous driving datasets with complex motion and dynamic background. 1.

Introduction
Videos contain visual information enriched by motion.
Motion is a useful cue for reasoning about human activities or interactions between objects in a video. Given a few initial frames of a video, our goal is to predict several frames into the future, as realistically as possible. By looking at a few frames, humans can predict what will happen next.
Surprisingly, they can even attribute semantic meanings to random dots and recognize motion patterns [15]. This shows the importance of motion to infer the dynamics of the video and to predict the future frames.
Motion cues have been heavily utilized for future frame prediction in computer vision. A common approach is to factorize the video into static and dynamic components
[30, 20, 22, 6, 9, 21, 14, 28]. First, most of the previous methods are deterministic and fail to model the uncertainty of the future. Second, motion is typically interpreted as local
Figure 1: Comparison of the first prediction frames (11th)
SLAMP (left) vs. state-of-the-art method, SRVP [8] (right) on KITTI [10] (top) and Cityscapes [3] (bottom) datasets.
Our method can predict both foreground and background objects better than SRVP. Full sequence predictions can be seen in Supplementary. changes from one frame to the next. However, changes in motion follow certain patterns when observed over some time interval. Consider scenarios where objects move with near-constant velocity, or humans repeating atomic actions in videos. Regularities in motion can be very informative for future frame prediction. In this work, we propose to explicitly model the change in motion, or the motion history, for predicting future frames.
Stochastic methods have been proposed to model the in-herent uncertainty of the future in videos. Earlier methods encode the dynamics of the video in stochastic latent vari-ables which are decoded to future frames in a deterministic way [4]. We first assume that both appearance and motion are encoded in the stochastic latent variables and decode them separately into appearance and motion predictions in a deterministic way. Inspired by the previous deterministic methods [7, 20, 9], we also estimate a mask relating the two. Both appearance and motion decoders are expected to predict the full frame but they might fail due to occlusions around motion boundaries. Intuitively, we predict a proba-bilistic mask from the results of the appearance and motion decoders to combine them into a more accurate final predic-tion. Our model learns to use motion cues in the dynamic parts and relies on appearance in the occluded regions. 1
eration are mostly deterministic, therefore failing to capture uncertainty of the future. There are a couple of attempts to learn multiple future trajectories from a single image with a conditional variational autoencoder [29] or to capture motion uncertainty with a probabilistic motion encoder [19]. The latter work uses separate decoders for flow and frame similar to our approach, however, predicts them only from the latent vector. We incorporate information from previous frames with additional modelling of the motion history.
Stochastic Video Generation: SV2P [1] and SVG [4] are the first to model the stochasticity in video sequences using latent variables. The input from past frames are encoded in a posterior distribution to generate the future frames. In a stochastic framework, learning is performed by maximiz-ing the likelihood of the observed data and minimizing the distance of the posterior distribution to a prior distribution, either fixed [1] or learned from previous frames [4]. Since time-variance in the model is proven crucial by the previous work, we sample a latent variable at every time step [4]. Sam-pled random variables are fed to a frame predictor, modelled recurrently using an LSTM. We model appearance and mo-tion distributions separately and train two frame predictors for static and dynamic parts.
Typically, each distribution, including the prior and the posterior, is modeled with a recurrent model such as an
LSTM. Villegas et al. [27] replace the linear LSTMs with convolutional ones at the cost of increasing the number of parameters. Castrejon et al. [2] introduce a hierarchical representation to model latent variables at different scales, by introducing additional complexity. Lee et al. [17] in-corporate an adversarial loss into the stochastic framework to generate sharper images, at the cost of less diverse re-sults. Our model with linear LSTMs can generate diverse and sharp-looking results without any adversarial losses, by incorporating motion information successfully into the stochastic framework. Recent methods model dynamics of the keypoints to avoid errors in pixel space and achieve stable learning [23]. This offers an interesting solution for videos with static background and moving foreground objects that can be represented with keypoints. Our model can gener-alize to videos with changing background without needing keypoints to represent objects.
Optical flow has been used before in future predic-tion [18, 22]. Li et al. [18] generate future frames from a still image by using optical flow generated by an off-the-shelf model, whereas we compute flow as part of prediction.
Lu et al. [22] use optical flow for video extrapolation and interpolation without modeling stochasticity. Long-term video extrapolation results show the limitation of this work in terms of predicting future due to relatively small motion magnitudes considered in extrapolation. Differently from flow, Xue et al. [31] model the motion as image differences using cross convolutions. t and xf t and motion zf t generating frames xp
Figure 2: Generative Model of SLAMP. The graphical model shows the generation process of SLAMP with motion history. There are two separate latent variables for appear-ance zp t (black).
Information is propagated between time-steps through the recurrence between frame predictions (blue), corresponding latent variables (green), and from frame predictions to latent variables (red). The final prediction Ë†xt is a weighted combi-nation of the xp t , xf t ).
Note that predictions at a time-step depend on all of the previous time-steps recurrently, but only the connections between consecutive ones are shown for clarity. t according to the mask m(xp t and xf
The proposed stochastic model with deterministic de-coders cannot fully utilize the motion history, even when motion is explicitly decoded. In this work, we propose a model to recognize regularities in motion and remember them in the motion history to improve future frame predic-tions. We factorize stochastic latent variables as static and dynamic components to model the motion history in addition to the appearance history. We learn two separate distribu-tions representing appearance and motion and then decode static and dynamic parts from the respective ones.
Our model outperforms all the previous work and per-forms comparably to the state-of-the-art method, SRVP, [8] without any limiting assumptions on the changes in the static component on the generic video prediction datasets, MNIST,
KTH and BAIR. However, our model outperforms all the pre-vious work, including SRVP, on two challenging real-world autonomous driving datasets with dynamic background and complex object motion. 2.