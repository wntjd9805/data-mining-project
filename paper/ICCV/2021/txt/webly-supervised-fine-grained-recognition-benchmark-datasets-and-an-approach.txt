Abstract
Learning from the web can ease the extreme depen-dence of deep learning on large-scale manually labeled datasets. Especially for fine-grained recognition, which it will targets at distinguishing subordinate categories, significantly reduce the labeling costs by leveraging free web data. Despite its significant practical and research value, the webly supervised fine-grained recognition problem is not extensively studied in the computer vi-sion community, largely due to the lack of high-quality datasets. To fill this gap, in this paper we construct two new benchmark webly supervised fine-grained datasets, termed WebFG-496 and WebiNat-5089, respectively.
In three sub-datasets concretely, WebFG-496 consists of containing a total of 53,339 web training images with 200 species of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196 models of cars (Web-car). For
WebiNat-5089, it contains 5089 sub-categories and more than 1.1 million web training images, which is the largest webly supervised fine-grained dataset ever. As a minor contribution, we also propose a novel webly supervised method (termed “Peer-learning”) for benchmarking these datasets. Comprehensive experimental results and analyses on two new benchmark datasets demonstrate that the proposed method achieves superior performance over the competing baseline models and states-of-the-art. Our benchmark datasets and the source codes of Peer-learning have been made available at https://github.com/
NUST-Machine-Intelligence-Laboratory/ weblyFG-dataset. 1.

Introduction
Recent success of deep learning has shown that a deep network in conjunction with abundant well-labeled training
*Corresponding author. data is the most promising approach for fine-grained recog-nition [1, 2, 19, 20]. However, even with the availability of scalable crowd-sourcing platforms like Amazon Mechani-cal Turk, constructing a large-scale fine-grained dataset like iNat2017 [4] is still an extremely difficult work since distin-guishing subtle differences among fine-grained categories (e.g., different animals [5, 11], or plants [12, 17]) usually requires domain-specific expert knowledge.
To reduce the cost of manual fine-grained annotation, many methods have been proposed, which are primarily fo-cused on semi-supervised learning [18]. These works in-evitably involve various forms of human intervention and remain labor-consuming [26]. To further reduce manual annotations as well as to learn more practical fine-grained models, training directly from web images is becoming in-creasingly popular [13, 14, 15, 16]. Nevertheless, the lack-ing of a benchmark dataset makes it difficult to fairly com-pare the performances of these algorithms. This is the mo-tivation of this work and we aim to provide a benchmark dataset for evaluating webly supervised fine-grained recog-nition algorithms.
Webly supervised fine-grained recognition consists of three challenges: (1) Label noise - Different from manually labeled datasets, fine-grained web images are often associ-ated with label noises due to the influence of error-prone automatic or non-domain-expert annotations. We consider two types of label noises, which we call “cross-domain” and
“cross-category” noises. To be specific, cross-domain noise is the portion of images that are not of any categories in the same fine-grained domain, e.g., for birds, it is the fraction of images that do not contain a bird (cf. images with purple bounding boxes in Fig. 1 (a)). In contrast, cross-category noise is the portion of images that have the wrong labels within a fine-grained class, e.g., an image of a bird with the wrong category label (cf. images with red bounding boxes in Fig. 1 (a)). According to existing works [24], deep neural networks have memorization effects, which will memorize
Figure 1. Examples of WebFG-496 and WebiNat-5089. (a) Cross-domain (purple bounding box) and cross-category (red bounding box) noises in WebFG-496. (b) Due to the variety of colors, poses and other factors, there are small inter-class and large intra-class variances among subcategories in the WebFG-496 dataset. (c) Distribution of training images per category in WebiNat-5089. The number of training images in the largest subcategory “Udea Rubigalis” is 563 while in the smallest subcategory “Hordnia Atropunctata” is only 4. incorrectly labeled training data and cause a poor general-ization performance [25]. (2) Small inter-class variance
- As shown in Fig. 1 (b), three fine-grained sub-categories have small inter-class variance while each sub-category has large intra-class variance. Aiming at recognizing hundreds of sub-categories belonging to the same super-category is an extremely challenging task. (3) Class imbalance - The nat-ural world is heavily imbalanced, as some species are much more likely to be observed [4]. As shown in Fig. 1 (c), we collected 563 web training images for “Udea Rubigalis”.
While for “Hordnia Atropunctata”, we only gathered 4 web images for training. Extreme class imbalance is a prop-erty of the real world fine-grained categories and recogni-tion models should can deal with it.
To address aforementioned issues, we leverage the cat-egories in three famous fine-grained datasets, i.e., FGVC-Aircraft [27], CUB200-2011 [3] and Stanford Cars [28], to construct a new webly supervised fine-grained benchmark dataset WebFG-496. To scale up the categories as well as to build a much larger and challenging fine-grained dataset, we reuse the categories in iNat2017 [4] to build a web version iNaturalist dataset, termed as WebiNat-5089. Compared to manually labeled iNat2017, WebiNat-5089 consists of 1,184,520 web training images, basically twice the number of training images (i.e., 579,184) in the original iNat2017.
Furthermore, we also propose a simple yet effective learning paradigm to train robust deep fine-grained mod-els directly from noisy web images. Our work is motivated by the following observations: 1) The performance of deep neural networks can be boosted by learning from wrongly classified instances (i.e., “hard examples”) [29]. 2) Deep neural networks always fit to “easy examples” first, and gradually adapt to “hard examples” [24]. 3) Distinct net-works have different learning abilities, and can collabora-tively boost their performance by mutually communicating
“useful information” [30]. Specifically, we train two deep neural networks simultaneously and let them mutually cor-rect their classification errors. For each mini-batch of web images, each network individually feeds forward all data to separately predict the labels, based on which the input data is split into two groups Gd (instances with different predic-tions) and Gs (instances with identical predictions). Then, the networks update their parameters with the instances in
Gd. Meanwhile, each network sorts and fetches small-loss instances in Gs as “useful knowledge”, and communicates this “useful knowledge” with its peer network for correcting the errors induced by instances in Gd during updating. Ex-tensive experiments on the new benchmark datasets demon-strate the effectiveness of our proposed approach. The main contributions of this work can be summarized as follows: (1) We construct two webly supervised fine-grained datasets, i.e., WebFG-496 and WebiNat-5089, for fairly benchmarking webly supervised fine-grained approaches.
Specifically, WebFG-496 consists of three sub-datasets:
Web-aircraft, Web-bird, and Web-car.
It can help re-searchers promptly verify the effectiveness of their pro-posed methods. The large-scale WebiNat-5089 contains 5089 fine-grained subcategories and more than 1.1 million web training images. To the best of our knowledge, it is the largest webly supervised fine-grained benchmark dataset. (2) We propose a novel deep learning paradigm, i.e., Peer-learning for dealing with webly supervised fine-grained recognition. Our model jointly leverages both “hard” and
“easy” examples for training, which can keep the peer net-works diverged and maintain their distinct learning abilities in removing noisy images. This strategy can alleviate both the accumulated error problem in MentorNet [55] and the consensus issue in Co-teaching [30], which thus can boost the performance of webly supervised learning. (3) We conduct extensive experiments of various base-line methods to benchmark the proposed WebFG-496 and
WebiNat-5089 datasets, as well as our Peer-learning. In ex-periments, results on WebFG-496 and WebiNat-5089 val-idate the superiority of our method over states-of-the-arts, and ablation studies justify both the merits of the proposed benchmark datasets and the effectiveness of our method. 2.