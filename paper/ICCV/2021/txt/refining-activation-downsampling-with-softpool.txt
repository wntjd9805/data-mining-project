Abstract
Convolutional Neural Networks (CNNs) use pooling to decrease the size of activation maps. This process is cru-cial to increase the receptive ﬁelds and to reduce computa-tional requirements of subsequent convolutions. An impor-tant feature of the pooling operation is the minimization of information loss, with respect to the initial activation maps, without a signiﬁcant impact on the computation and mem-ory overhead. To meet these requirements, we propose Soft-Pool: a fast and efﬁcient method for exponentially weighted activation downsampling. Through experiments across a range of architectures and pooling methods, we demon-strate that SoftPool can retain more information in the re-duced activation maps. This reﬁned downsampling leads to improvements in a CNN’s classiﬁcation accuracy. Ex-periments with pooling layer substitutions on ImageNet1K show an increase in accuracy over both original architec-tures and other pooling methods. We also test SoftPool on video datasets for action recognition. Again, through the direct replacement of pooling layers, we observe consistent performance improvements while computational loads and memory requirements remain limited1. 1.

Introduction
Pooling layers are essential in convolutional neural net-works (CNNs) to decrease the size of activation maps. They reduce the computational requirements of the network while also achieving spatial invariance, and increase the receptive
ﬁeld of subsequent convolutions [6, 27, 43].
A range of pooling methods has been proposed, each with different properties (see Section 2). Most architectures use maximum or average pooling, both of which are fast and memory-efﬁcient but leave room for improvement in terms of retaining important information in the activation map.
We introduce SoftPool, a kernel-based pooling method that uses the softmax weighted sum of activations. We largely preserves descriptive demonstrate that SoftPool 1Code is available at: http://www.tinyurl.com/softpool
Figure 1. SoftPool illustration. The original image is sub-sampled with a 2 × 2 (k = 2) kernel. The output is based on the exponentially weighted sum of the original pixels within the ker-nel region. This can improve the representation of high-contrast regions, present around object edges or speciﬁc feature activations. activation features, while remaining computationally and memory-efﬁcient. Owing to better feature preservation, models that include SoftPool consistently show improved classiﬁcation performance compared to their original im-plementations. We make the following contributions:
• We introduce SoftPool: a novel pooling method based on softmax normalization that can be used to down-sample 2D (image) and 3D (video) activation maps.
• We demonstrate how SoftPool outperforms other pool-ing methods in preserving the original features, mea-sured using image similarity.
• Experimental results on image and video classiﬁcation tasks show consistent improvement when replacing the original pooling layers by SoftPool.
The remainder of the paper is structured as follows. We
ﬁrst discuss related work on feature pooling. We then de-tail SoftPool (Section 3) and evaluate it in terms of feature loss and image and video classiﬁcation performance over multiple pooling methods and architectures (Section 4). 1
Figure 2. Pooling variants. R is the set of pixel values in the kernel neighborhood. (a,b) Average and maximum pooling are based on averaging and maximum activation selection in a kernel. (c) Power Average pooling (Lp) [10, 14] is proportional to average pooling raised to a power (p). The output is equal to max-pool for p → ∞, and sum pooling when p = 1. (d) Stochastic pooling [47] outputs a randomly selected activation from the kernel region. (e) Stochastic Spatial Sampling (S3Pool) [48] samples random horizontal and vertical regions given a speciﬁed stride. (f) Local Importance Pooling (LIP) [12] uses a trainable sub-net G to enhance speciﬁc features. (g) SoftPool (ours) exponentially weighs the effect of activations using a softmax kernel. 2.