Abstract
Synthesized Scene
Recent advancements in differentiable rendering and 3D reasoning have driven exciting results in novel view syn-thesis from a single image. Despite realistic results, meth-In order ods are limited to relatively small view change. to synthesize immersive scenes, models must also be able to extrapolate. We present an approach that fuses 3D rea-soning with autoregressive modeling to outpaint large view changes in a 3D-consistent manner, enabling scene synthe-sis. We demonstrate considerable improvement in single-image large-angle view synthesis results compared to a va-riety of methods and possible variants across simulated and real datasets. In addition, we show increased 3D consis-tency compared to alternative accumulation methods. 1.

Introduction
Imagine that you walk into the ofﬁce shown in Figure 1.
What will you see if you turn right? Is there a door onto a patio? What if you step backward then look left? While the image itself does not contain this information, you can imagine a rich world behind the image due to your experi-ence of other rooms. This task of single-image scene syn-thesis promises to bring arbitrary photos to life, but requires solving several key challenges. First, handling large view changes involves extrapolation far beyond the input pixels.
Second, generating multiple outputs from the same input re-quires consistency: turning left by 10◦ or 20◦ should reveal progressively more of a single underlying world. Finally, modeling view changes requires 3D-awareness to properly capture perspective changes.
Prior methods for view synthesis fall short of these goals.
There has been great progress at interpolating between many input views of a single scene [29, 28, 43, 44, 48, 50]; while these 3D-aware methods generate consistent out-puts, they do not attempt to extrapolate beyond their in-put views. Prior approaches to single-image view synthe-sis [15, 47, 53] can extrapolate to small rotations and trans-lations, but fail to model viewpoint changes at this scale.
For example, we show that na¨ıvely retraining SynSin [53]
Input Image
Figure 1: Single-Image Scene Synthesis. Our framework fuses the complementary strengths of 3D reasoning and autoregressive modeling to create an immersive scene from a single image. for larger angles leads to collapse.
In parallel, autoregressive models have shown impres-sive results for image generation and completion [4, 30, 35, 40, 49]. These methods are very successful at extrapolating far beyond the boundaries of an input image; however they make no attempt to explicitly model a consistent 3D world behind their generated images.
In this paper we present an approach for single-image scene synthesis that addresses these challenges by fusing the complementary strengths of 3D reasoning and autore-gressive modeling. We achieve extrapolation using an au-toregressive model to complete images when faced with large view changes. Generating all output views indepen-dently would give inconsistent outputs. Instead, we iden-tify a support set at the extremes of views to be generated (shown at the boundaries of Figure 1). Generated images for the support set are then lifted to 3D and added to a con-sistent scene representation. Intermediate views can then be re-rendered from the scene representation instead of gener-ated from scratch, ensuring consistency among all outputs.
Producing a system that can both do extreme view syn-thesis and lift the results to 3D without requiring auxiliary data poses a challenge. Our approach, described in Sec-tion 3, builds upon insights from both the view synthesis and autoregressive modeling communities. Each image and new viewpoint yields a large and custom region to be ﬁlled in, which we approach by adapting VQVAE2 [35] and ex-panding Locally Masked Convolutions [18] to learn image-speciﬁc orderings for outpainting. Once ﬁlled, we obtain 3D using techniques from SynSin [53]. This system can outpaint large and diverse regions, accumulate outpainting in 3D, and can be trained without any supervision beyond images and 6DOF relative pose.
We evaluate our approach as well as a variety of al-ternative methods and competing approaches on standard datasets, Matterport 3D+Habitat [3, 41] and RealEstate10K
[63], using substantially larger angle changes (6× larger than [53]). Throughout the experiments in Section 4, we evaluate with the standard metrics of human judgments,
PSNR, Perceptual Similarity, and FID. Our experimental results suggest that: (1) Our proposed approach produces meaningfully better results compared to training existing methods on our larger viewpoints. In particular, users select our approach 73% of the time vs. the best variant of multi-ple SynSin ablations. (2) Our approach of re-rendering sup-port sets outperforms alternate iterative approaches, being more consistent an average of 72% of images. 2.