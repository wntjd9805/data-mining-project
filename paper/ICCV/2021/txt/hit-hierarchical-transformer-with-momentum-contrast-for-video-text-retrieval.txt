Abstract
Video-Text Retrieval has been a hot research topic with the growth of multimedia data on the internet. Transformer for video-text learning has attracted increasing attention due to its promising performance. However, existing cross-modal transformer approaches typically suffer from two major limitations: 1) Exploitation of the transformer archi-tecture where different layers have different feature charac-teristics is limited; 2) End-to-end training mechanism limits negative sample interactions in a mini-batch. In this paper, we propose a novel approach named Hierarchical Trans-former (HiT) for video-text retrieval. HiT performs Hierar-chical Cross-modal Contrastive Matching in both feature-level and semantic-level, achieving multi-view and compre-hensive retrieval results. Moreover, inspired by MoCo, we propose Momentum Cross-modal Contrast for cross-modal learning to enable large-scale negative sample interactions on-the-ﬂy, which contributes to the generation of more pre-cise and discriminative representations. Experimental re-sults on the three major Video-Text Retrieval benchmark datasets demonstrate the advantages of our method. 1.

Introduction
Cross-modal Retrieval [58, 10, 13, 67, 8, 3, 35, 9, 11, 46, 59, 60, 25, 57, 29, 42] has attracted the increasing at-tention with the aim to search the semantic similar samples from different modalities. Specially, the explosive growth of video contents on the internet has brought great chal-lenges to accurate video-text retrieval.
In this paper, we focus on the learning of video-text retrieval and also hope to inspire other cross-modal tasks.
∗Corresponding author
Figure 1. Hierarchical Cross-modal Contrastive Matching consists of Feature-level and Semantic-level Contrastive Matching. No-tably, Momentum Cross-modal Contrast is not shown in this ﬁg-ure.
Recent works [49, 70, 13, 39, 12] have shown that trans-former can learn high level video representations, which capture semantically meaningful and temporally long-range structures for videos. Notably, existing approaches for cross-modal learning can be roughly categorized as two-stream, single-stream and dual stream architectures. Two-stream architecture, as shown in Figure 2-(a), utilizes a vision transformer and a text transformer to learn visual and textual representations independently, then introduces a multi-modal transformer [32, 70, 50] to achieve cross-modal information exchange. Singe-stream architecture
[28, 27, 47, 19], as shown in Figure 2-(b), fuses visual and textual representations at the initial stage of the transformer model. However, these two architectures are not suitable for large-scale cross-modal retrieval tasks, due to the re-on-the-ﬂy. We name it as Momentum Cross-modal Contrast (MCC). In MCC, we build several memory banks to save a rich set of negative representations, which help broader negative sample interactions during training. However, if we utilize video and text encoders that are updated dramat-ically by gradient descent to generate representations for memory banks, it would result in the representation incon-sistency in memory banks, thus largely affect the retrieval performance. Hence, key encoders for two modalities with momentum update (updated more smoothly) are required to maintain representation consistency.
Contributions: We propose Hierarchical Transformer (HiT) with Momentum Contrast for Video-Text Retrieval, which jointly performs Hierarchical Cross-modal Con-trastive Matching and Momentum Cross-modal Contrast.
Extensive experiments demonstrate the advantages of the proposed methods on three benchmarks, including MSR-VTT, ActivityNet and LSMDC. 2.