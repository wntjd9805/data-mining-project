Abstract
While single-view 3D reconstruction has made signifi-cant progress benefiting from deep shape representations in recent years, garment reconstruction is still not solved well due to open surfaces, diverse topologies and complex geometric details. In this paper, we propose a novel learn-able Anchored Unsigned Distance Function (AnchorUDF) representation for 3D garment reconstruction from a sin-gle image. AnchorUDF represents 3D shapes by predict-ing unsigned distance fields (UDFs) to enable open gar-ment surface modeling at arbitrary resolution. To capture diverse garment topologies, AnchorUDF not only computes pixel-aligned local image features of query points, but also leverages a set of anchor points located around the surface to enrich 3D position features for query points, which pro-vides stronger 3D space context for the distance function.
Furthermore, in order to obtain more accurate point pro-jection direction at inference, we explicitly align the spa-tial gradient direction of AnchorUDF with the ground-truth direction to the surface during training. Extensive experi-ments on two public 3D garment datasets, i.e., MGN and
Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art performance on single-view garment re-construction. Code is available at https://github. com/zhaofang0627/AnchorUDF. 1.

Introduction 3D garment reconstruction has a wide range of applica-tions in clothed human digitization, virtual try-on, online shopping and so on. Recently, image based 3D reconstruc-tion has made significant progress benefiting from shape representation learning with deep neural networks [14, 16, 19, 35, 26]. Compared to voxels, points and meshes, im-plicit functions, which define a surface as a level set of a function, can represent 3D surfaces at arbitrary resolu-*Corresponding author. tion and produce fine-scale detailed surfaces in a memory-efficient way, and have been successfully applied to single-view human reconstruction [32, 33]. However, recovering 3D garment shape from a single image is still a challeng-ing task because garments have open surfaces and diverse topologies in addition to complex geometric details like clothed humans.
Existing methods usually use parametric models to pro-vide shape priors of garments [11, 22, 40]. BCNet [22] introduces a layered garment representation on top of the
SMPL body model [25] and a generic skinning weights gen-erating network to improve the expression ability of the gar-ment model. Deep Fashion3D [40] proposes adaptable tem-plate meshes to fit garment shapes and incorporates implicit representations to refine surface details. These methods rely on pre-defined category-specific templates and have poor scalability to new garment categories.
In this paper, we propose Anchored Unsigned Dis-tance Function (AnchorUDF), a learnable unsigned dis-tance function that enriches 3D position features of query points by anchor points around 3D surfaces. With Ancho-rUDF, we establish a unified shape learning framework to reconstruct 3D garment from a single image. As shown in
Fig. 1, our method can handle non-closed garment surfaces, capture multiple topologies of different garment categories while retaining fine-scale geometric details.
Specifically, for a 3D query point, like PIFu [32], we first compute its pixel-aligned local image features. How-ever, instead of using an absolute depth value to encode the 3D position of the query point, we leverage a set of anchor points representing 3D shape profile to enrich its position feature to make the distance function better sense the gar-ment topologies. To obtain a small number of anchor points which can adequately cover the surface, we cluster points sampled from the surface with k-means and use cluster-ing centers as targets to learn a regression network on top of the backbone to predict anchor points. Note that com-pared to the full shape, these few anchor points (typically a few hundred) are easier to estimate. A 3D convolutional
Figure 1. Single-view garment reconstruction using our method, which handles non-closed garment surfaces, captures multiple topologies of different garment categories and retains fine-scale geometric details. network is further employed to compute 3D feature ten-sors from anchor points and a 3D position feature vector for the query point can be extracted via trilinear interpo-lation with its 3D coordinates, which encodes the relative position relationship between query and anchor points and provides stronger 3D space context information compared to the absolute depth value. Different from signed distance functions [29], we need to compute the gradient field of un-signed ones at inference to project the query point onto the surface along the negative gradient direction. Thus, in or-der to obtain more accurate estimation for the projection direction, we explicitly constrain the spatial gradient of our
AnchorUDF during training to align its direction with the ground-truth direction to the surface.
Our contributions can be summarized as follows: 1) We propose a unified shape learning framework for single-view garment reconstruction; 2) We introduce AnchorUDF, a learnable unsigned distance function with anchored 3D po-sition features; 3) We propose to learn the unsigned distance function with gradient direction alignment to more accu-rately project query points at inference. 2.