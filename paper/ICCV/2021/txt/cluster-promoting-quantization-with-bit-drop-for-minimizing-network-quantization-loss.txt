Abstract 1.

Introduction
Network quantization, which aims to reduce the bit-lengths of the network weights and activations, has emerged for their deployments to resource-limited devices. Although recent studies have successfully discretized a full-precision network, they still incur large quantization errors after train-ing, thus giving rise to a significant performance gap be-tween a full-precision network and its quantized counter-part. In this work, we propose a novel quantization method for neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal quantization grids while naturally en-couraging the underlying full-precision weights to gather around those quantization grids cohesively during training.
This property of CPQ is thanks to our two main ingredi-ents that enable differentiable quantization: i) the use of the categorical distribution designed by a specific probabilistic parametrization in the forward pass and ii) our proposed multi-class straight-through estimator (STE) in the backward pass. Since our second component, multi-class STE, is in-trinsically biased, we additionally propose a new bit-drop technique, DropBits, that revises the standard dropout reg-ularization to randomly drop bits instead of neurons. As a natural extension of DropBits, we further introduce the way of learning heterogeneous quantization levels to find proper bit-length for each layer by imposing an additional regular-ization on DropBits. We experimentally validate our method on various benchmark datasets and network architectures, and also support a new hypothesis for quantization: learn-ing heterogeneous quantization levels outperforms the case using the same but fixed quantization levels from scratch.
*Equal contribution
Deep neural networks have achieved great success in various computer vision applications. However, the state-of-the-art neural network architectures including ResNet [12] generally require too much computation and memory to be deployed to resource-limited devices. Therefore, researchers have explored diverse approaches to compress them to re-duce memory usage and computation cost.
α + 1
Among them, network quantization aims to reduce the bit-width of network parameters while maintaining competi-tive performance of a full-precision counterpart. One of the simplest methods is to round a weight or an activation of a network x to (cid:98)x = α⌊ x 2 ⌋ where α controls the grid interval size. However, this na¨ıve approach incurs severe performance degradation mainly due to the quantization loss.
Given that if the underlying full-precision weights x are clus-tered well around the optimal quantization grids, the perfor-mance difference between before and after the quantization can be marginal so that the performance of full-precision net-work can be preserved even with the quantized parameters.
Hence, we focus on jointly finding the optimal quantization grids and clustering the underlying full-precision weights x around those quantization grids cohesively.
Some recent studies in fact have experimentally con-firmed that their methods can partially give a clustering effect in the quantization process. VNQ [2] clusters the underly-ing full-precision weights x around quantization grids using multi-spike-and-slab prior, but it is restricted only to ternary precision. RQ [19] experimentally shows some clustering effects around several modes in low bit-width, but it does not equip any algorithm that explicitly encourages clustering around quantization grids. As a result, both methods incur a considerable performance gap between a full-precision network and its quantized counterpart.
In order to preserve the performance of a full-precision
network in low bit-width, we propose the Cluster-Promoting
Quantization (CPQ) that not only finds the optimal quantiza-tion grids but also encourages the underlying full-precision weights x to gather around those quantization grids cohe-sively in low bit-length regimes. Although CPQ does not have any explicit regularization or loss for clustering, the combination of the following two key components results in better clustering effect (and thus final performance) both theoretically and experimentally: i) choosing the mode of the categorical distribution parametrized by a particular proba-bilistic approach in the forward pass and ii) taking advantage of our multi-class straight-through estimator (STE) in the backward pass.
As our multi-class STE is biased like the original STE for the binary case [3], we present a novel bit-drop technique named DropBits to reduce the bias of the multi-class STE in CPQ. Motivated from Dropout [27], DropBits drops bits rather than neurons/filters to train low-bit neural networks under CPQ framework.
In addition, DropBits allows heterogeneous quantization, which learns different bit-width per parameter/channel/layer by dropping redundant bits. DropBits with learnable bit-drop rates adaptively finds out the optimal bit-width for each group of parameters, possibly further reducing the overall bits. In contrast to recent studies [30, 29] in heterogeneous quantization that exhibit almost all layers have at least 4-bit, up to 10-bit, our method yields much more resource-efficient low-bit neural networks with at most 4 bits for all layers.
With trainable bit-widths, we also articulate a new hy-pothesis for quantization where one can find the learned bit-width network (termed a ‘quantized sub-network’) which can perform better than the network with the same but fixed bit-widths from scratch.
Our contribution is threefold:
• We propose a new quantization method, Cluster-Promoting Quantization (CPQ) that not only finds the optimal quantization grids but also encourages the un-derlying full-precision weights to congregate around those quantization grids cohesively in low bit-width regimes by the combination of a particular probabilistic parametrization for discretization and our multi-class straight-through estimator. We further present a novel bit-drop technique coined DropBits to reduce the bias of the multi-class straight-through estimator in CPQ.
• Extending DropBits technique, we propose a more resource-efficient heterogeneous quantization algo-rithm to curtail redundant bit-widths across groups of weights and/or activations (e.g. across layers) and verify that our method is able to find out ‘quantized sub-networks’.
• We conduct extensive experiments on several bench-mark datasets to demonstrate the effectiveness of our method. We accomplish new state-of-the-art results for ResNet-18 and MobileNetV2 on the ImageNet dataset when all layers are uniformly quantized. 2.