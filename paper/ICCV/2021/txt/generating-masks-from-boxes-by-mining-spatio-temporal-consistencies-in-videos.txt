Abstract 1.

Introduction
Segmenting objects in videos is a fundamental computer vision task. The current deep learning based paradigm of-fers a powerful, but data-hungry solution. However, current datasets are limited by the cost and human effort of annotat-ing object masks in videos. This effectively limits the per-formance and generalization capabilities of existing video segmentation methods. To address this issue, we explore weaker form of bounding box annotations.
We introduce a method for generating segmentation masks from per-frame bounding box annotations in videos.
To this end, we propose a spatio-temporal aggregation mod-ule that effectively mines consistencies in the object and background appearance across multiple frames. We use our predicted accurate masks to train video object segmenta-tion (VOS) networks for the tracking domain, where only manual bounding box annotations are available. The ad-ditional data provides substantially better generalization performance, leading to state-of-the-art results on standard tracking benchmarks. The code and models are available at https://github.com/visionml/pytracking.
Segmenting objects in videos is an important but chal-lenging task with many applications in autonomous driv-ing [51, 55], surveillance [11, 15] and video editing. The field has been driven by the astonishing performance of deep learning based approaches [7, 44, 58]. However, these methods require large amount of training images with pixel-wise annotations. Manually annotating segmentation masks in videos is an extremely time-consuming and costly task.
Existing video datasets with segmentation labels [49, 63] therefore do not provide the large-scale diversity desired in deep learning. This effectively limits the potential of cur-rent state-of-the-art approaches.
To address this issue, it is tempting to consider weaker forms of human annotations. In particular, object bound-ing boxes offer an interesting alternative. Boxes provide horizontal and vertical constraints on the extent of the seg-mentation mask, while also being substantially faster to an-notate. Hence, a method of effectively leveraging bound-ing box annotations for training video segmentation models would greatly simplify the process of deploying these mod-els for novel domains. Ideally, simply converting the video
box annotations to object masks would allow existing video segmentation approaches to integrate these annotations us-ing standard supervised techniques, without requiring any modification of losses or architectures. Such a conversion network can itself be trained using available mask annotated data. We therefore investigate the problem of generating ob-ject segmentations from box-annotations in videos.
Generating masks from box-annotated videos is a decep-tively challenging task. The background scene is often clut-tered or contains similar objects. Objects can change ap-pearance rapidly and often undergo heavy occlusions. Ex-isting approaches [38, 60] only address the single frame case, where these ambiguities are difficult, or sometimes impossible to resolve due to the limited information. How-ever, the aforementioned problems can be greatly alleviated if we can utilize multiple frames in the video. As the object moves relative to the background, we can find consistencies over several example views of the object and background.
While object regions should consistently stay inside the box, background patches can move from inside to outside the object box over the duration of the video sequence. For instance, in Fig. 1 the single frame approach fails to prop-erly segment the scooter due to the background car. In con-trast, our video-based approach can identify the car as back-ground in earlier and later frames while the scooter is con-sistently within the box for all frames. The car is therefore easily excluded from the final segmentation in all frames.
Effectively exploiting the information encoded in the temporal information is however a highly challenging prob-lem. Since the object and background moves and changes in each frame, standard fusion operations cannot extract the desired consistencies and relations. Instead, we propose a spatio-temporal aggregation module by taking inspiration from the emerging direction of deep declarative networks
[17]. Our module is formulated as an optimization problem that aims to find the underlying object representation that best explains the observed object and background appear-ance in each frame. It allows our approach to mine spatio-temporal consistencies by jointly reasoning about all image patches in all input frames. The resulting mask embeddings for each frame are then processed by a decoder to generate the final segmentation output.
Contributions: Our main contributions are as follows. (i) We propose a method for predicting object masks from (ii) We develop a spatio-bounding boxes in videos. temporal aggregation module that effectively mines the ob-ject and background information over multiple frames. (iii)
Through an iterative formulation, we can further refine the masks through a second aggregation module. (iv) We utilize our method to annotate large-scale tracking datasets with object masks, which are then utilized to extend Video Ob-ject Segmentation (VOS) to the tracking domain. effectiveness of our approach in the limited data domain.
Moreover, we show that the data generated by our approach allows VOS methods to cope with challenges posed by the tracking setting. An existing VOS approach [7] trained on our pseudo-annotated tracking videos achieves state-of-the-art performance on standard tracking benchmarks, achiev-ing an EAO score of 0.510 on VOT2020, and 86.7 AO on
GOT-10k validation set. Code, models and generated anno-tations will be made publicly available. 2.