Abstract
Self-supervised pretraining has been shown to yield pow-erful representations for transfer learning. These perfor-mance gains come at a large computational cost however, with state-of-the-art methods requiring an order of mag-nitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning sig-nal per image, leading to state-of-the-art transfer accuracy on a variety of downstream tasks, while requiring up to less pretraining. In particular, our strongest ImageNet-10 pretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000
⇥ more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.
⇥ 1.

Introduction
Since the AlexNet breakthrough on ImageNet, transfer learning from large labeled datasets has become the dom-inant paradigm in computer vision [34, 50]. While re-cent advances in self-supervised learning have alleviated the dependency on labels for pretraining, they have done so at a tremendous computational cost, with state-of-the-art methods requiring an order of magnitude more computa-tion than supervised pretraining [7, 10, 21]. Yet the promise of self-supervised learning is to harness massive unlabeled datasets, making its computational cost a critical bottleneck.
In this work, we aim to alleviate the computational bur-den of self-supervised pretraining. To that end we intro-duce contrastive detection, a new objective which maxi-mizes the similarity of object-level features across augmen-tations. The beneﬁts of this objective are threefold. First, it extracts separate learning signals from all objects in an image, enriching the information provided by each train-ing example for free—object-level features are simply ob-Figure 1. Efﬁcient self-supervised pretraining with DetCon.
Self-supervised pretraining with SimCLR [9] matches the trans-fer performance of supervised pretraining only when given 10
⇥ more training iterations. Our proposed DetCon objective surpasses less computation than SimCLR. Transfer both, while requiring 5 performance is measured by ﬁne-tuning the representation on the
COCO dataset for 12 epochs, using a Mask-RCNN.
⇥ tained from intermediate feature arrays. Second, it provides a larger and more diverse set of negative samples to con-trast against, which also accelerate learning. Finally, this objective is well suited to learning from complex scenes with many objects, a pretraining domain that has proven challenging for self-supervised methods.
We identify approximate object-based regions in the im-age through the use of unsupervised segmentation algo-rithms. Perceptual grouping [32, 41]—the idea that low and mid-level regularities in the data such as color, orien-tation and texture allow for approximately parsing a scene into connected surfaces or object parts—has long been the-orized to be a powerful prior for vision [22, 40, 56]. We leverage these priors by grouping local feature vectors ac-cordingly, and applying our contrastive objective to each object-level feature separately. We investigate the use of
several unsupervised, image-computable masks [17, 2], and
ﬁnd our objective to work well despite their inaccuracies.
We test the ability of our objective to quickly learn transferable representations by applying it to the ImageNet dataset and measuring its transfer performance on challeng-ing tasks such as COCO detection and instance segmenta-tion, semantic segmentation on PASCAL and Cityscapes, and NYU depth estimation. Compared to representations obtained from recent self-supervised objectives such as
SimCLR and BYOL [9, 21], our representations are more accurate and can be obtained with much less training time.
We also ﬁnd this learning objective to better handle images of more complex scenes, bridging the gap with supervised transfer from the COCO dataset. In summary, we make the following contributions: 1. We formulate a new contrastive objective which max-imizes the similarity across augmentations of all objects in a scene, where object regions are provided by a simple, un-supervised heuristic.
⇥ 2. We ﬁnd this objective to alleviate the computational burden of self-supervised transfer learning, reducing by up the computation required to match supervised trans-to 10 fer learning from ImageNet. Longer training schedules lead to state-of-the-art transfer to COCO detection and instance segmentation, and our best model matches the very recent state-of-the-art self-supervised system SEER [20] which is trained on 1000 more—if less curated—images. 3. When transferring from complex scene datasets such as COCO, our method closes the gap with a supervised model which learns from human-annotated segmentations. 4. Finally, we assess to what extent the existing con-trastive learning paradigm could be simpliﬁed in the pres-ence of high quality image segmentations, raising questions and opening avenues for future work.
⇥ 2.