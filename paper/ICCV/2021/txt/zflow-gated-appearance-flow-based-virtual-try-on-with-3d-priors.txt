Abstract
Image-based virtual try-on involves synthesising percep-tually convincing images of a model wearing a particular garment and has garnered significant research interest due to its immense practical applicability. Recent methods in-volve a two stage process: i) warping of the garment to align with the model ii) texture fusion of the warped gar-ment and target model to generate the try-on output.
Is-sues arise due to the non-rigid nature of garments and the lack of geometric information about the model or the gar-ment. It often results in improper rendering of granular de-tails. We propose ZFlow, an end-to-end framework, which seeks to alleviate these concerns regarding geometric and textural integrity (such as pose, depth-ordering, skin and neckline reproduction) through a combination of gated ag-gregation of hierarchical flow estimates termed Gated Ap-pearance Flow, and dense structural priors at various stage of the network. ZFlow achieves state-of-the-art results as observed qualitatively, and on quantitative benchmarks of image quality (PSNR, SSIM, and FID). The paper presents extensive comparisons with other existing solutions includ-ing a detailed user study and ablation studies to gauge the effect of each of our contributions on multiple datasets. 1.

Introduction
With recent socio-cultural events accelerating the shift towards online commerce, there is an increasing interest in providing smart and intuitive experiences [19, 27, 3, 1, 6, 22] that can compensate for the lack of in-store interaction.
Virtual try-on is concerned with the visualization of clothes in a personalized setting and is of great importance to a plethora of real world applications. While attractive even
*equal contribution
†work done while working at Adobe MDSR Lab
‡work done as part of Adobe MDSR internship
Figure 1. Image-based virtual try-on involves synthesizing a try-on output where the target model is wearing the in-shop garment while other characteristics of the model and garment are preserved.
The above output is generated by our proposed method ZFlow before the renaissance of deep learning [37, 15, 8], recent advances in generative networks have inspired researchers to pursue image-based virtual try-on [41, 19, 40, 13, 43], based solely on RGB images, by formulating the problem as that of conditional image synthesis.
Given as input the images of an isolated in-shop gar-ment and a target model, the objective of image-based vir-tual try-on is to synthesise a perceptually convincing new image (referred to as the try-on output) where the target model is wearing the in-shop garment (Figure 1). Recent methods employ a two step process consisting of: a) warp-ing of in-shop garment to align with pose and body shape of the target model and, b) texture fusion of the warped gar-ment and target model images to generate the try-on output.
A successful try-on experience depends upon synthesizing a sharp, realistic image that preserves the textural and geo-metric integrity of both the garment and model. Issues arise from improper warping or incorrect texture fusion due to the non-rigid nature of garments and the lack of understanding of the 3D geometry of the garment and the model. This re-sults in unconvincing rendering of granular clothing details.
Alleviating these concerns is the focus of this work.
Recent research [14, 40, 19, 41] has been directed to-wards these challenges. [14, 40] proposed thin-plate spline (TPS) based warping of the garment image.
[19, 41] improve the stability of TPS warping via multi-stage cas-caded parameter estimation, and second order difference constraints respectively. However, TPS based warping leads to inaccurate transformation estimation when large geomet-ric deformation is required, since each parameter defines the spatial deformation for a coarse block of pixels. To address this issue, [13] proposes to use dense, per-pixel appearance flow [45] prediction to spatially deform the garment image.
But owing to the high degree of freedom and the absence of proper regularization, this method often causes drastic deformation during warping resulting in significant textural artefacts. To address both issues - the inability of TPS to handle large deformations, and over-warping with appear-ance flows - we introduce Gated Appearance Flow (GAF) which regularizes per-pixel appearance flow by aggregating candidate flow estimates predicted across multiple scales.
Next, for improving texture fusion, especially the issue of bleeding colors, [19, 41] propose to use an apriori esti-mate of target clothing segmentation for the try-on output as conditioning. However, this method results in ambiguities in depth perception and body-part ordering because of the absence of 3D geometric priors. This is prominently visi-ble in the generation of necklines, and handling cases with occlusion. For example, part of the garment that should go behind the neck appears in the front. To encode the 3D geometry information, we combine UV projection maps with dense body-part segmentation (obtained via Dense-Pose [12]) as priors during warping and texture fusion.
Our contributions can be summarized as follows:
• We propose ZFlow, an end-to-end try-on framework, that utilizes gated appearance flow estimates and dense geometric priors to render high quality try-on results.
• We present extensive quantitative and qualitative com-parisons as well as a detailed user study to show sig-nificant improvement over state-of-the-art methods.
• We present ablation studies to analyse impact of dif-ferent design choices in ZFlow. We further reinforce the efficacy of GAF by adapting it to improve state-of-the-art for human pose transfer. 2.