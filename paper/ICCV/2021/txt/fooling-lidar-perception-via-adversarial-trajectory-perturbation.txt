Abstract
LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When au-tonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estima-tion that is susceptible to wireless spooﬁng? We demon-strate such possibilities for the ﬁrst time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spooﬁng of a self-driving car’s trajectory with small perturbations is enough to make safety-critical objects undetectable or de-tected with incorrect positions. Moreover, polynomial tra-jectory perturbation is developed to achieve a temporally-smooth and highly-imperceptible attack. Extensive experi-ments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art de-tectors effectively, but also transfer to other detectors, rais-ing a red ﬂag for the community. The code is available on https://ai4ce.github.io/FLAT/. 1.

Introduction
Autonomous driving systems are generally equipped with all kinds of sensors to perceive the complex environ-ment [7]. Among the sensors, LiDAR has played a crucial role due to its plentiful geometric information sampled by incessantly spinning a set of laser emitters and receivers.
However, LiDAR scans are easily distorted by vehicle’s mo-tion, i.e., the points in a full sweep are sampled at different timestamps when vehicle is at different locations and ori-entations, as shown in Fig. 1. Imagine that a self-driving car is on a highway at a speed of 30m/s, its LiDAR with a 20Hz scanning frequency would move 1.5 meters during a
* indicates equal contribution. The work is done during Congcong’s visit at NYU. Chen Feng is the corresponding author.
Spoofer
GNSS Signal (cid:2202)=10ms (cid:2202)=20ms (cid:2202)=30ms (cid:2202)=100ms
Perception
Original Results
…
Spoofed Results
Motion 
Correction
Undetected
Real Trajectory
Spoofed Trajectory
Undetected
Figure 1: Illustration of our trajectory-based attack and the motion correc-tion process. The top right and bottom right ﬁgures are respectively origi-nal and distorted LiDAR sweep as well as the detection results. Green/red
In this example, our method boxes denote the ground truth/prediction. (named as FLAT) makes the detector miss eight out of eleven vehicles. full sweep, severely distorting the captured point cloud.
Such distortions are typically compensated by querying the vehicle/LiDAR’s pose at any time from a continuous vehicle ego-motion tracking module that fuses pose estima-tion from Global Navigation Satellite System (GNSS, e.g.,
GPS, GLONASS, and BeiDou), Inertial Navigation Sys-tem (INS), and SLAM-based localization using LiDAR or cameras. Well-known LiDAR datasets like KITTI [7] and nuScenes [2] have already corrected such motion distor-tions prior to the dataset release. Researchers then made impressive progresses by processing those distortion-free point clouds using deep neural networks (DNNs) for many tasks, e.g., 3D object detection, semantic/instance segmen-tation, motion prediction, multiple object tracking.
However, using DNNs on LiDAR point clouds cre-ates a potentially dangerous and less cognizant vulnera-bility in self-driving systems. First, the above perception tasks are functions of LiDAR point clouds implemented via DNNs. Second, the motion compensation makes Li-DAR point clouds a function of the vehicle trajectories.
This functional composition leads to a simple but surpris-ing fact that those perception tasks are now also func-tions of the trajectories. Thus, such a connection exposes the well-known adversarial robustness issue of DNNs to
hackers who could now fool a self-driving car’s safety-critical LiDAR-depending perception modules by calcu-latedly spooﬁng the area’s wireless GNSS signals, which is still a serious and unresolved security problem demon-strated on many practical systems [23, 30, 52]. Luckily, given that the aforementioned non-GNSS pose sources such as INS and visual SLAM are fused together for vehicle ego-motion estimation, large variations (meter-level) of GNSS trajectory spooﬁng could be detected and ﬁltered, ensuring a safe localization and mapping for self-driving cars. How-ever, what if a spoofed trajectory only has dozens of cen-timeters offset? Would current point cloud DNNs be robust enough under such small variations?
In this paper, we initiate the ﬁrst work to reveal and in-vestigate such a vulnerability. Different from existing works directly attacking point cloud coordinates with 3D point perturbations or adversarial object generation [9, 36, 41, 44], we propose to fool LiDAR-based perception modules by attacking the vehicle trajectory, which could be detri-mental and easily deployable in the physical world. Our investigation includes how to obtain LiDAR sweeps with simulated motion distortion from real-world datasets, con-vert LiDAR point clouds as a differentiable function of the vehicle trajectories, and eventually calculate the adversarial trajectory perturbation and make them less perceptible. Our principal contributions are as follows:
• We propose an effective approach for simulating mo-tion distortion using a sequence of real-world LiDAR sweeps from existing dataset.
• We propose a novel view of LiDAR point clouds as a differentiable function of the vehicle trajectories, based on the real-world motion compensation process. to Fool LiDAR perception with
Adversarial Trajectory (FLAT), which has better feasibility and transferability.
• We propose
• We conduct extensive experiments on 3D object detec-tion as a downstream task example, and show that the advanced detectors can be effectively blinded. 2.