Abstract tion. We demonstrate that our approach achieves state-of-the-art performance both qualitatively and quantitatively.
We present a framework for training GANs with explicit control over generated facial images. We are able to control the generated image by settings exact attributes such as age, pose, expression, etc. Most approaches for manipulating
GAN-generated images achieve partial control by leverag-ing the latent space disentanglement properties, obtained implicitly after standard GAN training. Such methods are able to change the relative intensity of certain attributes, but not explicitly set their values. Recently proposed meth-ods, designed for explicit control over human faces, harness morphable 3D face models (3DMM) to allow ﬁne-grained control capabilities in GANs. Unlike these methods, our control is not constrained to 3DMM parameters and is ex-tendable beyond the domain of human faces. Using con-trastive learning, we obtain GANs with an explicitly dis-entangled latent space. This disentanglement is utilized to train control-encoders mapping human-interpretable inputs to suitable latent vectors, thus allowing explicit control. In the domain of human faces we demonstrate control over identity, age, pose, expression, hair color and illumination.
We also demonstrate control capabilities of our framework in the domains of painted portraits and dog image genera-1.

Introduction
Generating controllable photorealistic images has appli-cations spanning a variety of ﬁelds such as cinematogra-phy, graphic design, video games, medical imaging, virtual communication and ML research. For faces in particular, impressive breakthroughs were made. As an example, in the ﬁlm industry, computer generated characters are replac-ing live actor footage. Earlier work on controlled face gen-eration primarily relied on 3D face rig modeling [32, 43], controlled by 3D morphable face model parameters, such as 3DMM [9, 19]. While easily controllable, such methods tend to suffer from low photorealism. Other methods that rely on 3D face scanning techniques may provide highly photorealistic images, but at a signiﬁcant cost and limited variability. Recent works on high resolution images synthe-sis using generative adversarial networks (GANs) [21] have demonstrated the ability to generate photorealistic faces of novel identities, indistinguishable from those of real hu-mans [27, 29, 30]. However, these methods alone lack in-terpretability and control over the generative process, com-pared to the 3D graphic alternatives.
These results have inspired the community to explore ways to beneﬁt from both worlds – generating highly pho-torealistic faces using GANs while controlling their ﬁne-grained attributes, such as pose, illumination and expression with 3DMM-like parameters. Deng et al. [15], Kowalski et al. [31] and Tewari et al. [50] introduce explicit control over
GAN-generated faces, relying on guidance from 3D face generation pipelines. Along with the clear beneﬁts, such as the precise control and perfect ground truth, reliance on such 3D face models introduces new challenges. For ex-ample, the need to overcome the synthetic-to-real domain gap [31, 15]. Finally, all these methods’ expressive power is bounded by the capabilities of the model they rely on. In particular, it is not possible to control human age if the 3D modeling framework does not support it. It is also impos-sible to apply the same framework to different but similar domains, such as paintings or animal faces, if these assets are not supported by the modeling framework. All of these stand in the way of creating a simple, generic and extend-able solution for explicitly controllable GANs.
In this work we present a uniﬁed approach for training a
GAN to generate high-quality, controllable images. Speciﬁ-cally, we demonstrate our approach in the domains of facial portrait photos, painted portraits and dogs (see Fig. 1). We depart from the use of the highly detailed 3D face mod-els [15, 31, 50] in favor of supervision signals provided by a set of pre-trained models, each controlling a different feature. We show that our approach signiﬁcantly simpli-ﬁes the generation framework, does not compromise image quality or control accuracy, and allows us to control addi-tional aspects of facial appearances, which cannot be mod-eled by graphical pipelines. We achieve this by combining several concepts. We construct the GAN’s latent space as a composition of sub-spaces, each corresponding to a spe-ciﬁc property. During training, we enforce images gener-ated by identical latent sub-vectors to have similar prop-erties, as predicted by some off-the-shelf model. Respec-tively, images generated by different latent sub-vectors are enforced to have different predicted properties. As a result, disentanglement between the latent sub-spaces is achieved.
Finally, to allow for human-interpretable control, for each attribute we train an encoder converting values from its fea-sible range to its corresponding sub-latent space. As an ad-ditional application, we present a novel image projection approach suitable for disentangled latent spaces.
We summarize our contributions as following: 1. We present a novel state-of-the art approach for train-ing explicitly controllable, high-resolution GANs. 2. Our approach is extendable to attributes beyond those supported by 3D modeling and rendering frameworks, making it applicable to additional domains. 3. We present a disentangled projection method that en-ables real image editing. 2.