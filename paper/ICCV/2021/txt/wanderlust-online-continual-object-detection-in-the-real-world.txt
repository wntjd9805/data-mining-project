Abstract 1.

Introduction
Online continual learning from data streams in dynamic environments is a critical direction in the computer vision
ﬁeld. However, realistic benchmarks and fundamental stud-ies in this line are still missing. To bridge the gap, we present a new online continual object detection benchmark with an egocentric video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate stu-dent. OAK provides exhaustive bounding box annotations of 80 video snippets (∼17.5 hours) for 105 object categories in outdoor scenes. The emergence of new object categories in our benchmark follows a pattern similar to what a sin-gle person might see in their day-to-day life. The dataset also captures the natural distribution shifts as the person travels to different places. These egocentric long running videos provide a realistic playground for continual learn-ing algorithms, especially in online embodied settings. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide base-line studies for online continual object detection. We believe this benchmark will pose new exciting challenges for learn-ing from non-stationary data in continual learning. The
OAK dataset and the associated benchmark are released at https://oakdata.github.io/.
Modern object detectors have made substantial progress on internet images [5, 16, 37]. Nevertheless, challenges re-main when detecting small objects [7], scaling to a large number of categories [15] or learning from only a few la-beled examples [19, 44]. The detector often degenerates signiﬁcantly when deployed on robots or ego-centric videos in an embodied environment [8].
If we take a closer look at the typical learning setup, most of the advances in object detection have been realized using static images in an ofﬂine learning setup. In this setup, the data is labeled with a ﬁxed set of categories and divided into two parts: training and testing. There is a training phase where the detectors are learned by randomly shufﬂing and feeding the training data for hundreds of epochs, followed by an evaluation on the test set. However, this ofﬂine train-ing and evaluation setup often does not reﬂect how humans or embodied AI agents learn.
Unlike the current static ofﬂine settings, humans receive a continuous temporal stream of visual data and train and test the model on the same visual data, which is an online continual setting. The categories of interest are unknown beforehand. The model needs to learn new object categories when objects belonging to previously unseen categories ap-pear. Most of the learning happens online; we cannot use the training data repeatedly across hundreds of epochs.
A side effect of this online continual learning setting is 1
catastrophic forgetting [30]. Though previous works [1, 21, 29, 40, 48] attempt to address the issue, they are usually evaluated ofﬂine and do not work well on structural predic-tion tasks like object detection. What hinders the progress in online continual learning is the lack of realistic datasets and benchmarks. Most of the current research [32, 41, 49] re-purpose existing static datasets such as VOC and COCO to evaluate continual object detection. These approaches use object categories one by one in a sequential manner.
These manual splits and artiﬁcial setups differ from the sce-narios often encountered by embodied agents, where the emergence of new tasks often follows the trajectories of the agents and the frequencies of the object instances vary from task to task. For example, the agents might observe the in-stances of the same category after a few hours or even days.
They may visit some objects more often than others and re-visit previously observed objects.
In this paper, we present a new online continual object detection benchmark. Our benchmark consists of a new labeled dataset – OAK (Objects Around Krishna). OAK uses the videos from the KrishnaCam [43] dataset – an ego-centric video dataset collected over nine months of a grad-uate student’s life. OAK contains 80 labeled video snippets totaling around 17.5 hours (roughly 1/4 of the raw videos in
KrishnaCam) with bounding box annotations of 105 object categories in outdoor scenes. OAK provides a natural data distribution and the task emergence following the trajecto-ries of a single person. A few objects frequently appear due to redundancy in daily routines, while new objects and cat-egories constantly appear as they visit various places. This dataset is a realistic playground to study online continual learning, enables the embodied agent to learn from a hu-man’s experience, and provides a unique opportunity for re-searchers to pursue the essence of lifelong learning by ob-serving the same person in a long time span.
We introduce several new evaluation metrics in the on-line continual learning setup. In contrast to the previous task incremental or class incremental settings in continual learn-ing, there is no explicit task boundary in our setup and new tasks emerge following the temporal order in the videos.
Therefore, we evaluate the overall performance (continual average precision, CAP), transfer (backward/forward trans-fer, BWT/FWT), and forgetting (forgetfulness, F) of the models with an additional temporal dimension. We evalu-ate the models periodically on the frames held out from the same training video frames. The overall performance is ag-gregated from these evaluations and the transfer/forgetting is deﬁned by the time intervals between the appearances of instances from the same tasks. We adapt several typical con-tinual learning algorithms (e.g., iCaRL [35], EWC [21], In-cremental ﬁne-tuning) to object detection and ﬁnd the per-formance of these approaches mediocre in the new bench-mark, which leaves substantial room for future studies. 2.