Abstract
We present a method that takes as input a single dual-pixel image, and simultaneously estimates the image’s de-focus map—the amount of defocus blur at each pixel—and recovers an all-in-focus image. Our method is inspired from recent works that leverage the dual-pixel sensors available in many consumer cameras to assist with autofocus, and use them for recovery of defocus maps or all-in-focus images.
These prior works have solved the two recovery problems independently of each other, and often require large labeled datasets for supervised training. By contrast, we show that it is beneficial to treat these two closely-connected prob-lems simultaneously. To this end, we set up an optimization problem that, by carefully modeling the optics of dual-pixel images, jointly solves both problems. We use data captured with a consumer smartphone camera to demonstrate that, after a one-time calibration step, our approach improves upon prior works for both defocus map estimation and blur removal, despite being entirely unsupervised. 1.

Introduction
Modern DSLR and mirrorless cameras feature large-aperture lenses that allow collecting more light, but also in-troduce defocus blur, meaning that objects in images appear blurred by an amount proportional to their distance from the focal plane. A simple way to reduce defocus blur is to stop down, i.e., shrink the aperture. However, this also re-duces the amount of light reaching the sensor, making the image noisier. Moreover, stopping down is impossible on fixed-aperture cameras, such as those in most smartphones.
More sophisticated techniques fall into two categories. First are techniques that add extra hardware (e.g., coded aper-tures [46], specialized lenses [47, 15]), and thus are imprac-tical to deploy at large scale or across already available cam-eras. Second are focus stacking techniques [76] that capture multiple images at different focus distances, and fuse them into an all-in-focus image. These techniques require long capture times, and thus are applicable only to static scenes.
Ideally, defocus blur removal should be done using data
*Work primarily done when Shumian Xin was an intern at Google.
Figure 1: Given left and right dual-pixel (DP) images and corresponding spatially-varying blur kernels, our method jointly estimates an all-in-focus image and defocus map. from a single capture. Unfortunately, in conventional cam-eras, this task is fundamentally ill-posed: a captured image may have no high-frequency content because either the la-tent all-in-focus image lacks such frequencies, or they are removed by defocus blur. Knowing the defocus map, i.e., the spatially-varying amount of defocus blur, can help sim-plify blur removal. However, determining the defocus map from a single image is closely-related to monocular depth estimation, which is a challenging problem in its own right.
Even if the defocus map were known, recovering an all-in-focus image is still an ill-posed problem, as it requires hallucinating the missing high frequency content.
Dual-pixel (DP) sensors are a recent innovation that makes it easier to solve both the defocus map estimation and defocus blur removal problems, with data from a sin-gle capture. Camera manufacturers have introduced such sensors to many DSLR and smartphone cameras to improve autofocus [2, 36]. Each pixel on a DP sensor is split into two halves, each capturing light from half of the main lens’ aper-ture, yielding two sub-images per exposure (Fig. 1). These can be thought of as a two-sample lightfield [61], and their sum is equivalent to the image captured by a regular sensor.
Figure 2: Overview of our proposed method. We use input left and right DP images to fit a multiplane image (MPI) scene representation, consisting of a set of fronto-parallel layers. Each layer is an intensity-alpha image containing the in-focus scene content at the corresponding depth. The MPI can output the all-in-focus image and the defocus map by blending all layers. It can also render out-of-focus images, by convolving each layer with pre-calibrated blur kernels for the left and right
DP views, and then blending. We optimize the MPI by minimizing a regularized loss comparing rendered and input images.
The two sub-images have different half-aperture-shaped de-focus blur kernels; these are additionally spatially-varying due to optical imperfections such as vignetting or field cur-vature in lenses, especially for cheap smartphone lenses.
We propose a method to simultaneously recover the de-focus map and all-in-focus image from a single DP cap-ture. Specifically, we perform a one-time calibration to de-termine the spatially-varying blur kernels for the left and right DP images. Then, given a single DP image, we op-timize a multiplane image (MPI) representation [77, 91] to best explain the observed DP images using the calibrated blur kernels. An MPI is a layered representation that ac-curately models occlusions, and can be used to render both defocused and all-in-focus images, as well as produce a de-focus map. As solving for the MPI from two DP images is under-constrained, we introduce additional priors and show their effectiveness via ablation studies. Further, we show that in the presence of image noise, standard optimization has a bias towards underestimating the amount of defocus blur, and we introduce a bias correction term. Our method does not require large amounts of training data, save for a one-time calibration, and outperforms prior art on both defocus map estimation and blur removal, when tested on images captured using a consumer smartphone camera. We make our implementation and data publicly available [85]. 2.