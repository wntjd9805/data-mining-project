Abstract
The unsupervised domain adaptation (UDA) has been widely adopted to alleviate the data scalability issue, while the existing works usually focus on classifying independently discrete labels. However, in many tasks (e.g., medical diag-nosis), the labels are discrete and successively distributed.
The UDA for ordinal classification requires inducing non-trivial ordinal distribution prior to the latent space. Tar-get for this, the partially ordered set (poset) is defined for constraining the latent vector. Instead of the typically i.i.d.
Gaussian latent prior, in this work, a recursively conditional
Gaussian (RCG) set is adapted for ordered constraint mod-eling, which admits a tractable joint distribution prior. Fur-thermore, we are able to control the density of content vector that violates the poset constraints by a simple “three-sigma rule”. We explicitly disentangle the cross-domain images into a shared ordinal prior induced ordinal content space and two separate source/target ordinal-unrelated spaces, and the self-training is worked on the shared space exclu-sively for ordinal-aware domain alignment. Extensive exper-iments on UDA medical diagnoses and facial age estimation demonstrate its effectiveness. 1.

Introduction
Deep learning is typically data-starved and relies on the i.i.d assumption of training and testing sets [5, 32]. How-ever, the real-world deployment of target tasks is usually significantly diverse, and massive labeling of the target do-main data can be expensive or even prohibitive [29]. To address this, the unsupervised domain adaptation (UDA) is developed, which proposes to learn from both labeled source domain and unlabeled target domain [12, 40, 39].
The existing UDA works [37, 61, 33, 17, 40] usually fo-Figure 1. (Left) Illustration of 4-class latent vectors (points) aligned in a poset formation in R2. For a vector ck, its adjoined horizontal and vertical lines specify the feasible quadrant where its superiors ck+i for i > 0 can be positioned. (Right) Conditional spacing model p(c1, c2, c3) = p(c1)p(c2|c1)p(c3|c2) for 3-class. The adopted Gaussian conditional densities (4–7) and the constraints (8) enforces the sample from p(c1, c2, . . . , cK ) satisfy c1 ≤ c2 ≤
· · · ≤ cK with high probability [48, 10, 23]. cus on the classification or segmentation tasks without con-sidering the inter-class correlations. However, tasks with dis-crete and successively labels are commonly seen in the real world. For instance, the Diabetic Retinopathy (DR) has five labels corresponding to different severity levels: 0→normal, 1→mild, 2→moderate, 3→severe, and 4→proliferative DR.
In ordinal setting, class y = 1 is closer to y = 2 than y = 4.
The misclassification of y = 1 to y = 2 or y = 4 may lead to different severity of misdiagnosis. The ordinal labeling sys-tem is widely adopted in medical diagnose, face age groups, and facial expression intensity, etc. Limited by the collecting cost, the UDA has great potential in the medical area, while the UDA for the ordinal label is under-presented.
A promising solution for UDA is to jointly learn the disentanglement of class-related/unrelated factors and the adaptation on the class-relevant latent space exclusively [60].
The well-performed model is expected to extract the class la-bel informative and domain invariant features [61]. However, 1
the disentanglement and adaptation of ordinal factors can be challenging [2]. The i.i.d. Gaussian distribution sets are usu-ally adopted as the prior of both disentangled class-related and unrelated latent space [2], which inherently postulates that the class is independently categorical.
In contrast to the i.i.d. categorical assumption in the con-ventional disentanglement framework MLVAE [2] with the vanilla Gaussian prior in VAEs [24], we propose a different setting that the extracted class-related factor c is also ordinal-the corresponding ordinal label y. In other valued w.r.t. words, the value of class-related factor c are also ordered.
We note that if y is an ordinal class label, i.e., y ∈ {1 < 2 < · · · K}, we would expect embed this ordinal structure to form the ordinal class-related latent space. Specifically, for two instances whose class labels are close to each other, their respective extracted ordinal class-related latent vectors c1 and c2 are expected to be approximate, and vice versa
[23]. For the triplet y1 < y2 < y3, we have:
||c1 − c3|| > max{||c1 − c2||, ||c2 − c3||}, (1) which aligns the class-related latent vectors and the ordinal class labels, thus yielding a model following the true data causal inference.
However, it is non-trivial to induce the ordinal inter-class correlation to the disentangled class-related latent space.
Actually, directly enforcing the triplet-constraints in Eq. 1 as regularization-term for the variational lower bound of disentanglement framework is almost intractable. For the
D-dimensional c, there is O(D2) inequalities for every pair of (ci, cj), and for each inequality, we may have to intro-duce a slack variable to be optimized [23]. Moreover, this heuristic regularization cannot guarantee Eq. 1 is satisfied in the embedded latent space.
Motivated by the aforementioned insights, we adopted an effective and principled manner for ordinal class-related factor modeling by constructing an appropriate prior for the extracted class-related latent space. Rather than the i.i.d. standard Gaussian prior [24] to the non-ordinal disentangle-ment methods [2], our prior is constructed by a recursively conditional Gaussian (RCG) set, which explicitly imposes a restrictive partially ordered set (poset) [48, 10, 23] con-straint. Since the joint prior is Gaussian, although with a full covariance, inheriting the closed form of the KL divergence term in the variational objective. Moreover, the prior can be fully factorized over the latent dimensions, which make the framework computational tractable. Furthermore, the number of parameters in the adapted prior needed to enforce the constraint for all of the pairs in a K class task can be
O(D · K), further reinforcing the model’s scalability [23].
This prior is able to assign negligible density that violates the poset constraint by a simple “three-sigma rule”. For the noisy labeled dataset, the confidence interval can be flexibly adjusted to achieve more conservative ordinal modeling.
Our contributions are summarized as follows:
• We propose to investigate the UDA of ordinal classifi-cation in a joint disentanglement and adaptation framework.
• An effective and principled ordinal prior for class-related latent space is constructed by a recursively condi-tional Gaussian (RCG) set [48, 10, 23]. More appealingly, we can adjust the ratio of poset violation to adapt the noise of the ordinal label. The closed form of the KL divergence with our adapted prior is computationally tractable for large-scale tasks.
• We extensively evaluate our method on both medical di-agnosis (i.e., DR and CHD) and age estimation, and observe a significant improvement over the previous non-ordinal approaches in UDA. 2.