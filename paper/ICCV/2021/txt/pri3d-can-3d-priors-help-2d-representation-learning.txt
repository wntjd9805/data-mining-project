Abstract 1.

Introduction
Recent advances in 3D perception have shown impres-sive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in ge-ometric understanding, we aim to imbue image-based per-ception with representations learned under geometric con-straints. We introduce an approach to learn view-invariant, geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effec-tively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view im-age constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learn-ing on the image-based tasks of semantic segmentation, in-stance segmentation and object detection on real-world in-door datasets, but moreover, provides significant improve-ment in the low data regime. We show significant improve-ment of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet.
Our code is open sourced at https://github.com/
Sekunde/Pri3D.
In recent years, we have seen rapid progress in learning-based approaches for semantic understanding of 3D scenes, particularly in the tasks of 3D semantic segmentation, 3D object detection, and 3D semantic instance segmentation
[40, 8, 51, 28, 22, 17, 12, 29, 38]. Such approaches lever-age geometric observations, exploiting the representation of points [40, 41], voxels [8, 22], or meshes [28] to obtain ac-curate 3D semantics. These have shown significant promise towards realizing applications such as depth-based scene understanding for robotics, as well as augmented or virtual reality. In parallel to the development of such methods, the availability of large-scale RGB-D datasets [46, 27, 3, 7], has further accelerated the research in this area.
One advantage of learning directly in 3D in contrast to learning solely from 2D images is that methods operate in metric 3D space; hence, it is not necessary to learn view-dependent effects and/or projective mappings. This allows training 3D neural networks from scratch in a relatively short time frame and typically requires a (relatively) small number of training samples; e.g., state-of-the-art 3D neu-ral networks can be trained with around 1000 scenes from
ScanNet. Our main idea is to leverage these advantages in
the form of 3D priors for image-based scene understanding.
Simultaneously, we have seen tremendous progress on representation learning in the image domain, mostly pow-ered by the success of recent contrastive learning based methods [54, 18, 4, 15, 2]. The exploration in 2D rep-resentation learning heavily relies on the paradigm of in-stance discrimination, where different augmented copies of the same instance are drawn closer. Different invariances can be encoded from those low-level augmentations such as random cropping, flipping and scaling, as well as color jittering. However, despite the common belief that 3D view-invariance is an essential property for a capable visual sys-tem [33], there remains little study linking the 3D priors and 2D representation learning. The goal of our work is to explore the combination of contrastive representation learn-ing with 3D priors, and offer some preliminary evidence to-wards answering an important question: can 3D priors help 2D representation learning?
To this end, we introduce Pri3D, which aims to learn with 3D priors in a pre-training stage and subsequently use them as initialization for fine-tuning on image-based down-stream tasks such as semantic segmentation, detection, and instance segmentation. More specifically, we introduce ge-ometric constraints to a contrastive learning scheme, which are enabled by multi-view RGB-D data that is readily avail-able. We propose to exploit geometric correlations through implicit multi-view constraints between different images through the correspondence of pixels which correspond to the same geometry, as well as explicit correspondence of geometric patches which correspond to image regions. This imbues geometric knowledge into the learned representa-tions of the image inputs which can then be leveraged as pre-trained features for various image-based vision tasks, particularly in the low training data regime.
We demonstrate our approach by pre-training on Scan-Net [7] under these geometric constraints for representation learning, and show that such self-supervised pre-training (i.e., no semantic labels are used) results in improved per-formance on 2D semantic segmentation, instance segmen-tation and detection tasks. We demonstrate this not only on ScanNet data, but also generalizing to improved per-formance on NYUv2 [46] semantic segmentation, instance segmentation and detection tasks. Moreover, leveraging such geometric priors for pre-training provides robust fea-tures which can consistently improve performance under a wide range of amount of training data available. While we focus on indoor scene understanding in this paper, we be-lieve our results can shed light on the the paradigm of repre-sentation learning with 3D priors and open new opportuni-ties towards more general 3D-aware image understanding.
In summary, our contributions are:
• A first exploration of the effect of 3D priors for 2D image understanding tasks, where we demonstrate the benefit of 3D geometric pre-training towards complex 2D perception such as semantic segmentation, object detection, and instance segmentation.
• A new pre-training approach based on 3D-guided view-invariant constraints and geometric priors from color-geometry correspondence, which learns features that can be transferred to 2D representations, comple-menting and improving image understanding across multiple datasets. 2.