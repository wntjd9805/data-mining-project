Abstract
Three dimensional hand pose estimation has reached a level of maturity, enabling real-world applications for single-hand cases. However, accurate estimation of the pose of two closely interacting hands still remains a challenge as in this case, one hand often occludes the other. We present a new algorithm that accurately estimates hand poses in such a challenging sce-nario. The crux of our algorithm lies in a framework that jointly trains the estimators of interacting hands, leveraging their inter-dependence. Further, we employ a GAN-type discriminator of interacting hand pose that helps avoid physically implausible configurations, e.g. intersecting fingers, and exploit the visibil-ity of joints to improve intermediate 2D pose estimation. We incorporate them into a single model that learns to detect hands and estimate their pose based on a unified criterion of pose estimation accuracy. To our knowledge, this is the first attempt to build an end-to-end network that detects and estimates the pose of two closely interacting hands (as well as single hands).
In the experiments with three datasets representing challenging real-world scenarios, our algorithm demonstrated significant and consistent performance improvements over state-of-the-arts. 1.

Introduction
Estimating hand pose finds numerous applications including augmented and virtual reality, sign language recognition, and gesture-based interfaces. The past decade has observed significant progress in this field thanks to advances in deep learning techniques. In particular, for isolated hands, skeletal pose estimation techniques are mature enough for use in practical applications. As such, recent effort has focused on challenging cases where one estimates e.g. the pose of hands captured in egocentric camera views [28, 54, 13] or interacting with objects [47, 7, 8, 14, 4], or restores hand shape as well as their skeletal pose [24, 3, 5, 57]. However, only recently, has attention been paid to estimating the pose of two interacting hands. This problem is challenging as interacting hands often cause severe self-occlusions (see Fig. 1 for examples).
Most existing work in this scenario takes generative, model fitting-based approaches, e.g. on depth maps [30, 49, 27, 42]
Figure 1: Example hand detection and 3D pose estimation results on the InterHand2.6M dataset [25]: (Rows 1–2) skeletal joints (overlaid on the inputs) estimated by Moon et al.’s state-of-the-art system [25] and ours, respectively. Ours provides more accurate pose estimation than [25] when two hands closely interact with each other (last two columns). For interacting hands, our joint estimation process is guided by the predicted joint visibility as shown in the third row (yellow: invisible, green and red: visible). For single-hands, visibility is not estimated (see supplemental for cases where single-hand joint visibility is also used). The last row visualizes our results in different views. or RGB images [52] while very recently, convolutional neu-ral network (CNN)-based discriminative learning approaches have been investigated [25, 21]. CNN-based approaches have been particularly successful in addressing occlusions occurring in the context of egocentric views or objects under interac-1
tion [28, 54, 13, 47, 7, 8, 14, 4]. However, applying these tech-niques to cases of interacting hands has been limited due to the lack of training data: The Ego3D dataset [21] provides syn-thetic hands of simulated characters from Mixamo while Tzionas dataset [49] provides few two-hand examples provided with 2D skeleton annotations. These datasets are limited in their scale, es-pecially in their coverage of closely interacting hands cases. Re-cent InterHand2.6M dataset provides million-scale real images captured by multi-view cameras [25]. Still, closely interacting cases (with the bounding box intersection over union, IOU score larger than 0.5) therein are limited to around 18,000 instances.
In this paper, we propose a new CNN-based hand pose estimation framework. Our system is trained on existing datasets (Ego3D and InterHand2.6M) containing limited instances of interacting hands. However, when tested on interacting hands, it provides an accuracy level comparable to that of state-of-the-art systems on single-hand pose estimation cases.
Our approach builds upon a hypothesis that the visible hands contain useful information for inferring the pose of occluded hands. We experimentally validate this via a statistical test of independence across the joint positions of closely interacting hands, and instantiate this into a new framework that leverages such dependence by jointly estimating their pose. Further, we exploit the structural dependence of two hands by training a
GAN-type discriminator helping avoid physically implausible joint hand configurations, e.g. two intersecting fingers. We also explicitly estimate visibility of the each joint and incorporate this information to improve 2D pose estimation.
To facilitate the training of the hand pose estimator in this scenario, we embed a hand detection network into our frame-work and classify the detected hands into (closely) interacting and non-interacting (or single-hand) categories, which are subsequently fed to the respective pose estimators. This enables us 1) to tailor our hand pose estimation system to challenging cases of interacting hands (via the pose estimator of interacting cases) while still retaining state-of-the-art performance on single-hand cases (via the single-hand pose estimator) and 2) to train the entire system in an end-to-end manner.
To the best of our knowledge, our system is the first end-to-end trainable pipeline that performs both detection and pose estimation of a single- or (two) interacting hands. In the experi-ments with Ego3D [21], InterHands2.6M [25], and Tzionas [49], we demonstrate that our joint estimation approach significantly improves upon 1) the baseline system that independently estimates hands and 2) state-of-the-art pose estimation systems. 2.