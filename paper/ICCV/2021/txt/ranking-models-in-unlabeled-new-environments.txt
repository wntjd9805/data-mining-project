Abstract
Consider a scenario where we are supplied with a num-ber of ready-to-use models trained on a certain source do-main and hope to directly apply the most appropriate ones to different target domains based on the models’ relative performance. Ideally we should annotate a validation set for model performance assessment on each new target en-vironment, but such annotations are often very expensive.
Under this circumstance, we introduce the problem of rank-ing models in unlabeled new environments. For this prob-lem, we propose to adopt a proxy dataset that 1) is fully labeled and 2) well reflects the true model rankings in a given target environment, and use the performance rank-ings on the proxy sets as surrogates. We first select labeled datasets as the proxy. Specifically, datasets that are more similar to the unlabeled target domain are found to better preserve the relative performance rankings. Motivated by this, we further propose to search the proxy set by sampling images from various datasets that have similar distributions as the target. We analyze the problem and its solutions on the person re-identification (re-ID) task, for which sufficient datasets are publicly available, and show that a carefully constructed proxy set effectively captures relative perfor-mance ranking in new environments. Code is avalible at https://github.com/sxzrt/Proxy-Set. 1.

Introduction
In real-world applications, it is not uncommon to see models trained on the source domain (hereafter called source models) directly applied to unlabeled new target en-vironments (hereafter called target domains) at the price of employing some unsupervised domain adaptation (UDA) techniques [15, 32, 19]. Assume that one has access to a pool of source models and can choose appropriate ones.
Under this context, it is desirable to obtain the relative per-formance of different models on the target domain without having to annotate data in the target environment.
To find the appropriate models, we usually evaluate each individual model on a labeled partition (e.g., a validation
Figure 1: Illustration of the proposed problem and a gen-eral solution. Given various models (blue circles) trained on source data (hereon denoted as source models) and an unlabeled target domain, we aim to rank them and find the best one for direct deployment to the target. A: Without ac-cess to image labels, this objective is unlikely to be achieved using only the target data. B: We find a proxy to rank the model performance and use this (red) ranking as a surro-gate. Specifically, this proxy should 1) be fully labeled and 2) well reflect the (green) true ranking on the target. set) of the target environment and rank them to find the best one (see Fig. 1 A). However, annotation is often expensive to obtain, and it becomes prohibitive if we consider data labeling for every new application scenario. As such, an interesting question arises: can we estimate model rankings in new environments in the absence of ground truth labels?
In this work, we aim to find a proxy (or surrogate) to rank the models in answer to the aforementioned question.
Specifically, we focus on the person re-identification (re-ID) task, which aims to retrieve persons of the same identity across multiple cameras. For this problem, it is desirable that the proxy can provide similar model rankings, since a target validation set is difficult to acquire in practice. To this end, the proxy should satisfy: 1) have labels for evaluation and 2) well reflect the true model rankings (see Fig. 1 B).
For the first requirement (labels), we can either use the target dataset with pseudo labels, or other labeled datasets.
However, due to the nature of pseudo labels, some of them might not be accurate. Existing works find that the inaccu-rate pseudo labels greatly influence the model accuracies when used in training [13]. We suspect such inaccurate pseudo labels may even do more harm when used for eval-uation. As such, we consider using labels that are real and not from the target domain.
For the second requirement (a good reflection of the true ranking), we should consider the target data distribution. If we intuitively use the model rankings on the source domain (assuming a labeled source validation set) for the ranking estimation, we might find them to be very different from the target rankings. This can often be attributed to the dis-tribution difference. For example, one model may outper-form another in a certain scenario, but their performances could be dis-similar or even reversed in a different scenario.
Therefore, in order to obtain accurate model rankings on the target domain, target data distribution should be considered.
We explore proxy sets that meet these two requirements.
First, we use existing datasets, where the labels of IDs are available.
It could be the source, an arbitrary dataset other than the source or target, or a composite one. This allows us to conveniently compute model accuracies using its labels. Second, the proxy is close to the target distribu-tion in terms of two distribution difference measurements:
Fr´echet Inception Distance (FID) [18] and feature variance gap [12, 23]. This is based on our observation that datasets more similar to the target domain (i.e., small FID and small variance gap) are more likely to form better proxies. This observation shares a similar spirit with some key findings in domain adaptation that reduced domain gap can benefit model training. Yet we derive it from a different viewpoint, i.e., the quality of a proxy set for performance ranking.
These two measurements are further investigated in a dataset search procedure. An image pool is collected from existing datasets and is partitioned into clusters. Images are sampled from each cluster with a probability proportional to the similarity (FID and variance gap) between the clus-ter and the target, forming the proxy. Overall, this paper contains the following main points.
• We study a new problem: ranking source model per-formance on an unlabeled target domain.
• We propose to use a labeled proxy that can give us a good estimation of model ranking. It is constructed via a search process such that the proxy data distribution is close to the target.
• Experiment verifies the efficacy of our method, and importantly, offers us insights into dataset similarities and model evaluation. 2.