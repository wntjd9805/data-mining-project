Abstract
To reconstruct a 3D scene from a set of calibrated views, traditional multi-view stereo techniques rely on two distinct stages: local depth maps computation and global depth maps fusion. Recent studies concentrate on deep neural architectures for depth estimation by using conventional depth fusion method or direct 3D reconstruction network by regressing Truncated Signed Distance Function (TSDF).
In this paper, we advocate that replicating the traditional two stages framework with deep neural networks improves both the interpretability and the accuracy of the results. As mentioned, our network operates in two steps: 1) the lo-cal computation of the local depth maps with a deep MVS technique, and, 2) the depth maps and images’ features fu-sion to build a single TSDF volume. In order to improve the matching performance between images acquired from very different viewpoints (e.g., large-baseline and rotations), we introduce a rotation-invariant 3D convolution kernel called
PosedConv. The effectiveness of the proposed architecture is underlined via a large series of experiments conducted on the ScanNet dataset where our approach compares favor-ably against both traditional and deep learning techniques. 1.

Introduction
Multi-view stereo (MVS) is a fundamental research topic that has been extensively investigated over the past decades [18]. The main goal of MVS is to reconstruct a 3D scene from a set of images acquired from different viewpoints. This problem is commonly framed as a cor-respondence search problem by optimizing photometric or geometric consistency among groups of pixels in differ-ent images. Conventional non-learning-based MVS frame-works [15, 2, 13] generally achieve the reconstruction using various 3D representations [12]: depth maps [15, 2], point cloud [13], voxels [29, 19], and meshes [11].
The recent use of deep neural networks for MVS [39, 20, 22, 32, 43, 4, 16] has proven effective in addressing the limi-tations of traditional techniques like repetitive patterns, low-texture regions, and reflections. Usually, deep-based MVS
Figure 1. Volume fusion. Given (a) multi-view images and their camera parameters, our network aims at 3D scene reconstruction. (b) First, we estimate local multi-view depth maps. (b) Second, we introduce differentiable depth fusion with the guidance of pose-invariant features from (d) our PosedConv. methods [43, 4, 16] center on the estimation of the pixel-wise correspondence between a reference image and its sur-rounding views. While this strategy can be elegantly inte-grated into deep learning frameworks, they can only work locally where frames largely overlap. For the full 3D re-construction of the entire scene, these methods [43, 4, 16] require to perform a depth map fusion [31, 14] to merge local reconstructions as post-processing.
More recently, Murez et al. [32] suggest that a direct re-gression of the Truncated Signed Distance Function (TSDF) volume of the scene is more effective than using intermedi-ate 3D representations, i.e. depth maps. The overall con-cept of their technique consists in the back-projection of all the extracted image features into a global scene volume from which the network directly regresses the TSDF vol-ume. This end-to-end approach [32] has the advantage of being straightforward and scalable to a large number of im-ages. However, this pioneering work [32] has difficulty in shaping the global structure of complex scenes, such as the corners of rooms or long hallways.
To address this issue, we propose to closely mimic the traditional 3D reconstruction pipeline with two distinct stages: local reconstruction and global fusion. However, unlike the previous studies [43, 10, 32] and concurrent pa-pers [38, 1], we integrate these two stages in an end-to-end manner. First, our network computes the local geome-try, i.e., dense depth maps from neighboring frames. Then, 1
we begin the depth fusion process by merging local depth maps as well as image features in a single volume represen-tation where our network regresses TSDF for the final 3D scene reconstruction. This enables our end-to-end frame-work to learn a globally consistent volumetric representa-tion in a single forward computation without the need for manually engineered fusion algorithms [37, 14]. To further enhance the robustness of our depth fusion mechanism, we propose the Posed Convolution Layer (Posed-Conv). Com-pared to the traditional 3D convolution layer that is solely invariant to translation, we propose a more versatile convo-lution layer that is invariant to both translation and rotation.
In short, our Posed Convolution Layer helps to extract pose-invariant feature representation regardless of the orientation of the input image. As a result, our method demonstrates globally consistent shape reconstruction even under wide baselines or large rotations between the views. Our contri-butions are summarized as follows:
• A novel network taking advantage of local MVS and global depth fusion for 3D scene reconstruction. 2.2. Depth Map Fusion
In their seminal work, Curless and Levoy [8] propose a volumetric depth map fusion approach able to deal with noisy depth maps through cumulative weighted signed dis-tance function. Follow-up research, such as KinectFu-sion [23] or voxel hashing [35, 25], concentrate on the prob-lem of volumetric representation via depth maps fusion. Re-cently, with the help of deep learning networks, learning-based volumetric approaches [24, 36, 40] have been pro-posed. For instance, SurfaceNet [24] and RayNet [36] infer the depth maps from multi-view images and their known camera poses. These methods are close to our strategy, but their networks are trained only by a per-view depth map which is not directly related to depth maps fusion. More re-cently, RoutedFusion [40] and Neural Fusion [41] introduce a new learning-based depth map fusion using RGB-D sen-sors. However, these papers [40, 41] concentrate on depth fusion algorithm using noisy and uncertain depth maps from multi-view images, not RGB-D sensors. To the best of our knowledge, our method is the first learning-based depth maps fusion from multi-view images for 3D reconstruction.
• A new rotation and translation invariant convolution layer, called PosedConv. 3. Volume Fusion Network 2.