Abstract
In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common compo-nents from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional col-lapse. We connect dimensional collapse with strong cor-relations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standard-izing the covariance matrix). The gains from feature decor-relation are verified empirically to highlight the importance and the potential of this insight. (a) complete collapse (b) dimensional collapse 1.

Introduction
Deep learning is prevailing in a wide range of domains, including computer vision [20], natural language process-ing [13] and speech recognition [47], while the utility of the most classical, supervised methods are sometimes re-stricted by limited or costly data labeling. Recently, self-supervised learning has proven capable of offering visual representations with high utility and therefore reducing the need for massive annotations. The past year has witnessed significant advancements in this field: A line of work fo-cuses on determining augmentations that better suit the self-supervised fashion, including revisiting typical augmenta-tions [43], using adversarial perturbations [24, 32, 23], and searching augmentation policies [36]; A line of work al-ters the sampling strategy to expand the source of positive
*Equal contribution.
†Corresponding to hangzhao@mail.tsinghua.edu.cn. (c) decorrelated (d) the concise framework
Figure 1: An overview of the key components of this work: 1a and 1b are two reachable collapse patterns in self-supervised settings; 1c is an illustration of the goal of fea-ture decorrelation; 1d is a sketch of the concise framework used in this work. pairs [49, 2, 44] and calibrate the contribution of negative samples [44, 38, 27]; Another line of work uses clustering-based mechanisms to characterize the relation of cross-sample views [5, 6, 1, 7]. Despite the technical variety, there is one high-level idea that remains in most if not all of the recent approaches, learning representations that are robust to augmentations [35, 8, 19, 43].
This idea is fairly intuitive but it does not rule out triv-ial, collapsed solutions by design. In consequence, existing work must incorporate a way that helps to mitigate the issue
of feature collapsing. Complete collapse, where the repre-sentations collapse into a constant as in Figure 1a, is the most well-known type of collapse and is addressed differ-ently in existing work with carefully chosen implementation details: To name a few, SimCLR [8] and Uniformity[45] use losses that maximize the distance between different samples; SwAV [7] includes an additional online cluster-ing branch that clusters data into a predefined number of groups; BYOL [18] relies on a predictor structure, a prop-erly inserted stop-gradient operator and a momentum en-coder; SimSiam [10] simplifies the framework of BYOL by removing the momentum encoder. Given their success in preventing complete collapse, the study of other potential collapse issues in self-supervised learning has been ignored.
Meanwhile, feature decorrelation appears to be a valu-able idea in the field of machine learning: In discrimina-tive tasks, [12, 48] introduce in objective functions addi-tional terms regularizing correlation matrices and [25, 26] develop normalization layers standardizing covariance ma-trices to obtain higher accuracy; In generative tasks, it is through feature decorrelation that [41] produces more re-alistic synthesized images and [39] offers better utility of domain adaptation.
In this work, we revisit the collapse issue of self-supervised learning and show how the idea of feature decor-relation helps to resolve the issue and improve the utility, using a framework presented in Figure 1d that contains the most common components of existing approaches. Our contribution includes:
• We verify the existence of complete collapse in self-supervised settings and address it successfully by standard-izing variance.
• We discover another reachable collapse pattern ignored by existing works, namely dimensional collapse.
• We reveal the connection between dimensional collapse and strong correlations, which leads to the idea of standard-izing covariance (i.e. feature decorrelation).
• Empirically, the performance gains from feature decorre-lation in a wide range of settings confirm the importance and the potential of this insight. 2.