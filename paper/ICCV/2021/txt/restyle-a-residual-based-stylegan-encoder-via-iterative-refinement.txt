Abstract
Recently, the power of unconditional image synthesis has signiﬁcantly advanced through the use of Generative Ad-versarial Networks (GANs). The task of inverting an image into its corresponding latent code of the trained GAN is of utmost importance as it allows for the manipulation of real images, leveraging the rich semantics learned by the net-work. Recognizing the limitations of current inversion ap-proaches, in this work we present a novel inversion scheme that extends current encoder-based inversion methods by in-troducing an iterative reﬁnement mechanism. Instead of di-rectly predicting the latent code of a given real image us-ing a single pass, the encoder is tasked with predicting a residual with respect to the current estimate of the inverted latent code in a self-correcting manner. Our residual-based encoder, named ReStyle, attains improved accuracy compared to current state-of-the-art encoder-based meth-ods with a negligible increase in inference time. We ana-lyze the behavior of ReStyle to gain valuable insights into its iterative nature. We then evaluate the performance of our residual encoder and analyze its robustness com-pared to optimization-based inversion and state-of-the-art encoders. Code is available via our project page: https:
//yuval-alaluf.github.io/restyle-encoder/ 1.

Introduction
Recently, Generative Adversarial Networks (GANs) have grown in popularity thanks to their ability to syn-thesize images of high visual quality and diversity. Be-yond their phenomenal realism and ﬁdelity on numerous domains, recent works have shown that GANs, e.g., Style-GAN [24, 25, 23], effectively encode semantic information in their latent spaces [16, 36, 21]. Notably, it has been shown that StyleGAN’s learnt latent space W has disentan-glement properties [9, 36, 46] which allow one to perform extensive image manipulations by leveraging a well-trained
StyleGAN generator. Such manipulations, however, have often been applied to synthetic images generated by the
GAN itself. To apply such edits on real images, one must
Input
Iterative Outputs −→
Figure 1. Different from conventional encoder-based inversion techniques, our residual-based ReStyle scheme incorporates an it-erative reﬁnement mechanism to progressively converge to an ac-curate inversion of real images. For each domain, we show the input image on the left followed by intermediate inversion outputs.
ﬁrst invert the given image into StyleGAN’s latent space.
That is, retrieve the latent code w such that passing w to the pre-trained StyleGAN generator returns the original im-age. To do so, it has become common practice to invert real images into an extension of W, denoted W+ [1].
Previous works have explored learning-based inversion approaches and train encoders to map a given real image into its corresponding latent code [10, 32, 50, 15, 35, 40].
Compared to per-image latent vector optimization [28, 10, 1, 2, 25], encoders are signiﬁcantly faster, as they invert using a single forward pass, and converge to areas of the latent space which are more suitable for editing [50, 40].
However, in terms of reconstruction accuracy, there remains a signiﬁcant gap between learning-based and optimization-based inversion methods. Hence, while signiﬁcant progress has been made in learning-based inversions, designing a proper encoder and training scheme remains a challenge with many works still resorting to using a per-image op-timization.
Recognizing that obtaining an accurate inversion in a single shot is difﬁcult, we introduce a novel encoder-based inversion scheme tasked with encoding real images into the extended W+ StyleGAN latent space. Unlike typical encoder-based inversion methods that infer the input’s in-verted latent code using a single forward pass, our scheme introduces an iterative feedback mechanism. Speciﬁcally, the inversion is performed using several forward passes by feeding the encoder with the output of the previous iteration along with the original input image. This allows the encoder to leverage knowledge learned in previous iterations to fo-cus on the relevant regions needed for achieving an accurate reconstruction of the input image. Viewing this formulation in terms of the latent space, our residual encoder is trained to predict the residual, or an offset, between the current la-tent code and the new latent code at each step. Doing so al-lows the encoder to progressively converge its inversion to-ward the target code and reconstruction, see Figure 1. Note also that the inversion is predicted solely using the encoder with no per-image optimization performed thereafter.
In a sense, our inversion scheme, named ReStyle, can be viewed as learning to perform a small number of steps (e.g., 10) in a residual-based manner within the latent space of a pre-trained unconditional generator. ReStyle is generic in the sense that it can be applied to various encoder architec-tures and loss objectives for the StyleGAN inversion task.
We perform extensive experiments to show that ReStyle achieves a signiﬁcant improvement in reconstruction qual-ity compared to standard feed-forward encoders. This is achieved with a negligible increase in inference time, which is still an order of magnitude faster than the time-costly optimization-based inversion. We also analyze the iterative nature of our approach. Speciﬁcally, we ﬁrst demonstrate which image regions are reﬁned at each iterative feedback step demonstrating that our scheme operates in a coarse-to-ﬁne manner. Second, we show that the absolute magnitude of change at each step decreases, with the predicted residu-als converging after only a small number of steps.
To demonstrate the generalization of ReStyle beyond the
StyleGAN inversion task and its appealing properties com-pared to current inversion techniques, we continue our anal-ysis by exploring the robustness of our scheme on down-stream tasks and special use-cases. To this end, we perform latent space manipulations [16, 36, 37] on the inverted latent codes to see if the embeddings are semantically meaningful.
We then explore an encoder bootstrapping technique allow-ing one to leverage two well-trained encoders to obtain a more faithful translation of a given real image. 2.