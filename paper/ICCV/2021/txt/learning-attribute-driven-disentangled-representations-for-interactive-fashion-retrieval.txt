Abstract
Interactive retrieval for online fashion shopping pro-vides the ability to change image retrieval results according to the user feedback. One common problem in interactive retrieval is that a specific user interaction (e.g., changing the color of a T-shirt) causes other aspects to change inad-vertently (e.g., the retrieved item has a sleeve type different than the query). This is a consequence of existing methods learning visual representations that are semantically entan-gled in the embedding space, which limits the controllability of the retrieved results. We propose to leverage on the se-mantics of visual attributes to train convolutional networks that learn attribute-specific subspaces for each attribute to obtain disentangled representations. Thus operations, such as swapping out a particular attribute value for another, impact the attribute at hand and leave others untouched.
We show that our model can be tailored to deal with dif-ferent retrieval tasks while maintaining its disentanglement property. We obtain state-of-the-art performance on three interactive fashion retrieval tasks: attribute manipulation retrieval, conditional similarity retrieval, and outfit com-plementary item retrieval. Code and models are publicly available1. 1.

Introduction
Content-based image retrieval has been a fundamental computer vision task for decades. More recently, this task has evolved in the direction of enabling users to provide additional forms of interaction (e.g., sentences, attributes and clicks) along with the query image.
Interactive im-age retrieval [11, 15, 57] is relevant in the context of on-line shopping, specifically for product categories for which appearance is one of the pre-eminent factors for selec-tion, such as fashion items. In this context, it is not only
âˆ—Work done during an internship with Amazon 1https://github.com/amzn/fashion-attribute-disentanglement
Figure 1: Disentangled representation of images with a sub-space for each attribute that can be used for attribute manip-ulation retrieval, conditional similarity retrieval and outfit complementary item retrieval. necessary to train expressive visual representations of im-ages [22, 33, 16, 42, 46], but also to empower the model with the ability of understanding interactions of the user and modify the search results accordingly.
One of the main limitations of existing methods for in-teractive image retrieval [11, 26, 15, 10, 57, 3, 36, 48] is that representations are semantically entangled in the em-bedding space. An interaction that involves a specific aspect of the image (e.g., changing the color of a T-shirt) causes other entangled aspects to change inadvertently (e.g., sleeve type or neck style).
In our work, we advocate that dis-entanglement plays a fundamental role in interactive fash-ion retrieval for obtaining more controllability and inter-pretability of the search results, and therefore handling the aforementioned limitation. We leverage on the semantics of visual attributes to train convolutional networks that learn attribute-specific subspaces via separate loss functions for each attribute. Our disentangled representations thus con-sist of the concatenation of attribute-specific embeddings, as shown in Figure 1. In this way, it is possible to apply operators directly on the desired subspace selected by the
interaction without affecting the other subspaces. Thus op-erations, such as swapping out a particular attribute value for another, impact the attribute at hand and leave others untouched.
The proposed disentangled representation is effective on several interactive retrieval tasks as showed in Fig. 1: at-tribute manipulation retrieval [57, 3], where the interaction comes from a change of attribute; conditional similarity re-trieval [45, 35], where the retrieved results are conditioned on specific attributes; and outfit complimentary item re-trieval [44, 18, 31]. In the latter problem, we are given an incomplete fashion outfit, and the user can search for items in the missing category (e.g. tops or bottoms) that are com-patible with the given outfit. Different from previous work where disentanglement is optimized to deal with one spe-cific task [45, 31, 3], our model can be tailored to preserve disentanglement of attribute-specific embeddings for differ-ent interactive retrieval tasks.
To perform attribute manipulation, we introduce a mem-ory module which stores the prototype embeddings of each attribute value. The memory module enables our model to swap out the attribute representation of the query image that should be modified with the stored prototype of the de-sired attribute. This generates a residual embedding which is composed to the attribute-specific representation of the original image to obtain the target representation, which is used for retrieval.
In order to allow the manipulation of disentangled representations, we enforce the memory block to be block-diagonal and introduce a memory-block loss to preserve its structure and update the prototype embed-dings. Moreover, we introduce a novel visual-semantic con-sistency loss that aims to align the prototype embedding projected from the attribute label with the embedding ex-tracted from the image. For the conditional similarity re-trieval task, we can directly select the subspace related to the conditioning attribute to perform the retrieval. Com-pared to [45, 35], our method is simpler, yet effective, lead-ing to state-of-the-art results.
Previous work for outfit compatibility and outfit comple-mentary item retrieval [44, 18, 31] does not investigate se-mantic disentanglement of representations. Our hypothesis is that disentanglement enables to capture complementar-ity in the different subspaces focusing on specific attributes.
In other words, a matching outfit should be composed of items that match along several attributes, e.g. in color, style, etc. We tailor our disentangled embedding via learnable at-tention weights which depend on the category of the query image and of the desired target. Our model shares similari-ties with [31], however our embedding can be separated and disentangled into attribute-specific subspaces.
To prove the effectiveness of our method, we ran experi-ments and a thorough ablation study, obtaining state-of-the-art results for: attribute manipulation retrieval, +2.63% top-10 accuracy on Shopping100k [4] and +8.58% on Deep-Fashion [33]; conditional similarity retrieval, +1.24% accu-racy on Shopping100k [4] and +0.58% on Zappos50k [54]; outfit complimentary item retrieval, +1.48% recall at 30 on
Polyvore-Outfit [44].
To summarize, our contributions are the following: 1)
We demonstrate that learning attribute-driven disentangled representations improves controllability and effectiveness of models on different interactive retrieval tasks. 2) We tai-lor our model to attribute manipulation retrieval while in-troducing a novel visual-semantic consistency loss and a block-diagonal memory module. 3) We show that disen-tangled representations can learn conditional similarity for image retrieval and compatibility for outfit complementary item retrieval. 4) We achieve state-of-the-art performance in three different applications. 2.