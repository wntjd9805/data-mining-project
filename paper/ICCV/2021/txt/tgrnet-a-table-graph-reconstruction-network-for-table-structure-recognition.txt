Abstract
A table arranging data in rows and columns is a very ef-fective data structure, which has been widely used in busi-ness and scientific research. Considering large-scale tab-ular data in online and offline documents, automatic table recognition has attracted increasing attention from the doc-ument analysis community. Though human can easily un-derstand the structure of tables, it remains a challenge for machines to understand that, especially due to a variety of different table layouts and styles. Existing methods usually model a table as either the markup sequence or the adja-cency matrix between different table cells, failing to address the importance of the logical location of table cells, e.g., a cell is located in the first row and the second column of the table. In this paper, we reformulate the problem of ta-ble structure recognition as the table graph reconstruction, and propose an end-to-end trainable table graph recon-struction network (TGRNet) for table structure recognition.
Specifically, the proposed method has two main branches, a cell detection branch and a cell logical location branch, to jointly predict the spatial location and the logical location of different cells. Experimental results on three popular ta-ble recognition datasets and a new dataset with table graph annotations (TableGraph-350K) demonstrate the effective-ness of the proposed TGRNet for table structure recogni-tion. Code and annotations will be made publicly available at https://github.com/xuewenyuan/TGRNet. 1.

Introduction
Tabular data have been widely used to help people man-age and extract important information in many real-world scenarios, including the analysis of financial documents, air pollution indices, and electronic medical records [19, 33].
Though human can easily understand tables with different
*Qingyong Li and Wen Wang are the corresponding authors.
Figure 1. A table parsing example shows the main concern of this paper. Before applying a table parser (e.g., TAPAS [7]) to answer a question over the above table, both the table and the question are represented as a sequence of tokens, in which the cell logical location (i.e., the index of column/row) provides the table struc-ture information. Though the cell logical location can be directly acquired from the semi-structured table (e.g., a CSV file), a table structure recognizer is required to obtain such important informa-tion from the unstructured table (e.g., an image file). layouts and styles, it remains a great challenge for machines to automatically recognize the structure of various tables.
Considering the massive amount of tabular data presented in unstructured formats (e.g., image and PDF files) and that most table analysis methods focus on semi-structured tables (e.g., CSV files) [7, 33, 19, 32], the community will signif-icantly benefit from an automatic table recognition system, facilitating large-scale tabular data analysis such as table parsing [7, 32], patient treatment prediction [33, 34], and credit card fraud detection [19].
To understand the structure of different tables, both the cell spatial and logical locations are of great importance
in many applications. As a table parsing example shown in Fig. 1, before applying a table parser (e.g., TAPAS [7]) to answer a question over a table, both the table and the question are tokenized, and the cell logical location (i.e., the index of column/row) provides the table structure in-formation. If the table is presented as an image instead of structured or semi-structured formats, a table structure rec-ognizer is required to detect the cell spatial location and in-fer the cell logical location. Existing table structure recog-nition methods usually utilize rule-based or statistical tech-niques with hand-crafted features [18, 23, 26], working well in only constrained settings (e.g., tables with fixed layouts).
As shown in Fig. 2, with the success of deep learning, recent deep learning-based table structure recognition approaches can be divided into three categories: 1) identify cell bound-ing boxes through visual detection and segmentation meth-ods [25, 27, 29, 28, 8]; 2) transform a table image into the markup sequence, such as LaTeX and HTML [10, 2]; and 3) explore the adjacency relation between different ta-ble cells [30, 22, 11]. Though the logical location of each cell can be inferred from the adjacency matrix of table cells, additional complex graph optimization algorithms are re-quired. As a result, the importance of the logical location of table cells has been poorly investigated in existing table structure recognition methods.
To further explore the logical relation between different table cells, we introduce a more powerful graph-based table representation, which is referred to as Table Graph. Specif-ically, the structure of each table can be represented as a graph: each node indicates a table cell and the edge be-tween two nodes reflects their logical relation on the row and column dimensions, which can be related to their row and column indices. With the proposed table graph, a table cell can be located in the image by the position of its pixels, and its relevant information can be retrieved along the row and column indices. As a result, if a model can reconstruct such a table graph from the given image, it then has a good understanding on the table structure. As the table parsing example shown in Fig. 1, the table structure is represented by the logical location of each cell, which can be inferred from the cell spatial location when the input table is pre-sented as an image. To this end, both the cell spatial and logical locations are important for table structure recogni-tion and further table understanding.
In this paper, we formulate the problem of table struc-ture recognition as the table graph reconstruction, which requires the model to jointly predict the cell spatial loca-tion and the cell logical location. To address this problem, an end-to-end trainable table reconstruction network (TGR-Net) is proposed. Specifically, the proposed method em-ploys a segmentation-based module to detect the cell spatial location, and the cell logical location prediction is solved as an ordinal node classification problem. We evaluate the
Figure 2. Three types of existing methods for table structure recog-nition. proposed method on four datasets and experimental results demonstrate the effectiveness of the proposed method for table structure recognition. Considering most table recog-nition datasets do not provide the cell logical location an-notation, we provide the table graph annotations for 350K table images from the TABLE2LATEX-450K dataset [2] as a new benchmark, TableGraph-350K. The contributions of this paper are summarized as follows:
• We reformulate the problem of table structure recog-nition as the table graph reconstruction, and further propose a table graph reconstruction network to jointly predict the spatial and logical locations of table cells.
• We contribute a new benchmark generated from the
TABLE2LATEX-450K dataset with 350K table graph annotations. 2.