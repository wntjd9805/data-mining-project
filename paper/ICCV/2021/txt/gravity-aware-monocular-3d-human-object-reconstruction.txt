Abstract 1.

Introduction
This paper proposes GraviCap, i.e., a new approach for joint markerless 3D human motion capture and ob-ject trajectory estimation from monocular RGB videos. We focus on scenes with objects partially observed during a free flight. In contrast to existing monocular methods, we can recover scale, object trajectories as well as human bone lengths in meters and the ground plane’s orienta-tion, thanks to the awareness of the gravity constraining object motions. Our objective function is parametrised by the object’s initial velocity and position, gravity direction and focal length, and jointly optimised for one or several free flight episodes. The proposed human-object interac-tion constraints ensure geometric consistency of the 3D re-constructions and improved physical plausibility of human poses compared to the unconstrained case. We evaluate
GraviCap on a new dataset with ground-truth annotations for persons and different objects undergoing free flights. In the experiments, our approach achieves state-of-the-art ac-curacy in 3D human motion capture on various metrics. We urge the reader to watch our supplementary video. Both the source code and the dataset are released; see http:
//4dqv.mpi-inf.mpg.de/GraviCap/.
Markerless 3D human motion capture from a single monocular RGB camera has many open challenges. Al-though state-of-the-art methods have seen great progress
[21, 24, 33, 14, 39], they still hardly work for scenes show-ing non-trivial interactions of humans with the environment as most of them do not model environmental constraints or physical laws. Further, 3D reconstruction of humans in-teracting with objects from monocular imagery is scarcely explored, and only a few works were proposed to date
[19, 50]. Most existing methods that consider interaction with the environment impose geometric constraints to avoid incorrect interpenetrations [49, 15, 50]. They often exhibit strong jitter, implausible posture with unnatural body lean-ing and depth instabilities. Recent physics-based methods for monocular 3D human pose estimation [34, 39] showed that explicit modelling of gravity and ground reaction forces (or friction) enables monocular reconstruction of humans of much higher biomechanical plausibility. However, these methods do not model object interactions, and without a pri-ori information about the human body, they cannot estimate posture and scene dimensions in absolute metric scale.
In this paper, we make the following observation: Ex-plicitly modelling physics and actively encouraging a spe-cific form of human-object interaction in the scene enables improved 3D human and 3D object trajectory reconstruc-tion in a metrically accurate way from a single monocular video. We consider scenarios when up to two persons are in-teracting with an object and bringing it to a free flight (e.g., throwing or tossing). Such scenarios are often observed in real everyday life while practising sports or playing outdoor games. We show that physics-based constraints allow us to obtain 3D estimates in the absolute units, which, other-wise, remains inaccessible for a monocular setting when no strong prior assumptions about the scene can be made, such as known bone lengths.
Our core findings are that 1) Projectile motion con-straints are sufficient to recover the 3D trajectory of an object undergoing free flight from 2D object coordinates, assuming known camera frame rate and gravity vector; 2)
Knowing the magnitude of the gravity and the focal length is sufficient to resolve the scale of the observed scene in meters and orientation of the ground plane, assuming that the direction of the gravity vector is opposite to the ground plane normal; 3) Localising humans with respect to the re-covered 3D object trajectory leads to improved 3D human motion capture.
See Fig. 1 for an overview of our framework. The in-puts are 2D coordinates of the object’s geometric centre and 2D human joint locations, along with initial uncon-strained kinematic 3D human poses. After that, we then minimise the proposed objective globally over multiple in-put frames and obtain 3D object trajectory over one or sev-eral free-flight episodes and improved 3D human poses.
Summarised, the contributions of this work are as follows:
• GRAVICAP, i.e., the new approach for joint 3D capture of human motions and trajectories of objects undergoing free flights (Sec. 3);
• New types of human-object interaction constraints im-proving the accuracy and physical plausibility of 3D human poses (Sec. 3.2.2). these constraints allow recovering camera-relative distances of moving and interacting objects, including humans, in me-ters from a single monocular RGB camera;
For the first time,
• A new dataset of human-object interactions for experi-mental evaluation, with ground-truth annotations of 3D human poses and object trajectories (Sec. 4).
We achieve state-of-the-art accuracy for global 3D hu-man motion capture using different metrics in extensive ex-periments with the new dataset (Sec. 5). Our estimates look more physically plausible and temporally consistent com-pared to results without human-trajectory localisation con-straints. Moreover, the proposed constraints significantly improve absolute root translations. The source code of
GRAVICAP and the dataset are publicly available at http:
//4dqv.mpi-inf.mpg.de/GraviCap/. 2.