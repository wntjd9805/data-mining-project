Abstract
We introduce Video Transformer (VidTr) with separable-attention for video classification. Comparing with com-monly used 3D networks, VidTr is able to aggregate spatio-temporal information via stacked attentions and provide better performance with higher efficiency. We first introduce the vanilla video transformer and show that transformer module is able to perform spatio-temporal modeling from raw pixels, but with heavy memory usage. We then present
VidTr which reduces the memory cost by 3.3× while keep-ing the same performance. To further optimize the model, we propose the standard deviation based topK pooling for attention (pooltopK std), which reduces the computation by dropping non-informative features along temporal dimen-sion. VidTr achieves state-of-the-art performance on five commonly used datasets with lower computational require-ment, showing both the efficiency and effectiveness of our design. Finally, error analysis and visualization show that
VidTr is especially good at predicting actions that require long-term temporal reasoning. 1.

Introduction
We introduce Video Transformer (VidTr) with separable-attention, one of the first transformer-based video ac-tion classification architecture that performs global spatio-temporal feature aggregation. Convolution-based archi-tectures have dominated the video classification literature in recent years [19, 32, 55], and although successful, the convolution-based approaches have two drawbacks: 1. they have limited receptive field on each layer and 2. informa-tion is slowly aggregated through stacked convolution lay-ers, which is inefficient and might be ineffective [31, 55].
Attention is a potential candidate to overcome these limi-tations as it has a large receptive field which can be lever-aged for spatio-temporal modeling. Previous works use at-tention to modeling long-range spatio-temporal features in
*Equally Contributed. videos but still rely on convoluational backbones [31, 55].
Inspired by recent successful applications of transformers on NLP [12, 52] and computer vision [14, 47], we propose a transformer-based video network that directly applies at-tentions on raw video pixels for video classification, aiming at higher efficiency and better performance.
We first introduce a vanilla video transformer that di-rectly learns spatio-temporal features from raw-pixel inputs via vision transformer [14], showing that it is possible to perform pixel-level spatio-temporal modeling. However, as discussed in [56], the transformer has O(n2) complex-ity with respect to the sequence length. The vanilla video transformer is memory consuming, as training on a 16-frame clip (224 × 224) with only batch size of 1 requires more than 16GB GPU memory, which makes it infeasible on most commercial devices. Inspired by the R(2+1)D con-volution that breaks down 3D convolution kernel to a spa-tial kernel and a temproal kernel [50], we further introduce our separable-attention, which performs spatial and tempo-ral attention separately. This reduces the memory consump-tion by 3.3× with no drop in accuracy. We can further re-duce the memory and computational requirements of our system by exploiting the fact that a large portion of many videos have redundant information temporally. This notion has been explored in the context of convolutional networks to reduce computation previously [32]. We build on this in-tuition and propose a standard deviation based topK pooling operation (topK std pooling), which reduces the sequence length and encourages the transformer network to focus on representative frames.
We evaluated our VidTr on 6 most commonly used datasets, including Kinetics 400/700, Charades, Something-something V2, UCF-101 and HMDB-51. Our model achieved state-of-the-art (SOTA) or comparable perfor-mance on five datasets with lower computational require-ments and latency compared to previous SOTA approaches.
Our error analysis and ablation experiments show that the
VidTr works significantly better than I3D on activities that requires longer temporal reasoning (e.g. making a cake vs. eating a cake), which aligns well with our intuition.
This also inspires us to ensemble the VidTr with the I3D convolutional network as features from global and local modeling methods should be complementary. We show that simply combining the VidTr with a I3D50 model (8 frames input) via ensemble can lead to roughly a 2% per-formance improvement on Kinetics 400. We further il-lustrate how and why the VidTr works by visualizing the separable-attention using attention rollout [1], and show that the spatial-attention is able to focus on informative patches while temporal attention is able to reduce the duplicated/non-informative temporal instances. Our contri-butions are: 1. Video transformer: We propose to efficiently and effec-tively aggregate spatio-temporal information with stacked at-tentions as opposed to convolution based approaches. We in-troduce vanilla video transformer as proof of concept with
SOTA comparable performance on video classification. 2. VidTr: We introduce VidTr and its permutations, including the VidTr with SOTA performance and the compact-VidTr with significantly reduced computational costs using the pro-posed standard deviation based pooling method. 3. Results and model weights: We provide detailed results and analysis on 6 commonly used datasets which can be used as reference for future research. Our pre-trained model can be used for many down-streaming tasks. 2.