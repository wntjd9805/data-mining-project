Abstract
Multi-action video recognition is much more challeng-ing due to the requirement to recognize multiple actions co-occurring simultaneously or sequentially. Modeling multi-action relations is beneficial and crucial to under-stand videos with multiple actions, and actions in a video
In this pa-are usually presented in multiple modalities. per, we propose a novel multi-action relation model for videos, by leveraging both relational graph convolutional networks (GCNs) and video multi-modality. We first build multi-modal GCNs to explore modality-aware multi-action relations, fed by modality-specific action representation as node features, i.e., spatiotemporal features learned by 3D convolutional neural network (CNN), audio and textual em-beddings queried from respective feature lexicons. We then joint both multi-modal CNN-GCN models and multi-modal feature representations for learning better relational action predictions. Ablation study, multi-action relation visualiza-tion, and boosts analysis, all show efficacy of our multi-modal multi-action relation modeling. Also our method achieves state-of-the-art performance on large-scale multi-action M-MiT benchmark. Our code is made publicly avail-able at https://github.com/zhenglab/multi-action-video. 1.

Introduction
Video understanding is a very complex and comprehen-sive task in computer vision as it aims to recognize activities occurring in a complex environment through complicated hearing and seeing videos [27, 21, 34, 35, 40]. Activities depicted in videos are often made of several actions that may occur simultaneously or sequentially. For example, when the action of “performing” occurs, it is often accom-panied by “applauding” and “cheering” actions [35]. Multi-action video recognition is such a task that aims to auto-*Corresponding author: Haiyong Zheng (zhenghaiyong@ouc.edu.cn).
This work was supported by the National Natural Science Foundation of China under Grant Nos. 61771440, 41927805, and 41776113.
Figure 1: Leveraging multi-modal multi-action relations to recognize all actions in a video (example in M-MiT [35]). matically recognize all the actions co-occurring in a video.
Although considerable progress has been made in action recognition [44, 46, 51, 6, 47, 52, 62, 12, 31, 58, 11], it’s still rather limited on multi-action recognition [35, 41, 60]. In this work, we are going further in more challenging multi-action video recognition to better understand the video.
In order to deal with the task of single-action video recognition, more and more efforts are being made to explore the relations between actions and objects from videos [22, 19, 36, 32, 62, 9, 53, 42, 3]. Therefore, to rec-ognize all actions co-occurring in a video for better solving multi-action recognition problem, it would be beneficial and crucial to explore relations among multiple actions, namely, multi-action relations. Actually, actions in a video are first presented as visual spatial and temporal frames, also they have strong correlation with synchronous recorded audio, finally they are related to each other in literal meaning (label
text), thus, making full use of these multi-modal informa-tion in videos (i.e., frames, audio, and text) to explore multi-action relations, can contribute greatly to recognize the mul-tiple actions as well as understand the complex videos.
Recent advances in multi-action video recognition mainly focus on, either developing hand-crafted spatiotem-poral features [18, 7] (e.g., harris corners [23], STIPs [30], optical flow [2], gradient [38]) to train classifiers, or design-ing 3D convolutional neural network (3D-CNN) architec-tures to learn discriminative spatiotemporal representation for classification [35, 41]. However, previous works didn’t particularly consider the relations among multiple actions in videos. Besides, although multi-modal information has been used to analyze multi-action videos [35], it is only used to extract the feature of corresponding modality (i.e., spatiotemporal and auditory features of visual and audio modalities) for fused classification, rather than exploring the multi-modal multi-action relations for more discrimi-native representation. Thus, how to take full advantage of multi-modal information to better explore multi-action re-lations is a key point for multi-action video recognition.
Graphs provide a generic way to model real-world rela-tional data [28, 61], and recently, graph convolutional net-works (GCNs) [29] have been proved to be very effective at tasks thought to have rich relational structure [64, 39, 56], which might be very helpful for discovering relations among actions from videos. Thus, in this work, we devote to exploring the multi-action relations implied in videos via
GCNs by setting multiple actions as graph nodes. Further-more, due to the unique and important multi-modality prop-erty of videos, we build multi-modal GCNs for better mod-eling modality-specific multi-action relations in videos.
We design our multi-modal GCNs for multi-action video recognition relying on three following observations: (1) vi-sual frames are much more important than other modalities for our daily experience as well as the way we understand the world (more than 80% of information transmitted to the brain is visual) [25], (2) sounds are determined by and in-formative about the attributes of their actions and we poten-tially build sound-action mapping from experience inside our brain [15], and (3) our brain can also connect actions with their linguistic labels (meaning words) to create text-action mapping [24, 45]. Figure 1 shows a video example with multiple actions, and visual frames can indicate rela-tions among actions of performing, dancing, drumming and hitting/colliding occurring simultaneously or sequentially, also audio from the video will be identified as actions of playing music, drumming and hitting/colliding according to our knowledge of sound-action relations, while the meaning words of occurred actions in this video have their underly-ing text-action relations semantically, hence, jointing auxil-iary textual (underlying) and audio (hearing) relational pre-dictions with primary visual (seeing) relational predictions, can produce more accurate recognition of multiple actions.
In this paper, to address challenging multi-action video recognition, we propose to develop multi-modal GCNs for exploring modality-specific multi-action relations by lever-aging graph’s powerful relational representation ability and video’s rich multi-modal information. Specifically, we con-struct multi-action graphs with multiple actions as nodes and action co-occurrence probabilities as adjacent matrix, then, we build multi-modal GCNs for exploring modality-aware multi-action relations, fed by modality-specific ac-tion representation as node features, i.e., spatiotemporal features learned by 3D-CNN, audio and textual embeddings queried from respective feature lexicons, finally, we impose audio and textual relations on spatiotemporal representation to produce respective relational action predictions that are further jointed together with visual relational action predic-tions to yield final predictions, presenting a novel way of multi-modal joint learning to recognize multiple actions.
Our contributions include: (1) we propose a novel way of taking advantage of relational GCNs and video multi-modality to explore multi-action relations for multi-action video understanding; (2) we devise modality-specific re-lational GCNs accompanied by multi-modal joint learning for better modeling modality-aware multi-action relations; (3) both ablation study and multi-action relation visualiza-tion as well as boosts analysis, show efficacy of our relation modeling, also our method achieves state-of-the-art perfor-mance on large-scale multi-action M-MiT benchmark. 2.