Abstract
Generalized zero-shot learning (GZSL) has achieved signiﬁcant progress, with many efforts dedicated to over-coming the problems of visual-semantic domain gap and seen-unseen bias. However, most existing methods di-rectly use feature extraction models trained on ImageNet alone, ignoring the cross-dataset bias between ImageNet and GZSL benchmarks. Such a bias inevitably results in poor-quality visual features for GZSL tasks, which poten-tially limits the recognition performance on both seen and unseen classes. In this paper, we propose a simple yet effec-tive GZSL method, termed feature reﬁnement for generalized zero-shot learning (FREE), to tackle the above problem.
FREE employs a feature reﬁnement (FR) module that in-corporates semantic→visual mapping into a uniﬁed gener-ative model to reﬁne the visual features of seen and unseen class samples. Furthermore, we propose a self-adaptive margin center loss (SAMC-loss) that cooperates with a se-mantic cycle-consistency loss to guide FR to learn class- and semantically-relevant representations, and concatenate the features in FR to extract the fully reﬁned features. Exten-sive experiments on ﬁve benchmark datasets demonstrate the signiﬁcant performance gain of FREE over its baseline and current state-of-the-art methods. The code is available at https://github.com/shiming-chen/FREE . 1.

Introduction
A key challenge of artiﬁcial intelligence is to generalize machine learning models from seen data to unseen scenarios.
Zero-shot learning (ZSL) is a typical research topic targeting this goal [25, 27, 41]. ZSL aims to classify the images of unseen classes by constructing a mapping relationship be-tween the semantic and visual domains. It is usually based on the assumption that both seen and unseen classes can be
∗Corresponding author
Figure 1. The core idea of our FREE. The cross-dataset bias be-tween ImageNet and GZSL benchmarks (e.g., CUB) is harmful for feature extraction from GZSL benchmarks, which results in poor-quality visual features for unsatisfying performance in GZSL. Our
FREE reﬁnes the visual features and improves the semantic→visual mapping using feature reﬁnement (FR) in a uniﬁed network for
GZSL classiﬁcation. described through a set of semantic vectors, e.g., sentence embeddings [45], and attribute vectors [26], in the same se-mantic space. According to their classiﬁcation range, ZSL methods can be categorized into conventional ZSL (CZSL) and generalized ZSL (GZSL) [58]. CZSL aims to predict un-seen classes, while GZSL can predict both seen and unseen classes. Recently, GZSL has attracted more attention as it is more realistic and challenging. We are thus also interested in the GZSL setting in this paper.
GZSL has achieved signiﬁcant progress, with many efforts focused on the problems of visual-semantic do-main gaps [26, 1, 2, 52, 51, 61] and seen-unseen bias
[57, 37, 66, 64, 49, 38, 36, 19]. Semantic embedding
[33, 8, 31, 66, 34] or generative methods (e.g., variational autoencoders (VAEs) [3, 47], generative adversarial nets (GANs) [57, 31, 60, 65, 21, 51], and generative ﬂows [49])
are typically applied to mitigate these challenges.
An important observation of ours is that the unsatisfying performance in GZSL that still nevertheless exists is closely related to the cross-dataset bias [50]. GZSL models usu-ally extract visual features from coarse- and ﬁne-grained benchmarks (e.g., AWA1 [26] and CUB [53]) using a convo-lutional neural network (CNN) backbone (e.g., ResNet-101
[16]) pre-trained on ImageNet [58]. However, cross-dataset bias, where the data collection procedure can be biased by human or systematic factors, can lead to a distribution mis-match between two datasets, e.g., Auklets are found in the
CUB dataset but not in ImageNet. Thus, it is unwise to directly transfer knowledge from ImageNet to a new dataset for GZSL without any further sequential learning, because cross-dataset bias limits knowledge transfer and results in the extraction of poor-quality visual features from GZSL benchmarks, as shown in Fig. 1. Further, the larger the bias between ImageNet and the GZSL benchmark, the poorer the knowledge transfer and feature extraction. Since there is a more obvious bias for ﬁne-grained datasets (e.g., CUB), these typically yield inferior performance to coarse-grained datasets (e.g., AWA) for all GZSL methods. The negative effect of cross-dataset bias on the performance of GZSL has been further validated experimentally. In [59], Xian ﬁne-tuned a ResNet pre-trained on ImageNet using seen classes from GZSL benchmarks. Before ﬁne-tuning, f-VAEGAN achieved a harmonic mean of 64.6% and 63.5% on FLO and AWA2, respectively, while these numbers increased to 75.1% and 65.2% afterward, as shown in Table 4. However,
Xian did not analyze or discuss this phenomenon. Further, although ﬁne-tuning may alleviate the cross-dataset bias to a certain degree, it inevitably results in other severer prob-lems, e.g., overﬁtting [17, 28]. Thus, properly addressing the problem of cross-dataset bias in GZSL has become very necessary. To the best of our knowledge, we are the ﬁrst to identify this as an open issue in GZSL, which will be tackled in this paper.
To address the above challenges, we propose a novel
GZSL method, termed feature reﬁnement for generalized zero-shot learning (FREE), to further boost the performance of GZSL. FREE reﬁnes visual features in a uniﬁed genera-tive model, which simultaneously beneﬁts semantic→visual learning, feature synthesis, and classiﬁcation. Speciﬁ-cally, we take f-VAEGAN [59] as a baseline to learn a semantic→visual mapping. To improve the visual features of seen and unseen class samples, we employ a feature re-ﬁnement (FR) module, which can be jointly optimized with f-VAEGAN to effectively avoid the drawbacks of ﬁne-tuning.
Since class label information is available, we introduce a self-adaptive margin center loss (SAMC-loss) to explicitly encourage intra-class compactness and inter-class separabil-ity that can adapt to different datasets, i.e., coarse-grained and ﬁne-grained, and guide FR to learn discriminative class-relevant features. Thus, the distributions of different classes can be easily separated, as shown in Fig. 1. To better learn semantically-relevant and more discriminative visual fea-tures, a semantic cycle-consistency loss is also added after the restitution of features. From the residual information
[16], we further concatenate the discriminative features of various layers in FR to extract the fully reﬁned features.
To summarize, this paper provides the following impor-tant contributions: (1) We propose a novel GZSL method, termed feature reﬁnement for generalized zero-shot learning (FREE), to address the problem of cross-dataset bias, which can further boost the performance of GZSL. To achieve this goal, a feature reﬁnement (FR) module that cooperates with semantic→visual mapping in a uniﬁed framework is explored. Importantly, the two modules can be jointly op-timized. (2) We propose a self-adaptive margin center loss (SAMC-loss) to explicitly encourage intra-class compactness and inter-class separability. The SAMC-loss also cooperates with a semantic cycle-consistency constraint to enable FR to learn more discriminative class- and semantically-relevant representations, which are especially important for GZSL. (3) Extensive experimental results on ﬁve benchmarks, i.e.,
CUB, SUN, FLO, AWA1, and AWA2, clearly demonstrate the advantages of the proposed FREE over its baseline and current state-of-the-art methods. 2.