Abstract
Text-to-image synthesis refers to generating an image from a given text description, the key goal of which lies in photo realism and semantic consistency. Previous methods usually generate an initial image with sentence embedding and then refine it with fine-grained word embedding. De-spite the significant progress, the ‘aspect’ information (e.g., red eyes) contained in the text, referring to several words rather than a word that depicts ‘a particular part or fea-ture of something’, is often ignored, which is highly helpful for synthesizing image details. How to make better utiliza-tion of aspect information in text-to-image synthesis still remains an unresolved challenge. To address this prob-lem, in this paper, we propose a Dynamic Aspect-awarE
GAN (DAE-GAN) that represents text information compre-hensively from multiple granularities, including sentence-level, word-level, and aspect-level. Moreover, inspired by human learning behaviors, we develop a novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) module are al-ternately employed. AGR utilizes word-level embedding to globally enhance the previously generated image, while
ALR dynamically employs aspect-level embedding to refine image details from a local perspective. Finally, a corre-sponding matching loss function is designed to ensure the text-image semantic consistency at different levels. Exten-sive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and COCO) demonstrate the supe-riority and rationality of our method. 1.

Introduction
Text-to-image synthesis requires an agent to generate a photo-realistic image according to the given text descrip-tion. Due to its significant potential in many applications
†Work done during an internship in Tencent AI Lab.
*Corresponding Authors.
Figure 1. Comparisons between DM-GAN [45] and our DAE-GAN. DM-GAN first generates a low-resolution image with sentence-level information and then refines it with word-level fea-tures. DAE-GAN refines images from both global and local per-spectives with word-level and aspect information contained. such as art generation [41] and computer-aided design [2] but challenging nature, it is arousing extensive research at-tention in recent years.
In the past few years, Generative Adversarial Net-works (GANs) [5] have been proved tremendously success-ful for this task [20]. Most existing methods make efforts on the two-stage framework by first generating initial low-resolution images and then refining them to high-resolution ones [38, 39, 36]. Among all these methods, the proposal of AttnGAN [36] plays an extremely important role. At the initial stage, sentence-level information is employed to generate a low-resolution image. Then, at the refinement stage, AttnGAN utilizes word-level features to refine the previously generated image by repeatedly adopting atten-tion mechanism to select important words. Based on At-tnGAN, text-to-image synthesis has been pushed by a large step forward [45, 4, 18]. A synthesis example by DM-GAN [45] is presented at the top of Figure 1.
Although remarkable performance has been accom-plished with these efforts, there still exist several limita-tions to be unresolved. For example, most previous meth-ods only employ sentence-level and word-level features, ig-noring the ‘aspect-level’ features.
‘Aspect’ here refers to several words rather than a word that depicts ‘a particu-lar part or feature of something’. There are often multiple aspect terms contained in a sentence to describe an object or a scene from different perspectives, e.g., ‘the black bird’ and ‘red eyes’ in the text description in Figure 1.
Semantic understanding of a sentence is highly dependent on both content and aspect[33]. Both industry and academia have realized the importance of the relationship between as-pect term and sentence [3, 40, 17]. In fact, aspect informa-tion contained in the text could be helpful for image syn-thesis, especially for the refinement of local image details.
Though the value of aspect information has been proved, how to make better utilization of aspect information in text-to-image synthesis still remains a big challenge.
Fortunately, some interesting studies of human learn-ing behaviors could give us some inspirations. Researchers have already demonstrated that human eyes have central vi-sion and peripheral vision [1, 34, 25]. Central vision con-centrates on what a person needs at the current time, while peripheral vision uses observation of the surroundings to support central vision. Through the dynamic use of central vision and peripheral vision, we could make an in-depth se-mantic understanding of text and visual content.
To this end, in this paper, we propose a novel Dynamic
Aspect-awarE GAN(DAE-GAN) for text-to-image synthe-sis. To be specific, we firstly encode text information from multiple granularities comprehensively, including sentence-level, word-level, and aspect-level. Then, at the two-stage generation, we first generate a low-resolution image with sentence-level embedding at the initial stage. Next, at the refinement stage, by viewing aspect-level features as central vision and word-level features as peripheral vision, we de-velop an Aspect-aware Dynamic Re-drawer (ADR), which alternately applies an Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement (ALR) mod-ule for image refinement. AGR utilizes word-level embed-ding to globally enhance the previously generated images.
ALR dynamically utilizes aspect-level embedding to refine image details from a local perspective. Finally, to provide supervision for intermediate synthesis procedures, a corre-sponding matching loss function is designed to ensure the text-image semantic consistency. The bottom of Figure 1 illustrates an example of our proposed method. When given the aspect ‘the black bird’, our method focuses on adjusting the color of the whole bird body based on the previously generated image. When dealing with the aspect
‘red eyes’, our model then correspondingly focuses on the refinement of bird eyes with remaining other parts.
Our main contributions are summarized as follows:
• We observe the great potential of aspect information and apply it to text-to-image synthesis.
• We propose a novel DAE-GAN, in which text infor-mation is comprehensively represented from multiple granularities, and an ADR is developed to refine im-ages from both local and global perspectives.
• Extensive experiments including quantitative and qual-itative evaluations show the superiority and rationality of our proposed method. Specially, the causality study demonstrates DAE-GAN as an interpretable model. 2.