Abstract
Explainability for machine learning models has gained considerable attention within the research community given the importance of deploying more reliable machine-learning systems.
In computer vision applications, gen-erative counterfactual methods indicate how to perturb a model’s input to change its prediction, providing details about the model’s decision-making. Current methods tend to generate trivial counterfactuals about a model’s deci-sions, as they often suggest to exaggerate or remove the presence of the attribute being classiﬁed. For the ma-chine learning practitioner, these types of counterfactu-als offer little value, since they provide no new informa-tion about undesired model or data biases. In this work, we identify the problem of trivial counterfactual genera-tion and we propose DiVE to alleviate it. DiVE learns a perturbation in a disentangled latent space that is con-strained using a diversity-enforcing loss to uncover multiple valuable explanations about the model’s prediction. Fur-ther, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the suc-cess rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. Code is available at https://github.com/ElementAI/ beyond-trivial-explanations. 1.

Introduction
Consider an image recognition model such as a smile classiﬁer. In case of erroneous prediction, an explainabil-ity system should provide information to machine learn-ing practitioners to understand why such error happened and how to prevent it. Counterfactual explanation meth-ods [3, 8, 9] can help highlight the limitations of an ML model by uncovering data and model biases. Counterfac-tual explanations provide perturbed versions of the input data that emphasize features that contributed the most to the ML model’s output. For the smile classiﬁer, if the model is confused by people wearing sunglasses then the system could generate alternative images of faces without sunglasses that would be correctly recognized.
In order to discover a model’s limitations, counterfactual generation systems could be used to generate images that would con-fuse the classiﬁer, such as people wearing sunglasses or scarfs occluding the mouth. This is different from other types of explainability methods such as feature importance methods [3, 39, 40] and boundary approximation meth-ods [29, 36], which highlight salient regions of the input like the sunglasses but do not indicate how the ML model could achieve a different prediction.
According to [30, 37], counterfactual explanations should be valid, proximal, and sparse. A valid counterfac-tual explanation changes the prediction of the ML model, for instance, adding sunglasses to confuse a smile classi-ﬁer. The explanation is sparse if it only changes a mini-mal set of attributes, for instance, it only adds sunglasses and it does not add a hat, a beard, or the like. An expla-nation is proximal if it is perceptually similar to the origi-nal image, for instance, a ninety degree rotation of an im-age would be a sparse but not proximal. In addition to the three former properties, generating a set of diverse expla-nations increases the likelihood of ﬁnding a useful expla-nation [30, 37]. A set of counterfactuals is diverse if each one proposes to change a different set of attributes. Fol-lowing the previous example, a diverse set of explanations would suggest to add or remove sunglasses, beard, or scarf, while a non-diverse set would all suggest to add or remove different brands of sunglasses. Intuitively, each explanation should shed light on a different action that a user can take to change the ML model’s outcome.
Current generative counterfactual methods like is not xGEM [18] generate a single explanation that constrained to be similar to the input. Thus, they fail to be proximal, sparse, and diverse. Progressive Exaggeration (PE) [42] provides higher-quality explanations that are more proximal than xGEM, but it still fails to provide a diverse set of explanations. In addition, the image generator of PE is trained on the same data as the image classiﬁer in
order to detect biases thereby limiting their applicability.
Both of these two methods tend to produce trivial explana-tions, which only address the attribute that was intended to be classiﬁed, without further exploring failure cases due to biases in the data or spurious correlations. For instance, an explanation that suggests to increase the ‘smile’ attribute of a ‘smile’ classiﬁer for an already-smiling face is trivial and it does not explain why a misclassiﬁcation occurred.
On the other hand, a non-trivial explanation that suggests to change the facial skin color would uncover a racial bias in the data that should be addressed by the ML practitioner.
In this work, we focus on diverse valuable explanations, that is, valid, proximal, sparse, and non-trivial.
We propose Diverse Valuable Explanations (DiVE), an explainability method that can interpret ML model predic-tions by identifying sets of valuable attributes that have the most effect on model output. In order to generate non-trivial explanations, DiVE leverages the Fisher information matrix of its latent space to focus its search on the less inﬂuential factors of variation of the ML model. This mechanism en-ables the discovery of spurious correlations learned by the
ML model. DiVE produces multiple counterfactual expla-nations which are enforced to be valuable, and diverse, re-sulting in more informative explanations for machine learn-ing practitioners than competing methods in the literature.
Our method ﬁrst learns a generative model of the data using a β-TCVAE [4] to obtain a disentangled latent representa-tion which leads to more proximal and sparse explanations.
In addition, the VAE is not required to be trained on the same dataset as the ML model to be explained. DiVE then learns a latent perturbation using constraints to enforce di-versity, sparsity, and proximity.
We provide experiments to quantify the success of ex-plainability systems at ﬁnding valuable explanations. We
ﬁnd that DiVE is more successful at ﬁnding non-trivial explanations than previous methods and baselines. In ad-dition, we provide experiments to compare the quality of the generated explanations with the current state-of-the-art.
First, we assess their validity on the CelebA dataset [26] and provide quantitative and qualitative results on a bias detec-tion benchmark [42]. Second, we show that the generated explanations are more proximal in terms of Fr´echet Incep-tion Distance (FID) [14], which is a measure of similarity between two datasets of images commonly used to evaluate the quality of generated images. In addition, we evaluate the proximity in latent space and face veriﬁcation accuracy, as reported by Singla et al. [42]. Third, we assess the sparsity of the generated counterfactuals by computing the average change in facial attributes.
We summarize the contributions of this work as follows: 1) We identify the importance of ﬁnding non-trivial expla-nations and we propose a new benchmark to evaluate how valuable the explanations are. 2) We propose DiVE, an ex-plainability method that can interpret an ML model by iden-tifying the attributes that have the most effect on its output. 3) We propose to leverage the Fisher information matrix of the latent space for ﬁnding spurious features that produce non-trivial explanations. 4) DiVE achieves state of the art in terms of the validity, proximity, and sparsity of its ex-planations, detecting biases on the datasets, and producing multiple explanations for an image. 2.