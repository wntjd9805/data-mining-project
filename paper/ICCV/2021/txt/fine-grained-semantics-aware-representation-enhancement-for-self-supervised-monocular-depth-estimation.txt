Abstract
Self-supervised monocular depth estimation has been widely studied, owing to its practical importance and re-cent promising improvements. However, most works suffer from limited supervision of photometric consistency, espe-cially in weak texture regions and at object boundaries. To overcome this weakness, we propose novel ideas to improve self-supervised monocular depth estimation by leveraging cross-domain information, especially scene semantics. We focus on incorporating implicit semantic knowledge into ge-ometric representation enhancement and suggest two ideas: a metric learning approach that exploits the semantics-guided local geometry to optimize intermediate depth rep-resentations and a novel feature fusion module that judi-ciously utilizes cross-modality between two heterogeneous feature representations. We comprehensively evaluate our methods on the KITTI dataset and demonstrate that our method outperforms state-of-the-art methods. The source code is available at https://github.com/hyBlue/
FSRE-Depth. 1.

Introduction
Depth measurement is a critical task in various appli-cations, including robotics, augmented reality, and self-driving vehicles. It measures the distance from all or a part of the pixels in the imaging device to target objects using ac-tive/passive sensors. Equipping such devices requires high cost and continuous operation, which makes its use limited.
Monocular depth estimation estimates the depth of pixels in a given 2D image without additional measurement.
It facilitates the understanding of 3D scene geometry from a captured image, which closes the dimension gap between the physical world and an image.
Because of its importance and cost benefits, there have been lots of studies [53, 35, 33, 48, 67] that have improved depth estimation accuracy, temporal consistency and depth ranges. Owing to the success of the convolutional neural e g a m
I
] 4 2
[
] 4 1
[ s r u
O
Figure 1: Depth predictions and error maps of recent state-of-the-arts [14, 24]. Apart from ours, both lack accuracy in low-texture regions and object boundaries, owing to weak supervision using contemporary self-supervised training. network, it has also been adapted to monocular depth esti-mation and has produced great improvements.
Many existing monocular depth estimation methods train their networks with supervised depth labels computed via synthetic data or estimated from depth sensor [30, 9, 28, 25]. Although such methods have provided significant im-provements in depth estimation, they still have multiple concerns related to the high cost of labeling and obtain-ing the depth labels on pixels, the limited available ground-truth depth data, the restricted depth range of sampled data, and the noticeable noise in the depth values. To avoid these shortcomings, self-supervised training methods have recently been proposed.
Notably, the SfM-Learner [64] method utilizes the en-sembles of consecutive frames in video sequences for joint training depth and pose networks. It demonstrates compa-rable performance to extant supervised methods; however, recent works [69, 39, 12, 2] based on SfM-Learner mostly rely on photometric loss [54] and smoothness constraints; hence, they suffer from limited supervision of weak texture
regions. Furthermore, moving objects and uncertainty in the pose network destabilize training, leading to incorrect depth values, especially on object boundaries (see Fig. 1).
Several recent methods have attempted to overcome this weakness by employing cross-domain knowledge learning, including leveraging scene semantics to improve monocu-lar depth predictions [24, 27, 13, 3]. They remove dynamic objects or explicitly model the object motion from the se-mantic instances to incorporate them into the scene geom-etry. In addition, a regularization of the depth smoothness within corresponding semantic objects enforces consistency between depth and semantic predictions [4, 38, 65].
In this study, we aim to improve self-supervised monoc-ular depth estimation via the implicit use of semantic seg-mentation. We do not explicitly identify moving objects or regularize depth values in accordance with the semantic labels. Instead, we focus on representation enhancement, optimizing the depth network in the representation spaces, to produce semantically consistent intermediate depth rep-resentations.
Inspired by the recent use of deep metric learning [52, 44, 23], we suggest a novel semantics-guided triplet loss to refine depth representations according to implicit seman-tic guidance. Here, our goal is to take advantage of local geometric information from the scene semantics. For ex-ample, the adjacent pixels within each object have similar depth values, whereas those that cross semantic boundaries may have large differences. Combined with a simple but ef-fective patch-based sampling strategy, our metric-learning approach exploits the semantics-guided local geometry in-formation to optimize pixel representations near the object boundary, thereby yielding improved depth predictions.
We also design a cross-task attention module for refin-ing depth features more semantically consistent.
It com-putes the similarity between the reference and target fea-tures through multiple representation subspaces and effec-tively utilizes the cross-modal interactions among the het-erogeneous representations. As a result, we quantify the semantic awareness of depth features as a form of attention and exploit it to produce better depth predictions.
Our contributions are summarized as follows. First, we present a novel training method that extracts semantics-guided local geometry with patch-based sampling and uti-lizes it to refine depth features in a metric-learning formu-lation. Second, we propose a new cross-task feature fusion architecture that fully utilizes the implicit representations of semantics for learning depth features. Finally, we com-prehensively evaluate the performances of these two meth-ods using the KITTI Eigen split and demonstrate that our method outperforms recent state-of-the-art self-supervised monocular depth prediction works in every metric. 2.