Abstract
Understanding the inner workings of deep neural net-works (DNNs) is essential to provide trustworthy artiﬁcial intelligence techniques for practical applications. Existing studies typically involve linking semantic concepts to units or layers of DNNs, but fail to explain the inference process.
In this paper, we introduce neural architecture disentangle-ment (NAD) to ﬁll the gap. Speciﬁcally, NAD learns to dis-entangle a pre-trained DNN into sub-architectures accord-ing to independent tasks, forming information ﬂows that describe the inference processes. We investigate whether, where, and how the disentanglement occurs through ex-periments conducted with handcrafted and automatically-searched network architectures, on both object-based and scene-based datasets. Based on the experimental results, we present three new ﬁndings that provide fresh insights into the inner logic of DNNs. First, DNNs can be di-vided into sub-architectures for independent tasks. Second, deeper layers do not always correspond to higher seman-tics. Third, the connection type in a DNN affects how the information ﬂows across layers, leading to different disen-tanglement behaviors. With NAD, we further explain why
DNNs sometimes give wrong predictions. Experimental re-sults show that misclassiﬁed images have a high proba-bility of being assigned to task sub-architectures similar to the correct ones. Our code is available at https:
//github.com/hujiecpp/NAD. 1.

Introduction
A fundamental problem in using deep neural networks (DNNs) is our inability to understand their inner workings, which is crucial in many real-world applications, includ-ing healthcare, criminal justice, and administrative regula-tion [28]. Recent studies also show that DNNs are easily fooled [25, 32, 19, 18] Thus, interpreting DNNs has at-*Corresponding author: caoliujuan@xmu.edu.cn
Figure 1: Illustration of the proposed neural architecture disentanglement (NAD). NAD aims to disentangle a pre-trained DNN into sub-architectures, each of which is re-sponsible for only one task. For example, the network in this ﬁgure is disentangled into two sub-architectures, i.e., the red one and the blue one, for the task of classifying
‘Panda’ and ‘Tabby’. Note that the sub-architectures can overlap with the same unit for different tasks. tracted ever-increasing research attention in recent years.
Existing endeavors typically link semantics to units or lay-ers of DNNs to determine the roles of these speciﬁc parts.
However, the hierarchical inference process is not effec-tively captured in this way, for two reasons. First, the trained networks entangle information together, and one unit can be responsible for multiple classes [23, 42]. Sec-ond, only knowing which unit or layer represents what class is insufﬁcient for understanding the reasoning process in
DNNs. The relationship between successive layers is not explored. For example, if we want to explain the inference process for ‘Airplane’, we know from previous works that the bottom layer activates ‘Blue Sky’ and the top layer ac-tivates ‘Airplane’, but this still fails to explain how DNNs infer from ‘Blue Sky’ to ‘Airplane’. Thus, if the network ar-chitectures could be disentangled into the sub-architectures in terms of tasks, such as classifying ‘Airplane’, the above concerns could be naturally addressed. For example, we can explain that the edges and colors from bottom layers are
clustered in middle layers to form the parts, such as ‘Wing’ and ‘Window’, of ‘Airplane’, and then the most representa-tive part will be selected to activate the class ‘Airplane’.
In this paper, we introduce a new method termed neural architecture disentanglement (NAD), which learns to dis-entangle a pre-trained DNN into sub-architectures for dif-ferent tasks. As illustrated in Fig. 1, the sub-architectures form the information ﬂows that describe the inference pro-cesses. Inspired by representation disentanglement, we de-sign an objective function constraining the information be-tween successive layers of DNNs. The hidden units are selected to construct the sub-architectures from a DNN’s bottom layers to its top layers. Extensive experiments are conducted to investigate whether, where, and how the dis-entanglement occurs. The architectures used in our exper-iments range from handcrafted to automatically-searched, i.e., VGG16 [31], ResNet50 [11], DenseNet121 [14] and
DARTS-Net [20]. Consistent results are obtained on both object-based and scene-based datasets, i.e., ImageNet [7] and Place365 [41], yielding three new ﬁndings that provide fresh insights into the inner logic of DNNs.
First, we provide evidence that DNNs can be divided ac-cording to independent tasks, i.e., DNNs can be disentan-gled. We compare the classiﬁcation results of the origi-nal architectures and the disentangled sub-architectures in
Fig. 2. After disentanglement, the Top@1 classiﬁcation ac-curacies are squashed into the interval of (90, 100] from (70, 90] on ImageNet and (40, 70] on Place365, which in-dicates that the sub-architectures correlate to the assigned tasks. Second, we ﬁnd that deeper layers do not neces-sarily correspond to higher semantics, i.e., the disentangle-ment can end before the last layer. Previous studies [2, 37] show that the bottom layers of DNNs extract low-level fea-tures (e.g., edges and colors), while the top layers extract high-level features (e.g., object parts) for classiﬁcation. Our new observation is that the disentanglement of high-level information can end before the last layer in architectures with skip-connections. For example, as shown in Figs. 4b and 4d, the top class hit rates of ResNet50 [11] (which has plain skip-connections) and DARTS-Net [20] (which has automatically-searched skip-connections) appear in the 16th and 15th layer, respectively, instead of the last layer.
Third, the connection type in a DNN affects how the infor-mation ﬂows across layers, leading to different disentangle-ment behaviors. Intuitively, direct connections successively transmit information layer by layer, while skip-connections amortize the information over all layers. This makes the overall inference processes behave differently in the archi-tectures with direct connections and with skip-connections.
Importantly, dense skip-connections severely mix up the information for classiﬁcation. For example, as shown in
Fig. 4c, the visualized feature maps of DenseNet121 acti-vate less useful patterns in some layers. However, the high-level information can still be extracted from the last layer, suggesting that the valuable information for classiﬁcation is amortized into each layer. Furthermore, from the per-spective of NAD, we provide an explanation for why DNNs sometimes give wrong predictions. Experimental results show that misclassiﬁed images have a high probability of being assigned to tasks with similar sub-architectures to the correct ones.
In summary, the contributions of this study include:
• We propose a new method, termed neural architecture disentanglement (NAD), to understand the inference process of DNNs.
• We study the properties of NAD with network archi-tectures ranging from handcrafted to automatically-searched. Results on scene-based and object-based datasets yield three new ﬁndings for the inner work-ings of DNNs. 2.