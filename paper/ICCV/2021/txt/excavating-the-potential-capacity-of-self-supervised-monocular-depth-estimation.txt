Abstract
Self-supervised methods play an increasingly important role in monocular depth estimation due to their great poten-tial and low annotation cost. To close the gap with super-vised methods, recent works take advantage of extra con-straints, e.g., semantic segmentation. However, these meth-ods will inevitably increase the burden on the model. In this paper, we show theoretical and empirical evidence that the potential capacity of self-supervised monocular depth esti-mation can be excavated without increasing this cost. In particular, we propose (1) a novel data augmentation ap-proach called data grafting, which forces the model to ex-plore more cues to infer depth besides the vertical image po-sition, (2) an exploratory self-distillation loss, which is su-pervised by the self-distillation label generated by our new post-processing method - selective post-processing, and (3) the full-scale network, designed to endow the encoder with the specialization of depth estimation task and enhance the representational power of the model. Extensive experiments show that our contributions can bring signiﬁcant perfor-mance improvement to the baseline with even less compu-tational overhead, and our model, named EPCDepth, sur-passes the previous state-of-the-art methods even those su-pervised by additional constraints. Code is available at https://github.com/prstrive/EPCDepth. 1.

Introduction
Depth estimation has always been a fundamental prob-lem of computer vision, which dominates the performance of various applications, e.g., virtual reality, autonomous driving, robotics, etc. As the cheapest solution, monocu-lar depth estimation (MDE) has made considerable progress due to the evolvement of Convolution Neural Networks
[27, 44, 45, 16]. However, most existing state-of-the-art ap-proaches rely on supervised training [7, 6, 8, 29, 1], whose training datasets collection is a cumbersome and formidable challenge. As an alternative, self-supervised methods elimi-Figure 1. Depth estimation from a single image. Our model (EPCDepth), trained only on stereo data, performs the best and produces the sharpest and most complete result with the least com-putational cost. nate the need for ground-truth depth through recasting depth estimation as the reconstruction problem among stereo im-ages [10, 12, 49, 68], monocular video [67, 2, 42, 32] or a combination of both [62, 13].
In terms of performance alone, recent works have shown that the gap between self-supervision and full-supervision has made a de facto reduction. But on the other hand, this reduction largely beneﬁts from the sophisticated model ar-chitecture and extra constraints from external modalities, e.g., semantic segmentation [2, 26, 68, 15], optical ﬂow
[57, 40], depth normal [54], etc. Apparently, these factors substantially increase the burden of the model training and run counter to the concept of self-supervision to some ex-tent. In this paper, we show the potential of self-supervised monocular depth estimation even without these additional
constraints from three aspects: data augmentation, self-distillation, and model architecture.
Generally, the closer the projection on the image is to the lower boundary, the smaller the depth of the object. This feature of vertical image position has been proven to be the main cue adopted by the MDE model to infer depth, while the apparent size and other cues that humans will rely on are ignored [47]. We conjecture that the reason is that in the traditional training mechanism that takes the entire im-age as input, the feature of vertical image position exists in almost every training sample, while the number of sam-ples for other cues is relatively small, which leads to a long-tailed distribution on cues. Obviously, this kind of paranoia tends to damage the generalization ability of the model. To solve this, we propose a novel data augmentation method called Data Grafting, which breaks this dilemma by ver-tically grafting a certain proportion from another image to appropriately weaken the relationship between depth and vertical image position. Moreover, there is another fact that the precision of different scales output by the multi-scale network is inconsistent at different pixels, and this moti-vates us to generate better disparity maps as pseudo-labels to realize the self-distillation of the model. Concretely, we propose Selective Post-Processing (SPP) to select the best prediction for each pixel among all scales according to the reconstruction error, which is inspired by the availability of all views during training, and the similar idea has been proven effective in the ﬁeld of multi-view stereo [55]. Fi-nally, we extend the traditional multi-scale network to the full-scale network by inserting prediction modules not only on the decoder but also on the encoder to advance the spe-cialization of depth prediction from decoder to encoder and absorb the representational power of the model. The supe-rior result of our model is shown in Figure 1.
To summarize, our main contributions are listed below in fourfold:
• We introduce a conceptually simple but empirically ef-ﬁcient data augmentation approach, which enables the model to learn more effective cues besides the vertical image position.
• We apply self-distillation to MDE for the ﬁrst time with-out any auxiliary network and generate better pseudo-labels based on our training-oriented selective post-processing method.
• We propose a more efﬁcient full-scale network to strengthen the constraints on the model and enhance the encoder’s speciﬁcity of depth estimation.
• Without bells and whistles, we achieve state-of-the-art performance within self-supervised methods even com-pared to those high-performance models that are trained by extra constraints. 2.