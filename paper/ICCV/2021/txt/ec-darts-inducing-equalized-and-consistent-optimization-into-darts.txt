Abstract
)
% ( y c a r u c c a h c r a e
S
Based on the relaxed search space, differential architec-ture search (DARTS) is efﬁcient in searching for a high-performance architecture. However, the unbalanced com-petition among operations that have different trainable pa-rameters causes the model collapse. Besides, the inconsis-tent structures in the search and retraining stages causes cross-stage evaluation to be unstable.
In this paper, we call these issues as an operation gap and a structure gap in
DARTS. To shrink these gaps, we propose to induce equal-ized and consistent optimization in differentiable architec-ture search (EC-DARTS). EC-DARTS decouples different operations based on their categories to optimize the op-eration weights so that the operation gap between them is shrinked. Besides, we introduce an induced structural tran-sition to bridge the structure gap between the model struc-tures in the search and retraining stages. Extensive experi-ments on CIFAR10 and ImageNet demonstrate the effective-ness of our method. Speciﬁcally, on CIFAR10, we achieve a test error of 2.39%, while only 0.3 GPU days on NVIDIA
TITAN V. On ImageNet, our method achieves a top-1 error of 23.6% under the mobile setting. 1.

Introduction
The computer science community has witnessed the re-markable achievements of Deep Neural Networks (DNN), especially in the ﬁeld of computer vision. However, the common DNNs are designed by human experts, which re-quire a lot of computation resources and domain-speciﬁc knowledges. Recently, Neural Architecture Search (NAS) has emerged to search neural architectures in an automated way, which greatly eases the dependence on human experts and achieves a remarkable performance.
Among various NAS methods, Differentiable Neural Ar-*Corresponding author: caoliujuan@xmu.edu.cn
Kendall Tau = 0.13
Kendall Tau = 0.13
（96.99, 88.06）
（96.99, 88.06）
（97.16, 84.07）
（97.16, 84.07）
（96.91, 83.9）
（97.03, 83.87）
（97.03, 83.87）
（96.91, 83.9） (97.02, 81.38) （97.09, 81.09） (97.02, 81.38) （97.09, 81.09） (97.24, 81.79） (97.24, 81.79） 88 84 80
)
% ( y c a r u c c a
（96.72, 78.54） 76 h c r a e
S
（96.72, 78.54）
（97.09, 76.46）
（97.09, 76.46） (96.76, 75.42) (96.76, 75.42) 70 65 60 55 50 88 84 80 76 72 96.6 96.7 96.6 72 96.8 97.1
Retraining accuracy (%) 97 96.8 97.1
Retraining accuracy (%) 96.7 96.9 97.2 97.3 96.9 97 97.2 45 97.3 96.6 96.8
Kendall Tau = 0.47
Kendall Tau = 0.47
（97.63, 68.19）
（97.63, 68.19）
（97.26, 64.16）
（97.37, 64.33）
（97.26, 64.16）
（97.37, 64.33） 70 65 60
（96.83, 59.65）
（96.83, 59.65） 55
（97.04, 57.09）
（97.48, 57.25）
（97.04, 57.09）
（97.48, 57.25） (97.28, 53.39) (97.17, 52.44) (97.25, 52.58) (97.28, 53.39) (97.17, 52.44) (97.25, 52.58) (96.98, 49.32) (96.98, 49.32) 50 45 97 96.6 97.2 96.8
Retraining accuracy (%) 97.6 97.2
Retraining accuracy (%) 97.8 97.4 97.4 97 97.6 97.8 (a) DARTS (2nd Oder) (a) DARTS (2nd Oder) (a) DARTS (2nd Order) (b) IDARTS (Ours) (b) IDARTS (Ours) (b) EC-DARTS(ours)
Figure 1. The correlation evaluation between the search rank and retraining rank of a single search. We summa-rize the results of 10 randomly selected architectures from a single run of DARTS and EC-DARTS in different colors. chitecture Search (DNAS) [11, 37], e.g., differentiable ar-chitecture search (DARTS) [26], has attracted a lot of atten-tion because it improves the efﬁciency of searching a neural network by orders of magnitudes. Motivated by DARTS, there are many works [5, 43, 7, 6, 23, 44, 1, 46, 21, 2, 42, 10, 45, 36] followed the similar scheme in DARTS, which have achieved considerable performance gains. Despite these achievements, it remains a challenging problem to optimize the search process of DARTS due to the optimization gaps.
The ﬁrst gap is the operation gap caused by the different numbers of trainable parameters contained in different op-erations. The second gap is the structure gap caused by the inconsistent model structures adopted in the search and re-training stages. To explain the operation gap, different op-erations contain different numbers of trainable parameters.
In this case, the costs of optimizing operations that have less trainable parameters will be smaller, which means that
DARTS is biased towards the optimization of the parameter-free operations (e.g., skip-connects and pooling layers).
Therefore, the searched architecture might be dominated by parameter-free operations, which leads to a poor perfor-mance. To explain the structure gap, the operations that are mixed in the one-shot model during the search stage will        
be partially pruned in the retraining stage. The inconsis-tent structures in the search and retraining stages lead to the structure gap. These optimization gaps have been pointed out by several works [39, 43, 7, 6, 23, 1, 46, 21, 11, 37]. To avoid a dramatic performance degradation caused by these optimization gaps, several solutions have been put forward
[43, 7, 6, 23]. DARTS+ [23] directly constrained the skip-connects to two per cell. Cyclic DARTS [43] introduced an evaluation network with 20 cells in the search stage. These solutions have alleviated the optimization gaps, while re-quiring strong prior information or additional computations.
PR-DARTS [46] alleviates unfair competition only between skip connection and other categories of operations. Our
CEN focuses on unfair competition among the whole op-erations and includes PR-DARTS as a special case.
×
In this paper, we propose to induce equalized and consis-tent optimization in differentiable architecture search (EC-DARTS). To shrink the operation gap, we devise a Cross-Edge Normalization (CEN) to equalize the dominance of each operation. Therefore, the operation weights can bet-ter reﬂect the importance of each operation. Speciﬁcally,
CEN normalizes the weights of operations by categories, e.g., normalize the weights of 3 3 separable convolutions from different edges. Besides, CEN eliminates the un-balanced competition between operations of different cate-gories, while introducing a balanced competition between operations of the same category. To shrink the structure gap, a Induced Structural Transition (IST) is proposed to construct an auxiliary model to induce the model structure type in the search stage to transform into a similar one that is used in the retraining stage. In order to quantify the de-gree to which the optimization gaps are alleviated by our method, we adopt the Kendall Tau metric [17] to measure the correlations between different ranks. Please refer to sup-plementary materials for the 3 modes of the the Kendall Tau metric used in this paper. As shown in Figure 1, we conduct a correlation evaluation among 10 architectures randomly selected from a single search. Compared with DARTS, our method achieves a stronger correlation between the search rank and retraining rank during the search process.
Our contributions are summarized as follows.
• We propose to induce equalized and consistent op-timization in differentiable architecture search (EC-DARTS) from the levels of operation and structure.
• We design a Cross-Edge Normalization (CEN) for
NAS. By normalizing the operation weights in a bal-anced competition conditions, our methods exhibits a stronger correlation between the operation weight and model performance. Equipped with CEN, the opera-tion gap is alleviated effectively.
• In order to improve the consistency between the search and retraining stages, we introduce the structure infor-mation of the retraining stage into the search stage by an Induced Structural Transition (IST).
We further conduct comprehensive experiments over four datasets to verify the effectiveness of our method.
Speciﬁcally, our method achieves state-of-the-art perfor-mance on CIFAR10, CIFAR 100, Tiny-ImageNet-200, and
ImageNet. Speciﬁcally, a test error of 2.39% on CIFAR10, and a top-1 error of 23.6% on ImageNet with a model size of 4.7M are reported by our method. 2.