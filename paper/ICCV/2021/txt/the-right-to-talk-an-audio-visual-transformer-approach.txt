Abstract
Turn-taking has played an essential role in structuring the regulation of a conversation. The task of identifying the main speaker (who is properly taking his/her turn of speak-ing) and the interrupters (who are interrupting or react-ing to the main speaker’s utterances) remains a challeng-ing task. Although some prior methods have partially ad-dressed this task, there still remain some limitations. Firstly, a direct association of Audio and Visual features may limit the correlations to be extracted due to different modalities.
Secondly, the relationship across temporal segments help-ing to maintain the consistency of localization, separation and conversation contexts is not effectively exploited. Fi-nally, the interactions between speakers that usually con-tain the tracking and anticipatory decisions about transition to a new speaker is usually ignored. Therefore, this work in-troduces a new Audio-Visual Transformer approach to the problem of localization and highlighting the main speaker in both audio and visual channels of a multi-speaker con-versation video in the wild. The proposed method ex-ploits different types of correlations presented in both vi-sual and audio signals. The temporal audio-visual rela-tionships across spatial-temporal space are anticipated and optimized via the self-attention mechanism in a Transformer structure. Moreover, a newly collected dataset is introduced for the main speaker detection. To the best of our knowl-edge, it is one of the first studies that is able to automati-cally localize and highlight the main speaker in both visual and audio channels in multi-speaker conversation videos. 1.

Introduction
Although human beings possess capabilities of localiz-ing and separating sounds from noisy environments, we still have trouble following a conversation with noises, back-ground voices, or interruptions from other speakers. Either
∗ denotes equal contributions
Figure 1. Given a multi-speaker video, our Audio-Visual Trans-former can localize and highlight the main speaker in both visual and audio channels. (Best viewed in color) with blind audio separation [42, 53, 62, 60, 70] or visual-aid audio separation [4, 6, 13, 31, 33, 35, 39, 48, 43, 57, 61, 65, 66] approaches, this outlier separation task still remains a challenge in the wide conditions beyond the lab settings.
The problem becomes especially harder when dealing with unknown numbers of speakers in an audio. Nachmani et
[54] make a comparison between methods and show al. how hard it is to separate voices when the number of sound sources increases. Existing methods achieve high perfor-mance with inputs from multiple microphones. Some meth-ods assume a clean set of single source audio examples are available for supervision [2, 28, 71, 72]. In practice, rather than solely trying to separate voices of all speakers in a con-versation and determining “who-spoke-when”, we tend to give more attentions to the main speaker, i.e. who is on his/her turn of speaking and his/her talk is the main chan-nel of communication, and ignore the voices of remaining speakers, i.e. interrupters or listener, or background noises.
Table 1. Comparisons of our proposed approach and other modeling methods. Sound Source Localization (SSL)
Goal
Temporal Model
People-Independent
Visual Context modeling (Visual-visual attention)
Audio-Visual Correlation
Ours
Main Speaker
Highlight
LWTNet[5]
Active Speaker
Highlight
Across-Segments Within-Segment Within-Segment
✓
SyncNet [14]
SSL
✓
✗
SoundOfPixel [72]
CocktailParty [28]
Audio Separation
Audio Separation
Within-Segment
✗
Within-Segment
✓
✓
Audio-Visual
Transformer
✗
Cosine
Distance
✗
✗
✗
Audio-Visual
Synchronization
Feature
Concatenation
Feature
Concatenation
Thus, an approach that highlights the main speaker in visual and audio channels would give new opportunities to popu-lar applications such as auto-muting in a tele-conference or main speaker refocusable video generation.
Given a video of multi-speaker conversation, our goal is to learn an audio-visual model that enables the capabilities of both (1) localizing the main speaker; (2) true cancella-tion of audio sources of interrupters or background noises; and (3) automatically switching to a new subject when the speakers change their roles. The interruptions from other subjects and the background are considered as noises and removed. In the scope of this work, we focus on turn-taking conversation as the turn-taking mechanism has been com-monly adopted for structuring conversation in social inter-actions. A subject is considered as the main speaker when he/she properly takes the turn of solo speaking and will con-tinue the talk even after a simultaneous speech occurs [16].
Previous approaches have partially addressed this prob-lem and can be divided into two categories, i.e. audio-visual synchronization [7, 14, 15, 56, 44] and mix-and-separate
[2, 18, 28, 32, 34, 40, 46, 71, 72, 73]. The former exploits the synchronization between audio and video frames within a specific time window to localize the image regions that are more sensitive to audio changes. Meanwhile, the latter learns to separate the speakers’ voices from a mix utterance based on audio and visual features. In both cases, there re-main some limitations. Firstly, audio-visual relationships are extracted via a concatenation operator or the cosine dis-tance metric. However, as audio and visual features dis-tribute in two different latent spaces by their nature, these methods may not maximize correlations between the two feature domains. Secondly, audio-visual relationships are only considered within a video segment, i.e. a short time window, while ignoring the ones across temporal segments, which helps to maintain the consistency of localization and separation, and contextual main-subject switching. Finally, the interactions between subjects in the temporal dimension to deliver accurate tracking and anticipatory decisions about transition to a new target are still ignored.
Contributions. This work introduces a novel Audio-Visual
Transformer approach, a cross-modality temporal-based computer vision algorithm, to highlight main speaker in both audio and visual channels (Fig. 1). The contributions of this work are four-fold. (1) The proposed approach ex-ploits various correlations presented in visual and audio sig-nals including “virtual” interactions between speakers in a video scene and relationships between visual and auditory modalities. (2) Rather than extracting audio-visual correla-tions within a video segment, relationships across segments are further exploited via a temporal self-attention mecha-nism in the proposed Transformer structure. This helps to engage the contextual information and enhance attentions with longer context so that the main speaker can be ro-bustly identified. (3) A Cycle Synchronization Loss is in-troduced to learn the main speaker localization in a self-supervised manner. (4) A newly dataset* is collected for the main speaker detection. To the best of our knowledge, it is one of the first works that is able to automatically localize and highlight the main speaker in multi-speaker conversa-tion videos on both visual and audio channels (Table 1). 2.