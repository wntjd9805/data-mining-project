Abstract
We address the problem of detecting human–object inter-actions in images using graphical neural networks. Unlike conventional methods, where nodes send scaled but oth-erwise identical messages to each of their neighbours, we propose to condition messages between pairs of nodes on their spatial relationships, resulting in different messages going to neighbours of the same node. To this end, we ex-plore various ways of applying spatial conditioning under a multi-branch structure. Through extensive experimentation we demonstrate the advantages of spatial conditioning for the computation of the adjacency structure, messages and the refined graph features.
In particular, we empirically show that as the quality of the bounding boxes increases, their coarse appearance features contribute relatively less to the disambiguation of interactions compared to the spa-tial information. Our method achieves an mAP of 31.33% on HICO-DET and 54.2% on V-COCO, significantly out-performing state-of-the-art on fine-tuned detections. 1.

Introduction
The task of detecting human–object interactions (HOIs) requires localising and describing pairs of interacting hu-mans and objects.
In particular, an HOI is defined as a (subject, predicate, object) triplet, following the definition of visual relations from Lu et al. [23], where the subject and object are typically represented as labelled bounding boxes.
For HOI triplets, the subject is always a human, so the in-teractions of interest simplify to pairs of predicates and ob-jects, e.g., riding a horse or sitting on a bench.
Since the output representations are inherently similar,
HOI detection is most often approached as a downstream task of object detection. Given a set of object detections from an image, one may construct candidate human–object pairs by exhaustively matching between the detected human and object instances. Indeed, the vast majority of previous works [3, 6, 10, 17, 25, 24, 28, 5, 11] use an off-the-shelf (a) An image with detected human and object instances (b) Adjacency matrices computed with appearance features, normalised by rows (left) and columns (right) (c) Adjacency matrices computed with spatial conditioning, normalised by rows (left) and columns (right)
Figure 1. Many images contain far more non-interactive human– object pairs than interactive ones (a). Correct inference of the in-teraction type and the correspondences requires a combination of appearance and spatial information. When using appearance fea-tures only, the adjacency matrix for a graphical neural network tends to be dominated by a few salient objects (b). Since messages from each node to its neighbours are identical apart from an ad-jacency scaling, this leads to the node features being dominated by those of the most salient objects, confusing the classifier. With spatial conditioning, the adjacency matrix is able to reflect the in-herent interactive pairs without explicit supervision (c). object detector [26] as a preprocessing stage. We take the same approach, leveraging the success of modern object de-tectors. While this converts the HOI detection task into the
Table 1. The use of appearance (A) and spatial (S) modalities at different stages of the graphical model, in recent HOI works. Re-finement refers to late-stage fusion that takes place after message passing and fuses the graph features with other modalities.
Methods
GPNN [25]
Wang et al. [11]
DRG [5]
VSGNet [28]
Ours
Adjacency (early fusion)
Message (mid fusion)
Refinement (late fusion)
A
A, S
S
A
A, S
A
A
S
A
A, S –
A, S –
A, S
A, S simpler HOI recognition task on a set of candidate human– object pairs, it is still far from being solved.
Recognising HOIs is extremely challenging. While im-age recognition discriminates between scene types [31] or prominent object types [27], focusing on the holistic under-standing of an image, HOI recognition requires an under-standing of the interactions between specific humans and objects at a much finer level. This requires reasoning about the subtle relationships between the instances as well as their contexts. This is particularly necessary when there are multiple human–object pairs with the same interaction type, where the model needs to correctly infer the interac-tion type and the correspondences between the individual instances. In addition, many interactions do not have strong visual cues and can be quite abstract, such as buying an ap-ple or inspecting a boat. This poses a big challenge for standard CNNs, which excel at recognising physical qual-ities such as texture and shape. HOI detection demands a more sophisticated architecture capable of performing logi-cal reasoning, not merely recognising the visual cues of the humans and objects of interest. The complexity and ambi-guity of the problem is such that even humans can fail to correctly recognise HOIs in images, despite our ability to reason about visual cues and spatial relationships. Follow-ing prior work, we make use of graphical models to model these interrelationships and perform structured prediction.
Since humans and objects in an image play different roles in the interactions, we build a bipartite graph to char-acterise these interrelationships, wherein each human node is connected to each object node. As is intuitive, we use the appearance features of a detected instance as the node encoding, be it a person or an object. Edge encodings, how-ever, have been under-explored in the HOI detection prob-lem. Previous works [25, 11] take the appearance feature extracted from the minimum covering rectangle of the hu-man and object boxes as the edge encoding. This represen-tation does not necessarily encode the spatial relationships between a human–object pair, and there could be additional objects in the tight box other than the intended pair. Instead, we use explicitly learned spatial representations as the edge encodings. To shed some light on their significance, let us consider the example shown in Figure 1a. Graphical mod-els allow the propagation of contextual information between nodes. In this instance, each human node will receive infor-mation suggesting the presence of bikes. However, con-ventional algorithms send identical messages from a node to its neighbours, with the sole variable being a learnable weight that characterises the connectivity. And Figure 1b shows that this connectivity matrix fails to identify correct human–bike pairs with only appearance information, which causes confusion when distinguishing between all putative human–bike pairs. As such, we contend that it is crucial to incorporate spatial information to regulate the message passing procedure. Our intuition is that, with spatial condi-tioning, each human node receives information of the pres-ence of a bike and its relative location. Therefore, the in-teraction riding a bike could potentially be suppressed for a human instance if all bikes in the image are, say, to its left, as opposed to being directly under it.
Our primary contribution is a spatially conditioned mes-sage passing algorithm that renders outgoing messages which are dependent on the receiving nodes. For our bipar-tite graph, the algorithm also passes anisotropic messages across the bipartition. Furthermore, we extend the spatial conditioning mechanism to other parts of the graph—the computation of the adjacency structure and the refinement of the graph features—through a proposed multi-branch fu-sion module. While previous works have also combined appearance and spatial modalities at these two stages of the network as shown in Table 1, our approach is consis-tent at each fusion stage and, in particular, gains signifi-cant performance improvements from using both modali-ties during message passing. Our secondary contribution is an analysis of the relative significance of the different modalities. We empirically show that as detection quality improves, the importance of the coarse appearance features decreases compared to that of the spatial information. We obtain state-of-the-art performance on the HICO-DET [3] and V-COCO [9] datasets, establishing a new benchmark for detecting human–object interactions. 2.