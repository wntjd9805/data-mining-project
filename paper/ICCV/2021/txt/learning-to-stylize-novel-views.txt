Abstract
We tackle a 3D scene stylization problem — generating stylized images of a scene from arbitrary novel views given a set of images of the same scene and a reference image of the desired style as inputs. Direct solution of combining novel view synthesis and stylization approaches lead to re-sults that are blurry or not consistent across different views.
We propose a point cloud-based method for consistent 3D scene stylization. First, we construct the point cloud by back-projecting the image features to the 3D space. Sec-ond, we develop point cloud aggregation modules to gather the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on two diverse datasets of real-world scenes validate that our method generates consistent stylized novel view synthesis results against other alternative approaches. 1.

Introduction
Visual content creation in 3D space has recently attracted increasing attention. Driven by the success of 3D scene rep-resentation approaches [38, 46, 64], recent methods make signiﬁcant progress on various content creation tasks for 3D scenes, such as semantic view synthesis [16, 19] and scene extrapolation [34]. In this work, we focus on the 3D scene stylization problem. As shown in Figure 1, given a set of images of a target scene and a reference image of the de-sired style, our goal is to render stylized images of the scene from arbitrary novel views. 3D scene stylization enables a variety of interesting virtual reality (VR) and augmented re-ality (AR) applications, e.g. augment the street scene at user locations to the Cafe Terrace at Night style by van Gogh.
Learning to modify the style of an existing 3D scene is challenging for two reasons. First, the synthesized novel views (i.e. 2D images) of the stylized 3D scene must con-tain the desired style provided by the reference image. Sec-ond, since our goal is to stylize the holistic 3D scene, the generated novel views need to be consistent across differ-ent viewpoints for the same scene, such as the texture in the yellow boxes shown in Figure 1.
To handle these challenges, one plausible solution is to combine existing novel view synthesis [46, 64] and image stylization approaches [30, 53]. However, such straightfor-ward approaches lead to problematic results since image stylization schemes are not designed to consider the con-sistency issue across different views for the same scene.
Figure 2. Motivation. While the existing methods can be used for the 3D scene stylization task, these methods either produce blurry (image stylization → novel view synthesis), short-range inconsistent (novel view synthesis → image stylization), or long-range inconsistent (novel view synthesis → video stylization) results.
We present the examples in Figure 2 where the results may be blurry if the input images of the target scene are styl-ized before conducting novel view synthesis. On the other hand, if we apply image stylization after novel view syn-thesis, the results are not consistent across different views.
Another possible solution is to treat a series of novel view synthesis results as a video, and use the video stylization frameworks [9, 11, 56] to obtain temporally consistent re-sults. However, as shown in Figure 2, these approaches are not able to enforce long-range consistency (i.e. between two far-away views) as the video stylization schemes only guar-antee the short-term consistency.
In this paper, we propose a point cloud-based method for consistent 3D scene stylization. To synthesize novel views that 1) match arbitrary style images and 2) render images with consistent appearance across different views, the core idea is to operate on the 3D scene representation, i.e. point cloud, of the target scene. Given a set of input images of the target scene, we ﬁrst construct the point cloud by back-projecting the image features to the 3D space according to the pre-computed 3D proxy geometry. To transfer the style of the holistic 3D scene, we develop a point cloud transfor-mation module. Speciﬁcally, we use a series of point cloud aggregation modules to gather the style information of the 3D scene. We then modulate the features in the point cloud with a linear transformation matrix [30] computed accord-ing to the style information of the point cloud and reference image. Finally, we project the transformed features from the point cloud to the 2D space to obtain the novel view synthe-sis results. Since our method synthesizes novel view images from the same stylized point cloud, the rendered results not only demonstrate the desired style, but also are consistent across different viewpoints.
We evaluate the proposed 3D scene stylization method through extensive qualitative and quantitative studies. The experiments are conducted on two diverse datasets of real-world scenes: Tanks and Temples [25] and FVS [45]. We conduct a user preference study to evaluate the stylization quality, i.e. whether the novel view synthesis results match
In addition, we use the the style of the reference image.
Learned Perceptual Image Patch Similarity (LPIPS) [65] metric to measure the consistency of the results synthesized across different novel views.
We make the following contributions in this paper:
• We propose a point cloud-based framework for the 3D scene stylization task.
• We design a point cloud transformation module that learns to transfer the style from an arbitrary 2D refer-ence image to the point cloud of a 3D scene.
• We validate that our method produces high-quality and consistent stylized novel view synthesis results on the
Tanks and Temples as well as FVS datasets. 2.