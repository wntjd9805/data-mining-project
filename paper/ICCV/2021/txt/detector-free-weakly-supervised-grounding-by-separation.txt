Abstract
Nowadays, there is an abundance of data involving im-ages and surrounding free-form text weakly correspond-ing to those images. Weakly Supervised phrase-Grounding (WSG) deals with the task of using this data to learn to lo-calize (or to ground) arbitrary text phrases in images with-out any additional annotations. However, most recent SotA methods for WSG assume an existence of a pre-trained ob-ject detector, relying on it to produce the ROIs for localiza-tion. In this work, we focus on the task of Detector-Free
WSG (DF-WSG) to solve WSG without relying on a pre-trained detector. The key idea behind our proposed Ground-ing by Separation (GbS) method is synthesizing ‘text to image-regions’ associations by random alpha-blending of arbitrary image pairs and using the corresponding texts of the pair as conditions to recover the alpha map from the blended image via a segmentation network. At test time, this allows using the query phrase as a condition for a non-blended query image, thus interpreting the test image as a composition of a region corresponding to the phrase and the complement region. Our GbS shows an 8.5% accu-racy improvement over previous DF-WSG SotA, for a range of benchmarks including Flickr30K, Visual Genome, and
ReferIt, as well as a complementary improvement (above 7%) over the detector-based approaches for WSG. 1.

Introduction
As multi-modal text + images data sources become abun-dant, so grows the importance of natural free-form text su-pervision [57] over the more traditional image labels or image bounding boxes annotation methods. Such multi-image-text pairs) can be almost effort-modal data (i.e. lessly and autonomously collected from web pages and doc-uments with illustrations, user captioned personal photos, transcribed videos, and many more. However, such form of automatic supervision poses significant challenges for learning. First, it is noisy in a sense that some of the text
*Equal contribution
Figure 1. Illustration of our compositional approach. (a) The model is trained to decompose random alpha-blendings of pairs of images conditioned on their associated texts; (b) At test time, the model interprets any image as a composition of two image re-gions, related and unrelated to the conditioning query phrase, thus grounding the phrase to the image pixels. words are not relevant to the image; second, it is not well localized in a sense that it is unknown which parts of the image correspond to which parts of the text. In contrast, in traditional annotation the training signal is highly localized: isolated and cropped object images are commonly used in classification, and bounding boxes or polygons around the objects in detection and/or segmentation. However, these annotations are commonly manual and are costly to collect.
The above discussion highlights the importance of weakly (and autonomously) supervised multi-modal (im-ages + text) learning in general, and Weakly Supervised
Grounding (WSG) in particular. In WSG, the model is ex-pected to learn to localize (highlight) image regions corre-sponding to text phrases. In a sense, WSG is a detection task where the traditional ‘noun object labels’ are replaced
by an unbounded set of things describable using natural lan-guage. Moreover, the WSG model is expected to learn from image + free-form corresponding text (e.g. caption) pairs without any annotations for correspondence of text words or phrases to image regions.
While earlier WSG methods [1, 28, 71, 78] were
’detector-free’, all the more recent state-of-the-art (SotA) methods rely on the existence of pre-trained object detec-tors being the source of the localization RoIs for grounding
[11, 23, 67, 8, 9, 46]. Although this ’detector-based’ setup benefits from higher performance compared to Detector-Free WSG (DF-WSG) methods, in a sense it shifts away from the true WSG, as the detector is trained using bound-ing boxes (which are forbidden in WSG). The use of a detector is indeed plausible when the set of objects sup-ported by the detector significantly overlaps the set of ob-jects (nouns or their taxonomy siblings) appearing in the
WSG texts. However, if we need to train for WSG in a dif-ferent domain (e.g. news [43] or technical documents) or for a significantly different set of objects, we are likely to be required to collect a large set of bounding boxes to train a new detector. Experimental evidence for this appears, for example, in a recent detector-based WSG work [11]1, where it was noted that using the 80-categories COCO-trained de-tector for the Flickr30K and Visual Genome (VG) WSG benchmarks performs poorly, as opposed to their best WSG result obtained with the VG trained detector that supports many more relevant categories.
In this work, we propose an approach for WSG that does not rely on pre-trained detectors and thus addresses the DF-WSG task. Our approach is based on the idea of image and text compositionality. Having an image + correspond-ing text pair, we can consider the image as a composition of image regions glued together (like puzzle pieces) to form the whole image, each corresponding to a phrase of the text.
While for a given single image + text pair the composition parts are not known (due to the WSG setting), we can eas-ily simulate a more complex composition by comprising it from any two random image + text pairs. To do so, we can blend the images of the two pairs using a random alpha map
α, thus making the respective texts of the pairs correspond to the known α and 1
α mapped complementary regions of the blended image. In this way, we can create a reliable localized synthetic training signal for the DF-WSG model that learns to perform text grounding by learning to separate the blended image to its α-mapped constituents conditioned on the respective texts (Figure 1a). At test time, we can ap-ply the trained model on a non-blended query image, which when conditioned on the query phrase is expected to decom-pose the image to constituents related and not related to the conditioning text (Figure 1b). In addition to the separation loss, we further propose two regularization loss terms which
− 1please see the footnote on page 6 in [11] are important for improving the model performance on non-blended test images. These losses help to prevent the model from learning blending artifacts, as well as to prevent the model from making incorrect references.
Our Grounding by Separation (GbS) approach obtains a significant, up to 8.5%, improvement over previous DF-WSG SotA [1] for a range of phrase grounding benchmarks including Flickr30K, Visual Genome, and ReferIt. More-over, our performance on these benchmarks is not only comparable to the detector-based WSG SotA [11, 23, 46], it is also complementary to them, as our approach is ‘detector-free’ and thus may better support classes that are unknown at the time of detector training. As a result, an ensemble of ours and detector-based SotA methods [23, 46] improves the Flickr30K detector-based WSG result by over 7%, un-derlining the benefits of our proposed GbS approach in sit-uations where a detector is available.
To summarize, our key contributions are as follows: (i) We propose a novel GbS approach for training DF-WSG models (WSG without assuming a pre-trained detec-tor) based on learning to separate randomly blended im-ages conditioned on the corresponding texts at train time, and applying the learned model on single images with ar-bitrary text phrase conditioning at test time; (ii) we estab-lish a new SotA for the DF-WSG task improving signifi-cantly the previous best result by up to 8.5% over a range of popular phrase-grounding benchmarks: Flickr30K, VG, and ReferIt; (iii) we provide an extensive ablation study and examine the relative contribution of the components of the GbS method; (iv) we obtain a new absolute SotA in
WSG on Flickr30K via an ensemble of our DF-WSG and the best detector based WSG model, significantly improv-ing the previous SotA result by over 7%. 2.