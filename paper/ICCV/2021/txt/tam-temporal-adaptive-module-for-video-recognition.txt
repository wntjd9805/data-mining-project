Abstract
Video data is with complex temporal dynamics due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adap-tive module (TAM) to generate video-specific temporal ker-nels based on its own feature map. TAM proposes a unique two-level adaptive modeling scheme by decoupling the dy-namic kernel into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture short-term information, while the aggregation weight is gener-ated from a global view with a focus on long-term struc-ture. TAM is a modular block and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The exten-sive experiments on Kinetics-400 and Something-Something datasets demonstrate that our TAM outperforms other tem-poral modeling methods consistently, and achieves the state-of-the-art performance under the similar complex-ity. The code is available at https://github.com/ liu-zhy/temporal-adaptive-module. 1.

Introduction
Deep learning has brought great progress for various recognition tasks in image domain, such as image classi-fication [21, 12], object detection [28], and instance seg-mentation [11]. The key to these successes is to devise flexible and efficient architectures that are capable of learn-ing powerful visual representations from large-scale image datasets [4]. However, deep learning research progress in video understanding is relatively slower, partially due to the high complexity of video data. The core technical prob-lem in video understanding is to design an effective tempo-ral module, that is expected to be able to capture complex temporal structure with high flexibility, while yet to be of (cid:66): Corresponding author. low computational consumption for processing high dimen-sional video data efficiently. 3D Convolutional Neural Networks (3D CNNs) [15, 34] have turned out to be mainstream architectures for video modeling [1, 8, 36, 27]. The 3D convolution is a direct extension over its 2D counterparts and provides a learn-able operator for video recognition. However, this sim-ple extension lacks specific consideration about the tem-poral properties in video data and might as well lead to high computational cost. Therefore, recent methods aim to model video sequences in two different aspects by combin-ing a lightweight temporal module with 2D CNNs to im-prove efficiency (e.g., TSN [40], TSM [23]), or designing a dedicated temporal module to better capture temporal re-lation (e.g., Nonlocal Net [41], ARTNet [38], STM [17],
TDN [39]). However, how to devise a temporal module with both high efficiency and strong flexibility still remains to be an unsolved problem. Consequently, we aim at ad-vancing the current video architectures along this direction.
In this paper, we focus on devising an adaptive module to capture temporal information in a more flexible way. In-tuitvely, we observe that video data is with extremely com-plex dynamics along the temporal dimension due to factors such as camera motion and various speeds. Thus 3D con-volutions (temporal convolutions) might lack enough repre-sentation power to describe motion diversity by simply em-ploying a fixed number of video invariant kernels. To deal with such complex temporal variations in videos, we argue that adaptive temporal kernels for each video are effective and as well necessary to describe motion patterns. To this end, as shown in Figure 1, we present a two-level adaptive modeling scheme to decompose the video specific temporal kernel into a location sensitive importance map and a loca-tion invariant (also video adaptive) aggregation kernel. This unique design allows the location sensitive importance map to focus on enhancing discriminative temporal information from a local view, and enables the video adaptive aggrega-tion to capture temporal dependencies with a global view of the input video sequence.
Specifically, the design of temporal adaptive module
Figure 1. Temporal module comparisons: The standard temporal convolution shares weights among videos and may lack the flexibility to handle video variations due to the diversity of videos. The temporal attention learns position sensitive weights by assigning varied importance for different time without any temporal interaction, and may ignore the long-range temporal dependencies. Our proposed temporal adaptive module (TAM) presents a two-level adaptive scheme by learning the local importance weights for location adaptive enhancement and the global kernel weights for video adaptive aggregation. ⊙ is attention operation, and ⊗ is convolution operation. (TAM) strictly follows two principles: high efficiency and strong flexibility. To ensure our TAM with a low computa-tional cost, we first squeeze the feature map by employing a global spatial pooling, and then establish our TAM in a channel-wise manner to keep the efficiency. Our TAM is composed of two branches: a local branch (L) and a global branch (G). As shown in Fig. 2, TAM is implemented in an efficient way. The local branch employs temporal con-volutions to produce the location sensitive importance maps to enhance the local features, while the global branch uses fully connected layers to produce the location invariant ker-nel for temporal aggregation. The importance map gen-erated by a local temporal window focuses on short-term motion modeling and the aggregation kernel using a global view pays more attention to the long-term temporal infor-mation. Furthermore, our TAM could be flexibly plugged into the existing 2D CNNs to yield an efficient video recog-nition architecture, termed as TANet.
We verify the proposed TANet on the task of action clas-sification in videos.
In particular, we first study the per-formance of the TANet on the Kinetics-400 dataset, and demonstrate that our TAM is better at capturing temporal information than other several counterparts, such as tempo-ral pooling, temporal convolution, TSM [23], TEINet [24], and Non-local block [41]. Our TANet is able to yield a very competitive accuracy with the FLOPs similar to 2D CNNs.
We further test our TANet on the motion dominated dataset of Something-Something, where the state-of-the-art perfor-mance is achieved. 2.