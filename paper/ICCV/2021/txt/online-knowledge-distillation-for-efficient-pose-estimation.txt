Abstract
Existing state-of-the-art human pose estimation methods require heavy computational resources for accurate predic-tions. One promising technique to obtain an accurate yet lightweight pose estimator is knowledge distillation, which distills the pose knowledge from a powerful teacher model to a less-parameterized student model. However, existing pose distillation works rely on a heavy pre-trained estima-tor to perform knowledge transfer and require a complex two-stage learning procedure. In this work, we investigate a novel Online Knowledge Distillation framework by distill-ing Human Pose structure knowledge in a one-stage man-ner to guarantee the distillation efficiency, termed OKDHP.
Specifically, OKDHP trains a single multi-branch network and acquires the predicted heatmaps from each, which are then assembled by a Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in reverse.
In-stead of simply averaging the heatmaps, FAU which consists of multiple parallel transformations with different receptive fields, leverages the multi-scale information, thus obtains target heatmaps with higher-quality. Specifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to mini-mize the discrepancy between the target heatmaps and the predicted ones, which enables the student network to learn the implicit keypoint relationship. Besides, an unbalanced
OKDHP scheme is introduced to customize the student net-works with different compression rates. The effectiveness of our approach is demonstrated by extensive experiments on two common benchmark datasets, MPII and COCO. 1.

Introduction
Human pose estimation aims to recognize and localize all the human anatomical keypoints in a single RGB im-age.
It’s a fundamental technique for high-level vision tasks, such as action recognition [11], virtual reality [44] and human-computer interaction. Since the invention of
DeepPose [55], deep neural networks have been the dom-*Corresponding author
Figure 1. To obtain an efficient 4-Stack network, (a) FPD [66] adopts a two-stage distillation scheme from the static pre-trained 8-Stack network. The proposed OKDHP distills the pose struc-tural knowledge with both (b) Balance and (c) Unbalance archi-tectures in one stage. The teacher is established online with the
FAU. inant solution for human pose estimation, based on which, the approaches [57, 61, 52] focus on exploiting richer repre-sentations with a sequential architecture and achieve state-of-the-art performance. However, the gains of such deep learning based approaches often come with a cost of train-ing and deploying the over-parameterized models, which limits the deployment in resource-intensive mobile devices.
To reduce the computation cost and enhance the model ef-ficiency, many efforts have been devoted to directly de-signing lightweight and real-time networks, e.g., PAF [4],
VNect [36] and MultiPoseNet [27].
As another powerful tool to achieve a good trade-off between speed and accuracy, knowledge distillation [19] follows the teacher-student paradigm. Traditional distilla-tion utilizes a two-stage scheme that starts with a cumber-some pre-trained teacher model, then distills the knowledge to a compact student model.
In the field of pose estima-tion, recent works [66, 22] adopt a traditional offline dis-tillation scheme which distills the knowledge from a large pre-trained pose estimator (teacher) to a lightweight pose estimator (student) as shown in Fig. 1(a). However, train-ing such a heavy teacher model is time-consuming and a
high-capacity model is not always available. Thus, online counterparts [67, 68] are proposed to simplify the distil-lation process to one stage, reducing the demand for the pre-trained teacher model. In ONE [68], a strong teacher model is established on-the-fly and all students share the same target distribution by averaging the predictions of all branches with learnable weights. Prior impressive works are mostly devoted to classification tasks, which neglect the valuable structural knowledge in the pixel-level tasks. Thus, our work focuses on the more challenging pixel-level tasks and proposes the first online pose distillation framework.
Existing pixel-level distillation works [66, 22] use mean squared error (MSE) as the distillation loss which is weak for knowledge transfer. It can not effectively measure the relative entropy between two probability distributions. Be-sides, MSE is used as the loss function of both task-specific supervised term and distillation term. These two loss terms have different optimization targets, one is the ground truth heatmap, the other is the predicted heatmap generated by the teacher. The conflict between two loss terms will devi-ate the optimization into a sub-optimal situation.
To alleviate those limitations, we investigate an online pose distillation approach for efficient pose estimation. The proposed method has two vital aspects for efficiency. One is that we simplify the distillation procedure to one stage.
The other one is that the proposed method significantly im-proves the pose estimation accuracy comparing with the original network. The whole framework is constructed with a Feature Aggregation Unit (FAU) and multiple auxiliary branches, where each branch is treated as a student. The student branch can be both the same or heterogeneous archi-tectures, making up the OKDHP-Balance and the OKDHP-Unbalance architectures respectively and enabling the cus-tomization of different compression rates. The teacher is established by the weighted ensemble of the predictions of all students through the FAU. The FAU here captures the multi-scale information to obtain the higher-quality target heatmap.
Besides, to transfer the pose structural knowledge, the pixel-wise KL divergence loss is utilized to minimize the discrepancy between the target heatmaps and the predicted ones.
In the final deployment, the target single-branch network is acquired by simply removing redundant auxil-iary branches from the trained multi-branch network, which doesn’t introduce any test-time cost increase.
The main contributions of this paper are listed below.
• To our best knowledge, we are the first to propose the online pose distillation approach, which distills the pose structure knowledge in one-stage manner.
• Both balanced and unbalanced versions of OKDHP are introduced, which can customize the target network with different compressing rates.
• Extensive experiments validate the effectiveness of our proposed method on two popular benchmark datasets:
MPII and COCO. 2.