Abstract
Despite the success of self-supervised representation learning on planar data, to date it has not been studied on 360° images. In this paper, we extend recent advances in contrastive learning to learn latent representations that are sufficiently invariant to be highly effective for spherical saliency prediction as a downstream task. We argue that omni-directional images are particularly suited to such an approach due to the geometry of the data domain. To ver-ify this hypothesis, we design an unsupervised framework that effectively maximizes the mutual information between the different views from both the equator and the poles. We show that the decoder is able to learn good quality saliency distributions from the encoder embeddings. Our model com-pares favorably with fully-supervised learning methods on the Salient360!, VR-EyeTracking and Sitzman datasets. This performance is achieved using an encoder that is trained in a completely unsupervised way and a relatively lightweight supervised decoder (3.8 × fewer parameters in the case of the ResNet50 encoder). We believe that this combination of supervised and unsupervised learning is an important step toward flexible formulations of human visual attention. The results can be reproduced on GitHub 1.

Introduction
Unlike traditional media, omni-directional images (ODIs) provide users with the ability to explore different regions of the viewing sphere. The average person’s head movements (HM) are typically a good prediction of the most probable viewport localized within the sphere, while eye movements (EM) reflect regions-of-interest (RoIs) inside the predicted viewports. Thus, when predicting the most salient pixels for 360° images, it is necessary to predict both HM and EM [68].
Despite remarkable advances in the field of visual attention
[34, 6, 68], existing approaches for 360° saliency prediction are still limited in scope/power for two main reasons.
First, all previous state-of-the-art 360° saliency static
*Equal contribution.
Figure 1. Given a set of 360° images and associated projections, a deep representation is learnt by maximizing the mutual information between views of the same scene in the embedding space, while discarding views of different scenes. approaches are trained end-to-end in a supervised manner.
This limits their capacity to leverage unlabelled data. Com-pared to the large-scale 2D video/image saliency datasets
[6] (i.e., up to 10000 images / 1000 video sequences), 360° video/image HM/EM datasets are rather small. This is due to the complex annotation process, which limits the capacity of the fully supervised approaches. Therefore, exploiting unlabelled data for learning better features is critical, and intuitively a good design to follow. Second, most previous approaches apply a CNN on each patch/cube resulting from the equi-rectangular (ERP) and cube map (CMP) projections.
The former suffer from geometric distortions near the poles, whereas the latter stretch the salient regions into different cube faces, forcing the model to lose the global contextual information. These methods are also of high computational complexity, which may limit their applicability.
Modeling visual attention in ODIs using a representation learning function (an encoder) has the core objective of dis-covering useful representations conditioned by the spherical
domain definition of the input. Using a simple convolutional encoder applied on non-euclidean data is often insufficient for learning good quality representations. Indeed the fil-ters will produce a weak response to the signal associated with distorted regions, mostly in the poles (i.e. the Zenith and Nadir), decreasing the prediction ability. We leverage the mutual information (MI) maximization approach and show that maximizing average MI between the representa-tion and local regions of the input (e.g. projections related to the poles) improves the expressive power of the encoding function for the downstream task of saliency prediction.
A powerful recent paradigm for estimating MI is con-trastive learning based on noise contrastive estimation (NCE)
[28], where multiple views of the same scene are brought to-gether in embedding space, while pushing apart views from different scenes. Additionally, as the choice of the views is important for contrastive learning, 360° data offers a new set of choices for more effective MI estimation. The projections used in the scope of this work are task-relevant, but also, make the optimization problem harder, since they are not as susceptible to optimization short-cuts [22] as simple augmen-tations like color jitter and horizontal flips. This improves the expressive power of the encoder. This also motivates us to discard the use of CMP at training/inference, as we argue that the encoder is inherently sensitive to the signal coming from the Zenith and Nadir regions.
Our goal in this paper is to learn representations that cap-ture signal shared between a support image V1 and its corre-sponding projections V2 and V3 as shown in Figure 1. This is achieved by maximizing the agreement between global and local representations of support images and its projections respectively. The approach is inspired from the notion of mutual information (MI) maximization as proposed in Deep
Info Max (DIM) [32] and Augmented Multi-scale DIM [3]; however, we introduce some important differences. First, we add self-attention to induce a soft feature selection mecha-nism over local representations (i.e. intermediate activation maps). Second, we formulate the total loss (Section 3.2) in a way to induce invariance to projections as in [49] and maximize the MI across different augmented (projected) views. Finally, unlike (AMDIM), instead of relying on batch sizes for negative samples, a memory bank is adopted for computational efficiency. Our contributions are as follows:
• We propose a framework to extend the idea of contrastive/self-supervised learning to a new data do-main, specifically 360° images, and show how it can be effectively used for a regression downstream task rather than a simple recognition task.
• Through extensive evaluation as shown in Table 1, we show that contrastive learning can be exploited for saliency prediction, and furthermore that it performs on par with fully supervised methods.
• Our approach addresses one of the key challenges en-countered when predicting 360° saliency by excluding any use of CMP. The design implicitly embeds the geo-metric specifications in the model weights.
• A single subsequent stream of learning on the equi-rectangular projection (ERM) images significantly re-duces the computational cost (8× faster than the most efficient model among other 360° saliency approaches). 2.