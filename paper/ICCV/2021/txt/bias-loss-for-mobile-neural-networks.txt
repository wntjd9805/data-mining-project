Abstract
Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in re-cent years. However, they still fail to provide the same pre-dictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the lay-ers is an important characteristic of these successful CNNs.
However, differences in this characteristic between large
CNNs and their compact counterparts have rarely been in-vestigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic.
Diverse features present in the activation maps derived from a data point during model inference may indicate the pres-ence of a set of unique descriptors necessary to distin-guish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipblockNet models whose archi-tectures are brought to boost the number of unique descrip-tors in the last layers. Experiments conducted on bench-mark datasets demonstrate the superiority of the proposed loss function over the cross-entropy loss. Moreover, our
SkipblockNet-M can achieve 1% higher classification accu-racy than MobileNetV3 Large with similar computational
Figure 1. Accuracy v.s. FLOPs on ImageNet. Our SkipblockNet model trained with the proposed bias loss outperforms previous well-performing compact neural networks trained with the cross-entropy loss. cost on the ImageNet ILSVRC-2012 classification dataset.
The code is available on the link - https://github. com/lusinlu/biasloss_skipblocknet. 1.

Introduction
Deep CNNs have shown superior performance on nu-merous computer vision tasks, such as classification, se-mantic segmentation, and object detection. Typically, mod-els with high predictive power contain a large number of pa-rameters and require a substantial amount of floating point operations (FLOPs); for example, Inception-v3 [45] has approximately 24M parameters and requires 6GFLOPs to process an image with a spatial size of 299 × 299 pixels.
With the advent of AI applications in mobile devices, sev-1
eral studies have focused on developing high-performance
CNNs for resource-constrained settings. Several studies have focused on compressing existing high-performance pretrained models. The compression of the models can be achieved by performing quantization [51, 25, 56, 38, 29, 55], pruning [14, 13, 10, 16], or knowledge distilla-tion [17, 4]. Typically, the downside of these methods is an inevitable degradation of performance.
Another research line has focused on designing com-pact neural networks and architectural units [34, 12, 49, 53, 6, 48].
For example, Xception [6] introduced a cost-efficient replacement for the conventional convolution, namely, depthwise separable convolution. ShuffleNet [53] replaced convolutional layers with a combination of point-wise group convolution with channel shuffle operation. The authors of EfficientNet [48] proposed a scaling method that uniformly scales a model’s width, depth, and resolu-tion using a set of fixed scaling coefficients. However, a significant performance improvement in these methods is mostly connected with an increase in the number of param-eters [49, 12]. The solution to this problem can be the de-sign of a task-specific objective function. The advantage of designing an objective function over the creation of a new architecture is that the former approach can improve the accuracy of a model without increasing the number of parameters. In general, the preferred loss function for clas-sification is the cross-entropy; however, there exist studies indicating that other objectives can outperform the stan-dard cross-entropy loss [46, 52, 32]. The authors of [46] proposed to compute cross-entropy with the weighted mix-ture of targets from the uniform distribution. In scenarios where the class imbalance problem exists, [32] proposed to down-weight the loss assigned to well-classified examples.
In [39], the authors proposed a meta-learning reweighting algorithm in order to tackle the problem of label noise in the dataset. Although these objectives achieve great perfor-mance boost, they target specific problems related mostly to the dataset and do not consider differences between the op-timization of compact neural networks and their large coun-terparts. Diverse and even abundant information in the fea-ture maps of high-performance CNNs often guarantees a comprehensive understanding of the input data. In compact
CNN, due to the small numbers of parameters, the amount of extracted features will be smaller, and may not be suffi-cient to describe the object to be classified. For certain data points, these features may lack unique descriptors required to distinguish between the objects of different classes. As a result, in the absence of a sufficient amount of unique de-scriptors, the model cannot produce a valid prediction. We refer to these as random predictions that contribute no use-ful learning signal to the optimization process.
To address this problem, we design Bias Loss, a new loss that weights each data point’s contribution in proportion to the diversity of features it provides. As a simple measure of diversity, we take the signal’s variance, which describes how far the feature maps’ values are spread from the aver-age. Based on the variance, we design a nonlinear function, whose values serve as weights for the cross-entropy. This way, we let data points with diverse features have a higher impact on the optimization process and reduce a mislead caused by random predictions.
To further realize bias loss’s full potential, we propose the SkipblockNet architecture to address the problem of a lack of extracted features in the last layer. Specifically, we design lightweight intermediate blocks to straightfor-wardly transfer the low-level features from the first layers to the lasts using skip connections. The usage of the pro-posed blocks will increase the number of data points with a large number of unique descriptors. Experimental results showed that the proposed Bias Loss is able to boost the performance of the existing mobile models, such as Mo-bileNetV3 Large [18] (+0.5%), ShuffleNetV2 0.5× [35] (+0.6%), SqueezeNet [23] (+1%). Moreover, Skipblock-Net can surpass state-of-the-art compact neural networks such as MobileNetV3, on numerous tasks with fast infer-ence on mobile devices.
To summarize, our contributions are three-fold: (1) we design a loss function to reduce the mislead in the opti-mization caused by random predictions in compact CNNs; (2) we propose an efficient neural architecture to increase the number of data points with a large number of unique descriptive features; (3) our model achieves state-of-the-art performance on the ImageNet classification task under re-source constrained settings. 2.