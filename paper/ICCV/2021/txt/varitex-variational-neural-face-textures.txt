Abstract 1.

Introduction
Deep generative models can synthesize photorealistic images of human faces with novel identities. However, a key challenge to the wide applicability of such techniques is to provide independent control over semantically meaning-ful parameters: appearance, head pose, face shape, and fa-cial expressions. In this paper, we propose VariTex - to the best of our knowledge the first method that learns a vari-ational latent feature space of neural face textures, which allows sampling of novel identities. We combine this gen-erative model with a parametric face model and gain ex-plicit control over head pose and facial expressions. To generate complete images of human heads, we propose an additive decoder that adds plausible details such as hair.
A novel training scheme enforces a pose-independent la-tent space and in consequence, allows learning a one-to-many mapping between latent codes and pose-conditioned exterior regions. The resulting method can generate geo-metrically consistent images of novel identities under fine-grained control over head pose, face shape, and facial ex-pressions. This facilitates a broad range of downstream tasks, like sampling novel identities, changing the head pose, expression transfer, and more.
The ability to generate images with user-controlled pa-rameters, such as identity-specific appearance, pose, and ex-pressions would have many applications in computer graph-ics and vision. Synthesizing photorealistic images of novel human faces has recently been made possible through deep generative adversarial networks [17, 23, 24] or variational auto encoders [26], that learn the distribution of real faces to generate new identities. However, such methods typically do not provide semantic control over shape, pose, and facial expressions. This results in undesired global appearance changes across different generated images, for example, a change in identity when viewing from a different angle.
In order to gain more control over the generated im-ages, recent work conditions neural networks on explicit 3D geometries [12, 16, 25, 27, 41, 42, 43]. Promising re-sults have been shown by first generating the 2D face im-age from a learned latent space, and then attempting to rig it using graphics techniques in a geometrically consistent manner [41, 42]. This approach suffers from an inherent disadvantage: since the image synthesis is performed in 2D space only, it is hard to enforce consistency under 3D manipulation. Strong supervision via multi-view images or multi-pose data from monocular videos at training time can alleviate this to some degree. However, the inherent 2D na-subject-specific neural ture of the solution prohibits a truly 3D consistent solution for novel test poses, views, and expressions. This prob-lem particularly manifests itself when synthesizing poses and expressions that lie outside the distribution of the train-ing data (Fig. 4). To increase geometric faithfulness, re-cent work has attempted to learn the distribution of faces in 3D, for example by leveraging neural textures to represent 3D scenes in texture space [32, 43, 44]. Especially when trained from video, rendering 3D geometry with neural tex-tures has been shown to produce highly consistent outputs for multiple poses and expressions, albeit at the cost of hav-ing to learn a texture per subject.
Our work generalizes tex-tures [18, 43, 44] to variational neural textures, enabling geometry-aware synthesis of novel identities (see Fig. 1 and 5). Neural textures represent the appearance of a 3D surface as 2D feature maps.
In contrast to prior works, which are trained per subject, variational neural textures do not require strong supervision in the form of multi-view images or minutes-long video sequences as input for each identity. Instead, they are generated by sampling from an underlying latent distribution of neural face textures. Im-portantly, this latent space is learned in a self-supervised scheme from monocular RGB images without requiring any annotations. We use a parametric face model [1] in combi-nation with a differentiable renderer to provide fine-grained control over face shape, head pose, and facial expression.
This is sufficient to generate face interiors that preserve a subject’s identity across pose and expression, but does not model other important details, such as ears, hair, and the mouth interior. To attain complete images of human heads, we propose a pose-aware additive decoder that generates features for visually plausible details (e.g., facial hair). We devise a novel training regime that allows the additive de-coder to learn a one-to-many mapping and in consequence to generate the exterior face region conditioned on different head poses from the same latent code (see Fig. 1).
We propose VariTex: Variational Neural Face Textures – a method to sample novel identities and synthesize consis-tent faces in multiple poses and expressions (Fig. 1 and 3).
We demonstrate state-of-the-art (SoA) photo-realistic re-sults for geometric control (Fig. 3), novel identity image synthesis (Fig. 5), and novel pose synthesis (Fig. 1 and 4).
Our method achieves higher visual identity-consistency than related work (Fig. 4). Quantitatively, we compare em-bedding distances between frontal and posed faces via a
SoA face recognition network [11] (Tbl. 2). Finally, we conduct a user study (Sec. 5.3), where participants rate con-sistency for posed faces and overall photo-realism.
In summary, we make the following contributions: 2. Combining the generative power of learned facial tex-tures with the explicit control of a parametric face model enables fine-grained control over facial expres-sions, head pose, face shape, and appearance. 3. We synthesize plausible outputs for difficult regions where no 3D geometries are available (e.g., hair, ears, and the mouth interior). 4. We show that our method is more identity consistent under geometric transformations. 2.