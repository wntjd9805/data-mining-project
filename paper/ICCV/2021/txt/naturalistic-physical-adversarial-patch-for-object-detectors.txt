Abstract
Most prior works on physical adversarial attacks mainly focus on the attack performance but seldom enforce any restrictions over the appearance of the generated adver-sarial patches. This leads to conspicuous and attention-grabbing patterns for the generated patches which can be easily identified by humans. To address this issue, we pro-pose a method to craft physical adversarial patches for ob-ject detectors by leveraging the learned image manifold of a pretrained generative adversarial network (GAN) (e.g.,
BigGAN and StyleGAN) upon real-world images. Through sampling the optimal image from the GAN, our method can generate natural looking adversarial patches while main-taining high attack performance. With extensive experi-ments on both digital and physical domains and several independent subjective surveys, the results show that our proposed method produces significantly more realistic and natural looking patches than several state-of-the-art base-lines while achieving competitive attack performance. 1 1.

Introduction
With the advancement of deep learning technologies, modern computer vision models can achieve comparable or even surpassing human performance on tasks such as face detection [10] and face recognition [10]. Although these technologies bring convenience to humans by automating daily routine tasks, they also significantly hurt our privacy since malicious people could easily utilize them to automat-ically collect private and sensitive personal information. To address this issue, some researchers propose to protect peo-ple from these threats by leveraging the adversarial exam-ples which can be used to fool deep learning systems by adding small or imperceptible perturbations to system in-puts. Adversarial attacks for deep learning systems can be categorized by two settings: (1) digital attacks, where deep 1Code is available at: https://github.com/aiiu-lab/
Naturalistic-Adversarial-Patch
Figure 1. It shows the crafted adversarial patch generated by the proposed approach along with others by recent methods (a) [47] (b) [45] (c) [42] (d) [19] (e) ours. Our patch is more natural look-ing and less conspicuous than others so it is harder to human ob-servers to identify it. learning models takes the digital attack images as inputs and (2) physical attacks, where the models take attack inputs that are retaken by a camera.
In this work, we focus on the second category due to its practical use in the real-world setting against the surveil-lance of various indoor and outdoor cameras around the world. Adversarial patch is one of most effective physi-cal adversarial examples for this purpose. There are several works developed in this direction, including [19,42,45,47].
To the best of our knowledge, most of prior works on phys-ical adversarial attacks mainly focus on the attack perfor-mance, and increasing adversarial strength of the perturba-tion is one of the most effective and direct ways for them.
However, this usually leads to conspicuous and attention-grabbing patterns for the generated patches, which can be easily identified by human observers. To address this issue, we propose a method to craft physical adversarial patches for object detectors by leveraging the learned image mani-fold of generative adversarial networks (GANs) (e.g., Big-GAN [5] and StyleGAN [23, 24]) pretrained on real-world images. Through sampling images from GANs that mini-mizes detection score of a target object (e.g., person), our
method can generate natural looking adversarial patches while maintaining acceptable attack performance. In addi-tion, we also apply a clipping strategy to constrain the range of traversal from the initial starting point for optimization on the latent space of GAN for better image quality of gen-erated patches. With extensive experiments on both digital and physical settings along with several independent sub-jective surveys, the results show that our proposed method produces significantly more realistic and natural looking patches than several state-of-the-art baselines while achiev-ing competitive attack performance. A qualitative example is shown in Figure 1, where the patch generated by the pro-posed approach is more naturalistic and much less conspic-uous than other compared methods.
From a security perspective, the existence of natural looking adversarial patches, which can not only fool detec-tors but also prevent suspicion from humans, is a potential issue. Thus, our work focuses on generating those natural-looking adversarial patches, verifying its existence and an-alyzing its properties.The main contributions of this work are summarized as follows:
• We leverage pretrained deep generative models (i.e.,
StyleGAN, BigGAN) by traversing upon their la-tent spaces to craft more natural looking adversar-ial patches than other state-of-the-art baselines while maintaining the comparable attack capability.
• We conduct a thorough performance and naturalness analysis of the proposed method under different digital and physical settings in both indoors and outdoors. 2.