Abstract
We present a deep learning pipeline that leverages net-work self-prior to recover a full 3D model consisting of both a triangular mesh and a texture map from the colored 3D point cloud. Different from previous methods either exploit-ing 2D self-prior for image editing or 3D self-prior for pure surface reconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep neural networks to signiﬁcantly improve the geometry quality and produce a high-resolution texture map, which is typically missing from the output of commodity-level 3D scanners. In particular, we ﬁrst gen-erate an initial mesh using a 3D convolutional neural net-work with 3D self-prior, and then encode both 3D informa-tion and color information in the 2D UV atlas, which is fur-ther reﬁned by 2D convolutional neural networks with the self-prior. In this way, both 2D and 3D self-priors are uti-lized for the mesh and texture recovery. Experiments show that, without the need of any additional training data, our method recovers the 3D textured mesh model of high qual-ity from sparse input, and outperforms the state-of-the-art methods in terms of both geometry and texture quality. 1.

Introduction
Textured mesh is one of the most desirable representa-tion for 3D objects, which has been widely used in many applications, such as industrial design and digital entertain-ment because it enables not only the 3D related task like
∗Equal contribution
†Corresponding author
Project page: https://yqdch.github.io/DHSP3D collision detection but also the rendering capability. As a re-sult, the ability to create a full 3D model consisting of both a 3D triangular mesh and a texture map is a long-lasting prob-lem and consistently draws attention. While purely image-based solution exists [24], the 3D scanners with active il-lumination usually provide much more accurate 3D models texture-less and are robust against challenging cases, e.g. regions. Unfortunately, many commodity-level 3D scan-ners, e.g. Artec Eva [1], iReal 2S [3], Einscan Pro [2] etc, only produce colored point clouds as the output, where the object surfaces and texture maps are missing. There are plenty of works that generate a mesh model from a point cloud, but they usually rely on strong assumptions [42, 36], require pre-training on large dataset [8, 39, 18, 11], and do not produce a texture.
In this work, we propose a method for reconstructing a full 3D model, i.e., a textured triangular mesh, from a colored point cloud. This task is highly under-constrained, and thus prior knowledge is extremely important. It is well known that deep learning model is good at learning prior from a large dataset [42, 18, 11], but also at the same time prone to overﬁtting to the dataset bias.
Instead, Ulyanov et al. [48] proposed to randomly initialize a convolutional neural network (CNN) to upsample a given image, which used the network structure as a prior without the need of any additional training data. Sharing the similar spirit, we resort to such self-prior naturally encoded in the neural net-work for the full 3D model reconstruction task. As one of the most related prior arts, Hanocka et al. [23] proposed to create a mesh, without texture, from a point cloud us-ing a MeshCNN [22] to deform from the convex hull. They found that the graph-based CNN can also learn the self-prior
from the input point cloud to reconstruct a 3D mesh with noise suppressed and missing parts ﬁlled. Despite signiﬁ-cant improvements over previous methods and the capabil-ity to handle challenging cases, however, its output quality highly depends on the input noise level and sparsity (See
Sec.4.3), and the effect of 3D network self-prior is not phe-nomenal as its 2D counterpart [48] empirically.
We propose to exploit the hybrid 2D-3D self-prior for full mesh reconstruction. Speciﬁcally, we ﬁrst utilize the 3D
MeshCNN [22] to exploit the 3D prior in a similar way as
Hanocka et al. [23] and generate an initial 3D mesh model.
Then we create a UV atlas encoding the 3D location of the points instead of color information, which is then reﬁned by a 2D CNN using the self-prior and used to update the 3D mesh. We ﬁnd this 2D network is surprisingly more effective, compared to the 3D MeshCNN, in learning self-prior, and can provide valuable regularization in producing high-resolution mesh with delicate details. The 3D-prior and 2D-prior network runs iteratively to reﬁne the 3D mesh model, and extensive experiments show that our model sig-niﬁcantly improves the geometry quality.
Besides the triangulated mesh, our method also recovers a high-resolution texture map. While it is not trivial to build such a texture map from colors on the sparse point cloud since the texture maps are usually in much higher resolu-tion than the 3D geometry, e.g., the number of faces, we borrow the help from the 2D self-learned CNN with the be-lief that the self-prior is stronger and easier to learn on 2D
CNN compared to a 3D graph-based convolutional neural network (GCN). Using the same UV atlas generated from the 3D mesh, we train a 2D CNN to recover the color from sparse point, and ﬁnd appealing texture maps can be gen-erated automatically. The texture map will be iteratively optimized together with the 3D mesh model.
Our contributions can be summarized as follows. First, we propose a deep learning pipeline that reconstructs a full 3D model with both a triangular mesh and a texture map from a sparse colored point cloud by leveraging self-prior from the network. Second, a novel hybrid 2D-3D self-prior is exploited in our pipeline without learning on any extra data for both geometry and texture recovery. Experiments demonstrate that our method outperforms both the tradi-tional and the existing state-of-the-art deep learning based methods, and both 2D prior and 3D prior beneﬁts the full mesh reconstruction. 2.