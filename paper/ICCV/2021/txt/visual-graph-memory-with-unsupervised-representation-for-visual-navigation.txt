Abstract
We present a novel graph-structured memory for visual navigation, called visual graph memory (VGM), which con-sists of unsupervised image representations obtained from navigation history. The proposed VGM is constructed in-crementally based on the similarities among the unsuper-vised representations of observed images, and these rep-resentations are learned from an unlabeled image dataset.
We also propose a navigation framework that can utilize the proposed VGM to tackle visual navigation problems. By incorporating a graph convolutional network and the at-tention mechanism, the proposed agent refers to the VGM to navigate the environment while simultaneously building the VGM. Using the VGM, the agent can embed its naviga-tion history and other useful task-related information. We validate our approach on the visual navigation tasks us-ing the Habitat simulator with the Gibson dataset, which provides a photo-realistic simulation environment. The ex-tensive experimental results show that the proposed navi-gation agent with VGM surpasses the state-of-the-art ap-proaches on image-goal navigation tasks. Project Page: https://sites.google.com/view/iccv2021vgm 1.

Introduction
Visual navigation has been one of the fundamental build-ing blocks in developing intelligent autonomous agents. To effectively navigate through a large-scale environment, an agent is required to build an internal representation of the environment from raw sensory inputs and its own actions.
Using this internal representation, the agent can store useful information such as its navigation history and successfully perform tasks with additional guidance.
Numerous studies in psychology and cognitive science have shown that animals build a landmark-based topologi-cal representation about the environment during navigation
[14, 16, 18, 31, 33]. Inspired by this observation, several navigation methods based on a topological map have been proposed [2, 3, 5, 15, 25, 29]. These methods use a graph, in which a vertex represents a landmark in the environment
*This work was supported by Institute of Information & Communica-tions Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-01190, [SW Star Lab] Robot Learning:
Efficient, Safe, and Socially-Acceptable Machine Learning). † These au-thors contributed equally to this work. and an edge represents the relationship between two ver-tices, such as reachability and proximity.
Recent works in computer vision and deep learning show remarkable improvements in visual navigation tasks using topological representation [4, 8, 9, 23, 24, 27]. The existing methods autonomously build a topological map about the environment using a pretrained classifier network, which can determine whether two image observations are close or not. However, there are limitations to currently available methods. (1) To train the classifier network, elaborately de-signed annotation rules based on accurate geometric infor-mation are required for preparing training datasets. (2) Their navigation performance depends on the quality of geometric information (e.g., odometry) for calculating distances be-tween nodes or providing local point-goals to the naviga-tion policy. (3) The majority of methods [4, 9, 23, 24, 27] assumes that the environment is previously observed be-fore. These methods use a pre-built graph and the size of the graph is fixed during the navigation, demanding a pre-exploration time to build the entire topological map before an actual navigation task can begin.
In order to address the aforementioned limitations, we propose the visual graph memory (VGM), a graph-structured memory for visual navigation. VGM is con-structed using an RGBD image encoder, which is trained in an unsupervised manner. This encoder can be trained us-ing only an unlabeled image dataset, and this can release the burden of annotating the training dataset. We also present a navigation framework which is designed to utilize the VGM to tackle visual navigation tasks. The proposed navigation framework consists of two components: a memory update module and a navigation module, as shown in Figure 1a.
The memory update module selectively stores image rep-resentations from the navigation history and constructs a
VGM using their spatio-temporal relationships. The navi-gation module uses the VGM to select an ideal action for a given navigation task. Specifically, this module encodes the VGM using a graph convolutional network and extracts the attention-guided context information from the encoded
VGM using the current and target images.
The proposed navigation framework does not require robot pose information unlike previous methods, thus the navigation performance is independent of the environmen-tal errors. Furthermore, the proposed navigation framework incrementally builds the VGM during navigation. Hence, the proposed method is able to conduct a given navigation
task in an unfamiliar environment without pre-exploration, unlike existing methods.
We evaluate the proposed method for the image-goal navigation task, which requires an explorative behavior and memory to find a target location based on an image. The proposed navigation method finds the target location in 76% of the test episodes, which is 11.8% better than the best per-forming baseline among the state-of-the-art approaches us-ing visual memories. In terms of success weighted by path length (SPL), the proposed method achieves 0.64, which is a 14.3% improvement over the baseline. The main contri-butions of this paper are as follows:
• We present the visual graph memory (VGM), which can be built without any geometric information-based annotations. The VGM enables unsupervised topologi-cal simultaneous localization and mapping (SLAM) in an unseen environment.
• We also present a navigation framework that can uti-lize the VGM for visual navigation tasks. The pro-posed navigation framework is able to conduct a given visual navigation task in a completely unseen environ-ment. Furthermore, the proposed method is highly ro-bust against pose information errors compared to ex-isting methods.
• The extensive experimental results show that the pro-posed navigation framework with VGM outperforms the state-of-the-art visual navigation methods based on visual memories. 2.