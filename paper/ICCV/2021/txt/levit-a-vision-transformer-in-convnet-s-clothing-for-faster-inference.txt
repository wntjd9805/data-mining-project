Abstract
We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural net-works to apply them to transformers, in particular activa-tion maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional infor-mation in vision transformers.
As a result, we propose LeViT: a hybrid neural network for fast inference image classification. We consider differ-ent measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenar-ios. Our extensive experiments empirically validate our technical choices and show they are suitable to most ar-chitectures. Overall, LeViT significantly outperforms ex-isting convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on
CPU. We release the code at https://github.com/ facebookresearch/LeViT. 1.

Introduction
Transformer neural networks were initially introduced for Natural Language Processing applications [1]. They now dominate in most applications of this field. They ma-nipulate variable-size sequences of token embeddings that are fed to a residual architecture. The model comprises two sorts for residual blocks: Multi-Layer Perceptron (MLP) and an original type of layer: the self-attention, which al-lows all pairs of tokens in the input to be combined via a bilinear function. This is in contrast to 1D convolutional approaches that are limited to a fixed-size neighborhood.
Recently, the vision transformer (ViT) architecture [2] obtained state-of-the-art results for image classification in the speed-accuracy tradeoff with pre-training on large scale dataset. The Data-efficient Image Transformer [3] obtains competitive performance when training the ViT models
It also introduces smaller models only on ImageNet [4].
Figure 1. Speed-accuracy operating points for convolutional and visual transformers. Left plots: on 1 CPU core, Right: on 1 GPU.
LeViT is a stack of transformer blocks, with pooling steps to re-duce the resolution of the activation maps as in classical convolu-tional architectures. adapted for high-throughput inference.
In this paper, we explore the design space to offer even better trade-offs than ViT/DeiT models in the regime of small and medium-sized architectures. We are especially interested in optimizing the performance–accuracy trade-off, such as the throughput (images/second) performance depicted in Figure 1 for Imagenet-1k-val [5].
While many works [6, 7, 8, 9, 10] aim at reducing the memory footprint of classifiers and feature extractors, in-ference speed is equally important, with high throughput corresponding to better energy efficiency. In this work, our goal is to develop a Vision Transformer-based family of models with better inference speed on both highly-parallel architectures like GPU, regular Intel CPUs, and ARM hard-ware commonly found in mobile devices. Our solution re-introduces convolutional components in place of trans-former components that learn convolutional-like features.
In particular, we replace the uniform structure of a Trans-former by a pyramid with pooling, similar to the LeNet [11] architecture. Hence we call it LeViT.
There are compelling reasons why transformers are faster than convolutional architectures for a given compu-tational complexity. Most hardware accelerators (GPUs,
TPUs) are optimized to perform large matrix multiplica-In transformers, attention and MLP blocks rely tions. mainly on these operations. Convolutions, in contrast, re-quire complex data access patterns, so their operation is of-ten IO-bound. These considerations are important for our exploration of the speed/accuracy tradeoff.
The contributions of this paper are techniques that allow
ViT models to be shrunk down, both in terms of the width and spatial resolution:
• A multi-stage transformer architecture using attention as a downsampling mechanism;
• A computationally efficient patch descriptor that shrinks the number of features in the first layers;
• A learnt, per-head translation-invariant attention bias that replaces ViT’s positional embeddding;
• A redesigned Attention-MLP block that improves the network capacity for a given compute time. 2.