Abstract
If an image tells a story, the image caption is the briefest narrator. Generally, a scene graph prefers to be an omni-scient “generalist”, while the image caption is more will-ing to be a “specialist”, which outlines the gist. Lots of previous studies have found that a scene graph is not as practical as expected unless it can reduce the trivial con-tents and noises.
In this respect, the image caption is a good tutor. To this end, we let the scene graph borrow the ability from the image caption so that it can be a specialist on the basis of remaining all-around, resulting in the so-called Topic Scene Graph. What an image caption pays attention to is distilled and passed to the scene graph for estimating the importance of partial objects, relationships, and events. Specifically, during the caption generation, the attention about individual objects in each time step is col-lected, pooled, and assembled to obtain the attention about relationships, which serves as weak supervision for regular-izing the estimated importance scores of relationships. In addition, as this attention distillation process provides an opportunity for combining the generation of image caption and scene graph together, we further transform the scene graph into linguistic form with rich and free-form expres-sions by sharing a single generation model with image cap-tion. Experiments show that attention distillation brings significant improvements in mining important relationships without strong supervision, and the topic scene graph shows great potential in subsequent applications. 1.

Introduction
A picture is worth a thousand words. However, only a few person prefers to know all of the “thousand words”, while others would like to be informed the “topic words”.
Therefore, the scene graph and the image caption are used for conveying the image contents out of different purposes.
Figure 1. Comparison of the (a) traditional general scene graph, and the (b) topic scene graph generated under the guidance of attention in the image caption, which gives pri-ority to the important relationships (highlighted nodes and edges), and expresses a relationship in the style of natural language.
Concretely, the scene graph [20] consists of objects in an image and the relationships between pairs of objects. A se-ries of studies have tried to generate the scene graph and realize its potential in advanced intelligence tasks, e.g., vi-sual Q&A [2, 47], visual reasoning [44], and vision-and-language navigation (VLN) [53], etc. Nevertheless, as pointed in [25, 35, 52], the scene graph is helpful only if it is informative, while the current generated scene graph with such a lot of noises does not meet this standard. This is mainly due to the explosive combination possibility of two objects [52, 65], which brings the double-edge effect that the scene graph is comprehensive but the key information is overwhelmed by massive trivial details. It is necessary and practical to make the scene graph well-circumscribed between important and trivial contents. Fortunately, the im-age caption exactly shows this ability and is a good teacher from which a scene graph should learn.
In the context of scene graph generation, few researches devote endeavor to discovering the important relationships, which is a meaningful step for restricting the scale of the
scene graph when it is used for downstream tasks. The most popular approach is to keep the relationships with large products of the predicted subject, object, and predi-cate scores. However, this product measures the accuracy of prediction rather than the importance. Yang et al. [58] and Lv et al. [33] either use a light-weight relationship pro-posal network to extract some probably related pairs, or pre-dict an attention score for each relationship, based on the perspective that annotated relationships are the important ones. This may be questionable because the mainstream scene graph datasets (e.g., Visual Genome [22]) are suffered from serious long-tailed problem [6, 46] and the annotated pairs (head pairs) are usually trivial ones [52]. To more pre-cisely define what is important relationship, image caption is found helpful because a caption almost exactly reveals what humans think important [14], as shown in Figure 1.
Consequently, Yu et al. [65] and Wang et al. [52] learn to mine the important relationships under the strong supervi-sion from the important relationship annotation which is ob-tained under the guidance of captions. But this process need to transform the captions into triplets first and then align two groups of heterologous triplets, which is so expensive and complicates the scene graph generation.
In this work, we propose to let the scene graph learn the important relationships from the image caption in an eco-nomical way, resulting in the Topic Scene Graph. The importance of relationship is estimated by distilling the vi-sual attention during the image caption generation, which is treated as the weak supervision. Specifically, most ad-vanced image captioners are able to fix its gaze on the cor-rect object regions. We apply an image captioner and col-lect the first-order attention information with respect to the object regions, which are used for assembling the second-order attention about the relationships. In this way, we ac-tually transform humans’ attention into a new form, con-verting it from the concern about individual to that about relational events. The second-order attention is used as the weak supervision for guiding the estimation of the impor-tance of the relationship. In this way, strong supervision is no longer necessary.
Furthermore, as the attention distillation process makes it possible to generate the image caption and scene graph simultaneously and both of them are the description of an image, why not generating them with a single model? It is noted that the most popular scene graph dataset, Vi-sual Genome (VG) [22] of world scale contains more than 40,000 types of relationships which are originally extracted from humans’ language, while the traditional definition of scene graph treats the relationship recognition as predicate classification and makes most of the relationships filtered, which is harmful to the diversity of relationship descrip-tion. What is worse, there exist huge interior differences in some certain predicate classes, e.g., the appearance of two relationship triplet instances for the predicate class rid-ing, person-riding-horse and dog-riding-skateboard are to-tally different. It is difficult to clearly define the semantic boundaries between different predicates. Inspired by [21], we redefine the scene graph as the set of short relational sen-tences. In this way, a shared captioning module can be used for the so-called linguistic scene graph generation and im-age captioning at the same time. 2.