Abstract
A distinctive feature of Doppler radar is the measure-ment of velocity in the radial direction for radar points.
However, the missing tangential velocity component ham-pers object velocity estimation as well as temporal integra-tion of radar sweeps in dynamic scenes. Recognizing that fusing camera with radar provides complementary informa-tion to radar, in this paper we present a closed-form so-lution for the point-wise, full-velocity estimate of Doppler returns using the corresponding optical ﬂow from camera images. Additionally, we address the association problem between radar returns and camera images with a neural network that is trained to estimate radar-camera correspon-dences. Experimental results on the nuScenes dataset ver-ify the validity of the method and show signiﬁcant improve-ments over the state-of-the-art in velocity estimation and accumulation of radar points. 1.

Introduction
Radar is a mainstream automotive 3D sensor, and along with LiDAR and camera, is used in perception systems for driving assistance and autonomous driving [34, 18, 2]. Un-like LiDAR, radar has been widely installed on existing ve-hicles due to its relatively low cost and small sensor size, which makes it an easy ﬁt into various vehicles without changing their appearance. Thus, advances in radar vision systems have potential to make immediate impact on ve-hicle safety. Recently, with the release of a couple of au-tonomous driving datasets with radar data included, e.g.,
Oxford Radar RobotCar [1] and nuScenes [5], there is great interest in the community to explore how to leverage radar data in various vision tasks such as object detection [24, 37].
In addition to measuring 3D positions, radar has the spe-cial capability of obtaining radial velocity of returned points based on the Doppler effect. This extra capability is a sig-niﬁcant advantage over other 3D sensors like LiDAR, en-abling, for instance, instantaneous moving object detection.
However, due to the inherently ambiguous mapping from (a) (b) (c)
Figure 1: (a) Full motion cannot be determined with a single sen-sor: all motions ending on the blue dashed line (i.e., blue dashed arrows) map to the same optical ﬂow and all motions terminated on the red dashed line (i.e., red dashed arrows) ﬁt the same ra-dial motion. However, with a radar-camera pair, the full motion can be uniquely decided: only the motion drawn in black satis-ﬁes both optical ﬂow and radial motion. (b) Optical ﬂow in the camera-image and (c) a bird’s-eye view of the observed vehicle.
This shows measured radar points with radial velocity (red), our predicted point-wise, full velocity (black), and ground truth full velocity of the vehicle (green). radial velocity to full velocity, using radial velocity directly to account for the real movement of radar points is inad-equate and sometimes misleading. Here, the full veloc-ity denotes the actual velocity of radar points in 2D or 3D space. While radial velocity can well approximate full ve-locity when a point is moving away from or towards the radar, these two can be very different when the point is moving in the non-radial directions. An extreme case oc-curs for objects moving tangentially as these will have zero radial velocity regardless of target speed. Therefore, ac-quiring point-wise full velocity instead of radial velocity is crucial to reliably sense the motion of surrounding objects.
Apart from measuring the velocity of objects, another important application of point-wise velocity is the accumu-lation of radar points. Radar returns from a single frame are much sparser than LiDAR in both azimuth and eleva-tion, e.g., typically LiDAR has an azimuth resolution 10× higher than radar [37]. Thus, it is often essential to accu-mulate multiple prior radar frames to acquire sufﬁciently dense point clouds for downstream tasks, e.g., object detec-tion [26, 6, 7]. To align radar frames, in addition to com-pensating egomotion, we shall consider the motion of mov-ing points in consecutive frames, which can be estimated by point-wise velocity and time of movement. As the radial velocity does not reﬂect the true motion, it is desirable to have point-wise full velocity for point accumulation.
To solve the aforementioned dilemma of radial velocity, we propose to estimate point-wise full velocity of radar re-turns by fusing radar with a RGB camera. Speciﬁcally, we derive a closed-form solution to infer point-wise full veloc-ity from radial velocity as well as associated projected im-age motion obtained from optical ﬂow. As shown in Fig. 1, constraints imposed by optical ﬂow resolve the ambiguities of radial-full velocity mapping and lead to a unique and closed-form solution for full velocity. Our method can be considered as a way to enhance raw radar measurement by upgrading point-wise radial velocity to full velocity, laying the groundwork for improving radar-related tasks, e.g., ve-locity estimation, point accumulation and object detection.
Moreover, a prerequisite for our closed-form solution is the association between moving radar points and image pixels. To enable a reliable association, we train a neural network to predict radar-camera correspondences as well as discerning occluded radar points. Experimental results demonstrate that the proposed method improves point-wise velocity estimates and their use for object velocity estima-tion and radar point accumulation.
In summary, the main contributions of this work are:
• We deﬁne a novel research task for radar-camera per-ception systems, i.e., estimating point-wise full veloc-ity of radar returns by fusing radar and camera.
• We propose a novel closed-form solution to infer full radar-return velocity by leveraging the radial velocity of radar points, optical ﬂow of images, and the learned association between radar points and image pixels.
• We demonstrate state-of-the-art (SoTA) performance in object velocity estimation, radar point accumula-tion, and 3D object localization. 2.