Abstract
Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as ob-jects.
In this paper, we propose one of the ﬁrst methods that learn from image-sentence pairs to extract a graphi-cal representation of localized objects and their relation-ships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an off-the-shelf object detector to identify and localize object in-stances, match labels of detected regions to concepts parsed from captions, and thus create “pseudo” labels for learn-ing scene graph. Further, we design a Transformer-based model to predict these “pseudo” labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30% relative gain over a latest method trained with human-annotated unlocalized scene graphs.
Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the ﬁrst result for open-set scene graph generation. 1.

Introduction
An image might have millions of pixels, yet its visual content can be often summarized using dozens of words.
Images and their text descriptions (i.e. captions) are avail-able in great abundance from the Internet [40], and offer a unique opportunity of image understanding aided by natu-ral language. Learning visual knowledge from image-text pairs has been a long-standing problem [50, 8, 14, 7, 22, 54, 58, 16, 61, 37, 11, 35], with recent success on learning deep models for visual representation [7, 22, 37, 11, 35], and for recognizing and detecting individual visual concepts (e.g. objects) [54, 58, 61, 35, 16]. In this paper, we ask the question: can we learn to detect visual relationships beyond individual concepts from image-text pairs? Fig. 1 (a) illus-trates an example of such relationships (“man drive boat”).
As a ﬁrst step, we focus on learning scene graph gen-eration (SGG) from image-sentence pairs. A scene graph is a symbolic and graphical representation of an image,
Figure 1. Top (our setting): Our goal is learning to generate local-ized scene graphs from image-text pairs. Once trained, our model takes an image and its detected objects as inputs and outputs the image scene graph. Bottom (our results): A comparison of re-sults from our method and [60] with varying levels of supervision. with each graph node as a localized object and each edge as a relationship (e.g. a predicate) between a pair of ob-jects. Scene graph has emerged as a structured repre-sentation for many vision tasks, including action recogni-tion [19], 3D scene understanding [1, 49], image genera-tion and editing [21, 13], and vision-language tasks (e.g. image captioning [55, 56, 66] and visual question answer-ing [41, 47, 17, 17]). Most previous scene graph meth-ods [52, 28, 62, 27, 53, 4, 46, 45, 65] follow a fully su-pervised approach, relying on human annotations of object bounding boxes, object categories and their relationships.
These annotations are very costly and difﬁcult to scale. Re-cently, Zareian et al. [60] considered weakly supervised learning of scene graphs from image-level labels of unlocal-ized scene graphs. Nonetheless, learning scene graphs from images and their text descriptions remains unexplored.
A major challenge of learning scene graphs from image-sentence pairs is the missing link between many candidate image regions and a few concepts (e.g. nouns and predi-cates) parsed from an image caption. To this end, we pro-pose to leverage off-the-shelf object detectors, capable of identifying and localizing object instances from hundreds of common categories. Our key idea is that object labels
of detected image regions can be further matched to sen-tence concepts, and thus provide “pseudo” labels for learn-ing scene graphs, thereby bridging the gap between region-concept pairs. Our hypothesis is that these “pseudo” labels, coupled with a large-scale dataset, can be used for training a deep model to detect scene graph of an input image. Our language supervised setting is shown in Fig. 1 (a).
Inspired by the recent success of vision-language pre-training [9, 26, 67, 31, 43, 44, 30], we develop a
Transformer-based model for learning to generate scene graphs supervised by image-sentence pairs. Speciﬁcally, our model takes inputs of visual features from a pair of detected object regions, text embeddings of their predicted categorical labels, and contextual features from other ob-ject regions, all provided by an off-the-shelf detector [36].
Our model then learns to recognize the visual relationship between the input object pair, represented as a localized subject-predicate-object (SPO) triplet. A scene graph can thus be generated by enumerating all pairs from a small set of detected objects. During training, our model learns from only image-sentence pairs using “pseudo” labels produced by matching the detected object labels to the parsed sen-tence concepts. During inference, our model generates a scene graph given an input image with its detection results.
Our model is trained on captioning datasets including
COCO Caption [6] and Conceptual Caption [40], and eval-uated on Visual Genome [23] — a widely used scene graph benchmark. Our results, summarized in Fig. 1 (b), signif-icantly outperform the state of the art [60] on weakly su-pervised SGG by a relative margin of 30%, despite that our model only requires image-sentence pairs for training while [60] is trained using human-annotated unlocalized scene graphs. With the same supervision as [60], our model achieves a relative gain of 112% in recall. Further, our model also demonstrates strong results on fully supervised
SGG. While these results are reported on closed-set setting with known target concepts during training, we also present promising results on open-set SGG where the concept vo-cabulary is crafted from image captions. Our work is among the ﬁrst methods for learning to detect scene graphs from only image-sentence pairs, and presents the ﬁrst result for open-set SGG. We believe our work provides a solid step towards structured image understanding. 2.