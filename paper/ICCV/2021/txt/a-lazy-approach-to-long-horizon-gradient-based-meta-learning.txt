Abstract
Gradient-based meta-learning ﬁrst trains task-speciﬁc models by an inner loop and then backpropagates meta-gradients through the loop to update the meta-model. To avoid high-order gradients, existing methods either take a small number of inner steps or approximate the meta-updates for the situations that the meta-model and task models lie in the same space. To enable long inner horizons for more gen-eral meta-learning problems, we instead propose an intuitive teacher-student strategy. The key idea is to employ a stu-dent network to adequately explore the search space of task-speciﬁc models, followed by a teacher’s “leap” toward the regions probed by the student. The teacher not only arrives at a high-quality model but also deﬁnes a lightweight com-putational graph for the meta-gradients. Our approach is generic; it performs well when applied to four meta-learning algorithms over three tasks: few-shot learning, long-tailed object recognition, and adversarial blackbox attack. 1.

Introduction
Humans can quickly learn the skills needed for new tasks by drawing from a fund of prior knowledge and experience.
To grant machine learners this level of intelligence, meta-learning studies how to leverage past learning experiences to more efﬁciently learn for a new task [48]. A hallmark experiment design provides a meta-learner a variety of few-shot learning tasks (meta-training) and then desires it to solve previously unseen and yet related few-shot learning tasks (meta-test). This design enforces “learning to learn” because the few-shot training examples are insufﬁcient for a learner to achieve high accuracy on any task in isolation.
Recent meta-learning methods focus on deep neural net-works. Some learn recurrent neural networks as an update rule to a model [36, 2]. Some transfer attention schemes across tasks [30, 49]. Gradient-based meta-learning gains momenta recently following the seminal work [14]. It is model-agnostic meta-learning (MAML), learning a global model initialization from which a meta-learner can quickly derive task-speciﬁc models by using a few training examples.
In its core, MAML is a bilevel optimization problem [10].
The upper level searches for the best global initialization, and the lower level optimizes individual models, which all share the common initialization, for particular tasks sampled from a task distribution. This problem is hard to solve. [14] instead propose a “greedy” algorithm, which comprises two loops. The inner loop samples tasks and updates the task-speciﬁc models by k steps using the tasks’ training examples.
The k-step updates write a differentiable computation graph.
The outer loop updates the common initialization by back-propagating meta-gradients through the computation graph.
This method is “greedy” in that the number of inner steps is often small (e.g., k = 1). The outer loop takes actions before the inner loop sufﬁciently explores its search space.
This “greedy” algorithm is due to practical constraints that backpropagating meta-gradients through the inner loop incurs high-order derivatives, big memory footprints, and the risk of vanishing or exploding gradients. For the same reason, some related work also turns to greedy strategies, such as meta-attack [13] and learning to reweigh examples [38].
To this end, two questions arise naturally. Would a less greedy gradient-based meta-learner (say, k>10 inner steps) achieve better performance? How to make it less greedy?
Some ﬁrst-order algorithms [14, 32, 15] have provided an afﬁrmative answer to the ﬁrst question above. [35] proposed a less “greedy” MAML by regularizing the inner loop. How-ever, they are highly tailored in that the meta-model and task models lie in the same space, preventing them from tackling other meta-learning problems, for example, the long-tailed classiﬁcation described later.
To answer the questions for more general meta-learning scenarios, we provide some preliminary results by introduc-ing a lookahead optimizer [55] into the inner loop. It can be viewed as a teacher-student scheme. We use a student neural network to explore the search space for a given task adequately (by a large number k of updates), and a teacher network then takes a “leap” toward the regions visited by the student. As a result, the teacher network not only ar-rives at a high-performing model but also deﬁnes a very lightweight computational graph for the outer loop. In con-trast to the traditionally “greedy” meta-learning framework used in MAML [14], meta-attack [13], learning to reweigh examples [38], etc., the teacher is “lazy”. It sends a student
to optimize for a task up to many steps and moves only once after that.
Our approach improves the gradient-based meta-learning framework rather than a single algorithm. Hence, we eval-uate it on different methods and tasks, including MAML and Reptile [32] for few-shot learning, a two-component weighting algorithm [20] for long-tailed classiﬁcation, and meta-attack [13]. Extensive results provide an afﬁrmative answer to the ﬁrst question above: long-horizon exploration in the inner loop improves a meta-learner’s performance. We expect our approach, along with the compelling experimen-tal results, can facilitate future work to address the second question above. 2.