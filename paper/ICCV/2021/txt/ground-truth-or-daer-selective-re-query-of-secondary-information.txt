Abstract
Many vision tasks use secondary information at infer-ence time—a seed—to assist a computer vision model in solving a problem. For example, an initial bounding box is needed to initialize visual object tracking. To date, all such work makes the assumption that the seed is a good one. However, in practice, from crowdsourcing to noisy au-tomated seeds, this is often not the case. We hence pro-pose the problem of seed rejection—determining whether to reject a seed based on the expected performance degrada-tion when it is provided in place of a gold-standard seed.
We provide a formal definition to this problem, and focus on two meaningful subgoals: understanding causes of er-ror and understanding the model’s response to noisy seeds conditioned on the primary input. With these goals in mind, we propose a novel training method and evaluation metrics for the seed rejection problem. We then use seeded versions of the viewpoint estimation and fine-grained classification tasks to evaluate these contributions. In these experiments, we show our method can reduce the number of seeds that need to be reviewed for a target performance by over 23% compared to strong baselines. 1.

Introduction
Many tasks in computer vision require not only a pri-mary input, such as an image or a video, but also additional information based on the primary input—a seed—to be pro-vided to the task model. This seed may be used to define the problem, such as in visual object tracking [28], video ob-ject segmentation [36], and visual question answering [1], or to provide additional information for common tasks such as fine-grained scene classification [27], visual concept pre-diction [57], or viewpoint estimation [52]. Critically, these tasks are evaluated using verified gold-standard seeds, ig-noring the noisy processes by which seeds are generated.
The performance of computer vision models with poor primary inputs has been explored in the context of nat-urally difficult [55, 69, 9, 14] and intentionally adversar-ial [58, 61, 10, 51] primary inputs, leading to a variety of
Figure 1: An example from keypoint-conditioned viewpoint estimation [52], with a heatmap of error caused by all po-tential clicks overlaid. Approaches focused on input-space accuracy [47, 49, 7, 43, 8, 40, 33] would select the red keypoint over the yellow keypoint as it closer to the gold-standard (green) keypoint, even though this results in higher error. methods designed to make models more robust [55, 69] or detect and reject difficult inputs [14]. However, no work to our knowledge has been performed on the identification and rejection of bad seeds: seeds that cause a significant increase in error on the task when used in place of the gold-standard seed. As reliability issues in crowdsourcing are well studied [24, 39, 48, 44] and automated systems that could be used to create seeds are subject to unpredictable failure modes [42, 61], not having any mechanism for de-tecting bad seeds is a critical oversight.
To emphasize the need for such a mechanism, we exam-ine Figure 1, where a human annotator is asked to click a se-mantically meaningful location on the image (e.g. rear seat) to resolve the viewpoint estimation model’s perceptual am-biguities. This example illustrates the complex, and some-times counterintuitive, interaction between the primary in-put, seed, and task model: while many seeds that are in-correct in the input space (e.g. the yellow seed) don’t de-gain an understanding of the task model’s response, and how a human’s intuition of a seed’s quality differs from its effect on the accuracy of the task model’s output. We again highlight the example shown in Figure 1, where a small Eu-clidean error in the input space (red keypoint) can cause a large increase in output error, while a much larger Euclidean error (yellow keypoint) may have little effect.
To address these challenges, we propose Dual-loss Ad-ditional Error Regression (DAER), a novel training method developed for the seed rejection problem. DAER con-siders the two challenges discussed above separately dur-ing training, and combines them during inference to pre-dict the effect of a candidate seed on the downstream task. We evaluate the performance of DAER on two tasks: keypoint-conditioned viewpoint estimation [52]—a human-in-the-loop extension of the canonical viewpoint estimation task [54, 50, 68, 35, 32]—and hierarchical scene classifi-cation [27]—a method that improves performance on fine-grained classification [56, 67, 31, 63] by integrating a coarse scene classification.
To evaluate DAER, we introduce a task-agnostic bench-mark evaluation method for seed rejection, centered around new metrics designed specifically to assess the performance of a seed rejection method: Additional Error (AE), Mean
Additional Error (MAE), and Area under the Mean Addi-tional Error curve (AMAE). Unlike existing metrics, such as selective risk [15], these metrics focus on the potential benefit of a new seed, instead of an oracle label of the target value that may be prohibitively difficult to obtain at scale.
The contributions of this paper are as follows: 1. A formalization and benchmark metrics for the seed re-jection problem, in which a model is tasked with de-termining if a candidate seed will produce significantly higher error than the corresponding (unknown at infer-ence time) gold-standard seed. 2. Dual-loss Additional Error Regression (DAER), a broadly applicable training and inference method for the task of seed rejection. 3. An evaluation of DAER on the tasks of keypoint-conditioned viewpoint estimation [52] (KCVE) and hi-erarchical scene classification [27] (HSC), which shows that DAER can reduce the the number of seeds that need to be reviewed for a given target performance by over 23% compared to the best-performing baseline. 2.