Abstract
Learning an effective representation of 3D point clouds requires a good metric to measure the discrepancy between two 3D point sets, which is non-trivial due to their irregular-ity. Most of the previous works resort to using the Chamfer discrepancy or Earth Mover’s distance, but those metrics are either ineffective in measuring the differences between point clouds or computationally expensive. In this paper, we conduct a systematic study with extensive experiments on distance metrics for 3D point clouds. From this study, we propose to use sliced Wasserstein distance and its variants for learning representations of 3D point clouds. In addition, we introduce a new algorithm to estimate sliced Wasserstein distance that guarantees that the estimated value is close enough to the true one. Experiments show that the sliced
Wasserstein distance and its variants allow the neural net-work to learn a more efficient representation compared to the Chamfer discrepancy. We demonstrate the efficiency of the sliced Wasserstein metric and its variants on several tasks in 3D computer vision including training a point cloud autoencoder, generative modeling, transfer learning, and point cloud registration. 1.

Introduction
Since the spark of the modern artificial intelligence, 3D deep learning on point clouds has become a power-ful technique for solving recognition problems such as ob-ject classification [44, 21], object detection [43], and se-mantic segmentation [41]. Generative modeling with 3D point clouds has also been studied with some promising re-sults [51, 46, 32, 22, 47, 33]. Another 3D computer vision problem that has seen the rise of deep learning approaches is point cloud matching [8, 11, 10, 17]. All of these problems share a common task — that is to learn a robust representa-tion of 3D point clouds.
One of the most important steps in learning represen-tations of 3D point clouds is to choose a metric to mea-sure the discrepancy between two point sets. There are two popular choices for such metric: the Chamfer divergence and the Earth Mover’s distance (EMD) [16]. While earlier works [16, 1] has shown that EMD performs better than
Chamfer in terms of learning representations, Chamfer is more favored [10, 52, 18, 15, 12, 19] due to its significantly lower computational cost.
In this article, we revisit the similarity metric problem in 3D point cloud deep learning. We propose to use the sliced
Wasserstein distance (SWD) [5], which is based on project-ing the points in point clouds into a line, and its variants as effective metrics to supervise 3D point cloud autoencoders.
Compared to Chamfer divergence, SWD is more suitable for point cloud reconstruction, while remaining computationally efficient (cf. Figure 1). We show that Chamfer divergence is weaker than the EMD and sliced Wasserstein distance (cf.
Lemma 1) while the EMD and sliced Wasserstein distance are equivalent. It suggests that even when two point clouds are close in Chamfer divergence, they may not be close in either the EMD or sliced Wasserstein distance. Furthermore, the sliced Wasserstein distance has a computational complex-ity in the order of N log N [5], which is comparable to that of the Chamfer divergence, while EMD has a complexity in the order of N 3 [39] where N is the number of points in 3D point clouds. Finally, under the standard point clouds settings, since the dimension of points is usually three, the projection step in sliced Wasserstein distance will only lead to small loss of information of the original point clouds. As a consequence, the sliced Wasserstein distance possesses both the computational and statistical advantages for point cloud learning over Chamfer divergence and EMD. To improve the quality of slices from SWD, we also discuss variants of sliced Wasserstein distance, including max-sliced Wasser-stein distance [13] and the proposed adaptive sliced Wasser-stein algorithm.By conducting a case study on learning a 3D point cloud auto-encoder, we provide a comprehensive benchmark on the performance of different metrics. These results align with our theoretical development. In summary, our main findings are:
• A first theoretical study about the relation between
Chamfer divergence, EMD, and sliced Wasserstein dis-tance for point cloud learning (Section 4).
Figure 1: We advocate the use of sliced Wasserstein distance for training 3D point cloud autoencoders. In this example, we try to morph a sphere into a chair by optimizing two different loss functions: Chamfer discrepancy (top, red) and sliced
Wasserstein distance (bottom, blue). The proposed sliced Wasserstein distance only takes 1000 iterations to converge, while it takes 50000 iterations for Chamfer discrepancy.
• A new algorithm named Adaptive-sliced Wasserstein to evaluate sliced Wasserstein distance that guarantees that the evaluated value is close enough to the true one (Section 4).
• An extensive evaluation of point cloud learning tasks including point cloud reconstruction, transfer learning, point cloud registration and generation based on Cham-fer discrepancy, EMD, sliced Wasserstein distance and its variants (Section 5). 2.