Abstract
Scene graph generation is an important task in com-puter vision aimed at improving the semantic understand-ing of the visual world. In this task, the model needs to de-tect objects and predict visual relationships between them.
Most of the existing models predict relationships in paral-lel assuming their independence. While there are differ-ent ways to capture these dependencies, we explore a con-ditional approach motivated by the sequence-to-sequence (Seq2Seq) formalism. Different from the previous research, our proposed model predicts visual relationships one at a time in an autoregressive manner by explicitly condi-tioning on the already predicted relationships. Drawing from translation models in NLP, we propose an encoder-decoder model built using Transformers where the en-coder captures global context and long range interactions.
The decoder then makes sequential predictions by condi-tioning on the scene graph constructed so far.
In addi-tion, we introduce a novel reinforcement learning-based training strategy tailored to Seq2Seq scene graph genera-tion. By using a self-critical policy gradient training ap-proach with Monte Carlo search we directly optimize for the (mean) recall metrics and bridge the gap between train-ing and evaluation. Experimental results on two public benchmark datasets demonstrate that our Seq2Seq learn-ing approach achieves strong empirical performance, out-performing previous state-of-the-art, while remaining efﬁ-cient in terms of training and inference time. Full code for this work is available here: https://github.com/ layer6ai-labs/SGG-Seq2Seq. 1.

Introduction
Analyzing natural images containing multiple objects and complex interactions between them is a challenging task. We consider a common formulation of this task, scene graph generation (SGG) [51], in which given an image, we
*Authors contributed equally and order is determined randomly
Figure 1: The top row shows predictions when the model is not conditioned on any (cid:104)subject, predicate, object(cid:105) triplets, and the model incorrectly predicts (cid:104)man, on, surfboard(cid:105).
In the bottom row, as neighboring relevant triplets are revealed to the model (non-shaded regions), the predictions shift to the correct predicate (cid:104)man, holding, surfboard(cid:105). need to detect and predict objects and relationships between them in the form of a scene graph [18]. SGG is important for many applications at the intersection of computer vision and language, such as VQA [61, 16, 39, 15, 10, 26], image captioning [56, 14], retrieval [18, 2, 43] and others [1, 53].
Humans successfully understand images by alternating between two primary steps: sequentially attending to dif-ferent regions of the image and applying high-level reason-ing about these regions [47]. The two steps are recurrent until the image is understood, so the overall process is in-herently both sequential and conditional. The beneﬁt of sequential conditioning also translates to machine learning models for the SGG task as demonstrated by the example in Figure 1. Given an image of a beach scene, we aim to predict the correct relationship (predicate) between “man” and “surfboard”. At the start, when the model is not shown any other relationships (grayed out portion in the top im-age) and hence there is no conditioning, the model predicts
“on” with high likelihood, as this is one of the most com-mon predicates occurring with “man” and “surfboard” in
In the next step we reveal neighboring rela-the dataset. tionships to the model and condition on them (non-shaded regions in the bottom image). After seeing relationships (cid:104)man, wear, shoe(cid:105) and (cid:104)man, on, beach(cid:105), the model infers that since the man is wearing shoes and there is an adja-cent man on the beach, the more appropriate relationship is
“holding”. Consequently the probabilities of “on” and “rid-ing” drop. This example illustrates how sequential condi-tioning can help resolve ambiguities and reduce bias learned from the training data.
The majority of SGG methods, except for a few notable exceptions such as Neural Motifs (NM) [60], predict the
ﬁnal relationship labels in parallel making a severely lim-In iting assumption of independence among the triplets.
NM, global context from detected objects is encoded via an LSTM, but the ﬁnal relationship prediction is still done independently for each pair of objects and no conditioning is applied. In contrast, we quantitatively analyze the impor-tance of incorporating both the sequential and conditional properties (§ 2). Based on that analysis and inspired by the Transformer architecture showing strong results both in neural machine translation [45] and computer vision [11, 4], we propose a Seq2Seq model that exploits sequential condi-tioning. We design a conditional Transformer decoder that sequentially leverages already predicted relationships to ad-just its beliefs about future predictions.
Another important limitation of the previous SGG works is related to the way the models are trained and evaluated.
In particular, common evaluation metrics, recall [51] and mean recall (mRecall) [6, 44], are not directly tied to the training objective of the SGG models, which typically min-imize the cross-entropy loss [60]. The problem is exacer-bated by the fact that these metrics focus on different and often conﬂicting properties, so training a single model that maximizes both metrics is challenging [43, 22]. For ex-ample, recall is dominated by frequent relationships [60], while mRecall assigns an equal weight to both frequent and rare relationships [6, 64]. A common method to improve the model on the target metric is to introduce an induc-tive bias favouring the metric via a carefully designed loss function [23, 29, 41] or features [60, 64]. To improve the model on the target metric, we take a different approach and leverage a reinforcement learning (RL)-based training strat-egy that enables the direct optimization of the target metric, bridging the gap between training and evaluation. The RL approach also aligns well with our Seq2Seq model as we train our RL policy to make sequential relationship predic-tions in an optimal order w.r.t. to the target metric (reward).
In summary, this paper makes the following contribu-tions:
• Inspired by neural machine translation and our con-ditional SGG analysis (§ 2), we propose an encoder-decoder model based on Transformers with a sequential autoregressive decoder (§ 4).
• We introduce an RL training strategy that enables the direct optimization of the target metrics, bridging the gap between training and evaluation (§ 4.4.2). by re-In particular, we employ a moving the exposure bias.
Monte Carlo search self-critical policy gradient training approach to accurately estimate the action-value function for our model (§ 4.4.2).
• We obtain state-of-the-art results on both recall and mRe-call metrics while maintaining computational efﬁciency during training and inference (§ 5). 2. Conditional Scene Graph Generation
A scene graph [18] is deﬁned as a set of objects and the relationships between them. We deﬁne a categorical triplet using a subject, object and their relationship. For instance ym = (cid:104)man, on, surfboard(cid:105), is the mth triplet in the image.
The scene graph can be viewed as a set of M such triplets
{ym}M 1 . We further assume some canonical order of triplets (e.g. from the left of the image to the right [60]) and deﬁne an ordered triplet set Y1:M = {y1, ..., yM }. Applying the chain rule, we deﬁne the conditional SGG as a task of se-quentially inferring the relationship triplets conditioned on all previously predicted triplets:
M (cid:89) m−1 (cid:92) p(Y1:M ) = p(ym| yj) (1) m=1 j=1
= p(yM |Y1:M −1)p(yM −1|Y1:M −2)...p(y2|y1)p(y1).
In the above formulation we ignore visual features as-suming that all predictions are conditioned on the image in the way speciﬁc to a particular method (see § 4). The ma-jority of SGG methods assume conditional independence of triplets and predict all triplets in parallel. To demonstrate that this assumption is limiting we analyse the relationship co-occurrence in the Visual Genome (VG) dataset [25]. We follow a setup similar to [60], and ﬁrst compute the co-occurrence likelihoods between pairs of relationship triplets p(y2|y1) using the training set of VG. We observe a strong co-occurrence bias, with most p(y2|y1) distributions being highly peaked (Figure 2, top). For example, for (cid:104)man, on, beach(cid:105) there are only a few triplets such as (cid:104)man, wear-ing, shorts(cid:105) and (cid:104)man, holding, surfboard(cid:105) that co-occur frequently in the dataset (Figure 2, top left). By extending this example to three triplets p(y3|y1, y2), the distribution remains steep but the top co-occurring triplets change. For example, by conditioning on (cid:104)man, on, beach(cid:105) and (cid:104)horse, on, beach(cid:105) the top triplet changes to (cid:104)person, riding, horse(cid:105) clearly demonstrating the effect of knowing that both “man” and “horse” are on the beach (Figure 2, top right).
We can expand the sequence of conditioning triplets to arbitrary size m. To avoid the prohibitive cost of com-puting joint probabilities we use a simple approximation:
tangle object detection error from relationship detection and focus on reasoning over the relationships. Recent re-search [64, 6, 44, 43, 29, 54, 8] address the long-tail is-sue by improving mean recall as opposed to simple recall dominated by most frequent relationships. Several recent works [43, 41, 23, 22, 30, 20] focus on compositional gen-eralization metrics in SGG, which is an interesting avenue to apply our method in the future.
Contextual Models. Context has been shown to be useful in generating better predictions in several recent works [17, 7, 48]. Our work is closest to Neural Motifs (NM) [60]. In NM, the Bi-LSTMs model is used to cap-ture the global context and structural regularities in scene graphs. NM constructs the global context from all detected objects in a given image. NM then leverages the global con-text to reﬁne feature-level representations for individual ob-jects and possible relationships between them. In contrast, our approach ﬁrst encodes global context and then sequen-tially updates it by leveraging information from triplets that have been decoded so far. This is achieved by applying a
Transformer architecture that enables joint conditioning on all predicted history which we demonstrate to be important for maximizing SGG performance.
Attention. Prior work [55, 35] that applies attention in visual relationship detection start by deﬁning a near-est neighbor graph. Attention is used to capture infor-mation about the graph structure by encoding it similar to graph attention networks (GAT) [46].
In particular,
Graph R-CNN[55] applies GAT over visual similarity while graph self-attention [35] additionally embeds a pair of ob-ject features and linguistic relationships jointly. Transform-ers [45] have been successfully adopted in computer vi-sion [36, 11, 12, 13]. We use Transformers in an encoder decoder based architecture. However, unlike other atten-tion based methods in this domain, our decoder makes se-quential predictions conditioned on previous outputs and the model is trained in an auto regressive way. In a parallel work [24] Relational Transformers were applied to visual relationship detection. However, our model is explicitly conditioned on triplet predictions and uses Reinforcement
Learning (RL) to optimize for the speciﬁc metric.
Reinforcement Learning. Using RL for SGG has re-mained under explored. [28] built a semantic action graph using language priors and formulated SGG as a single agent decision-making process. CMAT [5] proposed a counterfactual critic model using multi agent policy. DG-PGNN [19] proposed a probabilistic model together with
Q-learning to infer a scene graph in a sequential node-by-node fashion. In contrast to [28, 5, 19], our model works on subject-predicate-object triplets and leverages Transformers to capture global context. In addition, our work explores the use of mean recall as a reward to tackle the long-tail distri-bution on SGG datasets.
Figure 2: VG analysis based on scene graph annotations with-(top) Triplet co-out considering images or bounding boxes. occurrence in scene graphs has a peaked distribution. The top candidates can be selected reasonably well when conditioned on a single triplet (left) and two triplets (right), including rare relation-ships (in red). (bottom) Conditioning allows us to accurately pre-dict a triplet given other (already predicted or ground truth) triplets in the image. This motivates our conditional inference pipeline. p(ym+1|Y1:m) = (cid:80)m i p(ym+1|yi). For small m we veri-ﬁed that this approximation closely matches the target dis-tribution, particularly when it is used to rank the top co-occurring relationships. We then simulate a sequential pre-diction process and use these probabilities to predict triplets in a given test image while conditioning on increasingly larger set of ground truth triplets (i.e. assuming a perfect model): p(ˆym+1|Y1:m), where m is the history size. At each step we sample one ground truth triplet per test im-age and check if it is in the top-100 triplets co-occurring with triplets predicted from Y1:m, we then average this ac-curacy across all test images. To imitate the effect of the re-cently introduced mRecall metric [6, 44], we also compute the per-predicate accuracy for all images and then average over all predicate classes. We found that conditioning on more triplets (longer history m) substantially improves both accuracies (Figure 2, bottom). Our ﬁnding indicates that the model is able to make an increasingly better prediction by leveraging the information from the revealed relationships.
This motivates our Seq2Seq model described in the follow-ing sections. 3.