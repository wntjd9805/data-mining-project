Abstract
High dynamic range (HDR) photography is becoming increasingly popular and available by DSLR and mobile-phone cameras. While deep neural networks (DNN) have greatly impacted other domains of image manipulation, their use for HDR tone-mapping is limited due to the lack of a definite notion of ground-truth solution, which is needed for producing training data.
In this paper we describe a new tone-mapping approach guided by the distinct goal of producing low dynamic range (LDR) renditions that best reproduce the visual character-istics of native LDR images. This goal enables the use of an unpaired adversarial training based on unrelated sets of
HDR and LDR images, both of which are widely available and easy to acquire.
In order to achieve an effective training under this mini-mal requirements, we introduce the following new steps and components: (i) a range-normalizing pre-process which es-timates and applies a different level of curve-based com-pression, (ii) a loss that preserves the input content while allowing the network to achieve its goal, and (iii) the use of a more concise discriminator network, designed to promote the reproduction of low-level attributes native LDR possess.
Evaluation of the resulting network demonstrates its ability to produce photo-realistic artifact-free tone-mapped images, and state-of-the-art performance on different image fidelity indices and visual distances. 1.

Introduction
High dynamic range (HDR) photography gained a con-siderable popularity in the last decades among both profes-sional and non-professional photographers. HDR capabili-ties are available in many DSLR cameras as well as in main-stream mobile smartphones, capable of providing 10- and 12-bits of color depth. Printing or displaying these images on conventional low dynamic range (LDR) devices require a tone-mapping step for reducing their dynamic range. The latter revealed itself as a non-trivial task and drew a consid-Figure 1. Tone-mapped HDR images produced by our method.
The four exposures on the left of each image portray the very high dynamic range in the original scenes. erable research effort.
Unlike many image processing and restoration tasks, there is no ground-truth “solution” for tone-mapping HDR images, and the different methods developed over the years aim for different goals. Early approaches use global tone-reproduction curves (TRC) that make a better use of the out-put dynamic range than linear bracketing [5, 24, 47]. These curves avoid over- and under-exposed pixels, but their con-tractive nature leads to a severe reduction in local contrasts.
Consequently, more modern approaches focus their goal on preserving, or enhancing, the local contrasts using detail separation and enhancement techniques [6, 8, 9, 33, 35, 39, 48]. These methods produce highly-detailed images, but applying high levels of compression remains a challenge in terms of avoiding edge-related artifacts or achieving bal-anced levels of contrast and an overall photo-realistic ap-pearance.
Deep neural networks (DNNs) have greatly impacted various image processing tasks, such as super resolu-tion [20], and deblurring [43], by using large training sets containing ground-truth examples. In the absence of ground-truth data, the prevalent approach in HDR tone-mapping is using existing tone-mapping operators (TMOs) to produce a panel of image labels, and narrowing it down using an image quality index to obtain the final training ex-amples [34, 38].
The trained network is expected to reproduce the best available result on each image, but not surpass the perfor-mance of its underlining TMO algorithms. Moreover, the quality indices used reward for fulfilling a small number of regularities and hence they bias the training towards overfit-ting these attributes. Some approaches incorporate manual supervision, but they still rely on the indices in their final selection [55], or make a subjective decision by picking a single annotator [30].
In this paper we describe a new DNN-based TMO which is trained to produce images that bear the visual character-istics found in native LDR images. By formulating this dis-tinct goal as an adversarial training, we replace the need for obtaining paired training examples with the plenitude of available high-quality LDR images.
Unsupervised adversarial training is a rather delicate process, prone to various instabilities. In order to achieve an effective and successful training, our method uses the following several new steps and components: (i) Before feeding the input HDR image to the network, we map its luminance through a range compression curve that reduces its variance and fixes its range. In order to train and apply our method on images from arbitrary sources, we use an adaptive compression level which we estimate on the basis of each input image.
Moreover, (ii) we incorporate a structure-preservation loss that penalizes for changes beyond local adjustments in brightness and contrast. This term ensures the input image content is preserved and no mode-collapsing occurs.
Finally, (iii) we describe the use of an ensemble of relatively shallow discriminator networks in order to bet-ter match the low-level attributes of native LDR images, and suppress the edge-related artifacts that plague existing
TMOs. We demonstrate our method’s ability to efficiently produce naturally-looking and artifact-free LDR renditions of highly challenging HDR scenes. A quantitative evalua-tion over benchmark HDR images reveals its superior per-formance over established image quality metrics and visual distances. 2.