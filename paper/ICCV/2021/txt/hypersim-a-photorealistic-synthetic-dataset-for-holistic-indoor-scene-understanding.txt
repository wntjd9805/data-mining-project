Abstract
For many fundamental scene understanding tasks, it is difﬁcult or impossible to obtain per-pixel ground truth la-bels from real images. We address this challenge by in-troducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corre-sponding ground truth geometry. Our dataset: (1) relies ex-clusively on publicly available 3D assets; (2) includes com-plete scene geometry, material information, and lighting in-formation for every scene; (3) includes dense per-pixel se-mantic instance segmentations and complete camera infor-mation for every image; and (4) factors every image into diffuse reﬂectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.
We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, com-putation time, and annotation effort. Remarkably, we
ﬁnd that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also eval-uate sim-to-real transfer performance on two real-world scene understanding tasks – semantic segmentation and 3D shape prediction – where we ﬁnd that pre-training on our dataset signiﬁcantly improves performance on both tasks, and achieves state-of-the-art performance on the most chal-lenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.
Figure 1. Overview of the Hypersim dataset. For each color image (a), Hypersim includes the following ground truth layers: depth (b); surface normals (c); instance-level semantic segmen-tations (d,e); diffuse reﬂectance (f); diffuse illumination (g); and a non-diffuse residual image that captures view-dependent light-ing effects like glossy surfaces and specular highlights (h). Our diffuse reﬂectance, diffuse illumination, and non-diffuse residual layers are stored as HDR images, and can be composited together to exactly reconstruct the color image. 1.

Introduction
For many fundamental scene understanding tasks, it is difﬁcult or impossible to obtain per-pixel ground truth la-bels from real images.
In response to this challenge, the computer vision community has developed several photore-alistic synthetic datasets and interactive simulation environ-ments that have spurred rapid progress towards the goal of holistic indoor scene understanding [6, 7, 9, 10, 14, 15, 19, 22, 24, 31, 33, 36, 37, 39, 43, 44, 45, 46, 49, 52, 61, 62, 63, 65, 70, 72, 75, 76, 79, 83, 84].
Dataset/simulator
Images 3D
Seg.
Intrinsic
Real (3D reconstruction)
SceneNN [33] (cid:88)
Stanford 2D-3D-S [9, 10, 79]
Matterport3D [7, 15, 61, 62, 79] (cid:88) (cid:88)
ScanNet [19]
Gibson [62, 79]
Replica [62, 72]
Synthetic (artist-created)
AI2-THOR [39]
ARAP [14]
SceneNet-RGBD [52]
PBRS [70, 83]
CGIntrinsics [45, 70]
InteriorNet [43]
Jiang et al. [36]
RobotriX [24]
CG-PBR [63, 70]
DeepFurniture [49]
Structured3D [84]
TartanAir [65, 76]
Li et al. [44, 70] 3D-FUTURE [22]
OpenRooms [46]
Hypersim (ours) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
S+I
S+I
S+I
S+I
S+I
S+I
I
S+I
S+I
S+I
S+I
S+I
S
S
S+I
I
S+I
S+I
D+R
D
D
D+R
D
D
D+R
D+R
D+R
Table 1. Comparison to previous datasets and simulators for in-door scene understanding. We broadly categorize these datasets and simulators as being either real (i.e., based on 3D triangle mesh reconstructions from real sensors) or synthetic (i.e., artist-created), and we sort chronologically within each category. We limit our comparisons to synthetic datasets and simulators that aim to be photorealistic. The Images and 3D columns indicate whether or not images and 3D assets (e.g., triangle meshes) are publicly avail-able. The Seg. column indicates what type of segmentation in-formation is available: S indicates semantic; I indicates instance.
The Intrinsic column indicates how images are factored into dis-entangled lighting and shading components: D indicates that each image is factored into diffuse reﬂectance and diffuse illumination;
D+R indicates that each factorization additionally includes a non-diffuse residual term that captures view-dependent lighting effects.
Our dataset is the ﬁrst to include images, 3D assets, semantic in-stance segmentations, and a disentangled image representation.
However, existing synthetic datasets and simulators have important limitations (see Table 1). First, most synthetic datasets are derived from 3D assets that are not publicly available. These datasets typically include rendered images, but do not include the underlying 3D assets used during ren-dering (e.g., triangle meshes), and are therefore not suit-able for geometric learning problems that require direct 3D supervision (e.g., [27]). Second, not all synthetic datasets and simulators include semantic segmentations. Although it is common for synthetic datasets to include some kind of segmentation information, these segmentations may not include semantic labels, and may group pixels together at the granularity of low-level object parts, rather than se-mantically meaningful objects. Third, most datasets and simulators do not factor images into disentangled lighting and shading components, and are therefore not suitable for inverse rendering problems (e.g., [13, 40]). No existing synthetic dataset or simulator addresses all of these limita-tions, including those that target outdoor scene understand-ing [8, 20, 23, 34, 38, 41, 55, 56, 58, 60, 65, 76, 77].
In this work, we introduce Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding that addresses all of the limitations described above (see
Figure 1). To create our dataset, we leverage a large reposi-tory of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with de-tailed per-pixel labels and corresponding ground truth ge-ometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geome-try, material information, and lighting information for ev-ery scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reﬂectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects. Together, these features make our dataset well-suited for geometric learn-ing problems that require direct 3D supervision (e.g., [27]), multi-task learning problems that require reasoning jointly over multiple input and output modalities (e.g., [71]), and inverse rendering problems (e.g., [13, 40]).
To generate our dataset, we introduce a novel compu-tational pipeline that takes as input a collection of scenes downloaded from an online marketplace, and produces as output a collection of images with ground truth labels and corresponding geometry (see Figure 2). Our pipeline has three main steps. First, we generate camera views of each input scene using a novel view sampling heuristic that does not require the scene to be semantically labeled. Second, we generate images using a cloud rendering system that we built on top of publicly available cloud computing services.
Third, we obtain semantic segmentations from a human an-notator using an interactive mesh annotation tool we built ourselves.
We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computa-tion time, and annotation effort. Remarkably, we ﬁnd that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natu-ral language processing model. We also evaluate sim-to-real transfer performance on two scene understanding tasks – se-mantic segmentation on NYUv2 [68] and 3D shape predic-tion on Pix3D [73] – where we ﬁnd that pre-training on our dataset signiﬁcantly improves performance on both tasks, and achieves state-of-the-art performance on the most chal-lenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and per-form our experiments, is available online.1 1http://github.com/apple/ml-hypersim
Figure 2. Overview of our computational pipeline. In this simpliﬁed diagram, our pipeline takes as input a triangle mesh, an artist-deﬁned camera pose, and a V-Ray scene description ﬁle, and produces as output a collection of images with ground truth labels and corresponding geometry. The main steps of our pipeline are as follows. We estimate the free space in our scene, use this estimate to generate a collision-free camera trajectory, modify our V-Ray scene to include the trajectory, and invoke our cloud rendering system to render images. In parallel with the rest of our pipeline, we annotate the scene’s triangle mesh using our interactive tool.
In a post-processing step, we propagate mesh annotations to our rendered images (not shown). This pipeline design enables us to render images before mesh annotation is complete, and also enables us to re-annotate our scenes (e.g., with a different set of labels) without needing to re-render images. 2.