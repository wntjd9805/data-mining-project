Abstract
Input Point Cloud
Decoder Attention
Detections
We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3D-speciﬁc inductive biases, 3DETR requires minimal mod-iﬁcations to the vanilla Transformer block. Speciﬁcally, we ﬁnd that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3D-speciﬁc operators with hand-tuned hyperparameters. Nev-ertheless, 3DETR is conceptually simple and easy to im-plement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applica-ble to 3D tasks beyond detection, and can serve as a build-ing block for future research. 1.

Introduction 3D object detection aims to identify and localize ob-jects in 3D scenes. Such scenes, often represented us-ing point clouds, contain an unordered, sparse and irregu-lar set of points captured using a depth scanner. This set-like nature makes point clouds signiﬁcantly different from the traditional grid-like vision data like images and videos.
While there are other 3D representations such as multiple-views [60], voxels [1] or meshes [8], they require additional post-processing to be constructed, and often loose informa-tion due to quantization. Hence, point clouds have emerged as a popular 3D representation, and spurred the develop-ment of specialized 3D architectures.
Many recent 3D detection models directly work on the 3D points to produce the bounding boxes. Of particular in-terest, VoteNet [42] casts 3D detection as a set-to-set prob-lem, i.e., transforming an unordered set of inputs (point cloud), into an unordered set of outputs (bounding boxes).
VoteNet uses an encoder-decoder architecture: the encoder is a PointNet++ network [44] which converts the unordered
Figure 1: 3DETR. We train an end-to-end Transformer model for 3D object detection on point clouds. Our model has a Transformer encoder for feature encoding and a Transformer decoder for pre-dicting boxes. For an unseen input, we compute the self-attention from the reference point (blue dot) to all points in the scene and display the points with the highest attention values in red. The de-coder attention groups points within an instance which presumably makes it easier to predict bounding boxes. point set into a unordered set of point features. The point features are then input to a decoder that produces the 3D bounding boxes. While effective, such architectures have required years of careful development by hand-encoding inductive biases, radii, and designing special 3D operators and loss functions.
In parallel to 3D, set-to-set encoder-decoder models have emerged as a competitive way to model 2D object detec-tion. In particular, the recent Transformer [68] based model, called DETR [4], casts 2D object detection as a set-to-set problem. The self-attention operation in Transformers is de-signed to be permutation-invariant and capture long range contexts, making them a natural candidate for processing unordered 3D point cloud data. Inspired by this observation, we ask the following question: can we leverage Transform-ers to learn a 3D object detector without relying on hand-designed inductive biases?
To that end, we develop 3D DEtection TRansformer (3DETR) a simple to implement 3D detection method that uses fewer hand-coded design decisions and also casts de-tection as a set-to-set problem. We explore the similarities between VoteNet and DETR, as well as between the core mechanisms of PointNet++ and the self-attention of Trans-formers to build our end-to-end Transformer-based detec-tion model. Our model follows the general encoder-decoder structure that is common to both DETR and VoteNet. For the encoder, we replace the PointNet++ by a standard Trans-former applied directly on the point clouds. For the decoder, we consider the parallel decoding strategy from DETR with
Transformer layers making two important changes to adapt it to 3D detection, namely non-parametric query embed-dings and Fourier positional embeddings [64]. 3DETR removes many of the hard coded design deci-sions in VoteNet and PointNet++ while being simple to im-plement and understand. Unlike DETR, 3DETR does not employ a ConvNet backbone, and solely relies on Trans-formers trained from scratch. Our transformer-based de-tection pipeline is ﬂexible, and as in VoteNet, any compo-nent can be replaced by other existing modules. Finally, we show that 3D speciﬁc inductive biases can be easily incor-porated in 3DETR to further improve its performance. On two standard indoor 3D detection benchmarks, ScanNetV2 and SUN RGB-D we achieve 65.0% AP and 59.0% AP re-spectively, outperforming an improved VoteNet baseline by 9.5% AP50 on ScanNetV2. 2.