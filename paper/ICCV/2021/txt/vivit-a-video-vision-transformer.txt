Abstract
We present pure-transformer based models for video classiﬁcation, drawing upon the recent success of such mod-els in image classiﬁcation. Our model extracts spatio-temporal tokens from the input video, which are then en-coded by a series of transformer layers. In order to han-dle the long sequences of tokens encountered in video, we propose several, efﬁcient variants of our model which fac-torise the spatial- and temporal-dimensions of the input. Al-though transformer-based models are known to only be ef-fective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough abla-tion studies, and achieve state-of-the-art results on multiple video classiﬁcation benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. 1.

Introduction
Approaches based on deep convolutional neural net-works have advanced the state-of-the-art across many stan-dard datasets for vision problems since AlexNet [37]. At the same time, the most prominent architecture of choice in in natural language sequence-to-sequence modelling (e.g. processing) is the transformer [67], which does not use con-volutions, but is based on multi-headed self-attention. This operation is particularly effective at modelling long-range dependencies and allows the model to attend over all ele-ments in the input sequence. This is in stark contrast to convolutions where the corresponding “receptive ﬁeld” is limited, and grows linearly with the depth of the network.
The success of attention-based models in NLP has re-cently inspired approaches in computer vision to integrate transformers into CNNs [74, 7], as well as some attempts to replace convolutions completely [48, 3, 52]. However, it is only very recently with the Vision Transformer (ViT) [17],
∗Equal contribution
†Equal advising that a pure-transformer based architecture has outperformed its convolutional counterparts in image classiﬁcation. Doso-vitskiy et al. [17] closely followed the original transformer architecture of [67], and noticed that its main beneﬁts were observed at large scale – as transformers lack some of the inductive biases of convolutions (such as transla-tional equivariance), they seem to require more data [17] or stronger regularisation [63].
Inspired by ViT, and the fact that attention-based ar-chitectures are an intuitive choice for modelling long-range contextual relationships in video, we develop sev-eral transformer-based models for video classiﬁcation. Cur-rently, the most performant models are based on deep 3D convolutional architectures [8, 19, 20] which were a natu-ral extension of image classiﬁcation CNNs [26, 59]. Re-cently, these models were augmented by incorporating self-attention into their later layers to better capture long-range dependencies [74, 22, 78, 1].
As shown in Fig. 1, we propose pure-transformer mod-els for video classiﬁcation. The main operation performed in this architecture is self-attention, and it is computed on a sequence of spatio-temporal tokens that we extract from the input video. To effectively process the large number of spatio-temporal tokens that may be encountered in video, we present several methods of factorising our model along spatial and temporal dimensions to increase efﬁciency and scalability. Furthermore, to train our model effectively on smaller datasets, we show how to reguliarise our model dur-ing training and leverage pretrained image models.
We also note that convolutional models have been de-veloped by the community for several years, and there are thus many “best practices” associated with such models.
As pure-transformer models present different characteris-tics, we need to determine the best design choices for such architectures. We conduct a thorough ablation analysis of tokenisation strategies, model architecture and regularisa-tion methods. Informed by this analysis, we achieve state-of-the-art results on multiple standard video classiﬁcation benchmarks, including Kinetics 400 and 600 [34], Epic
Kitchens 100 [13], Something-Something v2 [25] and Mo-ments in Time [44].
Figure 1: We propose a pure-transformer architecture for video classiﬁcation, inspired by the recent success of such models for images [17].
To effectively process a large number of spatio-temporal tokens, we develop several model variants which factorise different components of the transformer encoder over the spatial- and temporal-dimensions. As shown on the right, these factorisations correspond to different attention patterns over space and time. 2.