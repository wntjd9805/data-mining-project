Abstract
Achieving backward compatibility when rolling out new models can highly reduce costs or even bypass feature re-encoding of existing gallery images for in-production vi-sual retrieval systems. Previous related works usually lever-age losses used in knowledge distillation which can cause performance degradations or not guarantee compatibility.
To address these issues, we propose a general framework called Learning Compatible Embeddings (LCE) which is applicable for both cross model compatibility and com-patible training in direct/forward/backward manners. Our compatibility is achieved by aligning class centers between models directly or via a transformation, and restricting more compact intra-class distributions for the new model.
Experiments are conducted in extensive scenarios such as changes of training dataset, loss functions, network archi-tectures as well as feature dimensions, and demonstrate that
LCE efficiently enables model compatibility with marginal sacrifices of accuracies. The code will be available at https://github.com/IrvingMeng/LCE. 1.

Introduction
Visual search or retrieval systems [36, 37] are widely used in many real-world applications such as face recog-nition [42, 39, 33, 26, 27], person re-identification [40, 16, 34], car re-identification [19] and image retrieval [2, 12]. To obtain steady improvement, models would be occasionally upgraded by training on larger or cleaner datasets, adopting more powerful network structures and training losses, or ap-plying techniques like network architecture search [56, 50], knowledge distillation [17] and network pruning [23, 11].
However, to harvest the benefits of new models, a pro-cess known as “backfilling” or “re-indexing” [38] is in-dispensable to re-encode all images in the gallery set to recreate clusters. This process could be impractical to ex-ecute when there are limited computational resources for re-encoding, or original images are legally forbidden to be preserved without user authorization. Model compatibility techniques, which can heavily reduce costs of or even by-1 and T 1
Figure 1: An overview of model compatibility problems. (a)
Model compatibility matches model 1 and model 2 through three optional directions. Features f 1, f 2 from two models can make a direct comparison, noted as direct compatible method. With transformations T 2 2 involved, we achieve either backward compatible by comparing f 2→1 with f 1, or forward compatible if compare f 1→2 and f 2. (b) Our compatibility is achieved by aligning the class centers across models through a direct compar-ison (by setting T s to identity mappings) or via transformations, and restricting more compact intra-class distributions for the new model. Best viewed in color. pass the process, therefore are of great practical values.
Because of the multiplicity of application scenarios in visual search/retrieval fields, as well as compatibility direc-tions to implement, model compatibility has never been em-ployed via a general, unified set of patterns. Viewing in the perspective of application scenarios, there are two types of model compatibility methods. The first type is called cross model compatibility (CMC) whose goal is to find compat-ible mappings between the previous and upgraded models, where the upgraded models already exist. As model per-formance may degrade if compatibility is considered, CMC serves well for the scenarios that model performances are more valuable than compatibility. The second type is com-patible training (CT) which aims at upgrading models with compatibility constraints. When bringing up against the scenarios where frequent iterations for online models are re-quired, CT possesses an inherent feasibility to take charge.
The perspective of compatible directions categorizes model compatibility into three types: backward, forward and direct compatible methods (Fig. 1a). Direct compati-ble methods compare new features with old gallery sets di-rectly, which is the most efficient as it completely prevents
“backfilling” processes. Backward compatible methods en-dow a backward transformation that maps new features into old feature spaces, and they are capable of waiving re-encoding large gallery sets. Lightweight improvement for cumbersome model architectures is another potential sce-nario for backward compatible methods since old models spawn more separable feature spaces than new models in the circumstances. Forward compatible methods utilize a forward transformation that maps old features into new feature spaces, which is aimed at upgrading small gallery sets, especially when new models have much better perfor-mances than old ones.
Despite its great practical values, model compatibil-ity is a relatively unexplored research area. Few efforts,
R3AN [4], RBT [44] (for CMC) and BCT [38] (for CT), are proposed and reach comparable results. However, these three works are only appropriate for limited application sce-narios or compatible directions. Moreover, their model per-formances and compatibility are still arduous to be guaran-teed, not only because of the further intensified nature of incompatibility for two different feature spaces when up-graded models dramatically changing training factors (e.g., backbones, losses, training datasets, or even settings of op-timizers), but also as a result of the point-wise losses they utilize, which align each feature pair and therefore prevent learning of more discriminative features.
To address the above issues, we propose a general frame-work called LCE for model compatibility as shown in
Fig. 1. Model 1 represents the target compatible model.
Model 2 is pre-existed for CMC while trained for CT. Trans-formations T 1, T 2 map features to another spaces and en-able direct/backward/forward compatibility. Our compati-bility is achieved by aligning feature classes across models and restricting the mapped features to be distributed within the original boundaries (i.e., more compact intra-class dis-tributions). By this means, the mapped feature f 2→1 is dis-tributed inside the correct class in the original feature space and that makes features comparable.
Besides the compatibility, our framework also benefits the learning of more discriminative features in the follow-ing aspects: (a) Our method works in a point-to-set manner instead of employing point-wise constraints. As shown in
Fig. 1b, features from same instance are not restricted to be distributed closely. (b) By the restriction of the original boundary, intra-class distributions are more compact com-pared to those from the old model. (c) The introduced trans-formation T relaxes the requirement for consistent inter-class distributions, as inter-class distributions may vary a lot across models (e.g., ResNet100 [15] can separate fea-tures more easily than MobileFace [5]). Overall, we make the following contributions:
• Compared to previous methods which work in a point-wise manner, we reformulate the model compatibility from the aspect of classes and highly decouple mod-els. Specifically, we align feature classes across mod-els and restrict the new-to-old mapped features to be distributed within the original boundaries. With the proposed point-to-set constraints and transformations, we achieve model compatibility with marginal sacri-fices of accuracies.
• We unify the two model compatibility problems (e.g.,
CMC and CT) into a unified training framework called
LCE, which can work in direct/backward/forward manners. Extensive experiments and ablation study are conducted on both problems with various factors such as network structures and losses. The proposed method achieves noteworthy results compared to cur-rent state-of-the-arts. 2.