Abstract
In this paper, we propose an end-to-end learning frame-work for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency.
To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consis-tency are exploited to enable self-supervision on the deblur-ring network with real-world data. Furthermore, a piece-wise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algo-rithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for event-based motion deblurring in real-world scenarios. 1.

Introduction
Due to motion ambiguities as well as the erasure of inten-sity textures [12], the task of motion deblurring is severely ill-posed [11, 25]. With the help of an event camera which can “continuously” emit events asynchronously with ex-tremely low latency (in the order of µs) [15, 6], inherently embedding motions and textures [2], the task of motion de-blurring [24, 22, 28] can be essentially alleviated. Many event-based motion deblurring methods have been proposed by learning from synthesized dataset composed of simu-lated events and blurry images as well as sequences of sharp clear ground-truth images [10, 31]. However, the inconsis-tency between synthetic and real data degrades the perfor-mance of inference on real-world event cameras [29].
The physical intrinsic noise of event cameras raises the
*Corresponding author
The research was partially supported by the National Natural Science
Foundation of China, No. 61871297 and the Fundamental Research Funds for the Central University of China, WHU No. 2042020kf0019 and DUT
No. 82232026. And the numerical calculations have been done on the su-percomputing system in the Supercomputing Center of Wuhan University.
Figure 1. Illustrative example of inconsistency between synthe-sized and real-world motion blurs with respect to the event time-surface [14]: (a) a real-world motion blurred image; (b) the time-surface of real-world events corresponding to (a); (c) the time-surface of events collected with the same trajectory as (a) but at a slow motion speed; (d) deblurred results respectively by eSL-Net [31] (top row), LEDVDI [16] (middle row) and our proposed
RED-Net (bottom row) that trained with real-world events and real-world motion blurs, sequence predictions of the blue area are shown on the right. Our proposed RED-Net generates less halo artifacts and achieves the best visualization performance. difficulty of simulating labeled events that highly match the real event data [26]. Even though the event simulator to some extent reduces the gap by considering the pixel-to-pixel variation in the event threshold [26], additional noise effects such as background activity noise and false nega-tives [1] still exist, leading to tremendous discrepancy be-tween the virtual events synthesized from event simulators
[26] and the real events emitted by event cameras. An al-ternative approach is to build a labeled dataset composed of real-world events accompanying with synthesized blurry images, and then train networks on it [10]. Unfortunately, obtaining such pairs is not always easy, which needs to be captured with a slow motion speed as well as under good lighting conditions to avoid motion blur. Subsequent blurry image synthesis and alignment on temporal domain is also tedious but indispensable. Furthermore, inconsistency still exists between the events associated with synthesized and real-world motion blur in that limited read-out bandwidth leads to more event timing variations, as shown in Fig. 1.
In this paper, we propose a novel framework of learn-ing the event-based motion deblurring network in a self-supervised manner, where real-world events and real-world motion blurred images are exploited to alleviate the perfor-mance degradation caused by data inconsistency and bridge the gap between simulations and real-world scenario. The proposed framework consists of two neural networks, an event-based motion deblurring network (Deblur-Net) and an event-based optical flow estimation network (OF-Net).
The Deblur-Net is fed with both events and a single mo-tion blurred image, and outputs a sequence of sharp clear images, while the OF-Net receives events and provides mo-tions between the reconstructed sharp clear images. We re-late motions and sharp clear images according to the pho-tometric constancy [36]. The real-world motion blurs with non-linearities are considered by a piece-wise linear motion (PLM) model which improves the accuracy of optical flow and thus provides a more precise blurring model between the reconstructed intensity images from Deblur-Net and the blurry input. The overall network is jointly trained end-to-end over partially labeled dataset composed of synthetic data using ESIM [26] with ground-truth sharp clear images and real-world data only containing real-world events and real-world motion blurred images, which can be captured simultaneously by the DAVIS [3] or with a dual camera set connecting the event camera and the RGB camera with a beam splitter [32].
The main contributions of our work are in three folds:
• We propose a framework of learning the event-based motion deblurring network with real-world events, which remarkably improves the performance of mo-tion deblurring in the real-world scenario.
• We propose a piece-wise linear motion model to con-sider the motion nonlinearity, based on which the OF-Net is ameliorated for real events and outputs accurate and dense motion flows.
• Extensive experiments show that the proposed method can yield high quality sharp frames and achieve state-of-the-art results on the real blurry event dataset. 2.