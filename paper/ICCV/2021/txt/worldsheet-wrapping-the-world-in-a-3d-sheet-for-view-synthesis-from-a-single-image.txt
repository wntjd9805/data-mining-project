Abstract
We present Worldsheet, a method for novel view synthe-sis using just a single RGB image as input. The main insight is that simply shrink-wrapping a planar mesh sheet onto the input image, consistent with the learned intermediate depth, captures underlying geometry sufﬁcient to generate photorealistic unseen views with large viewpoint changes.
To operationalize this, we propose a novel differentiable texture sampler that allows our wrapped mesh sheet to be textured and rendered differentiably into an image from a target viewpoint. Our approach is category-agnostic, end-to-end trainable without using any 3D supervision, and re-quires a single image at test time. We also explore a sim-ple extension by stacking multiple layers of Worldsheets to better handle occlusions. Worldsheet consistently outper-forms prior state-of-the-art methods on single-image view synthesis across several datasets. Furthermore, this simple idea captures novel views surprisingly well on a wide range of high-resolution in-the-wild images, converting them into navigable 3D pop-ups. Video results and code are available at https://worldsheet.github.io. 1.

Introduction
A 2D image is the projection of an underlying 3D world, but as humans, we have no trouble in understanding this structure and imagining how an image will look from other views. Consider the train shown in Figure 1, we can seam-lessly predict other views from a single image based on the abstractions we have learned from past experience of see-ing several trains, or similar shaped objects from different views. Enabling machines with such an ability to reason about 3D from a single image will bring trillions of still photos to life, with wide applications in virtual reality, ani-mation, image editing, and robotics. 1
The goal of synthesizing novel views from 2D images has been pursued for decades, from early efforts relying completely on multi-view geometry [7, 61, 38], to more re-cent learning based approaches [59, 43, 45, 1, 9, 30, 26, 35, 23, 51, 54]. Over the years, signiﬁcant progress has been made in this direction. However, despite impressive photorealistic output renderings, most of these previous ap-proaches require multiple images or ground-truth depth at test time, which severely hinders their practicality. To com-pensate for the lack of multiple views or 3D models at test time, methods for single-image 3D rely on statistical learn-ing from data. This line of work can be traced back to classic works of Hoiem et al. [13], followed by Saxena et al. [37], that obtain ‘qualitative 3D’ from a single image by
ﬁtting a collection of planes onto the image.
An ideal approach to general-purpose view synthesis should not only rely on a single image at test time, but also learn from easy-to-collect supervision signal during train-ing. In the deep learning era, there is growing interest in end-to-end methods with intermediate 3D representations supervised by multiple images and no explicit 3D infor-mation during training. However, they are mostly applied to objects [20, 47, 55, 15, 43], and are either category-speciﬁc, restricted to synthetic scenes, or both. Recent works [5, 53, 48] address these issues by training with mul-tiple views of real-world scenes, relying on point cloud or multiplane images as intermediate representations. How-ever, multiplane images only perform well with relatively small viewpoint changes as each plane is at a constant depth; for point clouds, one needs to represent each point in a scene individually, making it inefﬁcient to scale to high-resolution data or large viewpoint changes.
In contrast, meshes can provide a sparser scene representation, e.g., two triangular mesh faces can theoretically represent the entire
ﬂat surface of a wall, making it ideal for single-image view synthesis. However, mesh recovery from single images has been studied mostly for object images and in a category-speciﬁc manner [2, 15, 19] and not for scenes.
In this paper, we present an end-to-end approach for novel view synthesis from a single image of a scene via an intermediate mesh representation. Unlike mesh reconstruc-tion for objects of speciﬁc categories, generating meshes for a scene is challenging as there is no notion of mean or canonical shape to start from, or silhouette from seg-mentation for supervision. We circumvent this problem by wrapping a deformable mesh sheet over the 3D world – much like wrapping a 2D tinfoil onto a 3D pan before baking! We name this shrink-wrapped mesh Worldsheet, a term borrowed from physics for the 2D manifold of high-dimensional strings. After generating this Worldsheet for a given view, novel views are obtained by moving the camera in 3D space (Figure 2), which allows us to train from just two views of a scene using only rendering losses without any 3D or depth supervision.
To train our model end-to-end, both reconstruction of the mesh texture from input view and rendering from a novel camera view need to be differentiable. The latter is easily handled thanks to recent differentiable mesh renderers [16, 27, 33]. To address the former, we propose a differentiable texture sampler over projected 2D views, enabling gradient computation of the reconstructed texture map over the 3D mesh geometry. Furthermore, to better handle occlusions and depth discontinuities, we propose a simple extension by stacking multiple layers of Worldsheets onto the scene.
In summary, Worldsheet generates novel views by learn-ing to predict scene geometry from a single image. Al-though 3D mesh reconstruction via differentiable rendering is common for objects, to our best knowledge, this is the
ﬁrst work to show mesh recovery for scenes just from multi-view supervision. Our model consistently outperforms prior state-of-the-art by a signiﬁcant margin on three benchmark datasets (Matterport [3], Replica [46], and RealEstate10K
[59]), and is applicable to very high-resolution images in-the-wild as shown in Figure 1. 2.