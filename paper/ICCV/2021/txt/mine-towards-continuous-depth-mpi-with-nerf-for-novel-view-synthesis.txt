Abstract
In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruc-tion from a single image. Our approach is a continu-ous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel im-age (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in oc-cluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on
RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without anno-tated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE. 1.

Introduction
Interactive 3D scene is a fascinating way to achieve im-mersive user experience similar to augmented/virtual real-ity. To automate or simplify the creation of 3D scenes, in-creasing efforts are invested on novel view synthesis from a single or multiple image(s) that enables rendering at arbi-trary camera poses according to user’s interaction. Despite its usefulness, the novel view synthesis problem is challeng-ing because it requires precise geometry understanding, and inpainting of the occluded geometry and textures.
To tackle the problem of view synthesis, most existing methods focus on the design of 3D or 2.5D representa-tions of the scene, and the rendering techniques of novel views. A straightforward idea is to perform Structure-from-Motion (SfM) [38, 37] or monocular/multiview depth esti-mations [11, 10, 60, 52] to recover the 3D scene. Unfortu-nately, this naive approach is insufficient to acquire accurate dense 3D geometry and fill in the occluded contents of the scene. Consequently, this results to distorsion and artifacts
*Equal contribution.
Figure 1. Overview of our proposed method. in the rendered novel views. To alleviate this problem, more sophisticated representations including Layed Depth Image (LDI) [51, 40], Multiplane Images (MPI) [50] are used with deep networks to recovered 2.5D information from single / multiple images. However, 2.5D approaches usually suffer from limited resolution to represent the full 3D scene.
Recently, the MPI [50] representation attracts a lot of at-tentions. Specifically, it is a deep network supervised with other image views of the same scene to lift a RGB im-age into multiple planes of RGB and alpha values. Novel views are then rendered by performing homography warp-ing and integral over the planes. Despite its success, the
MPI method fails to represent continuous 3D space effec-tively. Its depth-wise resolution is limited by the number of discrete planes, and thus the MPIs cannot be converted to other 3D representations such as mesh, point cloud, etc.
In contrast, the Neural Radiance Fields (NeRF) [28] is con-currently proposed to recover 3D information from images using a Multi-layer Perceptron (MLP). The MLP takes a 3D position and a 2D viewing direction as input to predict the RGB and volume occupancy density at that query posi-tion. Although NeRF produces high quality 3D structures and novel views, it has to be trained per scene, i.e. one MLP represents only one scene.
In view that MPI [50] is unable to represent the full 3D space, we propose MINE that generalizes MPI to a contin-uous 3D representation similar to NeRF [28]. Specifically, an input image is first fed into an encoder network to ob-tain the image features. A decoder network then takes these
image features and an arbitrary depth as inputs to produce a 4-channel, i.e. RGB and volume density values, plane fronto-parallel to the input camera. As shown in Sec. 3.1, our MINE can effectively reconstructs the camera frustum in full 3D space since the plane depth is arbitrary. We prove in Sec. 3.5 that the MPI [50] representation is a limited spe-cial case of our approach. Our main contributions are:
• Performs continuous and occlusion-inpainted 3D re-construction from a single image.
• Our MINE is a continuous depth generalization of the
MPI by introducing the NeRF idea.
• Significantly outperforms existing state-of-the-art methods in indoor and outdoor view synthesis and depth estimation. 2.