Abstract 1.

Introduction
This paper introduces a pipeline to parametrically sam-ple and render static multi-task vision datasets from com-prehensive 3D scans from the real-world.
In addition to enabling interesting lines of research, we show the tooling and generated data suffice to train robust vision models. Fa-miliar architectures trained on a generated starter dataset reached state-of-the-art performance on multiple common vision tasks and benchmarks, despite having seen no bench-mark or non-pipeline data. The depth estimation network outperforms MiDaS and the surface normal estimation net-work is the first to achieve human-level performance for in-the-wild surface normal estimation—at least according to one metric on the OASIS benchmark.
The Dockerized pipeline with CLI, the (mostly python) code, PyTorch dataloaders for the generated data, the gen-erated starter dataset, download scripts and other utilities are all available through our project website.
∗Equal contribution.
This paper introduces a pipeline to bridge the gap be-tween comprehensive 3D scans and static vision datasets.
Specifically, we implement and provide a platform that takes as input one of the following:
• a textured mesh,
• a mesh with images from an actual camera/sensor,
• a 3D pointcloud and aligned RGB images, and generates a multi-task dataset with as many cameras and images as desired to densely cover the space. For each image, there are 21 different default mid-level cues, shown in Fig. 1. The software makes use of Blender [16], a power-ful physics-based 3D rendering engine to create the labels, and exposes complete control over the sampling and genera-tion process. With the proliferation of reasonably-priced 3D sensors (e.g. Kinect, Matterport, and the newest iPhone), we anticipate an increase in such 3D-annotated data.
In order to establish the soundness for training com-puter vision models, we used our pipeline to annotate sev-eral existing 3D scans and produce a medium-size starter dataset of mid-level cues. Samples of the data and differ-ent cues are shown in Fig. 5. Standard models trained on
this starter dataset achieve state-of-the-art performance for several standard computer vision tasks. For surface nor-mal estimation, a standard UNet [45] model trained on this starter dataset yields human-level surface normal estimation performance on the in-the-wild dataset OASIS [12], even though the model never saw OASIS data during training.
For depth estimation, our DPT-Hybrid [41] is comparable to or outperforms state-of-the-art models such as MiDaS DPT-Hybrid [42, 41]. The qualitative performance of these net-works (shown in Figs. 6, 7) is often better than the numbers suggest, especially for fine-grained details.
We further provide an ecosystem of tools and documen-tation around this platform. Our project website contains links to a Docker containing the annotator and all neces-sary libraries, PyTorch [39] dataloaders to efficiently load the generated data, pretrained models, scripts to generate videos in addition to images, and other utilities.
We argue that these results should not be interpreted nar-rowly. The core idea of the platform is that the “sectors of the ambient [light-field] array are not to be confused with temporary samples of the array” (J. J. Gibson [21]). That is, static images only represent single samples of the entire 360-degree panoramic light-field environment surrounding an agent. How an agent or model samples and represents this environment will affect its performance on downstream tasks. The proposed platform in this paper is designed to reduce the technological barriers for research into the effect of data sampling practices and into the interrelationships between data distribution, data representation, models, and training algorithms. We discuss directions here and analyze a few illustrative examples in the final section of the paper.
First, the pipeline proposed in this paper provides a pos-sible pathway to understand such sampling effects. That is, the rendering pipeline offers complete control over (hereto-fore) fixed design choices such as camera intrinsics, scene lighting, object-centeredness [40], the level of “photogra-pher’s bias” [6], data domain, and so on. This makes it possible to run intervention studies (e.g. A/B tests), without collecting and validating a new dataset or relying on a post-hoc analysis. As a consequence, this provides an avenue for a computer vision “dataset design guide”.
Second, vision is about much more than semantic recog-nition, but our datasets are biased towards that as the core problem. The best-studied, most diverse and largest dataset (>10M images) generally contains some form of textu-al/class labels [18, 51] and only RGB images. On the other hand, datasets for most non-classification tasks remain tiny by modern standards. For example, the indoor scene dataset NYU [47], still used for training some state-of-the-art depth estimation models [62], contains only 795 train-ing images—all taken with a single camera. The pipeline presents a way to generate datasets of comparable quality for non-recognition tasks.
Third, the generated data allows “matched-pair experi-mental design” that simplifies study into the interrelation-ships of different tasks, since the pipeline produces labels for every sample. In particular, it helps to avoid issues like the following: suppose a model trained for object classi-fication on ImageNet transfers to COCO [32] better than a model trained for depth estimation on NYU [47]–is that due to the data domain, the training task, the diversity of camera intrinsics, or something else?
Existing matched-pair datasets usually focus on a sin-gle domain (indoor scenes [64, 47, 3, 50], driving [20, 17], block-worlds [24], etc.) and contain few cues [17, 47, 3, 50]. The provided starter dataset may be a better candi-date for this research than these existing datasets, since it contains over 14.5 million images from different domains (more than the full ImageNet database), contains many dif-ferent cues (e.g. for depth, surface normals, curvature, panoptic segmentation, and so on), and models trained on this dataset reach excellent performance for several tasks and existing benchmarks. We demonstrate the value of such matched-pairs data in Sec. 5.3,
Though our pipeline is designed to facilitate understand-ing the principles of dataset design, vision beyond recog-nition, the interrelationships between data, tasks, and mod-els, this paper does not extensively pursue those questions themselves. It provides a few analyses, but these are merely intended as illustrative examples. Instead, the paper intro-duces tooling designed to facilitate such research as 3D data becomes more widely available and the capture technology improves. On our website, we provide a documented, open-sourced, and Dockerized annotator pipeline with a conve-nient CLI, runnable examples, the starter dataset, pretrained models, PyTorch dataloaders, and code for the paper (in-cluding annotator and models). 2.