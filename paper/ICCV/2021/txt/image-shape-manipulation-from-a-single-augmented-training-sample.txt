Abstract
In this paper, we present DeepSIM, a generative model for conditional image manipulation based on a single im-age. We find that extensive augmentation is key for enabling single image training, and incorporate the use of thin-plate-spline (TPS) as an effective augmentation. Our network learns to map between a primitive representation of the im-age to the image itself. The choice of a primitive represen-tation has an impact on the ease and expressiveness of the manipulations and can be automatic (e.g. edges), manual (e.g. segmentation) or hybrid such as edges on top of seg-mentations. At manipulation time, our generator allows for making complex image changes by modifying the primitive input representation and mapping it through the network.
Our method is shown to achieve remarkable performance on image manipulation tasks. 1.

Introduction
Deep neural networks have significantly boosted perfor-mance on image manipulation tasks for which large train-*Equal contribution ing datasets can be obtained, such as, mapping facial land-marks to facial images. In practice, however, there are many settings in which the image to be manipulated is unique, and a training set consisting of many similar input-output samples is unavailable. Moreover, in some cases using a large dataset might even lead to unwelcome outputs that do not preserve the specific characteristics of the desired im-age. Training generative models on just a single image, is an exciting recent research direction, which may hold the potential to extend the scope of neural-network-based im-age manipulation methods to unique images. In this paper, we introduce - DeepSIM, a simple-to-implement yet highly effective method for training deep conditional generative models from a single image pair. Our method is capable of solving various image manipulation tasks including: (i) shape warping (Fig. 2) (ii) object rearrangement (Fig. 5) (iii) object removal (Fig. 5) (iv) object addition (Fig. 2) (v) cre-ation of painted and photorealistic animated clips (Fig. 8 and videos on our project page).
Given a single target image, first, a primitive represen-tation is created for the training image. This can either be unsupervised (i.e. edge map, unsupervised segmentation), supervised (i.e. segmentation map, sketch, drawing), or a
Figure 2: Results produced by our model. The model was trained on a single training pair, shown to the left of each sample.
First row ”face” output- (left) flipping eyebrows, (right) lifting nose. Second row ”dog” output- changing shape of dog’s hat, removing ribbon, and making face longer. Second row ”car” output- (top) adding wheel, (bottom) conversion to sports car. combination of both. We use a standard conditional image mapping network to learn to map between the primitive rep-resentation and the image. Once training is complete, a user can explicitly design and choose the changes they want to apply to the target image by manipulating the simple primi-tive (serving as a simpler manipulation domain). The modi-fied primitive is fed to the network, which transforms it into the real image domain with the desired manipulation. This process is illustrated in Fig. 1.
Several papers have explored the topic of what and how much can be learned from a single image. Two recent semi-nal works SinGAN [28] and InGAN [29] propose to extend this beyond the scope of texture synthesis [6, 16, 21, 38].
SinGAN tackles the problem of single image manipulation in an unconditional manner allowing unsupervised gener-ation tasks.
InGAN, on the other hand, proposes a con-ditional model for applying various geometric transforma-tions to the image. Our paper extends this body of work by exploring the case of supervised image-to-image trans-lation allowing the modification of specific image details such as the shape or location of image parts. We find that the augmentation strategy is key for making DeepSIM work effectively. Breaking from the standard practice in the im-age translation community of using a simple crop-and-flip augmentation, we found that using a thin-plate-spline (TPS)
[11] augmentation method is essential for training condi-tional generative models based on a single image-pair input.
The success of TPS is due to its exploration of possible im-age manipulations, extending the training distribution to in-clude the manipulated input. Our model successfully learns the internal statistics of the target image, allowing both pro-fessional and amateur designers to explore their ideas while preserving the semantic and geometric attributes of the tar-get image and producing high fidelity results.
Our contributions in this paper:
• A general purpose approach for training conditional generators supervised by merely a single image-pair.
• Recognizing that image augmentation is key for this task, and the remarkable performance of thin-plate-spline (TPS) augmentation which was not previously used for single image manipulation.
• Achieving outstanding visual performance on a range of image manipulation applications. 2.