Abstract
We explore the zero-shot setting for day-night domain adaptation. The traditional domain adaptation setting is to train on one domain and adapt to the target domain by ex-ploiting unlabeled data samples from the test set. As gath-ering relevant test data is expensive and sometimes even impossible, we remove any reliance on test data imagery and instead exploit a visual inductive prior derived from physics-based reﬂection models for domain adaptation. We cast a number of color invariant edge detectors as train-able layers in a convolutional neural network and evalu-ate their robustness to illumination changes. We show that the color invariant layer reduces the day-night distribu-tion shift in feature map activations throughout the network.
We demonstrate improved performance for zero-shot day to night domain adaptation on both synthetic as well as nat-ural datasets in various tasks, including classiﬁcation, seg-mentation and place recognition. 1.

Introduction
Deep image recognition methods are sensitive to illumi-nation shifts caused by accidental recording conditions such as camera viewpoint, light color, and illumination changes caused by time of day or weather [1, 16, 78], as for example a model trained with daylight will not generalize to night time. Robustness to such recording conditions is essen-tial for autonomous driving and other safety-critical com-puter vision applications. An illumination shift between train and test data is typically addressed by unsupervised domain adaptation [55, 57, 76] where the labeled training set is from one domain and the test set is from a different domain. The main assumption is that the test data is readily available and the challenge is how to make use of the un-labeled test data in an unsupervised setting to address the domain shift. However, adding test data is often non-trivial as it may be expensive and time consuming to obtain and due to the long tail of the real world is impossible to collect for all possible scenarios in advance.
Instead of adding more data, prior knowledge can be
Figure 1: Feature map activations in various layers of a baseline ResNet-18 and a color invariant W -ResNet-18, av-eraged over all samples in a ‘Normal’ and ‘Darker’ test set (samples on right). The intensity change between the test sets causes an internal distribution shift throughout all lay-ers of the baseline model. W normalizes the input resulting in more domain invariant features. built-in as a visual inductive bias. The champion of such a bias is the convolution operator added to a deep net-work which yields a Convolutional Neural Network (CNN).
The CNN is translation invariant, and thus saves a massive amount of data as the deep network no longer needs train-ing samples at all possible locations. Here, we replace data by an inductive photometric bias. We introduce a novel zero-shot domain adaptation method for addressing day-night domain shifts exploiting learnable photometric invari-ant features as a physics-based visual inductive prior.
In contrast to unsupervised domain adaptation, our zero-shot method reduces the data dependency by removing any re-liance on the availability of test data.
Illumination changes to the source domain induce a dis-tribution shift of feature map activations throughout all lay-ers of a CNN. This is shown as the baseline in the top row of Fig. 1, where the activations of a CNN trained on day-time data are shown for a ‘Normal’ (source) and ‘Darker’ (target) test set. Such a distribution shift, in turn, has a se-vere detrimental effect on the accuracy of the CNN [39].
Because the distribution shift is between the training data and unavailable test data, this shift cannot be addressed in a data-driven manner using, for example, variants of Batch
Normalization [29, 39]. Instead, we normalize feature map 1
activations in a data-free setting by exploiting photomet-ric invariant features which are explicitly designed to tackle distribution shifts caused by illumination changes.
Photometric invariant features, or color invariants, repre-sent object properties irrespective of the accidental record-ing conditions [24, 25], including 1) scene geometry, which affects the formation of shadows and shading, the 2) color and 3) intensity of the light source, which changes the over-all tint and brightness of the scene, and 4) Fresnel reﬂec-tions occurring on shiny materials where the incoming light is directly reﬂected from the surface without interacting with the material color. Thanks to their robustness to these lighting changes, color invariants have been widely used in classical computer vision applications [6, 46], yet their use in a deep learning setting has remained largely unex-plored. We implement the color invariant edge detectors from [24] as a trainable Color Invariant Convolution (CI-Conv) layer which can be used as the input layer to any
CNN to transform the input to a domain invariant represen-tation. Fig. 1, bottom row, shows that CIConv reduces the distribution shift between the source and target test set in all network layers, improving target domain performance.
We have the following contributions: (i) we introduce
CIConv, a learnable color invariant CNN layer that re-duces the activation distribution shift in a CNN under an illumination-based domain shift; (ii) we evaluate several color invariants in the day-night domain adaptation setting on our two carefully curated classiﬁcation datasets; and (iii) we demonstrate performance improvements on tasks related to autonomous driving, including classiﬁcation, segmenta-tion and place recognition. All datasets and code will be made available on our project page.1 2.