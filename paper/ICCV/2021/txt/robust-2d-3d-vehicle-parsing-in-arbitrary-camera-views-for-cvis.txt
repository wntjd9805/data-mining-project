Abstract
We present a novel approach to robustly detect and per-ceive vehicles in different camera views as part of a coop-erative vehicle-infrastructure system (CVIS). Our formula-tion is designed for arbitrary camera views and makes no assumptions about intrinsic or extrinsic parameters. First, to deal with multi-view data scarcity, we propose a part-assisted novel view synthesis algorithm for data augmenta-tion. We train a part-based texture inpainting network in a self-supervised manner. Then we render the textured model into the background image with the target 6-DoF pose. Sec-ond, to handle various camera parameters, we present a new method that produces dense mappings between im-age pixels and 3D points to perform robust 2D/3D vehicle parsing. Third, we build the first CVIS dataset for bench-marking, which annotates more than 1540 images (14017 instances) from real-world traffic scenarios. We combine these novel algorithms and datasets to develop a robust ap-proach for 2D/3D vehicle parsing for CVIS. In practice, our approach outperforms SOTA methods on 2D detection, in-stance segmentation, and 6-DoF pose estimation by 3.8%, 4.3%, and 2.9%, respectively. 1.

Introduction
Cooperative vehicle-infrastructure system (CVIS) have become a key focus of research and technology in the field of autonomous driving (AD) [43, 41]. In a CVIS, camera, radar, LiDAR, and other sensors are mounted on vehicles and street-light poles at different locations (e.g., front, side, top, etc.). The simultaneous perception of vehicles and road terminals can minimize blind zones and provide warning for out-of-sight collisions in advance. An example is shown in
Fig. 1, which is a typical traffic scenario. From the front-view of the autonomous vehicle, many of the other vehi-†Joint first author
*Corresponding author: zhoubin@buaa.edu.cn
Figure 1: We highlight a scene rendered from multiple views in a CVIS. We use our approach to detect and parse novel-view vehicles from a single image corresponding to the front view. cles are partially or completely occluded, presenting per-ception challenges. From the side/top-view, we can clearly see many of these occluded vehicles’ views. Although mul-tiple views can get better perception results, in the end, per-formance relies on improvement w.r.t. each of these single views. The ultimate perception problem in a CVIS, there-fore, is how to effectively detect the vehicles in novel views.
Many widely used AD datasets (e.g., KITTI [15], CitySca-pers [7], ApolloScape [18], ApolloCar3D [45]) only pro-vide labeled front-view data.
If we directly use these la-beled front-view datasets to train the deep neural networks (DNNs), the detection performance would degrade dramat-ically when testing these networks on the data from other views (e.g., the side view or the top-view). A common strategy is to capture new images for manual annotation, which is labor-intensive, costly, and inefficient. As a result, we need a novel set of view synthesis algorithms for data
augmentation that can enable us to perform robust 2D/3D vehicle parsing for CVIS.
To address the data scarcity challenge, we propose aug-menting existing AD datasets via novel view synthesis. In computer vision, view synthesis has been extensively stud-ied. Researchers use 3D model rendering techniques (e.g.,
[46], [39]), image-based appearance flow approaches (e.g.,
[63], [64]), or generative adversarial network (GAN) (e.g.,
[65], [44]) to synthesize novel-view images. Although they can achieve good results, these methods have several lim-itations in the context of CVIS. First, their synthesized re-sults are mostly used for visualization, which are difficult for deep networks to learn due to the domain gap. Sec-ond, it is difficult to obtain ground-truth annotations, espe-cially 3D information (e.g., a 3D bounding box and a 6-DoF pose). Third, these approaches rely on multi-view or paired images as guidance for training, which can be obtained by 3D model rendering or a camera array in the laboratory set-ting. However, it is difficult to obtain such data in real AD scenarios due to occlusions and fast vehicle motion.
Some of these synthesis problems can also be addressed using techniques for 3D parsing from single images. 3D parsing from a single image is important for AD, but re-mains challenging because pinhole cameras cannot obtain absolute 3D positions due to projective mapping. Many state-of-the-art (SOTA) methods (e.g., [30], [3], [9]) require training a depth estimation network using the ground-truth depth map or stereo pairs, with fixed intrinsic and extrinsic parameters for the cameras. They can get good results on the training dataset, but their generalizability and robustness are limited because 1) it is difficult to obtain the depth map in the real world, especially the depth of the background and 2) cameras have distinct intrinsic and extrinsic parameters.
Therefore, it is difficult for the trained model in camera A to estimate the 6-DoF vehicle pose in camera B. 1.1. Main Contributions
In this paper, we address the problem of data augmen-tation to achieve novel-view 2D/3D vehicles parsing for
CVIS. We address two main challenges: 1) automatic view synthesis for data augmentation and ground-truth 2D/3D labels generation in novel views and 2) robust 3D parsing from a single image in novel views. We present a novel ap-proach to detect and parse novel-view vehicles from a single image. This includes a new method for data augmentation of front-views in AD datasets and a novel robust approach for 2D/3D vehicle parsing. The key innovation of our data augmentation approach is the use of part-based texture in-painting for novel view synthesis. Specifically, we use the existing vehicle-based datasets that provide 3D vehicle tem-plates and associated 6-DOF datasets. We first project the image pixels to the texture map of the template. Because there could be some holes or blank regions due to the pro-jective mapping, we then train a part-based texture inpaint-ing network to fill the missing data. After obtaining the complete texture map, we render the textured 3D templates with arbitrary 6-DoF poses into novel views.
Based on this synthesized data generated from vehicle templates, we present a new approach for 2D/3D parsing that is robust for arbitrary camera parameters. Instead of directly learning depth from images, the key idea of our approach is learning dense mappings between the image pixels and the canonical 3D vehicle template. Specifically, we design a multi-task network that outputs results of 2D detection, instance segmentation, a 3D bounding box (i.e. width, height, and length), and canonical 3D points. These canonical 3D points are one-to-one mapped to the image pixels; thus, we can compute the 6-DoF pose by solving the
RANSAC-PnP problem using the input intrinsic and extrin-sic parameters of the camera.
Finally, to benchmark our synthesized data and 2D/3D parsing approach, we have constructed, to the best of our knowledge, the first CVIS-oriented dataset with vehicles in different camera views, which contains 1540 labeled images and 14017 2D vehicle instances. For each vehicle instance in our dataset, we annotate its 2D bounding box, instance-level segmentation, 3D bounding box, and 6-DoF pose.
In summary, our contributions include: 1) We present a novel and compact data augmentation pipeline for vehicle parsing in CVIS. This includes synthe-sized data generation, data learning for 2D/3D parsing, and real-world dataset construction for benchmarking. 2) A 3D-assisted image augmentation approach is pro-posed to handle novel views. The key innovation is a part-based texture inpainting network with a self-supervised ap-proach, which can automatically synthesize novel-view im-ages with ground-truth annotations. 3) For robust 2D/3D vehicle parsing, we learn dense cor-respondences between the image pixels and the 3D points in a 3D vehicle template, which is robust for 6-DoF pose estimation under arbitrary camera parameters. 4) We build a real-world CVIS dataset with 2D/3D vehi-cle annotation for benchmarking. We compare with other
SOTA methods of data augmentation and 2D/3D vehicle parsing. We highlight the accuracy improvements. 2.