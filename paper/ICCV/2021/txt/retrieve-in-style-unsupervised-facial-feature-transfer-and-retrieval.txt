Abstract
We present Retrieve in Style (RIS), an unsupervised framework for facial feature transfer and retrieval on real images. Recent work shows capabilities of transferring lo-cal facial features by capitalizing on the disentanglement property of the StyleGAN latent space. RIS improves ex-isting art on the following: 1) Introducing more effective feature disentanglement to allow for challenging transfers (i.e., hair, pose) that were not shown possible in SoTA methods. 2) Eliminating the need for per-image hyper-parameter tuning, and for computing a catalog over a large batch of images. 3) Enabling fine-grained face re-trieval using disentangled facial features (e.g., eyes). To our best knowledge, this is the first work to retrieve face images at this fine level. 4) Demonstrating robust, natu-ral editing on real images. Our qualitative and quanti-tative analyses show RIS achieves both high-fidelity fea-ture transfers and accurate fine-grained retrievals on real images. We also discuss the responsible applications of
RIS. Our code is available at https://github.com/ mchong6/RetrieveInStyle. 1.

Introduction
Recent advancements in Generative Adversarial Net-works (GANs) [6, 18, 19] have shown capabilities to gen-erate realistic high resolution images, particularly for faces.
Under unconditional settings, it is often hard to interpret or control the outputs of GANs. Conditional GANs are more naturally amenable for semantic editing. However, the degree of meaningful control over the output images is largely dependent on how detailed the annotations are. This presents a challenge for fine-grained face editing as it is of-ten difficult or impossible to annotate datasets with the de-gree of detail needed for fine-grained editing.
Existing works on face editing typically leverage ad-ditional information to guide conditional generation, such as manual labels [3, 8, 21, 42, 44, 45], segmentation masks [12, 22], attribute classifiers [14], rendering models 1
[20, 38], etc. However, the additional information requires extra computation and is not always available in practice. In addition, the fine-grained facial features (e.g., a distinctive shape of eyes) are difficult to describe as labels or features.
As an alternative, unsupervised discovery of latent direc-tions in a pretrained GAN [13, 31, 39] allows for finding meaningful latent representations in a computationally effi-cient way. However, such approaches are less effective for fine-grained editing compared to supervised approaches.
Recently, Editing in Style (EIS) [10] proposed a mostly unsupervised method for facial feature transfer. While EIS allows semantic editing of spatially coherent facial features (e.g., eyes, nose and mouth), it requires computing a seman-tic catalog over the whole dataset and separate hyperparam-eter tuning for each image. Such requirements make EIS non-scalable to large datasets as commonly encountered in retrieval domains. In addition, it remains challenging for
EIS to control facial features that are difficult to describe as a spatial map, such as hair and head pose. More im-portantly, EIS works only on synthetic images and remains untested on how real images could be manipulated.
In this study, we propose Retrieve in Style (RIS), a sim-ple and efficient unsupervised framework that tackles both fine-grained facial feature transfer and retrieval. Fig. 1 il-lustrates the capabilities offered by RIS. RIS improves EIS in several aspects. First, we discover the “submember-ship” property in the style space, showing that style chan-nels corresponding to a particular feature (e.g., eyes) are different for every image and thus must be computed in-dividually instead of over the entire dataset. As the discov-ered channels are image-specific, RIS achieves more precise face editing for not only spatially coherent facial features (e.g., eyes, nose, mouth) but also challenging ones (i.e., hair, pose). Second, with the discovered “submembership”, we show it is possible to eliminate EIS’s requirements on per-image semantic catalog and per-image hyperparameter tun-ing, and offer better scalability to larger problems. Third, the image-specific representations naturally extend RIS for fine-grained facial feature retrieval that was not shown pos-sible in EIS. Lastly, we demonstrate that RIS offers editing and retrieval of real images when combined with GAN in-version methods, while EIS worked with synthetic images.
Although RIS is general and can be applied to a wide range of datasets, this study focuses on faces as there are estab-lished conventions on facial parts and its relevance in face retrieval applications (e.g., [4, 11, 23, 25]).
Our contributions are: 1. RIS improves over EIS based on our finding of “sub-membership”, obtaining better controllability over fa-cial features that are spatially coherent (eyes, nose, mouth) and incoherent (hair pose), while requiring no hyperparameter tuning. 2. We obtain feature-specific representations (e.g., eyes, nose, mouth, hair), which enable face retrieval by fine-grained features that are difficult to describe or anno-tate even for humans. To our best knowledge, this is the first work to address the fine-grained retrieval prob-lem without supervision. 3. We show that RIS generalizes to GAN-inverted im-ages, allowing transfer and retrieval on real images that was not shown possible in earlier studies. Results on
CelebA-HQ validates that RIS achieves high-quality retrieval on large, real-world datasets. 2.