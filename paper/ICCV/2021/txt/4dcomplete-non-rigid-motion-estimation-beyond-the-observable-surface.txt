Abstract 1.

Introduction
Tracking non-rigidly deforming scenes using range sen-sors has numerous applications including computer vision,
AR/VR, and robotics. However, due to occlusions and phys-ical limitations of range sensors, existing methods only han-dle the visible surface, thus causing discontinuities and in-completeness in the motion ﬁeld. To this end, we intro-duce 4DComplete, a novel data-driven approach that es-timates the non-rigid motion for the unobserved geometry. 4DComplete takes as input a partial shape and motion ob-servation, extracts 4D time-space embedding, and jointly infers the missing geometry and motion ﬁeld using a sparse fully-convolutional network. For network training, we con-structed a large-scale synthetic dataset called Deform-ingThings4D, which consists of 1,972 animation sequences spanning 31 different animals or humanoid categories with dense 4D annotation. Experiments show that 4DComplete 1) reconstructs high-resolution volumetric shape and mo-tion ﬁeld from a partial observation, 2) learns an entan-gled 4D feature representation that beneﬁts both shape and motion estimation, 3) yields more accurate and natural de-formation than classic non-rigid priors such as As-Rigid-As-Possible (ARAP) deformation, and 4) generalizes well to unseen objects in real-world sequences.
Understanding the motion of non-rigidly deforming scenes using a single range sensor lies at the core of many computer vision, AR/VR, and robotics applications. In this context, one fundamental limitation is that a single-view range sensor cannot capture data in occluded regions, lead-ing to incomplete observations of a 3D environment. As a result, existing non-rigid motion tracking methods are re-stricted to the observable part of the scene. However, the ability to infer complete motion from a partial observation is indispensable for many high-level tasks. For instance, as a nursing robot, to safely care for an elderly person (e.g., predict the person’s action and react accordingly), it needs to understand both the complete body shape and how the whole body moves even if the person is always partially oc-cluded.
In order to address these challenges, we pose the ques-tion how can we infer the motion of the unobserved geome-try in a non-rigidly deforming scene? Existing works such as DynamicFusion [38] and VolumeDeform [26] propose to propagate deformations from the visible surface to the invis-ible space through a latent deformation graph. Hidden de-formations are then determined by optimizing hand-crafted deformation priors such as As-Rigid-As-Possible [47] or
Embedded Deformation [49], which enforces that graph vertices locally move in an approximately rigid manner.
Such deformation priors have several limitations: 1) they re-quire heavy parameter tuning; 2) they do not always reﬂect natural deformations; and 3) they often assume a continuous surface. As a result, these priors are mostly used as regular-izers for local deformations, but struggle with larger hidden regions. One promising avenue towards solving this prob-lem is to leverage data-driven priors that learn to infer the missing geometry. Very recently, deep learning approaches for 3D shape or scene completion and other generative tasks involving a single depth image or room-scale scans have shown promising results [12, 46, 11, 7, 10]. However, these works primarily focus on static environments.
In this paper, we make the ﬁrst effort to combine geom-etry completion with non-rigid motion tracking. We argue that the shape and motion of non-rigidly deforming objects are highly entangled data modalities: on one hand, the abil-ity to infer the geometry of unobserved object parts pro-vides valuable information for motion estimation. On the other hand, motion is considered as the shape’s evolution in the time axis, as similarity in motion patterns are a strong indicator for structural connectivity. To leverage these syn-ergies, we propose 4DComplete, which jointly recovers the missing geometry and predicts motion for both seen and unseen regions. We build 4DComplete on a sparse, fully-convolutional neural network, which facilitates the joint es-timation of shape and motion at high resolutions. In addi-tion, we introduce DeformingThings4D, a new large-scale synthetic dataset which captures a variety of non-rigidly deforming objects including humanoids and animals. Our dataset provides holistic 4D ground truth with color, opti-cal/scene ﬂow, depth, signed distance representations, and volumetric motion ﬁelds.
In summary, we propose the following contributions:
• We introduce 4DComplete, the ﬁrst method that jointly recovers the shape and motion ﬁeld from partial obser-vations.
• We demonstrate that these two tasks help each other, resulting in strong 4D feature representations outper-forming existing baselines by a signiﬁcant margin.
• We provide a large-scale non-rigid 4D dataset for
The dataset consists training and benchmarking. of 1,972 animation sequences, and 122,365 frames.
Dataset is available at: https://github.com/ rabbityl/DeformingThings4D. 2.