Abstract
Generating conversational gestures from speech audio is challenging due to the inherent one-to-many mapping be-tween audio and body motions. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, resulting in plain/boring motions during inference.
In order to over-come this problem, we propose a novel conditional vari-ational autoencoder (VAE) that explicitly models one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-speciﬁc code. The shared code mainly models the strong correlation between audio and motion (such as the synchronized audio and mo-tion beats), while the motion-speciﬁc code captures diverse motion information independent of the audio. However, splitting the latent code into two parts poses training dif-ﬁculties for the VAE model. A mapping network facilitat-ing random sampling along with other techniques includ-ing relaxed motion loss, bicycle constraint, and diversity loss are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than state-of-the-art methods, quantitatively and qualitatively. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-speciﬁed motion clips on the timeline. Code and more results are at https:
//jingli513.github.io/audio2gestures. 1.

Introduction
In the real world, co-speech gestures help express one-self better, and in the virtual world, it makes a talking avatar act more vividly. Attracted by these merits, there has been a growing demand for generating realistic human motions for given audio clips recently. This problem is very chal-lenging because of the complicated one-to-many relation-ship between audio and motion. A speaker may act differ-ent gestures when speaking the same words due to different mental and physical states.
*Corresponding author: zhenyuhe@hit.edu.cn
Motion 1
Motion 2 Other motions
“Completely”
· · ·
Figure 1. Illustration of the existence of one-to-many mapping be-tween audio and motion in Trinity dataset [10]. Different gestures are performed when the subject says “completely”.Similar phe-nomena broadly exist in co-speech gestures. The character used for demonstration is from Mixamo [1].
Existing algorithms developed for audio to body dynam-ics have some obvious limitations. For example, [12] adapts a fully convolutional neural network to co-speech gesture synthesis tasks. Nevertheless, their model tends to predict averaged motion and thus generates motions lacking diver-sity. This is due to the underlying one-to-one mapping as-sumption of their model, which ignores that the relation-ship between speech and co-speech gesture is one-to-many in nature. Under such an overly simpliﬁed assumption, the model has no choice but to learn the averaged motion when several motions match almost the same audio clips in order to minimize the error. The above evidence inspires us to study whether or not explicitly modeling this multimodality improves the overall motion quality. To enhance the regres-sion capability, we introduce an extra motion-speciﬁc latent code. With this varying full latent code, which contains the same shared code and varying motion-speciﬁc code, the de-coder can regress different motion targets well for the same audio, achieving one-to-many mapping results. Under this formulation, the shared code extracted from audio input serves as part of the control signal. The motion-speciﬁc code further modulates the audio-controlled motion, en-abling multimodal motion generation.
Although this formulation is straightforward, it is not trivial to make it work as expected. Firstly, there exists an easy degenerated solution since the motion decoder could utilize only the motion-speciﬁc code to reconstruct the mo-tion. Secondly, we need to generate the motion-speciﬁc
code since we do not have access to the target motion dur-ing inference. Our solution to the aforementioned problems is providing random noise to the motion-speciﬁc code so that the decoder has to utilize the deterministic information contained in the shared code to reconstruct the target.
But under this circumstance, it is unsuitable for forcing the motion decoder to reconstruct the exact original target motion anymore. So a relaxed motion loss is proposed to apply to the motions generated with random motion-speciﬁc code. Speciﬁcally, it only penalizes the joints de-viating from their targets larger than a threshold. This loss encourages the motion-speciﬁc code to tune the ﬁnal motion while respecting the shared code’s control.
Our contributions can be summarized as:
• We present a co-speech gesture generation model whose latent space is split into shared code and motion-speciﬁc code to better regress the training data and generate diverse motions.
• We utilize random sampling and a relaxed motion loss to avoid degeneration of the proposed network and en-able the model to generate multimodal motions.
• The effectiveness of the proposed method has been veriﬁed on 3D and 2D gesture generation tasks by comparing it with several state-of-the-art methods.
• The proposed method is suitable for motion synthe-sis from annotations since it can well respect the pre-deﬁned actions in the timeline by simply using their corresponding motion-speciﬁc code. 2.