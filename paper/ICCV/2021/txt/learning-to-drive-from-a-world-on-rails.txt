Abstract
We learn an interactive vision-based driving policy from pre-recorded driving logs via a model-based approach. A forward model of the world supervises a driving policy that predicts the outcome of any potential driving trajectory. To support learning from pre-recorded logs, we assume that the world is on rails, meaning neither the agent nor its actions influence the environment. This assumption greatly simplifies the learning problem, factorizing the dynamics into a non-reactive world model and a low-dimensional and compact forward model of the ego-vehicle. Our approach computes action-values for each training trajectory using a tabular dynamic-programming evaluation of the Bellman equations; these action-values in turn supervise the final vision-based driving policy. Despite the world-on-rails assumption, the fi-nal driving policy acts well in a dynamic and reactive world.
It outperforms imitation learning as well as model-based and model-free reinforcement learning on the challenging
CARLA NoCrash benchmark. It is also an order of magni-tude more sample-efficient than state-of-the-art model-free reinforcement learning techniques on navigational tasks in the ProcGen benchmark. 1.

Introduction
Vision-based autonomous driving is hard. An agent needs to perceive, understand, and interact with its environment from incomplete and partial experiences. Most success-ful driving approaches [5, 27, 33, 34] reduce autonomous navigation to imitating an expert, usually a human actor.
Expert actions serve as a source of strong supervision, sen-sory inputs of the expert trajectories explore the world, and policy learning reduces to supervised learning backed by powerful deep networks. However, expert trajectories are often heavily biased, and safety-critical observations are rare.
After all, human operators drive hundreds of thousands of miles before observing a traffic incident [39]. This spar-sity of safety-critical training data makes it difficult for a behavior-cloning agent to learn and recover from mistakes.
Model-free reinforcement learning [25, 40] offers a solution,
Figure 1: We learn a reactive visuomotor driving policy that gets to explore the effects of its own actions at training time.
The policy simulates the effects of its own actions using a forward model in pre-recorded driving logs. It then learns to choose safe actions without explicitly experiencing unsafe driving behavior. Picture selected from the Waymo open dataset [37]. allowing an agent to actively explore its environment and learn from it. However, this exploration is even less data-efficient than behavior cloning, as it needs to experience mistakes to avoid them. For reinforcement learning, the re-quired sample complexity for safe driving is prohibitively large, even in simulation [40].
In this paper, we present a method to learn a navigation policy that recovers from mistakes without ever making them, as illustrated in Figure 1. We first learn a world-model on static pre-recorded trajectories. This world model is able to simulate the agent’s actions without ever executing them. Next, we estimate action-value functions for all pre-recorded trajectories. Finally, we train a reactive visuomotor policy that gets to observe the impact of all its actions as predicted by the action-value function. The policy learns to avoid costly mistakes, or recover from them. We use driving logs, recorded lane maps and locations of traffic participants, to train the world-model and compute the action-value function. However, our visuomotor policy drives using raw sensor inputs, namely RGB images and speed readings alone. Figure 2 shows an overview.
The core challenge in our approach is to build a suffi-ciently expressive and accurate world-model that allows the
(a) Forward model. (b) Bellman update. (c) Distillation.
Figure 2: Overview of our approach. Given a dataset of offline driving trajectories of sensor readings, driving states, and actions, we first learn a forward model of the ego-vehicle (a). Using the offline driving trajectories, we then compute action-values under a pre-defined reward and learned forward model using dynamic programming and backward induction on the Bellman equation (b). Finally, the action-values then supervise a reactive visuomotor driving policy through policy distillation (c). For a single image, we supervise the policy for all vehicle speeds and actions for a richer supervisory signal. agent to explore its environment and the impact of its actions.
For autonomous driving, this involves modeling the au-tonomous vehicles and all other traffic participants, i.e. other
In its raw form, vehicles, pedestrians, traffic lights, etc. the state space in which the agent operates is too high-dimensional to effectively explore. We thus make a sim-plifying assumption: The agent’s actions only affect its own state, and cannot directly influence the environment around it. In other words: the world is “on rails”. This naturally factorizes the world-model into an agent-specific component that reacts to the agent’s commands, and a passively moving world. For the agent, we learn an action-conditional forward-model. For the environment, we simply replay pre-recorded trajectories from the training data.
The factorization of the world model lends itself to a sim-ple evaluation of the Bellman equations through dynamic programming and backward induction. For each driving trajectory, we compute a tabular approximation of the value function over all potential agent states. We use this value function and the agent’s forward model to compute action-value functions, which then supervise the visuomotor policy.
Action values serve as denser supervisory signals. For a sin-gle training example, we supervise the visuomotor policy on all agent states, including variations of the camera viewpoint, vehicle speed, or high level command.
We evaluate our method in the CARLA simulator [13].
On the CARLA leaderboard1, we achieve a 25% higher driving score than the prior top-ranking entry while using 40× less training data. Notably, our method uses camera-only sensors, while some prior work relies on LiDAR. We also outperform all prior methods on the NoCrash bench-mark [10]. Finally, we show that our method generalizes to other environments using the ProcGen platform [7]. Our method successfully learns navigational policies in the Maze and Heist environments with an order of magnitude fewer observations than baseline algorithms. Code and data are available2. 1https://leaderboard.carla.org/leaderboard/ 2https://dotchen.github.io/world_on_rails 2.