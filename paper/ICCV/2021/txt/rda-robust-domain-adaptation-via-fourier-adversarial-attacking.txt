Abstract w/o FAA w/ FAA
Unsupervised domain adaptation (UDA) involves a su-pervised loss in a labeled source domain and an unsuper-vised loss in an unlabeled target domain, which often faces more severe overﬁtting (than classical supervised learning) as the supervised source loss has clear domain gap and the unsupervised target loss is often noisy due to the lack of annotations. This paper presents RDA, a robust domain adaptation technique that introduces adversarial attacking to mitigate overﬁtting in UDA. We achieve robust domain adaptation by a novel Fourier adversarial attacking (FAA) method that allows large magnitude of perturbation noises but has minimal modiﬁcation of image semantics, the for-mer is critical to the effectiveness of its generated adversar-ial samples due to the existence of ‘domain gaps’. Speciﬁ-cally, FAA decomposes images into multiple frequency com-ponents (FCs) and generates adversarial samples by just perturbating certain FCs that capture little semantic in-formation. With FAA-generated samples, the training can continue the ‘random walk’ and drift into an area with a
ﬂat loss landscape, leading to more robust domain adapta-tion. Extensive experiments over multiple domain adapta-tion tasks show that RDA can work with different computer vision tasks with superior performance. 1.

Introduction
Deep convolutional neural networks (CNNs) [39, 70, 21] have deﬁned new state of the arts in various computer vision tasks [7, 44, 60, 20, 39, 70, 21], but their trained models often over-ﬁt to the training data and experience clear per-formance drops for data from different sources due to the existence of domain gaps. Unsupervised domain adaptation (UDA) has been investigated to address the domain gaps by leveraging unlabeled target data. To this end, most existing
UDA works [34, 35, 75, 78, 48, 80, 8, 9] involve supervised losses on source data and unsupervised losses on target data
*Corresponding author. 1.2 1 0.8 0.6 0.4 0.2 s s o
L 1.2 1 0.8 0.6 0.4 0.2 s s o
L 0 1.2 1 0.8 0.6 0.4 0.2 s s o
L
Supervised source loss
Target test loss 1.2 1 0.8 0.6 0.4 0.2 s s o
L
Supervised source loss
Target test loss 0 0 2 4 6
Iteration 8 10 12 104 0 0 2 4 6
Iteration 8 10 12 104
Unsupervised target loss (AL)
Target test loss
Unsupervised target loss (AL)
Target test loss 1.2 1 0.8 0.6 0.4 0.2 s s o
L 0 0 2 4 6
Iteration 8 10 12 104 0 0 2 4 6
Iteration 8 10 12 104
Unsupervised target loss (ST)
Target test loss
Unsupervised target loss (ST)
Target test loss 1.2 1 0.8 0.6 0.4 0.2 s s o
L 0 2 4 0 8 8 4 0 2 10 10 12 104 6
Iteration 6
Iteration 12 104
Figure 1. Our robust domain adaptation alleviates overﬁtting ef-fectively: Both supervised learning with source data (row 1) and unsupervised learning with target data (in rows 2 and 3 for adver-sarial learning and self-training) in unsupervised domain adapta-tion suffer from clear overﬁtting as illustrated by decreased train-ing losses (blue curves) vs increased target test losses (red curves) in column 1. Our Fourier adversarial attacking (FAA) generates novel adversarial samples, which regulates the minimization of training losses and alleviates overﬁtting effectively with decreased target test loss as shown in column 2 (Best viewed in color). for learning a model that performs well in target domains.
However, as illustrated in Fig. 1, these methods often face more severe overﬁtting (as compared with the classical su-pervised learning) as supervised source losses in UDA has an extra domain gap (for test data in target domains) and unsupervised target losses are often noisy due to the lack of annotations.
Overﬁtting exists in almost all deep network training, which is undesired and often degrades the generalization of
the trained deep network models while applied to new data.
One way of identifying whether overﬁtting is happening is to check whether the generalization gap, i.e., the difference between the test loss and the training loss, is increasing or not [13] as shown in Fig. 1. Various strategies have been in-vestigated to alleviate overﬁtting through weight regulariza-tion [19], dropout [71], mixup [93], label smoothing [72], batch normalization [30], etc. However, all these strategies were designed for supervised and semi-supervised learning where training data and test data usually have very similar distributions. For domain adaptive learning, they do not ﬁt in well due to the negligence of domain gaps that widely exist between data of different domains.
We design a robust domain adaptation technique that in-troduces a novel Fourier adversarial attacking (FAA) tech-nique to mitigate the overﬁtting in unsupervised domain adaptation. FAA mitigates overﬁtting by generating ad-versarial samples that prevent over-minimization of super-vised and unsupervised UDA losses as illustrated in Fig. 1.
Speciﬁcally, FAA decomposes training images into multi-ple frequency components (FCs) and only perturbs FCs that capture little semantic information. Unlike traditional at-tacking that restricts the magnitude of perturbation noises to keep image semantics intact, FAA allows large magni-tude of perturbation in its generated adversarial samples but has minimal modiﬁcation of image semantics. This fea-ture is critical to unsupervised domain adaptation which usually involves clear domain gaps and so requires adver-sarial sample with large perturbations. By introducing the
FAA-generated adversarial samples in training, networks can continue the “random walk” and avoid over-ﬁtting and drift into an area with a ﬂat loss landscape [6, 36, 40], lead-ing to more robust domain adaptation.
The contributions of this work can be summarized in three aspects. First, we identify the overﬁtting issue in unsupervised domain adaptation and introduce adversarial attacking to mitigate overﬁtting by preventing training ob-jectives from over-minimization. Second, we design an in-novative Fourier Adversarial Attacking (FAA) technique to generate novel adversarial samples to mitigate overﬁtting in domain adaptation. FAA is generic which can work for both supervised source loss and unsupervised target losses ef-fectively. Third, we conducted extensive experiments over multiple computer vision tasks in semantic segmentation, object detection and image classiﬁcation. All experiments show that our method mitigates overﬁtting and improve do-main adaption consistently. 2.