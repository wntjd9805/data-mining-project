Abstract
The training of a deep face recognition system usually faces the interference of label noise in the training data.
However, it is difficult to obtain a high-precision cleaning
In this paper, we propose model to remove these noises. an adaptive label noise cleaning algorithm based on meta-learning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns re-liable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta-supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the ef-fectiveness of the proposed method to reach state-of-the-art performance on the IJB-C face recognition benchmark. 1.

Introduction
Deep face recognition depends heavily on the training data [57, 58, 59]. Due to the deficiencies in data collec-tion and preprocessing, there is usually label noise in the dataset. For the face datasets, it refers to the existence of one to multiple faces of different people in one class.
In recent years, increasing the data scale of face recog-nition datasets is proved essential for training deep mod-els [6, 20, 24, 56, 60], but the label noise rate also inevitably improved [47]. Some studies [4, 9, 47, 48] reveal the heavy harm of label noise in the training sets to face recognition accuracy. This leads to a contradiction between data size and cleanliness, which gives birth to the data cleaning task.
It goals to keep the face images of one person (noted as
“signals”), delete the face images of other people (noted as
“noise”), and keep as many images as possible in one class.
Many data cleaning solutions [1, 6, 47] are proposed to
Figure 1: Main idea of AMC for face data cleaning. (a)(b)
With meta-learning guidance, the learned signal-noise dis-tribution is closer to the real distribution. (c) Learn adaptive threshold of one class on the signal-noise graph manifold. eliminate label noise. For example, FaceGraph [56] deploys a global-local Graph Convolutional Network (GCN) [21, 27] as a binary classifier to classify signal and noise on a k-NN graph. The biggest contradiction in these kinds of methods is that the target data to be cleaned is generally unlabeled, so the cleaning model is usually trained on ad-ditional labeled data. Assuming that the additional labeled data is the source domain, and the unlabeled target data is the target domain, due to the domain gap, the trained model is difficult to adapt to the distribution of the target data.
In Figure 1(a), red triangles and rectangles represent the real signal-noise distribution of the target domain. When a cleaning model trained on the source domain is deployed to clean them, the signal and noise may not be separated well as the blue triangles and rectangles. To solve this prob-lem, many transfer learning methods [18, 31, 32, 39, 44] are
proposed to eliminate the domain gap [49]. In this paper, we propose the Adaptive Meta Cleaner (AMC) framework, which is a novel transferring method for face data cleaning based on meta-learning [23, 45]. AMC treats the source do-main as the meta-train set and the target domain as the meta-test set. Since the target domain is unlabeled, a graph-based unsupervised method is proposed to pseudo-label the target data inspired by some related work [35, 50, 55]. Noted that the signal-noise distribution of the pseudo label is also bi-ased, it is only used for transferring cleaning knowledge in-stead of directly used for training the cleaning model. In this way, the model learns reliable knowledge from the source domain, and gradually transfers it to the target domain.
This meta-learning-based transferring approach will raise a new problem, i.e., the drift of the decision bound-ary. The optimization target only measures the upper limit of the data distribution, which aims to predict the signals as close to 1 as possible and the noise as close to 0 as possible.
Then it is a common practice to take an empirical boundary threshold value such as 0.5 [7]. Samples with a predicted value greater than the threshold are judged positive, and the ones smaller than the threshold are judged negative. How-ever, in the transfer learning tasks, the model tries to learn decision boundary distribution that is fit for both the source and target set, but it is only expected to perform well on the target set. In this case, an empirical threshold may experi-ence drift between different domains and cannot completely describe the boundary of the target domain. To solve this problem, an adaptive threshold learning method is proposed in AMC along with the meta-learning procedure to dynam-ically adjust boundary thresholds for different classes.
To verify AMC on real data, we clean two in-the-wild noisy face datasets CASIA-WebFace [54] and Million-Celebs [56], guided by the MS-Celeb-1M [20] dataset with a high-quality signal-noise label. The effectiveness is as-sessed in terms of the comparative recognition performance of Arcface [10] trained on the cleaned datasets. Results show that AMC effectively improves the face recognition performance compared with previous cleaning methods on face verification tests and the IJB-C benchmark [33]. The subsequent discussion also specifically analyzes the reasons for the performance improvement of the proposed method.
The main contributions can be summarized as follows:
• We explore the data cleaning task from a new perspec-tive of domain gap, and provide one of the possible solutions for signals and noise distribution transferring for deep face recognition datasets, which can inspire more related discussions.
• We design the meta-learning-based AMC framework to clean label noise in face recognition datasets.
• Multiple datasets are cleaned and compared to test their performance limitations. 2.