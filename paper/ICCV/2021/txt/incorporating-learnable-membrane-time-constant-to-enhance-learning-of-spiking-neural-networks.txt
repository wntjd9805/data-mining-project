Abstract
Spiking Neural Networks (SNNs) have attracted enor-mous research interest due to temporal information pro-cessing capability, low power consumption, and high bio-logical plausibility. However, the formulation of efﬁcient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spik-ing neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neu-rons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain re-gions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the mem-brane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning.
In addition, we reevaluate the pooling methods in SNNs and ﬁnd that max-pooling will not lead to signiﬁcant in-formation loss and have the advantage of low computa-tion cost and binary compatibility. We evaluate the pro-posed method for image classiﬁcation tasks on both tradi-tional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Ges-ture datasets. The experiment results show that the pro-posed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangw ei1234 56/Parametric-Leaky-Integrate-and-Fire
-Spiking-N euron.
*Corresponding author (a) Spiking neuron (b) The membrane potential of a LIF neuron
Figure 1. (a) A Leaky Integrate-and-Fire (LIF) neuron with mem-brane potential V , membrane time constant τ , input I(t) and synaptic weight w. (b) The membrane potential V of the LIF neu-ron when constant input is received. Increasing or decreasing τ will stretch the v = f (t) curve in the t direction while increasing or decreasing w will stretch the v = f (t) curve in the V direction. 1.

Introduction
Spiking Neural Networks (SNNs) are viewed as the third generation of neural network models, which are closer to biological neurons in the brain [38]. Together with neu-ronal and synaptic states, the importance of spike timing is also considered in SNNs. Due to their distinctive prop-erties, such as temporal information processing capability, low power consumption [49], and high biological plausi-bility [16], SNNs increasingly arouse researchers’ great in-terest in recent years. Nevertheless, it remains challenging to formulate efﬁcient and high-performance learning algo-rithms for SNNs.
Generally, the learning algorithms for SNNs can be divided into unsupervised learning, supervised learning, reward-based learning, and Artiﬁcial Neural Network (ANN) to SNN conversion methodologies. Either way, we ﬁnd that most existing learning methods only con-sider learning the synaptic-related parameters like synap-tic weights and treat the membrane-related parameters as
hyperparameters. These membrane-related parameters like membrane time constants, which determine the dynamics of a single spiking neuron, are typically chosen to be the same for all neurons. Note, however, there exist different membrane time constants for spiking neurons across brain regions [39, 9, 30], which are proved to be essential for the representation of working memory and formulation of learning [20, 53]. Thus simply ignoring different time con-stants in SNNs will limit the heterogeneity of neurons and thus the expressiveness of the resulting SNNs.
In this paper, we propose a training algorithm that is capable of learning not only the synaptic weights but also membrane time constants of SNNs. As illustrated in Fig. 1, we ﬁnd that adjustments of the synaptic weight and the membrane time constants have different effects on neuronal dynamics. We show that incorporating learnable membrane time constants is able to enhance the learning of SNNs.
The main contributions of this paper can be summarized as follows: 1) We propose the backpropagation-based learning algo-rithm using spiking neurons with learnable membrane parameters, referred to as Parametric Leaky Integrate-and-Fire (PLIF) spiking neurons, which better repre-sent the heterogeneity of neurons and thereby enhanc-ing the expressiveness of the SNNs. We show that the
SNNs made of PLIF neurons are more robust to initial values and can learn faster than SNNs made of neurons with a ﬁxed time constant. 2) We reevaluate the pooling methods in SNNs and dis-credit the previous conclusion that max-pooling results in signiﬁcant information loss. We ﬁnd that compared to average-pooling, max-pooling is able to better pre-serve the asynchronous characteristic of neuron ﬁring, as well as reduce the computation cost. Our exper-iments show that the performance of max-pooling is comparable to average-pooling. 3) We evaluate our methods on both traditional static
MNIST [32], Fashion-MNIST [59], CIFAR-10 [31] datasets widely used in ANNs as benchmarks, and neuromorphic N-MNIST [44], CIFAR10-DVS [36],
DVS128 Gesture [1] datasets that focus on verifying the network’s temporal information processing capa-bility. The proposed method exceeds state-of-the-art accuracy on nearly all tested datasets, using fewer time-steps. 2.