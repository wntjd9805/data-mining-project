Abstract
Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize unsegmented signs from image streams. Overfitting is one of the most critical problems in CSLR training, and previous works show that the itera-tive training scheme can partially solve this problem while also costing more training time. In this study, we revisit the iterative training scheme in recent CSLR works and real-ize that sufficient training of the feature extractor is critical to solving the overfitting problem. Therefore, we propose a Visual Alignment Constraint (VAC) to enhance the fea-ture extractor with alignment supervision. Specifically, the proposed VAC comprises two auxiliary losses: one focuses on visual features only, and the other enforces prediction alignment between the feature extractor and the alignment module. Moreover, we propose two metrics to reflect overfit-ting by measuring the prediction inconsistency between the feature extractor and the alignment module. Experimental results on two challenging CSLR datasets show that the pro-posed VAC makes CSLR networks end-to-end trainable and achieves competitive performance. 1.

Introduction
Sign Language is a complete and natural language that conveys information through both manual components (hand/arm gestures) and non-manual components (facial expressions, head movements, and body postures) [10, 37] with its own grammar and lexicon [41]. Vision-based Con-tinuous Sign Language Recognition (CSLR) aims to auto-matically recognize signs from image streams, which can bridge the communication gap between the Deaf and hear-ing people. It also provides more non-intrusive communi-cation channel for sign language users.
Different from speech recognition, the data collection and annotation of sign language are costly, which poses a
Figure 1. Overview of the proposed non-iterative CLSR approach with the visual alignment constraint. To solve the insufficient training of the feature extractor, the proposed VAC enhances the generalization ability of the visual extractor by constraining the feature space with the alignment supervision. significant problem for recognition [2]. Therefore, most re-cent CSLR works solve this problem in a weakly supervised manner and adopt network architectures composed of the feature extractor and the alignment module. The feature ex-tractor abstracts visual information from each frame, and the alignment module searches the possible alignments be-tween visual features and the corresponding labeling. Dif-ferent to those works [27, 29, 31] adopt HMMs to update frame-wise state labels for the feature extractor, Graves et al. [15] provide a more elegant solution so-called Connec-tionist Temporal Classification (CTC) to align the predic-tion and labeling by maximizing the sum of probability of all feasible alignments, which is adopted by following works [3, 6, 8, 9, 27, 36, 46].
Although CTC-based CSLR methods provide conve-nience in training, previous studies [9, 39] show that end-to-end training limits the discriminative power of the fea-ture extractor. They leverage the iterative training scheme to enhance the feature extractor, which significantly improves the performance. Nevertheless, it requires an additional fine-tuning process besides the end-to-end training and in-creases the training time. Several recent works [6, 36] try to accelerate this training scheme by adopting fully convo-lutional networks and fine-grained labels.
In this study, we revisit CTC-based CSLR model at dif-ferent iterations and observe that only a few frames play key roles in training. The feature extractor abstracts vi-sual information and provides initial localizations of key frames for the alignment module. The alignment module further refines the recognition results from the feature ex-tractor and learns long-term relationships with its powerful temporal modeling ability. Due to the spike phenomenon of
CTC [14, 34], the alignment module converges much faster than the feature extractor on CSLR datasets with limited samples and cannot provide enough feedback to the feature extractor. The overfitting of the alignment module leads to insufficient training of the feature extractor and deteriorates the generalization ability of the trained model. The iterative training scheme tries to solve this problem by enhancing the feature extractor with iteratively refined pseudo labels.
Based on above observations, we conclude that con-straining the feature space is critical to efficiently train
CSLR models. To solve this problem, we propose a Visual
Alignment Constraint (VAC) to make CSLR networks end-to-end trainable. As shown in Fig. 1, the proposed VAC is composed of two auxiliary losses which provide extra supervision for the feature extractor. The visual enhance-ment loss enforces the feature extractor to make predictions based on visual features only and the visual alignment loss aligns the short-term visual predictions to long-term contex-tual predictions. With the combination of the two losses, the proposed method achieves competitive performance to the latest methods on PHOENIX14 [28] and CSL [23] datasets.
To better understand the performance gains, we present two metrics named Word Deterioration Rate (WDR) and
Word Amelioration Rate (WAR) to evaluate the contribu-tions of the feature extractor and the alignment module, which can also be used as indicators of overfitting. Compar-ing to the iterative training procedure, experimental results show that the proposed method can obtain a more powerful feature extractor and make better use of visual features.
The major contributions are summarized as follows:
• Revisiting the iterative training scheme in CSLR and showing that the overfitting of the alignment module leads to insufficient training of the feature extractor.
• Proposing a visual alignment constraint to make the network end-to-end trainable by enhancing the feature extractor and aligning visual and contextual features.
• Presenting two metrics to evaluate the contributions of the feature extractor and the alignment module, which verifies the effectiveness of the proposed method. 2.