Abstract
Video captioning is an important vision task and has been intensively studied in the computer vision community.
Existing methods that utilize the ﬁne-grained spatial infor-mation have achieved signiﬁcant improvements, however, they either rely on costly external object detectors or do not sufﬁciently model the spatial/temporal relations. In this pa-per, we aim at designing a spatial information extraction and aggregation method for video captioning without the need of external object detectors. For this purpose, we pro-pose a Recurrent Region Attention module to better extract diverse spatial features, and by employing Motion-Guided
Cross-frame Message Passing, our model is aware of the temporal structure and able to establish high-order rela-tions among the diverse regions across frames. They jointly encourage information communication and produce com-pact and powerful video representations. Furthermore, an
Adjusted Temporal Graph Decoder is proposed to ﬂexibly update video features and model high-order temporal rela-tions during decoding. Experimental results on three bench-mark datasets: MSVD, MSR-VTT, and VATEX demonstrate that our proposed method can outperform state-of-the-art methods. 1.

Introduction
Automatically generating sentences to describe video contents, i.e., video captioning, has been attracting re-search attention from both the computer vision and nat-ural language processing communities. From the vision perspective, extracting and fully utilizing the information contained in the video is the key of improving video cap-tioning. Recent advancements of video captioning meth-ods [6, 16, 55, 26, 50, 57, 58] can also be mainly attributed to the exploration of more ﬁne-grained spatial information within the video frames. An representative example, ORG-TRL [57] (Fig. 1 (c)), detects spatial bounding boxes of the important objects with an external object detector [32] and
∗Corresponding author.
Figure 1. Illustration of video captioning methods with different types of ﬁne-grained spatial information extraction strategies: (a)
Grid-based: e.g., MGSA [6], attends to one region per frame. (b) Region-based: our proposed method, extracts multiple regions and performs message passing to conduct relation modeling. (c)
Object-based: e.g., ORG-TRL [57], can extract multiple regions on each frame and model their relations, but are computationally less efﬁcient due to the use of object detectors. then builds an object relation graph to model the relations among all the objects. Objects together with their relations are undoubtedly crucial for video captioning, because the object interactions can be explicitly captured and lead to a better understanding of the video contents.
However, it is costly to extract localized object features by object detection and pretrained object detectors can not generalize well to animated or video game contents in some datasets [46]. The recent research of Jiang et al. [17] compares grid-based and object region-based features, and shows that incorporating object detectors into image visual question answering models can signiﬁcantly slow down the models by 4.6 to 23.8 times, while using such object fea-tures does not bring signiﬁcant advantages (in terms of ac-curacy) over using plain CNN feature maps (or grid fea-tures). They also conclude that the semantic content that features represent is more critical than the format of fea-tures. Besides, incorporating an object detector requires densely annotated external data and also increases the ﬁ-nal model size of the whole system. Inspired by [17], we revisit grid features for video-and-language models with a focus on video captioning, and preliminarily explore its ap-plication for temporal sentence localization in videos.
In fact, utilizing grid features is widely adopted by recent video captioning methods [6, 51, 21, 49, 43, 33]. They usu-ally calculate one spatial attention map for each CNN fea-ture map (Fig. 1 (a)) to capture the most salient object, then spatially aggregate each feature map into a condensed fea-ture vector. The core of these methods is emphasizing one important region1 in each video frame, and their major dif-ferences are in the ways to compute spatial attentions. But the problem with these methods is that only one region is at-tended in each frame, so some information may be missed, and there is no way to model the interactions among differ-ent regions. In this respect, the advantages of object-based methods [50, 16, 57, 55, 26, 58] are 1) the detected objects can comprehensively capture multiple regions of interest, and 2) the relation modeling among objects.
To tackle the issues existed in previous grid feature-based methods, we propose Recurrent Region Attention to extract multiple diverse regions from each video frame and design Motion Guided Cross-Frame Message Passing to encourage the interaction and information communication among regions of consecutive frames. Moreover, we pro-pose Adjusted Temporal Graph Decoder, which updates the high-order temporal relations among video features based on the decoding state to more ﬂexibly form compact video representations. So that our method possesses the two es-sential factors of object-based methods (Fig. 1 (b)). We also note that the goal of this paper is not to refute the use of object detectors or to reach a compromise between compu-tational cost and accuracy. Instead, we aim at fully utilizing the semantic content in grid features to further explore its potentials and scope of application.
The contributions are summarized as follows: (1) We proposed Recurrent Region Attention and Motion Guided
Cross-Frame Message Passing, which jointly are a new method of extracting and encoding ﬁne-grained spatial in-formation for video captioning. (2) We proposed Adjusted
Temporal Graph Decoder, which is a more ﬂexible cap-tioning decoder that can adjust video features based on high-order temporal relations. (3) We tested our proposed method on the popular MSVD and MSR-VTT datasets and the newly-released VATEX dataset, and achieved state-of-the-art video captioning performances on all datasets. 2.