Abstract
Few-shot Learning has been studied to mimic human visual capabilities and learn effective models without the need of exhaustive human annotation. Even though the idea of meta-learning for adaptation has dominated the few-shot learning methods, how to train a feature extrac-tor is still a challenge. In this paper, we focus on the design of training strategy to obtain an elemental representation such that the prototype of each novel class can be estimated from a few labeled samples. We propose a two-stage train-ing scheme, Partner-Assisted Learning (PAL), which ﬁrst trains a Partner Encoder to model pair-wise similarities and extract features serving as soft-anchors, and then trains a Main Encoder by aligning its outputs with soft-anchors while attempting to maximize classiﬁcation performance.
Two alignment constraints from logit-level and feature-level are designed individually. For each few-shot task, we per-form prototype classiﬁcation. Our method consistently out-performs the state-of-the-art methods on four benchmarks.
Detailed ablation studies of PAL are provided to justify the selection of each component involved in training. 1.

Introduction
Deep learning has achieved impressive success in many vision tasks, such as image classiﬁcation [21, 38, 17], object detection [36, 34, 37], and image segmentation [26, 3, 16], especially when sufﬁcient labeled data is available for train-ing. However, data annotation can be expensive and large scale annotated data is not always available [13, 24, 43, 49].
Few-shot learning has been proposed to mimic human vision systems, which is capable of learning the visual ap-pearance of new objects with only a few (e.g., 1 or 5) in-stances [24, 46]. To facilitate few-shot learning for fast model adaptation, meta-learning has been employed to sim-ulate few-shot tasks during training, by either designing an optimal algorithm for adaptation [10, 30] or learn-†Equal contribution.
Figure 1: (a) Prototype classiﬁcation calculates the few-shot prototypes and classiﬁes a sample by comparing its similarity to each prototype. (b)
The discriminative feature distribution with compact clusters beneﬁts the prototype classiﬁcation [47, 25]. (c) We propose a Partner-Assisted Learn-ing framework, in which a pre-trained partner encoder, fP , is used to gen-erate soft-anchors to regularize the learning of main encoder, fM , which will be used at the inference time. ing a shared feature space for prototype-based classiﬁca-tion [41, 31, 23]. As shown in Fig. 1(a), prototype classi-ﬁcation methods [6, 8, 41, 45] estimate the few-shot pro-totypes by averaging the features of a few labeled samples (i.e., support). A new sample (i.e., query) is classiﬁed by comparing its cosine similarity with all prototypes using nearest neighborhood search. As illustrated in Fig. 1(b), in a classiﬁcation context, the feature distribution is sup-posed to be (1) compact within each cluster (i.e., supporting high intra-class similarities), and (2) discriminative between clusters (i.e. supporting large inter-class distances).
Recent work has shown that pretraining a model on a large scale (base) dataset with full supervision can serve as a strong baseline [6] for the novel few-shot tasks by perform-ing prototype classiﬁcation [6, 8, 41, 45]. For each base class, conventional fully-supervised pretraining using class labels [45, 6] learns a unique weight vector, which serves as a hard-anchor. By minimizing the cross-entropy loss with respect to (w.r.t.) the class label, each image feature is pushed towards its corresponding class anchor. There-fore, for each class, the average of features is expected to represent the class during few-shot classiﬁcation.
The feature extractor pretrained on the base classes for classiﬁcation may suppress details irrelevant to the base do-main [4], while these details could be discriminative for novel classes. Thus, incorporating instance comparison to preserve details can facilitate few-shot learning on novel do-mains. Knowledge distillation formulates a teacher-student setting and compares the outputs from two models for the same image [45]. For each image, the teacher model gen-erates soft-labels to model the proximity between different classes. By comparing the outputs of teacher model and student model, the student model is trained with the soft-label so that more details indicating class relationship could be preserved. Thus, the student model achieves higher ac-curacy on few-shot tasks. Despite the success of knowl-edge distillation, the performance improvement is still lim-ited since the teacher model has once been rigidly optimized according to hard-anchors of base classes.
Besides comparing the outputs of the same instance from two networks using cosine similarity, a single network can be trained for pair-wise comparison, so that its outputs of a few randomly selected support samples can dynamically represent the class center [41]. Metric-based meta-learning methods, such as prototypical network [41], have been pro-posed to learn to represent a class by aggregating support features. This way, representative centers are dynamically estimated according to a few labeled data. Similarly, super-vised contrastive learning [19] performs pair-wise compari-son, where each feature is sampled from the training set and individually represents the class without aggregation.
Inspired by the dynamic and individual representative employed in prototypical learning and supervised con-trastive learning, to improve the generalization ability of feature extractor, we propose to extract features that can be used to dynamically represent classes, and set those features as soft-anchors to regularize the feature extractor trained with hard-anchors. Comparing with knowledge distillation, instead of aiming to iterate the feature extractor that has al-ready been optimized w.r.t hard-anchors, our method uses diverse features on the base domain to regularize a new fea-ture extractor which is trained with class label under cross-entropy loss from scratch. The contributions of this paper are as follows:
• We propose Partner-Assisted Learning (PAL): a frame-work for representation learning in few-shot classiﬁca-tion setting, in which the Partner Encoder and Main
Encoder are trained sequentially such that the features from Partner Encoder are used as soft-anchors to reg-ularize the training of Main Encoder from scratch.
• We propose two alignment approaches on both feature-level and logit-level, which utilizes the soft-anchors for regularization during training with class labels.
• We show that PAL consistently achieves state-of-the-art performance on four few-shot benchmarks, and improves the classiﬁcation accuracy in a supervised learning setting. We also provide comprehensive ab-lation studies to justify the design of each component. 2.