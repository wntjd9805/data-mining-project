Abstract
We consider the problem of ﬁlling in missing spatio-temporal regions of a video. We provide a novel ﬂow-based solution by introducing a generative model of images in re-lation to the scene (without missing regions) and mappings from the scene to images. We use the model to jointly in-fer the scene template, a 2D representation of the scene, and the mappings. This ensures consistency of the frame-to-frame ﬂows generated to the underlying scene, reducing ge-ometric distortions in ﬂow based inpainting. The template is mapped to the missing regions in the video by a new (L2-L1) interpolation scheme, creating crisp inpaintings and reduc-ing common blur and distortion artifacts. We show on two benchmark datasets that our approach out-performs state-of-the-art quantitatively and in user studies.1 1.

Introduction
Video inpainting is the problem of ﬁlling spatial-temporal regions, i.e., masked regions, with content that naturally blends with the remaining parts of the video. This is useful in video editing tasks, including removing water-marks or unwanted objects and video restoration. As videos exhibit temporal regularity, to inpaint a given frame, it is natural to use data from other frames, as the data in other frames may correspond to parts of the scene behind the masked region. Many state-of-the-art methods for video in-painting are ﬂow-guided [7, 29, 19, 5], which take the ap-proach of copying unmasked data from other frames into the masked region of a given frame by using optical ﬂow.
While these approaches inpaint with plausible data from the scene through other frames, unlike single image inpaint-ing methods (e.g. [3, 35]) that attempt to halluncinate image values in the masked region from other regions in the image or learned through datasets, they are highly dependent on the quality of the optical ﬂow. Even though optical ﬂow has advanced signiﬁcantly with the progress of deep learning to the point that recent methods on benchmark optical ﬂow 1Dataset and code: https://github.com/donglao/videoinpainting
Figure 1: Comparison with state-of-the-art. Our ap-proach uses a generative model of formation of images from the scene to infer ﬂows that are consistent with the scene.
This reduces visual distortions in inpainting compared with state-of-the-art: DFG [29], STTN [37], OnionPeel [19],
FGVC [5]. Animation in the supplementary materials. datasets produce only small errors, there are two complica-tions in applying optical ﬂow to inpainting. First, the ﬂow that is needed in the masked region for inpainting is the ﬂow had the contents within the masked region been removed to reveal the part of the scene that the mask occludes. As this is not possible to determine directly, it is hallucinated, typ-ically learned through data [29, 5]. However, there is no guarantee that this is consistent with the scene or halluci-nations from other frames, producing visual distortions in inpainting. Secondly, even small errors in optical ﬂow can produce noticeable visual distortions in the inpainted video, which is further ampliﬁed as ﬂow is aggregated over mul-tiple frames as data from far away frames may be needed.
Attempts have been made to reduce these errors by apply-ing temporal regularity to the ﬂow [29, 38, 37, 5], but these naive regularizers (ﬂow between frames is close) may not be consistent with the scene geometry and can still produce visual distortions (Figure 1).
In this paper, we aim to reduce visual distortions in video inpainting due to physically implausible and temporally in-consistent ﬂows by deriving a generative model that closely models the physical image formation process in generat-ing images from the scene, and using it to infer ﬂow. This model represents the 3D scene (all of the scene correspond-ing to the video outside the masked regions) as a 2D scene template. The inpainted image at frame t is a mapping (gen-eral piecewise smooth warping) of the part of the scene tem-plate in view in frame t to the image domain. The model constrains the warps to be consistent with the scene and the images. This induces temporal consistency by naturally and efﬁciently enforcing that all pairwise mappings between im-ages generated from the model must correctly match im-ages. This reduces implausible ﬂow hallucinations common in current approaches. Our inference procedure computes the warps and the scene template jointly from this model, which reduces distortions in inpainting. Our contributions are speciﬁcally: 1. We solve ﬂow-guided inpainting by applying a genera-tive model of the scene to images, and using it to infer ﬂow and a model of the scene (the scene template). This gives more temporally consistent and plausible pair-wise ﬂows compared with existing methods, which results in inpaint-ing results that are more temporally consistent and have less geometric distortions. 2. We propose a novel L2-L1 com-bined optimization procedure that generates the inpainting from the scene template together with a interpolation strat-egy that signiﬁcantly improves inpainting quality and fur-ther reduces geometric distortions and blurring artifacts. 3.
We introduce the ﬁrst benchmark dataset (Foreground Re-moval) on removing occluding objects from video. We in-troduce a quantitative protocol while previous art relies on visual comparisons. 4. We demonstrate the advantage of our algorithm on the DAVIS [21] and Foreground Removal datasets and demonstrate superior results (both through user studies and quantitatively) compared to state-of-the-art. 2.