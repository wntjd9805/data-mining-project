Abstract
Existing deep learning-based approaches for monocular 3D object detection in autonomous driving often model the object as a rotated 3D cuboid while the object’s geomet-ric shape has been ignored. In this work, we propose an approach for incorporating the shape-aware 2D/3D con-straints into the 3D detection framework. Speciﬁcally, we employ the deep neural network to learn distinguished 2D keypoints in the 2D image domain and regress their cor-responding 3D coordinates in the local 3D object coordi-nate ﬁrst. Then the 2D/3D geometric constraints are built by these correspondences for each object to boost the de-tection performance. For generating the ground truth of 2D/3D keypoints, an automatic model-ﬁtting approach has been proposed by ﬁtting the deformed 3D object model and the object mask in the 2D image. The proposed frame-work has been veriﬁed on the public KITTI dataset and the experimental results demonstrate that by using addi-tional geometrical constraints the detection performance has been signiﬁcantly improved as compared to the base-line method. More importantly, the proposed framework achieves state-of-the-art performance with real time. Data and code will be available at https://github.com/ zongdai/AutoShape 1.

Introduction
Perceiving 3D shapes and poses of surrounding obsta-cles is an essential task in autonomous driving (AD) per-ception systems. The accuracy and speed performance of 3D objection detection is important for the following mo-tion planning and control modules in AD. Many 3D ob-ject detectors [50, 14] have been proposed, mainly for depth sensors such as LiDAR [35, 45] or stereo cameras [46, 18], which can provide the distance information of the environ-ments directly. However, LiDAR sensors are expensive and stereo rigs suffer from on-line calibration issues. There-*Corresponding author
Figure 1: (a): 3D Bbox corners and center are commonly used for monocular 3D object detection. However, the rich structure information from 3D vehicle shapes and their projection on 2D images are not employed. (b) shows our shape-aware constraints constructed from an aligned 3D model. Such 2D-3D keypoints carry more semantic and geometric information and enable to con-struct stronger geometric constraints for monocular 3D detection. fore, monocular camera based 3D object detection becomes a promising direction.
The main challenge for monocular-based approaches is to obtain accurate depth information. In general, depth es-timation from a single image without any prior information is a challenging problem and recent many deep learning-based approaches achieve good results [7]. With the esti-mated depth map, pseudo LiDAR point cloud can be re-constructed via pre-calibrated intrinsic camera parameters and 3D detectors designed for LiDAR point cloud can be applied directly on pseudo LiDAR point cloud [33] [37].
Furthermore, [33] integrates the depth estimation and 3D object detection network together following an end-to-end manner. However, heavy computation burden is one main bottleneck of such two-stage approaches.
To improve the efﬁciency, many direct regression-based approaches have been proposed (e.g., SMOKE [24],
RTM3D [19]) and achieved promising results. By repre-senting the object as one center point, the object detec-tion task is formulated as keypoints detection and its cor-responding attributes (e.g., size, offsets, orientation, depth,
etc.) regression. With this compact representation, the com-putation speed of this kind of approach can reach 20∼30 fps (frame per second). However, the drawback is also ob-vious. One center point [48, 24] representation ignores the detailed shape of the object and results in location ambi-guity if its projected center point is on another object’s sur-face due to occlusion [47]. To alleviate this ambiguity, other geometrical constraints have been used to improve the per-formance. RTM3D [19] adds 8 more keypoints as addi-tional constraints which are deﬁned as the projected 2d lo-cation of the 3D bounding box’s corners. However, these keypoints have non-real context meanings and their 2D lo-cations vary differently with the changing of the camera view-point, even the object’s orientation. As shown in the left of Fig. 1, some keypoints are on the ground and some are on the sky or trees. This makes the keypoints detection network extremely difﬁcult to distinguish the keypoints or other image pixels.
In this paper, we propose a novel approach to learn the meaningful keypoints on the object surface and then use them as additional geometrical constraints for 3D object de-tection. Speciﬁcally, we design an automatic deformable model-ﬁtting pipeline ﬁrst to generate the 2D/3D corre-spondences for each object. Then, the center point plus sev-eral distinguished keypoints are learned from the deep neu-ral network. Based on these keypoints and other regressed objects’ attributes (e.g., orientation angle, object dimension etc.), the object’s 3D bounding box can be solved with lin-ear equations. The proposed framework can be trained in an end-to-end manner. Our contributions include: 1. We propose a shape-aware 3d object detection frame-work, which employs keypoints geometry constraints for 2D/3D regression to boost the detection performance. 2. We present a method for automatically ﬁtting the 3D shape to the visual observations and then generating ground-truth annotations of 2D/3D keypoints pairs for the training network. Our source code and dataset will be made public for the community. 3. The effectiveness of our approach has been veriﬁed on the public KITTI dataset and achieved SOTA per-formance. More importantly, the proposed framework achieves real-time (25 fps), which can be integrated into the AD perception module. 2.