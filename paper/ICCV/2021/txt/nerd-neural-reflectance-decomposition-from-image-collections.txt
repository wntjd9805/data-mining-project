Abstract 1.

Introduction
Decomposing a scene into its shape, reflectance, and il-lumination is a challenging but important problem in com-puter vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an uncon-strained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these tech-niques only enable view synthesis and not relighting. Ad-ditionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decompo-sition (NeRD) technique that uses physically-based render-ing to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to con-vert the learned reflectance volume into a relightable tex-tured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the pro-posed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality re-lightable 3D assets from image collections. The datasets and code are available at the project page: https:// markboss.me/publication/2021-nerd/.
Capturing the geometry and material properties of an object is essential for several computer vision and graph-ics applications such as view synthesis [10, 54], relight-ing [5, 10, 21, 22, 30, 55], object insertion [7, 20, 30] etc. This problem is often referred to as inverse render-ing [24, 41], where shape and material properties are esti-mated from a set of images, e.g., representing the surface properties as spatially-varying Bidirectional Reflectance
Distribution functions (SVBRDF) [38].
Modeled according to physics, the reflected color ob-served by a viewer is the integral of the product of SVBRDF and the incoming illumination over the hemisphere around that surface’s normal [23]. Disentangling this integral and estimating shape, illumination, and SVBRDF from images is a highly ill-posed and underconstrained inverse problem.
For instance, an image region may appear dark either due to a dark surface color (material), the absence of incident light at that surface (illumination), or due to the normal of that surface facing away from the incident light (shape).
Traditional SVBRDF estimation techniques involve cap-turing images using a light-stage setup where the light di-rection and camera view settings are controlled [4, 9, 26, 27, 28]. More recent approaches for SVBRDF estimation employ more practical capture setups [6, 7, 8, 10, 19, 37], 1
but limit the illumination to a single dominant source (e.g., a flash attached to a camera). Assuming known illumina-tion or constraining its complexity significantly reduces the ambiguity of shape and material estimation and limits the practical utility to laboratory settings or to flash photogra-phy in dark environments.
In contrast to standard SVBRDF and shape estima-tion techniques, recently introduced coordinate-based scene representation networks such as Neural Radiance Fields (NeRF) [34, 36, 60] can directly perform high-quality view synthesis without explicitly estimating shape or SVBRDF.
They represent the radiance field of the scene using a neu-ral network trained specifically for a single scene, using as input multiple images of that scene. These neural net-works directly encode the geometry and appearance as vol-umetric density and color functions parameterized by 3D coordinates of query points in the scene. Realistic novel views can be generated by raymarching through the vol-ume. Though these approaches are capable of reproducing view-dependent appearance effects, the radiance of a point in a direction is “baked in” to these networks, making them unusable for relighting. Even if such techniques could be extended to relighting, the rendering speed of these meth-ods limits their practicality — the time required by NeRF to generate a single view is about 30 seconds [36].
This work presents a shape and SVBRDF estimation technique that allows for a more flexible capture set-ting while enabling relighting under novel illuminations.
Our key technique is an explicit decomposition model for shape, reflectance, and illumination within a NeRF-like coordinate-based neural representation framework [36].
Compared to NeRF, our volumetric geometry representa-tion stores SVBRDF parameters at each 3D point instead of radiance. Each image is then differentiably rendered with a jointly optimized spherical Gaussian illumination model (see Figure 1). Shape, BRDF parameters, and illumination are all optimized simultaneously to minimize the photomet-ric rendering loss w.r.t. each input image. We call our ap-proach “Neural Reflectance Decomposition” (NeRD)
NeRD not only enables simultaneous relighting and view synthesis but also allows for a more flexible range of image acquisition settings: Input images of the object need not be captured under the same illumination conditions. NeRD supports both camera motion around an object as well as captures of rotating objects. All NeRD requires as input is a set of images of an object with known camera pose (com-puted for e.g. using COLMAP [43, 44]), where each image is accompanied by a foreground segmentation mask. Be-sides the SVBRDF and shape parameters, we also explicitly optimize the illumination corresponding to each image for varying illuminations or globally for static illumination.
As a post-processing step, we propose a way to ex-tract a 3D surface mesh along with SVBRDF parameters as textures from the learned coordinate-based representation network. This allows for a highly flexible representation for downstream tasks such as real-time rendering of novel views, relighting, 3D asset generation, etc. 2.