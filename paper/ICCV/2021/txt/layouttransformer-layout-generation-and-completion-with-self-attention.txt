Abstract
We address the problem of scene layout generation for diverse domains such as images, mobile applications, doc-uments, and 3D objects. Most complex scenes, natural or human-designed, can be expressed as a meaningful ar-rangement of simpler compositional graphical primitives.
Generating a new layout or extending an existing layout re-quires understanding the relationships between these prim-itives. To do this, we propose LayoutTransformer, a novel framework that leverages self-attention to learn contextual relationships between layout elements and generate novel layouts in a given domain. Our framework allows us to generate a new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout. Further-more, our analyses show that the model is able to au-tomatically capture the semantic properties of the primi-tives. We propose simple improvements in both represen-tation of layout primitives, as well as training methods to demonstrate competitive performance in very diverse data domains such as object bounding boxes in natural images (COCO bounding box), documents (PubLayNet), mobile applications (RICO dataset) as well as 3D shapes (Part-Net). Code and other materials will be made available at https://kampta.github.io/layout. 1.

Introduction
In the real world, there exists a strong relationship be-tween different objects that are found in the same environ-ment [43, 47]. For example, a dining table usually has chairs around it, a surfboard is found near the sea, horses do not ride cars etc. [3] provided strong evidence in cogni-tive neuroscience that perceiving and understanding a scene involves two related processes: perception and comprehen-sion. Perception deals with processing the visual signal
⋆ Corresponding authors.
Work started during an internship at Amazon.
Figure 1: Our framework can synthesize layouts in diverse natural as well as human designed data domains such as documents, mo-bile app wireframes, natural scenes or 3D objects in a sequential manner. or the appearance of a scene. Comprehension deals with understanding the schema of a scene, where this schema (or layout) can be characterized by contextual relationships (e.g., support, occlusion, and relative likelihood, position, and size) between objects. For generative models that syn-thesize scenes, this evidence underpins the importance of two factors that contribute to the realism or plausibility of a generated scene: layout, i.e., arrangement of different ob-jects, and their appearance (in terms of pixels). Generating a realistic scene necessitates both the factors to be plausible.
The advancements in the generative models for image synthesis have primarily targeted plausibility of the appear-ance signal by generating incredibly realistic images often with a single entity such as faces [23, 24], or animals [4, 61].
In the case of large and complex scenes, with strong non-local relationships between different elements, most meth-ods require proxy representations for layouts to be provided as inputs (e.g., scene graph, segmentation mask, sentence).
We argue that to plausibly generate large scenes without such proxies, it is necessary to understand and generate the layout of a scene, in terms of contextual relationships be-tween various objects present in the scene.
Learning to generate layouts is useful for several stand-alone applications that require generating layouts or tem-plates with/without user interaction. For instance, in the UI design of mobile apps and websites, an automated model for generating plausible layouts can significantly decrease the manual effort and cost of building such apps and websites.
Finally, a model to create layouts can potentially help gen-erate synthetic data for various tasks tasks [5, 6, 54, 55, 59].
Fig. 1 shows some of the layouts autoregressively generated by our approach in various domains such as documents, mo-bile apps, natural scenes, and 3D shapes.
Formally, a scene layout can be represented as an un-ordered set of graphical primitives. The primitive itself can be discrete or continuous depending on the data domain.
For example, in the case of layout of documents, primitives can be bounding boxes from discrete classes such as ‘text’,
‘image’, or ‘caption’, and in case of 3D objects, primitives can be 3D occupancy grids of parts of the object such as
‘arm’, ‘leg’, or ‘back’ in case of chairs. Additionally, in or-der to make the primitives compositional, we represent each primitive by a location vector with respect to the origin, and a scale vector that defines the bounding box enclosing the primitive. Again, based on the domain, these location and scale vectors can be 2D or 3D. A generative model for lay-outs should be able to look at all existing primitives and propose the placement and attributes of a new one. We pro-pose a novel framework LayoutTransformer that first maps the different parameters of the primitive independently to a fixed-length continuous latent vector, followed by a masked
Transformer decoder to look at representations of existing primitives in layout and predict the next primitive (one pa-rameter at a time). Our generative framework can start from an empty set, or a set of primitives, and can iteratively gen-erate a new primitive one parameter at a time. Moreover, by predicting either to stop or to generate the next primi-tive, our approach can generate variable length layouts. Our main contributions can be summarized as follows:
• We propose LayoutTransformer a simple yet power-ful auto-regressive model that can synthesize new lay-outs, complete partial layouts, and compute likelihood of existing layouts. Self-attention approach allows us to visualize what existing elements are important for generating the next category in the sequence.
• We model different attributes of layout elements sepa-rately - doing so allows the attention module to more easily focus on the attributes that matter. This is im-portant especially in datasets with inherent symmetries such as documents or apps and in contrast with exist-ing approaches which concatenate or fuse different at-tributes of layout primitives.
• We present an exciting finding – encouraging a model to understand layouts results in feature representations that capture the semantic relationships between objects automatically (without explicitly using semantic em-beddings, like word2vec [35]). This demonstrates the utility of the task of layout generation as a proxy-task for learning semantic representations,
• LayoutTransformer shows good performance with es-sentially the same architecture and hyperpameters across very diverse domains. We show the adaptabil-ity of our model on four layout datasets: MNIST Lay-out [29], Rico Mobile App Wireframes [9], PubLayNet
Documents [65], and COCO Bounding Boxes [32]. To the best of our knowledge, MMA is the first frame-work to perform competitively with the state-of-the-art approaches in 4 diverse data domains. 2.