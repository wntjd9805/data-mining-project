Abstract
Appearance and motion are two important sources of information in video object segmentation (VOS). Previous methods mainly focus on using simplex solutions, lower-ing the upper bound of feature collaboration among and across these two cues. In this paper, we study a novel frame-work, termed the FSNet (Full-duplex Strategy Network), which designs a relational cross-attention module (RCAM) to achieve the bidirectional message propagation across embedding subspaces. Furthermore, the bidirectional pu-rification module (BPM) is introduced to update the incon-sistent features between the spatial-temporal embeddings, effectively improving the model robustness. By considering the mutual restraint within the full-duplex strategy, our FS-Net performs the cross-modal feature-passing (i.e., trans-mission and receiving) simultaneously before the fusion and decoding stage, making it robust to various challeng-ing scenarios (e.g., motion blur, occlusion) in VOS. Exten-sive experiments on five popular benchmarks (i.e., DAVIS16,
FBMS, MCL, SegTrack-V2, and DAVSOD19) show that our
FSNet outperforms other state-of-the-arts for both the VOS and video salient object detection tasks. 1.

Introduction
Video object segmentation (VOS) [12, 32, 101, 104] is a fundamental topic in computer vision for intelligent video analysis, whose purpose is to delineate pixel-level mov-ing object1 masks in each frame.
It has been widely ap-plied to robotic manipulation [1], autonomous cars [58], video editing [34], medicine [36], optical flow estima-tion [18], interactive segmentation [9, 29, 60], URVOS [75], and video captioning [65]. There are two settings for ad-dressing this task (i.e., semi-supervised [95] and unsuper-vised [59] VOS), depending on whether or not the candi-In this date object is given manually in the first frame.
*Corresponding author: Deng-Ping Fan (dengpfan@gmail.com). Work was done while Ge-Peng Ji was an intern mentored by Deng-Ping Fan. 1We use ‘foreground object’ & ‘target object’ interchangeably.
Figure 1: Visual comparison between the simplex (i.e., (a) appearance-refined motion and (b) motion-refined appear-In contrast, our FS-ance) and our full-duplex strategy.
Net offers a collaborative way to leverage the appearance and motion cues under the mutual restraint of full-duplex strategy, thus providing more accurate structure details and alleviating the short-term feature drifting issue [117]. work, we focus on the unsupervised setting, i.e., zero-shot
VOS [126,127]. For semi-supervised VOS, we refer readers to prior works [5, 8, 43, 53, 73, 76, 114, 116, 120, 122].
Recent years have witnessed promising progress of ad-dressing video content understanding by exploiting appear-ance (e.g., color frame [119]) and motion (e.g., optical flow [33, 83] and pixel trajectory [78]) correlation between frames. However, short-term dependency estimation (i.e., one-step motion cues [33, 83]) produces unreliable results and suffers the common ordeals [30] (e.g., diffusion, noise, and deformation), while the capability of appearance-based modeling (e.g., recurrent neural network (RNN) [59, 85]) is severely hindered by blurred foregrounds or cluttered back-grounds [14]. Those conflicts are prone to accumulating inaccuracies with the propagation of spatial-temporal em-beddings, which cause short-term feature drifting [117].
Earlier solutions address this issue using direction-independent strategy [16, 35, 38, 85, 108], which would be to encode the appearance and motion features individually and fuse them directly. However, this implicit strategy will cause feature conflicts, since motion and appearance are two distinctive modalities, extracted from separate branches. A reasonable idea is to integrate them in a guided manner, and
F
J
Figure 2: Mean contour accuracy (F) vs. mean region sim-ilarity (J ) scores on DAVIS16 [71]. Circles indicate UVOS methods. Four variants of our FSNet are shown in bold-italic, in which ‘N ’ indicates the number of BPM. Com-pared with the best unsupervised VOS model (MAT [127] with CRF [42] post-processing), the proposed method FS-Net (N =4, CRF) achieves the new SOTA by a large margin. thus, several recent approaches opt for the simplex strat-egy [30, 50, 54, 62, 68, 88, 127], which is either appearance-based or motion-guided. Although these two strategies have achieved remarkable advances, they both fail to infer the mutual restraint between the appearance and motion cues that both guide human visual attention allocation during dy-namic observation, according to previous studies in cogni-tive psychology [40, 87, 105] and computer vision [35, 93].
For the same object, we argue that appearance and mo-tion characteristics should be homogeneous to a certain de-gree. Intuitively, as shown in Fig. 1, the foreground region of appearance (top-left) and motion (bottom-left) maps in-trinsically share the correlative patterns about perceptions, including semantic structure, movement posture. However, misguided knowledge in the individual modality, e.g., static spectators at the bullring and dynamic watermark on TV (blue boxes), will produce inaccuracies during the feature propagation, and thus, it easily stains the result (red boxes).
To alleviate the above conflicts, it is important to intro-duce a new modality transmission scheme, instead of em-bedding them individually. Inspired by this, we introduce the idea of full-duplex2 from the field of wireless commu-nication. As shown in Fig. 4 (c) & Fig. 5 (c), this is a bidirectional-attention scheme across motion and appear-ance cues, which explicitly incorporates the appearance and motion patterns in a unified framework. As can be seen in the first row of Fig. 1, the proposed Full-duplex Strategy 2On the same channel, information can be transmitted and received simultaneously [4].
Network (FSNet) visually performs better than the one with simplex strategy. To understand what enables good learning strategies, we comprehensively delve into the simplex and full-duplex strategies of our framework and present the fol-lowing contributions:
• We emphasize the importance of the full-duplex strat-egy for the spatial-temporal representations. Specifi-cally, a bidirectional interaction module, termed the rela-tional cross-attention module (RCAM), is used to extract discriminative features from the appearance and motion branches, which ensures the mutual restraint between each other.
• To further improve the model robustness, we intro-duce a bidirectional purification module (BPM), which is equipped with an interlaced decremental connection (IDC) to automatically update inconsistent features be-tween the spatial-temporal embeddings.
• We demonstrate that our FSNet performs superior per-formance on five mainstream benchmarks, especially for
FSNet (N =4, CRF) outperforms the SOTA UVOS model (i.e., MAT [127]) on the DAVIS16 [71] leaderboard by a margin of 2.4% in terms of F score (see Fig. 2), with less training data (i.e., Ours-13K vs. MAT-16K). This sug-gests that the mutual restraints within full-duplex strat-egy is promising for the spatial-temporal learning tasks. 2.