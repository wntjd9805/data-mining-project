Abstract
Interaction and navigation deﬁned by natural language instructions in dynamic environments pose signiﬁcant chal-lenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose
Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of vi-sual observations and actions. To improve training, we leverage synthetic instructions as an intermediate represen-tation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instruc-tions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED bench-mark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits. 1.

Introduction
Having an autonomous agent performing various house-hold tasks is a long-standing goal of the research com-munity. To benchmark research progress, several simu-lated environments [3, 53, 56] have recently emerged where the agents navigate and interact with the environment fol-lowing natural language instructions. Solving the vision-and-language navigation (VLN) task requires the agent to ground human instructions in its embodied perception and
In practice, the agent is often required to action space. perform long compositional tasks while observing only a small fraction of the environment from an egocentric point of view. Demonstrations manually annotated with human instructions are commonly used to teach an agent to accom-plish speciﬁed tasks.
This paper attempts to address two main challenges of
VLN: (1) handling highly compositional tasks consisting of many subtasks and actions; (2) understanding the complex human instructions that are used to specify a task. Figure 1
*Work done as an intern at Google Research.
Figure 1: An example of a compositional task in the ALFRED dataset [56] where the agent is asked to bring two vases to a cab-inet. We show several frames from an expert demonstration with corresponding step-by-step instructions. The instructions expect the agent to be able to navigate to a ﬁreplace which is not visible in its current egocentric view and to remember its previous loca-tion by referring to it as ”where you were standing previously”. shows an example task that illustrates both challenges. We show six key steps from a demonstration of 53 actions. To fulﬁll the task, the agent is expected to remember the loca-tion of a ﬁreplace at t = 0 and use this knowledge much later (at t = 31). It also needs to solve object- (e.g. “an-other vase”) and location-grounded (e.g. “where you were standing previously”) coreference resolution in order to un-derstand the human instructions.
Addressing the ﬁrst challenge requires the agent to re-member its past actions and observations. Most recent VLN approaches rely on recurrent architectures [39, 60, 68, 73] where the internal state is expected to keep information about previous actions and observations. However, the re-current networks are known to be inefﬁcient in capturing long-term dependencies [66] and may fail to execute long action sequences [25, 56]. Motivated by the success of the attention-based transformer architecture [65] at language understanding [9, 17] and multimodal learning [18, 59], we propose to use a transformer encoder to combine multi-modal inputs including camera observations, language in-structions, and previous actions. The transformer encoder has access to the history of the entire episode to allow long-term memory and outputs the action to take next. We name our proposed architecture Episodic Transformer (E.T.).
Addressing the second challenge requires revisiting dif-ferent ways to specify a task for the autonomous agent. We observe that domain-speciﬁc language [22] and temporal logic [24, 43] can unambiguously specify the target states and (optionally) their temporal dependencies, while being decoupled from the visual appearance of a certain environ-ment and the variations of human instructions. We hypoth-esize that using these synthetic instructions as an intermedi-ate interface between the human and the agent would help the model to learn more easily and generalize better. To this end, we propose to pretrain the transformer-based language encoder in E.T. by predicting the synthetic instructions from human instructions. We also explore joint training, where human instructions and synthetic instructions are mapped into a shared latent space.
To evaluate the performance of E.T., we use the AL-FRED dataset [56] which consists of longer episodes than the other vision-and-language navigation datasets [3,13,53] and also requires object interaction. We experimentally show that E.T. beneﬁts from full episode memory and is bet-ter at solving tasks with long horizons than recurrent mod-els. We also observe signiﬁcant gains by pretraining the language encoder with the synthetic instructions. Further-more, we show that when used for training jointly with nat-ural language such intermediate representations outperform conventional data augmentation techniques for vision-and-language navigation [20] and work better than image-based annotations [37].
In summary, our two main contributions are as follows.
First, we propose Episodic Transformer (E.T.), an attention-based architecture for vision-and-language navigation, and demonstrate its advantages over recurrent models. Second, we propose to use synthetic instructions as the intermediate interface between the human and the agent. Both contribu-tions combined allow us to achieve a new state-of-the-art on the challenging ALFRED dataset.
Code and models are available on the project page1. 2.