Abstract
Motivated by the success of Transformers in natural lan-guage processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervi-sion to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrow-ing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies.
Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw in-put images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.
Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generalization abil-ity of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of train-ing data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training it-erations, which can reduce the training cost significantly 1. 1.

Introduction
Transformers [37] have become the de-facto standard for natural language processing (NLP) tasks due to their abili-ties to model long-range dependencies and to train in par-allel. Recently, there exist some attempts to apply Trans-1Code is available at: https://github.com/coeusguo/ceit
Figure 1: The fast convergence ability of CeiT models.
CeiT models trained with 100 epochs obtain comparable results with DeiT models trained with 300 epochs. Other settings are given in Table 8. formers to vision domains [7, 10, 35, 5, 52, 6, 50], leading promising results in different tasks. Among them, Vision
Transformer (ViT) [10] is the first pure Transformer archi-tecture that is directly inherited from NLP, and applied to image classification. It obtains promising results compared to many state-of-the-art CNNs [25, 43, 19]. But it relies heavily on the large amount of dataset of JFT-300M [33], which limits the application in the scenarios with limited computing resources or labeled training data. To alleviate the dependence on a large amount of data, the Data-efficient image Transformers (DeiT) [35] introduce a CNN model as a teacher and applies knowledge distillation [14] to improve the student model of ViT. Thus DeiT that is only trained on
ImageNet can obtain satisfactory results. But the require-ment of trained high-performance CNN models is a po-tential computation burden. Besides, the choice of teacher models, distillation types may affect the final performance.
Therefore, we intend to design a new visual Transformer that can overcome these limitations.
Some existing observations in these work can help us de-sign desired architectures. In ViT, Transformer-based mod-els underperform CNNs in the realm of ∼10M training sam-ples. It claims that “Transformers lack some of the inductive
biases inherent to CNNs, and therefore do not generalize well when trained on insufficient data”. In DeiT, a CNN teacher gives better performance than using a Transformer one, which probably due to “the inductive bias inherited by the Transformer through distillation”. These observations make us rethink whether it is appropriate to remove all con-volutions from the Transformer. And should the inductive biases inherited in the convolution be forgotten?
Looking back to the convolution, the main characteris-tics are translation invariance and locality [22, 31]. Trans-lation invariance is relevant to the weight sharing mecha-nism, which can capture information about the geometry and topology in vision tasks [23]. For the locality, it is a common assumption in visual tasks [11, 26, 9] that neigh-boring pixels always tend to be correlated. However, pure
Transformer architectures do not fully utilize these prior bi-ases that existed in images. First, ViT performs direct to-kenization of patches from the raw input image with a size of 16 × 16 or 32 × 32. It is difficult to extract the low-level features which form some fundamental structures in images (e.g. corners and edges). Second, the self-attention modules concentrate on building long-range dependencies among to-kens, ignoring the locality in the spatial dimension.
To address these problems, we design a Convolution-enhanced image Transformer (CeiT) to combine the advan-tages of CNNs in extracting low-level features, strengthen-ing locality, and the advantages of Transformers in asso-ciating long-range dependencies. Three modifications are made compared to the vanilla ViT. To solve the first prob-lem, instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) mod-ule that extracts patches from generated low-level features, where patches are in a smaller size and then flattened into a sequence of tokens. Due to a well-designed structure, the
I2T module does not introduce more computation costs. To solve the second problem, the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension. To exploit the ability of self-attention, a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations to improve the final repre-sentation. In summary, our contributions are as follows:
• We design a new visual Transformer architecture namely Convolution-enhanced image Transformer (CeiT). It combines the advantages of convolutional neural networks in extracting low-level features, strengthening locality, and the advantages of Trans-formers in establishing long-range dependencies.
• Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generaliza-tion ability of CeiT compared with previous Trans-formers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teach-ers. For example, with a similar model size as ResNet-50, CeiT-S obtains a Top-1 accuracy of 82.0% on Im-ageNet. And the result boosts into 83.3% when fine-tuned in the resolution of 384 × 384.
• CeiT models demonstrate better convergence than pure
Transformer models with 3× fewer training iterations, which can reduce the training cost significantly. 2.