Abstract
The state-of-the-art object detection and image classiﬁ-cation methods can perform impressively on more than 9k classes. In contrast, the number of classes in semantic seg-mentation datasets is relatively limited. This is not surpris-ing when the restrictions caused by the lack of labeled data and high computation demand for segmentation are consid-ered. In this paper, we propose a novel training methodol-ogy to train and scale the existing semantic segmentation models for a large number of semantic classes without in-In our embedding-based creasing the memory overhead. scalable segmentation approach, we reduce the space com-plexity of the segmentation model’s output from O(C) to
O(1), propose an approximation method for ground-truth class probability, and use it to compute cross-entropy loss.
The proposed approach is general and can be adopted by any state-of-the-art segmentation model to gracefully scale it for any number of semantic classes with only one GPU.
Our approach achieves similar, and in some cases, even better mIoU for Cityscapes, Pascal VOC, ADE20k, COCO-Stuff10k datasets when adopted to DeeplabV3+ model with different backbones. We demonstrate a clear beneﬁt of our approach on a dataset with 1284 classes, bootstrapped from
LVIS and COCO annotations, with almost three times better mIoU than the DeeplabV3+. Our source code is available at: https://github.com/shipra25jain/ESSNet. 1 .

Introduction
With the advent of deep learning, signiﬁcant progress has been made in various image understanding tasks, including image classiﬁcation, object detection, and image segmenta-tion. The state-of-the-art methods can impressively classify images into 10k classes [15] and detect 9k different objects
[49]. In contrast, segmentation models have been trained for a fairly limited number of common classes. The ability to segment a greater variety of objects, including small and rare object classes, is critical to many real-life applications
Figure 1. The left y-axis shows the maximum batch size that can
ﬁt in a single GPU for DeepLabV3+ model vs number of classes in the dataset. The right y-axis with markers in yellow and green color shows pixel accuracy for our model and baseline for follow-ing datasets (number of classes): Cityscapes (19), ADE20k (150),
COCO-Stuff10k (182) and COCO+LVIS (1284). like autonomous driving [2] and the scene exploration [7].
The scaling of existing segmentation models has several un-resolved challenges. One of the challenges is the unbal-anced distribution of classes. As mentioned in [21], due to the Zipﬁan distribution of classes in natural settings, there is a long tail of rare and small object classes that do not have a sufﬁcient number of examples to train the model. The lack of segmentation datasets with a multitude of classes also limits us to develop scalable segmentation models. In fact, one can also argue from the other side. The reason for lim-ited classes in existing segmentation datasets is the discour-aging computational demand, alongside the labor-intensive annotations.
The task of semantic segmentation is essentially a pixel-level classiﬁcation of an image. Typically, it is performed by predicting an output tensor of H × W × C for image size H × W and C number of semantic classes [36]. This is desirable during the pixel-wise classiﬁcation by employ-ing cross-entropy loss on the C-dimensional predictions.
Unfortunately, the memory demand for such predictions
happens to be a major bottleneck for a large number of classes. Figure 1 also illustrates an example case: the max-imum adjustable batch size of 512×512 versus the number of classes, in one standard GPU (Titan XP) while training the DeepLabV3+ model with ResNet50 backbone. As ex-pected, the batch size sharply decreases, leading to only one image per batch for 1320 classes.
Most existing works [53, 63, 20, 8] primarily focus on the accuracy for datasets with a few hundred seman-tic classes using multiple GPUs. With the release of LVIS dataset [21], efforts are being made in scaling the instance segmentation models with a large number of classes. How-ever, for a rich and complete understanding of the scene, semantic segmentation followed by panoptic segmentation
[29] is the way to go forward. Therefore, it stands to reason that the semantic segmentation networks in the real-world will eventually have to get exposed to the classes at least as high as that of classiﬁcation, i.e. 10K. Unfortunately, the benchmark results on ADE20k dataset with 150 classes require 4-8 GPUs during training [65]. Such demand for computational resources hinders researchers in emerging economies and small-scale industries from leveraging these models for research and developing further applications.
Naive approaches for training segmentation models on large number of classes and limited GPU memory may be designed by reducing the image resolution or batch size.
Such solutions regrettably compromise the performance.
As shown in [55], lower resolutions (or higher strides) re-sult in blurry boundaries and coarse predictions and miss small but essential regions, such as poles and trafﬁc signs.
On the other hand, [66] has already demonstrated the need of larger batch size to achieve the state-of-the-art results.
While techniques such as gradient accumulation [24] and group normalization [58] help to reduce the effect of low batch size, they fail to solve the problem completely when even a single batch size does not ﬁt into the GPU memory.
When more than one GPU is available, the authors in [63] offer a promising synchronized multi-GPU batch normal-ization technique to increase the effective batch size. Such solutions allow scaling of classes at the cost of scaling the
GPUs. However, it is important to seek for the possibility of scaling the training on a high number of classes with a single GPU, which remains unexplored.
In this work, we propose a novel training methodology for which the memory requirement does not increase with the number of semantic classes. To the best of our knowl-edge, this is the ﬁrst work to study efﬁcient training methods for semantic segmentation models beyond 1K classes. Such scaling is achieved by reducing the output channels of ex-isting networks and learning a low dimensional embedding of semantic classes. We also propose an efﬁcient strategy to learn and exploit such embedding for the task of seman-tic image segmentation. Our main motive is to improve the scalability of the existing segmentation networks, instead of competing against, by endowing them the possibility of us-ing only one GPU during training for a very high number of semantic classes. The major contributions of this paper are summarized as follows:
• We propose a novel scalable approach for training se-mantic segmentation networks for a large number of classes using only one GPU’s memory.
• We experimentally demonstrate that the proposed method achieves 2.7x better mIoU scores on a dataset with 1284 classes, when compared against its counter-part, while retaining a competitive performance in the regime of a lower number of classes.
• For efﬁciency and generalization, we introduce an ap-proximate method to cross-entropy measure and a se-mantic embedding space regularization term.
• Our method is theoretically grounded in terms of prob-abilistic interpretation and underlying assumptions. 2 .