Abstract
Data augmentation is a widely adopted technique for avoiding overﬁtting when training deep neural networks.
However, this approach requires domain-speciﬁc knowl-edge and is often limited to a ﬁxed set of hard-coded trans-formations. Recently, several works proposed to use gen-erative models for generating semantically meaningful per-turbations to train a classiﬁer. However, because accurate encoding and decoding are critical, these methods, which use architectures that approximate the latent-variable in-ference, remained limited to pilot studies on small datasets.
Exploiting the exactly reversible encoder-decoder struc-ture of normalizing ﬂows, we perform on-manifold per-turbations in the latent space to deﬁne fully unsupervised data augmentations. We demonstrate that such perturba-tions match the performance of advanced data augmenta-tion techniques—reaching 96.6% test accuracy for CIFAR-10 using ResNet-18 and outperform existing methods, par-ticularly in low data regimes—yielding 10–25% relative im-provement of test accuracy from classical training. We ﬁnd that our latent adversarial perturbations adaptive to the classiﬁer throughout its training are most effective, yield-ing the ﬁrst test accuracy improvement results on real-world datasets—CIFAR-10/100—via latent-space perturbations. 1.

Introduction
Deep Neural Networks (DNNs) have shown impressive results across several machine learning tasks [17, 40], and— due to their automatic feature learning—have revolution-ized the ﬁeld of computer vision. However, their success de-pends on the availability of large annotated datasets for the task at hand. Thus, among other overﬁtting techniques— such as L1/L2 regularization, dropout [52], early stopping, among others—data augmentation remains a mandatory component that is frequently used in practice.
Traditional data augmentation (DA) techniques apply a predeﬁned set of transformations to the training samples
Correspondence to oguz.yuksel@epfl.ch. that do not change the corresponding class label, to increase the number of training samples. As this approach is lim-ited to making the classiﬁer robust only to the ﬁxed set of hard-coded transformations, advanced methods incorporate more loosely deﬁned transformations in the data space. For example, mixup [66] uses convex combinations of pairs of examples and their labels, and cutout [9] randomly masks square regions of the input sample. Albeit implicitly, these methods still require domain-speciﬁc knowledge that, for example, such masking will not change the label.
Surprisingly, in the context of computer vision, it has been shown that small perturbations in image space that are not visible to the human eye can fool a well-performing classiﬁer into making wrong predictions. This observation motivated an active line of research on adversarial train-ing [see 4, and references therein]—namely, training with such adversarial samples to obtain robust classiﬁers. How-ever, further empirical studies showed that such training re-duces the training accuracy, indicating the two objectives are competing [58, 54]. this
Stutz et al. robustness-[53] postulate that generalization trade-off appears due to using off-manifold adversarial attacks that leave the data-manifold and that on-manifold adversarial attacks can improve generaliza-tion. For verifying this hypothesis, the authors proposed to use perturbations in the latent space of a generative model. Their proposed method employs (class-speciﬁc) models named VAE-GANs [33, 48]—which are based on
Generative Adversarial Networks [16] and, to tackle their non-invertibility, further combine GANs with Variational
Autoencoders [28]. However, the VAE-GAN model in-troduces hard-to-tune hyperparameters, and notably, it op-timizes a lower bound on the log-likelihood of the data.
Moreover, improved test accuracy was only shown on toy datasets [53, Fig. 5], and yet in some cases, the test accu-racy did not improve relative to classical training. We ob-serve that on real-world datasets, such training can decrease the test accuracy, see §5.
In this work, we focus on the possibility of employ-ing advanced normalizing ﬂows such as Glow [27], to de-ﬁne entirely unsupervised augmentations—contrasting with
pre-deﬁned ﬁxed transformations—with the same goal of improving the generalization of deep classiﬁers. Although normalizing ﬂows have gained little attention in our com-munity relative to GANs and Autoregressive models, they offer appealing advantages over these models, namely: (i) exact latent-variable inference and log-likelihood eval-uation, and (ii) efﬁcient inference and synthesis that can be parallelized [27], respectively. We exploit the exactly reversible encoder-decoder structure of normalizing ﬂows to perform efﬁcient and controllable augmentations in the learned manifold space.
Contributions. Our contributions can be summarized as:
• Firstly, we demonstrate through numerical experiments that the previously proposed methods to generate on-manifold perturbations fail to improve the generalization of a trained classiﬁer on real-world datasets. In partic-ular, the test accuracy decreases with such training on
CIFRAR-10/100. In this work, we postulate that this oc-curs due to approximate encoder-decoder mappings.
• Motivated by this observation, we propose a data aug-mentation method based on exactly reversible normaliz-ing ﬂows. Namely, it ﬁrst trains the generative model and then uses simplistic random or adversarial domain-agnostic semantic perturbations to train the classiﬁer, de-ﬁned in §4.
• We demonstrate that our adversarial data augmentation method generates on-manifold and semantically mean-ingful data perturbations. Hence, we argue that our technique is a novel approach for generating perceptu-ally meaningful (natural adversarial examples), different from previous proposals.
• Finally, we empirically demonstrate that our on-manifold perturbations consistently outperform the stan-dard training on CIFAR-10/100 using ResNet-18. More-over, in a low-data regime, such training yields up to 25% relative improvement from classical training, of which—as most effective—we ﬁnd the adversarial per-turbations that are adaptive to the classiﬁer, see §5. 2.