Abstract
In this paper, we propose a novel solution for object-matching based semi-supervised video object segmentation, where the target object masks in the ﬁrst frame are pro-vided. Existing object-matching based methods focus on the matching between the raw object features of the current frame and the ﬁrst/previous frames. However, two issues are still not solved by these object-matching based meth-ods. As the appearance of the video object changes drasti-cally over time, 1) unseen parts/details of the object present in the current frame, resulting in incomplete annotation in the ﬁrst annotated frame (e.g. view/scale changes). 2) even for the seen parts/details of the object in the current frame, their positions change relatively (e.g. pose changes/camera motion), leading to a misalignment for the object match-ing. To obtain the complete information of the target ob-ject, we propose a novel object-based dynamic memory net-work that exploits visual contents of all the past frames. To solve the misalignment problem caused by position changes of visual contents, we propose an adaptive object align-ment module by incorporating a region translation function that aligns object proposals towards templates in the feature space. Our method achieves state-of-the-art results on lat-est benchmark datasets DAVIS 2017 (J of 81.4% and F of 87.5% on the validation set) and YouTube-VOS (the overall score of 82.7% on the validation set) with a very efﬁcient inference time (0.16 second/frame on DAVIS 2017 valida-tion set). Code is available at: https://github.com/ liang4sx/DMN-AOA. 1.

Introduction
Semi-supervised video object segmentation (VOS) is a task that distinguishes target objects from their background at the pixel level in a video, where ground-truth masks of
∗This work was done when the author was visiting Alibaba as a re-search intern.
†Corresponding author.
Figure 1. Two issues for the object matching based VOS. In the top two cases, corresponding parts (in red) of the query object do not present in the ﬁrst frame because of view/scale changes. The bottom two cases show the misalignment problem caused by pose changes and camera motion. Our method solve these issues by ex-ploiting object-speciﬁc memories and aligning objects adaptively. objects are provided in the ﬁrst frame. The task is then to predict the segmentation masks from the rest video frames.
One of the main challenges of VOS is that the appearances of the target object can change drastically across frames due to object movements, camera movements and occlusions.
For the semi-supervised video object segmentation commu-nity, the pixel matching based VOS (PVOS) and the object matching based VOS (OVOS) are investigated in parallel as two research topics. PVOS methods predict based on cross-frame pixel correlations (e.g. [7, 20, 28, 35]). OVOS methods predict by correlating proposal objects of the cur-rent frame with template objects of historical frames (e.g.
[4, 11, 29, 36]).
Existing OVOS methods focus on the matching be-tween the raw object features of the proposals in the cur-rent frame and the templates in the ﬁrst/previous frames
[2, 4, 11, 26, 29, 36]. However, two issues are still not solved by these methods (shown in Figure 1). Speciﬁcally, as the appearance of the video object changes drastically over time, ﬁrst, unseen parts/details of the object present in the current frame, resulting in incomplete information in the ﬁrst frame (e.g. view/scale changes). For example, in the top two cases, the information about the person’s back and the details of the vehicle’s side (which are required by the current frame) are not provided in the ﬁrst frame. And second, for the seen parts/details of the object in the current frame, their positions change relatively (e.g. pose/camera motion), leading to a misalignment for the object matching.
For example, in the bottom two cases, some object parts in the ﬁrst frame spatially correspond to the background (the dancer’s hand vs. the audience’s head) or to other dissimilar parts (the vehicle’s body vs. the vehicle’s wheel) in the cur-rent frame. Consequently, existing OVOS methods are not as strong as the state-of-the-art (SOTA) PVOS methods, es-pecially the PVOS methods exploiting all past frames with memory networks [12, 13, 16, 20, 24].
The key challenges for OVOS to exploit all past frames and surpass PVOS lie in two folds: a) The memory module for PVOS is straight-forward, where directly storing feature maps of raw frames is enough. While for OVOS, we need a special memory module to store speciﬁc object features, which is still not solved in OVOS (no memory is used in existing OVOS methods [4, 11, 29, 36]). b) Instead of di-rectly computing pixel-to-pixel similarities between frames as in PVOS, the matching of object features between tem-plates and proposals is faced with the misalignment prob-lem caused by object deformation across frames, which is still not solved by OVOS. Speciﬁcally, [4] aligns the object sizes rather than the deformed object appearances. Other works compute the object similarities by unaligned pixel-to-pixel distance [29] or global average pooled features with-out spatial information [4, 36].
To solve the aforementioned two challenges, we propose a novel dynamic memory network and a new adaptive object alignment module for OVOS. By reading relevant object in-formation from all available resources, memory of object features in past frames is used to generate dynamic object templates. In this way, the past frames with object masks form a dynamic memory, and the current frame as the query is used to decode templates that represent the current ap-pearances of objects. With the dynamic memory network, there is no restriction on the number of frames to use and new object information of a given frame can be easily accu-mulated in the memory. To solve the misalignment problem caused by position changes of visual contents in the target object, we propose an adaptive object alignment operation by incorporating a non-local region translation function that recomposes regions of the object templates based on the ob-ject features of proposals. Speciﬁcally, object templates and proposals are ﬁrstly projected to a shared feature space, then dense correspondences between them are utilized to trans-late object proposals towards the templates in the feature space.
Equipped with the proposed dynamic memory network and the adaptive object alignment module, we design an easy-to-extend framework for OVOS. Firstly, for a test frame, object proposals are generated through a pre-trained instance segmentation model. Secondly, current-frame fea-tures and object bounding boxes from the previous frame are combined as a query input to the dynamic memory net-work, which generates object templates for the frame incor-porating object information from memory frames. Thirdly, the object matching assignment between the generated tem-plates and proposals is produced based on the adaptive ob-ject alignment module and a differentiable matching layer
[36]. Finally, the object matching results are input to a mask reﬁnement network to make output segmentation.
Our contributions can be summarized as follows:
• We present a novel and easy-to-extend object-matching framework for the VOS task. Our proposed
OVOS model outperforms all SOTA PVOS methods for the ﬁrst time in the community, and we pave a new way for the development of OVOS.
• We are the ﬁrst to exploit all the frames in the video for OVOS. Speciﬁcally, we introduce a dynamic mem-ory network that computes spatio-temporal attention on object features of all past frames for each query frame, to obtain current-frame representations of tar-get objects.
• We propose a novel adaptive object alignment module to solve the misalignment between the object propos-als and templates. In detail, we design a region trans-lation function that recomposes template-like regions with proposal object features. 2.