Abstract
We present SemiHand, a semi-supervised framework for 3D hand pose estimation from monocular images. We pre-train the model on labelled synthetic data and fine-tune it on unlabelled real-world data by pseudo-labeling with con-sistency training. By design, we introduce data augmen-tation of differing difficulties, consistency regularizer, label correction and sample selection for RGB-based 3D hand pose estimation. In particular, by approximating the hand masks from hand poses, we propose a cross-modal consis-tency and leverage semantic predictions to guide the pre-dicted poses. Meanwhile, we introduce pose registration as label correction to guarantee the biomechanical feasibility of hand bone lengths. Experiments show that our method achieves a favorable improvement on real-world datasets after fine-tuning. 1.

Introduction
A key challenge of monocular 3D hand pose estimation is getting sufficient high-quality ground-truth poses. La-belling real-world data to an accurate enough degree often requires dedicated interfaces and or multi-view camera rigs.
This makes it non-trivial to gather “in-the-wild” data that is much sought-after for actual application deployment.
Synthesizing training data is considered an easy alterna-tive to get accurate labels and has been incorporated into many learning-based frameworks. Yet there exists a sig-nificant domain gap between synthetic and real-world im-ages so the performance of models trained on synthetic data deteriorates significantly when applied to real-world data.
The favoured approach for reducing the domain gap is a mix-and-train strategy [12], i.e. mixing multiple real-world datasets together with synthetic data for training. Such a strategy depends largely however on the quantity and qual-ity of the labelled samples in the combined datasets.
What if we tried to learn only from labelled synthetic data and fully unlabelled real-world data? We target exactly this scenario and present the first framework for domain-separated semi-supervised learning for 3D hand pose es-Figure 1: Pseudo-labelling of SemiHand. Our pseudo-label with confidence is generated based on the prediction from original (blue pose), the prediction from perturbation (green pose) and the corrected prediction (red pose). timation. A classic approach in semi-supervised learning is to generate pseudo-labels [16] for the unlabelled data, usually via a classifier learned from the labelled portion of the data [16, 25]. The utility of pseudo-labels is highly variable. Used naively, these labels are even detrimen-tal to learning because of confirmation bias [1], i.e. , the classifier over-fits to the pseudo-labels which tend to be noisy and or inaccurate, so additional corrections are nec-essary [1, 11, 43, 38]. Additionally, consistency training with unlabelled data [25, 1, 34] can increase the reliability of pseudo-labels.
We integrate these concepts and introduce SemiHand, a framework that considers spatial consistency and biome-chanical feasibility for semi-supervised hand pose estima-tion. We propose two consistency losses to encourage the predictions to be consistent with perturbations and other modalities. As our labelled and unlabelled data come from different domains, i.e. synthetic vs real RGB images, there is the added challenge of domain adaptation to the unla-belled data. To bridge the domain gap, we propose a cross-modal consistency and leverage semantic predictions [19] from an auxiliary task to provide guidance for the predicted poses. Meanwhile, we regard predictions on real-world data as noisy labels; further training the network from these pre-dictions directly may actually be detrimental due to their in-accuracy. To mitigate the impact of this confirmation bias, we introduce label correction and sample selection based on the feasibility so that we train with only corrected pseudo-labels with high-confidence. We show our pseudo-labelling
strategy in Fig. 1.
Pseudo-labelling and consistency training are already es-tablished in semi-supervised classification [16, 25, 1]. How-ever, extending such concepts for a regression task and in the context of 3D pose estimation is non-trivial and we are the first to present a unified framework to do so. For exam-ple, existing methods [11, 32] primarily learn a noise tran-sition matrix to correct pseudo-labels; such an approach is not applicable for regression and we instead focus on the confidence and feasibility of poses as a selection and cor-rection criteria. Similarly, consistency training in classifica-tion simply keeps the predicted categories unchanged under perturbation. Consistency in 3D pose estimation however needs to account for the change in label, i.e. the pose after perturbation. We summarize our contributions below:
• We propose a novel RGB-based hand pose estimation framework using labelled synthetic data and unlabelled real-world data; it is the first semi-supervised frame-work that combines pseudo-labeling with consistency training for RGB-based hand pose.
• Based on the feasibility of hand poses, we propose a method for pose registration and sample selection to correct noisy label outputs and select pseudo-labels of high confidence for training.
• We propose two consistency losses for 3D pose esti-mation to encourage the predictions to be consistent with perturbations and auxiliary modalities.
• Using a pre-trained synthetic model, we are able to adapt our model to challenging real-world datasets without any labels. Our results are compelling when compared to fully supervised frameworks and outper-form previous works on synthetic image enhancement. 2.