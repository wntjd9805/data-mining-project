Abstract
Single-frame temporal action localization (STAL) aims to localize actions in untrimmed videos with only one times-tamp annotation for each action instance. Existing meth-ods adopt the one-stage framework but couple the counting goal and the localization goal. This paper proposes a novel two-stage framework for the STAL task with the spirit of di-vide and conquer. The instance counting stage leverages the location supervision to determine the number of ac-tion instances and divide a whole video into multiple video clips, so that each video clip contains only one complete ac-tion instance; and the location estimation stage leverages the category supervision to localize the action instance in each video clip. To efficiently represent the action instance in each video clip, we introduce the proposal-based repre-sentation, and design a novel differentiable mask genera-tor to enable the end-to-end training supervised by cate-gory labels. On THUMOS14, GTEA, and BEOID datasets, our method outperforms state-of-the-art methods by 3.5%, 2.7%, 4.8% mAP on average. And extensive experiments verify the effectiveness of our method. 1.

Introduction
Temporal action localization (TAL) plays an important role in video understanding [35, 45, 38]. Its goal is to detect and classify all action instances in untrimmed videos. Re-cently, the fully-supervised setting [4, 18, 16, 34, 6, 14, 49] which requires frame-level supervision, has achieved im-pressive results; however, it is time-consuming and expen-sive to densely annotate each frame. On the other hand, the video-level weakly-supervised setting [19, 27, 28, 33, 20, 31] only needs the action category label of the whole video for localization. But lacking explicit location supervision fundamentally limits its empirical performance. To bridge the gap between fully-supervised and video-level weakly-supervised settings, a single-frame weakly-supervised TAL (STAL) is recently introduced [21], where a single frame (seedframe) is annotated for each action instance. STAL provides limited, yet precise action location supervision, and shows the potential to achieve great empirical perfor-Figure 1. Comparison. (A): The one-stage framework couples the counting goal and the localization goal via thresholding, causing inferior localization results. (B): The two-stage framework detects seedframes to divide a whole video into multiple video clips, each of which contains only one complete action instance; then, it sep-arately localizes the action instance in each video clip. mance and maintain cheap annotation overhead at the same time. This work explores this new STAL task.
The existing STAL method [21] considers a one-stage framework, similar to video-level weakly-supervised meth-ods [28, 11, 19]. Based on Multiple Instance Learning, this framework directly estimates the action probability at each individual frame; and then, by thresholding the action prob-ability sequence, the framework simultaneously determines the number of action instances (counting) and localizes each action instance (localization); see Figure 1 (A). Since this one-stage framework couples the counting goal and the lo-calization goal via thresholding, adjusting such a threshold empirically would highly affect both counting and localiza-tion performances, causing a serious coupling issue. Even tuning a threshold to provide the perfect counting results, this single and unified threshold might not be able to pre-cisely localize all the action instances because each action instance could have its local sensitivity.
To solve the coupling issue, this work introduces a strat-egy of divide and conquer to decouple the counting goal and the localization goal. In other words, we aim to strategi-cally divide the STAL task into multiple sub-tasks, each of which only needs to localize one action instance in a video
clip, then conquer each sub-task separately. Accordingly, we propose a novel two-stage framework, including the in-stance counting stage, which aims to determine the number of action instances and divide the whole video into several video clips, so that each clip contains only one complete ac-tion instance; and the location estimation stage, which aims to conquer each sub-task, i.e., localize the time interval of the action instance in each video clip; see Figure 1 (B). The intuition of considering two stages is to separately exploit the location supervision for the instance counting stage, and the category supervision for the location estimation stage, from the limited single-frame supervision.
In the instance counting stage, we propose a seedframe detector to detect all the seedframes based on the location supervision. Since each seedframe indicates a unique ac-tion instance, the number of seedframes can reflect the total number of action instances. As a result, the detected seed-frames guide us to divide a whole video into several video clips, each of which contains only one complete action in-stance. Next, in the location estimation stage, for each video clip, we localize the action instance with the category super-vision, i.e., adjust the location of the action instance so that the quality of action classification can be improved. Note that in this stage, since each video clip is supposed to cover a unique action instance, the single-frame weak supervision degenerates into the video-level weak supervision, and only handles simple single-instance localization.
To represent the location of the action instance in the video clip, existing video-level weakly-supervised methods mostly follow the frame-based representation [26, 19, 28], which directly estimates the action probability at each indi-vidual frame. However, the lack of precise frame-level su-pervision makes this representation inevitably suffer from a large solution space, resulting in two main issues: high false positives and lots of sparse and spiky actions. To avoid these issues, we introduce a more efficient proposal-based repre-sentation, which represents the action location with a gate-shaped proposal parameterized by the center and the length.
It has two distinct advantages: i) its parameterization uses only two degrees of freedom for each action instance, which greatly reduces the solution space; ii) it naturally represents a time interval, promoting temporal smoothness and ruling out sparse and spiky actions. To adjust the center and the length of the action proposal via the category supervision, we aim to aggregate the frames within the proposal and specifically extract action-related features for action clas-sification. Intuitively, a better estimation of the center and the length leads to better classification. To make this pro-cess trainable, we design a novel mask generator to trans-form the center and the length into a differentiable tempo-ral mask, which indicates a time interval. Then, supervised by action category labels, we can adjust the center and the length of the proposal in an end-to-end fashion.
On three benchmark datasets, BEOID [5], GTEA [13], and THUMOS14 [7], the experimental results show that our method improves the average performance by 4.8%, 2.7%, 3.5% over the state-of-the-art methods. We further perform extensive ablation studies to reveal the effectiveness of each component, both quantitatively and qualitatively.
To summarize, our contributions include:
•
We propose a novel two-stage framework for the STAL task with the spirit of divide and conquer. The instance counting stage leverages the location supervision to deter-mine the number of action instances and divide a whole video into multiple video clips; and the location estimation stage leverages the category supervision to localize the ac-tion instance in each video clip.
•
We adopt a proposal-based representation in the loca-tion estimation stage, which parameterizes the time interval of an action instance by the center and the length. To en-able the end-to-end training supervised by category labels, we design a novel differentiable mask generator to trans-form the center and the length into a temporal mask.
•
We conduct extensive experiments to validate the pro-posed method, which significantly outperforms the existing single-frame weakly-supervised method. 2.