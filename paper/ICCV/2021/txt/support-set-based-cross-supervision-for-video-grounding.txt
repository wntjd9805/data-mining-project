Abstract
Current approaches for video grounding propose kinds of complex architectures to capture the video-text relations, and have achieved impressive improvements. However, it is hard to learn the complicated multi-modal relations by only architecture designing in fact. In this paper, we intro-duce a novel Support-set Based Cross-Supervision (Sscs) module which can improve existing methods during train-ing phase without extra inference cost. The proposed Sscs module contains two main components, i.e., discriminative contrastive objective and generative caption objective. The contrastive objective aims to learn effective representations by contrastive learning, while the caption objective can train a powerful video encoder supervised by texts. Due to the co-existence of some visual entities in both ground-truth and background intervals, i.e. mutual exclusion, naively contrastive learning is unsuitable to video grounding. We address the problem by boosting the cross-supervision with the support-set concept, which collects visual information from the whole video and eliminates the mutual exclusion of entities. Combined with the original objectives, Sscs can enhance the abilities of multi-modal relation modeling for existing approaches. We extensively evaluate Sscs on three challenging datasets, and show that our method can im-prove current state-of-the-art methods by large margins, es-pecially 6.35% in terms of R1@0.5 on Charades-STA. 1.

Introduction
Video grounding aims to localize the target time inter-vals in an untrimmed video by a text query. As illustrated in
Fig. 1 (a), given a sentence ‘The person pours some water into the glass.’ and a paired video, the target is to localize the best matching segment, i.e., from 7.3s to 17.3s. There are various methods [51, 49, 12] have been proposed for
*Nannan Wang and Shiwei Zhang are the corresponding authors.
Figure 1. (a) Comparison of the attention map of the similarity be-tween video clips and text queries. The darker the color, the higher the similarity. ‘GT’ indicates the ground-truth. (b) The proposed
Support-Set based Cross-Supervision (Sscs) Module. Sscs makes the embedding of semantic-related clip-text pairs (dark circles and triangles) to be close in the shared feature space. this task, and they have made significant progresses. These methods can reach an agreement that video-text relation modeling is one of the crucial roles. An effective relation should be that semantically related videos and texts must have high responses, and vice versa.
To achieve this goal, existing methods focus on carefully designing complex video-text interaction modules. For ex-ample, Zeng et al. [49] propose a pyramid neural network to consider multi-scale information. Local-global strat-egy [30] and self-modal graph attention [26] are applied as the interaction operations to learning the multi-modal rela-tions. After that, they use the interacted features to perform
video grounding straightway. However, the multi-modal re-lations are complicated because the video and text have un-equal semantics, e.g., ‘person’ is just one word but may last a whole video. Hence, existing methods based on the archi-tecture improvements have limited capacities to learn video-caption relations; see Fig. 1 (a) (Please see ‘Baseline’).
Motivated by the advances of multi-modal pre-training
[28, 33, 29], we propose a Support-Set Based Cross-Supervision, termed Sscs, to improve multi-modal relation learning for video grounding in a supervision way com-pared with the hand-designed architectures. As shown in
Fig. 1, the Sscs module is an independent branch that can be easily embedded into other approaches in the training stage. The proposed Sscs includes two main components, i.e., contrastive objective and caption objective. The con-trastive objective is as typical discriminative loss function, that targets to learn multi-modal representations by apply-ing infoNCE loss function [28, 33]. In contrast, the cap-tion objective is a generative loss function, which can be used to train a powerful video encoder [15, 53]. For an untrimmed video, there are some vision entities appear in both ground-truth and background intervals, e.g., the per-son and glass in Fig. 2, but the original contrastive learning may wipe away the same parts between the foreground and background, including the vision entities. These vision enti-ties are also important for video grounding task, e.g., thus it is unsuitable to directly apply the contrastive learning into the video grounding task directly. To solve this problem, we apply the support-set concept, which captures visual in-formation from the the whole video, to eliminates the mu-tual exclusion of entities. By this means, we can improve the cross-supervision module naturally and further enhance the relation modeling. To prove the robustness, we choose two state-of-the-art approaches as our baselines, i.e., 2D-TAN [51] and LGI [30], and the experimental results show that the proposed Sscs can achieve a remarkable improve-ment. (a)
Our contributions are summarized as three-folds:
We introduce a novel cross-supervision module for video grounding, which can enhance the correlation modeling be-tween videos and texts but not bring in the extra inference cost. (b) We propose to apply support-set concept to ad-dress the mutual exclusion of video entities, which make it is more suitable to apply contrastive learning for video grounding. (c) Extensive experiments illustrate the effec-tiveness of Sscs on three public datasets, and the results show that our method can significantly improve the perfor-mance of the state-of-the-art approaches. 2.