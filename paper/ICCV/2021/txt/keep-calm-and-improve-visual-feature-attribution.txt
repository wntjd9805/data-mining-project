Abstract
The class activation mapping, or CAM, has been the cor-nerstone of feature attribution methods for multiple vision tasks. Its simplicity and effectiveness have led to wide appli-cations in the explanation of visual predictions and weakly-supervised localization tasks. However, CAM has its own shortcomings. The computation of attribution maps relies on ad-hoc calibration steps that are not part of the train-ing computational graph, making it difﬁcult for us to un-derstand the real meaning of the attribution values. In this paper, we improve CAM by explicitly incorporating a la-tent variable encoding the location of the cue for recogni-tion in the formulation, thereby subsuming the attribution map into the training computational graph. The result-ing model, class activation latent mapping, or CALM, is trained with the expectation-maximization algorithm. Our experiments show that CALM identiﬁes discriminative at-tributes for image classiﬁers more accurately than CAM and other visual attribution baselines. CALM also shows performance improvements over prior arts on the weakly-supervised object localization benchmarks. Our code is available at https://github.com/naver-ai/calm. 1.

Introduction
Interpretable AI [25, 40, 24, 52] is becoming an abso-lute necessity in safety-critical and high-stakes applications of machine learning. Along with good recognition and pre-diction accuracies, we require models to be able to trans-parently communicate the inner mechanisms with human users. In visual recognition tasks, researchers have devel-oped various feature attribution methods to inspect contri-butions of individual pixels or visual features towards the ﬁ-nal model prediction. Input gradients [55, 56, 58, 5, 63, 39, 28] and input perturbation methods [59, 70, 20, 43, 23, 47] have been actively researched.
In this paper, we focus on the class activation mapping (CAM) [68] method, which has been the cornerstone of
*Equal contribution. Majority of work done at NAVER AI Lab.
†Corresponding author.
Figure 1. CAM vs CALM. CALM is better at locating the actual cues used for the recognition than CAM. Two bird classes A and B only differ in their head and wing attributes. Attributions for class
A, B, and their difference are shown. While CAM fails to detect the head and wing, CALM captures them accurately. (cid:0) (cid:80) 1
HW hw fyhw the feature attribution research. CAM starts from the ob-servation that many CNN classiﬁers make predictions by aggregating location-wise signals. For example, p(y x) =
| softmax where f = f (x) is the extracted feature map in RC×H×W where C, H, W are the number of classes, height, and width of the feature map, respec-tively. CAM considers the pre-GAP feature map fyhw as the attribution, after scaling it to the [0, 1] range by dropping the negative values and dividing through by the maximum
[0, 1]H×W . Thanks value: s := (maxhw fhw)−1f + to the algorithmic simplicity and reasonable effectiveness,
CAM has been a popular choice as an attribution method with many follow-up variants [53, 9, 6, 69, 61, 45, 22].
∈ (cid:1)
Despite its popularity and contributions to the inter-pretability community, CAM still has its own limitations.
What does the attribution map s really mean? We fail to ﬁnd a reasonable linguistic description because s hardly encodes anything essential in the recognition process. s also violates key minimal requirements, or “axioms” [40, 59, 22], for an attribution method. For example, its dependence on the pre-softmax values f make it ill-deﬁned: translating f f + c yields an identical model because of the translation invari-ance of softmax, but it changes the attribution map s. (cid:55)→
We thus introduce a novel attribution method, class ac-∈ {
∈ {
, C
}
, HW 1,
· · · 1,
· · · tivation latent mapping (CALM). It builds a probabilistic graphical model on the last layers of CNNs, involving three
, and variables: input image X, class label Y the location of the cue for recognition Z
.
}
Since there is no observation for Z, we consider latent-variable training algorithms like marginal likelihood (ML) and expectation-maximization (EM). After learning the de-pendencies, we deﬁne the attribution map for image x of the joint likelihood class of the recognition cue being at z and the class being y. (cid:98)
CALM has many advantages over CAM. (1) It has a human-understandable probabilistic deﬁnition; (2) it satisﬁes the (cid:98) axiomatic requirements for attribution methods; (3) it is em-pirically more accurate and useful than CAM.
[0, 1]H×W , y as p( y, z x)
∈ (cid:98) (cid:98) (cid:98)
|
In our experimental analysis, we study how well CALM localizes the “correct cues” for recognizing the given class of interest. The “correct cues” for recognition are ill-deﬁned in general, making the evaluation of attribution methods dif-ﬁcult. We build a novel evaluation benchmark on pairs of bird classes in CUB-200-2011 [62] where the true cue loca-tions are given by the parts where the attributes for the class pair differ (Figure 1). Under this benchmark and a widely-used remove-and-classify type of benchmark, CALM shows better attribution performances than CAM and other base-lines. We also show that CALM advances the state of the art in the weakly-supervised object localization (WSOL) task, where CAM has previously been one of the best [13, 12].
In summary, our paper contributes (1) analysis on the lack of interpretability for CAM, (2) a new attribution method CALM that is more interpretable and communica-ble than CAM, and (3) experimental results on real-world datasets where CALM outperforms CAM in multiple tasks.
Our code is available at https://github.com/naver-ai/calm. 2.