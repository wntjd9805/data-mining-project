Abstract (a) Image (b) Comparison on CSet9 dataset
Deep image prior (DIP) serves as a good inductive bias for diverse inverse problems. Among them, denoising is known to be particularly challenging for the DIP due to noise fitting with the requirement of an early stopping. To address the issue, we first analyze the DIP by the notion of effective degrees of freedom (DF) to monitor the opti-mization progress and propose a principled stopping cri-terion before fitting to noise without access of a paired ground truth image for Gaussian noise. We also propose the ‘stochastic temporal ensemble (STE)’ method for incor-porating techniques to further improve DIP’s performance for denoising. We additionally extend our method to Pois-son noise. Our empirical validations show that given a sin-gle noisy image, our method denoises the image while pre-serving rich textual details. Further, our approach outper-forms prior arts in LPIPS by large margins with compara-ble PSNR and SSIM on seven different datasets. 1.

Introduction
Deep neural network has been widely used in many com-puter vision tasks, yielding significant improvements over conventional approaches since AlexNet [18]. However, im-age denoising has been one of the tasks in which conven-tional methods such as BM3D [7] outperformed many early deep learning based ones [5, 47, 48] until DnCNN [51] out-performs it for synthetic Gaussian noise at the expense of massive amount of noiseless and noisy image pairs [51].
Requiring no clean and/or noisy image pairs, deep image prior (DIP) [42, 43] has shown that a randomly initialized network with hour-glass structure acts as a prior for several inverse problems including denoising, super-resolution, and inpainting with a single degraded image. Although DIP ex-hibits remarkable performance in these inverse problems, denoising is the particular task that DIP does not perform well, i.e., a single run yields far lower PSNR than BM3D even for synthetic Gaussian noise set-up [42, 43]. Further-§: work done while with GIST. †: corresponding author.
Code: https://github.com/gistvision/DIP-denosing
Noise 20.83/0.56
BM3D 33.06/0.16
S2S 32.83/0.18
DIP 30.92/0.18
Ours 32.86/0.14
GT
P ↑ / L↓
Figure 1: Comparison of single image based denoising meth-ods. ‘L↓’ refers to LPIPS and it is lower the better. ‘P↑’ refers to
PSNR and it is higher the better. Our method denoises an image while preserves rich details; showing the best LPIPS with com-parable PSNR to Self2Self (S2S) [30]. Ours shows much better trade-off in PSNR and LPIPS than all other methods including all different ensembling attempts of state of the art (S2S). (Numbers in the circle of S2S denotes number of models in ensemble) more, for the best performance, one needs to monitor the
PSNR (i.e., the ground-truth clean image is required here) and stop the iterations before fitting to noise. Deep Decoder addresses the issue by proposing a strong structural regular-ization to allow longer iterations for the inverse problems including denoising [15]. However, it yields worse denois-ing performance than DIP due to low model complexity.
For better use of DIP for denoising without monitoring
PSNR with a clean image, we first analyze the model com-plexity of the DIP by the notion of effective degrees of free-dom (DF) [10, 12, 41]. Specifically, the DF quantifies the amount of overfitting (i.e., optimism) of a chosen hypothe-sis (i.e., a trained neural network model) to the given train-ing data [10]. In other words, when overfitting occurs, the
DF increases. Therefore, to prevent the overfitting of the
DIP network to the noise, we want to suppress the DF over
iterations. But obtaining DF again requires a clean (ground truth) image. Fortunately, for the Gaussian noise model, there are approximations for DF without using a clean im-age; Monte-Carlo divergence approximations in Stein’s un-biased risk estimator (SURE) (Eqs. 8, 9) (DFM C).
Leveraging SURE and improvement techniques in
DIP [43], we propose an objective with ‘stochastic tempo-ral ensembling (STE),’ which mimics ensembling of many noise realizations in a single optimization run. On the pro-posed objective with the STE, we propose to stop the it-eration when the proposed objective function crosses zero.
The proposed method leads to much better solutions than
DIP and outperforms prior arts for single image denoising.
In addition, inspired by PURE formulation [20, 24], we ex-tend our objective function to address the Poisson noise.
We empirically validate our method by comparing DIP based prior arts for denoising performance in various metrics that are suggested in the literature [13] such as
PSNR, SSIM and learned perceptual image patch similarity (LPIPS) [54] on seven different datasets. LPIPS has been widely used in super resolution literature to complement
PSNR, SSIM to measure the recovery power of details [21].
Since it is challenging for denoiser to suppress noise and preserve details together [4], we argue that LPIPS is another appropriate metric to evaluate denoisers. Note that it has not been widely used in denoising literature yet to analyze the denoising performance. Our method not only denoises the images but also preserves rich textual details, outperforming other methods in LPIPS with comparable classic measures including the PSNR and SSIM.
Our contributions are summarized as follows:
• Analyzing the DIP for denoising with effective degrees of freedom (DF) of a network and propose a loss based stopping criterion without ground-truth image.
• Incorporating noise regularization and exponential moving average by the proposed stochastic temporal ensembling (STE) method.
• Diverse evaluation in various metrics such as LPIPS,
PSNR and SSIM in seven different datasets.
• Extending our method to Poisson noise. 2.