Abstract
With increasing applications of 3D hand pose estimation in various human-computer interaction applications, con-volution neural networks (CNNs) based estimation models have been actively explored. However, the existing models require complex architectures or redundant computational resources to trade with the acceptable accuracy. To tackle this limitation, this paper proposes HandFoldingNet, an ac-curate and efficient hand pose estimator that regresses the hand joint locations from the normalized 3D hand point cloud input. The proposed model utilizes a folding-based decoder that folds a given 2D hand skeleton into the cor-responding joint coordinates. For higher estimation accu-racy, folding is guided by multi-scale features, which in-clude both global and joint-wise local features. Experimen-tal results show that the proposed model outperforms the ex-isting methods on three hand pose benchmark datasets with the lowest model parameter requirement. Code is available at https://github.com/cwc1260/HandFold. 1.

Introduction 3D hand pose estimation aims to estimate joint locations from input hand images. Accurate and real-time estima-tion is critical in various human-computer interaction appli-cations, especially in virtual reality and augmented reality
[20, 7, 23]. Recently, many studies achieved impressive progress by utilizing hand depth images from depth cam-eras. However, it still remains challenging to achieve accu-rate and real-time estimation, due to various issues such as self-occlusion, noise, high dimensionality, and various ori-entations of a hand [12, 9, 24, 6].
With the advancement of deep neural networks (DNNs), various DNN-based hand pose estimation techniques
In most of these tech-achieved powerful performances. niques, 2D convolution neural networks (CNNs) have been
*Jong Hwan Ko is the corresponding author.
Figure 1. Illustration of the folding concept. The network can be interpreted as emulating the ”force” through multi-scale features extracted from the point cloud. The ”force” will drive a 2D hand skeleton to ”fold” into the 3D joint coordinates representing the hand pose. adopted to perform direct hand depth image processing
[40, 10, 14, 30, 3]. However, 2D CNNs cannot fully take ad-vantage of 3D spatial information of the depth image, which is essential for achieving high accuracy. An intuitive solu-tion is to discretize hand depth images into a 3D voxelized representation and perform 3D-to-3D inference using a 3D
CNN [11, 24]. However, its critical limitation is the cubic growth of memory consumption with an increase in the im-age resolution [31]. Thus, application of 3D CNNs has been limited to low-resolution images, which may lead to lose of critical details for estimation.
In contrast, the point cloud is being regarded as an ef-ficient and precise representation for 3D hand pose esti-mation, as it models hand depth images into the continu-ous 3D coordinates without discretization. However, the point cloud could not be directly processed by conventional
DNNs due to the irregular order of points, until the emer-gence of PointNet [28]. With a concise symmetric architec-ture composed of a point-wise shared-weights multi-layer perceptron (MLP) and a max-pooling layer, PointNet is in-variant with the order of the input points.
Based on this architecture, a series of PointNet-based hand pose estimation models [9, 12, 4, 21] have been pro-Figure 2. The HandFoldingNet architecture. It takes the preprocessed normalized point cloud with surface normal vectors from a 2D depth image as an input. The hierarchical PointNet encoder is then exploited to extract features of various levels to summarize a global feature from the input point cloud. The global folding decoder receives the global feature to guide the folding of a pre-defined 2D hand skeleton into the initial joint coordinates. In the end, the local features near the initial joint coordinates are grouped and fed into the local folding blocks to estimate the accurate joint coordinates. posed. They can be summarized into two categories: 1) regression-based methods and 2) detection-based methods.
Regression-based methods [9, 4] encode the hand shape into a single global feature through a PointNet-based feature ex-tractor. The global feature representing the hand pose in the high dimensional latent space is fed into a non-linear regres-sion network that performs inference of the joint coordi-nates. On the other hand, detection-based methods [12, 21] adopt hierarchical features to compute heat-map features for each point. The point-wise features represent the pos-sibility distribution of each joint. However, the existing regression-based and detection-based strategies have limi-tations. The regression-based methods process only a sin-gle global feature, which is not sufficient for highly com-plex mapping into 3D hand poses. On the other hand, the detection-based methods propagate hierarchical features to each point including the points that contribute little to the specific joint estimation. Therefore, this redundant feature propagation significantly increases the computational cost and slows down the estimation.
To tackle these limitations, we propose HandFoldingNet, an accurate and efficient 3D hand pose estimation network.
The key idea of HandFoldingNet is to fold a 2D hand skele-ton into the 3D pose, guided by multi-scale features ex-tracted from both global and local information. The mo-tivation of adopting the folding-based design in FoldingNet
[45] is that it is suitable for a 3D hand pose estimation task. Essentially, a specific hand pose is a result of ap-plying a force on the human hand skeleton. The folding operation can be interpreted as emulating the ”force” ap-plied to the fixed 2D hand skeleton, as shown in Figure 1.
In order to guide folding, HandFoldingNet introduces two novel modules that handle different scales of features: 1) a global-feature guided folding (global folding) decoder and 2) a joint-wise local-feature guided folding (local folding) block.
Inspired by FoldingNet, a global folding decoder folds a 2D hand skeleton into the 3D hand joint coordi-nates. The global feature that guides folding is extracted from the input hand point cloud by a PointNet-based en-coder [29, 9, 12]. The local folding block utilizes local fea-tures as well as spatial dependencies between the joints, in order to augment joint-wise features and correct the coor-dinate estimation. Utilization of local features is supposed to compensate for the weakness of conventional regression-based methods. Additionally, unlike the detection-based methods that propagate local features to all the points, we only extract a small region of local features near each joint, in order to avoid massive computations.
We evaluate our network on ICVL [36], MSRA [35] and NYU [40] datasets, which are challenging benchmarks commonly used for evaluation of a 3D hand pose estimation task. The results show that our network generally outper-forms the previous state-of-the-art methods in terms of both accuracy and efficiency. The proposed network achieves the mean distance errors of 5.95mm, 7.34mm and 8.58mm on the ICVL, MSRA and NYU datasets, respectively. Mean-while, it contains only 1.28M parameters and runs in real-time with 84 frames per second on a single GPU.
The key contributions of this paper are as follows:
• We propose a novel neural network, HandFoldingNet, which takes the hand point cloud as input and estimates the 3D hand joint coordinates based on the multiscale-feature guided folding.
• We propose a global-feature guided folding decoder that infers joint-wise features and coordinates. The joint-wise features help the model exploit natural spa-tial dependencies between the joints for better estima-tion performance.
• We propose joint-wise local-feature guided folding to capture local features and spatial dependencies that augments joint-wise features for higher accuracy.
• We conduct extensive experiments to analyse the effi-ciency and accuracy of our proposed network and its key components.
2.