Abstract
We propose generalized convolutional kernels for 3D re-construction with ConvNets from point clouds. Our method uses multiscale convolutional kernels that can be applied to adaptive grids as generated with octrees.
In addition to standard kernels in which each element has a distinct spatial location relative to the center, our elements have a distinct relative location as well as a relative scale level.
Making our kernels span multiple resolutions allows us to apply ConvNets to adaptive grids for large problem sizes where the input data is sparse but the entire domain needs to be processed. Our ConvNet architecture can predict the signed and unsigned distance ﬁelds for large data sets with millions of input points and is faster and more accurate than classic energy minimization or recent learning approaches.
We demonstrate this in a zero-shot setting where we only train on synthetic data and evaluate on the Tanks and Tem-ples dataset of real-world large-scale 3D scenes. 1.

Introduction
Generating a description of the surface of objects or whole scenes is a key problem in 3D reconstruction. While the acquisition of images and scans becomes easier and eas-ier, combining this information into a global and consistent 3D structure becomes more difﬁcult with the increasing size of the datasets. However, large datasets are particularly in-teresting as they can digitize our 3D world and enable ap-plications like navigation or virtual sightseeing.
An important part of many 3D pipelines is volumetric fusion, which fuses partial observations into a global 3D description. In this approach, the problem of ﬁnding the 2D surface is turned into ﬁnding a 3D scalar ﬁeld from which the surface can be extracted as a level set. An inherent prob-lem of this approach is the cubic growth of the volume lead-ing to high computational costs. Another difﬁculty arises from noisy input data that requires the use of good priors in the fusion process.
To tackle these challenges many works have proposed to use specialized adaptive data structures like octrees to store 3D grids more efﬁciently and adapt algorithms to directly operate on these structures. These types of algorithms are often highly specialized and therefore have high engineer-ing costs. They also often employ PDEs, which implement rather simple priors that prefer surfaces with minimum cur-vature or minimum area.
In contrast to this are learning approaches that can learn complex priors from data, which makes them well suited for the fusion task. Especially ConvNets have become a stan-dard method in image processing pipelines due to their ﬂex-ibility and efﬁciency for data laid out in regular 2D grids.
While ConvNets naturally generalize to 3D data they also suffer from the cubic growth in complexity.
To make use of ConvNets for volumetric fusion, we propose to generalize the standard convolutional kernels to adaptive grids. Adaptive grids not only allow to efﬁciently store data but also allow to capture information at different scales as shown in Figure 1, which is important for the re-construction of large datasets where some regions are more
Figure 2: The input for our method is an oriented point cloud. We aggregate the information in an adaptive grid and use our multiscale convolutional kernels to compute distance functions. In the last stage we decode the distance functions and extract the zero-level set. important. We design multiscale convolutional kernels that span multiple scales. Compared to regular kernels where each element has a distinct spatial position relative to the center, our kernel elements additionally have a relative scale with respect to the center element allowing the network to learn spatial and scale relationships on adaptive grids. We show that we can use our convolutions with a simple U-Net-like architecture to learn an end-to-end trainable volumetric fusion pipeline that computes the signed and unsigned dis-tance ﬁeld.
Our approach achieves signiﬁcantly better reconstruc-tions than classic analytical volumetric fusion approaches and recent learning-based methods. We demonstrate this qualitatively and quantitatively in a zero-shot generalization setting on real-world datasets from Tanks and Temples [16] and [8]. In addition, we can show that our method can re-construct large datasets with hundreds of millions of points and is more than two times faster than the baselines. 2.