Abstract
Multi-person total motion capture is extremely chal-lenging when it comes to handle severe occlusions, dif-ferent reconstruction granularities from body to face and hands, drastically changing observation scales and fast body movements. To overcome these challenges above, we contribute a lightweight total motion capture system for multi-person interactive scenarios using only sparse multi-view cameras. By contributing a novel hand and face boot-strapping algorithm, our method is capable of efficient lo-calization and accurate association of the hands and faces even on severe occluded occasions. We leverage both pose regression and keypoints detection methods and further propose a unified two-stage parametric fitting method for achieving pixel-aligned accuracy. Moreover, for extremely self-occluded poses and close interactions, a novel feedback mechanism is proposed to propagate the pixel-aligned re-constructions into the next frame for more accurate associa-tion. Overall, we propose the first light-weight total capture system and achieves fast, robust and accurate multi-person total motion capture performance. The results and experi-ments show that our method achieves more accurate results than existing methods under sparse-view setups. 1.

Introduction
Marker-less motion capture, due to its great potentials for behaviour understanding, sports analysis, human anima-tion, video editing and virtual reality, has been a popular research topic in computer vision and graphics for decades.
Within this research field, total motion capture, pioneered by [22] using an extremely dense-view setup (hundreds of cameras), shows impressive results of simultaneous cap-ture of multi-person total interactive behaviours including facial expressions, body and hand poses, and has aroused widespread interest in computer vision community. How-ever, this work [22] suffers from expensive and sophisti-cated hardware setup and low run-time efficiency.
Recently, to reduce the capture complexity, more and more researches try to perform total motion capture from
* Corresponding Author
Figure 1. Our lightweight total capture system produces expressive human models with sparse multi-view cameras. only a single image or video [41, 55, 13, 45, 35, 64]. By either optimizing a parametric models like SMPL-X [41] and Adam [22] ( [55]) or regressing the model parame-ters directly from the input images [13], these methods even achieve real-time total motion capture performance for single-person [45, 64]. However, it remains difficult for the monocular methods to handle severe occlusions and chal-lenging poses under multi-person interactive scenarios.
To guarantee both lightweight setups and robust perfor-mance, we propose the first lightweight total capture system using only sparse multi-view cameras. However, extending the existing monocular total capture methods to sparse-view multi-person total capture is not trivial. Although the incor-poration of multi-view observations may resolve the depth ambiguities for monocular methods, the severe occlusions caused by complex-poses and multi-person interactions will significantly deteriorate the performance for current total capture methods. Specifically, the main challenges include: i) hand/face association across multiple views under dras-tically changing observation scales and unstable detection results, ii) pixel-aligned fitting between the reconstructed 3D model and the input images, and iii) robust and accurate body association under severe occlusions even for close in-teractions. To resolve all the challenges above, we propose, as far as we know, the first method to achieve fast, robust and accurate multi-person total motion capture using only light-weight sparse-view cameras.
First of all, compared with the relatively fixed body part scales and satisfactory occlusion-free view point in monoc-ular single-person total capture cases [41, 55, 13, 45, 64],
sparse multi-view setups suffer from hand/face fragments on account of severe occlusions, blurs on hands and even fingers by fast limb movements, and varying hand/face scales across different cameras. Moreover, it remains chal-lenging to associate hands correctly when different hands are located very closely on an image. To resolve these challenges, we propose a novel hand and face bootstrap-ping algorithm to extract accurate body part features effec-tively from the sparse and multi-scale images for accurate association. Benefiting from the recent progress in multi-person skeleton pose capture [62], the skeleton-level results are utilized to guide the following object-detection network for more robust and accurate detection. Moreover, we in-troduce the cross-modality consistency and cross-scale con-sistency to filter unexpected detection results of fragments caused by occlusions or improper view points.
Secondly, using only the pose-regression methods or the key-point detection methods cannot yet guarantee accurate parametric model fitting. Firstly, pose-regression methods
[13, 45, 64] are able to reconstruct decent hand gestures in self-occlusion cases, but these one-shot methods cannot guarantee pixel level alignment with 2D joint positions on the image. On the other hand, keypoint-detection meth-ods [41, 55] are capable of providing pixel-aligned geo-metric features for visible joints, but may need heavy post-processing optimizations, which is quite sensitive to the ini-tialization and usually fails due to self-occlusion. To fully leverage the advantages of both categories and avoid their drawbacks, we propose a new unified two-stage paramet-ric fitting method, in which we leverage the pose-regression result as the initial value to accelerate the convergence for parametric model fitting based on the detected keypoints, and finally achieves pixel-aligned fitting accuracy without losing the efficiency.
Last but not least, for extremely complex poses and close interactions, even 4D association [62] may fail in the body association step, which is an inherent and natural limita-tion for sparse multi-view setups. To this end, we propose a feedback mechanism in which the reconstructed pixel-aligned human parametric models in the previous frame are propagated into the current frame for enhancing soft visi-bility information and finally achieve accurate association result. Benefiting from this novel feedback mechanism, our method is able to capture accurate human behaviours even under scenarios with severe occlusions and close interac-tions.
Our contributions can be concluded as:
• A new hand and face bootstrapping method that involves the body-level skeleton guidance for more accurate body part localization and self-validated consistency scores to filter out the noise of fragmented detection results by un-expected view points or occlusion observations (Sec. 4).
• A new unified two-stage parametric fitting method that fully utilizes both pose-regression and keypoint-detection methods to produce accurate pixel-aligned 3D human models with expressive motion (Sec. 5).
• A new feedback mechanism that propagates the accurate reconstruction into the next frame to further improve the association accuracy especially on the severe occluded occasions (Sec. 6). 2.