Abstract
In real-life applications, machine learning models of-ten face scenarios where there is a change in data distri-bution between training and test domains. When the aim is to make predictions on distributions different from those seen at training, we incur in a domain generalization prob-lem. Methods to address this issue learn a model using data from multiple source domains, and then apply this model to the unseen target domain. Our hypothesis is that when training with multiple domains, conflicting gradients within each mini-batch contain information specific to the individ-ual domains which is irrelevant to the others, including the test domain. If left untouched, such disagreement may de-grade generalization performance. In this work, we char-acterize the conflicting gradients emerging in domain shift scenarios and devise novel gradient agreement strategies based on gradient surgery to alleviate their effect. We val-idate our approach in image classification tasks with three multi-domain datasets, showing the value of the proposed agreement strategy in enhancing the generalization capa-bility of deep learning models in domain shift scenarios. 1.

Introduction
Figure 1. Example images extracted from three multi-domain datasets: PACS [15], VLCS [6] and Office-Home [29]. The goal of domain generalization is to train a model that performs well on data sampled from domains different from those seen during training.
Deep learning models have shown remarkable results in diverse application areas such as image understanding
[13, 28], speech recognition [10, 19] and natural language processing [24, 26]. Such models are typically trained un-der the standard supervised learning paradigm, assuming that training and test data come from the same distribution.
However, in real life, training and test conditions may dif-fer by several factors, such as a change in data acquisition device or target population. This makes models perform poorly when applied to test data whose distribution differs from the training data and, therefore, limits their implemen-tation in such real scenarios. The goal is then to develop deep learning models that generalize outside the training distribution, under domain shift conditions.
Learning a model with data from different domains and then applying it to a new domain not seen during training entails a domain generalization (DG) problem [8]. In the
DG literature, training domains are often called source do-mains, while the test domain is referred as target. The prob-lem itself is highly challenging, since not even unlabeled data from the target domain is accessible during training.
Thus, the model must be trained without information about the target domain. In the particular case of image classifi-cation, for example, different domains may differ in their visual characteristics, e.g. photographic images or more ab-stract representations, such as paintings and sketches (see
Figure 1 for visual examples). In this scenario, the main challenge is how to guide the learning process in order to
capture information that is relevant to the task, and invari-ant to domain changes.
To address the challenges inherent to DG, different strategies have been developed over time. Proposed works i) training and fusing multiple have mainly focused on: domain-specific models [33, 18], ii) learning and extracting common knowledge from multiple source domains, such as domain-invariant representations [20, 7, 16] or domain-agnostic models [11, 15, 14], and iii) increasing the data space through data augmentation [25, 2, 30]. More re-cently, important contributions have been made regarding model selection in the presence of domain shift [8], ignored in most previous works. Albeit the great efforts made by the machine learning and computer vision communities, the gains in performance obtained by current domain general-ization techniques are still modest [2, 5]. Thus, further re-search is still necessary to better understand the reasons be-hind this phenomenon.
In contrast to previous approaches, in this work we are specifically interested in understanding the implications of multi-domain gradient interference in domain generaliza-tion. The recent work of [35] analyzes this problem in the context of multi-task learning (MTL) [3]. The authors find that one of the main optimization issues in MTL arises from gradients from different tasks conflicting with one another, in a way that is detrimental to making progress. The main hypothesis of our work is that multiple domains also give place to conflicting gradients, which are associated with dif-ferent domains instead of tasks. We characterize the con-flicting gradients emerging in domain shift scenarios and devise novel gradient agreement strategies based on gradi-ent surgery to alleviate their effect.
The gradient surgery framework was introduced in [35] to address multi-task learning, and is rooted in a simple and intuitive idea. In general, deep neural networks are trained using gradient descent, where gradients guide the optimiza-tion process through a loss landscape. This landscape is defined by the loss function and the training data. In MTL, a different loss function is employed for each task. This can lead to conflicting gradients, i.e. gradients which may point in opposite directions when associated to different tasks. The usual way to deal with conflicting gradients is to just average them. However, the works of [35, 17] recently showed that simply averaging them can lead to significantly degraded performance. Unlike MTL, in domain general-ization the task remains fixed but we must handle different domains. Here we hypothesize that similar conflicts emerge when training with multiple domains. In this case, conflict-ing gradients within each mini-batch contain information specific to the individual train domains, which is irrelevant to the test domains and, if left untouched, will degrade gen-eralization performance. Thus, we aim to distil domain in-variant information by updating the neural weights in di-rections which encourage gradient agreement among the source domains. Extensive evaluation in image classifica-tion tasks with three multi-domain datasets demonstrate the value of our agreement strategy in enhancing the general-ization capacity of deep learning models under domain shift conditions. 2.