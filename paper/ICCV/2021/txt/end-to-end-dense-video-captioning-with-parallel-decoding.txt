Abstract
Dense video captioning aims to generate multiple as-sociated captions with their temporal locations from the video. Previous methods follow a sophisticated “localize-then-describe” scheme, which heavily relies on numerous hand-crafted components. In this paper, we proposed a sim-ple yet effective framework for end-to-end dense video cap-tioning with parallel decoding (PDVC), by formulating the dense caption generation as a set prediction task. In prac-tice, through stacking a newly proposed event counter on the top of a transformer decoder, the PDVC precisely segments the video into a number of event pieces under the holis-tic understanding of the video content, which effectively in-creases the coherence and readability of predicted captions.
Compared with prior arts, the PDVC has several appealing advantages: (1) Without relying on heuristic non-maximum suppression or a recurrent event sequence selection network to remove redundancy, PDVC directly produces an event set with an appropriate size; (2) In contrast to adopting the two-stage scheme, we feed the enhanced representa-tions of event queries into the localization head and cap-tion head in parallel, making these two sub-tasks deeply interrelated and mutually promoted through the optimiza-tion; (3) Without bells and whistles, extensive experiments on ActivityNet Captions and YouCook2 show that PDVC is capable of producing high-quality captioning results, sur-passing the state-of-the-art two-stage methods when its lo-calization accuracy is on par with them. Code is available at https://github.com/ttengwang/PDVC. 1.

Introduction
As an emerging branch of video understanding, video captioning has received an increasing attention in the re-[2, 12, 13, 21, 32, 10, 15, 19, 20], aiming to cent past generate a natural sentence to describe one main event of
* Corresponding author
Figure 1: The defacto two-stage pipeline vs. the proposed PDVC.
The two-stage “localize-then-describe” pipeline requires a dense-to-sparse proposal generation and selection process before cap-tioning, which contains hand-crafted components and can not ef-fectively exploit the potential mutual benefits between localization and captioning. PDVC adopts the vision transformer to learn at-tentive interaction of different frames, where the learnable event queries are embedded to capture the relevance between the frames and the events. Two prediction heads run in parallel over query features, leveraging the mutual benefits between two tasks and im-proving their performance together. a short video. However, since realistic videos are usually long, untrimmed, and composed of a variety of events with irrelevant background contents, the above single-sentence captioning methods tend to generate sentences of blandness with less information. To circumvent the above dilemma, dense video captioning (DVC) [5, 7, 25, 29, 37] is devel-oped for automatically localizing and captioning multiple events in the video, which could reveal detailed visual con-tents and generate the coherent and complete descriptions.
Intuitively, dense video captioning can be divided into two subtasks, termed event localization and event caption-ing. As shown in Fig. 1, the previous methods usually solve this problem by a two-stage “localize-then-describe” pipeline. It firstly predicts a set of event proposals with ac-curate boundaries. By extracting fine-grained semantic cues
and visual contexts of the proposal, the detailed sentence description is finally decoded by the caption generator. The above scheme is straightforward but suffers from the fol-lowing issues: 1) By considering the captioning as the downstream task, the performance of such a scheme highly relies on the quality of the generated event proposals, which limits the mutual promotion of these two sub-tasks. 2) The performance of proposal generators in previous methods de-pends on careful anchor design [5, 31, 7, 24, 9, 34] and pro-posal selection post-processing (e.g., non-maximum sup-pression [5, 31, 7, 24, 9, 34]). These hand-crafted compo-nents introduce additional hyper-parameters that highly rely on manual thresholding strategies, hindering the progress toward a fully end-to-end captioning generation.
To tackle the above issues, this paper proposes a pure end-to-end dense Video Captioning framework with Paral-lel Decoding termed PDVC. As shown in Fig. 1, instead of invoking the two-stage scheme, we directly feed the inter-mediate representation used for proposal generation into a captioning head that is parallel to the localization head. By doing so, PDVC aims to directly exploit inter-task associ-ation at the feature level. The intermediate feature vectors and the target events could be matched in a one-to-one cor-respondence, making the feature representations more dis-criminative to identify a specific event.
In practice, we consider the dense video captioning task as a set prediction problem. The proposed PDVC directly decodes the frame features, which are extracted from a Vi-sion Transformer, into an event set with their locations and corresponding captions by applying two parallel prediction heads, i.e., localization head and captioning head. Since the appropriate size of the event set is an essential indica-tor for dense captioning quality [9, 48], a newly proposed event counter is also stacked on the top of the Transformer decoder to further predict the number of final events. By introducing such a simple module, PDVC could precisely segment the video into a number of event pieces under the holistic understanding of the video content, avoiding the in-formation missing as well as the replicated caption genera-tion caused by unreliable event number estimation.
We evaluate our model on two large-scale video bench-marks, ActivityNet Captions and YouCook2. Even with a lightweight caption head (vanilla LSTM), our method can achieve comparable performance against state-of-the-art methods which adopts well-designed attention-based
LSTM [24, 34] or Transformer [31]. In addition, we show quantitatively and qualitatively that the generated propos-als gain benefit from the paralleling decoding design. Even with a weakly supervised setting (without location annota-tions), we show our model can implicitly learn the location-aware features from captions.
To summarize, the major contributions of this paper are three folds. 1) We propose a novel end-to-end dense video captioning framework named PDVC by formulating DVC as a parallel set prediction task, significantly simplifying the traditional pipeline which highly depends on hand-crafted components. 2) We further improve PDVC with a novel event counter to estimate the number of events in the video, greatly increasing the readability of generated captions by avoiding the unrealistic event number estimation. 3) Exten-sive experiments on ActivityNet Captions and YouCook2 show state-of-the-art performance over existing methods. 2.