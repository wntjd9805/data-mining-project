Abstract
This paper proposes Panoptic Narrative Grounding, a spatially fine and general formulation of the natural lan-guage visual grounding problem. We establish an ex-perimental framework for the study of this new task, in-cluding new ground truth and metrics, and we propose a strong baseline method to serve as stepping stone for fu-ture work. We exploit the intrinsic semantic richness in an image by including panoptic categories, and we approach visual grounding at a fine-grained level by using segmen-In terms of ground truth, we propose an algo-tations. rithm to automatically transfer Localized Narratives anno-tations to specific regions in the panoptic segmentations of the MS COCO dataset. To guarantee the quality of our an-notations, we take advantage of the semantic structure con-tained in WordNet to exclusively incorporate noun phrases that are grounded to a meaningfully related panoptic seg-mentation region. The proposed baseline achieves a perfor-mance of 55.4 absolute Average Recall points. This result is a suitable foundation to push the envelope further in the development of methods for Panoptic Narrative Grounding. 1.

Introduction
Vision and language skills play a key role in humansâ€™ un-derstanding of the world and they are rarely used indepen-dently. Their interaction is crucial in achieving high-level tasks such as describing objects, narrating a visual scene, or answering questions based on visual cues.
Inspired by these capabilities of human intelligence, researchers have formulated tasks at the intersection of computer vi-sion and natural language processing, such as image cap-tioning [5, 18, 25, 40], referring expression comprehension and segmentation [15, 19, 31, 32], visual question answer-ing [2], among many others.
Current experimental frameworks approach vision and language tasks at different granularity levels. Image cap-Figure 1: Panoptic Narrative Grounding. Given an input image (left) with an associated caption (right), our goal is to produce a panoptic segmentation that grounds its visual objects densely (left). tioning is among the coarsest, aiming at pairing an im-age with a textual description of its content. With increas-ing granularity, there are frameworks that intend to assign specific regions in the image to short descriptions, such as referring expression comprehension and segmentation, and region descriptions in the Visual Genome [19] dataset.
The finest approaches tackle visual grounding at word level, with bounding boxes in the image linked to noun phrases in the caption, as in the Flickr30k Entities [49, 35] dataset.
Given the general dichotomy in computer vision tasks between things (countable objects) and stuff (amorphous regions of similar texture), these datasets mainly focus on things categories. However, several works [8, 1, 10, 17] have highlighted the importance of jointly considering both things and stuff classes towards real-world applications.
Kirillov et al. [17] defined the panoptic segmentation task as the unified formulation of semantic segmentation, which recognizes stuff, and instance segmentation, which detects and segments things.
Recently, Pont-Tuset et al. [36] proposed Localized Nar-ratives, a multimodal dataset for visual grounding based on the natural human task of describing images while simul-taneously pointing with the mouse at the regions being de-scribed. Their grounding annotations have the densest sam-pling of current datasets, in that they visually ground every word in the caption with mouse trace segments. Moreover,
this dataset naturally contains panoptic descriptions of the content of each image, including stuff regions.
Although some of the experimental frameworks above contain sufficiently dense annotations over language, visual annotations are still very sparse and rough. This paper pro-poses Panoptic Narrative Grounding (Figure 1), a spatially finer and more general task formulation of the natural lan-guage visual grounding problem in which (i) we propose a spatially detailed visual grounding that uses segmentations instead of bounding boxes, and (ii) we include all panoptic categories to fully exploit the intrinsic semantic richness in a visual scene. We establish an experimental framework for the study of this task, including new ground-truth annota-tions and metrics, and we propose a strong baseline to serve as stepping stone for future work.
Considering that collecting pixel-wise annotations has a significant cost, we design a method to transfer the Local-ized Narrative annotations to regions into the panoptic seg-mentation annotations provided by the MS COCO [5, 25] dataset. We select a specific region for each noun phrase considering the trace segment associated to it. Since hu-mans use different styles to pointing objects (circling, scrib-bling, and underlining) we evaluate the quality of the an-notation by determining if noun phrases are grounded to a meaningfully related panoptic segmentation region. To do so, we leverage the underlying semantic information in the
WordNet ontology [34]. To handle trace segments being not fully synchronized with the object description, we consider all the regions in the image ranked by relative distance to the visual grounding and assign the closest region with a strong meaning relationship. Finally, for plural noun phrases, we select all regions that (i) are within the tightest bounding box around the mouse trace and (ii) are from the same cat-egory as the main selected region. In terms of evaluation, in contrast to the traditional metric use in phrase-grounding task, we introduce a stricter metric that calculates recall at different Intersection over Union (IoU) thresholds between the target and the predicted mask. Thus, measuring both recognition accuracy and segmentation quality.
We propose a strong baseline that builds upon a state-of-the-art method, Cross-Modality Relevance [52] (CMR), de-veloped for reasoning tasks on language and vision. Specif-ically, this method is designed to perform two classification tasks: Visual Questions Answering [2] and Natural Lan-guage for Visual Reasoning [43, 44]. We generalize the model to perform Panoptic Narrative Grounding. Thus, our baseline is the first natural language visual grounding method able to align multiple noun phrases with panoptic region segmentations within the image.
Our main contributions can be summarized as follows: (1) We propose Panoptic Narrative Grounding, a new for-mulation of the natural language visual grounding prob-lem which, by using panoptic segmentation regions as visual grounding, is spatially denser and more general in semantic terms. (2) We establish an experimental framework for the study of this problem, with annotations coming from the transfer of Localized Narrative annotations to panoptic segmentations in the MS COCO dataset. (3) We introduce the first visual grounding method that matches segmentation regions to specific noun phrases in the caption, which serves as a strong baseline for the task of Panoptic Narrative Grounding.
To ensure the reproducibility of our results and to pro-mote further research on Panoptic Narrative Grounding, we make all the resources of this paper publicly available in our project web page1: our benchmark dataset annotations for the train and validation splits in MS COCO, an implemen-tation of the evaluation metrics, and the pretrained models and source code for our baseline. 2.