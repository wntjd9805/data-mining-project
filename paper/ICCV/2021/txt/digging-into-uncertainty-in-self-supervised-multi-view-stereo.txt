Abstract
Self-supervised Multi-view stereo (MVS) with a pre-text task of image reconstruction has achieved signiﬁcant progress recently. However, previous methods are built upon intuitions, lacking comprehensive explanations about the effectiveness of the pretext task in self-supervised MVS.
To this end, we propose to estimate epistemic uncertainty in self-supervised MVS, accounting for what the model ig-nores. Specially, the limitations can be categorized into two types: ambiguious supervision in foreground and invalid su-pervision in background. To address these issues, we pro-pose a novel Uncertainty reduction Multi-view Stereo (U-MVS) framework for self-supervised learning. To alleviate ambiguous supervision in foreground, we involve extra cor-respondence prior with a ﬂow-depth consistency loss. The dense 2D correspondence of optical ﬂows is used to reg-ularize the 3D stereo correspondence in MVS. To handle the invalid supervision in background, we use Monte-Carlo
Dropout to acquire the uncertainty map and further ﬁlter the unreliable supervision signals on invalid regions. Ex-tensive experiments on DTU and Tank&Temples benchmark show that our U-MVS framework1 achieves the best perfor-mance among unsupervised MVS methods, with competitive performance with its supervised opponents. 1.

Introduction
Multi-view stereo (MVS) [30] is a fundamental com-puter vision problem which aims to recover 3D informa-tion from multiple images on different views. Standing on the shoulder of giants in traditional methods [11, 29], recent learning-based methods [37, 38] have extended the
MVS pipeline to deep neural networks, achieving state-of-the-art performance in several benchmarks [1, 20]. How-ever, the fully supervised learning paradigm suffers from
*Corresponding author. 1Code: https://github.com/ToughStoneX/U-MVS
Figure 1. Illustration of the effectiveness of fully-supervised and self-supervised training in learning-based MVS via the visualiza-tion of uncertainty in supervision. the non-negligible problem of requiring tedious and expen-sive procedures for collecting ground truth depth annota-tions. Hence, it leads the community to consider competi-tive alternative of learning-based approaches which require fewer labels.
A prominent and appealing trend is to construct a self-supervised MVS task [12, 19, 7, 15], which further trans-forms the original depth estimation problem as RGB im-age reconstruction problem. However, previous methods are merely built upon intuitive motivations, lacking com-prehensive explanations about which image regions such self-supervision signal can effectively work for multi-view depth estimation. For fully-supervised MVS (Fig. 1(a)), the regions where supervision exists are explicit, if given the ground truth depth map. Whereas, for self-supervised
MVS shown in Fig. 1(b), the pretext task of image recon-struction actually provides indistinct supervision based on color similarity, which is agnostic to the exact presence of
Figure 2. Visualization of epistemic uncertainty in fully-supervised and self-supervised MVS. supervision in depth estimation. Hence, to provide a direct proof of the effectiveness in supervision, we utilize Monte-Carlo Dropout [18] to visualize the epistemic uncertainty for a comprehensive insight (Fig. 1(c)). In Bayesian mod-eling [8], the epistemic uncertainty inherently reﬂects what the supervision ignores.
In Fig.
What can we know from uncertainty? 2, we provide a direct comparison of uncertainty between fully-supervised and self-supervised MVS to explicitly un-derstand what factors may lead to the failure of self-supervision. It is found that the uncertain regions in self-supervision are more than the ones in fully-supervised train-ing. Revisiting the premise of self-supervision as an image reconstruction task, the problem can be distinguished into two groups: (1) Ambiguous supervision in foreground (Fig. 2(a)). Under the inﬂuence of unexpected factors such as color variation and occlusion in the foreground object, the pretext task of image reconstruction is inconsistent with the photometric consistency and unable to reﬂect the correct depth information. (2) Invalid supervision in background (Fig. 2(b)). The textureless background provides no ef-fective clues for depth estimation task, which is ignored in fully-supervised training. Whereas, the pretext task of im-age recostruction takes the whole image including the tex-tureless backgrounds into consideration, involving invalid supervisions and oversmoothing the self-supervised results.
How to handle these uncertainties? To address these problems, we propose a novel Uncertainty reduction Multi-view Stereo framework U-MVS for self-supervised learn-ing. It mainly consists of two distinct designs as follows: (1) To handle ambiguous supervision in foreground, we aim to append extra prior of correspondence to strengthen the reliability of self-supervision, and introduce a new multi-view ﬂow-depth consistency loss. The intuition is that the dense 2D correspondence of optical ﬂow can be utilized to regularize the 3D stereo correspondence in self-supervised
MVS. A differentiable Depth2Flow module is proposed to convert the depth map to virtual optical ﬂow among views and the RGB2Flow module unsupervisedly predict the op-tical ﬂow from corresponding views. Then the virtual ﬂow and the real ﬂow are enforced to be consistent. (2) To handle invalid supervision in background, we suggest to ﬁlter the unreliable supervision signals on invalid regions, and pro-pose an uncertainty-aware self-training consistency loss. In a totally unsupervised setting, we ﬁrstly annotate the dataset with a self-supervisedly pretrained model, while acquiring the uncertainty map with Monte-Carlo Dropout. Then the pseudo label ﬁltered by the uncertainty map is used to super-vise the model. Random data-augmentations on the input multi-view images are appended to enforce the robustness towards disturbance on the areas with valid supervision.
In summary, our contributions are: (1) We propose a novel self-supervised framework to handle the problems in-vestigated from the visualization analysis about the uncer-tainty gap between supervised and self-supervised supervi-sion signals. (2) We propose a novel self-supervision sig-nal based on the cross-view consistency of optical ﬂows (3) We pro-and depth maps among arbitrary views. pose an uncertainty-aware self-training consistency loss for self-supervised MVS. (4) Experimental results on DTU and Tanks&Temples show that our proposed method can achieve competitive performance with its supervised coun-terparts with same backbones. 2.