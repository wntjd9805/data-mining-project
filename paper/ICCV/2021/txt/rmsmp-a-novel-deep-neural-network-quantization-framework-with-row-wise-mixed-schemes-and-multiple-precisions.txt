Abstract
This work proposes a novel Deep Neural Network (DNN) quantization framework, namely RMSMP, with a Row-wise
Mixed-Scheme and Multi-Precision approach. Specifically, this is the first effort to assign mixed quantization schemes and multiple precisions within layers – among rows of the
DNN weight matrix, for simplified operations in hardware inference, while preserving accuracy. Furthermore, this pa-per makes a different observation from the prior work that the quantization error does not necessarily exhibit the layer-wise sensitivity, and actually can be mitigated as long as a certain portion of the weights in every layer are in higher precisions. This observation enables layer-wise uniformal-ity in the hardware implementation towards guaranteed in-ference acceleration, while still enjoying row-wise flexibil-ity of mixed schemes and multiple precisions to boost ac-curacy. The candidates of schemes and precisions are de-rived practically and effectively with a highly hardware-informative strategy to reduce the problem search space.
With the offline determined ratio of different quantization schemes and precisions for all the layers, the RMSMP quan-tization algorithm uses Hessian and variance based method to effectively assign schemes and precisions for each row.
The proposed RMSMP is tested for the image classification and natural language processing (BERT) applications, and achieves the best accuracy performance among state-of-the-arts under the same equivalent precisions. The RMSMP is implemented on FPGA devices, achieving 3.65× speedup in the end-to-end inference time for ResNet-18 on ImageNet, comparing with the 4-bit Fixed-point baseline. 1.

Introduction
With tremendous success of the deep learning or deep neural networks (DNNs), there exist urgent needs for de-ployments of inference models onto edge-computing plat-*Equal contribution. forms/devices. As a major type of model compression tech-nique, the DNN quantization becomes an essential method to reduce the computation, memory, and storage require-ments in on-device inference, especially for platforms with capability of customized architecture design, such as FPGA devices and ASIC chips. Generally speaking, DNN quanti-zation learns DNN models in low bit-width representation with accuracy performance close to that of the full-precision models, while accelerating inference speed.
Various quantization schemes have been investigated including binary [5, 6, 26, 22], ternary[20, 15, 39],
Fixed-point (Fixed) [38, 4, 12, 16, 3, 11], Power-of-Two (PoT) [24, 37, 19, 36], Additive Power-of-Two (APoT) [21], etc. Those schemes have diverse accuracy and hardware performance. Binary and ternary significantly re-duce computation by eliminating multiplication operations, but experience relatively large accuracy loss (> 2% in gen-eral). Low bit-width Fixed quantization has better accuracy
Figure 1. The proposed DNN quantization framework with row-wise mixed schemes and multiple precisions, which as-signs quantization scheme and precision to filters of the weight tensor (or rows of the weight matrix). It features (i) layer-wise uniformality to fulfill the requirement of practical hardware imple-mentation, (ii) row-wise flexibility for mixed schemes and multi-ple precisions, (iii) hardware-informative selection of candidate schemes and precisions (bit-widths) for significantly reducing the algorithm search space, and (iv) superior accuracy performance among the state-of-the-arts.
performance. For example, 4-bit Fixed can achieve negli-gible accuracy loss comparing with its 32-bit floating-point counterpart, although it still needs multiplication operations during inference computation.
Different from binary, ternary, and Fixed, PoT is a non-linear quantization scheme, where with quantization levels as power-of-two numbers, multiplications can be replaced with bit shifting operations, thereby reducing computation to speedup inference. However, PoT still results in mod-erate accuracy loss (1% ∼ 2%), which is due to the rigid resolution issue [21] that exhibits a high resolution around the mean and a low resolution at the tails of weight distribu-tion and that cannot be resolved even with higher bit-width.
Therefore, APoT was proposed by representing quantiza-tion levels as the sum of multiple power-of-two numbers, overcoming the rigid resolution issue of PoT, while enjoy-ing non-multiplication operations.
While exploring various quantization schemes, people found that the first and the last layers are of importance for preserving accuracy, and therefore in the above-mentioned works, the first and the last layers are either unquantized (in 32-bit floating-point) or quantized with no fewer than 8 bits.
Motivated by this, the layer-wise multi-precision quantiza-tion has been well investigated in [31, 33, 29, 10, 9, 34, 28], where various methods/algorithms have been used to deal with the large search space of the precision assignment for both weights and activations in each layer.
This work proposes a novel DNN quantization frame-work, namely RMSMP, with a Row-wise Mixed-Scheme and Multi-Precision approach. Figure 1 illustrates the pro-posed quantization framework on the DNN (a) weight ten-sor and (b) weight matrix. Specifically, each filter in the weight tensor or each row in the weight matrix is assigned a combination of quantization scheme and precision. The candidates of schemes and precisions are derived practi-cally for facilitating hardware implementation, and effec-tively for speeding up inference with the given resources on the hardware platforms/devices, while with the capability to preserve the accuracy as the unquantized (32-bit floating-point) models. This highly hardware-informative quantiza-tion strategy significantly reduces the search space of the
DNN quantization problem, making our framework distinc-tive from existing multi-precision quantization works.
This is the first effort to apply mixed quantization schemes and multiple precisions within layers, targeting for simplified operations in hardware inference, while preserv-ing the accuracy. Specifically, two quantization schemes i.e., Power-of-Two (PoT) and Fixed-point (Fixed), and two precisions i.e., 4-bit and 8-bit are adopted and explored for quantization on weights and activations, to reduce in-ference computation and preserve accuracy. Furthermore, as demonstrated by the previous works that either implic-itly use higher precisions for the first and the last layers, or explicitly assign multiple precisions to different layers, it is essential to use higher precisions at least for parts of the model, to boost the accuracy of quantized models close to that of the full-precision models. However, different from existing works, this paper makes the observation that this does not necessarily relate to layer-wise sensitivity, in-stead, as long as a certain portion of the weights in every layer use higher precisions, the quantization error can be mitigated. This observation enables layer-wise uniformal-ity towards practical hardware implementation of quantized models, while still enjoying row-wise flexibility of mixed schemes and multiple precision. The contributions of our quantization framework are summarized as follows:
• A novel row-wise mixed-scheme and multiple-precision quantization approach.
• A highly hardware-informative solution strategy significantly reducing the problem search space.
• The best accuracy performance when under the same equivalent precision as the existing works.
• The significant inference speedup on real devices, comparing to the network-wise uniform low-bit quantization i.e., the speed upper bound of the pop-ular layer-wise multi-precision approaches. 2.