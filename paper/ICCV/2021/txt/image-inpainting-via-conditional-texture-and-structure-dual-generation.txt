Abstract
Deep generative approaches have recently made consid-erable progress in image inpainting by introducing struc-ture priors. Due to the lack of proper interaction with im-age texture during structure reconstruction, however, cur-rent solutions are incompetent in handling the cases with large corruptions, and they generally suffer from distorted results. In this paper, we propose a novel two-stream net-work for image inpainting, which models the structure-constrained texture synthesis and texture-guided struc-ture reconstruction in a coupled manner so that they better leverage each other for more plausible generation. Further-more, to enhance the global consistency, a Bi-directional
Gated Feature Fusion (Bi-GFF) module is designed to ex-change and combine the structure and texture information and a Contextual Feature Aggregation (CFA) module is de-veloped to reﬁne the generated contents by region afﬁn-ity learning and multi-scale feature aggregation. Quali-tative and quantitative experiments on the CelebA, Paris
StreetView and Places2 datasets demonstrate the superi-ority of the proposed method. Our code is available at https://github.com/Xiefan-Guo/CTSDG. 1.

Introduction
Image inpainting [3] refers to the process of reconstruct-ing damaged regions of an image while simultaneously maintaining its overall consistency, which is a typical low-level visual task with many practical applications, such as photo editing, distracting object removal, and restoring cor-rupted parts.
As with most computer vision problems, image inpaint-ing has been largely advanced by the widespread use of deep learning during the past decade. Different from the traditional methods [2, 5] that gradually ﬁll in missing areas by searching for the most similar patches from known re-gions, the deep generative ones [19, 7, 31, 33] capture more high-level semantics and do a better job for images with
*Corresponding author. (a) (b) (c) (d)
Figure 1: High-quality inpainting results. From left to right: (a) input corrupted images, (b) our reconstructed structures, (c) our ﬁlled results, and (d) ground-truth images. non-repetitive patterns. There also exists another trend to combine the advantages of deep generative and traditional patch-based methods for image inpainting [35, 30, 24, 15], delivering inpainting contents with both realistic textures and plausible semantics. Moreover, updated versions of vanilla convolution are investigated [13, 27, 36], where op-erations are masked and normalized to be conditioned only on valid pixels, achieving promising performance for irreg-ular corruptions. Nevertheless, the methods above expose a common drawback in recovering the global structure of the image, as a generative network is not as powerful as ex-pected for this issue.
To deal with this problem, a number of multi-stage meth-ods are proposed to explicitly incorporate structure model-ing, which hallucinate structures of missing regions in the
ﬁrst stage and use them to guide pixel generation in the sec-ond stage. For instance, EdgeConnect [18] encodes such structures by edges, while [20] and [28] adopt interme-diate edge-preserved smooth images and foreground con-tours. These alternatives show the results with improved structures and textures. Unfortunately, acquiring reason-able edges from corrupted images is itself a very challeng-ing task, and taking unstable structural priors tends to incur large errors in those series-coupled frameworks.
More recently, a few attempts mix the modeling pro-cesses of structures and textures. PRVS (Progressive Re-construction of Visual Structure) [10] and MED (Mutual
Encoder-Decoder) [14] are the representatives, and they generally exploit a shared generator for both textures and structures. Despite some performance gains reported, the relationship between structures and textures is not fully con-sidered in this single entangling architecture. In particular, since image structures and textures correlate throughout the network, it is difﬁcult for them to convey holistic comple-mentary information to assist the other side. Such a fact indicates that there is still much space for improvement.
In this paper, we propose a novel two-stream net-work which casts image inpainting into two collaborative subtasks, i.e., structure-constrained texture synthesis and
In this way, the texture-guided structure reconstruction. two parallel-coupled streams are individually modeled and combined to complement each other. Correspondingly, a two-branch discriminator is developed to estimate the per-formance of this generation, which supervises the model to synthesize realistic pixels and sharp edges simultaneously for global optimization. In addition, we introduce a novel
Bi-directional Gated Feature Fusion (Bi-GFF) module to integrate the rebuilt structure and texture feature maps to enhance their consistency, along with a Contextual Feature
Aggregation (CFA) module to highlight the clues from dis-tant spatial locations to render ﬁner details. Due to the dual generation network as well as the speciﬁcally designed modules, our approach is able to achieve more visually con-vincing structures and textures (see Figure 1, zoom in for a better view).
Experiments are extensively conducted on the CelebA
[16], Paris StreetView [4] and Places2 [39] datasets for eval-uation. Qualitative and quantitative results demonstrate that our model signiﬁcantly outperforms the state-of-the-art.
The main novelties and contributions are as follows:
• We propose a novel two-stream network for image in-painting, which models structure-constrained texture synthesis and texture-guided structure reconstruction in a coupled manner so that the dual generation tasks better facilitate each other for more accurate results.
• We design a Bi-directional Gated Feature Fusion (Bi-GFF) module to share and combine information be-tween the structure and texture features for consistency enhancement and a Contextual Feature Aggregation (CFA) module to yield more vivid details by model-ing long-term spatial dependency.
• We achieve the new state-of-the-art performance on multiple public benchmarks both qualitatively and quantitatively. 2.