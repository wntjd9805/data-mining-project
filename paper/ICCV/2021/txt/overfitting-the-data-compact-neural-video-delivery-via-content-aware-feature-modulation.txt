Abstract 1.

Introduction
Internet video delivery has undergone a tremendous explosion of growth over the past few years. However, the quality of video delivery system greatly depends on the
Internet bandwidth. Deep Neural Networks (DNNs) are utilized to improve the quality of video delivery recently.
These methods divide a video into chunks, and stream LR video chunks and corresponding content-aware models to the client. The client runs the inference of models to super-resolve the LR chunks. Consequently, a large number of models are streamed in order to deliver a video.
In this paper, we ﬁrst carefully study the relation between models of different chunks, then we tactfully design a joint training framework along with the Content-aware Feature
Modulation (CaFM) layer to compress these models for neural video delivery. With our method, each video chunk only requires less than 1% of original parameters to be streamed, achieving even better SR performance.
We conduct extensive experiments across various SR backbones, video time length, and scaling factors to demonstrate the advantages of our method. Besides, our method can be also viewed as a new approach of video cod-ing. Our primary experiments achieve better video quality compared with the commercial H.264 and H.265 standard under the same storage cost, showing the great potential of the proposed method. Code is available at:https:
//github.com/Neural-video-delivery/
CaFM-Pytorch-ICCV2021
∗Equal Contribution.
†This work was done when Jiaming Liu was an intern at Intel Labs
China supervised by Ming Lu
‡Chuang Zhang is responsible for correspondence.
Internet video is achieving explosive growth over the past few years, which brings a huge burden to the video de-livery infrastructure. The quality of video heavily depends on the bandwidth between servers and clients. Techniques at both sides evolve over time to handle the scalability chal-lenges at Internet scale. Inspired by the increasing compu-tational power of client/server and recent advances in deep learning, several works are proposed to apply Deep Neural
Networks (DNNs) to video delivery system [14, 30]. The core idea of these works is to stream both the low resolu-tion video and content-aware models from servers to clients.
The clients run the inference of models to super-resolve the
LR videos. In this manner, better user Quality of Experi-ence (QoE) can be obtained under limited Internet band-width.
In contrast to current approaches on Single Image Super-Resolution (SISR) [24, 8, 20, 32, 15] and Video Super-Resolution (VSR) [2, 26, 3], content-aware DNNs leverage neural network’s overﬁtting property and use the training accuracy to achieve high performance. Speciﬁcally, a video is ﬁrst divided into several chunks, and then a separate DNN is trained for each chunk. The low resolution chunks and corresponding trained models are delivered to the clients over the Internet. Different backbones [24, 8, 20, 32, 15] can be used as the DNN for each chunk. This kind of
DNN-based video delivery system has achieved better per-formance compared with commercial video delivery tech-niques like WebRTC [14].
Although it is promising to apply DNNs to video deliv-ery, existing methods still have several limitations [18]. One major limitation is that they need to train one DNN for each chunk, resulting in a large number of separate models for a long video. This brings additional storage and bandwidth
cost for the practical video delivery system. In this paper, we ﬁrst carefully study the relation between models of dif-ferent chunks. Although these models are trained to overﬁt different chunks, we observe the relation between their fea-ture maps is linear and can be modelled by a Content-aware
Feature Modulation (CaFM) layer. This motivates us to de-sign a method, which allows the models to share the most of parameters and preserve only private CaFM layers for each chunk. However, directly ﬁnetuning the private parameter fails to obtain competitive performance compared with sep-arately trained models. Therefore, we further design a tact-ful joint training framework, which trains the shared param-eters and private parameters simultaneously for all chunks.
In this way, our method can achieve relative better perfor-mance compared with individually trained models.
Apart from video delivery, our method can also be con-sidered as a new approach of video coding. We conduct primary experiments to compare the proposed approach against commercial H.264 and H.265 standards under the same storage cost. Our method can achieve higher PSNR performance thanks to the overﬁtting property, showing the great potential of the proposed approach.
Our contributions can be concluded as follows:
• We propose a novel joint training framework along with the content-aware feature modulation layer for neural video delivery.
• We conduct extensive experiments across various SR backbones, video time length, and scaling factors to demonstrate the advantages of our method.
• We compare with commercial H.264 and H.265 stan-dard under the same storage cost and show promising results thanks to the overﬁtting property. 2.