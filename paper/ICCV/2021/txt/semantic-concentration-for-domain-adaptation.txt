Abstract
Domain adaptation (DA) paves the way for label annota-tion and dataset bias issues by the knowledge transfer from a label-rich source domain to a related but unlabeled tar-get domain. A mainstream of DA methods is to align the feature distributions of the two domains. However, the ma-jority of them focus on the entire image features where ir-relevant semantic information, e.g., the messy background, is inevitably embedded. Enforcing feature alignments in such case will negatively influence the correct matching of objects and consequently lead to the semantically neg-ative transfer due to the confusion of irrelevant seman-tics. To tackle this issue, we propose Semantic Concentra-tion for Domain Adaptation (SCDA), which encourages the model to concentrate on the most principal features via the pair-wise adversarial alignment of prediction distributions.
Specifically, we train the classifier to class-wisely maximize the prediction distribution divergence of each sample pair, which enables the model to find the region with large dif-ferences among the same class of samples. Meanwhile, the feature extractor attempts to minimize that discrepancy, which suppresses the features of dissimilar regions among the same class of samples and accentuates the features of principal parts. As a general method, SCDA can be easily integrated into various DA methods as a regularizer to fur-ther boost their performance. Extensive experiments on the cross-domain benchmarks show the efficacy of SCDA. 1.

Introduction
Deep neural network (DNN) has achieved great success in diverse machine learning problems [17, 3, 33]. Unfor-tunately, the impressive performance gain heavily relies on the access to massive well-labeled training data. And it is often time and cost prohibitive to manually annotate suf-ficient training data in practice. Besides, another drawback of conventional deep learning is the poor generalization on a
∗Corresponding author.
Figure 1. Illustration of the adversarial process of SCDA at the macro level. Classifier is trained to maximize the prediction distri-bution discrepancy of samples in the pairing region, which causes the decision boundary to pass through the high density area of the pairing region. While the feature extractor tries to minimize that discrepancy, which pushes the features away from the decision boundary. Finally, well-aligned features can be obtained through the adversarial game between the classifier and feature extractor. new dataset, due to the domain shift issue [2, 29, 1]. Hence, there is a strong motivation to utilize the knowledge of a label-rich domain (i.e., source domain) to assist the learn-ing in a related but unlabeled domain (i.e., target domain), which is often referred to as domain adaptation (DA).
To alleviate the domain shift problem, the common prac-tice of DA is to reduce the cross-domain distribution dis-crepancy by learning domain-invariant feature representa-tions. Generally, these DA methods can be roughly cate-gorized as the discrepancy-based methods [23, 26, 7, 12], which align the domain distributions by minimizing a well-designed statistic metric, and the adversarial-based methods
[8, 24, 41, 37, 20, 31], where the domain discriminator is designed to distinguish between source and target samples and the feature extractor tries to confuse the discriminator.
Although these DA methods have admittedly achieved promising results, most of them use the features encoded without emphasis to match the feature distributions of two domains.
In such case, irrelevant semantic information, e.g., the messy background is inevitably embedded, which may negatively influence the correct matching of objects and consequently lead to the semantically negative transfer.
To relieve this issue, we propose to achieve the Semantic
Concentration for Domain Adaptation (SCDA) by leverag-ing the dark knowledge [49] (i.e., knowledge on the wrong predictions). Actually, SCDA is motivated by the findings in [53] that the class prediction made by the model depends on what it has concentrated on and the concentrated region for each class prediction can be located with the feature maps and corresponding classification weights. Thus, we expect to find the concentrated regions for wrong predic-tions and suppress the features of these regions when en-coding the image into features.
For this purpose, we propose to class-wisely align the pair-wise prediction distributions in an adversarial manner, which is shown in Fig. 1. Samples of the same label from two domains compose the pairing region for each class. The paring of samples includes intra-domain paring (i.e., pairing within source domain) and inter-domain pairing (i.e., pair-ing between source and target samples). For any sample pair of the same label, the classifier is trained to maximize their prediction distribution discrepancy, while the feature extractor strives to minimize that discrepancy. From the micro perspective, when the feature extractor is fixed, maxi-mizing the prediction distribution discrepancy of the sample pair will cause the classification weights for dark knowl-edge to be larger. Then to reduce that discrepancy, features of these dark knowledge have to be suppressed, since the classification weights for them became larger in the previ-ous training of the classifier. From the macro perspective, to maximize the prediction discrepancy in the pairing region with the feature extractor fixed, the decision boundary will cross the high density area of the pairing region. Then, to reduce the discrepancy, features will be pushed away from the decision boundary. Finally, the model is able to con-centrate on the most principal features and achieves well-aligned features class-wisely via the min-max game.
Our contributions are summarized as follows:
• This paper proposes a novel adversarial method for
DA, i.e., the pair-wise adversarial alignment of pre-diction distribution discrepancy. Our method can sup-press the irrelevant semantic information and accen-tuate the class object when encoding features, thus achieving the semantic concentration.
• As a simple and generic method, SCDA can be eas-ily integrated as a regularizer into various DA methods and greatly improve their adaptation performances.
• Extensive experimental results and analysis demon-strate that SCDA greatly suppresses irrelevant seman-tics during the adaptation process, yielding state-of-the-art results on multiple cross-domain benchmarks. 2.