Abstract
When a camera is pointed at a strong light source, the re-sulting photograph may contain lens flare artifacts. Flares appear in a wide variety of patterns (halos, streaks, color bleeding, haze, etc.) and this diversity in appearance makes flare removal challenging. Existing analytical solutions make strong assumptions about the artifact’s geometry or brightness, and therefore only work well on a small sub-set of flares. Machine learning techniques have shown suc-cess in removing other types of artifacts, like reflections, but have not been widely applied to flare removal due to the lack of training data. To solve this problem, we explicitly model the optical causes of flare either empirically or using wave optics, and generate semi-synthetic pairs of flare-corrupted and clean images. This enables us to train neural networks to remove lens flare for the first time. Experiments show our data synthesis approach is critical for accurate flare removal, and that models trained with our technique gener-alize well to real lens flares across different scenes, lighting conditions, and cameras. 1.

Introduction
Photographs of scenes with a strong light source often exhibit lens flare—a salient visual artifact caused by unin-tended reflections and scattering within the camera. Flare artifacts can be distracting, reduce detail, and occlude image content. Despite significant efforts in optical design to min-imize lens flare, even small light sources can still produce substantial artifacts when imaged by consumer cameras.
Flare patterns depend on the optics of the lens, the loca-tion of the light source, manufacturing imperfections, and scratches and dust accumulated through everyday use. The diversity in the underlying cause of lens flare leads to the di-versity in its presentation. As Fig. 1 shows, typical artifacts
†This work was done while Yicheng Wu was an intern at Google Re-search. He is currently a Research Scientist at Snap Research.
Figure 1. Lens flare artifacts frequently occur in photographs with strong light sources. They exhibit a wide range of shapes and col-ors, which makes them difficult to remove with existing methods. include halos, streaks, bright lines, saturated blobs, color bleeding, haze, and many others. This diversity makes the problem of flare removal exceedingly challenging.
Most existing methods for lens flare removal [1, 3, 21] do not account for the physics of flare formation, but rather na¨ıvely rely on template matching or intensity thresholding to identify and localize the artifact. As such, they can only detect and potentially remove limited types of flares, such as saturated blobs, and do not work well in more complex real-world scenarios.
Despite the proliferation of deep neural networks, there seems to be no successful attempt at learning-based flare removal. What is it that makes this problem so hard?
The main challenge is the lack of training data. Collect-ing a large number of perfectly-aligned image pairs with and without lens flare would be tedious at best and impos-sible at worst: the camera and the scene would need to be static (a particularly difficult requirement given most lens flare occurs outdoors and involves the sun), and one would need some mechanism to “switch” the artifacts on and off without also changing the illumination of the scene. With significant effort this can be accomplished by collecting pairs of images taken on a tripod where the photographer manually places an occluder between the illuminant and the camera in one image. But this approach is too labor-intensive to produce the thousands or millions of image pairs usually required to train a neural network. Further-more, this approach only works when the flare-causing illu-minant lies outside of the camera’s field of view (e.g., real scenes in Fig. 7), which limits its utility.
To overcome this challenge, we propose to generate semi-synthetic data grounded on the principles of physics.
We make the key observation that lens flare is an additive layer on top of the underlying image, and that it is induced by either scattering or internal reflection. For the scatter-ing case (e.g., scratches, dust, other defects), we construct a wave optics model that we demonstrate closely approxi-mates reality. For the unintended reflections between lens elements, we adopt a rigorous data-driven approach, as an accurate optical model for a commercial camera is often un-available. With this formulation, we are able to generate a large and diverse dataset of semi-synthetic flare-corrupted images, paired with ground-truth flare-free images.
Another challenge is removing flare while keeping the visible light source intact. This is hard even with our semi-synthetic data, as we cannot separate the light source from the flare-only layer without affecting the flare it induces.
Hence, if trained na¨ıvely, the network will try to remove the light source along with the flare, leading to unrealistic out-puts. To this end, we propose a loss function that ignores the light source region, and a post-processing step to preserve the light source in the output.
To show the effectiveness of our dataset and procedures, we train two distinct convolutional neural networks origi-nally designed for other tasks. During training, we mini-mize a loss function on both the predicted flare-free image and the residual (i.e., inferred flare). At test time, the net-works require only a single RGB image taken with a stan-dard camera and are able to remove different types of flare across a variety of scenes. Although trained exclusively on semi-synthetic data, both models generalize well to real-world images. To the best of our knowledge, this is the first general-purpose method for removing lens flare from a single image.
Our code and datasets are publicly available at https://yichengwu.github.io/flare-removal/. 2.