Abstract
Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be fea-sible to get some target data labeled, but to be cost-effective it is desirable to select a maximally-informative subset via active learning (AL). We study the problem of AL under a domain shift, called Active Domain Adaptation (Active DA).
We demonstrate how existing AL approaches based solely on model uncertainty or diversity sampling are less effective for Active DA. We propose Clustering Uncertainty-weighted
Embeddings (CLUE), a novel label acquisition strategy for
Active DA that performs uncertainty-weighted clustering to identify target instances for labeling that are both uncertain under the model and diverse in feature space. CLUE con-sistently outperforms competing label acquisition strategies for Active DA and AL across learning settings on 6 diverse domain shifts for image classification. Our code is available at https://github.com/virajprabhu/CLUE. 1.

Introduction
Deep neural networks excel at learning from large labeled datasets but struggle to generalize this knowledge to new tar-get domains [32, 42]. This limits their real-world utility, as it is impractical to collect a large new dataset for every new deployment domain. Further, all target instances are usually not equally informative, and it is far more cost-effective to identify maximally informative target instances for labeling.
While Active Learning [2, 6, 8, 9, 35, 36] has extensively studied the problem of identifying informative instances for labeling, it typically focuses on learning a model from scratch and does not operate under a domain shift. In many practical scenarios, models are trained in a source domain and deployed in a different target domain, often with addi-tional domain adaptation [10, 17, 32, 44]. In this work, we study the problem of active learning under such a domain shift, called Active Domain Adaptation [29] (Active DA).
Concretely, given i) labeled data in a source domain, ii) unlabeled data in a target domain, and iii) the ability to ob-tain labels for a fixed budget of target instances, the goal of Active DA is to select target instances for labeling and
∗Work done partially at Georgia Tech.
Figure 1: The goal of Active Domain Adaptation [29] (Active
DA) is to adapt a source model to an unlabeled target domain by acquiring labels for selected target instances via an oracle. Existing active learning (AL) methods based solely on uncertainty [9, 30, 45] or diversity-sampling [13, 35] are less effective for Active DA (Row 1, 2). We propose CLUE, an AL method designed for Active DA that selects instances that are both uncertain (thus informative to the model) and diverse in feature space (thus minimizing redundancy,
Row 3), and leads to more cost-effective adaptation than competing
AL and Active DA methods (Sec. 4.4). learn a model with high accuracy on the target test set. Ac-tive DA has widespread utility as a means of cost-effective adaptation from cheaper to more expensive sources of labels (e.g. synthetic to real data), as well as when the quantity (e.g. autonomous driving) or cost (e.g. medical diagnosis) of label-ing in the target domain is prohibitive. Despite its practical utility, it is a challenging task that has seen limited follow-up work since its introduction over ten years ago [5, 29, 40].
The traditional AL setting typically focuses on techniques to select samples to efficiently learn a model from scratch, rather than adapting under a domain shift [36]. As a re-sult, existing state-of-the art AL methods based on either uncertainty or diversity sampling are less effective for Active
DA. Uncertainty sampling selects instances that are highly uncertain under the model’s beliefs [8, 9, 20, 41]. Under a domain shift, uncertainty estimates on the target domain may
be miscalibrated [39] and lead to sampling uninformative, outlier, or redundant instances (Fig. 1, top). A parallel line of work in AL based on diversity sampling instead selects instances dissimilar to one another in a learned embedding space [13, 35, 38]. In Active DA, this can lead to sampling uninformative instances from regions of the feature space that are already well-aligned across domains (Fig. 1, middle).
As a result, solely using uncertainty or diversity sampling is suboptimal for Active DA, as we demonstrate in Sec 4.4.
Recent work in AL and Active DA has sought to combine uncertainty and diversity sampling. AADA [40], the state-of-the-art Active DA method, combines uncertainty with diversity measured by ‘targetness’ under a learned domain discriminator. However, targetness does not ensure that the selected instances are representative of the entire target data distribution (i.e. not outliers), or dissimilar to one another.
Ash et al. [2] instead propose performing clustering in a hallucinated “gradient embedding” space. However, they rely on distance-based clustering in high-dimensional spaces, which often leads to suboptimal results.
In this work, we propose a novel label acquisition strategy for Active DA that combines uncertainty and diversity sam-pling in a principled manner without the need for complex gradient or domain discriminator-based diversity measures.
Our approach, Clustering Uncertainty-weighted Embeddings (CLUE), identifies informative and representative target in-stances from dense regions of the feature space. To do so,
CLUE clusters deep embeddings of target instances weighted by the corresponding uncertainty of the target model. Our weighting scheme effectively increases the density of in-stances proportional to their uncertainty. To construct non-redundant batches, CLUE then selects nearest neighbors to the inferred cluster centroids for labeling. Our algorithm then leverages the acquired target labels and, optionally, the labeled source and unlabeled target data, to update the model, consistently leading to more cost-effective domain alignment than competing (and frequently more complex) alternatives.
Contributions: 1. We benchmark the performance of state-of-the-art meth-ods for active learning on challenging domain shifts, and find that methods based purely on uncertainty or diversity sampling are not effective for Active DA. 2. We present CLUE, a novel and easy-to-implement label acquisition strategy for Active DA that uses uncertainty-weighted clustering to identify instances that are both uncertain under the model and diverse in feature space. 3. We present results on 6 diverse domain shifts from the DomainNet [27], Office [32], and DIGITS [22, 26] benchmarks for image classification. Our method
CLUE improves upon both the previous state-of-the art in Active DA across shifts (by as much as 9% in some cases), as well as state-of-the-art methods for active learning, across multiple learning strategies. 2.