Abstract
We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models.
Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early lay-ers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We eval-uate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recog-nition tasks where it outperforms concurrent vision trans-formers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classiﬁcation where it outperforms prior work on vision transformers. Code is available at: https:
//github.com/facebookresearch/SlowFast. 1.

Introduction
We begin with the intellectual history of neural network models for computer vision. Based on their studies of cat and monkey visual cortex, Hubel and Wiesel [60] developed a hierarchical model of the visual pathway with neurons in lower areas such as V1 responding to features such as oriented edges and bars, and in higher areas to more spe-ciﬁc stimuli. Fukushima proposed the Neocognitron [37], a neural network architecture for pattern recognition explic-itly motivated by Hubel and Wiesel’s hierarchy. His model had alternating layers of simple cells and complex cells, thus incorporating downsampling, and shift invariance, thus incor-porating convolutional structure. LeCun et al. [70] took the additional step of using backpropagation to train the weights of this network. But already the main aspects of hierarchy of visual processing had been established: (i) Reduction in spa-tial resolution as one goes up the processing hierarchy and (ii) Increase in the number of different “channels”, with each
*Equal technical contribution.
Figure 1. Multiscale Vision Transformers learn a hierarchy from dense (in space) and simple (in channels) to coarse and complex features. Several resolution-channel scale stages progressively increase the channel capacity of the intermediate latent sequence while reducing its length and thereby spatial resolution. channel corresponding to ever more specialized features.
In a parallel development, the computer vision com-munity developed multiscale processing, sometimes called
“pyramid” strategies, with Rosenfeld and Thurston [91], Burt and Adelson [10], Koenderink [66], among the key papers.
There were two motivations (i) To decrease the computing re-quirements by working at lower resolutions and (ii) A better sense of “context” at the lower resolutions, which could then guide the processing at higher resolutions (this is a precursor to the beneﬁt of “depth” in today’s neural networks.)
The Transformer [104] architecture allows learning ar-bitrary functions deﬁned over sets and has been scalably successful in sequence tasks such as language comprehen-sion [29] and machine translation [9]. Fundamentally, a transformer uses blocks with two basic operations. First, is an attention operation [4] for modeling inter-element re-lations. Second, is a multi-layer perceptron (MLP), which models relations within an element. Intertwining these oper-ations with normalization [2] and residual connections [49] allows transformers to generalize to a wide variety of tasks.
Recently, transformers have been applied to key com-puter vision tasks such as image classiﬁcation. In the spirit of architectural universalism, vision transformers [28, 101] approach performance of convolutional models across a va-riety of data and compute regimes. By only having a ﬁrst layer that ‘patchiﬁes’ the input in spirit of a 2D convolu-tion, followed by a stack of transformer blocks, the vision transformer aims to showcase the power of the transformer architecture using little inductive bias.
In this paper, our intention is to connect the seminal idea of multiscale feature hierarchies with the transformer model.
We posit that the fundamental vision principle of resolution and channel scaling, can be beneﬁcial for transformer models across a variety of visual recognition tasks.
We present Multiscale Vision Transformers (MViT), a transformer architecture for modeling visual data such as im-ages and videos. Consider an input image as shown in Fig. 1.
Unlike conventional transformers, which maintain a constant channel capacity and resolution throughout the network,
Multiscale Transformers have several channel-resolution
‘scale’ stages. Starting from the image resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of feature activations inside the transformer network, effectively connecting the principles of transformers with multi scale feature hierarchies.
Our conceptual idea provides an effective design advan-tage for vision transformer models. The early layers of our architecture can operate at high spatial resolution to model simple low-level visual information, due to the lightweight channel capacity. In turn, the deeper layers can effectively focus on spatially coarse but complex high-level features to model visual semantics. The fundamental advantage of our multiscale transformer arises from the extremely dense nature of visual signals, a phenomenon that is even more pronounced for space-time visual signals captured in video.
A noteworthy beneﬁt of our design is the presence of strong implicit temporal bias. We show that vision trans-former models [28] trained on natural video suffer no per-formance decay when tested on videos with shufﬂed frames.
This indicates that these models are not effectively using the temporal information and instead rely heavily on appear-ance. In contrast, when testing our MViT models on shufﬂed frames, we observe signiﬁcant accuracy decay, suggesting reliance on temporal information.
Our focus in this paper is video recognition, and we de-sign and evaluate MViT for video tasks (Kinetics [64, 12],
Charades [92], SSv2 [43] and AVA [44]). MViT provides a signiﬁcant performance gain over concurrent video trans-formers [84, 8, 1], without any external pre-training data.
In Fig. A.4 we show the computation/accuracy trade-off for video-level inference, when varying the number of tem-poral clips used in MViT. The vertical axis shows accuracy on Kinetics-400 and the horizontal axis the overall infer-ence cost in FLOPs for different models, MViT and concur-rent ViT [28] video variants: VTN [84], TimeSformer [8],
ViViT [1]. To achieve similar accuracy level as MViT, these models require signiﬁcant more computation and parameters (e.g. ViViT-L [1] has 6.8× higher FLOPs and 8.5× more pa-rameters at equal accuracy, more analysis in §A.2) and need large-scale external pre-training on ImageNet-21K (which contains around 60× more labels than Kinetics-400).
Figure 2. Accuracy/complexity trade-off on Kinetics-400 for varying # of inference clips per video shown in MViT curves.
Concurrent vision-transformer based methods [84,8,1] require over 5× more computation and large-scale external pre-training on
ImageNet-21K (IN-21K), to achieve equivalent MViT accuracy.
We further apply MViT to ImageNet [24] classiﬁcation, by simply removing the temporal dimension of the video architecture, and show signiﬁcant gains over single-scale vi-sion transformers for image recognition. Our code and mod-els are available in PySlowFast [31] and PyTorchVideo [32]. 2.