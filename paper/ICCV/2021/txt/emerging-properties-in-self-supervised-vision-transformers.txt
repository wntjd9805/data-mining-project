Abstract 1.

Introduction
In this paper, we question if self-supervised learning pro-vides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets).
Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the follow-ing observations: ﬁrst, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised
ViTs, nor with convnets. Second, these features are also ex-cellent k-NN classiﬁers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our ﬁndings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels.
We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.
∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000
Grenoble, France.
Correspondence: mathilde@fb.com
Code: https://github.com/facebookresearch/dino 1
Transformers [57] have recently emerged as an alternative to convolutional neural networks (convnets) for visual recog-nition [16, 56, 68]. Their adoption has been coupled with a training strategy inspired by natural language processing (NLP), that is, pretraining on large quantities of data and
ﬁnetuning on the target dataset [15, 45]. The resulting Vision
Transformers (ViT) [16] are competitive with convnets but, they have not yet delivered clear beneﬁts over them: they are computationally more demanding, require more training data, and their features do not exhibit unique properties.
In this paper, we question whether the muted success of
Transformers in vision can be explained by the use of super-vision in their pretraining. Our motivation is that one of the main ingredients for the success of Transformers in NLP was the use of self-supervised pretraining, in the form of close procedure in BERT [15] or language modeling in GPT [45].
These self-supervised pretraining objectives use the words in a sentence to create pretext tasks that provide a richer learning signal than the supervised objective of predicting a single label per sentence. Similarly, in images, image-level supervision often reduces the rich visual information contained in an image to a single concept selected from a predeﬁned set of a few thousand categories of objects [49].
While the self-supervised pretext tasks used in NLP are
text speciﬁc, many existing self-supervised methods have shown their potential on images with convnets [9, 11, 23, 26].
They typically share a similar structure but with different components designed to avoid trivial solutions (collapse) or to improve performance [14]. In this work, inspired from these methods, we study the impact of self-supervised pre-training on ViT features. Of particular interest, we have identiﬁed several interesting properties that do not emerge with supervised ViTs, nor with convnets:
• Self-supervised ViT features explicitly contain the scene layout and, in particular, object boundaries, as shown in Figure 1. This information is directly accessi-ble in the self-attention modules of the last block.
• Self-supervised ViT features perform particularly well with a basic nearest neighbors classiﬁer (k-NN) without any ﬁnetuning, linear classiﬁer nor data augmentation, achieving 78.3% top-1 accuracy on ImageNet.
The emergence of segmentation masks seems to be a property shared across self-supervised methods. However, the good performance with k-NN only emerge when com-bining certain components such as momentum encoder [26] and multi-crop augmentation [9]. Another ﬁnding from our study is the importance of using smaller patches with ViTs to improve the quality of the resulting features.
Overall, our ﬁndings about the importance of these components lead us to design a simple self-supervised ap-proach that can be interpreted as a form of knowledge distillation [28] with no labels. The resulting framework,
DINO, simpliﬁes self-supervised training by directly pre-dicting the output of a teacher network—built with a mo-mentum encoder—by using a standard cross-entropy loss.
Interestingly, our method can work with only a centering and sharpening of the teacher output to avoid collapse, while other popular components such as predictor [23], advanced normalization [9] or contrastive loss [26] add little beneﬁts in terms of stability or performance. Of particular impor-tance, our framework is ﬂexible and works on both convnets and ViTs without the need to modify the architecture, nor adapt internal normalizations [47].
We further validate the synergy between DINO and ViT by outperforming previous self-supervised features on the
ImageNet linear classiﬁcation benchmark with 80.1% top-1 accuracy with a ViT-Base with small patches. We also con-ﬁrm that DINO works with convnets by matching the state of the art with a ResNet-50 architecture. Finally, we discuss different scenarios to use DINO with ViTs in case of limited computation and memory capacity. In particular, training
DINO with ViT takes just two 8-GPU servers over 3 days to achieve 76.1% on ImageNet linear benchmark, which outperforms self-supervised systems based on convnets of comparable sizes with signiﬁcantly reduced compute require-ments [9, 23].
Figure 2: Self-distillation with no labels. We illustrate DINO in the case of one single pair of views (x1, x2) for simplicity. The model passes two different random transformations of an input image to the student and teacher networks. Both networks have the same architecture but different parameters. The output of the teacher network is centered with a mean computed over the batch.
Each networks outputs a K dimensional feature that is normalized with a temperature softmax over the feature dimension. Their similarity is then measured with a cross-entropy loss. We apply a stop-gradient (sg) operator on the teacher to propagate gradients only through the student. The teacher parameters are updated with an exponential moving average (ema) of the student parameters. 2.