Abstract
In this paper, we propose a talking face generation method that takes an audio signal as input and a short tar-get video clip as reference, and synthesizes a photo-realistic video of the target face with natural lip motions, head poses, and eye blinks that are in-sync with the input audio sig-nal. We note that the synthetic face attributes include not only explicit ones such as lip motions that have high cor-relations with speech, but also implicit ones such as head poses and eye blinks that have only weak correlation with the input audio. To model such complicated relationships among different face attributes with input audio, we propose a FACe Implicit Attribute Learning Generative Adversarial
Network (FACIAL-GAN), which integrates the phonetics-aware, context-aware, and identity-aware information to synthesize the 3D face animation with realistic motions of lips, head poses, and eye blinks. Then, our Rendering-to-Video network takes the rendered face images and the at-tention map of eye blinks as input to generate the photo-realistic output video frames. Experimental results and user studies show our method can generate realistic talking face videos with not only synchronized lip motions, but also nat-ural head movements and eye blinks, with better qualities than the results of state-of-the-art methods. 1.

Introduction
Synthesizing dynamic talking faces driven by input au-dio has become an important technique in computer vision, computer graphics, and virtual reality. There have been steady research progresses [4, 9, 10, 16, 26, 27, 31, 32, 40], it is still very challenging to generate photo-however, realistic talking faces that are indistinguishable from real captured videos, which not only contain synchronized lip motions, but also have personalized and natural head move-ments and eye blinks, etc.
The information contained in dynamic talking faces can be roughly categorized into two different levels: 1) the at-tributes that need to be synchronized with the input au-dio, e.g., the lip motion that has strong correlations with the signals of auditory phonetics; 2) the attributes that have only weak correlations with the phonetic signal, e.g., the head motion that is related to both the context of speech and the personalized talking style and the eye blinking whose rate is mainly decided by personal health condition as well as external stimulus. Here we call the ﬁrst type of attributes to be explicit attributes, and the second type to be implicit attributes.
It should be noted that the majority of existing works [9, 10, 16, 26, 27, 40] on talking face generation are focusing on explicit attributes only, by synchronizing the lip motions with input audio. Examples include Zhou et al. [40] dis-entangled the audio into subjected-related information and speech-related information to generate clear lip patterns, and Chen et al.’s Audio Transformation and Visual Gener-ation (ATVG) networks [9] to transfer audio to facial land-marks and generate video frames conditioned on landmarks.
There are only a few recent efforts [7, 36, 41] exploring the correlation between the implicit attributes of head pose with the input audio. For example, Chen et al. [7] adopted a multi-layer perceptron as the head pose learner to predict the transformation matrix of each input frame. However, it remains unclear on: (1) how the explicit and implicit at-tributes might potentially inﬂuence each other? (2) how to model implicit attributes, such as head poses and eye blinks, that depend not only on the phonetic signal, but also on the contextual information of speech as well as the personalized talking style?
To tackle these challenges, we propose a FACe Implicit
Attribute Learning (FACIAL) framework for synthesizing dynamic talking faces, as shown in Fig. 2. (1) Unlike the previous work [7] predicting implicit attributes using an in-dividual head pose learner, our FACIAL framework jointly learns the implicit and explicit attributes with the regular-ization of adversarial learning. We propose to embed all at-tributes, including Action Unit (AU) of eye blinking, head pose, expression, identity, texture and lighting, in a collabo-rative manner so their potential interactions for talking face generation can be modeled under the same framework. (2)
We design a special FACIAL-GAN in this framework to jointly learn phonetic, contextual, and personalized infor-mation. It takes a sequence of frames as a grouped input and generates a contextual latent vector, which is further en-coded together with the phonetic information of each frame, by individual frame-based generators. FACIAL-GAN is initially trained on our whole dataset (Sec. 4). Given a short reference video (2 ∼ 3 minutes) of the target sub-ject, FACIAL-GAN will be ﬁne-tuned with this short video, so it can capture the personalized information contained in it. Hence our FACIAL-GAN can well-capture all phonetic, contextual, and personalized information of the implicit at-(3) Our FACIAL-GAN can tributes, such as head poses. also predict the AU of eye blinks, which is further em-bedded into an auxiliary eye-attention map for the ﬁnal
Rendering-to-Video module, to generate realistic eye blink-ing in the synthesized talking face.
With the joint learning of explicit and implicit attributes, our end-to-end FACIAL framework can generate photo-realistic dynamic talking faces as shown in Fig. 1, supe-rior to the results produced by the state-of-the-art methods.
The contribution of this paper is threefold: (1) We propose a joint explicit and implicit attribute learning framework to synthesize photo-realistic talking face videos with audio-synchronized lip motion, personalized and natural head mo-(2) We design a FACIAL-tion, and realistic eye blinks.
GAN module to encode the contextual information with the phonetic information of each individual frame, to model the implicit attributes needed for synthesizing natural head mo-tions. (3) We embed the FACIAL-GAN generated AU of eye blinking into an eye-attention map of rendered faces, which achieves realistic eye blinks in the resulting video produced by the Rendering-to-Video module. 2.