Abstract 1.

Introduction
Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D
HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies.
Code and datasets are available for research purposes at https://spec.is.tue.mpg.de/.
Estimating 3D human pose and shape (HPS) from a sin-gle RGB image is a core challenge in computer vision and has many applications in robotics, computer graphics, and
AR/VR. Reconstructing a high dimensional 3D structure from 2D observations is ill-posed by nature. To overcome this, much attention has been given to structured prediction
[16, 20, 62] and incorporating shape and pose priors [47, 69] to guide estimation. Weakly-supervised training of HPS re-gressors leverages 2D-pose datasets [1, 25, 39] and requires various forms of regularization [28, 35, 71]. Data for full 3D supervision often relies on controlled lab settings [21, 58], synthetic images [61], or, more recently, in-the-wild capture of reference data [43, 63].
Despite rapid progress, we observe that most state-of-the-art (SOTA) methods [5, 7, 15, 24, 26, 28, 29, 32, 35, 36, 47, 52, 59, 73, 74] make several simplifying assump-tions about the image formation process itself. First, they all apply a weak perspective or orthographic projection as-sumption; resulting in a simplified camera model with only three parameters which capture the camera translation rela-tive to the body. Moreover, some [32, 35, 47] set the focal length to a predefined large constant for every input image.
Finally, they all assume zero camera rotation, which entan-gles body rotation and camera rotation, making it extremely hard to correctly estimate the body orientation in 3D. These 1
assumptions are valid for images where bodies are roughly perpendicular to the principal axis and are located far away from the camera. However, in most real world images of people, perspective effects are clearly evident, e.g. fore-shortening in selfies. Ignoring perspective projection leads to errors in pose, shape, and global orientation (see Fig. 1).
To overcome these limitations in existing methods, we present SPEC (Seeing People in the wild with Estimated
Cameras), the first 3D human pose and shape estimation framework that leverages cues present in the image to ex-tract perspective camera information and exploits this to better reconstruct 3D human bodies from images in the wild. SPEC consists of two parts: camera calibration and body reconstruction. We make contributions to each.
One might hope that embedded EXIF information would be sufficient to address this problem. However, many im-ages lack EXIF information, some applications strip this off, and even if present, converting the stored focal length in millimeters to pixels requires knowing specifics of the image sensor. Given the huge variety of cameras on the market, exploiting this is a non-trivial task. Furthermore, this does not give information about the camera rotation.
Instead, we estimate the camera directly from the RGB image. Recent work [19, 30, 65, 77] casts this ill-posed re-gression problem as a classification task. However, training such methods with their losses, e.g. cross-entropy and KL-divergence, ignores the natural notion of distance or order-ing of the original target space. To address this, we propose a new loss, Softargmax-L2, to preserve distance during loss calculation. Moreover, we observe that HPS accuracy is quite sensitive to underestimation of focal length and less sensitive to overestimates as also noted by [31, 72]. There-fore, we modify Softargmax-L2 to be asymmetric such that less penalty is applied when the focal length is overesti-mated. These novel losses help us to train a better regressor for direct camera calibration, which we term CamCalib.
We integrate the regressed camera parameters into two (1) an optimization-3D-body-reconstruction paradigms: based approach, SMPLify-X [47], and (2) a regression-based one similar to HMR or SPIN [28, 35].
Since
SMPLify-X estimates a 3D body my minimizing the differ-ence between projected 3D joints and observed 2D joints, improving the the projective geometry improves the esti-mated body.
In the case of direct HPS regression from pixels, the esti-mated camera is employed in two ways: (1) in the reprojec-tion loss similar to the one in SMPLify-X and (2) as condi-tioning for the network by appending the camera parameters to the CNN image features. This second contribution is a key novelty of SPEC, which enables us to disentangle cam-era and body orientation. SOTA methods [32, 33, 35, 73], cannot do this because the body is estimated in camera space, entangling body orientation and camera rotation.
Training such a body regressor requires in-the-wild im-ages annotated with both 3D human bodies and the cam-era parameters. Since existing 3D human body datasets
[21, 42, 63] contain little variation in camera parameters, we create two new datasets with rich camera variety. First, we create a photorealistic synthetic dataset which has accurate ground-truth human and camera annotations (SPEC-SYN) using ideas from [46]. This dataset is used both for testing and training. Second, we collect a crowdsourced dataset fol-lowing the Mimic-The-Pose framework [45] (SPEC-MTP).
We ask web participants to calibrate their camera and take videos from different angles while mimicking a predefined pose. Then, we obtain pseudo ground-truth labels by fitting the SMPL model to the provided videos while exploiting the predefined pose as a prior. Through extensive experi-ments and analysis using these new datasets, alongside an existing in-the-wild dataset (3DPW [63]), we show that go-ing beyond the weak-perspective/orthographic assumption improves human pose and shape estimation results.
In summary, our contributions are: (1) We propose a single-view, camera-aware, 3D human body estimation framework that estimates perspective camera parameters from in-the-wild images directly and reconstructs the 3D body without relying on weak-perspective assumptions or offline calibration. (2) We train a neural network to regress the perspective camera parameters given one RGB image, using two novel losses: Softargmax-L2 and the asymmet-ric variant to improve the calibration accuracy. (3) Us-ing the estimated camera parameters helps to reconstruct a better 3D body with the optimization-based SMPLify-X algorithm. (4) Conditioning on camera information helps a direct regression approach based on HMR [28] learn to regress better poses. (5) We present two different datasets with ground-truth camera and human body parameters: (i) a photorealistic synthetic dataset, SPEC-SYN, and (ii) a crowdsourced dataset, SPEC-MTP. 2.