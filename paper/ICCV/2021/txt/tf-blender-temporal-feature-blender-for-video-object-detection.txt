Abstract
Video objection detection is a challenging task because isolated video frames may encounter appearance deterio-ration, which introduces great confusion for detection. One of the popular solutions is to exploit the temporal informa-tion and enhance per-frame representation through aggre-gating features from neighboring frames. Despite achiev-ing improvements in detection, existing methods focus on the selection of higher-level video frames for aggregation rather than modeling lower-level temporal relations to in-crease the feature representation. To address this lim-itation, we propose a novel solution named TF-Blender, which includes three modules: 1) Temporal relation mod-els the relations between the current frame and its neigh-boring frames to preserve spatial information. 2). Fea-ture adjustment enriches the representation of every neigh-boring feature map; 3) Feature blender combines out-puts from the ﬁrst two modules and produces stronger fea-tures for the later detection tasks. For its simplicity, TF-Blender can be effortlessly plugged into any detection net-work to improve detection behavior. Extensive evalua-tions on ImageNet VID and YouTube-VIS benchmarks in-dicate the performance guarantees of using TF-Blender on recent state-of-the-art methods. Code is available at https://github.com/goodproj13/TF-Blender. 1.

Introduction
With the progress of learning-based computer vision, re-cent research efforts have been extended from image tasks to the more challenging video domains. Video tasks, such as object detection [11], video instance segmentation [40], and multi-object tracking and segmentation [33], hold valu-able potentials for real-world applications [24, 33, 25, 26]
*Equal contributions.
†Corresponding author. (a) Visualization of feature aggregation process (b) Current methods aggregation (c) Our aggregation meth-ods
Figure 1. Comparison of feature aggregation methods. (a) Features from the neighboring frames are weighted equally during aggrega-tion. (b) The current aggregation methods only reason the relations between the current frame and neighboring frames. (c) Our pro-posed method computes every pair of frames in the neighborhood in the aggregation process. (i.e., autonomous driving or video surveillance ).
A primary challenge of video object detection is to tackle the feature degradation on video frames caused by cam-era jitter or fast motion. Under the circumstance, detec-tion algorithms for still images are ill-posed for video tasks.
Nonetheless, the video has rich temporal information, on which the same object may appear in multiple frames for a certain time span. The value of such temporal informa-tion is explored in prior studies using the post-processing paradigm [16, 19, 19, 21]. These methods ﬁrstly perform still-image detection on single frames and then assemble the detection results across temporal dimensions using a dis-joint post-processing step (i.e., motion estimation and ob-ject tracking). None of the above methods, therefore, oper-ate in an end-to-end fashion. Moreover, if detection on sin-gle frames produces weak predictions, the assembling ap-proach cannot improve the detection results.
Alternatively, there have been several attempts to boost the performance of video detection using feature aggrega-tion. [25, 33, 43] leverage optical ﬂow to model the feature movement across frames and propagate temporal features to increase the feature representation for detection. With stronger features, the detection results are signiﬁcantly im-proved. However, such temporal features are exploited by an intuitive lumping operation, which is oversimpliﬁed.
In terms of how to organize features in aggregation, we recognize two important predecessors, FGFA [42] and
SELSA [36]. Compared to the lumping solution [25, 33, 43], both methods use similarity scores to select more help-ful features for aggregation. The aggregated feature is or-ganized by an adaptive weight at every spatial location for their representations (as shown in Figure 1(a)). Albeit being superior over the prior efforts, FGFA [42] and SELSA [36] encounter several obstacles from achieving optimal perfor-mance: 1) They focus on modeling the global relation for every neighboring frame while ignoring the preservation of the local spatial information for aggregation; 2) They primarily consider the global feature relations to the cur-rent frames, while having no constraint in feature learning among the neighboring frames (see Figure 1(b)); 3) They take a ﬁxed number of neighboring frames for the feature aggregation, which is heuristic than general.
In this work, we attempt to take a deeper look at video object detection and improve the performance guarantees by organizing temporal information in a more rigorous prin-ciple. Inspired by [42, 36, 7], we propose TF-Blender to or-ganically model features consistently and correspondingly in two ranges. Speciﬁcally, we reinforce local similarity in feature space on sequential video frames to depict the con-tinuous and coherence of visual patterns, while identifying semantic correspondence across frames, which makes the temporal representations robust to appearance variations,
In this design, shape deformations, and local occlusions.
TF-Blender is able to generalize feature aggregation by en-couraging the video representation and capturing helpful vi-sual content to improve detection performance. Concretely, we are able to achieve the following contributions:
• We propose a framework called TF-Blender, which de-picts the temporal feature relations and blends valuable neighboring features to increase the temporal-spatial feature representation across frames.
• In TF-Blender, we devise a temporal relation module to manage temporal information and a feature adjust-ment module to add constraints in feature learning to preserve spatial information during feature aggrega-tion. We, therefore, organize the feature learning be-tween every pair of frames and aggregate features in the whole neighborhood (see Figure 1(c))
• Our method is general and ﬂexible, which can be crafted on any detection network. With our novel fea-ture enhancement strategy, we can obtain an absolute gain of more than 0.7% in mAP on the ImageNet VID benchmark and 1.5% in mAP on YouTube-VIS bench-mark for recent state-of-the-arts methods. 2.