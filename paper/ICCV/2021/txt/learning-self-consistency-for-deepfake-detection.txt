Abstract
We propose a new method to detect deepfake images us-ing the cue of the source feature inconsistency within the forged images. It is based on the hypothesis that imagesâ€™ distinct source features can be preserved and extracted af-ter going through state-of-the-art deepfake generation pro-cesses. We introduce a novel representation learning ap-proach, called pair-wise self-consistency learning (PCL), for training ConvNets to extract these source features and detect deepfake images.
It is accompanied by a new im-age synthesis approach, called inconsistency image genera-tor (I2G), to provide richly annotated training data for PCL.
Experimental results on seven popular datasets show that our models improve averaged AUC over the state of the art from 96.45% to 98.05% in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset evaluation. 1.

Introduction
Deepfakes are synthetic media in which the identity or expression of a target subject is replaced by that of another source subject. They are predominantly generated by im-age stitching, which includes face detection, warping, and blending. Attacks using deepfakes have caused a significant amount of negative social impact, and also motivated meth-ods to detect these forged videos. Most of these defense methods [60, 26, 37, 51, 46, 5, 23, 21, 6] target detecting suspicious artifacts left in the stitching process, such as eye blinking [26], face warping [27], blending boundaries [23], and fake prototypes [48]. In the wake of these defenders, forgery techniques are also evolving on reducing these arti-facts to avoid detection, forming an enduring arms race.
In this paper, we propose a new method to detect deep-fakes generated by stitching-based methods. Unlike other methods focusing on detecting artifacts described above, our approach uses the cue of inconsistency of source fea-tures within the forged images. Conceptually, images carry
*Currently at University of Michigan, Ann Arbor. The work was con-ducted while at Amazon/AWS AI.
Figure 1: The forged image is generated by stitching target and source images. We hypothesize that each of them carries distinct source features that can uniquely identify their sources. There-fore, the forged image contains different source features at differ-ent locations, whereas those of a pristine image must be consistent across all positions. By extracting the local source features and measuring their self-consistency, we can detect forged images. content-independent [14], spatially-local information that can uniquely identify their sources. We call them the source features. They could come from either imaging pipelines (e.g., PRNU noise [30], specifications [14]), encoding ap-proaches (e.g., JPEG compression patterns [1], compres-sion rates) or image synthesis models [58]. We hypothesize that these source features are still preserved after the mod-ified image having gone through the state-of-the-art deep-fake generation processes [28, 8, 15, 22, 39, 35]. Therefore, a forged image would contain different source features at different positions, whereas those of a pristine image must be consistent across all positions. By extracting the lo-cal source features and measuring their self-consistency, we can detect forged images.
Specifically, we use a convolutional neural network (ConvNet) to extract source features in the form of down-sampled feature maps. Each feature vector represents the source features of a corresponding location in the input im-age. To train this ConvNet, we introduce a novel repre-sentation learning method, called pair-wise self-consistency learning (PCL), which uses the consistency loss for supervi-sion. We calculate the cosine similarity between very pairs of feature vectors in the source feature map, and compute the consistency loss on all pairs according to whether their
corresponding image locations come from the same source image. That is, we penalize the pairs that refer to locations from the same source image for having a low-similarity score and those from different source images for having a high-similarity score. We attach a non-linear binary classi-fier on the learned source feature map to perform the deep-fake detection. We train it with an additional loss to produce the image-level real vs. fake labels.
The consistency loss in PCL needs pixel-level annotation about whether a location has been modified. It is generally not available in deepfake detection datasets, on which the re-annotation could be laborious and error-prone. We use synthesized data generated from inconsistency image gen-It generates forged im-erator (I2G) to tackle this issue. ages following the latest techniques in deepfake generation methods. To save computational cost and enable online gen-eration, I2G only stitches together pristine source and target images instead of the synthesized ones from deep networks.
We randomly sample the forgery mask for stitching during generation, which becomes the pixel-level annotation we need for PCL. Experimental results show that, although us-ing a simplified generation process, the models learned with synthesized data from I2G effectively extract discriminative source features in both pristine and deepfake images.
We evaluate PCL on seven recent deepfake detection datasets and observe superior detection accuracy. Follow-ing the in-dataset evaluation, our method achieves the AUC scores of 99.79%, 99.98%, and 94.38% on FF++, CD2, and DFDC-P datasets, respectively. Because PCL uses the cue of source feature inconsistency which is less taken care of by current deepfake generation methods, we conjecture that a model trained with PCL on one dataset could effec-tively detect deepfakes generated by methods not seen in this dataset. To verify this, we adopt the cross-dataset eval-uation protocol introduced in [23] to test our models and observe affirmative results. We achieve the AUC scores of 99.11%, 99.07%, 99.41%, 98.30%, and 90.03% on FF++,
DFD, DFR, CD1, and CD2 datasets, respectively. We fur-ther visualize the consistency map of the learned source fea-tures on both real and fake images. We observe the consis-tency maps can lead to localization of the modified region.
It is worth noting that, as the race between forgers and defenders continues, the cue of source feature inconsis-tency can be negated. It can be done by either using entire face synthesis techniques [17, 29, 4] that directly output the whole fake image, such as GAN, or future development of stitching methods that completely removes or ambiguates the source feature. However, the state-of-the-art deepfake generation methods have not yet adopted these techniques.
Thus the effectiveness of our method on detecting deepfake images should only be evaluated on the images generated by existing deepfake detection methods, as depicted in deep-fake detection datasets we used [41, 10, 28, 8, 9, 15]. 2.