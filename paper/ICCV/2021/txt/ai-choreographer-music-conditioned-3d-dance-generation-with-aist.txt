Abstract 1.

Introduction
We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention
Cross-modal Transformer network for generating 3D dance motion conditioned on music.
The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 se-quences, covering 10 dance genres with multi-view videos with known camera poses—the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict N future motions. We empirically show that these changes are key factors in gen-erating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive ex-periments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualita-tively and quantitatively. The code and the dataset can be found at: https://google.github.io/aichoreographer.
∗ equal contribution. Work performed while Ruilong was an intern at
Google.
The ability to dance by composing movement patterns that align to musical beats is a fundamental aspect of hu-man behavior. Dancing is an universal language found in all cultures [50], and today, many people express themselves through dance on contemporary online media platforms.
The most watched videos on YouTube are dance-centric music videos such as “Baby Shark Dance”, and “Gangnam
Style” [75], making dance a more and more powerful tool to spread messages across the internet. However, dancing is a form of art that requires practice—even for humans, pro-fessional training is required to equip a dancer with a rich repertoire of dance motions to create an expressive chore-ography. Computationally, this is even more challenging as the task requires the ability to generate a continuous motion with high kinematic complexity that captures the non-linear relationship with the accompanying music.
In this work, we address these challenges by presenting a novel Full Attention Cross-modal Transformer (FACT) network, which can robustly generate realistic 3D dance motion from music, along with a large-scale multi-modal 3D dance motion dataset, AIST++, to train such a model.
Speciﬁcally, given a piece of music and a short (2 seconds) seed motion, our model is able to generate a long sequence of realistic 3D dance motions. Our model effectively learns the music-motion correlation and can generate dance se-1
Figure 2: Cross-Modal Music Conditioned 3D Motion Generation Overview. Our proposed a Full-Attention Cross-modal Transformer (FACT) network (details in Figure 3) takes in a music piece and a 2-second sequence of seed motion, then auto-regressively generates long-range future motions that correlates with the input music. quences that varies for different input music. We represent dance as a 3D motion sequence that consists of joint rota-tion and global translation, which enables easy transfer of our output for applications such as motion retargeting as shown in Figure 1.
In order to generate 3D dance motion from music, we propose a novel Full Attention Cross-modal Transformer (FACT) model, which employs an audio transformer and seed motion transformer to encode the inputs, which are then fused by a cross-modal transformer that models the dis-tribution between audio and motion. This model is trained to predict N future motion sequences and at test time is ap-plied in an auto-regressive manner to generate continuous motion. The success of our model relies on three key design choices: 1) the use of full-attention in an auto-regressive model, 2) future-N supervision, and 3) early fusion of two modalities. The combination of these choices is critical for training a model that can generate a long realistic dance mo-tion that is attuned to the music. Although prior work has explored using transformers for motion generation [3], we
ﬁnd that naively applying transformers to the 3D dance gen-eration problem without these key choices does not lead to a very effective model.
In particular, we notice that because the context window in the motion domain is signiﬁcantly smaller than that of language models, it is possible to apply full-attention trans-formers in an auto-regressive manner, which leads to a more powerful model.
It is also critical that the full-attention transformer is trained to predict N possible future motions instead of one. These two design choices are key for pre-venting 3D motion from freezing or drifting after several auto-regressive steps as reported in prior works on 3D mo-tion generation [4, 3]. Our model is trained to predict 20 future frames, but it is able to produce realistic 3D dance motion for over 1200 frames at test time. We also show that fusing the two modalities early, resulting in a deep cross-modal transformer, is important for training a model that generates different dance sequences for different music.
In order to train the proposed model, we also address the problem of data. While there are a few motion capture datasets of dancers dancing to music, collecting mocap data requires heavily instrumented environments making these datasets severely limited in the number of available dance sequences, dancer and music diversity. In this work, we pro-pose a new dataset called AIST++, which we build from the existing multi-view dance video database called AIST [82].
We use the multi-view videos to recover reliable 3D motion from this data. We will release code and this dataset for re-search purposes, where AIST++ can be a new benchmark for the task of 3D dance generation conditioned on music.
In summary, our contributions are as follows:
• We propose Full Attention Cross-Modal Transformer model, FACT, which can generate a long sequence of realistic 3D dance motion that is well correlated with the input music.
• We introduce AIST++ dataset containing 5.2 hours of 3D dance motions accompanied with music and multi-view images, which to our knowledge is the largest dataset of such kind.
• We provide extensive evaluations validating our design choices and show that they are critical for high quality, multi-modal, long motion sequence generation. 2.