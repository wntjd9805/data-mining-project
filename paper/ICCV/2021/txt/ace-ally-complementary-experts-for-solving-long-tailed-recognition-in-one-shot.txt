Abstract
One-stage long-tailed recognition methods improve the overall performance in a “seesaw” manner, i.e., either sac-riﬁce the head’s accuracy for better tail classiﬁcation or elevate the head’s accuracy even higher but ignore the tail.
Existing algorithms bypass such trade-off by a multi-stage training process: pre-training on imbalanced set and ﬁne-tuning on balanced set. Though achieving promising per-formance, not only are they sensitive to the generalizability of the pre-trained model, but also not easily integrated into other computer vision tasks like detection and segmenta-tion, where pre-training of classiﬁers solely is not applica-ble. In this paper, we propose a one-stage long-tailed recog-nition scheme, ally complementary experts (ACE), where the expert is the most knowledgeable specialist in a sub-set that dominates its training, and is complementary to other experts in the less-seen categories without being dis-turbed by what it has never seen. We design a distribution-adaptive optimizer to adjust the learning pace of each ex-pert to avoid over-ﬁtting. Without special bells and whis-tles, the vanilla ACE outperforms the current one-stage
SOTA method by 3∼ 10% on CIFAR10-LT, CIFAR100-LT,
ImageNet-LT and iNaturalist datasets. It is also shown to be the ﬁrst one to break the “seesaw” trade-off by improv-ing the accuracy of the majority and minority categories simultaneously in only one stage. Code and trained models are at https://github.com/jrcai/ACE. 1.

Introduction
Object recognition is one of the most essential and sub-stantial applications in computer vision. However, the per-formances of the state-of-the-art object recognition meth-ods have limited capability on classifying real-world enti-ties, which are skewed-distributed in a long-tailed manner naturally. Mostly driven by artiﬁcially-balanced datasets
[4, 13], current models are dominated by the sample-rich
Figure 1. Performance of representative long-tailed recognition methods in terms of majority and minority classes compared to the baseline model (a ResNet). The results indicate most re-balancing methods improve the performance of minority categories by sacri-ﬁcing that of the majority even with two-stage training (quadrant
IV). Data augmentations are effective on the heads but slightly hurt the tails (quadrant II). The proposed ACE is the ﬁrst one-stage
SOTA method that improves the majority and minority simulta-neously. Statistics for this ﬁgure are listed in the supplementary materials. classes and lose sight of the tails when adapting to long-tailed sets. Facing up to the reality, scarce as the tail cate-gories are, they are of the same or even higher signiﬁcance than the heads in various ﬁelds, such as biological species identiﬁcation [20], disease classiﬁcation [21] and web-spam message detection [33]. This long-lasting bottleneck signif-icantly restricts classiﬁcation-related computer vision tasks into practical use, including detection [19, 25, 28] and in-stance segmentation [22, 29].
To ensure a well-accepted recognition capability over all categories, a tail-sensitive classiﬁer becomes necessary. Ex-isting solutions fall in three categories: one-stage [8, 24], two-stage with pre-training[10, 1], and multi-stage multi-expert frameworks [26, 23]. The one-stage algorithms fol-low a straightforward idea to addressed the imbalance of training set by re-balancing, including re-sampling [10] and re-weighting [1, 3, 31]. Despite the promotion of the tails, balancing techniques show an obvious “seesaw” phe-nomenon (Figure 1), that the accuracy of majority classes is sacriﬁced, indicating the under-representation of the heads.
This raises a new concern that reducing the heads’ accuracy might lead to more serious consequences. Taking the ani-mal identiﬁcation system as an example, some species are
In-much richer in population than the endangered ones. creasing the recognition accuracy of the snow leopards has little chance to be veriﬁed as they are rarely seen; on the contrary, failing to precisely classify two bird kinds can eas-ily result in a misunderstanding of the local ecology.
Literature in the recent years [10, 23, 26, 32] handles the issue in a roundabout way: ﬁrstly train the feature extrac-tor (backbone) with the whole imbalanced set for generaliz-able representation learning, then re-adjust the classiﬁer by re-sampled data or build diverse experts for various tasks in cascading stages. Further improving the performance as they are, however, the general idea still holds old wine in a new bottle by making new trade-offs. To re-balance the data distribution, heavily relying on the well-adjusted pre-trained model and re-balancing skills make the frameworks sensitive to hyper-parameters and hard to ﬁnd a sweet point.
More importantly, the accumulated training steps make the multi-stage models redundant and less practical to be inte-grated with other tasks simultaneously, e.g., detection [22] and segmentation [29]. To guarantee the plug-in and play property, it is thus highly desirable to have a classiﬁer that overcomes the long-tail challenge with only one stage.
The hankerings of overcoming current long-tail chal-lenges make us look more profoundly to the human in-telligence. When human-beings make hard classiﬁcation choices, saying diagnosis of diseases, it is advantageous to involve specialists’ insights who are well-aware of their own ﬁelds. Moreover, for the rare diseases, panel discus-sion and consultation are indispensable to exclude interfer-ing potentials. Similarly, in the long-tailed issue, we are inspired to design a group of experts with complementary skills: (1) they share elementary knowledge from the most diverse data source; (2) they are professional at splits of data respectively, and aware of what they do not specialize in; (3) opinions from the experienced experts (who see more data) are incorporated to complement the judgment from junior experts (who see less) for optimal decision.
Following the idea, we propose the Ally Complementary
Experts (ACE) for one-stage long-tailed recognition. ACE is a multi-expert structure where experts are trained in par-allel with a shared backbone. The experts are assigned with diverse but overlapping imbalanced subsets, to beneﬁt from specialization in the dominating part. We also introduce a distribution-adaptive optimizer that controls the update of each expert according to the volume of its training set. Fi-nally, the outputs of all experts are re-scaled and aggregated by data splits. ACE is trained end-to-end without any pre-training or staged-training.
We evaluate ACE on various widely-used long-tailed datasets, including CIFAR10-LT, CIFAR100-LT [3],
ImageNet-LT [15] and iNaturalist2018 [20] extensively with various experimental settings. Our method becomes the new SOTA among all one-stage long-tailed recogni-tion methods with by 3-10% accuracy gain and is the ﬁrst one that improves performance on all the three frequency groups (many-shot, medium-shot and few-shot). ACE also surpasses several multi-stage methods [10, 11, 15, 26] by a large margin. 2.