Abstract
Semi-supervised video object segmentation is a task of segmenting the target object in a video sequence given only a mask annotation in the first frame. The limited infor-mation available makes it an extremely challenging task.
Most previous best-performing methods adopt matching-based transductive reasoning or online inductive learning.
Nevertheless, they are either less discriminative for similar instances or insufficient in the utilization of spatio-temporal information. In this work, we propose to integrate trans-ductive and inductive learning into a unified framework to exploit the complementarity between them for accurate and robust video object segmentation. The proposed ap-proach consists of two functional branches. The transduc-tion branch adopts a lightweight transformer architecture to aggregate rich spatio-temporal cues while the induction branch performs online inductive learning to obtain dis-criminative target information. To bridge these two diverse branches, a two-head label encoder is introduced to learn the suitable target prior for each of them. The generated mask encodings are further forced to be disentangled to bet-ter retain their complementarity. Extensive experiments on several prevalent benchmarks show that, without the need of synthetic training data, the proposed approach sets a se-ries of new state-of-the-art records. Code is available at https://github.com/maoyunyao/JOINT. 1.

Introduction
Semi-supervised video object segmentation (VOS) aims at segmenting the target object in a video sequence with the supervision given in the first frame by a pixel-wise segmen-tation mask. It has received tremendous attention in recent years for its wide applications. Since the target-specific in-formation is only given in the first frame, and the target may undergo fast-moving and dramatic deformation, how to make full use of the limited information to perform accu-*Corresponding authors: Wengang Zhou and Houqiang Li
Figure 1. An overview of our approach. The transduction branch (TB) aggregates rich spatio-temporal cues from sampled history frames and propagates them to the current frame, and the induc-tion branch (IB) performs online inductive learning to obtain dis-criminative target information. rate segmentation is thus extremely challenging.
Top-performing methods can be roughly categorized as
In transductive reasoning and online inductive learning. transductive formulation, direct reasoning from reference frames (labeled samples) to the current frame (unlabeled sample) is performed to facilitate segmentation. In recent transductive solutions [19, 23, 28, 33, 37, 44, 45, 54], fea-ture matching has become the mainstream choice, where pixel-level affinity or distance maps between the current frame and reference frames are obtained to deliver rich his-torical target information. This specific-to-specific reason-ing favorably retains the temporal information with attrac-tive time efficiency. Despite achieving the state-of-the-art performance, it heavily relies on the offline learned feature embeddings for accurate matching, thus suffers limited gen-eralization and discrimination capabilities.
On the other hand, online inductive learning utilizes ref-erence frames to train a target model (general rule), which is then applied to subsequent frames to perform segmentation.
Recently, efficient online discriminative learning [4, 12] in visual object tracking has been introduced to the VOS com-munity for its well-acknowledged adaptivity and generaliz-ability. The few-shot learner in [5, 36] provides superior distractor discrimination capability. Nevertheless, this in-ductive formulation treats reference frames as independent training samples and fails to explore the underlying con-text [48, 55, 58]. Rich temporal information that resides in the video flow is thus not fully exploited, which has been proven by previous transductive inference works [33, 57] to be crucial for obtaining spatio-temporal consistent results.
The above analysis indicates that transductive reasoning and online inductive learning are naturally complementary.
The former performs better in spatio-temporal dependency modeling but struggles to discriminate similar distractors, while the latter is just the other way around. Although it is intuitive to jointly integrate these two models, how to ex-plore their complementary potentials in a unified framework has been rarely involved. Since they deal with the VOS task via different perspectives, there exist two main chal-lenges for this seemingly straightforward integration. First, most transductive approaches rely on intermediate results as features [33, 37, 25, 28] or distance maps [41, 54], while the online inductive learning directly outputs masks [27, 36] or intermediate encodings [5]. How to design an appropri-ate merging strategy to effectively fuse these diverse repre-sentations while retaining their complementarity is an open problem. Second, how to tightly bridge these two different models to avoid redundant computations for efficient online
VOS deserves further exploration.
In this work, as shown in Figure 1, we propose a novel two-branch architecture to jointly integrate transductive rea-soning and online inductive learning within a unified frame-work for high-performance VOS. The transduction branch aggregates rich spatial-temporal information while the in-duction branch provides superior discrimination capabil-ity. To solve the aforementioned problems and narrow the gap between the two branches, we make several key de-signs in the proposed framework: (1) In the transduction branch, we extend the attention mechanism adopted in pre-vious matching-based VOS frameworks [33, 57, 37] into a lightweight transformer [7, 40] architecture, which is care-fully designed to facilitate temporal information propaga-tion. To unify the inputs and outputs of the two branches, we further adopt a two-head label encoder for them to gen-erate mask encodings as the target information carrier for
VOS. (2) We propose the mask encoding decoupling regu-larization to reduce their redundancy and make the learned target information more differentiable and complementary. (3) Finally, our two lightweight branches mutually share plentiful blocks such as backbone, partial label generator, and segmentation decoder, making our framework efficient and end-to-end trainable. We perform extensive experi-ments on DAVIS [35] and YouTube-VOS [52] benchmarks.
Our proposed approach outperforms other state-of-the-art methods with comparable running efficiency.
In summary, we make the following three contributions:
• We propose a novel two-branch architecture to tackle the video object segmentation, which absorbs the mer-its of both offline learned transductive reasoning and online inductive learning.
• For the transduction branch, a lightweight transformer architecture is proposed to conduct spatio-temporal de-pendency modeling and content propagation. To our best knowledge, this is the first attempt to leverage the transformer architecture in VOS.
• To bridge the gap between two branches and better ex-ploit their complementary characteristics, we propose to learn disentangled mask encodings. 2.