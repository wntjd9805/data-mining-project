Abstract
In order to robustly deploy object detectors across a wide range of scenarios, they should be adaptable to shifts in the input distribution without the need to constantly an-notate new data. This has motivated research in Unsu-pervised Domain Adaptation (UDA) algorithms for detec-tion. UDA methods learn to adapt from labeled source do-mains to unlabeled target domains, by inducing alignment between detector features from source and target domains.
Yet, there is no consensus on what features to align and how to do the alignment. In our work, we propose a framework that generalizes the different components commonly used by UDA methods laying the ground for an in-depth anal-ysis of the UDA design space. Specifically, we propose a novel UDA algorithm, ViSGA, a direct implementation of our framework, that leverages the best design choices and introduces a simple but effective method to aggregate fea-tures at instance-level based on visual similarity before in-ducing group alignment via adversarial training. We show that both similarity-based grouping and adversarial train-ing allows our model to focus on coarsely aligning feature groups, without being forced to match all instances across loosely aligned domains. Finally, we examine the applica-bility of ViSGA to the setting where labeled data are gath-ered from different sources. Experiments show that not only our method outperforms previous single-source approaches on Sim2Real and Adverse Weather, but also generalizes well to the multi-source setting. 1.

Introduction
Object detectors should be adaptable to “domain shift” that can occur due to many factors including changes in weather or camera, compared to the training data. Do-main shifts can cause a significant drop in object de-tector performance [5, 17]. Domain adaptation meth-ods [10, 9, 38, 26, 27, 29] study this problem, casting it as a task of learning models from a source domain and
Figure 1. Depiction of visual similarity based grouping proposed in our ViSGA method. Instance proposals from the detector are aggregated based on visual similarity to create an adaptive number of class-agnostic groups then they are aligned across the domains. adapting to a target domain.
In object detection, where collecting bounding box annotations is expensive, it be-comes critical that domain adaptation can be performed without the need to annotate every new domain. This moti-vates the challenging setting of unsupervised domain adap-tation (UDA) [42, 39, 28, 2], where one has access to la-beled source data and only unlabeled target data. Moreover, training data itself could be gathered under different con-ditions, a scenario typically referred to as a multi-source domain adaptation [31, 45, 46, 47].
A dominant line in UDA works is to learn invariant rep-resentations via aligning source and target domains, with various proposed alignment strategies. Specifically in ob-ject detection, the questions of what features to align and how to induce the alignment have been the subject of recent research. Early works [5, 20] propose aligning both image-level features from the backbone network and all instance-level features extracted from object proposals using adver-sarial training [13]. A recent state-of-the-art approach [44] argues that it is beneficial to aggregate object proposals be-fore alignment and suggests condensing all proposals into a single category prototype vector before inducing alignment using a contrastive loss. This raises questions on what is the right aggregation-level at which to do feature alignment and what is the right mechanism to induce this alignment.
In this work, we propose a novel UDA method for object
detection, called visually similar group alignment (ViSGA).
Our method harnesses the power of adversarial training, while leveraging the visual similarity of the different pro-posals as a basis for aggregating them. By relying on visual similarity, we aggregate proposals from potentially differ-ent spatial locations (Figure 1), increasing the effectiveness of adversarial training. Doing so, we drive a more powerful discriminator and hence better aligned features. To enhance the flexibility of proposal aggregation and to avoid introduc-ing unwanted noise in the alignment process as a result of a preset fixed number of groups, we opt for dynamic cluster-ing based on the distance at which proposals are aggregated.
This improves the adaptability of our method to a variable number of objects present in the input.
Our method design choices are based on an in-depth analysis of common components of UDA methods for de-tection. In particular we study what is the right aggregation-level to perform instance-level alignment, ranging from considering all instances [5], multiple groups based on clus-tering to single prototypes [44]. When aggregating object proposals, we analyze whether including the predicted class label is beneficial and which distance metric performs bet-ter, including spatial overlap and visual similarity. We fur-ther compare the effectiveness of using contrastive losses versus adversarial training, as the alignment mechanism.
In summary, our key contributions are as follows: 1) We propose a novel, simple yet effective, UDA method for ob-ject detection via adversarial training and dynamic visual similarity-based grouping of proposals from the source and target domains. 2) We perform an in-depth analysis answer-ing questions on what is the right level of alignment and how to induce alignment. 3) We evaluate our proposed ap-proach on three different domain shift scenarios including:
Adverse weather, Synthetic to Real data, and Cross camera and show state-of-the-art results. 4) We are the first to con-sider the important setting of multi-source domain adapta-tion for object detection where annotated data are gathered from different sources. We show that our method continues to improve in this highly relevant scenario, another evidence for the effectiveness of our approach. 2.