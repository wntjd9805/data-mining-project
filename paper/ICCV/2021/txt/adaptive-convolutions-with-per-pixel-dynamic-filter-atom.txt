Abstract
Applying feature dependent network weights have been proved to be effective in many ﬁelds. However, in prac-tice, restricted by the enormous size of model parameters and memory footprints, scalable and versatile dynamic con-volutions with per-pixel adapted ﬁlters are yet to be fully explored. In this paper, we address this challenge by de-composing ﬁlters, adapted to each spatial position, over dy-namic ﬁlter atoms generated by a light-weight network from local features. Adaptive receptive ﬁelds can be supported by further representing each ﬁlter atom over sets of pre-ﬁxed multi-scale bases. As plug-and-play replacements to convolutional layers, the introduced adaptive convolutions with per-pixel dynamic atoms enable explicit modeling of intra-image variance, while avoiding heavy computation, parameters, and memory cost. Our method preserves the appealing properties of conventional convolutions as be-ing translation-equivariant and parametrically efﬁcient. We present experiments to show that, the proposed method de-livers comparable or even better performance across tasks, and are particularly effective on handling tasks with signif-icant intra-image variance. 1.

Introduction
The idea of data or context dependent network weights have long been studied in the research of neural networks.
Many concepts, like fast weight [2, 32, 31] and dynamic plasticity [24, 25] are developed to explicitly model the evolution of model parameters, and have demonstrated im-proved performance on sequential data.
In [2, 32, 31], the parameters in these networks can be divided into two groups: slow weights that are learned through training with gradient descent, and fast weights that are generated on-the-ﬂy depending on both slow weights and observed data.
In recent years, similar idea has been extended to dy-namic convolutions. Standard convolutions as in Figure 1a uses shared ﬁlters across all samples and all spatial posi-tion. Dynamic convolutions, as in Figure 1b, allow convo-lutional ﬁlters to be adapted to data in one-shot (as opposite to evolving through sequential observations). Dynamic ﬁl-ter networks (DFN) [16], conditionally parameterized con-volutions (CondConv) [42], dynamic convolutions with at-tention (DY-CNN) [6] are introduced to allow convolutional
ﬁlters to be adapted to the current observed input, and ex-plicitly modeling the inter-sample variance among images.
While improvements on certain tasks have been shown, the
ﬂexibility comes at the cost of extra parameter and com-putation [6, 42], and sacriﬁcing the translation equivari-ant property of CNNs [16]. More importantly, it is prac-tically infeasible to extend such methods from per-image adapted ﬁlters to per-pixel adapted ﬁlters due to the pro-hibitive memory footprints of applying per-pixel adapted high-dimension ﬁlters as we will show in Section 4.4.
In this paper, we enable CNNs with per-pixel adaptive convolutions as illustrated in Figure 1c, at any network lay-ers to better model intra-image variance. We introduce
Adaptive Convolutions with Dynamic Atoms (ACDA), a ver-satile and scalable convolutional layer that allows per-pixel speciﬁc ﬁlters to be adaptively generated from each local input feature patch across spatial position. To remedy the prohibitively high cost on generating and applying per-pixel adaptive ﬁlters in high dimensions, we decompose ﬁlters over dynamically generated low-dimensional ﬁlter atoms at each spatial location. The adaptive ﬁlters can now be reconstructed by linearly combining these per-pixel spe-ciﬁc dynamic atoms with cross-location shared composi-tional coefﬁcients, as illustrated in Figure 2. Most impor-tantly, the decomposition enables a fast two-layer imple-mentation of the adaptive convolutions as shown in Fig-ure 3, which reduces the prohibitive computation and mem-ory footprints of applying per-pixel speciﬁc convolutions to a level that matches standard convolutions, allowing our method becomes a versatile replacement to standard convo-lutions across layers in any CNNs.
To achieve adaptive receptive ﬁelds, we further decom-pose each ﬁlter atom over sets of multi-scale pre-ﬁxed atom bases as in Figure 4, for a two-level ﬁlter decomposition.
Now, instead of directly generating atoms, only per-pixel basis coefﬁcients are required to be generated. The multi-scale atom bases and the generated basis coefﬁcients recon-struct dynamic atoms, and allow the receptive ﬁeld at each spatial position to be selectively decided from the local fea-tures. Meanwhile the adaptive ﬁlters are effectively regular-Per-image dynamic  convolutional filter 
Per-pixel dynamic  convolutional filter 
Regular  convolutional filter 
Filter prediction  component
*
*
Input features 
Output features 
Input features 
Output features 
Input features 
Output features  (a) Standard convolutions (b) Per-image adaptive convolutions (c) Per-pixel adaptive convolutions
Figure 1: Convolutional layers. (a) Standard convolutions with ﬁlters shared across all samples and spatial position. (b) Per-image adaptive convolutions with per-image speciﬁc ﬁlters as in [6, 16, 42]. (c) The proposed per-pixel adaptive convolutions dynamically generate ﬁlters conditioned on local feature patches, and explicitly model intra-image variance using per-pixel speciﬁc ﬁlters. ized by the preﬁxed atom bases, which accelerates the learn-ing of the large-size adaptive ﬁlter generation. Importantly, our approach maintains and even reduces parameters and computation, and preserves appealing properties of CNNs including translation equivariant and weight sharing.
We show empirically that, our approach can work as plug-and-play replacements to standard convolutional lay-ers. We demonstrate the effectiveness of the proposed method using image classiﬁcation, crowd counting, and real-world image restorations as example tasks that require handling signiﬁcant intra-image variance. 2.