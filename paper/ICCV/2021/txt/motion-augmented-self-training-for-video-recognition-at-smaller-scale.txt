Abstract
The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deploy-ment on small-scale video collections. As smaller video datasets beneﬁt more from motion than appearance, we strive to train our network using optical ﬂow, but avoid its computation during inference. We propose the ﬁrst motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efﬁcient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works.
As a result we obtain a strong motion-augmented represen-tation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets,
MotionFit outperforms alternatives for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised learning by 9%-18% using the same amount of class labels. 1.

Introduction
The goal of this paper is to self-train a 3D convolu-tional neural network on an unlabeled video collection, such that it can be effectively ﬁne-tuned on small scale datasets.
This is of interest for applications in small-sized compa-nies, a household, or search and rescue robotics where large amounts of labeled video are often unavailable and the de-ployment in compute-efﬁcient scenarios is preferred. The common self-training approach is to transfer knowledge from pre-trained appearance models by pseudo-label predic-∗Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc.
Figure 1: Motion-augmented self-training utilizes pseudo-labels obtained by a motion model trained on a small labeled video dataset. We transfer knowledge from the motion model to the appearance model which is suitable for downstream video tasks without the need for optical ﬂow computation. tion, e.g. [41, 46, 60]. Yan et al. [60], for example, cluster a pre-learned appearance space before training a new net-work from scratch using cluster membership as pseudo-label.
They transfer knowledge from 19 million weakly-labeled videos and use Kinetics [9] as their target with around 250k videos to ﬁne-tune their model. Unlike them, we aim to self-train a model that can be effectively ﬁne-tuned on small-scale datasets with around 10k videos. Such small video datasets beneﬁt more from motion information than appearance [48], but the added ﬂow computation affects the efﬁciency. Other semi-supervised [27, 46] and self-supervised [21, 22] alter-natives suitable for small-scale datasets either do not use motion [27, 46] or use it at inference time also [21, 22]. So, we strive to train a convolutional neural network using op-tical ﬂow, but avoid its computation during inference. We propose to transfer knowledge from the motion represen-tation through self-training, to enable effective ﬁne-tuning even on small-scale video collections.
We are inspired by generalized distillation [35]. During training it combines knowledge distillation [23] with privi-leged information [53]. For example, to transfer knowledge
from a pre-trained 2D convolutional neural network to a 3D convolutional neural network [12, 19] or from depth to an appearance stream [17,18]. In particular, the works of Crasto et al. [11] and Stroud et al. [50] helped shape our idea. Both these works explore the transfer of motion knowledge to an appearance model using optical ﬂow as privileged infor-mation, together with the large-scale labeled Kinetics [9] dataset. We also transfer from a motion to an appearance representation, but different from [11, 50] class labels on a large-scale dataset are unavailable during transfer in our setting. Instead, we propose to obtain pseudo-labels by ﬁrst training a motion model on a small-scale labeled dataset, like UCF101 [49] or HMDB51 [32]. We perform unsuper-vised K-means clustering on the extracted motion features to obtain cluster assignments as pseudo-labels. Then we train an appearance model to predict these pseudo-labels via a self-training procedure on a larger source dataset, without using any additional class labels, see Figure 1.
Our key contribution is a motion-augmented self-training procedure, we call MotionFit. It extracts motion knowledge and transfers it to the appearance model, via self-training on a large-scale unlabeled video dataset. By such motion transfer we avoid time-consuming optical ﬂow computation during inference, similar in objective to motion knowledge distillation methods [11, 50], but without the need for labels during transfer. Our second contribution is an empirical study to discover what form video pseudo-labels should take at smaller scale, starting from the training of the pseudo-label generator to temporally mapping pseudo-labels to videos.
We train the pseudo-label generator on the motion repre-sentation of a small-scale and labeled video dataset by a multi-clip loss that makes our motion model less susceptible to the background motion irrelevant to the video label. Dur-ing self-training with pseudo-labels we also study different levels of temporal video granularity by exploring several par-titions of whole videos, which was not taken into account in related approaches, e.g., [2, 60]. Finally, we experimentally evaluate the importance of each component of our method and compare with state-of-the-art for action classiﬁcation and clip retrieval on two datasets. For clip retrieval, we im-prove over the state-of-the-art on UCF101 and match it on
HMDB51. For action classiﬁcation, our self-trained repre-sentation performs considerably better than the alternative knowledge transfer (up to +8%), self-supervised (up to +7%) and semi-supervised (up to +18%) methods. 2.