Abstract
Most existing domain adaptation methods focus on adaptation from only one source domain, however, in prac-tice there are a number of relevant sources that could be leveraged to help improve performance on target domain.
We propose a novel approach named T-SVDNet to address the task of Multi-source Domain Adaptation (MDA), which is featured by incorporating Tensor Singular Value Decom-position (T-SVD) into a neural network’s training pipeline.
Overall, high-order correlations among multiple domains and categories are fully explored so as to better bridge the domain gap. Speciﬁcally, we impose Tensor-Low-Rank (TLR) constraint on a tensor obtained by stacking up a group of prototypical similarity matrices, aiming at cap-turing consistent data structure across different domains.
Furthermore, to avoid negative transfer brought by noisy source data, we propose a novel uncertainty-aware weight-ing strategy to adaptively assign weights to different source domains and samples based on the result of uncertainty esti-mation. Extensive experiments conducted on public bench-marks demonstrate the superiority of our model in address-ing the task of MDA compared to state-of-the-art meth-ods. Code is available at https://github.com/lslrh/
T-SVDNet. 1.

Introduction
Deep learning methods have shown superior perfor-mance with huge amounts of training data as rocket fuel.
However, directly transferring knowledge learned on a cer-tain visual domain to other domains with different distri-butions would degrade the performance signiﬁcantly due to
*Work partly done during an internship at Noah’s Ark Lab
†Corresponding Author the existence of domain shift [42]. To handle this prob-lem, the prominent approaches such as transfer learning and unsupervised domain adaptation (UDA) endeavor to ex-tract domain-invariant features. Discrepancy-based meth-ods reduce the domain gap by minimizing the discrep-ancy between source and target distributions, such as Max-imum Mean Discrepancy (MMD) [25], correlation align-ment [35], and contrastive domain discrepancy [16]. Ad-versarial methods attempt to align source and target do-mains through adversarial training [34, 38] or GAN-based loss [14, 46]. These methods only focus on domain adap-tation with only single source. However, in many practical application scenarios, there are a number of relevant sources collected in different ways available, which could be used to help improve performance on target domain.
Naively combining various sources into one is not an ef-fective way to fully exploit abundant information within multiple sources, and might even perform worse than single-source methods, because domain gap among mul-tiple sources causes confusion in the learning process
[44]. Some Multi-Source Domain Adaptation (MDA) ap-proaches [41, 24, 45, 32, 12] focus on aligning multiple source domains and a target domain by projecting them into a domain-invariant feature space. This is done by either explicitly minimizing the discrepancy of different domains
[12, 13, 32] or learning an adversarial discriminator to align distributions of different domains [41, 43, 24]. How-ever, eliminating distribution discrepancy of data has the risk of sacriﬁcing discrimination ability. Moreover, these methods only achieve pair-wise matching, neglecting un-derlying high-order relations among all domains. Another widely used way in MDA is distribution-weighted combin-ing rule [13, 43, 24], which takes a weighted combination of pre-trained source classiﬁers as the classiﬁer for target domain. In spite of reasonable performance on MDA task, they do not take into consideration intra-domain weightings
among different training samples, so that underlying noisy source data may hurt the performance of learning in the tar-get, which is referred to as “negative transfer” [30].
To address the aforementioned limitations, we propose a novel method named T-SVDNet which incorporates tensor singular value decomposition into a neural network’s train-ing pipeline. In MDA tasks, although there is large domain gap between different domains, data belonging to the same category do share essential semantic information across do-mains. Therefore, we assume that data from different do-mains should follow a certain kind of category-wise struc-ture. Based on this assumption, we explore high-order rela-tionships among multiple domains and categories in order to enforce the alignment of source and target at the proto-typical correlation level. Speciﬁcally, we impose Tensor-Low-Rank (TLR) constraint on a tensor which is obtained by stacking up a set of prototypical similarity matrices, so that the relationships between categories are enforced to be consistent across domains by pursuing the lowest-rank structure of tensor. Furthermore, to avoid negative trans-fer [30] caused by noisy training data, we propose a novel uncertainty-aware weighting strategy to guide the adapta-tion process.
It could dynamically assign weights to dif-ferent domains and training samples based on the result of uncertainty estimation. To train the whole framework with both classiﬁcation loss and low-rank regularizer, we adopt an alternative optimization strategy, that is, optimizing net-work parameters with the low-rank tensor ﬁxed and opti-mizing the low-rank tensor with network parameters un-changed. We conduct extensive evaluations on several pub-lic benchmark datasets, where a signiﬁcant improvement over existing MDA methods has been achieved. Overall, the main contributions of this paper can be summarized as follows:
• We propose the T-SVDNet to explore high-order relation-ships among multiple domains and categories from the per-spective of tensor, which facilitates both domain-invariance and category-discriminability.
• We devise a novel uncertainty-aware weighting strategy to balance different source domains and samples, so that clean data are fully exploited while negative transfer led by noisy data is avoided.
• We propose an alternative optimization method to train the deep model along with low-rank regularizer. Extensive evaluations on benchmark datasets demonstrate the superi-ority of our method. 2.