Abstract
In video highlight detection, the goal is to identify the interesting moments within an unedited video. Although the audio component of the video provides important cues for highlight detection, the majority of existing efforts fo-cus almost exclusively on the visual component. In this pa-per, we argue that both audio and visual components of a video should be modeled jointly to retrieve its best moments.
To this end, we propose an audio-visual network for video highlight detection. At the core of our approach lies a bi-modal attention mechanism, which captures the interaction between the audio and visual components of a video, and produces fused representations to facilitate highlight detec-tion. Furthermore, we introduce a noise sentinel technique to adaptively discount a noisy visual or audio modality. Em-pirical evaluations on two benchmark datasets demonstrate the superior performance of our approach over the state-of-the-art methods. 1.

Introduction
We have witnessed an explosion of online video con-tent in recent years, which may be partly attributed to the rapid adoption of video based social networks like Insta-gram and TikTok. This has led to increasing demands for video highlight detection, which aims to automatically detect interesting moments (called “highlights”) within a video. Highlight detection is important due to its broad range of downstream applications including video summa-rization, recommendation, editing, and browsing. In con-sequence, there has been significant progress in the field in recent years [37, 12, 46, 50, 17, 49, 7, 44, 31, 15, 42].
However, the majority of existing research efforts focus on visual highlight detection. Audio-visual highlight de-tection is largely unexplored territory.
In this paper, we posit that interesting moments can be identified from both
Figure 1. Audio-visual highlight detection: audio can be informa-tive about which part of the video is a highlight. In this video, a car crashes during a race in front of a large crowd. The highlight (in green) is the crash, and we also show the top three audio class probabilities from a pre-trained audio classification network [22].
The class probabilities (speech, cheering, and crowd) show that the cheering and crowd noise dies out during the crash, while talking increases. Intuitively, we understand that if people stop cheering suddenly, something must have happened. In this work, we learn to utilize such audio cues. visual and audio information. For example, Fig. 1, shows a video of a car crashing during a race. We may identify the interesting part of the video by seeing the crash. On the other hand, hearing people stop cheering and start talking could also be an indicator that the moment is interesting.
While we can process the visual and audio information sep-arately, our insight is that they also interact with each other.
We can imagine that if we were in the crowd depicted in
Fig. 1, we would look to see what had happened if every-body suddenly stopped cheering. It can also work the other way around: what we hear can reinforce our beliefs that the given moment is interesting. These observations moti-vate us to propose an approach that jointly learns from vi-sual and audio information to detect highlights in videos. It consists of two different attention mechanisms: a unimodal self-attention mechanism that models the relationships be-tween moments belonging to the same modality; and a bi-modal attention mechanism that models the interaction be-tween the two modalities.
Furthermore, we impart the ability to ignore a modality on our model. Intuitively, if we hear something interesting, but do not see anything interesting when we look, we would like to be able to ignore what we hear. In addition, if we do not hear anything of interest, it is also not worth looking.
Inspired by the visual sentinel [24], we introduce a noise sentinel in the bimodal attention mechanism that allows our model to “look-away” from a modality by attending to the noise sentinel instead.
The proposed attention mechanisms and the noise sen-tinel are novel and effective as empirically indicated in the ablation studies. We demonstrate our model’s superior per-formance on three well-known benchmarks, where our ap-proach significantly outperforms the state-of-the-art meth-ods. 2.