Abstract
Efficient reasoning about the semantic, spatial, and tem-poral structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a con-tinuous function which maps locations in Bird’s Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representa-tion. This allows our model to selectively attend to rele-vant regions in the input while ignoring information irrele-vant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting in-volving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, vi-sualizing the attention maps for models with NEAT inter-mediate representations provides improved interpretability. 1.

Introduction
Navigating large dynamic scenes for autonomous driv-ing requires a meaningful representation of both the spa-tial and temporal aspects of the scene.
Imitation Learn-ing (IL) by behavior cloning has emerged as a promising approach for this task [5, 10, 16, 53, 78]. Given a dataset of expert trajectories, a behavior cloning agent is trained through supervised learning, where the goal is to predict the actions of the expert given some sensory input regard-ing the scene [48]. To account for the complex spatial and temporal scene structure encountered in autonomous driv-ing, the training objectives used in IL-based driving agents have evolved by incorporating auxiliary tasks. Pioneering methods, such as CILRS [16], use a simple self-supervised auxiliary training objective of predicting the ego-vehicle ve-locity. Since then, more complex training signals aiming
*indicates equal contribution
Figure 1: Neural Attention Fields. We use an MLP to iter-atively compress the high-dimensional input into a compact low-dimensional representation ci based on the BEV query location (x, y, t). Our model outputs waypoint offsets and auxiliary semantics from ci continuously and with a low memory footprint. Training for both tasks jointly leads to improved driving performance on CARLA. to reconstruct the scene have become common, e.g. image auto-encoding [47], 2D semantic segmentation [29], Bird’s
Eye View (BEV) semantic segmentation [37], 2D semantic prediction [27], and BEV semantic prediction [58]. Per-forming an auxiliary task such as BEV semantic prediction, which requires the model to output the BEV semantic seg-mentation of the scene at both the observed and future time-steps, incorporates spatiotemporal structure into the inter-mediate representations learned by the agent. This has been shown to lead to more interpretable and robust models [58].
However, so far, this has only been possible with expensive
LiDAR and HD map-based network inputs which can be easily projected into the BEV coordinate frame.
The key challenge impeding BEV semantic prediction from camera inputs is one of association: given a BEV spa-tiotemporal query location (x, y, t) in the scene (e.g. 2 me-ters in front of the vehicle, 5 meters to the right, and 2 sec-onds into the future), it is difficult to identify which image pixels to associate to this location, as this requires reason-ing about 3D geometry, scene motion, ego-motion, and in-tention, as well as interactions between scene elements. In this paper, we propose NEural ATtention fields (NEAT), a
flexible and efficient feature representation designed to ad-dress this challenge. Inspired by implicit shape representa-tions [39, 50], NEAT represents large dynamic scenes with a fixed memory footprint using a multi-layer perceptron (MLP) query function. The core idea is to learn a function from any query location (x, y, t) to an attention map for fea-tures obtained by encoding the input images. NEAT com-presses the high-dimensional image features into a compact low-dimensional representation relevant to the query loca-tion (x, y, t), and provides interpretable attention maps as part of this process, without attention supervision [79]. As shown in Fig. 1, the output of this learned MLP can be used for dense prediction in space and time. Our end-to-end ap-proach predicts waypoint offsets to solve the main trajec-tory planning task (described in detail in Section 3), and uses BEV semantic prediction as an auxiliary task.
Using NEAT intermediate representations, we train sev-eral autonomous driving models for the CARLA driving simulator [20]. We consider a more challenging evalua-tion setting than existing work based on the new CARLA
Leaderboard [1] with CARLA version 0.9.10, involving the presence of multiple evaluation towns, new environ-mental conditions, and challenging pre-crash traffic scenar-ios. We outperform several strong baselines and match the privileged expert’s performance on our internal evaluation routes. On the secret routes of the CARLA Leaderboard,
NEAT obtains competitive driving scores while incurring significantly fewer infractions than existing methods.
Contributions: (1) We propose an architecture combining our novel NEAT feature representation with an implicit de-coder [39] for joint trajectory planning and BEV semantic prediction in autonomous vehicles. (2) We design a chal-lenging new evaluation setting in CARLA consisting of 6 towns and 42 environmental conditions and conduct a de-tailed empirical analysis to demonstrate the driving perfor-mance of NEAT. (3) We visualize attention maps and se-mantic scene interpolations from our interpretable model, yielding insights into the learned driving behavior. Our code is available at https://github.com/autonomousvision/neat. 2.