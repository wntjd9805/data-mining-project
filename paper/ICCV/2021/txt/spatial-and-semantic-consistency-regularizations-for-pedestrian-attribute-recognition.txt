Abstract
While recent studies on pedestrian attribute recognition have shown remarkable progress in leveraging complicated networks and attention mechanisms, most of them neglect the inter-image relations and an important prior: spatial consistency and semantic consistency of attributes under surveillance scenarios. The spatial locations of the same attribute should be consistent between different pedestrian images, e.g., the “hat” attribute and the “boots” attribute are always located at the top and bottom of the picture re-spectively. In addition, the inherent semantic feature of the
“hat” attribute should be consistent, whether it is a base-ball cap, beret, or helmet. To fully exploit inter-image re-lations and aggregate human prior in the model learning process, we construct a Spatial and Semantic Consistency (SSC) framework that consists of two complementary reg-ularizations to achieve spatial and semantic consistency for each attribute. Specifically, we first propose a spatial consistency regularization to focus on reliable and stable attribute-related regions. Based on the precise attribute lo-cations, we further propose a semantic consistency regular-ization to extract intrinsic and discriminative semantic fea-tures. We conduct extensive experiments on popular bench-marks including PA100K, RAP, and PETA. Results show that the proposed method performs favorably against state-of-the-art methods without increasing parameters. 1.

Introduction
Pedestrian attribute recognition [26, 20] aims to predict multiple human attributes, such as age, gender, and cloth-ing, as semantic descriptions for a pedestrian image. Due to the ubiquitous application in surveillance scenarios [21], scene understanding [18], and human perception [7], nu-merous methods [1, 10, 13, 20, 12, 15, 17, 2, 19] have been
*Corresponding author. proposed and significant progress has been made in the last decade.
Existing methods [16, 15, 17, 19] mainly utilize the com-plicated network, such as Feature Pyramid Network (FPN), to enrich attribute representation from multi-level feature maps, and combine the attention mechanisms to precisely locate attribute-related regions. Recently, VAC [2] utilizes a human prior, that attention regions of random augmenta-tions of the same image are consistent, to improve model robustness. The above methods [16, 15, 17, 19] mainly emphasize learning discriminative attribute features from an individual image, instead of exploiting the relation be-tween different pedestrian images of the same attribute. In contrast, our methods show that mining the inter-image re-lations between different images of the same attribute can significantly help the model locate attribute-related regions and extract inherent semantic features. We exploit inter-image relations from the perspective of spatial relation and semantic relation.
For the inter-image spatial relation, we hypothesize that the spatial location of the same attribute is basically con-sistent between different pedestrian images, which is called
SPAtial Consistency (SPAC) in this work. For example, the
“hat” attribute and the “boots” attribute mostly appears at the top and bottom of the picture, respectively, which is shown in the first row of Figure 1(a). However, we ob-serve that Class Activation Maps (CAMs) [25] of the same attribute of the baseline method have significant location variations. Some examples are shown in the second row of
Figure 1(a). These CAMs of the same attribute between dif-ferent pedestrians are inconsistent, some of which (with red boundary) deviate seriously from attribute-related areas, no matter for the “short sleeve”, “boots”, or “hat” attributes.
This phenomenon contradicts our spatial consistency hy-pothesis, and indicates that the baseline model easily in-clines to focus on the background, irrelevant foreground, or a small part of attribute-related regions, which is called
(a) Spatial consistency on the “short sleeve”, “boots”, and “hat” attributes. (b) Semantic consistency of different samples on the“hat” attribute.
Figure 1: Illustration of our main hypothesis on the spatial and semantic consistency. In (a), CAMs of the baseline method in “short sleeve”, “boots”, and “hat” attributes of the PA100K are visualized in the second row. Attribute-related regions of each attribute are plotted by the red dotted frame in the first row. Highlighted regions of the second CAM (with red boundary) of each attribute deviate from attribute-related regions severely, which are inconsistent with counterparts of the first CAM (with green boundary). In (b), we present several samples of the “hat” attribute. Although these samples differ greatly in shape, size, and color, the intrinsic semantic features of the “hat” attribute extracted by the model should remain unchanged. Best viewed in color. the “spatial attention deviation problem” in this work.
For the inter-image semantic relation, inherent seman-tic features of the same attribute between different images should be consistent, which is called SEMantic Consistency (SEMC) in this work. For example, as illustrated in Figure 1(b), regardless of the difference in shape, size, and color between various samples, the intrinsic semantic features of the “hat” attribute should remain basically unchanged. This property is also indispensable for learning discriminative features and obtaining a robust model.
To achieve the spatial and semantic consistency between pedestrian images of the same attribute, we propose a novel framework composed of the SPAC and SEMC module.
Specifically, the SPAC module generates reliable spatial locations for each attribute and maintains a stable spatial memory to suppressing location shift, which is caused by overfitting or label noise. Based on precise spatial locations, the SEMC module extracts intrinsic semantic features and maintains a stable semantic memory to suppress the influ-ence of irrelevant characteristics, such as shape, color, and size for the “hat” attribute.
We make the following three contributions in this work:
• We establish an effective consistency framework for pedestrian attribute recognition, which makes full use of inter-image spatial and semantic relations between images of the same attribute.
• We design spatial and semantic consistency modules to generate precise spatial attention regions and extract discriminative semantic features for each attribute.
• We confirm the efficacy of the proposed method by achieving state-of-the-art performance on three popu-lar datasets including PA100K, PETA, and RAP. 2.