Abstract
Point clouds can be represented in many forms (views), typically, point-based sets, voxel-based cells or range-based images(i.e., panoramic view). The point-based view is geometrically accurate, but it is disordered, which makes it difficult to find local neighbors efficiently. The voxel-based view is regular, but sparse, and computation grows cubicly when voxel resolution increases. The range-based view is regular and generally dense, however spherical pro-jection makes physical dimensions distorted. Both voxel-and range-based views suffer from quantization loss, espe-cially for voxels when facing large-scale scenes. In order to utilize different view’s advantages and alleviate their own shortcomings in fine-grained segmentation task, we propose a novel range-point-voxel fusion network, namely RPVNet.
In this network, we devise a deep fusion framework with multiple and mutual information interactions among these three views, and propose a gated fusion module (termed as
GFM), which can adaptively merge the three features based on concurrent inputs. Moreover, the proposed RPV interac-tion mechanism is highly efficient, and we summarize it to a more general formulation. By leveraging this efficient in-teraction and relatively lower voxel resolution, our method is also proved to be more efficient. Finally, we evaluated the proposed model on two large-scale datasets, i.e., Se-manticKITTI and nuScenes, and it shows state-of-the-art performance on both of them. Note that, our method cur-rently ranks 1st on SemanticKITTI leaderboard without any extra tricks. 1.

Introduction 3D computer vision is receiving more and more atten-tion due to its wide range of applications, such as AR/VR,
*Equal contribution. The first two authors are listed in the alphabetical order. †Corresponding author. This work was supported by National Key
R&D Program of China (Grant No.2020AAA010400X). (a) Point-based: disordered (b) Voxel-based: sparse, quantization loss (c) Range-based: physical dimensions distorted
Figure 1. All three views have shortcomings. (a) point-based: the points are irregular, which makes finding the neighbors of a point inefficient. (b) voxel-based: voxelization brings quantization loss, and the computation grows cubicly when resolution increases. (c) range-based: range-image distorts physical dimensions because of spherical projection.
In this paper, we aim robotics and autonomous driving. to improve the performance of semantic segmentation in driving scenario, so as to provide high-quality point-wise perception of the entire 3D scene. Collected by the Light
Detection and Ranging (LiDAR) sensors, 3D data usually comes in the format of point clouds. And there are several common forms (views) to represent it along with some spe-cific preprocesses.
Conventionally, researchers rasterize the point cloud into voxel cells, as depicted in Figure 1(b), and process them using 3D volumetric convolutions [11, 28]. With voxels de-fined based on the Cartesian coordinate system, voxel-based view retains physical dimensions and has a friendly mem-ory locality. Nevertheless, it is relatively sparse and requires very high resolution in order to get rid of quantized informa-tion loss, which brings a cubic increase in both computation and memory footprint.
Recently, the point-based view, as shown in Figure 1(a), has attracted more and more attention, and various works
PointNet [24] is the tend to consume points directly. pioneer work which uses per-point Multi-Layer Percep-tions(MLPs) to extract point features, but it lacks the lo-cal context modeling capability. Based on PointNet, later researches [26, 2, 37, 34, 31] have paid extensive atten-tion to each point’s local feature extraction by aggregating its neighboring features. However, points are unstructured, thus inefficient to search one point’s neighbors due to ran-dom memory access.
A parallel track of works [36, 38, 23, 12] follows a spher-ical projection scheme, i.e., range-based view, as shown in
Figure 1(c), where the sub-spaces of 3D information in the form of depth map are learned with well studied 2D net-works. In these approaches, a convolution tower on range-image can aggregate information across a large receptive field, helping to alleviate the point sparsity issue. Due to the spherical projection, however, physical dimensions are not distance-invariant and objects may overlap each other severely in a cluttered scene.
In aspect of point cloud segmentation in large-scale driv-ing scene, we discover that: 1). voxel-based methods are relatively higher in performance than point- and range-based methods, meanwhile the best ones of point- and range-based methods are basically the same, as depicted in
Figure 2(a); 2). range-based methods are relatively more efficient than point- and voxel-based methods because of highly optimized 2D convolution, and point-based methods are far from real-time requirements when involving local neighbors searching; 3). voxel-based methods are hard to keep a high voxel resolution when efficiency is also taken into account, and the performance drop sharply when reso-lution decreases, as shown in Figure 2(b).
It is intuitive to combine different views together, lever-aging the complementary information while preserving ad-vantages and alleviating shortcomings. One recent attempt is PVCNN [21], a point-voxel fusion scheme, in which voxel offers a coarse-grained local feature, while point pre-serves fine-grained geometrical features by running simple
It provides a great perspective, but the per-point MLPs. performance improvement brought by point-voxel fusion is limited, as depicted in Figure 2(b), and it is not sufficient to use a simple additive fusion.
In this paper, we propose a deep and adaptive range-point-voxel fusion framework aiming to synergize all the three views’ representations. More specifically, as shown in (a) methods vs mIoU (b) voxel resolution vs mIoU
Figure 2. Performance overview. (a) performance distribution of different methods. It indicates that voxel-based methods is gener-ally better than point- and range-based methods, and multi-view fusion based method is better than single-view methods. (b) per-formance versus voxel resolution. V denotes voxel-based method,
PV is point-voxel fusion method, and RPV is our range-point-voxel fusion method. We can discover that PV fusion can only get a limited improvement, but our RPV fusion can consistently boost the performance even when the voxel resolution is relatively high.
Figure 4, we design a fusion strategy using points as middle hosts, and transfer features on range-pixels and voxel-cells to points, then apply an adaptive feature selection in order to choose the best feature representation for each point, and finally transfer the fused features on points back to range-image and voxels. Compared to other previous multi-view fusion methods [6, 42, 33, 20, 39, 14], which either fuse in the front or the end of network, our method conducts afore-mentioned fusion multi-times in the network, which allows different views to enhance each other in a deeper and more flexible way. As for efficiency: Firstly, we propose an ef-ficient RPV interaction mechanism by utilizing hash map-ping. Secondly, we use a relatively lower voxel resolution and sparse convolution in voxel branch. Thirdly, we per-form simple MLPs on point branch similar to [21], getting rid of inefficient local neighbors searching. Finally, we em-ploy a highly efficient range branch to decrease computa-tion. Moreover, we find that the class is extremely imbal-anced in datasets, thus we design an instance CutMix aug-mentation in the training phase to alleviate the class imbal-ance problem.
Main contributions in this paper are listed as follows:
First, we devise a deep and adaptive range-point-voxel fusion framework, which allows different views to enhance each other in a more flexible way.
Second, we propose an efficient RPV interaction mecha-nism by utilizing hash mapping, and summarize it to a more general formulation for future extension.
Finally, we conduct massive experiments to evalute the effectiveness and efficiency of our proposed method, and our method achieves state-of-the-art results on both Se-manticKITTI [3] and nuScenes [4] datasets.
2.