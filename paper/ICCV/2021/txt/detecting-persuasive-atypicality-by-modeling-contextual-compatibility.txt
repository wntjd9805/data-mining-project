Abstract
We propose a new approach to detect atypicality in per-suasive imagery. Unlike atypicality which has been stud-ied in prior work, persuasive atypicality has a particular purpose to convey meaning, and relies on understanding the common-sense spatial relations of objects. We propose a self-supervised attention-based technique which captures contextual compatibility, and models spatial relations in a precise manner. We further experiment with capturing com-mon sense through the semantics of co-occurring object classes. We verify our approach on a dataset of atypicality in visual advertisements, as well as a second dataset cap-turing atypicality that has no persuasive intent. 1.

Introduction
Visually creative images, such as advertisements or pub-lic service announcements, may purposefully contain atyp-ical portrayals of objects as a rhetorical way for attracting viewers’ attention [15]. In the marketing and communica-tions research community, atypicality has gained attention because of its importance to understanding the persuasive-ness and rhetoric of visual media [28, 23, 48]. However, detecting this type of atypicality is challenging for intelli-gent systems. First, atypicality may involve metaphorical object transformations or intentionally surprising composed objects. Second, the atypicality transformation types are diverse and creative. Third, unpacking them may require common-sense reasoning. For example, Fig. 1a is an atyp-ical advertisement for a beverage. It is unusual for a pig to wear a bridal veil even though the pig and veil are both nor-mal objects. The ability to detect this type of purposefully atypical objects and understand their roles in conveying the intent of the image is necessary for an intelligent system to reason about information in persuasive media. In this work, we propose to model implicit knowledge of contextual com-patibility in order to detect persuasive atypicality.
Our first hypothesis is that persuasive atypicality can be detected by checking the compatibility between each pos-sibly atypical object and the rest of the image as context.
Figure 1. These images illustrate the importance of object interac-tions and their spatial relative position for atypicality detection. (a)
Pig wearing a bridal veil is atypical; (b) If a handled brush instead of a veil is on top of the pig’s head, then the image is typical; (c)
If the veil’s location is different, the image may also be typical.
For example, in Fig. 1a, the pig is not compatible with its context (a bridal veil on its head), and the veil is also not compatible with its context — on a pig’s head. We propose an unsupervised approach by using reconstruction losses of masked regions. We expect that a self-supervised model trained on masked region reconstruction could learn enough implicit knowledge of contextual compatibility; this pre-trained model may then be used to detect atypical images.
Our second hypothesis is that the interactions between objects and their spatial relative positions play a key role in detecting atypicality. If it were a handled brush instead of a bridal veil over the pig’s head (Fig. 1b), or if the veil were placed at another location instead of on top of the pig’s head (Fig. 1c), the image would no longer be atypical. In order to better interpret object-object spatial interaction, we propose a new method to compute the attention weights be-tween key-query regions of our transformer-based models.
Finally, our third hypothesis is that, for some types of persuasive atypicality, the semantic relation between nearby object classes may offer compatibility clues beyond visual features.
In Fig. 1a, knowing that there is a “pig” and a
“bridal veil” and their spatial relationship may be helpful to conclude that the image is atypical, instead of knowing exactly what that pig or veil look like. To take advantage of
semantic knowledge learned in language models, we fine-tune BERT on detected class labels of regions of interest.
Experiments on a recent visual advertising dataset [48] demonstrate the effectiveness of our approaches and sup-port our hypotheses. Our approach outperforms prior ap-proaches for abnormality detection (e.g. a One-Class SVM) by more than 9%. We have also gleaned insights on how different types of persuasive atypicality impact the detec-tion performance. We validate that atypicality transforma-tions involving spatial interactions between objects are bet-ter solved by our approaches than baselines. Then we eval-uate the generalization of our approaches using an existing dataset of real-scene, non-persuasive atypical images [46].
To understand the labelling requirement of both tasks, we compare our unsupervised approaches of contextual compatibility with supervised models which are trained on the ground-truth labels. We observe very different perfor-mances on the two datasets, which reveals that the labelling requirement depends on the training size ratio between su-pervised and unsupervised methods and the complexity of the atypicality transformations. Lastly, we investigate two possibilities for representing the image context: visual com-patibility versus semantic compatibility. Experimental re-sults show that visual features are essential, but the seman-tic compatibility can help when atypicality transformations feature unusual combination of normal objects. 2.