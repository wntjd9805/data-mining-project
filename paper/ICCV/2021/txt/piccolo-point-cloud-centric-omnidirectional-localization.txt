Abstract
We present PICCOLO, a simple and efﬁcient algorithm for omnidirectional localization. Given a colored point cloud and a 360˝ panorama image of a scene, our objective is to recover the camera pose at which the panorama im-age is taken. Our pipeline works in an off-the-shelf manner with a single image given as a query and does not require any training of neural networks or collecting ground-truth poses of images. Instead, we match each point cloud color to the holistic view of the panorama image with gradient-descent optimization to ﬁnd the camera pose. Our loss function, called sampling loss, is point cloud-centric, eval-uated at the projected location of every point in the point cloud. In contrast, conventional photometric loss is image-centric, comparing colors at each pixel location. With a simple change in the compared entities, sampling loss ef-fectively overcomes the severe visual distortion of omnidi-rectional images, and enjoys the global context of the 360˝ view to handle challenging scenarios for visual localiza-tion. PICCOLO outperforms existing omnidirectional lo-calization algorithms in both accuracy and stability when evaluated in various environments. 1.

Introduction
With the recent advancements in 3D sensing technology, 3D maps of the environment are often available for down-load [1] or can be easily captured with commodity sen-sors [9]. The 3D map and the accurate location of the user within the map provide crucial information for AR/VR ap-plications or other location-based services. Visual localiza-tion is a cheap localization method as it only uses an image input and utilizes the 3D map without additional sensors such as WIFI, GPS, or gyroscopes. However, visual local-ization is fragile to changes in illumination or local geomet-ric variations resulting from object displacements [37, 43].
Further, with the limited ﬁeld of view, perspective cameras often fail to regress the camera pose when the observed im-age lacks visual features (e.g., a plain wall) or the scene exhibits symmetric or repetitive structure [42, 40].
Figure 1: Overview of our approach. PICCOLO minimizes a novel, point cloud-centric loss function called sampling loss. After the initialization phase trims off local minima,
PICCOLO minimizes the sampling loss with gradient de-scent.
Omnidirectional cameras, equipped with a 360˝ ﬁeld of view, provide a holistic view of the surrounding environ-ment. Hence these cameras are immune to small scene changes and ambiguous local features [47], which gives them the potential to dramatically improve the performance of visual localization algorithms. However, the large ﬁeld of view comes with a cost: signiﬁcant visual distortion caused by the spherical projection equation. This makes it difﬁcult to directly apply conventional visual localization algorithms on omnidirectional cameras [21, 12, 45, 15, 7], as many vi-sual localization algorithms [46, 40, 42, 39] do not account for distortion. Furthermore, learning-based approaches are bound to the settings they are trained on, and cannot gener-alize to arbitrary scenes.
In this paper, we introduce PICCOLO, a simple yet ef-Stanford2D-3D-S
MPO
OmniScenes
OmniScenes (w. change)
Figure 2: Qualitative results of PICCOLO. We display the input query image (top), and the projected point cloud under the estimated camera pose (bottom). fective omnidirectional localization algorithm. PICCOLO optimizes over sampling loss, which samples color val-ues from the query image and compares them with the point cloud color. We only utilize the color information from point clouds, as it is usually available from raw mea-surements. With a simple formulation, PICCOLO can be adapted to any scene with 3D maps in an off-the-shelf man-ner. Further, PICCOLO can work seamlessly with any other point-wise information, such as semantic segmen-tation labels shown in Figure 5. Sampling loss is point cloud-centric, as every point is taken into consideration.
In contrast, conventional photometric loss widely used in computer vision [13, 11] evaluates the color difference at every pixel location [11, 30], thus is image-centric. Our point cloud-centric formulation leads to a signiﬁcant per-formance boost in omnidirectional localization, where the image-centric approach suffers from distorted omnidirec-tional images unless the distortion is explicitly considered with additional processing [44, 15].
The gradient of our proposed sampling loss can be efﬁ-ciently obtained with differentiable sampling [20]. While differentiable sampling is widely used to minimize discrep-ancies in the projected space, it is usually part of a learned module [13, 18]. Instead, we utilize the operation in a stand-alone fashion, making our framework cheap to compute.
We further accelerate the loss computation by ignoring the non-differentiable, costly components of projection, such as occlusion handling. These design choices make sampling loss very fast: it only takes 3.5 ms for 106 points on a com-modity GPU. With the rich information of the global con-text in point cloud color, our efﬁcient formulation is empiri-cally robust against visual distortions and more importantly, local scene changes. The algorithm quickly converges to the global minimum of the proposed loss function as shown in
Figure 1.
Equipped with a light-weight search for decent start-ing points, PICCOLO achieves stable localization in vari-ous datasets. The algorithm is extensively evaluated on in-door/outdoor scenes and scenes with dynamic camera mo-tion, scene changes, and arbitrary point cloud rotation. Sev-eral qualitative results of our algorithm are shown in Fig-ure 2 and 5. In addition, we introduce a new dataset called
OmniScenes to highlight the practicality of PICCOLO. Om-niScenes contains diverse recordings with signiﬁcant scene changes and motion blur, making it the ﬁrst dataset tar-geted for omnidirectional localization where visual local-ization algorithms frequently malfunction. PICCOLO con-sistently exhibits performance superior to the previous ap-proaches [6, 44] in all of the tested datasets under a ﬁxed hyperparameter conﬁguration, indicating the practical ef-fectiveness of our algorithm. 2.