Abstract
Most recent approaches for online action detection tend to apply Recurrent Neural Network (RNN) to capture long-range temporal structure. However, RNN suffers from non-parallelism and gradient vanishing, hence it is hard to be optimized.
In this paper, we propose a new encoder-decoder framework based on Transformers, named OadTR, to tackle these problems. The encoder attached with a task token aims to capture the relationships and global inter-actions between historical observations. The decoder ex-tracts auxiliary information by aggregating anticipated fu-ture clip representations. Therefore, OadTR can recognize current actions by encoding historical information and pre-dicting future context simultaneously. We extensively eval-uate the proposed OadTR on three challenging datasets:
HDD, TVSeries, and THUMOS14. The experimental re-sults show that OadTR achieves higher training and infer-ence speeds than current RNN based approaches, and sig-nificantly outperforms the state-of-the-art methods in terms of both mAP and mcAP. Code is available at https:
//github.com/wangxiang1230/OadTR. 1.

Introduction
The purpose of online action detection is to correctly identify ongoing actions from streaming videos without any access to the future. Recently, this task has received increas-ing attention due to its great potential of diverse applica-tion prospects in real life, such as autonomous driving [24], video surveillance [45], anomaly detection [37,38], etc. The crucial challenge of this task is that we need to detect the actions at the moment that video frames arrive with inade-quate observations. To solve the problem, it is important to learn the long-range temporal dependencies.
∗ Corresponding authors.
Figure 1: Comparison between OadTR and the state-of-the-art online action detection methods (i.e., TRN [54] and
IDN [15]): (a) Comparison of the training speeds; (b) Com-parison of the inference speeds; (c) Comparison of the per-formance on the challenge THUMOS14 dataset.
Current approaches tend to apply RNN to model the tem-poral dependencies and have achieved impressive improve-ments [11, 13, 15, 16, 28, 54]. Typically, Information Dis-crimination Network (IDN) [15] designs a RNN like ar-chitecture to encode long-term historical information, and then conduct action recognition at current moment. How-ever, RNN like architectures have the problems of non-parallelism and gradient vanishing [33, 39]. Thus it is hard to optimize the architectures, which may result in an unsat-isfactory performance. This is a challenging problem for current approaches, but much less effort has been paid to solve it. To further improve the performance, we need to design a new efficient and easily-optimized framework. For this purpose, we propose to apply Transformers [48]. Trans-formers possess the strong power of long-range temporal modeling by the self-attention module, and have achieved remarkable performance in both natural language process-ing [12, 48, 57] and various vision tasks [14, 58]. Existing works have proved that Transformers have a better conver-gence than RNN architectures [23, 26], and they are also computationally efficient. The above properties of Trans-formers can naturally provide alternative scheme for the on-line action detection task.
The above observations motivate this work.
In partic-ular, we propose a carefully designed framework, termed
OadTR, by introducing the power of Transformers to the online action detection task, as illustrated in Figure 2. The proposed OadTR is an encoder-decoder architecture which can simultaneously learn long-range historical relationships and future information to classify current action. The first step is to extract clip-level feature sequence from a given video by a standard CNN. We then embed a task token to the clip-level feature sequence and input them to the encoder module. By this means, the output of the task token can en-code the global temporal relationships among the historical observations. In contrast, the decoder is designed to predict the actions that may take place in the future moments. Fi-nally, we concatenate the outputs of both the task token and decoder to detect the online actions. We compare OadTR with other RNN based approaches in Figure 1, which shows that the proposed OadTR is both efficient and effective. To further demonstrate the effectiveness of OadTR, we conduct a large number of experiments on three public datasets, in-cluding HDD [41], TVSeries [10], and THUMOS14 [22], and achieve significant improvements in terms of both mAP and mcAP metrics.
Summarily, we make the following three contributions:
• To the best of our knowledge, we are the first to bring
Transformers into online action detection task and pro-pose a new framework, i.e., OadTR;
• We specially design the encoder and decoder of
OadTR which can aggregate long-range historical in-formation and future anticipations to improve online action detection;
• We conduct extensive experiments, and the results demonstrate the proposed OadTR significantly outper-forms state-of-the-art methods. The massive and com-prehensive ablation studies can further dissect the un-dergoing properties of OadTR. 2.