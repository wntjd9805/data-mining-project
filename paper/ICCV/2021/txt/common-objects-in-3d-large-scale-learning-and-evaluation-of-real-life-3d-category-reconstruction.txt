Abstract
Traditional approaches for learning 3D object cate-gories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facil-itate advances in this ﬁeld by collecting real-world data in a magnitude similar to the existing synthetic counterparts.
The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos cap-turing objects from 50 MS-COCO categories and, as such, it is signiﬁcantly larger than alternatives both in terms of the number of categories and objects.
We exploit this new dataset to conduct one of the ﬁrst large-scale “in-the-wild” evaluations of several new-view-synthesis and category-centric 3D reconstruction methods.
Finally, we contribute NerFormer - a novel neural render-ing method that leverages the powerful Transformer to re-construct an object given a small number of its views. 1.

Introduction
Recently, the community witnessed numerous advances in deeply learning to reconstruct category-centric 3D mod-els. While a large variety of technical approaches was pro-posed [56, 62, 13, 25, 26, 46, 8], they are predominantly trained and benchmarked either on synthetic data [6], or on real datasets of speciﬁc object categories such as birds [62] or chairs [37]. The latter is primarily a consequence of a lack of relevant real-world datasets with 3D ground truth.
Our main goal is therefore to collect a large-scale open real-life dataset of common objects in the wild annotated 1
with 3D ground truth. While the latter can be collected with specialized hardware (turn-table 3D scanner, dome [29]), it is challenging to reach the scale of synthetic datasets [6] comprising thousands of instances of diverse categories.
Instead, we devise a photogrammetric approach only re-quiring object-centric multi-view RGB images. Such data can be effectively gathered in huge quantities by means of crowd-sourcing “turn-table” videos captured with smart-phones, which are nowadays a commonly owned accessory.
The mature Structure-from-Motion (SfM) framework then provides 3D annotations by tracking cameras and recon-structs a dense 3D point cloud capturing the object surface.
To this end, we collected almost 19,000 videos of 50
MS-COCO categories with 1.5 million frames, each an-notated with camera pose, where 20% of the videos are annotated with a semi-manually veriﬁed high-resolution 3D point cloud. As such, the dataset exceeds alternatives
[9, 1, 21] in terms of number of categories and objects.
Our work is an extension of the dataset from [26]. Here, we signiﬁcantly increase the dataset size from less than 10 categories to 50 and, more importantly, conduct a human-in-the-loop check ensuring reliable accuracy of all cameras.
Finally, the dataset from [26] did not contain any point cloud annotations, the examples of which are in ﬁg. 1.
We also propose a novel NerFormer model that, given a small number of input source views, learns to reconstruct object categories in our dataset. NerFormer mates two of the main workhorses of machine learning and 3D computer vision: Transformers [65] and neural implicit rendering
[43]. Speciﬁcally, given a set of 3D points along a rendering ray, features are sampled from known images and stacked into a tensor. The latter is in fact a ray-depth-ordered se-quence of sets of sampled features which admits processing with a sequence-to-sequence Transformer. Therefore, by means of alternating feature pooling attention and ray-wise attention layers, NerFormer learns to jointly aggregate fea-tures from the source views and raymarch over them.
Importantly, NerFormer outperforms a total of 14 base-lines which leverage the most common shape representa-tions to date. As such, our paper conducts one of the
ﬁrst truly large-scale evaluations of learning 3D object cat-egories in the wild. 2.