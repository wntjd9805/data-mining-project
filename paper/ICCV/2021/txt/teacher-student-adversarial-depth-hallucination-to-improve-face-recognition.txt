Abstract
We present the Teacher-Student Generative Adversarial
Network (TS-GAN) to generate depth images from single
RGB images in order to boost the performance of face recognition systems. For our method to generalize well across unseen datasets, we design two components in the architecture, a teacher and a student. The teacher, which itself consists of a generator and a discriminator, learns a latent mapping between input RGB and paired depth im-ages in a supervised fashion. The student, which consists of two generators (one shared with the teacher) and a discrim-inator, learns from new RGB data with no available paired depth information, for improved generalization. The fully trained shared generator can then be used in runtime to hal-lucinate depth from RGB for downstream applications such as face recognition. We perform rigorous experiments to show the superiority of TS-GAN over other methods in gen-erating synthetic depth images. Moreover, face recognition experiments demonstrate that our hallucinated depth along with the input RGB images boost performance across vari-ous architectures when compared to a single RGB modality by average values of +1.2%, +2.6%, and +2.6% for IIIT-D, EURECOM, and LFW datasets respectively. We make our implementation public at: https://github.com/hardik-uppal/teacher-student-gan.git. 1.

Introduction
Facial recognition is an active research area, which has recently witnessed considerable progress thanks primar-ily to the effectiveness of deep neural networks such as
AlexNet [23], VGG [38], FaceNet [34], ResNet [12] and others. RGB-based face recognition methods tend to be generally sensitive to facial and environmental variations like illumination, occlusions, and poses [35, 48, 1, 29]. Uti-lizing the depth information, acquired with an RGB-D sen-sor such as the Microsoft Kinect or Intel Realsense, along-side RGB allows models to learn more robust face represen-tations. This is because depth provides complementary geo-metric information about the intrinsic shape of the face, fur-ther boosting recognition performance. Additionally, RGB-Figure 1. The proposed framework for our method. The ﬁrst step (blue) trains the generator for synthesizing depth from RGB im-ages, while the second step (orange) tests the efﬁcacy of the syn-thesized depth images by using it in face recognition pipelines.
D facial recognition methods are known to be less sensitive than pure RGB approaches to pose and illumination varia-tions [41, 3, 11, 42]. Despite these advantages, while RGB sensors are ubiquitous, depth sensors have been less preva-lent, resulting in an over-reliance on RGB alone. To tackle this, we present a method that uses available paired RGB-D training data to learn to hallucinate (i.e. generate synthetic) depth images, even for datasets for which corresponding ground-truth depth information is absent.
Generative Adversarial Networks (GANs) [8] and its variants (e.g., cGan [31], pix2pix [17], CycleGan [50],
StackGAN [47], StyleGAN [20], etc.) have proven to be viable solutions for data synthesis in many application do-mains.
In the context of facial images, GANs have been widely used to generate very high-quality RGB images when trained on large-scale datasets such as FFHQ [20, 21] and CelebA-HQ [19]. Nonetheless, only a lim-ited number of past works have attempted to synthesize depth from corresponding RGB images using a conditional
GAN [33], CycleGAN [24], and a Fully Convolutional
Network (FCN) [4]. Although cGAN has achieved im-pressive results for depth synthesis using paired RGB-D sets [33], it does not easily generalize to new test examples for which paired samples are not available, especially when the images are from an entirely different dataset with dras-tically different poses, expressions, and occlusions. Cycle-GAN [50] attempts to overcome this shortcoming through unpaired training with the aim of generalizing well to new test examples. However, as stated in [50], CycleGAN does not deal well with translating geometric shapes and features.
In this work, we propose a deep architecture using a
novel Teacher-Student GAN (TS-GAN) to generate depth images from RGB images for which no corresponding depth information is available. Our end-to-end model con-sists of two components, a teacher and a student. The teacher consists of a fully convolutional encoder-decoder network as a generator along with a fully convolutional clas-siﬁcation network as the discriminator. The generator takes
RGB images as inputs and aims to output the correspond-ing depth images.
In essence, our teacher aims to learn an initial latent mapping between RGB and co-registered depth images. The student consists of two generators in the form of encoder-decoders, one of which is shared with the teacher, along with a fully convolutional discriminator. The student takes as its input an RGB image for which the corre-sponding depth image is not available and maps it onto the depth domain as guided by the teacher. The purpose here is for the student to further reﬁne the strict mapping learned by the teacher and allow for better generalization through a less constrained training scheme. We demonstrate the high quality of our hallucinated depth images by compar-ing them to ground truth depth and several state-of-the-art depth generation alternatives. The performance of our ap-proach for using the generated depth in facial recognition is then validated for two RGB-D datasets, IIIT-D RGB-D and
EURECOM KinectFaceDb, across various facial recogni-tion networks. The results show that the depth images gen-erated using our approach enable a performance as good as, or in some cases surprisingly even better than using the ground-truth depth originally available in the dataset, and that it gives a signiﬁcant boost to recognition accuracy as compared to a pure RGB facial recognition system. We also evaluate the performance of our approach for an in-the-wild RGB dataset, Labeled-Faces-in-Wild (LFW), where no depth information is originally available, and show that the addition of hallucinated depth by our proposed method can considerably boost the recognition results by +2.4% with
SE-ResNet-50 architecture.
Our contributions are summarized as follows. (1) A novel teacher-student adversarial architecture is proposed to generate realistic depth images from a single RGB image.
Our method uses a student architecture to reﬁne the strict la-tent mapping between RGB and D domains learned by the teacher to obtain a more generalizable and less constrained (2) Our assessments reveal that our method relationship. creates realistic synthetic depth images as compared to the original co-registered depth images (where available) and other techniques. We then utilize the synthetic depth for RGB-D facial recognition and show that multimodal solutions that utilize the depth images produced by our method perform as good as using the ground-truth depths.
We also show that the facial recognition performance in-creases when utilizing our method to generate depth for an
RGB-only dataset and subsequently combining the gener-ated depth and original RGB images in a multimodal net-work. (3) We make our implementation publicly available1 to enable reproducibility and future comparisons. 2.