Abstract
Current 3D single object tracking approaches track the target based on a feature comparison between the target template and the search area. However, due to the com-mon occlusion in LiDAR scans, it is non-trivial to conduct accurate feature comparisons on severe sparse and incom-plete shapes.
In this work, we exploit the ground truth bounding box given in the first frame as a strong cue to en-hance the feature description of the target object, enabling a more accurate feature comparison in a simple yet effec-tive way. In particular, we first propose the BoxCloud, an informative and robust representation, to depict an object using the point-to-box relation. We further design an effi-cient box-aware feature fusion module, which leverages the aforementioned BoxCloud for reliable feature matching and embedding. Integrating the proposed general components into an existing model P2B [27], we construct a superior box-aware tracker (BAT)1. Experiments confirm that our proposed BAT outperforms the previous state-of-the-art by a large margin on both KITTI and NuScenes benchmarks, achieving a 12.8% improvement in terms of precision while running ∼20% faster. 1.

Introduction
Single object tracking (SOT) in 3D scene has a broad spectrum of practical applications, such as autonomous driving [20], semantic understanding [38, 37] and assistive robotics [21, 7]. Given a 3D bounding box (BBox) of an object as the template in the first frame, the SOT task is to keep track of this object across all frames. In real scenes,
LiDAR becomes a popular 3D sensor due to its precise mea-surement, reasonable cost and insensitivity to ambient light variations. In this paper, we focus on SOT on LiDAR data, which can be viewed as 3D point clouds in general.
Due to the moving environment and self-occlusion, point
* Corresponding author: Zhen Li. † Equal first authorship. 1 https://github.com/Ghostish/BAT
Figure 1. Motivation behind BAT. Partial scans of two different cars (i.e. c1, c2) collected by a LiDAR sensor both have highly similar shapes with the target template (1st row), though their 3D
BBoxes are quite different in size (2nd row). With explicit consid-eration of object bounding-boxes, our method can elegantly over-come such ambiguities. clouds generated by a LiDAR system are inevitably irregu-lar and incomplete, making the SOT task very challenging.
In 3D SOT, feature comparison plays an important role. The general idea to locate the target object is based on measur-ing the feature similarity between some candidate regions and the object template (initialized as the point cloud in the first given BBox). For example, SC3D [13] uses the ex-haustive search or Kalman Filter to generate a set of candi-date shapes at the current frame, and comparing them to the template using a siamese network. The candidate with the maximum similarity is chosen to be the target object for the frame. Inspired by the success of the siamese region pro-posal network (RPN) [17] in 2D SOT, P2B [27] proposes a point-based correlation based on the pair-wise feature com-parison. P2B executes such a correlation between the tem-plate and the search area to output the target-specific search features, on which a 3D RPN is applied to obtain the final target proposals. However, the features used for compar-ison are extracted from pure LiDAR point clouds, which face the following defects: 1) They do not encode the size information of objects. Since objects in LiDAR scans are mostly incomplete, it is hard to infer an object’s size only from the partial point cloud. 2) They cannot capture the explicit part-aware structure information within each object
BBox, e.g. some part belongs to the car front while others belong to the sunroof as Figure 1 shows. Therefore, the feature comparison among such features may bring consid-erable ambiguities which weaken the tracking performance.
What is a good representation for feature matching under 3D SOT? We revisit this problem by pointing out that the size and the part priors of the target object can be directly inferred from the template BBox given at the first frame.
Based on this observation, we propose to address the above issues by explicitly utilizing the BBox to enhance the ob-ject features. Thus, we propose the BoxCloud, a robust and informative object representation depicting the point-to-box relation. Instead of using the xyz coordinate, it represents an object point via a canonical box coordinate, where the i-th dimension corresponds to the Euclidean distance be-tween the object point and its i-th box point (i.e. the cor-ner or center of a BBox). Unlike the original LiDAR point cloud, a BoxCloud is defined based on both the object and its BBox. Therefore, it naturally encodes the size and part information of an object. Note that we can directly compute the BoxCloud for a target template using the BBox given at the first frame. After the supervised training using ground-truth object BBoxes, the BoxClouds of objects in the search area can be easily predicted for the inference usage.
Based on the BoxCloud representation, the box-aware feature fusion (BAFF) module is further proposed to per-form a correlation between the template and the search area to generate the target-specific search area features. It first measures the similarity between the search area and the template according to their BoxClouds. After such effective feature comparison, the BAFF module aggregates the top-k similar template points into each corresponding searching point, yielding a high-quality target-specific search area. Fi-nally, we construct a Box-Aware Tracker (BAT) by integrat-ing the two proposed components into P2B [27]. By taking the auxiliary 3D BBox as input, BAT captures more shape constraints and part-aware information, no matter whether the input shape is partial or not, enabling effective and ro-bust tracking on LiDAR point clouds.
Our main contributions can be summarized as follows:
• To the best of our knowledge, we are the first to use free box information to boost the performance on the 3D SOT task. Specifically, we improve the feature comparison by designing a size-aware and part-aware
BoxCloud feature, which is not only interpretable but also robust to sparseness and incompleteness.
• We propose a dedicated box-aware feature fusion mod-ule to generate better target-specific search areas in a box-aware manner.
• Experiments verify that our BAT achieves significant improvement over the state-of-the-arts on two bench-marks (i.e. KITTI and NuScenes), especially on ex-tremely sparse data. 2.