Abstract
The paper proposes a Dynamic ResBlock Generative Ad-versarial Network (DRB-GAN) for artistic style transfer.
The style code is modeled as the shared parameters for Dy-namic ResBlocks connecting both the style encoding net-work and the style transfer network.
In the style encod-ing network, a style class-aware attention mechanism is used to attend the style feature representation for gener-ating the style codes. In the style transfer network, multi-ple Dynamic ResBlocks are designed to integrate the style code and the extracted CNN semantic feature and then feed into the spatial window Layer-Instance Normalization (SW-LIN) decoder, which enables high-quality synthetic images with artistic style transfer. Moreover, the style collection conditional discriminator is designed to equip our DRB-GAN model with abilities for both arbitrary style trans-fer and collection style transfer during the training stage.
No matter for arbitrary style transfer or collection style transfer, extensive experiments strongly demonstrate that our proposed DRB-GAN outperforms state-of-the-art meth-ods and exhibits its superior performance in terms of vi-sual quality and efﬁciency. Our source code is available at https://github.com/xuwenju123/DRB-GAN . 1.

Introduction
Artistic style transfer is to synthesize an image sharing structure similarity of the content image and reﬂecting the style of the artistic style. Here, artistic style implies the genre of paintings by the artist, and the artistic images re-fer to a set of images created by the same artist, and each image has a unique character. As shown in Figure 1, style image 1 and all the style images in style collection 1 are
*This work was supervised by Chengjiang Long and Guanghui Wang.
Figure 1. Examples of two types of artistic style transfer: (a) ar-bitrary style transfer and (b) collection style transfer. Note that the style image 1 and style collection 1 are from the artist Pablo
Picasso, and the style image 2 and style collection 2 are from the artist Ukiyo-e. Our proposed DRB-GAN experimentally performs well on both arbitrary style transfer and collection style transfer. created by Pablo Picasso, in which the style includes color, brushstroke, form, or use of light. Therefore, an ideal artis-tic style transfer should be able to synthesize images with consistent style genre and also take the diverse artworks of the artist into account.
To facilitate more efﬁcient artistic style transfer, some prior works have explored arbitrary style transfer [17, 20], which heavily rely on only one arbitrary style image.
Hence, they are not effective to produce a bunch of re-sults that reﬂect the understanding of artistic style charac-terized by color, scale, and stroke size of the artistic work set. Some recent efforts on generative adversarial networks (GANs) [55, 41, 55, 41, 5, 36] have succeeded in collec-tion style transfer, which considers each style image in a style collection as a domain. However, the existing collec-tion style transfer methods only recognize and transfer the domain dominant style clues and thus lack the ﬂexibility of exploring style manifold.
In this paper, we propose a Dynamic ResBlock Gen-erative Adversarial Network (DRB-GAN) for artistic style transfer. As illustrated in Figure 2, it consists of a style encoding network, a style transfer network, and a style col-In particular, inspired by lection discriminative network. the ideas of DIN [20] and StyleGAN [23], we model the
“style code” as the shared parameters for Dynamic Convo-lutions and AdaINs in dynamic ResBlocks, and design mul-tiple Dynamic Residual Blocks (DRBs) at the bottleneck in the style transfer network. Note that each DRB consists of a Convolution Layer, a Dynamic Convolution [3] layer, a ReLU layer, an AdaIN [17] layer, and an instance nor-malization layer with a residual connection. Such treatment is to attentively adjust the shared parameters for Dynamic
Convolutions and adaptively adjust afﬁne parameters for
AdaINs to ensure the statistic matching in bottleneck fea-ture spaces between content images and style images.
We incorporate a ﬁxed pretrained VGG encoder [43] and a learnable encoder as feature extractor in the style encoding network to capture style class-aware feature representation.
The output style class-aware probabilities can be used as at-tention weights to attend to the style features for style code recalibration. As the style class-aware attention mechanism is learned via images in style collections, the output “style code” captures the underlying discriminative information in the style image collections. Note that our attention mecha-nism enforces a better clustering of the “style codes”, which is different from previous class activation mapping based methods [25, 53] that aim at highlighting spatial regions.
With the “style code” from the style encoding network, multiple DRBs can adaptively proceed the semantic fea-tures extracted from the CNN encoder in the style trans-fer network then feed them into the spatial window Layer-Instance Normalization (SW-LIN) decoder to generate syn-thetic images. Specially, we borrow the idea of local fea-ture normalization from [28] and design an SW-LIN func-tion that dynamically combines the local channel-wise and layer-wise normalization with a learnable parameter in each decoder block. With the spatial window constraint, our SW-LIN is able to ﬂexibly shift the mean and variance in the feature spaces. As a consequence, our SW-LIN decoder can avoid the possible artifacts and retain the capability to syn-thesize high-resolution stylization.
As the “style code” captures the underlying discrimina-tive information in the style encoding network, it is easy to apply the learned style network on each style image in a style collection and get a set of style codes. We can apply the weighted average on the style codes to obtain the “col-lection style code” and then feed it into the style transfer network to conduct the collection style transfer. Moreover, our discriminative network takes several style images sam-pled from the target style collection of the same artist as ref-erences to ensure consistency in the feature space. Together with the perception supervision, our well-designed discrim-inator provides good guidance for our DRB-GAN to own abilities for both arbitrary style transfer and collection style transfer gradually, shrinking their gap at the training stage.
With extensive experiments, we have demonstrated the ef-fectiveness of our proposed DRB-GAN on both arbitrary style transfer and collection style transfer.
Several aspects distinguish our work from previous style transfer models [41, 28, 44]. First of all, our DRB-GAN introduces a novel prototype for artistic style transfer, in which “style code” is modeled as the shared parameters for Dynamic ResBlocks, connecting both the style encoding network and the style transfer network, to shrink the gap be-tween arbitrary style transfer and collection style transfer in a uniﬁed model. Second, we introduce a style class-aware attention mechanism for style code recalibration and then employ well-designed multiple dynamic ResBlocks to in-tegrate the style code and the extracted semantic feature to realize artistic style transfer when generating high-quality synthetic images. Last but not least, the discriminative net-work makes full use of style images sampled from the target collection as a reference which enforces our DRB-GAN’s ability for collection style transfer. Together with percep-tion supervision, the ability for arbitrary style transfer can be well preserved and improved at the training stage.
Both quantitative and qualitative experiments demon-strate the effectiveness and efﬁciency of the proposed DRB-GAN, as well as its superior performance in artistic style transfer, regardless of arbitrary or collection style transfer. 2.