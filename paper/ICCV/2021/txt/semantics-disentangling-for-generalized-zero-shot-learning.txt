Abstract
Generalized zero-shot learning (GZSL) aims to classify samples under the assumption that some classes are not ob-servable during training. To bridge the gap between the seen and unseen classes, most GZSL methods attempt to as-sociate the visual features of seen classes with attributes or to generate unseen samples directly. Nevertheless, the vi-sual features used in the prior approaches do not necessar-ily encode semantically related information that the shared attributes refer to, which degrades the model generaliza-tion to unseen classes. To address this issue, in this paper, we propose a novel semantics disentangling framework for the generalized zero-shot learning task (SDGZSL), where the visual features of unseen classes are ﬁrstly estimated by a conditional VAE and then factorized into semantic-consistent and semantic-unrelated latent vectors. In partic-ular, a total correlation penalty is applied to guarantee the independence between the two factorized representations, and the semantic consistency of which is measured by the derived relation network. Extensive experiments conducted on four GZSL benchmark datasets have evidenced that the semantic-consistent features disentangled by the pro-posed SDGZSL are more generalizable in tasks of canon-ical and generalized zero-shot learning. Our source code is available at https://github.com/uqzhichen/
SDGZSL. 1.

Introduction
Human beings have a remarkable ability to learn new no-tions based on prior experience without seeing them in ad-vance. For example, given the clues that zebras appear like horses yet with black-and-white stripes, one can quickly recognize a zebra if he/she has seen horses before. Nev-ertheless, unlike humans, supervised machine learning al-gorithms can only classify samples belonging to the classes that have already appeared during the training phase, and they are not able to handle samples from previously unseen
Semantic-unrelated visual features, e.g., ear shape
Annotated attributes
Stripes 
Big 
Tail 
Muscle 
Grazer 
Arctic 
Tree 
Domestic 
Fields 
Bush 
Hunter 
Stalker 
Forager 
Nocturnal 
Tusks 
Strong 
Ocean 
Vegetation
Longleg 
Furry 
Strainteeth
Smelly
Walks
Fast
Slow
Bipedal
Meat
Active
Pads
Paws
Longneck
Hooves
Figure 1: An illustration of the visual features (red boxes) that are not associated with the annotated attributes. Learn-ing from such visual features that are semantically unrelated may jeopardize the model generalization to unseen classes. categories. This challenge motivates the study of generaliz-ing models to the unseen classes by transferring knowledge from intermediate semantics (e.g., attributes), which typi-cally refers to zero-shot learning (ZSL). intro
Particularly, the core idea of ZSL [19, 34, 1, 17] lies in learning to map features between the semantic space and vi-sual space, thereby closing the gap between the seen and un-seen classes. While effective, conventional ZSL techniques are built upon the assumption that the test set only contains samples from the unseen classes, which can be easily vio-lated in practice. Hence, it is more reasonable to consider a new protocol called generalized zero-shot learning (GZSL), where seen and unseen images are both to be identiﬁed.
Existing GZSL techniques can be roughly grouped into two types: embedding-based [9, 34, 22, 21, 13] and generative-based [38, 35, 28, 24, 15] approaches. The for-mer group learns a projection or an embedding function to associate the visual features of seen classes with the respective semantic vectors, while the latter one learns a visual generator for the unseen classes based on the seen points and semantic representations of both classes. How-ever, most GZSL approaches directly leverage the visual features extracted from the pre-trained deep models, such
as ResNet101 [11] pre-trained on ImageNet, which are not tailored for ZSL tasks. In [31], it is observed that not all the dimensions of the extracted visual features are semantically related to the pre-deﬁned attributes, which triggers the bias on learning semantic-visual alignment and causes negative transfer to unseen classes. Given an example from the AWA dataset shown in Figure 1, despite the features of animals’ ears are visually salient for discriminating image samples, it is ignored in the manually annotated attributes. When gen-eralizing to unseen classes such as cats, it is easy for them to be misclassiﬁed as tigers because the visual features cor-responding to the concepts “Big, Strong, Muscle” are not highlighted. From this case, we believe that GZSL will ben-eﬁt from using the visual features that can consistently align with the respective semantic attributes. We deﬁne this type of visual features as the semantic-consistent features, which are agnostic to both seen and unseen classes. In contrast, those visual features that are irrelevant to manually anno-tated attributes are deﬁned as semantic-unrelated.
To unravel semantic-consistent and semantic-unrelated features from the original visual spaces, we present a novel framework, namely Semantics Disentangling for General-ized Zero-Shot Learning (SDGZSL), as shown in Figure 2.
Speciﬁcally, we disentangle the underlying information of the extracted visual features into two disjoint latent vec-tors hs and hn. They are learned in an encoder-decoder architecture with a relation module and a total correlation penalty. The encoder network projects the original visual features to hs and hn. To make hs consistent with the se-mantic embeddings, the relation module calculates a com-patibility score between hs and semantic information to guide the learning of hs. We further apply the total cor-relation penalty to enforce the independence between hs and hn. Afterward, we reconstruct the original visual fea-tures ¯x from the two latent representations. This recon-struction objective ensures the two latent representations to cover both semantic-consistent and semantic-unrelated in-formation. The disentangling modules are incorporated into a conditional variational autoencoder and trained in an end-to-end manner. The proposed framework is evaluated on various GZSL benchmarks and achieves better performance compared to the state-of-the-art methods. The main contri-butions of this work are summarized as follows:
• We propose a novel feature disentangling framework, namely Semantic Disentangling for Generalized Zero-Shot Learning (SDGZSL), to disentangle the underly-ing information of visual features into two latent repre-sentations that are semantic-consistent and semantic-unrelated,
Exploiting the semantic-consistent representations can substantially increase the performance in GZSL comparing to directly using entangled visual features that are extracted from the pre-trained CNN models. respectively.
• To facilitate the feature disentanglement of the semantic-consistent and semantic-unrelated represen-tations, by introducing a total correlation penalty in our framework we arrive at a more accurate characteriza-tion of the semantically annotated features.
• Extensive experiments conducted on four benchmark datasets evidence that the proposed method performs better than the state-of-the-art methods. 2.