Abstract
Deep convolutional neural networks (CNNs) are achiev-ing great successes for image super-resolution (SR), where global context is crucial for accurate restoration. However, the basic convolutional layer in CNNs is designed to extract local patterns, lacking the ability to model global context.
With global context information, lots of efforts have been devoted to augmenting SR networks, especially by global feature interaction methods. These works incorporate the global context into local feature representation. However, recent advances in neuroscience show that it is necessary for the neurons to dynamically modulate their functions ac-cording to context, which is neglected in most CNN based
SR methods. Motivated by those observations and analyses, we propose context reasoning attention network (CRAN) to modulate the convolution kernel according to the global context adaptively. Speciﬁcally, we extract global context descriptors, which are further enhanced with semantic rea-soning. Channel and spatial interactions are then intro-duced to generate context reasoning attention mask, which is applied to modify the convolution kernel adaptively. Such a modulated convolution layer is utilized as basic compo-nent to build the blocks and networks. Extensive exper-iments on benchmark datasets with multiple degradation models show that CRAN obtains superior results and favor-able trade-off between performance and model complexity. 1.

Introduction
Image super-resolution (SR) aims to reconstruct an ac-curate high-resolution (HR) image given its low-resolution (LR) counterpart [14]. Image SR plays a fundamental role in various computer vision applications, ranging from secu-rity and surveillance imaging [71], medical imaging [48], to object recognition [45]. However, image SR is an ill-posed problem, since there exists multiple solutions for any LR input. To tackle such an inverse problem, lots of deep con-volutional neural networks (CNNs) have been proposed to learn mappings between LR and HR image pairs.
Deep CNNs have achieved remarkable successes for im-In age SR [10, 12, 26, 36, 62, 18, 66, 1, 23, 31, 67].
∗Corresponding author: Huan Wang (wang.huan@northeastern.edu)
HR
Bicubic
SAN [9]
Urban100: img 098
CSNLN [41]
RFANet [38]
CRAN (ours)
Figure 1. Visual examples for 4× SR with Bicubic (BI) degra-dation on Urban100 [22]. SAN, CSNLN, and RFANet recover parts of local textures. Global context guided convolution enables
CRAN to recover more structural textures with proper directions.
CNNs, convolution extracts local patches by a sliding win-dow, making it only capable of capturing local patterns.
However, recent advances in neuroscience reveal that neu-rons’ awareness of global context is essential for us to pro-cess complex perceptual tasks effectively [34, 15]. The slid-ing window mechanism in convolution limits its ability to utilize global context, being crucial for accurate image SR.
To alleviate this limitation, many SR methods have been recently proposed to introduce global context modeling modules into SR networks [64, 9, 38, 65, 41]. Zhang et al. proposed residual channel attention network [64], where the global context was modelled with global average pool-ing and used to rescale each feature channel. Dai et al. pro-posed second-order channel attention by considering higher order feature statistics in SAN [9]. Different from channel attention, Liu et al. proposed an enhanced spatial attention block in FRANet [38] to make the residual features be more focused on critical spatial contents.
Zhang et al. further proposed residual non-local atten-tion network [65] to rescale hierarchical features with mixed channel and spatial attentions adaptively. Such a non-local attention mechanism was further developed in cross-scale non-local attention (CSNLN) [41]. Mei et al. proposed a self-exemplar mining cell to exhaustively mine all the pos-sible intrinsic priors by combining local and in-scale/cross-scale non-local feature correlations in CSNLN [41]. As shown in Figure 1, SAN, RFANet, and CSNLN could re-cover some kind of local textures. But, it seems that the directions of those textures are not faithful to the ground truth. This is mainly because these methods mainly incor-porate the global context into the local features.
However, as investigated in neuroscience [15], the func-tion of neurons should be adaptively changed according to the behavioral context. Therefore, we can dynamically modify the convolution kernels based on context informa-tion [37].
Image SR has not witnessed works exploiting such a modulation mechanism, which was tentatively in-vestigated in other computer vision applications. Zhu et al. proposed to adaptively set the offset of each element in a convolution kernel and the gather value for each element in the local feature patch [70]. However, such an operation only changes the input features fed into the convolutional (Conv) layer. Wu et al. proposed to generate the convolu-tion kernel weights dynamically by taking local segments as inputs only [55]. Similar works in [24, 25] extracted fea-tures from the input image with another network and then generated convolution kernel weights. The feature extrac-tion process could be time-consuming, making it impracti-cal for very deep CNNs in image SR. Lin et al. proposed context-gated convolution to introduce context-awareness to Conv layers [37]. However, most of them neglected to mine the relationship among context information, which could also be important for high-quality image SR.
Motivated by the observations and analyses above, we propose a context reasoning attention network (CRAN) for image SR. This is the ﬁrst attempt in image SR to modulate the convolution kernel according to the global context adap-tively to the best of our knowledge (see Figure 2). Specif-ically, we project the input feature into latent representa-tions and extract global context descriptors. The context relationship descriptors are further enhanced by using the descriptor relationship with semantic reasoning. Channel and spatial interactions [37] are then introduced to generate context reasoning attention mask, which is applied to mod-ify the convolution kernel adaptively. We use the modulated convolution layer as a basic component to build blocks and the whole networks. Consequently, our CRAN can achieve much superior SR results (e.g., in Figure 1) against recent leading methods and favourable efﬁciency trade-off.
In summary, the main contributions of this work can be concluded in three parts:
• We propose a context reasoning attention network for accurate image SR. Our CRAN can adaptively modu-late the convolution kernel according to the global con-text enhanced by semantic reasoning. CRAN achieves superior SR results quantitatively and visually.
• We propose to extract context information into latent representations, resulting in a bag of global context de-scriptors. We further enhance the descriptors by using their relationship with semantic reasoning.
• We introduce channel and spatial interactions to gen-erate context reasoning attention mask used to modify convolution kernel. We ﬁnally obtain the context rea-soning attention convolution, which further serves as a base to build blocks and networks for image SR.
*
W (a) Previous Conv
*
Context
W
Guidance (b) Context Guided Conv
*W
Figure 2. Conv layers. Motivated by [37], we modify Conv kernel
W as W ∗ with context. (cid:2) denotes Conv operation. 2.