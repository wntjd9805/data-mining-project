Abstract
In recent years, sparse voxel-based methods have be-come the state-of-the-arts for 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs. Never-theless, being oblivious to the underlying geometry, voxel-based methods suffer from ambiguous features on spatially close objects and struggle with handling complex and ir-regular geometries due to the lack of geodesic information.
In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D deep architecture that operates on the voxel and mesh representations leveraging both the Euclidean and geodesic information.
Intuitively, the Euclidean in-formation extracted from voxels can offer contextual cues representing interactions between nearby objects, while the geodesic information extracted from meshes can help sepa-rate objects that are spatially close but have disconnected surfaces. To incorporate such information from the two domains, we design an intra-domain attentive module for effective feature aggregation and an inter-domain atten-tive module for adaptive feature fusion. Experimental re-sults validate the effectiveness of VMNet: specifically, on the challenging ScanNet dataset for large-scale segmen-tation of indoor scenes, it outperforms the state-of-the-art
SparseConvNet and MinkowskiNet (74.6% vs 72.5% and 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M parameters). Code release: https:
//github.com/hzykent/VMNet 1.

Introduction
Thanks to the tremendous progress of RGB-D scanning methods in recent years [63, 27, 10], reliable tracking and reconstruction of 3D surfaces using hand-held, consumer-grade devices have become possible. Using these methods, large-scale 3D datasets with reconstructed surfaces and se-mantic annotations are now available [8, 4]. Nevertheless, compared to 3D surface reconstruction, 3D scene under-standing, i.e., understanding the semantics of reconstructed scenes, is still a relatively open research problem.
*intern at Tencent Lightspeed & Quantum Studios
†corresponding author
Figure 1. Illustration of geodesic information loss caused by voxelization. Considering the green point at the arm of a chair, on an input 3D mesh surface (Left), its geodesic neighbors (blue) can be easily collected, and the points of different objects are nat-urally separated. After voxelization (Right), geodesic information is discarded and only Euclidean neighbors (red) that are agnostic to the underlying surface can be extracted. The scan section is taken from the ScanNet dataset [8].
Inspired by the success of 2D CNN in image semantic segmentation [5, 36], researchers have paid much attention to the straightforward extension of this idea to 3D, by per-forming volumetric convolution on regular grids [39, 66, 44]. Specifically, surface reconstructions are first projected to a discrete 3D grid representation, and then 3D convolu-tional filters are applied to extract features by sliding kernels over neighboring grid voxels [54, 62, 72]. Such features can be smoothly propagated in the Euclidean domain to accumulate strong contextual information. Unfortunately, dense voxel-based methods require intensive computational power and are thus limited to low-resolution cases [35]. To process large-scale data, sparse voxel convolutions [17, 7] have been proposed to lower the computational require-ment by ignoring inactive voxels. Benefiting from the ef-ficient sparse voxel convolutions, complex networks have been built, achieving leading results on several 3D seman-tic segmentation benchmarks [8, 4] and outperforming other methods by large margins.
Despite the remarkable achievements, voxel-based methods are not perfect. One of their major limitations is the geodesic information loss caused by the voxeliza-tion process (see Fig. 1). Recent public datasets like Scan-Net [8] provide 3D scene reconstructions in the form of high-quality triangular meshes, in which the surface infor-mation is naturally encoded. On these meshes, vertices be-longing to different objects are well separated, and geodesic
Figure 2. Limitations of voxel-based methods. (Upper) Some points of “chairs” are mistakenly classified into nearby classes in the
Euclidean space by SparseConvNet [17] since the convolutional filters produce ambiguous features for spatially close objects. (Lower) On areas with complex and irregular geometries (e.g., the base parts of “tables”), SparseConvNet fails to predict correct results due to the lack of geodesic information about shape surfaces. features can be easily aggregated through edge connectiv-ities. However, the voxelization process omits all mesh edges and only retains Euclidean positions of mesh ver-tices. Consequently, convolutional filters operating on vox-els are agnostic to the underlying surfaces and, therefore, result in two problems. First, these filters generate similar features for voxels that are close in the Euclidean domain, even though these voxels may belong to different objects and are distant in the geodesic domain. As shown in the top example of Fig. 2, these ambiguous features produce sub-optimal predictions for objects that are spatially close. Sec-ond, without the geodesic information about shape surfaces, these Euclidean convolutions may struggle with learning specific object shapes. As shown in the lower example of
Fig. 2, this property is problematic for segmentation on ar-eas with complex and irregular geometries.
We have discussed the advantages of voxel-based meth-ods on contextual learning and their problems on geodesic information loss.
It is appealing to design a method re-solving the problems while retaining these advantages by leveraging both the Euclidean and geodesic information. A possible solution is to take voxels and the original meshes as the sources for the Euclidean and geodesic information, respectively.
It is therefore natural to ask how these two representations can be combined in a common architecture.
To address this question, we propose the Voxel-Mesh network (VMNet), a novel deep hierarchical architecture for geodesic-aware 3D semantic segmentation. Starting from a mesh representation, to extract informative contextual fea-tures in the Euclidean domain, we first voxelize the input mesh and apply sparse voxel convolutions. Next, to in-corporate the geodesic information, the extracted contex-tual features are projected from the Euclidean domain to the geodesic domain, specifically, from voxels to mesh vertices.
These projected features are further fused and aggregated to combine both the Euclidean and geodesic information.
In order to build such a deep architecture that is ca-pable of effectively learning useful features incorporating information from the two domains, it is critical to design proper ways to aggregate intra-domain features and to fuse inter-domain features. In view of the great success of self-attention operators for feature processing [59, 41, 34], we therefore present two key components of VMNet: Intra-domain Attentive Aggregation Module and Inter-domain
Attentive Fusion Module. The former aims to aggregate the projected features on the original meshes to incorporate the geodesic information and the latter focuses on the effective fusion of features from the two domains.
We conduct extensive experiments to demonstrate the ef-fectiveness of our method on the popular ScanNet v2 bench-mark [8] and the recent Matterport3D benchmark [4]. VM-Net outperforms existing sparse voxel-based methods Spar-seConvNet [17] and MinkowskiNet [7] (74.6% vs 72.5% and 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M parameters) on the ScanNet dataset and sets a new state-of-the-art on the Matterport3D dataset.
To summarize, our contributions are threefold: 1. We propose a novel deep architecture, VMNet, which operates on the voxel and mesh representations, lever-aging both the Euclidean and geodesic information. 2. We propose an intra-domain attentive aggregation module, which effectively refines geodesic features through edge connectivities. 3. We propose an inter-domain attentive fusion module, which adaptively combines Euclidean and geodesic features. 2.