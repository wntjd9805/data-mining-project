Abstract
In this paper, we focus on recognizing 3D shapes from arbitrary views, i.e., arbitrary numbers and positions of viewpoints. It is a challenging and realistic setting for view-based 3D shape recognition. We propose a canonical view representation to tackle this challenge. We ﬁrst transform the original features of arbitrary views to a ﬁxed number of view features, dubbed canonical view representation, by aligning the arbitrary view features to a set of learnable reference view features using optimal transport. In this way, each 3D shape with arbitrary views is represented by a ﬁxed number of canonical view features, which are further ag-gregated to generate a rich and robust 3D shape represen-tation for shape recognition. We also propose a canonical view feature separation constraint to enforce that the view features in canonical view representation can be embedded into scattered points in a Euclidean space. Experiments on the ModelNet40, ScanObjectNN, and RGBD datasets show that our method achieves competitive results under the ﬁxed viewpoint settings, and signiﬁcantly outperforms the appli-cable methods under the arbitrary view setting. 1.

Introduction
Understanding the 3D world is a fundamental problem in computer vision. One of its central challenges is how to represent and recognize objects in the 3D space. Re-cently, many view-based methods [7, 13, 14, 15, 20, 22, 23, 33, 34, 38, 40, 42, 43] were proposed to recognize 3D shape with multi-view 2D images based on the aggregation of features learned by deep neural networks. Leveraging advances in 2D image descriptors (e.g. [18]) and massive image databases [10], they are among the state-of-the-art methods for 3D shape recognition.
However, most of these methods [7, 13, 14, 15, 20, 23, 33, 34, 38, 40, 42, 43] focus on settings with a pre-deﬁned
∗Equal contribution.
Figure 1. This paper addresses 3D shape recognition with arbitrary views as shown in (b), which is more challenging and realistic than the ﬁxed-viewpoint setting in (a). As shown in (c), given an arbitrary number of unaligned view images, our method learns canonical view features of a 3D shape aligned to a ﬁxed number of learnable reference view features using optimal transport. camera setup where the same set of viewpoints are used for every object, e.g., Fig. 1(a). In practical applications, 3D objects are often observed from arbitrary views with-out knowing their precise camera positions. In this work, we aim to tackle 3D shape recognition with arbitrary views.
The setting can be deﬁned as follows. (i) Views are taken from arbitrary viewpoints for each object. (ii) Objects have varying numbers of observation views, e.g., Fig. 1(b).
Compared with the ﬁxed-viewpoint setup, 3D shape recognition faces new challenges brought by the unaligned inputs from arbitrary views. It is difﬁcult to robustly ag-gregate features of structurally unaligned views. Moreover, representations learned from a typical neural network are also mutually-unaligned in the feature space, where feature aggregation could result in a loss of discriminability.
To tackle these challenges, an intuitive motivation is to recover the inherent alignment for the arbitrary views.
Speciﬁcally, if we ﬁnd a link between the unaligned features from arbitrary views and a set of virtual reference views for observing an object, we can transform the features into
aligned representations for the subsequent aggregation.
Driven by this motivation, we design a novel canonical view representation for 3D shape recognition with arbitrary views. Speciﬁcally, the input arbitrary views of each 3D shape are ﬁrst processed by an image-level feature encoder consisting of a CNN and a Transformer encoder [36]. Then these features of arbitrary views are transformed into canon-ical view features aligned to a ﬁxed number of learned ref-erence view features. The transformation mapping is de-rived by the optimal transport [9, 16, 37]. To ensure that the canonical view features are distinct, we require that the canonical view features can be embedded into a Euclidean space (e.g., R3) with mutually distant coordinates. In this way, each 3D shape is represented by a ﬁxed number of features over the reference views in the feature space, re-sulting in the canonical representation of each 3D shape.
The aligned canonical view features added with spatial em-beddings are further encoded and aggregated to generate a discriminative global representation of the 3D shape.
Our main contributions can be summarized as follows.
We tackle the challenge of 3D object recognition with arbi-trary views by introducing a novel canonical view represen-tation, which recovers the inherent mutual-alignment fea-tures among arbitrary views and produces a rich representa-tion of the 3D shape. We further propose a canonical view feature separation loss to ensure feature separability, which improves the discriminability and robustness of the ﬁnal representation. We conduct experiments on CAD, scanned model and real-world image datasets including Model-Net40 [41], ScanObjectNN [35] and RGBD [25] dataset.
The results show that our approach signiﬁcantly outper-forms the state-of-the-art methods under the challenging setting of 3D shape recognition with arbitrary views. 2.