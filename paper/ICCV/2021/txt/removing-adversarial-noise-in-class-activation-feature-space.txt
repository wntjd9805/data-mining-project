Abstract
Deep neural networks (DNNs) are vulnerable to adver-sarial noise. Pre-processing based defenses could largely remove adversarial noise by processing inputs. However, they are typically affected by the error ampliﬁcation effect, especially in the front of continuously evolving attacks. To solve this problem, in this paper, we propose to remove ad-versarial noise by implementing a self-supervised adversar-ial training mechanism in a class activation feature space.
To be speciﬁc, we ﬁrst maximize the disruptions to class activation features of natural examples to craft adversar-ial examples. Then, we train a denoising model to mini-mize the distances between the adversarial examples and the natural examples in the class activation feature space.
Empirical evaluations demonstrate that our method could signiﬁcantly enhance adversarial robustness in comparison to previous state-of-the-art approaches, especially against unseen adversarial attacks and adaptive attacks. 1.

Introduction
Deep neural networks (DNNs) are known to be vulnera-ble to adversarial examples. Adversarial examples are ma-liciously crafted by adding imperceptible but adversarial noise on natural examples [11, 35, 15, 23, 26, 38]. The vulnerability of DNNs poses a potential threat to many decision-critical deep learning applications, such as image processing [21, 14, 47, 33, 17, 27] and natural language processing [34]. Thus, it is important to ﬁnd an effective defense against adversarial noise.
Previous researches show that adversarial robustness of target models could be enhanced by processing inputs with certain transformations [12, 7, 29, 13, 23]. However, pre-processing based defenses may suffer from the error am-pliﬁcation effect, in which small residual adversarial noise
*Corresponding author
Figure 1. A visual illustration of class activation maps of natural examples and adversarial examples. The adversarial examples are crafted by distinct types of non-targeted attacks, e.g., PGD [28],
FWA [39] and AA [6]. Although adversarial noise is imperceptible in pixel level, there exists obvious discrepancies between the class activation maps of natural examples and adversarial examples. is ampliﬁed to a large perturbation in internal layers of the target model and leads to misleading predictions [23]. Fur-thermore, these pre-processing based approaches are shown to be less effective in front of unseen adversarial attacks
[39, 6, 45] as the adversarial perturbations of adversarial examples they used may not be the maximum in internal layers (see Section 4).
The class activation mapping technique [48] gives us an inspiration to solve this problem. Given a classiﬁcation net-work, the class activation mapping technique could iden-tify the importance of image regions by projecting back the class weights of the output layer on to the last convolutional features and performing a linear sum of the weighted fea-tures [48]. We ﬁnd that although adversarial noise is im-perceptible in pixel level, there exists obvious discrepancies
between the class activation maps of natural examples and adversarial examples crafted by existing attack approaches (see Figure 1). In addition, the weighted features are in the high-level layer of the network where small residual noise could cause large perturbations. This motivates us to han-dle the issue of error ampliﬁcation effect by designing a de-fense method which focuses on the weighted features called as class activation features.
In this paper, we propose an adversarial training mech-anism to remove adversarial noise by exploiting class acti-vation features. In a high level, we design a max-min for-mula in the class activation feature space to learn a denois-ing model in a self-supervised manner without seen types of adversarial examples and ground-truth labels. Speciﬁcally, we ﬁrst craft adversarial examples by maximally disrupt-ing the class activation features of natural examples. The discrepancies of class activation features make adversarial examples have different prediction results from natural ex-amples. We name such attack as class activation feature based attack (CAFA). Then, we train a denoising model, namely class activation feature based denoiser (CAFD), to remove adversarial noise. Instead of directly utilizing pixel-level loss functions to train our model, we minimize the dis-tances between the class activation features of the natural examples and the adversarial examples. Finally, an image discriminator is introduced to make restored examples close to the natural examples by enhancing the ﬁne texture details.
Achieved by such self-supervised adversarial training, our defense method could provide more signiﬁcant protec-tions against unseen types of attacks and adaptive attacks compared to previous defenses, which is empirically veri-ﬁed in Section 4.2. Furthermore, additional evaluations on ablation study and robustness of our model to the perturba-tion budget in Section 4.3 further demonstrate the effective-ness of our method.
The main contributions in this paper are as follows:
• We ﬁnd that although adversarial noise is impercep-tible in pixel level, it signiﬁcantly disrupts the class activation features of natural examples. To this end, we design a class activation features based denoiser (CAFD) to effectively remove adversarial noise by ex-ploiting class activation features.
• An self-supervised adversarial training mechanism is proposed to train the denoiser. We maximally disrupt-ing the class activation features of natural examples to craft adversarial examples, and use them to train the denoiser for learning to minimize the distances be-tween natural and adversarial examples in the class ac-tivation feature space.
• Empirical experiments show that our method could enhance adversarial robustness and it could be trans-ferred across different target models. Particularly, the success rates of unseen attacks and adaptive attacks are reduced signiﬁcantly in comparison to previous state-of-the-art approaches.
The rest of this paper is organized as follows. In Sec-tion 2, we brieﬂy review related work on attacks and de-fenses. In Section 3, we describe our defense method and present its implementation. Experimental results on differ-ent datasets are provided in Section 4. Finally, we conclude this paper in Section 5. 2.