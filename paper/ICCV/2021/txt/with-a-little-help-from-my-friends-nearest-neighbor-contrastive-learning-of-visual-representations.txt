Abstract
Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-deﬁned transformations of the same instance. While most meth-ods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-deﬁned transformations.
We ﬁnd that using the nearest-neighbor as positive in contrastive losses improves performance signiﬁcantly on
ImageNet classiﬁcation using ResNet-50 under the linear evaluation protocol, from 71.7% to 75.6%, outperform-ing previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance signiﬁcantly when only 1% ImageNet labels are available, from 53.8% to 56.5%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1% ImageNet Top-1 accu-racy when we train using only random crops. 1.

Introduction
How does one make sense of a novel sensory experi-ence? What might be going through someone’s head when they are shown a picture of something new, say a dodo?
Even without being told explicitly what a dodo is, they will likely form associations between the dodo and other simi-lar semantic classes; for instance a dodo is more similar to a chicken or a duck than an elephant or a tiger. This act of contrasting and comparing new sensory inputs with what one has already experienced happens subconsciously and
Figure 1: NNCLR Training. We propose a simple self-supervised learning method that uses similar examples from a support set as positives in a contrastive loss. might play a key role [22] in how humans are able to ac-quire concepts quickly. In this work, we show how an abil-ity to ﬁnd similarities across items within previously seen examples improves the performance of self-supervised rep-resentation learning.
A particular kind of self-supervised training – known as instance discrimination [9, 30, 54] – has become popular recently. Models are encouraged to be invariant to multi-ple transformations of a single sample. This approach has been impressively successful [9, 25] at bridging the perfor-mance gap between self-supervised and supervised models.
In the instance discrimination setup, when a model is shown a picture of a dodo, it learns representations by being trained to differentiate between what makes that speciﬁc dodo im-age different from everything else in the training set. In this work, we ask the question: if we empower the model to also
ﬁnd other image samples similar to the given dodo image, does it lead to better learning?
Current state-of-the-art instance discrimination methods
generate positive samples using data augmentation, random image transformations (e.g. random crops) applied to the same sample to obtain multiple views of the same image.
These multiple views are assumed to be positives, and the representation is learned by encouraging the positives to be as close as possible in the embedding space, without col-lapsing to a trivial solution. However random augmenta-tions, such as random crops or color changes, can not pro-vide positive pairs for different viewpoints, or deformations of the same object, or even for other similar instances within a semantic class. The onus of generalization lies heavily on the data augmentation pipeline, which cannot cover all the variances in a given class.
In this work, we are interested in going beyond single instance positives, i.e. the instance discrimination task. We expect by doing so we can learn better features that are invariant to different viewpoints, deformations, and even intra-class variations. The beneﬁts of going beyond sin-gle instance positives have been established in [27, 36], though these works require class labels or multiple modal-ities (RGB frames and ﬂow) to obtain the positives which are not applicable to our domain. Clustering-based meth-ods [5, 7, 60] also offer an approach to go beyond single instance positives, but assuming the entire cluster (or its prototype) to be positive samples could hurt performance
Instead we propose us-due to early over-generalization. ing nearest neighbors in the learned representation space as positives.
We learn our representation by encouraging proximity between different views of the same sample and their near-est neighbors in the latent space. Through our approach,
Nearest-Neighbour Contrastive Learning of visual Repre-sentations (NNCLR), the model is encouraged to general-ize to new data-points that may not be covered by the data
In other words, nearest-augmentation scheme at hand. neighbors of a sample in the embedding space act as small semantic perturbations that are not imaginary i.e. they are representative of actual semantic samples in the dataset. We implement our method in a contrastive learning setting sim-ilar to [9, 10].
To obtain nearest-neighbors, we utilize a support set that keeps embeddings of a subset of the dataset in memory.
This support set also gets constantly replenished during training. Note that our support set is different from memory banks [50, 54] and queues [11], where the stored features are used as negatives. We utilize the support set for nearest neighbor search for retrieving cross-sample positives. Fig-ure 1 gives an overview of the method.
We make the following contributions: (i) We introduce
NNCLR to learn self-supervised representations that go be-yond single instance positives, without resorting to cluster-ing; (ii) We demonstrate that NNCLR increases the perfor-mance of contrastive learning methods (e.g. SimCLR [10]) by ∼ 3.8% and achieves state of the art performance on ImageNet classiﬁcation for linear evaluation and semi-supervised setup with limited labels; (iii) Our method out-performs state of the art methods on self-supervised, and even supervised features (learned via supervised ImageNet pre-training), on 8 out of 12 transfer learning tasks; Finally, (iv) We show that by using the NN as positive with only random crop augmentations, we achieve 73.3% ImageNet accuracy. This reduces the reliance of self-supervised meth-ods on data augmentation strategies. 2.