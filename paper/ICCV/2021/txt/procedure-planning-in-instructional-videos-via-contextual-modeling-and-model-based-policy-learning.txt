Abstract
Learning new skills by observing humans’ behaviors is an essential capability of AI. In this work, we leverage in-structional videos to study humans’ decision-making pro-cesses, focusing on learning a model to plan goal-directed actions in real-life videos. In contrast to conventional ac-tion recognition, goal-directed actions are based on expec-tations of their outcomes requiring causal knowledge of po-tential consequences of actions. Thus, integrating the en-vironment structure with goals is critical for solving this task. Previous works learn a single world model will fail to distinguish various tasks, resulting in an ambiguous la-tent space; planning through it will gradually neglect the desired outcomes since the global information of the future goal degrades quickly as the procedure evolves. We address these limitations with a new formulation of procedure plan-ning and propose novel algorithms to model human behav-iors through Bayesian Inference and model-based Imitation
Learning. Experiments conducted on real-world instruc-tional videos show that our method can achieve state-of-the-art performance in reaching the indicated goals. Fur-thermore, the learned contextual information presents in-teresting features for planning in a latent space. 1.

Introduction
Humans can learn new skills by watching demo videos.
Although this seems natural to human, it is challenging for
AI. We have seen rich works on modeling human behaviors from videos with the majority focusing on recognizing ac-tions [21, 18, 26]. However, solely perceiving what actions are performed without modeling the underlying decision-making process is insufﬁcient for AI to learn new skills. The next-generation AI needs to ﬁgure out what actions are nec-essary to achieve the desired goals [5] with the considera-tion of actions’ potential consequences. In this paper, we fo-cus on learning the goal-directed actions from instructional videos. Recently, Chang et al. [6] proposed a new prob-lem known as procedure planning in instructional videos.
Figure 1: Overview of our proposed method. Given a starting observation (top-left image) and a desired visual goal (bottom-left image), we extract the contextual infor-mation of the planning trajectory upon which the Genera-tion Model outputs a sequence of actions. The model is re-sponsible for learning plannable latent representations with a focus on procedures and action consequences. Thus, we can retrieve images of intermediate steps (top-right images).
It requires a model to 1) plan a sequence of verb-argument actions and 2) retrieve the intermediate steps for achieving a given visual goal in real-life tasks such as making a straw-berry cake (see Fig. 1). This task is different from the typi-cal image-language translation problem in the way that cer-tain actions can be exchanged to achieve the same goal (e.g., the order of adding salt and sugar usually does not matter), making it difﬁcult to predict the same action sequence as ground-truth using sequence mapping.
Moreover, sequence-to-sequence based structure, suit-able for modeling events that tend to occur in sequence with high probability, is thought to involve no consideration of the likely outcome [9]. Therefore, we formalize this task as a planning problem with focus on two different sequential patterns that can be easily observed in Fig. 4: In the context of making a cake, mixing ingredients and washing cherries are interchangeable, i.e., short-term action separation, but both should be ahead of the action putting cherries on the top, i.e., long-term action association.
Inspired by Raab et al. [9], we think that when perform-ing goal-directed tasks, it is beneﬁcial to consider both the task contextual information and the potential action conse-quences. Contextual information here refers to the time-invariant knowledge (not changed during planning) that dis-Figure 2: Procedure planning example. Given a starting observation (picture of food ingredients) and a visual goal (picture of a made cake), the model needs to learn how to complete real-world tasks such as making a cake by planning a sequence of actions a1:T (blue circles) and retrieving the intermediate observations o2:T −1 (yellow circles). tinguishes a particular task from the others. For example, if we know the goal is to make a cake as shown in Fig. 1, it is less likely to plan an action like putting it on the grill.
Therefore, we model the dependency between the ac-tions and different goals as the long-term action association in a Bayesian framework. As we show later in the exper-iments, this serves a few purposes: a) it provides a more structured representation for the subsequent policy learning; b) we can sample from the posterior distribution for more diverse trajectories to facilitate the action exploration; and c) compared with the noisy pixel space, feature distances in the learned latent space are more meaningful. To achieve short-term action separation, we model the action sequence as a Markov Decision Process (MDP) as shown in Fig. 2, where the future action depends only upon the present state.
Besides, because goal-directed actions are often selected based on expectations of their consequent outcomes [7], we propose to incorporate a transition model into the Imitation
Learning (IL) framework [16, 19] so that we can explicitly model the environment jointly with policy learning.
This approach brings following advantages: a) it helps policy to ﬂexibly pursue a goal by leveraging causal knowl-edge of the actions’ potential consequences; b) when mod-el-based simulations produce states with alternative actions, the discrimination and selection between actions allow an agent to ﬁnd the currently most desired outcome [24, 34]; and c) it bypasses the need of an interactive environment that is required by classic planning algorithms [31, 30], making it suitable for modelling the web videos.
We demonstrate the effectiveness of our approach by evaluating it on a real-world instructional video dataset [43] (an example is shown in Fig. 2). The results on the proce-dure planning task show that our learned model can uncover the underlying human decision-making processes. Further-more, the results on the challenging walk-through planning task [22] conﬁrm that our model learns meaningful repre-sentations of the environment dynamics, which is crucial for efﬁcient plannings in the latent space. Finally, the vi-sualization of contextual information indicates that our pro-posed encoder structure can learn a concise representation to capture distinct knowledge of different real-world tasks.
The main contributions of our work are summarized as fol-lows: a) we propose a novel method to address the pro-cedure planning problem, which combines Bayesian Infer-ence with Model-based Imitation Learning; b) we propose a neural network structure based on variational inference that learns to embed sufﬁcient information to convey the desired task, incorporating the visual observations’ uncer-tainty; and c) we propose two model-based IL algorithms that explicitly learn the environment dynamics (in either a stochastic or deterministic way) and integrate with the tran-sition model to simultaneously learn a plannable latent rep-resentation for accurate planning. 2.