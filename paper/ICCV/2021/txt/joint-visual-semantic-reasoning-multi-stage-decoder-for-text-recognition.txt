Abstract
Although text recognition has signiﬁcantly evolved over the years, state-of the-art (SOTA) models still struggle in the wild scenarios due to complex backgrounds, varying fonts, uncontrolled illuminations, distortions and other artifacts.
This is because such models solely depend on visual infor-mation for text recognition, thus lacking semantic reasoning capabilities. In this paper, we argue that semantic informa-tion offers a complementary role in addition to visual only.
More speciﬁcally, we additionally utilize semantic informa-tion by proposing a multi-stage multi-scale attentional de-coder that performs joint visual-semantic reasoning. Our novelty lies in the intuition that for text recognition, predic-tion should be reﬁned in a stage-wise manner. Therefore our key contribution is in designing a stage-wise unrolling attentional decoder where non-differentiability, invoked by discretely predicted character labels, needs to be bypassed for end-to-end training. While the ﬁrst stage predicts using visual features, subsequent stages reﬁne on-top of it using joint visual-semantic information. Additionally, we intro-duce multi-scale 2D attention along with dense and resid-ual connections between different stages to deal with vary-ing scales of character sizes, for better performance and faster convergence during training. Experimental results show our approach to outperform existing SOTA methods by a considerable margin. 1.

Introduction
Text recognition has been a popular area of research
[1, 33, 53] for decades thanks to its wide range of com-mercial applications [42], from translation apps in mixed reality, street signs recognition in autonomous driving to as-sistive technology for the visually impaired [10], to name a few. Signiﬁcant progress in fundamental deep learning components [25, 2] alongside sequence-to-sequence learn-ing frameworks [53, 33, 46], have boosted unconstrained word recognition accuracy (WRA) in recent times. Despite
*Interned with SketchX
Figure 1: Compared to existing attentional decoder archi-tectures [53, 33], we design a novel multi-scale attention decoder for text recognition which is unpacked in a stage-wise manner . The problem of non-differentiability, due to discrete-character prediction is bypassed via straight-through Gumbel-Softmax operator [26], such that the later stages can learn reﬁning strategy over the previous predic-tion in an end-to-end differentiable way, with joint visual-semantic information. such developments, state-of-the-art text recognition frame-works [3, 14, 37, 55, 8, 7, 9] still struggle in wild scenarios
[1, 42] due to complex backgrounds, varying fonts, uncon-trolled illuminations, distortions and other artifacts. While machines struggle with a combination of these challenges, humans recognise them easily via joint visual-semantic rea-soning. Therefore, the question in focus is – how to develop a visual-semantic reasoning skill for text recognition?
State-of-the-art text recognition systems [1] mostly rely on extracted visual features to recognize a word image as a machine readable character-sequence. Follow-up ef-forts have been made towards improving reasoning ability by increasing the depth of convolutional feature extractor
[14] having larger receptive ﬁelds, or introducing pyramidal pooling [55] and stacking multiple Bi-LSTM layers [37].
Despite all these attempts that merely lead towards a better context modeling [1], a semantic reasoning potential [13] is largely missing beyond enriching the visual feature. In wild scenarios, a word image might be blurred, distorted, partly
noisy or have artifacts, making recognition extremely difﬁ-cult using visual feature alone. In such cases, we humans
ﬁrst try to interpret the easily recognizable characters using visual cues alone. A semantic reasoning skill is then applied to decode the ﬁnal text by jointly processing the visual and semantic information from previously recognized character sequence. Motivated by this intuition, we propose a novel multi-stage prediction paradigm for text recognition. Here the ﬁrst stage predicts using visual cues, while subsequent stages reﬁne on top of it using joint visual-semantic infor-mation, by iteratively [13, 12] building up the estimates.
Designing this joint visual-semantic reasoning frame-work for text recognition is non-trivial. One might ar-gue that attentional decoder being a sequence-to-sequence model, encapsulates the character dependency [53, 33, 46] and caters for semantic reasoning. However, due to its auto-regressive nature [2], only those characters predicted previously, could provide semantic context at a given step, thus making the semantic context ﬂow unidirectional dur-ing inference. While semantic context becomes negligible towards the initial steps, one wrong prediction here would deal a cumulative adverse impact on the later steps (which stays unreﬁned due to single stage prediction). Therefore, this single stage attentional decoder fails to model the global semantic context, leaving joint visual-semantic reasoning unaccomplished. To explore the entire global semantic con-text, we need the completely unrolled prediction from ﬁrst stage, upon which we can build up the global semantic in-formation. Hence as our ﬁrst contribution we propose a multi-stage attentional decoder (Figure 1), where we build up global semantic reasoning on the initial estimate of ﬁrst stage, which is further reﬁned by subsequent stages.
Let us consider the word ‘aeroplane’. For a single stage attentional decoder, if the model predicts ‘n’ instead of ‘r’,
‘aen’ would adversely affect rest of the prediction, without any chance of reﬁnement (being single stage). Also, it holds almost negligible semantic context while predicting the ﬁrst few characters. Considering we unroll the prediction stage-wise, if a character is predicted wrongly, like ‘aenoplane’, rest of the characters provide signiﬁcant context as semantic information. This helps in reﬁning ‘n’ to ‘r’ during the later stages coupled with visual information.
Moreover, obtaining the prediction from earlier stages, needs a non-differentiable argmax operation [26] as char-acters are discrete tokens. This leads to an inefﬁcient mod-elling of inﬂuence of a prior stage on the next predictions.
An apparent approach here might be to adapt teacher forc-ing [31] for the later stages during training. The later stages intend to learn how to reﬁne the initial (might be incorrect) hypothesis towards a correct prediction. This motivation however is defeated on feeding exact ground-truth labels as teacher forcing for subsequent stages. Consequently, we make use of Gumbel-Softmax operation [26] bypass-ing non-differentiability, and making the network end-to-end trainable even across stages.
In summary our contributions are: First and foremost, we propose a multi-stage character decoding paradigm with stage-wise unrolling. While the ﬁrst stage predicts using vi-sual features, subsequent stages reﬁne on-the-top of them using joint visual-semantic information. Secondly, we em-ploy a Gumbel-softmax layer to make visual-to-semantic embedding layer differentiable. The model thus learns its reﬁning strategy from initial to ﬁnal prediction in an end-to-end manner. Thirdly, from the architectural design, we in-troduce multi-scale 2D attention to deal with varying scales of character size, and empirically found adding dense and residual connection between different stages stabilize train-ing for better performance leading to outperforming other state-of-the-arts signiﬁcantly on benchmark datasets. 2.