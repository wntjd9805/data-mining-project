Abstract
Event cameras are bio-inspired sensors that respond to brightness changes asynchronously and output in the form of event streams instead of frame-based images. They own outstanding advantages compared with traditional cam-eras: higher temporal resolution, higher dynamic range, and lower power consumption. However, the spatial resolu-tion of existing event cameras is insufficient and challeng-ing to be enhanced at the hardware level while maintain-ing the asynchronous philosophy of circuit design. There-fore, it is imperative to explore the algorithm of event stream super-resolution, which is a non-trivial task due to the sparsity and strong spatio-temporal correlation of the events from an event camera.
In this paper, we propose an end-to-end framework based on spiking neural network for event stream super-resolution, which can generate high-resolution (HR) event stream from the input low-resolution (LR) event stream. A spatiotemporal constraint learning mechanism is proposed to learn the spatial and temporal distributions of the event stream simultaneously. We val-idate our method on four large-scale datasets and the re-sults show that our method achieves state-of-the-art perfor-mance. The satisfying results on two downstream applica-tions, i.e. object classification and image reconstruction, further demonstrate the usability of our method. To prove the application potential of our method, we deploy it on a mobile platform. The high-quality HR event stream gener-ated by our real-time system demonstrates the effectiveness and efficiency of our method. 1.

Introduction
Event cameras, e.g. Dynamic Vision Sensor (DVS) [6], are bio-inspired vision sensors. Different from traditional frame-based sensors that capture images at a fixed rate, the event cameras respond to the pixel-wise brightness changes of the scene asynchronously. Specifically, let I(x, y, t) be
*Corresponding author
Figure 1. A comparison of the outputs of an event camera and a frame-based camera while capturing (a) an ellipse and a star mov-ing rightward. Different from the frame-based camera (b) that cap-tures the scene at a fixed rate, the event camera asynchronously responds to the brightness changes of the scene and outputs a spa-tiotemporal event stream (c), where the red and blue points repre-sent the positive and negative events, respectively, i.e. the increase and decrease of the brightness. Figure inspired by [32]. the brightness intensity of the spatial coordinates (x, y) at time t. When the change of its logarithm reaches a thresh-old value C, i.e. |∆ log I(x, y, t)| > C, an event will be triggered [34]. Each event is denoted as ei = (xi, yi, ti, pi), where pi ∈ {1, −1} is the polarity, indicating whether the brightness is increased or decreased. Therefore, the output of the event camera is a tuple list called event stream, de-noted as E = {ei}N i=1. Figure 1 compares the output of an event camera and a traditional frame-based camera.
Compared with traditional cameras, event cameras own many outstanding properties: high dynamic range (140 dB vs. 60 dB for traditional cameras), high temporal resolution, and free from motion blur, which make them widely used in many applications, e.g. object recognition [31, 41], ges-ture recognition [1, 2], optical flow estimation [47, 33], high frame rate video reconstruction [32, 36, 16], visual-inertial odometry [46, 24], and 3D reconstruction [17, 8].
However, the spatial resolution of existing event cam-eras is insufficient. Many influential works are based on the 128 × 128 pixels DVS128 [34] or 240 × 180 pixels
DAVIS240 [6]. If we aim at improving the spatial resolution at the hardware level, i.e. reducing the pixel size, the asyn-chronous circuit design philosophy will tough to be main-tained [13]. Therefore, under the difficulty of increasing the spatial resolution with hardware design, it is imperative to explore the algorithm of event stream super-resolution.
The concerned event stream super-resolution is to in-crease the spatial resolution of the input low-resolution (LR) event stream without changing the data modality, i.e. directly generating high-resolution (HR) event stream in-stead of HR grid-based representation of the event stream, e.g. intensity image [27] and event frame [42]. This is be-cause the grid-based representations of the event stream are incomplete with a lower temporal resolution, leading to a deficiency of the temporal information of the original event stream. Some high-performance algorithms [39, 25] also directly deal with the event stream instead of the grid-based representation of it, and achieve a higher computing effi-ciency. So we claim that it is more significant to generate
HR event streams instead of its grid-based representation.
The event stream is a type of spatiotemporal event cloud containing high-precision timestamp information compared with traditional frame-based vision. The core question of the event stream super-resolution is how to effectively de-scribe the spatial and temporal distribution of the event stream. To the best of our knowledge, there are few studies on this task. Li et al. [22] use a sparse signal representation based method [44] to learn the event stream’s spatial dis-tribution and use a non-homogeneous Poisson processes to simulate the event stream at each pixel. The timestamp of each event in the output HR event stream is predicted by a sampling method according to the Poisson intensity, which causes a fatal error in temporal dimension due to the lack of effective supervised learning on the temporal distribution.
In this paper, we propose an end-to-end event stream super-resolution method based on the spiking neural net-work, which could better preserve the temporal component of the event stream. A spatiotemporal constraint learning mechanism is proposed to describe both the spatial and temporal distribution of the event stream. The proposed method could generate the HR event stream by precisely predicting each output event’s timestamp, instead of pre-dicting a grid-based representation. We evaluate our method on four large-scale datasets, i.e. N-MNIST [30], CIFAR10-DVS [23], ASL-DVS [2], and Event Camera Dataset [28].
Experimental results show that our method outperforms the state-of-the-art method by a large margin. For further test-ing the quality of super-resolution results, we evaluate our generated HR event streams on two downstream applica-tions, i.e. object classification and image reconstruction.
The marvelous performance shows the usability of the pro-posed method. We also deploy our method on a mobile system and show that our method can perform high-quality event stream super-resolution in real-time.
Our main contributions are summarized as follows:
• An end-to-end SNN-based model is proposed for the event stream super-resolution task. Compared with the previous method, a spatiotemporal constraint learning mechanism is proposed to learn the temporal and spa-tial distribution of the event stream simultaneously.
• Satisfying performances on two downstream applica-tions with the generated HR event streams as input, i.e. object classification and image reconstruction, demon-strate the usability of the proposed method.
• An embedded deployment of our method is imple-mented proving that the proposed method can gener-ate high-quality HR event streams in real-time, which shows the potential of deploying the proposed method on mobile systems. 2.