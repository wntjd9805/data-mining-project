Abstract 1.

Introduction
Inspired by the ability of StyleGAN to generate highly re-alistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of
StyleGAN to manipulate generated and real images. How-ever, discovering semantically meaningful latent manipula-tions typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Con-trastive Language-Image Pre-training (CLIP) models in or-der to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent map-per that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for map-ping text prompts to input-agnostic directions in StyleGAN’s style space, enabling interactive text-driven image manipu-lation. Extensive results and comparisons demonstrate the effectiveness of our approaches.
∗ Equal contribution, ordered alphabetically. Code and video are available on https://github.com/orpatashnik/StyleCLIP
Generative Adversarial Networks (GANs) [18] have rev-olutionized image synthesis, with recent style-based gener-ative models [24, 25, 22] boasting some of the most realistic synthetic imagery to date. Furthermore, the learnt interme-diate latent spaces of StyleGAN have been shown to possess disentanglement properties [9, 48, 19, 53, 58], which enable utilizing pretrained models to perform a wide variety of im-age manipulations on synthetic, as well as real, images.
Harnessing StyleGAN’s expressive power requires de-veloping simple and intuitive interfaces for users to eas-ily carry out their intent. Existing methods for seman-tic control discovery either involve manual examination (e.g., [19, 48, 58]), a large amount of annotated data, or pre-trained classifiers [49, 1]. Furthermore, subsequent manipu-lations are typically carried out by moving along a direction in one of the latent spaces, using a parametric model, such as a 3DMM in StyleRig [53], or a trained normalized flow in StyleFlow [1]. Specific edits, such as virtual try-on [27] and aging [2] have also been explored.
Thus, existing controls enable image manipulations only along preset semantic directions, severely limiting the user’s creativity and imagination. Whenever an additional, unmapped, direction is desired, further manual effort and/or large quantities of annotated data are necessary.
In this work, we explore leveraging the power of re-cently introduced Contrastive Language-Image Pre-training (CLIP) models in order to enable intuitive text-based se-mantic image manipulation that is neither limited to preset manipulation directions, nor requires additional manual ef-fort to discover new controls. The CLIP model is pretrained on 400 million image-text pairs harvested from the Web, and since natural language is able to express a much wider set of visual concepts, combining CLIP with the generative power of StyleGAN opens fascinating avenues for image manipulation. Figure 1 shows several examples of unique manipulations produced using our approach. Specifically, in this paper we investigate three techniques that combine
CLIP with StyleGAN: 1. Text-guided latent optimization, where a CLIP model is used as a loss network [20]. This is the most versatile approach, but it requires a few minutes of optimization to apply a manipulation to an image. 2. A latent residual mapper, trained for a specific text prompt. Given a starting point in latent space (the in-put image to be manipulated), the mapper yields a local step in latent space. 3. A method for mapping a text prompt into an input-agnostic (global) direction in StyleGAN’s style space, providing control over the manipulation strength as well as the degree of disentanglement.
The results in this paper and the supplementary mate-rial demonstrate a wide range of semantic manipulations on images of human faces, animals, cars, and churches.
These manipulations range from abstract to specific, and from extensive to fine-grained. Many of them have not been demonstrated by any of the previous StyleGAN manipula-tion works, and all of them were easily obtained using a combination of pretrained StyleGAN and CLIP models. 2.