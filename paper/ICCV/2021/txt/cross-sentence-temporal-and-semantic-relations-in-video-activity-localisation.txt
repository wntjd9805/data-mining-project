Abstract
Video activity localisation has recently attained increas-ing attention due to its practical values in automatically localising the most salient visual segments corresponding to their language descriptions (sentences) from untrimmed and unstructured videos. For supervised model training, a temporal annotation of both the start and end time index of each video segment for a sentence (a video moment) must be given. This is not only very expensive but also sensi-tive to ambiguity and subjective annotation bias, a much harder task than image labelling. In this work, we develop a more accurate weakly-supervised solution by introducing
Cross-Sentence Relations Mining (CRM) in video moment proposal generation and matching when only a paragraph description of activities without per-sentence temporal an-notation is available. Specifically, we explore two cross-sentence relational constraints: (1) Temporal ordering and (2) semantic consistency among sentences in a paragraph description of video activities. Existing weakly-supervised techniques only consider within-sentence video segment correlations in training without considering cross-sentence paragraph context. This can mislead due to ambiguous ex-pressions of individual sentences with visually indiscrimi-nate video moment proposals in isolation. Experiments on two publicly available activity localisation datasets show the advantages of our approach over the state-of-the-art weakly supervised methods, especially so when the video activity descriptions become more complex. 1.

Introduction
Video activity localisation by natural language is an im-portant yet challenging task, which aims to localise tempo-rally a video segment (moment1) that best corresponds to a query sentence in an untrimmed (and often unstructured) video [21, 8]. Most of the existing methods address this task in a fully supervised manner [22, 6], i.e. the untrimmed
Figure 1: Different video activity localisation methods: (a)
Given a paragraph description and the per-sentence tempo-ral annotation (start and end time index), fully-supervised methods learn to align sentences with ground-truth seman-tically matching video moments [6, 22]. (b) Without fine-grained temporal annotations, weakly-supervised models often generate proposals of video segments corresponding to sentences in a paragraph before learning the best visual-text alignment [20, 18]. (c) The CRM model explores the temporal order of different sentences in a paragraph to min-imise the ambiguities in matching the best video moments to specific sentences in the context of a paragraph. (d) To deal with ambiguous expressions in descriptions, CRM fur-ther explore plausible sentence expansion, e.g. pairing two sentences (concatenation) as a more complex query to con-strain the localisation of pairwise video moment proposals.
This explores cross-sentencing semantic consistency. video data are annotated by both a paragraph description, in which each sentence is describing a video moment-of-interest (MoI), and per-sentence temporal boundaries on the precise start and end time indices of every MoI. Given such fine-grained labelling, models can generate MoIs from the original videos to learn the best alignment of MoIs with their descriptions (Fig. 1 (a)). To avoid the high annotation cost and subjective annotation bias2, recent works focus on
*Corresponding authors. 1Video segment and moment are used interchangeably in this paper. 2Different temporal boundaries are marked for the same sentences [1].
weakly-supervised learning without per-sentence temporal boundary annotations in training [8, 11, 21].
Existing weakly-supervised solutions [37, 25, 18] lo-calise different MoIs individually (Fig. 1 (b)), which is not optimal as it neglects the fact that the cross-sentence relations in a paragraph play an important role in tempo-rally localising multiple MoIs. Critically, an individual sen-tence is sometimes ambiguous out of its paragraph con-text [30, 24, 39]. For example in Fig. 1 (c), without the consideration of the temporal relations with the second sen-tence, the first query sentence (purple) can be easily mis-matched with incorrect video segment, which is visually in-discriminate from the ground-truth moment. Our analysis on the ActivityNet-Captions [15] shows that the temporal relations of over 65% moment pairs predicted by a latest model [18] are contradictory with the true order of their descriptions. Yet, MoIs described by a paragraph are of-ten semantically related to each other in their correspond-ing sentences. For example in Fig. 1 (d), “the man” in the blue query exhibits ambiguity if its semantic relations with previous sentences are ignored. We also observed that more than 38% descriptions in ActivityNet-Captions [15] contain ambiguous ways of referring to expressions, e.g. pronouns.
To conclude, there are large error-margins in mis-localising individual sentences to video segments in isolation.
In this work, we introduce a weakly-supervised method for video activity localisation by natural language called
Cross-sentence Relations Mining (CRM). The key idea is to explore the cross-sentence relations in a paragraph as constraints to better interpret and match complex moment-wise temporal and semantic relations in videos. Given the one-to-one moment-sentence mappings, the inherent cross-moment relations are unknown and not straightforward to be modelled in videos but intrinsically available in the para-graph descriptions. Hence, we impose the same cross-sentencing relations to their potentially matching video mo-ments for more reliable proposal selections. The proposed
CRM method differs significantly from the existing weakly-supervised models [37, 20, 25] which localise per-sentence queries individually. They lack fundamentally any ability to make use of the cross-sentence relations for moment pro-posal selection in model training. Even though such rela-tional information is less complete than per-sentence fine-grained temporal annotation, it requires no annotation and avoids subjective bias from inherent ambiguity in tempo-ral labelling [1]. Specifically, by assuming different activ-ities in videos are described sequentially, we formulate a temporal consistency constraint to encourage the selected moments to be temporally ordered according to their de-scriptions in a paragraph (Fig. 1 (c)). This is different from the temporal pretext tasks in self-supervised video learn-ing where the temporal constraint is adopted within a single modality. We exploit it in a cross modality setup, i.e., con-straining the temporal order of event in visual modality by the sentences order in text modality. Moreover, we encour-age moment proposal selections to satisfy cross-sentence broader semantics in context to minimise video-text match-ing ambiguities. To that end, we introduce a semantic con-sistency constraint to ensure that a moment selected for any pairing of two sentences (concatenation) in a paragraph is consistent (overlapping) with the union of the selected seg-ments per sentence (Fig. 1 (d)).
Our contributions are: (1) To our best knowledge, this is the first idea to develop a model using cross-sentence re-lations in a paragraph to explicitly represent and compute cross-moment relations in videos, so as to alleviate the am-biguity of each individual sentence in video activity locali-sation. (2) We formulate a new weakly-supervised method for activity localisation by natural language called Cross-sentence Relations Mining (CRM), that trains a model with both temporal and semantic cross-sentence relations to im-prove per-sentence temporal boundary prediction in test-ing. (3) Our approach achieves the state-of-the-art perfor-mance on two available activity localisation benchmarks, especially so given more complex query descriptions. 2.