Abstract 1.

Introduction
We present a novel method for temporal coherent recon-struction and tracking of clothed humans. Given a monoc-ular RGB-D sequence, we learn a person-speciﬁc body model which is based on a dynamic surface function net-work. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is em-bedded into the canonical space of the SMPL body model.
With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh.
For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface rep-resentation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh se-quence from the input data. The underlying surface rep-resentation can be used to synthesize new animations of the reconstructed person including pose-dependent defor-mations.
Digital capture of human bodies is a rapidly growing research area in computer vision and computer graphics.
There are many high impact applications, in particular, in the ﬁeld of telepresencing and man-machine interaction that rely on the reconstruction of the surface as well as the mo-tion of a person. For instance, for telepresencing in virtual reality (VR) or augmented reality (AR), the ultimate goal is to photo-realistically re-render the people who want to inter-act with each other. A key challenge is to capture and repro-duce the natural motions, including the dynamically chang-ing surface (especially clothing). In our work, we present a novel representation for the surface of a clothed human called dynamic surface function networks which captures pose-dependent surface deformations such as wrinkles of the clothing. In contrast to recent works on implicit repre-sentations [11, 6, 23], our representation explicitly models the surface of the human. Both representations have their advantages and disadvantages. We use an explicit surface representation to leverage fast forward rendering (exploit-ing the rasterization units of a GPU), and global surface correspondences between different frames. Speciﬁcally, our dynamic surface function network is attached to the surface of the SMPL [28] body model which gives us access to the 1
kinematic chain, as well as to a topology for rendering. In particular, our model represents a continuous offset surface that can be evaluated at arbitrary points of the SMPL sur-face (also within triangles). However, note that our repre-sentation is agnostic to the underlying parametric model and can also be used for other dynamically changing surfaces, e.g., human faces. The dynamic surface function network is trained in a person-speciﬁc fashion. To this end, we as-sume a short sequence (few seconds) of the person moving in front of a single consumer-level RGB-D camera. Note that we do not assume any speciﬁc motion sequences (e.g., standing in T-pose or alike). We jointly optimize the surface representation network as well as the pose parameters of the underlying SMPL body model. This global optimization strategy allows us to fuse all captured data into a consistent surface representation. This person-speciﬁc surface model can be animated explicitly using the joint control handles of the SMPL model, thus, allowing pose transfer (see Fig. 1).
To summarize, we present a method that allows reconstruc-tion of a person-speciﬁc controllable body model based on a monocular input sequence captured by a commodity RGB-D sensor. Our key contributions are:
• an explicit surface representation network which is able to model the pose-dependent deformations of the surface, such as wrinkles of the clothing.
• a global analysis-by-synthesis formulation that allows for the joint optimization of the pose and the surface over the entire sequence, leading to a temporally con-sistent tracking of the person in the input sequence. 2.