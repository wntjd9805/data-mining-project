Abstract
With the success of deep neural networks, knowledge dis-tillation which guides the learning of a small student net-work from a large teacher network is being actively stud-ied for model compression and transfer learning. How-ever, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ.
In this paper, we propose a densely guided knowledge distillation using mul-tiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifi-cally, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previ-ous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teach-ing of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources.
We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and Ima-geNet. We also achieved significant performance improve-ments with various backbone architectures such as ResNet,
WideResNet, and VGG. 1
Figure 1. Problem definition of the large gap between a teacher and a student network. (a) In general, the difference between layers at KD is approximately 1.8 times, but (b) we are interested in the challenging problem of layer differences of more than 5 times. For solving this problem, TAKD [23] has been proposed.
However, (c) TAKD has a fundamental limitation such as the er-ror avalanche problem. Assuming that a unique error occurs one by one when a higher-level teacher assistant (TA) teaches a lower-level TA. The error case continues to increase whenever teaching more TAs. Meanwhile, in (d), the proposed densely guided knowl-edge distillation can be relatively free from this error avalanche problem because it does not teach TAs at each level alone. 1.

Introduction
While deep learning-based methods [11, 16, 10, 2], e.g., convolutional neural networks (CNNs), have achieved very impressive results in terms of accuracy, there have been many trials [9, 15, 34, 14] to apply them to many ap-plications such as classification, detection, and segmenta-tion. Among these attempts, Knowledge Distillation (KD)
[14, 28] transfers the knowledge of a teacher model (e.g., a deeper or wider network) in the form of soft prob-1Our code is available at https://github.com/wonchulSon/DGKD. logits) to improve the accuracy of a less-ability (e.g., parameterized student model (e.g., a shallower network) during a learning procedure. Specifically, the soft logits of the teacher network can train the student network more effi-ciently than the softmax based on the class label of the stu-dent network itself. Many studies [14, 28, 40, 36, 42, 33, 3] on the KD method have been proposed, most of which fo-cused on effectively guiding a teacher’s soft probability or outputs to a student. Recently, there have been ensemble-based attempts [4, 41, 38, 22] to train a student network based on many peers or students without considering the single teacher network, which is a slight lack of consider-ation for the diversity of ensemble classifiers teaching the student, especially when the gap between a teacher and a student is large like Figure 1 (a) and (b).
In [5, 18], it was shown that a teacher does not always have to be smart for a student to learn effectively. The
KD can not succeed when the student’s capacity is too low to successfully mimic the teacher’s knowledge. Recently, to overcome this problem, TA-based knowledge distillation (TAKD) [23] using intermediate-sized assistant models was introduced to alleviate the poor learning of a student net-work when the size gap between a student and a teacher is large. It achieved an effective performance improvement in the case of a large gap in teacher and student sizes. How-ever, further studies are required to determine whether us-ing middle-sized assistant models in series is the most ef-ficient KD method for bridging the gap between a teacher and a student. For example, TAKD tends to cause the error avalanche problem, as shown in Figure 1 (c). It sequentially trains the multiple TA models by decreasing the capacity of their own assistant models for efficient student learning. If an error occurred during a specific TA model learning, this
TA model will teach the next level assistant models includ-ing the same error. From then on, each time a TA is trained, the error snowballs gradually, as illustrated in Figure 1 (c).
This error avalanche problem becomes an obstacle to im-proving the student model’s performance as the total num-ber of TAs increases.
In this paper, we propose a novel densely guided knowl-edge distillation (DGKD) using multiple TAs for efficient learning of the student model despite the large size gap be-tween a teacher and a student model. As shown in Figure 1 (d), unlike TAKD, when learning a TA whose model size gradually decreases for the target student, the knowledge is not distilled only from the higher-level TA but guided from all previously learned higher-level TAs including the teacher. Thus, a trainee had distilled knowledge by con-sidering the relationship between the multiple trainers (e.g., teacher and TAs) with complementary characteristics. The error avalanche problem could be alleviated successfully. It is largely because the distilled knowledge previously used for the teaching of models disappears in TAKD, but the pro-posed method densely guides the whole distilled knowledge to the target network. In the end, the closer we are to stu-dent learning, the more educators we have, e.g., TAs and the teacher. Therefore, the final student model can get more opportunities to achieve better results.
For stochastic learning of a student model, we randomly remove a fraction of the guided knowledge from trainers during the student training, which is inspired from [31, 17].
Eventually, the student network is taught from trainers en-sembled slightly different for each iteration; this acts as a kind of regularization to solve the problem of overfitting which can often occur when a simple student learns from a complex teacher groups.
The major contributions of this paper are summarized as follows:
• We propose a DGKD that densely guides each TA net-work with the higher-level TAs as well as the teacher and it helps to alleviate the error avalanche problem whose probability of occurrence increases as the num-ber of TAs increases.
• We revise a stochastic DGKD learning algorithm to train the student network from the teacher and multiple
TAs efficiently.
• We demonstrate the significant accuracy improvement gained by the proposed method over well-known KD methods through extensive experiments on various datasets and network architectures. 2.