Abstract 1.

Introduction
This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly de-forming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained.
Moreover, these representations cannot be explicitly con-trolled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences.
Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with in-put skeletal motions to generate new deformation fields to animate the human model. Experiments show that our ap-proach significantly outperforms recent human synthesis methods. The code and supplementary materials are avail-able at https://zju3dv.github.io/animatable nerf/.
∗The first two authors contributed equally. The authors from Zhejiang
University are affiliated with the State Key Lab of CAD&CG.
†Corresponding author: Hujun Bao.
Rendering animatable human characters has a variety of applications such as free-viewpoint videos, telepresence, video games and movies. The core step is to reconstruct animatable human models, which tends to be expensive and time-consuming in traditional pipelines due to two factors.
First, high-quality human reconstruction generally relies on complicated hardware, such as a dense array of cameras
[56, 16] or depth sensors [10, 14]. Second, human ani-mation requires skilled artists to manually create a skeleton suitable for the human model and carefully design skinning weights [29] to achieve realistic animation, which takes countless human labor.
In this work, we aim to reduce the cost of human recon-struction and animation, to enable the creation of digital hu-mans at scale. Specifically, we focus on the problem of au-tomatically reconstructing animatable humans from multi-view videos, as illustrated in Figure 1. However, this prob-lem is extremely challenging. There are two core questions we need to answer: how to represent animatable human models and how to learn this representation from videos?
Recently, neural radiance fields (NeRF) [41] has pro-posed a representation that can be efficiently learned from images with a differentiable renderer. It represents static 3D scenes as color and density fields, which work particularly well with volume rendering techniques. To extend NeRF to handle non-rigidly deforming scenes, [46, 51] decom-pose a video into a canonical NeRF and a set of deformation fields that transform observation-space points at each video
frame to the canonical space. The deformation field is rep-resented as translational vector field [51] or SE(3) field [46].
Although they can handle some dynamic scenes, they are not suited for representing animatable human models due to two reasons. First, jointly optimizing NeRF with trans-lational vector fields or SE(3) fields without motion prior is an extremely under-constrained problem [51, 30]. Second, they cannot explicitly synthesize novel scenes given input motions for animation.
To overcome these problems, we propose a novel mo-tion representation named neural blend weight field. Based on the skeleton-driven deformation framework [29], blend weight fields are combined with 3D human skeletons to generate deformation fields. This representation has two advantages. First, since the human skeleton is easy to track
[22], it does not need to be jointly optimized and thus pro-vides an effective regularization on the learning of deforma-tion fields. Second, by learning an additional neural blend weight field at the canonical space, we can explicitly ani-mate the neural radiance field with input motions.
We evaluate our approach on the H36M [19] and ZJU-MoCap [49] datasets that capture dynamic humans in com-plex motions with synchronized cameras. Across all video sequences, our approach exhibits state-of-the-art perfor-mances on novel view synthesis and novel pose synthesis.
In addition, our method is able to reconstruct the 3D human shape at the canonical space and repose the geometry.
In summary, this work has the following contributions:
• We introduce a novel representation called neural blend weight field, which can be combined with NeRF and 3D human skeletons to recover animatable human models from multi-view videos.
• Our approach demonstrates significant performance improvement on novel view synthesis and novel pose synthesis compared to recent human synthesis meth-ods on the H36M and ZJU-MoCap datasets. 2.