Abstract
Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing ap-proaches require paired training images; i.e. images of the same person with the same clothing in different poses. How-ever, obtaining sufﬁciently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image
CrEation) that closes the image quality gap with super-vised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in sev-eral ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D en-ables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have simi-lar appearance features. Once trained, SPICE takes an im-age of a person and generates a new image of that person
*This work was done during an internship at Amazon. in a new target pose. SPICE achieves state-of-the-art per-formance on the DeepFashion dataset, improving the FID score from 29.9 to 7.8 compared with previous unsupervised methods, and with performance similar to the state-of-the-art supervised method (6.4). SPICE also generates tempo-rally coherent videos given an input image and a sequence of poses, despite being trained on static images only. 1.

Introduction
Given a single source image of a person, can we gener-ate a realistic image of what they would look like from a different viewpoint, in a different pose? While this prob-lem is inherently ambiguous, there is signiﬁcant statistical regularity in human pose, clothing, and appearance, that could make this possible as illustrated in Fig. 1. A solution to the problem would have widespread applications in on-line fashion, gaming, personal avatar creation or animation, and has consequently generated signiﬁcant research interest
[6, 16, 31, 33, 38, 42].
Recent work focuses on generative modeling [8, 13, 15, 49], especially using conditional image synthesis. One set of methods uses supervised training [6, 20, 21, 32], which requires paired training images of the same person in differ-ent poses with the same appearance and clothing. Requir-ing such paired data limits the potential size of the train-ing set, which can impair robustness and generalization.
Consequently, we address this problem without any paired data by developing a self-supervised approach. Such self-supervised formulations have also received signiﬁcant re-cent attention [7, 26, 30, 42].
Our novel formulation builds on the idea of cycle-consistency [49] with some important modiﬁcations. For the forward direction of the cycle, the method takes a source image, source pose and target pose and generates a target image conditioned on pose and appearance. The reverse direction takes this generated image and regenerates the source image by switching the source and target conditions.
The goal is to minimize the difference between the original input image and the one synthesized through the cycle. The problem is that this approach can have a trivial solution in which the cycle produces the identity mapping. To address this, previous methods [30, 38] constrain the target image generation with 2D information. Human bodies, however, are non-rigid 3D entities and their deformations and occlu-sions are not easily expressed in 2D. We show how leverag-ing 3D information, automatically extracted from images, constrains the model in multiple ways.
Speciﬁcally, our method, called SPICE (Self-supervised
Person Image CrEation), exploits the estimated 3D body to constrain the image generation, enabling self-supervised learning. In particular, we estimate the SMPL body model
[23] parameters corresponding to both the input and the generated target image. Since the input and target image only differ in terms of their pose, their body shape should be the same. SMPL makes this easy to enforce because it factors body shape from pose. Using this we introduce two losses. First, we use a pose loss that encourages the body pose in the generated image to match the target pose in 3D.
Second, we add a shape consistency loss that encourages the person in the generated image to have the same 3D shape as the person in the source image (Fig. 2).
These two constraints, however, are not sufﬁcient to gen-erate images with the correct appearance since they only force the model to generate an image with the right shape and pose. There is no constraint that the generated image has the appearance of the source image (e.g. clothing, hair, etc.). Prior work addresses this by enforcing a perceptual loss between patches at each 2D joint [30]. This is not suf-ﬁcient when the body is seen with large viewpoint changes or where a body part becomes occluded; see Fig. 3. We solve this problem by introducing pose-dependent appear-ance consistency on the body surface instead of at the joints.
Figure 2: Shape consistency: The ﬁrst column shows two images of the same person in two different poses and views.
The second column shows the 3D bodies predicted by our 3D regressor and posed in a T-pose. The estimated 3D body shape is similar for the same subject across poses and views.
The third column shows the per-vertex difference of both meshes, color coded from blue (0 mm) to red (20 mm).
The idea is that the projected surface of the 3D body in two different poses must have similar appearance features for matching parts of the body and this similarity should be weighted proportional to the relative global orientation difference between the 3D bodies.
In summary, we improve the realism of self-supervised human reposing by exploiting 3D body information in three novel ways: using a 3D pose loss, body shape consistency, and occlusion-aware appearance feature consistency. We train SPICE with our new constraints on unpaired data. Ex-tensive experiments on the DeepFashion [22] and Fashion
Video datasets [44] show the effectiveness of our model qualitatively and quantitatively. SPICE signiﬁcantly outper-forms the prior state-of-the-art (SOTA) un/self-supervised methods and is nearly as accurate as the best supervised methods. 2.