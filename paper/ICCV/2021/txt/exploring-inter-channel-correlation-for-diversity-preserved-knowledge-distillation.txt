Abstract
Knowledge Distillation has shown very promising abil-ity in transferring learned representation from the larger model (teacher) to the smaller one (student). Despite many efforts, prior methods ignore the important role of retaining inter-channel correlation of features, leading to the lack of capturing intrinsic distribution of the feature space and sufﬁcient diversity properties of features in the teacher network.
To solve the issue, we propose the novel Inter-Channel Correlation for Knowledge Distillation (ICKD), with which the diversity and homology of the fea-ture space of the student network can align with that of the teacher network. The correlation between these two channels is interpreted as diversity if they are irrelevant to each other, otherwise homology. Then the student is required to mimic the correlation within its own embed-ding space. In addition, we introduce the grid-level inter-channel correlation, making it capable of dense prediction tasks. Extensive experiments on two vision tasks, includ-ing ImageNet classiﬁcation and Pascal VOC segmentation, demonstrate the superiority of our ICKD, which consis-tently outperforms many existing methods, advancing the state-of-the-art in the ﬁelds of Knowledge Distillation. To our knowledge, we are the ﬁrst method based on knowl-edge distillation boosts ResNet18 beyond 72% Top-1 ac-curacy on ImageNet classiﬁcation. Code is available at: https://github.com/ADLab-AutoDrive/ICKD. 1.

Introduction
It is widely witnessed that larger networks are superior in learning capacity compared to smaller ones. Nevertheless, due to the great amount of energy consumption and compu-tation costs, a large network (e.g., ResNet-50 [9]), though powerful, is difﬁcult to deploy on mobile systems. Hence, there is a growing interest in reducing the model size while
*Corresponding Author.
†Equal contribution.
‡Work done when as an intern in DAMO Academy, Alibaba Group.
Figure 1: Illustration of inter-channel correlation. The channels orderly extracted from the second layer of
ResNet18 have been visualized. The channels denoted by red boxes are homologous both perceptually and mathemat-ically (e.g., inner-product), while the channels denoted by orange boxes are diverse. We show the inter-channel corre-lation can effectively measure that each channel is homolo-gous or diverse to others, which further reﬂects the richness of the feature spaces. Based on this insightful ﬁnding, our
ICKD can enforce the student to mimic this property from the teacher. preserving comparable performance, which bridges the gap between small networks and large networks.
Knowledge distillation is one of the promising meth-ods to this problem. It is acknowledged that Bucila et al.
[1] introduced the idea of knowledge distillation and Hin-ton et al. [12] further popularized this concept. The key idea of knowledge distillation is to let the student network mimic the teacher model. The underlying principle is that teachers can provide the knowledge that ground truth la-bels can not tell. Despite its success, this technique, de-voted to instance-level classiﬁcation, may lead the student to mainly learn the instance-level information but not struc-tural information, which limits its application. Prior works
[19, 20, 21, 27, 28] have been proposed to help the student network learn the structural representation for better gener-alization ability. These methods generally utilize the cor-relation of the instances to describe the geometry, similar-ity, or dissimilarity in the feature space. We call this fash-ion layer-wise relational knowledge distillation since they mainly focus on exploring the correlation between feature maps in the level of layer. Conversely, we pay more atten-tion to the inter-channel correlation.
Previous works [16, 25] make use of knowledge distil-lation to reduce the homology (i.e., redundancy) of the stu-dent’s feature space. Nevertheless, the success of Ghost-Net [7] suggests that small neural networks beneﬁt from in-creased feature homology. The rich representation can em-power the downstream tasks and both the diversity and ho-mology can reﬂect the richness. Existing literature neglects the importance of feature diversity and homology, yield-ing an issue that the proportion of feature diversity versus homology may be unbalanced against our expectation that student can learn the representation as rich as the teacher is for better generalization. In Fig. 1, the visualized feature maps show that feature diversity and homology co-exist in the networks. This property can be disclosed by the cor-relation between channels, where high relevance represents
In this homology and low relevance represents diversity. paper, we adopt the Inter-Channel Correlation (ICC) as the indicator of the diversity and homology of the feature dis-tribution. However, ﬁguring out the optimal inter-channel correlation manually is impractical. An intuitive solution is to let the student learn better inter-channel correlation from the teacher, as shown in Fig. 2. Due to the discrepancy of learning capacities [4], it is not viable to force the student
Instead, to mimic the whole feature map of the teacher. we let the student model learn the inter-channel correlation from the teacher, namely inter-channel correlation knowl-edge distillation (ICKD).
The correlation between the two channels is evaluated by the inner product in this paper. As the inner product col-lapses the spatial dimension, it naturally does not need to constrain the feature map spatial size of the teacher network and student network to be the same. On the other hand, when it comes to a large feature map, e.g. semantic seg-mentation models, the mapping between the inter-channel correlation measured by the inner product and the original feature space is of high freedom. Thus it will be more dif-ﬁcult to distill the inter-channel correlation distribution to anchor the teacher’s feature space distribution. To alleviate this problem, we propose a grid-level inter-channel corre-lation distillation method. By dividing the feature map of size h × w × c by a pre-deﬁned grid into n × m patches of size hG × wG × c. Distillation on patch-level is more controllable and we can perform the distillation on the en-tire feature map by aggregating the inter-channel correlation distillation across these patches. In addition, the local spa-tial information can be preserved since each patch can keep the knowledge in speciﬁc region.
In our experiments, we have evaluated our proposed method in different tasks including classiﬁcation (Cifar-100 and ImageNet) and semantic segmentation (Pascal VOC).
The proposed method shows a performance superior to the existing state-of-the-arts methods. To our knowledge, we are the ﬁrst knowledge distillation method that boosts
ResNet18 beyond 72% Top-1 accuracy on ImageNet clas-siﬁcation. And on Pascal VOC, we achieved 3% mIoU im-provement compared to the baseline model.
To summarize, our contributions are:
• We introduce the inter-channel correlation, with the characteristic of being invariant to the spatial dimen-sion, to explore and measure both the feature diversity and homology to help the student for better represen-tation learning.
• We further introduce the grid-level inter-channel cor-relation to make our framework capable of dense pre-diction task, like semantic segmentation.
• To validate the effectiveness of the proposed frame-work, extensive experiments have been conducted on different (a) network architectures, (b) downstream tasks and (c) datasets. Our method consistently outper-forms the state-of-the-arts methods by a large margin across a wide range of knowledge transfer tasks. 2.