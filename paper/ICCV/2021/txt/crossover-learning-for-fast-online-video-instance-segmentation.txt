Abstract
Modeling temporal visual context across frames is crit-ical for video instance segmentation (VIS) and other video understanding tasks. In this paper, we propose a fast on-line VIS model termed CrossVIS. For temporal informa-tion modeling in VIS, we present a novel crossover learn-ing scheme that uses the instance feature in the current frame to pixel-wisely localize the same instance in other frames. Different from previous schemes, crossover learn-ing does not require any additional network parameters for feature enhancement. By integrating with the instance seg-mentation loss, crossover learning enables efﬁcient cross-frame instance-to-pixel relation learning and brings cost-free improvement during inference. Besides, a global bal-anced instance embedding branch is proposed for better and more stable online instance association. We con-duct extensive experiments on three challenging VIS bench-marks, i.e., YouTube-VIS-2019, OVIS, and YouTube-VIS-2021 to evaluate our methods. CrossVIS achieves state-of-the-art online VIS performance and shows a decent trade-off between latency and accuracy. Code is available at https://github.com/hustvl/CrossVIS. 1.

Introduction
Video instance segmentation (VIS) [68] is an emerging task in computer vision that aims to perform per-pixel la-beling of instances within video sequences. This task pro-vides a natural understanding of the video scenes. There-fore achieving accurate, robust, and fast video instance segmentation in real-world scenarios will greatly stimulate the development of computer vision applications, e.g., au-tonomous driving, video surveillance, and video editing.
Recently, signiﬁcant progress has been witnessed in still-image object detection and instance segmentation. How-*Equal contributions. This work was done while Shusheng Yang was interning at Applied Research Center (ARC), Tencent PCG.
†Corresponding author, E-mail: xgwang@hust.edu.cn.
Figure 1. CrossVIS can predict more accurate video instance seg-mentation results (bottom row) compared with the baseline model without crossover learning (top row). ever, extending these methods to VIS remains a challenging work. Similar to other video-based recognition tasks, such as video object segmentation (VOS) [45, 46], video ob-ject detection (VOD) [49] and multi-object tracking (MOT)
[16, 21, 55, 71], continuous video sequences always bring great challenges, e.g., a huge number of frames required to be fast recognized, heavy occlusion, object disappearing and unconventional object-to-camera poses [18].
To conquer these challenges and obtain better perfor-mance on these video understanding tasks (VIS, VOS,
VOD, and MOT), fully utilizing the temporal information among video frames is critical. Previous deep learning based methods on this topic are in four folds. (1) Pixel-level feature aggregation enhances pixels feature of the current frame using other frames, e.g., STM-VOS [42] and STEm-Seg [1] aggregates pixel-level space-time feature based on
Non-local network [57] and 3D convolution, respectively. (2) Instance-level feature aggregation enhances region, pro-posal or instance features across frames, e.g., MaskProp [2] propagates instance features using deformable convolution
[15] for VIS and SELSA [63] fuses instance features us-ing spectral clustering for VOD. (3) Associating instances using metric learning, e.g., MaskTrack R-CNN [68] intro-duces an association head based on Mask R-CNN [24] and
SipMask-VIS [6] adds an adjunctive association head based
on FCOS [53]. (4) Post-processing, e.g., Seq-NMS [23] and
ObjLink [44] reﬁne video object detection results based on dynamic programming and learnable object tubelet linking, respectively.
In this paper, we propose a new scheme for temporal in-formation modeling termed crossover learning. The basic idea is to use the instance feature in the current frame to pixel-wisely localize the same instance in other frames. Dif-ferent from previous pixel/instance-level feature aggrega-tion methods, crossover learning does not require additional network blocks for feature alignment and fusion. It obtains temporal information enhanced features without increasing inference computation cost. Different from metric learning based instance associating methods that require additional metric learning losses, crossover learning is integrated with the instance segmentation loss. Besides, it enables efﬁcient many-to-many relation learning across frames, i.e., the in-stance pixel features are enforced to be close to the pixels that belong to the same instance and far from pixels that be-long to other instances and background. Different from the post-processing methods, crossover learning is end-to-end optimizable with back-propagation.
Since crossover learning is integrated with the instance segmentation loss, it is fully compatible with the other tem-poral information modeling strategies. In this paper, we fur-ther improve the instance association strategy by introduc-ing a global balanced instance embedding learning network branch. Our main contributions are summarized as follows:
• We propose a novel crossover learning scheme that information inherent leverages the rich contextual in videos to strengthen the instance representation across video frames, and weaken the background and instance-irrelevant information in the meantime.
• We introduce a new global balanced instance embed-ding branch to tackle the association problem in video instance segmentation, which yields better and more stable results than previous pair-wise identity mapping approaches.
• We propose a fully convolutional online video instance segmentation model CrossVIS that achieves strong i.e., results on three challenging VIS benchmarks,
YouTube-VIS-2019, OVIS, and YouTube-VIS-2021.
To our knowledge, CrossVIS achieves state-of-the-art performance among all online VIS methods and strikes a good speed-accuracy trade-off. lot to the rapid developments in this ﬁeld. Mask R-CNN
[24] adapts Faster R-CNN [48] with a parallel mask head to predict instance masks, and leads the two-stage fashion for a long period of time. [27, 9, 13] promote Mask R-CNN and achieve better instance segmentation results. The suc-cess of these two-stage models partially is due to the feature alignment operation, i.e., RoIPool [25, 22] and RoIAlign
[24]. Recently, instance segmentation methods based on one-stage frameworks without explicit feature alignment operation begin to emerge [5, 4, 8, 65, 58, 59]. As a repre-sentative, the fully convolutional CondInst [52] outperforms several state-of-the-art methods on the COCO dataset [36], which dynamically generates ﬁlters for mask head condi-tioned on instances. We build our framework on top of [52] and extend it to the VIS task.
Video Instance Segmentation (VIS). VIS requires clas-sifying, segmenting, and tracking visual instances over all frames in a given video. With the introduction of YouTube-VIS-2019 dataset [68], tremendous progresses [19, 56, 37, 17] have been made in tackling this challenging task. As a representative method, MaskTrack R-CNN [68] extends the two-stage instance segmentation model Mask R-CNN with a pair-wise identity branch to solve the instance associa-tion sub-task in VIS. SipMask-VIS [6] follows the similar pipeline based on the one-stage FCOS [53] and YOLACT
[5, 4] frameworks. [38] separates all sub-tasks in VIS prob-lem and designs speciﬁc networks for each of them, all networks are trained independently and combined during inference to generate the ﬁnal predictions. MaskProp [2] introduces a novel mask propagation branch on the multi-stage framework [9] that propagates instance masks from one frame to another. As an ofﬂine method, MaskProp achieves accurate predictions but suffers from high latency.
[32] introduces a modiﬁed variational auto-encoder to solve the VIS task. STEm-Seg [1] treats the video clip as 3D spatial-temporal volume and segments objects in a bottom-up fashion. [29] adopts recurrent graph neural networks for
VIS task. CompFeat [20] reﬁnes features at both frame-level and object-level with temporal and spatial context in-formation. VisTR [60] naturally adopts DETR [7] for VIS task in a query-based end-to-end fashion.
Recently, more challenging benchmarks such as OVIS
[47] and YouTube-VIS-2021 [67] are proposed to further promote the advancement of this ﬁeld. CrossVIS is evalu-ated on three VIS benchmarks and shows competitive per-formances. We hope CrossVIS can serve as a strong base-line to facilitate future research. 2.