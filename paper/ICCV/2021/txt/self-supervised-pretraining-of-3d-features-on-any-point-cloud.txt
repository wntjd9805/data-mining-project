Abstract
Pretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like image recognition, video understanding etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch.
A primary reason is the lack of large annotated datasets be-cause 3D data labelling is time-consuming. Recent work shows that self-supervised learning is useful to pretrain models in 3D but requires multi-view data and point corre-spondences. We present a simple self-supervised pretrain-ing method that can work with single-view depth scans ac-quired by varied sensors, without 3D registration and point correspondences. We pretrain standard point cloud and voxel based model architectures, and show that joint pre-training further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic seg-mentation, and object classification, where they achieve state-of-the-art results. Most notably, we set a new state-of-the-art for object detection on ScanNet (69.0% mAP) and
SUNRGBD (63.5% mAP). Our pretrained models are label efficient and improve performance for classes with few ex-amples. 1.

Introduction
Pretraining visual features on large labeled datasets is a pre-requisite to achieve good performance when access to annotations is limited [27, 46, 52, 87]. More recently, self-supervised pretraining has become a popular alternative to supervised pretraining especially for tasks where annota-tions are time-consuming, such as detection and segmenta-tion in images [9, 36, 37, 56, 93] or tracking in videos [41].
In 3D vision too, annotations are difficult to acquire. Label-ing a 3D scene composed of thousands of 3D points is time-consuming and can take around 22 minutes per scene [18].
This cumbersome annotation process results in a lack of large annotated 3D datasets. However, acquiring 3D data in the form of single-view depth maps has become easier than ever due to the advent of consumer grade depth sen-*Work done during an internship at Facebook.
Figure 1: Label-efficiency of our self-supervised pretraining.
We finetune detection models from scratch or using our pretraining as initialization. Our pretraining which uses unlabeled single-view 3D data, outperforms training from scratch, and achieves the same detection performance with about half the detection labels. sors, e.g., in phones [24, 73, 83]. While these depth maps can be leveraged to pretrain self-supervised 3D features, there is surprisingly little work that can be applied. Recent work [105] applies self-supervised pretraining to 3D mod-els but uses multi-view depth scans with point correspon-dences. Since 3D sensors only acquire single-view depth scans, multi-view depth scans and point correspondences are typically obtained via 3D reconstruction. Unfortunately, even with good sensors, 3D reconstruction can fail easily for a variety of reasons such as non-static environments, fast camera motion or odometry drift [16].
In this paper, we introduce a simple contrastive frame-work, DepthContrast, to representations from single-view depth scans. From a practical perspective, self-supervised learning from single-view depth scans is more broadly ap-plicable for 3D data. It is also an interesting scientific ques-tion whether just using single-view information can provide benefits for self-supervised learning in 3D. Our approach is based on the Instance Discrimination method by Wu et al. [103] applied to depth maps. We side-step the need of registered point clouds or correspondences, by considering each depth map as an instance and discriminating between them, even if they come from the same scene. Since differ-ent 3D applications require different 3D scene representa-1
tions such as voxels for segmentation [17], point clouds for detection [64], we use our method for both voxels and point clouds. We jointly learn features by considering voxels and point clouds of the same 3D scene as data augmentations that are processed with their associated networks [93].
Our contributions can be summarized as follows:
• We show that single-view 3D depth scans can be used for self-supervised learning.
• Our single-view representations perform comparably, or in some settings better than their multi-view counterparts, showing that single-view depth scans are indeed powerful for learning features.
• Our method is applicable across different model architec-tures, indoor/outdoor 3D data, single/multi-view 3D data.
We also show that it can be used to pretrain high capac-ity 3D architectures which otherwise overfit on tasks like detection and segmentation.
• We show that joint training of different input representa-tions like points and voxels is important for learning good representations, and a naive application of contrastive learning may not yield good results.
• We show performance improvements over nine down-stream tasks, and set a new state-of-the-art for two object detection tasks (ScanNet and SUNRGBD). Our models are efficient few-shot learners. 2.