Abstract
Temporal action localization (TAL) in videos is a chal-lenging task, especially due to the large variation in ac-tion temporal scales. Short actions usually occupy a ma-jor proportion in the datasets, but tend to have the low-est performance. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solu-tion dubbed as video self-stitching graph network (VSGN).
We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In
VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale.
We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the comple-mentary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more posi-tive anchors for short actions and more short training sam-ples. Experiments demonstrate that VSGN obviously im-proves the localization performance of short actions as well as achieving the state-of-the-art overall performance on
THUMOS-14 and ActivityNet-v1.3. VSGN code is available at https://github.com/coolbay/VSGN . 1.

Introduction
Nowadays has seen a growing interest in video under-standing both from industry and academia, owing to the rapidly produced video content on the Internet. Temporal action localization (TAL) in untrimmed videos is one im-portant task in this area, which aims to specify the start and the end time of an action as well as to identify its cate-gory. TAL is not only the key technique of various appli-cation such as extracting highlights in sports, but also lays the foundation for other higher-level tasks such as video grounding [10, 13] and video captioning [17, 27].
Though many methods (e.g., [1, 2, 8, 19, 20, 23, 40,
Figure 1. Short actions are the majority in numbers, but have the lowest performance. a) Distribution of action duration in
ActivityNet-v1.3 [6]. Actions are divided into five duration groups (in seconds): XS (0, 30], S (30, 60], M (60, 120], L (120, 180], and XL (180, inf). b) TAL Performance of different methods on actions of different duration. 41, 42, 44]) in recent years have been continuously break-ing the record of TAL performance, a major challenge hin-ders its substantial improvement â€“ large variation in ac-tion duration. An action can last from a fraction of a sec-ond to minutes in the real-world scenario as well as in the datasets [6, 14]. We plot the distribution of action duration in the dataset ActivityNet-v1.3 [6] in Fig. 1 a). We notice that actions shorter than 30 seconds dominate the distribu-tion, but their performance is obviously inferior to longer ones with all different TAL methods (Fig. 1 b)). Therefore, the accuracy of short actions is a key factor to determine the performance of a TAL method.
Why are short actions hard to localize? Short actions have small temporal scales with fewer frames, and there-fore, their information is prone to loss or distortion through-out a deep neural network. Most methods in the litera-ture process videos regardless of action duration, which as a consequence sacrifices the performance of short actions.
Recently, researchers attempt to incorporate feature pyra-mid networks (FPN) [21] from the object detection problem to the TAL problem [23, 26], which generates different fea-ture scales at different network levels, each level with differ-ent sizes for candidate actions. Though by this means short actions may go through fewer pooling layers to avoid be-ing excessively down-scaled, yet their original small scale
as the source of the problem still limits the performance.
Then how can we attack the small-scale problem of short actions? A possible solution is to temporally up-scale videos to obtain more frames to represent an action. Recent literature shows the practice of re-scaling videos via linear interpolation before feeding into a network [2, 19, 20, 42, 46], but these methods actually down-scale rather than up-scale videos (e.g., using only 100 snippets on AcitivityNet-v1.3). Even if we can adapt a method to using a larger-scale input, how can we ensure that the up-scaled videos contain sufficient and accurate information for detecting an action?
Moreover, it makes the problem even harder that re-scaling is usually not performed on original frames, but on video features which do not satisfy linearity.
Up-scaling a video could transform a short action into a long one, but may lose important information for local-ization. Thus both the original scale and the enlarged scale have their limitations and advantages. The original video scale contains the original intact information, while the en-larged one is easier for the network to detect. In contrast to other works that either use the original-scale video or a down-scaled video, in this paper, we use both to take ad-vantage of their complementary properties and mutually en-hance their feature representations.
Specifically, we propose a Video self-Stitching Graph
Network (VSGN) for improving performance of short ac-tions in the TAL problem. Our VSGN is a multi-level cross-scale framework that contains two major components: video self-stitching (VSS); cross-scale graph pyramid net-work (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to ob-tain a larger scale. Then using our self-stitching strategy, we piece together both the original-scale clip and its mag-nified counterpart into one single sequence as the network input. In xGPN, we progressively aggregate features from cross scales as well as from the same scale via a pyramid of cross-scale graph networks. Hence, we enable direct infor-mation pass between the two feature scales. Compared to simply using one scale, our VSGN adaptively rectifies dis-torted features in either scales from one another by learning to localize actions, therefore, it is able to retain more infor-mation for the localization task. In addition to enhancing the features, our VSGN augments the datasets with more short actions to mitigate the bias towards long actions dur-ing the learning process, and enables more anchors, even those with large scales, to predict short actions.
We summarize our contributions as follows: 1) To the best of our knowledge, this is the first work that sheds light on the problem of short actions for the task of temporal action localization. We propose a novel solution that utilizes cross-scale correlations of multi-level features to strengthen their representations and facilitate localiza-tion. 2) We propose a novel temporal action localization frame-work VSGN, which features two key components: video self-stitching (VSS); cross-scale graph pyramid network (xGPN). For effective feature aggregation, we design a cross-scale graph network for each level in xGPN with a hybrid module of a temporal branch and a graph branch. 3) VSGN shows obvious improvement on short actions over other concurrent methods, and also achieves new state-of-the-art overall performance. On THUMOS-14, VSGN reaches 52.4% mAP@0.5, compared to previous best score 40.4% under the same features. On ActivityNet-v1.3,
VSGN reaches an average mAP of 35.07%, compared to the previous best score 34.26% under the same features. 2.