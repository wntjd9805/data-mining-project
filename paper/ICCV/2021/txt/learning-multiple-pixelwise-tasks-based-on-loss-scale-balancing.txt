Abstract
We propose a novel loss weighting algorithm, called loss scale balancing (LSB), for multi-task learning (MTL) of pix-elwise vision tasks. An MTL model is trained to estimate multiple pixelwise predictions using an overall loss, which is a linear combination of individual task losses. The pro-posed algorithm dynamically adjusts the linear weights to learn all tasks effectively. Instead of controlling the trend of each loss value directly, we balance the loss scale — the product of the loss value and its weight — periodically.
In addition, by evaluating the difficulty of each task based on the previous loss record, the proposed algorithm focuses more on difficult tasks during training. Experimental re-sults show that the proposed algorithm outperforms conven-tional weighting algorithms for MTL of various pixelwise tasks. Codes are available at https://github.com/jaehanlee-mcl/LSB-MTL. 1.

Introduction
Multi-task learning (MTL) is a machine learning tech-nique to solve multiple learning tasks simultaneously while exploiting commonalities and differences across tasks.
MTL can improve the learning efficiency and prediction ac-curacy for related tasks using an integrated model, as com-pared to separate models trained for the multiple tasks inde-pendently. The effectiveness of MTL has been proven both theoretically and experimentally [1–4, 11]. With the ad-vances in deep learning, MTL has been employed in a wide range of applications, such as computer vision [14, 19, 32], natural language processing (NLP) [7, 10, 28], reinforce-ment learning [12,17,35], and speech recognition [8,16,39].
In computer vision, MTL models using convolutional neural networks (CNNs) have been proposed mainly. For example, MTL models have been developed to jointly per-form three pixelwise tasks of depth estimation, surface nor-mal estimation, and semantic segmentation from an input image [25, 46]. Also, the joint depth and motion estima-tion from a video [6, 44] and the classification of an image for multiple attributes [30, 37] have been studied. These re-searches have advanced the learning of closely related tasks using a single MTL model. Since a large part of the network is shared between tasks, an MTL model is advantageous in terms of complexity, inference time, and learning efficiency.
To develop an effective deep MTL algorithm, two fac-tors should be considered: architecture and training scheme.
First, the architecture should be designed to learn both task-across and task-specific representations by allocating net-work parameters appropriately for shared and task-specific purposes. Many architectures have been developed by con-sidering various factors, such as the CNN capacity, data type, and relationship between tasks [12, 29, 34]. Second, a training scheme should discourage any bias toward a spe-cific task [6,18,25]. Because an MTL model generates mul-tiple estimates, the corresponding losses should be defined and then combined to form an overall loss. Since each loss has a different scale during training, the overall loss may be dominated by a specific loss. Moreover, the losses may vary in different directions or even fluctuate during training.
It is hence important to balance loss contributions and thus enforce that MTL learns all tasks effectively.
In this paper, we propose a novel loss weighting al-gorithm, called loss scale balancing (LSB), for MTL of pixelwise computer vision tasks (e.g. depth estimation and semantic segmentation), which can dynamically adjust weights to learn all tasks effectively. Based on the obser-vation that balancing the loss scale, which is the product of a loss value and its weight, is more efficient than control-ling the trend of each loss value directly, we adjust the loss scales to be balanced periodically. Besides, by evaluating the difficulty of each task using the previous loss record, the proposed algorithm can focus more on difficult tasks. Ex-perimental results demonstrate that the proposed algorithm can successfully balance loss scales to improve the perfor-mance of each task. The proposed LSB algorithm outper-forms conventional weighting algorithms.
The main contributions of this paper are as follows:
• We propose balancing loss scales, instead of losses themselves, to improve the performance of MTL of pixelwise vision tasks.
• We further improve the performance by tuning loss
scales through the assessment of task difficulties.
• It is shown experimentally that the proposed algorithm outperforms conventional ones consistently, regardless of MTL architecture, dataset, and encoder backbone. 2.