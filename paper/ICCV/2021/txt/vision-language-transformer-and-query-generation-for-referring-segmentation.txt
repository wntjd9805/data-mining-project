Abstract
In this work, we address the challenging task of referring segmentation. The query expression in referring segmen-tation typically indicates the target object by describing its relationship with others. Therefore, to find the target one among all instances in the image, the model must have a holistic understanding of the whole image. To achieve this, we reformulate referring segmentation as a direct attention problem: finding the region in the image where the query language expression is most attended to. We introduce transformer and multi-head attention to build a network with an encoder-decoder attention mech-anism architecture that “queries” the given image with the language expression.
Furthermore, we propose a
Query Generation Module, which produces multiple sets of queries with different attention weights that represent the diversified comprehensions of the language expression from different aspects. At the same time, to find the best way from these diversified comprehensions based on visual clues, we further propose a Query Balance Module to adaptively select the output features of these queries for a better mask generation. Without bells and whistles, our approach is light-weight and achieves new state-of-the-art performance consistently on three referring segmen-tation datasets, RefCOCO, RefCOCO+, and G-Ref. Our code is available at https://github.com/henghuiding/Vision-Language-Transformer. 1.

Introduction
Referring segmentation targets to generate segmentation mask for the target object referred by a given query expres-sion in natural language [10, 16, 15, 3]. As the referring segmentation involves both natural language processing and computer vision, it is considered as one of the most fundamental and challenging multi-modal tasks. With the recent success of learning methods, a lot of deep-*Equal contribution
Figure 1. Our method detects multiple emphasis or understanding ways for one language expression, and produces a query vector for each of them. We use each vector to “query” the image, generating a response to each query. Then the network selectively aggregates these responses, in which queries that provide better comprehensions are spotlighted. learning-based works are proposed in this area and have achieved remarkable performance. However, there are still many challenges left in this task. The objects in images of referring segmentation are correlated in a complicated manner while the query expression frequently indicates the target object by describing the relationships with others, which requires a holistic understanding on the image and language expression. Another challenge is caused by the varieties of objects/images as well as the unrestricted expression of languages, which brings a high degree of randomness.
Firstly, to address the challenge of complicated corre-lations in the given image and language, we explore to enhance the holistic understanding of multi-modal features by building the network with global operations, in which direct interactions are modeled among all elements (e.g., pixel-pixel, word-word, pixel-word). Currently, the Fully
Convolutional Network (FCN)-like framework [17, 5, 6, 4] is widely used in referring segmentation methods [10, 22].
They usually perform convolution operations on the fused (e.g., concatenated) vision-language features to generate the target mask. However, the long-range dependencies modeling in regular convolution operation is indirect, as
In recent years, its large receptive field is typically achieved by stacking small-kernel convolutions. This oblique process brings in-efficiencies to information interactions among pixels/words in a distance [27], thus is undesirable for referring seg-mentation models to understand the global context of the image [28]. the attention mechanism is gaining respectable popularity in the computer vision community for its advantage in building direct interaction among all elements, which greatly helps the model in capturing the global semantic information. Some previous referring segmentation works also use attention to alleviate the long-range dependency issues [28, 11, 24]. However, most of them only utilize the attention mechanism as auxiliary modules based on the FCN-like pipeline, which limits their ability to model the global context.
In this work, we reformulate the referring segmentation problem in terms of attention and reconstruct the current FCN-like framework with Transformer [25]. We generate a set of query vectors from language features using vision-guided attention, and use these vectors to “query” the given image and generate the segmentation mask from the responses, as shown in Fig. 1. This attention-based framework enables us to implement global operation among multi-modal features in every stage of computation, making the network better at modeling the global context of both vision and language information.
For these methods,
Secondly, to deal with the randomness caused by the varieties of objects/images and the unconstrained expres-sion of languages, we propose to comprehend the language expression in different ways incorporating with vision fea-tures.
In many previous referring segmentation methods, such as [19, 29], the language self-attention is often used to extract the most informative part and emphasized word(s) their in the language expression. linguistic understanding is derived alone from the language expression itself without interacting with the image, so that they cannot distinguish which emphasis is more suitable and effective that better fit a particular image. Thus their detected emphases might be inaccurate or inefficient. On the other hand, in most previous vision-transformer works, queries for the transformer decoders are usually a set of fixed learned vectors, each of which is used to predict one object. Experiments show that each query vector has its own operating modes and specializes in certain kinds of objects [1].
In these works with fixed queries, there must imply a hypothesis that objects in the input image are distributed under some statistic rules, which does not match the randomness of referring segmentation. To address these issues, we propose a Query Generation Module (QGM) to produce multiple different query vectors based on the language, and with the aid of vision features. Each vector comprehends the language expression in its own way. With the proposed QGM, we improve the diversity of ways to understanding the image and query language, enhancing the network’s robustness in dealing with highly random inputs.
At the same time, to ensure the generated query vectors are valid and find the more suitable comprehension ways to the image and language, we further propose a Query Balance
Module to adaptively select the output features of these queries for a better mask generation.
Our approach builds deep interactions between language and vision features at different levels, greatly enhancing the fusion and utilization of multi-modal features. Besides, the proposed module is lightweight and its parameter size is roughly equivalent
In to seven convolution layers. summary, our main contributions are listed as follows:
• We design a Vision-Language Transformer (VLT) method to build deep interactions among multi-modal information and enhance the holistic understanding to vision-language features.
• We propose a Query Generation Module that un-derstands the language from different comprehension ways, and a Query Balance Module to focus on the suitable ways.
• The proposed method achieves new state-of-the-art on multiple datasets consistently, especially on hard and complex datasets. 2.