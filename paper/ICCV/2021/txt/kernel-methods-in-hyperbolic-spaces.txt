Abstract
Embedding data in hyperbolic spaces has proven ben-eﬁcial for many advanced machine learning applications such as image classiﬁcation and word embeddings. How-ever, working in hyperbolic spaces is not without difﬁcul-ties as a result of its curved geometry (e.g., computing the
Frechet mean of a set of points requires an iterative algo-rithm). Furthermore, in Euclidean spaces, one can resort to kernel machines that not only enjoy rich theoretical prop-erties but that can also lead to superior representational power (e.g., inﬁnite-width neural networks). In this paper, we introduce positive deﬁnite kernel functions for hyper-bolic spaces. This brings in two major advantages, 1. ker-nelization will pave the way to seamlessly beneﬁt from ker-nel machines in conjunction with hyperbolic embeddings, and 2. the rich structure of the Hilbert spaces associated with kernel machines enables us to simplify various opera-tions involving hyperbolic data. That said, identifying valid kernel functions on curved spaces is not straightforward and is indeed considered an open problem in the learning community. Our work addresses this gap and develops sev-eral valid positive deﬁnite kernels in hyperbolic spaces, in-cluding the universal ones (e.g., RBF). We comprehensively study the proposed kernels on a variety of challenging tasks including few-shot learning, zero-shot learning, person re-identiﬁcation and knowledge distillation, showing the supe-riority of the kernelization for hyperbolic representations. 1.

Introduction
This paper proposes a family of positive deﬁnite (pd) kernels to map the representations in hyperbolic spaces into
Reproducing Kernel Hilbert Spaces (RKHSs), which en-ables us to seamlessly beneﬁt from kernel machines to ana-lyze hyperbolic spaces.
In the machine learning community, the Euclidean space has been the “workhorse” for feature embeddings. This is mainly because the high-dimensional vector space is a natu-ral generalization from the familiar three-dimensional space we live in and performing basic operations for comparison (e.g., calculating distances and similarities) is straightfor-ward. However, embedding in Euclidean spaces can harm and distort the encoding of structured data, thereby losing the complex geometric information inherently present in the data. For example, the Euclidean space fails to encode the hierarchical information in graph-structured data [38].
Several recent studies in computer vision suggest that embedding images and video using hyperbolic geometry can be beneﬁcial compared to the common practice of us-ing Euclidean geometry. This includes tasks such as textual entailment [18], image classiﬁcation and retrieval [32], and graph classiﬁcation [38] to name a few.
The hyperbolic space is characterized by a constant neg-ative sectional curvature (in contrast to the ﬂat structure of the Euclidean space), and does not satisfy Euclid’s parallel postulate. One intriguing property of hyperbolic spaces is their capacity of encoding hierarchical data, as the volume of hyperbolic space expands exponentially [22], thereby increasing their representation power. Although several studies have successfully employed the hyperbolic geom-etry for inference [18, 32, 8], the difﬁculties of working with such non-linear spaces still overwhelm their wider use. For example, while averaging in Euclidean geometry is straightforward, its counterpart in hyperbolic space is ap-proximated by the Frechet mean. Computing the Frechet mean requires an iterative algorithm and could easily be-come costly [31, 40]. This motivates us to develop kernels to make it possible to seamlessly beneﬁt and employ kernel machines towards analyzing hyperbolic data.
To be able to make use of kernel machines, one needs to have a pd kernel function at its disposal. Loosely speaking, a kernel function is a measure of similarity. Many famil-iar kernels in the Euclidean space are deﬁned as functions of the Euclidean distance (which is indeed the geodesic distance of the space). Take the RBF kernel k(x, y) = exp(−ξd2(x, y)) as an example. This might imply that valid pd kernels in curved spaces, the hyperbolic space being one, can be constructed once the geodesic distance is known. Unfortunately, this is not the case as shown in [30, 15] (c.f ., theorem 6.2 in [30]), because such curved
Table 1. Summary of the proposed positive deﬁnite kernels in hyperbolic spaces and their properties.
Kernel
Formulation: k(zi, zj)
√ fD(z) = tanh−1( c(cid:107)z(cid:107)) c(cid:107)z(cid:107) , c > 0 and z ∈ Dn z√ c
Condition
Properties
Hyperbolic tangent kernel
Hyperbolic RBF kernel
Hyperbolic Laplace kernel
Generalized Hyperbolic Laplace kernel
Hyperbolic binomial kernel ktan(zi, zj) = (cid:104)fD(zi), fD(zj)(cid:105) krbf (zi, zj) = exp (cid:0) − ξ(cid:107)fD(zi), fD(zj)(cid:107)2(cid:1) klap(zi, zj) = exp (cid:0) − ξ(cid:107)fD(zi), fD(zj)(cid:107)(cid:1) kglap(zi, zj) = exp (cid:0) − ξ(cid:107)fD(zi), fD(zj)(cid:107)2α(cid:1) kbin(zi, zj) = (cid:0)1 − (cid:104)fD(zi), fD(zj)(cid:105)(cid:1)−α
-ξ > 0
ξ > 0
ξ > 0, 0 < α < 1
α > 0 pd pd, universal pd, universal pd, universal pd, universal spaces are not isometric to ﬂat Euclidean spaces.
Inter-estingly, the difﬁculty of deﬁning pd kernels on curved spaces is now considered an open problem in machine learning [14].
In this paper, we address the design challenge of pd kernels for hyperbolic representations using the Poincar´e model. Here, we propose several valid pd hyperbolic ker-nels, including the powerful universal ones. To this end, we
ﬁrst make use of a lemma to construct a valid linear-like kernel. Leveraging this lemma, we further deﬁne valid RBF and Laplace kernels for the hyperbolic geometry. Finally, we propose the binomial kernel. Table 1 summarizes the proposed kernels. The contributions of this work include:
• We propose four pd kernels for the hyperbolic spaces, namely, the hyperbolic tangent kernel, the hyperbolic
RBF kernel, the hyperbolic Laplace and the hyperbolic binomial kernel, in conjunction with their theoretical analysis. To the best of our knowledge, this is the ﬁrst work to develop pd kernels in hyperbolic spaces.
• To evaluate the power of the proposed kernels, we con-duct thorough experiments on various vision tasks in-cluding few-shot learning, zero-shot learning, person re-identiﬁcation, and knowledge distillation, and em-ploy the kernels along deep neural networks (DNNs) to attain rich models for inference. Empirically, we ob-served the superiority of the kernelization for the rep-resentation learning in hyperbolic spaces. 2.