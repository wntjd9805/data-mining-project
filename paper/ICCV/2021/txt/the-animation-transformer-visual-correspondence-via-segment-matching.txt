Abstract
Visual correspondence is a fundamental building block on the way to building assistive tools for hand-drawn ani-mation. However, while a large body of work has focused on learning visual correspondences at the pixel-level, few ap-proaches have emerged to learn correspondence at the level of line enclosures (segments) that naturally occur in hand-drawn animation. Exploiting this structure in animation has numerous benefits: it avoids the memory complexity of pixel attention over high resolution images and enables the use of real-world animation datasets that contain correspondence information at the level of per-segment colors. To that end, we propose the Animation Transformer (AnT) which uses a
Transformer-based architecture to learn the spatial and vi-sual relationships between segments across a sequence of images. By leveraging a forward match loss and a cycle consistency loss our approach attains excellent results com-pared to state-of-the-art pixel approaches on challenging datasets from real animation productions that lack ground-truth correspondence labels. 1.

Introduction
Hand-drawn animation has been around for over 100 years and is one of the most popular mediums of digi-tal entertainment today. Though the advent of drawing tablets and digital software have made the process of cre-ating hand-drawn animation substantially easier, it is still a highly manual process that involves drawing and editing each individual frame. Many of these tasks lie in the grey area between repetitively algorithmic processes and artistic choices, opening the door for new assistive tools that aug-ment artists’ workflows.
Existing commercial tools have applied heuristic algo-rithms in this domain with limited results, usually requir-ing artists to work in vector format or use complex char-acter rigging that removes the hand-drawn feel of the final product. Deep learning approaches, on the other hand, can act directly on top of raw pixel input but cannot scale eas-ily to HD resolutions and fail to properly exploit the struc-ture of hand-drawn animation drawings – specifically, the smaller line enclosures (segments) which can be extracted by a flood-fill or morphological algorithm.
In this paper, we focus on the task of learning visual correspondence across sequences of raster animation line drawings. This is a fundamental building block for build-ing assistive animation tools for tasks such as coloring, in-betweening, and texturing which make up a large portion of the tedious, non-creative work in the animation pipeline.
With correspondence information an animator can color or texture a few frames in a sequence and propagate the col-ors through the rest of the images, saving hours of manual labor. New in-between frames can be generated by morph-ing neighboring frames with correspondence information, which can reduce the amount of line drawings needed to make smooth looking motion.
Despite demand for a data-driven solution to the cor-respondence problem, little progress has been made be-cause of the difficult design requirements and lack of available data with correspondence labels. Suitable ap-proaches should (i) operate on raster input and scale to HD (1920×1080) and above resolutions; (ii) produce correspon-dences on the level of segments; (iii) be able to handle com-plex real-world animation; (iv) be trainable using colorized images as supervision; (v) be fast enough for interactive ap-plications.
In this paper, we propose the Animation Transformer (AnT) to address these issues. Unlike pixel-based video tracking methods which suffer from the intractability of computing attention over a large number of pixels, AnT operates over the line-enclosed segments (see Figure 2) in the line image and uses a Transformer-based architecture to learn the spatial and visual relationships between segments.
By operating on this representation AnT avoids the need to directly process HD images in their entirety and is both compute and memory efficient, scaling to 4K images and beyond. We optimize AnT with a forward matching loss and a cycle consistency loss that enables it to be trained on real-world animation datasets without full ground-truth cor-respondence labels.
Figure 2: Given an input image I each crop Ci is obtained by placing a bounding box around the center of each enclo-sure of I and resizing it to a common size.
We conduct extensive experiments to show our model’s effectiveness in a variety of settings. When trained on ground-truth correspondence labels generated from 3D ren-dering software, AnT demonstrates a large improvement over a strong pixel-based baseline even after domain spe-cific improvements are added to the baseline. When AnT is trained solely on colorized images from a real-world anima-tion dataset, its performance approaches that of the model trained on ground-truth correspondence labels – showing that AnT is not bounded by the availability of large datasets with correspondence labels. While AnT has broad appli-cability in animation, we highlight its potential as a creative tool through showcasing results on guided colorization from reference images. 2.