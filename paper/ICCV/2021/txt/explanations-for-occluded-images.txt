Abstract
Existing algorithms for explaining the output of image classifiers perform poorly on inputs where the object of interest is partially occluded. We present a novel, black-box algorithm for computing explanations that uses a principled approach based on causal theory. We have implemented the method in the DEEPCOVER tool. We obtain explanations that are much more accurate than those generated by the existing explanation tools on images with occlusions and observe a level of performance comparable to the state of the art when explaining images without occlusions. 1.

Introduction
Deep neural networks (DNNs) are now a primary building block of many computer vision systems. DNNs are complex non-linear functions with algorithmically generated (and not engineered) coefficients. In contrast to traditionally engi-neered image processing pipelines it is difficult to retrace how the pixel data are interpreted by the layers of the DNN.
This “black box” nature of DNNs creates demand for tech-niques that explain why a particular input yields the output that is observed.
An explanation of an output of an automated procedure is essential in many areas, including verification, planning, diagnosis and the like. A good explanation can increase a user’s confidence in the result. Explanations are also useful for determining whether there is a fault in the automated
*The work reported in this paper was done prior to joining Amazon. procedure: if the explanation does not make sense, it may indicate that the procedure is faulty. It is less clear how to define what a good explanation is. There have been a number of definitions of explanations over the years in various do-mains of computer science [3, 11, 23], philosophy [16] and statistics [26]. The recent increase in the number of machine learning applications and the advances in deep learning led to the need for explainable AI, which is advocated, among others, by DARPA [12] to promote understanding, trust, and adoption of future autonomous systems based on learning algorithms (and, in particular, image classification DNNs).
DARPA provides a list of questions that a good explanation should answer and an epistemic state of the user after receiv-ing a good explanation. The description of this epistemic state boils down to adding useful information about the out-put of the algorithm and increasing trust of the user in the algorithm.
Explanations for the results of image classifiers are typi-cally based on or are given in the form of a ranking of the pixels, which is a numerical measure of importance: the higher the score, the more important the pixel is for the
DNN’s classification outcome.
A user-friendly explanation can then be a subset of highest-ranked pixels that is sufficient for the original clas-sification outcome. Given an image that features an object, good algorithms are able to generate rankings that identify that object with a high accuracy. Another typical proxy for the quality of a ranking is how many of the high-ranked pixels (constituting an explanation) need to be masked be-fore the classification generated by the DNN changes. Good
explanations require very little masking.
In the absence of further analysis, the space of possible orderings (and hence rankings) is exponential in the size of the image, and the brute-force approach is therefore im-practical. Moreover, as we argue in this paper, the problem is NP-complete. It is therefore expected that explanation-generating algorithms approximate the solution using heuris-tics tuned for image classification. This assumption is en-tirely appropriate in many use cases, and in particular, works very well on the benchmark sets that are used in the area: the existing work has been evaluated using the ImageNet dataset and ImageNet has been curated so are all objects clearly visible. Consequently, the explanations that are generated are usually contiguous.
We argue that there is a use-case for explanations of the results of image classifiers for images where the trigger for the result is not contiguous. Obvious exemplars are images with partial occlusion, say by a person walking into a scene or simply by dirt on your camera lens. To quantify the quality of the explanations for such images objectively, we introduce a new image dataset we call Photo Bombing, in which we obscure ImageNet photos by masking parts of the object.
The difference between the modified image and the original one is the ground truth for the “photobomber”, and a good explanation has no overlap with it.
We observe that the existing methods for explaining the outcome of image classifiers perform poorly on such inputs.
To address this problem, we introduce a new algorithm that is grounded in causal theory. Our algorithm is iterative and highly parallelizable and delivers significantly better accuracy on an existing dataset with partial occlusion and on our own photo bombing data set. The tool, the new benchmark set, and the full set of results are available at https://www.cprover.org/deepcover/. 2.