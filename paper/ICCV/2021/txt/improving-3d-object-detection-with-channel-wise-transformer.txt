Abstract
Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Pre-vious works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set ab-straction and multi-scale feature fusion to produce pow-erful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal’s keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key in-teraction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate ob-ject predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors. 1.

Introduction 3D object detection from point clouds is envisioned as an indispensable part of future Autonomous Vehicle (AV).
*This work was done when the author was visiting Alibaba as a re-search intern. The code is available at https://github.com/hlsheng1/CT3D
†Corresponding author.
Unlike the developed 2D detection algorithms whose suc-cess is mainly due to the regular structure of image pix-els, LiDAR point clouds are usually sparse, unordered and unevenly distributed. This makes the CNN-like operations not well suited to process unstructured point clouds directly.
To tackle these challenges, many approaches employ vox-elization or custom discretization for point clouds. Several methods [28, 15] project point clouds to a birds-eye view (BEV) representation and apply the standard 2D convolu-tions, however, it will inevitably sacrifice certain geometric details which are vital for generating accurate localization.
Other methods [3, 33] rasterize point clouds into a 3D voxel grid and use regular 3D CNNs to perform computation in grid space, but this category of methods suffers from com-putational bottleneck associated with making the grid finer.
A major breakthrough in detection task on point clouds is due to the effective deep architectures for point clouds rep-resentation such as volumetric convolution [33] and permu-tation invariant convolution [22].
Recently, most state-of-the-art methods for 3D object detection adopt a two-stage framework consisting of 3D region proposal generation and proposal feature refine-ment. Notice that the most popular region proposal network (RPN) backbone [33] has achieved over 95% recall rate on the KITTI 3D Detection Benchmark, whereas this method only achieves 78% Average Precision (AP). The reason for such a gap stems from the difficulty in encoding an object and extracting the robust feature from 3D proposals in cases of occlusion or long-range distance. Therefore, how to ef-fectively model geometric relationships among points and exploit accurate position information during the proposal feature refinement stage is crucial for good performance.
An important family of models is PointNet [22] and its vari-ants [23, 19, 25], which use a flexible receptive field to ag-gregate features by local regions and permutation-invariant network. However, these methods have the drawback of involving plenty of hand-crafted designs, such as the neigh-bor ball radii and the grid size. Another family of models
is the voxel-based methods [33, 27, 39] which use 3D con-volutional kernels to gather information from neighboring voxels. But the performance of such methods is not opti-mal caused by the voxel quantization and sensitive to hyper-parameters. Later studies [43, 24, 4, 10] further apply the point-voxel mixed strategy to capture multi-scale features while retaining fine-grained localization but are strongly tied to the specific RPN architectures.
In this paper, we make two major contributions. First, we propose a novel end-to-end two-stage 3D objection detection framework called CT3D. Motivated by the re-cent Transformer-based 2D detection method DETR [1] that uses CNN backbone to extract features and encoder-decoder Transformer to enhance the RoI region features, we design our CT3D to generate 3D bounding boxes at the first stage, then learn per-proposal representation by incorporat-ing a novel Transformer architecture with channel-wise re-weighting mechanism in decoder. The proposed framework exhibits very strong performance in terms of accuracy and efficiency, and thus can be conveniently combined with any high-quality RPN backbones.
The second contribution is the custom Transformer that offers several benefits over the traditional point/voxel-based feature aggregation mechanism. Despite the point-wise or voxel convolutions have the ability of local and global con-text modelling, there still have been several limitations in increasing receptive field and parameter optimization.
In addition, point-cloud based 3D object detectors also have to deal with the challenging missing/noisy detections such as occlusion and distancing patterns with a few points. Self-attention in Transformers has recently emerged as a basic building block for capturing long-range interactions thus is a natural choice in acquiring context information for en-riching the faraway objects or increasing the confidence of
Inspired by this idea, we initially intro-false negatives. duce a proposal-to-point embedding to effectively encode the RPN proposal information in the encoder module. Fur-thermore, we exploit a channel-wise re-weighting approach to augment the standard Transformer decoder in considera-tion of both global and local channel-wise features for the encoded points. The purpose is to scale the feature decod-ing space where we can compute attention distribution over each channel dimension of key embeddings thus can en-hance the expressiveness of query-key interactions. Exten-sive experiments show that our proposed CT3D can out-perform the state-of-the-art published methods on both the
KITTI dataset and the large-scale Waymo dataset. 2.