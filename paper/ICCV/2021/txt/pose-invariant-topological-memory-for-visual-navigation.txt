Abstract
Planning for visual navigation using topological mem-ory, a memory graph consisting of nodes and edges, has been recently well-studied. The nodes correspond to past observations of a robot, and the edges represent the reacha-bility predicted by a neural network (NN). Most prior meth-ods, however, often fail to predict the reachability when the robot takes different poses, i.e. the direction the robot faces, at close positions. This is because the methods ob-serve ﬁrst-person view images, which signiﬁcantly changes when the robot changes its pose, and thus it is funda-mentally difﬁcult to correctly predict the reachability from them.
In this paper, we propose pose invariant topologi-cal memory (POINT) to address the problem. POINT ob-serves omnidirectional images and predicts the reachability by using a spherical convolutional NN, which has a rota-tion invariance property and enables planning regardless of the robot’s pose. Additionally, we train the NN by con-trastive learning with data augmentation to enable POINT to plan with robustness to changes in environmental condi-tions, such as light conditions and the presence of unseen objects. Our experimental results show that POINT outper-forms conventional methods under both the same and differ-ent environmental conditions. In addition, the results with the KITTI-360 dataset show that POINT is more applicable to real-world environments than conventional methods. 1.

Introduction
Planning for visual navigation has been widely studied.
Typically, a geometric map of an environment is constructed by visual simultaneous localization and mapping (vSLAM)
[33, 23], and a path to a goal is planned by using a plan-ning algorithm such as A* search [31]. However, in prac-tical cases, vSLAM requires much effort to obtain a geo-metrically accurate map, such as combination with high-precision odometry or global navigation satellite system
Figure 1. Overview of topological memory based planning. A robot moves around an environment to collect data and train an edge predictor with the data. Then, a topological memory is con-structed on the basis of the edge predictor. The topological mem-ory has nodes sampled from collected data and edges connected between nodes if the nodes are predicted to be close (thus reach-able) by the edge predictor. Finally, given current observations oc and goal observations og, planning is performed on the topological memory using an algorithm such as the Dijkstra algorithm [27]. (GNSS) sensors, and map modiﬁcation and parameter tun-ing by an expert. On the other hand, a number of works have proposed planning methods using topological memory
[11, 32, 9, 24, 26, 36, 3], which is a memory graph where each node corresponds to a past observation of a robot and the edge between two nodes represents their reachability, i.e. whether the positions corresponding to the nodes are physically close or distant. In particular, recent works, such as semi-parametric topological memory (SPTM) [32] and hallucinative topological memory (HTM) [26], leverage a neural network (NN) to predict the reachability. We call such an NN an edge predictor. These methods do not aim to build a geometrically accurate map, and thus do not re-quire any combination with other high-precision sensors or map modiﬁcation and parameter tuning by an expert. Since it is to be desired that visual navigation can be easily per-formed without such requirements, we focus on topological memory based methods. The overview of planning based on topological memory is shown in Figure 1.
Previous topological memory based planning methods, however, have a difﬁculty; the edge predictor often fails prediction given observations with different poses. To ex-plain the reason, we illustrate the procedure of the topologi-1st-person view
Feature map space predicted as FAR node
Conventional Methods
Past trajectory time step omnidirectional view
CNN
CNN
SCNN
SCNN
Predictor function
Edge predictor
POINT (Ours)
Topological memory predicted as CLOSE node
Predictor function
= Similarity
Edge predictor (a) Topological memory construction of conventional methods and POINT (b) Edge predictor training of POINT
Figure 2. (a) Illustration of topological memory construction of the conventional methods and POINT (our method). The left image represents the bird’s eye view image of the environment and the past trajectory of the robot where the color represents a time step. The green and blue arrows represent the positions and directions of the robot. The upper and bottom rows show the conventional methods and
POINT, respectively. (b) Overview of edge predictor training of POINT . We use contrastive learning with data augmentation. cal memory construction of the conventional methods in the top row of Figure 2(a). The left image represents the past trajectory of the robot. While the robot positions, shown as positions of the green and blue arrows in the left image, are close and thus supposed to be predicted as reachable, the robot poses, shown as the direction of the arrows, are differ-ent. The conventional methods use ﬁrst-person view images as observations and convolutional NNs (CNNs) as the edge predictor. Since ﬁrst-person view images observed at the green and blue arrows are totally dissimilar because their poses are different, the feature maps extracted by the CNNs are also dissimilar (see “Feature map space” in Figure 2(a)).
Therefore, it is fundamentally difﬁcult to correctly predict that the nodes corresponding to the blue and green arrows are close. This mis-prediction, moreover, leads to a fail-ure of localization and planning when the robot takes the pose it has never taken. To deal with the problems by the mis-prediction of the reachability due to the dissimilarity of the feature maps, the conventional methods exploit a train-able function, which is referred to as a predictor function in this paper, that maps two feature maps to the reachability from one to another, such as ones implemented as fully con-nected (FC) layers at the top of the siamese network [15], to implement the edge predictor functions. However, such a predictor function increases the risk of incorrectly pre-dicting dissimilar images observed at distant points as ones observed at close points, as depicted by the pink line in the topological memory in Figure 2(a).
To address the difﬁculty, we propose pose invariant topological memory (POINT). The bottom row of Fig-ure 2(a) shows an overview of the topological memory con-struction of POINT. POINT observes omnidirectional view images and uses an edge predictor consisting of spherical
CNNs (SCNNs) [5] and a cosine similarity predictor func-tion. Thanks to a rotation invariance property of SCNNs, the feature maps corresponding to observations at close po-sitions are similar even if the robot takes different poses, as shown by the green and blue arrows in Figure 2(a). Hence, the corresponding nodes can be easily predicted to be close by simply determining the similarity between feature maps.
Besides, the predictor function outputting the similarity of feature maps rarely predicts dissimilar images observed at distant positions to be close. In addition, considering more practical situations, we attempt to achieve robust planning to environmental condition changes, e.g. shadow direction, light conditions, and the presence of unseen objects such as pedestrians. To obtain the robustness, we train the edge predictor by contrastive learning with data augmentation [4] that learns to obtain similar features from randomly trans-formed images, as shown in Figure 2(b).
In our experi-ments, we show that POINT outperforms the conventional methods under both the same and different environmen-tal conditions. In addition, the results with the KITTI-360 dataset [37] indicate that POINT is more applicable to real-world environments than conventional methods. 2.