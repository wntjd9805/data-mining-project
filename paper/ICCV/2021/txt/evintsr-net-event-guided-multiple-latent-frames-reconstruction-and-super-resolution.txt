Abstract
An event camera detects the scene radiance changes and sends a sequence of asynchronous event streams with high dynamic range, high temporal resolution, and low latency.
However, the spatial resolution of event cameras is limited as a trade-off for these outstanding properties. To recon-struct high-resolution intensity images from event data, we propose EvIntSR-Net that converts Event data to multiple latent Intensity frames to achieve Super-Resolution on in-tensity images in this paper. EvIntSR-Net bridges the do-main gap between event streams and intensity frames and learns to merge a sequence of latent intensity frames in a recurrent updating manner. Experimental results show that EvIntSR-Net can reconstruct SR intensity images with higher dynamic range and fewer blurry artifacts by fusing events with intensity frames for both simulated and real-world data. Furthermore, the proposed EvIntSR-Net is able to generate high-frame-rate videos with super-resolved frames. 1.

Introduction
Event cameras with bio-inspired silicon retina sensors work radically different from conventional frame-based cameras. The unconventional sensor design enables them to measure scene radiance changes in an asynchronous man-ner [12, 31], instead of capturing images at a ﬁxed frame rate. Event cameras detect brightness changes in a scene in log scale, and send a stream of event data that are binary-signed recordings of brightness changes (“+1” for bright-ness increase and “-1” for brightness decrease). The partic-ular properties of event sensors include: very high dynamic range (HDR, up to 140 dB), high temporal resolution (in the order of µs), low latency, and low power consumption. The latest Dynamic and Active Pixel Vision Sensor (DAVIS [4]) combines a conventional Active Pixel Sensor (APS) with
∗Corresponding author: shiboxin@pku.edu.cn
Figure 1: An example of 4× super-resolution results from eSL-Net [40] and our reconstruction on real-world data.
Both of them take APS and event data as inputs. the event sensor, which can capture intensity frames simul-taneously with event data. However, most of the available event cameras bear low spatial resolution (e.g., 240 × 180 for DAVIS240, 346 × 260 for DAVIS346) partially due to the consideration of data transmission efﬁciency.
Event data contains visual information that can be uti-lized for reconstructing high-quality intensity images. Pre-vious reconstruction approaches [32, 35, 41] can only achieve low-resolution (LR) intensity reconstruction that is restricted by the spatial resolution of event cameras. How-ever, high-resolution (HR) intensity images with higher quality (more structural details, higher dynamic range, less blurry artifacts) signiﬁcantly contribute to many other event-based vision tasks (e.g., object recognition [6], detection[3], tracking[2], etc.). It is therefore of practical interest to conduct super-resolution (SR) for event-guided intensity image reconstruction.
Super-resolving intensity images for event cameras can be achieved in several ways. One kind of approach is
ﬁrst converting event data E to intensity images I [32, 35,
41] at the same spatial scale, then using existing SR ap-proaches [19, 39, 48] to get the SR results S. Such an ap-proach can be expressed as:
S =↑ (Γ(E)) , (1) where ↑ () and Γ() represent SR operation and conver-sion from events to intensity images, respectively. Another method is directly super-resolving LR event streams to re-construct HR intensity images without the assistance of in-tensity frames [7, 42], which is expressed using the follow-ing expression:
S =↑ (E). (2)
Moreover, hybrid signals (e.g., APS images ˆI and event data
E) can be taken as input to realize spatial resolution en-hancement of intensity images [40]:
S = Γ↑(ˆI, E), (3) where Γ↑() denotes jointly reconstructing and super resolv-ing operation.
However, the three types of event-based intensity image
SR described as Eq. (1)∼Eq. (3) have some disadvantages.
Firstly, Eq. (1) relies heavily on the performance of Γ() due to the domain gap between upsampling events and inten-sity data independently. Secondly, Eq. (2) does not take intensity information into account.
Ignoring the intensity information from APS frames that faithfully record scene radiance with less motion results in fewer details and unsta-ble intensity for video reconstruction. Thirdly, method [40] like Eq. (3) jointly achieving image deblurring, denoising, and SR may not produce high-quality images because dif-ferent degradation problems are caused by various reasons, as shown in Fig. 1.
In this paper, we propose to fuse intensity frames with event data to achieve high-quality super-resolution of inten-sity images by utilizing the information provided by hybrid types of input data. The APS frames record spatial irra-diance with rich semantic information of a scene at each pixel, while event data encode the rapid temporal irradiance changes along the edges of objects. The static intensity val-ues and dynamic events are complementary to each other.
We turn the SR problem into a better-posed multiple image version, as described in Eq. (4):
S = ↑Σ (cid:17) (cid:16)ˆΓ(ˆI, Ei)
, (4) where ↑Σ() denotes the multi-image super-resolution (MISR) operator. ˆΓ() differs from Γ() in that the conver-sion from event data to multiple latent intensity frames is provided with the assistance of APS, and i is the index of different batches of events.
We therefore propose EvIntSR-Net, a neural network that learns to convert Event data to multiple latent Intensity frames to achieve SR for reconstructing high-resolution in-tensity images. As described Eq. (4), such a merging pro-cess can be described as two steps: 1) The events repre-sent residuals from low-quality APS frame to high-quality latent frames. Given APS frame and its preceding and/or following event streams, we can reconstruct multiple latent frames with higher dynamic range and sharper details. 2)
The reconstructed latent frames could then be treated as a sequence of video frames, which beneﬁt from MISR to enhance the resolution of a target APS frame. EvIntSR-Net is composed of two sub-networks: latent frame recon-struction network (LFR-Net), which estimates the residu-als between intensity frames and reconstructs multiple la-tent frames; and multi-image fusion network (MIF-Net), which solves the misalignment issue among latent frames
ﬁrst, then learns to effectively merge them in a recurrent updating manner.
Extensive experiments on synthetic data, as well as real-world data (e.g., DAVIS346) demonstrate EvIntSR-Net can successfully reconstruct 2× and 4× super-resolved inten-sity images with higher ﬁdelity comparing to state-of-the-art approaches. An example is shown in Fig. 1, compared to eSL-Net [40], which also takes ˆI and E as input data,
EvIntSR-Net (ours) recovers much sharper edges and richer structural details in 4× SR on images captured by a real event camera. In addition to image SR, EvIntSR-Net can generate high-frame-rate (HFR) videos with SR frames. 2.