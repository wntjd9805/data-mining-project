Abstract
In this paper, we introduce a novel self-supervised visual representation learning method which understands both im-ages and videos in a joint learning fashion. The proposed neural network architecture and objectives are designed to obtain two different Convolutional Neural Networks for solving visual recognition tasks in the domain of videos and images. Our method called Video/Image for Visual
Contrastive Learning of Representation(Vi2CLR) uses un-labeled videos to exploit dynamic and static visual cues for self-supervised and instances similarity/dissimilarity learn-ing. Vi2CLR optimization pipeline consists of visual clus-tering part and representation learning based on groups of similar positive instances within a cluster and negative ones from other clusters and learning visual clusters and their distances. We show how a joint self-supervised visual clus-tering and instance similarity learning with 2D (image) and 3D (video) CovNet encoders yields such robust and near to supervised learning performance.
We extensively evaluate the method on downstream tasks like large scale action recognition, image and object classi-ﬁcation on datasets like Kinetics, ImageNet, Pascal VOC’07 and UCF101 and achieve outstanding results compared to state-of-the-art self-supervised methods. 1.

Introduction
Learning strong and discriminative representations is im-portant for diverse applications in computer vision tasks such as image classiﬁcation, object detection, image seg-mentation, activity recognition, video classiﬁcation, medi-cal imaging as well as natural language processing. More recently unsupervised or self-supervised representation learning has received a lot of attention as these methods are not dependent on manually curated ground-truth labels but rather utilize the supervision coming from the data itself and still rapidly close the performance gap with the supervised training. Most recent state-of-the-art methods are largely driven by instance [77, 12, 29] or prototype [44, 11] dis-crimination tasks. These discrimination methods rely on combination of two key components: (a) contrastive loss and (b) image [77, 29, 44, 12, 11] or video [28, 27, 56] augmentation. The contrastive loss [25] encourages small distances by pulling samples from the same label together and pushing far apart at least by the margin for the sam-ples of different labels in feature space. The current con-trastive loss functions are in the form of noise contrastive estimator [24] to compare instances (InfoNCE [51]), pro-totypes (ProtoNCE [44]), instances that include samples with the same semantic labels (UberNCE [28]) or comple-mentary views, and multiple instances (Multi-Instance In-foNCE [45]). The data augmentation or transformation can be categorized into two types on the basis of the datatype, namely for images, and for videos. For instance discrimi-nation [12] each sample of the dataset is treated as a class and enforce the augmented version of the same sample to be more similar, while in case of prototypes [44, 11] enforcing the augmented version of the samples to be closer to the prototype. Data augmentation plays a crucial role in the im-ages and videos contrastive representation learning. In par-ticular, for images [12, 11] the most popular augmentation methods are color transformation, geometric transformation and multi-crop; and for videos [28, 27, 56] randomly min-ing clips from the same video as positives, temporally con-sistent spatial augmentation, and mining complementary in-formation from different views of the RGB-stream/optical-ﬂow data are the transformation methods. See Section 2 in the related work for an extended review.
In this paper, we propose to extend the self-supervised training of ConvNets for solving visual recognition tasks both in videos and images simultaneously. Our contribu-tion named as Vi2CLR is a method that jointly optimizes two ConvNets for Videos and Images for Visual Contrastive
Learning of Representation as a multi-task learning prob-lem. We achieve this by learning both dynamic and static vi-sual cues simultaneously in an end-to-end learning pipeline.
Vi2CLR optimization utilizes clustering as supervision for learning an effective visual (2D ConvNets) and video (3D ConvNets) representations. We believe clustering of-fers an ability to bring together a diverse set of samples from images/videos across the whole dataset, which in turn pro-vides variability and diversity which is a good way to learn representations and is an important factor for the increased performance shown in Section 4. For learning an effective representation we considered two aspects, they are: (a) all image or video instances in a given cluster are considered positive pairs, and negative pairs are mined from the batch minimizing Multi-Instance InfoNCE loss; and (b) all joint image-video representations in a given cluster are enforced to be closer to the cluster centroid, and negatives are the centroids of the other clusters minimizing our centroid In-foNCE loss. We name this loss CenterNCE.
We validate our Vi2CLR based 2D and 3D ConvNets by
ﬁne-tuning them on downstream tasks and evaluating them on several standard downstream video and image classiﬁca-tion benchmarks. For 3D ConvNets they are ﬁne-tuned on target action recognition datasets, and for 2D ConvNets we use the learned features without ﬁne-tuning and rather only employ an MLP projection head (i.e. linear classiﬁer) on top of the frozen features, following [11]. Our 3D ConvNet is evaluated on three challenging benchmark action recogni-tion datasets namely UCF101, HMDB51 and Kinetics-400.
We experimentally show that our Vi2CLR achieves state-of-the-art performance on UCF101 (88.9%), HMDB51 (55.7%) and Kinetics-400 (71.2%) outperforming all cur-rent video contrastive learning methods [28, 27, 56]. Our 2D ConvNet is evaluated using the ImageNet linear evalua-tion protocol. We have also presented that our Vi2CLR out-performs SimCLR [12], SwAV [11] with achieving 74.6% top-1 accuracy on ImageNet. 2.