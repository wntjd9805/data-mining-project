Abstract
In order to train 3D gaze estimators without too many annotations, we propose an unsupervised learning frame-work, Cross-Encoder, to leverage the unlabeled data to learn suitable representation for gaze estimation. To ad-dress the issue that the feature of gaze is always intertwined with the appearance of the eye, Cross-Encoder disentangles the features using a latent-code-swapping mechanism on eye-consistent image pairs and gaze-similar ones. Specif-ically, each image is encoded as a gaze feature and an eye feature. Cross-Encoder is trained to reconstruct each im-age in the eye-consistent pair according to its gaze fea-ture and the other’s eye feature, but to reconstruct each image in the gaze-similar pair according to its eye feature and the other’s gaze feature. Experimental results show the validity of our work. First, using the Cross-Encoder-learned gaze representation, the gaze estimator trained with very few samples outperforms the ones using other unsu-pervised learning methods, under both within-dataset and cross-dataset protocol. Second, ResNet18 pretrained by
Cross-Encoder is competitive with state-of-the-art gaze es-timation methods. Third, ablation study shows that Cross-Encoder disentangles the gaze feature and eye feature. 1.

Introduction
Gaze indicates where someone is looking toward.
It serves as one of the cues in understanding human desires, intents and states of mind. 3D gaze estimation retrieves the direction of the line from an observer’s eye to a sight.
Automatically estimating gaze direction show potential ap-plications in psychological research [22], human-computer interaction [26], driver distraction detection [1], and other areas. Recently, significant efforts have been devoted to de-veloping non-intrusive gaze direction estimator based on fa-cial or eyes images. Specially, the growing strength of Con-Acknowledgement: This work is partially supported by National Key
R&D Program of China (No. 2017YFA0700800) and National Natural
Science Foundation of China (No. 61976203, 61702481).
Figure 1. (a) The mutual information between the auto-encoder learned representation and the eye’s identity or the gaze. (b) Disen-tangled gaze features and eye features learned by Cross-Encoder.
The images of the same eye from different frames have consistent eye features. The images of the two eyes from one frame have similar gaze features. volutional Neural Network (CNN) [11] makes it relatively easy to deal with some practical problems in gaze estima-tion like head pose variations, eye occlusions, and variable eye shapes [7, 8, 31, 38, 40].
Despite its representational power, a well-performed
CNN-based method is usually trained on sufficiently large and diverse labeled data. However, acquiring precise gaze labels is difficult. The gaze direction could not be mea-sured directly, but be measured by complicated setups and computations according to the geometry [20, 41]. The lim-ited access to labeled data hinders the development of gaze estimation methods. When trained on a small amount of monotonous annotated samples, supervised learning meth-ods are easy to overfit the training data and preserve features that do not represent the gaze. The redundant and unrelated features lead methods to perform poorly on data beyond the training ones.
Various unsupervised or self-supervised learning strate-gies are proposed and show potentials in addressing the is-sue of scarce annotations [4, 6, 29]. Most of them focus on learning a representation for relatively general purposes, e.g., image classification [4], object detection [9], segmen-tation [28]. However, these representations and methods are not the best for gaze estimation. It is also controversial that a universal representation exists for all the tasks.
Learning a good representation for gaze is non-trivial be-cause the features of gaze direction are always intertwined with those of what the eye looks like. Fig. 1(a) illustrates the mutual information between the top 10 principal compo-nents of the unsupervised features learned by auto-encoder and the eye-identity or the gaze. The eye-identity is defined as the distinguishing shape and appearance of an eye. The left and the right eye of a same person have different eye-identities. As can be seen, if we learn representations from eye images in an unsupervised way without any priors, the learned features are more related to the eye-identity than to the gaze. To our knowledge, Yu and Odobez [37] are the first to learn gaze-specific representation without annota-tions. Unlike their work, which leverages the gaze redirec-tion task and ignores the intertwined factors, we explicitly disentangle the features of gaze and those of what makes the eye look like the eye, in an unsupervised manner.
To this end, we propose an unsupervised learning frame-work to learn disentangled gaze feature and eye feature from eye images. The key component is an auto-encoder-like architecture, dubbed as Cross-Encoder.
It is trained with two types of paired images simultaneously: Paired images of the same eye or with similar gaze directions.
Fig. 1(b) shows the intuition of our method. As can be seen, we select the same right (or left) eyes of a subject from different frames to constitute the eye-consistent pairs.
For the gaze-similar pair, we use the right and left eyes of the subject in one frame, because when someone is looking at a distant object, the gaze direction of the two eyes are nearly parallel [27]. Cross-Encoder aims at encoding eye’s identity information in the eye feature and gaze information in the gaze feature. Our contributions are summarized in three folds. 1) We propose a simple and effective unsuper-vised representation learning method called Cross-Encoder.
It disentangles the representation by reconstructing the im-ages according to switched features. 2) We learn unsu-pervised gaze-specific representation using Cross-Encoder by introducing two strategies to select the training pairs. 3) Extensive experiments demonstrate the advantage of the learned gaze representation and validate the effectiveness of each component in Cross-Encoder. 2.