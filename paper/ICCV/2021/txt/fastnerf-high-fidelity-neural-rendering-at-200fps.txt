Abstract
Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very compu-tationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed re-ality devices, we propose FastNeRF, the ﬁrst NeRF-based system capable of rendering high ﬁdelity photorealistic im-ages at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each po-sition in space, (ii) efﬁciently querying that map using ray directions to estimate the pixel values in the rendered im-age. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and ex-tensibility. 1.

Introduction
Rendering scenes in real-time at photorealistic quality has long been a goal of computer graphics. Traditional ap-proaches such as rasterization and ray-tracing often require
*Denotes equal contribution
Project website: https://microsoft.github.io/FastNeRF signiﬁcant manual effort in designing or pre-processing the scene in order to achieve both quality and speed. Recently, neural rendering [10, 27, 17, 25, 19] has offered a disrup-tive alternative: involve a neural network in the rendering pipeline to output either images directly [24, 18, 27] or to model implicit functions that represent a scene appro-priately [5, 23, 29, 25, 40]. Beyond rendering, some of these approaches implicitly reconstruct a scene from static or moving cameras [25, 36, 1], thereby greatly simplifying the traditional reconstruction pipelines used in computer vi-sion.
One of the most prominent recent advances in neural ren-dering is Neural Radiance Fields (NeRF) [25] which, given a handful of images of a static scene, learns an implicit volu-metric representation of the scene that can be rendered from novel viewpoints. The rendered images are of high quality and correctly retain thin structures, view-dependent effects, and partially-transparent surfaces. NeRF has inspired sig-niﬁcant follow-up work that has addressed some of its lim-itations, notably extensions to dynamic scenes [31, 7, 46], relighting [2, 3, 37], and incorporation of uncertainty [22].
One common challenge to all of the NeRF-based ap-proaches is their high computational requirements for ren-dering images. The core of this challenge resides in NeRF’s volumetric scene representation. More than 100 neural network calls are required to render a single image pixel, which translates into several seconds being required to ren-der low-resolution images on high-end GPUs. Recent ex-plorations [35, 15, 28, 16] conducted with the aim of im-proving NeRF’s computational requirements reduced the
render time by up to 50×. While impressive, these ad-vances are still a long way from enabling real-time render-ing on consumer-grade hardware. Our work bridges this gap while maintaining quality, thereby opening up a wide range of new applications for neural rendering. Furthermore, our method could form the fundamental building block for neu-ral rendering at high resolutions.
To achieve this goal, we use caching to trade memory for computational efﬁciency. As NeRF is fundamentally a function of positions p ∈ R3 and ray directions d ∈ R2 to color c ∈ R3 (RGB) and a scalar density σ, a na¨ıve ap-proach would be to build a cached representation of this function in the space of its input parameters. Since σ only depends on p, it can be cached using existing methodolo-gies. The color c, however, is a function of both ray direc-tion d and position p. If this 5 dimensional space were to be discretized with 512 bins per dimension, a cache of around 176 terabytes of memory would be required – dramatically more than is available on current consumer hardware.
Ideally, we would treat directions and positions sepa-rately and thus avoid the polynomial explosion in required cache space. Fortunately this problem is not unique to
NeRF; the rendering equation (modulo the wavelength of light and time) is also a function of R5 and solving it ef-ﬁciently is the primary challenge of real-time computer graphics. As such there is a large body of research which investigates ways of approximating this function as well as efﬁcient means of evaluating the integral. One of the fastest approximations of the rendering equation involves the use of spherical harmonics such that the integral results in a dot product of the harmonic coefﬁcients of the material model and lighting model. Inspired by this efﬁcient approxima-tion, we factorize the problem into two independent func-tions whose outputs are combined using their inner product to produce RGB values. The ﬁrst function takes the form of a Multi-Layer Perceptron (MLP) that is conditioned on position in space and returns a compact deep radiance map parameterized by D components. The second function is an
MLP conditioned on ray direction that produces D weights for the D deep radiance map components.
This factorized architecture, which we call FastNeRF, al-lows for independently caching the position-dependent and ray direction-dependent outputs. Assuming that k and l de-note the number of bins for positions and ray directions re-spectively, caching NeRF would have a memory complex-ity of O(k3l2). In contrast, caching FastNeRF would have a complexity of O(k3 ∗ (1 + D ∗ 3) + l2 ∗ D). As a re-sult of this reduced memory complexity, FastNeRF can be cached in the memory of a high-end consumer GPU, thus enabling very fast function lookup times that in turn lead to a dramatic increase in test-time performance.
While caching does consume a signiﬁcant amount of memory, it is worth noting that current implementations of
NeRF also have large memory requirements. A single for-ward pass of NeRF requires performing hundreds of for-ward passes through an eight layer 256 hidden unit MLP per pixel. If pixels are processed in parallel for efﬁciency this consumes large amounts of memory, even at moder-ate resolutions. Since many natural scenes (e.g. a living room, a garden) are sparse, we are able to store our cache sparsely. In some cases this can make our method actually more memory efﬁcient than NeRF.
In summary, our main contributions are:
• The ﬁrst NeRF-based system capable of rendering photorealistic novel views at 200FPS, thousands of times faster than NeRF.
• A graphics-inspired factorization that can be com-pactly cached and subsequently queried to compute the pixel values in the rendered image.
• A blueprint detailing how the proposed factorization can efﬁciently run on the GPU. 2.