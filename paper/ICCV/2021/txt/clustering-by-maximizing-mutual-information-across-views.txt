Abstract
We propose a novel framework for image clustering that incorporates joint representation learning and clustering.
Our method consists of two heads that share the same back-bone network - a “representation learning” head and a
“clustering” head. The “representation learning” head captures ﬁne-grained patterns of objects at the instance level which serve as clues for the “clustering” head to ex-tract coarse-grain information that separates objects into clusters. The whole model is trained in an end-to-end manner by minimizing the weighted sum of two sample-oriented contrastive losses applied to the outputs of the two heads. To ensure that the contrastive loss correspond-ing to the “clustering” head is optimal, we introduce a novel critic function called “log-of-dot-product”. Exten-sive experimental results demonstrate that our method sig-niﬁcantly outperforms state-of-the-art single-stage cluster-ing methods across a variety of image datasets, improving over the best baseline by about 5-7% in accuracy on CI-FAR10/20, STL10, and ImageNet-Dogs. Further, the “two-stage” variant of our method also achieves better results than baselines on three challenging ImageNet subsets. 1.

Introduction
The explosion of unlabeled data, especially visual con-tent in recent years has led to the growing demand for effective organization of these data into semantically dis-tinct groups in an unsupervised manner. Such data clus-tering facilitates downstream machine learning and reason-ing tasks. Since labels are unavailable, clustering algo-rithms are mainly based on the similarity between sam-ples to predict the cluster assignment. However, common similarity metrics such as cosine similarity or (negative)
Euclidean distance are ineffective when applied to high-dimensional data like images. Modern image clustering methods [7, 17, 18, 37, 40, 41], therefore, leverage deep neural networks (e.g., CNNs, RNNs) to transform high-dimensional data into low-dimensional representation vec-tors in the latent space and perform clustering in that space.
Ideally, a good clustering model assigns data to clusters to keep inter-group similarity low while maintaining high intra-group similarity. Most existing deep clustering meth-ods do not satisfy both of these properties. For example, autoencoder-based clustering methods [19, 40, 42] often learn representations that capture too much information in-cluding distracting information like background or texture.
This prevents them from computing proper similarity scores between samples at the cluster-level. Autoencoder-based methods have only been tested on simple image datasets like MNIST. Another class of methods [7, 17, 18] directly use cluster-assignment probabilities rather than represen-tation vectors to compute the similarity between samples.
These methods can only differentiate objects belonging to different clusters but not in the same cluster, hence, may in-correctly group distinct objects into the same cluster. This leads to low intra-group similarity.
To address the limitations of existing methods, we pro-pose a novel framework for image clustering called Con-trastive Representation Learning and Clustering (CRLC).
CRLC consists of two heads sharing the same backbone net-work: a “representation learning” head (RL-head) that out-puts a continuous feature vector, and a “clustering” head (C-head) that outputs a cluster-assignment probability vector.
The RL-head computes the similarity between objects at the instance level while the C-head separates objects into dif-ferent clusters. The backbone network serves as a medium for information transfer between the two heads, allowing the C-head to leverage disciminative ﬁne-grained patterns captured by the RL-head to extract correct coarse-grained cluster-level patterns. Via the two heads, CRLC can ef-fectively modulate the inter-cluster and intra-cluster simi-larities between samples. CRLC is trained in an end-to-end manner by minimizing a weighted sum of two sample-oriented contrastive losses w.r.t. the two heads. To ensure that the contrastive loss corresponding to the C-head leads to the tightest InfoNCE lower bound [30], we propose a novel critic called “log-of-dot-product” to be used in place of the conventional “dot-product” critic.
In our experiments, we show that CRLC signiﬁ-cantly outperforms a wide range of state-of-the-art single-stage clustering methods on ﬁve standard image clus-tering datasets including CIFAR10/20, STL10,
Ima-geNet10/Dogs. The “two-stage” variant of CRLC also achieves better results than SCAN - a powerful two-stage clustering method on three challenging ImageNet subsets with 50, 100, and 200 classes. When some labeled data are provided, CRLC, with only a small change in its objective, can surpass many state-of-the-art semi-supervised learning algorithms by a large margin.
In summary, our main contributions are: 1. A novel framework for joint representation learning and clustering trained via two sample-oriented con-trastive losses on feature and probability vectors; 2. An optimal critic for the contrastive loss on probability vectors; and, 3. Extensive experiments and ablation studies to validate our proposed method against baselines. 2. Preliminaries 2.1. Representation learning by maximizing mutual information across different views1
Maximizing mutual information across different views (or ViewInfoMax for short) allows us to learn view-invariant representations that capture the semantic informa-tion of data important for downstream tasks (e.g., classiﬁ-cation). This learning strategy is also the key factor behind recent successes in representation learning [16, 28, 33, 36].
Since direct computation of mutual information is difﬁ-cult [24, 32], people usually maximize the variational lower bounds of mutual information instead. The most common lower bound is InfoNCE [30] whose formula is given by:
I(X, ˜X) ≥ IInfoNCE(X, ˜X) (cid:34) (cid:44) Ep(x1:M )p(˜x|x1) log (cid:35) ef (˜x,x1) i=1 ef (˜x,xi) (cid:80)M
= −Lcontrast + log M (1)
+ log M (2) (3) where X, ˜X denote random variables from 2 different views. x1:M are M samples from pX , ˜x is a sample from p ˜X associated with x1. (˜x, x1) is called a “positive” pair and (˜x, xi) (i = 2, ..., M ) are called “negative” pairs. f (x, y) is a real value function called “critic” that character-izes the similarity between x and y. Lcontrast is often known as the “contrastive loss” in other works [8, 33].
Since log i=1 ef (˜x,xi) ≤ 0, IInfoNCE(X, ˜X) is upper-bounded by log M . It means that: i) the InfoNCE bound ef (˜x,x1) (cid:80)M is very loose if I(X, ˜X) (cid:29) log M , and ii) by increasing
M , we can achieve a better bound. Despite being biased,
IInfoNCE(X, ˜X) has much lower variance than other unbi-ased lower bounds of I(X, ˜X) [30], which allows stable training of models.
In practice, f (˜x, xi) is imple-Implementing the critic mented as the scaled cosine similarity between the repre-sentations of ˜x and xi as follows: f (˜x, xi) = f (˜z, zi) = ˜z(cid:62)zi/τ (4) where ˜z and zi are unit-normed representation vectors of ˜x and xi, respectively; (cid:107)˜z(cid:107)2 = (cid:107)zi(cid:107)2 = 1. τ > 0 is the “tem-perature” hyperparameter. Interestingly, f in Eq. 4 matches the theoretically optimal critic that leads to the tightest In-foNCE bound for unit-normed representation vectors (de-tailed explanation in Appdx. A.4)
In Eq. 4, we use f (˜z, zi) instead of f (˜x, xi) to emphasize that the critic f in this context is a function of representa-tions. In regard to this, we rewrite the contrastive loss in
Eq. 3 as follows:
LFC = Ep(x1:M )p(˜x|x1)
− log (cid:34) (cid:35) ef (˜z,z1) i=1 ef (˜z,zi) (cid:80)M (5)
= Ep(x1:M )p(˜x|x1) (cid:34)
˜z(cid:62)z1/τ − log (cid:35) exp(˜z(cid:62)zi/τ )
M (cid:88) i=1 (6) where FC stands for “feature contrastive”. 3. Method 3.1. Clustering by maximizing mutual information across different views
In the clustering problem, we want to learn a parametric classiﬁer sθ that maps each unlabeled sample xi to a cluster-assignment probability vector qi = (qi,1, ..., qi,C) (C is the number of clusters) whose component qi,c characterizes how likely xi belongs to the cluster c (c ∈ {1, ..., C}). In-tuitively, we can consider qi as a representation of xi and use this vector to capture the cluster-level information in xi by leveraging the “ViewInfoMax” idea discussed in Section 2.1. It leads to the following loss for clustering:
Lcluster = Ep(x1:M )p(˜x|x1)
− log (cid:34) (cid:35) ef (˜q,q1) i=1 ef (˜q,qi) (cid:80)M
− λH( ˜Qavg)
= LPC − λH( ˜Qavg) (7) (8) 1Here, we use “views” as a generic term to indicate different transfor-mations of the same data sample. where λ ≥ 0 is a coefﬁcient; ˜q, qi are probability vectors associated with ˜x and xi, respectively. LPC is the proba-Figure 1: Overview of our proposed framework for Contrastive Representation Learning and Clustering (CRLC). Our frame-work consists of a “clustering” head and a “representation learning” head sharing the same backbone network. x denotes an input images and T1(x), T2(x) denote two different transformations of x. bility contrastive loss similar to LFC (Eq. 5) but with fea-ture vectors replaced by probability vectors. H is the en-tropy of the marginal cluster-assignment probability ˜qavg =
Ep(x1)p(˜x|x1) [˜q]. Here, we maximize H( ˜Qavg) to avoid a degenerate solution in which all samples fall into the same cluster (e.g., ˜q is one-hot for all samples). However, it is free to use other regularizers on ˜qavg rather than −H( ˜Qavg).
Choosing a suitable critic
It is possible to use the con-ventional “dot-product” critic for LPC as for LFC (Eq. 4).
However, this will lead to suboptimal results (Section 5.3) since LPC is applied to categorical probability vectors rather than continuous feature vectors. Therefore, we need to choose a suitable critic for LPC so that the InfoNCE bound associated with LPC is tightest.
Ideally, f (˜x, xi) should match the theoretically optimal critic f ∗(˜x, xi) which is proportional to log p(˜x|xi) (detailed explanation in Ap-pdx. A.3). Denoted by ˜y and yi the cluster label of ˜x and xi respectively, we then have: p(˜y = c|yi = c) log p(˜x|xi) ≈ log
∝ log
C (cid:88) c=1
C (cid:88) c=1 other nonoptimal critics in Appdx. A.1. Empirical compar-ison of the “log-of-dot-product” critic with other critics is provided in Section 5.3.
In addition, to avoid the gradient saturation problem of minimizing LPC when probabilities are close to one-hot (ex-planation in Appdx. A.5), we smooth out the probabilities as follows: q = (1 − γ)q + γr where r = (cid:0) 1 (cid:1) is the uniform probability vector over
C classes; 0 ≤ γ ≤ 1 is the smoothing coefﬁcient set to 0.01 if not otherwise speciﬁed.
C , ..., 1
C
Implementing the contrastive probability loss To im-plement LPC, we can use either the SimCLR framework [8] or the MemoryBank framework [36]. If the SimCLR frame-work is chosen, both ˜q and qi (i ∈ {1, ..., M }) are computed directly from ˜x and xi respectively via the parametric clas-siﬁer sθ. On the other hand, if the MemoryBank framework is chosen, we maintain a nonparametric memory bank M
- a matrix of size N × C containing the cluster-assignment probabilities of all N training samples, and update its rows once a new probability is computed as follows:
˜qcqi,c = log(˜q(cid:62)qi) (9) qn,t+1 = αqn,t + (1 − α)ˆqn (10)
Thus, the most suitable critic is f (˜q, qi) = log(˜q(cid:62)qi) which we refer to as the “log-of-dot-product” critic. This critic achieves its maximum value when ˜q and qi are the same one-hot vectors and its minimum value when ˜q and qi are different one-hot vectors. Apart from this critic, we also list where α is the momentum, which is set to 0.5 in our work if not otherwise speciﬁed; qn,t is the probability vector of the training sample xn at step t corresponding to the n-th row of M; ˆqn = sθ(xn) is the new probability vector. Then, except ˜q computed via sθ as normal, all qi in Eq. 7 are sam-pled uniformly from M. At step 0, all the rows of M are
C , ..., 1 initialized with the same probability of (cid:0) 1 (cid:1). We also tried implementing LPC using the MoCo framework [14] but found that it leads to unstable training. The main reason is that during the early stage of training, the EMA model in
MoCo often produces inconsistent cluster-assignment prob-abilities for different views.
C 4.