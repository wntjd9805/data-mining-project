Abstract
The key challenge in designing a sketch representa-tion lies with handling the abstract and iconic nature of sketches. Existing work predominantly utilizes either, (i) a pixelative format that treats sketches as natural images em-ploying off-the-shelf CNN-based networks, or (ii) an elabo-rately designed vector format that leverages the structural information of drawing orders using sequential RNN-based methods. While the pixelative format lacks intuitive ex-ploitation of structural cues, sketches in vector format are absent in most cases limiting their practical usage. Hence, in this paper, we propose a lattice structured sketch repre-sentation that not only removes the bottleneck of requiring vector data but also preserves the structural cues that vec-tor data provides. Essentially, sketch lattice is a set of points sampled from the pixelative format of the sketch using a lattice graph. We show that our lattice structure is par-ticularly amenable to structural changes that largely ben-efits sketch abstraction modeling for generation tasks. Our lattice representation could be effectively encoded using a graph model, that uses significantly fewer model parame-ters (13.5 times lesser) than existing state-of-the-art. Exten-sive experiments demonstrate the effectiveness of sketch lat-tice for sketch manipulation, including sketch healing and image-to-sketch synthesis. 1.

Introduction
Research on freehand human sketches has become in-creasingly popular in recent years. Due to its ubiquitous ability in recording visual objects [6], sketches form a nat-ural medium for human-computer interaction. Deriving a tailor-made representation for sketches sits at the core of sketch research, and has direct impact on a series of down-stream applications such as sketch recognition [33, 27, 12], sketch-based image retrieval [3, 32, 17, 16], sketch-3D reconstruction [15, 9, 25, 19, 22], and sketch synthesis
[14, 23]. Albeit a pivotal component, designing an effec-tive representation is challenging since sketches are typi-cally abstract and iconic.
*Equal contribution
Figure 1. (a) Given lattice points sampled on input sketches (Left), our proposed Lattice-GCN-LSTM network can recreate a corre-sponding vector sketch (Right). (b) Given a corrupted sketch, the resulting lattice points are used to reconstruct a similar sketch ac-cordingly. (c) The abstraction level of generated sketches is con-trollable by varying the density of latticed points. (d) Image-to-sketch synthesis by dropping a few lattice points along the edge of an object.
Prior works predominantly relied on encoding sketch in a pixelative format (i.e., an image) [6, 33, 21]. Although it provided the convenience of using off-the-shelf convolu-tional neural network effortlessly re-purposed for sketches, pixelative format lacks the intuitive exploitation of struc-tural information. The presence of structural information is vital for sketch abstraction modeling [6, 20], which in turn is essential for downstream tasks that dictate structural ma-nipulation such as sketch generation [7, 4, 10] and sketch synthesis [14, 23].
RNN-based approaches have consequently emerged as means to fully explore the sequential nature of sketches [7].
The research convention is to use QuickDraw [8] vector for-mat where each sketch is represented as a list of offsets in x and y. Thanks to the stroke-level modeling, these ap-proaches do offer a degree of flexibility in generation and synthesis tasks, yet they do so by imposing a strong assump-tion – all sketches have sequential stroke data to them. This assumption largely prohibits the application of RNN-based methods to work with sketches such as those drawn on a piece of paper. A natural question is therefore – is there a way to remove the bottleneck on requiring vector data but
at the same time preserve the structural cues that vector data provides?
To answer this question, we propose an alternative sketch representation inspired by the concept of lattice structures –
SketchLattice. We define SketchLattice as a set of points sampled from the original 2D sketch image using a lattice graph as shown in Figure 1 (a). Such latticed sketch rep-resentation, although seemingly simplistic, is remarkably amenable to structural deformations, thereby providing vi-tal benefits for sketch abstraction modeling and in subse-quent downstream sketch generation tasks. Our proposed latticed representation can be easily and effectively encoded using a simple off-the-shelf graph convolutional network (GCN) [12, 5, 28], resulting in considerably fewer model parameters (13.5 times lesser) as compared to recent state-of-the-art techniques. This not only makes our proposed sketch representation easily deployable, thus making further progress towards practicality, but also reduces the difficulty in optimization and training to give a competitive perfor-mance.
Specifically, each point in SketchLattice is regarded as a graph node. Geometric proximity between nodes serves as guiding principle for constructing the adjacency matrix to form graph links. Intuitively, the proposed GCN-based fea-ture extractor learns the topology of points in a sketch ob-ject. Despite being simple, our novel sketch representation is surprisingly effective for sketch generation. In particular, using the proposed latticed representation, we show how to recover a corrupted sketch using our Lattice-GCN-LSTM network, as represented in Figure 1(b). Additionally, we present a novel aspect in sketch representation, where the abstraction level in the generated sketch is controllable as shown in Figure 1(c), subject to the density of points sam-pled by the lattice graph. Furthermore, our method is also applicable to the problem of image-to-sketch synthesis by simply dropping a few key points along the edge of a target object as depicted in Figure 1(d).
Our contributions are summarized as follows: (i) we propose SketchLattice, a novel latticed representation for sketches using an extremely simple formulation i.e., a set of points sampled from a sketch image using a lattice graph. (ii) Our latticed representation can be easily and effectively encoded using a simple graph model that use fewer model parameters, thereby making important progress towards ef-ficiency. (iii) We show how the abstraction level of gener-ated sketches is controllable by varying the density of points sampled from an image using our lattice graph. a conventional 2D image with pixel values (i.e., the pixela-tive format), the latter considers sketch as an elaborately de-signed set of ordered stroke points (i.e., vector format), rep-resented by offset coordinates along x and y directions with pen states (touch, lift and end) [7]. Traditional sketch fea-ture extractors are usually CNN-based approaches [33, 4] that can directly take sketch image in pixelative format dur-ing input. However, this is highly redundant due to the sparsity of line drawings in a sketch image that necessitates heavy engineering efforts [33]. Additionally, CNN-based approaches cannot effectively capture structural cues since it does not encode position and orientation of objects, lead-ing to sub-par results on generation models.
In contrast, sequential representation is sketch-specific
[8], designed according to the drawing habit of humans which is constructed stroke-by-stroke. Such sequential, vector format representation allows modeling sketches us-ing RNN-based methods. This resulted in impressive re-sults such as sketch generation and sketch synthesis using long short-term memory (LSTM) [7, 24]. Although promis-ing, such RNN-based approaches require a vectorized data format at the input, which leads to a major bottleneck limit-ing practical usage in the absence of vector sketches, like sketches drawn on a piece of paper. Therefore, we aim to propose an unexplored technique, by employing a more practical lattice sketch representation, i.e., SketchLattice, that avoids storing stroke orders while still keeping strong spatial evidence that vector sketches typically provide.
Graphical Sketch Embedding Graph convolutional net-works (GCNs) [2, 12] were originally designed to deal with structured data, such as knowledge graphs or social net-works, by generalizing neural networks on graphs. In the past few years, exciting developments have been made that explore GCNs capabilities for various vision tasks includ-ing image classification [5], captioning [31], image under-standing [1], action recognition [13], 3D object detection
[34], and shape analysis [26]. Yet, until recently, a few at-tempts [30, 29] started to apply GCNs on sketch embedding.
The existing visual sparsity and spatial structure of sketch strokes are naturally compatible with graphical representa-tions. However, the dominating approach in sketch research assumes access to a vector format where the stroke orders are required, thus resulting in a major limitation in real-world cases. On contrary, ours provides a generic approach that explores the geometrical proximity in a latticed repre-sentation of sketch. We also show how our proposed graph-ical sketch embedding can be used additionally for tasks, in-volving sketch generation and image-to-sketch translation. 2.