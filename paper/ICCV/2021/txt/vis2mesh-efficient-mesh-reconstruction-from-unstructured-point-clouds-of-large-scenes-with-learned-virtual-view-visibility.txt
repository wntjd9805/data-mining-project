Abstract
We present a novel framework for mesh reconstruction from unstructured point clouds by taking advantage of the learned visibility of the 3D points in the virtual views and traditional graph-cut based mesh generation. Speciﬁcally, we ﬁrst propose a three-step network that explicitly employs depth completion for visibility prediction. Then the visi-bility information of multiple views is aggregated to gen-erate a 3D mesh model by solving an optimization prob-lem considering visibility in which a novel adaptive visibil-ity weighting in surface determination is also introduced to suppress line of sight with a large incident angle. Compared to other learning-based approaches, our pipeline only exer-cises the learning on a 2D binary classiﬁcation task, i.e., points visible or not in a view, which is much more gener-alizable and practically more efﬁcient and capable to deal with a large number of points. Experiments demonstrate that our method with favorable transferability and robust-ness, and achieve competing performances w.r.t. state-of-the-art learning-based approaches on small complex ob-jects and outperforms on large indoor and outdoor scenes.
Code is available at https://github.com/GDAOSU/vis2mesh. 1.

Introduction 3D surface reconstruction is essential to drive many com-puter vision and VR/AR applications, such as vision-based localization, view rendering, animation, and autonomous navigation. With the development of 3D sensors, e.g., Li-DAR or depth sensors, we can directly capture accurate 3D point clouds. However, determining surfaces from unstruc-tured point clouds remains a challenging problem, espe-cially for point clouds of objects of complex shapes, vary-ing point density, completeness, and volumes. A favorable mesh reconstruction method should be 1) capable of recov-ering geometric details captured by the point clouds; 2) ro-bust and generalizable to different scene contexts; 3) scal-able to large scenes and tractable in terms of memory and
*Contribution initiated and performed at ETH Z¨urich.
†Corresponding author. Email: qin.324@osu.edu
) s t p
K 0 0 1 ( 1 m o o
R
) s t p
M 5 ( k r a
P n o g a r
D
Points
Our Reconstruction
Figure 1. Examples of reconstructed surfaces with our approach on both indoor and large outdoor scenes. The number of points ranges from thousands to millions. computation.
Typical reconstruction methods either explicitly explore the local surface recovery through connecting neighbor-ing points (e.g. Delaunay Triangulation [5]), or sort solu-tions globally through implicit surface determination (e.g.
Screened Poisson Surface Reconstruction [21, 22] ). How-ever, both of them favor high-density point clouds. With the development of 3D deep learning, many learning-based methods have demonstrated their strong capability when reconstructing mesh surfaces from moderately dense or even comparatively sparse point clouds of complex objects.
However, these end-to-end deep architectures, on one hand, encode the contextual/scene information which often runs into generalization issues; on the other hand, the heavy net-works challenge the memory and computations when scal-ing up to large scenes.
The point visibility in views has been shown as a good source of information, which can greatly beneﬁt surface re-construction [47]. For example, a 3D point visible on a physical image view alludes to the fact the line of sight from
the 3D point to the view perspective center, travels through free space (no objects or parts occludes), which as a result, can serve as a strong constraint to guide the mesh recon-struction.
Intuitively, the more visible a point is in more views, the more information about the free space between points and views that one can explore [17]. This principle has been practiced in multi-view stereo (MVS) and range images and shows promising results [25, 26, 51], in which the visibility of each 3D point to each physical is recorded through dense matching (in MVS) or decoded directly from the sensors. However, in both cases, the visibility is limited by the number of physical views and accounts for only lim-ited knowledge of the free space, thus leading to the lack of reconstruction details.
To address it, we propose a novel solution by generat-ing a large number of virtual views around the point clouds and utilize the learned visibility through a graph-cut based surface reconstruction approach. This will literally give us unlimited visibility information for high-quality mesh gen-eration through traditional approaches based on visibility.
However, it is non-trivial to design such a system. First, visibility prediction in virtual views is not easy as the pro-jected pixels from the point clouds to different views can have varying sparsity. Second, unlike the MVS views which normally assume a good incidence angle to the generated point clouds, the generated virtual views may present a large variation in terms of their incident angle and distance to the surface. To solve these problems, we design a network for visibility prediction, which utilizes partial convolution to model the sparsity of the input in the neural network. To further improve the accuracy, we propose a cascade network that explicitly employs depth completion as an intermedi-ate task. Secondly, to suppress the side effect of unfavored virtual rays (large-incidence angle to the surface), we pro-pose a novel adaptive visibility weighting term that adap-tively weights the virtual rays. Compared to the end-to-end learning-based methods, our method has better generaliza-tion across different scene contexts because a very simple and learning-friendly task, i.e., visibility prediction, is in-volved in our pipeline. Moreover, our method also main-tains the capability to process an extremely large volume of outdoor point clouds with high quality.
Our contributions can be summarized as follows:
Firstly, we present a surface reconstruction framework from unstructured point clouds, that exploits the power of both traditional and learning-based methods; Secondly, we propose a three-step network that explicitly employs the depth completion for the visibility prediction of 3D points;
Thirdly, we propose a novel adaptive visibility weighting term into the traditional graph-cut based meshing pipeline, to increase the geometric details; Lastly, we demonstrate through our experiments that our proposed method yields better generalization capability, and can be scaled up to pro-cess point clouds of a large scene. It is also robust against different types of noises and incomplete data and has better performance than the state-of-the-art methods. 2.