Abstract
Event camera is an emerging imaging sensor for captur-ing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called
EventHPE. The first-stage, FlowNet, is trained by unsu-pervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploit-ing the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empiri-cal evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach. 1.

Introduction
Visual human pose and shape estimation have played a critical role in computer vision, with numerous research ac-tivities over the years [18, 25]. Existing research efforts are predominantly based on images from the conventional RGB or RGB-D cameras [13, 15, 14, 29, 34]. Meanwhile, the recent development in event cameras [7] offers new oppor-tunities. Its working mechanism is a paradigm shift from theses conventional frame-based cameras. Inspired by bi-ological vision process, event cameras [7] asynchronously measure per-pixel brightness changes, which enables them to best detect and capture object local motions. This new
Figure 1. An illustration of our approach, which takes as input a stream of events and only the first frame of gray-scale image from an event camera. Our goal is to estimate 3D human poses and shapes through time from the events stream as the sole data source given the beginning body posture in the first frame of gray-scale image, where optical flows are inferred from events as an intermediate step. imaging paradigm has already stimulated a range of com-puter vision research activities, such as camera pose estima-tion [8], gesture recognition [1] and 3D reconstruction [24], as well as commercial interests spanning a range of use sce-narios including robotics, augmented and virtual reality, and auto-driving applications [7]. However, its potential in esti-mating 3D human shape is rarely explored.
DHP19 [3] is an early work that estimates only 2-D poses by treating a packet of events as a static image. The re-cent effort of EventCap [28] is the first in estimating 3D hu-man shapes from an event camera. However, in addition to the input events, EventCap relies on an additional stream of gray-scale image sequence as input, to establish the initial shape estimation at each time step. This motivates us to con-sider the problem of inferring 3D human shape from events as the major source of input: events are the sole source of input data to estimate 3D human shapes over time, given that the beginning shape is known or extracted from the first frame of gray-scale image. Fig. 1 provides an overview of
our two-stage approach, called EventHPE.
Considering the fact that the two modalities, events and optical flow, are both closely related to human motions, and optical flow can provide explicit geometric information to describe human body movements, we place the inference of optical flow from events (i.e. FlowNet) in the first stage of our framework, which is trained without supervision. The inclusion of optical flow makes it feasible to estimate hu-man poses and shapes mainly from events, which means we do not require a stream of gray-scale images as input in addition to the events. The second stage, denoted by
ShapeNet, is to estimate shape variations over time, given the events and the inferred optical flows as input. A novel flow coherence loss is proposed to enforce consistency of image-based flow (optical flow) and shape-based flow (ver-tices movement of human body shape), as both modalities are originated from the same human motion.
Our main contributions are summarized below. (1) We present an approach to a new and challenging problem, es-timating 3D human parametric shape mainly from events.
We propose to leverage optical flow inferred from events to relieve the reliance on the gray-scale image sequence as ad-ditional input. A novel coherence loss is also introduced to ensure consistency between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape). Empirical evaluations demonstrate the superior per-formance of our approach against several state of the arts. (2) A home-grown dataset is introduced, referred as Multi-Modality Human Pose and Shape Dataset (or MMHPSD) 1.
It includes 240k frames, with each frame containing 12 images from multiple imaging modalities including event camera. To our knowledge, MMHPSD is the largest event-based 3D human pose and shape dataset, and is the first publicly available dataset of such type, since the dataset of
EventCap [28] is not publicly available. The multi-modality property of MMHPSD also renders its great potential in fa-cilitating existing and new research directions. 2.