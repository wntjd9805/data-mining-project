Abstract
For machine learning models trained with limited la-beled training data, validation stands to become the main bottleneck to reducing overall annotation costs. We pro-pose a statistical validation algorithm that accurately esti-mates the F-score of binary classiﬁers for rare categories, where ﬁnding relevant examples to evaluate on is partic-ularly challenging. Our key insight is that simultaneous calibration and importance sampling enables accurate es-timates even in the low-sample regime (< 300 samples).
Critically, we also derive an accurate single-trial estima-tor of the variance of our method and demonstrate that this estimator is empirically accurate at low sample counts, enabling a practitioner to know how well they can trust a given low-sample estimate. When validating state-of-the-art semi-supervised models on ImageNet and iNatural-ist2017, our method achieves the same estimates of model performance with up to 10 fewer labels than competing
× approaches. In particular, we can estimate model F1 scores with a variance of 0.005 using as few as 100 labels. 1.

Introduction
As model training techniques become increasingly la-bel efﬁcient, model validation stands to become a dominant fraction of overall data annotation costs. For example, state-of-the-art semi-supervised [3, 9, 2], weakly supervised [19], few-shot [7, 15], and active learning [25, 4, 8, 24] tech-niques all offer the promise of training models using a small number of human-labeled examples, but validating the resulting models typically uses large, human-annotated datasets. As a result, the cost of annotating validation sets is a signiﬁcant factor limiting rapid model development.
In this paper we focus on the challenge of efﬁciently val-idating binary image classiﬁers for rare categories (posi-tive instances are 0.1% of the dataset). Building binary classiﬁcation models for rare categories is common in real-world settings—wildlife preservation monitoring requires identifying rare ﬂora and fauna species; autonomous vehi-≤
* Both authors contributed equally to this paper 1 Stanford University 2 Carnegie Mellon University 3 Google 4 Argo AI
Figure 1: Recent model training techniques such as self-supervised learning, few-shot learning, and weakly super-vised learning have made it possible to train models with a fraction of the traditional fully supervised training set.
However, these methods still mostly evaluate using a large validation set. In this paper, we focus on low-shot valida-tion, which addresses the high relative cost of collecting la-beled validation data for models trained using label-efﬁcient techniques. cles must recognize rare categories, like baby strollers, to avoid collisions. The validation problem is particularly dif-ﬁcult for rare categories: while it is easy to collect a large amount of unlabeled data, ﬁnding even a small number of positives via uniform random sampling can require labeling thousands of images.
Given a binary classiﬁcation model to validate and a large unlabeled dataset, our goal is to estimate the model’s
F-score [29] on the data using a small number of annotated data samples. The F-score of a model depends on the dis-tribution of the model’s predicted labels, which are known, and the dataset’s ground-truth labels, which require data annotation. Importance sampling [27] is a powerful theo-retical tool for stochastically sampling the most important points in a dataset to label, but the efﬁciency of estimating
F-scores using importance sampling depends on accurate knowledge of the likelihood that a given sample is a posi-tive. Therefore, the key challenge in using importance sam-pling for efﬁciently computing F-scores is model calibra-tion, the task of predicting the likelihood that a given sam-ple is a positive, conditioned on the model scores. Given this observation, we propose an active sampling algorithm that alternates between acquiring labels used to train an iso-tonic regression model [34] for calibrating model probabil-ities, then using the calibrated model scores to importance-sample batches of data for metric estimation. Using this al-ternating strategy, our scheme generates progressively bet-ter estimates of the model’s F-score.
We demonstrate that, particularly in the low-sample (< 300 labeled samples) regime, our algorithm can estimate
F1 with signiﬁcantly lower error than a variety of baselines, including semi-supervised Gaussian Mixture Models [13], prior importance-sampling approaches [22], and “herding” algorithms [32]. Not only are we able to estimate F1 ef-ﬁciently, we are also able to estimate the variance of our estimate accurately, even in low-sample regimes. This con-tribution has important practical ramiﬁcations in that it al-lows a practitioner to know if they should trust the estimate generated by a small set of labeled validation data. Our con-tributions are as follows: 1. An algorithm for joint active calibration and importance sampling-based F-score estimation. Our algorithm pro-duces accurate and reliable estimates of a model’s F-score, and it signiﬁcantly outperforms baseline methods in low-sample regimes (< 300 labeled samples). 2. A single-trial estimator of variance for our method. We demonstrate that our variance estimator is empirically accurate, even for low-sample counts, offering a valu-able diagnostic tool when using our algorithm in real-world settings. 3. A study that demonstrates that validation sets chosen speciﬁcally for a given model can also efﬁciently vali-date other models trained for the same task. 2.