Abstract
Preserving maximal information is one of principles of designing self-supervised learning methodologies. To reach this goal, contrastive learning adopts an implicit way which is contrasting image pairs. However, we believe it is not fully optimal to simply use the contrastive estimation for preservation. Moreover, it is necessary and complemental to introduce an explicit solution to preserve more informa-tion. From this perspective, we introduce Preservational
Learning to reconstruct diverse image contexts in order to preserve more information in learned representations. To-gether with the contrastive loss, we present Preservational
Contrastive Representation Learning (PCRL) for learning self-supervised medical representations. PCRL provides very competitive results under the pretraining-finetuning protocol, outperforming both self-supervised and super-vised counterparts in 5 classification/segmentation tasks substantially. Codes are available at https://github. com/Luchixiang/PCRL. 1.

Introduction
It is common practice that training deep neural networks often requires a large amount of manually labeled data. This requirement is easy to satisfy in natural images as both the cost of labor and the difficulty of labeling can be accept-able. However, in medical image analysis, reliable med-ical annotations usually come from domain experts’ diag-noses which are hard to access considering the scarcity of target disease, the protection of patient’s privacy and the limited medical resources. To address these problems, self-supervised learning has been widely adopted as a practical
*First two authors contributed equally.
†Corresponding author.
Figure 1: Conceptual illustration of proposed method. GO stands for global operations which convert feature maps to feature vectors. The blue feature vector comes from the mo-mentum encoder. vec(T ) represents the indicator vector of
T which contains a set of transformation functions. Each component in vec(T ) is 1 or 0 denoting whether the corre-sponding transformation is applied or not. ⊙ stands for a channel-wise multiplication operation. way to learn medical image representations without manual annotations.
Nowadays, contrastive representation learning has been widely applied and outstandingly successful in medical im-age analysis [51, 33, 6]. The goal of contrastive learning is to learn invariant representations via contrasting medi-cal image pairs, which can be regarded as an implicit way to preserve maximal information. Nonetheless, we think it is still beneficial and complemental to explicitly preserve more information in addition to the contrastive loss. To achieve this goal, an intuitive solution is to reconstruct the original inputs using learned representations so that these representations can preserve the information closely related to the inputs. However, we discover that directly adding a plain reconstruction branch for restoring the original inputs would not significantly improve the learned representations.
To address this problem, we introduce Preservational Con-trastive Representation Learning to reconstruct diverse con-texts using representations learned from the contrastive loss.
As shown in Fig.1, we attempt to incorporate the diverse image reconstruction, as a pretext task, into contrastive learning. The main motivation is to encode more infor-mation into the learned representations. Specifically, we introduce Transformation-conditioned Attention and Cross-model Mixup to enrich the information carried by represen-tations. The first module embeds a transformation indica-tor vector (vec(T ) in Figure 1) to high-level feature maps following an attentional mechanism. Based on the embed-ded vector, the network is required to dynamically recon-struct different image targets while the input is fixed. Cross-model Mixup is developed to generate a hybrid encoder by mixing the feature maps of the ordinary and the momentum encoders, where the hybrid encoder is asked to reconstruct mixed image targets. We show that both modules can help to encode more information and produce stronger represen-tations compared to using contrastive learning only.
Besides the learning algorithm, this paper also addresses another issue when using unlabeled medical images for pre-training, that is lacking a fair and thorough comparison of different self-supervised learning methodologies. In this pa-per, we design extensive experiments to analyze the perfor-mance of different algorithms across different datasets and data modalities. Generally speaking, the contributions of this paper can be summarized into three aspects:
• Preservational Contrastive Representation Learning is introduced to encode more information into the repre-sentations learned from the contrastive loss by recon-structing diverse contexts.
• In order to restore diverse images, we propose two modules: Transformation-conditioned Attention and
Cross-model Mixup to build a triple encoder, single decoder architecture for self-supervised learning.
• Extensive experiments and analyses show that the pro-posed PCRL has observable advantages in 5 classi-fication/segmentation tasks, outperforming both self-supervised and supervised counterparts by substantial and significant margins. position [14, 24], rotation degree [17, 14], object color
[23, 46], the number of objects [25] and the applied transformation function [30]. Contrastive-estimation based approaches also utilize pretext tasks to learn invariant repre-sentations by contrasting image pairs [43, 26, 10, 5, 20, 48].
Recently, there are some works trying to remove the nega-tive pairs in contrastive learning [18, 12]. By comparison, our method follows a different principle which is making representations able to fully describe their sources (i.e., corresponding input images).
Self-supervised learning in medical image analysis.
Before contrastive learning, solving the jigsaw problem
[54, 53, 35] and reconstructing corrupted images [9, 52] are two major topics for pretext-based approaches in medical images. Besides them, Xie et al. [44] introduced a triplet loss for self-supervised learning in nuclei images.
Haghighi et al. [19] improved [52] by appending a clas-sification branch to classify the high-level features into different anatomical patterns.
For contrastive learning,
Zhou et al. [51] applied contrastive loss to 2D radio-Similar ideas have also appeared in few-shot graphs.
[49] and semi-supervised learning [50]. Taleb et al. [34] proposed 3D Contrastive Predictive Coding from utilizing 3D medical images. There are two works [16, 8] most related to ours. Feng et al. [16] showed that the process of reconstructing part images displays similar effects with those of employing a contrastive loss. Chakraborty et al.
[8] introduced a denoising autoencoder to capture a latent space representation. However, both methods failed to improve contrastive learning with context reconstruction while our methodology succeeds in this aspect.
Mixup in medical imaging. Mixup [45], as an augmen-tation strategy, has been widely adopted in medical imaging
[27, 7, 22, 15, 2, 37]. The proposed Cross-model Mixup is most related to Manifold mixup [36, 22, 2]. However, as far as we know, there is no previous method applying manifold mixup to cross-model representations, which is exactly the core contribution of our CROSS-MODEL Mixup. 2.