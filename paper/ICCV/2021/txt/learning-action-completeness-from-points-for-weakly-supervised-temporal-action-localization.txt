Abstract
We tackle the problem of localizing temporal intervals of actions with only a single frame label for each action in-stance for training. Owing to label sparsity, existing work fails to learn action completeness, resulting in fragmentary action predictions. In this paper, we propose a novel frame-work, where dense pseudo-labels are generated to provide completeness guidance for the model. Concretely, we ﬁrst select pseudo background points to supplement point-level action labels. Then, by taking the points as seeds, we search for the optimal sequence that is likely to contain complete action instances while agreeing with the seeds. To learn completeness from the obtained sequence, we introduce two novel losses that contrast action instances with background ones in terms of action score and feature similarity, respec-tively. Experimental results demonstrate that our complete-ness guidance indeed helps the model to locate complete action instances, leading to large performance gains es-pecially under high IoU thresholds. Moreover, we demon-strate the superiority of our method over existing state-of-the-art methods on four benchmarks: THUMOS’14, GTEA,
BEOID, and ActivityNet. Notably, our method even per-forms comparably to recent fully-supervised methods, at the 6× cheaper annotation cost. Our code is available at https://github.com/Pilhyeon. 1.

Introduction
The goal of temporal action localization lies in locating starting and ending timestamps of action instances and clas-sifying them. Thanks to the various applications [33, 45, 48], it has drawn much attention from researchers, leading to the rapid and remarkable progress in the fully-supervised setting (i.e., frame-level labels) [27, 41, 43, 50]. Meanwhile, there appear attempts to reduce the prohibitively expensive cost of annotating individual frames by devising weakly-supervised models with video-level labels [7, 32, 46, 56].
*Corresponding author
Figure 1: Simpliﬁed illustration of our idea. We use points as seeds to ﬁnd the optimal sequence, which in turn provides completeness guidance to the model.
However, they fall largely behind the fully-supervised coun-terparts, mainly on account of their weak ability to distin-guish action and background frames [17, 18, 38, 52].
To narrow the performance gap between them, another level of weak supervision has been proposed recently, namely the point-supervised setting. In this setting, only a single timestamp (point) with its action category is anno-tated for each action instance during training. In terms of the labeling cost, point-level labels require a negligible extra cost compared to video-level ones, while being 6× cheaper than frame-level ones (50s vs. 300s per 1-min video) [31].
Despite the affordable cost, it offers coarse locations as well as the total number of action instances, thus bring-ing a strong ability in spotting actions to the models. Con-sequently, point-supervised methods show comparable or even superior performances to fully-supervised counter-parts under low intersection over union (IoU) thresholds.
However, it has been revealed that they suffer from incom-plete predictions, resulting in highly inferior performances in the case of high IoU thresholds. We conjecture that this problem is attributed to the sparse nature of point-level la-bels that induces the models to learn only a small part of ac-tions rather than the full extent of action instances. In other words, they fail to learn action completeness from the point annotations. Although SF-Net [31] mines pseudo action and background points to alleviate the label sparsity, they are discontinuous and thus do not provide completeness cues.
In this paper, we aim to allow the model to learn action completeness under the point-supervised setting. To this end, we introduce a new framework, where dense pseudo-labels (i.e., sequences) are generated based on the point an-notations to provide completeness guidance to the model.
The overall workﬂow is illustrated in Fig. 1.
Technically, we ﬁrst select pseudo background points to augment point-level action labels. As aforementioned, such point annotations are discontiguous, so it is infeasible to learn completeness from them. To that end, we propose to search for the optimal sequence covering complete action instances among candidates consistent with the point labels.
However, it is non-trivial to measure how complete the in-stances in each candidate sequence are, without full super-vision. To realize it, we borrow the outer-inner-contrast con-cept [42] as a proxy for instance completeness. Intuitively, a complete action instance generally shows large score con-trast, i.e., much higher action scores for inner frames than those for surrounding frames. In contrast, a fragmentary in-stance probably has high action scores in its outer region (still within the action), leading to small score contrast. This can be generalized for background instances as well. Based on this property, we derive the score of an input sequence by aggregating the score contrast of action and background in-stances constituting the sequence. By maximizing the score, we can obtain the optimal sequence that is likely to be well-aligned with the ground-truth we do not have. In experi-ments, we present the accuracy of optimal sequences and the correlation between score contrast and completeness.
From the obtained sequence, the model is supposed to learn action completeness. To this end, we design score con-trastive loss to maximize the agreement between the model outputs and the optimal sequence, by enlarging the com-pleteness of the sequence. With the loss, the model is trained to discriminate each action (background) instance from its surroundings in terms of action scores. Moreover, we in-troduce feature contrastive loss to encourage feature dis-crepancy between action and background instances. Exper-iments validate that the proposed losses complementarily help the model to detect complete action instances, leading to large performance gains under high IoU thresholds.
To summarize, our contributions are three-fold.
• We introduce a new framework, where the dense op-timal sequence is generated to provide completeness guidance to the model in the point-supervised setting.
• We propose two novel losses that facilitate the action completeness learning by contrasting action instances with background ones with respect to action score and feature similarity, respectively.
• Our model achieves a new state-of-the-art with a large gap on four benchmarks. Furthermore, it even per-forms favorably against fully-supervised approaches. 2.