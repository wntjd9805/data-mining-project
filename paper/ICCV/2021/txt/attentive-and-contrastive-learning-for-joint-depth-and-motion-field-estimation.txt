Abstract
Estimating the motion of the camera together with the 3D structure of the scene from a monocular vision system is a complex task that often relies on the so-called scene rigidity assumption. When observing a dynamic environment, this assumption is violated which leads to an ambiguity between the ego-motion of the camera and the motion of the objects.
To solve this problem, we present a self-supervised learn-ing framework for 3D object motion field estimation from monocular videos. Our contributions are two-fold. First, we propose a two-stage projection pipeline to explicitly disen-tangle the camera ego-motion and the object motions with dynamics attention module, called DAM. Specifically, we design an integrated motion model that estimates the mo-tion of the camera and object in the first and second warp-ing stages, respectively, controlled by the attention module through a shared motion encoder. Second, we propose an object motion field estimation through contrastive sample consensus, called CSAC, taking advantage of weak semantic prior (bounding box from an object detector) and geomet-ric constraints (each object respects the rigid body motion model). Experiments on KITTI, Cityscapes, and Waymo
Open Dataset demonstrate the relevance of our approach and show that our method outperforms state-of-the-art al-gorithms for the tasks of self-supervised monocular depth estimation, object motion segmentation, monocular scene flow estimation, and visual odometry. 1.

Introduction
The simultaneous estimation of the camera motion and scene geometry is a fundamental research topic in 3D com-puter vision. Traditionally, this problem is tackled by feature-based methods [33], or direct approaches [14] that min-imizes the photometric inconsistency among warped ad-jacent frames. A pioneering work based on deep neural network (DNN) [44] uses the photometric error map as a self-supervisory signal to jointly train a depth and a motion network. Inspired by this baseline structure, self-supervised depth, and motion learning framework has been widely stud-(a) Baseline (b) Ours
Figure 1. We introduce a unified motion modeling with our dynam-ics attention module (DAM) and a novel motion learning technique via contrastive sample consensus (CSAC). The last row shows synthesized views from the predicted depth and motion field. Com-pared to the baseline, our model learns the object motion fields in a more semantically plausible way, which enhances the distinction between the object and the background area. ied [44, 30, 40, 32, 41] with an additional self-supervisory signal such as geometric consistency [32], optical flow [36], segmentation map [43], edge and normal map [41]. These photo-consistency-based optimization methods assume a static scene or require to mask out moving objects to disre-gard non-rigid motions. Such works aim at predicting the depth and ego-motion from a camera but are not suitable for
dynamic scenes.
Recently, learning the objects’ motion together with the camera’s ego-motion and the depth has gain interest for dy-namic scene understanding [6, 26, 19, 7, 8, 25, 12, 4, 23, 28].
We can distinguish mostly two types of approaches, namely the stereo-based and the monocular-based techniques. The stereo-based techniques [6, 26] take advantage of this sen-sor to disentangle the motion of static background and that of moving objects in the scene. For monocular systems, the ambiguity between the depth, ego-motion, and objects’ motion becomes more intricate due to the unavailability of metric depth for each frame. Therefore, Monocular-based systems [7, 25, 28] rely on instance segmentation labels to reduce this ambiguity. Despite compelling results, the need for highly expensive human-labeled data constitute an impor-tant limitation for their deployment and reduce the interest of the self-supervised depth and motion prediction framework.
To reduce the data dependency problem and to offer more versatility, we propose a novel self-supervised learn-ing framework for depth, camera motion, and object motion field estimation using weak semantic prior (i.e., 2D object bounding boxes) as illustrated in Fig. 1. A major benefit of the proposed pipeline is that it helps to reduce the am-biguity between the camera’s ego-motion and the objects’ motion with cheaper data labels. The distinctive points of our approach are summarized as follows:
⋄ We design a dynamics attention module that enables to train motion features dynamically when estimating the motion of a camera and objects through a two-stage projection. We highlight that motion features can be efficiently extracted by disentangling dynamic objects and static backgrounds through the simple mechanism of attention modules within the shared motion encoder.
⋄ We propose a contrastive sample consensus for semanti-cally plausible object motion field learning. Considering the rigid body characteristics of dynamic objects, we de-sign a learning technique that effectively improves the capability to distinguish object’s motion boundary.
⋄ We show that the proposed scheme achieves favorable results in motion segmentation, monocular depth and scene flow estimation, and visual odometry on the KITTI,
Cityscapes, Waymo Open Dataset. 2.