Abstract 1.

Introduction
Metric learning has received conflicting assessments concerning its suitability for solving instance segmentation tasks.
It has been dismissed as theoretically flawed due to the shift equivariance of the employed CNNs and their respective inability to distinguish same-looking objects.
Yet it has been shown to yield state of the art results for a variety of tasks, and practical issues have mainly been reported in the context of tile-and-stitch approaches, where discontinuities at tile boundaries have been observed.
To date, neither of the reported issues have undergone thorough formal analysis.
In our work, we contribute a comprehensive formal analysis of the shift equivariance properties of encoder-decoder-style CNNs, which yields a clear picture of what can and cannot be achieved with metric learning in the face of same-looking objects.
In particular, we prove that a standard encoder-decoder network that takes d-dimensional images as input, with l pooling layers and pooling factor f , has the capacity to distinguish at most f dl same-looking objects, and we show that this upper limit can be reached. Furthermore, we show that to avoid discontinuities in a tile-and-stitch approach, assuming standard batch size 1, it is necessary to employ valid convolutions in combination with a training output window size strictly greater than f l, while at test-time it f l before stitching, is necessary to crop tiles to size n 1. We complement these theoretical findings by with n discussing a number of insightful special cases for which we show empirical results on synthetic and real data.
Code:https://github.com/Kainmueller-Lab/ shift_equivariance_unet
≥
·
*equal contribution
Metric learning is a popular proposal-free technique for instance segmentation that often yields state-of-the-art results, particularly in applications from the biomedical domain for which proposal-based techniques do not ap-ply [6, 7, 11, 13, 14, 19, 24]. In discord with its empirical success, numerous works from the computer vision commu-nity have noted a theoretical deficiency of metric learning for instance segmentation, namely that same-looking ob-jects cannot be distinguished by means of shift equivariant
CNNs [15, 19]. Empirical attempts at tackling this apparent deficiency include leveraging pixel coordinates or encod-ings of said as additional inputs or features [11, 18, 19, 27], or limiting the problem to distinguishing neighboring ob-jects [6, 12], while related theoretical work is limited to dis-cussions of shift equivariance properties of individual CNN layers like pooling [2, 25, 28] and upsampling [20].
What is thus lacking to date is a comprehensive for-mal analysis of the shift equivariance properties of the encoder-decoder style CNNs typically employed for metric-learning-based instance segmentation, as well as an assess-ment of respective implications concerning the capacity of said CNNs to distinguish same-looking objects. To this end, in this paper, we prove that an encoder-decoder-style CNN with l pooling layers and pooling factor f is periodic-f l shift equivariant, and in consequence has the capacity to distinguish at most f dl instances of identical appearance in d-dimensional input images.
Concerning practical issues, biomedical applications of-ten deal with large 3d input images and thus apply CNNs for instance segmentation in a tile-and-stitch manner to cope with GPU memory constraints. Here, issues with disconti-nuities in predictions at output tile boundaries, which lead to false splits of objects, have been reported [13, 21]. How-ever, again, a formal analysis of the causes is lacking to date. To this end, we show that the potential for discontinu-ities to arise is intricately tied to the shift equivariance prop-erties of the employed CNNs. We focus on metric learning with discriminative loss as a showcase [7], because it facili-tates theoretical insights via cleanly visible effects: Training for constant embeddings within individual instances conve-niently entails that discontinuities in predictions manifest as jumps. Our respective theoretical analysis entails sim-ple rules for designing CNNs that are necessary to avoid discontinuities when predictions are obtained in a tile-and-stitch manner. 2. Analysis of Shift Equivariance Properties
We first define the broad family of U-Net-style encoder-decoder CNNs [22] we consider, followed by a definition of periodic-t shift equivariance. Based on these prerequisites, we prove periodic-f l shift equivariance of U-Nets.
We consider CNNs consisting of l downsampling and l upsampling blocks. A downsampling block consists of a number of conv+nonlinearity layers, followed by max-pooling with downsampling factor (i.e. kernel size and stride) f . An upsampling block consists of a number of conv+nonlinearity layers, followed by upsampling by factor f , either via nearest-neighbor interpolation (fixed upsam-pling) or via transposed convolution (learnt upsampling).
At each downsampling level of the U-Net, skip connections concatenate the output of the downsampling block before pooling to the input of the respective upsampling block af-ter upsampling, except for the bottom level (also called bot-tleneck).
In the following, we refer to any achitecture of the above family as a U-Net, and to a U-Net with specific weights as a U-Net instance. A U-Net has the capacity to have some property iff there exists an instance of that
U-Net with said property. If not noted otherwise, we as-sume that a U-Net outputs all predictions for an image in one go. Sliding-window / tile-and-stitch mode will be dis-cussed in Section 2.1. Furthermore, if not noted otherwise, we assume valid convolutions in all conv layers. Non-valid padding will be discussed in Section 2.2.
Formally, a U-Net is a function U that maps a discrete, d-dimensional input image I with resolution X in
X in
X out
× d and C in channels to an output image with resolution and C out channels: 1 ×
X out d 1 ×
...
...
×
U : RX in
I (cid:55)→
..
X in
Cin
× 1 × d ×
→
U(I) = (ux(I))x
RX out 1 ×
..
×
X out
Cout d ×
,
X out 1 ×
∈
..
×
X out d (1) denotes the function that yields the output at output location x
. Concerning functions ux, two distinct notions of equality can be defined: 1 ×
X out d
X out
...
×
∈
Definition 1 (Absolute and Relative Equality). Two func-I : ux1(I) = ux2(I), tions ux1, ux2 are absolute-equal iff
∀ and absolute-distinct otherwise. Two functions ux1, ux2 are relative-equal iff x1 (I)), with
I : ux1 (I) = ux2(Tx2
∀
∆x) denoting an image shift by ∆x.
T∆x(I(x)) := I(x
−
Otherwise ux1 and ux2 are relative-distinct.
−
We provide examples for absolute and relative equality of
U-Net functions in Suppl. Sec. 1. Following [28], we define periodic-t shift equivariance as follows:
Definition 2 (Periodic-t Shift Equivariance). A function
F that maps an input image I to an output image F(I) is periodic-t shift equivariant iff F(T∆x(I)) = T∆x(F(I))
, and t is the smallest
∆x zi ∈
∀
| number for which this holds. t, ..., zd · (z1 ·
Z
}
∈ { t)
Lemma 1 (Relative-distinct functions u of a U-Net).
Every U-Net has the capacity to implement f dl relative-distinct functions u, but not more.
−
−
∈ {
∆x(T 0, .., f l
Proof 1. Part I: For any U-Net, we construct an instance and an image I with unique outputs ux
∆x(I)) for all
− d, proving that every U-Net has the ca-∆x 1
} pacity to implement at least f dl relative-distinct functions.
Part II: We prove that every U-Net is equivariant to image shifts f l, and hence no U-Net has the capacity to implement more than f dl relative-distinct functions.
Proof part I: We construct a U-Net instance and an input image I which yields f dl relative-distinct output function values, as described in the following. Fig. 1 shows a sketch of our construction for d = 1. First, for any given U-Net, construct an instance U with fixed upsampling (i.e. all up-sampling kernel weights set to 1), all convolutions set to identity, and ignore skip connections by setting respective convolution kernel entries to 0. For a d-dimensional input image I, this U-Net instance yields outputs f l+∆x)
∆x 0, ..., f l ux(I) = max
I(
{ x/f l
⌊
|
⌋·
∈ {
}
Second, construct a single-channel image I such that I(x) is strictly increasing for increasing positions x w.r.t. an or-dering of positions along diagonals first by the sum of their components and second by their components in increasing order (cf. [5, 23]):
− d 1
}
. where ux : RX in 1 ×
...
×
X in d ×
Cin
RCout
I
→ (cid:55)→ ux(I) = U(I)(x) (2) xi > xj ⇐⇒ x(k) j
 i > (cid:80) (cid:80) x(k)
 k i > x(k) x(k)
 k j k if (cid:80) k if (cid:80) k and x(l)
= (cid:80) x(k) i i = (cid:80) x(k) i = x(l) k x(k) j x(k) j j ∀ l < k (3)
̸
Figure 1: Illustration of a U-Net instance and a 1-dimensional image I such that the functions u∆x are relative-distinct for all
∆x with 0
∆x < f l.
≤
For this image, the maximum intensity in any image pixel block of edge length f l is found at the maximum position 1)d. Consequently, as each distinct pixel (f l block of edge length f l covers a unique maximum position, 1, ..., f l
−
− x
∀ ux
∆xi ̸
∀
∆xi(T
= ∆xj ∈{
∆xi(I)) 0, ..., f l
= ux d : 1
−
}
∆xj (T
∆xj (I)),
−
−
−
− (4) i.e. the constructed U-Net instance implements f dl relative-distinct functions u. Proof part II: See Suppl. Sec. 2.
Corollary 1 (Periodic-f l Shift Equivariance of U-Nets).
Every U-Net has the capacity to be periodic-f l shift equiv-ariant.
Proof 2. Directly follows from the proof of Lemma 1, which shows in Part I that every U-Net has the capacity to be non-equivariant to any shifts < f l, and in Part II that every
U-Net is shift equivariant to shifts f l. 2.1. Tile-and-stitch mode
In practice, to deal with limited GPU memory, a U-Net is commonly trained on fixed-size input image tiles, yielding fixed-size output tiles. At test time, the output for a full in-put image is then obtained in a tile-and-stitch manner, where it is common to employ the same tile size as during training, yet larger tile sizes are sometimes employed as inference is less memory-demanding than training.
Concerning shift equivariance in a tile-and-stitch ap-proach with output tile size w during inference, we get (1) periodic-f l shift equivariance within output tiles, and (2) trivially, periodic-w shift equivariance across output tiles.
Periodic-f l shift equivariance across the whole output only holds if w is a multiple of f l. 2.2. Non-valid padding
The concept of shift equivariance runs counter to the concept of non-valid padding, as the latter does not allow for “clean” input image shifts: Shifting+padding, in gen-eral, changes the input image beyond the shift. As a no-table consequence, e.g., zero padding renders a CNN with sufficiently large receptive field location-aware [10, 1], thus eviscerating shift equivariance. See [10] for an in-depth dis-cussion of zero-padding and other padding schemes. 3. Analysis of the Impact on Metric Learning for Instance Segmentation
We assess implications of a U-Net’s periodic-f l shift equivariance on the application of instance segmentation via metric learning with discriminative loss [7]. The respective loss function has three terms, a pull-force that pulls pixel embeddings towards their respective instance centroid, a push force that pushes centroids apart, and a penalty on em-bedding vector lengths. Given predicted embeddings, in-stances are inferred by mean-shift clustering. For more de-tails, see [7]. First, we assess how many “same-looking” in-stances a U-Net trained with discriminative loss can distin-guish. We call two instances “same-looking” iff the image itself is invariant to shifting by the offset between instance center points. Second, we show the necessity to follow a concise set of simple rules to avoid inconsistencies in a tile-and-stitch approach. 3.1. Distinguishing Same-looking Instances (thus
Avoiding False Merges)
Corollary 2. A U-Net has the capacity to distinguish at most f dl same-looking instances.
Proof 3. Lemma 1 entails that a U-Net can assign at most
̸
(a) Object spacing 16 pixels, i.e. a multiple of f l = 8. Learnt upsampling. (b) Object spacing 15 pixels, co-prime with f l = 8. Learnt upsampling. (c) Object spacing 15 pixels, co-prime with f l = 8. Fixed upsampling.
Figure 2: A U-Net with l pooling layers and pooling factor f cannot distinguish any instances in an f l-periodic d-dimensional image of same-looking instances (a). However, it can distinguish up to f d l instances in a p-periodic image of same-looking
· instances for p, f l co-prime (b,c). Showcase: l = 3, f = 2, f l = 8, f dl = 64. The red box in the input image (top left) shows the valid output window. Analogous results can be achieved for the same object spacings and l = 4, f = 2 (not shown). f dl different embeddings to a representative pixel of an ob-ject instance (say the “central pixel”), namely when posi-tioned at the f dl different relative locations w.r.t. its max-pooling regions. This holds true iff same-looking instances are located at offsets p with p, f l co-prime.
Whether a U-Net is also able to assign same embeddings to all pixels within any instance, thus yielding f dl correct segments, is up to its capacity and the success of training.
Corollary 3. A U-Net cannot distinguish same-looking in-stances located at offsets n f l, n
N.
·
∈
Proof 4. Periodic-f l shift equivariance of the U-Net entails that it necessarily assigns same embeddings to pixels at same relative locations in the objects.
Fig. 2 showcases Corollaries 2 and 3 on images of peri-odically arranged disks, for which we trained U-Nets with
R3. In partic-discriminative loss to predict embeddings ular, it shows that the upper bound of separating f dl same-looking instances, as stated in Corollary 2, can be reached.
∈ 3.2. Avoiding False Split Errors in Tile-and-Stitch
In the following, we analyze the impact of output tile size on training with discriminative loss, as well as on inference in a tile-and-stitch manner. To this end, we assess which of the f dl potentially relative-distinct output functions of a U-Net contribute to the loss, and which pairs of functions that predict directly neighboring outputs in a stitched solution contribute to an instance’s pull force loss term during train-ing. Fig. 3 exemplifies our analysis on a 1-d input image that contains a couple of two-pixel-wide instances.
Training output tile size < f l:
In this case, some of the f dl output functions of the U-Net never contribute to the loss, i.e. they are not explicitly trained. In effect, they may yield nonsensical predictions when used during inference.
Training output tile size = f l:
In this case, all output functions of the U-Net are considered during training in each batch. However, some pairs of functions that predict neighboring outputs during inference are never considered as neighbors during training. E.g. for d = 1, u0 and uf l 1
− never predict directly neighboring embeddings during train-ing, and hence never contribute to the pull force loss term as direct neighbors. They do, however, predict directly neigh-boring embeddings during inference, no matter if stitching f l-sized output tiles or employing larger output tiles (po-tentially alleviating the need for stitching) during inference.
Consequently, in this case, embeddings predicted at neigh-boring pixels at f l-grid-boundaries may be inconsistent.
Figure 3: Stitching errors occur when two relative-distinct output functions ui and uj are adjacent to each other during inference, but not during training. Shown here is a 1-d sketch with l = 3, f = 2, i.e. with f l = 8 relative-distinct output functions ui, and an exemplary input image containing two instances shown as black filled pixels (where each instance is two pixels wide). In (a) u7 is adjacent to u0 at the stitching boundary during inference, but during training they were never adjacent due to training output tile size f l. This is fixed in (b) where the training output tile size is > f l, but during inference u2 is adjacent to u0 which never occurred during training. In (c) the U-Net was trained as in (b) with training output tile size f l to ensure that only functions that were adjacent to each
> f l; During inference, however, output tiles are cropped to n other during training are adjacent at tile boundaries, thus allowing to overcome inconsistencies.
·
·
Training output tile size > f l: All possible direct neigh-borhoods of output functions are considered during training, given that inference output tile size is a multiple of f l. f l: Similar to the case of
Inference output tile size
= n training output tile size = f l, functions that predict neigh-boring outputs on two sides of a stitching boundary have never contributed to the same pull force term as neighbors during training (assuming batch size 1). Consequently, in-consistencies may occur at stitching boundaries. f l: Tile-and-stitch process-Inference output tile size = n ing is guaranteed to not be causal for any inconsistencies, as formalized by the following Corollary:
·
Corollary 4. If valid padding and output tiles of size n
· f l are employed, tile-and-stitch is equivalent to processing whole images at once.
Proof 5. This directly follows from identical arrangements of respective output functions, namely arrangement into a regular grid of d-dimensional blocks of size f dl. put window size w, and sufficiently large receptive field implements up to wd relative-distinct functions [10]. As-suming batch size 1, this yields inconsistencies at stitch-ing boundaries analogous to the valid-padding cases dis-cussed above.