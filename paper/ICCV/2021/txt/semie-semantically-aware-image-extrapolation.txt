Abstract
We propose a semantically-aware novel paradigm to per-form image extrapolation that enables the addition of new object instances. All previous methods are limited in their capability of extrapolation to merely extending the already existing objects in the image. However, our proposed ap-proach focuses not only on (i) extending the already present objects but also on (ii) adding new objects in the extended region based on the context. To this end, for a given im-age, we first obtain an object segmentation map using a state-of-the-art semantic segmentation method. The, thus, obtained segmentation map is fed into a network to com-pute the extrapolated semantic segmentation and the cor-responding panoptic segmentation maps. The input image and the obtained segmentation maps are further utilized to generate the final extrapolated image. We conduct exper-iments on Cityscapes and ADE20K-bedroom datasets and show that our method outperforms all baselines in terms of FID, and similarity in object co-occurrence statistics.
Project url: https://semie-iccv.github.io/ 1.

Introduction
Image extrapolation or out-painting refers to the problem of extending an input image beyond its boundaries. While the problem has applications in virtual reality, sharing pho-tos on social media like Instagram, and even generating scenes during game development especially if the scenes are repetitive, it is relatively under-explored compared to the image inpainting counterpart, which has been extensively researched. Image inpainting solutions based on deep net-works and generative adversarial networks (GANs), when applied to the out-painting problem, have been shown to yield poor results [36]. This has led to researchers explor-ing and proposing new solutions to the out-painting prob-lem [47, 41, 38]. However, the solutions have been mainly
*BK, SRD, AB contributed to the work during their Adobe internship.
Figure 1. Illustration of our results. Dotted white rectangle refers to the input image. Our method not only extrapolates the objects present in the input but also generates new objects (blue bounding boxes) while maintaining the texture consistency. restricted to images that involve outdoor domains like natu-ral scenes where the problem is limited to just extending the existing textures for ‘stuff’ classes like mountains, water, trees [11, 36] or single-object images of classes like faces, flowers, and cars. These methods are not suitable to other domains like traffic scenes and indoor scenes where a desir-able image extrapolation necessitates 1) extending not only the ‘stuff’ classes but also the ‘things’ classes like cars, per-sons, beds, tables that have very definite structure as well as 2) adding new objects based on the context that were not present in the original image. So, why cannot we use the existing techniques [47, 41, 38, 36] for such domains? The answer is they fail spectacularly by filling the extrapolated region with artifacts (see figures 5 and 6). They attempt to extrapolate the image by capturing the low-level statistics like textures and colors from the input image while ignor-ing the high-level information like object semantics and ob-ject co-occurrence relationships. In short, they are limited in their ability to perform satisfactory image extrapolation that demands the creation of new object instances and the extension of multiple objects from diverse classes.
We address the shortcomings of the previous works by extrapolating the image in the semantic label map space, which enables us to generate new objects in the extrapo-lated region. Additionally, semantic label maps belong to a lower dimensional manifold than images, making it easier to extrapolate them. However, just having a semantic label map does not allow us to have control over every instance in the extrapolated image. We propose to generate an es-timate of the panoptic label directly from the extrapolated semantic label map, different from [20, 4]. Instance bound-ary maps obtained from panoptic labels also help in cre-ating crisper boundaries between objects belonging to the same semantic category. Unlike semantic label map to im-age generation [31, 15, 37], we have to maintain texture consistency between the input region and the extrapolated output. To account for this, we propose Instance-aware con-text normalization (IaCN), which leverages the estimated panoptic label maps to transfer average color information as a feature map for texture consistency in the extrapolated object instances. In addition, we propose the use of patch co-occurrence discriminator [32] to maintain global texture similarity in input and extrapolated region.
Our contributions can be summarized below:
• We propose a novel paradigm for image out-painting by extrapolating the image in the semantic label space to generate novel objects in the extrapolated region.
• We propose the generation of panoptic label maps from the extrapolated semantic label maps to facilitate the generation of high quality object boundaries in the ex-trapolated image.
• We propose Instance-aware Context Normalization (IaCN) and the use of patch co-occurrence discrimi-nator to maintain texture consistency of extrapolated instances. experiments on Cityscapes
Through extensive and
ADE20K datasets, we show that our method outperforms all previous state-of-the-art methods in terms of FID and similarity in object co-occurrence metrics. 2.