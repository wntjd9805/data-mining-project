Abstract
Do GANs replicate training images? Previous studies have shown that GANs do not seem to replicate training data without significant change in the training procedure.
This leads to a series of research on the exact condition needed for GANs to overfit to the training data. Although a number of factors has been theoretically or empirically identified, the effect of dataset size and complexity on GANs replication is still unknown. With empirical evidence from
BigGAN and StyleGAN2, on datasets CelebA, Flower and
LSUN-bedroom, we show that dataset size and its complex-ity play an important role in GANs replication and percep-tual quality of the generated images. We further quantify this relationship, discovering that replication percentage decays exponentially with respect to dataset size and com-plexity, with a shared decaying factor across GAN-dataset combinations. Meanwhile, the perceptual image quality fol-lows a U-shape trend w.r.t dataset size. This finding leads to a practical tool for one-shot estimation on minimal dataset size to prevent GAN replication which can be used to guide datasets construction and selection. 1.

Introduction
Generative Adversarial Networks (GANs) has attracted consistent attention since its first proposal in [9]. Since then, the photorealism of the synthetic images has seen dramatic improvement with methods like BigGAN [2], StyleGAN
[12, 13], etc. This low cost generation of photo-realistic samples brings new possibility to applications like content creation and dataset augmentation, all of which are based on the assumption that the generator does not merely repli-cate training data. The GAN replication (or memorization, overfitting [4, 3]) problem, besides its obvious theoretical value, is also practically important.
A number of recent studies have explored the option of
Qianli Feng and Chenqi Guo contribute equally to the paper.
Code is available at https://github.com/chenqiguo/GAN_ replication. Authors were supported by NIH grant R01DC014498,
R01EY020834 and HFSP grant RGP0036/2016.
Figure 1: An example of GAN replications in Flower dataset. The complexity is measured by Intrinsic Dimen-sionality. We study the condition of GAN replication with a particular focus on the effect dataset size/complexity. For a given image synthesis task, when the dataset size decreases, the replication become more prominent. using GANs to augment training datasets to improve the performance of downstream machine learning algorithms
[5, 25, 24, 6], especially for medical applications where patients data is scarce. When these data augmentation
GANs overfit to the training data, the augmented samples will provide little or no additional value to the downstream algorithms, defying the purpose of augmenting the data.
Replicating training samples is also problematic for con-tent creation due to potential copyright infringement [7].
This is more problematic when the GAN based face swap-ping technique is used for preserving patient privacy (de-identification) as used in [30]. If a replication happens dur-ing the swapping process, the technique is then protecting one’s privacy by costing another’s portrait right and poten-tially spreading mis-information about the individual. With-out deep understanding on the mechanism and conditions
of GANs replication, one might even found the issue more complicated as the replication itself might not be an inten-tional action from the user or the creator of the GAN. Thus, it is highly important to further our understanding on the replication behavior of GANs.
Fortunately, researchers have already started to look into the underlying mechanism and contributing factors of potential GANs replication/memorization [26, 19, 17, 1], which we discuss in Section 2. These works, although pro-viding important insight on the effect of factors like latent code, discriminator complexity, have not yet explored the role of dataset size and complexity in GANs replication.
In this paper, we attempt to fill this gap by empiri-cally study the relationship between GANs replication and dataset size/complexity. We show that it is possible for
GANs to replicate with unmodified training procedure, challenging the common view that GANs tend not to mem-orize training data under a normal training setup [26]. This finding not only sheds new light on the potential mechanism of GAN replication, but also provides practical guidance on estimating minimal dataset size when new a dataset is being constructed for image synthesis purpose. 2.