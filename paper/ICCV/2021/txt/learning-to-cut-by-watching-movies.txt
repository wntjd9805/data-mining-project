Abstract
Video content creation keeps growing at an incredible pace; yet, creating engaging stories remains challenging and requires non-trivial video editing expertise. Many video editing components are astonishingly hard to automate pri-marily due to the lack of raw video materials. This paper fo-cuses on a new task for computational video editing, namely the task of raking cut plausibility. Our key idea is to leverage content that has already been edited to learn fine-grained audiovisual patterns that trigger cuts. To do this, we first collected a data source of more than 10K videos, from which we extract more than 255K cuts. We devise a model that learns to discriminate between real and artificial cuts via contrastive learning. We set up a new task and a set of base-lines to benchmark video cut generation. We observe that our proposed model outperforms the baselines by large mar-gins. To demonstrate our model in real-world applications, we conduct human studies in a collection of unedited videos.
The results show that our model does a better job at cutting than random and alternative baselines. 1.

Introduction
The lack of video editing expertise is a common blocker for aspiring video creators. It takes many training hours and extensive manual work to edit videos that convey engaging stories. Arguably, the most time-consuming and critical task in video editing is to compose the right cuts, i.e., decide how (and when) to join two untrimmed videos to create a single clip that respects continuity editing [58]. To the untrained eye, cutting might seem easy; however, experienced editors spend hours selecting the best frames for cutting and joining clips. In light of this complexity, it is pertinent to ask: could artificial systems rank video cuts by how plausible they are?
Before delving into addressing the question above, it is worth defining the task of video cut ranking in detail. As
Figure 1 illustrates, given two untrimmed input videos, the goal is to find the best moments (in each video) to trigger cuts, which join the pair into a single continuous sequence.
Figure 1: Ranking Cut Plausibility. We illustrate the pro-cess of ranking video cuts. The first row show a pair of untrimmed videos (raw footage) as the input. The output would be the ranking of all the possible cuts across the pair of shots. Ideally, the top ranked cuts should be the more plausible cuts providing a smooth transition between the shots, and the worst cuts would be places in where there is break of spatial-temporal continuity.
A key challenge is to generate videos that make the audience believe actions unfold continuously. This type of cutting is often called continuity editing and aims to evoke an illusion of reality [7, 58], even though, the source videos could be recorded at different times. Figure 1 shows a typical trigger for cuts – the moment when the speaker changes. In practice, the director could give the shot order via a storyboard or script, and it is the editor’s job to realize which patterns make smooth transitions between shots. Our hypothesis is that many of those cut-trigger patterns can be found by carefully analyzing of audio-visual cues.
Despite its importance, potential impact, and research
challenges, the computer vision community has overlooked the video cut ranking problem. While there has been signifi-cant progress in shot boundary detection [52], video scene segmentation [57], video summarization [48], and video-story understanding [9, 33], few to no works have focused on pushing the envelope of computational video editing.
The most relevant works at addressing video cut ranking and generation are found in the graphics and HCI commu-nities [10, 19, 41, 65, 69]. These attempts take on a different perspective and focus on human-computer experiences for faster video editing. Yet, they still demand extensive work from an editor in the loop. We hypothesize that the cut rank-ing task has been neglected due to the lack of data, i.e., raw footage, and its corresponding cuts done by an editor.
In this paper, we introduce the first learning-based method to rank the plausibility of video cuts. It is important to note that we do not have access to the raw footage for each shot since it is difficult, i.e., requires expertise and extensive manual work, to gather a dataset of raw videos with the corresponding edits and cuts. Therefore, Our key idea is to leverage edited video content to learn the audio-visual patterns that commonly trigger cuts.
While this data is not the same as the real-world input for generating and ranking cuts, we can still model the audio-visual data before and after the cut, thus modeling what good cuts look like. Additionally, this type of data can be found abundantly, which enables the development of data-driven models. Our results show that a model learned to solve this proxy task can be leveraged to practical use cases.
Our approach begins with a pair of consecutive shots that form a cut (similar to the bottom row in Figure 1). We look for a representation that discriminates between the good cuts (actual cuts found on edited video) against all alternative options (random alignments). To achieve this goal, we first collect a large-scale set of professionally edited movies, from which we extract shot boundaries to create more than 260K cuts and shot pairs. Using this new dataset, we train an audio-visual model, Learning-to-Cut, which learns to rank cuts via contrastive learning. Our experimental results show that, while challenging, it is possible to build data-driven models to rank the plausibility of video cuts, improving upon random chance and other standard audio-visual baselines.
Contributions. To the best of our knowledge, we are the first to address video cut ranking from a learning-based per-spective. To this end, our work brings two contributions. (1) We propose Learning-to-Cut, an audio-visual approach based on contrastive learning. Our method learns cross-shot patterns that trigger cuts in edited videos (Section 3). (2) We introduce a benchmark and performance metrics for video cut ranking, where we show the effectiveness of Learn-ing to Cut. Moreover, we showcase that expert editors more likely prefer the cuts generated by our method as compared to cuts randomly ranked and other baselines (Section 4). 2.