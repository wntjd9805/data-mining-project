Abstract
In this paper, we propose Recurrent Multi-Scale Fea-ture Modulation (R-MSFM), a new deep network architec-ture for self-supervised monocular depth estimation. R-MSFM extracts per-pixel features, builds a multi-scale fea-ture modulation module, and iteratively updates an inverse depth through a parameter-shared decoder at the fixed res-olution. This architecture enables our R-MSFM to main-tain semantically richer while spatially more precise rep-resentations and avoid the error propagation caused by the traditional U-Net-like coarse-to-fine architecture wide-ly used in this domain, resulting in strong generalization and efficient parameter count. Experimental results demon-strate the superiority of our proposed R-MSFM both at model size and inference speed, and show the state-of-the-art results on the KITTI benchmark. Code is available at https://github.com/jsczzzk/R-MSFM 1.

Introduction
The objective of depth estimation is to determine the depth of each pixel in an image. From the early stages of computer vision, depth estimation from images has al-ways been one of the major challenges for researchers.
Depth estimation as a low-level task is crucial to complete higher-level tasks, including 3-D reconstruction[23], au-tonomous driving[6], 3-D target detection[36], underwater image restoration[43], and many more.
Depth estimation is traditionally regarded as a stereo matching problem between the left and right images, which mainly approaches as a hand-crafted optimization[18], supervised[4] or self-supervised manner[24]. Although decades of developments have significantly improved its accuracy, the time-consuming matching process inevitably
Inspired by the do-limits the scope of the deployment. main of traditional structure from motion (SFM)[38], re-∗Corresponding author. cent studies[13, 2, 31, 14] have demonstrated the feasi-bility of estimating depth from a single image as a view-synthesis problem using a photometric reconstruction loss in a self-supervised manner. Following this successful, new design paradigm, recent works mostly focus on the design-ing of the specific loss function[28, 14] and the improve-ments to the currently widely used U-Net like coarse-to-fine architecture[19, 15].
In this paper, inspired by the domain of optical flow [37], we introduce Recurrent Multi-Scale Feature Modulation(R-MSFM) , a new and effective lightweight deep learning ar-chitecture, to extend the architecture choice for monocular depth estimation. The three most significant strengths of
R-MSFM are as follows:
• Lightweight architecture: R-MSFM reduces the pa-rameters of Monodepth2 by 73 percent, from 14.3M to 3.8M, which is suitable for memory-limited scenarios.
• State-of-the-art accuracy: R-MSFM achieves state-of-the-art performance, which gets a 4.470 RM SE lower than Monodepth2 (4.701) on the KITTI Eigen split test set.
• Reasonable inference speed.: R-MSFM processes 640 × 192 videos at 44 frames per second on a
RTX2060 GPU. It can flexibly choose the update num-ber and get a trade-off between speed and accura-cy with half of the overall updates, which runs on 72 frames per second while still outperforms Mon-odepth2.
R-MSFM consists of four main components: i) a depth encoder that extracts the per-pixel representations from
ResNet18 except for the last two blocks, producing multi-scale features; ii) a parameter-shared depth decoder that it-eratively updates an inverse depth initialized at zero, avoid-ing the spatial imprecision at coarse level propagating to fine part; iii) a parameter-learned upsampling module that adaptively upsamples the estimated inverse depth, preserv-ing its motion boundaries; iv) a multi-scale feature modu-lation module that modulates content across the multi-scale feature maps, maintaining semantically richer while spatial-ly more precise representations for each iterative update.
At each iterative update, R-MSFM maintains and re-fines a single inverse depth at the fixed 1/8 input reso-lution and then directly upsamples it to the full resolu-tion with the learned mask. This is different from tra-ditional U-Net like coarse-to-fine architecture in previous works[14, 15, 45, 19], where depth is first estimated at coarse resolution(1/32 input resolution) and then gradually upsampled and refined until the full resolution. By the pro-gressive refinement at the fixed fine resolution, R-MSFM overcomes several limitations of the coarse-to-fine architec-ture: the error propagation from coarse to fine resolution, the difficulty of delineating small objects, the independence of the multi-scale decoders. Experimental results show R-MSFM achieves state-of-the-art performance both at accu-racy and model size with reasonable inference speed. 2.