Abstract
Active speaker detection requires a mindful integration of multi-modal cues. Current methods focus on modeling and fusing short-term audiovisual features for individual speakers, often at frame level. We present a novel approach to active speaker detection that directly addresses the multi-modal nature of the problem and provides a straightfor-ward strategy, where independent visual features (speak-ers) in the scene are assigned to a previously detected speech event. Our experiments show that a small graph data structure built from local information can approximate an instantaneous audio-visual assignment problem. More-over, the temporal extension of this initial graph achieves a new state-of-the-art performance on the AVA-ActiveSpeaker dataset with a mAP of 88.8%. 1.

Introduction
Active speaker detection aims at identifying the cur-rent speaker (if any) from a set of candidate face detec-tions in an arbitrary video. This research problem is an inherently multi-modal task that requires the integration of subtle facial motion patterns and the characteristic wave-form of speech. Despite its multiple applications such as speaker diarization [3, 44, 46, 48], human-computer inter-action [16, 58] and bio-metrics [34, 40], the detection of active speakers in-the-wild remains an open problem.
Current approaches for active speaker detection are based on recurrent neural networks [41, 43] or 3D convo-lutional models [1, 6, 60]. Their main focus is to jointly model the audio and visual streams to maximize the per-formance of single speaker prediction over short sequences.
Such an approach is suitable for single speaker scenarios, but is overly simplified for the general (multi-speaker) case.
The general (multi-speaker) scenario has two major chal-lenges. First, the presence of multiple speakers allows for incorrect face-voice assignations. For instance, false posi-tives emerge when facial gestures closely resemble the mo-tion patterns observed while speaking (e.g. laughing, grin-ning). Second, it must enforce temporal consistency over multi-modal data, which quickly evolves over time, e.g., when active speakers switch during a fluid conversation.
In this paper, we address the general multi-speaker prob-Figure 1. Audiovisual assignment for active speaker detection.
Active speaker detection is highly ambiguous. Even if we analyze joint audiovisual information, unrelated facial gestures can easily resemble the natural motion of lips while speaking. In a) we show two face crops from a sequence, where a speech event was de-tected. The gestures, illumination, and capture angle make it hard to asses which face (if any) is the active speaker. Our strategy b) focuses on the attribution of speech segments in video. If a speech event is detected, we holistically analyse every speaker along with the audio track to discover the most likely active speaker. lem in a principled manner. Our key insight is that, instead of optimizing active speaker predictions over individual au-diovisual embeddings, we can jointly model a set of visual representations from every speaker in the scene along with a single audio representation extracted from the shared au-dio track. While simple, this modification allows us to map the active speaker detection task into an assignation prob-lem, whose goal is to match multiple visual representations with a singleton audio embedding. Figure 1 illustrates some of the challenges in active speaker detection and provides a general insight for our approach.
Our approach, dubbed “Multi-modal Assignation for
Active Speaker detection” (MAAS) relies on multi-modal graph neural networks [27, 50] to approach the local (frame-wise) assignation problem, but it is flexible enough to also propagate information from a long-term analysis window by simply updating the underlying graph connectivity. In this framework, we define the active speaker as the local visual representation with the highest affinity to the audio embedding. Our empirical findings highlight that reformu-lating the problem into a multi-modal assignation problem brings sizable improvements over current state-of-the-art
methods. On the AVA Active speaker benchmark, MAAS outperforms all other methods by at least 1.7%. Addition-ally, when compared with methods that analyze a short tem-poral span, MAAS brings a performance boost of at least 1.1%.
Contributions. This paper proposes a novel strategy for ac-tive speaker detection, which explicitly learns multi-modal relationships between audio and facial gestures by sharing information across modalities. Our work brings the follow-ing contributions: (1) We devise a novel formulation for the active speaker detection problem. It explicitly matches the visual features from multiple speakers to a shared audio em-bedding of the scene (Section 3.2). (2) We empirically show that this assignation problem can be solved by means of a
Graph Convolutional Network (GCN), which endows flex-ibility on the graph structure and is able to achieve state (3) We present a novel of the art results (Section 4.1). dataset for active speaker detection, called “Talkies”, as a new benchmark composed of 10,000 short clips gathered from challenging and diverse scenes (Section 5).
To ensure reproducible results and promote future re-search, all the resources of this project, including source code, model weights, official benchmark results, and la-beled data will be publicly available. 2.