Abstract 1.

Introduction
Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construc-tion of these parametric models is often tedious, as it re-quires heavy manual tweaking, and they struggle to rep-resent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Paramet-ric Models (NPMs), a novel, learned alternative to tradi-tional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representa-tions of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional paramet-ric model, e.g., SMPL. This enables NPMs to achieve a sig-nificantly more accurate and detailed representation of ob-served deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space in-terpolation as well as shape / pose transfer experiments fur-ther demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.
Modeling deformable surfaces is fundamental towards understanding the 4D world that we live in, as well as cre-ating or manipulating dynamic content. While significant progress has been made in understanding the reconstruction of 3D shapes [12, 15, 16, 46, 37, 31], representing dynamic, deforming surfaces remains challenging.
Over the past years, parametric 3D models have seen re-markable success for domain-specific representations, such as for human bodies (e.g., SCAPE [2], SMPL [26], Adam
[20]), hands (MANO [43]), animals (SMAL [50]) and faces ([39], FLAME [23], [40]). These models have enabled a wide range of exciting applications and are instrumental in modeling deformable 3D objects. However, the construc-tion of such a parametric model is a rather complex and te-dious task, requiring notable manual intervention and incor-poration of object-specific constraints in order for the para-metric model to well-represent the space of possible shapes and deformations. Moreover, such parametric models of-ten struggle to represent additional complexity and details of deforming shapes, e.g., clothing, hair, etc.
We propose Neural Parametric Models (NPMs), an al-ternative formulation to traditional parametric 3D models where we learn a disentangled shape and pose representa-tion that can be used like a traditional parametric model to fit to new observations. We leverage the representation
power of implicit functions to learn disentangled shape and pose spaces from a dataset that does not require surface reg-istration among all samples; this flexibility enables training on a wider variety of data. We also do not make object-specific assumptions about the kinematic chain, the number of parts or the skeleton. For training, our approach only requires that the same identity or shape can be seen in dif-ferent poses, including a canonical pose.
Once trained, we can leverage our learned shape and pose representations as regularized spaces to be smoothly optimized over to fit to new observations at test time. Addi-tionally, our disentangled implicit representations of shape and pose enable modeling arbitrary connectivity and topol-ogy, as well as finer-scale levels of detail. Thus, optimizing over our shape and pose spaces during inference enables representation of a globally consistent shape and temporally consistent poses while maintaining geometric fidelity.
Given a dataset of various shape identities and possi-bly different topologies, as well as various deformations of each shape identity (but without requiring registration of any identity to the others), we then train both shape and pose spaces in auto-decoder fashion. We learn a shape code for each identity, with shape codes representing an SDF of the shape geometry. Pose codes represent the flow field from the canonical shape of an identity to a given posed shape of the same identity. Flow predictions are conditional on both a shape and a pose latent code, in order to represent shape-dependent deformations as well as to help learning disentangled shape and pose spaces.
We demonstrate our Neural Parametric Models on the task of reconstruction and tracking of monocular depth se-quences, as well as their capability in shape and pose trans-fer and interpolation.
In comparison with state-of-the-art parametric 3D models and implicit 4D representations, our
NPMs capture higher-quality reconstructions with finer de-tail and more accurate non-rigid tracking.
In summary, we present the following key contributions:
• We propose an alternative formulation to traditional parametric 3D deformable models, where shape and pose are disentangled into separate latent spaces via two feed-forward networks that are learned from data alone, i.e., without requiring domain-specific knowl-edge such as the kinematic chain or number of parts.
• Importantly, our approach shows regularization capa-bilities that enable test-time optimization over the la-tent spaces of shape and pose to tackle the challenging task of fitting a model to monocular depth sequences, while retaining the detail present in the data. 2.