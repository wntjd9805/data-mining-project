Abstract
Learning a good representation for space-time corre-spondence is the key for various computer vision tasks, including tracking object bounding boxes and performing video object pixel segmentation. To learn generalizable representation for correspondence in large-scale, a vari-ety of self-supervised pretext tasks are proposed to explic-itly perform object-level or patch-level similarity learning.
Instead of following the previous literature, we propose to learn correspondence using Video Frame-level Similar-ity (VFS) learning, i.e, simply learning from comparing video frames. Our work is inspired by the recent success in image-level contrastive learning and similarity learning for visual recognition. Our hypothesis is that if the repre-sentation is good for recognition, it requires the convolu-tional features to ﬁnd correspondence between similar ob-jects or parts. Our experiments show surprising results that
VFS surpasses state-of-the-art self-supervised approaches for both OTB visual object tracking and DAVIS video ob-ject segmentation. We perform detailed analysis on what matters in VFS and reveals new properties on image and frame level similarity learning. Project page with code: https://jerryxu.net/VFS. 1.

Introduction
Learning visual correspondence across space and time is one of the most fundamental problems in computer vi-sion. It is widely applied in 3D reconstruction, scene un-derstanding, and modeling object dynamics. The research of learning correspondence in videos can be cast into two categories: the ﬁrst one is learning object-level correspon-dence for visual object tracking [62, 60, 69], relocalizing the object with bounding boxes along the video; the other one is learning ﬁne-grained correspondence, which is com-monly applied in optical ﬂow estimation [31, 21] and video object segmentation [9, 80]. While both lines of research have been extensively explored, most approaches acquire training supervision from simulations or limited human an-notations, which increases the difﬁculty for generalization
Figure 1. Video Frame-level Similarity (VFS) learning. It com-pares the fully-connected layer embeddings of frames from the same video for learning. By minimizing the frame-level feature distance, the ﬁne-grained and object-level correspondence can au-tomatically emerge in res4 and res5 blocks in the ResNet architec-ture, without using any explicit tracking-based pretext task. across different data and tasks.
One way to tackle this problem is to learn representa-tions for correspondence using free temporal supervision signals in videos. Recently, a lot of efforts have been made in self-supervised learning of space-time visual correspon-dence [70, 74, 33], where different pretext tasks are de-signed to learn to track pixels or objects in videos. For ex-ample, Wang et al. [74] propose to use the cycle-consistency of time (i.e., forward-backward object tracking) as a super-visory signal for learning. Building on this, Jabri et al. [33] combine the cycle-consistency of time with patch-level sim-ilarity learning and achieve a signiﬁcant improvement in learning correspondence. Given this encouraging result, we take a step back and ask the questions: Do we really need to design self-supervised object (or patch) tracking task ex-plicitly to learn correspondence? Can image-level similarity learning alone learn the correspondence?
Recent development in image-level similarity learning (e.g., contrastive learning) has shown the self-supervised
representation can be applied to different downstream se-mantic recognition tasks, and even surpassing the ImageNet pre-training networks [27, 12, 13, 10, 51, 77, 29, 66, 67].
Our hypothesis is that if the higher-level fully-connected layer feature encodes the object structure and semantic in-formation, it needs to be supported by the ability of ﬁnd-ing correspondence between similar object instances and between object parts [83]. This forces the convolutional representation to implicitly learn visual correspondence.
With this hypothesis, we propose to perform Video
Frame-level Similarity (VFS) learning for space-time corre-spondence without any explicit tracking-based pretext task.
As illustrated in Figure 1, we forward one pair or multi-ple pairs of frames from the same video into a siamese network, and compute the similarity between the frame-level features (fully-connected layer embeddings) for learn-ing the network representation. We examine the learning with negative pairs as [12, 27] and without negative pairs as [26, 14] under our VFS framework. We build our model based on the ResNet architecture [28]. During inference, we use the res4 features for ﬁne-grained correspondence task (e.g., DAVIS object segmentation [57]) and the res5 features for object-level correspondence task (e.g., OTB ob-ject tracking [76]). Surprisingly, we ﬁnd VFS can surpass state-of-the-art self-supervised correspondence learning ap-proaches [33, 43]. Based on our experiments, we observe the following key elements for VFS: (i) Training with large frame gaps and multiple frame pairs improves correspondence. When sampling a pair of frames from the same video for similarity learning, we ob-serve that increasing the time differences between the two frames can improve ﬁne-grained correspondence noticeably on DAVIS ( 3%), and object-level correspondence signiﬁ-cantly on OTB (> 10%). Training with multiple frame pairs at the same time achieves further improvement.
∼ (ii) Training with color augmentation is harmful for ﬁne-grained correspondence, but beneﬁcial for object-level cor-respondence. With color augmentation, the performance on 3%) while it signiﬁ-the DAVIS dataset is decreased ( cantly improved performance on the OTB dataset ( 10%), which indicates the feature learns better object invariance.
∼
∼ (iii) Training without negative pairs improves both ﬁne-grained and object-level correspondences. Recent literature has shown that similarity learning for visual representation is achievable even without negative pairs [26, 14]. While the results are surprising, it was still unclear how it can be beneﬁcial for performance. In this paper, we show that VFS without negative training pairs can improve representations for different levels of correspondence. (iv) Training with deeper networks gives signiﬁcant im-provements. While deeper networks generally improves recognition performance, it is not the case when training with self-supervised tracking pretext tasks for correspon-dence. We observe very small improvement or even worse correspondence results when using ResNet-50 compared to
ResNet-18 with previous approaches [74, 33, 43]. When learning correspondence with VFS implicitly, we achieve much better performance when using a deeper model.
Given our detailed analysis and state-of-the-art perfor-mance, we hope VFS can serve as a strong baseline for self-supervised correspondence learning. The study of in-termediate representations for correspondence also provides a better understanding on what image-level self-supervised similarity learning has learned. Finally, VFS also reveals the new property of similarity learning without negative pairs: it improves both object-level and ﬁne-grained cor-respondence. 2.