Abstract
Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, we reduce natural accuracy degradation. We use the model logits from one clean model to guide learning of an-other one robust model, taking into consideration that logits from the well trained clean model embed the most discrim-inative features of natural data, e.g., generalizable classi-fier boundary. Our solution is to constrain logits from the robust model that takes adversarial examples as input and makes it similar to those from the clean model fed with cor-responding natural data. It lets the robust model inherit the classifier boundary of the clean model. Moreover, we ob-serve such boundary guidance can not only preserve high natural accuracy but also benefit model robustness, which gives new insights and facilitates progress for the adversar-ial community. Finally, extensive experiments on CIFAR-10,
CIFAR-100, and Tiny ImageNet testify to the effectiveness of our method. We achieve new state-of-the-art robustness on
CIFAR-100 without additional real or synthetic data with auto-attack benchmark 1. Our code is available at https:
//github.com/dvlab-research/LBGAT. 1.

Introduction
Deep neural networks have achieved great success in many tasks, especially with the surge of neural architecture search [58, 24, 40, 11, 3]. However, with the concern of se-curity of deep models, several methods [14, 51, 39, 36, 43, 57, 43, 17, 20, 37] have shown that deep models could be vulnerable to adversarial attack. Data that is intentionally created may easily fool strong classifiers.
In response to the vulnerability of deep neural networks, adversarial defense has become an essential topic in com-puter vision. There are now a sizable body of work ex-ploring different ways to get adversarial settings, including defensive distillation [30], feature squeezing [53], random-ization based methods [49, 13] and augmenting the training 1https://github.com/fra31/auto-attack
Figure 1: Model robustness on CIFAR-100 evaluated with 20 iterations PGD under white-box attack. “Natural Acc” represents classification accuracy on natural (clean) data.
“Robust Acc” represents classification accuracy on adver-sarial data. Our method (LBGAT+TRADES with α = 0) improves robustness with the least natural accuracy degra-dation. with adversarial examples [56, 21, 27, 43], i.e., adversarial training. However, training a robust model is still challeng-ing. Recently, adversarial training with PGD attack [27] becomes an effective defense strategy. However, when plot-ting results of recent work [56, 21, 27] in Fig. 1, it is still noticeable that higher robustness is often accompanied with more accuracy degradation on natural data classification.
Different from previous work that mainly pursues var-ious ways to improve robustness, we meanwhile pursue
In this paper, we accuracy preservation on natural data. propose a novel adversarial training scheme, which signif-icantly improves classification accuracy on natural data. It also achieves high robustness under black- and white-box attack. We take advantage of logits from a clean model, which is trained only on natural data, to guide the learning of a robust model.
A conceptual illustration is shown in Fig. 2 to explain our motivation. As shown in (a), when only trained on natu-ral (clean) data, the learned model Mnatural separates nat-ural data (plotted in yellow) well. But it may fail to clas-sify perturbed data and misclassifies the dark circle into the
Figure 2: Conceptual illustration of our method vs. previous adversarial training approaches. Solid lines denote real classifier boundary of the trained model, while the dotted line is the classifier boundary of the clean model Mnatural. Different shapes represent logits of images in various classes. Black color marks adversarial examples. rectangle category. Previous standard adversarial training methods, e.g., Madry et al. [27], mainly improve the ro-bustness towards adversarial examples. As shown in Fig. 2(c), adversarial examples (plotted in black) can be mostly correctly classified with this strategy. However, some clean data is wrong. Thus, our motivation is to leverage the clean model Mnatural to improve the natural data accuracy of
Mrobust.
In order to seek guidance from clean model Mnatural, we expect the logit output of adversarial example xadv from
Mrobust to be similar to logits output of corresponding nat-ural data x that goes through Mnatural. As plotted in Fig. 2(b), the classifier boundary of our Mrobust is constrained by that of the clean model, which helps classify the clean data into correct categories. At the same time, adversarial examples are also correctly labeled, benefiting from the ad-versarial training scheme.
Instead of constraining Mrobust with the classifier boundary from one well trained static Mnatural, we fur-ther generalize our method to Learnable Boundary Guided
Adversarial Training (LBGAT) by training Mnatural and our required model Mrobust at the same time to dynami-cally adjust the classifier boundary of Mnatural and learn the most robustness-friendly one to further help Mrobust enhance robustness. To show the flexibility of our method, we incorporate our model into state-of-the-art methods Ad-versarial Logit Pairing (ALP) [21] and TRADES [56] re-spectively and accomplish remarkable improvement over the baselines. Interestingly, in our exploration, we observe the classifier boundary guidance from Mnatural can also enhance model robustness, which gives us new insights and potentially facilitates progress for adversarial robustness.
We conduct experiments on CIFAR-10, CIFAR-100, and
Tiny ImageNet to evaluate the performance of our mod-els under both white- and black-box attacks. Our mod-els achieve impressive performance on these datasets and outperform previous work in a large margin. Particularly, we achieve state-of-the-art model robustness on CIFAR-100 without extra real or synthetic data under current the most popular auto-attack. 2.