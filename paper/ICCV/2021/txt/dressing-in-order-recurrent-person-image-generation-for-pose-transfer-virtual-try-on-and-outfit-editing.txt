Abstract
We proposes a ﬂexible person generation framework called Dressing in Order (DiOr), which supports 2D pose transfer, virtual try-on, and several fashion editing tasks.
The key to DiOr is a novel recurrent generation pipeline to sequentially put garments on a person, so that trying on the same garments in different orders will result in different looks. Our system can produce dressing effects not achiev-able by existing work, including different interactions of garments (e.g., wearing a top tucked into the bottom or over it), as well as layering of multiple garments of the same type (e.g., jacket over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each garment, enabling these ele-ments to be edited separately. Joint training on pose trans-fer and inpainting helps with detail preservation and coher-ence of generated garments. Extensive evaluations show that DiOr outperforms other recent methods like ADGAN
[28] in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision. 1.

Introduction
Driven by increasing power of deep generative models as well as commercial possibilities, person generation re-search has been growing fast in recent years. Popular ap-plications include virtual try-on [3, 9, 14, 16, 29, 40, 42], fashion editing [4, 11], and pose-guided person generation
[5, 8, 20, 23, 26, 34, 35, 36, 37, 38, 45]. Most existing work addresses only one generation task at a time, despite simi-larities in overall system designs. Although some systems
[8, 28, 35, 36] have been applied to both pose-guided gen-eration and virtual try-on, they lack the ability to preserve details [28, 35] or lack ﬂexible representations of shape and texture that can be exploited for diverse editing tasks
[8, 28, 35, 36].
This paper proposes a ﬂexible 2D person generation pipeline applicable not only to pose transfer and virtual try-on, but also fashion editing, as shown in Figure 1. The
Figure 1. Applications supported by our DiOr system: Virtual try-on supporting different garment interactions (tucking in or not) and overlay; pose-guided person generation; and fashion editing (tex-ture insertion and removal, shape change). Note that the arrows indicate possible editing sequences and relationships between im-ages, not the ﬂow of our system. architecture of our system is shown in Figure 2. We sep-arately encode pose, skin, and garments, and the garment encodings are further separated into shape and texture. This allows us to freely play with each element to achieve differ-ent looks.
In real life, people put on garments one by one, and can layer them in different ways (e.g., shirt tucked into pants, or worn on the outside). However, existing try-on methods start by producing a mutually exclusive garment segmenta-tion map and then generate the whole outﬁt in a single step.
This can only achieve one look for a given set of garments, and the interaction of garments is determined by the model.
By contrast, our system incorporates a novel recurrent gen-eration module to produce different looks depending on the order of putting on garments. This is why we call our sys-tem DiOr, for Dressing in Order.
After a survey of related work in Section 2, we will de-scribe our system in Section 3. Section 3.1 will introduce our encoding of garments into 2D shape and texture, en-abling each to be edited separately. The shape is encoded using soft masks that can additionally capture transparency.
A ﬂow ﬁeld estimation component at encoding time allows for a more accurate deformation of the garments to ﬁt the target pose. Section 3.2 will describe our recurrent gener-ation scheme that does not rely on garment labels and can handle a variable number of garments. Section 3.3 will dis-cuss our training approach, which combines pose transfer with inpainting to enable preservation of ﬁne details. Sec-tion 4 will present experimental results (including compar-isons and user study), and Section 5 will illustrate the edit-ing functionalities enabled by our system. 2.