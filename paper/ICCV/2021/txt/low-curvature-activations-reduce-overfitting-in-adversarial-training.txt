Abstract
Adversarial training is one of the most effective defenses against adversarial attacks. Previous works suggest that overﬁtting is a dominant phenomenon in adversarial train-ing leading to a large generalization gap between test and train accuracy in neural networks. In this work, we show that the observed generalization gap is closely related to the choice of the activation function. In particular, we show that using activation functions with low (exact or approxi-mate) curvature values has a regularization effect that sig-niﬁcantly reduces both the standard and robust generaliza-tion gaps in adversarial training. We observe this effect for both differentiable/smooth activations such as SiLU as well as non-differentiable/non-smooth activations such as
LeakyReLU. In the latter case, the “approximate” curva-ture of the activation is low. Finally, we show that for ac-tivation functions with low curvature, the double descent phenomenon for adversarially trained models does not oc-cur. 1.

Introduction
Deep Neural Networks can be readily fooled by adver-sarial examples, which are computed by imposing small perturbations on clean inputs [65]. Adversarial attacks have been well studied in the machine learning community in re-cent years [10, 43, 23, 54, 21, 36, 37]. There have been several defenses proposed against adversarial attacks in the literature [53, 64, 7]. In our work we focus on adversar-ial training [43, 23, 35], one of the most effective empirical defenses.
Adversarial training involves training the network on ad-versarially perturbed data instead of clean data to produce a classiﬁer with better robustness on the test set. How-ever, it has been shown that networks produced through vanilla adversarial training do not robustly generalize well
[59, 56, 18]. The gap between robust train and test accu-racy for adversarially trained neural networks i.e. the robust generalization gap can be far greater than the generaliza-tion gap achieved during standard empirical risk minimiza-Figure 1: Learning curves for a robustly trained ResNet-18 model on CIFAR-10. Using an activation function with low curvature such as SiLU prevents robust overﬁtting, achiev-ing and maintaining low test robust loss, even compared to the best early-stop checkpoint of a network with ReLU ac-tivation function. The learning rate is decreased by a factor of 10 at the 100th and 150th epoch. tion. In this work, we show that the robust generalization gap is signiﬁcantly impacted by the curvature of the acti-vation function, and activations with low curvature can act as efﬁcient regularizers for adversarial training, effectively mitigating this phenomenon.
Rice et al. [56] showed for adversarially trained ReLU networks, the best robust test accuracy is not achieved by allowing models to train until convergence. Adversarial training has the characteristic that, after a certain point, fur-ther training will continue to decrease the robust training loss, while the robust test loss starts increasing. This phe-nomenon is referred to as robust overﬁtting and ultimately leads to poor robust accuracy on the test set. Rice et al. also showed that while traditional approaches against overﬁtting such as l1, l2 regularization can mitigate robust overﬁtting, no approach works better than simple early stopping. Since standard accuracy continues to improve even after the net-work overﬁts to adversarial examples, early stopping leads to trade-off between selecting a model with high robust ac-curacy versus a model with high standard accuracy [11].
In this work, we systematically study the impact of ac-tivation functions on generalization. We ﬁrst theoretically analyze the relation between maximum curvature of the ac-tivation function and adversarial robustness. A key obser-vation of our paper is that for smooth activation functions the maximum value of the second derivative of the func-tion, i.e. the maximum curvature has a signiﬁcant impact on robust generalization. Speciﬁcally by using activations with low curvature the robust generalization gap can be re-duced, whereas with high curvature the robust generaliza-tion gap increases. For instance, in Figure 1 for an adver-sarially trained CIFAR-10 model, test error on adversarial examples for the ReLU activation function decreases after the ﬁrst learning rate drop, and keeps increasing afterwards.
However, for SiLU [55] a smooth activation function with low curvature, robust test loss keeps decreasing. We also show that the choice of activation has a similar effect on
In other words, activa-the standard generalization gap. tions that show a large robust generalization gap also have a large standard generalization gap, and vice versa. Our work therefore provides novel insights to the robust overﬁtting phenomenon. The main objective of our work is to under-stand the relation between curvature of the activation func-tion and adversarial training, and highlight ﬁndings which can be useful for training adversarially robust models.
Xie et al. [72] showed that replacing ReLU, a widely used activation function, by “smooth" 1 activation functions such as Softplus or SiLU with a weak adversary (single step
PGD), improves adversarial robustness on Imagenet [14] for “free". They posit smooth activations allow adversar-ial training to ﬁnd harder adversarial examples and com-pute better gradient updates to weight parameters. Further works have however demonstrated that while smooth acti-vation functions can positively affect clean and robust accu-racy, the trend is not as clear as the one observed by Xie et al. Thus, ReLU networks remains a prominent choice for robust classiﬁcation [26, 51].
In contrast to Xie et al. [72], we consider a strong adver-sary for training and show that smoothness of activations is not required to obtain a regularization effect on adversarial training. In our experiments, we show that the same reg-ularization can be achieved using non-smooth activations with low “approximate"" curvature. For non-smooth acti-vations however, curvature is not well-deﬁned. We consider
LeakyReLU which is a non-smooth activation function and use the difference of activation slopes in positive and neg-ative domains as the approximate maximum curvature of the activation function. Even for such a non-smooth acti-vation function, we observe that if the approximate curva-ture is low, the robust overﬁtting phenomenon does not oc-cur. Also in contrast to Xie et al. [72] we empirically show 1We use the same deﬁnition of smoothness as Xie et al., that the func-tion is C1 smooth, that is, that the ﬁrst derivative is continuous everywhere. that smooth activations can perform worse than ReLU, if the smooth activation has high curvature.
Finally, we study the phenomenon of double descent generalization curves seen in standard training [4] and ro-bust training [48]. Double descent describes the following phenomenon. Increasing model complexity causes test ac-curacy to ﬁrst increase and then decrease. Then upon reach-ing a critical point known as interpolation threshold, test ac-curacy starts increasing again. We show that double descent curves reported by [56] for robust overﬁtting using ReLU do not hold for activation functions with low curvature such as
SiLU. 2.