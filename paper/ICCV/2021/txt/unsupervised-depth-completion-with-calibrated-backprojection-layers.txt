Abstract
We propose a deep neural network architecture to infer dense depth from an image and a sparse point cloud.
It is trained using a video stream and corresponding synchro-nized sparse point cloud, as obtained from a LIDAR or other range sensor, along with the intrinsic calibration parame-ters of the camera. At inference time, the calibration of the camera, which can be different than the one used for train-ing, is fed as an input to the network along with the sparse point cloud and a single image. A Calibrated Backprojec-tion Layer backprojects each pixel in the image to three-dimensional space using the calibration matrix and a depth feature descriptor. The resulting 3D positional encoding is concatenated with the image descriptor and the previous layer output to yield the input to the next layer of the en-coder. A decoder, exploiting skip-connections, produces a dense depth map. The resulting Calibrated Backprojection
Network, or KBNet, is trained without supervision by min-imizing the photometric reprojection error. KBNet imputes missing depth value based on the training set, rather than on generic regularization. We test KBNet on public depth completion benchmarks, where it outperforms the state of the art by 30% indoor and 8% outdoor when the same cam-era is used for training and testing. When the test camera is different, the improvement reaches 62%. 1.

Introduction
Sensor platforms designed to enable interaction with physical space often include optical as well as range sen-sors. From cars to phones, cameras are paired with active sensors such as LIDARs, Sonars or Radars. We address the case of a single camera and a single sensor that returns the three-dimensional (3D) coordinates of a number of points far fewer than the number of pixels in the RGB image. The range sensor alone provides a sparse estimate of the Eu-clidean geometry of the surrounding environment, but often insufficient for planning in applications such as autonomous navigation or manipulation. We wish to leverage the com-plementarity of the optical and range modalities to provide a dense depth map, whereby a range value1 is associated to every pixel in the image (in the millions) as opposed to just the LIDAR or radar returns (in the thousands).
Depth completion consists of mapping a single RGB im-age and a sparse 3D point cloud onto a dense depth map, which requires inferring a depth value where missing. This can be done by means of regularization, or inductively using previously observed data for scenes other than the present.
We assume we have available a training set consisting of monocular videos, corresponding sparse 3D point cloud, and intrinsic calibration matrix of the camera used for cap-ture,2 but without any manual annotation or ground-truth dense depth i.e. unsupervised.
Our goal is to use the training set to learn a function that, for a scene and camera not used for training, can map a sparse point cloud registered to an image, along with the matrix of intrinsic calibration parameters of the camera, and produce a dense depth map associated with the test image.
We propose a novel deep neural network architecture that leverages a sparse-to-dense (S2D) module and calibrated backprojection (KB) layers. S2D is comprised of various pooling and convolutional layers to yield a dense represen-tation of the sparse points. A KB layer then maps camera intrinsics, input image, and current depth estimate onto the 3D scene. This can be thought of as a form of spatial (Eu-clidean) positional encoding of the image. Unlike previous architectures, camera intrinsics are an input to our model, as opposed to a fixed set of parameters in the training loss.
This allows us more flexibility to transfer the trained model to sensor platforms other than that used for training.
Our network is trained unsupervised with the standard
Photometric Euclidean Reprojection Loss (PERL) i.e. the absolute difference between a reconstructed image and the actual image measured at a time instant. We also penalize the reconstruction error of the input sparse points and Total 1The depth associated with the pixel is the Euclidean distance of the closest point in the scene along the projection ray through that pixel and the optical center. We assume the sensors to be calibrated and synchronized, and in particular the intrinsic calibration matrix of the camera is known so that pixel coordinates can be converted to Euclidean 3D coordinates. 2Typically, range and optical sensors are calibrated mechanically and pre-registered, so extrinsic calibration is not needed.
Variation of the estimated depth map, a standard sparsity-inducing prior to reduce the penalty for large depth changes at adjacent pixels that straddle occluding boundaries. At test time, no video is necessary and inference is performed on each image and sparse point cloud independently.
These innovations allow us to improve the baseline [41] and state of the art [39] by an average of 13% and 8%, re-spectively, on outdoors (KITTI [35]), and 51.7% and 30.5% indoors (VOID [41]), when calibration is the same for train-ing and testing. When different calibrations are used, our method generalizes better than the baseline and state of the art by 83% and 62%, respectively, in relative error. All of this is achieved with a smaller computational footprint thanks to the inductive bias induced by KB layers, which allows us to use a smaller network than current methods. 1.1.