Abstract
In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D proper-ties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufﬁcient.
In this paper, we propose a uniﬁed, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian represen-tation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics-based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrin-sic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessi-ble. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects. 1.

Introduction
The task of inverse rendering, originally proposed by
Barrow and Tenenbaum [3] in 1978, aims to reverse the ren-dering process by estimating reﬂectance, shape and lighting from a single image. Estimating these intrinsic properties enable downstream applications in augmented and mixed reality, such as realistic insertion of 3D objects into a given 2D image. Inverse rendering also facilitates semantic scene analysis such as object segmentation [5].
Given only observed pixel values, the problem of dis-ambiguating reﬂectance, geometry and their complex inter-actions with lighting is challenging and ill-posed. Classic optimization-based methods leverage hand-crafted priors to constrain the ill-posed nature of the problem. However, these priors do not always hold for complex real world images and can lead to artifacts. Indoor scenes commonly encountered in AR applications are considered especially challenging due (a) Input image (b) Albedo (c) Normals (d) Depth (e) Lighting (f) Specular / Diffuse / Transparent
Sphere Insertion (g) Specular Object Insertion
Figure 1: From a single image, our model jointly estimates albedo, normals, depth, and the HDR lighting volume. Key to our method is inferring continuous HDR 3D spatially-varying lighting, which is critical in producing high quality virtual object insertion with realistic cast shadows and angular high-frequency details. to complex 3D light transport that occurs indoors.
In this work, we address the problem of scene-level in-verse rendering, focusing speciﬁcally on producing high dynamic range (HDR) 3D spatially-varying lighting with high-frequency details, as shown in Fig. 1. Estimating both
HDR and 3D spatially-varying lighting is critical for pho-torealistic virtual object insertion; HDR enables realistic cast shadows and 3D spatially-varying lighting enables high-frequency details. We use the HDR lighting inferred by our model to insert highly specular objects and produce realistic cast shadows and high-frequency details, which were not possible in previous works [11, 22, 33, 38].
Existing learning-based methods usually exploit powerful 2D CNNs and formulate the inverse rendering problem as image-to-image translation. Lighting is usually represented with spherical lobes such as spherical Harmonics and spheri-cal Gaussian [2, 41], and environment maps [10, 33], which ignores spatially-varying effects. Recent works attempt to predict 2D spatially-varying spherical lobes [11, 22], but still lack one degree of freedom (depth) and compromise in terms of angular high-frequency effects. As a consequence, the 2D representation of the scene lighting is not sufﬁciently performant for many downstream applications.
In this paper, we propose a holistic inverse rendering framework for jointly estimating reﬂectance, shape and 3D
spatially-varying lighting, by formulating the complete ren-dering process in an end-to-end trainable way with a 3D lighting representation. We propose a novel Volumetric
Spherical Gaussian representation for lighting, which is a voxel representation for the scene surfaces. Spherical
Gaussian parameters in each voxel control the emission di-rection and sharpness of the light source, which captures view-dependent effects and can handle strong directional lighting. Since ground-truth for HDR lighting is not eas-ily available, we design a raytracing based differentiable renderer that leverages our lighting representation and for-mulates the energy-conserving image formation process. We use the renderer to jointly train all intrinsic properties by enforcing the re-rendering constraint, ensuring that predic-tions are physically correct. To the best of our knowledge, our approach is the ﬁrst to estimate a complete continuous light ﬁeld function from a single image, including both HDR and high-frequency spatial and angular details, despite being trained with only LDR images.
We experimentally show that our approach outperforms existing state-of-the-art inverse rendering and lighting esti-mation methods. We demonstrate that our method learns to produce complex lighting effects of real-world indoor scenes and better disambiguates intrinsic properties. Our lighting representation enables realistic cast shadows and angular high-frequency details and is therefore capable of producing signiﬁcantly more realistic object insertion results for AR applications that were not possible previously, most impor-tantly including the insertion of highly specular objects. 2.