Abstract
Event cameras, which output events by detecting spatio-temporal brightness changes, bring a novel paradigm to image sensors with high dynamic range and low latency.
Previous works have achieved impressive performances on event-based video reconstruction by introducing convolu-tional neural networks (CNNs). However, intrinsic lo-cality of convolutional operations is not capable of mod-eling long-range dependency, which is crucial to many
In this paper, we present a hybrid CNN-vision tasks.
Transformer network for event-based video reconstruction (ET-Net), which merits the ﬁne local information from CNN and global contexts from Transformer. In addition, we fur-ther propose a Token Pyramid Aggregation strategy to im-plement multi-scale token integration for relating internal and intersected semantic concepts in the token-space. Ex-perimental results demonstrate that our proposed method achieves superior performance over state-of-the-art meth-ods on multiple real-world event datasets. The code is available at https://github.com/WarranWeng/
ET-Net. 1.

Introduction
Event cameras, also known as neuromorphic cameras
[45], are novel bio-inspired visual sensors, providing re-searchers with a radically different sensing paradigm.
Rather than directly reporting frame-based representation at a ﬁxed rate in conventional cameras, event cameras are speciﬁcally designed to detect and record spatio-temporal changes for each pixel. Compared with frame-based coun-terparts, event cameras possess several superior properties: high temporal resolution (about 1 µs), high dynamic range (140 dB) and low power consumption (5 mW) [24], which are suitable in scenarios that are challenging for conven-tional cameras, such as HDR scenes and high speed mov-ing scenes. However, the event streams are not convenient for observation and post-processing due to their sparse, ir-regular and unstructured properties. To better utilize the
*Correspondence should be addressed to zhyuey@ustc.edu.cn advantages of event cameras, an intuitive way is to con-vert event streams to video composed of sequential intensity frames, which serves as a bridge that connects the off-the-shelf frame-based algorithms [12, 13, 11, 26, 42, 41, 44, 43] to event cameras.
Deep learning techniques, especially convolutional neu-ral networks, have achieved great successes in the area of computer vision. Recently, several works performed event-based video reconstruction via deep learning methods and demonstrated impressive performance. Using supervised learning, Rebecq et al. [29, 28] ﬁrst proposed the E2VID
CNN-based model, achieving signiﬁcant performance boost in terms of image quality and temporal consistency against hand-crafted methods [4, 21, 32]. Based on E2VID, Scheer-linck et al. [33] reduced inference time and model capac-ity using a light-weight network FireNet with only a minor drop in accuracy incurring. Further, Stoffregen et al. [35] presented that these supervised training methods showed a strong dependence on the synthetic data generated by event camera simulators, such as ESIM [27]. Consequently, for relaxing this data dependency, Federico et al. [22] ap-proached, for the ﬁrst time, the reconstruction problem from a self-supervised learning perspective via combining esti-mated optical ﬂow and the event-based photometric con-stancy to train neural networks without ground-truth.
These CNN-based architectures [29, 28, 35, 22] show the preponderance in video reconstruction for event cameras.
However, classic CNN-based models are not capable of modeling the long-range dependency due to the essential lo-cality of convolution operations. Actually, capturing long-range dependency plays a crucial role in deep neural net-works for both sequential data in NLP tasks and image data in vision tasks. Especially, CNN-based models are not ef-fective to deal with structures that show large internal vari-ation in terms of texture, shape and size. In order to tackle this limitation, some works have been proposed recently.
Wang et al. [39] proposed a non-local operation, which can be plugged into multiple existing CNN models. Schlemper et al. [34] integrated additive attention gate modules into the skip-connections for global contexts. More recently, Trans-former [36], designed for sequence-to-sequence prediction, has emerged as a popular architecture in both NLP and vi-sion tasks [36, 38, 6, 10, 7]. Built upon the self-attention mechanisms solely instead of CNNs, Transformer shows an appealing potential in modeling global context information.
In this paper, we present the ﬁrst attempt that explores the application of Transformer in the context of high speed video reconstruction for event cameras. Based on the novel perspective of sequence-to-sequence prediction, we pro-pose Event Transformer Network (ET-Net) to exploit the powerful potentials of Transformer for reconstructing video from pure events. Different from previous works [10, 48], our ET-Net adopts a hybrid CNN-Transformer architecture to leverage both detailed multi-resolution spatial informa-tion from CNN features and the global context encoded by
Transformer. We verify that the combination of localized features and global contexts is able to further promote the reconstruction quality. Additionally, we propose a novel
Token Pyramid Aggregation (TPA) module to implement multi-scale token integration, which is a core component of ET-Net. The proposed TPA represents the 2-D features using visual tokens and learns to directly relate semantic concepts in token-space instead of convolution operators, yielding a better reconstruction accuracy. Extensive ex-periments conducted on the existing frequently-used event camera datasets show that our proposed architecture ET-Net outperforms existing CNN-based methods, substantiating effectiveness of our transformer-based method.
We summarize our contributions as three-fold. (1) We propose ET-Net, a novel hybrid CNN-Transformer frame-work, to leverage both ﬁne local information from CNN and global context from Transformer for approaching the event-based video reconstruction task. (2) We propose a To-ken Pyramid Aggregation module to perform multi-scale to-ken integration for relating internal and intersected seman-(3) We comprehensively tic concepts in the token-space. demonstrate the effectiveness of our architectural design via extensive experiments, achieving a substantial performance boost over CNN-based methods. 2.