Abstract
Deep neural networks have achieved remarkable perfor-mance on a range of classification tasks, with softmax cross-entropy (CE) loss emerging as the de-facto objective func-tion. The CE loss encourages features of a class to have a higher projection score on the true class-vector compared to the negative classes. However, this is a relative constraint and does not explicitly force different class features to be well-separated. Motivated by the observation that ground-truth class representations in CE loss are orthogonal (one-hot encoded vectors), we develop a novel loss function termed ‘Orthogonal Projection Loss’ (OPL) which imposes orthogonality in the feature space. OPL augments the prop-erties of CE loss and directly enforces inter-class separation alongside intra-class clustering in the feature space through orthogonality constraints on the mini-batch level. As com-pared to other alternatives of CE, OPL offers unique ad-vantages e.g., no additional learnable parameters, does not require careful negative mining and is not sensitive to the batch size. Given the plug-and-play nature of OPL, we eval-uate it on a diverse range of tasks including image recog-nition (CIFAR-100), large-scale classification (ImageNet), domain generalization (PACS) and few-shot learning (mini-ImageNet, CIFAR-FS, tiered-ImageNet and Meta-dataset) and demonstrate its effectiveness across the board. Further-more, OPL offers better robustness against practical nui-sances such as adversarial attacks and label noise. Code is available at: https://github.com/kahnchana/opl. 1.

Introduction
Recent years have witnessed great success across a range of computer vision tasks owing to progress in deep neural networks (DNNs) [23]. Effective loss functions for DNNs training have been a crucial component of these advance-ments [15]. In particular, the softmax cross entropy (CE) loss, commonly used for tackling classification problems,
Figure 1: Orthogonal Projection Loss: During training of a deep neural network, within each mini-batch, OPL enforces separation between features of different class samples while clustering to-gether features of the same class samples. OPL integrates well with softmax CE loss as it simply complements its intrinsic angu-lar property, leading to consistent performance improvements on various classification tasks with a variety of DNN backbones. has been pivotal for stable and efficient training of DNNs.
Multiple variants of CE have been explored to enhance discriminativity and generalizability of feature representa-tions learned during training. Contrastive [16] and triplet
[43] loss functions are a common class of methods that have gained popularity on tasks requiring more discriminative features. At the same time, methods like centre loss [57] and contrastive centre loss [37] have attempted to explic-itly enforce inter-class separation and intra-class clustering through Euclidean margins between class prototypes. An-gular margin based losses [32, 31, 7, 53, 52] compose an-other class of objective functions that increase inter-class margins through altering the logits prior to the CE loss.
While these methods have proven successful at promot-ing better inter-class separation and intra-class compact-ness, they do possess certain drawbacks. Contrastive and triplet loss functions [16, 43] are dependent on carefully designed negative mining procedures, which are both time-consuming and performance-sensitive. Methods based on centre loss [57, 37], that work together with CE loss, pro-mote margins in Euclidean space which is counter-intuitive to the intrinsic angular separation enforced through CE loss
[31]. Further, these methods introduce additional learnable parameters in the form of new class centres. Angular mar-gin based loss functions [31, 32] which are highly success-ful for face recognition tasks, make strong assumptions for face embeddings to lie on the hypersphere manifold, which does not hold universally for all computer vision tasks [45].
Some loss designs are also specific to certain architecture classes e.g., [45] can only work with DNNs which output
Class Activation Maps [65].
In this work, we explore a novel direction of simultane-ously enforcing inter-class separation and intra-class clus-tering through orthogonality constraints on feature repre-sentations learned in the penultimate layer (Fig. 1). We propose Orthogonal Projection Loss (OPL), which can be applied on the feature space of any DNN as a plug-and-play module. We are motivated by how image classifi-cation inherently assumes independent output classes and how orthogonality constraints in feature space go hand in hand with the one-hot encoded (orthogonal) label space used with CE. Furthermore, orthogonality constraints pro-vide a definitive geometric structure in comparison to ar-bitrarily increasing margins which are prone to change de-pending on the selected batch, thus reducing sensitivity to batch composition. Finally, simply maximizing the margins can cause negative correlation between classes and thereby unnecessarily focus on well-separated classes while we tend to ensure independence between different class features to successfully disentangle the class-specific characteristics.
Compared with contrastive loss functions [16, 43], OPL operates directly on mini-batches, eliminating the require-ment of complex negative sample mining procedures. By enforcing orthogonality through computing dot-products between feature vectors, OPL provides a natural augmen-tation to the intrinsic angular property of CE, as opposed to methods [57, 37, 17] that enforce an Euclidean mar-gin in feature space. Furthermore, OPL introduces no additional learnable parameters unlike [57, 51, 37], oper-ates independent of model architecture unlike [45], and in contrast to losses operating on the hypersphere manifold
[31, 32, 7, 53], performs well on a wide range of tasks.
In summary, our main contributions are as follows:
• We propose a novel loss, OPL, that directly enforces inter-class separation and intra-class clustering via or-thogonality constraints with no learnable parameters.
• Our orthogonality constraints are efficiently formulated compared to existing methods [26, 46], allowing mini-batch processing without the need to explicitly obtain sin-gular values. This leads to a simple vectorized implemen-tation of OPL directly integrating with CE.
• We extensively evaluate on a diverse range of image clas-sification tasks highlighting the discriminative ability of
OPL. Further, our results on few-shot learning (FSL) and domain generalization (DG) datasets establish the trans-ferability and generalizability of features learned with
OPL. Finally, we establish the improved robustness of learned features to adversarial attacks and label noise. 2.