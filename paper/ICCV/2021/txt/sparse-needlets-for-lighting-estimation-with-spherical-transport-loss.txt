Abstract
Accurate lighting estimation is challenging yet criti-cal to many computer vision and computer graphics tasks such as high-dynamic-range (HDR) relighting. Existing ap-proaches model lighting in either frequency domain or spa-tial domain which is insufficient to represent the complex lighting conditions in scenes and tends to produce inaccu-rate estimation. This paper presents NeedleLight, a new lighting estimation model that represents illumination with needlets and allows lighting estimation in both frequency domain and spatial domain jointly. An optimal thresholding function is designed to achieve sparse needlets which trims redundant lighting parameters and demonstrates superior localization properties for illumination representation. In addition, a novel spherical transport loss is designed based on optimal transport theory which guides to regress lighting representation parameters with consideration of the spatial information. Furthermore, we propose a new metric that is concise yet effective by directly evaluating the estimated illumination maps rather than rendered images. Extensive experiments show that NeedleLight achieves superior light-ing estimation consistently across multiple evaluation met-rics as compared with state-of-the-art methods. 1.

Introduction
Lighting estimation aims to recover illumination from a single image with limited field of view. It has a wide range of applications in various computer vision and computer graphics tasks such as high-dynamic-range (HDR) relight-ing in mixed reality, etc. However, lighting estimation is a challenging and ill-posed problem as it needs to predict the illumination coming from a full sphere of directions includ-ing those unobserved from the current view in the scene.
Additionally, it often requires to infer HDR illuminations from low-dynamic-range (LDR) observations so as to light
*Equal contribution
â€ Corresponding author virtual objects realistically while inserting them into real scene images as illustrated in Fig. 1.
Lighting estimation has been tackled by regressing the parameters of various lighting representations in either fre-quency domain [7, 15]) or spatial domain [14, 13, 33, 34].
However, lighting estimation in frequency domain nor-mally represents illumination with Spherical Harmonics (SH) which lack spatial localization capabilities. Thus it tends to capture global lighting instead of the exact spa-tial locations of the light sources which often leads to weak shading and shadow effects as illustrated in Garon et al. [15] of Fig. 1. Lighting estimation in spatial domain has been addressed by direct generation of the illumination maps or indirect reconstruction through spherical Gaussian function.
However, direct generation of illumination maps often leads to worse generalization as lighting estimation is an under-constrained problem by itself, and spherical Gaussian often involves a complicated training process as described in [13].
Both types of approaches in spatial domain do not explic-itly consider lighting frequency, and thus lead to inaccurate relighting performance as illustrated in Gardner et al. [13] of Fig. 1. The high frequency information also tends to be blurred due to the use of naive L2 loss in the training. Ad-ditionally, existing evaluation metrics in lighting estimation usually assess the objects rendered with the predicted illu-mination maps, which is time-consuming and sensitive to the test setting.
In this work, we propose NeedleLight, a new model that introduces needlet for accurate and robust lighting es-timation from a single image. As a new generation of spherical wavelets, needlet enjoys good localization prop-erties in both frequency and spatial domain which makes it ideal to be the basis for illumination representation. More-over, to remove the redundant parameters in needlet coeffi-cients which will disturb the regression of principle light sources, we design an optimal thresholding function to achieve sparse needlets which improve the lighting estima-tion greatly.
Unlike spherical harmonic coefficients, needlet coeffi-cients are spatially localized over a unit sphere. To utilize
(a) Garon et al. [15] (b) Gardner et al. [13] (c) Ours (d) Ground Truth
Figure 1. The proposed NeedleLight estimates a parametric lighting representation from a single scene image which is critical to many tasks such as virtual object insertion. Unlike previous methods that predict lighting in either frequency domain [15] (losing spatial localization) or spatial domain [13] (losing frequency information) only, we introduce a novel needlets basis which is capable of representing and estimating lighting accurately in both frequency and spatial domains. the spatial information in regression, we propose a Spheri-cal Transport Loss (STL) based on optimal transport theory.
STL is able to capture spatial information via a cost matrix and estimate the needlet coefficients more accurately than a naive L2 loss. Besides, STL employs auxiliary point strat-egy to preserve high frequency information and greatly re-duce the dimension of required parameters. Based on STL, we design a new metric for the evaluation of lighting esti-mation by measuring the discrepancy between illumination maps. The new metric highly simplifies the evaluation pro-cedure and provides concise yet effective evaluation with regard to the lighting color, intensity and position.
The contribution of this work can be summarized in three aspects. First, we introduce a novel needlet basis for il-lumination representation which allows to regress the pa-rameters in both frequency and spatial domains simultane-ously. Second, we develop an optimal thresholding func-tion to achieve sparse needlets which effectively removes the redundant needlet coefficients and improves the lighting estimation. Third, we design a novel Spherical Transport
Loss (STL) that effectively utilizes the spatial information of needlet coefficients in regression. With STL, we also design a new evaluation metric that is more concise and ef-fective than existing evaluation metrics. 2.