Abstract
The scale of deep learning nowadays calls for efficient distributed training algorithms. Decentralized momentum
SGD (DmSGD), in which each node averages only with its neighbors, is more communication efficient than vanilla
Parallel momentum SGD that incurs global average across all computing nodes. On the other hand, the large-batch training has been demonstrated critical to achieve runtime speedup. This motivates us to investigate how DmSGD per-forms in the large-batch scenario.
In this work, we find the momentum term can amplify the inconsistency bias in DmSGD. Such bias becomes more ev-ident as batch-size grows large and hence results in severe performance degradation. We next propose DecentLaM, a novel decentralized large-batch momentum SGD to remove the momentum-incurred bias. The convergence rate for both strongly convex and non-convex scenarios is established.
Our theoretical results justify the superiority of DecentLaM to DmSGD especially in the large-batch scenario. Experi-mental results on a a variety of computer vision tasks and models show that DecentLaM promises both efficient and high-quality training. 1.

Introduction
Efficient distributed training across multiple computing nodes is critical for large-scale deep learning tasks nowa-days. As a principal training algorithm, Parallel SGD com-putes a globally averaged gradient either using the Parame-ter Server (PS) [24] or the All-Reduce communication prim-itive [38]. Such global synchronization across all nodes ei-ther incurs significant bandwidth cost or high latency, which hampers the training scalability.
Decentralized SGD [36, 7, 25, 26, 4, 10] based on par-tial averaging has become one of the major approaches in the past decade to reduce communication overhead in distributed optimization. Partial averaging, as opposed to global averaging used in Parallel SGD, requires every node compute the average of the nodes in its neighborhood, see
*Equal contribution. Correspondence can be addressed to Kun Yuan
<kun.yuan@alibaba-inc.com>
Figure 1. Illustration of decentralized methods. Nodes receive information from neighbors; they do not relay information. For example, nodes 4 and 6 collect information from their neighbors
{1, 5} and {3, 5}, respectively. Other nodes do the same but not depicted. Topology connectivity can be represented in a matrix, as shown in the right figure, see more details in Sec. 3.
Fig. 1. If a sparse topology such as one-peer exponential graph [4] is utilized to connect all nodes, each node only communicates with one neighbor each iteration and hence saves remarkable communications. Decentralized SGD can typically achieve 1.3 ∼ 2× training time speedup without performance degradation [25, 4, 23].
Existing Decentralized SGD methods [25, 26, 47, 27, 4] and their momentum accelerated variants [56, 44, 15, 5] primarily utilize small-batch in their algorithm design.
However, recent hardware advances make it feasible to store large batches in memory and compute their gradi-ents timely. Furthermore, the total batch size naturally grows when more computing nodes participate into train-ing. These two primary reasons lead to the recent explo-ration of large-batch deep training algorithms.
In fact, large-batch training has been extensively studied in Parallel SGD. Pioneering works [17, 52, 54] find large-batch can significantly speed up Parallel SGD. First, the computation of a large-batch gradient can fully utilize the computational resources (e.g., the CUDA cores and GPU memories). Second, large-batch gradient will result in a re-duced variance and hence enables a much larger learning rate. With newly-proposed layer-wise adaptive rate scaling (LARS) [52] and its variant [54], large-batch Parallel mo-mentum SGD (PmSGD) can cut down the training time of
BERT and Resnet-50 from days to hours [55].
This naturally motivates us to study how Decentralized momentum SGD (DmSGD) performs with large batch-size.
Dataset
Batch-size
Cifar-10 2K 8K
ImageNet 2K 32K
PmSGD
DmSGD 91.6% 89.2% 76.5% 75.3% 91.5% 88.3% 76.5% 74.9%
Table 1. Top-1 validation accuracy comparison between PmSGD and DmSGD under the small-batch and large-batch settings. No layer-wise adaptive rate scaling is used in any of these algorithms.
All hypter-parameters are exactly the same. More experimental details can be referred to Appendix G.1
To this end, we compared PmSGD and DmSGD over Cifar-10 (Resnet-20) and ImageNet (Resnet-50) with both small and large batches. Their performances are listed in Table 1.
While DmSGD achieves the same accuracy as PmSGD with small batch-size, it has far more performance degradation in the large-batch scenario. This surprising observation, which reveals that the extension of DmSGD to large-batch is non-trivial, raises two fundamental questions:
• Why does DmSGD suffer from severe performance degradation in the large-batch scenario?
• How to enhance the accuracy performance of large-batch
DmSGD so that it can match with or even beat PmSGD?
This paper focuses on these questions and provides affir-mative answers. In particular, our main contributions are:
• We find large-batch DmSGD has severe performance degradation compared to large-batch PmSGD and clarify the reason behind this phenomenon. It is discovered that the momentum term can significantly amplify the incon-sistency bias in DmSGD. When batch-size is large and the gradient noise is hence dramatically reduced, such incon-sistency bias gets dominant and thus degrades DmSGD’s performance notably.
• We propose DecentLaM, a novel decentralized large-batch momentum SGD to remove the momentum-incurred bias in DmSGD. We establish its convergence rate for strongly convex and non-convex scenarios. Our theoretical results show DecentLaM has superior perfor-mance to existing decentralized momentum methods, and such superiority gets more evident as batch size grows.
• Experimental results on a variety of computer vision tasks and models show that DecentLaM outperforms DmSGD,
DA/QG/D2-DmSGD, PmSGD, and PmSGD with LARS in terms of both training speed and accuracy.
The rest of this paper is organized as follows: We briefly summarize related works in Sec. 2 and review DmSGD in
Sec. 3. We identify the issue that causes performance degra-dation in DmSGD (Sec. 4) and propose DecentLaM to re-solve it (Sec. 5). The convergence analysis and experiments are established in Sec. 6 and Sec. 7. 2.