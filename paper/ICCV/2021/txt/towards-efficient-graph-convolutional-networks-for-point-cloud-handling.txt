Abstract
We aim at improving the computational efﬁciency of graph convolutional networks (GCNs) for learning on point clouds. The basic graph convolution that is composed of a
K-nearest neighbor (KNN) search and a multilayer percep-tron (MLP) is examined. By mathematically analyzing the operations there, two ﬁndings to improve the efﬁciency of
GCNs are obtained. (1) The local geometric structure infor-mation of 3D representations propagates smoothly across the GCN that relies on KNN search to gather neighborhood features. This motivates the simpliﬁcation of multiple KNN searches in GCNs. (2) Shufﬂing the order of graph fea-ture gathering and an MLP leads to equivalent or similar composite operations. Based on those ﬁndings, we optimize the computational procedure in GCNs. A series of experi-ments show that the optimized networks have reduced com-putational complexity, decreased memory consumption, and accelerated inference speed while maintaining comparable accuracy for learning on point clouds. 1.

Introduction
Recently, graph convolutional networks (GCN) [8, 2, 10, 53, 43, 58, 48] have achieved state-of-the-art performances in 3D representation learning on point clouds for classiﬁ-cation [35, 36], part segmentation [3], semantic segmen-tation [49, 19], and surface reconstruction [14]. A typi-cal GCN is composed of a stack of multilayer perceptrons (MLPs) that progressively learn a hierarchy of deep fea-tures. For a better modelling of the locality on point clouds, neighborhood information gathering modules are placed be-fore MLPs. A certain point gathers information from its neighbors and propagates its information to them. The neighbors can be predeﬁned (i.e., borrowed from an initial mesh in Point2Mesh [14]) or more commonly established by K-nearest neighbor (KNN) search on point clouds (static
*Co-ﬁrst author. (a) Overall Accuracy (b) Runtime (c) GPU memory (d) FLOPs
Figure 1: Comparison between a representative GCN and the accelerated version in this paper. (a) The overall ac-curacy for point cloud classiﬁcation. Mean and Variance reported for 5 runs. The (b) runtime, (c) GPU memory consumption, and (d) FLOPs of the original GCN explodes with an increasing number of points. By contrast, the opti-mized network can achieve a signiﬁcant reduction of com-putational resources without a drop in accuracy.
GCN [36, 23]) or on the feature representation (dynamic
GCN [49, 54]).
Yet, this design faces several technical challenges.
Firstly, the computational cost grows quadratically with the number of points [38, 37]. The problem is exacerbated when KNN search is conducted in a high-dimensional fea-ture space. Secondly, the graph feature gathering operation expands the dimension of the resultant features. Consider a point cloud with N points and d coordinates. The dimen-sion of the tensor grows from N × d to N × K × d after the K graph feature gathering operation, where K is the number of neighbors. Then the same operation is applied to the expanded tensor with repeated entries, which leads to
redundant computations. Thirdly, due to the computational complexity and the expanded features, the GPU memory re-quired for GCN computations explodes when the number of processed points increases. The inference speed also slows down drastically.
As shown in Fig. 1, each time the number of processed points doubles, the computational complexity, inference time, and consumed GPU memory of the examined GCN al-most quadruple. Thus, the aim of this paper is to analyze the basic operations in GCNs and seek opportunities to build ef-ﬁcient GCNs for learning on point clouds. Compared with the representative GCN in Fig. 1, the computationally opti-mized GCN in this paper reduces the computational burden and accelerates the inference. This signiﬁcant improvement relies on the following two ﬁndings.
Finding 1. The local geometric structure information of 3D representations propagates smoothly across the aforemen-tioned multilayer GCN that relies on KNN search for graph feature gathering.
This ﬁnding is supported by the mathematical analysis of the distances between two points before and after one layer of an MLP. In Sec. 4.2, we show that the distance be-tween two points after one layer of MLP is upper bounded by the neighborhood distance and lower bounded by the neighborhood centroid distance between the corresponding points before the MLP. This means that across a GCN the distance between two points in the feature space does not abruptly change. Thus, it is not necessary to conduct KNN search every time a neighbor retrieval is needed in MLPs.
Instead, a couple of MLPs (referred to as shareholder MLP) can share the results of the same KNN search. Moreover, to ensure a progressively enlarged receptive ﬁeld across the shareholder MLPs, a larger pool of neighbors can be kept from the ﬁrst KNN search. Each time neighbor retrieval is needed, the neighbors are sampled from the pool. The shareholder MLPs in the shallower layers can only sample from the near neighbors while the deep shareholders have the chance to sample from far-away neighbors.
Finding 2. Shufﬂing the order of the graph feature gather-ing operation and the MLP used for feature extraction leads to equivalent or similar composite operations for GCNs.
This ﬁnding is also supported by a general analysis in
Sec. 4.3. As said, in existing GCNs, the graph feature gath-ering operation happens before the MLP and expand the di-mension of the features. By moving the feature extracting
MLP before the graph feature gathering operation, the MLP is conducted merely on the non-expanded feature tensors.
And this leads to a signiﬁcant reduction in computations.
The two ﬁndings directly lead to the proposed change in computational procedure as shown in Fig. 2, which re-duces the computational complexity and accelerates the in-ference of the GCNs. Here, the proposed techniques are (a) Conventional GCN (b) Optimized GCN
Figure 2: Comparison between (a) conventional GCN and
Instead of calling (b) the optimized GCN in this paper.
KNN search for each graph convolution, we enforce several graph convolutions to share the same KNN search with pro-gressively enlarged receptive ﬁelds. The shufﬂing of graph feature gathering and MLP avoids the expansion of features, which leads to accelerated computation in the MLP. applied to four representative GCNs [49, 23, 14, 54].
It is shown that they can improve the efﬁciency of existing
GCNs signiﬁcantly, indeed. For example, for ModelNet40 point cloud classiﬁcation with 2048 points, compared with the original DGCNN, the accelerated version is about ×3 times faster, reduces GPU memory by 57.1% and compu-tation by 86.7% without loss of accuracy. More results are shown in Sec. 5. Thus, the contributions of this paper can be summarized as follows. 1. Starting with the analysis of basic operations in repre-sentative GCNs, two theorems enabling their accelera-tion are proved. 2. Based on the proved theorems, two strategies for shuf-ﬂing operations are proposed to speciﬁcally improve the time and memory efﬁciency of existing GCNs. 3. Extensive experiments on four GCNs for four point cloud learning tasks are carried out, to validate the ef-ﬁciency of the proposed method.
It is demonstrated that both the inference time and memory consumption decreased signiﬁcantly. 2.