Abstract
There is a big sphere behind the brown  cylinder; does it have the same color as  the small rubber sphere on the left side  of the small red rubber ball?
We study the problem of concept induction in visual rea-soning, i.e., identifying concepts and their hierarchical re-lationships from question-answer pairs associated with im-ages; and achieve an interpretable model via working on the induced symbolic concept space. To this end, we ﬁrst de-sign a new framework named object-centric compositional attention model (OCCAM) to perform the visual reasoning task with object-level visual features. Then, we come up with a method to induce concepts of objects and relations using clues from the attention patterns between objects’ vi-sual features and question words. Finally, we achieve a higher level of interpretability by imposing OCCAM on the objects represented in the induced symbolic concept space.
Experiments on the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of the art without human-annotated functional programs; 2) our induced con-cepts are both accurate and sufﬁcient as OCCAM achieves an on-par performance on objects represented either in vi-sual features or in the induced symbolic concept space. 1.

Introduction
Recent advances in Visual Question Answering (VQA) [1, 37, 19, 3, 17, 24, 16, 47, 33] usually rely on carefully designed neural attention models over images, and rely on pre-deﬁned lists of concepts to enhance the compositional reasoning ability of the attention modules.
Human prior knowledge plays an essential role in the success of the model design.
We focus on a less-studied problem in this ﬁeld – given only question-answer pairs and images, induce the visual concepts that are sufﬁcient for completing the visual reason-ing tasks. By sufﬁciency, we hope to maintain the predictive accuracy for VQA when using the induced concepts in place of the original visual features. We consider concepts that are important for visual reasoning, including properties of objects (e.g., red, cube) and relations between objects (e.g.,
*Correspondence to H. Shi and M. Yu. big cylinder sphere ball train concept regression concept induction sphere  ball cylinder big super concept concepts concept obj_0 obj_1 obj_2 behind obj_0 obj_1 obj_2 gray red blue green brown purple cyan yellow sphere cube cylinder large small metal matte 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 obj_0 0 1 1 front obj_0 obj_1 obj_2 0 0 0 0 0 1 obj_1 obj_2 0 1 obj_0 obj_1 obj_2 right obj_0 obj_1 obj_2 0 0 0 0 1 0 1 1 0 0 0 1 1 obj_0 obj_1 obj_2 left obj_0 obj_1 obj_2 0 obj_0 0 0 0 0 obj_1 obj_2 1 1 0 1 0 0 0 visual  features
What is the big thing that is in front of  the block that is behind the block that is  in front of the large shiny block made of? compositional  reasoning matte
Figure 1: Illustration of our framework. Our model induces the concepts and super concepts with the attention correlation between the objects and question words in image-question pairs as the paths shown in blue arrows. Then, it answers a question about an image via compositional reasoning on the induced symbolic representa-tions of objects and object relations, shown as the orange paths. left, front). The aforementioned scope and sufﬁciency crite-rion require accurately associating the induced symbols of concepts to both visual features and words, so that each new instance of question-image pair can be transformed into the induced concept space for further computations. Addition-ally, it is necessary to identify super concepts, i.e., hyper-nyms of concept subsets (e.g., shape). The concepts inside a super concept are exclusive, so that the system knows each object can only possess one value in each subset. This intro-duces structural information to the concept space (multiple one-hot vectors for each visual object) and further guaran-tees the accuracy of the aforementioned transformation.
The value of the study has two folds. First, our pro-posed problem aims to identify visual concepts, their ar-gument patterns (properties or relations) and their hierar-chy (super concepts) without using any concept-level su-pervision. Solving the problem frees both the efforts of human annotations and human designs of concept schema required in previous visual reasoning works. At the same time, the problem is technically more challenging compared to the related existing problem like unsupervised or weakly-supervised visual grounding [46]. Second, by constrain-ing the visual reasoning models to work over the induced concepts, the ability of concept induction improves the in-terpretability of visual reasoning models. Unlike previous interpretable visual reasoning models that rely on human-written rules to associate neural modules with given con-cept deﬁnitions [16, 33, 40], our method resolves the con-cept deﬁnitions and associations interpretability automati-cally in the learning process, without the need of trading off for hand-crafted model designs.
We achieve the proposed task in three steps. First, we propose a new model architecture, object-centric compo-sitional attention model (OCCAM), that performs object-level visual reasoning instead of pixel-level by extracting object-level visual features with ResNet [14] and pooling the features according to each object’s bounding box. The object-level reasoning not only improves over the state-of-the-arts, but also provides a higher-level interpretability for concept association and identiﬁcation. Second, we beneﬁt from the trained OCCAM’s attention values over objects to create classiﬁers mapping visual objects to words; then de-rive the concepts and super concepts from the object-word cooccurrence matrices as shown in Figure 1. Finally, our concept-based visual reasoning framework predicts the con-cepts of objects and object relations; then performs com-positional reasoning using the predicted symbolic concept embeddings instead of the original visual features.
Experiments on the CLEVR and GQA datasets conﬁrm that our overall approach improves the interpretability of neural visual reasoning, and maintains the predictive accu-racy: (1) our OCCAM improves over the previous state-of-the-art models that do not use external training data; (2) our induced concepts and concept hierarchy are accurate in hu-man study; and (3) our induced concepts are sufﬁcient for visual reasoning – replacing visual features with concepts leads to only 2.