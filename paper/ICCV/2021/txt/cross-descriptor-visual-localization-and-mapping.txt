Abstract
Visual localization and mapping is the key technology underlying the majority of mixed reality and robotics sys-tems. Most state-of-the-art approaches rely on local fea-tures to establish correspondences between images. In this paper, we present three novel scenarios for localization and mapping which require the continuous update of fea-ture representations and the ability to match across different feature types. While localization and mapping is a funda-mental computer vision problem, the traditional setup sup-poses the same local features are used throughout the evo-lution of a map. Thus, whenever the underlying features are changed, the whole process is repeated from scratch.
However, this is typically impossible in practice, because raw images are often not stored and re-building the maps could lead to loss of the attached digital content. To over-come the limitations of current approaches, we present the first principled solution to cross-descriptor localization and mapping. Our data-driven approach is agnostic to the fea-ture descriptor type, has low computational requirements, and scales linearly with the number of description algo-rithms. Extensive experiments demonstrate the effectiveness of our approach on state-of-the-art benchmarks for a vari-ety of handcrafted and learned features. 1.

Introduction
Mixed reality and robotics blend the physical and digi-tal worlds to unlock the next evolution in human, machine, and environment interaction. This promise largely relies on the technical capability to build and localize against maps of the environment. For example, collaborative experiences require co-localization of multiple devices into the same co-ordinate space. Similarly, re-localization against existing maps enables persistence and retrieval of digital content at-tached to the real world. Most systems build upon localiza-tion and mapping pipelines predominantly based on vision sensors using local features for image matching.
Traditionally, these systems run in real-time on the de-vice, but the industry has been increasingly moving local-ization and mapping capabilities to the cloud (e.g., Face-book LiveMaps [2], Google VPS [42], Magic Leap’s Mag-Figure 1: Mixed reality and robotics systems typically do not store raw images. This complicates deployment of new feature representations for visual localization and mapping.
Our approach enables the continuous update of feature rep-resentations and the ability to match across heterogeneous devices using different features. icverse [3], or Microsoft Azure Spatial Anchors [23]). This is for a variety of reasons, such as reducing on-board compute or enabling collaborative experiences and crowd-sourced mapping. In most settings, images are typically not shared across devices or uploaded to the cloud primarily due to privacy reasons [25], but also to reduce bandwidth and storage requirements. Instead, image features are com-puted locally on the device and only the extracted keypoints and descriptors are shared. In other words, it is impossible to re-extract features whenever a different representation is required, as the images are no longer available.
There are two fundamental limitations of existing sys-tems. First, they cannot adopt new feature algorithms be-cause incompatibilities of feature representations make re-localization against maps built using older features impos-sible. In a world of continuous progress on local features in the research community, this severely limits progress on localization and mapping. One might argue for sim-1
ply re-building the maps from scratch whenever a signifi-cantly improved feature algorithm is available. However, content attached to the old maps would be lost and map-ping is an expensive process, where it could take weeks or even months until the whole area is re-visited. Second, co-localization and collaborative mapping scenarios with de-vices using different features is impossible. The situation is further complicated by the fact that many devices im-plement specific algorithms in hardware for efficiency rea-sons, making a client-side upgrade of algorithms impossi-ble. This also means that many of the existing commercial solutions might be significantly behind the state-of-the-art art in the research community, because they cannot easily upgrade their algorithms and representations.
In this paper, we first define three novel scenarios ad-dressing the challenges in a world of changing local feature representations (c.f . Figures 1 and 2):
• Continuous deployment of feature representations without requiring an explicit re-mapping phase.
• Cross-device localization when the localization and mapping devices use different features.
• Collaborative mapping with multiple heterogeneous devices and features.
Note that these scenarios are completely different from ex-isting industrial or academic setups, where generally a fixed local feature extraction algorithm is used for co-localization and throughout the evolution of a map. Section 3 introduces the scenarios in more detail.
As a first step towards enabling these scenarios, we fo-cus on local feature descriptors and propose what to our knowledge is the first principled and scalable approach to the underlying challenges. Our learned approach translates descriptors from one representation to another without any assumptions about the structure of the feature vectors and enables matching of features with incompatible dimension-ality as well as distance metrics. For instance, we can match 512 dimensional binary BRIEF [10] against 128 dimen-sional floating point SIFT [28] or even deep learning mod-els such as HardNet [33] or SOSNet [51] and vice versa.
Our method has linear scalability in the number of algo-rithms and is specifically designed to have a small compu-tational footprint to enable deployment on low-compute de-vices. The training data is generated automatically by com-puting different descriptors from the same image patches.
We evaluate our approach on relevant geometric tasks in the context of the newly proposed scenarios. We first con-sider localization (pose estimation) on the Aachen Day &
Night benchmark [46]. Next, we assess the performance for 3D mapping from crowd-sourced images using the bench-mark of Sch¨onberger et al. [48]. In the supplementary mate-rial, we also show results on the HPatches descriptor bench-mark [7], the Image Matching Workshop challenge [22], and the InLoc Indoor Visual Localization dataset [50]. Our experiments demonstrate the effectiveness and high practi-cal relevance of our method for real-world localization and mapping systems.
To summarize our contributions, we i) introduce three novel scenarios to localization and mapping in a world of changing feature representations, ii) propose the first prin-cipled and scalable approach tackling these newly intro-duced scenarios, and iii) demonstrate the effectiveness of our method on challenging real-world datasets. 2.