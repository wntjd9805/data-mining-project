Abstract
We introduce the task of weakly supervised learning for detecting human and object interactions in videos. Our task poses unique challenges as a system does not know what types of human-object interactions are present in a video or the actual spatiotemporal location of the human and the object. To address these challenges, we introduce a contrastive weakly supervised training loss that aims to jointly associate spatiotemporal regions in a video with an action and object vocabulary and encourage temporal con-tinuity of the visual appearance of moving objects as a form of self-supervision. To train our model, we introduce a dataset comprising over 6.5k videos with human-object interaction annotations that have been semi-automatically curated from sentence captions associated with the videos.
We demonstrate improved performance over weakly super-vised baselines adapted to our task on our video dataset. 1.

Introduction
In this paper, we study the problem of weakly super-vised human-object interaction detection in videos. Given a video sequence, as illustrated in Figure 1, a system must correctly identify and localize the person and interacted ob-ject (“bike”) in the scene, in addition to identifying the ac-tion (“washing”) taken by the human, for the duration of the interaction in the video without bounding box supervi-sion. While there has been an impressive progress in learn-ing visual-language representations [32, 21, 25] from hun-dreds of millions of captioned images or videos recently, the learnt representations focus on classifying or retrieving entire images or videos given a language query. Our task is more challenging as it requires the models to correctly de-tect both the human and object bounding boxes in multiple frames of the video. 2Czech Institute of Informatics, Robotics and Cybernetics at the
Czech Technical University in Prague
∗Work done at Adobe Research during SL’s summer internship
Figure 1: We seek to detect human-object interactions in videos.
In this example, our system is able to detect “human washing bike” in the given video. Our approach learns to detect such interactions in a weakly supervised fashion, i.e., without requiring bounding box annotations at training time. (Video credit: Dude Chennai [6])
Human-object interaction detection has been primarily studied in the context of still images [3, 4, 14, 19, 47, 48, 31, 50, 60, 46, 40]. However, they are naturally temporal events that take place over a period of time. Interactions such as “drinking” or “pushing” occur between a human and an object over time, making videos a natural modality for studying this problem.
Existing video-based methods primarily rely on strong bounding box supervision and having access to a fully an-notated video dataset. However, relying on strong super-vision has significant drawbacks. First, exhaustively an-notating the spatial location of objects in a video is time consuming given the large number of frames in a video.
Second, scaling to the large number of possible interactions and obtaining a sufficient number of ground truth bound-ing boxes is challenging due to the potentially open vocab-ulary of objects and actions and the combinatorial nature of human-object interactions. Third, interactions typically fol-low a long-tailed distribution, with common human-object interactions occurring much more frequently than others
[41, 19]. While supervised learning usually prefers com-mon interactions, a robust human-object interaction detec-tion system should instead perform equally well on both common and rare interactions.
In this work, we seek to leverage videos with verb and noun phrase annotations derived from natural language sen-tence captions to learn to detect human-object interactions in videos in a weakly supervised manner. Such an approach
is advantageous as obtaining video-level annotations is sig-nificantly less costly than bounding boxes in videos. Lever-aging such data makes it possible to scale training to a larger number of videos and vocabulary of objects and actions.
Our task is challenging as we do not know the corre-spondence between the verb-object queries and spatiotem-poral regions in the training videos. A system must learn to establish these correspondences without spatial bounding box supervision. We thus propose a contrastive loss over spatiotemporal regions for detecting human-object interac-tions in videos. Our loss jointly associates candidate spa-tiotemporal regions with an action and object vocabulary in a weakly supervised manner and leverages cues about the temporal continuity of objects in motion as a form of self-supervision. Such a formulation allows us to deal with an open vocabulary of language queries which is especially de-sirable in human-object interaction, due to the high preva-lence of rare and unseen action and object combinations.
Our paper has three main contributions: (1) We present an approach that integrates spatiotemporal information for humans and objects for weakly supervised human-object in-teraction detection in videos. Our approach does not re-quire manual bounding box annotations. (2) We present a contrastive loss over spatiotemporal regions that leverages weak verb-object supervision from video captions and self-supervision from temporal continuity in video. It allows de-tecting rare and unseen human-object interactions in a zero-shot manner. (3) We introduce a new dataset of over 6.5k videos to evaluate human-object interaction in videos. We demonstrate improved performance over weakly supervised baselines adapted to our task. The dataset is made public to facilitate further research1. 2.