Abstract 1.

Introduction
Despite significant progress, we show that state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable.
To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature repre-sentations, making them sensitive to even small occlusions.
In contrast, PARE’s part-guided attention mechanism over-comes these issues by exploiting information about the vis-ibility of individual body parts while leveraging informa-tion from neighboring body-parts to predict occluded parts.
We show qualitatively that PARE learns sensible atten-tion masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. The code and data are available for research purposes at https://pare.is.tue.mpg.de/
Regressing 3D human pose and shape (HPS) directly from RGB images has many applications in robotics, com-puter graphics, AR/VR and beyond. The task is to take a single image [24, 29, 40] or video sequence [25, 27, 35] as input and to regress the parameters of a human body model such as SMPL [33] as output. Powered by deep CNNs, this task has seen rapid progress [24, 27, 29, 40]. However, in fully in-the-wild settings, people often appear under occlu-sion either due to self-overlapping body-parts, due to close-range interaction with other people or due to occluding ob-jects such as furniture or other scene content. While pose estimation under occlusion has been treated in the literature
[8, 9, 14, 19, 42, 43, 53, 54, 59], we highlight that this issue is particularly important in the context of direct regression methods. Such methods use all the pixels in the input to pre-dict a single set of pose and shape parameters. Thus their pose estimates are particularly sensitive to even small per-turbations in the observations of the body and its parts.
In this paper, we apply a visualization technique [58] for occlusion sensitivity analysis that yields insights into when 1
and why such methods fail. This indicates that, for state-of-the-art (SOTA) methods, relatively small occlusions, even of only a single joint, can lead to entirely implausible pose predictions. This is illustrated in Fig. 1, where we slide an occluder over the image, regress body pose, and compute the average 3D joint error with respect to ground truth. The heatmaps in Fig. 1 (d,g) illustrate a method’s sensitivity to a square occluder centered at each pixel location (shown in white). The visualization reveals that methods like SPIN
[40] are highly sensitive to localized part occlusion. To ad-dress this issue, we propose a method, based on a novel part-guided attention mechanism, making direct regression approaches more robust to occlusion.
The proposed method is called Part Attention REgres-sor (PARE). It has two tasks: the primary one is learning to regress 3D body parameters in an end-to-end fashion, and the auxiliary task is learning attention weights per body part. Each task has its own pixel-aligned feature extrac-tion branch. We guide the attention branch with part seg-mentation labels in the early stages of training and continue without them for the later stages, thus we call it body-part-driven attention. Our key insight is that, to be robust to oc-clusions, the network should leverage pixel-aligned image features of visible parts to reason about occluded parts.
Given the success of attention-based methods on other tasks [11, 18, 34, 55], we exploit insights gained from the occlusion sensitivity analysis to focus attention on body parts. Therefore, we supervise the attention mask with part segmentations, but then train end-to-end with pose super-vision only, allowing the attention mechanism to leverage all useful information from the body and the surrounding pixels. This gives the network freedom to attend to regions it finds informative in an unsupervised way. As a result,
PARE learns to rely on visible parts of the body to improve robustness to occluded parts and overall performance on 3D pose estimation (Fig. 1 e-f).
To quantitatively evaluate the performance of PARE, we perform experiments on the 3DPW [52], 3DOH [59], and 3DPW-OCC [52] datasets. The results show that PARE yields consistently lower error than the state-of-the-art for both occlusion and non-occlusion cases.
In summary, our key contributions are: (1) We apply a visualization technique [58] to study how local part occlu-sion can influence global pose; we call this occlusion sen-sitivity analysis. (2) This analysis motivates a novel body-part-driven attention framework for 3D HPS regression that leverages pixel-aligned localized features to regress body pose and shape. (3) The network uses part visibility cues to reason about occluded joints by aggregating features from the attended regions, and by doing so, achieves robustness to occlusions. (4) We achieve SOTA results on a 3D pose estimation benchmark featuring occluded bodies, as well as a standard benchmark. 2.