Abstract
Multi-modal learning, which focuses on utilizing vari-ous modalities to improve the performance of a model, is widely used in video recognition. While traditional multi-modal learning offers excellent recognition results, its com-putational expense limits its impact for many real-world applications. In this paper, we propose an adaptive multi-modal learning framework, called AdaMML, that selects on-the-fly the optimal modalities for each segment condi-tioned on the input for efficient video recognition. Specif-ically, given a video segment, a multi-modal policy net-work is used to decide what modalities should be used for processing by the recognition model, with the goal of im-proving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model us-ing standard back-propagation. Extensive experiments on four challenging diverse datasets demonstrate that our pro-posed adaptive approach yields 35% − 55% reduction in computation when compared to the traditional baseline that simply uses all the modalities irrespective of the in-put, while also achieving consistent improvements in ac-curacy over the state-of-the-art methods. Project page: https://rpand002.github.io/adamml.html. 1.

Introduction
Videos are rich in multiple modalities: RGB frames, mo-tion (optical flow), and audio. As a result, multi-modal learning which focuses on utilizing various modalities to improve the performance of a video recognition model, has attracted much attention in the recent years. Despite en-couraging progress, multi-modal learning becomes com-putationally impractical in real-world scenarios where the videos are untrimmed and span several minutes or even hours. Given a long video, some modalities often provide irrelevant/redundant information for the recognition of the action class. Thus, utilizing information from all the input modalities may be counterproductive as informative modali-ties are often overwhelmed by uninformative ones in long videos. Furthermore, some modalities require more compu-tation than others and hence selecting the cheaper modality with good performance can significantly save computation leading to more efficient video recognition.
Let us consider the video in Figure 1, represented by eight uniformly sampled video segments from a video. We ask,
Do all the segments require both RGB and audio stream to recognize the action as “Mowing the Lawn” in this video?
The answer is clear: No, the lawn mower is moving with relevant audio only in the third and sixth segment, therefore we need both RGB and audio streams for these two video segments to improve the model confidence for recognizing the correct action, while the rest of the segments can be processed with only one modality or even skipped (e.g., the first and last video segment) without losing any accuracy, resulting in large computational savings compared to pro-cessing all the segments using both modalities. Thus, in contrast to the commonly used one-size-fits-all scheme for multi-modal learning, we would like these decisions to be made individually per input segment, leading to different amounts of computation for different videos. Based on this intuition, we present a new perspective for efficient video recognition by adaptively selecting input modalities, on a per segment basis, for recognizing complex actions.
In this paper, we propose AdaMML, a novel and differen-tiable approach to learn a decision policy that selects optimal modalities conditioned on the inputs for efficient video recog-nition. Specifically, our main idea is to learn a model (re-ferred to as the multi-modal policy network) that outputs the posterior probabilities of all the binary decisions for using or skipping each modality on a per segment basis. As these de-cision functions are discrete and non-differentiable, we rely on an efficient Gumbel-Softmax sampling approach [23] to learn the decision policy jointly with the network pa-rameters through standard back-propagation, without resort-ing to complex reinforcement learning as in [60, 61]. We design the objective function to achieve both competitive performance and efficiency required for video recognition.
We demonstrate that adaptively selecting input modalities by a lightweight policy network yields not only significant savings in computation (e.g., about 47.3% and 35.2% less
Figure 1: A conceptual overview of our approach. Rather than processing both RGB and Audio modalities for all the segments, our approach learns a policy to select the optimal modalities per input segment, that is needed to correctly recognize an action in a given video.
In the figure, the lawn mower is moving with relevant audio only in the third and sixth segment, therefore those segments could be processed using both modalities, while the rest of the segments require only one modality (e.g., only audio is relevant for the fourth segment as the lawn mower moves outside of the camera but its sound is still clear) or even skipped (e.g., both of the modalities are irrelevant in the first and the last segment), without losing any accuracy. Note that our approach can be extended to any number of modalities as shown in experiments.
GFLOPS compared to a weighted fusion baseline that simply uses all the modalities, on Kinetics-Sounds [2] and Activi-tyNet [6] respectively), but also consistent improvement in accuracy over the state-of-the-art methods.
The main contributions of our work are as follows:
• We propose a novel and differentiable approach that automatically determines what modalities to use per segment per input for efficient video recognition. This is in sharp contrast to current multi-modal learning approaches that utilizes all the input modalities without considering their relevance to the video recognition.
• We efficiently train the multi-modal policy network jointly with the recognition model using standard back-propagation through Gumbel-Softmax sampling.
• We conduct extensive experiments on four video benchmarks (Kinetics-Sounds [2], ActivityNet [6],
FCVID [24] and Mini-Sports1M [25]) with different multi-modal learning tasks (RGB + Audio, RGB +
Flow, and RGB + Flow + Audio) to demonstrate the su-periority of our approach over state-of-the-art methods. 2.