Abstract
The problem of long-tailed recognition, where the num-ber of examples per class is highly unbalanced, is consid-ered.
It is hypothesized that the well known tendency of standard classifier training to overfit to popular classes can be exploited for effective transfer learning. Rather than eliminating this overfitting, e.g. by adopting popular class-balanced sampling methods, the learning algorithm should instead leverage this overfitting to transfer geometric in-formation from popular to low-shot classes. A new classi-fier architecture, GistNet, is proposed to support this goal, using constellations of classifier parameters to encode the class geometry. A new learning algorithm is then proposed for GeometrIc Structure Transfer (GIST), with resort to a combination of loss functions that combine class-balanced and random sampling to guarantee that, while overfitting to the popular classes is restricted to geometric parame-ters, it is leveraged to transfer class geometry from popular to few-shot classes. This enables better generalization for few-shot classes without the need for the manual specifica-tion of class weights, or even the explicit grouping of classes into different types. Experiments on two popular long-tailed recognition datasets show that GistNet outperforms existing solutions to this problem. 1.

Introduction
The availability of large-scale datasets, with large num-bers of images per class [3], has been a major factor in the success of deep learning for tasks such as object recogni-tion. However, these datasets are manually curated and ar-tificially balanced. This is unlike most real world appli-cations, where the frequencies of examples from different classes can be highly unbalanced, leading to skewed distri-butions with long tails.
This has motivated recent interest in the problem of long-tailed recognition [13], where the training data is highly un-balanced but the test set is kept balanced, so that equally good performance on all classes is crucial to achieve high overall accuracy.
Success in the long-tailed recognition setting requires specific handling of class imbalance during training, since a classifier trained with the standard cross-entropy loss will overfit to highly populated classes and perform poorly on low-shot classes. This has motivated several works to fight class overfitting with methods, like data re-sampling [27] or cost-sensitive losses [10], that place more training emphasis on the examples of lower populated classes.
It is, however, difficult to design augmentation or class weighting schemes that do not either under or over-emphasize the few-shot classes. In this work, we seek an approach that is fully data driven and leverages overfitting to the popular classes, rather than combat it. The idea is to transfer some properties of these classes, which are well learned by the standard classifier, to the classes with insuf-ficient data, where this is not possible.
For this, we leverage the interpretation of a deep clas-sifier as the composition of an embedding, or feature ex-tractor, implemented with several neural network layers and a parametric classifier, implemented with a logistic regres-sion layer, at the top of the network. While the embedding is shared by all classes, the classifier parameters are class-specific, namely a weight-vector per class, as shown in Fig-ure 1.
We exploit the fact that the configuration of these weight vectors determines the geometry of the embedding. This consists of the class-conditional distribution, and associated metric, of the feature vectors of each class, which define the class boundaries. For a well learned network, this geometry is identical for all classes.
In the long-tailed setting, the geometry is usually well learned for many-shot classes, but not for classes with insufficient training samples, as shown in the left of Figure 1.
Figure 1. Left: in long-tailed recognition, the small number of samples from medium- and few-shot classes make it difficult to learn their geometry, leading to inaccurate class boundaries. This is unlike many-shot classes, whose natural geometry can usually be learned.
Middle: the boundaries are corrected by transferring the geometric structure of the many-shot classes to the classes with few examples.
Right: GistNet implements geometric structure transfer by implementing constellations of classification parameters. These consist of a class-specific center and a set of displacements shared by all classes. Under GIST training, these tend to follow the natural geometry of the many-shot classes, which is transferred to the medium- and few-shot classes.
The goal is to transfer the geometric structure of the many-shot classes to the classes with few examples, as shown in the middle of the figure, to eliminate this prob-lem. The challenge is to implement this transfer using only the available training data, i.e. without manual specification of class-weights or heuristic recipes, such as equating these weights to class frequency.
We address this challenge with a combination of con-tributions. First, we enforce a globally learned geometric structure, which is shared by all classes. To avoid the com-plexity of learning a full-blown distance function, which frequently requires a large covariance matrix, we propose a structure composed by a constellation of classifier param-eters, as shown on the right of Figure 1. This consists of a class-specific center, which encodes the location of the class, and a set of displacements, which are shared by all classes and encode the class geometry.
Second, we rely on a mix of randomly sampled and class-balanced mini-batches to define two losses that are used to learn the different classifier parameters. Class-balanced sampling is used to learn the class-specific cen-ter parameters. This guarantees that the learning is based on the same number of examples for all classes, avoiding a bias towards larger classes. Random sampling is used to learn the shared geometry parameters (displacements). This leverages the tendency of the standard classifier to overfit to the popular classes, making them dominant for the learning of class geometry, and thus allowing the transfer of geomet-ric structure from these to the few-shot classes. In result, the few shot classes are learned equally to the popular classes with respect to location but inherit their geometric structure, which enables better generalization.
We propose a new learning algorithm, denoted Geomet-rIc Structure Transfer (GIST), that combines the two types of sampling, so as to naturally account for all the data in the training set, without the need for the manual specification of class weights, or even the explicit grouping of classes into
Figure 2. GistNet approximates the shared geometry by a constel-lation (mixture) of spherical Gaussians. different types. While we adopt the standard division into many-, medium-, and few-shot classes for evaluation, this is not necessary for training.
A deep network that implements the parameter constel-lations of Figure 1 and GIST training is then introduced and denoted as the GistNet. Experiments on two popular long-tailed recognition datasets show that it outperforms previ-ous approaches to long-tailed recognition.
Overall, this work makes several contributions to long-tailed recognition. First, we point out that the tendency of the standard classifier to overfit to popular classes can be ad-vantageous for transfer learning. The goal should not be to eliminate this overfitting, e.g. by uniquely adopting the now popular class-balanced sampling, but leverage it to trans-fer geometric information from the popular to the low-shot classes.
Second, we propose a new GistNet classifier architecture to support this goal, using constellations of classifier param-eters to encode the class geometry.
Third, we introduce a new learning algorithm, GIST, that combines class-balanced and random sampling to leverage overfitting to the popular classes and enable the transfer of class geometry from popular to few-shot classes.
ent based methods such as MAML and its variants [4, 5], or
LEO [17]. These methods take advantage of second deriva-tives to update the model from few-shot samples. Alterna-tively, the problem has been addressed with metric based solutions, such as the matching [22], prototypical [19], and relation [20] networks. These approaches learn metric em-beddings that are transferable across classes.
There have also been proposals for feature augmenta-tion, aimed to augment the data available for training, e.g. by combining GANs with meta-learning [23], synthesizing features across object views [11] or other forms of data hal-lucination [7]. All these methods are designed specifically for few-shot classes and often under-perform for many-shot classes.
Learning without forgetting aims to train a model se-quentially on new tasks without forgetting the ones already learned. This problem has been recently considered in the few-shot setting [6], where the sequence of tasks includes a mix of many-shot and few-shot classes.
Proposed solutions [6, 16] attempt to deal with this by training on many-shots first, using the many-shot class weights to generate few-shot class weights, and combining them together. These techniques are difficult to generalize to long-tailed recognition, where the transition from many-to few- shot classes is continuous and includes a large num-ber of medium-shot classes. 3. Geometric Structure Transfer
In this section, we introduce the proposed solution of the long-tailed recognition problem by geometric structure transfer and the GistNet architecture. 3.1. Regularization by Geometric Structure Trans-fer
A popular architecture for classification is the softmax classifier. This consists of an embedding that maps images x ∈ X into feature vectors fϕ(x) ∈ F, implemented by multiple neural network layers, and a softmax layer that es-timates class posterior probabilities according to p(y = k|x; ϕ, wk) = k fϕ(x)] exp[wT k′ exp[wT k′fϕ(x)] (cid:80) (1) where ϕ denotes the embedding parameters and wk is the weight vector of the kth class.
The model is learned with a training set S = i=1 of ns examples, by minimizing the cross en-{(xi, yi)}ns tropy loss
LCE = (cid:88) (xi,yi)∈S
− log p(yi|xi). (2)
Recognition performance is evaluated on a test set T =
{(xi, yi)}nt i=1, of nt examples.
Figure 3. t-SNE visualization of 3 few-shot classes on ImageNet-LT test set, together with the constellations wkj. 2.