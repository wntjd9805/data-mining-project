Abstract
With the development of computational power and tech-niques for data collection, deep learning demonstrates a superior performance over most existing algorithms on vi-sual benchmark data sets. Many efforts have been devoted to studying the mechanism of deep learning. One impor-tant observation is that deep learning can learn the dis-criminative patterns from raw materials directly in a task-dependent manner. Therefore, the representations obtained by deep learning outperform hand-crafted features signifi-cantly. However, for some real-world applications, it is too expensive to collect the task-specific labels, such as visual search in online shopping. Compared to the limited avail-ability of these task-specific labels, their coarse-class labels are much more affordable, but representations learned from them can be suboptimal for the target task. To mitigate this challenge, we propose an algorithm to learn the fine-grained patterns for the target task, when only its coarse-class labels are available. More importantly, we provide a theoretical guarantee for this. Extensive experiments on real-world data sets demonstrate that the proposed method can significantly improve the performance of learned repre-sentations on the target task, when only coarse-class infor-mation is available for training. 1.

Introduction
Deep learning attracts more and more attentions due to its tremendous success in computer vision [11, 14, 19] and
NLP applications [7, 21]. With modern neural networks, deep learning can even achieve a better performance than human beings on certain fundamental tasks [14, 26]. The improvement from deep learning makes many applications, e.g., autonomous driving [5], visual search [23], question-answering system [30], etc., become feasible.
*Corresponding author
Figure 1. Illustration of different patterns learned from different tasks on the same synthetic data consisting of 512 images. Accord-ing to different combinations of patches, three tasks are included: 32 coarse-class classification (i.e., 32-class with big patches), 128-class classification (i.e., 128-class with small patches) and instance-level classification (i.e., Individual with big and small patches). The detailed setting of the experiment can be found in supplementary.
Compared with many existing models, which are de-signed for hand-crafted features, deep learning works in an end-to-end learning manner.
It can explore the most dis-criminative patterns (i.e., features) from raw materials di-rectly for a specific task. Without an explicit phase of gen-erating features, deep learning demonstrates a significant improvement over existing methods [14, 19]. Using the fea-tures generated by deep learning, conventional methods can also perform better than the counterpart with hand-crafted features [2, 8, 11, 12]. This observation implies that neural networks can learn the task-related patterns sufficiently.
In deep learning, representations are often learned with respect to a specific task. Therefore, different patterns can be extracted even on the same data set for different appli-cation scenarios as shown in the example of Fig. 1. This
phenomenon demonstrates that neural networks will only pay attention to those patterns that are helpful for the train-ing task and ignore the unrelated patterns. Therefore, deep learning has to access a massive amount of labeled exam-ples to achieve the ideal performance while the label infor-mation has to be closely related to the target task.
With the development of deep learning, a large training data size has been emphasized and many large-scale labeled data sets [6, 20] become available. However, the correlation between the learned representations from provided labels and the target task is less investigated. In some real-world applications, it is often too expensive to gather task-specific labels, while their coarse-class labels are much more ac-cessible. Taking visual search [23] as an example, given a query image of “husky”, a result of “husky” is often ex-pected than a “dog”. Apparently, the label information like
“husky” is much more expensive than that like “dog”. The problem becomes more challenging in the online shopping scenario, where many items (e.g., clothes) have very subtle differences. The gap between the available labels and the target task makes the learned representations suboptimal.
To improve the performance of learned representations for a target task, a straightforward way is to label a sufficient number of examples specifically for that task, which can align the supervised information and the target task well.
However, this strategy is not affordable. Unlike coarse-class labels, some task-specific labels (e.g., species of dogs) can only be identified by very experienced experts, which is ex-pensive and inefficient. For the visual search task in the on-line shopping scenario, even experts cannot label massive examples accurately.
Recently, unsupervised methods become popular for rep-resentation learning [3, 9, 13, 24, 29]. These methods first learn a deep model without any supervision on the source domain. After that, the learned model will be fine-tuned with the labeled data from the target domain. Although the pre-trained model is learned in an unsupervised manner, task-specific labels are required in the phase of fine-tuning, which are often very limited or have no access in some real-world applications. Considering that their coarse-class labels are much more affordable, in this work, we study the problem when data is from the target domain but only coarse-class labels are available.
Concretely, we aim to mitigate the issue by leveraging the information from coarse classes to learn appropriate rep-resentations for a target task. We verify that fine-grained patterns, which are essential for a target task, are often ne-glected when the deep model is trained only with coarse-class labels. Meanwhile, the popular pretext task in un-supervised representation learning, i.e., instance classifica-tion, may introduce too many noisy patterns that are irrel-evant to the target task. Fortunately, we can theoretically prove that incorporating the task of coarse-class classifi-cation, representations learned from instance classification will be more appropriate for the target task. Based on this, we propose a new algorithm to learn appropriate represen-tations for a target task when the task-specific labels are not available but their coarse-class labels are accessible. Be-sides, inspired by our analysis, a novel instance proxy loss is proposed to further improve the performance. Extensive ex-periments on benchmark data sets demonstrate that the pro-posed algorithm can significantly improve the performance on real-world applications when only coarse-class labels are available. 2.