Abstract
We present a novel approach for estimating depth from a monocular camera as it moves through complex and crowded indoor environments, e.g., a department store or a metro station. Our approach predicts absolute scale depth maps over the entire scene consisting of a static background and multiple moving people, by training on dynamic scenes.
Since it is difficult to collect dense depth maps from crowded indoor environments, we design our training framework without requiring depths produced from depth sensing de-vices. Our network leverages RGB images and sparse depth maps generated from traditional 3D reconstruction methods to estimate dense depth maps. We use two constraints to handle depth for non-rigidly moving people without track-ing their motion explicitly. We demonstrate that our ap-proach offers consistent improvements over recent depth estimation methods on the NAVERLABS dataset, which in-cludes complex and crowded scenes. 1.

Introduction
There is considerable interest in using robotics and aug-* These two authors contributed equally.
Correspondence to dongki.jung@naverlabs.com, kevchoi@umd.edu mented reality technologies in crowded real-world spaces corresponding to malls, airports, or public places.
In or-der to perform safe navigation or combine real and virtual worlds, robots [29, 28, 44] or mobile devices [50, 35] need a 3D geometric representation of large-scale indoor envi-ronments. While there is considerable progress in terms of capturing depth using LiDARs or stereo cameras, existing devices still have their own limitations. For example, 3D
LiDARs [8] tend to produce sparse depth maps for distant objects and may result in noisy point cloud maps due to the high level of occlusions caused by multiple moving people.
Moreover, because of the high prices and large volumes of the depth sensors, there is a critical need to consider the case where only a single camera is available.
Given a large number of video frames, traditional 3D re-construction methods such as structure-from-motion (SfM) and multi-view stereo (MVS) [14, 49, 45, 46] can generate a 3D model. Recently, visual localization techniques [38] have been developed to allow mobile devices to obtain the location and camera pose from which the image is taken.
Given the 3D model and visual position, however, mobile devices moving through crowded indoor environments are only able to capture sparse and highly noisy depth maps.
This is because traditional reconstruction methods are based on both a static scene assumption (that static areas of the 1
scenes can be observed from two different viewpoints) and correct feature matching in two or more images. However, the moving pedestrians in crowded indoor environments tend to violate the static scenes assumption. Moreover, tra-ditional 3D reconstruction methods may fail to perform the correct matching on non-textured (e.g., walls), specular, and reflective regions (e.g., glass) of the scene. Consequently, these problems lead to a 3D model on complex and crowded indoor environments.
Main Results: To address these limitations, we explore new methods that can utilize the 3D model generated us-ing traditional 3D reconstruction methods [45, 46] into learning-based depth estimation algorithms for dynamic scenes. Given the 3D model, our method can be used for general applications because it can compute dense depth maps of dynamic scenes without reconstructing this 3D model iteratively. In contrast to supervised learning meth-ods [27, 5, 26], our method does not rely on dense depth maps generated from depth sensing devices. Our approach, shown in Fig. 1, takes an RGB image and a sparse depth map projected from the 3D model as input and outputs a dense depth map. Given the pose obtained from the SfM
[45], we propose using the photometric consistency loss, which enables our method to estimate dense depth maps, and depth loss, forcing our network to learn absolute-scale depth. Although these loss functions are useful for provid-ing dense depth maps in static background regions, there still exist great challenges in estimating depths of multi-ple non-rigidly moving objects, i.e. the pedestrians. To overcome this limitation, we propose two constraints: 1) a flow-guided shape constraint to refine the depth maps for human regions by filling missing parts of the human regions and removing visual artifacts, and 2) a normal-guided scale constraint to force our neural network to learn the absolute scale depth in human regions guided by depth values in the human’s ground contact point.
Compared to traditional reconstruction approaches or re-cent learning-based methods [26, 66, 25], our approach (DnD) shows a better ability to predict plausible depth in both human and non-human regions, though we only use a monocular camera. We evaluate our approach on the
NAVERLABS dataset [24], the first dataset that provides both metric 3D SfM models and dynamic scenes collected from a department store and a metro station. The main con-tributions of this paper are summarized as follows:
• We introduce a novel approach to estimate the depth maps using both dynamic scenes collected from a moving monocular camera and given sparse depth maps. We train the monocular depth estimation net-work with a 3D model generated by traditional 3D re-construction algorithms [45, 46].
• We present two novel constraints based on optical flow and surface normal that improve the accuracy of our monocular depth estimation network to predict abso-lute scale depths for moving people.
• We highlight the benefits of our approach over the state-of-the-art methods on crowded indoor envi-ronments and observe 3.6% -10.2% improvement in
RMSE. Furthermore, our method works well in diverse indoor datasets like TUM RGB-D and NYUv2. 2.