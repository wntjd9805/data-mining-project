Abstract
Self-supervised representation learning is able to learn semantically meaningful features; however, much of its re-cent success relies on multiple crops of an image with very few objects. Instead of learning view-invariant representa-tion from simple images, humans learn representations in a complex world with changing scenes by observing ob-ject movement, deformation, pose variation and ego motion.
Motivated by this ability, we present a new self-supervised learning representation framework that can be directly de-ployed on a video stream of complex scenes with many mov-ing objects. Our framework features a simple flow equiv-ariance objective that encourages the network to predict the features of another frame by applying a flow transformation to the features of the current frame. Our representations, learned from high-resolution raw video, can be readily used for downstream tasks on static images. Readout experi-ments on challenging semantic segmentation, instance seg-mentation, and object detection benchmarks show that we are able to outperform representations obtained from pre-vious state-of-the-art methods including SimCLR [6] and
BYOL [18]. 1.

Introduction
Rich and informative visual representations epitomize the revolution of deep learning in computer vision in the past decade. Deep neural nets deliver surprisingly com-petitive performance on tasks such as object detection
[15, 34, 9] and semantic segmentation [4, 50]. Until very recently, visual representations have been learned by large scale supervised learning. However, for more challenging tasks such as semantic or instance segmentation, it is much more expensive to obtain labels compared to object classi-fication. On the other hand, the human brain learns generic visual representations from raw video of the complex world without much explicit supervision. This is the direction that we would like to get one step closer towards in this paper.
*This work was done by all authors while at Uber ATG
Figure 1: Our proposed self-supervised representation learning from Flow Equivariance (FlowE). Our method is based on BYOL [18], a state of the art method for static image representation learning. We encourage the features to obey the same flow transformation as the input image pairs.
Recent advances in self-supervised or unsupervised rep-resentation learning, such as SimCLR [6] and BYOL [18], seem to point us to a bright path forward: by simply min-imizing the feature distance between two different views of a single image, and performing linear readout at test time on top, state-of-the-art approaches are now able to match the classification performance of networks trained with full supervision end-to-end [23, 36, 21]. While not us-ing any class labels, these methods still rely on the dataset curation process of carefully selecting clean and object-centered images with a balanced class distribution. In con-trast, videos in the wild feature crowded scenes and se-vere data imbalance. As a result, different crops of the same frame can often lead to either uninteresting regions or erroneous alignment of different instances in crowded areas. Moreover, none of these methods leverage tempo-ral information, which contains a rich set of object move-ment, deformation, and pose variations. While there has been a large body of literature on learning representation from video [43, 24, 44, 35, 12, 13, 41, 42], they typically focus on the predicting correspondence across frames and have not shown better performance on generic downstream
tasks such as semantic and instance segmentation than pre-trained supervised representations from ImageNet [20]. samples, and SimCLR [6] proposed to add a non-linear pro-jection head to make the core representation more general.
In this paper, we are interested in learning generic repre-sentation from raw high-resolution videos that are directly useful for object detection as well as semantic and instance segmentation. Whereas prior invariance-based learning al-gorithms completely disregard ego-motion and flow trans-formations across frames, we argue these are essential ele-ments responsible for the learning of visual representations in complex scenes [1]. Instead of enforcing multiple crops of the same image (or adjacent frames) to be close in the feature space, as advocated in prior literature [19, 6, 18, 16], we propose a simple flow equivariance objective that can be applied densely at every pixel on the feature map, sum-marized in Figure 1. In particular, given two consecutive video frames, we estimate an optical flow map that denotes a pixel-wise transformation between the two frames. We
T then train the network to minimize the distance between the the first frame h1 and the warped features of the second 1(h2). Using optical flow ensures that crowded frame regions are handled with precise instance alignment. It is also worth noting that off-the-shelf flow estimators can be trained using either from graphics simulation [2, 11, 26] or from ego-motion and depth estimation [33], without any hu-man labeling effort.
T
âˆ’
Experiments are carried out on two complex driving video datasets, BDD100K [48] and our in-house dataset Ur-banCity, which are collected from a front camera on a mov-ing car, just like seeing from a mobile agent in the wild. Our approach, learning from raw videos, can achieve competi-tive readout performance on semantic and instance segmen-tation tasks. Surprisingly, we are also able to outperform pre-trained representations from ImageNet [21], likely be-cause of the large domain gap between ImageNet images and driving videos. 2.