Abstract
Self-supervised depth estimation for indoor environ-ments is more challenging than its outdoor counterpart in at least the following two aspects: (i) the depth range of in-door sequences varies a lot across different frames, making it difficult for the depth network to induce consistent depth cues, whereas the maximum distance in outdoor scenes mostly stays the same as the camera usually sees the sky; (ii) the indoor sequences contain much more rotational mo-tions, which cause difficulties for the pose network, while the motions of outdoor sequences are pre-dominantly trans-lational, especially for driving datasets such as KITTI. In this paper, special considerations are given to those chal-lenges and a set of good practices are consolidated for improving the performance of self-supervised monocular depth estimation in indoor environments. The proposed method mainly consists of two novel modules, i.e., a depth factorization module and a residual pose estimation mod-ule, each of which is designed to respectively tackle the aforementioned challenges. The effectiveness of each mod-ule is shown through a carefully conducted ablation study and the demonstration of the state-of-the-art performance on three indoor datasets, i.e., EuRoC, NYUv2 and 7-Scenes. 1.

Introduction
Depth estimation plays an essential role in a variety of 3D perceptual tasks, such as autonomous driving, virtual reality (VR), and augmented reality (AR). In this paper, we tackle the problem of estimating the depth map from a sin-gle image in a self-supervised manner. Compared to the su-pervised methods [5, 8], self-supervision [9, 46, 12] frees us from having to capture the ground-truth depth using depth sensors (e.g., LiDAR) and therefore, it is more attractive in scenarios where obtaining the ground-truth is not possible.
*Joint first authorship. is the corresponding author (pe-terji530@gmail.com). R. Li’s contribution was made during an internship with OPPO US Research Center.
P. Ji
Recently, self-supervised methods [12] have achieved significant success, producing depth prediction that is com-parable to that produced by the supervised methods [14, 8].
For example, on the KITTI dataset [10], Monodepth2 [12] achieves an absolute relative depth error (AbsRel) of 10.6%, which is not far from the AbsRel of 7.2% by supervised
DORN [8]. However, most of these self-supervised depth prediction methods [9, 46, 12] are only evaluated on outdoor datasets such as KITTI, leaving their performance opaque for indoor environments. A few methods [45, 44] have con-sidered indoor self-supervised depth prediction, but their performance still trail far behind the one on the outdoor datasets by methods such as [9, 46, 12] or the supervised counterparts [8, 41] on indoor datasets. For instance, on the indoor NYUv2 dataset [33], the method by Zhao et al. [44] reaches an AbsRel of 18.9%, which is much higher than what Monodepth2 can achieve on KITTI.
In view of the performance discrepancies between the indoor and outdoor scenes, we examine what makes in-door depth prediction more challenging than the outdoor case. Our first conjecture is that this is partly due to the fact that the scene depth range of indoor sequences varies a lot more than in the outdoor. This results in more diffi-culties for the depth network in inducing consistent depth cues across images. Our second observation is that the pose network, which is commonly used in self-supervised meth-ods [46, 12], tends to have large errors in rotation predic-tion. A similar finding in [47] shows that predicted poses have much higher rotational errors (e.g., 10 times larger) than geometric SLAM [26] even after using a recurrent pose network. This problem is not prominent on KITTI because the motions therein are mostly translational. How-ever, since indoor datasets are often captured by hand-held cameras [33] or MAVs [31] which inevitably undergo fre-quent rotations, the inaccurate rotation prediction becomes detrimental to the self-supervised training of a depth model for indoor environments.
Given the above considerations, we propose MonoIn-door, a monocular self-supervised depth estimation method tailored for indoor environments. Our MonoIndoor consists of two novel modules: a depth factorization module and a
residual pose estimation module. In the depth factorization module, we factorize the depth map into a global depth scale (for the current image) and a relative depth map. The depth scale factor is separately predicted by an extra branch in the depth network. In such a way, the depth network has more model plasticity to adapt to the depth scale changes during training. In the residual pose estimation module, we miti-gate the issue of inaccurate rotation prediction by perform-ing residual pose estimation in addition to an initial large pose prediction. Such a residual approach leads to more ac-curate computation of the photometric loss [12], which in turn leads to better model training for the depth network.
In summary, our contributions are:
• A novel depth factorization module that helps the depth network adapt to the rapid scale changes;
• A novel residual pose estimation module that mitigates the inaccurate rotation prediction issue in the pose net-work and in turn improves depth prediction;
• State-of-the-art performance of self-supervised depth prediction on three publicly available indoor datasets, i.e., EuRoC [31], NYUv2 [33], and 7-Scenes [32]. 2.