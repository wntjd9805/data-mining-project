Abstract
We propose an end-to-end pipeline, named Watch Once
Only (WOO), for video action detection. Current meth-ods either decouple video action detection task into sep-arated stages of actor localization and action classifica-tion or train two separated models within one stage.
In contrast, our approach solves the actor localization and action classification simultaneously in a unified network.
The whole pipeline is significantly simplified by unifying the backbone network and eliminating many hand-crafted components. WOO takes a unified video backbone to si-multaneously extract features for actor location and action classification.
In addition, we introduce spatial-temporal action embeddings into our framework and design a spatial-temporal fusion module to obtain more discriminative fea-tures with richer information, which further boosts the ac-tion classification performance. Extensive experiments on
AVA and JHMDB datasets show that WOO achieves state-of-the-art performance, while still reduces up to 16.7%
GFLOPs compared with existing methods. We hope our work can inspire re-thinking the convention of action de-tection and serve as a solid baseline for end-to-end action detection. Code is available at https://github.com/
ShoufaChen/WOO. 1.

Introduction
Figure 1: Motivation of WOO. (a) Previous dominant video action detection methods usually adopt two separate networks: an independent 2D detection model for actor lo-calization from every key frames, and a 3D video model for action classification from video clips. (b) Our end-to-end unified framework uses a single backbone network to han-dle both 2D image detection and 3D video classification (i.e. 2D spatial dimensions plus a temporal dimension). This unified backbone only “watches” an input video once, and directly produces both actor localization and action classifi-cation.
Video action detection consists of actor bounding box localization and action type classification.
It has a sig-nificant impact on applications such as robotics, security, health, and so on. Although considerable progress on accu-racy performance has been made, the complex and isolated pipelines of existing methods obstruct their scalability and practicality in the real world.
The complexity of current methods comes from a funda-mental dilemma between actor localization and action clas-sification, that is, using a single key frame is “positive” for actor localization but “negative” for action classification, while using multiple frames has a reverse impact. This is because actor localization requires a 2D detection model to predict actor bounding boxes on the key frame of a video clip. At this stage, taking neighboring frames of the clip into account brings extra computation and memory cost as well as localization noise.
In contrast, action classifica-tion heavily relies on a 3D video model to extract tempo-ral knowledge embedded in the video sequence. A single key frame brings a poor temporal motion representation for action classification.
Two possible workarounds are proposed by previous
methods to relieve this dilemma. The first one [7, 8, 38, 37, 25] uses an off-the-shelf person detector, which is not jointly trained with action classification models, to generate actor proposals. Then, an independent video model adopts these actor proposals and the raw frames as input to predict action classes. The person detector model alone is com-plicated enough, which is pre-trained on the ImageNet [4] and COCO [22] human keypoint detection, and further fine-tuned on the target action detection dataset. This workaround has a complex and computational-expensive pipeline, which requires two separate models and two train-ing stages. Furthermore, the separate optimization on two sub-problems leads to a sub-optimal solution [19].
The second type of methods [30, 19] jointly train the actor detection and action classification models in a single training stage. Although the training pipeline is simplified to some extent, both of these two models still need to extract features independently and directly from raw images. Thus, the overall framework still brings a heavy computation and memory cost.
A natural question is: “Is it possible to design a simple and unified framework to simultaneously address actor lo-calization and action classification in a single end-to-end model?”
This paper answers this question by proposing a novel video framework, termed Watch Once Only (WOO), which solves video action detection in an end-to-end manner.
WOO directly predicts coordinates of actor’s bounding boxes as well as probabilities of action classes from a video clip, as illustrated in Figure 1(b). Benefiting from our simple design, one could “watch” a video clip only once and predict where the actors are and what they are doing.
Specifically, our method consists of three key components, including a unified backbone, a spatial-temporal action em-bedding, and a spatial-temporal knowledge fusion mecha-nism.
First, we design a simple yet effective module that en-ables a single backbone network to provide task-specific feature maps for both the actor localization head and the action classification head. This module is light-weight and used to isolate the key frame features from the features of all frames in the early stage of the backbone network. The motivation is that the key frame feature gets more interac-tion with the neighboring frames as the model going deeper.
The proposed module can be easily plugged into many ex-isting video backbones, such as single-pathway I3D [18, 3],
SlowOnly [8], X3D [7], and two-pathway SlowFast [8].
Second, we notice that the unified architecture tends to behave well for actor localization, but is still limited for ac-tion classification. As observed in [12, 8], the difficulty in action detection mainly lies in action classification. Thus, we suspect that a single backbone for both tasks would bias towards localization and find an undesired solution, thus hurting the performance of action classification. Based on this observation, we propose spatial and temporal action embedding and the interaction mechanism between them, to make the action classification features more discrimina-tive in both spatial and temporal perspectives.
Third, we further propose a spatial-temporal fusion mod-ule to aggregate spatial and temporal knowledge together.
The spatial properties such as shape and pose, as well as the temporal properties such as dynamic motion and the temporal scale of action are combined through our spatial-temporal fusion module, to generate the action features for action classification.
Our main contributions are as follows. 1. We propose an end-to-end framework for video action detection, which directly produces the bounding boxes and action classes simultaneously, given a video clip as input. Our framework does not need an indepen-dent person detector, which is indispensable in existing works [7, 8, 38, 37, 25]. 2. We propose a spatial-and-temporal embedding, and an embedding interaction mechanism, which improve discriminativeness of the features for action classifica-tion. A spatial-temporal fusion module is further pro-posed to aggregate features from spatial and temporal dimensions. 3. Extensive experiments on AVA and JHMDB demon-strate that the performance of WOO could outperform or on par with previous well-established and compli-cated two-stage action detectors, while still reducing up to 16.7% FLOPs. 2.