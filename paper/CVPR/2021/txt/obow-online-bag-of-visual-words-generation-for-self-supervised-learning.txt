Abstract clustering-based approaches [4, 6, 7].
Learning image representations without human super-vision is an important and active research ﬁeld. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual represen-tations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited.
With this in mind, we propose a teacher-student scheme to learn representations by training a convolutional net to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn represen-tations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Ex-tensive experiments demonstrate the interest of our BoW-based strategy, which surpasses previous state-of-the-art methods (including contrastive-based ones) in several ap-plications. For instance, in downstream tasks such Pascal object detection, Pascal classiﬁcation and Places205 clas-siﬁcation, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also signiﬁcantly better even than those of super-vised pre-training. We provide the implementation code at https://github.com/valeoai/obow. 1.

Introduction
Learning unsupervised image representations based on convolutional neural nets (convnets) has attracted a signiﬁ-cant amount of attention recently. Many different types of convnet-based methods have been proposed in this regard, including methods that rely on using annotation-free pre-text tasks [15, 27, 43, 48, 52, 76], generative methods that model the image data distribution [16, 17, 20], as well as
Several recent methods opt to learn representations via instance-discrimination training [9, 19, 33, 71], typically implemented in a contrastive learning framework [12, 32].
The primary focus here is to learn low-dimensional image / instance embeddings that are invariant to intra-image varia-tions while being discriminative among different images. Al-though these methods manage to achieve impressive results, they focus less on other important aspects in representation learning, such as contextual reasoning, for which alternative reconstruction-based approaches [14, 52, 76, 77] might be better suited. For instance, the task of predicting from an image region the contents of the entire image requires to rec-ognize the visual concepts depicted in the provided region and then to infer from them the structure of the entire scene.
So, training for such a task has the potential of squeezing out more information from the training images and of learning richer and more powerful representations.
However, reconstructing image pixels is an ambiguous and hard-to-optimize task that forces the convnet to spend a lot of capacity on modeling low-level pixel details. It is all the more unnecessary when the ﬁnal goal deals with high-level image understanding, such as image classiﬁcation. To leverage the advantages of the reconstruction principle with-out focusing on unimportant pixel details, one can focus on reconstruction over high-level visual concepts, referred to as visual words. For instance, BoWNet [25] derived a teacher-student learning scheme following this principle. In this teacher-student setting, given an image, the teacher extracts feature maps that are then quantized in a spatially-dense way over a vocabulary of visual words. Then, the resulting vi-sual words-based image description is exploited for training the student on the self-supervised task of reconstructing the distribution of the visual words of an image, i.e., its bag-of-words (BoW) representation [72], given as input a perturbed version of that same image. By solving this reconstruction task the student is forced to learn perturbation-invariant and context-aware representations while “ignoring” pixel details.
Despite its advantages, the BoWNet approach exhibits some important limitations that do not allow it to fully exploit the potential of the BoW-based reconstruction task. One of them is that it relies on the availability of an already pre-6830
trained teacher network. More importantly, it assumes that this teacher remains static throughout training. However, due to the fact that during the training process the quality of the student’s representations will surpass the teacher’s ones, a static teacher is prone to offer a suboptimal supervisory signal to the student and to lead to an inefﬁcient usage of the computational budget for training.
In this paper, we propose a BoW-based self-supervised approach that overcomes the aforementioned limitations. To that end, our main technical contributions are three-fold: 1. We design a novel fully online teacher-student learning scheme for BoW-based self-supervised training (Fig. 1).
This is achieved by online training of the teacher and the student networks. 2. We signiﬁcantly revisit key elements of the BoW-guided reconstruction task. This includes the proposal of a dynamic BoW prediction module used for recon-structing the BoW representation of the student image.
This module is carefully combined with adequate on-line updates of the visual-words vocabulary used for the BoW targets. 3. We enforce the learning of powerful contextual rea-soning skills in our image representations. Revisiting data augmentation with aggressive cropping and spatial image perturbations, and exploiting multi-scale BoW reconstruction targets, we equip our student network with a powerful feature representation.
Overall, the proposed method leads to a simpler and much more efﬁcient training methodology for the BoW-guided reconstruction task that manages to learn signiﬁcantly better image representations and therefore achieves or even sur-passes the state of the art in several unsupervised learning benchmarks. We call our method OBoW after the online
BoW generation mechanism that it uses. 2.