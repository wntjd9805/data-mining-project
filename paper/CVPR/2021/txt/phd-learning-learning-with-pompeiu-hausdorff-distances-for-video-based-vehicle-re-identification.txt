Abstract
Vehicle re-identiﬁcation (re-ID) is of great signiﬁcance to urban operation, management, security and has gained more attention in recent years. However, two critical chal-lenges in vehicle re-ID have primarily been underestimated, i.e., 1): how to make full use of raw data, and 2): how to learn a robust re-ID model with noisy data. In this paper, we ﬁrst create a video vehicle re-ID evaluation benchmark called VVeRI-901 and verify the performance of video-based re-ID is far better than static image-based one. Then we propose a new Pompeiu-hausdorff distance (PhD) learn-ing method for video-to-video matching.
It can alleviate the data noise problem caused by the occlusion in videos and thus improve re-ID performance signiﬁcantly. Exten-sive empirical results on video-based vehicle and person re-ID datasets, i.e., VVeRI-901, MARS and PRID2011, demon-strate the superiority of the proposed method. The source code of our proposed method is available at https:// github.com/emdata-ailab/PhD-Learning. 1.

Introduction
Vehicle re-identiﬁcation (re-ID) aims to locate and rec-ognize a vehicle of interest across multiple non-overlapping cameras in various trafﬁc intersections. It is of great sig-niﬁcance to urban operation, management, security [31, 72, 38], and has gained more attention in recent years
[33, 65, 50, 57, 46]. The challenges are exponentially increasing for the visual appearance based vehicle re-ID tasks, such as tiny intra-class variations, multiple camera viewpoints, various illumination conditions, severe occlu-sions, and complex trafﬁc conditions [51, 46, 64], e.g., a car might glow different colors at varying viewing angles and light beams, while the vehicles of same model usually exhibit limited visual differences. Vehicle re-ID can be con-ducted on either images or videos. The existing vehicle re-ID has been extensively studied for still images via match-∗Work done while an intern at Shanghai Em-Data Technology Co., Ltd.
†Contact Author (Email: lin.xu5470@gmail.com)
Figure 1. Schematic illustration of the advantage of video-based vehicle re-ID and the proposed Pompeiu-hausdorff distance (PhD)
Learning method. (a): We create a video-based vehicle re-ID benchmark from complex trafﬁc intersections. The rich spatial-temporal information in video can resist visual ambiguities. (b):
Severe partial and full occlusions frequently occur in a trafﬁc in-tersection surveillance video. It would introduce a large number of occlusion samples for recognition. (c): The proposed set-to-set PhD Learning method for video-to-video matching. The noisy sample (e.g., x1) can be eliminated automatically during the op-timization process. Colors (i.e., yellow and red) indicate the se-mantical visual appearance, while shapes (i.e., squares and circles) represent the annotated label (i.e., ground truth). ing spatial appearance features [50, 38, 57, 46, 65]. How-ever, static image-based approaches are intrinsically limited due to the visual ambiguities (e.g., occlusions, viewpoints, and resolutions) and the lack of spatio-temporal informa-tion. Video sequences contain richer spatial and tempo-ral clues and are beneﬁcial for identifying a vehicle under complex surveillance conditions. Currently, making use of videos brings new challenges to vehicle re-ID. The difﬁ-2225
culties mainly come from the following two aspects: 1)
An adequate quantity and high-quality video-based vehi-cle re-ID dataset is absent. To the best of our knowledge, most of the current vehicle re-ID datasets are constructed from sampled static images [31, 33, 50, 38, 57, 46, 65], where the consecutive spatial-temporal information are in-sufﬁcient. Moreover, the diversity (e.g., variations in view-point, occlusion, illumination, and resolution) of cameras’ captured data are also oversimpliﬁed. These restrictions might make limited contributions to construct a reliable and robust appearance-based model. The right subﬁgure in Fig-ure 1(a) presents a toy example to illustrate the advantages of the successive video data for re-ID. Two video tracklets with the same identity (ID) would be matched more accu-rately at frame t3. 2) An effective video-based vehicle re-ID method of seeking discriminative features from the videos is also critically needed. Video-based re-ID beneﬁts from rich spatial-temporal data to resist the aforementioned vi-sual ambiguities. However, it also brings additional difﬁcul-ties in accurately matching video sequences, especially the problem of matching frames from the videos with occlusion
[41, 52]. As illustrated in Figure 1(b), two video tracklets are labeled with IDs X and Y , x0, x1, x2
{ respectively. At Frame t1, the visual appearance of bound-ing box x1 is heavily occluded by that of y1. It leads to x1 has a very similar visual feature with y1, while still been la-beled as ID X. These kinds of occlusion samples frequently occur in a surveillance video captured from crowded scenes (e.g., trafﬁc intersections).
It will cause great difﬁculties for subsequent identiﬁcation and deteriorate the recognition performance signiﬁcantly. y0, y1, y2 and
}
{
}
In this paper, we have done the following two works to overcome the above limitations: 1) We ﬁrstly create a new Video-based Vehicle Re-Identiﬁcation benchmark named VVeRI-901 1. Some distinctive characteristics are summarized as: a) Unconstrained capture conditions in-volving multiple intersections motivate visual information diversity in viewpoint, resolution, and illumination, etc, as shown in Figure 2. b) Successive spatial and tempo-ral information without any further down-sampling is con-tained to enhance the appearance-based model’s robustness in tackling visual ambiguities. c) With the aid of rich in-formation, more related research areas can be facilitated, like cross-resolution re-ID [29], cross-view matching [66], and multi-view synthesis [5]. 2) We then propose a set-to-set Pompeiu-hausdorff Distance (PhD) learning method
It can eliminate the occlu-for video-to-video matching. sion samples automatically during the optimization process.
Figure 1(c) illustrates the PhD learning method’s optimiza-tion process.
In the conventional metric learning method
[23, 18, 1, 43], all the images within a mini-batch will be 1Part of the VVeRI-901 dataset is preliminarily released at https:
//gas.graviti.cn/dataset/hello-dataset/VVeRI901. employed for optimizing the metric space, and the occlusion samples would play an adverse inﬂuence on the optimiza-tion, e.g., the distance of positive pairs x0 and x1 (noise) with considerable visual discrepancy will be narrowed for the large ground distance, and the case is opposite for neg-ative pairs (e.g., x1 and y1).
In contrast, as for the pro-posed PhD metric learning, the aforementioned detrimental positive pairs (e.g., x0 and x1) could be excluded automati-cally due to the selected pairs to be optimized in PhD met-ric space can only be composed of samples from different video tracklets. Additionally, in terms of the negative with the highest similarity (e.g., x1 and y1), it can also be elim-inated in that the PhD measures the maximum mismatch between two point sets via the max-min optimization. In a nutshell, our main contributions are summarised as follows: 1. We create a new VVeRI-901 benchmark for video-based vehicle re-ID. It is the ﬁrst successive video-based vehicle re-ID benchmark captured from unconstrained real-world trafﬁc intersections. 2. We propose a new PhD learning method for video-to-video matching in re-ID tasks. It can alleviate the occlusion problem in video-based re-ID and improve recognition per-formance signiﬁcantly. 3. We verify the superiority of our proposed method on video-based re-ID tasks, including video-based vehicle re-ID and video-based person re-ID. 2.