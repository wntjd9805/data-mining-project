Abstract
Object detection with transformers (DETR) reaches com-petitive performance with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by the great success of pre-training transformers in natural language process-ing, we propose a pretext task named random query patch detection to Unsupervisedly Pre-train DETR (UP-DETR) for object detection. Speciﬁcally, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the original image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classiﬁcation and localization preferences in the pretext task, we freeze the
CNN backbone and propose a patch feature reconstruction branch which is jointly optimized with patch detection. (2) To perform multi-query localization, we introduce
UP-DETR from single-query patch and extend it to multi-query patches with object query shufﬂe and attention mask.
In our experiments, UP-DETR signiﬁcantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr. 1.

Introduction
Object detection with transformers (DETR) [5] is a re-cent framework that views object detection as a direct pre-diction problem via a transformer encoder-decoder [39].
Without hand-designed sample selection [46] and non-maximum suppression, DETR reaches a competitive per-formance with Faster R-CNN [34]. However, DETR comes
*This work is done when Zhigang Dai was an intern at Tencent Wechat
AI.
†Corresponding author.
Figure 1: The VOC learning curves (AP50) of DETR and
UP-DETR with ResNet-50 backbone. Here, they are trained on trainval07+12 and evaluated on test2007. We plot the short and long training schedules, and the learning rate is reduced at 100 and 200 epochs, respectively. with training and optimization challenges, which needs large-scale training data and an extreme long training sched-ule. As shown in Fig. 1 and Section 4.1, we ﬁnd that DETR performs poorly in PASCAL VOC [13], which has insuf-ﬁcient training data and fewer instances than COCO [28].
With well-designed pretext tasks, unsupervised pre-training models achieve remarkable progress in both natu-ral language processing (e.g. GPT [32, 33] and BERT [11]) and computer vision (e.g. MoCo [16, 9] and SwAV [7]). In
DETR, the CNN backbone (ResNet-50 [19] with 23.2M parameters) has been pre-trained to extract a good visual representation, but the transformer module with 18.0M parameters has not been pre-trained. More importantly,
∼
∼ 1601
although unsupervised visual representation learning (e.g. contrastive learning) attracts much attention in recent stud-ies [16, 8, 14, 4, 6, 1], existing pretext tasks can not di-rectly apply to pre-train the transformers of DETR. The main reason is that DETR mainly focuses on spatial local-ization learning instead of image instance-based [16, 8, 14] or cluster-based [4, 6, 1] contrastive learning.
Inspired by the great success of unsupervised pre-training in natural language processing [11], we aim to un-supervisedly pre-train the transformers of DETR on a large-scale dataset (e.g. ImageNet), and treat object detection as the downstream task. The motivation is intuitive, but ex-isting pretext tasks seem to be impractical to pre-train the transformers of DETR. To overcome this problem, we pro-pose Unsupervised Pre-training DETR (UP-DETR) with a novel unsupervised pretext task named random query patch detection to pre-train the detector without any human annotations — we randomly crop multiple query patches from the given image, and pre-train the transformers for de-tection to predict bounding boxes of these query patches in the given image. During the pre-training procedure, we ad-dress two critical issues as follows: (1) Multi-task learning: Object detection is the coupling of object classiﬁcation and localization. To avoid query patch detection destroying the classiﬁcation fea-tures, we introduce frozen pre-training backbone and patch feature reconstruction to preserve the fea-ture discrimination of transformers. (2) Multi-query localization: Different object queries fo-cus on different position areas and box sizes. To illus-trate this property, we propose a simple single-query pre-training and extend it to a multi-query version. For multi-query patches, we design object query shufﬂe and attention mask to solve the assignment problems between query patches and object queries.
In our experiments, UP-DETR performs better than
DETR on PASCAL VOC [13] and COCO [28] object detec-tion with faster convergence and better average precision.
Besides, UP-DETR also transfers well with state-of-the-art performance on one-shot detection and panoptic segmen-tation. In ablations, we ﬁnd that freezing the pre-training
CNN backbone is the most important procedure to preserve the feature discrimination during the pre-training. 2.