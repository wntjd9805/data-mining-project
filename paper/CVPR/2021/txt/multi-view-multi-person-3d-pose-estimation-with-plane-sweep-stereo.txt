Abstract
Existing approaches for multi-view multi-person 3D pose estimation explicitly establish cross-view correspon-dences to group 2D pose detections from multiple camera views and solve for the 3D pose estimation for each per-son. Establishing cross-view correspondences is challeng-ing in multi-person scenes, and incorrect correspondences will lead to sub-optimal performance for the multi-stage pipeline. In this work, we present our multi-view 3D pose estimation approach based on plane sweep stereo to jointly address the cross-view fusion and 3D pose reconstruction in a single shot. Speciﬁcally, we propose to perform depth regression for each joint of each 2D pose in a target cam-era view. Cross-view consistency constraints are implic-itly enforced by multiple reference camera views via the plane sweep algorithm to facilitate accurate depth regres-sion. We adopt a coarse-to-ﬁne scheme to ﬁrst regress the person-level depth followed by a per-person joint-level rel-ative depth estimation. 3D poses are obtained from a sim-ple back-projection given the estimated depths. We evaluate our approach on benchmark datasets where it outperforms previous state-of-the-arts while being remarkably efﬁcient.
Our code is available at the project website. 1 1.

Introduction 3D human pose estimation has been an active research area in the ﬁeld of computer vision due to its large number of real-world applications such as human-computer inter-action, virtual and augmented reality, camera surveillance, etc. However, 3D human pose estimation for multiple per-sons from monocular images is an ill-posed and challenging problem due to both the loss of depth information and se-vere occlusions under a single camera viewpoint. On the other hand, multi-view images captured by multiple cam-eras provide complementary information of the scene that can be used to effectively alleviate projective ambiguities.
Unlike its multi-view single person [15, 16, 20, 21, 26] 1https://github.com/jiahaoLjh/PlaneSweepPose
Reference View 1
Reference View 2
Back Projection
Projection
Figure 1: Our method is based on plane sweep stereo to regress depths for 2D pose detections. 2D poses are back-projected to successive depth planes and warped to refer-ence views for consistency measurement which is utilized for depth regression. counterpart, the fusion of information from multi-view im-ages with multiple persons is more challenging since the identity of the 2D poses from each camera view is unknown.
Previous works such as [5, 7, 11] address this problem in three steps. The 2D poses are ﬁrst estimated for each cam-era view independently. Subsequently, the 2D poses from different views that correspond to the same person are iden-tiﬁed and grouped together. Finally, the 3D pose of each person is estimated with triangulation or optimization-based pictorial structure models using the set of grouped 2D pose detections from multiple views.
The establishment of cross-view correspondences is crit-ical for multi-view multi-person 3D pose estimation. Tradi-tional methods use either greedy matching approach [11] for fast inference speed, or optimization-based approach
[5, 7, 8] for better global consistency. Recently, Voxel-Pose [25] is proposed to jointly solve the challenging cross-view matching and 3D pose estimation problems in an
Instead of explicitly search-object-detection paradigm. ing for 2D pose correspondences, VoxelPose projects the 2D pose heatmaps from multiple views to a common 3D 11886
space, and performs both 3D pose detection and estima-tion in the 3D volumetric space. The 3D object detection formulation avoids the explicit cross-view matching step, thus effectively reduces the impact from incorrectly estab-lished cross-view correspondences. Despite its effective-ness, several limitations exist for the object-detection-based pipeline: 1) Prior knowledge of the common 3D space di-mension according to the multi-camera settings is needed to deﬁne the volumetric space for 3D object detection. 2)
The back-projection of the 2D pose detections on each 3D voxel is not scalable to larger scenes. 3) 3D convolution that is applied to all voxel locations incurs unnecessary heavy computations, especially for large sparse scenes.
In this work, we present our plane-sweep-based ap-proach for multi-view multi-person 3D pose estimation.
Our approach avoids explicit cross-view matching and ag-gregates multiple views for 3D pose estimation in a single shot. Speciﬁcally, we build our framework upon the con-cept of plane sweep stereo [6] to estimate the depth for each joint of each person in a target camera view. As illustrated in Figure 1, 2D poses are ﬁrst back-projected to successive virtual depth planes, and then warped to the respective refer-ence camera views. We measure the cross-view consistency at each depth level, which is then used to regress the depths from standard convolutional neural networks. Our depth re-gression adopts a two-stage coarse-to-ﬁne scheme. Person-level depth is ﬁrst estimated for each 2D pose. Joint-level relative depth with respect to the person-level depth within a much smaller depth range is then regressed for each joint.
The two stages can be trained together in an end-to-end manner. During inference, we obtain the 3D poses by back-projecting the 2D poses with the estimated depths. Multiple 3D poses of the same person from different views can be easily merged via a simple distance-based clustering.
We evaluate our plane-sweep-based framework on three benchmark datasets, i.e., the Campus and the Shelf datasets, and CMU Panoptic dataset, where we outperform exist-ing state-of-the-arts. In addition to the removal of explicit cross-view matching and triangulation compared to the tra-ditional three-step approaches, our method is also more ef-ﬁcient compared to VoxelPose in two aspects: 1) In contrast to VoxelPose that builds voxels in the 3D space, we leverage on the plane sweep algorithm that is proportional to only the number of virtual depth planes. 2) Instead of perform-ing 3D convolution on all voxel locations, we utilize the much faster 1D convolutions for each 2D pose. Further-more, our method is more generablizable to scenarios with no prior knowledge of the multi-camera settings since only the range of virtual depth planes needs to be pre-deﬁned for each camera view.
Our contributions in this work are:
• We present a plane-sweep-based approach to perform multi-view multi-person 3D pose estimation without the need for explicit cross-view matching.
• Our approach outperforms existing state-of-the-arts on benchmark datasets, while being much more efﬁcient compared to existing works. 2.