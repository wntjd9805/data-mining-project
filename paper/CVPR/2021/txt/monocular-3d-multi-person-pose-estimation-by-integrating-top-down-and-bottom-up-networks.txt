Abstract
Frame 140
Frame 150
Frame 180
In monocular video 3D multi-person pose estimation, inter-person occlusion and close interactions can cause hu-man detection to be erroneous and human-joints grouping to be unreliable. Existing top-down methods rely on hu-man detection and thus suffer from these problems. Ex-isting bottom-up methods do not use human detection, but they process all persons at once at the same scale, caus-ing them to be sensitive to multiple-persons scale varia-tions. To address these challenges, we propose the integra-tion of top-down and bottom-up approaches to exploit their strengths. Our top-down network estimates human joints from all persons instead of one in an image patch, making it robust to possible erroneous bounding boxes. Our bottom-up network incorporates human-detection based normal-ized heatmaps, allowing the network to be more robust in handling scale variations. Finally, the estimated 3D poses from the top-down and bottom-up networks are fed into our integration network for ﬁnal 3D poses. Besides the inte-gration of top-down and bottom-up networks, unlike exist-ing pose discriminators that are designed solely for a sin-gle person, and consequently cannot assess natural inter-person interactions, we propose a two-person pose discrim-inator that enforces natural two-person interactions. Lastly, we also apply a semi-supervised method to overcome the 3D ground-truth data scarcity. Quantitative and qualitative evaluations show the effectiveness of the proposed method.
Our code is available publicly. 1 1.

Introduction
Input  frames
Top-down
Bottom-up
Our  results
Figure 1. Incorrect 3D multi-person pose estimation from ex-isting top-down (2nd row) and bottom-up (3rd row) methods.
The top-down method is RootNet [32], the bottom-up method is
SMAP [53]. The input images are from MuPoTS-3D dataset [29].
The top-down method suffers from inter-person occlusion and the bottom-up method is sensitive to scale variations (i.e., the 3D poses of the two persons in the back are inaccurately estimated).
Our method substantially outperforms the state-of-the-art.
Estimating 3D multi-person poses from a monocular video has drawn increasing attention due to its importance for real-world applications (e.g., [32, 28, 2, 7]). Unfortu-nately, it is generally still challenging and an open prob-1https://github.com/3dpose/3D-Multi-Person-Pose lem, particularly when multiple persons are present in the scene. Multiple persons can generate inter-person occlu-sion, which causes human detection to be erroneous. More-over, multiple persons in a scene are likely in close con-tact with each other and interact, which makes human-joints grouping unreliable. 7649
Although existing 3D human pose estimation methods (e.g., [31, 52, 35, 13, 36, 8, 7]) show promising results on single-person datasets like Human3.6M [16] and Hu-manEva [40], these methods do not perform well in 3D multi-person scenarios. Generally, we can divide exist-ing methods into two approaches: top-down and bottom-up. Existing top-down 3D pose estimation methods rely considerably on human detection to localize each person, prior to estimating the joints within the detected bounding boxes, e.g., [36, 8, 32]. These methods show promising performance for single-person 3D-pose estimation [36, 8], yet since they treat each person individually, they have no awareness of non-target persons and the possible interac-tions. When multiple persons occlude each other, human detection also become unreliable. Moreover, when target persons are closely interacting with each other, the pose es-timator may be misled by the nearby persons, e.g., predicted joints may come from the nearby non-target persons.
Recent bottom-up methods (e.g., [53, 24, 22]) do not use any human detection and thus can produce results with higher accuracy when multiple persons interact with each other. These methods consider multiple persons simulta-neously and, in many cases, better distinguish the joints of different persons. Unfortunately, without using detection, bottom-up methods suffer from the scale variations, and the pose estimation accuracy is compromised, rendering infe-rior performance compared with top-down approaches [5].
As shown in Figure 1, neither top-down nor bottom-up ap-proach alone can handle all the challenges at once, partic-ularly the challenges of: inter-person occlusion, close in-teractions, and human-scale variations. Therefore, in this paper, our goal is to integrate the top-down and bottom-up approaches to achieve more accurate and robust 3D multi-person pose estimation from a monocular video.
To achieve this goal, we introduce a top-down network to estimate human joints inside each detected bounding box.
Unlike existing top-down methods that only estimate one human pose given a bounding box, our top-down network predicts 3D poses for all persons inside the bounding box.
The joint heatmaps from our top-down network is feed to our bottom-up network, so that our bottom network can be more robust in handling the scale variations. Finally, we feed the estimated 3D poses from both top-down and bottom-up networks into our integration network to obtain the ﬁnal estimated 3D poses given an image sequence.
Moreover, unlike existing methods’ pose discriminators, which are designed solely for single person, and conse-quently cannot enforce natural inter-person interactions, we propose a two-person pose discriminator that enforces two-person natural interactions. Lastly, semi-supervised learn-ing is used to mitigate the data scarcity problem where 3D ground-truth data is limited.
In summary, our contributions are listed as follows.
• We introduce a novel two-branch framework, where the top-down branch detects multiple persons and the bottom-up branch incorporates the normalized image patches in its process. Our framework gains beneﬁts from the two branches, and at the same time, over-comes their shortcomings.
• We employ multi-person pose estimation for our top-down network, which can effectively handle the inter-person occlusion and interactions caused by detection errors.
• We incorporate human detection information into our bottom-up branch so that it can better handle the scale variation, which addresses the problem in existing bottom-up methods.
• Unlike the existing discriminators that focus on single person pose, we introduce a novel discriminator that enforces the validity of human poses of close pairwise interactions in the camera-centric coordinates. 2.