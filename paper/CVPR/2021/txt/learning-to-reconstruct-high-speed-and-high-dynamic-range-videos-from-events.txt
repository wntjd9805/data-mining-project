Abstract
Event cameras are novel sensors that capture the dynam-ics of a scene asynchronously. Such cameras record event streams with much shorter response latency than images captured by conventional cameras, and are also highly sen-sitive to intensity change, which is brought by the triggering mechanism of events. On the basis of these two features, previous works attempt to reconstruct high speed and high dynamic range (HDR) videos from events. However, these works either suffer from unrealistic artifacts, or cannot pro-vide sufﬁciently high frame rate. In this paper, we present a convolutional recurrent neural network which takes a sequence of neighboring events to reconstruct high speed
HDR videos, and temporal consistency is well considered to facilitate the training process. In addition, we setup a pro-totype optical system to collect a real-world dataset with paired high speed HDR videos and event streams, which will be made publicly accessible for future researches in this ﬁeld. Experimental results on both simulated and real scenes verify that our method can generate high speed HDR videos with high quality, and outperform the state-of-the-art reconstruction methods. 1.

Introduction
Compared with ordinary cameras that capture scene in-tensities at a ﬁxed frame rate, event cameras work in a quite different way of detecting pixel-wise intensity changes.
One unique feature of event camera is that an event is triggered whenever the intensity change of a pixel reaches certain contrast threshold, thus events are recorded asyn-chronously. Event cameras have quite a few advantages over conventional frame-based ones, e.g., low latency, low power, high temporal resolution, and high dynamic range (HDR) [6]. Thanks to these features, event cameras are ben-eﬁcial for different vision tasks including real-time object tracking [24], high speed motion estimation [17], optical
ﬂow estimation [7, 18], depth map prediction [42], egomo-∗Corresponding Author: fuying@bit.edu.cn tion estimation [41], and on-board robotics [37].
Since event cameras record intensity changes without any absolute intensity, they cannot be directly used for ex-isting image-based vision algorithms. Very recently, re-searches have been conducted to reconstruct high speed and HDR intensity images/videos [22, 32, 33] from events, which open up new usage of event cameras. In spite of that, the reconstructed high speed and HDR videos are still un-satisfactory in terms of visual quality. This motivates us to explore a better way of video reconstruction from events.
One possible reason for the insufﬁciency of reconstruc-tion quality might be the shortage of high-quality learning data. For example, existing researches [22, 32, 33] unani-mously try to simulate events by using a simulator, such as
ESIM [21]. However, high-speed HDR videos appropriate for data simulation are extremely rare, and the movement of a virtual event camera might not be realistic. Furthermore, although substantial efforts [28] have been made to improve the simulator, it still remains unknown how the simulated events comply with real events recorded by event cameras, especially when considering that complex factors like noise and data transfer bandwidth limitations are presented in a real event camera. This triggers us to develop proper imag-ing devices to capture paired high speed HDR videos and events. Besides, these works [22, 32, 33] either ignore tem-poral constraints, or use a suboptimal ﬂow warping loss.
These also affect the video reconstruction quality.
In this paper, we make full use of the high frame rate and
HDR features of event streams, to reconstruct high speed
HDR videos from events. Speciﬁcally, we propose a con-volutional recurrent neural network for high speed HDR video reconstruction. In order to minimize the temporal dis-continuity along frame sequences, a temporal consistency constraint is designed based on the physical formation of events. In addition, we collect paired high speed HDR and event data in real scenes through our elaborately designed imaging prototype, which will be accessible publicly to fa-cilitate other researchers in this ﬁeld. Experimental results show that our method can achieve state-of-the-art recon-struction performance, and introducing paired real-world data in the training stage further help the model to handle 2024
real HDR scenes.
The main contributions of our work can be summarized as follows 1. We propose a convolutional recurrent neural network for the reconstruction of high speed HDR videos from events, along with a temporal consistency to constrain the temporal discontinuity. 2. We design a special imaging system to collect the high speed and high bit-depth HDR videos with the corre-sponding event streams, which stands out as a novel alternative for data preparation beyond numerical sim-ulation. 3. We collect a high-quality real dataset which contains paired high speed HDR videos and event streams of outdoor dynamic scenes, and verify the effectiveness of our method on this real-world dataset. 2.