Abstract
Monocular 3D object detection is an important task in autonomous driving.
It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight ﬂuctuation of road smoothness and slope. Due to the lack of insight in industrial appli-cation, existing methods on open datasets neglect the cam-era pose information, which inevitably results in the detec-tor being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we pro-pose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Speciﬁcally, the proposed framework predicts camera extrinsic param-eters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works indepen-dent of the extrinsic parameter variations and produces ac-curate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets. 1.

Introduction 3D object detection plays an important role in a vari-ety of computer vision tasks, such as automated driving vehicles, autonomous drones, robotic manipulation, aug-mented reality applications, etc. Most existing 3D detec-tors require accurate depth-of-ﬁeld information. To acquire such resource, majority of the methods resort to the LiDAR pipeline [10, 33, 37, 38, 23, 60], some to the radars solu-tion [27, 54, 18, 20] or others to the multi-camera frame-work [8, 9, 21, 32, 35, 51]. In this paper, we address this problem in a monocular camera setting and curate it specif-ically for automated driving scenarios With the difﬁculty in directly acquiring a depth of ﬁeld information, monocu-lar 3D detection (Mono3D) is an ill-posed and challenging task. However, Mono3D approaches have the advantage of
*Co-corresponding authors
Figure 1. The effect of extrinsic parameter perturbations on 3D detection task. When the vehicle undergoes a slight pose change on an uneven road, the 3D detection results are less accurate (sec-ond row). This happens often in realistic applications and the de-tection offset can be viewed more evidently in the bird-eye’s view. low cost, low power consumption, and easy-to-deployment in real-world applications. Therefore, monocular 3D de-tection has received increasing attention over the past few years [2, 7, 28, 29, 34, 39].
Current Mono3D methods have achieved considerable high accuracy given a speciﬁcally ﬁxed camera coordinate system. However, in real scenarios, the unevenness (pertur-bation) of the road surface often causes the camera extrinsic parameters to be disturbed, which introduces a signiﬁcant algorithmic challenge. To the best of our knowledge, there are no 3D detection datasets that takes into account the cam-era pose change under perturbation.
As shown in Figure 1, current datasets or detectors as-sume there is no perturbation, i.e., the extrinsic parameters are set to be constant. Therefore the accurate 3D results are obtained (top row). However, as depicted in the bottom per-turbation case, the object information viewed by the camera deviates from the real object information. This makes the detection results unreliable by recovering a large offset in form of both 3D boxes and bird-eye’s view. Straightfor-ward methods to address this problem are to design com-plementary branches or networks to improve the general-ization ability, and yet this solution yields limited improve-ment [5, 50, 34, 45, 12]. Some approaches utilize vehicle
CAD models or keypoints to reconstruct vehicle geometry 7556
[5, 50], while others exploit existing networks to predict pixel-level or instance-level depth map by mimicking state-of-the-art (SOTA) LiDAR 3D detection methods, namely pseudo-LiDAR methods [34, 45, 12].
Our work is inspired by the visual odometry methods that resolve camera pose change in adjacent frames from images [16, 22, 36, 46, 56, 58]. Note that this idea dif-ferentiates from those that focus solely on detecting ob-jects in the perturbation-prone camera coordinate system
[39, 2, 24, 59, 11, 42, 3]. These approaches focus on some less critical issues regarding to realistic industrial applica-tions. For example, the modeling of occlusive objects [11], depth branches [42], kinematic motion information (object orientation) [3], etc. Moreover, it is similar to human behav-ior patterns that one can naturally adapt to changing road gradients and gradually deduce the accurate position of ob-jects even on potholes. Formulating our network to encode such learning patterns is feasible on a biological basis.
In this paper, we propose to leverage the extrinsic param-eter change implicitly in the image. Our key idea is to esti-mate camera pose change w.r.t. the ground plane from im-ages and optimize predicted 3D locations of objects guided by the camera extrinsic geometry constraint. We abbreviate the proposed framework as MonoEF (extrinsic parameter free detector). Speciﬁcally, a novel detector is proposed to extract the vanishing point and horizon information from the image to estimate the camera extrinsic corresponding to the image. The model is thus capable of capturing the ex-trinsic parameter perturbations to which the current image is subjected in the geometric space. During inference, we transform latent feature space using extrinsic parameters as seed to remove the effect of extrinsic perturbations on fea-tures fed from the input image. Note that the transformation network is learned in a supervised manner, which allows the image features to recover from camera perturbation. By do-ing so, we impose our detector exclusive from the effects of the extrinsic parameter. The resultant 3D locations are obtained via the extrinsic parameter-free predictor and pro-jected back into the real-world coordinate system.
Experiments on both the KITTI 3D benchmark [15] and nuScenes dataset [4] demonstrate that our method outper-forms the SOTA methods by a large margin, especially for perturbative examples with a distinguished improvement.
To sum up, the contributions of our paper are as follows:
• We introduce a novel Mono3D detector by capturing the perturbative information of the extrinsic parame-ters from monocular images to make the detector free from extrinsic ﬂuctuation.
• We design a feature transformation network, using camera extrinsic parameters as seed, to recover the non-perturbative image information from the perturba-tive latent feature space.
• We propose an extrinsic module that complements the camera’s pose in 3D object detection. Such a plug-and-play can be applied to existing detectors and pragmatic for industrial applications, e.g., autonomous driving scenarios.
The whole suite of the codebase will be released and the ex-perimental results will be pushed to the public leaderboard. 2.