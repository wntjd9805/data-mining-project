Abstract
One-shot neural architecture search (NAS) methods sig-niﬁcantly reduce the search cost by considering the whole search space as one network, which only needs to be trained once. However, current methods select each operation in-dependently without considering previous layers. Besides, the historical information obtained with huge computation costs is usually used only once and then discarded. In this paper, we introduce a sampling strategy based on Monte
Carlo tree search (MCTS) with the search space modeled as a Monte Carlo tree (MCT), which captures the depen-dency among layers. Furthermore, intermediate results are stored in the MCT for future decisions and a better exploration-exploitation balance. Concretely, MCT is up-dated using the training loss as a reward to the architec-ture performance; for accurately evaluating the numerous nodes, we propose node communication and hierarchical node selection methods in the training and search stages, respectively, making better uses of the operation rewards and hierarchical information. Moreover, for a fair com-parison of different NAS methods, we construct an open-source NAS benchmark of a macro search space evaluated on CIFAR-10, namely NAS-Bench-Macro. Extensive ex-periments on NAS-Bench-Macro and ImageNet demonstrate that our method signiﬁcantly improves search efﬁciency and performance. For example, by only searching 20 architec-tures, our obtained architecture achieves 78.0% top-1 ac-curacy with 442M FLOPs on ImageNet. Code (Benchmark) is available at: https://github.com/xiusu/NAS-Bench-Macro.
*Equal contributions.
†Corresponding authors.
Independent	Sampling
Architecture Sampling 1 1 1 2 2 2 3 3 3 1 3 2
Parent
Evolution
Children 1 3 1 1 3 1 2 1 2 3 2 3 1 3 2 2 3 1
...
...
...
Prioritized	Sampling 1 1 3 1 3 root 2 root 2 3 root 2 3 1 3 3 1 3 3 1 2 3 1
Operation candidates 1 32 1 2
Figure 1. Comparison between existing methods (left) and our method (right). The existing method treats each layer indepen-dently in training (top-left) and search (bottom-left) stages, while our method models the search space with dependencies to a uniﬁed tree structure. 1.

Introduction
Deep learning has not only thrived in various tasks as im-age recognition and object detection [18, 32, 11], but also achieved remarkable performance on mobile edge devices
[23, 12, 36, 28, 5, 10]. Neural architecture search (NAS) makes a step further; it even liberates the reliance on expert knowledge and obtains higher performance by developing more promising and cost-effective architectures [27, 24, 7].
Despite the inspiring success of NAS, the search space of conventional NAS algorithms is extremely large, leading the exhaustive search for the optimal network will be com-putation prohibited. To accommodate the searching budget, heuristic searching methods are usually leveraged and can be mainly categorized into reinforcement learning-based
[25, 26], evolution-based [9, 35], Bayesian optimization-10968
based [37, 29], and gradient-based methods [19, 1, 34, 14].
Among these methods, one-shot NAS methods enjoy signiﬁcant efﬁciency since they only train the whole search space(i.e., supernet) simultaneously. In current mainstream methods of one-shot NAS, a supernet is usually considered as a performance evaluator for all architectures within it, and the optimal architecture is obtained by evaluating the architectures with a validation set. Since it is unrealistic to evaluate all the architectures in the search space, current methods usually deal with a restricted search number, e.g., 1000 vs. 1321. For the sake of efﬁciently searching a good architecture with this limited search number, several heuris-tic search methods have been developed [9, 35], e.g., evolu-tionary algorithms (EA), Bayesian optimization.
Though existing one-shot NAS methods have achieved impressive performance, they often consider each layer sep-arately while ignoring the dependencies between the oper-ation choices on different layers, which leads to an inaccu-rate description and evaluation of the neural architectures during the search. For example, Gaussian Processes (GP) in Bayesian optimization requires that the input attributes (OPs) are independent of each other [37, 29], and the cross mutations of OPs in evolutionary search are often carried out separately in each layer [9, 35].
In fact, for a feed-forward neural network, the choice of a speciﬁc layer relates to its previous layers and contributes to its post layers.
In this paper, we highlight the dependencies between op-erations on different layers through establishing a Monte
Carlo Tree (MCT) in the architecture search space and de-velop an effective sampling strategy based on Monte Carlo
Tree Search (MCTS) for NAS (see Figure 1), The training loss is considered as a reward representation of each node (OP) in MCT, which is used for determining which archi-tecture to be explored. Meanwhile, for a better evaluation of numerous posterior nodes, we propose a node commu-nication technique to share the rewards among nodes with the same operation and depth. The dependencies between different operations on different layers can be accurately modeled with MCT. During searching on the supernet, to evaluate the nodes more accurately, we propose a hierarchi-cal node selection of MCT, which hierarchically updates the rewards on those less-visited nodes using validation data.
For a better comparison between different NAS meth-ods, we propose a NAS benchmark on macro search space named NAS-Bench-Macro, which trains 6561 networks iso-latedly on CIFAR-10 dataset. Experiments on NAS-Bench-Macro show our superiority in efﬁciently searching the op-timal architectures. We also implement our MCTS-NAS on the large-scale benchmark dataset ImageNet [22] with extensive experiments. Under the same FLOPs budgets or acceleration, our method signiﬁcantly improves the search efﬁciency with better performances compared to other one-shot NAS methods [9, 35]. For example, we decrease the search number of sub-networks (subnets) from 1000 to 20, which reduces a large amount of search cost. Besides, the obtained architecture achieves 78.0% Top-1 accuracy with the MobileNetV2 search space with only 442M FLOPs. 2.