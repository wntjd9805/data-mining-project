Abstract
Language-queried video actor segmentation aims to pre-dict the pixel-level mask of the actor which performs the actions described by a natural language query in the tar-get frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolu-tions are amenable to recognizing which actor is perform-ing the queried actions, it also inevitably introduces mis-aligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccu-rate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which con-tains a 3D temporal encoder over the video clip to recog-nize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to ﬂexibly integrate spatial and tem-poral features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial- and temporal-relevant lin-guistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new state-of-the-art performance on two popular benchmarks with less computational overhead than previous approaches. 1.

Introduction
Deep models have achieved notable progress in com-puter vision and other ﬁelds [10, 26, 24, 17]. Language-queried video actor segmentation [12] is an emerging task
∗Equal contribution
†Corresponding author
Figure 1. Illustration of our motivation. (a) The target frame. (b)
The input video clip. (c) The spatial encoder can generate ﬁne segmentation but may misidentify other actors due to weak ac-tion recognition ability. (d) The temporal encoder can recognize which actor is performing the queried action but may introduce misaligned spatial feature into the target frame, yielding inaccurate segmentation. (e) By integrating spatial and temporal encoders, the correct actor in the target frame can be well segmented. whose goal is to predict pixel-level mask for the actor per-forming some actions in a video described by a natural lan-guage query. Different from language-queried video spa-tial or temporal localization [44, 3, 1, 47], this task requires more ﬁne-grained spatial-temporal modeling and visual-linguistic interaction to generate pixel-level prediction, thus is more challenging. At the intersection of computer vision and natural language processing [16, 18, 46, 25, 11, 31], this task enjoys a wide range of applications such as language-driven video editing [21], intelligent surveillance video pro-cessing [34] and human-robot interaction [29]. 4187
As illustrated in Figure 1, given an input query “a white and brown cat is jumping backward” and an input video clip (we show 3 frames for brevity where the target frame is in the middle), language-queried video actor segmenta-tion aims to segment the queried cat on the target frame.
Since the output is based on the context of the whole video clip, we claim that both temporal modeling over the video clip and spatial modeling over the target frame are essen-tial to solve this task. On one hand, as there are two white and brown cats in the target frame, spatial modeling can-not identify the correct cat by exploiting only appearance information. It instead inclines to producing ﬁne but false-positive predictions on other cats. Therefore, the queried action needs to be recognized by incorporating information from adjacent frames to distinguish the jumping cat from the sitting one, leading to the necessity of temporal mod-eling over the video clip. On the other hand, the jumping cat has various poses and locations in 3 frames. Features of these spatially-misaligned pixels from adjacent frames will disturb the feature representation of the target frame during temporal modeling. The correspondence between the fea-ture of the target frame and its ground-truth mask is hence broken. Thus, spatial modeling over the target frame is also necessary to provide precise spatial feature.
However, existing approaches [12, 39, 28, 38] conduct only temporal modeling over the video clip. Concretely, they ﬁrst feed the video clip into a temporal encoder (3D
CNN) to extract cross-frame video features, then apply tem-poral pooling over the time dimension to obtain a mixed feature of the target frame. As discussed above, mixing multi-frame spatial information will result in confused spa-tial feature of the target frame, leading to inaccurate seg-mentation. To tackle this limitation, we propose a collab-orative spatial-temporal framework which contains two en-coders to conduct spatial modeling over the target frame and temporal modeling over the video clip respectively. For the temporal encoder, we adopt a 3D CNN to identify the ac-tor performing the queried action, which can be regarded as the coarse localization of the correct actor by temporal modeling. For the spatial encoder, we adopt a 2D CNN to extract precise spatial feature of the target frame, which serves as the ﬁne segmentation of the correct actor by spatial modeling. To effectively integrate features from the two en-coders, we introduce a Language-Guided Feature Selection (LGFS) module in the decoder to combine the two features with ﬂexible channel selection weights, which are generated from the linguistic feature. Thus, language query serves as a selector to form comprehensive spatial-temporal feature for accurate segmentation.
In addition, language query contains both spatial-relevant information (appearance words, e.g., “white and brown”) and temporal-relevant information (action words, e.g., “jumping backward”). When interacting with vi-sual feature from the spatial encoder, features of spatial-relevant words should play a more important role than temporal-relevant words and vice versa. Therefore, we also propose a Cross-Modal Adaptive Modulation (CMAM) module which dynamically recombines linguistic features by cross-modal attention, yielding spatial- or temporal-relevant linguistic features to adaptively modulate corre-sponding visual features. By densely inserting our CMAM module into each stage of the two encoders, visual features can interact with linguistic features hierarchically and dy-namically to highlight regions of the correct actor in spatial and temporal aspects.
The main contributions of our paper are summarized as follows: 1) We propose a collaborative spatial-temporal framework which contains a temporal encoder to recognize the queried action and a spatial encoder to generate accu-rate segmentation of the actor. A Language-Guided Fea-ture Selection (LGFS) module is proposed in the decoder to aggregate spatial and temporal features comprehensively. 2) We also propose a Cross-Modal Adaptive Modulation (CMAM) module to conduct spatial- and temporal- relevant multimodal interaction dynamically in each stage of the two encoders. 3) Extensive experiments on two popular bench-marks show our method outperforms previous state-of-the-arts by large margins with 3× less computational overhead. 2.