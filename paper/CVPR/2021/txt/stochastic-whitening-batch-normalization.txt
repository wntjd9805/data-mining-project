Abstract
Batch Normalization (BN) is a popular technique for training Deep Neural Networks (DNNs). BN uses scaling and shifting to normalize activations of mini-batches to ac-celerate convergence and improve generalization. The re-cently proposed Iterative Normalization (IterNorm) method improves these properties by whitening the activations iter-atively using Newton’s method. However, since Newton’s method initializes the whitening matrix independently at each training step, no information is shared between con-secutive steps. In this work, instead of exact computation of whitening matrix at each time step, we estimate it gradually during training in an online fashion, using our proposed
Stochastic Whitening Batch Normalization (SWBN) algo-rithm. We show that while SWBN improves the convergence rate and generalization of DNNs, its computational over-head is less than that of IterNorm. Due to the high efﬁciency of the proposed method, it can be easily employed in most
DNN architectures with a large number of layers. We provide comprehensive experiments and comparisons between BN,
IterNorm, and SWBN layers to demonstrate the effectiveness of the proposed technique in conventional (many-shot) image classiﬁcation and few-shot classiﬁcation tasks. 1.

Introduction
Gradient descent-based methods are the de-facto training algorithms for DNN, and mini-batch Stochastic Gradient
Decent (SGD) has become the most popular ﬁrst-order op-timization algorithm. In mini-batch SGD, instead of com-puting the gradients for the entire training set as in batch gradient descent, or based on one training sample as in con-ventional SGD, the gradients are computed based on a small random subset of the training set called mini-batch. The stochastic nature of mini-batch SGD helps a DNN ﬁnd bet-ter local optima or even the global optima than batch gradient descent. We use SGD to refer to mini-batch SGD in the rest of the paper.
* Equal contribution.
Figure 1: The SWBN diagram shows how whitening parameters and task parameters inside an SWBN layer are updated in a decoupled way. Whitening parameters (in blue rectangles) are updated only in the forward phase, and are ﬁxed in the backward phase. Task parameters (in red rectangles) are ﬁxed in the forward phase, and are updated only in the backward phase.
Due to the change in the distribution of the inputs of DNN layers at each training step, the network experiences Internal
Covariate Shift (ICS) as deﬁned in the seminal work of [17].
ICS affects the input statistics of the subsequent layers, and as a result, it degrades the training efﬁciency.
Eliminating the effect of ICS can accelerate the training of DNN by helping the gradient ﬂow through the network, stabilizing the distributions of the activations, and enabling the use of a larger learning rate. To alleviate these effects,
BN has been proposed in [17].
Recent studies have shown that whitening (decorrelat-ing) the activations can further reduce the training time and improve the generalization [8, 24, 15]. However, when dealing with high-dimensional data, the requirement of eigen-decomposition [8, 24], Singular Value Decomposi-tion (SVD), or Newton’s iteration [15] for computing whiten-ing matrices has been the bottleneck of these methods.
In this paper, we propose a method called SWBN, which gradually learns whitening matrices separately in each layer during training. The proposed method eliminates the need for expensive matrix decomposition and inversion for whiten-ing data as SWBN learns the whitening matrix in an online fashion. Our formulation clearly shows the computational advantage of SWBN to other whitening techniques applied in DNNs, such as [8], [15] and [14].
In Section 2, we review the related works on normal-10978
ization and whitening techniques for DNN. In Section 3, we derive and discuss the SWBN algorithm. In Section 4, we present extensive experimental results to show the ef-fectiveness of the proposed method for different network architectures and datasets. In summary, the key advantages of the proposed method are as follows:
• SWBN is designed to be a drop-in replacement for BN.
It decouples the learning process of whitening matrices from the back-propagation algorithm. Thus there is no need to modify the network architecture, task loss, or the optimization algorithm to enable whitening.
• SWBN takes advantages of both BN and whitening to improve convergence rate and generalization per-formance of DNN models with small computational overhead.
• There are a few whitening approaches that aim to serve as drop-in replacements for BN layers. However, they can only replace a small number of BN layers in a DNN with their whitening layers due to the computational burden. In contrast, SWBN learns whitening matrices without expensive matrix decomposition or matrix in-version operations, enabling SWBN layers to replace a large number of BN layers. 2.