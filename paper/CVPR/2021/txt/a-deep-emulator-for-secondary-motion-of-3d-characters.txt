Abstract
Fast and light-weight methods for animating 3D charac-ters are desirable in various applications such as computer games. We present a learning-based approach to enhance skinning-based animations of 3D characters with vivid sec-ondary motion effects. We design a neural network that encodes each local patch of a character simulation mesh where the edges implicitly encode the internal forces be-tween the neighboring vertices. The network emulates the ordinary differential equations of the character dynamics, predicting new vertex positions from the current accelera-tions, velocities and positions. Being a local method, our network is independent of the mesh topology and general-izes to arbitrarily shaped 3D character meshes at test time.
We further represent per-vertex constraints and material properties such as stiffness, enabling us to easily adjust the dynamics in different parts of the mesh. We evaluate our method on various character meshes and complex motion sequences. Our method can be over 30 times more efﬁcient than ground-truth physically based simulation, and outper-forms alternative solutions that provide fast approximations. 1.

Introduction
Fast and light-weight methods for animating 3D charac-ters are desirable in various applications including computer games and ﬁlm visual effects. Traditional skinning-based mesh deformation provides a fast geometric approach but often lacks realistic dynamics. On the other hand, physically-based simulation can add plausible secondary motion to skinned animations, augmenting them with visually realistic and vivid effects, but at the cost of heavy computation.
Recent research has explored deep learning methods to approximate physically-based simulation in a much more time-efﬁcient manner. While some approaches have focused on accelerating speciﬁc parts of the simulation [18, 7, 20], others have proposed end-to-end solutions that predict dy-namics directly from mesh based features [1, 11, 11, 25].
While demonstrating impressive results, these methods still have some limitations. Most of them assume a ﬁxed mesh topology and thus need to train different networks for dif-ferent character meshes. Moreover, in order to avoid the 5932
computational complexity of training networks on high reso-lution meshes, some methods operate on reduced subspaces with limited degrees of freedom, leading to low accuracy.
In this paper, we propose a deep learning approach to predict secondary motion, i.e., the deformable dynamics of given skinned animations of 3D characters. Our method addresses the shortcomings of the recent learning-based ap-proaches by designing a network architecture that can reﬂect the actual underlying physical process. Speciﬁcally, our network models the simulation using a volumetric mesh con-sisting of uniform tetrahedra surrounding the character mesh, where the mesh edges encode the internal forces that depend on the current state (i.e., displacements, velocities, acceler-ations), material properties (e.g., stiffness), and constraints on the vertices. Mesh vertices encode the inertia. Motivated by the observation that within a short time instance the sec-ondary dynamics of a vertex is mostly affected by its current state, as well as the internal forces due to its neighbors, our network operates on local patches of the volumetric mesh.
In addition to avoiding the computational complexity of en-coding high resolution character meshes as large graphs, this also enables our method to be applied to any character mesh, independent of its topology. Finally, our network encodes per-vertex material properties and constraints, giving the user the ability to easily prescribe varying properties to different parts of the mesh to control the dynamic behaviour.
As a unique beneﬁt of the generalization capability of our model, we demonstrate that it is not necessary to construct a massive training dataset of complex meshes and motions.
Instead, we construct our training data from primitive geome-tries, such as a volumetric mesh of a sphere. Our network trained on this dataset can generate detailed and visually plausible secondary motions on much more complex 3D characters during testing. By assigning randomized motions to the primitives during training, we are able to let the local patches cover a broad motion space, which improves the network’s online predictions in unseen scenarios.
We evaluate our method on various character meshes and complex motion sequences. We demonstrate visually plausi-ble and stable secondary motion while being over 30 times faster than the implicit Euler method commonly used in physically-based simulation. We also provide comparisons to faster methods such as the explicit central differences method and other learning-based approaches that utilize graph convolutional networks. Our method outperforms those approaches both in terms of accuracy and robustness. 2.