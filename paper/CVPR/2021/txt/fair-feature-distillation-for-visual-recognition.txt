Abstract
Fairness is becoming an increasingly crucial issue for computer vision, especially in the human-related decision systems. However, achieving algorithmic fairness, which makes a model produce indiscriminative outcomes against protected groups, is still an unresolved problem.
In this paper, we devise a systematic approach which reduces al-gorithmic biases via feature distillation for visual recogni-tion tasks, dubbed as MMD-based Fair Distillation (MFD).
While the distillation technique has been widely used in general to improve the prediction accuracy, to the best of our knowledge, there has been no explicit work that also tries to improve fairness via distillation. Furthermore, We give a theoretical justiﬁcation of our MFD on the effect of knowledge distillation and fairness. Throughout the exten-sive experiments, we show our MFD signiﬁcantly mitigates the bias against speciﬁc minorities without any loss of the accuracy on both synthetic and real-world face datasets. 1.

Introduction
Based on the remarkable performance of deep neural net-works, computer vision has become one of the core tech-nologies in many applications that affect various aspects of society; e.g., facial recognition [24], AI-assisted hiring [25], healthcare diagnostics [13], and law enforcement [11]. Due to these social applications of computer vision algorithms, it is becoming increasingly essential for them to be fair; namely, the outcomes of systems should not be discrimi-native against any certain groups on the basis of sensitive attributes. For example, any automated system that incor-porates photographs into a decision process (e.g., job inter-view) should not rely on certain sensitive attributes, such as race or gender [29]. However, recent studies demonstrate that commercial API systems for facial analysis expose the gender/race bias in widely used face datasets [6, 34].
In this work, we are interested in the setting in which an
∗The ﬁrst three authors have contributed equally.
†Corresponding author (E-mail: tsmoon@snu.ac.kr)
Common
Facial 
Features
Skin Color-correlated
Features
Teacher
⋯
Fair Student
Transfer
⋯
Unattractive
Attractive
Attractive
Attractive
Figure 1. An illustrative example of motivation to our work. The
“teacher” model may depend heavily on the skin color when de-ciding whether the face is attractive, while it may also have learned useful common (unbiased) facial features. To train a fair “student” model via feature distillation, only the unbiased common features from the teacher should be transferred to the student so that both high accuracy and fairness can be achieved. already deployed model has been identiﬁed as unfair. The usual approach of the so-called in-processing methods to mitigate the unfair bias is to re-train the model from scratch with an additional fairness constraint [1, 18, 37]. However, such approaches typically do not utilize any predictive in-formation already learned out by the deployed model, and hence, would lead to sacriﬁcing the accuracy for the im-proved fairness. To address above limitation, the knowledge distillation (KD) [16] technique can be considered as a po-tential tool for leveraging the deployed model’s predictive power while re-training with fairness constraints. Nonethe-less, the typical existing KD methods [16, 30, 38, 31, 17] focused only on improving the accuracy, and considering both the accuracy and fairness during the process of KD is not straightforward. We aim to resolve this challenge by proposing a new fairness-aware feature distillation scheme.
Figure 1 illustrates the key idea of our work. We assume that even when the original deployed model, the “teacher” model, may be heavily biased (i.e., heavily use the sensi-12115
tive “skin color” attribute), it could also have learned useful group-indistinguishable common (unbiased) features that are effective for achieving high prediction accuracy (e.g.,
“face shape”, etc.). Our intuition is that when training a
“student” model, if only those common unbiased features can be transferred from the teacher, the student should be able to achieve higher accuracy, compared to the ordinary in-processing methods that re-train from scratch, as well as better fairness, compared to the original teacher.
In order to realize above intuition, we propose a fair fea-ture distillation technique by utilizing the maximum mean discrepancy (MMD), dubbed as MMD-based Fair Distilla-tion (MFD); this is, to the best of our knowledge, the ﬁrst approach to improve both accuracy and fairness via distil-lation. More concretely, we devise a regularization term for training a student that enforces the distribution of the group-conditioned features of the student to get closer to the distribution of the group-averaged features of the teacher in the MMD sense. We further provide a theoretical un-derstanding that our MFD regularization can indeed lead to improving both the accuracy and fairness of the student in a principled way. Namely, we show our regularization term induces the distributions of the group-conditioned fea-tures of the student to get close to each other across all the sensitive groups (i.e., promotes fairness), while making all those distributions also get close to the distribution of the group-averaged features of the highly accurate teacher (i.e., improves accuracy via the distillation effect).
As a result, we convincingly show through extensive ex-periments that our MFD can simultaneously improve the ac-curacy as well as considerably mitigate the unfair bias of a model. Firstly, we construct a synthetic dataset, CIFAR-10S [35], and systematically validate our motivation illus-trated in Figure 1. Then, with additional experiments on two real-world datasets, UTKFace [42] and CelebA [22], we identify that our MFD is the only method that can con-sistently improve both accuracy and fairness of the original unfair teacher on all three datasets, compared to the three types of baselines: ordinary KD methods, representative in-processing methods that re-train from scratch, and methods that naively combine the in-processing methods with KD methods. Finally, we demonstrate the validity of our theo-retical bound via systematic ablation studies. 2.