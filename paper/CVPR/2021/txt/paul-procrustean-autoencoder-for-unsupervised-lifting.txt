Abstract
Recent success in casting Non-rigid Structure from Mo-tion (NRSfM) as an unsupervised deep learning problem has raised fundamental questions about what novelty in
In this pa-NRSfM prior could the deep learning offer. per we advocate for a 3D deep auto-encoder framework to be used explicitly as the NRSfM prior. The framework is unique as: (i) it learns the 3D auto-encoder weights solely from 2D projected measurements, and (ii) it is Procrustean in that it jointly resolves the unknown rigid pose for each shape instance. We refer to this architecture as a Pro-custean Autoencoder for Unsupervised Lifting (PAUL), and demonstrate state-of-the-art performance across a number of benchmarks in comparison to recent innovations such as
Deep NRSfM [21] and C3PDO [32]. 1.

Introduction
Inferring non-rigid 3D structure from multiple unsyn-chronized 2D imaged observations is an ill-posed problem.
Non-Rigid Structure from Motion (NRSf M) methods ap-proach the problem by introducing additional priors – of particular note in this regards are low rank [10, 6, 3] and union of subspaces [25, 44] methods.
Recently, NRSfM has seen improvement in performance by recasting the problem as an unsupervised deep learning problem [32, 8, 33]. These 2D-3D lifting networks have inherent advantages over classical NRSf M as: (i) they are more easily scalable to larger datasets, and (ii) they al-low fast feed-forward prediction once trained. These im-provement, however, can largely be attributed to the end-to-end reframing of the learning problem rather than any fundamental shift in the prior/constraints being enforced within the NRSfM solution. For example, both Cha et al. [8] and Park et al. [33] impose a classical low rank con-straint on the recovered 3D shape.
It is also well under-stood [10, 21, 44, 25] that such low rank priors have poor performance when applied to more complex 3D shape vari-ations.
The NRSfM ﬁeld has started to explore new non-rigid shape priors inspired by recent advances in deep learning.
Kong & Lucey [21] proposed the use of hierarchical spar-sity to have a more expressive shape model while ensuring the inversion problem remains well conditioned. Although achieving signiﬁcant progress in several benchmarks, the approach is limited by the somewhat adhoc approximations it employs so as to make the entire NRSfM solution real-izable as a feed-forward lifting network. Such approxima-tions hamper the interpretability of the method as the ﬁnal network is a substantial departure from the actually pro-posed objective. We further argue that this departure from the true objective also comes at the cost of the overall effec-tiveness of the 2D-3D lifting solution.
In this paper we propose a prior that 3D shapes aligned to a common reference frame are compressible with an un-dercomplete auto-encoder. This is advantageous over pre-vious linear methods, because the deeper auto-encoder is naturally capable of compressing more complicated non-rigid 3D shapes. What makes learning such an auto-encoder challenging is: (i) it observes only 2D projected measure-ments of the non-rigid shape; (ii) it must automatically resolve the unknown rigid transformation to align each projected shape instance. We refer to our solution as a
Procustean Autoencoder for Unsupervised Lifting (PAUL).
PAUL is considered unsupervised as it has to handle un-known depth, shape pose, and occlusions. Unlike Deep
NRSfM [21], the optimization process of PAUL does not have to be realizable as a feed-forward network – allow-ing for a solution that stays tightly coupled to the proposed mathematical objective.
We also explore other alternative deep shape priors such as: decoder only and decoder + low-rank. A somewhat sim-ilar approach is recently explored by Sidhu et al. [34] for dense NRSf M. Our empirical results demonstrate the fun-damental importance of the auto-encoder architecture for 2D-3D lifting.
Contributions: We make the following contributions:
• We present an optimization objective for joint learning the 2D-3D lifting network and the Procrustean auto-encoder solely from 2D projected measurements. 1434
Non-sequential 2D keypoints learn 3D shape at canonical frame
M 6
. 3
H r i a h c l e n a p o r e a
Figure 1: PAUL learns to reconstruct 3D shapes aligned to a canonical frame, using 2D keypoint annotations only. Bot-tom rows show 3D shapes interpolated from the learned la-tent space of the Procrustean auto-encoder.
• A naive implementation of PAUL through gradient de-scent would result in poor local minima, so instead we advocate for a bilevel optimization, whose lower level problem can be efﬁciently solved by orthographic-N-point (OnP) algorithms.
• Our method achieves state-of-the-art performance across multiple benchmarks, and is empirically shown to be robust against the choice of hyper-parameters such as the dimension of the latent code space. 2.