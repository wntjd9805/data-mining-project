Abstract
Text recognition is a popular research subject with many associated challenges. Despite the considerable progress made in recent years, the text recognition task itself is still constrained to solve the problem of reading cropped line text images and serves as a subtask of optical character recognition (OCR) systems. As a result, the ﬁnal text recog-nition result is limited by the performance of the text de-tector. In this paper, we propose a simple, elegant and ef-fective paradigm called Implicit Feature Alignment (IFA), which can be easily integrated into current text recogniz-ers, resulting in a novel inference mechanism called IFA-inference. This enables an ordinary text recognizer to pro-cess multi-line text such that text detection can be com-pletely freed. Speciﬁcally, we integrate IFA into the two most prevailing text recognition streams (attention-based and CTC-based) and propose attention-guided dense pre-diction (ADP) and Extended CTC (ExCTC). Furthermore, the Wasserstein-based Hollow Aggregation Cross-Entropy (WH-ACE) is proposed to suppress negative predictions to assist in training ADP and ExCTC. We experimentally demonstrate that IFA achieves state-of-the-art performance on end-to-end document recognition tasks while maintain-ing the fastest speed, and ADP and ExCTC complement each other on the perspective of different application sce-narios. Code will be available at https://github.com/Wang-Tianwei/Implicit-feature-alignment. 1.

Introduction
Text is extensively used in people’s daily life, delivering rich and useful visual information. Reading text in images is one of the most important tasks in the ﬁeld of computer
∗Equal contribution. This work is done during Tianwei Wang and
Yuanzhi Zhu’s internship at Alibaba Group. Lianwen Jin is the correspond-ing author.
Segmentation-based  text recognition
CTC-based or 
Attention-based text  recognition
Training
Inference
Label: x1, y1, h1, w1, (cid:1111); x2, y2, h2, w2, (cid:2670); …
Character-level annotation
Line-level recognition
Label: (cid:1111)(cid:2670)(cid:28573)(cid:2502)(cid:2632)(cid:2545)(cid:1111)(cid:2670)
Line-level annotation
Line-level recognition
Detection-Recognition text spotter
Label:  x1, y1, h1, w1, (cid:1111)(cid:2670)(cid:28573)(cid:2502)(cid:2632)(cid:2545)(cid:1111)(cid:2670); …
Line-level annotation with  bounding-box of each line  
Page-level recognition
Implicit Feature 
Alignment
Label: (cid:1111)(cid:2670)(cid:28573)(cid:2502)(cid:2632)(cid:2545)(cid:1111)(cid:2670)
Line-level annotation
Page-level recognition
Line-level recognition
Figure 1. The development of text recognizers. vision.
Most OCR systems follow the pipeline that ﬁrst uses a text detector to detect the location of each text line and then recognizes the detected line texts with a text recognizer. Un-der this pipeline, the performance of the entire system is determined by the cascading of each module, and the per-formance degradation of each module leads to the deterio-ration of the overall performance. Although many end-to-end (E2E) OCR methods [4, 7, 15, 22, 26, 29, 31, 38] have been proposed in recent years, they have still used such a pipeline, and considerable efforts have been made to better develop the bridge between text detectors and text recog-nizers. Thus, the error accumulation problem has not been solved. To remedy this issue, a direct way is shortening this 5973
pipeline by extending text recognizer from text-line to full-page level. In this paper, we propose a simple, yet effective, paradigm, called Implicit Feature Alignment (IFA), to re-alize detection-free full-page text recognition with state-of-the-art performance and signiﬁcantly faster inference speed as compared to previous models.
Alignment is the core issue in the design of a text rec-ognizer. This means the way to teach the recognizer the location of each character, as well as the category to which it belongs. As shown in Fig. 1, the development of text recognition methods has shown a trend of more general recognition with fewer annotations. In the early research, integrated segmentation-recognition methods [39,43,55,56] construct the segmentation-recognition lattice based on se-quential character segments of line text images, followed by optimal path searching by integrating the recognition scores, geometry information, and semantic context. These methods require text annotations with a bounding box for each character to train a line-level recognizer. With the rapid development of deep learning technology, Connec-tionist Temporal Classiﬁcation (CTC) [8, 9] and the atten-tion mechanism [2, 14, 34, 35] can realize training of text recognizers with text annotations. More speciﬁcally, CTC assumes that the character order in the label and that in the image are monotonous, and thereby designs a series of many-to-one rules to align the dense frame-wise output with the label. The attention mechanism uses a parameterized at-tention decoder to align and transcribe each character on the image. However, both CTC and the attention mechanism have their own limitations, which makes them impossible to conduct full-page recognition. CTC can only align two 1-D sequences, ignoring the height information. Thus, it cannot process multi-line images. Attention relies on a pa-rameterized attention module, whose performance decays as the sequence length increases [6, 41], not to mention the full-page level.
The proposed IFA aims to overcome the aforementioned limitations and convert a text recognizer trained on line text images to recognize multi-line texts. IFA leverages the learnable aligning ability of the deep learning model to au-tomatically align and train positive pixels (i.e., pixels on a feature map that contains character information), thereby realizing conversion from a text recognizer to a text spotter.
We integrate IFA with CTC and the attention mechanism, producing two new methods called attention-guided dense prediction (ADP) and extended CTC (ExCTC). For ADP, we theoretically analyze the optimization objective of the attention mechanism at each step and ﬁnd it equivalent to ensuring the aligned pixels being correctly recognized. For
ExCTC, we add a pluggable squeeze module that learns to align the positive pixels on each column to conduct feature collapse. In addition to aligning positive pixels by ADP and
ExCTC, we modify Aggregation Cross-Entropy (ACE) [44] and propose Wasserstein-based Hollow ACE (WH-ACE) to suppress negative noise. Both ADP and ExCTC have a uni-ﬁed inference form called IFA-inference, which can pro-cess single-line, multi-line, or even full-page text recogni-tion tasks. 2.