Abstract
As a vital problem in classiﬁcation-oriented trans-fer, unsupervised domain adaptation (UDA) has attracted widespread attention in recent years. Previous UDA meth-ods assume the marginal distributions of different domains are shifted while ignoring the discriminant information in the label distributions. This leads to classiﬁcation perfor-mance degeneration in real applications. In this work, we focus on the conditional distribution shift problem which is of great concern to current conditional invariant model-s. We aim to seek a kernel covariance embedding for con-ditional distribution which remains yet unexplored. Theo-retically, we propose the Conditional Kernel Bures (CKB) metric for characterizing conditional distribution discrep-ancy, and derive an empirical estimation for the CKB metric without introducing the implicit kernel feature map. It pro-vides an interpretable approach to understand the knowl-edge transfer mechanism. The established consistency the-ory of the empirical estimation provides a theoretical guar-antee for convergence. A conditional distribution matching network is proposed to learn the conditional invariant and discriminative features for UDA. Extensive experiments and analysis show the superiority of our proposed model. 1.

Introduction
Large-scale data with sufﬁcient annotations are vital sources of machine learning. However, the data collect-ed from the real-world scenarios are usually unlabeled and the manual annotations are expensive. Recent advances in transfer learning yields plenty of methods for dealing with the shortage of labeled data. These methods aim to transfer the knowledge on a labeled source domain to a target do-main with few or no annotations, such setting is also known as domain adaptation [27].
The most common assumption in Unsupervised Domain
Adaptation (UDA) is that the labeled source domain and
Figure 1. Illustration of the conditional shift problem. Previous metrics that only consider the marginal distribution discrepancy may lead to a misaligned conditional distribution, i.e., the red cir-cle region. On the bottom, the class-level alignment is achieved by exploiting the conditional distribution embedding metric.
X
X s = t, P s
X 6 unlabeled target domain have the same feature spaces, but different marginal distributions [27], i.e.,
=
P t
X . This assumption is also called covariate shift [29] and sample selection bias [35]. Ben-David et al. [2] give a the-oretical insight into the domain adaptation problem, they show that the risk of the target domain is mainly bounded by the risk of the source domain and the discrepancy between distributions of two domains. Inspired by this theory, many methods are proposed to mitigate the discrepancy between feature distributions of the source and target domains, e.g., explicit discrepancy minimization via Maximum Mean Dis-crepancy (MMD) [13, 21], domain invariant feature learn-ing [26], Optimal Transport (OT) based feature matching
[7, 20, 37], manifold based feature alignment [10], statisti-cal moment matching [21, 32] and adversarial domain adap-tation [9]. These methods are proved to be effective in mini-mizing the marginal discrepancy and alleviating the domain shift problem. However, this assumption may lead to the omission of discriminant information in the label distribu-tions, which is described in Figure 1. Recent advancements
[19, 22, 24] show that the adaptation models will be more discriminative on the target domain if the target label infor-mation (e.g., pseudo labels) is explored carefully.
∗Corresponding Author.
Extended from the marginal shift assumption, the con-13989
|
Y = P t
X ditional shift problem is studied to build a conditional in-variant model [36], i.e., P s
Y . The most critical
X
| problem is to construct a framework which can explicitly reﬂect the relation between different conditional distribu-tions. Zhao et al. [38] prove a new generalization bound which quantitatively reﬂects the underlying structure of the conditional shift problem. Several works have also been made in the ﬁeld of conditional/joint distribution matching for domain adaptation, e.g., multi-layer feature approxima-tion [23], conditional variants of MMD [16, 19, 39], condi-tional invariant learning with causal interpretations [11, 28],
OT based joint distribution models [4, 6].
In this paper, we aim to estimate the transport cost in Re-producing Kernel Hilbert Space (RKHS) for the continuous conditional distributions. Inspired by pioneering work [8], which employs the conditional covariance operator on the
RKHS to characterize the independence, we deﬁne trans-port cost estimation on the set of conditional covariance op-erators called Conditional Kernel Bures (CKB) metric. By virtue of the conditional covariance operator and OT theory, we prove that the CKB metric reﬂects the discrepancy be-tween two conditional distributions directly. This result can be taken as an extension of the marginal distribution embed-ding property in MMD [13] and kernel Bures metric [37].
An explicit empirical estimation of the CKB metric and its consistency theory are presented. Further, we apply it to the proposed conditional distribution matching network. Exten-sive experiment results show the effectiveness of the CKB metric and the superiority of the proposed model. Our con-tributions are summarized as follows.
•
•
•
A novel CKB metric for characterizing conditional dis-tribution discrepancy is proposed, and the kernel em-bedding property of the CKB metric is proved to show that it is well-deﬁned on conditional distributions. This metric is also exactly the OT between conditional dis-tributions, which provides an interpretable approach to understand the knowledge transfer mechanism.
An explicit empirical estimation of the CKB metric is derived, which provides a computable measuremen-t for conditional domain discrepancy. The asymptotic property of the estimation is proved which provides a rigorous theoretical guarantee for convergence.
A conditional distribution matching network based on the CKB metric is proposed for discriminative domain alignment, and a joint distribution matching variant is further extended. The SOTA results in extensive ex-periments validate the model’s effectiveness. 2.