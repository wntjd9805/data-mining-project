Abstract
Unsupervised domain adaptation (UDA) methods for learning domain invariant representations have achieved remarkable progress. However, most of the studies were based on direct adaptation from the source domain to the target domain and have suffered from large domain dis-crepancies.
In this paper, we propose a UDA method that effectively handles such large domain discrepancies.
We introduce a ﬁxed ratio-based mixup to augment mul-tiple intermediate domains between the source and tar-get domain. From the augmented-domains, we train the source-dominant model and the target-dominant model that have complementary characteristics. Using our conﬁdence-based learning methodologies, e.g., bidirectional matching with high-conﬁdence predictions and self-penalization us-ing low-conﬁdence predictions, the models can learn from each other or from its own results. Through our proposed methods, the models gradually transfer domain knowledge from the source to the target domain. Extensive experi-ments demonstrate the superiority of our proposed method on three public benchmarks: Ofﬁce-31, Ofﬁce-Home, and
VisDA-2017. 1 1.

Introduction
Recently, we have seen considerable improvements in several computer vision applications using deep learning; however, this success has been limited to supervised learn-ing methods with abundant labeled data. Collecting and la-beling data from various domains is an expensive and time-consuming task. To address this problem, semi-supervised learning [45, 3, 34] and unsupervised learning [9] have been studied; however, in most cases, it was assumed that learn-ing of the model occurred in a similar domain.
UDA refers to a set of transfer learning methods for transferring knowledge learned from the source domain to the target domain under the assumption of domain discrep-ancy. Moreover, it is useful when the source domain con-1Our code is available at https://github.com/NaJaeMin92/FixBi.
Figure 1. Comparison of previous domain adaptation meth-ods and our proposed method. Top: Previous methods try to adapt directly without any consideration of large domain discrepancies. Bottom: Our proposed method utilize aug-mented domains between the source and target domain for efﬁcient domain adaptation. tains enough labeled data to learn, but not much labeled data are present in the target domain. Domain adaptation (DA) generally assumes that the two domains have the same con-ditional distribution, but different marginal distributions.
Under these assumptions, effective knowledge transfer is difﬁcult when the two domains have large marginal distri-bution gaps. This becomes much more challenging in a sce-nario where the target domain has no labeled data at all.
In previous UDA methods, a domain discriminator [8, 37] was introduced to encourage domain confusion through domain-adversarial objectives and minimize the gap be-tween the source and target distributions.
In [21, 24], domain discrepancy based approaches used metrics such as maximum mean discrepancy (MMD) and joint MMD (JMMD) to reduce the difference between two feature 1094
spaces. Moreover, inspired by the generative adversarial network (GAN), GAN-based DA methods [15, 7] have at-tempted to generate transferable representations to mini-mize domain discrepancy. Most of these studies have di-rectly adapted the knowledge learned from the source do-main to the target domain. However, fundamentally, this does not take into account the case where the distance be-tween the source and target domain is large, as shown in
Figure 1.
In this paper, our goal is to compensate efﬁciently for the large domain discrepancies. To address this challenge, we construct multiple intermediate augmented domains, whose characteristics are different and complementary to each other. To achieve this, we propose a ﬁxed ratio-based mixup. Our proposed mixup approach minimizes the do-main randomness of [41, 43] between the source and tar-get samples and generates multiple intermediate domains, as shown in Figure 1. For example, an augmented domain close to the source domain has more reliable label infor-mation, but it has a lower correlation with the target do-main. By contrast, label information in an augmented do-main close to the target domain is relatively inaccurate, but the similarity to the target domain is much higher.
In these augmented domains, we train the complemen-tary models that teach each other to bridge between the source and target domain. Speciﬁcally, we introduce a bidi-rectional matching based on the high-conﬁdence predic-tions of each model for the target samples, moving the inter-mediate domains to the target domain. We also apply self-penalization, which penalizes its own model to improve per-formance through self-training. Moreover, to properly im-pose the characteristics of models that change with each it-eration, we use an adaptive threshold by the conﬁdence dis-tribution of each mini-batch, not a predeﬁned one [12, 44].
Finally, to prevent divergence of the augmented models generated in different domains, we propose a consistency regularization using an augmented domain with the same ratio of source and target samples.
We conduct extensive ablation studies for a detailed analysis of our proposed method and achieve compara-ble performance to state-of-the-art methods in standard DA benchmarks such as Ofﬁce-31 [29], Ofﬁce-Home [40], and
VisDA-2017 [27]. The main contributions of this paper are summarized as follows.
• We propose a ﬁxed ratio-based mixup to efﬁciently bridges the source and target domains utilizing the in-termediate domains.
• We propose conﬁdence-based learning methodologies: a bidirectional matching and a self-penalization using positive and negative pseudo-labels, respectively.
• We empirically validate the superiority of our method to UDA with extensive ablation studies and evaluations on three standard benchmarks. 2.