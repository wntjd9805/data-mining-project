Abstract
We propose a new type of full-body human avatars, which combines parametric mesh-based body model with a neural texture. We show that with the help of neural textures, such avatars can successfully model clothing and hair, which usually poses a problem for mesh-based ap-proaches. We also show how these avatars can be cre-ated from multiple frames of a video using backpropaga-tion. We then propose a generative model for such avatars that can be trained from datasets of images and videos of people. The generative model allows us to sample random avatars as well as to create dressed avatars of people from one or few images. The code for the project is available at saic-violet.github.io/style-people. 1.

Introduction
Creating realistically-looking and articulated human avatars is a challenging task with many applications in telepresence, gaming, augmented and virtual reality. In re-cent years, sophisticated and powerful models of “naked” people that model shape of the body including facial and hand deformations have been developed [33, 50]. These models are based on mesh geometry and are learned from several existing datasets of body scans. Clothing and hair literally add an extra layer of complexity on top of body modeling, they are even more challenging for appearance modeling, and accurate 3D data for these elements are scarce and hard to obtain. And yet creating realistic avatars is not possible without modeling of these elements.
Here, we propose a new approach that we call neural dressing that allows to create 3D realistic full body avatars from videos and in a few-shot mode (from one or sev-eral images). Similarly to previous works, this approach uses deformable meshes (speciﬁcally, SMPL-X model [33]) to model and animate body geometry in 3D. On top of the body mesh, the approach superimposes a multi-channel neural texture [44] that is processed by a rendering network
∗equal contribution in order to generate images of a full-body avatar with cloth-ing and hair. Our ﬁrst contribution is thus to show that the combination of deformable mesh models and neural tex-tures (neural dressing) can model appearance of full-body avatars with loose clothing and hair well and to account for the geometry missing in parametric body models.
As our second contribution, we build a generative model of full-body avatars. The key component of the new model is a generative network for neural body texture. The gener-ative network is derived from StyleGANv2 [21] generator.
To build the complete model, we thus incorporate neural texture synthesis, mesh rendering, and neural rendering into the joint generation process, which is trained in an adversar-ial fashion on a large-scale dataset of full-body images. We also address the need to ensure that avatars have consistent appearance across variety of poses and camera positions.
This is ensured by adding an additional discriminator net-work and by modifying the training process accordingly.
Using the resulting generative model, we can sample new realistic 3D “artiﬁcial humans” (StylePeople – Fig-ure 1). Furthermore, the availability of a generative model allows us to build avatars for existing people by ﬁtting the model to a single image or few images of a given person.
As the generative model is heavily over-parameterized, we investigate how the ﬁtting process can be regularized. In the experiments, we then compare our model to previous approaches to few-shot avatar modeling. 2.