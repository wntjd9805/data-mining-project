Abstract
Multi-Source Domain Adaptation (MSDA), which ded-icates to transfer the knowledge learned from multiple source domains to an unlabeled target domain, has drawn increasing attention in the research community. By assum-ing that the source and target domains share consistent key feature representations and identical label space, existing studies on MSDA typically utilize the entire union set of fea-tures from both the source and target domains to obtain the feature map and align the map for each category and do-main. However, the default setting of MSDA may neglect the issue of “partialness”, i.e., 1) a part of the features con-tained in the union set of multiple source domains may not present in the target domain; 2) the label space of the tar-get domain may not completely overlap with the multiple source domains. In this paper, we unify the above two cases to a more generalized MSDA task as Multi-Source Partial
Domain Adaptation (MSPDA). We propose a novel model termed Partial Feature Selection and Alignment (PFSA) to jointly cope with both MSDA and MSPDA tasks. Speciﬁ-cally, we ﬁrstly employ a feature selection vector based on the correlation among the features of multiple sources and target domains. We then design three effective feature align-ment losses to jointly align the selected features by preserv-ing the domain information of the data sample clusters in the same category and the discrimination between differ-ent classes. Extensive experiments on various benchmark datasets for both MSDA and MSPDA tasks demonstrate that our proposed PFSA approach remarkably outperforms the state-of-the-art MSDA and unimodal PDA methods. 1.

Introduction
Domain adaptation methods focus on reducing the do-main shift [21, 30] between a single labeled source domain and an unlabeled target domain. Recently, a more practi-cal task termed Multi-Source Domain Adaptation (MSDA), which dedicates to transfer the knowledge learned from
*Corresponding author. multiple source domains to an unlabeled target domain, has drawn much attention in the research community.
With MSDA datasets [20, 22], a variety of approaches have been proposed aiming at different application scenar-ios, e.g., text classiﬁcation [8], semantic segmentation [33], person re-identiﬁcation [7], and visual sentiment classiﬁca-tion [13]. Recently, different MSDA strategies, such as ad-versarial learning [24, 32], and source distilling [34], have been proposed to improve the performance on target domain using labeled source domains. However, most of the exist-ing MSDA methods conduct feature alignment on the entire common features, ignoring the fact that some of the source features may not present in the target domain, which may contribute to negative transfer especially when the target domain only shares a part of features with distinct source domains. More practically, the label space of the target do-main is unknown, i.e., the label shift between the source and the target domain is ubiquitous.
Although several partial domain adaptation (PDA) re-searches [2, 3, 4, 31] have reported promising results, very few previous works have paid attention to the situation where multiple source domains are introduced while the target domain does not share the identical label space as the source domains. Thus, we raise a more challenging but practical research topic named Multi-Source Partial Do-main Adaptation (MSPDA): Leveraging multiple source do-mains with distinct label spaces, and perform tasks on the target domain that does not share an identical label space with any speciﬁc source domain. As illustrated in Figure 1, we make a comparison between the problem settings of
MSDA and MSPDA tasks. In MSDA tasks, all source do-mains and the target domain share an identical label space, so all circles completely overlap each other. As for MSPDA tasks, any pair of domains do not perfectly share the label space, so some domain-speciﬁc labels can be witnessed. In other words, the MSDA task can be considered as a special case of the MSPDA task.
In this paper, we propose a novel end-to-end train-able model termed Partial Feature Selection and Alignment (PFSA) to jointly tackle both MSDA and MSPDA prob-lems. Based on the assumption that all of the target’s in-16654
practical and valuable than single-source UDA tasks. With theoretical analysis done by [1, 9, 18], multiple trending strategies have been designed for MSDA tasks, such as ad-versarial and GAN-based approaches [13, 24, 32, 33]. La-tent space learning and domain generation are also applied
[17]. Other techniques such as source distilling [34] are pro-posed to select related source samples as training data. In
[8], different distance-based metrics are compared and data samples are chosen dynamically during training according to the correlation between source and target domain. [20] aligns features using high-order moment distance. Class confusion is utilized as a novel metric in [11]. In our work, we dig further into the process of feature alignment and de-rive partial features that are related to the target domain.
Partial Domain Adaptation. Ordinary domain adaptation stands on the assumption that source and target domains share an identical label space, while practically, the label space of the target domain is unknown, but the abundant source samples can cover the entire label space of the target domain. In recent years, partial domain adaptation (PDA) approaches have been proposed [2, 3, 4, 31], concentrating on tackling situations where the target domain only contains a part of the source labels. Previous works of PDA have been focusing on different aspects, e.g., [2] applies a selec-tive weighting mechanism to multiple adversarial networks and [3] uses one adversarial network and class-level weight to judge source samples. In [31], an auxiliary domain clas-siﬁer is utilized to derive the possibility that a source sample is contained in the target label space. A domain discrimina-tor is also introduced to qualify the sample transferability and to re-weight source examples [4].
In this paper, we introduce multiple source domains to
PDA, aiming at opening a novel topic named Multi-Source
Partial Domain Adaptation (MSPDA) to the research com-munity. Note that our MSPDA setting differs from the one proposed in [11], i.e., we don’t assume that the source do-mains share an identical label space. 3. Proposed Method 3.1. Problem Deﬁnition
Suppose that there are N source domains DS1 , DS2 ,
DS3 , . . . , DSN with label spaces YS1 , YS2 , . . . , YSN and one target domain DT without labels. fD represents the fea-ture map of domain D (extracted by common feature extrac-tors such as ResNet) and FD stands for the reﬁned features through our proposed model (note that F is derived through source-target pairs). We use YT to represent the target label space. According to the illustration in Figure 1, the problem settings of the MSDA and MSPDA tasks are as follows: (1) MSDA. All domains share an identical label space:
YS1 = YS2 = · · · = YSN = YT . (2) MSPDA. Any pair of source domains or source-target pair does not share the la-bel space, but the union set of source domains contains the 16655
Figure 1: Illustration of the different settings for the MSDA and MSPDA tasks. formative features are contained in the union set of multiple source domains, we introduce a selecting vector in PFSA to derive the reﬁned feature map that can reduce the dis-crepancy between the source and the target features. The selected partial features are highly associated with the tar-get domain and further improve the adaptation performance with three effective feature alignment losses, which are re-spectively derived from the class-level, domain-level, and discrimination aspects. We conduct extensive experiments on various benchmark datasets to exhibit its superior ca-pability of adapting multiple domains for both MSDA and
MSPDA tasks comparing to the state-of-the-art MSDA and
PDA approaches.
We summarize our contributions in this paper as follows: (1) We propose a general framework termed Partial Feature
Selection and Alignment (PFSA) that is capable of tackling both MSDA and MSPDA problems. (2) We utilize the sim-ilarity between the source and the target domain to derive feature selection vectors, aiming at preserving the features that are highly related to the target domain. (3) Three novel feature alignment losses are proposed to further align the selected features, aiming to improve the model’s capability of generating discriminative feature representations. 2.