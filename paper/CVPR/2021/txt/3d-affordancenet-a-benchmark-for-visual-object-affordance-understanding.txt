Abstract
The ability to understand the ways to interact with ob-jects from visual cues, a.k.a. visual affordance, is essential to vision-guided robotic research. This involves categoriz-ing, segmenting and reasoning of visual affordance. Rele-vant studies in 2D and 2.5D image domains have been made previously, however, a truly functional understanding of ob-ject affordance requires learning and prediction in the 3D physical domain, which is still absent in the community. In this work, we present a 3D AffordanceNet dataset, a bench-mark of 23k shapes from 23 semantic object categories, an-notated with 18 visual affordance categories. Based on this dataset, we provide three benchmarking tasks for evaluat-ing visual affordance understanding, including full-shape, partial-view and rotation-invariant affordance estimations.
Three state-of-the-art point cloud deep learning networks are evaluated on all tasks. In addition we also investigate a semi-supervised learning setup to explore the possibility to beneﬁt from unlabeled data. Comprehensive results on our contributed dataset show the promise of visual affordance understanding as a valuable yet challenging benchmark. 1.

Introduction
The concept of affordance was ﬁrst deﬁned as what the environment offers the animal, introduced by [6]. Affor-dance understanding is concerned with the interactions be-tween human and environment. For instance, human can sit on the chair, grasp a cup or lift a bag. Being able to under-stand the affordance of objects is crucial for robots to oper-ate in dynamic and complex environments [8]. Many appli-cations are supported by affordance understanding includ-ing, anticipating and predicting future actions[12, 10, 13], recognizing agent’s activities[21, 4, 26], providing valid functionality of the objects[7], understanding social scene situations[2] and understanding the hidden values of the
* indicates equal contribution.
†Correspondence to Kui Jia <kuijia@scut.edu.cn>.
Pour
Wrap-Grasp
Contain
Grasp
Cut
Stab
Grasp
Contain
Lift
Figure 1. The 3D AffordanceNet dataset. The mesh was ﬁrst anno-tated with affordance keypoints. Then we densely sample points and obtain the ground truth data via label propagation. objects[33]. Tasks including affordance categorization, rea-soning, semantic labeling, activity recognition, etc. are de-ﬁned as speciﬁc instantiations of affordance understanding
[8]. Among all these we ﬁnd semantic labeling [22, 33] is of the most importance because the ability to localize the position of possible affordance is highly desired by robotic research. We refer semantic labeling as affordance estima-tion throughout this paper.
The most important and proper modality for affordance understanding is through visual sensors [8]. Visual affor-dance understanding has been extensively studied recently with computer vision techniques. Many algorithms are built upon deep neural networks [19, 3, 23] thus require large labeled affordance dataset for benchmarking. Rele-vant datasets are developed for these purposes with data col-lected from 2D (RGB) sensors [32, 24, 22] or 2.5D (RGBD) sensors [18, 19, 23]. Nevertheless, we believe that the af-fordance understanding requires learning in the 3D domain which conveys the geometric properties. For example, the affordance of grasp is highly correlated with vertical struc-ture with small perimeter and sittable is correlated with ﬂat surface. Unfortunately, such detailed geometry is not cap-tured by the existing 2D datasets while the 2.5 ones [19, 23] are often captured with small depth variation and do not carry enough geometric information. 1778
To encourage research into visual affordance under-standing in more realistic scenarios, a benchmark on real 3D dataset is highly desired. Therefore, we are inspired by PartNet[17], a recently proposed dataset containing the
ﬁne-grained part hierarchy information of 3D shapes based on the large-scale 3D CAD model dataset ShapeNet[1] and 3D Warehouse. Although PartNet mentioned affordance as potential application, there is still no benchmark purposely established for affordance yet. More importantly, we dis-cover, via user annotations, that the human perceived affor-dance often do not fully overlap with the individual parts speciﬁed in PartNet dataset. For example, In the ﬁrst row of
Fig. 1, the Pour, Wrap-Grasp and Contain affordance from
Mug do not perfectly match any part indicated by the col-ored image on the 1st column. Therefore, we believe it is necessary to provide a new set of affordance labels on the
PartNet dataset.
Creating 3D visual affordance benchmark is challenging due to the subjective deﬁnition. We take into account the affordance deﬁnitions from existing research on visual af-fordance learning in 2D and 2.5D domains [8] and select possible interactions that one can take with 3D shapes from
PartNet. Finally, 18 types of affordance were formally de-ﬁned over 23 semantic objects. Additional challenge asso-ciated with annotation on 3D model is the scalability issue.
In order to provide highly quality annotation on such a large scale, we use label propagation method to propagate affor-dance sparsely labeled on individual points. Eventually, we obtain point-wise probabilistic score of affordance for each individual shape in PartNet. We name the new benchmark 3D AffordanceNet to reﬂect the focus on visual affordance on 3D point cloud data. 3D AffordanceNet enables benchmarking a diverse set of tasks, in particular, we put forward full-shape, partial-view and rotation-invariant affordance estimations. Three state-of-the-art point cloud deep learning networks are evaluated on all tasks. We also propose a semi-supervised affordance estimation method to take the advantage of large amount of unlabeled data for affordance estimation.
In summary, we make the following contributions:
• We introduce 3D AffordanceNet, consisting of 56307 well-deﬁned affordance information annotations for 22949 shapes covering 18 affordance classes and 23 semantic object categories. To the best of our knowl-edge, this is the ﬁrst large-scale dataset with well-deﬁned probabilistic affordance score annotations;
• We propose three affordance learning tasks which are supported by 3D AfffodanceNet to demonstrate the value of annotated data: full-shape affordance estima-tion, partial-view affordance estimation and rotation-invariant affordance estimation. https://3dwarehouse.sketchup.com
• We benchmark three baseline methods for proposed affordance learning tasks and further propose a semi-supervised affordance estimation method to take ad-vantage of unlabeled data for affordance estimation. 2.