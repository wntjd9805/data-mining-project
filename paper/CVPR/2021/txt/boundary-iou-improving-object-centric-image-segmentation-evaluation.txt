Abstract
We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on bound-ary quality. We perform an extensive analysis across dif-ferent error types and object sizes and show that Boundary
IoU is signiﬁcantly more sensitive than the standard Mask
IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new qual-ity measure displays several desirable characteristics like symmetry w.r.t. prediction/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Bound-ary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary qual-ity improvements that are generally overlooked by current
Mask IoU-based evaluation metrics. We hope that the adop-tion of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that im-prove boundary quality.1 1.

Introduction
The Common Task Framework [23], in which standard-ized tasks, datasets, and evaluation metrics are used to track research progress, yields impressive results. For exam-ple, researchers working on the instance segmentation task, which requires an algorithm to delineate objects with pixel-level binary masks, have improved the standard Average
Precision (AP) metric on COCO [24] by an astonishing 86% (relative) from 2015 [9] to 2019 [20].
However, this progress is not equal across all error modes, because different evaluation metrics are sensitive (or insensitive) to different types of errors. If a metric is used for a prolonged time, as in the Common Task Framework,
∗Work done during an internship at Facebook AI Research. 1Project page: https://bowenc0221.github.io/boundary-iou
Ground Truth (LVIS [13])
Mask R-CNN [14]
BMask R-CNN [6]
PointRend [18]
Mask IoU
Boundary IoU
Mask R-CNN 89% 69%
BMask R-CNN 92% (+3%) 78% (+9%)
PointRend 97% (+8%) 91% (+22%)
Figure 1: Given the bounding box for a horse, the mask predicted by Mask R-CNN scores a high Mask IoU value (89%) relative to the ground truth despite having low-ﬁdelity, blobby boundaries.
The recently proposed BMask R-CNN [6] and PointRend [18] methods predict masks with higher ﬁdelity boundaries, yet these clear visual improvements only marginally improve Mask IoU (+3% and +8%, respectively). In contrast, our proposed Bound-ary IoU measure demonstrates greater sensitivity to boundary er-rors, and thus provides a clear, quantitative gradient that rewards improvements to boundary segmentation quality. then the corresponding sub-ﬁeld most rapidly resolves the types of errors to which this metric is sensitive. Research directions that improve other error types typically advance more slowly, as such progress is harder to quantify.
This phenomenon is at play in instance segmentation, where, among the multitude of papers contributing to the impressive 86% relative improvement in AP (e.g., [36, 4, 1, 16, 21]), only a few address mask boundary quality.
Note that mask boundary quality is an essential aspect of image segmentation, as various downstream applications directly beneﬁt from more precise object segmentations [34, 29, 30]. However, the dominant family of Mask R-CNN-based methods [14] are well-known to predict low-ﬁdelity, blobby masks (see Figure 1). This observation suggests that the current evaluation metrics may have limited sensitivity to mask prediction errors near object boundaries.
To understand why, we start by analyzing Mask
Intersection-over-Union (Mask IoU), the underlying mea-15334
sure used in AP to compare predicted and ground truth masks. Mask IoU divides the intersection area of two masks by the area of their union. This measure values all pixels equally and, therefore, is less sensitive to boundary qual-ity in larger objects: the number of interior pixels grows quadratically in object size and can far exceed the number of boundary pixels, which only grows linearly. In this paper we aim to identify a measure for image segmentation that is sensitive to boundary quality across all scales.
Towards this goal we start by studying standard segmen-tation measures like Mask IoU and boundary-focused mea-sures such as Trimap IoU [19, 5] and F-measure [26, 8, 28].
We study error-sensitivity characteristics of each measure by generating a variety of error types on top of the high-quality ground truth masks from the LVIS dataset [13]. Our analysis conﬁrms that Mask IoU is less sensitive to errors in larger objects. In addition, the analysis reveals limitations of existing boundary-focused measures, such as asymme-tries and instability to small changes in mask quality.
Based on these insights we propose a new Boundary
IoU measure. Boundary IoU is simple and easy to com-Instead of considering all pixels, it calculates the pute. intersection-over-union for mask pixels within a certain dis-tance from the corresponding ground truth or prediction boundary contours. Our analysis demonstrates that Bound-ary IoU measures boundary quality of large objects well, unlike Mask IoU, and it does not over-penalize errors on small objects. An illustrative examples compares Boundary
IoU to Mask IoU in Figure 1.
Boundary IoU enables new task-level evaluation met-rics. For the task of instance segmentation [24], we pro-pose Boundary Average Precision (Boundary AP), and for panoptic segmentation [17], we propose Boundary Panop-tic Quality (Boundary PQ).
Boundary AP assesses all relevant aspects of instance segmentation, simultaneously taking into account catego-rization, localization, and segmentation quality, unlike prior boundary-focused metrics for instance segmentation like
AF [22] that ignore false positive rates. We test Bound-ary AP on three common datasets: COCO [24], LVIS [13], and Cityscapes [7]. With real predictions from recent in-stance segmentation methods that directly aim to improve boundary quality [18, 6], we verify that Boundary AP tracks improvements better than Mask AP. With synthetic predic-tions, we show that Boundary AP is signiﬁcantly more sen-sitive to large-object boundary quality than Mask AP.
For panoptic segmentation, we apply Boundary PQ to the COCO [17] and Cityscapes [7] panoptic datasets. We test the new metric with synthetic predictions and show that it is more sensitive than the previous metric based on Mask
IoU. Finally, we evaluate the performance of various recent instance and panoptic segmentation models with the new evaluation metrics to ease comparison for future research.
These new metrics reveal improvements in boundary quality that are generally ignored by Mask IoU-based eval-uation metrics. We hope that the adoption of these new boundary-sensitive evaluations can enable faster progress towards segmentation models with better boundary quality. 2.