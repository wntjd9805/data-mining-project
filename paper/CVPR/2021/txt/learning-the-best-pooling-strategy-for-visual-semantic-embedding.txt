Abstract
Visual Semantic Embedding (VSE) is a dominant ap-proach for vision-language retrieval, which aims at learning a deep embedding space such that visual data are embedded close to their semantic text labels or descriptions. Recent
VSE models use complex methods to better contextualize and aggregate multi-modal features into holistic embeddings.
However, we discover that surprisingly simple (but care-fully selected) global pooling functions (e.g., max pooling) outperform those complex models, across different feature extractors. Despite its simplicity and effectiveness, seeking the best pooling function for different data modality and feature extractor is costly and tedious, especially when the size of features varies (e.g., text, video). Therefore, we pro-pose a Generalized Pooling Operator (GPO), which learns to automatically adapt itself to the best pooling strategy for different features, requiring no manual tuning while staying effective and efﬁcient. We extend the VSE model using this proposed GPO and denote it as VSE∞.
Without bells and whistles, VSE∞ outperforms previous
VSE methods signiﬁcantly on image-text retrieval bench-marks across popular feature extractors. With a simple adap-tation, variants of VSE∞ further demonstrate its strength by achieving the new state of the art on two video-text retrieval datasets. Comprehensive experiments and visualizations conﬁrm that GPO always discovers the best pooling strategy and can be a plug-and-play feature aggregation module for standard VSE models. Code and pre-trained models are available at http://jcchen.me/vse_infty/ 1.

Introduction
Recognizing and describing the visual world with natural language is an essential capability for artiﬁcial intelligence.
It motivates the research of image-text matching, which challenges a learning agent to establish accurate and general-izable alignment between visual and textual data, so that one can identify images or videos by text queries or vice versa.
Visual semantic embedding (VSE) [9, 10, 22] tackles this
∗Authors contributed equally challenge by learning a semantic embedding space, where the distance between paired visual and textual instances in the embedding space is optimized to be small. The core idea of the VSE has three steps:
Step 1. Extract a set (or sequence) of features from data, using feature extractors (e.g., ConvNets for visual data).
Step 2. Contextualize and aggregate the extracted features to project them into the joint embedding space as holistic vectors, using feature aggregators.
Step 3. Compute the matching score between embeddings with a similarity metric (e.g., cosine distance).
With the feature extractor determined, one might expect that a complex aggregator is required to achieve good results.
However, we show (in § 3) that a surprisingly simple and efﬁcient aggregator, a carefully selected pooling function (e.g., max pooling), can surpass prior state-of-the-art VSE methods with complex aggregators [17, 27, 43, 45, 46].
Such pooling functions are both simple and effective.
However, searching for the optimal pooling requires exten-sive manual tuning and repetitive experiments (e.g., grid search) for each data modality and features, which is tedious and costly as it enumerates over a combinatorial number of conﬁgurations. This search procedure could be even more complicated when the sets of features have varying sizes.
Can we discover the best pooling strategy automatically?
In this paper, we propose a novel parameterized pooling op-erator, Generalized Pooling Operator (GPO), to fully exploit the strengths of pooling-based feature aggregation. GPO generalizes over various pooling functions and learns to ad-just itself to the best one for different data modalities and feature extractors. Speciﬁcally, GPO learns a generator that predicts the pooling coefﬁcients to weight the elements of sorted feature vectors, and use their weighted sum as the pooling output. The coefﬁcient generator is instantiated as a tiny sequence model to handle variable-sized features. GPO learns to adapt to the optimal pooling strategy, and improve
VSE models at a negligible extra computational cost.
With the proposed GPO, we build our multi-modal match-ing system as VSE∞, which extends a standard VSE framework[22] by using GPO as the feature aggregators for both visual and text features. We train our system optimizing a margin-based triplet ranking objective similar to [9], with 15789
Inputs
Feature Extractor
Feature Vectors
Visual Feature Set
⋯ or
!
ConvNet
Girl blowing out  the candle on an  ice-cream.
" sos
A
B C
D eos
SeqModel
Contextualized Word Sequence
⋯
⋯
Girl
⋯
⋯ candle an ice-cream
Aggregator
Embedding Space
Similarity Score (
&
'
GPO
)*+,-./
GPO
)0120
Figure 1. Illustration of the standard Visual Semantic Embedding framework with the proposed pooling-based aggregator, i.e., Generalized
Pooling Operator (GPO). It is simple and effective, which automatically adapts to the appropriate pooling strategy given different data modality and feature extractor, and improves VSE models at negligible extra computation cost. the online hard-negative mining. visual features and text features, respectively:
Without bells and whistles, VSE∞ surpasses all pre-vious state-of-the-art VSE-based methods on the image-text retrieval tasks, over COCO [21] and Flickr30K [49].
With a straightforward extension, variants of VSE∞ also achieve the best video-text retrieval results on two bench-mark datasets, i.e., MSR-VTT [48] and VaTeX [44].
In additional experiments, we show that GPO consistently out-performs other alternative learnable poolings from the litera-ture. To better understanding GPO, we further visualize the pooling strategy found by VSE∞, and compare it with the one from a thorough grid search process.
Our contributions are summarized as the following:
• We empirically ﬁnd that carefully selecting simple pooling functions can outperform complex visual aggregators in prior VSE methods for image-text matching.
• We propose a novel Generalized Pooling Operator (GPO) that generalizes various pooling functions. It learns to automatically discover the best pooling function for image, text, and video data with various feature extractors.
• We build up VSE∞ with GPO, which achieves the new state-of-the-art performances among VSE methods on image-text and video-text retrieval.
• We visualize the pooling strategies learned by GPO, and verify that GPO learns the best pooling strategies given the data by comparing VSE∞ with a thorough grid search over pooling functions of all modalities. 2. Visual Semantic Embedding for
Multi-modal Matching
We begin by revisiting the formal formulation of Visual
Semantic Embedding (VSE). A VSE model (illustrated in
Figure 1) leverages a visual embedding function Φ(x) such as convolutional neural networks (e.g. CNNs [12, 47]), and a text embedding function Ψ(t) such as sequence models (e.g. LSTMs [14], Transformers [42]), to compute the set of
N
ConvNet(x) : x → {φn} n=1,
M
SeqModel(t) : t → {ψm} m=1
Here the set of visual features {φn}N n=1 has N elements of convolutional local representations with φn ∈ Rd1 . As aforementioned, the concrete form of φn can be feature vectors of spatial grids from the feature map, object propos-als [1], or spatial-pyramids [13], depending on the feature ex-tractor. Similarly, text features {ψt}M m=1 denotes a sequence of M contextualized word token features out of a sequence model where M is the number of words and ψm ∈ Rd2 .
Here d1 and d2 are the feature dimensions.
The output visual features {φn}N n=1 and textual features
{ψt}M m=1 are then aggregated by visual and textual aggre-gators fVISUAL(·) and fTEXT(·), to further encode the holistic visual and text embedding v, u ∈ Rd3 as follows: v = fVISUAL
N
{φn} n=1
, u = fTEXT
{ψm}
M m=1
.
The compatibility score is then deﬁned as the cosine similar-ity between v and u, formally as: (cid:16) (cid:17) (cid:16) (cid:17) s(x,t) = v⊤u kvk · kuk
During the inference, the s(x,t) scores are used to rank a query text against all candidate images, and the top candidate are returned as the prediction. We note that the inference procedure is efﬁcient as the visual and text embedding v and u can be pre-computed. The pair-wise scores are then computed by matrix multiplication.
Learning Multi-modal Matching To learn a VSE model, existing methods mostly optimize the hinge-based triplet ranking loss with online hard negative mining proposed by
VSE++ [9]. The concrete matching objective is deﬁned by:
ℓMATCH =
[α − s(x,t) + s(x,ˆt)]+
X(x,t)∼D
+ [α − s(x,t) + s( ˆx,t)]+ (1) 15790
Image-text retrieval results in R@1 of VSE models
Table 1. with different visual aggregator, evaluated with MS-COCO 1K.
See § 5.1 for details.
Region [1]
Grid [18]
Aggregator
#Param T → I
I → T T → I
I → T
AvgPool [9]
Seg2Seq [15]
SelfAttn [43, 45]
GCN+AvgPool [27]
GCN+Seg2Seq [27] 54.0 0 58.5 6.3M 56.2 3.2M 4.2M 54.9 23.1M 60.7
Best Pooling Function 0 60.7 68.5 69.9 70.2 69.0 72.5 74.5 58.9 61.5 60.3 59.5 59.5 61.6 72.4 73.3 73.0 71.8 71.1 76.3 where α is a hyper-parameter. (x, t) is a positive image-text pair in the dataset D and [x]+ ≡ max(0, x). We represent
ˆt = argmaxt′6=ts(x,t′) and ˆx = argmaxx′6=xs(x′,t) as the hardest negative text and image examples measured by the learned VSE model within a mini-batch. 3. VSE∞ with Generalized Pooling Operator
In this section, we ﬁrst present an empirical ﬁnding that highlights the effectiveness of well-selected pooling function in VSE model, which motivates our methodological pursuit (§ 3.1). We then propose our method, Generalized Pooling
Operator (GPO), with a introduction of its formal deﬁnition (§ 3.2), followed by the details of GPO’s concrete model architecture (§ 3.3). Finally, we summarize our multi-modal system (VSE∞) that leverages GPO (§ 3.4). 3.1. Simple Pooling Works the Best
As aforementioned in § 1, complex aggregators f have been investigated in the VSE literature [17, 27, 43, 45, 46], such as sequence-to-sequence encoder (Seq2Seq), graph con-volution network (GCN), self-attention encoder (SelfAttn), etc. However, we surprisingly ﬁnd that these aggregation models with millions of parameters underperform carefully selected pooling functions.
Table 1 highlights a comparison between different aggre-gators, across two widely used image feature extractors in the literature [18] – Grid feature is the feature maps from
ConvNets and Region feature is the ROI features from object detectors [1] (details in § 5). The results are reported in re-call@1 for text-based image retrieval (T→I) and vice versa.
Given the candidates of Average Pooling (AvgPool), Max
Pooling (MaxPool) and K-Max Pooling (K-MaxPool [20], details in § 3.2) with different K, it shows that the best among them consistently outperform complex aggregators. Here, the best results for Region and Grid feature are achieved by
MaxPool and K-MaxPool (K=20), respectively.
Analyses of the Empirical Findings. Most complex ag-gregators are designed to contextualize the input features spatially, leveraging the relationship between spatial grids or regions. However, these aggregators introduce a large set of parameters in addition to the vanilla VSE model, which causes a higher risk of over-ﬁtting comparing to simple pool-ing functions. In this paper, instead of investigating why complex aggregators are suboptimal, we focus on maximiz-ing the advantages of pooling-based aggregation.
While the optimal pooling strategy enjoys simplicity and effectiveness, searching it requires repetitive experi-ments over numerous conﬁgurations (e.g., different K for
K-MaxPool), which is both tedious and costly. This process can be more complicated when the feature extractor changes, or when the features have variable lengths (e.g., text).
Motivated by these, we aim for a general and plug-and-play pooling operator that generalizes over different pooling patterns (e.g., Avg, Max and K-MaxPool with arbitrary K) for variable-sized inputs, and learns to automatically adapt itself to the best strategy according to the data (e.g., image, text, video etc.) and feature extractors. We denote our pro-posed module as the Generalized Pooling Operator (GPO). n}N 3.2. Generalizing over Different Pooling Strategies
Suppose that we have a set of N feature vectors {φi n=1 and our goal is to obtain a holistic vectorized embedding vi out from the N elements, for each dimension i = 1, . . . , d1.
Here we use the superscript i to index the i-th dimension of the feature vector. We further denote maxk(·) as the operator that takes the k-th maximum value from an ordered list. Then, we can formally deﬁne commonly used pooling strategies as the following:
• AvgPool The average pooling computes the mean value among the N elements, as vi = 1
N
N n=1 φi n, ∀i.
• MaxPool The max pooling computes the maximum value among the N elements, as vi = max1({φi n}N n=1), ∀i.
P n=1), ∀i.
K k=1 maxk({φi
• K-MaxPool The K-max pooling computes the mean value of the top-K maximum values among the N elements, as vi = 1
K n}N
Main Idea As described above, GPO aims to generalize over various pooling strategies, so that the pooling operator can automatically ﬁnd the most appropriate strategy for different features. Therefore, GPO learns to generate the pooling coefﬁcients θ, and the pooling is deﬁned as a weighted sum over sorted features:
P vi =
N k=1
X
θk · maxk
{φi
N n} n=1
, ∀i, (2) (cid:16) (cid:17)
N where
θk = 1. k=1
X
Here, the coefﬁcients θ are of the size N, with a scalar weight
θk for the k-th maximum value among the N elements. The
N k=1 θk = 1 is enforced via Softmax. The constraint parameterized pooling operator can approximate AvgPool,
P 15791
used in Transformers [42] to vectorize positional indices: pi k = ( sin(wj, k), when i = 2j cos(wj, k), when i = 2j + 1
, ∀i. (4) 1 100002j/d3 and d3 is the number of dimensions where wj = for the positional encoding.
Generating Pooling Coefﬁcients with a Sequence Model
Using the positional encoding above, we transform every position index k into a dense vector pk ∈ Rd3 . Next, we learn a sequence model to produce the pooling coefﬁcients.
Since the size of feature set N varies, it is necessary for the coefﬁcient generator to be aware of the size of feature set.
Therefore, we make use of a sequence-to-sequence decoder function, which takes the sequence of positional encodings p = {pk}N k=1 as input and outputs the sequence of pooling coefﬁcients θ = {θk}N k=1. The decoder function consists of a small BiGRU and a multi-layer perceptron (MLP):
N
N k=1 = BiGRU({pk}
{hk} k=1),
θk = MLP(hk) (5)
Here hk is the output of the BiGRU at the position k.
Learning Generator with Diverse Set Sizes To make
GPO’s coefﬁcient generator g(·, ·) better approximate differ-ent pooling patterns for variable-sized inputs, we perform a data augmentation strategy to allow it observing a larger variety of feature set sizes. During the training, we randomly drop 20% inputs vectors to perturb the size of the input fea-ture set, which we call Size Augmentation. We show in
Appendix that applying this strategy to both image and text effectively improve the performance of VSE models.
Figure 2. Detailed illustration of the GPO architecture.
MaxPool, K-MaxPool with arbitrary K, and more complex pooling functions. For instance, the learned pooling strat-egy could weight the top-K elements unevenly, or only set non-zero values for θ1 and θN . We visualize some learned pooling coefﬁcients in § 5.3.
Learning to Generate the Pooling Coefﬁcients The most straightforward way to parameterize θ is to deﬁne it as a trainable vector, but this can only deal with the scenario where N is a constant integer. When the features are of variable sizes, which is common in video and text sequences, learning a ﬁxed set of coefﬁcients θ is no longer feasible.
To address this issue, we propose to learn a parameterized function g(·, ·) as the coefﬁcient generator:
θk = g(k, N), where k = 1, . . . , N. (3) 3.4. Building up VSE∞ using GPO
As a consequence, for each position k, the coefﬁcient gener-ator g(·, ·) outputs a coefﬁcient θk to aggregate {φi n}N n=1. 3.3. Implementing Generalized Pooling Operator
Now we discuss the concrete implementation of the GPO function g(·, ·). Figure 2 provides an illustration of the archi-tecture. There are two major components in the GPO design: (1) A positional encoding function based on trigonometric function; (2) A sequence model that takes the positional encoding sequence to generate pooling coefﬁcients, based on bidirectional Gated Recurrent Unit (BiGRU).
Encoding Position Every position index k is uniquely repre-sented by a dense vector, such that the vector can be further transformed to θk by parameterized functions. A common approach here is to learn an embedding matrix in which row k is the embedding for k. However, this presumes the input positions {1, . . . , k, . . . , N} orthogonal to each other. To make more efﬁcient use of the prior information between position indices, we adopt the positional encoding strategy
We build up our multi-modal matching model (dubbed
VSE∞) by pluging GPO into the standard VSE framework (§ 2). Speciﬁcally, we replace the visual and text aggregators in the standard VSE framework (i.e., AvgPool) with two
GPOs. The two GPOs project the image feature vectors and text feature vectors independently into two holistic em-beddings, to further compute the matching score. VSE∞ is closely related to previous VSE models. We adopt the learning framework of VSE++ [9] (Eq. 1), which improves early VSE models [10, 22] with an additional online hard negative mining procedure. We refer to § 5 for more details. 4.