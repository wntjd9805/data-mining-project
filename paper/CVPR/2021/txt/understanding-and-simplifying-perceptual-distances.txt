Abstract
Perceptual metrics based on features of deep Convolu-tional Neural Networks (CNNs) have shown remarkable success when used as loss functions in a range of com-puter vision problems and signiﬁcantly outperform classi-cal losses such as L1 or L2 in pixel space. The source of this success remains somewhat mysterious, especially since a good loss does not require a particular CNN architecture nor a particular training method. In this paper we show that similar success can be achieved even with losses based on features of a deep CNN with random ﬁlters. We use the tool of inﬁnite CNNs to derive an analytical form for percep-tual similarity in such CNNs, and prove that the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distribu-tions of small patches in the two images. We use this equiva-lence to propose a simple metric for comparing two images which directly computes the MMD between local distribu-tions of patches in the two images. Our proposed metric is simple to understand, requires no deep networks, and gives comparable performance to perceptual metrics in a range of computer vision tasks. 1.

Introduction
Following the enormous success of CNNs for object recognition tasks [37, 24] and evidence for the generality of their learned representations [11], Gatys et al. [15] sug-gested comparing images in the feature space of such net-works’ intermediate layers instead of in pixel space. They suggested that such representations are more sensitive to the semantic content of the image and less to the exact ap-pearance of the objects in the image. Following their suc-cess, the use of perceptual losses has spread and showed promising results for a variety of tasks: ranging from image restoration tasks, such as super-resolution [20, 26], image deblurring [25] and image inpainting [28], image genera-tion [12, 14] and image domain transfer [6]. Despite its universal success and applicability, perceptual losses suffer from several drawbacks:
• Computational cost - computing intermediate feature representations and propagating gradients backwards through a large object recognition CNN can be very expensive.
• Domain speciﬁcity - while representations learned on
ImageNet [10] tend to transfer well to a range of com-puter vision tasks, such features may not be applica-ble for domains where the image statistics differ dras-tically from those in ImageNet.
• Interpretability - it is not well understood when, how and why perceptual losses succeed, and how to tune their hyperparameters. As evidence for the severity of this problem, one can observe the large inconsistency in the literature in the choice of most hyper-parameters for perceptual loss - the choice of speciﬁc layers (or combinations of them), whether features are extracted pre or post activation and whether to normalize activa-tions prior to distance computation.
In this work we derive a better understanding of the rea-sons behind the success of the commonly used perceptual losses, and use this understanding to derive a simpler, well understood loss that can serve as an alternative. Speciﬁ-cally, we show that losses based on CNNs with random ﬁl-ters are almost equally “perceptual” and can serve as a suit-able loss function in image prediction tasks. We then use the recent tool of inﬁnite random networks to derive an ana-lytical form for perceptual loss in such CNNs. Speciﬁcally, we prove that the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distributions of small patches in the two images. We use this equivalence to propose a simple metric for comparing two images which directly computes the MMD between the distribution of patches in the two im-ages. Our proposed metric is simple to understand, requires no deep networks, and gives comparable performance to perceptual metrics in a range of computer vision tasks. 1.1. The Success of Perceptual Losses
The effectiveness of perceptual losses compared to stan-dard losses is evidenced by two empirical successes: (1) 112226
(A) (B)
…
…
…
…
Samples
Mean Image
L2
VGG
Trained
VGG
Random
S-CNN
Random
MMD
Figure 1. The generalized mean image problem. Given a set of target images (left), we ﬁnd an image that minimizes the sum of losses with respect to all targets (right). In (A) sets were generated using StyleGAN as described in Section 5, and in (B) target images consist of slightly different crops of a single image from ImageNet. When the loss is L2, the optimal image is blurry , while both perceptual losses and our new, proposed loss, give sharp, realistic images. when used as losses in tasks in which the targets are natural images they lead to sharp, realistic images while standard losses lead to blurry, non-realistic images (e.g. [3]) and (2) they better correlate with human judgements of patch simi-larity [40]. In this section, we illustrate these successes and also show the surprising utility of perceptual losses based on random ﬁlters in those two scenarios.
To investigate the effectiveness of perceptual losses in tasks in which the targets are image prediction tasks, i.e. natural images, we ﬁrst consider why the mean squared er-ror (MSE) over pixel values fails on such tasks. Consider for example, the task of image super-resolution in which for every low resolution (LR) image one needs to predict the correct high resolution (HR) image. A trained model which generalizes well will minimize the reconstruction loss with respect to the true distribution of HR images given LR im-ages. Since such a model is deterministic, for a single LR image it should predict a single HR image even though many plausible HR images could have been mapped to the same LR image. The optimal model will choose the im-age that minimizes the average expected reconstruction loss over the distribution of possible HR images. When MSE is used, this is simply the mean. Since the mean of many sharp images with non-aligned textures and edges is a blurry im-age, the predictions of the model will be blurry. Thus, a sensible requirement from a loss function would be that the image which minimizes the expected loss over all probable target images is sharp and realistic.
To empirically investigate this property in controlled set-ting, we deﬁne the “Generalized Image Mean” (GIM) op-timization problem, where given a set of target images
Y = {y1, ..., yN } and an image reconstruction loss L we seek to ﬁnd an image ˆy that minimizes the sum of the losses with respect to all target images:
ˆy = arg min y 1
N
Xi
L(y, yi) (1)
For the MSE loss this reduces to the mean of the set Y and for any differentiable loss we can approximate it by opti-mizing the value of ˆy directly, using Stochastic Gradient
Descent (SGD). Figure 1 shows results for the GIM prob-lem for different sets and losses. In the ﬁrst two rows, Y is a set of samples generated by StyleGAN [22] for a small neighborhood of latent codes and in the last two rows these are slightly different crops of the same image. When the L2 loss is used, the optimal image is clearly blurry and non-realistic. However, when the loss function is the percep-tual loss with a trained VGG network, the optimal image is 212227
sharper and much more realistic. These illustrative results conﬁrm previous reports (e.g. [3, 5]) and demonstrate that the effectiveness of perceptual losses can be observed even in simple prediction problems. 1.2. Unreasonable Effectiveness of Random Filters
In the work of Zhang et al. [40] randomly initialized net-works were found to be as “perceptual” as low level losses and far less consistent with human judgements compared to trained CNNs. The judgements were based on a two alter-native forced choice (2AFC) task, where each trial is com-posed of a random reference image patch and two random deformations of the patch. Subjects are asked to choose which of the two deformations is more similar perceptu-ally to the reference patch. In contrast to their results, we found that by adding a few simple modiﬁcations, random networks can achieve comparable performance to learned networks on this task. First, to reduce the variance of pre-dicted judgements, we compute each 2AFC judgement by initializing 20 random networks and using voting between all results. Second, we constrain the random ﬁlters in the
ﬁrst layer to have zero-mean, thus ignoring the DC com-ponent for every channel (by subtracting the mean of the randomly drawn ﬁlter). We evaluate random VGG16 net-works using this scheme and obtain accuracy closer to that of supervised methods as can be seen in ﬁgure 2.
Motivated by the surprising success of these random net-works, we turn to analyze a simpler CNN architecture that consists of the standard “ingredients” of common CNNs: convolutions and pooling layers, which we call the Simple-CNN (S-CNN). This architecture consists of a single spatial convolution with kernel size P followed by D 1×1 convolu-tion layers and a single average pooling layer with window size W and strides S. All intermediate layers have constant width C and all convolutional layers are followed by ReLU activation. This architecture shares some similarities with previous works which investigated the use of limited local receptive ﬁelds and 1 × 1 convolutions [4, 34]. As shown in
ﬁgure 2, random networks of this architecture with param-eters P = 3, D = 6, W = 32, S = 16 and C = 1024 work signiﬁcantly better than low-level metrics and slightly better than random VGG in the 2AFC experiment.
Figure 1 shows that random networks also work surpris-ingly well when used as loss functions for the GIM task.
Again, we compute ˆy using equation 1 where the loss is a
“perceptual” loss with random CNNs. The computed image
ˆy is sharp and realistic, comparable to the predictions when a trained VGG is used for the loss. This is true both for a random VGG network and a random S-CNN.
In summary, our results reafﬁrm previous reports regard-ing the remarkable effectiveness of perceptual losses com-pared to standard losses. However, the success of random
CNNs challenges the conventional wisdom that this success
Figure 2. Test accuracy on the CNN-based and traditional image distortion 2AFC task from [40]. Trained CNNs, random CNNs and our suggested MMD loss, signiﬁcantly outperform classic low-level distance metrics. has to do with the fact that CNNs trained on image discrim-ination learn representations that are semantically meaning-ful. How, then, can we understand the success of perceptual losses? 2. Analysis: Perceptual Similarity in Ran-dom S-CNNs Converges to MMD Between
Patch Distributions.
Our main analytical result shows that perceptual sim-ilarity between two images using a random S-CNN con-verges to a distance between the local distribution of small patches in the two images. The distance between distri-butions is measured using Maximum Mean Discrepancy (MMD) [17]. Roughly speaking, given two distributions
P (x), Q(x), MMD measures the maximal difference be-tween the expectations of a smooth function of x in the two distributions. We now give the formal deﬁnition of MMD and describe its properties before stating our result.
Deﬁnition: given two distributions P, Q, and a norm over functions kf kH whose reproducing kernel is K, the
MMD between P, Q is given by:
M M D(P, Q) = max kf kH <1
EP [f (x)] − EQ[f (x)] (2)
Similar to the Wasserstein distance [1], the MMD be-tween the distributions P and Q is given by a critic function f (x). If two distributions are identical, then for any critic function we will have EP [f ] = EQ[f ] and the more dissim-ilar the distributions, the larger the difference. The critic is constrained to have unit Hilbert norm in function space. For the commonly used RBF Kernel, kf kH penalizes the mag-nitude of the Fourier transform of f at different frequencies and thus measures the smoothness of f . 312228
As shown by [17], under reasonable assumptions on the kernel K, the MMD distance between two distributions is zero if and only if P = Q. However, unlike the Wasserstein distance and other distances between distributions, it can be efﬁciently computed in high-dimensions.
Given N IID samples si from P and N IID samples ti from Q, an empirical (biased) estimate of the MMD be-tween P and Q is given by:
ˆM M DK(S, T ) = 1
N 2
Xi,j
K(si, sj)−2K(si, tj)+K(ti, tj)
Theorem 2.1. L2 distance over normalized representa-tions in the post-pooling layer of an inﬁnite random S-CNN with patch size P and pooling window size W is equal to
ˆM M DK distance between the distributions the average of patches of size P within windows of size W in the two images. The kernel K(p1, p2) of two patches is a robust, monotonically decreasing function of the L2 distance be-tween the two patches.
Proof. The proof follows from a line of recent results re-garding inﬁnitely-wide CNNs [33, 8]. Given two images x1, x2, a neural network architecture and a layer h, these results allow us to calculate K(x1, x2), deﬁned as the dot product between h(x1) and h(x2) in an inﬁnite CNN with that architecture. When computing perceptual similarity, we are measuring the L2 distance between representations which can be written as kh(x1) − h(x2)k2 = K(x1, x1) − 2K(x1, x2) + K(x2, x2). By using the form of K derived in previous work, we arrive at the result. For completeness, a full proof is given in the supplementary material.
Corollary: If the set of possible outputs {yi} for a given input x are a sequence of images that all have the same dis-tribution over patches of size P in all pooling windows of size W , then ˆy deﬁned by equation 1 using a perceptual loss with an inﬁnite random S-CNN should have the same local distribution over patches of size P as each of the original images.
Proof:
This follows directly from the fact that
M M D(P, Q) is zero if and only if P = Q.
While our theorem is for the simpliﬁed CNN with ran-dom weights and inﬁnite width, similar results can be ob-tained for an S-CNN with learned weights and ﬁnite width: the perceptual distance is still the MMD between the two distributions, but the kernel between patches K is now a learned kernel which may not satisfy the conditions that en-sure that the MMD is zero if and only if the two distributions are the same.
Our analysis suggests an alternative explanation for the success of perceptual losses in many settings.
If all the training images have the same local distributions over patches, the predicted image will have the same distribu-tion. Thus if all the training images have sharp gradients, we should expect the predicted image to have sharp gradi-ents as well. 3. A New Loss
If indeed the success of perceptual loss is largely due to its minimization of the distance between distributions of lo-cal patches, we should be able to achieve similar success with a more direct loss. To test this hypothesis, we deﬁne
ˆM M DK dis-a new loss that directly measures the average tance between the distributions of patches of size P within windows of size W in the two images. Rather than using deep networks to deﬁne the kernel between two patches, we simply replace it with the standard Gaussian RBF kernel, deﬁned as: k(p1, p2) = exp kp1 − p2k2 2σ2 (cid:19)
− (cid:18) (3)
As mentioned above, the MMD distance between two dis-tributions over patches P (p) and Q(p) with this kernel mea-sures how different EP (f ) can be from EQ(f ), where f is constrained to be a smooth function of the patches.
Our MMD loss has four hyper-parameters: the band-width of the Gaussian kernel (σ), the size (W ) and strides (S) of the pooling window and the patch size (P ). In ad-dition, one can decide whether or not to use channel nor-malization (in which the DC of the patch in each channel is ignored when comparing the two patches). All the results in this paper used patch size of 3, W = 32 and S = 16 corresponding to the post-pooling features of the S-CNN.
We varied σ for different applications but it was always in the range (0.5, 0.75). We used channel normalization only in the 2AFC experiments.
Computing the distance for all pairs of patches within ev-ery pooling window can become computationally intensive for large pooling windows. Therefore, we approximate the
Gaussian kernel using Random Fourier Features [36]. Us-ing this approximation, the MMD loss can be represented in any auto differentiation framework as a two layer CNN, where the ﬁrst layer is a convolution with random weights and biases b ∼ U (0, 2π), followed by co-w ∼ N sine activations and an average pool layer with a pooling window W and pooling strides S. 0, 1
σ2 (cid:0) (cid:1)
Our new loss function was motivated by the success of random S-CNNs in the GIM experiments and the 2AFC task of [40]. As can be seen in ﬁgures 1 and 2, our loss which di-rectly computes distances between distributions of patches in the two images performs very well on both tasks. In the 2AFC task it is within the conﬁdence interval of the pre-trained VGG. 4.