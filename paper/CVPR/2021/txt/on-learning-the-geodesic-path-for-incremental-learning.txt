Abstract
Neural networks notoriously suffer from the problem of catastrophic forgetting, the phenomenon of forgetting the past knowledge when acquiring new knowledge. Overcom-ing catastrophic forgetting is of signiﬁcant importance to em-ulate the process of “incremental learning”, where the model is capable of learning from sequential experience in an efﬁ-cient and robust way. State-of-the-art techniques for incre-mental learning make use of knowledge distillation towards preventing catastrophic forgetting. Therein, one updates the network while ensuring that the network’s responses to previ-ously seen concepts remain stable throughout updates. This in practice is done by minimizing the dissimilarity between current and previous responses of the network one way or another. Our work contributes a novel method to the arsenal of distillation techniques. In contrast to the previous state of the art, we propose to ﬁrstly construct low-dimensional manifolds for previous and current responses and minimize the dissimilarity between the responses along the geodesic connecting the manifolds. This induces a more formidable knowledge distillation with smooth properties which pre-serves the past knowledge more efﬁciently as observed by our comprehensive empirical study. 1 1.

Introduction
Humans are able to gather and grow their knowledge grad-ually and effortlessly while preserving their past knowledge.
In pursuing a similar capability at an algorithmic level and in the so-called Incremental Learning (IL), an artiﬁcial machine must learn and remember various tasks or concepts [22] to accomplish a more broadened set of objectives. Formally, a learning algorithm in IL will receive its objectives (e.g., recognizing new classes) gradually. This is in contrast to con-ventional learning paradigm where the objectives and data are available from the beginning to the learning algorithm. 1Our code is available at https://github.com/chrysts/ geodesic_continual_learning
Figure 1: The visualization of our proposed method. To regularize the network for incoming tasks, knowledge from previous tasks is preserved by distilling the features follow-ing the geodesic path between two subspaces of different models. The distillation is based on the projection of two sets of features from two different networks.
The gradual nature of learning in IL poses serious difﬁcul-ties as now the model has to sequentially learn and adapt to new tasks, while being vigilant to its needs and require-ments (e.g., limited memory to preserve the knowledge). As shown in the seminal works of Kirkpatrick et al. [20] and
Rebufﬁ [34], the gradual nature of learning plus the presence of constraints and limitations in IL can degrade the perfor-mance of the model drastically, which is formally known as
“Catastrophic forgetting” [7, 28, 29]. The phenomenon refers to a neural network experiencing performance degradation at previously learned concepts when trained sequentially on learning new concepts. This forgetting problem appears even worse when the model is sequentially updated for new tasks without considering the previous tasks as shown in [20, 23] i.e., learning new tasks overrides the knowledge from previ-ous tasks. Finding the balance among tasks, also known as the stability-plasticity dilemma in [33], is crucial to achieve the IL ultimate goal.
In classical classiﬁcation on visual data, a Convolutional
Neural Network (CNN) is used to encode the input images 11591
into features and a ﬁnal layer consists of classiﬁers to map the features to the ﬁxed number of classes. In class IL [38], a
CNN keeps learning and grows its capacity to accommodate new classes or tasks. In order to achieve the equilibrium performance between learning new tasks and maintaining existing base performance, some IL methods store observed tasks in the memory and replay them [10, 25] to prevent catastrophic forgetting. However, learning through a large number of tasks limits the exemplars that can be reserved in the memory. Hou et al. [17] suggest to replay the exemplars in the memory on both old and current models and prevent the forgetting phenomenon with knowledge distillation.
The design of distillation loss remains an open research problem. A knowledge distillation on the output space (after the classiﬁer) is ﬁrstly proposed in [23] so-called Learning without Forgetting (LwF). However, the study in [17, 34] shows that LwF lets the parameters of the new classes to become more dominant than the old parameters. As a result, the model tends to classify all test data to the new classes.
Because of this reason, the classiﬁer is created based on the nearest mean of exemplars and the distillation loss with the cosine distance in the feature space is suggested in [17].
All these prior methods do not consider the gradual change between tasks. In fact, human minds learn from one task to another task by gradual walk as described in [3]. Inspired by the idea of gradual change in human minds, we propose a distillation loss in IL by adopting the concept of geodesic
ﬂow between two tasks (see Fig. 1) called as GeoDL. To summarize, the contributions of our work are: 1. We propose a novel distillation loss that considers the information geometry aspect in incremental learning and leads to improved performance. 2. We show that using geodesic path for knowledge dis-tillation yields less forgetful networks compared to the distillation losses using the Euclidean metric. 2.