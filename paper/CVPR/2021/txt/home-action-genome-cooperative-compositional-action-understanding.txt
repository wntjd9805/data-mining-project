Abstract
Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the ben-eﬁts of formulating actions as a combination of atomic-actions have shown promise in improving action under-standing with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple view-points and multiple modalities of data for representation learning. To promote research in this direction, we intro-duce Home Action Genome (HOMAGE): a multi-view ac-tion dataset with multiple modalities and view-points sup-plemented with hierarchical activity and atomic action la-bels together with dense scene composition labels. Lever-aging rich multi-modal and multi-view settings, we propose
Cooperative Compositional Action Understanding (CCAU), a cooperative learning framework for hierarchical action recognition that is aware of compositional action elements.
CCAU shows consistent performance improvements across all modalities. Furthermore, we demonstrate the utility of co-learning compositions in few-shot action recognition by achieving 28.6% mAP with just a single sample. 1.

Introduction
Action understanding in videos is a critical task with var-ious use-cases and real-world applications, from robotics
[1, 2] and human-computer interaction [3] to healthcare
[4, 5] and elderly behavior monitoring [6, 7]. Despite the recent success of deep learning methods for image classiﬁ-cation, complex and holistic action or event understanding remains an elusive task.
There are several challenges associated with the task of action understanding. The inherent variability in executing complex activities poses one of the most critical difﬁcul-ties in building action understating models. To understand these challenges, it is essential to understand what actions are composed of. As opposed to bounding boxes in the ob-ject detection task, actions are composed of various parts
Audio
Third
Person
View
Ego
View faudio caudio fthird c third fego c ego
Cooperative
Learning
Activity do laundry (single-label)
Atomic
Action holding a basket holding detergent (multi-label)
Compositional
Outputs
Figure 1: Given an activity instance (e.g., ‘do laundry’) and cor-responding multiple views, we compute features using modality-speciﬁc deep encoders (f modules). Different modalities may capture different semantic information regarding the action. Co-operatively training all modalities together allows us to see im-proved performance. We utilize training using both video-level and atomic action labels to allow both the videos and atomic ac-tions to beneﬁt from the compositional interactions between the two. As discussed in the results, we see signiﬁcantly improved performance when using the above components together. spanned in space and time. For instance, the action of “laun-dry” involves multiple entities, e.g., humans, objects, and their relationships, and is composed of a number of atomic actions. Such partonomy of actions [8, 9, 10] both in space and time deﬁnes a hierarchical structure. Furthermore, to capture the variability in executing complex activities, un-derstanding each part (e.g., body limbs, objects, or atomic actions) becomes crucial. Since actions happen in the 3D world, a holistic understanding of the world requires captur-ing the subtle movements or parts using multiple modalities (e.g., RGB and audio) and from multiple viewpoints.
Each of these challenges has previously been separately investigated using different datasets and advanced meth-ods. For instance, numerous datasets were put together for generic action recognition and spatio-temporal localiza-tion in YouTube or broadcasting third-person videos, such as Kinetics [11], Charades [12], ActivityNet [13], UCF101
[14]. Other datasets such as EPIC Kitchens [15] were used for ego-centric action recognition. Action Genome [10] focused on using scene information in action recognition, 11184
while others [16] focused on hierarchical action modeling from events to low-level atomic actions. Several studies target learning from long instructional videos and release datasets [17, 18, 19, 20] for the same, exploring the parton-omy of actions in long sequences. Others also focused on observing and recognizing actions from multiple views, such as LEMMA [21] and HumanEva [22].
In parallel, there have been numerous recent advances in contrastive and cooperative learning [23, 24] applied to multi-modal and multi-view datasets as a self-supervised pre-training strategy to improve downstream recognition results. De-spite all these advances, action understanding and gener-alizability of such models remains a challenging problem due to complexities brought by their complicated nature and numerous object interactions. Multi-modal approaches
[25, 26, 27] have shown superior performance in tackling such issues. However, there is still a need for a benchmark that uniﬁes all these challenges and tasks. In this paper, we release a dataset along with a novel method for hierarchical action recognition to tackle these problems.
We introduce a new benchmark for action recognition,
Home Action Genome (HOMAGE), that includes multi-modal synchronized videos from multiple viewpoints along with hierarchical action and atomic-action labels. Actions in homes are challenging as we deal with long-term ac-tions, interactions with objects, and frequent occlusions.
Having multiple views and sensors to handle occlusions and scene graph information to capture object interaction allows us to tackle these complexities.
In addition, syn-chronous videos provide implicit alignment that facilitates multi-modal training. Additionally, access to sensor infor-mation enables future research in privacy-aware recognition where we avoid audio-visual modalities. HOMAGE also provides temporal annotations of high-level activity and low-level atomic action supplemented with spatio-temporal scene-graphs. Annotations regarding interaction of objects within actions and atomic actions within high-level actions enable research in explainable video understanding, early action prediction, and long-range action recognition.
For this new benchmark, we introduce a novel method to perform simultaneous co-training with multiple modalities (RGB, audio, and annotations of scene composition) and viewpoints that enable the learning of rich video representa-tions. Training involves a co-training strategy that leverages information from all views and modalities to build the rep-resentation space. During inference, we set up different ex-periments and observe improved action recognition perfor-mance even when only a single modality is used, which sug-gests training on HOMAGE improves performance with no need for other modalities during inference. In this paper, we explore audio-visual data (of interest to the vision commu-nity). Future sensor-fusion work can further exploit other modalities we release (e.g., for privacy-preserving studies).
HOMAGE aims to unify various aspects and challenges of action recognition, speciﬁcally targeting multi-modal and compositional perception for home actions. Moreover, the presence of a large number of modalities in our dataset encourages research in areas such as privacy-aware recog-nition and sensor-fusion. To summarize, our contributions are as follows: (1) We introduce a new dataset, Home Action Genome (HOMAGE) with multiple views and modalities densely an-notated with scene graphs and hierarchical activity labels (overall activity and atomic actions). (2) We propose a novel learning framework (CCAU) that leverages multiple modalities and hierarchical action labels and improves the performance of the baselines trained on each individual modality. We demonstrate the beneﬁts of our approach with an improvement of +6.4% using only ego-view during inference. 2.