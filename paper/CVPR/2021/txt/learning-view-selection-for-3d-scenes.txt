Abstract
Efﬁcient 3D space sampling to represent an underlying 3D object/scene is essential for 3D vision, robotics, and be-yond. A standard approach is to explicitly sample a dense collection of views and formulate it as a view selection prob-lem, or, more generally, a set cover problem. In this paper, we introduce a novel approach that avoids dense view sam-pling. The key idea is to learn a view prediction network and a trainable aggregation module that takes the predicted views as input and outputs an approximation of their generic scores (e.g., surface coverage, viewing angle from surface normals). This methodology allows us to turn the set cover problem (or multi-view representation optimization) into a continuous optimization problem. We then explain how to effectively solve the induced optimization problem using con-tinuation, i.e., aggregating a hierarchy of smoothed scoring modules. Experimental results show that our approach ar-rives at similar or better solutions with about 10 x speed up in running time, comparing with the standard methods. 1.

Introduction
Computing the multi-view representation of an underly-ing 3D object/scene is a fundamental problem in 3D vision and beyond. The desired output shall use a small set of views to maximize a pre-deﬁned scoring function that in-volves objectives such as coverage of the underlying scene and viewing constraints among salient regions. A standard approach is ﬁrst to densely sample a set of views and then formulate a generalized set cover problem, e.g., by optimally selecting a subset of views to maximize the scoring function.
However, this approach usually incurs signiﬁcant computa-tional overheads due to the cost of rendering and solving the induced optimization problem.
In this paper, we introduce a novel approach that avoids explicitly sampling a dense set of views. Our approach is motivated by training view prediction networks that take a small collection of views as input and outputs predicted views that correspond to other camera poses. Speciﬁcally, our approach approximates the scoring function using a neu-ral network that combines a view prediction network and a scoring module.
The view prediction takes a 3D model and a camera pose as input and outputs a concise representation under that view.
The scoring module then takes the view predictions under multiple camera poses as input and outputs an approximation of the pre-deﬁned score. The entire network is trained end-to-end. This approach allows us to turn the difﬁcult multi-view representation optimization problem into a simple continu-ous optimization problem. In particular, our approach avoids explicitly sampling rendered images (See Figure 1).
Our approach is motivated by the fact that although a rendered image may contain rich geometric details of the underlying scene, the visibility pattern, which affects the coverage problem’s results, has low-dimensional structures.
A 3D scene usually consists of primitive geometric shapes, and contours of shapes (e.g., sharp edges of box-shape ob-jects) mostly capture the visibility patterns. Recent advances in representations of 3D models and neural network models make it possible to learn representations of visibility patterns 114464
to approximate the scoring function’s loss surface.
Speciﬁcally, our approach represents a 3D scene using a volumetric representation. The view prediction network combines a scene encoder, a camera transformation encoder, and a coverage estimator decoder. We also design a separate scoring module, which outputs the coverage score for a spe-ciﬁc camera conﬁguration. The resulting network allows us to optimize an initial camera conﬁguration by continuously optimizing the camera conﬁgurations. This procedure is efﬁcient and effective.
Our learning-to-optimize formulation made two major contributions: (1) we turn a discretized multiple-view se-lection problem into a continuous optimization problem by training a deep neural network that approximates the cov-erage scoring function; (2) we provide the solver to ﬁnd the optimal camera poses given the formulation with much less computational cost. We have evaluated our approach on a benchmark dataset through greedy sampling over the multi-view camera arrays in 339 high-resolution models of real scenes. The evaluation results show that we achieve similar coverage results comparing to greedy methods on dense sampling grids with about 10× speed. Our method also outperforms other fast baseline methods within similar time budgets. 2.