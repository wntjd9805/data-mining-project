Abstract
Learning discriminative image representations plays a vital role in long-tailed image classiﬁcation because it can ease the classiﬁer learning in imbalanced cases. Given the promising performance contrastive learning has shown re-cently in representation learning, in this work, we explore effective supervised contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost the classiﬁcation accuracy thereon.
Speciﬁcally, we propose a novel hybrid network structure be-ing composed of a supervised contrastive loss to learn image representations and a cross-entropy loss to learn classiﬁers, where the learning is progressively transited from feature learning to the classiﬁer learning to embody the idea that bet-ter features make better classiﬁers. We explore two variants of contrastive loss for feature learning, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. One of them is the recently proposed supervised contrastive (SC) loss, which is designed on top of the state-of-the-art unsu-pervised contrastive loss by incorporating positive samples from the same class. The other is a prototypical supervised contrastive (PSC) learning strategy which addresses the in-tensive memory consumption in standard SC loss and thus shows more promise under limited memory budget. Exten-sive experiments on three long-tailed classiﬁcation datasets demonstrate the advantage of the proposed contrastive learn-ing based hybrid networks in long-tailed classiﬁcation. 1.

Introduction
In the real world, the image classes are normally pre-sented in a long-tailed distribution [25]. While some com-mon classes (head classes) can have sufﬁcient image sam-ples, some uncommon or rare categories (tail classes) can be underrepresented by limited samples. The data imbalance poses great challenge to learning unbiased classiﬁers.
Most existing work addresses the data imbalance issue by
Figure 1. Illustration of cross-entropy (upper), standard supervised contrastive (SC) (bottom left), and prototypical supervised con-trastive (PSC) (bottom right) loss based feature learning for long-tailed image classiﬁcation. Cross-entropy loss learns skewed fea-tures, which can result in biased classiﬁers. Supervised contrastive learning (bottom two) learns more intra-class compact and inter-class separable features, which ease classiﬁer learning. In standard
SC learning, an anchor sample together with positive samples from the same class are pulled together and the anchor is pushed away from negatives from other classes. In PSC learning, each sample is pulled towards the prototype (marked by star) of its class and pushed away from prototypes of other classes. mitigating the data shortage in tail classes in order to prevent the model from being dominated by the head classes. Typi-cal methods include data re-sampling [1, 8, 35, 4, 28], loss re-weighting [26, 7, 30, 33], margin modiﬁcation [3], and data augmentation [19, 6, 29]. Recently a new line of work is proposed which approaches long-tailed image classiﬁca-tion by decoupling the representation learning and classiﬁer learning into two stages [15, 37, 36]. The shared motiva-tion of such work [15, 37, 36] is that image feature learning and classiﬁer learning may favor different data sampling strategies and thus the focus thereon is to identify suitable sampling strategies for these two tasks. Speciﬁcally, they
ﬁnd under cross-entropy loss, random data sampling can beneﬁt feature learning more while class-balanced sampling 1943
is a better option for classiﬁer learning. Despite promis-ing accuracy achieved, these methods leave the question of whether typical cross-entropy is an ideal loss for learning features from imbalanced data untouched. Intuitively, as shown in Fig. 1, the feature distribution learned from typical cross-entropy can be highly skewed, which can lead to biased classiﬁers [24, 12] that harm long-tailed classiﬁcation.
In this work, we explore effective contrastive learning strategies and tailor them to learn better image represen-tations from imbalanced data in order to boost long-tailed image classiﬁcation. Speciﬁcally, we propose a novel hybrid network structure composed of a contrastive loss for learning image representations and a cross-entropy loss to learn clas-siﬁers. To embody the idea that better features make better classiﬁers, we follow a curriculum to progressively transit the learning from feature learning to classiﬁer learning. We realize two variants of supervised contrastive learning strate-gies, as shown in Fig. 1, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. By doing this, less skewed features and consequently less biased classiﬁers are expected to be obtained.
The ﬁrst contrastive learning we explore to learn fea-tures in imbalanced scenario is the recently proposed super-vised contrastive (SC) learning [18], which is extended from the state-of-the-art unsupervised contrastive learning [5] by incorporating different within-class samples as positives for each anchor. Following unsupervised contrastive learn-ing [5, 9] that have two independent stages for feature learn-ing and classiﬁer learning, the original SC learning [18] learns features using SC loss ﬁrst and then freezes the fea-tures to learn classiﬁers. We argue in this paper such two-stage learning may not be an optimal choice in fully su-pervised scenario, which can harm the compatibility of the features and classiﬁers. We propose a hybrid framework to jointly learn features and classiﬁers, and empirically demon-strate the advantage of our joint learning mode.
One issue of incorporating within-class positive samples in SC learning is that it leads to extra memory consumption.
In SC learning [18], the distances to positives from the same class are contrasted with the distances to negatives from other classes, which results in memory consumption linear to the product of the positive size and negative size. Due to this, when under limited memory budget, the negative size needs to be shrunk. This can compromise the quality of the features learned from contrastive loss [5], especially when dealing with dataset that has a large number of classes, e.g., iNaturalist [11].
To address the aforementioned memory bottleneck from
SC loss, we further propose a prototypical supervised con-trastive (PSC) learning strategy, which shares the similar goal with standard SC learning but avoids explicitly sam-pling positives and negatives. In PSC learning, we learn a prototype for each class and force each sample to be pulled towards the prototype of its class and pushed away from prototypes of all the other classes. In this sense, the PSC strategy enables more ﬂexible and efﬁcient data sampling akin to softmax-based cross-entropy. It observes advantages when dealing with large-scale dataset under limited memory budget. In addition, the PSC loss has some other appealing properties that can beneﬁt imbalanced classiﬁcation, such as less sensitive to data sampling and the potential to cap-ture ﬁner within-class data distribution by using multiple prototypes per class.
Experiments on three long-tailed image classiﬁcation datasets demonstrate the proposed contrastive learning based hybrid networks can obviously outperform the cross-entropy based counterparts and establish new state-of-the-art long-tailed image classiﬁcation performance. The contributions of this work can be summarized as follows:
• We propose a novel hybrid network structure for long-tailed image classiﬁcation. The network is designed to be composed of a contrastive loss for feature learning and a cross-entropy loss for classiﬁer learning. These two learning tasks are performed following a curricu-lum to embody the idea that better features can ease classiﬁer learning.
• We explore effective supervised contrastive learning strategies to learn better features to boost long-tailed classiﬁcation performance. A prototypical supervised contrastive (PSC) learning is proposed to resolve the memory bottleneck resulted from standard supervised contrastive (SC) learning.
• We unveil supervised contrastive learning can be a bet-ter substitute for typical cross-entropy loss for feature learning in long-tailed classiﬁcation. Beneﬁted from the better features learned, our hybrid network substantially outperforms the cross-entropy based counterparts.
Our code is publicly available at https://k-han. github.io/HybridLT. 2.