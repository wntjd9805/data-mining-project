Abstract
Real-world scenes have a dynamic range of up to 280 dB that todays imaging sensors cannot directly capture. Exist-ing live vision pipelines tackle this fundamental challenge by relying on high dynamic range (HDR) sensors that try to recover HDR images from multiple captures with different exposures. While HDR sensors substantially increase the dynamic range, they are not without disadvantages, includ-ing severe artifacts for dynamic scenes, reduced ﬁll-factor, lower resolution, and high sensor cost. At the same time, traditional auto-exposure methods for low-dynamic range sensors have advanced as proprietary methods relying on image statistics separated from downstream vision algo-rithms.
In this work, we revisit auto-exposure control as an alternative to HDR sensors. We propose a neural net-work for exposure selection that is trained jointly, end-to-end with an object detector and an image signal process-ing (ISP) pipeline. To this end, we use an HDR dataset for automotive object detection and an HDR training pro-cedure. We validate that the proposed neural auto-exposure control, which is tailored to object detection, outperforms conventional auto-exposure methods by more than 6 points in mean average precision (mAP). 1.

Introduction
From no ambient illumination at night to bright sunny day conditions, the range of possible luminances computer vision systems have to measure and analyze can exceed 280 dB, expressed here as the ratio of the highest to the lowest luminance value [54]. While the luminance range found at the same time in a typical outdoor scene is 120 dB, it is the “edge cases” that are challenging. For example, exiting a tunnel can include scene regions with almost no ambient illumination, the sun, and scene points with inter-mediate luminances, all in one image. Capturing this large dynamic range has been an open challenge for image sens-ing, and today’s conventional CMOS image sensors are ca-pable of acquiring around 60-70 dB in a single capture [51].
This sensing constraint poses a fundamental problem for low-level and high-level vision tasks in uncontrolled sce-narios, and it is critical for applications that base decision-making on vision modules in-the-wild, including outdoor robotics, drones, self-driving vehicles, driver assistance sys-tems, navigation, and remote sensing.
To overcome this limitation, existing vision pipelines rely on HDR sensors that acquire multiple captures with different exposures of the same scene. A large body of work explores different HDR sensor designs and acquisi-tion strategies [59, 8, 51], with sequential capture meth-ods [67, 69, 47, 64] and sensors that split each pixel into two sub-pixels [66, 29, 30, 2] as the most successfully de-ployed HDR sensor architectures. Although modern HDR sensors are capable of capturing up to 140 dB at increasing resolutions, e.g., OnSemi AR0820AT, the employed multi-capture acquisition approach comes with fundamental lim-itations. As exposures are different in length or start at dif-ferent times, dynamic scenes cause motion artefacts that are an open problem to eliminate [11, 63, 2]. Custom sensor ar-chitectures come at the cost of reduced ﬁll-factor, and hence resolution, and sensor cost, compared to conventional in-tensity sensors. Moreover, capturing HDR images does not only require a sensor that can measure the scene but also necessitates optics for HDR acquisition, without glare and lens ﬂare. Note also that in contrast to LDR sensors, in-terleaved HDR sensors cannot implement global shutter ac-quisition.
In this work, we revisit low dynamic range (LDR) sen-sors, paired with learned exposure control, as a compu-tational alternative to the popular direction of HDR sen-sors. Existing auto-exposure (AE) control methods have been largely designed as proprietary compute blocks, of-ten embedded by the sensor manufacturer on the same sil-icon as the sensor, producing perceptually pleasing images for human consumption. Conventional AE methods rely on image statistics [56, 4, 58], such as histogram or gra-dient statistics, and, as such, do not receive feedback from the task-speciﬁc vision module that ingests the camera im-ages. Similarly, the vision module responsible for a higher-level vision task is designed, trained, and evaluated ofﬂine, often using JPEG images without any dependence on the live imaging pipeline [16, 43, 7]. We explore whether de-parting from conventional AE methods developed in isola-tion and instead learning a neural exposure control that is 7710
jointly learned with a downstream vision module, allows us to overcome the limitations of conventional LDR sensors and recent HDR sensors.
We propose a neural auto-exposure network that predicts optimal exposure values for a downstream object detection task. This control network and the downstream detector are trained in an end-to-end fashion jointly with a differ-entiable image processing pipeline between both models, which maps the RAW sensor measurements to RGB images ingested by the object detector model. The training of this end-to-end model is challenging as AE dynamically mod-iﬁes the RAW sensor measurement.
Instead of an online training approach which would require a camera and anno-tation in-the-loop, we train the proposed model by simu-lating the image formation model of a low-dynamic range sensor from input HDR captures. To this end, we acquire an automotive HDR dataset. We validate the proposed method in simulation and using an experimental vehicle prototype that evaluates detection scores for fully independent cam-era systems with different AE methods placed side-by-side and separately annotated ground truth labels. The proposed method outperforms conventional auto-exposure methods by 6.6 mAP points across diverse automotive scenarios.
Speciﬁcally, we make the following contributions:
• We introduce a novel neural network architecture that predicts exposure values driven by an object detection downstream network in real time.
• We propose a synthetic training procedure for our auto-exposure network that relies on a synthetic LDR image formation model.
• We validate the proposed method in simulation and on an experimental prototype, and demonstrate that the proposed neural auto-exposure control method outper-forms conventional auto-exposure methods for auto-motive object detection across all tested scenarios. 2.