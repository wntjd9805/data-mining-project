Abstract
How should representations from complementary sen-sors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for percep-tion tasks such as object detection and motion forecasting.
However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in trafﬁc light state can affect the behavior of a vehicle geometrically distant from that trafﬁc light. Geometry alone may therefore be insuf-ﬁcient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global con-textual reasoning, such as handling trafﬁc oncoming from multiple directions at uncontrolled intersections. There-fore, we propose TransFuser, a novel Multi-Modal Fusion
Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efﬁcacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reduc-ing collisions by 76% compared to geometry-based fusion. 1.

Introduction
Image-only [16, 8, 41, 3, 42, 64, 53] and LiDAR-only [46, 23] methods have recently shown impressive re-sults for end-to-end driving. However, these studies focus primarily on settings with limited dynamic agents and as-sume near-ideal behavior from other agents in the scene.
With the introduction of adversarial scenarios in the re-cent CARLA [21] versions, e.g. vehicles running red lights, uncontrolled 4-way intersections, or pedestrians emerg-ing from occluded regions to cross the road at random locations, image-only approaches perform unsatisfactory (Tab. 1) since they lack the 3D information of the scene re-*indicates equal contribution
Figure 1: Illustration. Consider an intersection with on-coming trafﬁc from the left. To safely navigate the intersec-tion, the ego-vehicle (green) must capture the global con-text of the scene involving the interaction between the traf-ﬁc light (yellow) and the vehicles (red). However, the trafﬁc light state is not visible in the LiDAR point cloud and the vehicles are not visible in the camera view. Our TransFuser model integrates both modalities via global attention mech-anisms to capture the 3D context and navigate safely. quired in these scenarios. While LiDAR consists of 3D in-formation, LiDAR measurements are typically very sparse (in particular at distance), and additional sensors are re-quired to capture information missing in LiDAR scans, e.g. trafﬁc light states.
While most existing methods for end-to-end driving fo-cus on a single input modality, autonomous driving sys-tems typically come equipped with both cameras and Li-DAR sensors [21, 47, 25, 59, 17, 26, 48, 1, 62]. This raises important questions: Can we integrate representa-tions from these two modalities to exploit their comple-mentary advantages for autonomous driving? To what ex-tent should we process the different modalities indepen-dently and what kind of fusion mechanism should we em-ploy for maximum performance gain? Prior works in the 7077
ﬁeld of sensor fusion have mostly focused on the per-ception aspect of driving, e.g. 2D and 3D object detec-tion [22, 12, 66, 9, 44, 31, 34, 61, 33, 37], motion fore-casting [22, 36, 5, 35, 63, 6, 19, 38, 32, 9], and depth es-timation [24, 60, 61, 33]. These methods focus on learn-ing a state representation that captures the geometric and semantic information of the 3D scene. They operate pri-marily based on geometric feature projections between the image space and different LiDAR projection spaces, e.g.
Bird’s Eye View (BEV) [22, 12, 66, 9, 44, 31, 34, 61, 33] and Range View (RV) [39, 37, 22, 38, 9, 51]. Information is typically aggregated from a local neighborhood around each feature in the projected 2D or 3D space.
While these approaches fare better than image-only methods, we observe that the locality assumption in their ar-chitecture design hampers their performance in complex ur-ban scenarios (Tab. 1a). For example, when handling trafﬁc at intersections, the ego-vehicle needs to account for inter-actions between multiple dynamic agents and trafﬁc lights (Fig. 1). While deep convolutional networks can be used to capture global context within a single modality, it is non-trivial to extend them to multiple modalities or model in-teractions between pairs of features. To overcome these limitations, we use the attention mechanism of transform-ers [54] to integrate global contextual reasoning about the 3D scene directly into the feature extraction layers of dif-ferent modalities. We consider single-view image and Li-DAR inputs since they are complementary to each other and our focus is on integrating representations from different types of modalities. We call the resulting model TransFuser and integrate it into an auto-regressive waypoint prediction framework (Fig. 2) designed for end-to-end driving.
Contributions: (1) We demonstrate that imitation learning policies based on existing sensor fusion approaches are un-able to handle adversarial scenarios in urban driving, e.g., unprotected turnings at intersections or pedestrians emerg-ing from occluded regions. (2) We propose a novel Multi-Modal Fusion Transformer (TransFuser) to incorporate the global context of the 3D scene into the feature extraction layers of different modalities. (3) We experimentally vali-date our approach in complex urban settings involving ad-versarial scenarios in CARLA and achieve state-of-the-art performance. Our code and trained models are available at https://github.com/autonomousvision/transfuser. 2.