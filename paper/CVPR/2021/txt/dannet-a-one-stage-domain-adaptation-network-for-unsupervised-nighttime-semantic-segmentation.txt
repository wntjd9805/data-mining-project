Abstract
Semantic segmentation of nighttime images plays an equally important role as that of daytime images in au-tonomous driving, but the former is much more challenging due to poor illuminations and arduous human annotations.
In this paper, we propose a novel domain adaptation net-work (DANNet) for nighttime semantic segmentation with-out using labeled nighttime image data. It employs an ad-versarial training with a labeled daytime dataset and an unlabeled dataset that contains coarsely aligned day-night image pairs. Speciﬁcally, for the unlabeled day-night im-age pairs, we use the pixel-level predictions of static object categories on a daytime image as a pseudo supervision to segment its counterpart nighttime image. We further design a re-weighting strategy to handle the inaccuracy caused by misalignment between day-night image pairs and wrong predictions of daytime images, as well as boost the predic-tion accuracy of small objects. The proposed DANNet is the
ﬁrst one-stage adaptation framework for nighttime seman-tic segmentation, which does not train additional day-night image transfer models as a separate pre-processing stage.
Extensive experiments on Dark Zurich and Nighttime Driv-ing datasets show that our method achieves state-of-the-art performance for nighttime semantic segmentation. 1.

Introduction
Aiming to label each pixel of a given image to an object category, semantic segmentation is a fundamental computer vision task and plays an important role in many applications such as autonomous driving [11], medical imaging [5] and human parsing [49]. With the advancement of deep learn-ing and computing power, the state-of-the-art performance of semantic segmentation for natural scene images taken at the daytime has been signiﬁcantly improved in recent
∗Equal contribution.
†Co-corresponding authors. Code is available at https://github. com/W-zx-Y/DANNet.
Input image
Ground truth
MGCDA
DANNet (Ours)
Figure 1. Visual comparison of the nighttime semantic segmen-tation results between the state-of-the-art transfer-based approach
“MGCDA” [35] and our proposed DANNet. years [10, 17]. Many researchers have started to segment more challenging images under various kinds of degrada-tions, such as those taken in foggy weather [34] or at the nighttime [33]. In this paper, we focus on semantic segmen-tation of nighttime images, which has wide and important applications in autonomous driving.
With many indiscernible regions and visual hazards [47], e.g., under/over exposure and motion blur, it is usually dif-ﬁcult even for human to build high-quality pixel-level an-notations of the nighttime scene images as ground truth, which, however, is a prerequisite for training many deep neural networks for semantic image segmentation. To han-dle this problem, several domain adaptation methods have been proposed to transfer the semantic segmentation models from daytime to nighttime without using labels in the night-time domain. For example, in [8, 33, 35], an intermediate twilight domain is taken as a bridge to build the adaptation between daytime to nighttime. In [33, 30, 37, 26, 35], an image transferring network is trained to stylize nighttime or daytime images and construct synthetic datasets. All these methods require an additional pre-processing stage of train-ing an image transfer model between daytime and night-time. This is not only time-consuming but also making the 15769
second stage closely rely on the ﬁrst one. Especially, it is difﬁcult to generate a transferred image that shares the ex-actly same semantic information with the original images when the domain gap is large.
In this paper, we propose a novel one-stage domain adaptation network (DANNet) based on adversarial learn-ing for nighttime semantic segmentation (shown in Fig-ure 1) by using the newly released Dark Zurich dataset [33], which contains unlabeled day-night scene image pairs that are coarsely aligned using GPS recordings. The pro-posed DANNet performs a multi-target adaptation from
Cityscapes data to Dark Zurich daytime (Dark Zurich-D) and nighttime data (Dark Zurich-N). Speciﬁcally, we ﬁrst adapt the model from Cityscapes, which contains large-scale training data with labels, to Dark Zurich-D since they are all taken at the daytime. Then, the prediction of Dark
Zurich-D is used as a pseudo supervision for Dark Zurich-N in the network training. We apply an image relighting subnetwork to make the intensity distribution of the images from different domains to be close. Following [38], we in-corporate a weight-sharing semantic segmentation network to make predictions for the relighted images and perform an adversarial learning in the output space to ensure very close layout across different domains. We further design a re-weighting strategy to handle the inaccuracy caused by mis-alignment between day-night image pairs and wrong pre-dictions of daytime images, as well as boost the prediction accuracy of small objects. We conduct extensive experi-ments on Dark Zurich and Nighttime Driving datasets to justify the effectiveness of the proposed DANNet for night-time semantic segmentation. The main contributions of our work are summarized in the following:
• We propose a multi-target domain adaptation network,
DANNet, for nighttime semantic segmentation via ad-versarial learning. DANNet consists of an image re-lighting network and a semantic segmentation net-work, as well as two discriminators. To the best of our knowledge, the proposed DANNet is the ﬁrst one-stage adaptation framework for nighttime semantic segmen-tation.
• We demonstrate that the segmentation of Dark Zurich-D images can provide pseudo supervision for segment-ing the corresponding Dark Zurich-N images, by con-sidering only static object categories.
In particular, it is shown that the specially designed probability re-weighting strategy can signiﬁcantly enhance the seg-mentation of small objects.
• Experiments on Dark Zurich-test and Nighttime Driv-ing datasets show that the proposed DANNet achieves a new state-of-the-art performance of nighttime se-mantic segmentation. Ablation study also veriﬁes the effectiveness of each component in DANNet. 2.