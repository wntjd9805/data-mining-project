Abstract
Though it is well known that the performance of deep neural networks (DNNs) degrades under certain light con-ditions, there exists no study on the threats of light beams emitted from some physical source as adversarial attacker on DNNs in a real-world scenario. In this work, we show by simply using a laser beam that DNNs are easily fooled.
To this end, we propose a novel attack method called Adver-sarial Laser Beam (AdvLB), which enables manipulation of laser beam’s physical parameters to perform adversar-ial attack. Experiments demonstrate the effectiveness of our proposed approach in both digital- and physical-settings.
We further empirically analyze the evaluation results and reveal that the proposed laser beam attack may lead to some interesting prediction errors of the state-of-the-art DNNs.
We envisage that the proposed AdvLB method enriches the current family of adversarial attacks and builds the founda-tion for future robustness studies for light. 1.

Introduction
Natural phenomena may play the role of adversarial at-tackers, e.g. a blinding glare results in a fatal crash of a
Tesla self-driving car. What if a beam of light can adver-sarially attack a DNN? Further, how about using a beam of light, speciﬁcally the laser beam, as the weapon to per-form attacks. If we can do that, with the fastest speed in the world, the laser beam could achieve the fastest attack with no doubts. As shown in Figure 1, by using an off-the-shelf lighting device such as a laser pointer, the attacker can ma-liciously shoot a laser beam onto the target object to make the self-driving car fail to recognize target objects correctly.
We regard the attack illustrated in Figure 1 as a new type of adversarial attack, which is crucial but not yet ex-ploited. Up to now, most researchers study the security of
DNNs by exploring various adversarial attacks in digital-settings, where input images are added with deliberately
Work done when intern at Alibaba Group, China
Code is available at https://github.com/RjDuan/Advlight
†Correspondence to: A. K. Qin
Figure 1: An example. When the camera of self-driving car captures object shot by the laser beam, it fails to recognize
”trolleybus” and ”street sign”. crafted perturbations and then fed to the target DNN model
[23, 10, 6, 3, 18]. However, in physical-world scenarios, images are typically captured by cameras and then directly fed to the target models, where attackers cannot directly ma-nipulate the input image. Some recent efforts in developing physical-world attacks are addressed in [21, 8, 2, 7, 14].
The physical-world adversarial examples typically require large perturbations, because small perturbations are hard
In addition, the attacking ef-to be captured by cameras. fects of adversarial examples of small perturbations can be easily mitigated in complex physical-world environments
[21, 9, 7]. Meanwhile, physical-world adversarial exam-ples require high stealthiness to avoid being discovered by either the victim or defender before performing an attack successfully. Thus for creating physical-world adversarial examples, there is always a compromise between stealthi-ness and adversarial strength.
Most existing physical-world attacks adopt a ”sticker-pasting” setting, i.e., the attacker prints adversarial pertur-bation as a sticker and then pastes it onto the target ob-ject [16, 2, 7, 8]. These attacks achieve the stealthiness of adversaries with extra efforts of designing adversarial per-turbation or camouﬂaging adversarial images and ﬁnding the most effective area in the target object to impose them
[16, 26, 21, 7]. Besides the challenge of stealthiness, the
”sticker-pasting” setting also requires the target object to be physically accessible by the attacker to paste stickers. 16062
(a) AdvCam [7] (b) RP2 [2] (c) AdvLB (Ours)
Figure 2: Visual comprison.
However, this may not be always possible. Several works explore physical-world threats beyond the ”sticker-pasting” setting: Shen et al. [22] and Nguyen et al. [19] proposed using a projector to project the adversarial perturbation on the target to perform an attack. However, these works still require manual effort to craft adversarial perturbation.
In our work, we propose a new type of physical-world attack, named adversarial laser beam (AdvLB). Unlike ex-isting methods, we utilize the laser beam as adversarial per-turbation directly rather than crafting adversarial perturba-tion from scratch. As AdvLB does not require physically changing the target object to be attacked as in the ”sticker-pasting” setting, it features high ﬂexibility to attack any ob-ject actively, even from a long distance. In terms of stealth-iness, a visual comparison between our proposed attack and other works can be seen in Figure 2. Though the adversar-ial example generated via AdvLB may appear less stealthy than some generated by other approaches such as AdvCam.
AdvLB may introduce high temporal stealthiness due to its unique physical-attack mechanism. Speciﬁcally, with the nature of light, AdvLB can perform the attack in a blink right before the attacked target object gets captured by the imaging sensor, and thus avoid being noticed by victims and defenders in advance. Existing works focus on secu-rity issues of DNNs in the daytime whilst potential security threats at night are often ignored. Our proposed AdvLB provides a complementary in this regard. Figure 2 illus-trates the advantage of AdvLB when the lighting condition is poor.
To launch such an attack, we formulate the laser beam with a group of controllable parameters. Then we devise an optimization method to search for the most effective laser beam’s physical parameters in a greedy manner. It enables
ﬁnding where and how to make an effective physical-world attack in a black-box setting. We further apply a strategy called k-random-restart to avoid falling into the local opti-mum, which increases the attack success rate.
We conduct extensive experiments for evaluation of the proposed AdvLB. We ﬁrst evaluate AdvLB in a digital-setting, which is able to achieve 95.1% attack success rate on a subset of ImageNet. We further design two physical-world attacks including indoor and outdoor tests, achieving 100% and 77.43% attack success rates respectively. Then ablation studies are presented on the proposed AdvLB.
Furthermore, we analyze the prediction errors of DNNs caused by the laser beam. We ﬁnd the causes of predic-tion errors could be roughly divided into two categories. 1).
The laser beam’s color feature changes the raw image and forms a new cue for DNNs. 2). The laser beam introduces some dominant features of speciﬁc classes, especially light-ing related classes, e.g. candle. When the laser beam and target object appear simultaneously, there is a chance that the DNN is more biased towards the feature introduced by the laser beam and thus resulting in misclassiﬁcation. These interesting empirical ﬁndings open a door for both attackers and defenders to investigate how to better manipulate this new type of attack. Our major contributions are summa-rized as follows:
• We propose a new type of physical-world attack based on the laser beam named AdvLB, which leverages light itself as adversarial perturbation. It provides high
ﬂexibility for attacker to perform the attack in a blink.
Besides, the deployment of such attack is rather sim-ple: by using a laser pointer, it may become a common threat due to its convenience to perform attack.
• We conduct comprehensive experiments to demon-strate the effectiveness of AdvLB. Speciﬁcally, we perform physical test to validate AdvLB and show the real-world threats of laser beam by simply using a laser pointer. Thus AdvLB can be a useful tool to explore such threats in the real-world.
• We make an in-depth analysis of the prediction errors caused by the AdvLB attack to have revealed some interesting ﬁndings which would motivate future study on AdvLB from the perspectives of attackers and de-fenders. 2.