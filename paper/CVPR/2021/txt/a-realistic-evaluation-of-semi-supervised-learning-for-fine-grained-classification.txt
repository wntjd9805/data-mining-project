Abstract
We evaluate the effectiveness of semi-supervised learning (SSL) on a realistic benchmark where data exhibits con-siderable class imbalance and contains images from novel classes. Our benchmark consists of two ﬁne-grained classiﬁ-cation datasets obtained by sampling classes from the Aves and Fungi taxonomy. We ﬁnd that recently proposed SSL methods provide signiﬁcant beneﬁts, and can effectively use out-of-class data to improve performance when deep net-works are trained from scratch. Yet their performance pales in comparison to a transfer learning baseline, an alternative approach for learning from a few examples. Furthermore, in the transfer setting, while existing SSL methods provide improvements, the presence of out-of-class is often detri-mental. In this setting, standard ﬁne-tuning followed by distillation-based self-training is the most robust. Our work suggests that semi-supervised learning with experts on re-alistic datasets may require different strategies than those currently prevalent in the literature.
Figure 1. Accuracy of semi-supervised learning (SSL) algorithms on the Semi-Aves and Semi-Fungi datasets (see Fig. 2) using (i) different pre-trained models, and (ii) in-class (Uin) and out-of-class (Uin + Uout) unlabeled data. The performances of the supervised baseline and supervised oracle are also shown. Transfer learning from experts is far more effective than SSL from scratch, while in the transfer setting SSL provides modest gains. Though out-of-class data (Uout) is valuable when training from scratch, it is not the case when training from experts (details in Tab. 2 and 3). 1.

Introduction
Semi-supervised learning (SSL) aims to exploit unlabeled data to train models from a few labels, making them practi-cal for applications where labels are a bottleneck. Yet, the current literature on SSL with deep networks for image clas-siﬁcation has two main shortcomings. First, most methods are evaluated on curated datasets such as CIFAR, SVHN, or ImageNet, where class distribution is or is close to uni-form and unlabeled data contains no novel classes. This is implicit in methods that rely on the assumption that the data is uniformly clustered, use a uniform instead of class-balanced loss, or categorize unlabeled data into one of the labeled classes. In practice, however, class distribution can be highly unbalanced or even unknown, and the unlabeled data may contain novel classes. How effective is SSL in these situations?
Second, most literature has focused on training models from scratch. However, a practical approach for few-shot learning is to use expert models trained on large labeled datasets such as ImageNet [36] or iNaturalist [46]. What gains does SSL provide in this setting, especially since many
SSL methods are based on learning invariances from data based on transformations which might have already been learned by the experts during supervised training? Moreover, is out-of-domain data beneﬁcial when experts are available?
Our paper aims to answer these questions by conduct-ing a systematic study of SSL techniques (Fig. 1) on two
ﬁne-grained classiﬁcation datasets that exhibit a long-tailed distribution of classes and contain a large number of out-of-class images (Fig. 2). These datasets are obtained by sampling classes under the Aves (birds) and Fungi taxonomy.
The out-of-class images are other Aves (or Fungi) images not belonging to the classes within the labeled set. The ﬁrst dataset was part of the semi-supervised challenge at FGVC7 workshop [41], while the second one is constructed from 12966
Figure 2. The proposed benchmark for semi-supervised learning. The benchmark contains two datasets, with classes from the Aves and
Fungi taxa respectively. Each represents a 200-way classiﬁcation task and the training set contains (i) labeled images from these classes
Lin, (ii) unlabeled images from these classes Uin, and (iii) unlabeled images from related classes Uout, as seen on ﬁgures to the right.
Moreover, the classes exhibit a long-tailed distribution with an imbalance ratio of 8 to 10. The benchmark captures conditions observed in some realistic applications that are not present in existing datasets used to evaluate semi-supervised learning. See § 3 and Tab. 1 for details. the FGVC fungi challenge [1] following a similar scheme, details of which are described in § 3. We also provide a benchmark on the CUB dataset [48] in the supp. material.
On these datasets, we conduct a systematic study of existing deep-learning-based semi-supervised learning ap-proaches for image classiﬁcation. We perform experiments on SSL methods including Pseudo-Label [25], Curriculum
Pseudo-Label [8], FixMatch [39], self-training using distil-lation [52], self-supervised learning (MoCo [18]), as well as their combinations when effective. We investigate strate-gies for using unlabeled data when models are initialized from experts. We also evaluate the performance of methods that use unlabeled data from the same classes as the labeled dataset (Uin) and a practical setting where the unlabeled data includes out-of-class images (Uin + Uout). The high-level summary of the experiments reported in Fig. 1, Tab. 2, 3, and Fig. 3 are as follows:
• Some of the SSL methods are effective when models are trained from scratch, especially those with self-supervised pre-training can signiﬁcantly beneﬁt from out-of-class data (long blue whiskers and longer orange whiskers above the baseline for scratch in Fig. 1). In this setting, self-supervised learning followed by distillation-based self-training performs the best (Tab. 2 and 3).
• The best SSL approach signiﬁcantly under-performs the supervised ﬁne-tuning model trained on the labeled por-tion of the datasets (the baseline performance of Ima-geNet and iNat is higher than any SSL model trained from scratch in Fig. 1).
• Picking the right expert provides further gains in this few-shot setting but not when training using the entire labeled dataset (oracle performance in Fig. 1).
• When training with experts, FixMatch gives the most im-provements when having Uin only. However, the presence of out-of-class unlabeled data often hurts performance.
Self-Training was the most robust to the presence of out-of-class data (Tab. 2, 3 and Fig. 3).
• Surprisingly, we found that no method was able to reli-ably use out-of-class data even though the domain shift is relatively small (the orange group is not higher than the blue groups for ImageNet and iNat unlike scratch in Fig. 1), echoing the experience of participants in the
FGVC7 challenge [41].
• The performance of SSL is far below the model trained using labels of the entire in-class data suggesting that there is signiﬁcant room for improvement (oracle performance in Fig. 1).
In summary, we conduct a systematic evaluation of sev-eral recently proposed SSL techniques on two challeng-ing datasets representing a long-tailed distribution of ﬁne-grained categories. We vary the initialization and the domain of the unlabeled data and analyze the robustness of vari-ous SSL approaches. Our experiments indicate that SSL does not work out-of-the-box in a transfer learning setting, especially in the presence of out-of-domain data. These results are in a similar vein to prior work on the evalua-tion of SSL approaches that have analyzed the robustness of
SSL techniques to the choice of hyper-parameters [30], net-work architectures [9, 51], and domain shifts [30, 42, 49], etc.
However, the evaluation in a transfer learning setting on the proposed benchmarks reveals additional insights. We hope these experiments inspire practical methods that combine the beneﬁts of supervised learning and task-speciﬁc learning on partially labeled datasets. 12967
2.