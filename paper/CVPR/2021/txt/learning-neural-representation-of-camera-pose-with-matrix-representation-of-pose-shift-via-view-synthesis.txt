Abstract
How to effectively represent camera pose is an essential problem in 3D computer vision, especially in tasks such as camera pose regression and novel view synthesis. Tradi-tionally, 3D position of the camera is represented by Carte-sian coordinate and the orientation is represented by Euler angle or quaternions. These representations are manually designed, which may not be the most effective representa-tion for downstream tasks.
In this work, we propose an approach to learn neural representations of camera poses and 3D scenes, coupled with neural representations of lo-cal camera movements. Speciﬁcally, the camera pose and 3D scene are represented as vectors and the local camera movement is represented as a matrix operating on the vector of the camera pose. We demonstrate that the camera move-ment can further be parametrized by a matrix Lie algebra that underlies a rotation system in the neural space. The vector representations are then concatenated and generate the posed 2D image through a decoder network. The model is learned from only posed 2D images and corresponding camera poses, without access to depths or shapes. We con-duct extensive experiments on synthetic and real datasets.
The results show that compared with other camera pose rep-resentations, our learned representation is more robust to noise in novel view synthesis and more effective in camera pose regression. 1.

Introduction
With the advance of deep neural network (DNN), there has been a series of successful works that employ DNN in camera pose estimation [17, 16, 2, 26, 1, 20] or object pose estimation [5]. In contrast, novel view synthesis is in the opposite direction that maps the camera pose and 3D scene representation back to the posed 2D image under certain view [6, 30]. A fundamental problem in both lines of work is to ﬁnd effective representations of the camera pose [39].
Existing methods include representing the agent’s position in 3D Cartesian coordinate, and the 3D orientation can be represented by Euler angle, axis-angle, SO(3) rotation ma-trices, quaternions or log quaternions. These representa-tions are mainly deﬁned in manually designed coordinates where each dimension has highly abstract semantics, which could be suboptimal when involved in the optimization with deep neural networks. It is desirable to have learning-based representations for camera poses.
Recently,
[8] proposes a representational model of grid cells in the entorhinal cortex of mammalian brains.
Grid cells have been found participating in mental self-navigation and they ﬁre at strikingly regular hexagon grids of positions when the agent moves within an open ﬁeld. The representational model in [8] consists of a vector represen-tation of agent’s self-position, coupled with a matrix repre-sentation of agent’s self-motion. When the agent undergoes a certain self-motion in the 2D space, the vector of self-position is rotated by the matrix of self-motion on a 2D sub-manifold in the mental space. Such a model achieves self-navigation and learns hexagon grid patterns of grid cells, which has the promise to be biologically plausible.
Inspired by [8, 9], we propose an approach towards learning neural representation of camera pose, coupled with representation of local camera movement. Speciﬁcally, given 2D posed images of a 3D scene and their correspond-ing camera poses, we assume a shared vector representation for the underlying 3D scene and a distinct vector representa-tion for the camera pose of each 2D image. When the cam-era has a local displacement, the vector of 3D scene remains unchanged while the vector of camera pose is rotated by the matrix representation of camera movement (Figure 1). We further parametrize the matrix representation by matrix Lie group and the corresponding matrix Lie algebra. The vector representations of camera poses and matrix presentations of camera movements can be shared across multiple scenes, so that they can be learned from multiple scenes to boost performance. The vectors of 3D scene and camera pose are concatenated together to generate the 2D image through a decoder network (Figure 2). The model is learned with only posed 2D images and camera poses, without extra knowl-9959
Figure 1: Illustration of our proposed pose representation. Take axis x as an example. The agent’s position on axis x is mapped to a high dimensional vector and the agent’s movement along axis x is modeled as a rotation of the vector. edge such as depths or shapes. We perform various experi-ments on synthetic and real datasets in the context of novel view synthesis and camera pose regression.
The contributions of our work include: 1. We propose a method for learning neural camera pose representation coupled with neural camera movement representation. 2. We associate this representational model with the agent’s visual input through a generative model. 3. We demonstrate that the learned neural representation is effective as the target representation in camera pose regression. 2.