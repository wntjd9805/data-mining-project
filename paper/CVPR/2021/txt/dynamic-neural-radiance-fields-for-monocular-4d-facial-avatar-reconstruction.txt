Abstract 1.

Introduction
We present dynamic neural radiance ﬁelds for model-ing the appearance and dynamics of a human face1. Digi-tally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduc-tion of the appearance including novel viewpoint or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit rep-resentation of the head based on scene representation net-works. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional mor-phable model which provides explicit control over pose and expressions. We use volumetric rendering to generate im-ages from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a spe-cialized capture setup. In our experiments, we show that this learned volumetric representation allows for photore-alistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods. 1gafniguy.github.io/4D-Facial-Avatars
Reconstructing 4D models of humans and, especially, the human face, is an ongoing research problem in the
ﬁeld of computer vision and computer graphics. 4D avatars are essential for augmented reality (AR) and virtual reality (VR) telepresence applications as well as for video editing, such as visual dubbing in movie productions. These ap-plications need a faithful reconstruction of the human’s ap-pearance, as well as the ability to change the viewpoint or head pose (especially, in VR) and the expressions (e.g., for visual dubbing). Representing a human head with explicit geometry and material properties (e.g., albedo, reﬂectance) is challenging; the skin has effects like subsurface scatter-ing, the eyes are highly reﬂective and the hair has a com-plex geometry with ﬁne scale details. While the explicit reconstruction of high quality geometry of the skin surface in a multi-view studio setup is tractable [4, 16, 52], hair is often approximated by retrieval and reﬁnement of hair styles [18, 50], which leads to an unrealistic visual repro-duction.
To handle the material properties and complex geome-try of a 4D facial avatar, we introduce dynamic neural ra-diance ﬁelds. Our approach is a neural rendering method combining classical volume rendering with a novel neural 8649
scene representation network to achieve novel head pose and expression synthesis.
In contrast to related work on learned scene representations that focuses on static objects and multi-view input data, we are able to represent the dy-namically changing surface of a human’s face only based on monocular camera recordings. The representation is a step-ping stone towards reconstruction of 4D facial avatars using commodity hardware, allowing for novel viewpoint synthe-sis of the head in a virtual reality setting, pose changes in videos or even facial reenactment where the expressions of one person are transferred to another person (represented by our scene representation network). The learned scene rep-resentation is a volumetric representation which is key to capturing hair, but also the mouth interior where classical methods struggle because of missing 3D geometry. The im-plicit representation of the geometry and appearance deﬁnes a continuous function in space that does not suffer from dis-cretization artifacts of voxel grids (e.g., limited resolution) and is optimized to represent the head as good as possi-ble w.r.t. the ﬁnal re-renderings and the underlying network architecture.
In contrast to state-of-the-art facial reenact-ment and video editing approaches [21, 39], our volumetric approach is able to synthesize 3D-consistent content with large head pose changes. Large head pose changes (or view changes) are required for VR or AR applications, but can also be used for face frontalization or to dampen the vari-ance of motion. The semantically meaningful conditioning used in our method also allows for user-driven edits of a video in a post-processing scenario.
Speciﬁcally, our method is based on a short portrait video sequence of a person. To represent the expres-sions of the face, we leverage a low-dimensional morphable model [7, 41]. Given the pose of the model and the ex-pression parameters of a speciﬁc frame of a sequence that has to be synthesized, we dispatch rays in a canonical space where our neural scene representation network is embed-ded. Along the rays, we perform volumetric integration of density and color values predicted by our scene representa-tion network that is inspired by the work of Mildenhall et al. [28], which focuses on high quality multi-view recon-struction of a static scene. Note that the scene representa-tion network is not only conditioned on the sample point lo-cations but also on the expressions of the morphable model which allows for the dynamically changing content that has to be stored in the neural network. During test time, this conditioning allows us to apply novel head poses as well as expressions to synthesize a new image. We demonstrate that our technique is able to faithfully represent a 4D facial avatar and show photorealistic results that surpass state-of-the-art facial reenactment methods.
To summarize, we show that neural scene representation networks can be used to store and represent the dynamically changing surface of a human head in a controllable manner.
Our contributions are:
• Dynamic Neural Radiance Fields to represent 4D fa-cial avatars based on a low dimensional morphable model.
• An efﬁcient end-to-end learnable approach that uses a single camera to reconstruct such a radiance ﬁeld. 2.