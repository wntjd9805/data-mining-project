Abstract
Close
Close
In this paper, we present CLCC, a novel contrastive learn-ing framework for color constancy. Contrastive learning has been applied for learning high-quality visual represen-tations for image classiﬁcation. One key aspect to yield useful representations for image classiﬁcation is to design illuminant invariant augmentations. However, the illuminant invariant assumption conﬂicts with the nature of the color constancy task, which aims to estimate the illuminant given a raw image. Therefore, we construct effective contrastive pairs for learning better illuminant-dependent features via a novel raw-domain color augmentation. On the NUS-8 dataset, our method provides 17.5% relative improvements over a strong baseline, reaching state-of-the-art performance without increasing model complexity. Furthermore, our method achieves competitive performance on the Gehler dataset with 3× fewer parameters compared to top-ranking deep learning methods. More importantly, we show that our model is more robust to different scenes under close proximity of illuminants, signiﬁcantly reducing 28.7% worst-case error in data-sparse regions. Our code is available at https:// github.com/ howardyclo/ clcc-cvpr21. 1.

Introduction
The human visual system can perceive the same canon-ical color of an object even under different illuminants.
This feature can be mimicked by computational color con-stancy, an essential task in the camera pipeline that processes raw sensor signals to sRGB images. Conventional meth-ods [10, 20, 21, 40, 60] utilize statistical properties of the scene to cope with this ill-posed problem, such as the most widely used gray world assumption. Such statistical meth-ods, however, often fail where their assumptions are violated in complex scenes.
* Indicates equal contribution.
Far
Far
Input
Representation
Output
Figure 1: Our main idea of CLCC: The scene-invariant, illuminant-dependent representation of the same scene under different illuminants should be far from each other, while different scenes under the same illuminant should be close to each other.
Until recently, deep learning based methods [31, 48, 66, 67] have been applied to the color constancy problem and achieve considerable quality improvements on challenging scenes. Yet, this ill-posed and sensor-dependent task still suffers from the difﬁculty of collecting massive paired data for supervised training.
When learning with insufﬁcient training data, a com-mon issue frequently encountered is the possibility of learn-ing spurious correlations [62] or undesirable biases from data [59]: misleading features that work for most training samples but do not always hold in general. For instance, previous research has shown that a deep object-recognition model may rely on the spuriously correlated background instead of the foreground object to make predictions [65] or be biased towards object textures instead of shapes [24]. In the case of color constancy, outdoor scenes often have higher correlations with high color temperature illuminants than indoor scenes. Thus, deep learning models may focus on scene related features instead of illuminant related features. 8053
This leads to a decision behavior that tends to predict high color temperature illuminants for outdoor scenes, but suffers high error on outdoor scenes under low color temperature illuminants. This problem becomes worse when the sparsity of data increases.
To avoid learning such spurious correlations, one may seek to regularize deep learning models to learn scene-invariant, illuminant-dependent representations. As illus-trated in Fig.1, in contrast to image classiﬁcation problem, the representation of the same scene under different illu-minants should be far from each other. On the contrary, the representation of different scenes under the same illumi-nant should be close to each other. Therefore, we propose to learn such desired representations by contrastive learn-ing [13, 27, 30], a framework that learns general and robust representations by comparing similar and dissimilar samples.
However, conventional self-supervised contrastive learn-ing often generates easy or trivial contrastive pairs that are not very useful for learning generalized feature represen-tations [37]. To address this issue, a recent work [13] has demonstrated that strong data augmentation is crucial for conducting successful contrastive learning.
Nevertheless, previous data augmentations that have been shown effective for image classiﬁcation may not be suitable for color constancy. Here we illustrate some of them. First, most previous data augmentations in contrastive learning are designed for high-level vision tasks (e.g., object recognition) and seek illuminant invariant features, which can be detri-mental for color constancy. For example, color dropping converts an sRGB image to a gray-scale one, making the color constancy task even more difﬁcult. Moreover, the color constancy task works best in the linear color space where the linear relationship to scene radiance is preserved. This prevents from using non-linear color jittering augmentations, e.g., contrast, saturation, and hue.
To this end, we propose CLCC: Contrastive Learning for
Color Constancy, a novel color constancy framework with contrastive learning. For the purpose of color constancy, ef-fective positive and negative pairs are constructed by exploit-ing the label information, while novel color augmentations are designed based on color domain knowledge [2, 49, 36].
Built upon a previous state-of-the-art [31], CLCC pro-vides additional 17.5% improvements (mean angular er-ror decreases from 2.23 to 1.84) on a public benchmark dataset [14], achieving state-of-the-art results without in-creasing model complexity. Besides accuracy improvement, our method also allows deep learning models to effectively acquire robust and generalized representations even when learning from small training datasets.
Contribution We introduce CLCC, a fully supervised con-trastive learning framework for the task of color constancy.
By leveraging label information, CLCC generates more di-verse and harder contrastive pairs to effectively learn feature representations aiming for better quality and robustness. A novel color augmentation method that incorporates color do-main knowledge is proposed. We improve the previous state-of-the-art deep color constancy model without increasing model complexity. CLCC encourages learning illuminant-dependent features rather than spurious scene content fea-tures irrelevant for color constancy, making our model more robust and generalized, especially in data-sparse regions. 2.