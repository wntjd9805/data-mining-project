Abstract
Active Learning for discriminative models has largely been studied with the focus on individual samples, with less emphasis on how classes are distributed or which classes are hard to deal with. In this work, we show that this is harmful. We propose a method based on the Bayes’ rule, that can naturally incorporate class imbalance into the Ac-tive Learning framework. We derive that three terms should be considered together when estimating the probability of a classiﬁer making a mistake for a given sample; i) probability of mislabelling a class, ii) likelihood of the data given a pre-dicted class, and iii) the prior probability on the abundance of a predicted class. Implementing these terms requires a generative model and an intractable likelihood estimation.
Therefore, we train a Variational Auto Encoder (VAE) for this purpose. To further tie the VAE with the classiﬁer and facilitate VAE training, we use the classiﬁers’ deep feature representations as input to the VAE. By considering all three probabilities, among them, especially the data imbalance, we can substantially improve the potential of existing meth-ods under limited data budget. We show that our method can be applied to classiﬁcation tasks on multiple different datasets – including one that is a real-world dataset with heavy data imbalance – signiﬁcantly outperforming the state of the art.
Figure 1. Class matters – We show an example where existing methods fail to identify important samples to put into the updated training set. We show the importance metrics given by various methods – DBAL [10], CoreSet [38], Max. Entropy [45], and our method – for selected images when the dataset is dominated by plane images. We further show below each image, the ground-truth class y and the predicted class y. Because of the data imbalance, b existing methods heavily favour the plane class, even when the results are correct, as seen on the left. And for rare classes in the right column, they fail to recognise that the samples are important, which leads to the improved performance. On the other hand, our method correctly identiﬁes them as important, by considering also the class difﬁculty and the class prior. 1.

Introduction
Active learning focuses on efﬁcient labelling of data and has drawn much interest lately [38,41,48], due to deep learn-ing being attempted at new domains, such as biomedical imaging [3, 16] and industrial imaging [27, 49], where ac-* These authors contributed equally to this work. quiring data can be costly [39]. Even for cases where data is not scarce, the effective usage of data may reduce train-ing time, therefore the computational cost, including carbon foot-prints required to train each model. There have been various studies based on semi-supervised [7, 21] and unsu-pervised [37, 46] learning schemes to improve the training efﬁciency of data. However, with limited labelling budget, the performance of the studies are signiﬁcantly worse to the 6749
supervised learning with the additionally labelled data [35].
In other words, their label efﬁciency could be improved. during training, is critical when analysing the uncertainty of estimates.
Existing methods [4, 10, 38, 41, 48], regardless of how they are formulated, have a common underlying assumption that all classes are equal – they do not consider that some classes might just be harder to learn compared to others, or some classes might be more prevalent in the dataset than others. Instead, they focus on, given a data sample, how much error a trained model is expected to make, or the estimated uncertainties [10, 48]. These assumptions could be harmful, as in practice, since data is often imbalanced and not all classes are of the same difﬁculty [1, 51]. This can create a bias in the labelled data pool, leading to the trained classiﬁer and active learning methods also being biased in deciding which samples to the label. As we show in Fig. 1, this can damage the capabilities of an active learning method signiﬁcantly, even for typical benchmark datasets [6, 22].
In this work, we present a novel formulation for active learning, based on the classical Bayes’ rule that allows us to incorporate multiple factors of a classiﬁcation network together. Through derivation, we show that the probability of a classiﬁer making mistakes can be decomposed into three terms; i) the probability of misclassiﬁcation for a given pre-dicted class, ii) the likelihood of a sample given predicted class, and iii) the prior probability of a class being predicted.
In other words, one needs to take into account i) the difﬁ-culty of a class, ii) the performance of the classiﬁer and iii) the abundance of a certain class of data holistically when determining the potential of a classiﬁcation error. We take all of them into account and choose samples to be labelled by selecting those that have the highest misclassiﬁcation probability.
While the task is discriminative, our method requires the estimation of likelihood, which could be intractable.
We, therefore, propose to use a Variational Auto Encoder (VAE) [20] to model the lower bound of the likelihood of a sample. To make VAE conditioned on a predicted label, a naive way would be applied to train multiple VAEs for each predicted class. However, this quickly becomes impractical with a large number of classes. We thus propose to train a single VAE, with regularisation that acts as conditioning based on the predicted label. To further tie the VAE with the classiﬁer, and for quick training of the VAE, we use the deep feature representations of the classiﬁer as inputs to the VAE.
Being generative, this training of VAE does not involve any labels, and we utilise the vast amount of unlabelled data with their predicted labels while inferring probabilities that are independent of data samples on the labelled ones.
We show empirically in Section 4 that our method al-lows a signiﬁcant leap in performance for the benchmark dataset and the real application dataset, especially when the labelling budget is highly limited. Furthermore, we posit that considering the prior, that is, the distribution of labels
In summary, our contributions are four-fold:
• we derive a novel formulation for active learning based on the Bayes’ rule and posterior probability;
• we propose a framework based on VAE that realises this formulation;
• we reveal that considering the differences between classes – abundance and difﬁculty – is important;
• we outperform the state of the art in the various experi-ments. 2.