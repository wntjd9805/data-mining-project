Abstract
Machine learning classiﬁers are critically prone to eva-sion attacks. Adversarial examples are slightly modiﬁed inputs that are then misclassiﬁed, while remaining percep-tively close to their originals. Last couple of years have witnessed a striking decrease in the amount of queries a black box attack submits to the target classiﬁer, in order to forge adversarials. This particularly concerns the black box score-based setup, where the attacker has access to top predicted probabilites: the amount of queries went from to millions of to less than a thousand.
This paper presents SurFree, a geometrical approach that achieves a drastic reduction in the amount of queries in the hardest setup: black box decision-based attacks (only the top-1 label is available). We ﬁrst highlight that the most recent attacks in that setup, HSJA [3], QEBA [14] and GeoDA [23] all perform costly gradient surrogate es-timations. SurFree proposes to bypass these, by instead focusing on careful trials along diverse directions, guided by precise indications of geometrical properties of the clas-siﬁer decision boundaries. We motivate this geometric ap-proach before performing a head-to-head comparison with previous attacks with the amount of queries as a ﬁrst class citizen. We exhibit a faster distortion decay under low query amounts (few hundreds to a thousand), while remaining competitive at higher query budgets.1 1.

Introduction
The literature on adversarial examples is divided into two shares, depending on the threat model: either the attacker has full knowledge of the target classiﬁer [2, 26, 17] (white-box setting) or she/he has an unrestricted query access to the unknown classiﬁer [18, 1, 14, 23, 3, 28, 13, 27, 11, 5, 12, 4] (black-box setting). The latter scenario is deemed as more relevant to gauge the intrinsic robustness of classiﬁers in 1Work supported by ANR / AID under Chaire IA SAIDA. Code avail-able at https://github.com/t-maho/SurFree
SurFree
GeoDA [23]
QEBA [14] 10-HSJA [3] 70 60 50 40 30 20 10 n o i t r o t s i
D 0 1000 3000 2000
Number of queries 4000 5000
Figure 1. The perturbation distortion (ℓ2 norm) vs. the number of queries for image ‘lizard’. Competitor attacks waste queries to estimate a gradient surrogate resulting in plateaus of distortion. real-world applications (typically queried through an API).
Black box attacks are iterative procedures that keep on reﬁning the quality of an adversarial example based on the pairs of submitted input / observed output. They are coined score-based when the attacker observes the top-k predicted probabilities or decision-based (a.k.a. hard label) when she/he only learns the top-k labels (k
Indeed, the latter case where the output is solely the top-1 label is the most challenging because the attacker cannot rely on any rich information for crafting these adversarial examples. 1).
≥
It is striking that black-box attacks always use substitu-tion to replace information they are missing. Early black-box attacks used a surrogate model (trained from a huge number of input / output pairs) mimicking the targeted model [20, 21]. The attack then boils down to a white-box setting on the surrogate with the hope that the adversarial example transfers to the target classiﬁer. Almost all recent score-based attacks resort to gradient estimation to compen-sate for the lack of back-propagation, which is the key in-strument of any white-box attack [4, 27, 13, 28]. The Hop-SkipJump attack (HSJA [3]) estimates the decision bound-ary by an hyperplane. As a last example, authors in [12] turn a decision-based setup into a score-based by probing noisy versions of an image to derive a score-like function from the top-k labels. The trend is thus to substitute missing infor-10430
mation by estimates in order to fall back to an easier setup.
The need for faster attacks consuming fewer queries is already present in the literature. Most notably, research works on score-based attacks managed to reduce query amount from millions of requests [12] to less than a thou-sand with most recent approaches [28]. Surprisingly, this impressive decrease has not reached comparable levels in the hard-label setup. In particular, paper [12] questions the model surrogate approach: while a considerable amount of queries is spent for training the surrogate, not a single ad-versarial example is forged. Moreover, access to the target model in practice is usually not free and not unlimited2.
This argument should challenge any substitution mecha-nisms. They all consume a fair amount of queries and it is not clear whether they are worth the gain in term of distor-tion. Especially, many techniques trade some query amount for an accurate gradient estimate giving birth to good per-turbation directions [23, 3]. During this step, the adversarial is not updated and the distortion stalls as Fig. 1 shows.
This paper considers the query amount as a central cri-terion. It presents a fast black-box decision-based attack, named SurFree, motivated by practical applications in which a low amount of queries is key. Fast means that it out-performs the state of the art when it comes to the distortion of adversarials under a low query budget (as exampliﬁed in
Fig. 1 with the purple curve).
The main contributions of this paper are:
• SurFree, a black box decision-based attack not us-ing any substitution mechanism: no surrogate of the target model, no score reconstruction, no estimation of gradient. It is inspired by the early works [8, 1].
• a geometrical mechanism to get the biggest distortion decrease for a given direction to be explored under the assumption of a hyperplane boundary [10].
• a head to head comparison of the recent approaches with distortion as a function of query number.
Experimental results show that SurFree overcomes state of the art on the query amount factor (a thousand of queries), while still remaining competitive with unlimited queries (normal scenario for competitors). 2.