Abstract
Unsupervised representation learning with contrastive learning achieved great success. This line of methods du-plicate each training batch to construct contrastive pairs, making each training batch and its augmented version for-warded simultaneously and leading to additional computa-tion. We propose a new jigsaw clustering pretext task in this paper, which only needs to forward each training batch it-self, and reduces the training cost. Our method makes use of information from both intra- and inter-images, and outper-forms previous single-batch based ones by a large margin.
It is even comparable to the contrastive learning methods when only half of training batches are used.
Our method indicates that multiple batches during train-ing are not necessary, and opens the door for future re-search of single-batch unsupervised methods. Our mod-els trained on ImageNet datasets achieve state-of-the-art results with linear classiﬁcation, outperforming previous single-batch methods by 2.6%. Models transferred to
COCO datasets outperforms MoCo v2 by 0.4% with only half of the training batches. Our pretrained models outper-form supervised ImageNet pretrained models on CIFAR-10 and CIFAR-100 datasets by 0.9% and 4.1% respectively. 1.

Introduction
Unsupervised visual representation learning, or self-supervised learning, is a long-standing problem, which aims at obtaining general feature extractors without human su-pervision. This goal is usually achieved by carefully de-signing pretext tasks without annotation to train feature ex-tractors.
According to the deﬁnition of pretext tasks, most main-stream approaches fall into two classes: intra-image tasks and inter-images tasks. Intra-image approaches, including colorization [43, 20] and jigsaw puzzle [29], design a trans-form of one image and train the network to learn the trans-form. Since only the training batch itself is forwarded each time, we name them single-batch methods. This kind of
Figure 1. Sketch of our proposed pretext task.
Images in the same batch are split into multiple patches, which are shufﬂed and stitched to form a new batch as input images for the network. The target is to recover the batch similar to the original images. We use two images here as an example. tasks can be achieved using only one image’s information, limiting the learning ability of feature extractors.
Inter-images tasks are developed rapidly in recent years, which require the network to discriminate among different images. Contrastive learning is popular now since it reduces the distance between representation of positive pairs and en-larges the distance between representation of negative pairs.
To construct positive pairs, another batch of images with different augmented views are used in the training process
[5, 15, 26]. Since each training batch and its augmented ver-sion are forwarded simultaneously, we name these methods dual-batches methods. They greatly raise resource required for training an unsupervised feature extractor. The way to design an efﬁcient single-batch based method with similar performance to dual-batches methods is still an open prob-lem.
In this paper, we propose a framework for efﬁcient train-ing of unsupervised models using Jigsaw Clustering (Jig-Clu). Our method combines advantages of solving jig-saw puzzles and contrastive learning, and makes use of both intra- and inter-image information to guide feature
It learns more comprehensive representations. extractor.
Our method only needs a single batch during training and yet greatly improves results compared to other single-batch 11526
methods. batches methods with only half of the training batches.
It even achieves comparable results with dual-however, ignored in recent methods of [5, 26, 15, 6, 22].
We note this piece of information is still important to fur-ther improve results.
Jigsaw Clustering Task In our proposed Jigsaw Clus-tering task, every image in a batch is split into different patches. They are randomly permuted and stitched to form a new batch for training. The goal is to recover these dis-rupted parts back to the original images, as shown in Figure 1. Different from [29], the patches are permuted in a batch instead of a single image. The image each patch belongs to and the location of each patch in the origin are predicted in our work.
Also, we use montage images instead of single patches as input of the network. This modiﬁcation greatly improves the difﬁculty for the task of [29] and provides more use-ful information for the network to learn. The network now has to distinguish between different parts of one image and identiﬁes their original positions to recover the original im-age from multiple montage input images.
This task allows the network to learn both intra- and inter-images information by only forwarding the stitched images, using half of the training batches compared to other contrastive learning methods.
To recover patches across images, we design a clustering branch and a location branch as shown in Figure 2. Specif-ically, we ﬁrst decouple the global feature map of stitched images into the representation of each patch. Then these two branches operate on representation of each patch. The clustering branch is to separate these patches into clusters, each of which only contains patches from the same image.
The location branch, on the other hand, predicts location of every patch in an image agnostic manner.
With prediction from these two branches, the Jigsaw
Clustering problem is solved. The clustering branch is trained as a supervised clustering task since we know the patches are from the same image, or not. The location branch is considered as a classiﬁcation problem, where each patch is assigned with a label to indicate its location in the origin image. This branch predicts the label of every patch.
The reason that our method achieves decent results is that models trained with our proposed task can learn different kinds of information. At ﬁrst, discriminating among differ-ent patches in one stitched image forces the model to cap-ture instance-level information inside an image. This level of feature is missing in general in other contrastive learning methods.
Further, clustering different patches from multiple input images helps the model learn image-level features across images. This is the key that recent methods [15, 6, 5] achieve high-quality results. Our method retain this impor-tant property. Finally, arranging every patch to the correct location requires detailed location information, which was considered in single-batch methods [29, 43] before. It is,
Performance of Our Method Learning by our method yields both intra- and inter-images information. This com-prehensive learning brings a spectrum of superiority. First, with only one batch during training, our method outper-forms other single-batch ones by 2.6% on linear evalua-tion on the ImageNe-1k dataset. Second, our method is more data-efﬁcient. When the training data size is not large, our method can still produce decent results, much better than many other existing ones. On the ImageNet-100 and
ImageNet-10% datasets, our system outperforms MoCo v2 by 6.2% and 6.0% respectively. Our method also converges more quickly with less training time. We use only a quarter of epochs of MoCo v2 to achieve the same results on the
ImageNet-100 dataset.
Finally, the comprehensive information learned by our models is suitable for many other vision tasks. On the
COCO detection dataset, our result is 0.4% better than
MoCo v2, with only half of training batches. On the
CIFAR-10 and CIFAR-100 datasets, models tuned with our pretrained weights achieve 0.9% and 4.1% higher results than that with supervised training weights, respectively.
The extensive experiments demonstrate the superiority of our proposed pretext method. 2.