Abstract
This paper deals with the scarcity of data for training op-tical ﬂow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Speciﬁcally, we introduce a framework to generate accurate ground-truth optical ﬂow annotations quickly and in large amounts from any readily available single real pic-ture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vec-tors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical ﬂow ﬁeld con-necting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art opti-cal ﬂow networks achieve superior generalization to unseen real data compared to the same models trained either on an-notated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images. 1.

Introduction
The problem of estimating per-pixel motion between video frames, also known as optical ﬂow [50], has a long history in computer vision and remains far from being solved. On top of it, several higher-level tasks such as track-ing, action recognition and more are typically performed.
Among the main challenges for optical ﬂow systems, there are occlusions, motion blur and lack of texture.
Deep learning has played a crucial role in the latest years of research on this topic, at ﬁrst to learn a data term [1, 65] and then to directly infer the dense optical ﬂow ﬁeld in end-to-end manner [7, 20, 51, 52, 18, 19, 17, 53], currently representing the state-of-the-art in this ﬁeld. This achieve-ment has been made possible by the availability of exten-sive training data labeled with ground-truth optical ﬂow
ﬁelds, most of them obtained through computer graphics
∗Joint ﬁrst authorship. a) b) c) d)
Figure 1. Depthstillation from still images. From left to right: a) single input image, b) estimated depth map, c) optical ﬂow ﬁeld consequence of virtual camera motion, d) virtual view. We show b) as inverse depth to improve visualization.
[5, 7, 20]. Unfortunately, these large datasets alone are not enough to train a neural network for its deployment in real environments, because of the well-known domain shift oc-curring when moving from synthetic images to real ones.
A notable example is represented by the KITTI optical
ﬂow benchmarks [10, 35], over which deep networks that have been trained only on synthetic data perform poorly, as witnessed by recent works [20, 51, 53]. This problem is known in literature and has been faced for other tasks such as semantic segmentation [15, 39, 43, 55] or stereo depth estimation [56, 57, 68, 61]. To fully restore a level of accuracy comparable to the one achieved on synthetic data, ﬁne-tuning on imagery similar to the testing domain is usually required. Anyway, obtaining ground-truth optical
ﬂow labels for real images is particularly challenging be-cause there exists virtually no sensor capable of acquiring ground-truth correspondences between points in challeng-ing real-world scenes [37]. A viable strategy consists into passing through depth sensors (e.g., LiDARs), indeed opti-cal ﬂow ﬁelds can be obtained by projecting the 3D points from a given frame into the next frame [10], although it cannot take into account independently moving objects, for which manual post-processing or annotation remains neces-sary [35, 37]. The literature is rich of self-supervised strate-15201
gies [34, 30, 28, 23] from unlabeled videos to soften this constraint, but they mostly excel when deployed on data similar to those observed for training, a scenario unlikely to occur in most real applications.
Given both the aforementioned domain shift issue and the lack of real imagery annotated for optical ﬂow, we pro-pose an alternative scheme to distill proxy labels from real images for effective training of optical ﬂow estimation net-works. Following the observation that depth is required to obtain dense matching across views through reprojection
[10, 35, 37], we use a monocular depth estimation network to revert the annotation process: given a single image and its estimated depth, we suppose a virtual motion of the cam-era to compute a dense optical ﬂow ﬁeld and, consequently, synthesize a new virtual image accordingly. For instance, in Figure 1 from a) pictures of a person and a cat, we esti-mate b) monocular depth and generate c) a ﬂow ﬁeld used to synthesize d) a novel view. We dub this process Depthstilla-tion, and any single image is eligible for producing optical
ﬂow annotations through it.
Experiments carried out on synthetic (Sintel) and real (KITTI 2012 and 2015) datasets support our main claims:
• We show that it is possible to train an optical ﬂow net-work on a collection of unrelated images, e.g. single pictures readily available online
• Using real images through our technique allows us to train networks that better transfer to real data than their counterparts trained on synthetic images, while ﬁne-tuning these latter on dephtstilled frames and then on real data improves specialization
• Networks trained on our dephtstilled frames and ﬂow labels better transfer to new real datasets than state-of-the-art self-supervised strategies using real videos [23] 2.