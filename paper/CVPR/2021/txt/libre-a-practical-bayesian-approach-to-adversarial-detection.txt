Abstract
Despite their appealing ﬂexibility, deep neural networks (DNNs) are vulnerable against adversarial examples. Vari-ous adversarial defense strategies have been proposed to re-solve this problem, but they typically demonstrate restricted practicability owing to unsurmountable compromise on uni-versality, effectiveness, or efﬁciency. In this work, we pro-pose a more practical approach, Lightweight Bayesian Re-ﬁnement (LiBRe), in the spirit of leveraging Bayesian neu-ral networks (BNNs) for adversarial detection. Empow-ered by the task and attack agnostic modeling under Bayes principle, LiBRe can endow a variety of pre-trained task-dependent DNNs with the ability of defending heteroge-neous adversarial attacks at a low cost. We develop and integrate advanced learning techniques to make LiBRe ap-propriate for adversarial detection. Concretely, we build the few-layer deep ensemble variational and adopt the pre-training & ﬁne-tuning workﬂow to boost the effectiveness and efﬁciency of LiBRe. We further provide a novel in-sight to realise adversarial detection-oriented uncertainty quantiﬁcation without inefﬁciently crafting adversarial ex-amples during training. Extensive empirical studies cover-ing a wide range of scenarios verify the practicability of Li-BRe. We also conduct thorough ablation studies to evidence the superiority of our modeling and learning strategies.1 1.

Introduction
The blooming development of deep neural networks (DNNs) has brought great success in extensive industrial applications, such as image classiﬁcation [23], face recog-nition [9] and object detection [49]. However, despite their promising expressiveness, DNNs are highly vulnera-ble to adversarial examples [56, 19], which are generated by adding human-imperceptible perturbations upon clean ex-amples to deliberately cause misclassiﬁcation, partly due to their non-linear and black-box nature. The threats from ad-*Corresponding author 1Code at https://github.com/thudzj/ScalableBDL.
Bayesian   sub-module
• benign 
• adversarial
Deterministic layers task-dependent   prediction accept/reject
Figure 1: Given a pre-trained DNN, LiBRe converts its last few layers (excluding the task-dependent output head) to be Bayesian, and reuses the pre-trained parameters. Then, LiBRe launches several-round adversarial detection-oriented ﬁne-tuning to render the posterior effective for prediction and meanwhile appropriate for adversarial detection. In the inference phase, LiBRe estimates the predictive uncertainty and task-dependent predictions of the input concurrently, where the former is used for adversarial detec-tion and determines the ﬁdelity of the latter. versarial examples have been witnessed in a wide spectrum of practical systems [51, 12], raising an urgent requirement for advanced techniques to achieve robust and reliable deci-sion making, especially in safety-critical scenarios [13].
Though increasing methods have been developed to tackle adversarial examples [41, 67, 25, 18, 66], they are not problemless. On on hand, as one of the most popu-lar adversarial defenses, adversarial training [41, 67] intro-duces adversarial examples into training to explicitly tailor the decision boundaries, which, yet, causes added training overheads and typically leads to degraded predictive per-formance on clean examples. On the other hand, adversar-ial detection methods bypass the drawbacks of modifying the original DNNs by deploying a workﬂow to detect the adversarial examples ahead of decision making, by virtue of auxiliary classiﬁers [43, 18, 66, 5] or designed statis-tics [14, 39]. Yet, they are usually developed for speciﬁc tasks (e.g., image classiﬁcation [66, 31, 18]) or for speciﬁc adversarial attacks [38], lacking the ﬂexibility to effectively generalize to other tasks or attacks.
By regarding the adversarial example as a special case of out-of-distribution (OOD) data, Bayesian neural net-works (BNNs) have shown promise in adversarial detec-tion [14, 37, 53]. In theory, the predictive uncertainty ac-quired under Bayes principle sufﬁces for detecting hetero-972
geneous adversarial examples in various tasks. However, in practice, BNNs without a sharpened posterior often present systematically worse performance than their deterministic counterparts [60]; also relatively low-cost Bayesian infer-ence methods frequently suffer from mode collapse and hence unreliable uncertainty [15]. BNNs’ requirement of more expertise for implementation and more efforts for training than DNNs further undermine their practicability.
In the work, we aim to develop a more practical adversar-ial detection approach by overcoming the aforementioned issues of BNNs. We propose Lightweight Bayesian Reﬁne-ment (LiBRe), depicted in Fig. 1, to reach a good balance among predictive performance, quality of uncertainty esti-mates and learning efﬁciency. Concretely, LiBRe follows the stochastic variational inference pipeline [2], but is em-powered by two non-trivial designs: (i) To achieve efﬁcient learning with high-quality outcomes, we devise the Few-lAyer Deep Ensemble (FADE) variational, which is reminis-cent of Deep Ensemble [30], one of the most effective BNN methods, and meanwhile inspired by the scalable last-layer
Bayesian inference [28]. Namely, FADE only performs deep ensemble in the last few layers of a model due to their crucial role for determining model behaviour, while keeps the other layers deterministic. To encourage various ensem-ble candidates to capture diverse function modes, we de-velop a stochasticity-injected learning principle for FADE, which also beneﬁts to reduce the gradient variance of the (ii) To further ease and accelerate the learn-parameters. ing, we propose a Bayesian reﬁnement paradigm, where we initialize the parameters of FADE with the parameters of its pre-trained deterministic counterpart, thanks to the high alignment between FADE and point estimate. We then per-form ﬁne-tuning to constantly improve the FADE posterior.
These designs make the whole learning procedure analo-gous to training a standard DNN, freeing the end users from the piecemeal details of Bayesian learning.
As revealed by [22], the uncertainty quantiﬁcation purely acquired from Bayes principle may be unreliable for per-ceiving adversarial examples, thus it is indispensable to pur-sue an adversarial detection-oriented uncertainty correction.
For universality, we place no assumption on the adversarial examples to detect, so we cannot take the common strat-egy of integrating the adversarial examples crafted by spe-ciﬁc attacks into detector training [39]. Alternatively, we cheaply create uniformly perturbed examples and demand high predictive uncertainty on them during Bayesian reﬁne-ment to make the model be sensitive to data with any style of perturbation. Though such a correction renders the learned posterior slightly deviated from the true Bayesian one, it can signiﬁcantly boost adversarial detection performance.
The task and attack agnostic designs enable LiBRe to quickly and cheaply endow a pre-trained task-dependent
DNN with the ability to detect various adversarial examples when facing new tasks, as testiﬁed by our empirical studies in Sec 5. Furthermore, LiBRe has signiﬁcantly higher in-ference (i.e., testing) speed than typical BNNs thanks to the adoption of lightweight variational. We can achieve further speedup by exploring the potential of parallel computing, giving rise to inference speed close to the DNN in the same setting. Extensive experiments in scenarios ranging from image classiﬁcation, face recognition, to object detection conﬁrm these claims and testify the superiority of LiBRe.
We further perform thorough ablation studies to deeply un-derstand the adopted modeling and learning strategies. 2.