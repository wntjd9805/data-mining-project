Abstract
Knowledge distillation transfers knowledge from the teacher network to the student one, with the goal of greatly improving the performance of the student network. Previ-ous methods mostly focus on proposing feature transforma-tion and loss functions between the same level’s features to improve the effectiveness. We differently study the factor of connection path cross levels between teacher and stu-dent networks, and reveal its great importance. For the ﬁrst time in knowledge distillation, cross-stage connection paths are proposed. Our new review mechanism is effective and structurally simple. Our ﬁnally designed nested and com-pact framework requires negligible computation overhead, and outperforms other methods on a variety of tasks. We apply our method to classiﬁcation, object detection, and in-stance segmentation tasks. All of them witness signiﬁcant student network performance improvement. 1.

Introduction
Deep convolution neural networks (CNNs) have achieved remarkable success in a variety of computer vision tasks. However, the success of CNN is often accompanied with considerable computation and memory consumption, making it a challenging topic to apply to devices with limited resource. There have been techniques for training fast and compact neural networks, including designing new architectures [10, 2, 11, 26], network prun-ing [20, 15, 34, 4, 19], quantization [13] , and knowledge distillation [9, 25].
We focus on knowledge distillation in this paper consid-ering its practicality, efﬁciency, and most importantly the potential to be useful. It forms a very general line, appli-cable to almost all network architectures and can combine with many other strategies, such as network pruning and quantization [32], to further improve network design.
Knowledge distillation is ﬁrst proposed in [9]. The pro-cess is to train a small network (also known as the stu-dent) under the supervision of a larger network (a.k.a. the teacher). In [9], knowledge is distilled though the teacher’s logit, which means the student is supervised by both ground truth labels and teacher’s logits. Recently, effort has been made to improve distillation effectiveness. FitNet [25] dis-tilled knowledge though intermediate features. AT [38] fur-ther optimized FitNet and used the attention map of features to deliver knowledge. PKT [23] modeled knowledge of the teacher as a probability distribution while CRD [28] used a contrastive objective to transfer knowledge. All these solu-tions focused on transformation and loss functions.
Our New Finding We in this paper tackle this challeng-ing problem from a new perspective regarding the connec-tion path between the teacher and student. To brieﬂy un-derstand our idea, we ﬁrst show how previous work deals with these paths. As shown in Figure 1(a)-(c), all previ-ous methods only use the-same-level information to guide the student. For example, when supervising the student’s fourth-stage output, always the teacher’s fourth-stage infor-mation is utilized. This procedure looks intuitive and easy to construct. But we intriguingly reveal that it is in fact a bottleneck in the whole knowledge distillation framework – quick update of the structure surprisingly improves the whole-system performance consistently for many tasks.
We investigate the previously neglected importance of designing connection paths in knowledge distillation and propose a new effective framework accordingly. The key modiﬁcation is to use low-level features in the teacher net-work to supervise deeper features for the student, which re-sults in much improved overall performance.
We further analyze the network structure and discover the fact that the student high-level stage has the great capac-ity to learn useful information from the teacher’s low-level features. More analysis is provided in Section 4.4. This process is analogous to human learning curve [35] where a young kid can only comprehend a small portion of knowl-edge that is taught. During the course of grow-up, more and more knowledge from past years may be gradually under-stood and remembered as experience.
Our Knowledge Review Framework Based on these discoveries, we propose to use multi-level information of the teacher to guide one-level learning of the student net-5008
truth labels and the soft-labels provided by the teacher. Fit-Net [25] distilled knowledge through one stage intermediate feature. The idea in FitNet is simple, where the student net-work feature is transferred to the same shape of the teacher though convolution layers. L2 distance is used to measure the distance between them.
Many methods follow FitNet and use one-stage feature to distill knowledge. PKT [23] modeled knowledge of the teacher as a probability distribution and used KL divergence to measure the distance. RKD [22] used multiple example relation to guide learning of the student. CRD[28] com-bined contrastive learning and knowledge distillation, and used a contrastive objective to transfer knowledge.
There are also methods using multi-stage information to transfer knowledge. AT [38] used multiple layer attention maps to transfer knowledge. FSP [36] generated FSP matrix from layer feature and used the matrix to guide the student.
SP [29] further improved AT. Instead of single input infor-mation, SP uses the similarity between examples to guide the student. OFD [8] contained a new distance function to distill major information between the teacher and student using marginal ReLU.
All previous methods do not discuss the possibility to
“review knowledge”, which, however, is found in our work very effective to quickly improve system performance. 3. Our Method
We ﬁrst formalize the knowledge distillation process and the review mechanism. Then we propose a novel framework and introduce attention based fusion module and hierarchi-cal context loss function. 3.1. Review Mechanism
Given an input image X and student network S, we let
Ys = S(X) represent the output logit of the student. S can be separated into different parts (S1, S2, · · · , Sn, Sc), where Sc is the classiﬁer and S1, · · · , Sn are different stages separated by downsample layers. Thus, the process of generating output Ys can be denoted as
Ys = Sc ◦ Sn ◦ · · · ◦ S1(X). (1)
We refer to “◦” as nesting of functions where g ◦ f (x) = g(f (x)). Ys is the output of student, and intermidate fea-tures are (F1 s ). The ith feature is calculated as s, · · · , Fn
Fi s = Si ◦ · · · ◦ S1(X). (2)
For the teacher network T , the process is almost the same and we omit the details. Following previous notations, single-layer knowledge distillation can be represented as
Figure 1. (a)-(c) Previous knowledge distillation frameworks.
They only transfer knowledge within the same levels. (d) Our pro-posed “knowledge review” mechanism. We use multiple layers of the teacher to supervise one layer in the student. Thus, knowledge passing arises among different levels. work. Our novel pipeline is shown in Figure 1(d), which we call “knowledge review”. The review mechanism is to use previous (shallower) features to guide the current fea-ture. It means a student has to always check what has been studied before for refreshing understanding and context of
“old knowledge”. It is a common practice for our human study to connect knowledge taught at different stages dur-ing a period of time of study.
However, how to extract useful information from multi-level information from the teacher and how to transfer them to the student are open and challenge problems. To tackle them, we propose a residual learning framework to make the learning process stable and efﬁcient. Further, a novel at-tention based fusion (ABF) module and a hierarchical con-text loss (HCL) function are designed to boost performance.
Our proposed framework makes the student network much improve the effectiveness of learning.
By applying this idea, we achieve better performance in many computer vision tasks. Extensive experiments in Sec. 4 manifest the vast advantage of our proposed knowledge review strategy.
Main Contributions
• We propose a new review mechanism in knowledge distillation, utlizing multi-level information of the teacher to guide one-level learning of the student net.
• We propose a residual learning framework to better re-alize the learning process of the review mechanism.
• To further improve the knowledge review mechanism, we propose an attentation based fusion (ABF) module and a hierarchical context loss (HCL) function.
• We achieve state-of-the-art performance of many com-pact models in multiple computer vision tasks by ap-plying our distillation framework. 2.