Abstract
Recent work [28, 5] has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for chal-lenging scenes that mesh reconstruction fails on. How-these methods entangle geometry and appearance ever, in a “black-box” volume that cannot be edited.
In-stead, we present an approach that explicitly disentangles geometry—represented as a continuous 3D volume—from appearance—represented as a continuous 2D texture map.
We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric rep-resentations. We constrain this texture mapping network us-ing an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be re-constructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps. 1.

Introduction
Capturing and modeling real scenes from image inputs is an extensively studied problem in vision and graphics.
One crucial goal of this task is to avoid the tedious process of manual 3D modeling and directly build a renderable and editable 3D model that can be used for realistic rendering in applications like e-commerce, VR and AR. Traditional 3D reconstruction methods [39, 40, 20] usually reconstruct objects as meshes. Meshes are widely used in rendering pipelines; they are typically combined with mapped textures for appearance editing in 3D modeling pipelines.
However, mesh-based reconstruction is particularly chal-lenging and often cannot synthesize highly realistic images for complex objects. Recently, various neural scene rep-resentations have been presented to address this scene ac-quisition task. Arguably the best visual quality is obtained
Research partially done when F. Xiang was an intern at Adobe Research.
Figure 1. NeuTex is a neural scene representation that represents geometry as a 3D volume but appearance as a 2D neural texture in an automatically discovered texture UV space, shown as a cube-map in (e). NeuTex can synthesize highly realistic images (b) that are very close to the ground-truth (a). Moreover, it enables intu-itive surface appearance editing directly in the 2D texture space; we show an example of this in (c), by using a new texture (f) to modulate the reconstructed texture. Our discovered texture map-ping covers the object surface uniformly, as illustrated in (d), by rendering the object using a uniform checkerboard texture (g). by approaches like NeRF [28] and Deep Reﬂectance Vol-umes [5] that leverage differentiable volume rendering (ray marching). However, these volume-based methods do not (explicitly) reason about the object’s surface and entangle both geometry and appearance in a volume-encoding neural network. This does not allow for easy editing—as is possi-ble with a texture mapped mesh—and signiﬁcantly limits the practicality of these neural rendering approaches.
Our goal is to make volumetric neural reconstruction more practical by enabling both realistic image synthesis and ﬂexible surface appearance editing. To this end, we present NeuTex—an approach that explicitly disentangles scene geometry from appearance. NeuTex represents geom-etry with a volumetric representation (similar to NeRF) but represents surface appearance using 2D texture maps. This allows us to leverage differentiable volume rendering to re-construct the scene from multi-view images, while allowing for conventional texture-editing operations (see Fig. 1).
As in NeRF [28], we march a ray through each pixel, regress volume density and radiance (using fully connected
MLPs) at sampled 3D shading points on the ray, accumu-late the per-point radiance values to compute the ﬁnal pixel color. NeRF uses a single MLP to regress both density and 7119
radiance in a 3D volume. While we retain this volumetric density-based representation for geometry, NeuTex repre-sents radiance in a 2D (UV) texture space. In particular, we train a texture mapping MLP to regress a 2D UV coordi-nate at every 3D point in the scene, and use another MLP to regress radiance in the 2D texture space for any UV lo-cation. Thus, given any 3D shading point in ray marching, our network can obtain its radiance by sampling the recon-structed neural texture at its mapped UV location.
Naively adding a texture mapping network to NeRF (and supervising only with a rendering loss) leads to a degenerate texture mapping that does not unwrap the surface and can-not support texture editing (see Fig. 3). To ensure that the estimated texture space reasonably represents the object’s 2D surface, we introduce a novel cycle consistency loss.
Speciﬁcally, we consider the shading points that contribute predominantly to the pixel color along a given ray, and cor-respond to the points either on or close to the surface. We train an additional inverse mapping MLP to map the 2D
UV coordinates of these high-contribution points back to their 3D locations. Introducing this inverse-mapping net-work forces our model to learn a consistent mapping (sim-ilar to a one-to-one correspondence) between the 2D UV coordinates and the 3D points on the object surface. This additionally regularizes the surface reasoning and texture space discovery process. As can be seen in Fig. 1, our full model recovers a reasonable texture space, that can support realistic rendering similar to previous work while also al-lowing for intuitive appearance editing.
Our technique can be incorporated into different volume rendering frameworks. In addition to NeRF, we show that it can be combined with Neural Reﬂectance Fields [4] to re-construct BRDF parameters as 2D texture maps (see Fig. 6), enabling both view synthesis and relighting.
Naturally, NeuTex is more constrained than a fully-volumetric method; this leads to our ﬁnal rendering quality to be on par or slightly worse than NeRF [28]. Nonethe-less, we demonstrate that our approach can still synthesize photo-realistic images and signiﬁcantly outperform both traditional mesh-based reconstruction methods [40] and previous neural rendering methods [43, 42]. Most impor-tantly, our work is the ﬁrst to recover a meaningful surface-aware texture parameterization of a scene and enable sur-face appearance editing applications (as in Fig. 1 and 5).
This, we believe, is an important step towards making neu-ral rendering methods useful in 3D design workﬂows. 2.