Abstract
We present Self-Ensembling Single-Stage object Detec-tor (SE-SSD) for accurate and efﬁcient 3D object detec-tion in outdoor point clouds. Our key focus is on exploit-ing both soft and hard targets with our formulated con-straints to jointly optimize the model, without introducing extra computation in the inference. Speciﬁcally, SE-SSD contains a pair of teacher and student SSDs, in which we design an effective IoU-based matching strategy to ﬁlter soft targets from the teacher and formulate a consistency loss to align student predictions with them. Also, to maximize the distilled knowledge for ensembling the teacher, we design a new augmentation scheme to produce shape-aware aug-mented samples to train the student, aiming to encourage it to infer complete object shapes. Lastly, to better exploit hard targets, we design an ODIoU loss to supervise the stu-dent with constraints on the predicted box centers and ori-entations. Our SE-SSD attains top performance compared with all prior published works. Also, it attains top preci-sions for car detection in the KITTI benchmark (ranked 1st and 2nd on the BEV and 3D leaderboards1, respectively) with an ultra-high inference speed. The code is available at https://github.com/Vegeta2020/SE-SSD. 1.

Introduction
To support autonomous driving, 3D point clouds from
LiDAR sensors are often adopted to detect objects near the vehicle. This is a robust approach, since point clouds are readily available regardless of the weather (fog vs. sunny) and time of the day (day vs. night). Hence, various point-cloud-based 3D detectors have been proposed recently.
To boost the detection precision, an important factor is the quality of the extracted features. This applies to both single-stage and two-stage detectors. For example, the series of works [24, 4, 25, 23] focus on improving the region-proposal-aligned features for a better reﬁne-ment with a second-stage network. Also, many meth-ods [3, 10, 29, 12, 33, 19] try to extract more discrimina-1On the date of CVPR deadline, i.e., Nov 16, 2020
Figure 1. Our SE-SSD attains top precisions on both 3D and BEV car detection in KITTI benchmark [6] with real-time speed (30.56 ms), clearly outperforming all state-of-the-art detectors. Please refer to Table 1 for a detailed comparison with more methods. tive multi-modality features by fusing RGB images and 3D point clouds. For single-stage detectors, Point-GNN [26] adapts a graph neural network to obtain a more compact representation of point cloud, while TANet [17] designs a delicate triple attention module to consider the feature-wise relation. Though these approaches give signiﬁcant insights, the delicate designs are often complex and could slow down the inference, especially for the two-stage detectors.
To meet the practical need, especially in autonomous driving, 3D object detection demands high efﬁciency on top of high precision. Hence, another stream of works, e.g., SA-SSD [8] and Associate-3Ddet [5], aim to exploit auxiliary tasks or further constraints to improve the feature represen-tation, without introducing additional computational over-head during the inference. Following this stream of works, we formulate the Self-Ensembling Single-Stage object De-tector (SE-SSD) to address the challenging 3D detection task based only on LiDAR point clouds. 14494
To boost the detection precision, while striving for high efﬁciency, we design the SE-SSD framework with a pair of teacher SSD and student SSD, as inspired by [27]. The teacher SSD is ensembled from the student. It produces rel-atively more precise bounding boxes and conﬁdence, which serve as soft targets to supervise the student. Compared with manually-annotated hard targets (labels), soft targets from the teacher often have higher entropy, thus offering more information [9] for the student to learn from. Hence, we exploit both soft and hard targets with our formulated constraints to jointly optimize the model, while incurring no extra inference time. To encourage the bounding boxes and conﬁdence predicted by the student to better align with the soft targets, we design an effective IoU-based matching strategy to ﬁlter soft targets and pair them with student pre-dictions, and further formulate a consistency loss to reduce the misalignment between them.
On the other hand, to enable the student SSD to effec-tively explore a larger data space, we design a new augmen-tation scheme on top of conventional augmentation strate-gies to produce augmented object samples in a shape-aware manner. By this scheme, we can encourage the model to in-fer the complete object shape from incomplete information.
It is also a plug-and-play and general module for 3D detec-tors. Furthermore, hard targets are still essential in the su-pervised training, as they are the ﬁnal targets for the model convergence. To better exploit them, we formulate a new orientation-aware distance-IoU (ODIoU) loss to supervise the student with constraints on both the center and orienta-tion of the predicted bounding boxes. Overall, our SE-SSD is trained in a fully-supervised manner to best boost the de-tection performance, in which all the designed modules are needed only in the training, so there are no extra computa-tion during the inference.
In summary, our contributions include (i) the Self-Ensembling Single-Stage object Detector (SE-SSD) frame-work, optimized by our formulated consistency constraint to better align predictions with the soft targets; (ii) a new augmentation scheme to produce shape-aware aug-mented ground-truth objects; and (iii) an Orientation-aware
Distance-IoU (ODIoU) loss to supervise the detector us-ing hard targets. Our SE-SSD attains state-of-the-art per-formance on both 3D and BEV car detection in the KITTI benchmark [6] and demonstrates ultra-high inference speed (32 FPS) on commodity CPU-GPU, clearly outperforming all prior published works, as presented in Figure 1. 2.