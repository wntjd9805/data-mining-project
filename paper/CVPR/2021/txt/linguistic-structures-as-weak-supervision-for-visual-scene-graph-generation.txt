Abstract
Prior work in scene graph generation requires categor-ical supervision at the level of triplets—subjects and ob-jects, and predicates that relate them, either with or without bounding box information. However, scene graph gener-ation is a holistic task: thus holistic, contextual supervi-sion should intuitively improve performance. In this work, we explore how linguistic structures in captions can beneﬁt scene graph generation. Our method captures the informa-tion provided in captions about relations between individ-ual triplets, and context for subjects and objects (e.g. visual properties are mentioned). Captions are a weaker type of supervision than triplets since the alignment between the exhaustive list of human-annotated subjects and objects in triplets, and the nouns in captions, is weak. However, given the large and diverse sources of multimodal data on the web (e.g. blog posts with images and captions), linguis-tic supervision is more scalable than crowdsourced triplets.
We show extensive experimental comparisons against prior methods which leverage instance- and image-level supervi-sion, and ablate our method to show the impact of leverag-ing phrasal and sequential context, and techniques to im-prove localization of subjects and objects. 1.

Introduction
Recognizing visual entities and understanding the rela-tions among them are two fundamental problems in com-puter vision. The former task is known as object detection (OD) and the latter as visual relation detection (VRD). In turn, scene graph generation (SGGen) requires to jointly de-tect the entities and classify their relations.
While scene graphs are a holistic, contextual represen-tation of an image, the types of supervision that have been used capture context in an impoverished way. In particular, prior methods use supervision in the form of either subject-predicate-object triplets with bounding boxes for the subject and object [27, 34, 49] or subject-predicate-object triplets at the image level only [54, 57]. Thus, information in the su-pervision is local (separate triplets) while the scene graph
Figure 1. We tackle the problem of generating scene graphs with supervision in the form of captions at training time. Parsing from captions enables utilization of the huge amount of image-text data available on the internet. The linguistic structure extracted main-tains the relational information described in the caption without the loss of cross-triplet references, and facilitates disambiguation. to be output captures the entire image. This discrepancy between the properties of the desired output (global) and training data (local) becomes problematic due to potential ambiguity in the visual input. For example, in Fig. 1, multi-ple persons are standing on the rails. Thus, standard super-vision (top) which breaks down a scene graph into triplets, may create confusion.
In contrast, captions capture global context that allows us to link multiple triplets, and localize a man who is both standing on the rails, and wearing a (checkered) shirt. Cap-tions are linguistic constructs, and language could be ar-gued to capture common sense (e.g., BERT [11] models are good at question-answering and commonsense tasks).
Captions are also advantageous in terms of cost: humans naturally provide language descriptions of visual content they upload, thus caption-like supervision can be seen as
“free”. However, caption supervision contains noise, which presents some challenges. First, captions provide supervi-sion at the image level, similar to prior work in weakly-supervised scene graph generation [54]. Second, prior work
[32, 51] shows that captions do not cover all relevant ob-jects: not all content is mentioned, and some of the men-tioned content is not referring to the image explicitly or is not easily localizable. Because captions are noisy, the su-pervision we use is even weaker than prior work [54]. 8289
We propose an approach that leverages global context, using captions as supervision. Our approach models con-text for scene graphs in two ways. First, it extracts infor-mation from captions beyond the subject-predicate-object entities (e.g., in the form of attributes like “checkered”, in
Fig. 1). This context enables more accurate representations of concepts, and thus more accurate localization of each subject-predicate-object triplet. Second, visuo-linguistic context provides a way to reason about common-sense rela-tionships within each triplet, to prevent non-sensical triplets from being generated (e.g., “rails standing on man” is un-likely, while “man standing on rails” is likely). To cope with the challenges of the noise contained in captions, we rely on an iterative detection method which helps prune some spurious relations between caption words and image regions, via boostrapping. While the captions we use are crowdsourced, our method paves the road for using image-caption pairs harvested from the internet for free, using text accompanying images on the web, from blogs, social media posts, YouTube video descriptions, and instructional videos [31, 42, 53]. Note that our method internally uses a graph with broad types of nodes, including adjectives, even though these are not part of the graph that is being output at test time. A side contribution is an adaptation of techniques from weakly-supervised object detection to improve local-ization of subject and object through iterative reﬁnement, which has not been used for scene graph generation before.
To isolate the contribution of global context from the noise contained in captions (i.e., objects not being men-tioned), we verify our approach in two settings. First, we construct a ground-truth triplet graph by connecting triplets with certain overlap. We show that our full method greatly outperforms prior work (it boosts the performance of [54] by 59%-67%). Second, we use two types of actual captions.
This causes overall performance to drop, but we observe that modeling phrasal (cross-triplet) and sequential (within-triplet) linguistic context achieves strong results, signiﬁ-cantly better than more direct uses of captions, and com-petitive with methods using clean image-level supervision.
To summarize, our contributions are as follows:
• We examine a new mechanism for scene graph gener-ation using a new type of weak supervision.
• We contextualize embeddings for subject/object enti-ties based on linguistic structures (e.g. noun phrases).
• We propose new joint classiﬁcation and localization of subject, object and predicate within a triplet.
• We leverage weakly-supervised object detection tech-niques to improve scene graph generation. 2.