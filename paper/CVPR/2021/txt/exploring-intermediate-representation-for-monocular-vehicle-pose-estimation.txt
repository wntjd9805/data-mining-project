Abstract 1.

Introduction
We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map local appearance to observa-tion angles, we explore a progressive approach by extract-ing meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This ap-proach features a deep model that transforms perceived in-tensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate sys-tem. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that de-rives from primitive 3D annotation readily. The latter ques-tion motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the train-ing stage to improve representation learning. Without addi-tional labels, our system outperforms previous monocular
RGB-based methods for joint vehicle detection and pose es-timation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this HTTPS URL1. 1https://github.com/Nicholasli1995/EgoNet
Part of this work was done when the ﬁrst author was an intern at Sense-Time Research Shanghai Autonomous Driving Group.
“The usefulness of a representation depends upon how well suited it is to the purpose for which it is used”. –David Marr [43]
Understanding 3D properties of the surrounding world is critical for vision-based autonomous driving and trafﬁc surveillance systems [15]. Accurate 3D vehicle orientation estimation (VOE) can imply a driver’s intent of travel di-rection, help predicting its position a moment later and help identifying anomalous behaviors. To pursue high accuracy, recent studies have exploited active sensors to obtain point clouds [63, 69, 47] or the use of stereo cameras [32, 55, 46].
The former enjoys the convenience of directly working with 3D data and the latter beneﬁt from multi-view geometry.
As downsides, LiDAR sensors cost more than commodity
RGB cameras and stereo systems incur extra complexity.
Pushing the limit of a single-view RGB-based method is not only cost-effective, but can also complement other con-ﬁgurations through sensor fusion.
However, estimating 3D parameters from a single RGB image is highly challenging due to the lack of depth infor-mation and other geometrical cues. Thanks to the strong representation learning power brought by recent advances in deep architectures, state-of-the-art (SOTA) RGB-based methods can regress 3D parameters from visual appearance
In this paradigm, in an end-to-end manner [45, 3, 52]. paired images and 3D annotations are speciﬁed as inputs and learning targets respectively for supervising a deep 1873
Figure 2: Detailed architecture of Ego-Net. Feature maps are ﬁrst computed with a fully convolution model H from a detected instance. Heatmaps representing the projection of a 3D cuboid are regressed and mapped to local coordinates with several strided convolution layers. The local coordinates are transformed to screen coordinates φg(xi) and mapped to a 3D cuboid representation ψ(xi), whose orientation directly represent egocentric pose in the camera coordinate system. k=33 when q=2 as in Sec. 3. model. No intermediate representation is designed in such end-to-end solutions, which need a large amount of training pairs to approximate the highly non-linear mapping from the pixel space to 3D geometrical quantities.
In contrast to the aforementioned methods, we take a different approach and ask can a neural network extract explicit geometrical quantities from RGB images and use them for monocular vehicle pose estimation? Our inspira-tion is from the representational framework for vision [43], which ﬁrst extracts geometrically meaningful 2 1 2 -D sketch from perceived intensities and converts them to 3D rep-resentations. We are also inspired by recent advances in two-stage 3D human pose estimation [36]. Previous ap-proaches [45, 3, 52] specify the start (pixels) and the end (3D pose) of such a process while ignoring the learning of any intermediate representation. Apart from the motivation from the framework [43], geometrical representations have two other beneﬁts: (1) they are easier for human to compre-hend than the dense feature maps when debugging safety-critical applications, (2) the mapping from them to 3D pose is easy to learn as a few layers would sufﬁce in our experi-ments.
Towards this end, we propose a novel VOE approach fea-turing learning Intermediate Geometrical Representations (IGRs). We ﬁrst explicitly deﬁne a set of IGRs based on existing ground truth without any extra manual annotation.
We then design a novel deep model, Ego-Net, which ﬁrst regresses IGRs and then maps them to the 3D space, achiev-ing SOTA performance. Apart from the high accuracy, we propose a novel loss function for self-supervising the learn-ing of IGRs based on prior knowledge of projective invari-ants. With a hybrid source of supervision, our model can beneﬁt from unlabeled data for enhanced generalization.
Our contributions can be summarized in the following:
• To our best knowledge, our proposed learning-based model, Ego-Net, is the ﬁrst one which regresses inter-mediate geometrical representations for direct estima-tion of egocentric vehicle pose.
• Our novel loss function based on the preservation of projective invariants introduces self-supervision for the learning of such representations.
• Our approach does not require additional labels, and outperforms previous monocular RGB-based VOE methods. Our monocular system achieves perfor-mance comparable to some stereo methods for joint vehicle detection and pose estimation on KITTI.
• Ego-Net, a plug-and-play module, can be incorporated into any existing 3D object detection models for im-proved VOE. Equipped with Ego-Net, a 3D detection system can achieve high 3D location and orientation estimation performance simultaneously, while previ-ous works [3, 13, 4] usually fail to achieve both.
After discussing relevant works in Sec. 2, we detail the design of Ego-Net in Sec. 3. In Sec. 4 we demonstrate how unlabeled data can be utilized to learn IGRs with prior ge-ometry knowledge. Sec. 5 presents experimental results. 2.