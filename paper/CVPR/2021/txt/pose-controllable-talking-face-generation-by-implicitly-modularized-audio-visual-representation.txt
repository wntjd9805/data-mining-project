Abstract
While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efﬁciently drive the head pose re-mains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme con-ditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on non-aligned raw face images, using only a single photo as an identity refer-ence. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substan-tially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be deﬁned by learning the intrinsic synchro-nization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework.
Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are con-trollable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization.1 1.

Introduction
Driving a static portrait with audio is of great impor-tance to a variety of applications in the ﬁeld of enter-tainment, such as digital human animation, visual dub-bing in movies, and fast creation of short videos. Armed with deep learning, previous researchers take two differ-ent paths towards analyzing audio-driven talking human 1Code, models, and demo videos are available at https://hangz-nju-cuhk.github.io/projects/PC-AVS. 4176
faces: 1) through pure latent feature learning and im-age reconstruction [14, 76, 9, 71, 50, 54, 44], and 2) to borrow the help of structural intermediate representa-tions such as 2D landmarks [51, 10, 18] or 3D represen-tations [1, 52, 49, 8, 74, 46, 28]. Though great progress has been made in generating accurate mouth movements, most previous methods fail to model head pose, one of the key factors for talking faces to look natural.
It is very challenging to control head poses while gen-erating lip-synced videos with audios. 1) On the one hand, pose information can rarely be inferred from audios. While most previous works choose to keep the original pose in a video, very recently, a few works have addressed the prob-lem of generating personalized rhythmic head movements from audios [8, 74, 65]. However, they rely on a short clip of video to learn individual rhythms [65, 8], which might be inaccessible for a variety of scenes. 2) On the other hand, all the above methods rely on 3D structural interme-diate representations [65, 8, 74]. The pose information is inherently coupled with facial movements, which affects both reconstruction-based [14, 71] and 2D landmark-based methods [10, 18]. Thus the most plausible way is to lever-age 3D models [33, 52, 8, 70] where the pose parameters and expression parameters are explicitly deﬁned [2]. Nev-ertheless, long-term videos are normally needed in order to learn person-speciﬁc renderers for 3D models. More im-portantly, such representations would be inaccurate under extreme cases such as large pose or low-light conditions.
In this work, we propose Pose-Controllable Audio-Visual System (PC-AVS), which achieves free pose control when driving arbitrary talking faces with audios. Instead of learning pose motions from audios, we leverage another pose source video to compensate only for head motions as illustrated in Fig. 1. Speciﬁcally, no structural information is needed in our work. The key is to devise an implicit low-dimension pose code that is free of mouth shape or identity information. In this way, audio-visual representations are modularized into spaces of three key factors: speech content, head pose, and identity information.
In particular, we identify the existence of a non-identity latent embedding from the visual domain through data aug-mentation. Intuitively, the complementary speech content and pose information should originate from it. Extracting the shared information between visual and audio representations could lead to the speech content space by synchronizing both the modalities, which is also proven to be beneﬁcial for various downstream audio-visual tasks [17, 40, 72, 36, 25].
However, there is no explicit way to model pose without precisely recognized structural information. Here we lever-age the prior knowledge of 3D pose parameters, that a mere vector of 12 dimensions, including a 3D rotation matrix, a 2D positional shifting bias, and a scale factor, is sufﬁcient to represent a head pose. Thus we deﬁne a mapping from the non-identity space to a low dimension code which implicitly stands for the pose. Notably, we do not use other 3D priors to model the transformation between different poses. Then with additional identity supervision, the modularization of the whole talking face representations has been completed.
The last key is the cross-frame reconstruction between video frames, where all representations are complementarily learned. A generator whose convolutional kernels are modu-lated by the embedded features is also designed. Speciﬁcally, we assemble the features from the modularized spaces and use them to scale the weights of the kernels as proposed in [31]. The expressive ability of the weight modulation also enforces our modularization to audio-visual representations in an implicit manner, i.e., in order to ensure low reconstruc-tion loss, the low-dimensional code automatically controls pose while the speech content embedding takes care of the mouth. During inference, we can drive an arbitrary face by a clip of audio with head movements controlled by another video. Multiple advanced properties can also be achieved such as extreme view robustness and frontalizing talking faces.
Our contributions are summarized as follows: 1) We pro-pose to modularize the representations of talking human faces into the spaces of speech content, head pose, and iden-tity respectively, by devising a low-dimensional pose code inspired by 3D pose prior in talking faces. 2) The mod-ularization is implicitly and complementarily learned in a construction-based framework with modulated convolution. 3) Our model generates pose-controllable talking faces with accurate lip synchronization. 4) As no structural intermedi-ate information is used in our system, our model requires little pre-processing and is robust to input views. 2.