Abstract
In this paper, we present an efﬁcient spatial-temporal representation for video person re-identiﬁcation (reID).
Firstly, we propose a Bilateral Complementary Network (BiCnet) for spatial complementarity modeling. Speciﬁ-cally, BiCnet contains two branches. Detail Branch pro-cesses frames at original resolution to preserve the detailed visual clues, and Context Branch with a down-sampling strategy is employed to capture long-range contexts. On each branch, BiCnet appends multiple parallel and diverse attention modules to discover divergent body parts for con-secutive frames, so as to obtain an integral characteristic of target identity. Furthermore, a Temporal Kernel Selection (TKS) block is designed to capture short-term as well as long-term temporal relations by an adaptive mode. TKS can be inserted into BiCnet at any depth to construct BiCnet-TKS for spatial-temporal modeling. Experimental results on multiple benchmarks show that BiCnet-TKS outperforms state-of-the-arts with about 50% less computations. The source code is available at https://github.com/ blue-blue272/BiCnet-TKS. 1.

Introduction
Person re-identiﬁcation (reID) [34, 50, 11] aims at re-trieving a particular person across multiple non-overlapped cameras. Recently, with the emergence of large video benchmarks [50, 20] and the growth of computational re-source, video person reID has been attracting a lot of at-tention. The video data contain richer spatial and temporal clues, which can be utilized to reduce visual ambiguities for more robust reID.
Figure 1: An example of class activation maps [53] of a pair of input video sequences of existing method [50] and our method. isting methods do not take full advantage of the rich spatial-temporal clues in videos. For spatial clues, most meth-ods [28, 26, 12] conduct the same operation on each frame at same input resolution, resulting in highly redundant spa-tial features for consecutive frames. The redundant fea-tures easily focus on the same most representative local re-gion [14], which may be indistinguishable for the two per-sons with seemingly similar local body parts. For example, as shown in Fig. 1 (a), the green T-shirt of the sequence pair attracts the most attention, but is difﬁcult to distinguish the two pedestrians. Therefore, it is desirable to automatically capture the diverse spatial clues across consecutive frames to form a full characteristic of each identify.
Despite the signiﬁcant progress in video reID, most ex-For temporal clues, most existing methods only model 2014
Figure 2: Short and long-term temporal relations have varying importance for different sequences. (a) A sequence with partial occlusion. The long-term temporal clues are desired to alleviate occlusion. (b) A sequence of a fast-moving pedestrian. The short-term temporal clues are desired to model detailed motion patterns. either short-term [28, 44, 9] or long-term temporal rela-tions [39, 46, 13]. To enhance the temporal modeling abil-ity, a few works [20, 21] attempt to jointly capture short and long-term temporal relations and fuse the two relations with equal weights. However, the two temporal relations have varying importance for different sequences. For example, as shown in Fig. 2, for a sequence with partial occlusion, the long-term temporal relations are more important to alle-viate occlusion. For a fast-moving pedestrian sequence, the short-term temporal relations play a greater role to model the detailed motion patterns. So it is necessary to adaptively capture short and long-term temporal relations of videos.
To explicitly fulﬁll above goals, we present an efﬁ-cient spatial-temporal representation for video reID. We
ﬁrst propose a Bilateral Complementary Network (BiCnet) to extract complementary spatial features across consecu-tive frames. Firstly, BiCnet contains two scale-speciﬁc branches, Detail Branch operating on frames at original res-olution to retain spatial details, and Context Branch process-ing frames at down-sampled resolution to enlarge receptive
ﬁeld for long-range contexts. As shown in Fig. 1 (b), with larger receptive ﬁeld, the third-frame feature of the ﬁrst se-quence can capture broader visual clues of a green T-shirt with a backpack strap on it, which can help differentiate the two similar pedestrians. Then on each branch, BiC-net appends multiple parallel spatial attention modules. By enforcing the diversity of individual attention modules, the attention modules can focus on different regions for con-secutive frames. As shown in Fig. 1 (b), with the diverse at-tention modules, the consecutive-frame features from same branch can focus on complementary body regions, covering the whole body of the target identity. Finally, BiCnet ag-gregates the complementary features from the two branches to a comprehensive spatial representation.
Furthermore, we develop a Temporal Kernel Selection (TKS) block to adaptively model the short and long-term temporal relations. Utilizing both small kernel and large kernel along the temporal dimension can capture the short and long-term temporal relations simultaneously. So TKS is designed to contain several parallel temporal convolu-tion paths with various kernel sizes. More importantly,
TKS selects a dominant temporal scale according to the global information from the multiple paths. With the se-lection strategy, TKS can adaptively vary the scale of tem-poral modeling depending on the properties of input videos, thereby exhibiting stronger temporal representational capa-bility. TKS is computationally lightweight and imposes a slight increase in model complexity. It can be readily in-serted into BiCnet, called “BiCnet-TKS”, to progressively learn spatial-temporal patterns.
We evaluate our approach on multiple challenging video reID benchmarks. The evaluations show that our approach outperforms state-of-the-arts. Moreover, by down-sampling some frames to low-resolution, BiCnet-TKS greatly reduces the computations, requiring about 50% less computation cost than state-of-the-arts. 2.