Abstract
We present a method to estimate an HDR environment map from a narrow ﬁeld-of-view LDR camera image in real-time. This enables perceptually appealing reﬂections and shading on virtual objects of any material ﬁnish, from mirror to diffuse, rendered into a real environment using augmented reality. Our method is based on our efﬁcient convolutional neural network, EnvMapNet, trained end-to-end with two novel losses, ProjectionLoss for the generated image, and ClusterLoss for adversarial training. Through qualitative and quantitative comparison to state-of-the-art methods, we demonstrate that our algorithm reduces the directional error of estimated light sources by more than 50%, and achieves 3.7 times lower Frechet Inception Dis-tance (FID). We further showcase a mobile application that is able to run our neural network model in under 9 ms on an iPhone XS, and render in real-time, visually coherent vir-tual objects in previously unseen real-world environments. 1.

Introduction
In this work, we discuss video see-through augmented reality (AR) applications, in which virtual objects are super-imposed on camera frames of the real environment shown on an opaque display, e.g. on a phone as shown in Fig. 1(e).
Creating immersive and believable AR experiences involves many aspects of computer vision and graphics. One of the requirements is visual coherence: the problem of matching visual appearance of rendered objects to their real-world background, such that virtual and real objects become in-distinguishable in the composited video. Accomplishing this involves matching various scene and camera properties, such as lighting, geometry, and sensor noise.
This paper focusses on creating reﬂections and lighting for virtual objects by estimating an omnidirectional HDR environment map. To support rendering objects with a va-riety of geometry, material properties, and dimensions, the environment map must be high dynamic range, and have sufﬁcient image resolution to represent objects and features in the scene. We use the equirectangular projection and
RGB color space for the environment maps. As shown in Fig. 1(a), the challenge in mobile AR is limited cam-era ﬁeld of view (FoV) and motion by the user, hence an application is usually able to accumulate less than 100 de-grees effective FoV. A virtual object placed in front of the user, however, is expected to reﬂect what is behind the cam-era, and parts which are not present in the captured frames.
The problem is thus to estimate, given this incomplete en-vironment map, a plausible estimation for rest of the scene and its lighting. We show that our method is not only able to estimate the light information, but also to synthesize a high resolution completed scene. For instance in the scene shown in Fig. 1(b), the estimated environment map is high resolution, continuous, and a plausible extrapolation of the input. The synthesized parts not only match low frequency information (ambient light temperature and intensity), but also ﬁner details such as the type of light sources (in this case, ceiling area lights). As detailed in Sec. 3, we achieve this context-aware scene completion using the framework of generative adversarial networks (GANs) [12] along with novel loss functions, ProjectionLoss and ClusterLoss, de-signed for accurate light estimation.
In mobile AR frameworks [1, 2], we can obtain camera frames, poses and scene geometry, around the 3D location where the virtual object is to be placed. This allows a real-time renderer to create light probes [27] at the 3D location.
RGB texture information from the frames can be rendered into an equirectangular image at these probe locations. In this work, we focus on processing the partial environment map at these probes. Our method takes as input a partial environment map that is composed from one or more low dynamic range (LDR) camera frames (8 bit per channel), and outputs a completed environment map that is higher dynamic range (HDR, 16 bit channel). Thus we perform both lifting of input pixels from LDR to HDR, as well as spatial HDR image extrapolation. The output environment map retains the color and details from pixels that were in the input, while ﬁlling the unknown pixels with plausible con-tent that is coherent with the known. That is, we want the completed environment map to represent the textures from a plausible real scene. Through detailed quantitative and 11298
Figure 1. Given a partial LDR environment map from a camera image (a), we estimate a visually coherent and completed HDR map that can be used by graphics engines to create light probes. These enable rendering virtual objects with any material ﬁnish into the real environment in a perceptually pleasing and realistic way (b), more similar to ground truth (c) than state-of-the-art methods (d). Our mobile app renders a virtual teapot using our estimated environment map in real-time (e). See supplementary material for videos. qualitative comparisons, we demonstrate that our method surpasses the current state-of-the art to estimate high qual-ity, perceptually plausible, and accurate HDR environment maps. We reduce the directional (angular) error for lights by more than half, and achieve a signiﬁcantly lower Frechet
Inception Distance (FID).