Abstract
Predicting accurate and realistic future human poses from historically observed sequences is a fundamental task in the intersection of computer vision, graphics, and arti-ﬁcial intelligence. Recently, continuous efforts have been devoted to addressing this issue, which has achieved re-markable progress. However, the existing work is seriously limited by complete observation, that is, once the historical motion sequence is incomplete (with missing values), it can only produce unexpected predictions or even deformities.
Furthermore, due to inevitable reasons such as occlusion and the lack of equipment precision, the incompleteness of motion data occurs frequently, which hinders the practical application of current algorithms.
In this work, we ﬁrst notice this challenging problem, i.e., how to generate high-ﬁdelity human motion predic-tions from incomplete observations. To solve it, we pro-pose a novel multi-task graph convolutional network (MT-GCN). Speciﬁcally, the model involves two branches, in which the primary task is to focus on forecasting future 3D human actions accurately, while the auxiliary one is to re-pair the missing value of the incomplete observation. Both of them are integrated into a uniﬁed framework to share the spatio-temporal representation, which improves the ﬁ-nal performance of each collaboratively. On three large-scale datasets, for various data missing scenarios in the real world, extensive experiments demonstrate that our ap-proach is consistently superior to the state-of-the-art meth-ods in which the missing values from incomplete observa-tions are not explicitly analyzed. 1.

Introduction 3D human motion prediction has present considerable potential in many computer vision applications, such as hu-man behavior understanding, machine intelligence, and au-tonomous driving [51, 31, 6, 42, 5, 41, 47]. For instance, robots in our daily life plan their actions in advance to perform seamless human-machine interaction by accurately
∗Corresponding author
Figure 1. Example results. In the upper part, the middle is an in-complete observation (single leg or arm is missing) of the original motion. Our approach focuses particularly on generating the pre-dicted poses directly from the incomplete data while repairing the missing value incidentally. anticipating the human actions [19, 30, 32].
Recently, due to its increasing signiﬁcance, this fasci-nating topic has been extensively investigated by various emerging technologies [16, 33, 23, 58, 2]. Researchers typ-ically regard it as a sequence-to-sequence (seq2seq) gener-ation task and then resort to RNNs to speculate the next plausible human movement from the historical observa-tion [26, 21]. Current approaches have attempted to ex-ploit GCNs to effectively access the topological relationship of 3D human skeleton for predicting future human motion
[41, 13, 34]. These solutions fully analyze the temporal and spatial correlation of human motion sequences.
Although encouraging progress has been achieved, from the actual scene of human motion prediction, we suggest that the existing literature ignores an essential aspect, i.e., the incompleteness of historical observations has not been considered. Stated in a different way, state-of-the-art ap-proaches [21, 34, 10, 13] are over-sensitive to the missing items of the observed data that are very common in real-world scenarios [16, 46, 22, 8]. For example, due to the mutual occlusion of joints or the occlusion of objects in the environment, the sensor measurement frequently involves missing values, as shown in Figure 1. Even for professional motion capture (MoCap) devices, the incompleteness of the 4801
raw motion data is also inevitable [55, 12, 38]. Current pre-dictive algorithms never consider the realistic scenario of incomplete historical observations, which may yield unex-pected or even distorted predictions, leading to the failure of the human motion prediction task.
To investigate this new issue, we develop a novel multi-task graph convolutional network (MT-GCN), which si-multaneously considers two supervised learning tasks, i.e., predicting human actions and repairing the incomplete ob-servation. Speciﬁcally, MT-GCN mainly includes three modules, including a shared context encoder (SCE), a se-quence repairing module (SRM), and a human action pre-dictor (HAP). From temporal and spatial perspectives, the
SCE resorts to the GCN [7, 28] and temporal convolutional networks (i.e., TCNs) [4] to extract the context code of 3D skeleton sequences.
In back-propagation, this shared context is supervised by both HAP and SRM. For SRM, in addition to GCNs, it is also embedded with a tempo-ral self-attention mechanism to select the most related in-formation from the whole sequence to repair the corrupted pose [3, 57]. This strategy can also be regarded as an al-ternative to RNNs or TCNs to capture the temporal pattern.
For HAP, we propose a multi-head graph attention network (GAT) to aggregate information from neighboring nodes, to bring a richer topological representation and stable training
[54]. Besides, we design a non-autoregressive pipeline to generate each predicted frame independently, thus avoiding error propagation over the time dimension. Meanwhile, in-spired by neural machine translation (NMT) [14, 48], posi-tion embedding is introduced into the HAP to ensure conti-nuity of the predicted sequence. Finally, the above modules are jointly optimized in a uniﬁed framework to improve the prediction performance from the incomplete sequence.
The major contributions are threefold: (1) To best our knowledge, this is the ﬁrst research that explicitly focuses on predicting human motion when the observed poses in-volve missing values; (2) We propose a multi-task learning framework to consider both tasks of repairing the corrupted observation and predicting future human actions; (3) On three large-scale benchmarks, our model achieves the state-of-the-art (SoTA) performance against the existing work. 2.