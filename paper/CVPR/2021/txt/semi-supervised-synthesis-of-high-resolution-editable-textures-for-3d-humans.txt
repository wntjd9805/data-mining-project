Abstract
We introduce a novel approach to generate diverse high
ﬁdelity texture maps for 3D human meshes in a semi-supervised setup. Given a segmentation mask deﬁning the layout of the semantic regions in the texture map, our net-work generates high-resolution textures with a variety of styles, that are then used for rendering purposes. To accom-plish this task, we propose a Region-adaptive Adversarial
Variational AutoEncoder (ReAVAE) that learns the proba-bility distribution of the style of each region individually so that the style of the generated texture can be controlled by sampling from the region-speciﬁc distributions. In addition, we introduce a data generation technique to augment our training set with data lifted from single-view RGB inputs.
Our training strategy allows the mixing of reference image styles with arbitrary styles for different regions, a property which can be valuable for virtual try-on AR/VR applica-tions. Experimental results show that our method synthe-sizes better texture maps compared to prior work while en-abling independent layout and style controllability.
∗This work was conducted during an internship at FRL Research. 7991
1.

Introduction 3D human avatar creation has recently gained popularity with the growing use of AR/VR devices and virtual com-munication. A human body is represented by a 3D sur-face mesh modeling its shape, and a texture map (an image in UV space) encoding its appearance mapped to the 3D surface. Realistic textures for avatars are crucial for more immersive experiences with believable digital humans. To date, it is still tedious to create texture maps as it may re-quire hours of manual work by a technical artist or special equipment (e.g., 3D scans, multiview-camera setting, etc.) to capture all the body and cloth details. Hence in this work, we develop a novel method to synthesize photorealistic tex-ture maps for human 3D meshes in a semi-supervised setup with the following properties: i) high resolution, ii) high
ﬁdelity, iii) large diversity, and iv) editability.
Recent deep learning-based techniques for textured 3D human generation [22, 15, 24] infer the textures from 2D clothed human images, which cause their textures to be lim-ited to the garment styles in the image dataset. The ﬁdelity of the inferred textures is also constrained by the resolution of the 2D images. Prior work [23, 29, 44] relies on image-to-image translation networks to convert a human body part segmentation mask into a textured image. These techniques directly generate a clothed human image instead of a tex-ture image that can be applied to a 3D mesh. Besides, their style controllability is limited to mostly changing gar-ment colors but not the actual styles like ﬂoral or check-ered patterns. Among the unsupervised image synthesis works, StyleGAN [18] and StyleGAN2 [19] can generate high-resolution and high-ﬁdelity results with their uncon-ditional image synthesis setup, but such a setup does not allow easy controllability for texture maps that come with a predeﬁned layout in the UV space. Conditional image synthesis techniques like Pix2PixHD [42] and SPADE [33] use a conditional GAN to associate each input segmentation mask to a unique output image. While the VAE version of
SPADE introduces some controllability, it can only control the global style but not class-speciﬁc styles of the output image. The authors of SEAN [52] overcame this problem by encoding class-speciﬁc styles that are then used to learn the normalization parameters for the conditional GAN. This allows them to apply different styles to different regions using different exemplar images, one per region. As a re-sult, exemplar-based approaches are limited to reconstruct-ing the existing textures or linearly interpolating between them. Besides, it is difﬁcult and time-consuming to ﬁnd several different exemplar images for different styles.
To address these issues, we propose a novel architec-ture that we call Region-adaptive Adversarial Variational
AutoEncoder (ReAVAE) that learns the probability distri-butions of per-region styles from texture maps using a VAE in a semi-supervised setup and allows per-region style con-trollability of the output texture using the learned distribu-tions. Our architecture has three components. First, the style encoder encodes an input texture map and performs region-wise average pooling of the encoded features based on the semantic segmentation mask corresponding to the in-put texture to produce per-class feature vectors. Second, the
VAE bottleneck learns to approximate the features of each class by a standard normal distribution, from which a ran-dom sample is generated to produce a transformed feature vector. Lastly, the generator takes the per-class transformed feature vectors, a segmentation mask, and random Gaus-sian noise as inputs to generate the desired texture map.
The generated map is then converted to higher resolution by passing it through a pretrained image super-resolution net-work and ﬁnally rendered using a differentiable renderer.
During inference, we solely use the generator that enables independent layout controllability through the input mask and per-region style controllability through the input ran-dom vectors, which results in the generation of a wide va-riety of textures. We also introduce a training strategy that enables our network to perform both reconstruction of an input image and generation of an arbitrary image. Hence, we can mix the styles of some regions of the input image with arbitrary styles for the remaining regions by manipu-lating the input per-region feature vectors of the generator.
Finally, to alleviate the problem of having limited data orig-inating from textures from 3D scans, we introduce a method to generate training data for our network by lifting textures from full-body clothed human images to the UV space. In summary, our contributions are: 1. We propose a novel architecture for semi-supervised synthesis of diverse high-ﬁdelity texture maps for 3D humans, given the layouts (segmentation masks) as in-put, with independent layout and style controllability.
The textures can be used for high-resolution render-ing. To the best of our knowledge, no existing work has tackled this task to date. 2. We utilize a VAE to learn the distributions of the styles of each region separately, thereby allowing the user to sample from region-speciﬁc distributions during infer-ence to generate a variety of textures. Our training scheme allows mixing styles from exemplar images for some regions with arbitrary styles for other regions, a useful property for 3D virtual try-on applications. 3. We introduce a training data generation technique that lifts textures from single-view RGB images of full-body clothed humans to the UV space. 2.