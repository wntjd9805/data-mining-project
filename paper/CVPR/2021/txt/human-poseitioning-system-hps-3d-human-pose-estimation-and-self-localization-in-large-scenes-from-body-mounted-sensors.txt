Abstract
We introduce (HPS) Human POSEitioning System, a method to recover the full 3D pose of a human registered with a 3D scan of the surrounding environment using wear-able sensors. Using IMUs attached at the body limbs and a head mounted camera looking outwards, HPS fuses cam-era based self-localization with IMU-based human body tracking. The former provides drift-free but noisy position and orientation estimates while the latter is accurate in the short-term but subject to drift over longer periods of time.
We show that our optimization-based integration exploits the beneﬁts of the two, resulting in pose accuracy free of drift. Furthermore, we integrate 3D scene constraints into our optimization, such as foot contact with the ground, re-sulting in physically plausible motion. HPS complements more common third-person-based 3D pose estimation meth-ods.
It allows capturing larger recording volumes and longer periods of motion, and could be used for VR/AR ap-* Joint ﬁrst authors with equal contribution. plications where humans interact with the scene without re-quiring direct line of sight with an external camera, or to train agents that navigate and interact with the environment based on ﬁrst-person visual input, like real humans.
With HPS, we recorded a dataset of humans interact-ing with large 3D scenes (300-1000 m2) consisting of 7 subjects and more than 3 hours of diverse motion. The dataset, code and video will be available on the project page: http://virtualhumans.mpi-inf.mpg.de/hps/. 1.

Introduction
Capturing the full 3D pose of a human, while localizing and registering it with a 3D reconstruction of the environ-ment, using only wearable sensors, opens the door to many applications and new research directions. For example, it will allow Augmented / Mixed / Virtual Reality users to move freely and interact with virtual objects in the scene, 4318    
without the need for external cameras. From the captured data, we could train digital humans that plan and move like real humans, based on visual data arriving at their eyes.
Moreover, by relying only on ego-centric data, we could capture a wider variety of human motion, outside of a re-stricted recording volume imposed by external cameras.
The dominant approach in vision has been to analyze hu-mans from an external third-person camera, often without considering scene context [4, 30, 39, 45, 51, 55]. A few re-cent methods capture 3D scenes and humans [24], but again using a third-person camera. Capturing with external cam-eras is undoubtedly a central problem in vision, but it has its limitations – occlusions are a problem, and interactions across multiple rooms or beyond the viewing area cannot be captured; consequently recordings are typically short.
We propose Human POSEitioning System (HPS), the
ﬁrst method to recover the full body 3D pose of a human registered with a large 3D scan of the surrounding envi-ronment relying only on wearable sensors – body-mounted
IMUs and a head mounted camera, approximating the vi-sual ﬁeld of view of the human. Inspired by visual-inertial odometry and localization [29, 40], as well as IMU-based human pose estimation [50, 71, 73], HPS fuses information coming from body-mounted IMUs with camera pose ob-tained from camera self-localization [57,59,64] (see Fig. 1).
Instead of placing the camera towards the body [52, 67], we place it towards the scene, which allows us to capture what the human observes together with their 3D pose. In com-parison to third-person pose methods, the body is not seen by the camera, which poses new challenges.
Pure IMU-based tracking is known to drift over time and camera localization produces many outliers. By jointly inte-grating IMU tracking with camera self-localization, we are able to remove drift [29, 40], and recover the human tra-jectory when self-localization fails. Furthermore, since we can approximately locate the person in the 3D scene, we in-corporate scene constraints when foot contact is detected.
Overall, with HPS we recover natural human motions, reg-istered with the 3D scene and free of drift, during long pe-riods of time, and over large areas.
To demonstrate the capabilities of HPS, we capture a dataset of real people moving in large scenes. Our HPS dataset consists of 8 types of environments - some being larger than 1000m2, and 7 subjects performing a variety of activities such as walking, excercising, reading, eating, or simply working in the ofﬁce. The dataset can be used as a testbed for ego-centric tracking with scene constraints, to learn how humans interact and move within large scenes over long periods of time, and to learn how humans process visual input arriving at their eyes.
We make the following contributions: 1) to the best of our knowledge, HPS is the ﬁrst approach to estimate the full 3D human pose while localizing the person within a pre-scanned large 3D scene using wearable sensors. 2) we intro-duce a joint optimization which integrates camera localiza-tion, IMU-based tracking and scene constraints, resulting in smooth and accurate human motion estimates. 3) we pro-vide the HPS dataset, a new dataset consisting of 3D scans of large scenes (some larger than 1000 m2), ego-centric video, IMU data, and our 3D reconstructed humans moving and interacting with the scene. In contrast to existing 3D pose datasets, which are captured from a third-person view, ours is captured from an egocentric view. We believe both
HPS and HPS dataset will provide a step towards develop-ing future algorithms to understand and model 3D human motion and behavior within the 3D environment from an egocentric (or third-person) perspective. 2.