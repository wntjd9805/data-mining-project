Abstract
Despite signiﬁcant progress in monocular depth estima-tion in the wild, recent state-of-the-art methods cannot be used to recover accurate 3D scene shape due to an unknown depth shift induced by shift-invariant reconstruction losses used in mixed-data depth prediction training, and possible unknown camera focal length. We investigate this problem in detail, and propose a two-stage framework that ﬁrst pre-dicts depth up to an unknown scale and shift from a single monocular image, and then use 3D point cloud encoders to predict the missing depth shift and focal length that al-low us to recover a realistic 3D scene shape. In addition, we propose an image-level normalized regression loss and a normal-based geometry loss to enhance depth prediction models trained on mixed datasets. We test our depth model on nine unseen datasets and achieve state-of-the-art perfor-mance on zero-shot dataset generalization. Code is avail-able at: https://git.io/Depth 1.

Introduction 3D scene reconstruction is a fundamental task in com-puter vision. The established approach to address this task is SLAM or SfM [16], which reconstructs 3D scenes based on feature-point correspondence with consecutive frames or multiple views. In contrast, this work aims to achieve dense 3D scene shape reconstruction from a single in-the-wild im-*Correspondence should be addressed to C. Shen. age. Without multiple views available, we rely on monocu-lar depth estimation. However, as shown in Fig. 1, existing monocular depth estimation methods [10, 38, 48, 40] alone are unable to faithfully recover an accurate 3D point cloud.
Unlike multi-view reconstruction methods, monocular depth estimation requires leveraging high level scene priors, so data-driven approaches have become the de facto solu-tion to this problem [24, 29, 37, 49, 47]. Recent works have shown promising results by training deep neural networks on diverse in-the-wild data, e.g. web stereo images and stereo videos [5, 7, 29, 37, 43, 44, 49]. However, the diver-sity of the training data also poses challenges for the model training, as training data captured by different cameras can exhibit signiﬁcantly different image priors for depth estima-tion [11]. Moreover, web stereo images and videos can only provide depth supervision up to a scale and shift due to the unknown camera baselines and stereoscopic post process-ing [23]. As a result, state-of-the-art in-the-wild monocular depth models use various types of losses invariant to scale and shift in training. While an unknown scale in depth will not cause any shape distortion, as it scales the 3D scene uniformly, an unknown depth shift will (see Sec. 3.1 and
Fig. 1). In addition, the camera focal length of a given im-age may not be accessible at test time, leading to more dis-tortion of the 3D scene shape. This scene shape distortion is a critical problem for downstream tasks such as 3D view synthesis and 3D photography.
To address these challenges, we propose a novel monoc-ular scene shape estimation framework that consists of a 204
depth prediction module and a point cloud reconstruction module. The depth prediction module is a convolutional neural network trained on a mixture of existing datasets that predicts depth maps up to a scale and shift. The point cloud reconstruction module leverages point cloud encoder net-works that predict shift and focal length adjustment factors from an initial guess of the scene point cloud reconstruction.
A key observation that we make here is that, when operat-ing on point clouds derived from depth maps, and not on images themselves, we can train models to learn 3D scene shape priors using synthetic 3D data or data acquired by 3D laser scanning devices. The domain gap is signiﬁcantly less of an issue for point clouds than that for images, al-though these data sources are signiﬁcantly less diverse than internet images.
We empirically show that these point cloud encoders generalize well to unseen datasets.
Furthermore, to train a robust monocular depth predic-tion model on mixed data from multiple sources, we pro-pose a simple but effective image-level normalized regres-sion loss, and a pair-wise surface normal regression loss.
The former loss transforms the depth data to a canonical scale-shift-invariant space for more robust training, while the latter improves the geometry of our predicted depth maps. To summarize, our main contributions are:
• A novel framework for in-the-wild monocular 3D scene shape estimation. To the best of our knowledge, this is the ﬁrst fully data-driven method for this task, and the ﬁrst method to leverage 3D point cloud neural networks for improving the structure of point clouds derived from depth maps.
• An image-level normalized regression loss and a pair-wise surface normal regression loss for improving monocular depth estimation models trained on mixed multi-source datasets.
Experiments show that our point cloud reconstruction mod-ule can recover accurate 3D shape from a single image, and that our depth prediction module achieves state-of-the-art results on zero-shot dataset transfer to 9 unseen datasets. 2.