Abstract 1.

Introduction
Advances in supervised learning approaches to object recognition ﬂourished in part because of the availability of high-quality datasets and associated benchmarks. How-ever, these benchmarks—such as ILSVRC—are relatively task-speciﬁc, focusing predominately on predicting class labels. We introduce a publicly-available dataset that em-bodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments exten-sion to ImageNet (ImageNet-HSJ) is composed of a large set of human similarity judgments that supplements the ex-isting ILSVRC validation set. The new dataset supports a range of task and performance metrics, including evalua-tion of unsupervised algorithms. We demonstrate two meth-ods of assessment: using the similarity judgments directly and using a psychological embedding trained on the simi-larity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous ef-forts based on human judgments. We were able to scale to the full 50,000 image ILSVRC validation set through a se-lective sampling process that used variational Bayesian in-ference and model ensembles to sample aspects of the em-bedding space that were most uncertain. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several pop-ular models conform to human similarity judgments. One
ﬁnding is that the more complex models that perform better on task-speciﬁc benchmarks do not better conform to hu-man semantic judgments. In addition to the human similar-ity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made pub-licly available. ImageNet-HSJ supports the appraisal of in-ternal representations and the development of more human-like models.
One interesting question is how models’ internal repre-sentations compare to human-perceived similarities. While people make such judgments with little effort, human-perceived similarity ﬂexibly adapts to different contexts, re-ﬂecting a rich understanding of the world [32, 23]. For example, people may perceive a beer bottle as similar to cigarettes because both are age-restricted, while perceiving a beer bottle as similar to a soda because both are beverages.
Humans may perceive two objects as similar for many rea-sons, including the two objects playing related roles within encompassing systems, sharing perceptual properties, or simply interacting with one another [23]. Human-perceived similarity has been leveraged in applications such as im-age retrieval [11, 15] and human-in-the-loop categorization
[42, 62, 65], but has been under-utilized in the general de-velopment of computer vision algorithms. The lack of re-search is partly due to the absence of an appropriate dataset and the technical challenges associated with collecting such a dataset. This work introduces the Human Similarity Judg-ments extension to ImageNet (ImageNet-HSJ), designed to include maximally informative similarity judgments for the widely-used ILSVRC dataset [10, 45].
As algorithm development shifts from learning task-speciﬁc representations towards task-general representa-tions, the evaluation metrics used to assess models may also beneﬁt from an equivalent shift. While task-speciﬁc metrics—such as classiﬁcation accuracy—will always be relevant, complementary task-general evaluation metrics seem increasingly necessary. One strategy for creating a task-general metric is to assess how well model-perceived similarity aligns with human-perceived similarity. Compar-ing human and model similarity allows researchers to fo-cus on the internal representations that precede task-speciﬁc output. Internal representation metrics create a level playing
ﬁeld when comparing across diverse training paradigms; such as supervised, unsupervised, and self-supervised ap-3547
proaches. Helping machines think in a more human-like way may also be better for human-machine interactions.
The main aims of this work are to assemble a dataset that embodies human-perceived similarity and demonstrate how the dataset can be used to assess arbitrary models. Ex-tending previous work [56, 43], we employed psycholog-ical embeddings to concisely model the information con-tained in the similarity judgments. A psychological em-bedding includes an embedding of the stimuli, as well as functions that link the embedding to observed behavior.
The psychological embeddings serve three different roles: as a means of modeling uncertainty and performing ac-tive learning (§ 4), assessing whether sufﬁcient data has been collected (§ 5), and evaluating the internal represen-tations of arbitrary models (§ 6). To handle the large num-ber of stimuli, existing approaches are extended using vari-ational inference and ensembles. The dataset and com-panion pre-trained psychological embeddings are hosted at https://osf.io/cn2s3/. An open-source python package for inferring variational psychological embeddings is available at github.com/roads/psiz. 2.