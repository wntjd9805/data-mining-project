Abstract
CNN Encoder
CNN Decoder
In this work, we present FFB6D, a Full Flow
Bidirectional fusion network designed for 6D pose estima-tion from a single RGBD image. Our key insight is that appearance information in the RGB image and geometry information from the depth image are two complementary data sources, and it still remains unknown how to fully leverage them. Towards this end, we propose FFB6D, which learns to combine appearance and geometry information for representation learning as well as output representa-tion selection. Speciﬁcally, at the representation learning stage, we build bidirectional fusion modules in the full
ﬂow of the two networks, where fusion is applied to each encoding and decoding layer.
In this way, the two net-works can leverage local and global complementary in-formation from the other one to obtain better representa-tions. Moreover, at the output representation stage, we designed a simple but effective 3D keypoints selection al-gorithm considering the texture and geometry information of objects, which simpliﬁes keypoint localization for pre-cise pose estimation. Experimental results show that our method outperforms the state-of-the-art by large margins on several benchmarks. Code and video are available at https://github.com/ethnhe/FFB6D.git.
Dense
Fusion
Pose  Estimation
Point Cloud
Encoder
Point Cloud
Decoder (a) The DenseFusion [65] Network. The two networks extract features from different modalities of data separately without any communication, util the ﬁnal layers of the encoding-decoding architecture.
CNN Encoder
CNN Decoder
Fusion module
Concate-nate
Pose  Estimation
Point Cloud
Encoder
Point Cloud
Decoder (b) The Proposed Full Flow Bidirectional Fusion Network. Bidirec-tional fusion modules are added as bridges for information communication in the full ﬂow of the two networks, where fusion is applied on each encod-ing and decoding layers. Local and global supplementary information from each other is shared between the two networks for better appearance and geometry representation learning, which is crucial for 6D pose estimation.
Figure 1: Network Comparison 1.

Introduction 6D Pose Estimation is an important component in lots of real-world applications, such as augmented reality [40], autonomous driving [13, 8, 69] and robotic grasping [9, 62, 17]. It has been proven a challenging problem due to sensor noise, varying lighting, and occlusion of scenes. Recently, the dramatic growth of deep learning techniques motivates several works to tackle this problem using convolution neu-ral networks (CNNs) on RGB images [68, 46, 70, 34]. How-ever, the loss of geometry information caused by perspec-tive projection limits the performance of these approaches in challenging scenarios, such as poor lighting conditions, low-contrast scenes, and textureless objects. The recent ad-vent of inexpensive RGBD sensors provides extra depth in-formation to ease the problem [5, 19, 23, 22] and also leads to an interesting research question: How to fully leverage the two data modalities effectively for better 6D pose esti-mation?
One line of existing works [68, 32] leverage the advan-tage of the two data sources within cascaded designs. These works ﬁrst estimate an initial pose from RGB images and then reﬁne it on point clouds using either the Iterative Clos-est Point (ICP) algorithm or multi-view hypothesis veriﬁca-tion. Such reﬁnement procedures are time-consuming and can not be optimized with the pose from RGB images end-to-end. On the other hand, works like [48, 69] apply a point cloud network (PCN) and a CNN to extract dense features from the cropped RGB image and point cloud respectively and the extracted dense features are then concatenated for pose estimation [65]. Recently, DenseFusion [65] proposed a better fusion strategy which replaced the naive concatena-3003
tion operation with a dense fusion module, shown in Figure 1(a), and delivered improved performance. However, both feature concatenation and DenseFusion suffers from perfor-mance degeneration due to the separation of CNN and PCN in several scenarios, including objects with similar appear-ance or with reﬂective surfaces. Such cases are challenging either for the isolated CNN or the PCN feature extraction.
In this work, we propose a full ﬂow bidirectional fusion network that perform fusion on each encoding and decod-ing layers for representation learning from the RGBD im-age, shown in Figure 1(b). Our key insight is that appear-ance information in RGB and geometry information in point cloud can serve as complementary information during their feature extraction procedure. Speciﬁcally, during the CNN encoding-decoding procedure, it’s hard for CNN to learn a distinctive representation for similar objects from the RGB image, which, however, is obvious in the PCN’s view. On the other hand, the miss of depth caused by reﬂective sur-faces of objects challenges the point cloud only geometry reasoning. Whereas, these objects are visible by CNN from
RGB images. Hence, it’s necessary to get through the two separated feature extraction branches in the early encoding-decoding stages and the proposed full ﬂow bidirectional fu-sion mechanism bridges this information gap.
We further leverage the learned rich appearance and ge-ometry representation for the pose estimation stage. We fol-low the pipeline proposed in PVN3D [17], which opens up new opportunities for the 3D keypoint based 6D pose es-timation. However, it only considers the distance between keypoints for 3D keypoint selection. Some selected key-points might appear in non-salient regions like smooth sur-faces without distinctive texture, making it hard to locate.
Instead, we take both the object texture and geometry infor-mation into account and propose the SIFT-FPS algorithm for automatic 3D keypoint selections. Salient keypoints ﬁl-tered in this way are easier for the network to locate and the pose estimation performance is facilitated.
To fully evaluate our method, we conduct experiments the YCB-Video, on three popular benchmark datasets,
LineMOD, and Occlusion LineMOD datasets. Experimen-tal results show that the proposed approach without any time-consuming post-reﬁnement procedure outperforms the state-of-the-art by a large margin.
To summarize, the main contributions of this work are:
• A novel full ﬂow bidirectional fusion network for rep-resentation learning from a single scene RGBD image, which can be generalized to more applications, such as 3D object detection.
• A simple but effective 3D keypoint selection algorithm that leverages texture and geometry information of ob-ject models.
• State-of-the-art 6D pose estimation performance on the YCB-Video, LineMOD, and Occlusion LineMOD datasets.
• In-depth analysis to understand various design choices of the system. 2.