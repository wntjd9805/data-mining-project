Abstract
For the single image rain removal (SIRR) task, the per-formance of deep learning (DL)-based methods is main-ly affected by the designed deraining models and training datasets. Most of current state-of-the-art focus on con-structing powerful deep models to obtain better derain-ing results.
In this paper, to further improve the derain-ing performance, we novelly attempt to handle the SIRR task from the perspective of training datasets by exploring a more efﬁcient way to synthesize rainy images. Speciﬁ-cally, we build a full Bayesian generative model for rainy image where the rain layer is parameterized as a gen-erator with the input as some latent variables represent-ing the physical structural rain factors, e.g., direction, s-cale, and thickness. To solve this model, we employ the variational inference framework to approximate the expect-ed statistical distribution of rainy image in a data-driven manner. With the learned generator, we can automatically and sufﬁciently generate diverse and non-repetitive training pairs so as to efﬁciently enrich and augment the existing benchmark datasets. User study qualitatively and quan-titatively evaluates the realism of generated rainy images.
Comprehensive experiments substantiate that the proposed model can faithfully extract the complex rain distribution that not only helps signiﬁcantly improve the deraining per-formance of current deep single image derainers, but al-so largely loosens the requirement of large training sam-ple pre-collection for the SIRR task. Code is available in https://github.com/hongwang01/VRGNet. 1.

Introduction
Recently, single image rain removal (SIRR) has attracted considerable attention, which is usually regarded as a neces-sary pre-processing step of outdoor image processing tasks,
†Corresponding author
∗Equal contribution
Figure 1. Interpolation results in latent space z representing rain factors. (a) The rain distribution is implicitly modeled as a gen-erator G; (b) Three groups of generated rain layers through in-terpolations in latent space. For each group, ra and rb (marked as green) represent the rain layers in the original training dataset, while the ones (marked as red) between them are generated from latent codes (marked as red points) in z space. These codes are obtained by linearly interpolating between za and zb which are the latent codes of ra and rb, respectively. e.g., autonomous driving [14], scene segmentation [5], and object tracking [6]. Due to the complex and diverse rain structures in real scenes, SIRR is still a typical challenging task in computer vision [33, 45, 54].
Driven by massive training data (rainy images) and the powerful ﬁtting capability of deep convolutional neural net-work (CNN), deep learning (DL) represents the current re-search trend in the SIRR task. Clearly, the performance of DL-based methods is mainly affected by two key fac-tors, i.e., the rationality and capacity of deraining mod-els and the quality of training datasets. Most of current works focus on the former and aim to improve the derain-ing results mainly by building more sophisticated network-s [8,10,22,29,35,39,41,43,44,49,53,55,58] and designing 14791
better learning manners [23, 31, 37, 46, 48, 50, 56]. Albeit achieving satisﬁed performance in some scenarios, they put less emphasis on the impact of training data and largely rely on the off-the-shelf datasets to train their deraining models.
Curiously, are the existing datasets sufﬁciently good? Is it possible to further improve the performance of current DL-based derainers directly by ameliorating the quality of these datasets? This paper mainly concentrates on these issues.
Currently, for the SIRR task, the existing datasets are mainly obtained by the following manners: 1) The com-mon one is to synthesize rain streaks with the photo-realistic rendering technique [12] and then add them on clear im-ages [10, 33, 36, 53, 58, 59]. 2) Instead of such simple ad-dition operation, inspired by [13], some works [18, 20] ex-plored better fusion mechanisms between clear images and rain streaks. However, the exploited rains are still synthe-sized by manually setting some oscillation parameters of raindrops [12]. 3) The unpaired image translation strategy is another new generation manner, which attempts to learn a mapping from a clean image to rainy one with adversarial learning so as to generate paired rainy-clean images, such as [38, 51, 62]. 4) There is one real rain dataset proposed by [49], which is semi-automatically generated through rain videos shot in real rain scenes by manually adjusting camera parameters, including exposure duration and ISO.
Although these existing datasets can be used to train deep derainers to some extent, their generation manners still possess some evident limitations. Speciﬁcally, for 1) and 2), rains are synthesized by empirically setting some parameters through human subjective assumptions, which would restrict the generated rain types. Besides, the ac-quisition process of training samples needs human super-vision and physical simulators. This is time-consuming and labor-cumbersome. As for 3), the intrinsic mechanisms of rains are more or less ignored and thus it has less physi-cal interpretability. While for 4), it is always hard to shoot enough rain scenes for sufﬁciently representing the compli-cated rain shapes in real world. All these deﬁciencies tend to adversely affect the quality and the diversity of training datasets and limit the performance improvement of current deep SIRR derainers. Thus, it is critical to build a proper model representing the rain statistical distribution in order to automatically and faithfully generate diverse rains.
In this work, we attempt to explore the intrinsic genera-tive mechanism underlying rain streaks and propose a better generation process. As seen in Fig. 1, high quality of rain streaks, with diverse and non-repetitive shapes, can be easi-ly obtained through the learned generator G by our method that represents the implicit distribution of rains. It is worth mentioning that the generated rain streaks r (marked as red in Fig. 1(b)) exhibit more unseen patterns in the original training dataset ra and rb (marked as green). Especial-ly, such generator with an explicit mapping form tends to provide intrinsic clues for understanding the generation of rains, which is meaningful for general tasks on rainy im-ages. In summary, our contributions are mainly three-fold:
Firstly, this work speciﬁcally proposes a generative mod-el to depict the generation process of a rainy image. Speciﬁ-cally, different from hand-crafted priors for rains [16,50,57] or physics-based imaging analysis about rains [13], the pro-posed model makes effort to explore an implicit distribution of rain layer in statistics. A deep variational inference algo-rithm is speciﬁcally designed to to approximate the expect-ed distribution of rainy images.
Secondly, an interpretable rain generator can be ob-tained, capable of delivering the intrinsic manifold projec-tion from latent factors, such as direction and thickness, to rain streaks as shown in Fig. 1. This makes it possible to efﬁciently generate diverse and non-repetitive rain streak-s without subjective human intervention and empirical pa-rameter settings. Disentanglement and interpolation experi-ments substantiate the rationality of the proposed generator, and a user study evaluates the realism of generated rainy im-ages. Moreover, the small sample experiment exhibits the potentials of the proposed model in real applications.
Thirdly, the proposed generator facilitates an easy aug-mentation of diverse rains for current DL-based SIRR de-rainers. Comprehensive experiments on synthetic and re-al datasets validate that the performance of these DL-based derainers can be signiﬁcantly improved by retraining them on augmented datasets. This coincides with our motivation that improving the quality of datasets is rational and helpful. 2.