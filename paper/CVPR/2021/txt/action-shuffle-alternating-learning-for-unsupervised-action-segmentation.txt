Abstract
This paper addresses unsupervised action segmentation.
Prior work captures the frame-level temporal structure of videos by a feature embedding that encodes time locations of frames in the video. We advance prior work with a new self-supervised learning (SSL) of a feature embedding that accounts for both frame- and action-level structure of videos. Our SSL trains an RNN to recognize positive and negative action sequences, and the RNN’s hidden layer is taken as our new action-level feature embedding. The posi-tive and negative sequences consist of action segments sam-pled from videos, where in the former the sampled action segments respect their time ordering in the video, and in the latter they are shufﬂed. As supervision of actions is not available and our SSL requires access to action segments, we specify an HMM that explicitly models action lengths, and infer a MAP action segmentation with the Viterbi algo-rithm. The resulting action segmentation is used as pseudo-ground truth for estimating our action-level feature embed-ding and updating the HMM. We alternate the above steps within the Generalized EM framework, which ensures con-vergence. Our evaluation on the Breakfast, YouTube In-structions, and 50Salads datasets gives superior results to those of the state of the art. 1.

Introduction
This paper is about unsupervised action segmentation, latent actions in is to localize salient where the goal untrimmed videos. The actions are salient as differences in their features allow for segmentation, and the actions are latent as they may not have a distinct semantic meaning, since no supervision about the actions is available. The ground truth can be used only for evaluation. This is a long-standing vision problem with a wide range of applications.
It can be used for mapping a long video to a signiﬁcantly shorter sequence of action segments, and thus for facilitat-ing and reducing complexity of subsequent video interpre-tation.
It can also be used in applications where manual video annotation is prohibitively expensive or not reliable.
In this paper, we focus on a particular setting studied in recent work [18], where all videos show the same ac-tivity (e.g., a cooking activity) which can be decomposed into a temporal sequence of simpler actions (e.g., cooking includes cutting, mixing, peeling). While the activity ex-hibits variations across videos, they are mostly manifested in varying lengths and features of each action of the activity, whereas variations in the total number and temporal order-ing of actions are limited by the very nature of the activity (e.g., cooking usually requires a certain order of actions).
For such a setting, related work [18] makes the following restrictive assumptions that every action appears only once, and all actions always occur and follow the same temporal ordering in all videos, referred to as ﬁxed transcript. Based on these assumptions, they learn a temporal feature embed-ding by training a regression model to predict a temporal position of every frame in the video, where the frame po-sitions are normalized to the video length. Then, they use the K-means for clustering these embedded features of all video frames, and interpret the resulting clusters as repre-senting the latent actions. After computing a likelihood for every cluster, they run the standard Viterbi algorithm on ev-ery video for localizing the latent actions.
We make three contributions, as illustrated in Fig. 1.
First, we relax the above assumption about the ﬁxed tem-poral ordering of actions in videos. We specify a Hidden
Markov Model (HMM), and thus infer a MAP ordering of actions, instead of the ﬁxed transcript. Unlike [18], our
HMM explicitly models the varying lengths of latent ac-tions, and thus constrains implausible solutions in the do-main (e.g., chopping cannot take a few frames). Also, our
HMM uses a multilayer perceptron (MLP) for estimating the likelihood of the frame labeling with the latent actions.
Second, we specify a new self-supervised learning (SSL) for action-level temporal feature embedding. As other SSL approaches [18, 22, 13, 19, 5, 14, 8, 3, 33], ours exploits the temporal structure of videos, where the structure we mean that videos are sequences of actions with small variations in action ordering. However, the cited references typically focus on capturing the temporal structure at the frame level (e.g., by shufﬂing or permuting frames [22, 19], encoding 12628
Stage 2: Joint Learning of the Model and Embedding
Action Shuffle Self-Supervised Learning
MLP
HMM
Action Level 
Embedding
Action Shuffle 
SSL
Viterbi  decoding
HMM learning backprop sampling
Action  1
Action 2
…
Action 
K
Action Segmentation (b)
Action Segmentation
Action  1
Action  2
…
Action 
K time
Action Level 
Embedding backprop 5  frames 5  frames 5  frames
Sampled
Positive 
Sequence time
Action
Shuffling time random selection
RNN (c)
Binary Cross 
Entropy Loss
Are the  actions  shuffled? (a)
Figure 1. An overview of our unsupervised learning. (a) Initial clustering of frame features for identifying latent actions, and their temporal ordering; the frame-level feature embedding is from [18]. (b) The iterative joint training of HMM, MLP, and Action Shufﬂe SSL for inferring action segmentation within the generalized EM framework. (c) SSL for learning our action-level temporal feature embedding.
The data manipulation samples positive and negative sequences, where the former respect temporal ordering of actions in the predicted action segmentation and the latter shufﬂe the ordering of actions. frames’ positions in video [18]) or the entire video level (e.g., by manipulating the playback speed [8, 3, 33]).
In contrast, we seek to learn a feature embedding that would account for the correct ordering of actions along the video.
As illustrated in Fig. 1 and Fig. 2, our SSL is based on recognizing positive and negative sequences of actions, both generated as sequences of action segments randomly sam-pled from a video. The key difference between the positive and negative sequences is that in the former the actions are laid out in the same temporal ordering as in the video, and in the latter the actions are shufﬂed resulting in an incorrect ordering. As shown in Fig. 1, our SSL trains an RNN on these positive and negative sequences, and minimization of the incurred binary cross-entropy loss produces our action-level temporal embedding of frame features.
As our third contribution, we formulate a joint training of HMM, MLP, and Action Shufﬂe SSL within the Gener-alized EM framework [28]. This extends prior work (e.g.
[18, 32]) where different components of their approaches are usually learned independently. Note that our Action
Shufﬂe SSL requires access to action segments for gener-ating the positive and negative examples. Action segmenta-tion, in turn, requires the HMM inference. After the HMM inference, we can update the action-level feature embed-ding through the Action Shufﬂe SSL, as well as update the
HMM parameters. As these updates will change both fea-tures and HMM, it seems reasonable to run the HMM infer-ence again. We integrate all these steps within the general-ized EM framework. A convergence guarantee of our joint training follows from the generalized EM algorithm [28].
Fig. 1 shows an overview of our unsupervised learning that consists of two stages. The initial stage follows [18].
Given frame features from all videos, we ﬁrst compute the frame-level feature embedding of [18], and then run the
K-means for identifying labels of the latent actions. As shown in Fig. 1a, for every cluster, we compute the mean of frames’ normalized positions in videos, then, sort the clus-ters by their means in the ascending order, and ﬁnally take the sorted cluster indices as labels of the corresponding la-Figure 2. Action shufﬂing: Both positive and negative examples are generated as sequences of 3 action segments randomly sam-pled from a video, where the former respects the action ordering in the video, and the latter shufﬂes the ordering. tent actions. This ascending order of action labels is taken as a likely transcript, and used in the HMM inference for constraining the Viterbi algorithm to respect the transcript.
In the second stage, we perform the generalized EM algo-rithm for iteratively updating the HMM, MLP, and action-level embedding.
Evaluation on the challenging Breakfast [15], YouTube
Instructional [2], and 50Salads [27] datasets demonstrate our superior performance over the state of the art.
The rest of the paper is organized as follows: Sec. 2 re-views related work, Sec. 3 presents our SSL, Sec. 4 speci-ﬁes our HMM and its inference, Sec. 5 formalizes our joint training, Sec. 6 extends our approach to a more general set-ting, and Sec. 7 presents our experiments. 2.