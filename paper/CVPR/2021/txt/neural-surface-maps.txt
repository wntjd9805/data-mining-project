Abstract
Maps are arguably one of the most fundamental con-cepts used to deﬁne and operate on manifold surfaces in differentiable geometry. Accordingly, in geometry process-ing, maps are ubiquitous and are used in many core appli-cations, such as paramterization, shape analysis, remesh-ing, and deformation. Unfortunately, most computational representations of surface maps do not lend themselves to manipulation and optimization, usually entailing hard, dis-crete problems. While algorithms exist to solve these prob-lems, they are problem-speciﬁc, and a general framework for surface maps is still in need.
In this paper, we advocate considering neural networks as encoding surface maps. Since neural networks can be composed on one another and are differentiable, we show it is easy to use them to deﬁne surfaces via atlases, compose them for surface-to-surface mappings, and optimize differ-entiable objectives relating to them, such as any notion of distortion, in a trivial manner. In our experiments, we repre-sent surfaces by generating a neural map that approximates a UV parameterization of a 3D model. Then, we compose this map with other neural maps which we optimize with re-spect to distortion measures. We show that our formulation enables trivial optimization of rather elusive mapping tasks, such as maps between a collection of surfaces. 1.

Introduction
Maps are one of the most fundamental concepts in sur-in differential geometry, a surface, i.e., a face geometry: 2-manifold, is usually (locally) deﬁned as the image of a (non-degenerate) map f : R2 → Rn.
Not surprisingly, maps are also used to deﬁne correspon-dences between different parts of surfaces in an atlas, to evaluate similarity between surface pairs, or across surface collections.
Accordingly, computing maps is central in most geome-try processing tasks operating on surfaces. The ubiquitous f
ψ
φ h
Figure 1. Two surfaces are respectively represented by two neu-ral maps, φ and ψ, each mapping the unit square to 3D. A distortion-minimizing surface-to-surface map f is visualized by texture transfer. This surface map is achieved by a third neural map h between the square to itself, which yields the surface map by an implicit composition of the three neural maps. The distortion of the composed map f is trivially optimized by deﬁning it as a loss in pytorch and optimizing with respect to h. Inset shows the initial random map of the hippo to the cow is a poor initialization, cover-ing only a very tiny region of the cow model. Optimization results in a ﬁnal map that covers the whole target surface, and reducing isometric distortion, resulting in an average Symmetric Dirichlet energy of 11 between these highly-nonisometric surfaces. As in-put, 4 keypoint constraints were used, each on one leg. concept of a UV map [54], mapping a surface into the plane, provides a local coordinate system on surfaces, and hence enables downstream tasks such as texturing, surface corre-spondence, remeshing, quad-meshing [11] to name only a few. Similarly, surface-to-surface maps [53] enable deﬁning correspondences between surfaces, which are at the heart of shape analysis, transfer of properties, deformations, or deﬁning morph sequences.
Indeed, almost all shape pro-cessing tasks, including parameterization, surface corre-spondence, remeshing, and deep learning on surfaces, heav-ily rely on access to such surface maps. 14639
However, many of the tasks related to maps and their computation become extremely hard to handle when the tar-get domain is a surface, i.e., a 3D mesh (n = 3). This is due mostly to the fact that meshes are combinatorial representa-tions, which in turn leads to a combinatorial representation of the surface maps, and taints the optimization task with a combinatorial nature as well. Although elegant solutions in the form of discrete differential geometry [50, 52], meshing invariant spectral analysis [42, 40], functional maps [45, 35] have been proposed to work around the combinatorial rep-resentation, the diverse choices and different data represen-tations inhibit easy end-to-end optimization and adaptation outside the specialized geometry processing community.
As an example, consider the problem of computing a mesh-to-mesh mapping in which a continuous map from one surface to the other is computed: one needs to account for the image of each source vertex, which lands on a tri-angle of the other mesh, and the image of a source edge may span several triangles of the target; this leads to ex-tensive bookkeeping, and any attempt to optimize, e.g., the map’s inter-surface distortion leads to combinatorial opti-mization of the choice of target triangle for each source vertex as in [53, 31]. An alternative is to optimize proxy maps into a common base domain [4] in the hope that the re-sulting surface-to-surface map will be optimized by proxy.
Such an approach, however, does not yield surface maps that are even a local minimizer of the energy they set to minimize. This is particularly problematic when optimiz-ing inter-surface maps across shape collections.
In this work, we consider neural networks as a para-metric representation of both individual surfaces as well as inter-surface maps. Speciﬁcally, we consider networks with parameters θ that receive 2D points as input and out-put points either in 2D or 3D, φθ : R2 → Rn. While this deﬁnition is similar to, e.g., AtlasNet [25], we do not aim to perform any learning task, and our network does nothing more than map 2D points with the aim of performing one task: approximate a single surface map φθ ∼ f : R2 → Rn, so we can work with neural networks instead of with, e.g., mappings of triangular meshes.
Speciﬁcally, we use a map φθ ∼ f : R2 → R3 to di-rectly characterize a given manifold shape (restricted to sur-face patches homeomorphic to a disc), and use another map
ψβ ∼ g : R2 → R2 to update the surface map by restrict-ing movements on the underlying 2-manifold. These neural networks are, by construction, differentiable and compos-able with one another, hence they lend us a simple model for deﬁning a differentiable algebra of surface maps, en-abling us to compose maps with one another and optimize objectives directly over their composition, rather than pro-pose approximations via intermediate proxy domains. deﬁnition of a surface as a map from 2D to 3D, by over-ﬁtting a neural network to a given UV parameterization computed via a standard parameterization algorithm, such as Tutte’s embedding [59] or SLIM [51]. Two such maps,
φ, ψ, are shown in Figure 1. This gives us a parametric, differentiable representation of the surface, from a canoni-cal domain. Second, we compose the overﬁtted map with other maps, either to optimize the distortion of the map, or to compute distortion-minimizing maps between two or more surfaces. Figure 1 shows an example of a distortion-minimizing map f deﬁned by composing h with φ, ψ.
We evaluate our method on a variety of triangular meshes with varying complexity and show their efﬁcacy in com-putation of parameterizations, surface-to-surface distortion-minimizing mapping, and also for mapping across collec-tions of shapes. We also provide comparison to baseline methods. In summary, our main contribution is introducing neural surface map as a novel representation and utilizing it towards addressing a variety of geometry processing ap-plications. We particularly stress the modular nature of the representation that enables harnessing the power of current deep learning frameworks to solve many (classical) shape analysis tasks in a uniform framework. Code available from the project page http://geometry.cs.ucl.ac. uk/projects/2021/neuralmaps/. 2.