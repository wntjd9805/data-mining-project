Abstract
Automated radiographic report generation is a challeng-ing task since it requires to generate paragraphs describing
ﬁne-grained visual differences of cases, especially for those between the diseased and the healthy. Existing image cap-tioning methods commonly target at generic images, and lack mechanism to meet this requirement. To bridge this gap, in this paper, we propose a self-boosting framework that improves radiographic report generation based on the cooperation of the main task of report generation and an auxiliary task of image-text matching. The two tasks are built as the two branches of a network model and inﬂuence each other in a cooperative way. On one hand, the image-text matching branch helps to learn highly text-correlated visual features for the report generation branch to output high quality reports. On the other hand, the improved re-ports produced by the report generation branch provide ad-ditional harder samples for the image-text matching branch and enforce the latter to improve itself by learning better visual and text feature representations. This, in turn, helps improve the report generation branch again. These two branches are jointly trained to help improve each other it-eratively and progressively, so that the whole model is self-boosted without requiring external resources. Experimen-tal results demonstrate the effectiveness of our method on two public datasets, showing its superior performance over multiple state-of-the-art image captioning and medical re-port generation methods. 1.

Introduction
Everyday a large amount of medical imaging data are acquired, stored and examined in clinics. This has exerted mounting pressure to radiologists to analyse images and re-port the ﬁndings in time. Automated medical report gener-ation is therefore in demand as it can reduce workload and diagnostic errors as well as accelerate the clinic workﬂow.
Automated medical report generation is very challeng-ing and it associates with a broader research topic of image captioning in computer vision. In image captioning, free-form text descriptions are generated to narrate the content of images. A basic deep learning model for image cap-tioning follows the encoder-decoder structure [30], where the visual encoder extracts the visual features from images and the text decoder converts the visual features to text out-put. Research in this ﬁeld focuses on advancing encoder and decoder, respectively, by employing carefully-designed attention mechanisms [22, 1, 26], relationship among image regions [40, 38], improved language models [4, 3], or rein-forcement learning on language metric [26, 21]. A detailed review could be found in Section 2.
Despite recent achievements in image captioning, when directly applying image captioners for medical report gen-eration, there is often a visible performance decline. This is because compared with generic images, radiographic im-ages are more similar to each other and ﬁne-grained visual differences, such as the ﬁndings of clinic importance, need to be narrated. This requires further tightening the visual and text representations. In addition to linking image and text by attention mechanism, some medical report genera-tion methods [10, 41] additionally train classiﬁers to learn visual representations by predicting tags of medical reports or disease-class labels. This often requires additional anno-tations and external medical datasets or knowledge, which are task-speciﬁc and often unavailable, limiting the gener-alization of these methods. Moreover, either tags or disease classes only sparsely cover the reports’ information, leading to relatively loosely correlated visual and text features.
To bridge this gap, in this paper, we propose a self-boosting framework to promote radiographic report gener-ation. It utilizes an auxiliary task to predict the match of an image-report pair (i.e., image-text matching) by explicitly learning strongly correlated visual and text features. These 2433
features could better serve the ﬁne-grained recognition task in radiographic report generation. More importantly, the auxiliary image-text matching task is deeply coupled with the main report generation task through our proposed self-boosting framework so that these two tasks could mutu-ally and progressively boost each other via the cooperative interactions between them. Speciﬁcally, these two tasks are built as two branches of a neural network model and jointly trained. During the training iterations, the image-text matching branch provides better features to the report generation branch to generate ground-truth-like reports, and these improved reports, in turn, serve as additional harder samples to push the image-text matching branch to become stronger by learning better features. In this way, the whole network is gradually self-boosted and improves the ultimate report generation performance.
Our main contributions are summarized as follows.
First, we utilize an auxiliary task of image-text matching to help learn text-correlated visual features that could help capture the ﬁne-grained visual differences of diagnostic im-portance. This improves the main task of report generation without requiring additional annotations, external medical datasets, or external task-speciﬁc knowledge.
Secondly and more importantly, we propose a self-boosting framework that leverages the cooperation between image-text matching and report generation to mutually boost each other progressively. These two tasks are tightly coupled and jointly trained, while the stronger features learned by image-text matching help improve report gen-eration, and the improved reports, as additional harder sam-ples, in turn enforce image-text matching to continue im-proving feature learning so that the ﬁner mismatch between an image and its generated report could be identiﬁed.
Third, additionally utilizing image-text matching also al-lows us to learn an effective text feature extractor, which is used to evaluate the feature similarity between the generated reports and the ground-truth, providing a new loss term to further promote the report generation.
Fourth, our approach shows promising performance on two benchmarks, generating reports from both classic Chest
X-ray images and CT images with COVID-19. It outper-forms multiple state-of-the-art methods in image captioning and medical report generation. 2.