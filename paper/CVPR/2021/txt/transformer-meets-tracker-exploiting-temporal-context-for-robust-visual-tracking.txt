Abstract
In video object tracking, there exist rich temporal con-texts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The trans-former encoder promotes the target templates via attention-based feature reinforcement, which beneﬁts the high-quality tracking model generation. The transformer decoder prop-agates the tracking cues from previous templates to the cur-rent frame, which facilitates the object searching process.
Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed trans-former, a simple Siamese matching approach is able to out-perform the current top-performing trackers. By combin-ing our transformer with the recent discriminative track-ing pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks. 1.

Introduction
Visual object tracking is a basic task in computer vision.
Despite the recent progress, it remains a challenging task due to factors such as occlusion, deformation, and appear-ance changes. With the temporal error accumulation, these challenges are further ampliﬁed in the online process.
It is well recognized that the rich temporal information in the video ﬂow is of vital importance for visual track-ing. However, most tracking paradigms [28, 27, 48] handle this task by per-frame object detection, where the tempo-*Corresponding Author: Wengang Zhou and Houqiang Li.
†Source code, pretrained model, and raw tracking results are available at https://github.com/594422814/TransformerTrack.
Figure 1. An overview of our transformer-assisted tracking frame-work. The transformer encoder and decoder are assigned to two parallel branches in a Siamese-like tracking pipeline. Thanks to the encoder-decoder structure, isolated frames are tightly bridged to convey rich temporal information in the video ﬂow. ral relationships among successive frames have been largely overlooked. Take the popular Siamese tracker as an exam-ple, only the initial target is considered for template match-ing [1, 44, 19, 28]. The merely used temporal information is the motion prior (e.g., cosine window) by assuming the target moves smoothly, which is widely adopted in visual trackers. In other tracking frameworks with update mech-anisms [20, 39, 8, 58, 60, 3], previous prediction results are collected to incrementally update the tracking model.
Despite the historical frames considered in the above ap-proaches, the video frames are still considered as indepen-dent counterparts without mutual reasoning. In real-world videos, some frames inevitably contain noisy contents such as occluded or blurred objects. These imperfect frames will hurt the model update when serving as the templates and will challenge the tracking process when performing as the search frames. Therefore, it is a non-trivial issue to convey rich information across temporal frames to mutually rein-force them. We argue that the video frames should not be treated in isolation and the performance potential is largely restricted due to the overlook of frame-wise relationship.
To bridge the isolated video frames and convey the rich temporal cues across them, in this work, we introduce the transformer architecture [46] to the visual tracking commu-nity. Different from the traditional usage of the transformer 1571
Figure 2. Top: the transformer encoder receives multiple template features to mutually aggregate representations. Bottom: the trans-former decoder propagates the template features and their assigned masks to the search patch feature for representation enhancement. in language modeling and machine translation [46, 12], we leverage it to handle the context propagation in the tempo-ral domain. By carefully modifying the classic transformer architecture, we show that its transformation characteristic naturally ﬁts the tracking scenario. Its core component, i.e., attention mechanism [46, 55], is ready to establish the pixel-wise correspondence across frames and freely convey vari-ous signals in the temporal domain.
Generally, most tracking methods [1, 45, 28, 42, 7, 3] can be formulated into a Siamese-like framework, where the top branch learns a tracking model using template features, and the bottom branch classiﬁes the current search patch. As shown in Figure 1, we separate the transformer encoder and decoder into two branches within such a general Siamese-like structure. In the top branch, a set of template patches are fed to the transformer encoder to generate high-quality encoded features. In the bottom branch, the search feature as well as the previous template contents are fed to the trans-former decoder, where the search patch retrieves and aggre-gates informative target cues (e.g., spatial masks and target features) from history templates to reinforce itself.
The proposed transformer facilitates visual tracking via:
• Transformer Encoder. It enables individual template features to mutually reinforce to acquire more compact target representations, as shown in Figure 2. These en-coded high-quality features further beneﬁt the tracking model generation.
• Transformer Decoder. It conveys valuable temporal information across frames. As shown in Figure 2, our decoder simultaneously transfers features and spatial masks. Propagating the features from previous frames to the current patch smooths the appearance changes and remedies the context noises while transforming the spatial attentions highlights the potential object loca-tion. These manifold target representations and spatial cues make the object search much easier.
Finally, we track the target in the decoded search patch. To verify the generalization of our designed transformer, we integrate it into two popular tracking frameworks includ-ing a Siamese formulation [1] and a discriminative corre-lation ﬁlter (DCF) based tracking paradigm [3]. With our designed transformer, a simple Siamese matching pipeline is able to outperform the current top-performing trackers.
By combining with the recent discriminative approach [3], our transformer-assisted tracker shows outstanding results on seven prevalent tracking benchmarks including LaSOT
[13], TrackingNet [38], GOT-10k [23], UAV123 [36], NfS
[24], OTB-2015 [56], and VOT2018 [26] and sets several new state-of-the-art records.
In summary, we make three-fold contributions:
• We present a neat and novel transformer-assisted track-ing framework. To our best knowledge, this is the ﬁrst attempt to involve the transformer in visual tracking.
• We simultaneously consider the feature and attention transformations to better explore the potential of the transformer. We also modify the classic transformer to make it better suit the tracking task.
• To verify the generalization, we integrate our designed transformer into two popular tracking pipelines. Our trackers exhibit encouraging results on 7 benchmarks. 2.