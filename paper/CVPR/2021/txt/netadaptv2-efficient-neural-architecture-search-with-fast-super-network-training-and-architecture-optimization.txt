Abstract
Neural architecture search (NAS) typically consists of three main steps: training a super-network, training and evaluating sampled deep neural networks (DNNs), and train-ing the discovered DNN. Most of the existing efforts speed up some steps at the cost of a signiﬁcant slowdown of other steps or sacriﬁcing the support of non-differentiable search metrics. The unbalanced reduction in the time spent per step limits the total search time reduction, and the inabil-ity to support non-differentiable search metrics limits the performance of discovered DNNs.
In this paper, we present NetAdaptV2 with three innova-tions to better balance the time spent for each step while supporting non-differentiable search metrics. First, we pro-pose channel-level bypass connections that merge network depth and layer width into a single search dimension to re-duce the time for training and evaluating sampled DNNs.
Second, ordered dropout is proposed to train multiple DNNs in a single forward-backward pass to decrease the time for training a super-network. Third, we propose the multi-layer coordinate descent optimizer that considers the interplay of multiple layers in each iteration of optimization to im-prove the performance of discovered DNNs while supporting non-differentiable search metrics. With these innovations,
NetAdaptV2 reduces the total search time by up to 5.8× on
ImageNet and 2.4× on NYU Depth V2, respectively, and dis-covers DNNs with better accuracy-latency/accuracy-MAC trade-offs than state-of-the-art NAS works. Moreover, the discovered DNN outperforms NAS-discovered MobileNetV3 by 1.8% higher top-1 accuracy with the same latency.1 1.

Introduction
Neural architecture search (NAS) applies machine learn-ing to automatically discover deep neural networks (DNNs) with better performance (e.g., better accuracy-latency trade-offs) by sampling the search space, which is the union of all discoverable DNNs. The search time is one key metric for 1The project website: http://netadapt.mit.edu.
Figure 1: The comparison between NetAdaptV2 and related works. The number above a marker is the corresponding total search time measured on NVIDIA V100 GPUs.
NAS algorithms, which accounts for three steps: 1) train-ing a super-network, whose weights are shared by all the
DNNs in the search space and trained by minimizing the loss across them, 2) training and evaluating sampled DNNs (referred to as samples), and 3) training the discovered DNN.
Another important metric for NAS is whether it supports non-differentiable search metrics such as hardware metrics (e.g., latency and energy). Incorporating hardware metrics into NAS is the key to improving the performance of the discovered DNNs [1–5].
There is usually a trade-off between the time spent for the three steps and the support of non-differentiable search metrics. For example, early reinforcement-learning-based
NAS methods [2, 6, 7] suffer from the long time for train-ing and evaluating samples. Using a super-network [8–16] solves this problem, but super-network training is typically time-consuming and becomes the new time bottleneck. The gradient-based methods [3, 17–24] reduce the time for train-ing a super-network and training and evaluating samples at the cost of sacriﬁcing the support of non-differentiable search metrics. In summary, many existing works either have an unbalanced reduction in the time spent per step (i.e., optimiz-ing some steps at the cost of a signiﬁcant increase in the time for other steps), which still leads to a long total search time, 2402
or are unable to support non-differentiable search metrics, which limits the performance of the discovered DNNs.
In this paper, we propose an efﬁcient NAS algorithm,
NetAdaptV2, to signiﬁcantly reduce the total search time by introducing three innovations to better balance the reduction in the time spent per step while supporting non-differentiable search metrics:
Channel-level bypass connections (mainly reduce the time for training and evaluating samples, Sec. 2.2): Early
NAS works only search for DNNs with different numbers of ﬁlters (referred to as layer widths). To improve the per-formance of the discovered DNN, more recent works search for DNNs with different numbers of layers (referred to as network depths) in addition to different layer widths at the cost of training and evaluating more samples because net-work depths and layer widths are usually considered inde-pendently. In NetAdaptV2, we propose channel-level bypass connections to merge network depth and layer width into a single search dimension, which requires only searching for layer width and hence reduces the number of samples.
Ordered dropout (mainly reduces the time for train-ing a super-network, Sec. 2.3): We adopt the idea of super-network to reduce the time for training and evaluating sam-In previous works, each DNN in the search space ples. requires one forward-backward pass to train. As a result, training multiple DNNs in the search space requires multi-ple forward-backward passes, which results in a long train-ing time. To address the problem, we propose ordered dropout to jointly train multiple DNNs in a single forward-backward pass, which decreases the required number of forward-backward passes for a given number of DNNs and hence the time for training a super-network.
Multi-layer coordinate descent optimizer (mainly re-duces the time for training and evaluating samples and supports non-differentiable search metrics, Sec. 2.4):
NetAdaptV1 [1] and MobileNetV3 [25], which utilizes Ne-tAdaptV1, have demonstrated the effectiveness of the single-layer coordinate descent (SCD) optimizer [26] in discovering high-performance DNN architectures. The SCD optimizer supports both differentiable and non-differentiable search metrics and has only a few interpretable hyper-parameters that need to be tuned, such as the per-iteration resource re-duction. However, there are two shortcomings of the SCD optimizer. First, it only considers one layer per optimiza-tion iteration. Failing to consider the joint effect of multiple layers may lead to a worse decision and hence sub-optimal performance. Second, the per-iteration resource reduction (e.g., latency reduction) is limited by the layer with the small-est resource consumption (e.g., latency). It may take a large number of iterations to search for a very deep network be-cause the per-iteration resource reduction is relatively small compared with the network resource consumption. To ad-dress these shortcomings, we propose the multi-layer co-ordinate descent (MCD) optimizer that considers multiple layers per optimization iteration to improve performance while reducing search time and preserving the support of non-differentiable search metrics.
Fig. 1 (and Table 1) compares NetAdaptV2 with related works. NetAdaptV2 can reduce the search time by up to 5.8× and 2.4× on ImageNet [27] and NYU Depth V2 [28] respectively and discover DNNs with better performance than state-of-the-art NAS works. Moreover, compared to
NAS-discovered MobileNetV3 [25], the discovered DNN has 1.8% higher accuracy with the same latency. 2. Methodology: NetAdaptV2 2.1. Algorithm Overview
NetAdaptV2 searches for DNNs with different network depths, layer widths, and kernel sizes. The proposed channel-level bypass connections (CBCs, Sec. 2.2) en-ables NetAdaptV2 to discover DNNs with different network depths and layer widths by only searching layer widths be-cause different network depths become the natural results of setting the widths of some layers to zero. To search kernel sizes, NetAdaptV2 uses the superkernel method [12, 21, 22].
Fig. 2 illustrates the algorithm ﬂow of NetAdaptV2. It takes an initial network and uses its sub-networks, which can be obtained by shrinking some layers in the initial net-In other words, a work, to construct the search space. sample in NetAdaptV2 is a sub-network of the initial net-work. Because the optimizer needs the accuracy of sam-ples for comparing their performance, the samples need to be trained. NetAdaptV2 adopts the concept of jointly training all sub-networks with shared weights by training a super-network, which has the same architecture as the initial network and contains these shared weights. We use
CBCs, the proposed ordered dropout (Sec. 2.3), and su-perkernel [12, 21, 22] to efﬁciently train the super-network that contains sub-networks with different layer widths, net-work depths, and kernel sizes. After training the super-network, the proposed multi-layer coordinate descent op-timizer (Sec. 2.4) is used to discover the architectures of
DNNs with optimal performance. The optimizer iteratively samples the search space to generate a bunch of samples and determines the next set of samples based on the performance of the current ones. This process continues until the given stop criteria are met (e.g., the latency is smaller than 30ms), and the discovered DNN is then trained until convergence.
Because of the trained super-network, the accuracy of sam-ples can be directly evaluated by using the shared weights without any further training. 2.2. Channel Level Bypass Connections
Previous NAS algorithms generally treat network depth and layer width as two different search dimensions. The reason is evident in the following example. If we remove 2403
Figure 2: The algorithm ﬂow of the proposed NetAdaptV2. a ﬁlter from a layer, we reduce the number of output chan-nels by one. As a result, if we remove all the ﬁlters, there are no output channels for the next layer, which breaks the
DNN into two disconnected parts. Hence, reducing layer widths typically cannot be used to reduce network depths.
To address this, we need an approach that keeps the network connectivity while removing ﬁlters; this is achieved by our proposed channel-level bypass connections (CBCs).
The high-level concept of CBCs is “when a ﬁlter is re-moved, an input channel is bypassed to maintain the same number of output channels”. In this way, we can preserve the network connectivity when all ﬁlters are removed from a layer. Assuming the target layer in the initial network has C input channels, T ﬁlters, and Z output channels2, we gradu-ally remove ﬁlters from the layer, where there are M ﬁlters remaining. Fig. 3 illustrates how CBCs handle three cases in this process based on the relationship between the number of input channels (C) and the initial number of ﬁlters (T ) (only M changes, and C and T are ﬁxed):
• Case 1, C = T (Fig. 3a): When the i-th ﬁlter is re-moved, we bypass the i-th input channel, so the number of output channels (Z) can be kept the same. When all the ﬁlters are removed (M = 0), all the input channels are bypassed, which is the same as removing the layer.
• Case 2, C < T (Fig. 3b): We do not bypass input channels at the beginning of ﬁlter removal because we have more ﬁlters than input channels (i.e., M > C) and there are no corresponding input channels to bypass.
The bypass process starts when there are fewer ﬁlters than input channels (M < C), which becomes case 1.
• Case 3, C > T (Fig. 3c): When the i-th ﬁlter is re-moved, we bypass the i-th input channel. The extra (C − T ) input channels are not used for the bypass.
These three cases can be summarized in a rule: when the i-th ﬁlter is removed, the corresponding i-th input channel is bypassed if that input channel exists. Therefore, the number of output channels (Z) when using CBCs can be computed by Z = max(min(C, T ), M ). The proposed CBCs can be 2If we do not use CBCs, Z is equal to T . efﬁciently trained when combined with the proposed ordered dropout, as discussed in Sec. 2.3.
As a more advanced usage of T , we can treat T as a hyper-parameter. Please note that we only change M , and C and T are ﬁxed. From the formulation Z = max(min(C, T ), M ), we can observe that the function of T is limiting the number of bypassed input channels and hence the minimum number of output channels (Z). If we set T ≥ C to allow all C input channels to be bypassed, the formulation becomes Z = max(C, M ), and the minimum number of output channels is C. If we set T < C to only allow T input channels to be bypassed, the formulation becomes Z = max(T, M ), and the minimum number of output channels is T .
Setting T < C enables generating the bottleneck, where we have fewer output channels than input channels (Z < C).
The bottleneck has been shown to be effective in improving the accuracy-efﬁciency (e.g., accuracy-latency) trade-offs in MobileNetV2 [29]/V3 [25]. We take the case 1 as an example. In Fig. 3a, we can observe that the number of output channels is always 4, which is the same as the number of input channels (Z = C = 4) no matter how many ﬁlters are removed. Therefore, the bottleneck cannot be generated.
In contrast, if we set T to 2 as the case 4 in Fig. 3d, no input channels are bypassed until we remove the ﬁrst two
ﬁlters because Z = max(min(4, 2), 2) = 2. After that, it becomes the case 3 in Fig. 3c, which forms a bottleneck. 2.3. Ordered Dropout
Training the super-network involves joint training of mul-tiple sub-networks with shared weights. After the super-network is trained, comparing sub-networks of the super-network (i.e., samples) only requires their relative accuracy (e.g., sub-network A has higher accuracy than sub-network
B). Generally speaking, the more sub-networks are trained, the better the relative accuracy of sub-networks will be. How-ever, previous works usually require one forward-backward pass for training one sub-network. As a result, training more sub-networks requires more forward-backward passes and hence increases the training time.
To address this problem, we propose ordered dropout (OD) to enable training N sub-networks in a single forward-backward pass with a batch of N images. OD is inserted after each convolutional layer in the super-network and zeros 2404
(a) Case 1: Same number of input channels and initial ﬁlters. (C = T = 4) (b) Case 2: Fewer input channels than initial ﬁlters. (C = 4 < T = 6) (c) Case 3: More input channels than initial ﬁlters. (C = 4 > T = 2) (d) Case 4: Same number of in-put channels and initial ﬁlters but with a given T. (C = 4, T = 2)
Figure 3: An illustration of how CBCs handle different cases based on the relationship between the number of input channels (C) and the initial number of ﬁlters (T ) (only the number of ﬁlters remaining (M ) changes, and C and T are ﬁxed). For each case, it shows how the architecture changes with more ﬁlters removed from top to bottom. The numbers above lines correspond to the letters below lines. Please note that the number of output channels (Z) will never become zero. out different output channels for different images in a batch.
As shown in Fig. 4, OD simulates different layer widths with a constant number of output channels. Unlike the standard dropout [30] that zeros out a random subset of channels regardless of their positions, OD always zeros out the last channels to simulate removing the last ﬁlters. As a result, while sampling the search space, we can simply drop the last
ﬁlters from the super-network to evaluate samples without other operations like sorting and avoid a mismatch between training and evaluation.
When combined with the proposed CBCs, OD can train sub-networks with different network depths by zeroing out all output channels of some layers to simulate layer removal.
As shown in Fig. 5, to simulate CBCs, there is another OD in the bypass path (upper) during training, which zeros out the complement set of the channels zeroed by the OD in the corresponding convolution path (lower).
Because NAS only requires the relative accuracy of sam-ples, we can decrease the number of training iterations to further reduce the super-network training time. Moreover, for each layer, we sample each layer width almost the same number of times in a forward-backward pass to avoid biasing towards any speciﬁc layer widths. 2.4. Multi Layer Coordinate Descent Optimizer
The single-layer coordinate descent (SCD) optimizer [26], used in NetAdaptV1 [1], is a simple-yet-effective optimizer with the advantages such as supporting both differentiable and non-differentiable search metrics and having only a few interpretable hyper-parameters that need to be tuned. The
SCD optimizer runs an iterative optimization. It starts from the super-network and gradually reduces its latency (or other search metrics such as multiply-accumulate operations and energy). In each iteration, the SCD optimizer generates K
Figure 4: An illustration of how NetAdaptV2 uses the pro-posed ordered dropout to train two different sub-networks in a single forward-backward pass. The ordered dropout is inserted after each convolutional layer to simulate different layer widths by zeroing out some channels of activations.
Note that all the sub-networks share the same set of weights. samples if the super-network has K layers. The k-th sample is generated by shrinking (e.g., removing ﬁlters) the k-th layer in the best sample from the previous iteration to reduce its latency by a given amount. This amount is referred to as per-iteration resource reduction and may change from one iteration to another. Then, the sample with the best performance (e.g., accuracy-latency trade-off) will be chosen 2405
Figure 5: An illustration of how NetAdaptV2 uses the pro-posed channel-level bypass connections and ordered dropout to train a super-network that supports searching different layer widths and network depths. and used for the next iteration. The optimization terminates when the target latency is met, and the sample with the best performance in the last iteration is the discovered DNN.
The shortcoming of the SCD optimizer is that it generates samples by shrinking only one layer per iteration. This property causes two problems. First, it does not consider the interplay of multiple layers when generating samples in an iteration, which may lead to sub-optimal performance of discovered DNNs. Second, it may take many iterations to search for very deep networks because the layer with the lowest latency limits the maximum value of the per-iteration resource reduction; the lowest latency of a layer becomes small when the super-network is deep. To address these problems, we propose the multi-layer coordinate descent (MCD) optimizer. It generates J samples per iteration, where each sample is obtained by randomly shrinking L layers from the previous best sample. In NetAdaptV2, shrinking a layer involves removing ﬁlters, reducing the kernel size, or both.
Compared with the SCD optimizer, the MCD optimizer considers the interplay of L layers in each iteration so that the performance of the discovered DNN can be improved.
Moreover, it enables using a larger per-iteration resource reduction (i.e., up to the total latency of L layers) to reduce the number of iterations and hence the search time. 3.