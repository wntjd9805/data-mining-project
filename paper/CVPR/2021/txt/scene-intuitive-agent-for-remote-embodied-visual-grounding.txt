Abstract
Humans learn from life events to form intuitions toward-s the understanding of visual environments and languages.
Envision that you are instructed by a high-level instruction,
“Go to the bathroom in the master bedroom and replace the blue towel on the left wall”, what would you possibly do to carry out the task? Intuitively, we comprehend the se-mantics of the instruction to form an overview of where a bathroom is and what a blue towel is in mind; then, we navigate to the target location by consistently matching the bathroom appearance in mind with the current scene.
In this paper, we present an agent that mimics such human behaviors. Speciﬁcally, we focus on the Remote Embod-ied Visual Referring Expression in Real Indoor Environ-ments task, called REVERIE, where an agent is asked to correctly localize a remote target object speciﬁed by a con-cise high-level natural language instruction, and propose
In the ﬁrst stage, we pre-a two-stage training pipeline. train the agent with two cross-modal alignment sub-tasks, namely the Scene Grounding task and the Object Grounding task. The agent learns where to stop in the Scene Ground-ing task and what to attend to in the Object Grounding task respectively. Then, to generate action sequences, we propose a memory-augmented attentive action decoder to smoothly fuse the pre-trained vision and language repre-sentations with the agent’s past memory experiences. With-out bells and whistles, experimental results show that our method outperforms previous state-of-the-art(SOTA) signif-icantly, demonstrating the effectiveness of our method. 1.

Introduction
Vision and Language tasks, such as Vision-and-Language
[2], Visual Question Answering
Navigation (VLN)
∗Corresponding author is Guanbin Li. This work was supported in part by the National Key Research and Development Program of China (No.2020YFC2003902), in part by the Guangdong Basic and Applied Ba-sic Research Foundation under Grant No.2020B1515020048, in part by the
National Natural Science Foundation of China under Grant No.61976250 and No.U1811463. This work was also sponsored by CCF-Tencent Open
Research Fund. (VQA) [3, 4] and Referring Expression Comprehension (REF) [17, 40, 39] etc., have been extensively studied in the wave of deep neural networks. In particular, VLN [2, 5] is a challenging task that combines both natural language under-standing and visual navigation. Recent works have shown promising performance and progress. They mainly focus on designing agents capable of grounding ﬁne-grained natural language instructions, where detailed information is provid-ed, to ﬁnd where to stop, for example “Leave the bedroom and take a left. Take a left down the hallway and walk s-traight into the bathroom at the end of the hall. Stop in front of the sink” [10, 23, 37, 36, 34, 18]. However, a prac-tical issue is that ﬁne-grained natural language instructions are not always available in real life and human-machine in-teractions are mostly based on high-level instructions such as “Go to the bathroom at the end of the hallway”. In oth-er words, designing an agent that could perform high-level natural language interpretation and infer the probable target location using knowledge of the environments is of more practical use.
In this paper, we focus on the REVERIE task [30] which is an example of the above mentioned high-level instruc-tion task. Here, we brieﬂy introduce the settings. Given a high-level instruction that refers to a remote target object at a target location within a building, a robot agent spawns at a starting location in the same building and tries to navigate closer to the object. The output of the task is a bounding box encompassing the target object. The success of the task is evaluated based on explicit object grounding at the correc-t target location. A straightforward solution is to integrate
SOTA navigation model with SOTA object grounding mod-el. This strategy has proven to be inefﬁcient in [30] and instead, they proposed an interactive module to enable the navigation model to work together with the object ground-ing model. Although the performance is improved, we ob-serve that such method has a key weakness: it is unreason-able to discern high-level instruction by directly borrowing the ﬁne-grained instruction navigation model that consists of simple trainable language attention mechanism based on the fact that the perception of high-level instruction primar-ily depends on commonsense knowledge prior as well as past experiences in memory. Therefore, the overall design 7036
is not in line with human intuitions in high-level instruction navigation.
Figure 1. The overview of two pre-training tasks, the Scene
Grounding task and the Object Grounding task. The Scene
Grounding task empowers the agent the ability to reason where the target location is and the Object Grounding task learns what to attend to.
Designing an agent to solve the problem like the
REVERIE task is still under explored and there are stil-l no systematic ways to design such an agent. Then, how does human wisdom solve this task? Human beings have in-stincts to understand surrounding visual environments and languages.
Intuitively, given a high-level instruction, we would ﬁrst extract high-level what and where information and then form an overview of the appearance of the target location in mind based on common sense knowledge. Dur-ing navigation, we would consistently match current scene and objects in the scene to the instruction semantics and decide where to navigate next. According to such intuition-s, we approach this problem from a new perspective and present an agent that imitates such human behaviors. Con-cretely, we deﬁne our problem as designing an agent that is able to solve where and what problem in the REVER-IE task. We propose a two-stage training pipeline. In the
ﬁrst stage, we design two pre-training tasks, mimicking the aforementioned two human intuitions. The second stage is training the agent with a memory-augmented attentive ac-tion decoder, further increasing the agent’s navigation ca-pability under high-level instructions.
Pre-training Stage. As is shown in Fig. 1, we intro-duce a new subtask called the Scene Grounding task that is trained to recognize which viewpoint in a set of viewpoints is best aligned with the high-level instruction and another subtask called the Object Grounding task that helps the a-gent identify the best object that matches to the instruction among a set of candidate objects located at a target view-point. Experimental results show that the Scene Grounding model recognizes the target viewpoint with a high accuracy and the Object Grounding model outperforms the previous best model used in [41, 30] by more than 10%.
Action Decoding Stage.
In this stage, with the pre-trained models serving as scene and language encoders, we propose a memory-augmented attentive action decoder that leverages a scene memory structure as the agent’s internal past state memory. This design is based on the fact that the computation of action at a speciﬁc time step could de-pend on any provided information in the past. Experimental results indicate that the proposed structure is effective and achieves new state-of-the-art performance.
To sum up, this paper has the following contributions:
•
•
•
We propose a new framework that borrows human in-tuitions for designing agent capable of understanding high-level instructions, which closely integrate navi-gation and visual grounding in both training and in-ference. Speciﬁcally, the visual grounding models are pre-trained and serve as vision and language encoders for training navigation action decoder in the training phase. In inference, the action is predicted by consid-ering logits from both the visual grounding models and the navigation decoder.
We introduce two novel pre-training tasks, called
Scene Grounding task and Object Grounding task, and a new Memory-augmented attentive action decoder in our framework. The pre-training tasks attempt to help the agent learn where to stop and what to attend to, and the action decoder effectively exploits past obser-vations to fuse visual and textual modalities.
Without bells and whistles, our method outperform-s all previous methods, achieving new state-of-the-art performance on both seen and unseen environments on the REVERIE task. 2.