Abstract
We present a technique for estimating the relative 3D ro-tation of an RGB image pair in an extreme setting, where the images have little or no overlap. We observe that, even when images do not overlap, there may be rich hidden cues as to their geometric relationship, such as light source directions, vanishing points, and symmetries present in the scene. We propose a network design that can automatically learn such implicit cues by comparing all pairs of points between the two input images. Our method therefore constructs dense feature correlation volumes and processes these to predict relative 3D rotations. Our predictions are formed over a
ﬁne-grained discretization of rotations, bypassing difﬁculties associated with regressing 3D rotations. We demonstrate our approach on a large variety of extreme RGB image pairs, including indoor and outdoor images captured under dif-ferent lighting conditions and geographic locations. Our evaluation shows that our model can successfully estimate relative rotations among non-overlapping images without compromising performance over overlapping image pairs.1 1.

Introduction
Estimating the relative pose between a pair of RGB im-ages is a fundamental task in computer vision with appli-cations including 3D reconstruction [39, 34], camera lo-calization [3, 40, 42], simultaneous localization and map-ping [8, 32] and novel view synthesis [29, 38]. Standard methods for computing relative pose are highly dependent on accurate correspondence. But what if the poses are so dif-ferent that there is no overlap and hence no correspondence?
Our work takes a step towards this seemingly impossible goal of estimating relative pose for pairs of RGB images that have little or no overlap. In particular, we present a technique for estimating relative 3D rotation for a pair of images with (possibly) extreme relative motion. There are many appli-cations where dense imagery is difﬁcult to obtain that can beneﬁt from rotation estimation from non-overlapping views 1https://ruojincai.github.io/ExtremeRotation/
Figure 1: How can we estimate relative rotation between images in extreme non-overlapping cases? Above we show two non-overlapping image pairs capturing an urban street scene (top) and a church (bottom). Possible cues to their relationship include sunlight and direction of shadows in outdoor scenes (highlighted in red) and lines parallel in 3D in indoor scenes (marked with yellow and blue line segments), from which vanishing points can be derived.
[25, 1]. For example, when advertising homes on online real estate sites, users may only provide a small number of images—too sparse for current 3D reconstruction methods.
Rotation estimation can simplify downstream tasks, such as 3D reconstruction from sparse views.
How can we reason about relative rotation in extreme non-overlapping settings? As humans, there are a number of cues we might leverage. Consider the two image pairs in Fig. 1.
For the outdoor pair, we can infer relative orientation using illumination cues, e.g., by analyzing which buildings are lit or the directions of cast shadows. Geometric cues are also useful. For example, from the pair of indoor images we can infer a change in camera pitch from the set of parallel vertical 14566
W
W
H
H (cid:521),(cid:521)
Input Image Pair
Encoder
Feature Maps 4D Correlation Volume
Decoders Predicted Rotation
Figure 2: Method overview. Given a pair of images, a shared-weight Siamese encoder extracts feature maps. We compute a 4D correlation volume using the inner product of features, from which our model predicts the relative rotation (here, as distributions over Euler angles). lines in 3D (colored in yellow) that suggest a vanishing point, and we can infer a rightward camera rotation by analyzing symmetries and the layout of the benches.
Given the presence of such “hidden” cues, one approach to computing relative rotation would be to explicitly learn such cues via supervision, e.g., by labeling vanishing points and learning to predict them. However, in addition to the drawbacks of requiring additional supervision, we do not want to restrict our model to a set of handcrafted cues which may or may not be relevant for the image pair provided at test time. Instead, we want to learn to predict relative rotation from pose supervision alone. As such, we ask: Can we guide the network to reason about such hidden cues implicitly?
And what architecture would best achieve this goal?
Our key insight is that reasoning about cues such as van-ishing points and illumination—while not achievable from direct feature correspondence alone—nonetheless can be realized through comparison of local properties like line ori-entations (in the case of vanishing points) and shadows and light sources (in the case of illumination). Crucially, any pair of points between the image pair can provide evidence for their geometric relationship.
We therefore turn to correlation volumes, a tool used in correspondence tasks like optical ﬂow or stereo. In a full correlation volume, every pair of points from feature maps derived from an image pair are compared. While dense cor-relation volumes have demonstrated superior performance for tasks like optical ﬂow [46, 27, 43] and stereo matching
[33, 23, 47, 14] that compare highly overlapping images, we ﬁnd that they are also effective in ﬁnding implicit cues that are not in the form of direct correspondence. As such, we process image pairs—whether they overlap or not—by constructing a dense 4D correlation volume (see Fig. 3). This design allows us to both ﬁnd explicit pixelwise correspon-dence, in the case of overlapping pairs, as well as leverage implicit cues for non-overlapping pairs.
To estimate the relative rotation, we process the corre-lation volume with another network that computes proba-bilities estimates over a ﬁne-grained discretization over the space of 3D rotations. Our framework is end-to-end trainable and optimizes simple loss formulations, bypassing difﬁcul-ties associated with regressing 3D rotations.
We evaluate our method on a large variety of extreme
RGB image pairs, including indoor and outdoor images cap-tured in different geographic locations under varying illumi-nation. We also show that our models yield state-of-the-art performance for overlapping pairs. Our models generalize surprisingly well to new data—e.g., training a model on out-door scenes in Manhattan yields median errors below 6◦ for images captured in Pittsburgh and London. 2.