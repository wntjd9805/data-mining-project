Abstract
Cross-Entropy Loss
Seesaw Loss
Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail.
Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative sam-ples on tail classes lead to a biased learning process for classiÔ¨Åers. Consequently, objects of tail categories are more likely to be misclassiÔ¨Åed as backgrounds or head categories.
To tackle this problem, we propose Seesaw Loss to dynam-ically re-balance gradients of positive and negative sam-ples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The miti-gation factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassiÔ¨Åed instances to avoid false posi-tives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains signiÔ¨Åcant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on
LVIS dataset without bells and whistles. Code is available at https://github.com/open-mmlab/mmdetection. 1.

Introduction
Deep learning-based object detection and instance seg-mentation approaches have achieved immense success on datasets with relatively balanced category distribution, e.g.,
COCO dataset [26]. However, the distribution of cate-gories in the real world is long-tailed [28]. There are a few head classes containing abundant instances, while most other classes comprise relatively few instances.
On long-tailed datasets, existing instance segmentation gradients of positive samples on a tail class gradients of negative samples on a tail class
Mitigation
Compensation
Classification Accuracy
Instance Segmentation 
Cross-Entropy
Seesaw Loss
ùë®ùë∑
Cross-Entropy
Seesaw Loss
Sorted Category Index
Sorted Category Index
Figure 1: Seesaw Loss dynamically re-balances the gradients of positive and negative samples on a tail class with two complemen-tary factors. It mitigates the overwhelming punishments on the tail class as well as compensates them to reduce the risk of inducing false positives. In Mask R-CNN [15], Seesaw Loss achieves re-markable higher classiÔ¨Åcation accuracy of tail classes than Cross-Entropy Loss on LVIS [12] dataset. As a result, instance segmenta-tion AP on tail classes is signiÔ¨Åcantly improved, leading to better overall performance. frameworks [1, 3, 15] fail to perform as accurately as on the datasets with balanced category distribution, exhibiting unsatisfactory performance on tail classes. Figure 1 shows the classiÔ¨Åcation accuracy and instance segmentation per-formance of Mask R-CNN [15] on LVIS [12] dataset. The classiÔ¨Åer in Mask R-CNN trained by Cross-Entropy Loss tends to misclassify tail categories as backgrounds or other confusing head classes, which leads to extremely low accu-racy on tail classes. 9695
The primary reason for this undesired phenomenon is that the instances from head classes are predominant in a long-tailed dataset. These instances contribute an over-whelmingly large quantity of negative samples for tail classes. Thus, the gradients of positive and negative sam-ples on a tail class are heavily imbalanced, leading to a biased learning process for the classiÔ¨Åer. One can imag-ine that gradients of positive and negative samples resemble two objects positioned on each end of a seesaw (see Fig. 1).
To balance them, a viable solution is to shorten the arm of the heavier end in the seesaw, which is equivalent to scaling down the overwhelming gradients of negative samples on the tail class by a factor. Nevertheless, blindly reducing the gradients of negative samples increases the risk of inducing false positives of tail classes, since samples of other classes are less punished when they are misclassiÔ¨Åed as tail classes.
Thus, a specialized mechanism is needed to compensate for the excessively reduced penalties on tail classes.
In this work, we propose Seesaw Loss that dynamically re-balances positive and negative gradients for each cate-gory with two complementary factors, i.e., mitigation fac-tor and compensation factor. According to the ratio be-tween categories‚Äô cumulative sample numbers during train-ing, the mitigation factor reduces the penalty to relatively rare classes. When a false positive sample of one category is observed, the compensation factor will increase the penalty to that category. The synergy of the two above factors en-ables Seesaw Loss to mitigate the overwhelming punish-ments to tail classes as well as compensate for the risk of misclassiÔ¨Åcation caused by diminished penalties.
Seesaw Loss has three appealing properties. 1) See-saw Loss is dynamic.
It explores the ratios of cumula-tive training sample numbers between different categories and instance-wise misclassiÔ¨Åcation during training. This differs signiÔ¨Åcantly to previous solutions that rely either on static group split [23] or loss reweighting with con-stant values [2, 6, 38]. 2) Seesaw Loss is self-calibrated.
The mitigation and the compensation factor synergize to relieve the overwhelming punishments on tail classes as well as avoid increasing false positives of tail categories.
On the contrary, previous methods blindly reduce punish-ments on tail classes [38] or decrease the loss weights of head categories [6]. 3) Seesaw Loss is distribution-agnostic. It does not rely on pre-computed datasets‚Äô distri-bution [2, 6, 28, 38], and it can operate well with any data sampler [12, 17]. By accumulating the number of samples in each class, Seesaw Loss gradually approximates the real data distribution during training to achieve more accurate balancing.
Through extensive experiments, we show consistent im-provements of Seesaw Loss in different instance segmen-tation frameworks and data samplers. On the challeng-ing LVIS [12] dataset, Seesaw Loss achieves signiÔ¨Åcant improvements of 6.0% AP and 2.1% AP upon Mask R-CNN [15] with random sampler and repeat factor sam-pler [12], respectively. Even if switching to the stronger
Cascade Mask R-CNN [1], we still observe an impressive improvement of 6.4% AP and 2.3% AP with random sam-pler and repeat factor sampler. To show the versatility of
Seesaw Loss, we integrate it into the long-tailed image clas-siÔ¨Åcation task. Seesaw Loss signiÔ¨Åcantly improves the clas-siÔ¨Åcation accuracy by 6% on ImageNet-LT [28] dataset. Be-sides, we also explore the necessity of the decoupling train-ing pipeline [20, 23] in Seesaw Loss. Experimental results demonstrate that Seesaw Loss provides a simpler and more effective solution to long-tailed instance segmentation with-out relying on complex training pipelines. 2.