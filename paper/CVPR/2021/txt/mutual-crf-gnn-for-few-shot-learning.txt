Abstract
Graph-neural-networks (GNN) is a rising trend for few-shot learning. A critical component in GNN is the afﬁnity.
Typically, afﬁnity in GNN is mainly computed in the feature space, e.g., pairwise features, and does not take fully advan-tage of semantic labels associated to these features. In this paper, we propose a novel Mutual CRF-GNN (MCGN). In this MCGN, the labels and features of support data are used by the CRF for inferring GNN afﬁnities in a principled and probabilistic way. Speciﬁcally, we construct a Conditional
Random Field (CRF) conditioned on labels and features of support data to infer a afﬁnity in the label space. Such afﬁnity is fed to the GNN as the node-wise afﬁnity. GNN and CRF mutually contributes to each other in MCGN. For
GNN, CRF provides valuable afﬁnity information. For CRF,
GNN provides better features for inferring afﬁnity. Exper-imental results show that our approach outperforms state-of-the-arts on datasets miniImageNet, tieredImageNet, and
CIFAR-FS on both 5-way 1-shot and 5-way 5-shot settings. 1.

Introduction
Few-shot learning attempts to classify unlabelled data (i.e., query samples) when only a few labelled data (i.e., support samples) are available. Instead of relying on reg-ularization to compensate for the data scarcity, researchers have explored ways to learn a distribution of similar tasks (also called “meta-learning”). Meta-learning method intro-duces the concept of episodic, which means that one round of model training contains only few samples (e.g., 1 or 5) for each class. By episodic training, meta-learning methods aim to train a meta-learner that can quickly propagate labels from support samples to query samples.
Recently, Graph Neural Network (GNN) [60, 31] be-comes a rising method to transfer the knowledge from the
†This work was done when Shixiang Tang was an intern at SenseTime. (a) (b)
Label
Prediction
CRF
GNN
Class A: feature label variable
Class B: feature label variable
Binary compatibility: 
Unary compatibility: 
GNN affinity:
Figure 1. Illustration of Mutual CRF-GNN (MCGN). Green and purple indicate different classes. Unary compatibility contains la-bel information and binary compatibility contains feature informa-tion from GNN. (a): The marginal distribution for pairwise vari-ables can be used to predict the afﬁnity for GNN. (b): the marginal distribution of single variable is used for label prediction. support samples to the query samples. In particular, Garcia and Bruna [15] ﬁrst modelled the few-shot learning prob-lem as a supervised graph message passing task by deﬁning each sample in the support set and query set as a node in
GNN.
Afﬁnity, which measures the similarity between two samples/nodes, is a key component in GNN. Therefore, lots of approaches are proposed to have better afﬁnity represen-tation. EGNN [26] proposes to utilize labels for GNN afﬁn-ity initialization and propagate the edge labels for explicitly modeling the intra-cluster similarity and inter-cluster dis-similarity. DPGN [53] propose to incorporates distribution propagation with GCN and combines both distribution-level relations with instance-level relations.
In this regard, we leverage CRF [48], which is a pow-erful probabilistic graphical model, to manipulate depen-dencies between variables, to collaborate with GNN. We model the labels as random variables in CRF. In our ap-2329
proach, the marginalized distributions in the uniﬁed CRF model have two functionalities. First, the marginalized dis-tribution for single variable reﬂects the predicted possibility of label. Second, the marginalized probability of pair-wise variables deﬁnes the similarity of two samples, which is the afﬁnity for GNN. Our design of MCGN is from the follow-ing two observations for CRF and GNN.
First, for CRF, the marginalized probabilities for single variable and pair-wise variables should be obtained by fus-ing feature information and label information. The unary compatibility term of each variable is used to model the relation between the variable/sample and the correspond-ing observed label information. The binary compatibility term utilizes the feature information. Speciﬁcally, it mod-els the relation between two random variables/samples and is intuitively deﬁned by the feature similarities of two cor-responding random variables/samples. Since marginaliz-ing the states of variables in CRF requires to multiply both unary and binary compatibility terms, the marginal distribu-tion fuses the feature information and label information in a principled and probabilistic way.
Second, for GNN, its afﬁnity should be deﬁned by the probability in the label space, reﬂecting the possibility that two samples belong to the same class. Unlike typical GNN that determines the pairwise afﬁnity in the feature space, e.g. using similarity of features, afﬁnity determination by probabilities in label space merits two advantages. First, afﬁnity deﬁned in the label space is less sensitive to out-liers. Take two samples that are visually similar but belong to different classes as an example. When using feature sim-ilarities to determine afﬁnities, their afﬁnity could be large, which leads to inappropriate feature aggregation between two samples. However, such afﬁnity could be reduced in the label space because it is additionally guided by the pro-vided semantic labels, i.e., two samples have different class labels. Second, the labels given in the support set can guide the afﬁnities in a probabilistic instead of deterministic man-ner. Different with EGNN and DPGN that initialize afﬁni-ties to zero or one according to the corresponding labels, the unary compatibility term in CRF can set a tolerance for mislabelled samples, which makes our classiﬁcation more robust than original GNN-based models.
Considering the above observations, we propose a uni-ﬁed model named Mutual CRF-GNN (MCGN), where the
GNN and CRF are mutually correlated and can contribute to each other. The network consists of multiple layers and each layer alternately implements the CRF-based afﬁnity infer-ence and GNN-based feature aggregation. As illustrated in
Fig. 1, we use features in GNN to deﬁne the binary com-patibility in CRF. Next, with unary and binary compatibil-ity, we estimate the marginal distribution of each variable.
Afterwards, the obtained marginal distribution of each vari-able infers afﬁnities in GNN. At last, more robust features are obtained by aggregation in GNN, which further leads to better compatibility in the next CRF layer.
In such a feed-forward process, CRF produces better afﬁnities with compatibilities deﬁned by robust features in GNN and GNN produces robust features by taking afﬁnity inferred by CRF.
In summary, our main contributions are two folds. First, we propose to introduce CRF to GNN, where CRF helps to implement dependencies between the predictions and de-ﬁne afﬁnities of GNN in label space. Second, we propose a novel Mutual CRF-GNN where feature aggregation and re-lation inference could contribute to each other. Extensive experiments conducted on three popular datasets proved the effectiveness of Mutual CRF-GNN by a signiﬁcant im-provement in few-shot classiﬁcation accuracy. 2.