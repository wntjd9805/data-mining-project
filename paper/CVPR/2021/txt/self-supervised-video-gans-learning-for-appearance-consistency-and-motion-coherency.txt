Abstract
Appearance Space
A video can be represented by the composition of ap-pearance and motion. Appearance (or content) expresses the information invariant throughout time, and motion de-scribes the time-variant movement. Here, we propose self-supervised approaches for video Generative Adversarial
Networks (GANs) to achieve the appearance consistency and motion coherency in videos. SpeciÔ¨Åcally, the dual dis-criminators for image and video individually learn to solve their own pretext tasks; appearance contrastive learning and temporal structure puzzle. The proposed tasks enable the discriminators to learn representations of appearance and temporal context, and force the generator to synthesize videos with consistent appearance and natural Ô¨Çow of mo-tions. Extensive experiments in facial expression and hu-man action public benchmarks show that our method out-performs the state-of-the-art video GANs. Moreover, con-sistent improvements regardless of the architecture of video
GANs conÔ¨Årm that our framework is generic. 1.

Introduction
Generative Adversarial Networks (GANs) [16] are one of the major research topics in the spotlight, due to their impressive capability to model the data distribution in an unsupervised way. The recent advances in the aspects of objectives [5, 4, 26] and architectures [20, 21, 22] alleviate the chronic problems of GANs such as the mode collapse and training instability. Thanks to these sustained research efforts, the latest techniques enable us to synthesize visually plausible and diverse images.
With these advances in the image domain, the problem of generating videos has emerged in recent years. Pioneering attempts [37, 30] have started with mapping a latent vector to a video with spatio-temporal convolutions. On the other hand, the following methods [35, 38] have proposed the video generation frameworks mainly targeting to decom-pose spatio-temporal latent space into motion and content
*Corresponding author
ùíïùüé
ùíïùíè
Motion Space
Which trajectory is reasonable?
Figure 1. Illustration of conditions for realistic videos. Video can be represented as the composition of appearance and its mo-tions. For the natural and realistic video, appearances and motions in the same video have to be consistent throughout time. In other words, appearance representation should be similar among frames of the same video compared to the frames from different videos, and the Ô¨Çow of the motion would be Ô¨Åt into the temporal context. subspaces. Their efforts to disentangle the latent space to appearance and motion ones have reduced the complexity of generating videos.
In spite of the aforementioned successes, extending
GANs to the video domain is still challenging. One of the main causes is high dimensionality of videos. Compared to images, videos have one more dimension, time. The time dimension exponentially expands the video space from the image space with respect to the number of frames. In this huge space, perceptually satisfying videos could account for an extremely small portion because they have to fulÔ¨Åll not only spatial realness but also temporal coherency. There-fore, it is obvious that mapping low-dimensional latent vec-tors to visually plausible videos is a lot more complex than the case of images.
From this perspective, we suggest to reduce the video 10826
space. That is, we have to contract the possible space by ex-ploiting the prior knowledge about ‚Äúrealistic‚Äù videos. This provides essential constraints towards the realness of the synthesized videos. For instance, let us assume real videos consist of only grayscale frames instead of color ones. In this case, typical RGB representations are redundant since videos are fully represented in grayscale. Hence, we can re-duce the space by forcing the videos to have a single chan-nel with the prior knowledge in the assumption. By this, the task of GANs can be turned into a simpler one because it shrinks many possible mappings.
Then what are the major components to achieve for gen-erating ‚Äúrealistic‚Äù videos? We hypothesize two prominent constraints for realistic videos; consistency of appearance and coherency of motion. Fig. 1 describes the two types of conditions. First, appearance should be consistent over time, especially for short videos. For example, the identity of an actor should be retained over time. Second, the mo-tion should be naturally progressed with coherency along time. A physically impossible human movement can be a counterexample for it. These constraints should be satisÔ¨Åed when generating realistic videos.
To meet these necessary conditions, we present Self-supervised Video GANs (SVGAN), which imposes explicit constraints on appearance and motion with two pretext self-supervision tasks; appearance contrastive learning and tem-poral structure puzzle. Appearance contrastive learning makes the discriminator to learn the representations of ap-pearance which is invariant throughout time in videos. On the other hand, temporal structure puzzle forces the discrim-inator to Ô¨Ågure out whether the video is coherent or not in temporal ordering. Furthermore, we distill these explicit constraints to the generator so that it synthesizes videos sat-isfying those conditions in a collaborative way. Eventually, these constraints signiÔ¨Åcantly reduce the huge video space to its small portion of spots where realistic videos can exist so that the video generation problem becomes less complex.
Different from previous approaches [37, 30, 35, 38] mainly focusing on the architecture of the generator, we bring the focus into the objective of the discriminator in video GANs.
The proposed self-supervision tasks directly involve the ob-jective of the discriminator as a new direction towards real-istic videos in the literature.
Our main contributions are summarized as follows: ‚Äì We mainly focus on the objectives of the discriminator and its effects on video GANs with self-supervision.
To our best knowledge, it is the Ô¨Årst attempt to shed light on the discriminator objectives in video GANs. ‚Äì We propose Self-supervised Video GANs (SVGAN), which explicitly imposes constraints on GANs with two pretext self-supervision tasks; appearance con-trastive learning and temporal structure puzzle. They constrain GANs to synthesize videos with (1) invariant appearance through time and (2) naturally progressed
Ô¨Çow of motion, respectively. ‚Äì Our extensive experiments on challenging benchmarks of facial expressions and human actions validate that our method signiÔ¨Åcantly enhances video generation performance of the state-of-the-art techniques regard-less of the generator architectures which previous ap-proaches have mostly focused on. 2.