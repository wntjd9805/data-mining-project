Abstract
In this paper we introduce a Transformer-based approach to video object segmentation (VOS). To address compound-ing error and scalability issues of prior work, we propose a scalable, end-to-end method for VOS called Sparse Spa-tiotemporal Transformers (SST). SST extracts per-pixel rep-resentations for each object in a video using sparse atten-tion over spatiotemporal features. Our attention-based for-mulation for VOS allows a model to learn to attend over a history of multiple frames and provides suitable inductive bias for performing correspondence-like computations nec-essary for solving motion segmentation. We demonstrate the effectiveness of attention-based over recurrent networks in the spatiotemporal domain. Our method achieves com-petitive results on YouTube-VOS and DAVIS 2017 with im-proved scalability and robustness to occlusions compared with the state of the art. Code is available at https:
//github.com/dukebw/SSTVOS. 1.

Introduction
Video object segmentation (VOS) involves simultaneous tracking and segmentation of one or more objects through-out a video clip. VOS is a challenging task in which al-gorithms must overcome object appearance changes, occlu-sion and disocclusion, as well as distinguish similar objects in motion over time.
A highly performant VOS system is important in down-stream tracking applications where pixelwise tracking in-formation is useful, such as player tracking in sports ana-lytics, person tracking in security footage, and car and road obstacle tracking in self-driving vehicle applications. VOS methods are also relevant in interactive annotation of video data, where annotator time can be used more efﬁciently by using automatic video object segmentation in an annotate-predict-reﬁne loop. Our work uses VOS as a proxy task to investigate scalable algorithms for extracting spatiotempo-*Corresponding Author: brendanw.duke@gmail.com
Figure 1: We propose a Transformer-based model for video object segmentation featuring self-attention over time and over space. To segment an output frame, the model learns to look up similar regions in the temporal history and to search for reference masks. We address the high computa-tional complexity of the problem with a sparse Transformer formulation, which allows each cell to attend to each other cell over one or multiple hops. Here, interactions propagate from a given feature cell via our sparse spatiotemporal at-tention variants: (a) grid attention, and (b) strided attention. ral representations from video in general, and these algo-rithms can be re-used for yet other video prediction tasks.
Previous methods that attempt to solve VOS can be di-vided into three major categories: online ﬁnetuning, mask reﬁnement, and temporal feature propagation. Each of these categories, reviewed in detail in §2, has inherent drawbacks. 5912
Online ﬁnetuning methods cannot adapt to changes in ob-ject appearance throughout a sequence. The dominant mask reﬁnement and temporal feature propagation methods are recurrent. Due to their sequential nature, recurrent methods for VOS exhibit compounding error over time, and are not parallelizable across a single example.
Motivated by the success of Transformer architectures in NLP (see §2) we propose a novel method for semi-supervised VOS that overcomes the drawbacks of online
ﬁnetuning and sequential methods. Our method, Sparse
Spatiotemporal Transformers (SST), processes videos in a single feedforward pass of an efﬁcient attention-based net-work. At every layer of this net, each spatiotemporal fea-ture vector simultaneously interacts with all other feature vectors in the video. SST does not require online ﬁnetun-ing, although it may beneﬁt from this practice at the cost of additional runtime. Furthermore, since SST is feedforward, it avoids the compounding error issue inherent in recurrent methods. Finally, SST is fully parallelizable across a single example and can therefore take advantage of the scalability of current and future compute architectures.
Applying spatiotemporal attention operators to VOS raises two challenges: computational complexity and dis-tinguishing foreground objects. Na¨ıve spatiotemporal at-tention is square in the dimensionality of the video feature tensor, i.e., O((T HW )2C), C=# of channels. We resolve this computational complexity issue with sparse attention operators, of which we compare two promising candidates.
SST reduces feature matching FLOPs by an order of magnitude, and achieves an overall score of 81.8 on the ofﬁcial YouTube-VOS 2019 validation set, comparing favourably with prior work. Furthermore, we observed qualitatively improved robustness to occlusions using SST’s temporal buffer of preceding frame embeddings.
Contributions — We propose a Transformer-based model for VOS, and link its inductive bias to correspon-dence calculations. While there is work on Transformers for representation learning in video [28, 39], these models at-tend over time and not densely over space. There is also re-cent work that adapts Transformers to video action recogni-tion [14], however, we are unaware of work that uses Trans-formers in VOS, which requires dense predictions. We also contribute empirical evaluation of Transformer models ap-plied to VOS, and argue superiority over recurrent models.
We address computational complexity using sparse at-tention operator variants, making it possible to apply self-attention on high-resolution videos. We extend sparse at-tention variants to video so that they can be used for VOS.
Speciﬁcally we extend to 3D, with two spatial axes and one temporal axis, Criss-Cross Attention [18] from 2D semantic segmentation, and Sparse Attention [6] from 1D language translation. Our sparse video attention operators are not
VOS speciﬁc, and could be applied to other dense video prediction tasks. We provide our implementation [10, 11]. 2.