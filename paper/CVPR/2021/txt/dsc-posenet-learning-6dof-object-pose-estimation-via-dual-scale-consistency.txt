Abstract
Compared to 2D object bounding-box labeling, it is very difﬁcult for humans to annotate 3D object poses, especially when depth images of scenes are unavailable. This paper investigates whether we can estimate the object poses ef-fectively when only RGB images and 2D object annotations are given. To this end, we present a two-step pose estima-tion framework to attain 6DoF object poses from 2D object bounding-boxes. In the ﬁrst step, the framework learns to segment objects from real and synthetic data in a weakly-supervised fashion, and the segmentation masks will act as a prior for pose estimation.
In the second step, we de-sign a dual-scale pose estimation network, namely DSC-PoseNet, to predict object poses by employing a differential renderer. To be speciﬁc, our DSC-PoseNet ﬁrstly predicts object poses in the original image scale by comparing the segmentation masks and the rendered visible object masks.
Then, we resize object regions to a ﬁxed scale to estimate poses once again. In this fashion, we eliminate large scale variations and focus on rotation estimation, thus facilitating pose estimation. Moreover, we exploit the initial pose esti-mation to generate pseudo ground-truth to train our DSC-PoseNet in a self-supervised manner. The estimation results in these two scales are ensembled as our ﬁnal pose esti-mation. Extensive experiments on widely-used benchmarks demonstrate that our method outperforms state-of-the-art models trained on synthetic data by a large margin and even is on par with several fully-supervised methods. 1.

Introduction
The goal of object pose estimation is to estimate 6 de-grees of freedom (DoF) of a given object, including 3D orientations and 3D translations, with respect to a cam-era. Considering an object may undergo various lighting changes and severe occlusions, estimating accurate poses from a single RGB image is quite challenging.
Thanks to the recent advance of deep neural networks, many deep learning based pose estimation algorithms (e.g., [47, 49]) have been proposed recently and achieved
*This work was done when Zongxin Yang interned at Baidu Research.
Yi Yang is the corresponding author. promising performance. However, due to the notorious data-hunger nature of deep networks, state-of-the-art meth-ods usually require a large number of real images with 3D pose annotations for training. Unlike 2D image labeling tasks, annotating 3D object poses is difﬁcult, especially when only RGB images are provided to labelers. However, without sufﬁcient accurately annotated 3D poses of RGB images, pose estimation methods may suffer severe perfor-mance degradation [41].
Different from real images of which poses are very dif-ﬁcult to obtain, pose labels in synthetic data are easily ac-cessible. However, due to the domain gap between syn-thetic and real data, such as illumination variations and smoothed 3D reconstructed models, state-of-the-art fully-supervised pose estimation approaches would suffer drastic performance degradation when they are trained on synthe-sized images [41]. Therefore, we attempt to use minimal annotation efforts, i.e., 2D bounding boxes, to reduce the domain gap between real and synthetic images for pose es-timation. We employ bounding-boxes to ensure objects of interest are correctly identiﬁed in images rather than highly relying on synthesized data’s photorealism.
In this paper, we present a two-step object pose estima-tion framework to predict object poses by only leveraging easily obtained 2D annotations.
In the ﬁrst step, we em-ploy a weakly supervised segmentation method to distin-guish object pixels from background ones. Speciﬁcally, we
ﬁrstly initialize a segmentation network by training it on synthetic images. Motivated by [4, 48], we use the seg-mentation network to predict pseudo masks for the unla-beled real data and re-train the network on both synthetic and pseudo-labeled data. Since some pixels in the back-ground might be recognized as foreground ones, we fully exploit our 2D bounding-boxes to remove outliers and thus facilitate learning segmentation. The segmentation results, in return, provide a strong prior for pose estimation.
In the second step, we present a dual-scale pose estima-tion network, dubbed DSC-PoseNet. Our DSC-PoseNet is
ﬁrstly initialized by training on synthetic images. Then, we employ DSC-PoseNet to estimate object poses in the original image scale. DSC-PoseNet regresses an offset of each pixel to an object keypoint and employs an attention map to obtain the keypoint locations. We adopt differen-tiable EPnP [20] and a renderer [32] to achieve rendered object masks, and the intersection over union (IoU) between the segmentation results and rendered visible masks is em-ployed to train our DSC-PoseNet on real images. Once real image poses are attained, we resize object regions to a ﬁxed scale and then project the keypoints computed by our esti-mated poses to the local regions as self-supervised signals to train DSC-PoseNet once again. By doing so, we improve the feature extraction ability of DSC-PoseNet and enforce the pose estimation consistency across different scales.
As DSC-PoseNet outputs pose estimation results on two scales, we ensemble these two-scale results to improve the estimation robustness. Extensive experiments on three popular widely-used datasets demonstrate that our method achieves superior performance compared to state-of-the-art
RGB image based methods without real pose annotations.
Overall, our contributions are summarized as follows:
• We propose a weakly- and self-supervised learning based pose estimation framework to estimate object poses from single RGB images with easily obtained 2D bounding-box annotations.
• We present a self-supervised dual-scale pose esti-mation network, named DSC-PoseNet. The DSC-PoseNet signiﬁcantly alleviates the domain gap be-tween synthetic and real data by constructing cross-scale self-supervision with a differentiable renderer.
• To the best of our knowledge, our work is the ﬁrst attempt to estimate 6DoF object poses from an RGB image without using 3D pose annotations and depth images during both training and testing phases. Con-trast results show that DSC-PoseNet outperforms RGB based competitors trained on synthetic data. 2.