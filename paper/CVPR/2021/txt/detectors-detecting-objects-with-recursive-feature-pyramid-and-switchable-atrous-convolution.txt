Abstract
Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and think-ing twice. In this paper, we explore this mechanism in the backbone design for object detection. At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level, we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results us-ing switch functions. Combining them results in DetectoRS, which signiﬁcantly improves the performances of object de-tection. On COCO test-dev, DetectoRS achieves state-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance segmentation, and 50.0% PQ for panoptic segmen-tation. The code is made publicly available1. 1.

Introduction
To detect objects, human visual perception selectively enhances and suppresses neuron activation by passing high-level semantic information through feedback connections
[2, 20, 21]. Inspired by the human vision system, the mech-anism of looking and thinking twice has been instantiated in computer vision, and demonstrated outstanding perfor-mance [5, 6, 62]. Many popular two-stage object detectors, e.g., Faster R-CNN [62], output object proposals ﬁrst, based on which regional features are then extracted to detect ob-jects. Following the same direction, Cascade R-CNN [5] develops a multi-stage detector, where subsequent detector heads are trained with more selective examples. The success of this design philosophy motivates us to explore it in the neural network backbone design for object detection. In particular, we deploy the mechanism at both the macro and micro levels, resulting in our proposed DetectoRS which signiﬁcantly improves the performance of the state-of-art object detector HTC [8] by a great margin while a similar 1https://github.com/joe-siyuan-qiao/DetectoRS
Method
Backbone
APbox APmask
FPS
HTC [8]
ResNet-50
DetectoRS ResNet-50 43.6 51.3 38.5 44.4 4.3 3.9
Table 1: A glimpse of the improvements of the box and mask
AP by our DetectoRS on COCO test-dev. inference speed is maintained, as shown in Tab. 1.
At the macro level, our proposed Recursive Feature Pyra-mid (RFP) builds on top of the Feature Pyramid Networks (FPN) [48] by incorporating extra feedback connections from the FPN layers into the bottom-up backbone layers, as il-lustrated in Fig. 1a. Unrolling the recursive structure to a sequential implementation, we obtain a backbone for object detector that looks at the images twice or more. Similar to the cascaded detector heads in Cascade R-CNN trained with more selective examples, our RFP recursively enhances FPN to generate increasingly powerful representations. Resem-bling Deeply-Supervised Nets [39], the feedback connec-tions bring the features that directly receive gradients from the detector heads back to the low levels of the bottom-up backbone to speed up training and boost performance. Our proposed RFP implements a sequential design of looking and thinking twice, where the bottom-up backbone and FPN are run multiple times with their output features dependent on those in the previous steps.
At the micro level, we propose Switchable Atrous Convo-lution (SAC), which convolves the same input feature with different atrous rates [12,32,57] and gathers the results using switch functions. Fig. 1b shows an illustration of the con-cept of SAC. The switch functions are spatially dependent, i.e., each location of the feature map might have different switches to control the outputs of SAC. To use SAC in the detector, we convert all the standard 3x3 convolutional lay-ers in the bottom-up backbone to SAC, which improves the detector performance by a large margin. Some previous methods adopt conditional convolution, e.g., [43, 80], which also combines results of different convolutions as a single output. Unlike those methods whose architecture requires 10213
(a) Macro Design: Recursive Feature Pyramid. (b) Micro Design: Switchable Atrous Convolution.
Figure 1: (a) Our Recursive Feature Pyramid adds feedback connections (solid lines) from the top-down FPN layers to the bottom-up backbone layers to look at the image twice or more. (b) Our Switchable Atrous Convolution looks twice at the input features with different atrous rates and the outputs are combined together by soft switches. to be trained from scratch, SAC provides a mechanism to easily convert pretrained standard convolutional networks (e.g., ImageNet-pretrained [63] checkpoints). Moreover, a new weight locking mechanism is used in SAC where the weights of different atrous convolutions are the same except for a trainable difference.
Combining the proposed RFP and SAC results in our De-tectoRS. To demonstrate its effectiveness, we incorporate
DetectoRS into the state-of-art HTC [8] on the challenging
COCO dataset [51]. On COCO test-dev, we report box AP for object detection [23], mask AP for instance segmenta-tion [28], and PQ for panoptic segmentation [37]. DetectoRS with ResNet-50 [30] as backbone signiﬁcantly improves
HTC [8] by 7.7% box AP and 5.9% mask AP. Additionally, equipping our DetectoRS with ResNeXt-101-64x4d [77] achieves state-of-the-art 55.7% box AP and 48.5% mask AP.
Together with the stuff prediction from DeepLabv3+ [15] with Wide-ResNet-41 [11] as backbone, DetectoRS sets a new record of 50.0% PQ for panoptic segmentation. 2.