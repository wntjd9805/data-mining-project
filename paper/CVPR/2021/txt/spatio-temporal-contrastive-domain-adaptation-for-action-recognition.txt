Abstract
Compared with image-based UDA, video-based UDA is comprehensive to bridge the domain shift on both spa-tial representation and temporal dynamics. Most previ-ous works focus on short-term modeling and alignment with frame-level or clip-level features, which is not dis-criminative sufﬁciently for video-based UDA tasks. To ad-dress these problems, in this paper we propose to estab-lish the cross-modal domain alignment via self-supervised contrastive framework, i.e., spatio-temporal contrastive do-main adaptation (STCDA), to learn the joint clip-level and video-level representation alignment. Since the ef-fective representation is modeled from unlabeled data by self-supervised learning (SSL), spatio-temporal contrastive learning (STCL) is proposed to explore the useful long-term feature representation for classiﬁcation, using self-supervision setting trained from the contrastive clip/video pairs with positive or negative properties. Besides, we in-volve a novel domain metric scheme, i.e., video-based con-trastive alignment (VCA), to optimize the category-aware video-level alignment and generalization between source and target. The proposed STCDA achieves stat-of-the-art results on several UDA benchmarks for action recognition. 1.

Introduction
Unsupervised domain adaptation (UDA) has made great progress on computer vision tasks with the improvement of the representation ability on convolutional neural networks (CNNs). It aims at transferring the knowledge from the la-beled source domain with speciﬁc supervision to the target domain with unlabeled data and the different domain dis-tribution, for reducing dependencies on the comprehensive
∗ Corresponding authors.
This work was done during Xiaolin Song’s internship at Didi Chuxing. e c r u o
S t e g r a
T
Spatio-temporal Contrastive learning
Spatial ➀➂
Temporal ➁➃
Contrastive  loss
Supervised Source Classification
Spatial ➀
Temporal ➁ n o i s u
F
Classifier
Cross-entropy  loss
Video-based Contrastive Alignment
Source ➀➁
Target ➂➃
Video-level Contrastive 
Domain Metric
!'(⋅)
➀
!&(⋅)
➁
!'(⋅)
!&(⋅)
➂
➃
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
..........
Figure 1. Overview of STCDA framework. Spatio-temporal con-trastive learning (STCL) and video-based contrastive alignment (VCA) are proposed to model and align cross-domain features with long-term spatio-temporal representation. annotations and particular datasets. A great number of UDA methods have been proposed for image-based benchmarks, e.g., image classiﬁcation [39, 34, 26], object detection
[11, 1, 13], and semantic segmentation [3, 6], which supply the applications with unsupervised learning and leverage impressive performance. However, the progress on UDA for video analysis is still limited, since video-based UDA tasks are more challenging. Firstly, video-based tasks need to model the multi-dimension information, which includes richer spatial appearance and temporal dynamics than im-ages. Secondly, they require exploration of the association and interaction in the space and time dimensions. Finally, video UDA is essential to optimize the domain alignment in both spatial and temporal association. Even though larger datasets have been released with great diversity for video understanding tasks, the applications on different scenes are promoted in slow progress with limited generalization, which rely on numerous unannotated videos for representa-tion in the corresponding feature spaces.
In this paper, we address the challenging and valuable task of UDA for action recognition in videos, and explore 9787
the spatio-temporal representation to facilitate video-level modeling. Most previous UDA methods for action recog-nition [15, 4] focus on the short-term temporal representa-tion with frame-level or clip-level modeling, using adver-sarial learning to align source and target features following image-based UDA works. However, it is unreasonable to ignore long-term modeling and alignment for video predic-tion, which are essential for video-based UDA tasks.
Integrating self-supervision module is a creative explo-ration for UDA to analyze the unannotated data for more effective feature. With the development of self-supervised algorithms, conducting supervised tasks with some cus-tomized rules becomes essential to explore the intrinsic in-formation and statistical characteristics of unlabeled data.
And these customized tasks are suitable for UDA to learn the implicit properties of source and target data without any labels. These tasks are suitable for UDA to learn the implicit properties of source and target data without any labels. Fur-thermore, self-supervision on the target data alone would not exploit the performance in UDA tasks, and therefore it is more reasonable to apply self-supervision on both the source and target data. Most self-supervised learning (SSL) methods [24, 9, 22, 18] are based on generative/predictive tasks, with particular supervised functions. In contrast, con-trastive methods learn representations by contrasting posi-tive and negative samples, which are ﬂexible for improv-ing the capacity to model correlations or complex structures with latent generalization, rather than overly pay attention to detail tasks in generative methods.
Compared with pre-training on labeled data, CNNs could obtain promising representing capability with contrastive-learning-based pre-training. MoCo [10] bene-ﬁts downstream tasks with contrastive pre-training on Im-ageNet dataset, and outperforms most image-based su-pervised pre-training approaches. Thus the selection of positive-negative samples plays a decisive role in ensur-ing the quality of feature expression. Recent methods
[32, 31] exploit contrastive modeling for videos using cou-pling networks, which are trained with frame-level or clip-level positive-negative examples. However, self-supervised contrastive learning for videos has been not leveraged for long-term video-level representation. From this point of view, we propose a novel decoupling framework for video understanding based on spatio-temporal contrastive learn-ing, with jointly global(video-level)-local(clip-level) mod-eling in time dimension.
In addition, to optimize the feature alignment between source and target domains, we propose a category-aware video-level contrastive domain distance metric for UDA, which aims at improving the discrimination via cross-domain feature alignment. The alignment on clip and video levels are utilized to draw closer the samples under the same class and push apart the samples among different classes between source and target domains. Speciﬁcally, we mea-sure self-modal and cross-modal metric respectively for re-ducing spatio-temporal domain shift. As shown in Figure 1, a novel spatio-temporal contrastive domain adaptation (STCDA) framework is proposed to overcome the misalign-ment in feature space and category confusion in different domains for UDA on action recognition.
In summary, our contributions of the proposed frame-work are as follows: (1) We design a novel spatio-temporal contrastive learning (STCL) framework, for learning joint clip-level and video-level feature representations by self-supervision, which improves the generalization of local-global (2) We propose a novel domain metric, i.e., video-based contrastive align-ment (VCA), to measure the video-level discrepancy be-tween source and target domains. (3) Our proposed frame-work achieves stat-of-the-art results on several domain adaptation benchmarks for action recognition. temporal content modeling. 2.