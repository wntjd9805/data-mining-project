Abstract
We propose a novel framework for creating large-scale photorealistic datasets of indoor scenes, with ground truth geometry, material, lighting and semantics. Our goal is to make the dataset creation process widely accessible, trans-forming scans into photorealistic datasets with high-quality ground truth for appearance, layout, semantic labels, high quality spatially-varying BRDF and complex lighting, in-cluding direct, indirect and visibility components. This en-ables important applications in inverse rendering, scene understanding and robotics. We show that deep networks trained on the proposed dataset achieve competitive perfor-mance for shape, material and lighting estimation on real images, enabling photorealistic augmented reality applica-tions, such as object insertion and material editing. We also show our semantic labels may be used for segmenta-tion and multi-task learning. Finally, we demonstrate that our framework may also be integrated with physics engines, to create virtual robotics environments with unique ground truth such as friction coefﬁcients and correspondence to real scenes. The dataset and all the tools to create such datasets will be made publicly available.1 1.

Introduction
Indoor scenes represent important environments for vi-sual perception and scene understanding, for applications such as augmented reality and robotics. However, their appearance is a complex function of multiple factors such as shape, material and lighting, and demonstrates phe-nomena like signiﬁcant occlusions, shadows, interreﬂec-tions and large spatial variations in lighting. Reasoning about these underlying, entangled factors requires large-scale high-quality ground truth, which remains hard to ac-quire. While ground truth geometry can be captured us-ing a 3D scanner, it is extremely challenging (if not nearly impossible) to accurately acquire the complex spatially-varying material and lighting of indoor scenes. An alter-native is to consider synthetic datasets, but large-scale syn-1Webpage: https://ucsd-openrooms.github.io/ thetic datasets of indoor scenes with plausible geometry, materials and lighting are also non-trivial to create.
This paper presents OpenRooms, a framework for syn-thesizing photorealistic indoor scenes, with broad applica-bility across computer vision, graphics and robotics. It has several advantages over prior works, summarized in Ta-ble 1. First, rather than use artist-created scenes and as-sets, we ascribe high-quality material and lighting to RGBD scans of real indoor scenes. Beyond just the data, we provide all the tools necessary to accomplish this, allow-ing any researcher to inexpensively create such datasets.
While prior works can align CAD models to scanned point clouds [5, 26, 6], they do not explore how to assign mate-rials and lighting appropriately to build a large-scale pho-torealistic dataset. Second, we provide extensive high-quality ground truth for complex light transport that is un-matched in prior works. Our material is represented by a spatially-varying microfacet bidirectional reﬂectance distri-bution function (SVBRDF), and our lighting includes win-dows, environment maps and area lights, along with their per-pixel spatially-varying effects to account for visibility, shadows and inter-reﬂections. Third, we render photoreal-istic images with our data and tools, which include a custom
GPU-accelerated physically-based renderer.
We create an instance of such a dataset by building on existing repositories: 3D scans from ScanNet [16],
CAD model alignment [5], reﬂectance [1] and illumination
[23, 24]. The resulting dataset contains over 100K HDR im-ages with ground-truth depths, normals, spatially-varying
BRDF and light sources, along with per-pixel spatially-varying lighting and visibility masks for every light source.
We also provide per-pixel semantic labels. Besides be-ing publicly available, the dataset can be signiﬁcantly ex-tended through future community efforts based on our tools.
We also demonstrate applicability of our method to other choices for material [4] and geometry [48].
We believe that our effort will signiﬁcantly accelerate research in multiple areas. Inverse rendering tasks are di-rectly related, including single-view [17] and multi-view
[55] depth prediction, intrinsic decomposition [33, 11], ma-terial classiﬁcation [10] and lighting estimation [20, 21, 32]. 7190
!"##"$%&'()*(+,-+".
;<'+%0133'=>1+,$(3%9<&%-9
!"#23,:(#1&,.%13+
A-8,.+,(.,-$,.%-9 749#,-&,$(.,13%&' 64.-%&4.,(!7*(
#"$,3(.,&.%,813
/01--,$(+21.+,(1-$(
-"%+'(2"%-&(03"4$ 5""#(31'"4&(
.,0"-+&.40&%"-!7*(#"$,3+(13%9-#,-&(
?1.9,=+013,(+'-&<,&%0(
$1&1+,&("@(%-$"".(+0,-,+ 5">"&%0+
/,9#,-&1&%"-B"8,3(4+,.=0.,1&,$($1&1+,&+(C%&<(
D2,-5""#+ &""3+
!"#$%&'()*$"(#+,-$.#)/01%1"#$.&,%&-)2'311")4-#'#)5$%$,#% 677.&-$%&1',
Figure 1: Our framework for creating a synthetic dataset of complex indoor scenes with ground truth shape, SVBRDF and SV-lighting, along with the resulting applications. Given possibly noisy scans acquired with a commodity 3D sensor, we generate consistent layouts for room and furniture. We ascribe per-pixel ground truth for material in the form of high-quality SVBRDF and for lighting as spatially-varying physically-based representations. We render a large-scale dataset of images associated with this ground truth, which can be used to train deep networks for inverse rendering and semantic segmentation. We further motivate applications for augmented reality and robotics, while suggesting that the open source tools we make available can be used by the community to create other large-scale datasets too.
To demonstrate the efﬁcacy of the dataset, we train a state-of-the-art inverse rendering network and achieve accurate results on real images. We also demonstrate that Open-Rooms may be used for training semantic segmentation net-works [60, 15], as well as multi-task learning to jointly es-timate shape, material and semantics. Our high-quality and extensive ground truth may help better understand complex light transport in indoor scenes and enable new applications in photorealistic augmented reality, where we demonstrate object insertion, material editing light source detection as examples, and may include light editing in the future.
Studies in robotics may also beneﬁt by using our ground truth to enhance existing simulation environments
[52, 43, 53, 37]. We demonstrate this possibility by com-bining OpenRooms assets with the PyBullet engine [3] and mapping our SVBRDFs to friction coefﬁcients, to motivate navigation and rearrangement under different material and lighting. We also note that OpenRooms allows a one-to-one correspondence between real videos and simulations, which can be valuable for sim-to-real transfer [27].
In Figure 1 we illustrate the OpenRooms framework for creating large-scale, high-quality synthetic indoor datasets from commodity RGBD sensor scans and demonstrate some of the applications that our work enables. 2.