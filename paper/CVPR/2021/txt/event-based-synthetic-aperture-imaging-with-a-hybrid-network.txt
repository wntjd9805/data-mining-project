Abstract
Synthetic aperture imaging (SAI) is able to achieve the see through effect by blurring out the off-focus foreground occlusions and reconstructing the in-focus occluded targets from multi-view images. However, very dense occlusions and extreme lighting conditions may bring signiﬁcant dis-turbances to the SAI based on conventional frame-based cameras, leading to performance degeneration. To address these problems, we propose a novel SAI system based on the event camera which can produce asynchronous events with extremely low latency and high dynamic range. Thus, it can eliminate the interference of dense occlusions by measuring with almost continuous views, and simultaneously tackle the over/under exposure problems. To reconstruct the occluded targets, we propose a hybrid encoder-decoder network com-posed of spiking neural networks (SNNs) and convolutional neural networks (CNNs). In the hybrid network, the spatio-temporal information of the collected events is ﬁrst encoded by SNN layers, and then transformed to the visual image of the occluded targets by a style-transfer CNN decoder.
Through experiments, the proposed method shows remark-able performance in dealing with very dense occlusions and extreme lighting conditions, and high quality visual images can be reconstructed using pure event data. 1.

Introduction
Harsh environments, e.g. with dense occlusions and ex-treme lighting conditions, often prohibit the efﬁcient imag-ing of real scenes, due to the fact that the collected light information is very limited and moreover severely dis-turbed. Synthetic aperture imaging (SAI) tackles the prob-lem of seeing through occlusions via multi-view exposures
[18, 17], forming the light ﬁeld [29] of the target scene un-†Corresponding author
*Equal contribution
The research was partially supported by the National Natural Science
Foundation of China under Grants 61871297, the National Natural Science
Foundation of China Enterprise Innovation Development Key Project un-der Grant U19B2004 and the Fundamental Research Funds for the Central
University under Grant 2042020kf0019.
Figure 1: Prototype of the event-based synthetic aper-ture imaging (E-SAI) system and the illustrative examples of see through imaging under harsh environments via (b) the conventional frame-based SAI and (c) the event-based
SAI. Under either very dense occlusions or extreme lighting scenes, the proposed E-SAI method can successfully gener-ate high quality visual images for the occluded scenes. der occlusions. The basic idea of SAI is to extract the light information of the occluded scenes while ﬁlter out fore-ground occlusions [11, 26]. However, very dense occlu-sions and extreme lighting scenes may bring severe distur-bances, leading to serious degradation on imaging quality or even failure reconstructions (e.g., Fig. 1).
• Very dense occlusions: With conventional frame-based cameras, the light cues are captured via bright-ness intensities. Very dense occlusions will greatly de-crease the “signal”, i.e. the light from target scenes, while increase the “noise”, i.e. disturbances from fore-14235
ground occlusions, leading to considerable reduction of the Light-SNR (ratio of “signal” to “noise”).
• Extreme lighting scenes: Due to the low dy-namic range (e.g. about 60 dB), images from con-ventional frame-based cameras usually suffer from the over/under exposure problems under extreme lighting conditions. It will severely degrade the imaging qual-ity and thus reduce the conﬁdence of the light informa-tion from target scenes.
As a consequence, conventional frame-based SAI (F-SAI) often fails in these cases, and it is of great demand to de-velop new SAI methods to handle such harsh environments.
In this paper, we address the aforementioned prob-lems by presenting a novel SAI method with event cam-eras [1]. Event cameras only measure the pixel-wise bright-ness changes of scenes in an asynchronous manner, leading to many outstanding properties including extremely low la-tency (in the order of µs), high dynamic range (> 120 dB) and low power consumption [6, 1]. Instead of using frame-based intensity images, as shown in Fig. 1, event-based SAI (E-SAI) collects the light information from occluded tar-gets via event streams, representing the brightness differ-ence between the foreground occlusions and the occluded targets. This mechanism means that a higher density of occasions produces more events from occluded targets, i.e. more light information of targets can be recorded. With the low latency, event cameras can capture adequate informa-tion of the occluded object from almost continuous view-points. Thanks to the high dynamic range of event camera,
E-SAI is able to collect conﬁdent light information from oc-cluded targets even under extreme lighting conditions, mak-ing the reconstruction of scenes feasible (e.g., Fig. 1).
Although E-SAI can easily handle the aforementioned problems, we still have to answer the following question: how to effectively process the event stream and recon-struct the high quality visual images of occluded targets?
Since the working mechanism of event camera differs rad-ically from that of the frame-based one, conventional com-puter vision methods, e.g. convolutional neural networks (CNNs), cannot be directly applied to such asynchronous event streams, where the temporal and spatial information of events should be simultaneously considered [1].
The spiking neural network (SNN) [8, 7] serves as a per-fect model for integrating spatio-temporal information. Un-like other artiﬁcial neural networks, spiking neurons do not respond to stimulus in a synchronous fashion. Instead, the membrane potential of spiking neurons updates over time, and a spike will be generated whenever the membrane po-tential exceeds a speciﬁc spiking threshold. Thus the spatio-temporal information is naturally encoded in the spike po-sition and timing. Exploiting this, the inﬂuence of noise events can be further mitigated from the temporal dimen-sion, leading to the improvement of Light-SNR. However, recent researches have observed the vanishing spike phe-nomenon [10] in deep spiking layers. Thus SNNs often suffer from performance degradation when the number of layers increases.
To tackle this, we propose a hybrid neural network that contains a SNN encoder and a CNN decoder. With initial spiking layers, the spatio-temporal information of events can be efﬁciently integrated and encoded. Then, the CNN is able to decode the rich output of SNN, and effectively re-construct the visual image of occluded targets. Therefore, this architecture not only utilizes sufﬁcient information of events, but also guarantees the overall performance of re-construction.
In a nutshell, contributions of this paper are three-fold:
• We present a novel event-based SAI algorithm with systematic analysis, which can overcome the dilemma that the conventional F-SAI faces under very dense oc-clusions and extreme lighting conditions.
• We propose a hybrid SNN-CNN encoder-decoder net-work to reconstruct high quality visual images for E-SAI. By leveraging the merits of SNN and CNN, the spatio-temporal information of events can be well re-tained and utilized, and thus the occluded target can be effectively reconstructed.
• We construct an event-based SAI dataset to evaluate the proposed method, and make them available to the research community. 2.