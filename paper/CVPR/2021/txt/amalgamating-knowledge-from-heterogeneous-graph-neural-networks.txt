Abstract
In this paper, we study a novel knowledge transfer task in the domain of graph neural networks (GNNs). We strive to train a multi-talented student GNN, without accessing hu-man annotations, that “amalgamates” knowledge from a couple of teacher GNNs with heterogeneous architectures and handling distinct tasks. The student derived in this way is expected to integrate the expertise from both teach-ers while maintaining a compact architecture. To this end, we propose an innovative approach to train a slimmable
GNN that enables learning from teachers with varying fea-ture dimensions. Meanwhile, to explicitly align topolog-ical semantics between the student and teachers, we in-troduce a topological attribution map (TAM) to highlight the structural saliency in a graph, based on which the stu-dent imitates the teachers’ ways of aggregating informa-tion from neighbors. Experiments on seven datasets across various tasks, including multi-label classiﬁcation and joint segmentation-classiﬁcation, demonstrate that the learned student, with a lightweight architecture, achieves gratifying results on par with and sometimes even superior to those of the teachers in their specializations. Our code is pub-licly available at https://github.com/ycjing/
AmalgamateGNN.PyTorch. 1.

Introduction
An increasing number of pre-trained deep neural net-works (DNNs) have been generously released online for the sake of handy reproducibility [49]. As such, reusing these pre-trained models to alleviate training effort or to enhance performance, has emerged as a trending research topic in recent years. The seminal work of Hinton et al. [12], for instance, ﬁrst raises Knowledge Distillation, where a pre-trained teacher model is utilized to generate soft labels so as to learn a lightweight student model with compe-tent performance. Following this student-teacher paradigm, many other distillation-based approaches have been applied
Pool of Pre-trained Teacher Models
Teacher GNN (Segmentation)
Teacher GNN (Classification) 
Knowledge 
Amalgamation
Unlabeled Point Cloud
Versatile & Compact 
Student GNN 
Airplane
Figure 1. Illustrations of amalgamating knowledge from heteroge-neous teacher GNN models. “Teacher GNN (Segmentation)” and
“Teacher GNN (Classiﬁcation)” are pre-trained point cloud part segmentation and classiﬁcation models, respectively. Knowledge amalgamation aims to learn a multi-talented and lightweight stu-dent GNN from teacher GNNs without human annotations. to various domains and have demonstrated promising re-sults [7, 30, 47, 48, 55].
Almost all existing approaches on knowledge transfer from pre-trained models have been focused on convolu-tional neural networks (CNNs), which take data in regu-lar domains, like images, as input. Nevertheless, many other data samples take irregular forms and thereby re-sort to graph representations, calling for graph neural net-works (GNNs). The work of [43], as the ﬁrst attempt, gen-eralizes knowledge distillation to GNNs, and introduces a customized approach tailored for irregular data. In spite of the improved performance, this approach is limited to the scenario where the student learns from a single teacher, and meanwhile holds a homogeneous architecture and tackles the same task as the teacher does.
In this paper, we strive to make one step further towards knowledge transfer from pre-trained GNNs, by studying a novel knowledge amalgamation task. Our goal is to train a multi-talented student GNN, from a couple of pre-trained teacher GNNs with heterogeneous architectures and spe-cializes in different tasks, for example one working on point cloud segmentation and the other on classiﬁcation, as shown in Fig. 1. We further assume that, in the knowledge amal-gamation process, no human annotations are available. The 15709
student learned in this way is anticipated to integrate both teachers’ expertise yet comes with a compact size, making it competent for resource-constrained applications such as edge computing.
Nevertheless, such an ambitious goal is accompanied with challenges. The ﬁrst challenge regards handling graph features with varying dimensions. Unlike CNNs that take as input grid-structured data with ﬁxed channel numbers, such as RGB images, in our scenario, GNNs pre-trained on dif-ferent datasets work with distinct feature dimensions. For example, nodes in the citation network dataset Cora have 1433 features, while those in Citeseer have 3703 features.
The student GNN would therefore have to accommodate the diverse feature dimensions. The second challenge lies in en-coding topological semantics of graphs. As GNNs are de-signed to explicitly account for the topological information concealed in the graph data, aligning the topological seman-tics between teachers and the student emerges as a critical issue to be addressed in GNN knowledge amalgamation.
Towards this end, we propose a slimmable graph con-volutional operation that enables adaptive activation or de-activation of layer channels; graph data of different input channels can therefore be simultaneously accounted for un-der one student model. Furthermore, we introduce topo-logical attribution map (TAM), a general graph represen-tation scheme to highlight structural saliency in terms of information propagation from neighbors. The derived stu-dent model is enforced to produce a TAM that resembles those from the teachers, in which way the student imitates the teachers’ fashions of aggregating features to the center node. Notably, TAM is free of data labels and readily ap-plied to heterogeneous GNN architectures.
Our contribution is therefore a novel GNN-based knowl-edge amalgamation approach to train a versatile student model that covers the specialties from heterogeneous-task teachers, without human annotations. This is typically ac-complished through a slimmable graph convolutional op-eration to accommodate varying-dimension features from teachers, together with a TAM scheme for learning the teachers’ topological semantics. We evaluate the proposed method on four different tasks across various domains, in-cluding single- and multi-label node classiﬁcations, 3D ob-ject recognition, and part segmentation. Experimental re-sults demonstrate that, the learned student GNN model is competent to handle all different tasks of the heterogeneous teachers, sometimes with a performance even superior to those of the teachers, and meanwhile comes at a signiﬁcant reduction in computational cost. 2.