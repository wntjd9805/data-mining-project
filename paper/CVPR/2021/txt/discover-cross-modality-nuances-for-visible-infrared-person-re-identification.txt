Abstract
Visible-infrared person re-identiﬁcation (Re-ID) aims to match the pedestrian images of the same identity from dif-ferent modalities. Existing works mainly focus on alleviat-ing the modality discrepancy by aligning the distributions of features from different modalities. However, nuanced but discriminative information, such as glasses, shoes, and the length of clothes, has not been fully explored, especially in the infrared modality. Without discovering nuances, it is challenging to match pedestrians across modalities using modality alignment solely, which inevitably reduces feature distinctiveness.
In this paper, we propose a joint Modal-ity and Pattern Alignment Network (MPANet) to discover cross-modality nuances in different patterns for visible-infrared person Re-ID, which introduces a modality alle-viation module and a pattern alignment module to jointly extract discriminative features. Speciﬁcally, we ﬁrst pro-pose a modality alleviation module to dislodge the modality information from the extracted feature maps. Then, We de-vise a pattern alignment module, which generates multiple pattern maps for the diverse patterns of a person, to dis-cover nuances. Finally, we introduce a mutual mean learn-ing fashion to alleviate the modality discrepancy and pro-pose a center cluster loss to guide both identity learning and nuances discovering. Extensive experiments on the public
SYSU-MM01 and RegDB datasets demonstrate the superi-ority of MPANet over state-of-the-arts. 1.

Introduction
Person re-identiﬁcation (Re-ID) [3] aims at matching in-dividual pedestrian images in a query set to ones in a gallery
It is challenging due set captured by different cameras. to the variations of viewpoints, body poses, illuminations,
*Corresponding Author. (a) (b)
Infrared Modality
Visible Modality
Difference (c)
Figure 1. (a) Infrared and (b) visible pedestrian images, where the images in the same column are captured from the same identity.
The difference among the visible identities is much more obvious than that among the infrared ones due to the limited information in the infrared modality. The nuances among different infrared images in different patterns provide a great number of differences which worth it discovering. and backgrounds. Most existing person Re-ID methods
[10, 20, 21, 22, 27, 32, 37, 39, 41, 42] focus on match-ing pedestrian images captured by visible cameras which can be formulated as a single-modality matching problem.
However, these methods are not workable for images cap-tured by visible surveillance cameras under poor illumina-tion conditions (e.g., at night), from which it is difﬁcult to extract discriminative information.
Cutting-edge surveillance systems are able to automat-ically switch from visible to infrared mode, which have accumulated a signiﬁcant amount of cross-modality data.
Re-ID problem in such a cross-modality setting thereby be-comes extremely challenging, which is essentially a cross-modality retrieval problem. Compared to conventional per-14330
son Re-ID, new challenges arise from the modality by dif-ferent spectrum cameras. As shown in Fig. 1, the in-frared images of different identities in Fig. 1(a) are indis-cernible, while the visible images in Fig. 1(b) are easy to
In addition, the appearances of a person in-distinguish. ter modalities are completely different which is known as modality discrepancy. To perform visible-infrared person
Re-ID, several methods [1, 4, 12, 29, 33] have been pro-posed, which aim to alleviate the modality discrepancy by aligning features or pixel distributions. Despite the encour-aging achievement, the existing approaches still have lim-ited ability in learning discriminative features across differ-ent modalities due to the efﬁcient information buried in the infrared images that are not discovered. In cross-modality person Re-ID, the nuances in different image pairs arise in various patterns, such as the lengths of T-shirts and pants, the type of shoes, and wearing glasses or not. If this in-formation is not well discovered, the discriminability of in-frared features will be worse than the visible ones as shown in Fig. 1(c). Discovering nuances while alleviating modal-ity discrepancy plays an important role in visible-infrared person Re-ID. Quite a few ﬁne-grained person Re-ID ap-proaches [15, 24, 27, 36, 40, 43] have been proposed re-cently, which mainly brings together identity classiﬁcation, person auxiliary information into a framework to consider the details of a person. However, these methods require ad-ditional labeled priors, e.g., attributes, key points, and hu-man parsing information, looking for certain parts and treat these parts equally rather than selecting them adaptively.
Due to the lack of necessary information and the variations of modality, these methods fail to learn discriminative fea-tures in the cross-modality setting. Therefore, discovering nuances that are not fully exploited in existing methods can naturally improve the discrimination of features.
To fully explore nuanced information, we propose a novel cross-modality person Re-ID framework, termed joint Modality and Pattern Alignment Network (MPANet), which discovers cross-modality nuances while alleviating the modality discrepancy for visible-infrared person Re-ID.
As shown in Fig. 2 , the proposed MPANet framework con-sists of two Modality Alleviation Modules (MAM) to al-leviate modality discrepancy, a Pattern Alignment Module (PAM) to discover nuances in different patterns, and a mu-tual mean learning fashion to train the model with a center cluster loss and a cross-entropy loss for identity recognition.
Speciﬁcally, MAM uses an instance normalization to allevi-ate the modality discrepancy while maintaining discrimina-tive to the extend. By a light-weight generator, the pattern alignment module generates a group of pattern maps, which attend different patterns to discover nuances. The output of this module is obtained by concatenating both pattern fea-tures and the global feature. To discover nuances in an un-supervised manner, a region separation constraint is devised to ensure each pattern map attends to a different pattern. A center cluster loss is then proposed to reduce the distance among certain pattern features of the same identity while increasing the distance among the feature centers of differ-ent identities. We further apply two modality-speciﬁc clas-siﬁers to learn the identity of features from each modality and predict classiﬁcation results of the same feature with them. Moreover, modality discrepancy is alleviated by re-ducing the distribution discrepancy between the predictions of the same image generated by different modality-speciﬁc classiﬁers in a mutual mean learning fashion. Finally, these two modules are cascaded and jointly optimized in an end-to-end manner. With the above work, the features extracted by MPANet are modality-invariant and can represent the nu-ances in different patterns.
Our main contributions are summarized below:
• We address the nuances discovery and modality dis-crepancy for visible-infrared person Re-ID in a uniﬁed framework. The former is not explored in the litera-ture, while the latter is the key to matching the person across modalities.
• To discover the nuances and extract discriminative fea-tures, the pattern alignment module (PAM) is proposed to discover nuances in different patterns with a pro-posed center cluster loss and separation loss in an un-supervised manner.
• To alleviate the modality discrepancy while keeping the identity information, the modality alleviation mod-ule (MAM) is proposed which selectively applies in-stance normalization with the guide of a mutual mean learning manner. 2.