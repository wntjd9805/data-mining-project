Abstract
Deep learning models suffer from catastrophic forget-ting when trained in an incremental learning setting.
In this work, we propose a novel approach to address the task incremental learning problem, which involves training a model on new tasks that arrive in an incremental man-ner. The task incremental learning problem becomes even more challenging when the test set contains classes that are not part of the train set, i.e., a task incremental general-ized zero-shot learning problem. Our approach can be used in both the zero-shot and non zero-shot task incremental learning settings. Our proposed method uses weight rec-tiﬁcations and afﬁne transformations in order to adapt the model to different tasks that arrive sequentially. Speciﬁ-cally, we adapt the network weights to work for new tasks by
“rectifying” the weights learned from the previous task. We learn these weight rectiﬁcations using very few parameters.
We additionally learn afﬁne transformations on the outputs generated by the network in order to better adapt them for the new task. We perform experiments on several datasets in both zero-shot and non zero-shot task incremental learning settings and empirically show that our approach achieves state-of-the-art results. Speciﬁcally, our approach outper-forms the state-of-the-art non zero-shot task incremental learning method by over 5% on the CIFAR-100 dataset. Our approach also signiﬁcantly outperforms the state-of-the-art task incremental generalized zero-shot learning method by absolute margins of 6.91% and 6.33% for the AWA1 and
CUB datasets, respectively. We validate our approach us-ing various ablation studies. 1.

Introduction
Deep learning models are used to solve many real-world problems, and they have even surpassed human-level performance in many tasks. However, deep learn-ing models generally require all the training data to be
If this is not available at the beginning of the training. the case, deep learning models suffer from catastrophic
∗ The ﬁrst two authors contributed equally. forgetting [17], and their performance on the previously seen classes or tasks starts degrading. In contrast, human beings can continually learn new classes of data without losing the previously gained knowledge. To avoid catas-trophic forgetting, deep learning models should perform lifelong/continual/incremental learning [5, 18]. Deep learn-ing models also require all the classes to be present in the train set. When the test set contains classes not seen during training, the performance of these models degrades signif-icantly [27, 39]. This is known as the zero-shot learning problem. The incremental learning problem becomes even more challenging when coupled with the zero-shot learning problem. In this paper, we solve for the task incremental learning problem in the zero-shot and non zero-shot setting.
The task incremental learning problem involves training the model on one task at a time, where each task has a set of non-overlapping classes. When a new task becomes avail-able to the network for training, the previous task data is no longer accessible. Training on only the new task data causes the network to forget all the previous task knowl-edge. Therefore, the model has to prevent the forgetting of older tasks when training on new tasks. The task incremen-tal generalized zero-shot learning problem involves training the model on one task at a time, where each task has a set of unseen classes not seen during training, which is the same as the zero-shot learning setting. The objective of the model is to successfully identify the seen and unseen classes of all the trained tasks.
We propose a novel approach called Rectiﬁcation-based
Knowledge Retention (RKR) for the task incremental learn-ing problem in the zero-shot and non zero-shot setting. Our approach (RKR) learns weight rectiﬁcations to adapt the network weights for a new task. After learning these weight rectiﬁcations, we can quickly adapt the network to work for images from that task by simply applying these weight rec-tiﬁcations to the network weights. We utilize an efﬁcient technique for learning these weight rectiﬁcations to limit the model size. We also learn afﬁne transformations (scal-ing factors) for all the intermediate outputs of the network that allow better adaptation of the network to the respective task. 15282
We perform various experiments for the task incremen-tal learning problem in both the zero-shot and non zero-shot settings in order to show the effectiveness of our approach.
Using various ablation experiments, we validate the com-ponents of our approach. Our contributions can be summa-rized as follows:
• We propose a novel approach for the task incremental learning problem in the zero-shot and non zero-shot settings that learns weight rectiﬁcations and scaling factors in order to quickly adapt the network to the re-spective tasks.
• RKR introduces very few parameters during training for learning the weight rectiﬁcations and scaling fac-tors. The model size growth in our method is signif-icantly low as compared to other dynamic network-based task incremental learning methods.
• We show that experimentally our method
Rectiﬁcation-based Knowledge Retention (RKR) signiﬁcantly outperforms the existing state-of-the-art methods for the task incremental learning problem in both the zero-shot and non zero-shot settings. 2.