Abstract
Unsupervised video object segmentation (UVOS) aims at segmenting the primary objects in videos without any hu-man intervention. Due to the lack of prior knowledge about the primary objects, identifying them from videos is the ma-jor challenge of UVOS. Previous methods often regard the moving objects as primary ones and rely on optical ﬂow to capture the motion cues in videos, but the ﬂow infor-mation alone is insufﬁcient to distinguish the primary ob-jects from the background objects that move together. This is because, when the noisy motion features are combined with the appearance features, the localization of the pri-mary objects is misguided. To address this problem, we propose a novel reciprocal transformation network to dis-cover primary objects by correlating three key factors: the intra-frame contrast, the motion cues, and temporal coher-ence of recurring objects. Each corresponds to a repre-sentative type of primary object, and our reciprocal mech-anism enables an organic coordination of them to effec-tively remove ambiguous distractions from videos. Addi-tionally, to exclude the information of the moving back-ground objects from motion features, our transformation module enables to reciprocally transform the appearance features to enhance the motion features, so as to focus on the moving objects with salient appearance while re-moving the co-moving outliers. Experiments on the public benchmarks demonstrate that our model signiﬁcantly out-performs the state-of-the-art methods. Code is available at https://github.com/OliverRensu/RTNet. 1.

Introduction
Video object segmentation (VOS) aims at localizing and segmenting objects in videos. As one of the fundamen-tal tasks in computer vision, VOS has many applications, e.g., object tracking [22, 30, 51] autonomous driving [5, 13], video surveillance [45]. In speciﬁc, the existing techniques of VOS can be roughly categorized into: semi-supervised
∗Corresponding author (hesfe@scut.edu.cn).
Input
Optical Flow
Prediction from Flow
Ours
GT
Figure 1: Segmenting primary objects based on optical ﬂow is usually distracted by the co-moving outliers. We attack this problem by reciprocally transforming appearance fea-tures to motion features, and thus avoid misleading motion information from corrupting the localization of primary ob-jects. video object segmentation [14, 32, 56] in which the seg-mentation mask of the primary object(s) is given at the
ﬁrst frame, and unsupervised video object segmentation (UVOS) [31, 47, 48, 60] that aims to extract the mask of the primary object(s) without any prior knowledge. In this pa-per, we focus on the task of UVOS.
Due to the lack of prior knowledge, the UVOS models have to handle the major concern for identifying the sources of primary objects in the videos. We observe that there are three types of candidate primary objects: the salient objects in a single frame, the moving objects, and recurring objects in the video. In general, human attention will be drawn on the salient objects [18, 28, 52] within an image, thus these visually distinct objects may be the candidate primary ob-jects. However, these methods may not be applicable for identifying primary objects in videos, as human attention will naturally shift to various patterns of dynamics or mo-tions in video [9, 37]. Thus, the objects that are indistinct in a single frame but moving in video may be the primary objects according to motion cues, yet ignored by the image-based models. Besides, people also tend to memorize the objects that appear repeatedly in the video, therefore these objects may be treated as another type of primary ones.
Previous methods [24, 65] apply optical ﬂow to capture motion information. However, the optical ﬂow can hardly 15455
distinguish the dynamic background objects from the fore-ground objects. For instance, in Fig. 1, it is ambiguous to categorize the car and the billboard, or the human and the spray, into foreground and background by optical ﬂow only. Therefore, directly mapping these motion cues to the appearance features [24, 65] may misguide UVOS models when localizing the primary objects.
To address the aforementioned limitation, we propose a uniﬁed framework, Reciprocal Transformation Network (RTNet), to identify primary objects beyond the distrac-tion of co-moving outliers. Our idea is to mutually evolve and integrate the appearance and motion representations in the network such that all three types of candidate pri-mary objects can be taken into consideration and produces a holistic decision. To this end, we propose a Reciprocal
Transformation Module (RTM) within the network to en-able in-domain and cross-domain feature interactions.
In particular, the proposed reciprocal transformation scheme computes similarities for all the pairwise features including motion-motion, appearance-appearance, and appearance-motion pairs of features. The underlying information will be transformed to each other in order to replenish the ap-pearance/motion object representation and remove the am-biguity from the inconsistent appearance or the inaccurate optical ﬂow.
Applying the proposed RTM on different source fea-tures results in different types of primary object properties, i.e., 1) self-similarities of appearance and motion features lead to intra-frame contrast; 2) appearance-motion similar-ity produces motion cues; and 3) cross-frame appearance-appearance and motion-motion feature similarities yield temporal coherence. Each corresponds to one of the three types of primary objects. Besides, instead of simply skip connecting the encoder and decoder as FCN [29] does, we propose a Spatial Temporal Attentive Fusion Mod-ule (STAFM) to leverage the appearance and motion fea-tures from the corresponding encoder stage, and segment spatio-temporally consistent primary objects.
In experi-ments, we evaluate our approach against the state-of-the-art methods on public benchmarks DAVIS [34] and achieve the performance gain of 4% on region similarity J and 5% on boundary accuracy F over the second best method [65].
To sum up, the contributions of our paper are three-fold:
• We delve into three types of primary objects in videos, and present a novel reciprocal transformation network (RTNet), which is able to effectively exploit the intra-frame contrast, motion cues, and temporal coherence of recurring objects to identify and segment primary objects from the videos.
• To eliminate the co-ocurring moving outliers from the optical ﬂow and extract the moving objects with salient appearance, we propose a new reciprocal transforma-tion approach that mutually evolves the appearance features to the motion features.
• We propose a Spatial Temporal Attentive Fusion Mod-ule (STAFM) to selectively integrate the appearance and motion features.
• Our method signiﬁcantly outperforms state-of-the-art methods in the public benchmark. Even if we use a much smaller backbone and less training data, our lightweight model can still achieve comparable perfor-mance against latest competitors. 2.