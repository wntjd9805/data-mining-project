Abstract
DNN-based frame interpolation—that generates the in-termediate frames given two consecutive frames—typically relies on heavy model architectures with a huge number of features, preventing them from being deployed on systems with limited resources, e.g., mobile devices. We propose a compression-driven network design for frame interpola-tion (CDFI), that leverages model pruning through sparsity-inducing optimization to signiﬁcantly reduce the model size while achieving superior performance. Concretely, we ﬁrst compress the recently proposed AdaCoF model and show that a 10× compressed AdaCoF performs similarly as its original counterpart; then we further improve this com-pressed model by introducing a multi-resolution warping module, which boosts visual consistencies with multi-level details. As a consequence, we achieve a signiﬁcant per-formance gain with only a quarter in size compared with the original AdaCoF. Moreover, our model performs fa-vorably against other state-of-the-arts in a broad range of datasets. Finally, the proposed compression-driven frame-work is generic and can be easily transferred to other DNN-based frame interpolation algorithm. Our source code is available at https://github.com/tding1/CDFI. 1.

Introduction
Video frame interpolation is a lower level computer vi-sion task referring to the generation of intermediate (non-existent) frames between actual frames in a sequence, which is able to largely increase the temporal resolution. It plays an important role in many applications, including frame rate up-conversion [4], slow-motion generation [27], and novel view synthesis [20, 66]. Though fundamental, the problem is challenging in that the complex motion, occlusion and feature variation in real world videos are difﬁcult to esti-mate and predict in a transparent way.
∗Equal contribution. This work was done when Tianyu Ding was an intern at Applied Sciences Group, Microsoft.
†Corresponding author.
Figure 1. A challenging example consists of large motion, se-vere occlusion and non-stationary ﬁner details. From top to bot-tom: the overlaid two inputs, the ground-truth middle frame, the frame generated by AdaCoF [32], the frame generated by the 10× compressed AdaCoF, and the frame generated by our method. The compressed AdaCoF even outperforms the full one in this case.
Recently, a large number of researches have been con-ducted in this area, especially those based on deep neural networks (DNN) for their promising results in motion esti-mation [18, 26, 55, 58], occlusion reasoning [2, 27, 46] and image synthesis [19, 20, 28, 30, 66]. In particular, due to the 8001
rapid expansion in optical ﬂow [1, 60], many approaches either utilize an off-the-shelf ﬂow model [2, 41, 42, 61] or estimate their own task-speciﬁc ﬂow [27, 36, 62, 63, 45] as a guidance of pixel-level motion interpolation. However, integrating a pre-trained ﬂow model makes the whole ar-chitecture cumbersome, while with only pixel-level infor-mation the task-oriented ﬂow alone is still insufﬁcient in handling complex occlusion and blur. As opposed to this, kernel-based methods [43, 44, 46] synthesize the interme-diate frames by convolution operations over local patches surrounding each output pixel. Nevertheless, it cannot deal with large motions beyond the kernel size and it typically suffers from high computational cost. There are also hybrid methods [2, 3] that combine the advantages of ﬂow-based and kernel-based methods, but the networks are much heav-ier and thus limit their applications.
We observe a growing tendency that more and more complicated and heavy DNN-based models are designed for interpolating video frames. Most of the methods pro-posed in the past few years [2, 3, 12, 16, 27, 32, 44, 61] involve training and inference on DNN models consisting of over 20 million parameters. For example, the hybrid
MEMC-Net [3] consists of more than 70 million param-eters and requires around 280 megabytes if stored in 32-bit ﬂoating point. Normally, large models are difﬁcult to train and inefﬁcient during inference. Moreover, they are not likely to be deployed on mobile devices, which restricts their scenarios to a great extent. In the mean time, other work [14, 36, 62, 63] directly focus on simple and light-weight video interpolation algorithms. However, they ei-ther perform less competitively on benchmark datasets or are bound to speciﬁc design that lack of transferability.
In this paper, we propose a compression-driven network design for video interpolation (CDFI) that takes advantage of model compression [5, 13, 67]. To the best of our knowl-edge, we are the ﬁrst to explore the over-parameterization issue appearing in the state-of-the-art DNN models for video interpolation. Concretely, we compress the recently proposed AdaCoF [32] via ﬁne-grained pruning [67] based on sparsity-inducing optimization [7], and show that a 10× compressed AdaCoF is still able to maintain a similar benchmark performance as before, indicating a consider-able amount of redundancy in the original model. The com-pression provides us two direct beneﬁts: (i) it helps us un-derstand the model architecture in depth, which in turn in-spires an efﬁcient design; (ii) the obtained compact model makes more room for further improvements that could po-tentially boost the performance to a new level. Towards justifying the latter point, observing that AdaCoF is capa-ble of handling large motion while is short of dealing with occlusion or preserving ﬁner details, we improve upon the compact model by introducing a multi-resolution warping module that utilizes a feature pyramid representation of the
Figure 2. Pipeline of CDFI. Stage (I): compression of the base-line; Stage (II): improvements upon the compression. input frames to help with the image synthesis. As a result, our ﬁnal model outperforms AdaCoF on three benchmark datasets with a large margin (more than 1 dB of PSNR on the Middlebury [1] dataset) while is only a quarter of its ini-tial size. Note that typically it is difﬁcult to implement the same improvements on the original heavy model. Experi-ments show that our model also performs favorably against other state-of-the-art methods.
In short, we present a compression-driven framework for video interpolation, in which we take a step back with reﬂections on over-parameterization. We ﬁrst compress
AdaCoF and obtain a compact model but performs simi-larly well, then we improve on top of it. The pipeline of
CDFI is illustrated in Figure 2. This retrospective approach leads to superior performance and can be easily transferred to any other DNN-based frame interpolation algorithm. 2.