Abstract
Quantization-based methods are widely used in LiDAR points 3D object detection for its efﬁciency in extracting context information. Unlike image where the context infor-mation is distributed evenly over the object, most LiDAR points are distributed along the object boundary, which means the boundary features are more critical in LiDAR points 3D detection. However, quantization inevitably in-troduces ambiguity during both the training and inference stages. To alleviate this problem, we propose a one-stage and voting-based 3D detector, named Point-Voxel-Grid Net-work (PVGNet). In particular, PVGNet extracts point, voxel and grid-level features in a uniﬁed backbone architecture and produces point-wise fusion features.
It segments Li-DAR points into foreground and background, predicts a 3D bounding box for each foreground point, and performs group voting to get the ﬁnal detection results. Moreover, we observe that instance-level point imbalance due to oc-clusion and observation distance also degrades the detec-tion performance. A novel instance-aware focal loss is pro-posed to alleviate this problem and further improve the de-tection ability. We conduct experiments on the KITTI and
Waymo datasets. Our proposed PVGNet outperforms previ-ous state-of-the-art methods and ranks at the top of KITTI 3D/BEV detection leaderboards. 1.

Introduction
In autonomous driving, 3D object detection is a critical task that has received signiﬁcant attention from both indus-try and academia [8, 10, 11, 20, 24]. LiDAR sensors are the most widely used 3D sensors. The generated point clouds provide accurate distance measurements and geometric in-formation for environment understanding. Different from 2D images, 3D point clouds are irregular, sparse, and un-evenly distributed. As shown in Fig. 1, the LiDAR points are mainly located along the object boundary. Therefore,
*corresponding author: zhenwei.mzw@alibaba-inc.com
Figure 1. An example of an object with representation of (a) image, (b) grid-quantization of LiDAR points, (c) raw LiDAR points. it is important to extract the boundary feature. Moreover, compared with images, the boundary of an object in LiDAR points is crystally clear with its background without ambi-guity. Under such LiDAR points merits, effectively extract-ing the object boundary features is critical for 3D detection.
According to the type of input point cloud representa-tion, 3D object detectors can be divided into two categories: point-based and quantization-based methods. Point-based detectors [18, 19] extract local features by searching for lo-cal neighborhoods from a set of speciﬁcally selected key points. This local search process is time-consuming, pre-venting point-based detectors from being widely adopted in real-time autonomous driving systems. Quantization-based methods include voxel-based and grid-based ones. Voxel-based detectors [32] split point clouds into evenly spaced voxels and use sparse 3D convolution [7] for computational acceleration. Grid-based detectors [10] project point clouds onto 2D grids and utilize 2D convolution for feature extrac-tion. The quantization inevitably loses detailed geometric information and leads to an ambiguous object boundary.
This ambiguity will introduce training uncertainty and de-grade the discrimination of object boundary representation.
To alleviate the above quantization problem and take ad-vantage of the efﬁcient feature representation, we propose a uniﬁed architecture Point-Voxel-Grid Network (PVGNet).
As illustrated in Fig. 2, PVGNet extracts these multi-level features simultaneously and applies a point-wise supervi-sion signal on the point-level fusion feature. As a result, detailed geometric information and rich semantic informa-tion are integrated into a single feature vector. The crystal-lized object boundary in LiDAR points is preserved in the
PVGNet from the supervision signal. A bottom-up detec-tion head is used to predict a 3D bounding box for each 3279
Figure 2. Our proposed PVGNet is a one-stage 3D object detector using LiDAR point clouds, which integrates point, voxel and grid-level features in a uniﬁed backbone network. foreground point, followed by a group voting algorithm to generate the ﬁnal detection results.
As a one-stage bottom-up detector, the training of
PVGNet faces severe sample imbalance problems. In ad-dition to the imbalance of foreground/background points, we observe that the imbalance of instance-level points also exists. Speciﬁcally, an unoccluded and closer instance usu-ally contains more points than an occluded and farther in-stance. Therefore, we propose a novel instance-aware focal loss (IAFL) to alleviate the imbalance problem and further boost the detection performance.
Our contributions are summarized as follows:
• We propose PVGNet framework by extracting point, voxel, and grid-level features in a uniﬁed backbone and detecting 3D objects in a bottom-up one-stage manner.
This framework enables learning integrated point-wise features for accurate object bounding box prediction.
• We propose a group voting-based method to generate the ﬁnal detection results, which is more efﬁcient and accurate than NMS based post-processing.
• We propose a new instance-aware focal loss for deal-ing with instance-level foreground point number im-balance. Extensive experiments are carried out to ex-plore its effectiveness.
Unlike the grid feature map-based method such as PVR-CNN [20] and HVNet [29], the proposed PVGNet is a bottom-up 3D detector with point-level supervision, which segments point clouds and generates bounding boxes simul-taneously in one-stage manner. Different from the Hough voting in VoteNet [16] that is used to generate votes for a reﬁnement deep network module, the group voting in our algorithm is proposed to directly generate the ﬁnal detection results from the point-level 3D bounding box predictions. 2.