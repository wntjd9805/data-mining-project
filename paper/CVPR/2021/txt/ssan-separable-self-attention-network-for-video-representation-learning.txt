Abstract
Self-attention has been successfully applied to video rep-resentation learning due to the effectiveness of modeling long range dependencies. Existing approaches build the dependencies merely by computing the pairwise correla-tions along spatial and temporal dimensions simultane-ously. However, spatial correlations and temporal corre-lations represent different contextual information of scenes and temporal reasoning. Intuitively, learning spatial con-textual information ﬁrst will beneﬁt temporal modeling. In this paper, we propose a separable self-attention (SSA) module, which models spatial and temporal correlations se-quentially, so that spatial contexts can be efﬁciently used in temporal modeling. By adding SSA module into 2D CNN, we build a SSA network (SSAN) for video representation learning. On the task of video action recognition, our ap-proach outperforms state-of-the-art methods on Something-Something and Kinetics-400 datasets. Our models often outperform counterparts with shallower network and fewer modalities. We further verify the semantic learning abil-ity of our method in visual-language task of video retrieval, which showcases the homogeneity of video representations and text embeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by SSA signiﬁcantly improve the state-of-the-art performance. 1.

Introduction
Video representation learning is crucial for tasks such as detection, segmentation and action recognition. Although 2D and 3D CNN based approaches have been extensively explored to capture the spatial-temporal correlations for these tasks, learning strong and generic video representa-tions is still challenging. One possible reason is that videos contain not only rich semantic elements within individual frames, but also the temporal reasoning across time which links those elements to reveal the semantic-level informa-tion for actions and events. Effective modeling of long
∗The work was done when the author was with MSRA as an intern.
Figure 1. Comparison between separable self-attention and spatial-temporal self-attention on Kinetics. The red points are query regions, and the arrows point to the relevant regions (Top-10 attention weights) with the query regions. Separable self-attention learns more action related regions (hand and basket). range dependencies among pixels is essential to capture such contextual information, which current CNN operations can hardly achieve. RNN based methods [23] have been used for this purpose. However, they suffer from the high computational cost. More importantly, RNN cannot estab-lish the direct pairwise relationship between positions re-gardless of their distance.
Self-attention mechanism has been recognized as an ef-fective way to build long range dependencies.
In natural language processing, self-attention based transformer [35] has been successfully used to capture contextual informa-tion from sequential data, e.g., sentences. Recent efforts have also introduced self-attention to computer vision do-main for visual tasks such as segmentation and classiﬁca-tion [40, 47, 37, 14]. The work from Wang et al. [37] pro-posed a generic self-attention form, i.e., non-local mean, for video action recognition, which builds pairwise corre-lations for pixel locations from space and time simultane-ously. However, the correlations from space and time rep-resent different contextual information. The former often relates to scenes and objects, and the latter often relates to temporal reasoning for actions (short-term activities) and 12618
events (long-term activities). Human cognition always no-tices scenes and objects before their actions. Learning cor-relations along spatial and temporal dimensions together might capture irrelevant information, leading to the ambigu-ity for action understanding. This drawback becomes even worse for videos with complex activities. To efﬁciently cap-ture the correlations in videos, decoupling the spatial and temporal dimensions is necessary. Meanwhile, short-term temporal dependencies should also be considered for cap-turing episodes of complex activities.
In this paper, we fully investigate the relationship be-tween spatial and temporal correlations in video, and pro-pose a separable self-attention (SSA) module, which can efﬁciently capture spatial and temporal contexts for tempo-ral modeling.
In our design, spatial self-attention is ﬁrst performed independently for input frames. The attention maps, which convey the spatial contextual information, are then aggregated along temporal dimension and sent to tem-poral attention module. In this way, the spatial contextual information will help better capture the temporal correla-tions for both short-term and long-term, so that the actions in videos can be fully understood.
We verify our approach on video action recogni-tion task on Something-Something and Kinetics datasets.
Something-Something (V1&V2) contains ﬁne-grained video action classes with high temporal reasoning, e.g.,
“Moving something across a surface without it falling”.
By comparing with state-of-the-art 3D [26, 39] and 2D
[20, 36, 6, 44] based methods, our models show the supe-rior performance. Moreover, our methods can outperform counterparts with shallower network structure, i.e., ResNet-50 vs. ResNet-101, and fewer modalities, i.e., RGB-only vs. RGB and optical ﬂow. Since the goal of our design is to capture semantic information as well as possible, we further demonstrate the effectiveness of SSA in video-language task, i.e., video retrieval, which searches candidate video clips by text query. As text embeddings contain explicit semantic information, the homogeneity of video represen-tations and text embeddings can better prove the efﬁciency of video learning method. 2.