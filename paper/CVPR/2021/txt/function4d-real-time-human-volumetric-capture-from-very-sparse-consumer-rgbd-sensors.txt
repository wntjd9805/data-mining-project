Abstract
Human volumetric capture is a long-standing topic in computer vision and computer graphics. Although high-quality results can be achieved using sophisticated off-line systems, real-time human volumetric capture of complex scenarios, especially using light-weight setups, remains challenging. In this paper, we propose a human volumet-ric capture method that combines temporal volumetric fu-sion and deep implicit functions. To achieve high-quality and temporal-continuous reconstruction, we propose dy-namic sliding fusion to fuse neighboring depth observa-tions together with topology consistency. Moreover, for de-tailed and complete surface generation, we propose detail-preserving deep implicit functions for RGBD input which can not only preserve the geometric details on the depth inputs but also generate more plausible texturing results.
Results and experiments show that our method outperforms existing methods in terms of view sparsity, generalization capacity, reconstruction quality, and run-time efﬁciency. 1.

Introduction
Real-time volumetric capture of human-centric scenar-ios is the key to a large number of applications ranging from telecommunications, education, entertainment, and so on. And the underlying technique, volumetric capture, is a challenging and long-standing problem in both computer vision and computer graphics due to the complex shapes, fast motions, and changing topologies (e.g., human-object manipulations and multi-person interactions) that need to be faithfully reconstructed. Although high-end volumetric capture systems [4, 13, 25, 5, 27, 36] based on dense camera rigs (up to 100 cameras [8]) and custom-designed lighting conditions [47, 14] can achieve high-quality reconstruction, they all suffer from complicated system setups and are lim-ited to professional studio usage.
In contrast, light-weight volumetric/performance capture systems are more practical and attractive. Given a pre-scanned template,
[22, 59, 15] track dense surface de-formations from single-view RGB input [17, 18]. How-ever, the prerequisite of a ﬁxed-topology template restricts their applications for general volumetric capture. In 2015,
DynamicFusion [33] proposed the ﬁrst template-free and single-view dynamic 3D reconstruction system. The fol-lowing works [53, 54, 51, 42] further improve the recon-struction quality for human performance capture by incor-porating semantic body priors. However, it remains chal-lenging for them to handle large topological changes like dressing or taking-off clothes. Recently, a line of research
[32, 37, 38, 23] leverages deep implicit functions for tex-tured 3D human reconstruction only from a single RGB image. However, they still suffer from off-line reconstruc-tion performance [38, 37] or over-smoothed, temporally discontinuous results [23]. State-of-the-art real-time volu-metric capture systems are volumetric fusion methods like
Fusion4D [10] and Motion2Fusion [9]. But both of them rely on custom-designed high-quality depth sensors (up to 15746
120 fps and 1k resolution) and multiple (up to 9) high-end
GPUs, which is infeasible for consumer usage.
In this paper, we propose Function4D, a volumetric cap-ture system using very sparse (as sparse as 3) consumer
RGBD sensors. Compared with existing systems, our sys-tem is able to handle various challenging scenarios, in-cluding human-object manipulations, dressing or taking off clothes, fast motions and even multi-person interactions, as shown in Fig. 1.
Our key observations are: To generate complete and tem-poral consistent results, current volumetric fusion methods have to fuse as much temporal depth observations as pos-sible. This results in heavy dependency on accurate and long-term non-rigid tracking, which is especially challeng-ing under severe topology changes and large occlusions. On the contrary, deep implicit functions are good at complet-ing surfaces, but they cannot recover detailed and temporal continuous results due to the insufﬁcient usage of depth in-formation and severe noise from consumer RGBD sensors.
To overcome all the limitations above, we propose a novel volumetric capture framework that organically com-bines volumetric fusion with deep implicit functions. By introducing dynamic sliding fusion, we re-design the vol-umetric fusion pipeline to restrict tracking and fusion in a sliding window and ﬁnally got noise-eliminated, topology-consistent, and temporally-continuous volumetric fusion re-sults. Based on the sliding fusion results, we propose detail-preserving deep implicit functions for ﬁnal surface recon-struction to eliminate the heavy dependency on long-term tracking. Moreover, by encoding truncated projective SDF (PSDF) values explicitly and incorporating attention mech-anism into the multi-view feature aggregation stage, our networks not only achieve detailed reconstruction results but also orders of magnitude faster than existing methods.
Our contributions can be summarized as:
• The ﬁrst real-time volumetric capture system which com-bines volumetric fusion with deep implicit functions us-ing very sparse consumer RGBD sensors.
• Dynamic Sliding Fusion for generating noise-eliminated and topology consistent volumetric fusion results.
• Detail-preserving Implicit Functions speciﬁcally de-signed for sufﬁcient utilization of RGBD information to generate detailed reconstruction results.
• The training and evaluation dataset, which contains 500 high-resolution scans of various poses and clothes, will be publicly available to stimulate future research. 2.