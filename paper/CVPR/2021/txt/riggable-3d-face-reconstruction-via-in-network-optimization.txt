Abstract
This paper presents a method for riggable 3D face recon-struction from monocular images, which jointly estimates a personalized face rig and per-image parameters includ-ing expressions, poses, and illuminations. To achieve this goal, we design an end-to-end trainable network embedded with a differentiable in-network optimization. The network
ﬁrst parameterizes the face rig as a compact latent code with a neural decoder, and then estimates the latent code as well as per-image parameters via a learnable optimization.
By estimating a personalized face rig, our method goes be-yond static reconstructions and enables downstream appli-cations such as video retargeting. In-network optimization explicitly enforces constraints derived from the ﬁrst prin-ciples, thus introduces additional priors than regression-based methods. Finally, data-driven priors from deep learn-ing are utilized to constrain the ill-posed monocular setting and ease the optimization difﬁculty. Experiments demon-strate that our method achieves SOTA reconstruction accu-racy, reasonable robustness and generalization ability, and supports standard face rig applications. 1.

Introduction 3D face reconstruction has been an important research topic due to the increasing demands on 3D face understand-ing in ﬁelds like AR/VR, communication, games, and secu-∗Corresponding authors rity. Some approaches go beyond merely estimating static reconstructions and aim to reconstruct face rigs, which are personalized parametric models that can produce 3D faces under different expressions of a speciﬁc person. The rig can either be used on character animations such as face retarget-ing and voice puppetry, or on 3D face tracking serving as a personalized prior to ease the tracking difﬁculty.
When 3D data is available, various approaches [7,23,24] have been proposed to automatically reconstruct face rigs in the forms of blendshapes. Progress has also been made to develop more sophisticated rigs based on anatomical con-straints [44] and deep neural networks [27, 45] to faith-fully capture facial details. However, these methods heav-ily depend on the 3D data provided by specialized equip-ment such as dense camera/lighting arrays and depth sen-sors, which limits the application realms.
To release the restricted hardware requirements, meth-ods were enhanced to work on monocular imagery. Given the ill-posedness of monocular reconstruction, algorithms usually use a low dimensional parametric face model as priors, e.g., 3D morphable model (3DMM) [5] and multi-linear model [9, 43], whose parameters are estimated via the analysis-by-synthesis optimization [14, 16, 19, 49]. Ad-ditional components such as corrective basis [16], shading-based dynamic details [16, 21], image-based representa-tion [10], as well as hair [10, 19] and other secondary com-ponents [21] are adopted to further personalize the esti-mated rig. However, these approaches may assume spe-ciﬁc properties of the input, e.g., requiring the subject to be static and in the neutral pose for a portion of the in-6216
put [10, 21]; need manual intervention [21]; and are often inefﬁcient [16].
The recent boom in deep learning also advanced monoc-ular 3D face reconstruction. Various learning-based meth-ods were proposed to regress face model parameters or face shapes [30, 34, 42], learn with novel supervisions
[13, 17, 32], build better face models [38–41], as well as in-tegrate with traditional multi-view geometry [3]. Neverthe-less, these methods mainly focus on static reconstructions and fail to produce personalized face rigs. Very recently,
Chaudhuri et al. [11] used neural networks for regressing blendshape face rigs from monocular images. Despite the appealing textures produced by their method, the estimated 3D geometry, which is an important aspect for 3D recon-struction, still has considerable room for improvement.
In this paper, we propose a monocular riggable 3D face reconstruction algorithm. The riggable reconstruction con-sists of a personalized face rig and per-image parameters in-cluding expressions, poses, and illuminations. Our method is an end-to-end trainable network embedded with a dif-ferentiable in-network optimization. Two modules are in-volved. One is a neural decoder conditioned on the in-put images to parameterize the face rig into a latent code (termed as rig code) to control the person-speciﬁc aspects (e.g. identity). The other is a learnable optimization that estimates the rig code and the per-image parameters.
Our main novelty is the integration of deep learning and optimization for face rig. In contrast to prior static recon-struction methods [3], our riggable reconstruction can be re-animated by another face or even voices, enabling ex-tra applications such as face retargeting and voice puppetry.
Different from previous learning-based methods [11] that directly regress rig parameters, our in-network optimization iteratively solves rig parameters with explicit constraints governed by the ﬁrst-principles (e.g. multi-view consis-tency, landmark alignment, and photo-metric reconstruc-tion), achieving better geometry accuracy and good data generalization. Unlike traditional optimizations [16, 21] us-ing hand-crafted priors, we adopt a learned deep rig model and a learned optimization to leverage deep priors to con-strain the ill-posedness and ease the hardness of the op-timization. Our method is able to achieve state-of-the-art (SOTA) reconstruction accuracy, reasonable robustness and generalization ability, and can be used in standard face rig applications as demonstrated in experiments. 2.