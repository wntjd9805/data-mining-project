Abstract
Trimmed	support	videos
This paper introduces the task of few-shot common ac-tion localization in time and space. Given a few trimmed support videos containing the same but unknown action, we strive for spatio-temporal localization of that action in a long untrimmed query video. We do not require any class labels, interval bounds, or bounding boxes. To address this challenging task, we introduce a novel few-shot transformer architecture with a dedicated encoder-decoder structure op-timized for joint commonality learning and localization pre-diction, without the need for proposals. Experiments on re-organizations of the AVA and UCF101-24 datasets show the effectiveness of our approach for few-shot common action lo-calization, even when the support videos are noisy. Although we are not speciﬁcally designed for common localization in time only, we also compare favorably against the few-shot and one-shot state-of-the-art in this setting. Lastly, we demonstrate that the few-shot transformer is easily extended to common action localization per pixel.
Few-shot transformer
Untrimmed	query	video
Figure 1: Few-shot common action localization in time and space. Given a few trimmed support videos sharing a common action, our proposed few-shot transformer is able to localize the spatio-temporal tubelet of the common action in a long untrimmed query video, without requiring the action class label, or any temporal or spatial annotations. 1.

Introduction
The goal of this paper is to localize an action in video time and space, without the need for class labels, interval bounds or box annotations. Class-agnostic action propos-als, for either temporal, e.g. [13, 25, 32] or spatio-temporal, e.g., [18, 48, 50] action localization, have the same goal.
However, to be effective they need to generate many propos-als and a secondary supervised step to ﬁnd the best ﬁtting one. To avoid the need for extensive supervision, Feng et al. [7] introduced one-shot localization of actions in time and space. They rely on proposals as well, but rather than us-ing class supervision, a matching model between a trimmed support video and a long untrimmed video determines the best proposal. In similar spirit, Yang et al. [45] introduced common action localization in time. Given a few trimmed support videos containing the same (unknown) action, they are able to localize with the aid of proposals an action in a long untrimmed video. Their setup also avoids the need for temporal and class annotations. In this work, we extend upon both [7] and [45] and propose the new task of few-shot common action localization in time and space. Our approach does not require any box annotations or class labels to obtain the spatio-temporal localization, and neither do we need pro-posals as in [7, 45]. All we require are a handful of trimmed videos showing a common unnamed action, see Figure 1.
Our approach to few-shot common action localization is inspired by the success of transformers [1] in object detec-tion as demonstrated by Carion et al. [3]. Their approach eliminates the need for proposals and the accompanying hand-crafted components, while maintaining competitive performance. Such a proposal-free method avoids the needle-in-the-haystack problem with proposals in object detection, a problem which is even more severe in spatio-temporal action localization. Further, they exploit the versatile and powerful relation modeling capability of transformers. This is naturally suitable for common action localization as well, where the core challenge is to model the commonality be-tween the few support videos and the single query video. To that end, we propose a transformer with an encoder-decoder structure that is adapted for our task of few-shot common action localization in time and space. 16031
We make three contributions. First, we introduce the task of few-shot common action localization in time and space. We localize the spatio-temporal tube encapsulating an unknown action, based only on the commonality be-tween a long untrimmed query video and a few trimmed support videos containing the same unknown action. Sec-ond, we propose a few-shot transformer with a dedicated encoder-decoder structure optimized for joint commonal-ity learning and localization prediction, without the need for proposals. As a third contribution, we reorganize the videos in AVA [11] and UCF101-24 [36] to allow for eval-uation of few-shot common action localization in time and space. Our experiments show the effectiveness of our ap-proach, even when support videos are noisy. Moreover, we demonstrate compatibility to few-shot and one-shot tempo-ral action localization, outperforming the respective state-of-the-art. Last but not least, we show that the few-shot transformer easily extends to common action localization per pixel. Code: https://github.com/PengWan-Yang/few-shot-transformer. 2.