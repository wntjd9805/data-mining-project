Abstract
Training deep networks for semantic segmentation re-quires large amounts of labeled training data, which presents a major challenge in practice, as labeling seg-mentation masks is a highly labor-intensive process. To address this issue, we present a framework for semi-supervised semantic segmentation, which is enhanced by self-supervised monocular depth estimation from unlabeled image sequences. In particular, we propose three key con-tributions: (1) We transfer knowledge from features learned during self-supervised depth estimation to semantic seg-mentation, (2) we implement a strong data augmentation by blending images and labels using the geometry of the scene, and (3) we utilize the depth feature diversity as well as the level of difﬁculty of learning depth in a student-teacher framework to select the most useful samples to be annotated for semantic segmentation. We validate the pro-posed model on the Cityscapes dataset, where all three modules demonstrate signiﬁcant performance gains, and we achieve state-of-the-art results for semi-supervised se-mantic segmentation. The implementation is available at https://github.com/lhoyer/improving_segmentation_ with_selfsupervised_depth. 1.

Introduction
Convolutional Neural Networks (CNNs)
[31] have achieved state-of-the-art results for various computer vi-sion tasks including semantic segmentation [36, 4]. How-ever, training CNNs typically requires large-scale annotated datasets, due to millions of learnable parameters involved.
Collecting such training data relies primarily on manual an-notation. For semantic segmentation, the process can be particularly costly, due to the required dense annotations.
For example, annotating a single image in the Cityscapes dataset took on average 1.5 hours [8].
Recently, self-supervised learning has shown to be a promising replacement for manually labeled data. It aims to learn representations from the structure of unlabeled data, instead of relying on a supervised loss, which in-volves manual labels. The principle has been successfully applied in depth estimation for stereo pairs [14] or im-age sequences [69]. Additionally, semantic segmentation is known to be tightly coupled with depth. Several works have reported that jointly learning segmentation and super-vised depth estimation can beneﬁt the performance of both tasks [57]. Motivated by these observations, we investigate the question: How can we leverage self-supervised depth estimation to improve semantic segmentation?
In this work, we propose a threefold approach to utilize self-supervised monocular depth estimation (SDE) [14, 69, 15] to improve the performance of semantic segmentation and to reduce the amount of annotation needed. Our contri-butions span across the holistic learning process from data selection, over data augmentation, up to cross-task repre-sentation learning, while being uniﬁed by the use of SDE.
First, we employ SDE as an auxiliary task for seman-tic image segmentation under a transfer learning and multi-task learning framework and show that it noticeably im-proves the performance of semantic segmentation, espe-cially when supervision is limited. Previous works only cover full supervision [29], pretraining [23], or improving
SDE instead of segmentation [18]. Second, we propose a strong data augmentation strategy, DepthMix, which blends images as well as their labels according to the geometry of the scenes obtained from SDE. In comparison to previous methods [65, 43], DepthMix explicitly respects the geomet-ric structure of the scenes and generates fewer artifacts (see
Fig. 1). And third, we propose an Automatic Data Selection for Annotation, which selects the most useful samples to be 11130
annotated in order to maximize the gain. The selection is iteratively driven by two criteria: diversity and uncertainty.
Both of them are conducted by a novel use of SDE as proxy task in this context. While our method follows the active learning cycle (model training → query selection → an-notation → model training) [49, 62], it does not require a human in the loop to provide semantic segmentation labels as the human is replaced by a proxy-task SDE oracle. This greatly improves ﬂexibility, scalability, and efﬁciency, espe-cially considering crowdsourcing platforms for annotation.
The main advantage of our method is that we can learn from a large base of easily accessible unlabeled image se-quences and utilize the learned knowledge to improve se-mantic segmentation performance in various ways. In our experimental evaluation on Cityscapes [8], we demonstrate signiﬁcant performance gains of all three components and improve the previous state-of-the-art for semi-supervised segmentation by a considerable margin. Speciﬁcally, our method achieves 92% of the full annotation baseline per-formance with only 1/30 available labels and even slightly outperforms it with only 1/8 labels. Our contributions sum-marize as follows: (1) To the best of our knowledge, we are the ﬁrst to utilize
SDE as an auxiliary task to exploit unlabeled image sequences and signiﬁcantly improve the performance of semi-supervised semantic segmentation. (2) We propose DepthMix, a strong data augmentation strategy, which respects the geometry of the scene and achieves, in combination with (1), state-of-the-art re-sults for semi-supervised semantic segmentation. (3) We propose a novel Automatic Data Selection for An-notation based on SDE to improve the ﬂexibility of ac-tive learning. It replaces the human annotator with an
SDE oracle and lifts the requirement of having a hu-man in the loop of data selection. 2.