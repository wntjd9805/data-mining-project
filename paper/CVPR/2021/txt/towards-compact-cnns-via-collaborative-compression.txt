Abstract
Channel pruning and tensor decomposition have re-ceived extensive attention in convolutional neural network compression. However, these two techniques are tradition-ally deployed in an isolated manner, leading to signiÔ¨Åcant accuracy drop when pursuing high compression rates. In this paper, we propose a Collaborative Compression (CC) scheme, which joints channel pruning and tensor decom-position to compress CNN models by simultaneously learn-ing the model sparsity and low-rankness. SpeciÔ¨Åcally, we
Ô¨Årst investigate the compression sensitivity of each layer in the network, and then propose a Global Compression
Rate Optimization that transforms the decision problem of compression rate into an optimization problem. After that, we propose multi-step heuristic compression to remove re-dundant compression units step-by-step, which fully con-siders the effect of the remaining compression space (i.e., unremoved compression units). Our method demonstrates superior performance gains over previous ones on vari-ous datasets and backbone architectures. For example, we achieve 52.9% FLOPs reduction by removing 48.4% pa-rameters on ResNet-50 with only a Top-1 accuracy drop of 0.56% on ImageNet 2012. 1.

Introduction
Remarkable achievements have been attained by convo-lutional neural networks (CNNs), such as object classiÔ¨Åca-tion [16, 36, 7], detection [34, 33] and segmentation [1].
*Equal contribution.
‚Ä†Corresponding author.
However, the explosive growth of parameters and computa-tional cost in CNN models has restricted their deployment on resource-limited devices, such as mobile or wearable de-vices. To this end, extensive efforts have been made for
CNN compression and acceleration, including but not lim-ited to, parameter pruning [40, 28, 19], tensor decomposi-tion [17, 22, 41] and quantization [13, 45].
Parameter pruning and tensor decomposition are two widespread directions in CNN compression, which both aim to remove intrinsic redundancy in parameters with dif-ferent removing strategies. Parameter pruning removes correlated weight connections [5, 6] or structured neurons
[28, 10, 24] based on importance measurement methods, resulting in sparse weight structures. In contrast, tensor de-composition approximates weights of low-rank Ô¨Ålters based on the intrinsic low-rankness of parameters [44, 41, 22, 14].
It is thus a natural thought to combine these two compres-sion strategies, which might lead to the signiÔ¨Åcant accu-racy drop when pursuing high compression rate. For in-stance, Dubey et al. [4] proposed to compress the weights by sequentially employing pruning and tensor decomposi-tion, which assumes that they are complementary without any mutual inÔ¨Çuence. However, as demonstrated in pre-vious work [43], although the pruning and decomposition explore different redundancy in parameters, they are not completely orthogonal. Thus, the above method [4] does not exploit the complementary nature of pruning and de-composition, which is sub-optimal as exploring only within each sub-task (i.e., each compression method), not from the global compression scope.
To leverage the beneÔ¨Åts of both compression operations, training-aware methods [43, 29, 20] use two regulariza-tions to separately handle the sparsity on channel pruning 16438
Global Compression 
Rate Optimization
Multi-step Heuristic 
Compression
CNN
Compression 
Sensitivity Learning
Compression Rate 
Decision
Step 1
Compute ùëÉ"
Remove Least-T
#,%
‚Ä¶
Step N
Compute ùëÉ"
Remove Least-T
#,&
Compact
CNN
Fine-tuning
Figure 1: The framework of our method. We Ô¨Årst compute the group values between information loss and compression rate in each layer and then obtain the compression rate of each layer by solving a global compression rate optimization problem based on them. After that, we compress each layer independently by removing the less important compression units step-by-step based on the removed units and the exploration of remaining compression space. and the low-rankness on tensor decomposition, which are jointly minimized the regularization loss during training.
They show that simultaneously handle sparsity and low-rankness in weights can explore the richer structure infor-mation of parameters. However, these methods are difÔ¨Å-cult to control the compression rate, which needs to adjust hyper-parameters by trial-and-error to achieve the trade-off between compression rate and model accuracy.
In this paper, we propose a novel uniÔ¨Åed compression framework, named collaborative compression (CC), to si-multaneously deal with the sparsity and low-rankness in weights, with an essential innovation in automatic compres-sion rate control. Our method is a post-training compres-sion algorithm that simultaneously explores the sparsity and low-rankness in pre-trained networks, which is easier to ap-ply than the training-aware methods. As shown in Fig. 1, towards a fast and practical compression, we Ô¨Årst determine the compression rate of each layer by global compression rate optimization and then compress each layer indepen-dently by multi-step heuristic compression. It avoids deter-mining the compression rate and the compression strategy (i.e., which compression units should be removed) simul-taneously. In particular, the global compression rate opti-mization analyzes the compression sensitivity of each layer by constructing the relationship between the proposed in-formation loss and the compression rate. We Ô¨Ånd that this relationship can be Ô¨Åtted well by an exponential function, and the compression sensitivity can be viewed as the Ô¨Årst-order derivative of this exponential function. Based on the exponential model, we construct an optimization process to search the best compression rate of each layer. After deter-mining the compression rate, we compress each layer syn-chronously and independently. Considering that removing units will affect the importance of the remaining compres-sion units, we propose a multi-step heuristic compression method, which removes less important units step-by-step.
At each step, the importance calculation of each unit fully considers the effect of removed compression units and the remaining compression space.
In experiments, we demonstrate the effectiveness of the proposed CC framework using four widely-used net-works (VGGNet, GoogleNet, ResNet and DenseNet) on two datasets (CIFAR-10 and ImageNet 2012). Compared to the state-of-the-art methods, CC achieves superior per-formance. For example, we obtain 52.9% FLOPs reduction by removing 48.4% parameters, with only a Top-1 accu-racy drop of 0.56% on ResNet-50. Meanwhile, the com-pressed MobileNet-V2 obtained by our method achieves performance gains over the state-of-the-art pruning meth-ods based on AutoML. 2.