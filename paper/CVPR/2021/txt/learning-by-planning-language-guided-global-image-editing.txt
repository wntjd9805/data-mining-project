Abstract
Request: Make the image lighter
Operation Planning Input
Output
Language-Guided Image Editing Input target #% intermediate images #$ input #!
Recently, language-guided global image editing draws increasing attention with growing application potentials.
However, previous GAN-based methods are not only con-ﬁned to domain-speciﬁc, low-resolution data but also lack-ing in interpretability. To overcome the collective difﬁcul-ties, we develop a text-to-operation model to map the vague editing language request into a series of editing operations, e.g., change contrast, brightness, and saturation. Each op-eration is interpretable and differentiable. Furthermore, the only supervision in the task is the target image, which is insufﬁcient for a stable training of sequential decisions.
Hence, we propose a novel operation planning algorithm to generate possible editing sequences from the target im-age as pseudo ground truth. Comparison experiments on the newly collected MA5k-Req dataset and GIER dataset show the advantages of our methods. Code is available at https://github.com/jshi31/T2ONet. 1.

Introduction
Image editing is ubiquitous in our daily life, especially when posting photos on social media such as Instagram or
Facebook. However, editing images using professional soft-ware like PhotoShop requires background knowledge for image processing and is time-consuming for the novices who want to quickly edit the image following their inten-tion and post to show around. Furthermore, as phones and tablets becoming users’ major mobile terminal, people pre-fer to take and edit photos on mobile devices, making it even more troublesome to edit and select regions on the small screen. Hence, automatic image editing guided by the user’s voice input (e.g. Siri, Cortana) can signiﬁcantly alleviate such problems. We research global image editing via language: given a source image and a language edit-ing request, generate a new image transformed under this request, as ﬁrstly proposed in [34]. Such a task is chal-lenging because the model has to not only understand the language but also edit the image with high ﬁdelity. Rule-based methods [22, 21] transfer the language request into sentence templates and further map the templates into a se-≈
"!
""
"#
Contrast (-1.1)
Tone Curve
Color Curve
Figure 1. Language-Guided Global Image Editing: given the in-put image I0 and the request, we predict a sequence of actions at to edit the image progressively with a series of intermediate im-ages It generated. And the ﬁnal edited image is our output, which should accord with the request. Operation Planning: the input im-age I0 and target image Ig are given, and we plan a sequence of action to make the ﬁnal edited image reach the target image Ig. quence of executable editing operations. However, they re-quire additional language annotations and suffer from un-speciﬁc editing requests. [30] directly maps the language to operations with the capability to accept the vague editing re-quest, yet still need the operation annotation for training. A more prevalent track is the GAN-based method [34], which models the visual and textual information by inserting the image and language features into a neural network genera-tor that directly outputs the edited image. However, GAN-based models lack the interpretability about how an image was edited through a sequence of common editing opera-tions (e.g. tone, brightness). Thus, they fail to allow users to modify the editing results interactively. Moreover, GANs struggle with high-resolution images and is data-hungry.
To provide an interpretable yet practical method for language-guided global image editing, in this paper, we pro-pose a Text-to-Operation Network (T2ONet). The network sequentially selects the best operations from a set of prede-ﬁned everyday editing operations to edit the image progres-sively according to the language’s comprehension and the visual editing feedback. As the operations are resolution-independent, such method will not deteriorate the image resolution. Fig. 1 shows the process of mimicking human experts for professional photo editing and opens the possi-bility for human-computer interactions in future work. 13590
One crucial difﬁculty for training our model is the lack of supervision information for editing sequences—we do not have access to intermediate editing operations and their parameters. The only available supervision is the input im-age’s tuple, the target image, and the language editing re-quest. One possible solution is to train our model by Re-inforcement Learning (RL). For example, the model can try different editing sequences and get rewards by compar-ing the edited images to the target images. However, it is well-known that RL is highly sensitive to hyper-parameters and hard to train when the action space is large (e.g. high-dimensional continuous action). On the other hand, it is demanding yet infeasible to collect annotations for all inter-mediate operations and their parameters in practice. There-fore, a novel training schema is expected to solve our task.
To overcome this difﬁculty, we devise a weakly-supervised method to generate pseudo operation supervision. Inspired from the classical forward search planning [29], we propose an operation-planning algorithm to search the sequence of operations with their parameters that can transform the in-put image into the target image, as shown in Fig. 1. It works as an inverse engineering method to recover the editing pro-cedure, given only the input and the edited images. Such searched operations and parameters serve as pseudo super-vision for our T2ONet. Also, as the target image is used as the pixel-level supervision, we prove its equivalence to RL.
Besides, we show the potential of the planning algorithm to be extended to local editing and used to edit a new image directly.
In summary, our contributions are fourfold. First, we propose T2ONet to predict interpretable editing operations for language-guided global image editing dynamically. Sec-ond, we create an operation planning algorithm to obtain the operation and parameter sequence from the input and target images, where the planned sequences help train T2ONet effectively. Third, a large-scale language-guided global image editing dataset MA5k-Req is collected.
Fourth, we reveal the connection between pixel supervision and
RL, demonstrating the superiority of our weakly-supervised method compared with RL and GAN-based methods on
AM5k-Req and GIER [30] datasets through both quantita-tive and qualitative experimental results. 2.