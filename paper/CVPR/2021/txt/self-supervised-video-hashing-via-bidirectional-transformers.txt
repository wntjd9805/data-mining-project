Abstract
Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efﬁ-cient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transform-ers (BTH). Based on the encoder-decoder structure of trans-formers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we fur-ther develop a similarity reconstruction task by establish-ing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Ex-tensive experiments implemented on three public bench-mark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach. 1.

Introduction
Scalable video retrieval aims at automatically seeking similar videos from a large database related to the con-tent of a query video. Because of the ever-growing abun-dance of videos from a variety of social media and search engines, developing effective video retrieval technologies has become an urgent need.
In order to meet the expec-tations of efﬁcient scalable retrieval and low storage cost, hashing methods have won lots of interests which trans-forms high-dimensional data to compact binary codes while preserving the similarity structure between data [3, 5–8, 13, 23, 25, 27, 28, 41, 43]. Among them, learning-based hash-ing which leverages data properties or label supervision to learn reliable hash functions has achieved promising perfor-mance in image retrieval. Compared with images, however,
*Corresponding author
Figure 1: Basic idea of BTH. We design BTH model as bidirectional transformers to better capture the correlations among frames. Then we design a self-supervised learning framework to learn discriminative binary codes. structured data such as videos have not been adequately studied in hashing ﬁeld [9, 10, 18, 20, 34, 35, 47–49]. For one thing, exploiting the correlations between video frames adds difﬁculties to video hashing. For another, lack of large-scale labeled dataset such as ImageNet [31] brings further challenges to capture the similarity structure in video data.
Therefore, developing effective unsupervised video hashing methods is urgently needed.
State-of-the-art unsupervised video hashing approaches focus on exploiting the inherent visual information in each video by using deep neural networks such as Recurrent
Neural Networks (RNNs) [1, 17, 19, 35, 35, 49]. Some of them further equip the network with manually constructed similarity guidance [19, 35]. However, most of these meth-ods are built on unidirectional models which underuse the bidirectional correlations among frames. Besides, they fail to adequately exploit the similarity structure in the video data due to suboptimal similarity matrix construction.
In this paper, we propose a self-supervised video hash-ing method based on bidirectional transformers, which ad-13549
equately exploits the correlations among frames within a video and similarity structure between videos. Unlike most existing video hashing methods which exploit unidirec-tional correlations among frames, we build our hash model on basis of masked language models to capture the bidi-rectional correlations [4, 36]. To better unveil the similar-ity structure among videos, we design a pairwise similarity reconstruction objective. Speciﬁcally, we derive a way to establish reliable and effective pairwise similarity connec-tions in the video space. Moreover, we develop a pseudo center set from the training dataset and enforce cluster as-signment on the latent outputs of transformers to learn more discriminative hash functions. Figure 1 depicts the basic idea of BTH. We conduct extensive experiments on three widely-used video datasets, FCVID [14], ActivityNet [12],
YFCC [38]. The experimental results demonstrate the ad-vantage of BTH over state-of-the-art unsupervised video hashing methods. 2.