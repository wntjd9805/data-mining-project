Abstract
We present deep neural network methodology to recon-struct the 3d pose and shape of people, including hand ges-tures and facial expression, given an input RGB image. We rely on a recently introduced, expressive full body statisti-cal 3d human model, GHUM, trained end-to-end, and learn to reconstruct its pose and shape state in a self-supervised regime. Central to our methodology, is a learning to learn and optimize approach, referred to as HUman Neural De-scent (HUND), which avoids both second-order differentia-tion when training the model parameters, and expensive state gradient descent in order to accurately minimize a semantic differentiable rendering loss at test time. Instead, we rely on novel recurrent stages to update the pose and shape pa-rameters such that not only losses are minimized effectively, but the process is meta-regularized in order to ensure end-progress. HUND’s symmetry between training and testing makes it the ﬁrst 3d human sensing architecture to natively support different operating regimes including self-supervised ones. In diverse tests, we show that HUND achieves very competitive results in datasets like H3.6M and 3DPW, as well as good quality 3d reconstructions for complex imagery collected in-the-wild. 1.

Introduction
Automatic 3d human sensing from images and video would be a key, transformative enabler in areas as diverse as clothing virtual apparel try-on, ﬁtness, personal well-being, health or rehabilitation, AR and VR for improved communi-cation or collaboration, self-driving systems with emphasis to urban scenarios, special effects, human-computer inter-action or gaming, among others. Applications in shopping, telepresence or ﬁtness would increase human engagement and stimulate collaboration, communication, and the econ-omy, during a lock-down.
The rapid progress in 3D human sensing has recently relied on volumetric statistical human body models [24, 43] and supervised training. Most, if not all, state of the art architectures for predicting 2d, e.g., body keypoints [5] or 3d, e.g., body joints, kinematic pose and shape [30, 48, 16, 19, 36, 8, 17, 20, 2, 44, 18, 40, 15, 29, 45, 49, 33, 37, 27, 26, 14] rely, ab initio, at their learning core, on complete supervision.
For 2d methods this primarily enters as keypoint or semantic segmentation annotations by humans, but for complex 3D articulated structures human annotation is both impractical and inaccurate. Hence for most methods, supervision comes in the form of synchronous 2d and 3d ground truth, mostly available in motion capture datasets like Human3.6M [13] and more recently also 3DPW [41].
Supervision-types aside, the other key ingredient of any successful system is the interplay between 3d initialization using neural networks and non-linear optimization (reﬁne-ment) based on losses computed over image primitives like keypoints, silhouettes, or body part semantic segmentation maps. No existing feedforward system, particularly a monoc-ular one, achieves both plausible 3d reconstruction and veridical image alignment1 without non-linear optimization – a key component whose effectiveness for 3d pose estimation has been long since demonstrated [34, 35].
The challenge faced by applying non-linear optimization in high-dimensional problems like 3d human pose and shape estimation stems from its complexity. On one hand, ﬁrst-order model state updates are relatively inefﬁcient for very ill-conditioned problems like monocular 3d human pose estimation where Hessian condition numbers in the 10−3 are typical [34]. Consequently, many iterations are usually necessary for good results, even when BFGS approxima-tions are used. On the other hand, nonlinear output state optimization is difﬁcult to integrate as part of parameter learning, since correct back-propagation would require po-tentially complex, computationally expensive second-order 1To be understood in the classical model-based vision sense of best
ﬁtting the model predictions to implicitly or explicitly-associated image primitives (or landmarks), within modeling accuracy. 114484
updates, for the associated layers. Such considerations have inspired some authors [19] to replace an otherwise desirable integrated learning process, with a dual system approach, where multiple non-linear optimization stages, supplying potentially improved 3d output state targets, are interleaved with classical supervised learning based on synchronized 2d and 3d data obtained by imputation. Such intuitive ideas have been shown to be effective practically, but remain ex-pensive in training, and lack not just an explicit, integrated cost function, but also a consistent learning procedure to guarantee progress, in principle. Moreover, applying the system symmetrically, during testing, would still require po-tentially expensive non-linear optimization for precise image alignment.
In this paper, we take a different approach and replace the non-linear gradient reﬁnement stage at the end of a classical 3d predictive architecture with neural descent, in a model called HUND (Human Neural Descent). In HUND, recurrent neural network stages reﬁne the state output (in this case the 3d human pose and shape of a statistical GHUM model [43]) based on previous state estimates, loss values, and a context encoding of the input image, similarly in spirit to non-linear optimization. However, differently from models relying on gradient-based back-ends, HUND can be trained end-to-end using stochastic gradient descent, offers no asymmetry between training and testing, supports the possibility of po-tentially more complex, problem-dependent step updates compared to non-linear optimization, and is signiﬁcantly faster. Moreover, by using such an architecture, symmetric in training and testing, with capability of reﬁnement and self-consistency, we show, for the ﬁrst time, that a 3d human pose and shape estimation system trained from monocular images can entirely bootstrap itself. The system would thus no longer necessarily require, the completely synchronous supervision, in the form of images and corresponding 3d ground truth conﬁgurations that has been previously un-avoidable. Experiments in several datasets, ablation studies, and qualitative results in challenging imagery support and illustrate the main claims.