Abstract
The precise localization of 3D objects from a single im-age without depth information is a highly challenging prob-lem. Most existing methods adopt the same approach for all objects regardless of their diverse distributions, leading to limited performance for truncated objects.
In this pa-per, we propose a ﬂexible framework for monocular 3D ob-ject detection which explicitly decouples the truncated ob-jects and adaptively combines multiple approaches for ob-ject depth estimation. Speciﬁcally, we decouple the edge of the feature map for predicting long-tail truncated ob-jects so that the optimization of normal objects is not inﬂu-enced. Furthermore, we formulate the object depth estima-tion as an uncertainty-guided ensemble of directly regressed object depth and solved depths from different groups of keypoints. Experiments demonstrate that our method out-performs the state-of-the-art method by relatively 27% for the moderate level and 30% for the hard level in the test set of KITTI benchmark while maintaining real-time efﬁ-ciency. Code will be available at https://github. com/zhangyp15/MonoFlex. 1.

Introduction 3D object detection is an indispensable premise for machines to perceive the physical environment and has been widely used in autonomous driving and robot navi-gation. In this work, we focus on solving the problem with only information from monocular images. Most existing methods for 3D object detection require the LiDAR sen-sors [22, 33, 35, 40, 41, 49] for precise depth measurements or stereo cameras [8, 24, 37, 45] for stereo depth estimation, which greatly increases the implementation costs of practi-cal systems. Therefore, monocular 3D object detection has been a promising solution and received much attention in the community [2, 3, 7, 10, 13, 20, 27, 31, 34].
For the challenging localization of 3D objects, most ex-*Corresponding author (a) M3D-RPN [3] (b) D4LCN [13] (c) Baseline (d) Ours
Figure 1: Qualitative comparison among prior arts [3, 13], our baseline, and the proposed method. The cyan and pink bounding boxes represent detected cars and pedestrians.
Our approach can effectively detect the heavily truncated object highlighted by the red arrow. isting methods handle different objects with a uniﬁed ap-proach. For example, [10, 25, 28, 52] utilize fully convo-lutional nets to predict objects of diverse distributions with shared kernels. However, we observe that the equal and joint processing of all objects can lead to unsatisﬁed per-formance: (1) As shown in Figure 1, the heavily truncated objects can be hardly detected by state-of-the-art meth-ods [3, 13] but these objects are important to the safety of autonomous vehicles. (2) We empirically found that these hard samples can increase the learning burden and affect the prediction of general objects. Thus, uniﬁed approaches can fail in both ﬁnding every object and predicting precise 3D locations. To this end, we propose a ﬂexible detector that considers the difference among objects and estimates their 3D locations in an adaptive way. Since the estimation of an object’s 3D location is usually decomposed into ﬁnding the projected 3D center and the object depth [10, 28, 36, 52], we also consider the ﬂexibility from these two aspects.
To localize the projected 3D center, we divide objects according to whether their projected centers are “inside” or
“outside” the image. Then we represent inside objects ex-actly as the projected centers and outside objects as deli-cately chosen edge points so that two groups of objects are handled by the inner and edge regions of the feature map re-spectively. Considering it is still difﬁcult for convolutional 3289
ﬁlters to manage spatial-variant predictions, the edge fusion module is further proposed to decouple the feature learning and prediction of outside objects.
To estimate the object depth, we propose to combine dif-ferent depth estimators with uncertainty estimation [18, 19].
The estimators include direct regression [10, 25, 36, 52] and geometric solutions from keypoints [2, 5]. We observe that computing depth from keypoints is usually an over-determined problem, where simply averaging results from different keypoints [5] can be sensitive to the truncation and occlusion of keypoints. As a result, we further split keypoints into M groups, each of which is exactly sufﬁ-cient for solving the depth. To combine M keypoint-based estimators and the direct regression, we model their uncer-tainties and formulate the ﬁnal estimation as an uncertainty-weighted average. The proposed combination allows the model to ﬂexibly choose more suitable estimators for robust and accurate predictions.
Experimental results on KITTI [14] dataset demonstrate that our method signiﬁcantly outperforms all existing meth-ods, especially for moderate and hard samples. The main contributions of this paper can be summarized in two as-pects: (1) We reveal the importance to consider the dif-ference among objects for monocular 3D object detection and propose to decouple the prediction of truncated objects; (2) We propose a new formulation for object depth estima-tion, which utilizes uncertainties to ﬂexibly combine inde-pendent estimators. 2.