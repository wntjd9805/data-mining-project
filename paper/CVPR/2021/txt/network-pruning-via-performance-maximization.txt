Abstract
Channel pruning is a class of powerful methods for model compression. When pruning a neural network, it’s ideal to obtain a sub-network with higher accuracy. How-ever, a sub-network does not necessarily have high accu-racy with low classiﬁcation loss (loss-metric mismatch).
In the paper, we ﬁrst consider the loss-metric mismatch problem for pruning and propose a novel channel pruning method for Convolutional Neural Networks (CNNs) by di-rectly maximizing the performance (i.e., accuracy) of sub-networks. Speciﬁcally, we train a stand-alone neural net-work to predict sub-networks’ performance and then max-imize the output of the network as a proxy of accuracy to guide pruning. Training such a performance prediction net-work efﬁciently is not an easy task, and it may potentially suffer from the problem of catastrophic forgetting and the imbalance distribution of sub-networks. To deal with this challenge, we introduce a corresponding episodic memory to update and collect sub-networks during the pruning pro-cess.
In the experiment section, we further demonstrate that the gradients from the performance prediction network and the classiﬁcation loss have different directions. Exten-sive experimental results show that the proposed method can achieve state-of-the-art performance with ResNet, Mo-bileNetV2, and ShufﬂeNetV2+ on ImageNet and CIFAR-10. 1.

Introduction
Convolutional Neural Networks (CNNs) have demon-strated great successes in many computer vision and ma-chine learning applications, like classiﬁcation [32], de-tection [47, 48], action recognition [51] and self-driving cars [2]. To achieve better performances on these tasks, the
*This work was partially supported by NSF IIS 1845666, 1852606, 1838627, 1837956, 1956002, 2040588. design of CNNs becomes more and more complex in terms of depth and width [52, 13, 21] since AlexNet [32]. How-ever, the huge consumption of computing power and mem-ory footprint prevents these complex CNNs to be deployed on embedded or mobile devices. To overcome this problem, model compression emerges as a promising solution to get a compact sub-network from the original model. Popular model compression techniques include weight pruning [12], quantization [3], structural pruning [34] and so on.
Channel pruning, which belongs to structural pruning, effectively reduces FLOPs and memory footprint from the original model without any post-processing steps. On the contrary, weight pruning or quantization usually requires speciﬁcally designed software or hardware to achieve ac-tual acceleration. As a result, we aim to develop a novel model compression method for channel pruning.
In the context of channel pruning, a sub-network with high accuracy is believed as a good candidate for the ﬁnal solution [16, 37]. To ﬁnd such a sub-network, many existing channel pruning approaches [28, 57, 58] use the classiﬁca-tion loss as guidance. However, the classiﬁcation loss is not always a good approximation to the accuracy, which is also termed as loss-metric mismatch [20]. To tackle this prob-lem, we train a performance prediction network to predict the accuracy of sub-networks. We then guide the search of sub-networks by directly maximizing the accuracy. In ad-dition, we do not abandon the classiﬁcation loss (usually, cross-entropy loss), and both classiﬁcation loss and per-formance maximization are considered in the ﬁnal pruning problem deﬁned in Eq. 8, which is inspired by the idea of multi-objective learning. The rationale behind this is that both the classiﬁcation loss and performance maximization provide useful but different information for pruning, and merging them will lead to better results.
The training of the performance prediction network has several difﬁculties. How to collect samples for training this network is not clear. Random sampling often produces triv-9270
ial sub-networks (performance near-random guessing). To get meaningful sub-networks, we start from the original net-work and prune it to the given budget using a differentiable pruning approach. We can directly use the sub-network and mini-batch accuracy as a sample to train the performance prediction network during this pruning process. However, only using the latest sub-network will lead to catastrophic forgetting [10], where the performance prediction network may forget the information about previous sub-networks.
To tackle this issue, we use an episodic memory module to collect samples along the pruning trajectory. Directly us-ing these samples is problematic since the accuracy distri-bution of these samples is far from uniform. This problem is solved by re-sampling these samples. With above tech-niques, the performance prediction network is incremen-tally trained during the pruning process. After the perfor-mance prediction network visits enough samples and is con-ﬁdent enough, it is then put into the pruning process to pro-vide additional supervision for channel pruning. Since the training of the performance prediction network and pruning proceed simultaneously, there is no extra cost.
Our main contributions can be summarized as follows: 1) We propose a novel channel pruning method for CNNs by directly maximizing the accuracy of sub-networks.
To the best of our knowledge, this is the ﬁrst paper to consider the problem of loss-metric mismatch for net-work pruning. 2) We train a performance prediction network, and use it as a proxy of accuracy metric for sub-networks. Our method further leverages the beneﬁts from both per-formance maximization and classiﬁcation loss to guide search of sub-networks. 3) Extensive experimental results show that our method can achieve the state-of-the-art performance with
ResNet, MobileNetV2, and ShufﬂeNetV2+ on Ima-geNet and CIFAR-10. 2.