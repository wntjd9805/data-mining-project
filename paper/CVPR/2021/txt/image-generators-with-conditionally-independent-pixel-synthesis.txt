Abstract 1.

Introduction
Existing image generator networks rely heavily on spa-tial convolutions and, optionally, self-attention blocks in or-der to gradually synthesize images in a coarse-to-ﬁne man-ner. Here, we present a new architecture for image genera-tors, where the color value at each pixel is computed inde-pendently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are in-volved during the synthesis. We analyze the modeling capa-bilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve simi-lar generation quality to state-of-the-art convolutional gen-erators. We also investigate several interesting properties unique to the new architecture.
State-of-the-art in unconditional image generation is achieved using large-scale convolutional generators trained against adversarial discriminators [10, 11, 1]. While lots of nuances and ideas have contributed to the state-of-the-art recently, for many years since the introduction of DC-GAN [21] such generators are based around spatial con-volutional layers, also occasionally using the spatial self-attention blocks [32]. Spatial convolutions are also invari-ably present in other popular generative architectures for images, including autoencoders [13], autoregressive gener-ators [30], or ﬂow models [3, 12]. Thus, it may seem that spatial convolutions (or at least spatial self-attention) is an unavoidable building block for state-of-the-art image gen-erators.
Recently, a number of works have shown that individual 114278
images or collections of images of the same scene can be en-coded/synthesized using rather different deep architectures (deep multi-layer perceptrons) of a special kind [19, 25].
Such architectures do not use spatial convolutions or spatial self-attention and yet are able to reproduce images rather well. They are, however, restricted to individual scenes. In this work, we investigate whether deep generators for un-conditional image class synthesis can be built using simi-lar architectural ideas, and, more importantly, whether the quality of such generators can be pushed to achieve state-of-the-art.
Perhaps surprisingly, we come up with a positive answer (Fig. 1), at least for the medium image resolution (of 256
× 256). We have thus designed and trained deep generative architectures for diverse classes of images that achieve sim-ilar quality of generation to state-of-the-art convolutional generator StyleGANv2 [11], even surpassing this quality for some datasets. Crucially, our generators are not using any form of spatial convolutions or spatial attention in their computational graphs. Instead, they use coordinate encod-ings of individual pixels, as well as multiplicative condition-ing (weight modulation) on random latent vectors. Aside from such conditioning, the color of each pixel in our archi-tecture is predicted independently (hence we call our im-age generator architecture Conditionally-Independent Pixel
Synthesis (CIPS) generators).
In addition to suggesting this class of image genera-tors and comparing its quality with state-of-the-art con-volutional generators, we also investigate the extra ﬂexi-bility that is permitted by the independent processing of pixels. This includes easy extention of synthesis to non-trivial topologies (e.g. cylindrical panoramas), for which the extension of spatial convolutions is known to be non-trivial [16, 2]. Furthermore, the fact that pixels are synthe-sized independently within our generators, allows sequen-tial synthesis for memory-constrained computing architec-tures. It enables our model to both improve the quality of photos and generate more pixel values in a speciﬁc areas of image (i.e. to perform foveated synthesis). 2.