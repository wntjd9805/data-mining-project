Abstract
We propose a framework, called LiftedGAN, that disen-tangles and lifts a pre-trained StyleGAN2 for 3D-aware face generation. Our model is “3D-aware” in the sense that it is able to (1) disentangle the latent space of StyleGAN2 into texture, shape, viewpoint, lighting and (2) generate 3D components for rendering synthetic images. Unlike most pre-vious methods, our method is completely self-supervised, i.e. it neither requires any manual annotation nor 3DMM model for training. Instead, it learns to generate images as well as their 3D components by distilling the prior knowledge in StyleGAN2 with a differentiable renderer. The proposed model is able to output both the 3D shape and texture, allow-ing explicit pose and lighting control over generated images.
Qualitative and quantitative results show the superiority of our approach over existing methods on 3D-controllable
GANs in content controllability while generating realistic high quality images. 1.

Introduction quality face images with a wide variety of styles. However, since these models are trained to generate random faces, they do not offer direct manipulation over the semantic attributes such as pose, expression etc. in the generated image. A number of studies have been devoted to achieving control over the generation process in order to be able to adjust pose and other semantic attributes in the generated face images.
Among all these attributes, 3D information, such as pose, is the most desirable due to its applicability in face recog-nition [26] and face synthesis [32]. To achieve this, most existing approaches attempt to disentangle the latent feature space of GANs by leveraging external supervision on the semantic factors such as pose labels [26, 25], landmarks [9] or synthetic images [2, 33, 15], while some others [17] have explored an unsupervised approach for 3D controllability in the latent space. Although these feature manipulation based methods have shown ability to generate faces with high visual quality under assigned poses, it is unclear whether important content, such as identity, is indeed preserved when we change the pose parameters. Potential errors could arise from the generation process when the manipulated features are parsed by the network parameters (see Section 5.2).
Generative Adversarial Networks (GANs), such as Style-GAN [13, 14] have been demonstrated to generate high
In contrast to solutions that only output 2D images, build-ing generative models with explicit 3D shapes could give 6258
a stricter control of the content. For general 3D objects, a thread of recent studies train 3D generative models from 2D images, but they mostly work on rendered images with coarse shapes [6, 7, 16], e.g. cars. For faces, which contain many ﬁne-grained details, existing solutions for 3D image generation [20, 23] have suffered from collapsed results un-der large pose variations due to the difﬁculty of learning reasonable shapes.
In this paper, we propose a framework that shares the advantages of both 2D and 3D solutions. Given a pre-trained
StyleGAN2, we distill it into a 3D-aware generator, which not only outputs the generated image, but its view points, light direction and 3D information, such as surface normal map. Compared with 3D generative models, our approach is able to output rendered images with higher quality, close to 2D generative models. Compared with 2D solutions based on feature manipulation, our model allows a stricter 3D control over the content by maneuvering the view point and shading of textured meshes, as in 3D generators. Qualitative and quantitative results show the superiority of our approach over existing methods in preserving identity as well as generating realistic high quality images.
The main contributions of the paper can be summarized as follows:
• A framework for 3D-aware face image generation, called
LiftedGAN, which distills the knowledge from a pre-trained StyleGAN2.
• A self-supervised method for disentangling and distilling the 3D information in the latent space of StyleGAN2.
• A generator that outputs both high quality face images and their 3D information, allowing explicit control over pose and lighting. 2.