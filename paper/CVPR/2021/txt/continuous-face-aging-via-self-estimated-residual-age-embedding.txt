Abstract
Face synthesis, including face aging, in particular, has been one of the major topics that witnessed a substantial im-provement in image ﬁdelity by using generative adversarial networks (GANs). Most existing face aging approaches di-vide the dataset into several age groups and leverage group-based training strategies, which lacks the ability to provide
ﬁne-controlled continuous aging synthesis in nature. In this work, we propose a uniﬁed network structure that embeds a linear age estimator into a GAN-based model, where the embedded age estimator is trained jointly with the encoder and decoder to estimate the age of a face image and pro-vide a personalized target age embedding for age progres-sion/regression. The personalized target age embedding is synthesized by incorporating both personalized residual age embedding of the current age and exemplar-face aging basis of the target age, where all preceding aging bases are derived from the learned weights of the linear age estimator.
This formulation brings the uniﬁed perspective of estimat-ing the age and generating personalized aged face, where self-estimated age embeddings can be learned for every sin-gle age. The qualitative and quantitative evaluations on dif-ferent datasets further demonstrate the signiﬁcant improve-ment in the continuous face aging aspect over the state-of-the-art. 1.

Introduction
Face aging, also known as age progression, aims to aes-thetically render input face images with natural aging and rejuvenating effects while preserving identity information of the individual. With recent advances in deep learning, face synthesis has also shown substantial improvement on image ﬁdelity and the age precision in the simulated face images [10, 41, 24]. A major challenge to solve a variety of remaining problems (e.g. continuous aging) is the lack of data. For example, many research works of face aging
[20, 41, 43, 10] need to group images into 4-5 age groups
*This work is done during Zeqi Li’s full-time employment at ModiFace. (such as <30, 30-40, 40-50, 50+) and can only generate im-ages within a target age group, due to the limited amount of data at each age. Another important problem is how to maintain personal traits in age progression, as aging pat-terns may differ for each individual.
Traditional face aging contains mainly two approaches: physical model-based [3, 42] and prototype-based [37, 16].
The physical model-based methods often consist of com-plex physical modeling, considering skin wrinkles, face shape, muscle changes, and hair color, etc. This type of method typically requires a tremendous amount of data and is very expensive computationally. Prototype-based meth-ods ﬁrstly explore group-based designs by computing an av-erage face within the pre-deﬁned age groups, which fails to retain personalized aging information. Further, all those methods are not applicable to continuous face aging.
Following the success of recent generative models, such as variational autoencoders (VAEs) and generative adver-sarial networks (GANs) [9], on the image translation tasks, researchers have dedicated efforts in adapting those meth-ods to face synthesis. IPCGAN [41] has shown signiﬁcant progress in generating face images with evident aging ef-fects by enforcing an age estimation loss. Later variation
[43] creates a pyramid structure for the discriminator to im-prove face aging understanding at multiple scales. Contin-uous aging was not explored among these methods. He et al. [10] introduced a multi-branch generator for the group-based training and proposed the idea to approximate contin-uous aging via linear interpolation of latent representations between two adjacent age groups. The authors of [24] also tackle the problem using a similar linear interpolation ap-proach, which is performed on the learned age latent code between two neighboring groups instead. These types of methods make an assumption that the age progression is lin-ear between the two adjacent groups and the learned group embedding can be used directly as the median age embed-ding. Consequently, this may result in a shift of target age in the generated images. Intuitively, this nonlinearity can be interpreted as: people do not age at the same speed for dif-ferent stages. Moreover, such interpolation-based methods may alter personal traits when disentanglement is imperfect. 115008
To address the aforementioned problems, we propose a novel approach to achieve continuous aging by a uniﬁed network where a simple age estimator is embedded into a regular encoder-decoder architecture. This allows the net-work to learn self-estimated age embeddings of all ages, thus representing the continuous aging information without manual efforts in selecting proper anchor age groups. Given a target age, we derive a personalized age embedding which considers two aspects of face aging: 1) a personalized resid-ual age embedding at the current age, which preserves the individual’s aging information; 2) exemplar-face aging ba-sis at the target age, which encodes the shared aging pat-terns among the entire population. We describe the detailed calculation and training mechanism in Method. The cal-culated target age embedding is then used for ﬁnal image generation. We experiment extensively on FFHQ [15] and
CACD2000 [5] datasets. Our results, both qualitatively and quantitatively, show signiﬁcant improvement over the state-of-the-art in various aspects. Our main contributions are:
• We propose a novel method to self-estimate continu-ous age embeddings and derive personalized age em-beddings for face aging task by jointly training an age estimator with the generator. We quantitatively and qualitatively demonstrate that the generated images better preserve the personalized information, achieve more accurate aging control, and present more ﬁne-grained aging details.
• We show that our continuous aging approach generates images with more well-aligned target ages, and better preserves detailed personal traits, without manual ef-forts to deﬁne proper age groups.
• Our proposed idea to self-estimate personalized age embedding from a related discriminative model can be easily applied to other conditional image-to-image translation tasks, without introducing extra complex-ity. In particular, tasks involving a continuous condi-tion and modeling (e.g. non-smile to smile), can bene-ﬁt from this setup. 2.