Abstract
Simulation has the potential to massively scale evalua-tion of self-driving systems, enabling rapid development as well as safe deployment. Bridging the gap between simu-lation and the real world requires realistic multi-agent be-haviors. Existing simulation environments rely on heuristic-based models that directly encode trafﬁc rules, which cannot capture irregular maneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding, merging). In contrast, we leverage real-world data to learn directly from human demonstration, and thus capture more naturalistic driving behaviors. To this end, we propose TRAFFICSIM, a multi-In agent behavior model for realistic trafﬁc simulation. particular, we parameterize the policy with an implicit la-tent variable model that generates socially-consistent plans for all actors in the scene jointly. To learn a robust policy amenable for long horizon simulation, we unroll the policy in training and optimize through the fully differentiable simu-lation across time. Our learning objective incorporates both human demonstrations as well as common sense. We show
TRAFFICSIM generates signiﬁcantly more realistic trafﬁc scenarios as compared to a diverse set of baselines. Notably, we can exploit trajectories generated by TRAFFICSIM as ef-fective data augmentation for training better motion planner. 1.

Introduction
Self-driving has the potential to make drastic impact on our society. One of the key remaining challenges is how to measure progress. There are three main approaches for measuring the performance of a self-driving vehicle (SDV): 1) structured testing in the real world, 2) virtual replay of pre-recorded scenarios, and 3) simulation. These approaches are complementary, and each has its key advantages and shortcomings. The use of a test track enables structured and repeatable evaluation in the physical world. While this approach is perceptually realistic, testing is often limited to a few scenarios due to the long setup time and high cost for each test. Moreover it is hard and often impossible to test
Figure 1. Generating realistic multi-agent behaviors is a key com-ponent in self-driving simulation safety critical situations, such as unavoidable accidents. Vir-tual replay allows us to leverage diverse scenarios collected from the real world, but it is still limited to what we observe.
Furthermore, since the replay is immutable, actors in the environment do not react when the SDV plan diverges from what happened and the sensor data does not reﬂect the new viewpoint. These challenges make simulation a particularly attractive alternative: in a virtual environment we can evalu-ate against a large number of diverse and dynamic scenarios in a safe, controllable, and cost-efﬁcient manner.
Simulation systems typically consist of three steps: 1) specifying the scene layout which includes the road topology and actor placement, 2) simulating the motion of dynamic agents forward, and 3) rendering the generated scenario with realistic geometry and appearance, as shown in Figure 1. In this paper, we focus on the second step: generating realistic multi-agent behaviors automatically. This can aid simulation design in several important ways: it can expedite scenario creation by automating background actors, increase scenario coverage by generating variants with emergent behaviors, and facilitate interactive scenario design by generating pre-view of potential interactions.
However, bridging the behavior gap between the sim-ulated world and the real world remains an open chal-lenge. Manually specifying each actor’s trajectory is not scalable and results in unrealistic simulations since the ac-tors do not react to the SDV actions. Heuristic-based models
[39, 20, 25] capture basic reactive behavior, but rely on di-rectly encoding trafﬁc rules such as ”vehicles follow the road and do not collide”. While this approach generates plausible trafﬁc ﬂow, the generated behaviors lack the diversity and 10400
Figure 2. Diversity and nuance of human driving behaviors observed in the real world: red is actor of interest, green are interacting actors nuance of human behaviors and interactions present in real-world urban trafﬁc scenes. For instance, they cannot capture irregular maneuvers that do not follow the lane graph such as
U-turns, or complex multi-agent interplays such as nudging past a vehicle stopped in a driving lane, or negotiations at an unprotected left turn. In contrast, learning-based approaches
[11, 38, 35] are ﬂexible and can capture a diverse set of be-haviors. However, they often lack common sense and are generally brittle to distributional shift. Furthermore, they can also be computationally expensive if not optimized for simulating large numbers of actors over long horizon.
To tackle these challenges, we present TRAFFICSIM, a multi-agent behavior model for trafﬁc simulation. We lever-age recent advances in motion forecasting, and formulate the joint actor policy with an implicit latent variable model
[11], which can generate multiple scene-consistent samples of actor trajectories in parallel. Importantly, we present a novel learning framework to train robust policy amenable for trafﬁc simulation over long time horizon. In particular, we leverage: 1. closed-loop training with back-propagation through the fully differentiable simulation, and 2. time-adaptive multi-task loss to balance between learn-ing from demonstration and common sense.
Our experiments show that TRAFFICSIM is able to sim-ulate trafﬁc scenarios that remain realistic over long time horizon, with minimal collisions and trafﬁc rule violations.
In particular, it achieves the lowest scenario reconstruction error in comparison to a diverse set of baselines including heuristic, motion forecasting, and imitation learning models.
We also show that we can train better motion planners by exploiting trajectories generated by TRAFFICSIM. Lastly, we show experiments in trading off simulation quality and computation. In particular, we can achieve up to 4x speedup with multi-step updates, or further reduce collisions with additional optimization at simulation-time. 2.