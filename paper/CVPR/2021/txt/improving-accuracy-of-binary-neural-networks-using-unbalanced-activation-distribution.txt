Abstract
Binarization of neural network models is considered as one of the promising methods to deploy deep neural net-work models on resource-constrained environments such as mobile devices. However, Binary Neural Networks (BNNs) tend to suffer from severe accuracy degradation compared to the full-precision counterpart model. Several techniques were proposed to improve the accuracy of BNNs. One of the approaches is to balance the distribution of binary activa-tions so that the amount of information in the binary acti-vations becomes maximum. Based on extensive analysis, in stark contrast to previous work, we argue that unbalanced activation distribution can actually improve the accuracy of
BNNs. We also show that adjusting the threshold values of binary activation functions results in the unbalanced distri-bution of the binary activation, which increases the accu-racy of BNN models. Experimental results show that the accuracy of previous BNN models (e.g. XNOR-Net and Bi-Real-Net) can be improved by simply shifting the threshold values of binary activation functions without requiring any other modiﬁcation. 1.

Introduction
Deep Neural Networks (DNNs) have achieved human-level performance in many computer vision tasks such as image classiﬁcation, object detection, and segmentation.
However, the increased compute cost and memory require-ment of large DNN models pose a burden on resource-constrained environments such as mobile devices. To mit-igate this problem, various techniques including network quantization [7, 17, 23, 29, 37], network pruning [15, 34], and efﬁcient architecture design [16, 31, 35] were intro-duced to reduce the compute cost and memory requirement of DNN models. Among them, the network quantization technique is being actively studied and recent works have shown that a DNN model can even be quantized to a 1-bit model [17, 25, 26, 29]. When a DNN model is binarized to a
Binary Neural Network (BNN) model, the memory require-ment of the model is reduced by 32x since 32-bit ﬂoating-point weights can be represented by 1-bit weights. In ad-dition, high precision multiply-and-accumulate operations can be replaced by XNOR-and-popcount logics in BNNs since both activation and weight have 1-bit precision. Due to the lightweight nature, BNNs are garnering interests as a promising solution for DNN computing on edge devices.
However, BNNs still suffer from the accuracy degrada-tion caused by the aggressive quantization (32-bit to 1-bit).
While recent researches have shown that a DNN model can be quantized to 2-bit precision with marginal accu-racy loss [11], a severe performance gap still exists be-tween a BNN model and its full-precision counterpart DNN model. In general, it is known that weight quantization is much easier than activation quantization [5, 20, 37]. In ad-dition, when quantizing the activation, the accuracy loss is marginal until 2-bit quantization, but a signiﬁcant ac-curacy drop occurs when quantizing the activation to 1-bit precision [20, 23, 27]. Previous works tried to ex-plain the sharp accuracy drop from 2-bit activation to 1-bit case based on the gradient mismatch problem caused by the non-differentiable binary activation function [9, 25].
Since the quantization functions are non-differentiable, gra-dients cannot propagate through the quantization layer in the back-propagation process. Therefore, previous works used straight-through-estimator (STE) to compute the ap-proximate gradient on non-differentiable layers [1, 17].
While STE enables back-propagation through quantization layers, the discrepancy between the actual function and the approximated function causes gradient mismatch problem.
Especially in BNNs, sign function is used to binarize ac-tivations and is usually approximated as hardtanh function in back-propagation. Compared to other multi-bit quanti-zation functions, sign function shows more severe gradi-ent mismatch which leads to sharp accuracy degradation.
Hence, several works tried to design better approximation function [9, 25] or to reduce the gradient mismatch using 7862
neuron coupling [20].
In this work, we argue that there is another reason for the poor performance of BNN in addition to the gradient mis-match. We speculate that the symmetry of the sign function is also partly responsible for the degradation of BNN per-formance. Most DNN models use ReLU activation func-tion instead of sigmoid or Tanh functions. While the output distributions of sigmoid and Tanh functions are symmetric with respect to zero, ReLU function replaces all the nega-tive values to zero so that the distribution of ReLU output is highly skewed. When quantizing activations to 2-bit or a higher precision, ReLU-based quantization functions are usually used [6, 7, 11, 37]. In other words, multi-bit quan-tization functions also output unbalanced activation distri-butions similar to ReLU function. However, when binariz-ing activations, sign function is used, and hence the distri-bution of the binary activation becomes symmetric (or bal-anced). We show that a model with unbalanced activation distribution performs better than that with balanced activa-tion distribution. We ﬁrst show that the claim is valid even in the full precision activation case by comparing hardtanh and ReLU6 activation functions. We then show that the per-formance of BNN can be improved by simply shifting the threshold of the sign function. We also analyze the effect of training the threshold of the sign function and show that the thresholds cannot be trained efﬁciently through back-propagation. Our contributions can be summarized as fol-lows:
• To the best of our knowledge, we are the ﬁrst to report that the unbalanced distribution of binary activation ac-tually helps improve the accuracy of BNNs.
• We propose to shift the thresholds of binary activation functions to make the distribution of binary activation unbalanced.
• Experimental results show that the accuracy of previ-ous BNN models can be improved at almost no cost by simply shifting the threshold of the binary activa-tion function. 2.