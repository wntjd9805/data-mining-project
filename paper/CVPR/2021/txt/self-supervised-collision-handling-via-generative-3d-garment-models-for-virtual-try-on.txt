Abstract
We propose a new generative model for 3D garment de-formations that enables us to learn, for the ﬁrst time, a data-driven method for virtual try-on that effectively ad-dresses garment-body collisions.
In contrast to existing methods that require an undesirable postprocessing step to ﬁx garment-body interpenetrations at test time, our ap-proach directly outputs 3D garment conﬁgurations that do not collide with the underlying body. Key to our success is a new canonical space for garments that removes pose-and-shape deformations already captured by a new diffused human body model, which extrapolates body surface prop-erties such as skinning weights and blendshapes to any 3D point. We leverage this representation to train a genera-tive model with a novel self-supervised collision term that learns to reliably solve garment-body interpenetrations. We extensively evaluate and compare our results with recently proposed data-driven methods, and show that our method is the ﬁrst to successfully address garment-body contact in unseen body shapes and motions, without compromising re-alism and detail. 1.

Introduction
The digitalization of 3D garments has important appli-cations in many areas of our everyday lives such as online shopping, video games, visual effects, and fashion design, and it has traditionally been addressed with physics-based methods [37, 38]. However, even if these methods offer so-lutions that generalize well to any type of garment, produce physically-accurate results, and solve body-garment con-tact, they require computationally expensive runtime eval-uations. Consequently, they do not meet the combined ro-bustness and performance needed for real-time applications such as virtual try-on. Furthermore, they are not easily dif-ferentiable and cannot be integrated into computer vision pipelines that, for example, ﬁt deformable models into im-ages to extract information about the scene.
Data-driven methods have emerged as a popular alterna-tive to physics-based methods. The core idea is to learn a function that mimics the garment behavior observed in a large dataset. To this end, recent methods leverage the ca-pability of neural networks to learn nonlinear functions, and propose differentiable models that output 3D deformed gar-ments as a function of the target shape, motion, style, size, and other design parameters [62, 53, 41, 61, 64, 16, 31].
These methods showcase great realism and robustness, however, we identify a fundamental limitation in all existing works: despite using a loss term that penalizes unphysical body-garment interpenetrations at training time [16, 4], pre-dicted garments commonly suffer from body-garment inter-penetrations in test sequences. This is usually addressed with an added postprocessing step that pushes the problem-11763
atic regions of the garment, identiﬁed by exhaustive search, outside of the body [41, 53].
The undesired interpenetrations arise from natural resid-ual errors in test samples when optimizing neural networks which, combined with the extremely narrow gap between body surface and garment, can produce artifacts even if the predicted 3D mesh closely matches the ground truth de-formed garment.
In this work, we address this inherent limitation and propose, to the best of our knowledge, the
ﬁrst data-driven method to reliably solve garment-body in-terpenetrations without requiring any postprocessing step.
We achieve this through three main contributions.
First, we propose to enhance existing human body mod-els [29] by learning to smoothly expand the surface parame-ters to any 3D point. Intuitively, this allows us to model the deformation at any 3D point, e.g., a vertex of a deformed loose garment, leveraging the deformation capabilities of existing human body models. This expanded human body represents a fundamental building block for our method.
Our second contribution addresses the common assump-tion, made by existing data-driven models, that garment deformations closely follow the underlying body deforma-tions. This popular simpliﬁcation is often used to deﬁne garment models that use skinning parameters based on the closest body vertex in rest pose, and subsequently articulate the garment using a standard linear blend skinning (LBS) approach. We show that simpliﬁed transformations to bring ground truth data into a normalized representation, e.g. via inverse LBS [53, 41], cannot correctly represent the com-plex deformations that garments exhibit, and often intro-duce undesirable artifacts. Instead, we propose a garment model that represents deformations in a novel unposed and deshaped canonical space by removing deformations al-ready captured via our expanded human body model. Since it yields correct skinning attributes for any 3D point, our garment model is designed to not to introduce collisions during projection operations between the canonical space and the posed space.
Our third and most important contribution is to lever-age the novel canonical representation of garments to learn a generative subspace of deformations. Garments in this canonical space are encoded with respect to a constant ref-erence body conﬁguration. This not only gives an improved representation of garment deformations, but also allows us to reliably learn to solve collisions via self-supervision, by exhaustively sampling the generative space. We then learn a regressor that outputs mesh deformations encoded in this subspace, and use our garment model to project them to the
ﬁnal deformed state. Since both the deformation subspace and the projection step are designed to avoid collisions, our
ﬁnal deformed 3D garments do not interpenetrate the under-lying body mesh, regardless the shape and pose parameter. 2.