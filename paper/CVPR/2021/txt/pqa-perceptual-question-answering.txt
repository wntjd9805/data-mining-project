Abstract
Q & A
Perceptual organization remains one of the very few es-tablished theories on the human visual system.
It under-pinned many pre-deep seminal works on segmentation and detection, yet research has seen a rapid decline since the preferential shift to learning deep models. Of the limited attempts, most aimed at interpreting complex visual scenes using perceptual organizational rules. This has however been proven to be sub-optimal, since models were unable to effectively capture the visual complexity in real-world im-agery. In this paper, we rejuvenate the study of perceptual organization, by advocating two positional changes: (i) we examine purposefully generated synthetic data, instead of complex real imagery, and (ii) we ask machines to synthe-size novel perceptually-valid patterns, instead of explaining existing data. Our overall answer lies with the introduc-tion of a novel visual challenge – the challenge of percep-tual question answering (PQA). Upon observing example perceptual question-answer pairs, the goal for PQA is to solve similar questions by generating answers entirely from scratch (see Figure 1). Our ﬁrst contribution is therefore the ﬁrst dataset of perceptual question-answer pairs, each generated speciﬁcally for a particular Gestalt principle. We then borrow insights from human psychology to design an agent that casts perceptual organization as a self-attention problem, where a proposed grid-to-grid mapping network directly generates answer patterns from scratch. Experi-ments show our agent to outperform a selection of naive and strong baselines. A human study however indicates that ours uses astronomically more data to learn when com-pared to an average human, necessitating future research (with or without our dataset). 1.

Introduction
The ultimate goal for computer vision, as one might ar-gue, lies with imitating the human visual system. Machine vision systems in their infancy were largely inspired by
*Equal contribution
Observation
Challenge
Figure 1. Perceptual Question and Answer (PQA). Given an ex-emplar PQA pair (Left), a new question (right) is required to be addressed, i.e. generate answer-grid from scratch. theories on visual perception. Perceptual organization in particular played an important part, leading to the state-of-the-art performances on image segmentation [6, 12], con-tour detection [1, 22] or shape parsing [19, 14] for exam-ple. Research efforts have diminished thereafter with the introduction of deep learning, where the need for mod-eling human visual systems are no longer deemed neces-sary, as performances on certain vision tasks have already reached/exceeded human level [13, 20, 7].
Perceptual organization theory essentially boils down to a ﬁnite set of rules (called Gestalt) that collectively guide the reasoning of our visual system [33, 30, 31]. Cognitive scientists have been mostly interested in computationally modeling these rules [38, 4], and reason using them in a collective manner [32, 5]. These models are however very limited in their power to represent the complex underlying mechanism of perceptual organization. Computer vision re-search took the opposite route, where the focus had been on using the very limited knowledge of perceptual organiza-tion to explain complex imagery [24]. Yet, performances reported are largely sub-optimal and only applicable to a few niche topics such as contour grouping [37, 28, 17]. It is however interesting and encouraging to notice very recent works on unsupervised 3D reconstruction achieved impres-sive results by exploring just the rule of symmetry [34].
In this paper, we set out to rejuvenate the research on perceptual organization. Unique to previous attempts that use simple Gestalt rules to explain complicated image data, we (i) approach the problem via purposefully generated syn-thetic data speciﬁc to each Gestalt principle, and (ii) ask an 112056
agent to solve unseen problems by generating the answer from scratch. We encapsulate these design changes to form a novel perceptual organization challenge – the challenge of perceptual question answering (PQA), where upon ob-serving example PQA pairs, an agent is asked to solve sim-ilar perceptual questions by generating answers completely from scratch (see Figure 1 for an example).
Our ﬁrst contribution is therefore a perceptual question and answer (PQA) dataset. Constructing such a dataset is non-trivial as it dictates an explicit measure for percep-tual organization – one needs to design a set of elementary tasks each reﬂecting a speciﬁc Gestalt law. This is in con-trast with existing datasets [15, 10] that are built around real-world scenarios, which do not explicitly model each
Gestalt. We are inspired by the recently proposed Abstrac-tion and Reasoning Corpus (ARC) [3].
In particular, we
ﬁnd the simplicity in forming visual question as color-coded patterns a good match for spelling out speciﬁc grouping insights. PQA however comes with two salient features speciﬁcally designed for perceptual organization. First, contrary to measuring general intelligence in ARC, PQA gravitates to rule-speciﬁc reasoning for perceptual group-ing. Second, the size of the hand-crafted ARC dataset can be limited [3]. Instead, our dataset can be deterministically generated thereby producing potentially unlimited number of instances.
With this dataset, we can train an agent to perform the task of PQA. To realize our agent, we resort to important discoveries in the psychological literature – that the organi-zation of visual elements could be modeled as an attentive process [18, 35]. We thus propose a model based on a self-attention mechanism, and formulate the challenge as a grid-to-grid mapping problem. More speciﬁcally, our proposed model is built on Transformer [29] but with three problem-speciﬁc extensions: (i) a context embedding module based on a self-attention mechanism to encode an exemplar PQA pair, (ii) positional encoding adapting to the 2D grid nature of our problem, instead of the default 1D case, and (iii) a tai-lored decoder to predict all symbols (colored blocks on the grid) in parallel, i.e., the entire canvas is produced in one pass to form an answer. Note that unlike conventional vi-sual reasoning tasks which assume a few candidate answers are accessible [26, 36, 25], our model directly generates an-swers from scratch.
Our contribution can be summarized as follows: (i) we rejuvenate the study of perceptual organization through a novel challenge of perceptual question answering, (ii) we propose the ﬁrst dataset speciﬁcally targeting PQA, where each question-answer pair is speciﬁc to a particular Gestalt principle, (iii) we formulate perceptual organization via a self-attention mechanism, and propose a grid-to-grid map-ping network which is able to directly generate answer-grid from scratch, and (iv) we show our model to outperform a few baselines re-purposed for PQA, yet a human study shows that ours uses signiﬁcantly more data to learn, when compared with an average human. 2.