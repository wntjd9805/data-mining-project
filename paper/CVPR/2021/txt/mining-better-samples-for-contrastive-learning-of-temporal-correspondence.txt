Abstract
We present a novel framework for contrastive learning of pixel-level representation using only unlabeled video. With-out the need of ground-truth annotation, our method is ca-pable of collecting well-deﬁned positive correspondences by measuring their conﬁdences and well-deﬁned negative ones by appropriately adjusting their hardness during train-ing. This allows us to suppress the adverse impact of am-biguous matches and prevent a trivial solution from being yielded by too hard or too easy negative samples. To ac-complish this, we incorporate three different criteria that ranges from a pixel-level matching conﬁdence to a video-level one into a bottom-up pipeline, and plan a curriculum that is aware of current representation power for the adap-tive hardness of negative samples during training. With the proposed method, state-of-the-art performance is attained over the latest approaches on several video label propaga-tion tasks. 1.

Introduction
Learning pixel-level representation for visual correspon-dence can facilitate numerous downstream applications [26, 5, 31]. In contrast to the image-level representation which demands a semantic invariance among object instances of the same category, the pixel-level representation further re-quires the ﬁne-grained localization ability to discriminate a distinctive match from all possible matching candidates.
Supervising the representation for pixel-level correspon-dence, however, often requires costly annotations deﬁned for all pixels. Constructing such dense annotations be-come even more problematic in the presence of occlusions and non-rigid object deformations. Synthetically generated data [34, 4, 43] would be an alternative of high-quality an-notation maps, but it has the downside of limiting general-ization to real scenes.
Several methods [51, 52, 30, 29, 28, 20] have attempted
∗Co-corresponding author (a) image pair (b) collected positive correspondences (c) collected negative correspondences for a positive one (green-colored)
Figure 1. Visualization of collected samples used in our con-trastive learning: given (a) an image pair, we collect (b) positive correspondences that are aware of matching uncertainty, and (c) negative ones that are neither too easy nor too difﬁcult. to alleviate this by leveraging abundant unlabeled videos as a source of free supervision. Unlike to the synthetic su-pervisions [34, 4, 43], richer appearance and shape varia-tions captured from real world strengthen their generaliza-tion ability. Furthermore, the nature of temporal coherence in video allows the correspondences likely to exist across adjacent frames, providing useful constraints for training.
Standing on these bases, they ﬁrst track points over time and then learn from the inconsistency between the original points and tracked ones in a form of reconstruction.
Establishing correspondences in the unconstrained videos, however, imposes additional challenges due to the existence of temporal discontinuities. For frames sampled with a large temporal stride, the self-supervised loss often 1034
becomes invalid in the presence of complex object defor-mations, illumination changes, and occlusions. This could be partially addressed by considering more matching candi-dates over additional adjacent frames that are likely to con-tain valid correspondences, e.g. tracking cycle with multi-ple lengths [52] or augmenting the model with a memory bank [28]. However, the number of ambiguous matches in-creases at the same time due to the larger candidates, which is problematic as the loss is evenly inﬂuenced by them.
Very recently, the concurrent work [20] casts this task into a probabilistic inference of a path through the graph constructed from an input video. In contrast to the previous works [51, 52, 30, 29, 28] that learn from a reconstruction-based loss, their consideration of negative correspondences produce better performances through the contrastive objec-tive [37]. However, the formulation of assigning graph nodes only within an image is still challenged by occlusions where the correspondences disappear to be out of the given nodes. Furthermore, composing negative examples with all pairs of nodes that do not meet cycle-consistency constraint may let too easy negative samples to degrade the contribu-tion of harder ones that are useful for contrastive learning.
In this work, we present a novel contrastive learning ap-proach that is capable of collecting well-deﬁned positive correspondences by measuring their uncertainties and well-deﬁned negative ones by controlling their hardness during training, as exempliﬁed in Fig. 1. Unlike previous works, our approach is able to suppress the adverse impact of am-biguous matches and simultaneously prevent a trivial solu-tion from being yielded by too easy negative samples.
Speciﬁcally, to measure reliable matching conﬁdence without ground-truth annotation, we formulate a bottom-up pipeline by incorporating three different criteria; Start-ing from checking forward-backward consistency, the ini-tial scores are further optimized by solving the optimal transport problem, which enforces the total uncertainty for all possible matches over an image to be minimized, and then imposing a temporal coherence constraint to be less susceptible to background clutters and repetitive patterns.
Furthermore, from the observation in metric learning liter-ature [53, 46] that using too hard or too easy negative sam-ples may produce worse representations, we collect semi-hard negative samples by specifying the upper and lower thresholds of their hardness. These thresholds are dynam-ically reconﬁgured during training with the proposed cur-riculum that is conditioned on the capability of the current representation. With the proposed method, state-of-the-art performance is attained over the latest approaches on sev-eral video label propagation tasks. 2.