Abstract
Most 3D shape completion approaches rely heavily on partial-complete shape pairs and learn in a fully super-vised manner. Despite their impressive performances on in-domain data, when generalizing to partial shapes in other forms or real-world partial scans, they often obtain unsat-isfactory results due to domain gaps. In contrast to previ-ous fully supervised approaches, in this paper we present
ShapeInversion, which introduces Generative Adversarial
Network (GAN) inversion to shape completion for the ﬁrst time. ShapeInversion uses a GAN pre-trained on complete shapes by searching for a latent code that gives a complete shape that best reconstructs the given partial input. In this way, ShapeInversion no longer needs paired training data, and is capable of incorporating the rich prior captured in a well-trained generative model. On the ShapeNet bench-mark, the proposed ShapeInversion outperforms the SOTA unsupervised method, and is comparable with supervised methods that are learned using paired data. It also demon-strates remarkable generalization ability, giving robust re-sults for real-world scans and partial inputs of various forms and incompleteness levels. Importantly, ShapeInver-sion naturally enables a series of additional abilities thanks to the involvement of a pre-trained GAN, such as producing multiple valid complete shapes for an ambiguous partial in-put, as well as shape manipulation and interpolation. 1.

Introduction 3D shape completion estimates the complete geome-try from a partial shape in the form of a partial point cloud, and is important to many downstream applications such as robotics navigation [11, 24] and scene understand-ing [9, 14]. Most works [27, 21, 15, 29, 31] for shape completion are trained in a fully supervised manner with paired partial-complete data. While they obtain promising results on in-domain data, it is challenging for these meth-ods to generalize to out-of-domain data, which are real-world scans or data with different partial forms, as shown in Fig. 1 (a)-(d).
We take an unsupervised approach in this study. Inspired by the success of GAN inversion in 2D tasks such as image restoration and editing, we propose to apply GAN inver-sion to 3D shape completion for the ﬁrst time, which we refer to as ShapeInversion. Speciﬁcally, given a partial in-put, ShapeInversion looks for a latent code in the GAN’s latent space that gives a complete shape that best recon-structs the input. By incorporating prior knowledge stored in the pre-trained GAN, no assumptions on the input partial forms are made, thus ShapeInversion generalizes well to in-puts of various partial forms and real-world scans. More-over, the involvement of GAN in ShapeInversion brings several side-beneﬁts, including giving multiple reasonable complete shapes for some partial input, as well as shape jit-tering and shape manipulation.
While ShapeInversion shares some similarity with GAN inversion methods for 2D images, the former possesses sev-eral intrinsic challenges due to the nature of 3D data: (1)
Unlike 2D images that follow a grid-like structure, where the positions of pixels are well deﬁned, point clouds of dif-ferent 3D shapes are highly unstructured. Often, GANs trained on 3D shapes would generate point clouds with sig-niﬁcant non-uniformity, i.e., points are unevenly distributed over the shape surface. Such non-uniformity may lead to shapes with undesired holes, undermining the completeness of our predictions. (2) The unordered nature of point clouds makes the completion task signiﬁcantly different from 2D image inpainting. In 2D image inpainting, one can easily measure the reconstruction consistency between the visi-ble regions of partial input and predicted output given the lattice-aligned pixel correspondences. Such comparison is challenging in 3D shape completion since the correspond-ing regions of two 3D shapes may reside at different loca-tions in the 3D space. Without accurate point correspon-1768
Figure 1. ShapeInversion incorporates the prior captured by a well-trained GAN. It shows exceptional generalization ability for shape is invariant to partial form changes, i.e., (a) virtual scans, generated by back-projecting 2.5D depth images into 3D, (b) completion: ball-holed partial shapes, generated by removing points within a random ball from a complete shape (PF-Net [15]), (c) semantic part-level incompleteness, generated by randomly removing some semantic parts (PartNet [23]); and generalizes well to (d) real-world scans.
Moreover, it can give (e) multiple valid outputs when there is ambiguity in the partial shape dences, GAN inversion would suffer from poor reconstruc-tion and in turn jeopardize the shape completion task.
We present two new components to address these unique challenges. First, to improve the uniformity of estimated point clouds, we introduce a simple and effective uniform loss, PatchVariance. The loss samples small patches, to ensure the planar assumption, over the object surface, and penalize the variance of average distances between the patch centers and their respective nearest neighbors. Unlike existing methods [18, 34] that are typically conducted at the patch level, ours is a soft regularizer that enhances unifor-mity at the object level on-the-ﬂy while training GANs. As a result, we achieve improved uniformity across all cate-gories, ranging from bulk to ﬁne structures while preserving the shape plausibility and variety.
Second, we devise an effective masking mechanism, k-Mask, to estimate the point correspondences between the partial input and predicted shape. To mitigate the am-biguous correspondences caused by the unordered nature of point cloud, our method lets each point in the partial input look for its k-nearest neighbors from the predicted shape.
The indices of all these k-nearest neighbors deﬁne the mask of the visible regions, from which we can compute for re-construction loss. Our method is dynamic, thus performing better than baseline approaches that use predeﬁned voxels or distance thresholds. It shows high robustness even when the semantics parts between the partial input and the pre-dicted shape are not within a close vicinity in the space.
ShapeInversion demonstrates compelling performance for shape completion in different scenarios. First, on a common benchmark derived from ShapeNet, it outperforms the SOTA unsupervised method pcl2pcl [7] by a signiﬁcant margin, and is comparable to various supervised methods.
Second, our method shows considerable generalization abil-ity and robustness when it comes to real-world scans or vari-ation in partial forms and incompleteness levels, whereas supervised methods exhibit signiﬁcant performance drops due to domain mismatches. Third, given more extreme in-completeness that causes ambiguity, our method is able to provide multiple valid complete shapes, all of which remain faithful to the visible parts presented in the partial input. 2.