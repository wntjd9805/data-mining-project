Abstract
Video-based person re-identiﬁcation (Re-ID) aims to au-tomatically retrieve video sequences of the same person under non-overlapping cameras. To achieve this goal, it is the key to fully utilize abundant spatial and temporal cues in videos. Existing methods usually focus on the most conspicuous image regions, thus they may easily miss out
ﬁne-grained clues due to the person varieties in image se-quences. To address above issues, in this paper, we propose a novel Global-guided Reciprocal Learning (GRL) frame-work for video-based person Re-ID. Speciﬁcally, we ﬁrst propose a Global-guided Correlation Estimation (GCE) to generate feature correlation maps of local features and global features, which help to localize the high- and low-correlation regions for identifying the same person. Af-ter that, the discriminative features are disentangled into high-correlation features and low-correlation features un-der the guidance of the global representations. Moreover, a novel Temporal Reciprocal Learning (TRL) mechanism is designed to sequentially enhance the high-correlation semantic information and accumulate the low-correlation sub-critical clues. Extensive experiments are conducted on three public benchmarks. The experimental results indi-cate that our approach can achieve better performance than other state-of-the-art approaches. The code is released at https://github.com/ﬂysnowtiger/GRL. 1.

Introduction
Person re-identiﬁcation (Re-ID) aims to retrieve speciﬁc pedestrians cross different cameras at different times and places. Recently, this task has become a hot research topic due to its importance in advanced applications, such as safe
*Corresponding Authors community, intelligent surveillance and criminal investiga-tion. Compared with other related Re-ID tasks, video-based person Re-ID provides a video as the input to retrieve rather than a single image. Although videos can provide compre-hensive appearance information, motion cues, pose varia-tions in temporal, at the same time, they bring more illumi-nation changes, complicated backgrounds and person oc-clusions in a clip. Thus, there are still many challenges for researches to handle in video-based person Re-ID.
Previous methods [30, 28, 17] can be coarsely summa-rized into two steps: spatial feature extraction and temporal feature aggregation. First, Convolutional Neural Networks (CNNs) are utilized to extract frame-level spatial features from each single image. Then, frame-level spatial features are temporally aggregated into a feature vector as the video representation to compute the similarity scores. Naturally, how to fully explore the discriminative spatial-temporal cues from multiple frames is seen as the key to tackle video-based person Re-ID. Generally speaking, the average pool-ing for spatial-temporal features can directly focus on main targets, but it has some obvious drawbacks, such as the in-ability to tackle the misalignment in temporal, the pollu-tion of background noises, and the difﬁculty of capturing small but meaningful subjects in videos. To address these drawbacks, in recent years, researchers have proposed some rigid-partition-based methods or soft-attention-based meth-ods to instead the direct average operation. These methods are beneﬁcial to learn more discriminative and diverse lo-cal features, resulting in higher performance of video-based person Re-ID. However, previous methods generally ignore the role of the global features in whole person recognition while strengthening the local features. Based on this con-sideration, Zhang et al. [33] utilize the local afﬁnities with respect to inference global features to help assign differ-ent weights to local features. Although effective, it tends to ignore inconspicuous yet ﬁne-grained clues. Different 13334
from [33], we correlate the global feature with the pixel-level local features in a frame to generate two correlation maps, which are utilized to disentangle generic features into high- and low-correlation features. Intuitively, features with high correlation mean they appear frequently in temporal and are spatially conspicuous. Features with low correlation mean they are inconspicuous and discontinuous yet mean-ingful. We further explore suitable strategies for disentan-gled features in temporal and fully mine ﬁne-grained cues.
Based on above considerations, we proposed a novel
Global-guided Reciprocal Learning (GRL) framework for video-based person Re-ID. The whole framework mainly consists of two key modules. To begin with, we proposed a Global-guided Correlation Estimation (GCE) module to estimate the correlation values of frame-level local features under the global guidance. With GCE, each frame-level fea-ture map will be disentangled into two kinds of discrimina-tive features with distinct correlation degrees. The one with high correlation, usually covers the most conspicuous and continuous visual information. Another with inverse cor-relation, as the supplement, is exploited to mine the ﬁne-grained and sub-critical cues. Besides, we propose a novel
Temporal Reciprocal Learning (TRL) module to fully ex-ploit all the discriminative features in the forward and back-ward process. More speciﬁcally, for high-correlation fea-tures, we adopt a semantic enhancement strategy to mine spatial conspicuous and temporal aligned information. For low-correlation features, we introduce a temporal memory strategy to accumulate the discontinuous but discriminative cues frame by frame. In this way, our proposed method can not only explore the most conspicuous information from the high-correlation regions in a sequence, but also capture the sub-critical information from the low-correlation regions.
Extensive experiments on public benchmarks demonstrate that our framework delivers better results than other state-of-the-art approaches.
In summary, our contributions are four folds:
• We propose a novel Global-guided Reciprocal Learn-ing (GRL) framework for video-based person Re-ID.
• We propose a Global-guided Correlation Estimation module to generate the correlation maps under the guidance of video representations for disentanglement.
• We introduce a Temporal Reciprocal Learning (TRL) module to effectively capture the conspicuous infor-mation and the ﬁne-grained clues in videos.
• Extensive experiments on public benchmarks demon-strate that our framework synthetically attains a better performance than several state-of-the-art methods. 2.