Abstract
Our goal is to learn a deep network that, given a small number of images of an object of a given category, recon-structs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the avail-ability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual an-notations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or im-plicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which signiﬁcantly improves reconstruc-tion while obtaining a detailed implicit representation of 1Work completed during an internship at Facebook AI Research. the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset.
For additional material please visit: https://henzler. github.io/publication/unsupervised_videos/. 1.

Introduction
Understanding and reconstructing categories of 3D ob-jects from 2D images remains an important open challenge in computer vision. Recently, there has been progress in using deep learning methods to do so but, due to the difﬁculty of the task, these methods still have signiﬁcant limitations. In particular, early efforts focused on clean synthetic data such as ShapeNet [5], further simplifying the problem by assum-ing the availability of several images of each object instance, 4700
knowledge of the object masks, object-centric viewpoints, etc. Methods such as [8, 7, 46, 55, 24] have demonstrated that, under these restrictive assumptions, it is possible to obtain high-quality reconstructions, motivating researchers to look beyond synthetic data.
Other methods have attempted to learn the 3D shape of object categories given a number of independent views of real-world objects, such as a collection of images of different birds. However, in order to simplify the task, most of them use some form of manual or automatic annotations of the 2D images. We seek to relax these assumptions, avoiding the use of manual 2D annotations or a priori constraints on the reconstructed shapes.
When it comes to high-quality general-purpose recon-structions, methods such as [36, 31, 33, 39, 65] have demon-strated that these can be obtained by training a deep neural network given only multiple views of a scene or object with-out manual annotations or particular assumptions on the 3D shape of the scene. Yet, these techniques can only learn a single object or scene at a time, whereas we are interested in modelling entire categories of 3D objects with related but different shapes, textures and reﬂectances. Nevertheless, the success of these methods motivates the use of multi-view supervision for learning collections of 3D objects.
In this paper, our ﬁrst goal is thus to learn 3D object categories given as input multiple views of a large collection of different object instances. To the best of our knowledge, this is the ﬁrst paper to conduct such a large-scale study of reconstruction approaches applied to learning 3D object categories from real-world 2D image data. Unfortunately, existing datasets for 3D category understanding are either small or synthetic. Thus, our ﬁrst contribution is to introduce a new dataset of videos collected ‘in the wild’ by Mechanical
Turkers (ﬁg. 3). These videos capture a large number of object instances from the viewpoint of a moving camera, with an effect similar to a turntable. Viewpoint changes are estimated with high accuracy using off-the-shelf Structure from Motion (SfM) techniques. We collect hundreds of videos of several different categories.
Our second contribution is to assess current reconstruc-tion technology on our new ‘in the wild’ data. For example, since each video provides several views of a single object with known camera parameters, it is suitable for an appli-cation of recent methods such as NeRF [36], and we ﬁnd that learning individual videos works very well, as expected.
However, we show that a direct application of such models to several videos of different but related objects is much harder. In fact, we experiment with related representations such as voxels and meshes, and ﬁnd that they also do not work well if applied na¨ıvely to this task. This is true even though reconstructions are focused on a single object at a time — thus disregarding the background — suggesting that these architectures have a difﬁcult time at handling even relatively mild geometric variability.
Our ﬁnal contribution is to propose a novel deep neural network architecture to better learn 3D object categories in such difﬁcult conditions. We hypothesize that the main chal-lenge in extending high-quality reconstruction techniques, that work well for single objects, to object categories is the difﬁculty of absorbing the geometric variability that comes in tackling many different objects together. An obvious but important source of variability is viewpoint: given only real images of different objects, it is not obvious how these should align in 3D space, and a lack of alignment adds to the variability that the model must cope with. We address this issue with a novel idea of Warp-Conditioned Ray Embed-dings (WCR), a new neural rendering approach that is far less sensitive to inaccurate 3D alignment in the input data. Our method modiﬁes previous differentiable ray marchers to pool information at variable locations in input views, conditioned on the 3D location of reconstructed points.
With this, we are able to train deep neural networks that, given as input a small number of images of new object in-stances in a given target category, can reconstruct them in 3D, including generating high-quality new views of the objects.
Compared to existing state-of-the-art reconstruction tech-niques, our method achieves better reconstruction quality in challenging datasets of real-world objects. 2.