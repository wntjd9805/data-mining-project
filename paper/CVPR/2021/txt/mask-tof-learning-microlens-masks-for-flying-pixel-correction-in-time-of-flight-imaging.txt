Abstract
We introduce Mask-ToF, a method to reduce ﬂying pixels (FP) in time-of-ﬂight (ToF) depth captures. FPs are perva-sive artifacts which occur around depth edges, where light paths from both an object and its background are integrated over the aperture. This light mixes at a sensor pixel to pro-duce erroneous depth estimates, which can adversely affect downstream 3D vision tasks. Mask-ToF starts at the source of these FPs, learning a microlens-level occlusion mask which effectively creates a custom-shaped sub-aperture for each sensor pixel. This modulates the selection of fore-ground and background light mixtures on a per-pixel basis and thereby encodes scene geometric information directly into the ToF measurements. We develop a differentiable
ToF simulator to jointly train a convolutional neural net-work to decode this information and produce high-ﬁdelity, low-FP depth reconstructions. We test the effectiveness of
Mask-ToF on a simulated light ﬁeld dataset and validate the method with an experimental prototype. To this end, we manufacture the learned amplitude mask and design an op-tical relay system to virtually place it on a high-resolution
ToF sensor. We ﬁnd that Mask-ToF generalizes well to real data without retraining, cutting FP counts in half. 1.

Introduction
Large-scale image datasets such as ImageNet [15] and
CIFAR [31, 32], in tandem with a boom in computational resources, drastically reshaped the ﬁeld of image process-ing. In the depth domain, a similar trend [61, 8, 13] has recently made the mass-acquisition of high-quality depth maps a vital prerequisite for a range of 3D graphics and vi-sion applications. These include human-centered tasks such as pose tracking [59, 29], action recognition [26, 51], and facial analysis [56], as well as scene-understanding prob-lems including mapping [19], segmentation [14], and object reconstruction [70, 10, 75]. While methods look to captured depth datasets for ground truth, the devices used to capture them are subject to a slew of error sources which, if not addressed, can hurt task performance and generalizability.
Figure 1: (a) 3D visualization of a microlens mask selec-tively blocking light entering a sensor pixel. (b) The equiv-alent mask pattern for a global aperture setup, all sensor pixels equally susceptible to FPs. (c) A learned mask pat-tern with spatially multiplexed noise/FP susceptibility.
One of many approaches to depth acquisition is passive sensing: exploiting parallax cues to infer distances solely from input monocular [66, 41, 18] or multiview [24, 27, 73] images. These methods can use standard RGB cameras for data acquisition, but struggle with textureless regions and complex geometries [63, 36]. Active sensing approaches tackle this challenge by ﬁrst sending out a known illumina-tion into the scene and reconstructing depth with the help of the returned light. These include structured light meth-ods such as active stereo, where spatially patterned light is projected into the scene to aid in the stereo feature match-ing process [1]. While being robust to textureless scenes, their accuracy is fundamentally limited by illumination pat-tern density and sensor baseline, resulting in a bulky cam-era form-factor. Some of the most successful active depth sensing methods are time-of-ﬂight (ToF) approaches, where depth is estimated from the travel time of light leaving and returning to the device. Direct ToF systems such as LiDAR send out individual laser pulses and measure their time to return using time-resolved sensors such as avalanche photo diodes [12]. These can provide high-accuracy and long-range depth estimates, but use a scanning approach to col-lect data, leading to poor depth completeness and/or expen-sive sensor array systems. In contrast, amplitude-modulated continuous wave (AMCW) ToF cameras, the focus of this 9116
paper, ﬂood-illuminate a scene with periodic amplitude-modulated light and estimate the phase shift of returned light to infer depth. These devices do not need to time-resolve captured light like their direct ToF counterparts, and so can rely on an easy-to-manufacture CMOS sensor array to produce complete depth maps at a high framerate. This, when combined with their small sensor-illumination base-line, makes AMCW ToF cameras compact and affordable, and has led to their widespread adoption in the vision com-munity. Devices such as those in the Microsoft Kinect series have subsequently helped create community-made freely-available scene understanding benchmarks that lower the barrier of entry for 3D vision research [64, 2].
Although they promise to democratize low-cost dense depth imaging, AMCW ToF methods are still subject to fun-damental limitations of the sensing process: noise from am-bient light, photon shot, phase wrapping, multipath interfer-ence (MPI), and ﬂying pixels (FPs) [16]. There has accord-ingly arisen a large body of work in computational post-processing methods to address these issues; methods con-cerning depth denoising [17, 74], phase unwrapping [35], and MPI correction [42]. Contrastingly, while conﬁdence-based methods [55] are able to identify ﬂying-pixels, recti-fying them — recovering the depth of their corresponding chief ray — has remained a great challenge.
FPs are formed when light from both an object and its background reaches the same sensor pixel, generating a mixed depth measurement. These often appear to be ﬂoat-ing in empty space in the resultant point cloud, hence ﬂying pixels. Computationally unmixing these FPs often leads to edge blur or severe artifacts [72]. As they originate in the optical pipeline, artifacts of the light collection process by the main lens, we argue that an effective strategy to miti-gate them should also start in the optical pipeline. Unfortu-nately, a direct masking approach, such as simply reducing aperture size to block stray light paths, is not efﬁcient for overall light throughput, and so signiﬁcantly lowers SNR.
With Mask-ToF we learn a microlens amplitude mask, allowing us to generate per-pixel aperture conﬁgurations with spatially-varying susceptibility to noise and FPs, as shown in Figure 1. We train an encoder-decoder network which learns to aggregate this spatial information and lever-ages mask structural cues to produce reﬁned depth esti-mates. We then backpropagate this net’s loss to jointly learn high-level mask patterns. We photolithographically manu-facture the learned mask, and virtually place it on the sensor with a custom optical relay system to validate Mask-ToF on real-world data. In the future, we expect this mask can be fabricated directly on the camera sensor in a similar manner to a polarization sensor [47], preserving its form factor.
In summary, we make the following contributions:
• We develop a differentiable AMCW ToF image forma-tion model, including sub-aperture light transport.
• We incorporate sub-aperture masking and a reﬁnement network into this framework and learn an optimal mask structure through a patch-based gradient descent ap-proach from synthetic data.
• We test the masks in simulation, evaluating on over-all error and FP reduction, then manufacture them and construct an experimental setup to validate the pro-posed method on real data. 2.