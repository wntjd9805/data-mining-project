Abstract
We propose a novel approach for large-scale nonlin-ear least squares problems based on deep learning frame-works. Nonlinear least squares are commonly solved with the Levenberg-Marquardt (LM) algorithm for fast conver-gence. We implement a general and efﬁcient LM solver on a deep learning framework by designing a new backward jacobian network to enable automatic sparse jacobian ma-trix computation. Furthermore, we introduce a stochastic domain decomposition approach that enables batched op-timization and preserves convergence for large problems.
We evaluate our method by solving bundle adjustment as a fundamental problem. Experiments show that our op-timizer signiﬁcantly outperforms the state-of-the-art solu-tions and existing deep learning solvers considering quality, efﬁciency, and memory. Our stochastic domain decomposi-tion enables distributed optimization, consumes little mem-ory and time, and achieves similar quality compared to a global solver. As a result, our solver effectively solves non-linear least squares on an extremely large scale. Our code will be available based on Pytorch1 and Mindspore2. 1.

Introduction
Numerical optimization is an important stage in differ-ent problems across science and engineering. In computer vision and graphics, many problems are related to model
ﬁtting and can be formulated as nonlinear least squares in-cluding mesh processing [16, 43, 40, 34], motion and model reconstruction [46, 61, 47, 25, 19], and bundle adjust-ment [49, 36, 3, 54, 35, 57, 58] as a fundamental problem in 3D reconstruction. Nonlinear least squares are widely solved using Levenberg-Marquardt (LM) algorithm [30, 33] for its robustness and fast convergence. General solvers have been developed for academia and industry use like 1https://github.com/hjwdzh/DeepLM 2https://gitee.com/mindspore/mindspore/tree/ master/model_zoo/research/3d/DeepLM
Figure 1. Large-scale bundle adjustment on a deep learning frame-work using stochastic domain decomposition. We solve problems on a very large scale with high convergence quality.
G2o [21] and Ceres [2]. However, they do not ﬁt the in-creasing demand to efﬁciently handle extremely large prob-lems with the availability of advanced hardware like GPUs.
Typical algorithms are speciﬁcally designed for efﬁ-ciency or scalability. PBA [54] uses GPUs to accelerate the optimization for bundle adjustment. [19] proposes lo-cal schur complement for facial performance capture. [57] segments the graph and achieves camera consensus with
ADMM [7]. STBA [58] accelerates the step computation by solving smaller problems with a correction stage. How-ever, these methods cannot handle problems with billions of residuals, potentially sacriﬁce quality, or do not handle general nonlinear least squares problems.
Recently, deep learning frameworks like Tensorﬂow [1] and Pytorch [37] opens the potential to efﬁciently solve large-scale problems. It makes full use of computation re-sources like GPUs, provides an automatic backpropagation system to ease the user programming for general problems, and implements effective solvers for stochastic optimiza-tion. Unfortunately, they are not ideal for solving nonlin-ear least squares since gradient descent based optimizers 10308
converge much slower than commonly used LM algorithm.
Therefore, we target at developing a stochastic LM opti-mizer inside deep learning frameworks to solve general and large nonlinear least squares with dramatic improvement in efﬁciency and scalability. Speciﬁcally, we mainly address two challenges for developing such an optimizer.
First, backpropagation cannot be directly used to com-pute sparse jacobians required by the LM algorithm. While backpropagation provides the partial derivative of a scalar function to each variable, jacobian matrix require deriva-tives of all residuals to related variables. It is impractical to call backpropagation for each residual since it yields a dense jacobian matrix and consumes a huge amount of memory and time. We ﬁnd that indices of variables for residuals specify an one-to-one correspondence between nonzero en-tries of the jacobian and the derivatives of a scalar function.
Based on it, We design a novel backward jacobian network to compute this scalar function, call a single backpropaga-tion to collect derivatives and recover the jacobian.
Second, while batch-based stochastic optimization han-dles large problems, it easily causes LM solver to diverge since LM steps are usually large and require accurate ja-cobians computation from all data. We ﬁnd that domain decomposition [9] can avoid the divergence. This approach segments variables into different domains and alternatively optimizes each domain by ﬁxing variables inside other do-mains. From the training perspective, we view domains as batches and alternative optimization as batched optimiza-tion with multiple epochs. However, such an approach re-duces convergence quality especially for boundary variables between different domains. To achieve high convergence quality, we propose a novel stochastic domain decomposi-tion approach. Different from traditional domain decompo-sition, we derive different segmentation of domains using stochastic clustering so that variables at the boundaries of domains are changing for each epoch. This dramatically enhances the convergence quality for the optimization. We further provide an optional global reinitialization step at the beginning of each epoch. We abstract each domain with a descriptor preserving intra-domain residuals and globally optimize all descriptors for different domains.
In our experiments, we mainly study a fundamental problem as bundle adjustment. Figure 1 shows a large-scale bundle adjustment problem solved using our stochastic do-main decomposition. Our LM solver signiﬁcantly outper-forms existing solutions considering quality, efﬁciency, and memory. Experiments show that our stochastic approach converges as fast as a global LM solver, and signiﬁcantly reduces the consumption of memory and time. Ablation studies show that the key contribution to quality comes from stochastic decomposition. We provide a general, efﬁcient, and scalable stochastic LM solver on deep learning frame-works for large-scale nonlinear least squares. 2.