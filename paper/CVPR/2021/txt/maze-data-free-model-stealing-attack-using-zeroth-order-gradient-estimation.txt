Abstract
High quality Machine Learning (ML) models are of-ten considered valuable intellectual property by companies.
Model Stealing (MS) attacks allow an adversary with black-box access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing
MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset nec-essary to query the target model. In this paper, we show that preventing access to the target dataset is not an ade-quate defense to protect a model. We propose MAZE – a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created us-ing a generative model to perform MS.
Our evaluation with four image classiﬁcation models shows that MAZE provides a normalized clone accuracy in the range of 0.90⇥ to 0.99⇥, and outperforms even the recent attacks that rely on partial data (JBDA, clone accu-racy 0.13⇥ to 0.69⇥) and on surrogate data (KnockoffNets, clone accuracy 0.52⇥ to 0.97⇥). We also study an ex-tension of MAZE in the partial-data setting, and develop
MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97⇥ to 1.0⇥) and reduces the query budget re-quired for the attack by 2⇥-24⇥. 1.

Introduction
The ability of Deep Neural Networks (DNNs) to achieve state of the art performances in a wide variety of challenging computer-vision tasks has spurred the wide-spread adop-tion of these models by companies to enable various prod-ucts and services such as self-driving cars, license plate reading, disease diagnosis from medical images, activity classiﬁcation from images and video, and smart cameras.
As the performance of ML models scales with the train-ing data [11], companies invest signiﬁcantly in collecting vast amounts of data to train high-performance ML mod-els. Protecting the conﬁdentiality of these models is vital for companies to maintain a competitive advantage and to prevent the stolen model from being misused by an adver-sary to compromise security and privacy. For example, an adversary can use the stolen model to craft adversarial ex-amples [9, 30, 32], compromise user membership privacy through membership inference attacks [29, 34, 21], and leak sensitive user data used to train the model through model in-version attacks [6, 35, 38]. Thus, ML models are considered valuable intellectual properties of the owner and are closely guarded against theft and data leaks.
Step 1: Construct Training dataset by querying the target model (a) (b) x3 y3 = T (x3) x0, y0 x1, y1 x2, y2
Dataset      Black -Box       
Target Model (T )
Step 2: Use the constructed dataset to train the clone model x0, y0 x1, y1 x2, y2
·
· xn, yn
Dataset
Clone Model (C)
Figure 1. Model stealing attacks: The target model is queried us-ing a set of inputs {xi}n i=1 to obtain a labeled training dataset
{xi, yi}n i=1, which is used to train the clone model.
Model functionality stealing attacks compromise the conﬁdentiality of ML models by allowing an adversary to train a clone model that closely mimics the predictions of the target model, effectively copying its functionality.
These attacks only require black-box access to the target model where the adversary can access the predictions of the model for any given input. Fig. 1 illustrates the steps involved in carrying out a MS attack. The adversary ﬁrst 113814
queries the target model T with various inputs {xi}n i=1 and uses the predictions of the target model yi = T (xi) to con-struct a labeled dataset D = {xi, yi}. This dataset is then used to train a clone model C to match the predictions of T .
In the current state of the art methods (e.g., [26, 24]), the availability of in-distribution or similar surrogate data to query the target model plays a key role in the ability of the attacker to train high accuracy clone models. However, in most real-world scenarios, the training data is not readily available to the attacker as companies typically train their models using proprietary datasets. To carry out MS in such a data-limited setting, existing attacks either assume par-tial availability of the target dataset or the availability of a surrogate dataset that is semantically similar to the tar-get dataset (e.g., using CIFAR-100 to attack a CIFAR-10 model). For example, Jacobian-Based Dataset Augmenta-tion (JBDA) [26] is an attack that uses a subset of the train-ing data to create additional synthetic data, which is used to query the target model. KnockoffNets [24] is another MS at-tack that uses a surrogate dataset to query the target model.
These attacks become ineffective without access to the tar-get dataset or a representative surrogate dataset. 1
This paper is the ﬁrst to show that a highly accurate MS attack is feasible without relying on any access to the tar-get dataset or even a surrogate dataset – our method only relies on synthetically-generated out-of-distribution data – but results in high-accuracy clones on in-distribution data.
We make the following key contributions in our paper:
Contribution 1: We propose MAZE– the ﬁrst data-free model stealing attack capable of training high-accuracy clone models across multiple image classiﬁcation datasets and complex DNN target models.
In contrast to exist-ing attacks that require some form of data to query the target, MAZE uses synthetic data created using a genera-tive model to carry out MS attack. Our evaluations across
DNNs trained on various image classiﬁcation tasks show that MAZE provides a normalized clone accuracy of 0.90⇥ to 0.99⇥ (normalized clone accuracy is the accuracy of the clone model expressed as a fraction of the target-model ac-curacy). Despite not using any data, MAZE outperforms recent attacks that rely on partial data (JBDA, clone accu-racy of 0.13⇥ to 0.69⇥) or surrogate data (KnockoffNets, clone accuracy of 0.52⇥ to 0.97⇥).
Contribution 2: Our key insight is to draw inspira-tion from data-free knowledge distillation (KD) and zeroth-order gradient estimation to train the generative model used to produce synthetic data in MAZE. Similar to data-free
KD, the generator is trained on a disagreement objective, which encourages it to produce synthetic inputs that maxi-mize the disagreement between the predictions of the target 1We refer the interested readers to Section 6.1 of the KnockoffNets paper [24] for a discussion on the importance of using semantically similar datasets to carry out the attack. (teacher) and the clone (student) models. By training the clone model on such synthetic examples we can improve the alignment of the clone model’s decision boundary with that of the target, resulting in a high-accuracy clone model.
In data-free KD, training the generator on the disagree-ment objective is possible since white-box access to the teacher model is available. But, unlike in data-free KD,
MAZE operates in a black-box setting. We therefore lever-age zeroth-order gradient estimation (ZO) [22, 7] to approx-imate the gradient of the black-box target model and use this to train the generator. Unfortunately, we found a di-rect application of ZO gradient estimation to be impractical on real-world image classiﬁcation models since the dimen-sionality of the generator’s parameters can be in the order of millions. We propose a way to overcome the dimensionality problem by estimating gradients with respect to the signiﬁ-cantly lower-dimensional synthetic input and show that our method can be successfully used to train a generator in a query-efﬁcient manner.
Contribution 3: In some cases, partial datasets may be available. Recognizing that, we propose an extension of
MAZE, called MAZE-PD, for scenarios where a small par-tial dataset (e.g., 100 examples) is available to the attacker.
MAZE-PD leverages the available data to produce queries that are closer to the training distribution than in MAZE by using generative adversarial training. Our evaluations show that MAZE-PD provides near-perfect clone accuracy (0.97⇥ to 1.0⇥), while reducing the number of queries by 2⇥-24⇥ compared to MAZE.
In summary, our key ﬁnding is that an attacker only requires black-box access to the target model and no in-distribution data to create high-accuracy clone models in the image classiﬁcation domain. If even a very limited amount of in-distribution data is available, near-perfect clone ac-curacy is feasible. This raises questions on how machine learning models can be better protected from competitors and bad actors in this domain. 2.