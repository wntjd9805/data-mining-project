Abstract
Grounding referring expressions in RGBD image has been an emerging ﬁeld. We present a novel task of 3D visual grounding in single-view RGBD image where the referred objects are often only partially scanned due to occlusion.
In contrast to previous works that directly generate object proposals for grounding in the 3D scenes, we propose a bottom-up approach to gradually aggregate content-aware information, effectively addressing the challenge posed by the partial geometry. Our approach ﬁrst fuses the lan-guage and the visual features at the bottom level to gen-erate a heatmap that coarsely localizes the relevant regions in the RGBD image. Then our approach conducts an adap-tive feature learning based on the heatmap and performs the object-level matching with another visio-linguistic fu-sion to ﬁnally ground the referred object. We evaluate the proposed method by comparing to the state-of-the-art meth-ods on both the RGBD images extracted from the ScanRefer dataset and our newly collected SUNRefer dataset. Experi-ments show that our method outperforms the previous meth-ods by a large margin (by 11.2% and 15.6% Acc@0.5) on both datasets. 1.

Introduction
Figure 1: We present a novel task of 3D visual grounding in single-view RGBD images given a referring expression, and propose a bottom-up neural approach to address it. Our goal is to estimate the bounding box that encloses the full shape of the referred object even this object is only partially ob-served (top-left). Predicted bounding boxes of the referred objects are in green.
Localizing objects described by referring expressions in vision signals, also known as visual grounding, has long been a major motive for robotics and embodied vi-sion. So far, we have seen growing efforts devoted to visual grounding in images [17, 36, 13, 40, 24, 29, 33, 5, 41, 11, 42, 10, 9, 12, 19, 47, 18, 35, 38, 39, 20] and videos [46, 45, 43, 37, 30, 31, 44]. Suppose that a robot is going to take ‘the spoon on the table in the kitchen’ following your command [14, 23]; this would require a
*Corresponding Email: hanxiaoguang@cuhk.edu.cn
†Shenzhen Research Institute of Big Data
‡The Future Network of Intelligence Institute, The Chinese University of Hong Kong, Shenzhen more accurate localization result, preferable the 3D coordi-nate of the referred object rather than a 2D bounding box.
Recent works [4, 1] extend the visual grounding task to 3D scenes [7] and localize the object referred by a natu-ral language expression. While promising results are pro-duced, these methods can only perform 3D visual ground-ing in complete scenes that are reconstructed and/or seg-mented [4, 1] in advance. Thus, they are not readily appli-cable to single-view RGBD images with partial observation,
RGBD streaming data, or any dynamically changing envi-ronments.
To this end, we propose a novel task for 3D visual grounding: Given a single-view RGBD image of a scene, 6032
we aim at estimating the 3D bounding box of the full target object described by a given referring expression. While this novel task opens up many promising possibilities, it also poses a major challenge due to the nature of single-view
RGBD images that they contain only incomplete informa-tion about the scene and often partial observation of the re-ferred object. Compared to 2D visual grounding on image and 3D visual grounding on complete scene where geom-etry information is complete in 2D and 3D space, various occlusion cases in our task require a holistic understanding of the object geometry to infer the 3D bounding boxes en-closing the full target object.
Image-based grounding methods may be applied to
RGBD images but require an extra effort to lift the produced 2D bounding boxes to 3D. Chen et al. [4] proposed a one-stage search and match strategy for 3D grounding. How-ever, it fails to handle single-view RGBD images where the referred objects are partially observed. The reason is two-fold: First, it is inadequate to directly match between features of these object proposals and the referring expres-sion to achieve reliable grounding, as each proposal con-tains only incomplete information due to the partial obser-vation. Moreover, this is worsened by the fact that only content-free object proposals are generated by a detection network that searches the scene globally, thus failing to ac-cumulate useful information about the referred object for grounding.
With these observations, we propose a novel, bottom-up matching approach for ﬁne-grained grounding of the referred objects in given single-view RGBD images. To this end, our approach ﬁrst matches the query expression to the input RGBD image and generates a content-aware heatmap on the voxel domain converted from the RGBD image. This bottom-level matching amounts to coarse lo-calization of regions, which are relevant to the referred ob-ject. Then, based on the content-aware coarse localization, an adaptive search-and-match strategy is employed. This enables our network to conduct ﬁne-grained search in the relevant regions and generate visio-linguistic features by fusing the query with more informative features from the visual modality. These fused features are used to gener-ate and reﬁne the 3D object proposals to the ﬁnal bounding box enclosing the target object in the given referring expres-sion. Compared to previous works, our bottom-up approach exploits the language features at different levels, and thus enables our network to be content-aware during searching and matching stages. The adaptive search guided by the content-aware heatmap also ensures the feature learning to be concentrated in the relevant regions, mitigating the chal-lenge posed by the partial geometry.
Mauceri et al. [22] present the SUN-Spot dataset that provides spatial referring expressions to raw single-view
RGBD images in the SUNRGBD dataset[32]. However, the amount of language annotations as well as their linguistic variations (only spatial references) are inadequate. Alter-natively, one can extract the RGBD frames from 3D scene datasets [3, 28, 7, 4, 1] that also provide rich object-centric language descriptions. Yet, referring expressions provided in these scene datasets are constructed based on the scene context; they may contain other supporting objects that ex-ist in the scene but are not observed in a particular frame.
This artifact between annotations and the extracted frames from the 3D scenes motivates us to contribute a large-scale annotation dataset, SUNRefer, to facilitate future studies on visual grounding in single-view RGBD images. Built on the SUNRGBD dataset, our dataset contains 7,699 RGBD images with a total of 38,495 annotations of referring ex-pressions on 7,699 objects.
We evaluated our proposed 3D visual grounding method on both SUNRefer, our newly constructed dataset, and the ScanRefer dataset using the extracted RGBD frames.
We show that our approach outperforms the state-of-the-art methods by a signiﬁcant margin and validate our des-gin choices via extensive ablation study. Our method can also be applied to 3D visual grounding in streaming RGBD images at a processing rate of 10 frame-per-second.
Our key contributions are summarized as follows: 1) We present a novel and challenging task – 3D visual grounding in single-view RGBD images with possible in-complete information or partial occlusion 2) We propose a content-aware, bottom-up approach that signiﬁcantly outperforms the state-of-the-art methods on both our newly collected dataset and the ScanRefer dataset. 3) We contribute a large-scale dataset of referring phrases and the corresponding ground-truth bounding boxes for a large amount of publicly available RGBD images. 2.