Abstract
Unsupervised domain adaptation is a promising tech-nique for semantic segmentation and other computer vi-sion tasks for which large-scale data annotation is costly and time-consuming. In semantic segmentation, it is attrac-tive to train models on annotated images from a simulated (source) domain and deploy them on real (target) domains.
In this work, we present a novel framework for unsupervised domain adaptation based on the notion of target-domain consistency training. Intuitively, our work is based on the idea that in order to perform well on the target domain, a model’s output should be consistent with respect to small perturbations of inputs in the target domain. Speciﬁcally, we introduce a new loss term to enforce pixelwise consis-tency between the model’s predictions on a target image and a perturbed version of the same image. In comparison to popular adversarial adaptation methods, our approach is simpler, easier to implement, and more memory-efﬁcient during training. Experiments and extensive ablation studies demonstrate that our simple approach achieves remarkably strong results on two challenging synthetic-to-real bench-marks, GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes. 1.

Introduction
Deep neural network approaches for semantic image seg-mentation have shown widespread success in the past decade, but they remain reliant on large datasets with pixel-level annotations. Data labeling for semantic segmentation is no-toriously laborious and expensive, especially in domains where experts are required (e.g. medical image segmenta-tion). Even for annotations that can be performed by non-experts like parsing an urban scene into familiar objects, as in the Cityscapes dataset [15], it takes an estimated 90 minutes to annotate a single image [43].
The need to build generalizable models with limited data has motivated work on unsupervised domain adaptation (UDA) approaches for semantic segmentation [63, 49, 52, 7, 23, 11], where annotated images from a simulated (source) domain, which are plentiful, are used in conjunction with unlabeled images from a real (target) domain. The simulated source domain in this “synthetic to real” translation task can be creative, such as the video game Grand Theft Auto V in the GTA5-to-Cityscapes benchmark [40] and the simula-tion platform SYNTHIA as in the SYNTHIA-to-Cityscapes benchmark [41].
The literature on UDA for semantic segmentation is dom-inated by adversarial methods, which aim to learn domain-invariant representations across multiple domains by intro-ducing adversarial losses [23]. These methods have shown strong performance, but due to the instability of their adver-sarial losses, they are well-known to be highly sensitive to hyperparameters and difﬁcult to train [54, 8, 19].
Recently, a new line of work on UDA for semantic segmentation has emerged around self-training [63, 52, 7].
These methods add loss terms to the training objective that encourage the segmentation model to make more conﬁdent predictions on the target domain (for example, by encourag-ing low-entropy predictions) [52, 7, 63].
This paper begins with the observation that we do not simply desire a model that makes conﬁdent predictions in target domain, rather we desire a model that makes consistent predictions in the target domain. That is, we desire a model for which small perturbations of inputs in the target domain lead to small, consistent changes in the output segmentation.
If a model’s predictions are always conﬁdent, but they are not stable with respect to small perturbations of target images, the model is likely to be poorly-adapted to the target domain.
Conversely, if a model behaves smoothly with respect to perturbations of images in the target domain, the model is likely to be better-adapted to that domain.
We propose a consistency training-based framework to directly enforce this notion of smoothness in the target do-main. Our method, denoted PixMatch, adds a loss term that encourages the segmentation model’s predictions on a target domain image and a perturbed version of the same image to 12435
be pixelwise consistent.
We experiment with four different perturbation functions, two of which are inspired by work in semi/self-supervised learning (Data Augmentations; CutMix) and two of which are inspired by work in domain adaptation (Style Transfer;
Fourier Transform).
Surprisingly, we ﬁnd that our baseline model, which uses heavy data augmentation as its perturbation function, performs best. This simple baseline delivers extremely strong results on GTA5-to-Cityscapes [15] and SYNTHIA-to-Cityscapes [41]. Using only a source (supervised) loss and a target consistency loss, it outperforms complex prior methods that used combinations of source, adversarial, and self-training losses.
Compared to existing adversarial approaches, PixMatch is easier to implement, more stable during training, and less memory-intensive to train. It introduces only one hyperpa-rameter, which controls the relative weighting of the source (supervised) loss and the target consistency loss. Moreover, the simplicity of PixMatch means that it it may be easily integrated into existing UDA methods and pipelines; com-bining PixMatch with self-training yields the state-of-the-art results.
The main contributions of this paper are:
• We introduce a novel consistency-based framework for unsupervised domain adaptation that encourages pix-elwise consistency between a model’s predictions on a target image and a perturbed version of the same image.
• We investigate multiple perturbation functions, ﬁnding that a simple baseline using data augmentation performs extremely well.
• We perform extensive ablation studies on our baseline method in order to better understand its strong perfor-mance, showing that the setting of domain adaptation differs markedly from that of semi-supervised image classiﬁcation.
• We ﬁnd that our model may be easily combined with self-training for further performance improvements.
Doing so, we achieve a new state-of-the-art on the chal-lenging GTA5-to-Cityscapes benchmark, using a much simpler training approach than recent adversarial meth-ods. 2.