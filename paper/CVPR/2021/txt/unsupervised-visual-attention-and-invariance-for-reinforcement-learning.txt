Abstract
Existing Methods
Vision-based reinforcement learning (RL) is successful, but how to generalize it to unknown test environments re-mains challenging. Existing methods focus on training an RL policy that is universal to changing visual domains, whereas we focus on extracting visual foreground that is universal, feeding clean invariant vision to the RL policy learner. Our method is completely unsupervised, without manual annota-tions or access to environment internals.
Given videos of actions in a training environment, we learn how to extract foregrounds with unsupervised keypoint detection, followed by unsupervised visual attention to auto-matically generate a foreground mask per video frame. We can then introduce artiﬁcial distractors and train a model to reconstruct the clean foreground mask from noisy obser-vations. Only this learned model is needed during test to provide distraction-free visual input to the RL policy learner.
Our Visual Attention and Invariance (VAI) method sig-niﬁcantly outperforms the state-of-the-art on visual domain generalization, gaining 15∼49% (61∼229%) more cumula-tive rewards per episode on DeepMind Control (our Drawer-World Manipulation) benchmarks. Our results demonstrate that it is not only possible to learn domain-invariant vision without any supervision, but freeing RL from visual distrac-tions also makes the policy more focused and thus far better. 1.

Introduction
Vision-based deep reinforcement learning (RL) has achieved considerable success on robot control and manipu-lation. Visual inputs provide rich information that are easy and cheap to obtain with cameras [31, 32, 27, 9, 8]. However, vision-based RL remains challenging: It not only needs to process high-dimensional visual inputs, but it is also required to deal with signiﬁcant variations in new test scenarios (Fig. 1), e.g. color/texture changes or moving distractors [34, 2].
One solution is to learn an ensemble of policies, each handling one type of variations [44]. However, anticipating
*Equal contribution.
Generalizable/Universal 
RL Algorithm
Unknown Test 
Environments
Our Method (VAI)
VAI (•)
VAI (•)
VA I   ( • )
Distraction-invariant 
Observation Space
RL Algorithm
Figure 1: Top) Two ways to make vision-based reinforce-ment learning generalizable to unknown environments at the test time: Existing methods focus on learning an RL policy that is universal to varying domains, whereas our proposed
Visual Attention and Invariance (VAI) extracts visual fore-ground that is universal, feeding clean and invariant vision to RL. Bottom) VAI signiﬁcantly outperforms PAD (SOTA), increasing cumulative rewards by 49% and 61% respectively in random color tests on DeepMind control and random texture tests on our DrawWorld manipulation benchmarks. all possible variations quickly becomes infeasible; domain randomization methods [48, 53, 42, 43, 58] apply augmenta-tions in a simulated environment and train a domain-agnostic universal policy conditioned on estimated discrepancies be-tween testing and training scenarios.
Two caveats limit the appeal of a universal RL policy. 1) 6677
1) Unsupervised Keypoint Detection 2) Unsupervised Visual Attention 3) Self-supervised Visual Invariance
K e y
N e t
E n c o d e r os ot t r o p s n a r t r e d o c e
D
ˆot ot
Reconstruction Loss ℒ!"#
D(ot)
Causal Inference
Decoder
K e y
N e t
E n c o d e r
⊗
It
ˆ
D
Is
I
Stop 
Gradient
E n c o d e r
E n c o d e r
D
E(It)
E t
E(Is) r e d o c e
D
It
ˆD(Is)
We thank all three
Feature Matching 
Loss ℒ!"#$%
Reconstruction Loss 
ℒ&’(
Figure 2: Our VAI method has three components. 1) Unsupervised keypoint detection: Given two adjacent video frames, we learn to predict keypoints and visual features from each image so that foreground features and target keypoints can be used to reconstruct the target frame, without any manual annotations. 2) Unsupervised visual attention: We apply causal inference to remove the model bias from the foreground mask derived from detected keypoints. 3) Self-supervised visual invariance:
We are then able to add artiﬁcial distractors and train a model to reconstruct the clean foreground observations. Keypoint and attention modules are only used during training to extract foregrounds from videos without supervision, whereas only the last encoder/decoder (colored in green) trained for visual invariance is used to remove distractors automatically at the test time.
Model complexity. The policy learner must have enough complexity to ﬁt a large variety of environments. While there are universal visual recognition and object detection models that can adapt to multiple domains [45, 55], it would be hard to accomplish the same with a RL policy network often containing only a few convolutional layers. 2) Training in-stability. RL training could be brittle, as gradients for (often non-differentiable) dynamic environments can only be ap-proximated with a high variance through sampling. Adding strong augmentations adds variance and further instability, causing inability to converge. [15] handles instability with weaker augmentations, in turn reducing generalization.
The state-of-the-art (SOTA) approach, PAD [15], per-forms unsupervised policy adaption with a test-time auxil-iary task (e.g. inverse dynamic prediction) to ﬁne-tune the visual encoder of the policy network on the ﬂy. However, there is no guarantee that intermediate representations would
ﬁt the control part of the policy network. Drastic environ-ment changes such as background texture change from grid to marble can cause feature mismatches between adapted layers and frozen layers, resulting in high failure rates.
Instead of pursuing a policy that is universal to changing visual domains, we propose to extract visual foreground that is universal, and then feed clean invariant vision to a standard
RL policy learner (Fig. 2). As the visual observation varies little between training and testing, the RL policy can be simpliﬁed and focused, delivering far better results.
Our technical challenge is to deliver such clean visuals with a completely unsupervised learning approach, without mannual annotations or access to environment internals.
Given videos of actions in a training environment, we
ﬁrst learn how to extract visual foreground with unsuper-vised keypoint detection followed by unsupervised visual attention to automatically generate a foreground mask per video frame. We can then introduce artiﬁcial distractors and train a model to reconstruct the clean foreground mask from noisy observations. Only this learned model, not the keypoint or attention model, is needed during test to provide distraction-free visual input to the RL policy learner.
Our unsupervised Visual Attention and Invariance (VAI) method has several desirable properties. 1. Unsupervised task-agnostic visual adaption training.
Our foreground extraction only assumes little background change between adjacent video frames, requiring no man-ual annotations or knowledge of environment internals (e.g. get samples with altered textures). It does not de-pend on the task, policy learning, or task-speciﬁc rewards associated with RL. That is, for different tasks in the same environment, we only need to collect one set of visual observations and train one visual adapter, which gets us a huge saving in real-world robotic applications. 2. Stable policy training, no test-time adaptation. By freeing RL from visual distractions, our policy learning is stable and fast without being subject to strong domain augmentations, and our policy deployment is immediate without test-time ﬁne-tuning. 3. Clear interpretation and modularization. We extract keypoints from videos to identify foreground, based on which attentional masks can be formed. This unsuper-vised foreground parsing allows us to anticipate visual distractions and train a model to restore clean foregrounds.
Compared to existing methods that work on intermediate features, our method has clear assumptions at each step, which can be visualized, analyzed, and improved.
We conduct experiments on two challenging benchmarks with diverse simulation environments: DeepMind Control suite [51, 15] and our DrawerWorld robotic manipulation tasks with texture distortions and background distractions during deployment. Our VAI signiﬁcantly outperforms the state-of-the-art, gaining 15∼49% (61∼229%) more cumula-6678
tive rewards per episode on DeepMind Control (our Drawer-World Manipulation) benchmarks.
To summarize, we make the following contributions. 1. We propose a novel domain generalization approach for vision-based RL: Instead of learning a universal policy for varying visual domains, we decouple vision and ac-tion, learning to extract universal visual foreground while keeping the RL policy learning intact. 2. We propose a fully unsupervised, task-agnostic visual adaptation method that removes unseen distractions and restores clean foreground visuals. Without manual anno-tations, strong domain augmentations, or test-time adapta-tion, our policy training is stable and fast, and our policy deployment is immediate without any latency. 3. We build unsupervised keypoint detection based on
KeyNet [22] and Transporter [24]. We develop a novel unsupervised visual attention module with causal infer-ence for counterfactual removal. We achieve visual in-variance by unsupervised distraction adaptation based on foreground extraction. Each step is modularized and has clear interpretations and visualizations. 4. We propose DrawerWorld, a pixel-based robotic manip-ulation benchmark, to test the adaptation capability of vision-based RL to various realistic textures. 5. Our results demonstrate that it is not only possible to learn domain-invariant vision from videos without supervision, but freeing RL from visual distractions also leads to better policies, setting new SOTA by a large margin. 2.