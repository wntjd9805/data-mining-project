Abstract
Untrimmed(cid:2)Video
Given an untrimmed video and a query sentence, cross-modal video moment retrieval aims to rank a video moment from pre-segmented video moment candidates that best matches the query sentence. Pioneering work typically learns the representations of the textual and visual content separately and then obtains the interactions or alignments between different modalities. However, the task of cross-modal video moment retrieval is not yet thoroughly ad-dressed as it needs to further identify the ﬁne-grained differences of video moment candidates with high repeata-bility and similarity. Moveover, the relation among objects in both video and sentence is intuitive and efﬁcient for understanding semantics but is rarely considered.
Toward this end, we contribute a multi-modal relational graph to capture the interactions among objects from the visual and textual content to identify the differences among similar video moment candidates.
Speciﬁcally, we ﬁrst introduce a visual relational graph and a textual relational graph to form relation-aware representations via message propagation. Thereafter, a multi-task pre-training is designed to capture domain-speciﬁc knowledge about objects and relations, enhancing the structured visual representation after explicitly deﬁned relation. Finally, the graph matching and boundary regression are employed to perform the cross-modal retrieval. We conduct extensive experiments on two datasets about daily activities and cooking activities, demonstrating signiﬁcant improvements over state-of-the-art solutions. 1.

Introduction
Entering the era of information explosion, individuals spend more time in seeking their desired information and the video is not an exception. However, traditional video retrieval methods are speciﬁcally designed for whole video retrieval and are not suitable for more ﬁne-grained video
∗Corresponding authors.
Multi(cid:3)scale(cid:2)pre(cid:3)segmented(cid:2)video(cid:2)moment(cid:2)candidates (a) Video moment candidates with high similarity
Query(cid:2)sentence:(cid:2)The(cid:2)person puts a(cid:2)book(cid:2)in(cid:2)a(cid:2)bag.
...(cid:2)... (b) Interactions of limited objects
Figure 1: Challenges in cross-modal video moment re-trieval. Fig.1a reveals the difﬁculty of retrieving desired video moment from candidates with high similarity, while
Fig.1b exhibits the difﬁculty of modeling the spatial-temporal interactions of objects. moment retrieval scenario. To alleviate people’s expectation of quickly retrieving a desired video moment, the task of cross-modal video moment retrieval [1, 8] is proposed. In particular, given an untrimmed video and a query sentence, the task of cross-modal video moment retrieval aims to extract a video moment from the untrimmed video that best matches the query.
In fact, a great effort has been made to address the cross-modal video moment retrieval issue. Existing work mostly relies on multi-scale pre-segmented video moment candidates via the sliding window strategy, and then retrieves a suitable video moment from them [36]. Similar to the cross-modal retrieval task [2], the cross-modal video moment retrieval needs to understand and stitch text-video semantics. The typical method is to extract the global [5] and local [3, 17] information of the sentence and video ﬁrst, then leverage attention mechanism [21, 22, 24] and seman-tic matching [34] to fuse modalities, and ﬁnally rank video moment candidates based on the learned representation. As 2215
compared to cross-modal video retrieval, the task of cross-modal video moment retrieval is more complicated since it needs to further identify the slight differences of video moment candidates generated from a same video. As shown in Fig.1a, video moment candidates are of high similarity due to the segmentation via the sliding window strategy, which requires more sophisticated intra-modal recognition capabilities. Although recent work has emerged to ﬁnd the relationship among video moment candidates [38] or generate some more reasonable candidates instead of pre-segmented clips [4], they are not speciﬁcally designed for understanding semantics on video frames.
Further observations have found that the background of video moment candidates changes slightly, while the sematic differences of generated candidates are determined by limited objects. As revealed in Fig.1b, for the query sentence, the essential difference between the expected and the deviated candidates is whether the moment of “book enters bag” is covered, which brings the dawn of distin-guishing video moment candidates with high similarity.
In other words, exploring the interaction pattern among limited objects (i.e., person, book, and bag) is helpful to reduce redundant information and highlight key clues to distinguish video moment candidates. Especially, in the pattern where an object disappears or two objects no longer interact, modeling the interaction of objects can be regarded as a signiﬁcant signal. Therefore, how to understand the relation among objects in the video and its query sentence, and sensitively capture the differences of video moment candidates with high similarity is of great importance.
To address aforementioned issues, we propose a multi-modal relational graph (MMRG) framework to investigate the cross-modal video moment retrieval task comprehen-sively. The general framework of MMRG is illustrated in
Fig.2. To be speciﬁc, we ﬁrst construct graphs for visual and textual objects separately, where the visual objects are constrained by textual objects instead of modeling all visible objects. Meanwhile, the relations among objects is explicitly treat as nodes to solve the problem of ambiguous relation deﬁnition. Moreover, we innovatively propose a customized multi-task pre-training strategy in the visual relation understanding, which can highlight objects and relations, and enhance the performance of visual repre-sentation with explicitly deﬁned relation. Finally, both graph matching and boundary regression are introduced to regularize the cross-modal retrieval.
The main contributions of this work are three-fold:
• To the best of our knowledge, this is the ﬁrst work that attempts to perform the cross-modal video moment retrieval by investigating the interactions among visual and textual objects, which is able to distinguish video moment candidates with high intra-modal similarity.
• We propose a graph-based solution, MMRG, to improve the performance of cross-modal video moment retrieval, which is well suited for modeling the cross-modal semantic consistency and interactions among objects.
• Extensive experiments are conducted on two well-known datasets, which demonstrate the effectiveness of our method. Meanwhile, we have released the dataset and implementation to facilitate the research community1. 2.