Abstract
We introduce DatasetGAN: an automatic procedure to generate massive datasets of high-quality semantically seg-mented images requiring minimal human effort. Current deep networks are extremely data-hungry, beneﬁting from training on large-scale datasets, which are time consuming to annotate. Our method relies on the power of recent GANs to generate realistic images. We show how the GAN latent code can be decoded to produce a semantic segmentation of the image. Training the decoder only needs a few labeled examples to generalize to the rest of the latent space, result-ing in an inﬁnite annotated dataset generator! These gen-erated datasets can then be used for training any computer vision architecture just as real datasets are. As only a few images need to be manually segmented, it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations. To showcase the power of our approach, we generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outper-forms all semi-supervised baselines signiﬁcantly and is on par with fully supervised methods, which in some cases re-quire as much as 100x more annotated data as our method. 1.

Introduction
Curating image datasets with pixel-wise labels such as semantic or instance segmentation is very laborious (and expensive). Labeling a complex scene with 50 objects can take anywhere between 30 to 90 minutes – clearly a bottle-neck in achieving the scale of a dataset that we might desire.
In this paper, we aim to synthesize large high quality labeled datasets by needing to label only a handful of examples.
Semi-supervised learning has been a popular approach in the quest of reducing the need for labeled data, by lever-aging an additional large unlabeled dataset. The dominant approach trains a model on a labeled dataset using ground truth annotations while utilizing pseudo-labels [3, 46] and consistency regularization [46, 48] on the unlabeled ex-amples. While most methods were showcased on clas-siﬁcation tasks, recent work also showed success for the task of semantic segmentation [40]. On the other hand, contrastive learning aims to train feature extractors us-ing contrastive (unsupervised) losses on sampled image pairs [42, 49, 8, 39], or image patches [23]. Once a powerful image representation is trained using unsupervised losses alone, only a small subset of labeled images is typically re-quired to train accurate predictors. In our work, we show that the latest state-of-the-art generative models of images learn extremely powerful latent representations that can be leveraged for complex pixel-wise tasks.
We introduce DatasetGAN which generates massive datasets of high-quality semantically segmented images re-quiring minimal human effort. Key to our approach is an observation that GANs trained to synthesize images must acquire rich semantic knowledge in their ability to render 10145
diverse and realistic examples of objects. We exploit the feature space of a trained GAN and train a shallow decoder to produce a pixel-level labeling. Our key insight is that only a few labeled images are needed to train a success-ful decoder, leading to an inﬁnite annotated dataset genera-tor. These generated datasets can then be used for training any computer vision architecture just as real datasets are.
Since we only need to label a few examples, we annotate images in extreme detail and generate datasets with rich ob-ject and part segmentations. We generated datasets for 7 image segmentation tasks which include pixel-level labels for 34 human face parts, and 32 car parts. Our approach outperforms all semi-supervised baselines signiﬁcantly and is on par with fully supervised methods, while in some cases requiring two orders of magnitude less annotated data.
The ability of training successful computer vision mod-els with as little as 16 labeled examples opens the door to exciting downstream applications. In our work, we show-case 3D reconstruction of animatable objects where we ex-ploit the detailed part labels our method produces. 2.