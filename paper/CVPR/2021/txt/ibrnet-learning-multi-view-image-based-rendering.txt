Abstract
We present a method that synthesizes novel views of com-plex scenes by interpolating a sparse set of nearby views.
The core of our method is a network architecture that in-cludes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the ﬂy from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that opti-mizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes.
We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis meth-ods that also seek to generalize to novel scenes. Further, if
ﬁne-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods.1 1.

Introduction
Given a set of posed images of a scene, the goal of novel view synthesis is to produce photo-realistic images of the same scene at novel viewpoints. Early work on novel view synthesis focused on image-based rendering (IBR). Starting from the pioneering work on view interpolation of Chen and Williams [6], and proceeding through light ﬁeld ren-dering [3, 15, 28], view-dependent texturing [8], and more modern learning-based methods [17], IBR methods gener-ally operate by warping, resampling, and/or blending source views to target viewpoints. Such methods can allow for high-resolution rendering, but generally require either very dense input views or explicit proxy geometry, which is difﬁcult to estimate with high quality leading to artifacts in rendering. 1https://ibrnet.github.io/
More recently, one of the most promising research direc-tions for novel view synthesis is neural scene representations, which represent scenes as the weights of neural networks.
This research area has seen signiﬁcant progress through the use of Neural Radiance Fields (NeRF) [40]. NeRF shows that multi-layer perceptrons (MLPs) combined with posi-tional encoding can be used to represent the continuous 5D radiance ﬁeld of a scene, enabling photo-realistic novel view synthesis on complex real-world scenes. NeRF’s use of con-tinuous scene modeling via MLPs, as opposed to explicit discretized volumes [56] or multi-plane images [12, 70] al-lows for more compact representations and scales to larger viewing volumes.
Although neural scene representations like NeRF can represent scenes faithfully and compactly, they typically require a lengthy optimization process for each new scene before they can synthesize any novel views of that scene, which limits the value of these methods for many real-world applications.
In this work, we leverage ideas from both IBR and NeRF into a new learning-based method that generates a contin-uous scene radiance ﬁeld on-the-ﬂy from multiple source views for rendering novel views. We learn a general view interpolation function that simultaneously performs den-sity/occlusion/visibility reasoning and color blending while rendering a ray. This enables our system to operate with-out any scene-speciﬁc optimization or precomputed proxy geometry.
At the core of our method is a lightweight MLP network that we call IBRNet, which aggregates information from source views along a given ray to compute its ﬁnal color.
For sampled 3D locations along the ray, the network ﬁrst fetches latent 2D features, derived from nearby source views, that encode spatial context. IBRNet then aggregates these 2D features for each sampled location to produce a density feature that captures information about whether that feature seems to be on a surface. A ray transformer module then computes a scalar density value for each sample by consid-ering these density features along the entire ray, enabling visibility reasoning across larger spatial scales. Separately, a 4690
color blending module uses the 2D features and view direc-tion vectors from source views to derive a view-dependent color for each sample, computed as a weighted combination of the projected colors of the source views. A ﬁnal color value is then computed for each ray using volume rendering.
Our approach is fully differentiable and can therefore be trained end-to-end using multi-view images. Our exper-iments show that when trained on large amounts of data, our method can render high-resolution photo-realistic novel views for unseen scenes that contain complex geometry and materials, and our quantitative evaluation shows that it im-proves upon state-of-the-art novel view synthesis methods designed to generalize in a single shot to new test scenes.
Moreover, for a particular scene, we can ﬁne-tune IBRNet to improve the quality of synthesized novel views to match the performance of state-of-the-art neural scene representation methods like NeRF [40]. In summary, our contributions are: – a new learning-based multi-view image-based rendering approach that outperforms existing one-shot view synthe-sis methods on novel scenes, – a new model architecture called IBRNet that enables the continuous prediction of colors and densities in space from multiple views, – a per-scene ﬁne-tuning procedure that achieves compara-ble performance to state-of-the-art novel view synthesis methods designed only for single-scene inference. 2.