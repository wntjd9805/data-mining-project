Abstract
Current video retrieval efforts all found their evaluation on an instance-based assumption, that only a single caption is relevant to a query video and vice versa. We demonstrate that this assumption results in performance comparisons of-ten not indicative of models’ retrieval capabilities. We pro-pose a move to semantic similarity video retrieval, where (i) multiple videos/captions can be deemed equally relevant, and their relative ranking does not affect a method’s re-ported performance and (ii) retrieved videos/captions are ranked by their similarity to a query. We propose several proxies to estimate semantic similarities in large-scale re-trieval datasets, without additional annotations. Our anal-ysis is performed on three commonly used video retrieval datasets (MSR-VTT, YouCook2 and EPIC-KITCHENS). 1.

Introduction
Video understanding approaches which incorporate lan-guage have demonstrated success in multiple tasks includ-ing captioning [61, 66], video question answering [68, 72] and navigation [5, 28]. Using language to search for videos has also become a popular research problem, known as video retrieval. Methods learn an underlying multi-modal embedding space to relate videos and captions. Along with large-scale datasets [18, 41, 57, 64, 71], several video re-trieval benchmarks and challenges [2, 70] compare state-of-the-art, as methods inch to improve evaluation metrics such as Recall@K and Median Rank.
In this paper, we question the base assumption in all these datasets and benchmarks—that the only relevant video to a caption is the one collected with that video. We offer the ﬁrst critical analysis of this assumption, propos-ing semantic similarity relevance, for both evaluation and training. Our effort is inspired by works that question as-sumptions and biases in other research problems such as
VQA [27, 69], metric learning [46], moment retrieval [50], action localisation [4] and action recognition [16, 34, 45].
As shown in Fig. 1, current approaches target instance-based retrieval—that is, given a query caption such as “A man doing an origami tutorial”, only one origami video is
*Now at University of Amsterdam.
Figure 1: All current video retrieval works treat caption collected for a certain video as relevant, even when other videos are equally relevant to a query text. This makes the evaluation of popular datasets ad hoc at times. We propose to use continuous similarity, allowing multiple videos to be treated as equally relevant. Ex. from MSR-VTT [64]. considered as the correct video to retrieve. In fact, many videos within the dataset can be similar to the point of be-ing identical. The order in which such videos are retrieved should not affect the evaluation of a method. Instead, we propose utilising semantic similarity between videos and captions, where we assign a similarity score between items of differing modalities. This allows multiple videos to be considered relevant to a caption and provides a way of rank-ing videos from most to least similar.
Our contributions can be summarised: (i) We expose the shortcoming of instance retrieval in current video retrieval benchmarks and evaluation protocols. (ii) We propose video retrieval with semantic similarity, both for evaluation and training, where videos are ranked by their similarity to a caption, allowing multiple videos to be considered rele-vant and vice-versa. (iii) Avoiding large annotation effort, we propose several proxies to predict semantic similarities, using caption-to-caption matching. (iv) We analyse three benchmark datasets, using our semantic similarity proxies, noting their impact on current baselines and evaluations. 2.