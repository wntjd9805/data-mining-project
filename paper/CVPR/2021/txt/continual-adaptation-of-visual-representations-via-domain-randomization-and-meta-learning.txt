Abstract
Most standard learning approaches lead to fragile mod-els which are prone to drift when sequentially trained on samples of a different nature—the well-known catastrophic forgetting issue.
In particular, when a model consecu-tively learns from different visual domains, it tends to for-get the past domains in favor of the most recent ones.
In this context, we show that one way to learn models that are inherently more robust against forgetting is do-main randomization—for vision tasks, randomizing the cur-rent domain’s distribution with heavy image manipulations.
Building on this result, we devise a meta-learning strat-egy where a regularizer explicitly penalizes any loss associ-ated with transferring the model from the current domain to different “auxiliary” meta-domains, while also easing adaptation to them. Such meta-domains are also gener-ated through randomized image manipulations. We empir-ically demonstrate in a variety of experiments—spanning from classiﬁcation to semantic segmentation—that our ap-proach results in models that are less prone to catastrophic forgetting when transferred to new domains. 1.

Introduction
Modern computer vision approaches can reach super-human performance in a variety of well-deﬁned and iso-lated tasks at the expense of versatility. When confronted to a plurality of new tasks or new visual domains, they have trouble adapting, or adapt at the cost of forgetting what they had been initially trained for. This phenomenon, that has been observed for decades [39], is known as catastrophic forgetting. Directly tackling this issue, lifelong learning ap-proaches, also known as continual learning approaches, are designed to continuously learn from new samples without forgetting the past.
*www.europe.naverlabs.com
Figure 1. This paper tackles the continual domain adaptation task (bottom), compared here with standard domain adaptation, domain generalization, and domain randomization (top).
In this work, we are concerned with the problem of con-tinual and supervised adaptation to new visual domains (see
Figure 1). Framing this task as continual domain adap-tation, we assume that a model has to learn to perform a given task while being exposed to conditions which con-stantly change throughout its lifespan. This is of particu-lar interest when deploying applications to the real-world where a model is expected to seamlessly adapt to its en-vironment and can encounter different domains from the one(s) observed originally at training time. A possible so-lution to mitigate catastrophic forgetting is storing samples from all domains encountered throughout the lifespan of the model [41]. While effective, this solution may not be appro-priate when retaining data is not allowed (e.g., due to pri-vacy concerns) or when working under strong memory con-straints (e.g., in mobile applications). For these reasons, we are interested in developing methods for learning visual rep-resentations that are inherently more robust against catas-trophic forgetting, without necessarily requiring any infor-mation storage or model expansion.
To tackle this problem, we start from a simple intuition: when adapting a model trained on a domain D1 to a sec-ond domain D2, we can anticipate the severity of forgetting to depend on how demanding the adaptation process is— 4443
that is, how close D2 is to D1. The natural issue is that we cannot control whether sequential domains will be more or less similar to each other. Motivated by results on domain randomization [58, 65] and single-source domain general-ization [60], we propose to heavily perturb the distribution of the current domain to increase the probability that sam-ples from future domains will be closer to the current data distribution—hence (generally) requiring a lighter adapta-tion process. Focusing on computer vision tasks, we use image transformations for the randomization process, and show that models trained in this fashion are signiﬁcantly more robust against catastrophic forgetting in the context of continual, supervised domain adaptation. This result repre-sents our ﬁrst contribution.
Further, we question whether we can learn representa-tions that are inherently robust against transfer to new do-mains (that is, against gradient updates on samples from distributions different than the current one). We tackle this problem through the lens of meta-learning and devise a regularization strategy that forces the model to train for the task of interest on the current domain, while learning to be resilient to potential parameter updates on domains different from the current one.
In general, meta-learning approaches require access to a number of different meta-tasks (or meta-domains in our case), but our setting only allows access to samples from the current domain at any point in time. To overcome this issue, we introduce “aux-iliary” meta-domains that are produced by randomizing the original distribution—also here, using standard image trans-formations. Additionally, inspired by Finn et al. [18], we encourage our model to train in a way that will allow it to efﬁciently adapt to new domains. The devised meta-learning algorithm, based on the new concept of auxiliary meta-domains, constitutes our second contribution.
To extensively assess the effectiveness of our continual domain adaptation strategies, we start with an experimen-tal protocol which tackles digit recognition. Further, we increase the difﬁculty of the task and focus on the PACS dataset [33] from the domain generalization literature, used here to deﬁne learning trajectories across different visual domains. Finally, we focus on semantic segmentation, ex-ploring learning sequences across different simulated urban environments and weather conditions. In all the aforemen-tioned experiments, we show the beneﬁts of the proposed approaches. To conclude our analysis, we show that our methods can further be improved by combining them with a small memory of samples from previous domains [6]. 2.