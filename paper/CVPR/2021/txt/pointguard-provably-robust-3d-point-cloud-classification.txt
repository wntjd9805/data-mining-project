Abstract 3D point cloud classiﬁcation has many safety-critical applications such as autonomous driving and robotic grasp-ing. However, several studies showed that it is vulnerable to adversarial attacks. In particular, an attacker can make a classiﬁer predict an incorrect label for a 3D point cloud via carefully modifying, adding, and/or deleting a small num-ber of its points. Randomized smoothing is state-of-the-art technique to build certiﬁably robust 2D image classiﬁers.
However, when applied to 3D point cloud classiﬁcation, randomized smoothing can only certify robustness against adversarially modiﬁed points.
In this work, we propose PointGuard, the ﬁrst defense that has provable robustness guarantees against adversar-ially modiﬁed, added, and/or deleted points. Speciﬁcally, given a 3D point cloud and an arbitrary point cloud classi-ﬁer, our PointGuard ﬁrst creates multiple subsampled point clouds, each of which contains a random subset of the points in the original point cloud; then our PointGuard pre-dicts the label of the original point cloud as the majority vote among the labels of the subsampled point clouds pre-dicted by the point cloud classiﬁer. Our ﬁrst major theoret-ical contribution is that we show PointGuard provably pre-dicts the same label for a 3D point cloud when the number of adversarially modiﬁed, added, and/or deleted points is bounded. Our second major theoretical contribution is that we prove the tightness of our derived bound when no as-sumptions on the point cloud classiﬁer are made. Moreover, we design an efﬁcient algorithm to compute our certiﬁed robustness guarantees. We also empirically evaluate Point-Guard on ModelNet40 and ScanNet benchmark datasets. 1.

Introduction 3D point cloud, which comprises a set of 3D points, is a crucial data structure in modelling a 3D shape or object.
In recent years, we have witnessed an increasing interest in 3D point cloud classiﬁcation [23, 17, 24, 30] because it
∗The ﬁrst two authors made equal contributions.
Modify 10 points
Prediction: radio
Add 10 points
Prediction: radio
Label: plane
Prediction: plane
Delete 274 points
Prediction: tent
Figure 1: Top: point modiﬁcation attack. Middle: point addition attack. Bottom: point deletion attack. Red points are added and yellow points are deleted. has many applications, such as robotic grasping [28], au-tonomous driving [2, 36], etc.. However, multiple recent studies [34, 31, 38, 39, 35, 20] showed that 3D point cloud classiﬁers are vulnerable to adversarial attacks. In particu-lar, given a 3D point cloud, an attacker can carefully mod-ify, add, and/or delete a small number of points such that a 3D point cloud classiﬁer predicts an incorrect label for it.
We can categorize these attacks into four types based on the capability of an attacker: point modiﬁcation, addition, dele-tion, and perturbation attacks. In particular, in a point modi-ﬁcation/addition/deletion attack [34, 31, 38, 35], an attacker can only modify/add/delete points in a 3D point cloud. An attacker, however, can apply one or more of the above three operations, i.e., modiﬁcation, addition, and deletion, to a 3D point cloud in a point perturbation attack. Figure 1 illus-trates the point modiﬁcation, addition, and deletion attacks.
These adversarial attacks pose severe security concerns to point cloud classiﬁcation in safety-critical applications.
Several empirical defenses [18, 40, 35, 6] have been pro-posed to mitigate the attacks. Roughly speaking, these de-fenses aim to detect the attacks or train more robust point cloud classiﬁers. For instance, Zhou et al. [40] proposed a defense called DUP-Net, whose key step is to detect outlier points and discard them before classifying a point cloud. 6186
These defenses, however, lack provable robustness guaran-tees and are often broken by more advanced attacks. For instance, Ma et al. [20] proposed a joint gradient based at-tack and showed that it can achieve high attack success rates even if DUP-Net [40] is deployed.
Therefore, it is urgent to study certiﬁed defenses that have provable robustness guarantees. We say a point cloud classiﬁer is provably robust if it certiﬁably predicts the same label for a point cloud when the number of modiﬁed, added, and/or deleted points is no larger than a threshold. Random-ized smoothing [4] is state-of-the-art technique for build-ing provably robust 2D image classiﬁers. For instance, via adding Gaussian noise to a 2D image, randomized smooth-ing provably predicts the same label for the image when the
ℓ2-norm of the adversarial perturbations added to the image is no larger than a threshold. Randomized smoothing can be applied to point cloud classiﬁcation. For instance, we can add Gaussian noise to each point of a point cloud and randomized smoothing can predict the same label for the point cloud when the adversarial modiﬁcation of its points is bounded. However, randomized smoothing requires the size of the input (e.g., the number of pixels in a 2D image or number of points in a 3D point cloud) remains unchanged under adversarial attacks. Therefore, randomized smooth-ing is only applicable to certify robustness against the point modiﬁcation attacks that do not change the size of a point cloud, leaving the other three types of attacks untouched.
Our work:
In this work, we propose PointGuard, the
ﬁrst defense that has provable robustness guarantees against point modiﬁcation, addition, deletion, and perturbation at-tacks. Suppose we are given a 3D point cloud and an arbi-trary point cloud classiﬁer. PointGuard ﬁrst creates a sub-sampled point cloud, which contains a random subset of k points subsampled from the original point cloud. Since the subsampled point cloud is random, its label predicted by the point cloud classiﬁer is also random. We use pi (called label probability) to denote the probability that the point cloud classiﬁer predicts label i for the random subsampled point cloud. Our PointGuard predicts the label that has the largest label probability for the original 3D point cloud.
Our major theoretical contributions are twofold. First, we show that, with any point cloud classiﬁer, our Point-Guard provably predicts the same label for a point cloud when the number of modiﬁed, added, and/or deleted points is no larger than a threshold. We call the threshold certi-ﬁed perturbation size. Note that the certiﬁed perturbation size may be different for different testing point clouds and point cloud classiﬁers. We derive the certiﬁed perturbation size via leveraging the Neyman-Pearson Lemma [21]. Sec-ond, we prove that, if no assumptions on the point cloud classiﬁer are made, our derived certiﬁed perturbation size is tight, i.e., it is impossible to derive a certiﬁed perturbation size larger than ours.
Our derived certiﬁed perturbation size for a point cloud is the solution to an optimization problem, which relies on the point cloud’s label probabilities. However, it is challenging to compute the exact label probabilities in practice since it requires predicting the labels for an exponential number of subsampled point clouds. In particular, computing the exact label probabilities for a point cloud requires predicting the labels for (cid:0)n k(cid:1) subsampled point clouds if the point cloud contains n points. To address the challenge, we develop a Monte-Carlo algorithm to estimate the lower and upper bounds of the label probabilities with probabilistic guaran-tees via predicting labels for N << (cid:0)n k(cid:1) subsampled point clouds. Given the estimated label probability bounds, we solve the optimization problem to obtain the certiﬁed per-turbation size.
We empirically evaluate PointGuard on ModelNet40 and
ScanNet. To demonstrate the generality of PointGuard, we consider two point cloud classiﬁers, i.e., PointNet [23] and
DGCNN [30]. We adopt certiﬁed accuracy as our eval-uation metric.
In particular, the certiﬁed accuracy at r perturbed points is the fraction of the testing point clouds whose labels are correctly predicted and whose certiﬁed perturbation sizes are no smaller than r. Since the certiﬁed accuracy of a standard point cloud classiﬁer is unknown, we measure a standard point cloud classiﬁer using its empirical accuracy under an empirical attack, i.e., we use an empir-ical attack to perturb the testing point clouds and use the point cloud classiﬁer to classify them. Our experimental re-sults show that the certiﬁed accuracy of PointGuard is sub-stantially higher than the empirical accuracy of a standard point cloud classiﬁer in many cases. For instance, on Mod-elNet40, PointNet achieves 0% empirical accuracy while
PointGuard with k = 16 achieves 69.7% certiﬁed accuracy when an attacker can arbitrarily modify 30 points of each testing point cloud. We also compare PointGuard with ran-domized smoothing for point modiﬁcation attacks, as ran-domized smoothing is only applicable to such attacks. Our results show that PointGuard substantially outperforms ran-domized smoothing, e.g., randomized smoothing achieves 0% certiﬁed accuracy under the above setting.
In summary, our key contributions are as follows:
We propose PointGuard, the ﬁrst 3D point cloud classi-ﬁcation system that is provably robust against different types of adversarial attacks.
We derive the certiﬁed robustness guarantee of Point-Guard and prove its tightness. Moreover, we design an algorithm to efﬁciently compute our certiﬁed robust-ness guarantee.
We evaluate our PointGuard on two datasets.
•
•
• 6187
2.