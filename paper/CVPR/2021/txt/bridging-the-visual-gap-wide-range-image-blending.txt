Abstract 1.

Introduction
In this paper we propose a new problem scenario in im-age processing, wide-range image blending, which aims to smoothly merge two different input photos into a panorama by generating novel image content for the intermediate re-gion between them. Although such problem is closely re-lated to the topics of image inpainting, image outpainting, and image blending, none of the approaches from these top-ics is able to easily address it. We introduce an effective deep-learning model to realize wide-range image blending, where a novel Bidirectional Content Transfer module is pro-posed to perform the conditional prediction for the feature representation of the intermediate region via recurrent neu-In addition to ensuring the spatial and se-ral networks. mantic consistency during the blending, we also adopt the contextual attention mechanism as well as the adversarial learning scheme in our proposed method for improving the visual quality of the resultant panorama. We experimen-tally demonstrate that our proposed method is not only able to produce visually appealing results for wide-range im-age blending, but also able to provide superior performance with respect to several baselines built upon the state-of-the-art image inpainting and outpainting approaches.
Digital image processing, which carries out computer-based processing and manipulation on image data, has been playing an important role in our daily life, such as image inpainting for image restoration or object removal, image blending for image composition, and image outpainting (i.e. extrapolation) for digital content generation. In this paper, we propose a novel task of image processing: wide-range image blending, in which it aims to smoothly merge two different images into a panorama by generating novel image content for the intermediate region between them, as shown in Figure 1. Such technique can contribute to bringing in more interesting ways for the content generation and image composition. For example, we could easily create a full panoramic image based on the photos taken by the front and rear cameras of a cellphone via applying wide-range image blending on them with two opposite spatial arrangements (i.e. one is putting the front photo on the left and the rear photo on the right, while the other one is opposite).
The main challenge of wide-range image blending lies in the requirement that the generated content for the inter-mediate region should be not only visually realistic but also semantically reasonable to achieve seamless transition from 843
one input photo to another. Although there exists no ap-proach for addressing wide-range image blending, such task is closely related to several topics of image processing. For instance, the extrapolation from input photos beyond their boundary towards the intermediate region ﬁts exactly the scenario of image outpainting; by contrast, if we treat input photos as the given context and the intermediate region is what to be ﬁlled, the task of image inpainting appears.
However, no prior works of these topics is able to easily resolve wide-range image blending. For instance, although previous works for image inpainting [8, 9, 12, 16, 20, 21, 22, 23] are able to learn semantics from context and gen-erate coherent structure for the missing region, they how-ever could create artifacts and blurry textures as the size of the missing region increases. Especially, if the content of two input photos is quite different, the inpainting ap-proaches are also likely to have hard time on generating satisfactory results with smooth transition across input pho-tos. On the other hand, even if we can apply the existing image outpainting model (e.g. [3, 15, 17, 19]) respectively on the two input photos for generating the image content of the intermediate region, there is no guarantee to have seam-less composition between those two extrapolation results.
Later in this paper, we will provide experimental evidence to demonstrate that directly adopting inpainting or outpaint-ing methods without any modiﬁcation leads to poor results under the problem scenario of wide-range image blending.
We propose a novel deep-learning-based model to per-form wide-range image blending with all the aforemen-tioned challenges/issues being well addressed. The archi-tecture of our proposed model stems from the U-Net [13] framework where the encoder takes two photos as input and the decoder outputs the resultant image of blending. Partic-ularly, in the bottleneck of such U-Net-alike framework, we introduce a Bidirectional Content Transfer module for pre-dicting the image content of the intermediate region, which is encouraged to ensure the continuity of the spatial con-ﬁguration between the intermediate region and two input photos. Moreover, for making better use of the rich texture information from input photos and generating more delicate blending results, we propose to integrate the contextual at-tention mechanism [21] on the skip connection between the encoder and the decoder. Last but not least, we adopt ad-versarial learning [2] for improving the realness of the in-termediate region, even when the two input photos are from signiﬁcantly different scenes. It is also worth noting that our model learning does not require any supervision in the training data therefore being unsupervised. We conduct ex-tensive ablation study to verify the contribution of our de-sign choices, as well as provide both qualitative and quan-titative comparisons with respect to several inpainting and outpainting baselines for demonstrating the efﬁcacy of our proposed method in the task of wide-range image blending. 2.