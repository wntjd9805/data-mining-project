Abstract
Recent breakthroughs of Neural Architecture Search (NAS) extend the ﬁeld’s research scope towards a broader range of vision tasks and more diversiﬁed search spaces.
While existing NAS methods mostly design architectures on a single task, algorithms that look beyond single-task search are surging to pursue a more efﬁcient and universal solution across various tasks. Many of them leverage transfer learn-ing and seek to preserve, reuse, and reﬁne network design knowledge to achieve higher efﬁciency in future tasks. How-ever, the enormous computational cost and experiment com-plexity of cross-task NAS are imposing barriers for valu-able research in this direction. Existing NAS benchmarks all focus on one type of vision task, i.e., classiﬁcation. In this work, we propose TransNAS-Bench-101, a benchmark dataset containing network performance across seven tasks, covering classiﬁcation, regression, pixel-level prediction, and self-supervised tasks. This diversity provides oppor-tunities to transfer NAS methods among tasks and allows for more complex transfer schemes to evolve. We explore two fundamentally different types of search space: cell-level search space and macro-level search space. With 7,352 backbones evaluated on seven tasks, 51,464 trained mod-els with detailed training information are provided. With
TransNAS-Bench-101, we hope to encourage the advent of exceptional NAS algorithms that raise cross-task search ef-ﬁciency and generalizability to the next level. Our dataset and code will be available at Mindspore1 and VEGA2. 1.

Introduction
In recent years, networks found by Neural Architec-ture Search (NAS) methods are surpassing human-designed ones, setting state-of-the-art performance on various tasks
*Equal contribution. ({kmdaniel, cyn0531}@connect.hku.hk)
†Corresponding author. (xdliang328@gmail.com) 1https://download.mindspore.cn/dataset/TransNAS-Bench-101 2https://www.noahlab.com.hk/opensource/vega/page/doc.html? path=datasets/transnasbench101
[25, 21]. Ultimately, NAS calls for algorithmic solutions that can discover near-optimal models for any task within any search space under moderate computational budgets.
To pursue this goal, recent research in NAS quickly ex-panded its scope into broader vision domains such as object detection [27] and semantic segmentation [5]. [24] seeks to bridge the gap between sample-based and one-shot ap-proaches. Besides searching for an optimal cell design in earlier works [18], many recent works also investigate the macro skeleton search of a network [29, 27].
Although many algorithms successfully shortened the search time of NAS from months to hours [18, 9], some re-search has shown their reliance on speciﬁc search spaces and datasets [28]. There are also questions on these al-gorithms’ efﬁciency when dealing with a large number of tasks [6]. A rising direction of NAS research is thus look-ing for solutions that acquire transferable knowledge and generalize well across multiple tasks and search spaces
[20, 26, 23, 14, 10].
[6] explores meta-learning to trans-fer network design knowledge from small tasks to larger tasks, surpassing many efﬁcient solutions based on pa-rameter sharing.
[3] proposes a highly memory-efﬁcient and effective transfer solution that does not require back-propagation for adaptation. Central to these works’ inves-tigations are two key considerations: the transferability of an algorithm, namely how much information an algorithm can effectively reuse, and generalizability, which evaluates whether a solution can be applied to different settings (e.g., search spaces) and still performs well.
Meanwhile, NAS is prohibitively costly. The computa-tional intensity of single-task NAS can be discouraging, not to mention cross-task NAS experiments. To solve this com-putation limitation, NAS-Bench-101 [30] and NAS-Bench-201 [8] were proposed. These benchmarks have offered great values for the NAS community, but we believe the research scope of NAS can be further enlarged beyond clas-siﬁcation problems and cell-based search spaces. To avoid confusion, throughout this paper we use dataset to refer to a set of original images, and task for pointing to certain vi-sion domain. Hence, there can be multiple tasks for one 15251
Transfer-NAS Search Spaces
Various Tasks
Macro-level Search Space
Image
Searched
Backbone
Task-specific
Decoder
Cell-level Search Space
Cell
OP2
OP1
OP6
OP3
OP5
OP4
Object Classification
Scene Classification
Room Layout
Jigsaw
AutoEncoder
Surface Normal
Semantic Segmentation
Operation (OP)
Candidates
•
Zeroize
•
Skip-connection
• 1x1 conv 
• 3x3 conv
Stage 1
Stage 2
Stage 3
Module 1
Module 2
Module 3
Module 4 k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R k c o
B l l a u d i s e
R
Downsampling
@ 2nd Module
Doubling Channels
@ 3rd Module
@ 4th Module
@ 4th Module
Figure 1: Our cell-level and macro-level search space in TransNAS-Bench-101. We design a cell-level search space that treats each cell as a DAG and a macro-level search space that allows ﬂexible macro skeleton network design. dataset (e.g., COCO detection and segmentation).
The goal of ﬁnding universal solutions across tasks and search spaces, the comparability problem, and the compu-tational barriers of transferable NAS research lead to our proposal of TransNAS-Bench-101, which studies networks over seven distinct vision tasks: object classiﬁcation, scene classiﬁcation, semantic segmentation, autoencoding, room layout, surface normal, and jigsaw. Two types of search spaces are provided: one is the macro skeleton search space based on residual blocks, where the network depth and block operations (e.g., where to raise the channel or down-sample the resolution) are decided. Another one is the widely-studied cell-based search space, where each cell can be treated as a directed acyclic graph (DAG). The macro-level and cell-level search space contains 3,256 and 4,096 networks, respectively. The 7,352 backbones are evalu-ated on all seven tasks, and we provide detailed diagnos-tic information such as task performance, inference time, and FLOPs for all models. We also evaluated four trans-fer schemes compatible with both search spaces to serve as benchmarks for future research.
Our key contribution is a benchmark dataset with net-works fully evaluated on seven tasks across two search spaces. Generating the benchmark takes 193,760 GPU hours, i.e., 22.12 years of computation on one NVIDIA
V100 GPU, but it signiﬁcantly lowers the cost of further research into cross-task neural architecture search. We also highlight problems and provide suggestions for future NAS research: (1) To extend NAS into different vision domains, it is important to look beyond cell-based search spaces, as we found that network macro structures can have bigger im-(2) The extent to pact on performance on certain tasks. which an algorithm surpasses random search is a crucial performance indicator. (3) Investigations of evolutionary-based transfer strategies, along with effective mechanisms to tweak transferred architectures and policies, are two promising directions for future research. With diversiﬁed settings in TransNAS-Bench-101, we hope to encourage the emergence of exceptional transferable NAS algorithms that generalize well under various settings. 2. The TransNAS-Bench-101 dataset 2.1. Search Spaces and Architectures
To plug in different networks for various tasks, our search space focuses on evolving the backbone, i.e., the mutual component of all the tasks considered. We provide two search spaces: a) A macro-level search space that de-signs the macro skeleton of a network, which was previ-ously studied towards NAS in object detection and seman-tic segmentation; b) A cell-level search space following the widely studied cell-based search space, which applies to most weight-sharing NAS methods.
Macro-level Search Space. Most NAS methods follow a ﬁxed macro skeleton with a searched cell. However, the macro-level structure of the backbone can be crucial for the
ﬁnal performance. Early-stage feature maps in a backbone have larger sizes as they capture texture details, whereas feature maps at later stages are smaller and usually are more discriminative [13]. The allocation of computations over different stages is also vital for a backbone [15]. Therefore, our search space contains networks with different depth (the total number of blocks), locations to down-sample feature maps, and locations to raise the channels. As is illustrated in Figure 1, we group two residual blocks [11] into a mod-ule, and the networks are stacked with 4 to 6 modules. The module positions can be chosen to downsample the feature map 1 to 4 times, and each time the spatial size will shrink by a factor of 2. The network can double its channels 1 to 25252                
Raw Image
Object 
Classification
• Home theater
• Entertainment  center
• Day bed
• Sofa
Scene 
Classification
• Living Room
• Television 
Room
Room Layout
Jigsaw Puzzle
Autoencoding
Surface Normal Semantic Segm.
Figure 2: Vision tasks considered in our benchmarks. We carefully select those 7 tasks above to ensure both diversity and similarity across tasks from Taskonomy [31]. 3 times at chosen locations. This search space thus consists of 3,256 unique architectures.
Cell-level Search Space. We follow NAS-Bench-201[8] to design our cell-level search space. It consists of 4,096 densely connected DAGs, which enables the evaluation of some weight-sharing NAS methods such as DARTS [18] and ProxylessNAS [4]. As is shown in Figure 1, our cell-level search space is obtained by assigning different op-erations (as edges) transforming the feature map from the source node to the target node. The predeﬁned operation set has L=4 representative operations: zeroize, skip connection, 1x1 convolution, and 3x3 convolution. The convolution in our setting represents an operation sequence of ReLU, con-volution, and batch normalization. Each DAG consists of 4 nodes and 6 edges, including basic residual block-like cell designs. The macro-level skeleton is ﬁxed, which contains
ﬁve modules with doubling channel and down-sampling feature map operations at the 1st, 3rd, 5th modules.
Adding up the 3,256 and 4,096 networks from the macro-level search space and the cell-level search space, we have 7,352 unique architectures in total. All the architec-tures in both search spaces are carefully trained and evalu-ated across all the selected tasks. 2.2. Dataset
Unlike most NAS benchmarks that focus on classiﬁca-tion tasks only, TransNAS-Bench-101 encourages the eval-uation of algorithms across different tasks. Considered that
Transferable NAS research is still in its infancy, we hope to provide as many common grounds for transfer as possible.
This makes selecting proper datasets challenging since, ide-ally, the datasets should share some commonalities while covering a diversity of tasks. Thanks to the great previ-ous work Taskonomy [31], which provides sufﬁcient im-ages with labels on different tasks (see Figure 2), we can study algorithms without worrying that the datasets are by nature too distinct for transfer. The original dataset consists of 4.5M images of indoor scenes from about 600 buildings.
To control the computational budget, we randomly select 24 buildings containing a total of 120K images from the orig-inal dataset and split the subset into 80K train / 20K val / 20K test set. 2.3. Vision Tasks
We carefully selected seven tasks that lie on the inter-section of (1) covering all major task categories in Taskon-omy’s task similarity tree [31], and (2) align with the community’s general research interests (e.g., classiﬁcation tasks, or common and cheap pretrain tasks). As is shown in Figure 2, the selected tasks include a) image classiﬁca-tion tasks: Object Classiﬁcation and Scene Classiﬁcation; b) pixel-level prediction tasks: Surface Normal and Seman-tic Segmentation; c) self-supervised task: Jigsaw Puzzle and Autoencoding; d) point regression task: Room Layout.
Object Classiﬁcation. Object classiﬁcation is a 75-way classiﬁcation problem that recognize objects. Labels pro-Table 1: Training hyperparameters and details of each task considered in this benchmark. All the architectures in the search space have been fully trained. We provide multiple metrics for evaluation on the train/valid/test set. Each task requires a backbone-decoder network structure with task-speciﬁc decoder and loss function. GAP denotes global average pooling. CE denotes the cross entropy loss. All the tasks except Autoencoding and Surface Normal use a cosine annealing with a linear warmup learning rate scheduler.
Tasks
Decoder
LR
Optimizer
Output dim.
Loss
Eval. Metrics
Object Class.
Scene Class.
Room Layout
Jigsaw
Autoencoding
Surface Normal
Sem. Segment.
GAP + Linear
GAP + Linear
GAP + Linear
GAP + Linear 14 Conv & Deconv 14 Conv & Deconv 8 Conv & Deconv 0.1 0.1 0.1 0.1 0.0005 0.0001 0.1
SGD
SGD
SGD
SGD
Adam
Adam
SGD 35253 75 47 9 1000 256x256 256x256 256x256
Softmax+CE
Softmax+CE
MSE loss
Softmax+CE
GAN loss+L1
GAN loss+L1
Softmax+CE
Loss, Acc
Loss, Acc
Loss
Loss, Acc
Loss, SSIM
Loss, L1, SSIM
Loss, Acc, mIoU
Table 2: Comparisons of TransNAS-Bench-101 with previous benchmarks. Although TransNAS-Bench-101 has a smaller search space, it contains more datasets, domains, and search space types.
NAS-Bench-101
NAS-Bench-201
TransNAS-Bench-101
# Data-sets 1 3 1
# Task
Domains 1 1 7
# Search Space
Size 510M 15.6K 7.3K
Search Space
Type
Cell
Cell
Cell & Macro vided by Taskonomy dataset [31] are activations generated by a ResNet-152 model [11] pre-trained on ImageNet [7].
Scene Classiﬁcation. Like object classiﬁcation, scene classiﬁcation is a 47-way classiﬁcation problem that pre-dicts the room type in the image. Its labels come from a
ResNet-152 model pre-trained on MIT Places dataset [32].
Our selected dataset contains 47 classes out of the original 365 classes.
Room Layout.
This task is to estimate and align a 3D bounding box deﬁning the room layout. It requires a network to semantic information, such as ”what constitutes a room,” and includes scene geometry.
Jigsaw. We follow [19] to design the self-supervised task Jigsaw. The input image is divided into nine patches and shufﬂed according to one of 1,000 preset permutations.
The objective is to classify which permutation is used. This task’s inclusion is inspired by a recent work [17] that ex-plores neural architecture search without using labels.
Autoencoding. Autoencoding is a pixel-level prediction task that encodes images into low-dimensional latent repre-sentations then reconstructs the raw image. The training set-tings follow Conditional Adversarial Nets in Pix2Pix [12].
Surface Normal. Like autoencoding, surface normal is a pixel-wise prediction task that predicts surface normal statistics. The network structure and training procedure is the same as autoencoding.
Semantic Segmentation. Semantic Segmentation con-ducts pixel-level prediction on the class of its components.
The labels provided by the Taskonomy dataset are gener-ated through a network pre-trained on the MSCOCO [16] dataset. Our selected subset contains 17 semantic classes. 2.4. Training Details
In TransNAS-Bench-101, the seven different tasks re-quire different network structures and loss functions. To train the networks on a given task, we deﬁne a de-fault backbone-decoder network structure ﬁrst, then iterate through the search space and change its backbone architec-ture. For pixel-level prediction tasks and autoencoding, the decoders’ input channels and resolutions will change ﬂexi-bly but minimally according to different feature maps gen-erated by the backbone. Since the original paper’s imple-mentation is based on an early version of Tensorﬂow, we reimplemented both the training and testing script with Py-Torch for reproducibility.
We mostly follow the Taskonomy paper to set up the hyper-parameters and training strategies shown in Table 1.
For all the tasks, the batch size is 128, and the input resolu-tion is resized to 256 × 256. We record multiple evaluation metrics in each epoch for all the architectures, as is listed in Table 1. Since we train every architecture in our search space for all the 7 tasks (i.e., 7352 × 7 ≈ 5 × 104 arch), the total computation cost is 193,760 GPU hours on V100 to generate the whole TransNAS-Bench-101. Users can use our API to conveniently query each architecture’s informa-tion across tasks without additional computation costs. In
Table 3: We show the basic statistics of the two search spaces, including average performance metric scores, average model FLOPs, the average number of model parameters, and average training time. Note that here model denotes the whole backbone-decoder model structure.
Cls. Object Cls. Scene Autoencoding Surf. Normal Sem. Segment. Room Layout
Jigsaw
Tasks
Metric
Acc.
Acc.
Performance
FLOPs (×108) 39.71±5.92 45.23±12.15 2.44±1.44 2.44±1.44
Cell level Model Params (×106) 1.17±0.71 1.17±0.71 2.08±0.23 2.13±0.23 44.24±1.38 52.92±2.08 6.49±9.68 6.49±9.68
Macro level Model Params (×106) 1.18±0.91 1.18±0.91 2.56±2.25 2.57±2.35
Performance
FLOPs (×108)
Train Time (hr)
Train Time (hr) mIoU
SSIM
SSIM 19.95±5.52 0.52±0.06 0.46±0.10 5.19±1.44 4.90±1.44 4.90±1.44 2.27±0.71 3.97±0.71 3.97±0.71 5.98±1.69 4.90±0.55 5.03±0.54 0.52±0.08 24.47±2.07 0.57±0.02 12.44±14.40 12.44±14.40 10.66±11.56 2.13±1.02 3.83±1.02 3.83±1.02 7.04±2.99 7.10±4.02 7.07±3.91
L2 loss
Acc.
-0.73±0.12 76.57±28.34 1.38±0.81 2.44±1.44 2.19±0.71 1.17±0.71 1.28±0.19 2.01±0.31 93.43±2.18
-0.65±0.03 3.66±5.45 6.49±9.68 1.91±1.14 1.17±0.91 1.13±0.56 2.59±2.25 45254
(a) Macro: FLOPs v. Rank (b) Macro: Params v. Rank (c) Cell: FLOPs v. Rank (d) Cell: Params v. Rank
Figure 3: The architecture average performance ranking, FLOPs, parameters, and training time on both search spaces. (a)-(b) display the overall landscape of the macro-level search space and (c)-(d) show the cell-level search space. this way, researchers can signiﬁcantly speed up their re-search and focus solely on improving the algorithms with-out tediously implementing and tuning different tasks. 2.5. Network Information in TransNAS Bench API
We provide the train / validation / test performance infor-mation of each network at every epoch. The benchmark also contains each network’s inference time, FLOPs, the total number of parameters, and time elapsed during each train-ing epoch. Each model’s inference time is measured on one
Tesla V100 with one image of shape (3, 720, 1080). FLOPs are computed with one image of shape (3, 224, 224). 3.