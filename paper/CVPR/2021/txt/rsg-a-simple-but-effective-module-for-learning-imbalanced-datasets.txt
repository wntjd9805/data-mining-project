Abstract
Imbalanced datasets widely exist in practice and are a great challenge for training deep neural models with a good generalization on infrequent classes. In this work, we propose a new rare-class sample generator (RSG) to solve this problem. RSG aims to generate some new samples for rare classes during training, and it has in particular the following advantages: (1) it is convenient to use and highly versatile, because it can be easily integrated into any kind of convolutional neural network, and it works well when combined with different loss functions, and (2) it is only used during the training phase, and therefore, no ad-ditional burden is imposed on deep neural networks during the testing phase. In extensive experimental evaluations, we verify the effectiveness of RSG. Furthermore, by leveraging
RSG, we obtain competitive results on Imbalanced CIFAR and new state-of-the-art results on Places-LT, ImageNet-LT, and iNaturalist 2018. The source code is available at https://github.com/Jianf-Wang/RSG. 1.

Introduction
Computer vision research has made great progress in the past few years, driven by the development of deep convo-lutional neural networks (CNNs) [20, 27, 11, 15, 35, 33, 32, 5, 12, 28] as well as large-scale datasets of high qual-ity [7, 22]. However, these large-scale datasets are usually well-designed, and the number of instances in each class is balanced artiﬁcially, which is inconsistent with the data distributions in real-world scenaries. It is common that the images of some categories are difﬁcult to be collected, re-sulting in a dataset with an imbalanced data distribution. In general, imbalanced datasets can be classiﬁed into two cate-gories in terms of data distributions: long-tailed imbalanced distributions [6] and step imbalanced distributions [2], which will both be the focus of this work.
Generating new samples for rare classes during training is a good solution [8, 36, 38], which is regarded as a data
*Corresponding author.
Figure 1: RSG in a simple CNN. The part in the dotted box is only used during training. RSG learns to generate new rare-class samples, which are used to reshape the decision boundary and enlarge the feature space of rare classes. augmentation method. However, these methods have dif-ferent drawbacks, which limit their performance. Firstly, some frameworks [8, 38] were not trained in an end-to-end manner, so that the gradients cannot be backpropagated from the top to the bottom of CNNs. But it is well known that deep models can usually beneﬁt from end-to-end training.
Secondly, some methods [8, 38] utilized variation informa-tion, such as different poses or lighting, among samples from the same frequent class to generate new rare-class samples.
However, these methods did not introduce any mechanism to ensure that the variation information obtained from frequent classes is class-irrelevant. As a result, if the variation infor-mation (which still contains the class-relevant information) is directly combined with real rare-class samples to generate new rare-class ones for training the classiﬁer and reshaping decision boundaries, the performance will be hurt due to the aliasing of different class-relevant information. Finally,
Wang et al. [36] use noise vectors to encode the variation information mentioned above. But using such noise vectors for generation can possibly generate unstable or low-quality samples, since noise vectors are too random to reﬂect the true variations among real images.1 1Note that [38] has proposed to avoid sampling random vectors due to their randomness, and [8] also has conducted experiments and veriﬁed that using random vectors to generate new samples for training classiﬁers can degrade the performance. 3784
To alleviate the above drawbacks, in this paper, we pro-pose a simple but efﬁcient fully parameterized generator, called rare-class sample generator (RSG), which can be trained end-to-end with any backbone. RSG directly uses the variation information, which usually reﬂects different poses or lighting, among the real samples from the same frequent class to generate new samples rather than using random vec-tors to encode such information, and therefore, RSG can generate more reasonable and stable samples. Besides, RSG introduces a new module that is designed to further ﬁlter out the frequent-class-relevant information that possibly exists in the variation information, solving the aliasing problem mentioned above.
Figure 1 shows how it is integrated into a simple CNN for imbalanced datasets. RSG only requires the feature maps of samples from any speciﬁc layer, and it generates some new samples during training to impact on rare classes in order to adjust their decision boundaries and enlarging their feature space. In the testing phase, RSG is removed, so that no additional computational burden is imposed on the network.
Note that we only show a simple CNN in Fig. 1, but RSG can be used in any network architecture, such as ResNet
[11], DenseNet [15], ResNeXt [35], and Inception [27]. 2.