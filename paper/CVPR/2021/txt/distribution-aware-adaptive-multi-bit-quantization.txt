Abstract
In this paper, we explore the compression of deep neu-ral networks by quantizing the weights and activations into multi-bit binary networks (MBNs). A distribution-aware multi-bit quantization (DMBQ) method that incorporates the distribution prior into the optimization of quantization is proposed.
Instead of solving the optimization in each iteration, DMBQ search the optimal quantization scheme over the distribution space beforehand, and select the quan-tization scheme during training using a fast lookup table based strategy. Based upon DMBQ, we further propose loss-guided bit-width allocation (LBA) to adaptively quan-tize and even prune the neural network. The ﬁrst-order Tay-lor expansion is applied to build a metric for evaluating the loss sensitivity of the quantization of each channel, and au-tomatically adjust the bit-width of weights and activations channel-wisely. We extend our method to image classiﬁca-tion tasks and experimental results show that our method not only outperforms state-of-the-art quantized networks in terms of accuracy but also is more efﬁcient in terms of train-ing time compared with state-of-the-art MBNs, even for the extremely low bit width (below 1-bit) quantization cases. 1.

Introduction
In the past decades, deep neural networks have achieved great successes in many ﬁelds [15, 21, 37, 7]. However, the requirements of huge memory footprint and computa-tional resources impede the practical deployment of net-work based algorithms on resource-constraint devices, e.g., mobile phones, smart dresses and autopilot vehicles.
To facilitate deployability, network quantization meth-ods quantize the weights and activations into low precision, greatly compressing the model size and reducing the re-quired computational resources in inference [19, 39, 13, 5, 3]. These network quantization methods need to solve two main problems: (1) How to quantize the weights and activa-tions with lower precision and higher accuracy? (2) How to allocate the bit-width to quantize different parts of weights and activations for optimal performance?
As for the former problem, three main types of quantiza-tion schemes are proposed, including ﬁxed-point [19, 13], power of two [24, 38, 18] and binary/ternary [6, 16, 40, 28, 20, 26] quantization. Recently, the multi-bit quantization (MBQ) [20] that quantize weights and activations into the combination of multiple binary bases is proposed, which could achieve better performance and require less computa-tional resources. Based upon MBQ strategy, various meth-ods [20, 36, 32, 27] have been proposed to optimize the quantization problem in the training process to improve the accuracy of MBQ and have made signiﬁcant improvements.
However, how to derive the optimal quantization schemes, i.e., the multi-bit binary ﬁlters and the corresponding co-ordinates, efﬁciently during the iterations of training-aware quantization remains a knotty problem [27], suffering from the difﬁculty of the integer optimization (theoretically NP-hard). In this paper, inspired by Banner et al. [1] that the distribution can be utilized in the quantization, we explore the distribution of weights and dedicate to ﬁnd the opti-mal quantization scheme of MBQ under the distribution as-sumption. By minimizing the expected mean square error
, a lookup table based strategy is proposed to optimize the quantization schemes with very few computational cost dur-ing the iterations.
As for the latter one, except the globally uniﬁed bit-width allocation, mixed precision quantization which allo-cates different bit-width to different parts of the network, either layer-wise [9, 8, 10, 31] or channel-wise [22, 17, 5], is proposed to reduce the degradation of quantization, es-pecially for low-bit cases. However, the bit-width alloca-tion is quite challenging. Existing methods, e.g., deep rein-forcement learning [22, 10, 31], Hessian information [9, 8], and pruning-based optimization [27], require considerable computation cost, hindering their application in practice. In this paper, through modeling the quantization effect upon the loss of network with Taylor expansion, we formulate a metric to evaluate the quantization sensitivity of weights, i.e., the loss variation of the network with the quantization of weights. Since only gradients of quantized weights are required, which could be directly obtained from the back-19281
ward propagation, the metric can be easily computed, and thus we can adaptively adjust the quantization bit-width of weights and activations in the training process without too much computational load.
In all, we make the following contributions:
• We introduce a distribution-aware multi-bit quantiza-tion (DMBQ) method for efﬁcient and optimal MBQ quantization.
• We propose a ﬁrst-order Taylor expansion based met-ric for evaluating the loss-sensitivity of the quantized weights and activations and introduce a loss-guided bit-width allocation (LBA) method.
• We demonstrate the effectiveness and efﬁciency of the proposed method through extensive comparisons with the state of the art methods. 2.