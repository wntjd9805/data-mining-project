Abstract
Input
PONO mean
PONO std
The moments (a.k.a., mean and standard deviation) of latent features are often removed as noise when training image recognition models, to increase stability and reduce training time. However, in the ﬁeld of image generation, the moments play a much more central role. Studies have shown that the moments extracted from instance normal-ization and positional normalization can roughly capture style and shape information of an image. Instead of being discarded, these moments are instrumental to the genera-tion process. In this paper we propose Moment Exchange, an implicit data augmentation method that encourages the model to utilize the moment information also for recogni-tion models. Speciﬁcally, we replace the moments of the learned features of one training image by those of another, and also interpolate the target labels—forcing the model to extract training signal from the moments in addition to the normalized features. As our approach is fast, operates en-tirely in feature space, and mixes different signals than prior methods, one can effectively combine it with existing aug-mentation approaches. We demonstrate its efﬁcacy across several recognition benchmark data sets where it improves the generalization capability of highly competitive baseline networks with remarkable consistency. 1.

Introduction
Image recognition and image generation are two cor-ner stones of computer vision. While both are burgeon-ing ﬁelds, specialized techniques from both sub-areas can sometimes form a dichotomy. Examples are mixup [69] and squeeze-and-excitation [19] from the former, and adap-tive instance normalization [22] from the latter, although exceptions exist. Historically, the ﬁeld of deep learning was widely popularized in discriminative image classiﬁ-cation with AlexNet [32], and image generation through
GANs [12] and VAEs [30].
One particular aspect of this dichotomy is that in image i n b o
R i n i r o t n a
S
Figure 1. PONO mean and std captures structural information. recognition, popularized by batch normalization [24], the
ﬁrst and second moments (a.k.a., mean and standard devia-tion) in image recognition are computed across instances in a mini-batch and typically removed as noise [24, 60]. Stud-ies have shown that this smoothes the optimization land-scape [46] and enables larger learning rates [2], which leads to faster convergence in practice.
In contrast, for tech-niques like instance normalization [55] and positional nor-malization [34], moments play a central role in the im-age generation process. For example, exchanging mo-ments of latent features across samples has become a pop-ular way to control for the style or shape of generated im-ages [22, 23, 27, 34, 41]. Here, moments are viewed as fea-tures, not noise, with research showing that they encode the style of an image [22, 27], as well as the underlying struc-ture [34]. To substantiate this point, we depict the ﬁrst and second moments of the features extracted in the ﬁrst layer of a ResNet [16] in Fig. 1 , using the technique described in
[34]. The class label can still be inferred visually from both moments, which is a testament to the signal that remains in these statistics. To further substantiate our observation, we also show in Fig. 2 that simply using moments (from the
ﬁrst ResNet layer) for image classiﬁcation already yields non-trivial performance (red bar) as compared to random guessing (gray bar). Similarly, removing the moments from positional normalization has a detrimental effect (blue bar vs. green bar).
As there is evidently important signal in the moments and in the normalized features, we would like to introduce a
∗: Equal contribution.
Example images are from Shutterstock. 12383
way to regulate how much attention the deep net should pay to each source. One approach to direct neural networks to a particular signal source is to introduce dedicated feature augmentation. For example, it has been shown that Con-vNets trained on ImageNet [6] are biased towards textures instead of shapes [10]. To overcome this, Geirhos et al. [10] introduce a style transfer model to create a set of images with unreal textures. For example, they generate cats with elephant skins and bears with Coca-Cola bottle texture. An image classiﬁer is trained to recognize the shape (cats or bears) instead of the textures (elephants or bottles).
In this paper we propose a novel data augmentation scheme that, to our knowledge, is the ﬁrst method to sys-tematically regulate how much attention a network pays to the signal in the feature moments. Concretely, we extract the mean and variance (across channels) after the ﬁrst layer, but instead of simply removing them, we swap them be-tween images. See Fig. 3 for a schematic illustration, where we extract and remove the feature moments of a cat image, and inject the moments of a plane. Knowing that the result-ing features now contain information about both images, we make the network predict an interpolation of the two labels.
In the process, we force the network to pay attention to two aspects of the data: the normalized feature (from the cat) and the moments (from the plane). By basing its prediction on two different signals we increase the robustness of the classiﬁcation, as during testing both would point towards the same label. We call our method Moment Exchange or
MoEx for short.
Through exchanging the moments, we swap the shape (or style) information of two images, which can be viewed as an implicit version of the aforementioned method pro-posed by Geirhos et al. [10]. However, MoEx does not re-quire a pre-trained style transfer model to create a dataset explicitly. In fact, MoEx is very effective for training with mini-batches and can be implemented in a few lines of code:
During training we compute the feature mean and variance for each instance at a given layer (acros channels), permute them across the mini-batch, and re-inject them into the fea-ture representation of other instances (while interpolating the labels).
MoEx operates purely in feature space and can therefore easily be applied jointly with existing data augmentation methods that operate in the input space, such as cropping,
ﬂipping, rotating, but even label-perturbing approaches like
Mixup [69] or Cutmix [67].
Importantly, because MoEx only alters the ﬁrst and second moments of the pixel dis-tributions, it has an orthogonal effect to existing data aug-mentation methods and its improvements can be “stacked” on top of their established gains in generalization. We conduct extensive experiments on eleven different tasks/-datasets using more than ten varieties of models. The re-sults show that MoEx consistently leads to signiﬁcant im-Figure 2. Error rates of ResNet-110 using different features on
CIFAR-100. The numbers are averaged over three random runs.
We compute the moments after the ﬁrst convolutional layer, and either adds a PONO layer right after the ﬁrst Conv-BN-ReLU block or computes the ﬁrst two moments and concatenate them as a two-channel feature map. A ResNet which can only see the moments can still make nontrivial predictions (Red is much bet-ter than gray). Additionally, using only the normalized feature (i.e., removing the PONO moments) hurts the performance (Blue is worse than green), which also shows that these moments con-tains important information. Finally, MoEx improves the perfor-mance by encouraging the model to use both sources of signal during training. provements across models and tasks, and it is particularly well suited to be combined with existing augmentation ap-proaches. Further, our experiments show that MoEx is not limited to computer vision, but is also readily applicable and highly effective in applications within speech recognition and natural language processing—suggesting that MoEx re-veals a fundamental insight about deep nets that crosses areas and data types. Our implementation is available at https://github.com/Boyiliee/MoEx. 2.