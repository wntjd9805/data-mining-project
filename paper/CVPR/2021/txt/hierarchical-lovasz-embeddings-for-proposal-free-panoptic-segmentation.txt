Abstract
Panoptic segmentation brings together two separate tasks: instance and semantic segmentation. Although they are re-lated, unifying them faces an apparent paradox: how to learn simultaneously instance-speciﬁc and category-speciﬁc (i.e. instance-agnostic) representations jointly. Hence, state-of-the-art panoptic segmentation methods use complex mod-els with a distinct stream for each task. In contrast, we pro-pose Hierarchical Lov´asz Embeddings, per pixel feature vectors that simultaneously encode instance- and category-level discriminative information. We use a hierarchical
Lov´asz hinge loss to learn a low-dimensional embedding space structured into a uniﬁed semantic and instance hi-erarchy without requiring separate network branches or object proposals. Besides modeling instances precisely in a proposal-free manner, our Hierarchical Lov´asz Embeddings generalize to categories by using a simple Nearest-Class-Mean classiﬁer, including for non-instance “stuff” classes where instance segmentation methods are not applicable.
Our simple model achieves state-of-the-art results compared to existing proposal-free panoptic segmentation methods on
Cityscapes, COCO, and Mapillary Vistas. Furthermore, our model demonstrates temporal stability between video frames. 1.

Introduction
Holistic scene understanding is an important task in com-puter vision, where a model is trained to explain each pixel in an image, whether that pixel describes stuff – uncountable regions of similar texture such as grass, road or sky – or thing – a countable object with individually identifying character-istics, such as people or cars. While holistic scene under-standing received some early attention [49, 55, 48], modern deep learning-based methods have mainly tackled the tasks of modeling stuff and things independently under the task names semantic segmentation and instance segmentation.
Recently, Kirillov et al. proposed the panoptic quality (PQ) metric for unifying these two parallel tracks into the holistic task of panoptic segmentation [24]. Panoptic segmentation
*Correspondence to tommi@preferred.jp
Figure 1. Example panoptic segmentation using our method (bot-tom left). Predictions are decoded from our Hierarchical Lov´asz
Embeddings, which encode both instance and class information.
The rightmost columns illustrate distances in embedding space between all pixels and a target pixel in red (warmer colors denote smaller distances). We can see the hierarchical structure: pixels on the same instance being closest, and pixels on other instances in the same category being closer than pixels from other categories. is a key step for visual understanding, with applications in
ﬁelds such as autonomous driving or robotics, where it is crucial to know both the locations of dynamically trackable things, as well as static stuff classes. For example, an au-tonomous car needs to be able to both avoid other cars with high precision, as well as understand the location of the road and sidewalk to stay on a desired path.
A strong but complex baseline for panoptic segmentation is to run independent methods for semantic segmentation and instance segmentation, and then fusing the results. To improve upon this, previous works combine both tasks in a joint model [52, 23, 27]. Early methods focus more on a joint model and the majority of them leverage two-stage instance detection models [43, 52, 23, 29, 13]; some recent works propose bottom-up [15, 9, 53], yet instance and semantic segmentation are still treated separately. Performing panoptic segmentation as a single task without duplicated information across sub-tasks remains an interesting question.
Intuitively, instances are contained in semantics, where semantic representations have higher variance in the embed-ding space to describe a general category, whereas instances have smaller variance to capture object-speciﬁc character-istics. This constitutes a natural hierarchical relationship between instances and semantics (cf. Figure 4).
In this work, we propose to model panoptic segmentation as a uniﬁed task via a novel formulation of the problem: learning hierarchical pixel embeddings. Creating a uniﬁed embedding for the task opens up the potential of leverag-14413
Hierarchical Lov´asz Embedding Space ei
σi si










Image
Fully Convolutional Network
Panoptic Decoding
Panoptic Segmentation
Figure 2. Overview of our method. We train a single-shot fully convolutional network to predict for each pixel i a hierarchical embedding ei as well as an instance seed si and variance σi. The seed map represents probable instance locations, and the variance deﬁnes the margins of the hierarchical embedding space. These are used for panoptic decoding of the embedding space.
Seed Map ing embedding space analysis as conducted in the natural language processing community [34, 37, 36, 35].
Leveraging hierarchical structure for representation learn-ing is well studied [16, 14, 45]. However, previous work has not taken advantage of the semantic-instance visual hierar-chy for end-to-end uniﬁed scene parsing. In this paper, we leverage advances in structural representation learning and encode “instance” and “category” features in a hierarchical embedding space. By doing so, we reduce the redundant information in the output space and optimize the information efﬁciency of network parameters for panoptic segmentation.
Our main contribution is a novel representation learn-ing approach for panoptic segmentation that treats it as a uni-ﬁed task. We propose a simple architecture and loss to learn pixel-wise embeddings to represent instances, object cate-gories and stuff classes, thus enabling uniﬁed embedding-based single-shot panoptic segmentation. In particular, we leverage the Lov´asz hinge loss to learn a structured Hierar-chical Lov´asz Embedding space where categories can be rep-resented with categories jointly. An overview of our method is shown in Figure 2.
Compared to conventional panoptic segmentation models, our model displays temporal stability between video frames, creating temporal smoothness in predictions that can be di-rectly used in downstream applications such as object track-ing and prediction in autonomous driving or mobile robotic systems, where data association is a key component (cf. Fig-ure 8). Experiments on the Cityscapes [10], COCO [30] and
Vistas [38] datasets show that our method establishes state-of-the-art results for proposal-free methods, and also yields competitive results compared with two-stage models. 2.