Abstract
We present a novel group collaborative learning frame-work (GCoNet) capable of detecting co-salient objects in real time (16ms), by simultaneously mining consensus rep-resentations at group level based on the two necessary cri-teria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their in-herent shared attributes using our novel group afﬁnity mod-ule; 2) inter-group separability to effectively suppress the inﬂuence of noisy objects on the output by introducing our new group collaborating module conditioning the inconsis-tent consensus. To learn a better embedding space without extra computational overhead, we explicitly employ auxil-iary classiﬁcation supervision. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and
Cosal2015, demonstrate that our simple GCoNet outper-forms 10 cutting-edge models and achieves the new state-of-the-art. We demonstrate this paper’s new technical contri-butions on a number of important downstream computer vi-sion applications including content aware co-segmentation, co-localization based automatic thumbnails, etc. Code has been made publicly available: https://github.com/ fanq15/GCoNet. 1.

Introduction
Co-salient object detection (CoSOD) targets at detect-ing common salient objects sharing the same attributes given a group of relevant images. CoSOD is more chal-lenging than the standard salient object detection (SOD) task [1, 2, 3] and RGB-D SOD [4, 5, 6, 7], because CoSOD needs to distinguish co-occurring objects across multiple images [8] in presence of other objects. That is, both intra-class compactness and inter-class separability should be simultaneously maximized. With this favorable fea-ture CoSOD is thus often employed as a pre-processing
∗This work was done when Qi was an intern at Kuaishou Technology.
†Corresponding author: Deng-Ping Fan (dengpfan@gmail.com). d1 d2
Other Method d2 >> d1
Our Method
Figure 1. t-SNE [18] visualization of consensuses, where each point represents one consensus of an image group. Highlighted here are two similar but different groups (guitar & violin) to il-lustrate the effectiveness of GCoNet. The consensus strategy in traditional CoSOD model (CoEGNet [8]) tends to cluster consen-suses together even they belong to different groups, resulting in ambiguous co-saliency detection. In contrast, our consensus strat-egy with effective inter-group constraint enables higher diversity with a very large group variance (d2 ≫ d1) and thus better inter-group separability. step for various computer vision tasks, such as image re-trieval [9], image quality assessment [10], collection-based crops [11], co-segmentation [12, 13], semantic segmen-tation [14], image surveillance [15], video analysis [16], video co-localization [17], etc.
Previous works attempt to leverage the consistency among relevant images to facilitate CoSOD within an image group by exploring different shared cues [19, 20, 21] or se-mantic connections [22, 23, 24]. Some of them [25, 26] use predicted saliency maps by computing various inter-image cues to discover co-salient objects. Other works [8, 27] ex-ploit a uniﬁed network to jointly optimize co-saliency infor-mation and saliency maps. 12288
Despite their promising results, most current models only extract their CoSOD representations in an individual group, which introduces a number of limitations. First, im-ages from the same group contain similar foregrounds (i.e., co-salient objects) only provide positive relations while lacking the negative relations between different objects.
Training the model only using positive pairs may lead to overﬁtting and result in ambiguous results for outlier im-ages. Moreover, the number of images in a group is typi-cally limited (20 to 40 images for most CoSOD datasets), so using a single group cannot provide enough information for learning a discriminative representation. Finally, individual groups also fall short in offering high-level semantic infor-mation, which is necessary for distinguishing noisy objects during inference in complex real-world scenarios.
To address the above issues, we propose a novel group collaborative learning framework (GCoNet) to mine the se-mantic correlation between different image groups. The proposed GCoNet consists of three important components: group afﬁnity module (GAM), group collaborating module (GCM) and auxiliary classiﬁcation module (ACM), which simultaneously learn the intra-group compactness and inter-group separability. The GAM makes the network learn the consensus feature within the same image group, while the GCM discriminates target attributes between dif-ferent groups, thus enabling the model to be trained on the existing large-scale SOD datasets.1 We further improve the feature representation at a global semantic level through our
ACM on each image to learn a better embedding space. In summary, our contributions are:
• We introduce a novel group collaborative learning strategy to address the CoSOD problem, and validate its effectiveness with extensive ablation studies.
• We design a novel uniﬁed Group Collaborative Learn-ing Network (GCoNet) for CoSOD by simultaneously considering intra-group compactness and inter-group separability to mine the consensus representation.
• Our group afﬁnity module (GAM) and group collab-orating module (GCM) collaborate with each other to achieve better intra- and inter-group collaborative learning. The auxiliary classiﬁcation module (ACM) further promotes learning at a global semantic level.
• Extensive experiments on three challenging CoSOD benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, show that our GCoNet achieves the new state-of-the-art. Furthermore, we present two downstream appli-cations based on our technical contributions, i.e., co-segmentation and co-localization. 1Note that the existing CoSOD datasets altogether contain about 6k images, while there are more than 12 SOD datasets, containing about 60k images. It may partially alleviate the insufﬁcient training data issue in co-salient object detection. 2.