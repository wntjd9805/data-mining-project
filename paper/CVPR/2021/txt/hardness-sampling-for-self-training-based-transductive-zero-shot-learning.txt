Abstract
Transductive zero-shot learning (T-ZSL) which could al-leviate the domain shift problem in existing ZSL works, has received much attention recently. However, an open prob-lem in T-ZSL: how to effectively make use of unseen-class samples for training, still remains. Addressing this prob-lem, we ﬁrst empirically analyze the roles of unseen-class samples with different degrees of hardness in the training process based on the uneven prediction phenomenon found in many ZSL methods, resulting in three observations. Then, we propose two hardness sampling approaches for selecting a subset of diverse and hard samples from a given unseen-class dataset according to these observations. The ﬁrst one identiﬁes the samples based on the class-level frequency of the model predictions while the second enhances the for-mer by normalizing the class frequency via an approximate class prior estimated by an explored prior estimation algo-rithm. Finally, we design a new Self-Training framework with Hardness Sampling for T-ZSL, called STHS, where an arbitrary inductive ZSL method could be seamlessly em-bedded and it is iteratively trained with unseen-class sam-ples selected by the hardness sampling approach. We in-troduce two typical ZSL methods into the STHS framework and extensive experiments demonstrate that the derived T-ZSL methods outperform many state-of-the-art methods on three public benchmarks. Besides, we note that the unseen-class dataset is separately used for training in some existing transductive generalized ZSL (T-GZSL) methods, which is not strict for a GZSL task. Hence, we suggest a more strict
T-GZSL data setting and establish a competitive baseline on this setting by introducing the proposed STHS framework to
T-GZSL. 1.

Introduction
Recently, zero-shot learning (ZSL), which aims to recog-nize unseen-class samples given a set of labeled seen-class samples and the semantic features of both the seen and un-seen classes, has attracted increasing attention in the ﬁelds of machine learning and computer vision. The key to ZSL is to learn an appropriate mapping between visual and se-mantic features, which could be adapted to the unseen-class domain.
According to whether the unlabeled unseen-class sam-ples are available for training, the existing ZSL methods inductive ZSL (I-could be divided into two categories:
ZSL) methods [5, 49, 42, 17, 51, 12, 13, 36, 20, 22, 10] where only the labeled seen-class samples are available for training and transductive ZSL (T-ZSL) methods [35, 33, 43, 18, 28, 46, 38, 39, 25] where both the labeled seen-class samples and the unlabeled unseen-class samples are avail-able for training. As pointed out in [15, 38, 46, 25] that the I-ZSL methods generally suffer from the domain shift problem that the learned model from the seen-class domain may not be suitable for the unseen-class one. In contrast to the I-ZSL methods, the T-ZSL methods could alleviate the domain shift problem to some extent due to the used unseen-class samples. However, it still remains the follow-ing open problem for T-ZSL: how to make use of unlabeled unseen-class samples for training more effectively?
Addressing this problem, we ﬁrstly investigate the pre-diction accuracies of unseen classes on the public dataset by several popular I-ZSL methods, ﬁnding an uneven pre-diction phenomenon that for each of these methods, its pre-diction accuracies of some unseen classes are considerably different from those of the other unseen classes, that is to say, it is relatively harder to classify some unseen classes but easier to classify the others. The uneven prediction phe-nomenon encourages us to further investigate the roles of unseen-class samples with different degrees of classiﬁcation hardness in the training process, resulting in three observa-tions: When unseen-class samples are available for training, (1) samples from ‘hard’ classes are more effective for im-proving the performance of a given ZSL method than those from ‘easy’ classes; (2) the predicted pseudo labels of the hard classes are less noisy than those of the easy classes; (3) the diversity of hard classes is beneﬁcial to improve the 16499
performance of a given ZSL method. 2.