Abstract
Neural network pruning is an essential approach for reducing the computational complexity of deep models so that they can be well deployed on resource-limited devices.
Compared with conventional methods, the recently devel-oped dynamic pruning methods determine redundant ﬁlters variant to each input instance which achieves higher accel-eration. Most of the existing methods discover effective sub-networks for each instance independently and do not utilize the relationship between different inputs. To maximally ex-cavate redundancy in the given network architecture, this paper proposes a new paradigm that dynamically removes redundant ﬁlters by embedding the manifold information of all instances into the space of pruned networks (dubbed as
ManiDP). We ﬁrst investigate the recognition complexity and feature similarity between images in the training set.
Then, the manifold relationship between instances and the pruned sub-networks will be aligned in the training pro-cedure. The effectiveness of the proposed method is ver-iﬁed on several benchmarks, which shows better perfor-mance in terms of both accuracy and computational cost compared to the state-of-the-art methods. For example, our method can reduce 55.3% FLOPs of ResNet-34 with only 0.57% top-1 accuracy degradation on ImageNet. The code will be available at https://github.com/huawei-noah/Pruning/tree/master/ManiDP. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved state-of-the-art performance on a large variety of computer vision tasks, e.g., image classiﬁcation [16, 49, 24, 60], objection detection [11, 45, 12], and video anal-ysis [8, 13, 22]. Besides the model performance, recent re-searches pay more attention on the model efﬁciency, espe-cially the computational complexity [57, 48, 6, 47]. Since
*Corresponding author. there are considerable real-world applications required to be deployed on resource constrained hardwares, e.g., mo-bile phones and wearable devices, techniques that effec-tively reduce the cost of modern deep networks are re-quired [32, 14, 59, 61].
To this end, a number of model compression algo-rithms have been developed without affecting network per-formance. For instance, quantization [54, 15, 44, 58] uses less bits to represent network weights and knowledge dis-tillation [20, 55, 4, 41, 56] is to train a compact network based on the knowledge of a teacher network. Low-rank ap-proximation [36, 25, 30] tries to decompose the original ﬁl-ters to smaller ones while pruning method directly discards the redundant neurons to get a sparser network. Among them, channel pruning (or ﬁlter pruning) [19, 50, 37, 34] is regarded as a kind of structured pruning method, which directly discards redundant ﬁlters to obtain a compact net-work with lower computational cost. Since the pruned net-work can be well employed on mainstream hardwares to ob-tain considerable speed-up, channel pruning is widely used in industrial products.
The conventional channel pruning methods obtain a static network applied to all input samples, which do not excavate redundancy maximally, as the diverse demands for network parameters and capacity from different instances are neglected.
In fact, the importance of ﬁlters is highly input-dependent. A few methods proposed recently prune channels according to individual instances dynamically and achieve better performance. For example, Gao et al. [10] in-troduce small auxiliary modules to predict the saliencies of channels with given input data, and prune unimportant ﬁl-ters at run-time. Instance-wise sparsity is adopted in [33] to induce different sub-networks for different samples. How-ever, the existing methods prune channels for individual in-stances independently, which neglects the relationship be-tween different instances. A sparsity constraint with same intensity is usually used for different input instances, re-gardless of the diversity of instance complexity. Besides, the similarity between instances is also valuable informa-5018
tion deserving to explore.
In this paper, we explore a new paradigm for dynamic pruning to maximally excavate network redundancy corre-sponding to arbitrary instance. The manifold information of all samples in the given dataset is exploited in the train-ing process and corresponding sub-networks are derived to preserve the relationship between different instances (Fig-ure 1). Speciﬁcally, we ﬁrst propose to identify the com-plexity of each instances in the training set and adaptively adjust the penalty weight on channel sparsity. Then, we fur-ther preserve the similarity between samples in the pruned results, i.e., the sub-network for each input sample. In prac-tice, the features with abundant semantic information ob-tained by the network are used for calculating the simi-larity. By exploiting the proposed approach, we can allo-cate the overall resources more reasonably, and then ob-tain pruned networks with higher performance and lower costs. Experiments are throughly conducted on a series of benchmarks for demonstrating the effectiveness of the new method. Compared with the state-of-the-art pruning algo-rithms, we obtain higher performance in terms of both net-work accuracy and speed-up ratios. 2.