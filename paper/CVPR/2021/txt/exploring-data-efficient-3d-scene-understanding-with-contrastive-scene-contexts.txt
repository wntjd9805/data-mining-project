Abstract 1.

Introduction
The rapid progress in 3D scene understanding has come with growing demand for data; however, collecting and an-notating 3D scenes (e.g. point clouds) are notoriously hard.
For example, the number of scenes (e.g. indoor rooms) that can be accessed and scanned might be limited; even given sufﬁcient data, acquiring 3D labels (e.g. instance masks) requires intensive human labor. In this paper, we explore data-efﬁcient learning for 3D point cloud. As a
ﬁrst step towards this direction, we propose Contrastive
Scene Contexts, a 3D pre-training method that makes use of both point-level correspondences and spatial contexts in a scene. Our method achieves state-of-the-art results on a suite of benchmarks where training data or labels are scarce. Our study reveals that exhaustive labelling of 3D point clouds might be unnecessary; and remarkably, on
ScanNet, even using 0.1% of point labels, we still achieve 89% (instance segmentation) and 96% (semantic segmenta-tion) of the baseline performance that uses full annotations.
Recent advances in deep learning on point clouds, such as those obtained from LiDAR or depth sensors, together with a proliferation of public, annotated datasets [9, 13, 53, 32, 64, 2, 40, 55], have led to swift progress in 3D scene understanding. However, compared to large-scale 2D scene understanding on images [14, 38, 23], the scale of 3D scene understanding—in terms of the amount and diversity of data and annotations, the model size, the number of semantic categories, and so on—still falls behind. We argue that one major bottleneck is the fact that collecting and annotating diverse 3D scenes are signiﬁcantly more expensive. Unlike 2D images that comfortably exists on the Internet, collect-ing real world 3D scene datasets usually involves travers-ing the environment in real life and scanning with 3D sen-sors. Therefore, the number of indoor scenes that can be scanned might be limited. What is more concerning is that, even given sufﬁcient data acquisition, 3D semantic labelling (e.g. bounding boxes and instance masks) requires complex pipelines [13] and labor-intensive human effort.
In this work, we explore a new learning task in 3D, i.e. 15587
data-efﬁcient 3D scene understanding, which focuses on the problem of learning with limited data or supervision1. We note that the importance of data-efﬁcient learning in 3D is two-fold. One concerns the status quo: given limited data we have right now, can we design better methods that per-form better? The other one is more forward-looking: is it possible to reduce the human labor for annotation, with a goal of creating 3D scene datasets on a much larger scale?
To formally study the problem, we ﬁrst introduce a suite of scene understanding benchmarks that encompasses two complementary settings for data-efﬁcient learning: (1) lim-ited scene reconstructions (LR) and (2) limited annotations (LA). The ﬁrst setting concerns the scenario where the bot-tleneck is the number of scenes that can be scanned and reconstructed. The second one focuses on the case where in each scene, the budget for labeling is constrained (e.g. one can only label a small set of points). For each setting, the evaluation is done on a diverse set of scene understanding tasks including object detection, semantic segmentation and instance segmentation.
For data-efﬁcient learning in 2D [28], representation learning, e.g. pre-training on a rich source set and ﬁne-tuning on a much smaller target set, often comes to the res-cue; in 3D, representation learning for data-efﬁcient learn-ing is even more wanted but long overdue. With this per-spective, we focus on studying data-efﬁcient 3D scene un-derstanding through the lens of representation learning.
Only recently, PointContrast [65] demonstrates that net-work weights pre-trained on 3D partial frames can lead to a performance boost when ﬁne-tuned on 3D semantic seg-mentation and object detection tasks. Our work is inspired by PointContrast. However, we observe that the simple contrastive-learning based pretext task used in [65] only concerns point-level correspondence matching, which com-pletely disregards the spatial conﬁgurations and contexts in a scene. In Section 3, we show that this design limits the scalibility and transferability; we further propose an ap-proach that integrates the spatial information into the con-trastive learning framework. The simple modiﬁcation can signiﬁcantly improve the performance over PointContrast, especially on complex tasks such as instance segmentation.
Our exploration in data-efﬁcient 3D scene understand-ing provides some surprising observations. For example, on
ScanNet, even using 0.1% of point labels, we are still able to recover 89% (instance segmentation) and 96% (semantic segmentation) of the baseline performance that uses full an-notations. The results imply that exhaustive labelling of 3D point clouds might not be necessary. In both scenarios of limited scene reconstructions (LR) and limited annotations (LA), our pre-trained network, when used as the initializa-1Sometimes a distinction is drawn between data-efﬁciency and label-efﬁciency, to separate the scenarios of limited amount of data samples and limited supervision; here, we use data-efﬁciency to encompass both cases. tion for supervised ﬁne-tuning, offers consistent improve-ment across multiple tasks and datasets. In the scenario of
LA, we also show that an active labeling strategy can be en-abled by clustering the pre-trained point features.
In summary, the contributions of our work include:
• A systematic study on data-efﬁcient 3D scene under-standing with a comprehensive suite of benchmarks.
• A new 3D pre-training method that can gracefully transfer to complex tasks such as instance segmenta-tion and outperform the state-of-the-art results.
• Given the pre-trained network, we study practical so-lutions for data-efﬁcient learning in 3D through ﬁne-tuning as well as an active labeling strategy. 2.