Abstract
The observation that computer vision methods overﬁt to dataset speciﬁcs has inspired diverse attempts to make ob-ject recognition models robust to domain shifts. However, similar work on domain-robust visual question answering methods is very limited. Domain adaptation for VQA dif-fers from adaptation for object recognition due to addi-tional complexity: VQA models handle multimodal inputs, methods contain multiple steps with diverse modules result-ing in complex optimization, and answer spaces in different datasets are vastly different. To tackle these challenges, we
ﬁrst quantify domain shifts between popular VQA datasets, in both visual and textual space. To disentangle shifts be-tween datasets arising from different modalities, we also construct synthetic shifts in the image and question domains separately. Second, we test the robustness of different fam-ilies of VQA methods (classic two-stream, transformer, and neuro-symbolic methods) to these shifts. Third, we test the applicability of existing domain adaptation methods and de-vise a new one to bridge VQA domain gaps, adjusted to speciﬁc VQA models. To emulate the setting of real-world generalization, we focus on unsupervised domain adapta-tion and the open-ended classiﬁcation task formulation. 1.

Introduction
Visual question answering (VQA) borders on AI-completeness: it requires perception (visual and linguistic) and cognition. Despite the strong performance of recent
VQA methods, they fall short of generalization and true reasoning: they are known to suffer from dataset bias [22], require domain-speciﬁc languages or domain-speciﬁc exe-cutable program annotations [34, 41], or must be trained separately for each new dataset.
Prior work in domain adaptation for object recognition examines how robust methods are when trained and tested on different datasets (domains), and further proposes tech-niques to bridge domain gaps. In contrast, there is a short-age of analyses of how domain-robust visual question an-swering methods are. Importantly, domain adaptation tech-Figure 1. The same visual setting can be captured in different ways in VQA datasets, and paired with different information needs (questions). They may require deduction using visual contents, reading from a speciﬁc region of the image, or reasoning about complex spatial relationships. All examples are selected from real
VQA datasets, i.e. VQA v2, VQA Abstract, VizWiz and GQA. niques cannot successfully be applied in the VQA setting in a straight-forward manner. First, VQA models take inputs across multiple modalities, each of which could contribute to the domain speciﬁcity of the trained models. Second, dif-ferent VQA methods have multiple intermediate stages and processing steps over the inputs, which makes optimization challenging. Domain adaptation techniques could be ap-plied at multiple of these stages, and domain adaptation can be performed jointly or separately from VQA training, with varying success. Third, answer spaces in different datasets are vastly different. While domain adaptation methods exist to tackle non-identical answer spaces in object recognition, this setting is not very common. Conversely, in VQA, it is the norm, since many datasets are highly specialized (for example, VizWiz [23] contains special answers “unanswer-able” or “unsuitable image” because image-question pairs 7046
are provided by visually impaired users).
To tackle each of these challenges, we propose the fol-lowing steps. First, to understand how the multiple modal-ities contribute to domain shifts, we break down and mea-sure both visual and textual domain shifts across datasets.
We disentangle shifts in image and question space by con-structing synthetic dataset variants, to test how VQA meth-ods respond to these separate shifts. To understand how the multiple steps and mechanisms in recent VQA models make them robust or fragile to shifts, we compare different fam-ilies (classic two-stream, transformer, and neuro-symbolic methods) by exposing them to different shifts. We exam-ine multiple mechanisms to bridge domain gaps for these methods, in the challenging setting of unsupervised adapta-tion where no labels from the target set are available, and discuss the differences in successful versus unsuccessful at-tempts. Third, to examine the contribution of answer space differences, we use the open-ended VQA classiﬁcation for-mulation. Because no embedding is available for the answer options, the gap in answer spaces is more pronounced. We compare performance across datasets and observe relations between particular modality shifts and domain robustness.
In more detail, we compare image and question repre-sentations across nine datasets: VQA v1 and v2, VQA Ab-stract, Visual 7w, Visual Genome, COCO QA, CLEVR,
GQA and VizWiz. We ﬁnd there are large shifts in both visual and textual space, both at a low- and high-level (e.g. syntax and meaning). We separately apply automatic style transfer (for the visual modality) and paraphrasing (for the textual modality) to disentangle VQA methods’ robustness separately to each of these artiﬁcial shifts. We also observe disparate contributions of these shifts in methods’ perfor-mance across real domain gaps.
We ﬁnd evidence that neuro-symbolic, compositional models are more robust to domain shift than others, because in those methods, perception and reasoning are more disen-tangled. We argue that reasoning has the potential to be domain-independent: for example, the process of reason-ing about spatial relationships can in theory be abstracted away from pixel space, thus should not need retraining if the pixel space changes. Inspired by the potential of perception-reasoning disentanglement, we design a two-stage domain adaptation technique to bridge domain gaps. We show that this two-stage variant is more successful than a direct, one-stage application of [17], and a version of [47], for recover-ing performance lost due to domain gaps.
We are only aware of two prior works on domain adap-tation for VQA [10, 38]. Both of these consider supervised domain adaptation (labels present in target dataset) while we operate in an unsupervised setting (labels on source dataset only). They work with fewer datasets (2-5) and ap-ply domain adaptation to fewer and simpler VQA methods.
Our work can be seen as a “reality check” for VQA meth-ods, similar to prior reality checks for metric learning and weakly supervised object detection [13, 43].
To summarize, our contribution is to answer the follow-ing questions: (1) In what ways (visual, semantic, syntactic) are image-question pairs from recent VQA datasets differ-ent? (2) What kind of dataset differences most affect VQA generalization? (3) Which methods are more robust to syn-thetic visual shifts? (4) Which methods allow more gener-alization when training/testing on different VQA datasets? (5) What domain adaptation techniques most successfully bridge domain gaps? (6) What are the challenges of per-forming domain adaptation in unsupervised VQA? 2.