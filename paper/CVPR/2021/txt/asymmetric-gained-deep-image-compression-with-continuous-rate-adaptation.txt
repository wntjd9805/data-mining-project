Abstract
Input Image
With the development of deep learning techniques, the combination of deep learning with image compression has drawn lots of attention. Recently, learned image compres-sion methods had exceeded their classical counterparts in terms of rate-distortion performance. However, continuous rate adaptation remains an open question. Some learned image compression methods use multiple networks for mul-tiple rates, while others use one single model at the ex-pense of computational complexity increase and perfor-mance degradation.
In this paper, we propose a contin-uously rate adjustable learned image compression frame-work, Asymmetric Gained Variational Autoencoder (AG-VAE). AG-VAE utilizes a pair of gain units to achieve dis-crete rate adaptation in one single model with a negligi-ble additional computation. Then, by using exponential in-terpolation, continuous rate adaptation is achieved without compromising performance. Besides, we propose the asym-metric Gaussian entropy model for more accurate entropy estimation. Exhaustive experiments show that our method achieves comparable quantitative performance with SOTA learned image compression methods and better qualitative performance than classical image codecs. In the ablation study, we conÔ¨Årm the usefulness and superiority of gain units and the asymmetric Gaussian entropy model. 1.

Introduction
Image compression is one of the most fundamental and valuable problems in image processing and computer vi-sion.
In the last decades, many researchers have worked for the development and optimization of the classical image compression codecs, such as JPEG [32], JPEG2000 [27] and BPG [8]. To remove redundancy within images, basic modules of the classical codes, including transform coding, entropy coding and quantization, have been sophistically designed and applied. Since these modules are artiÔ¨Åcially designed and optimized separately, it is not easy to obtain
Encoder
Gain
Reconstruction
Decoder
ùëôùë†
Inverse
Gain
Q
AE bit stream
AD y r t e m m y s
A n a i s s u a
G
Figure 1. AG-VAE framework. We achieve rate adaptation by in-serting a gain unit after encoder and an inverse gain unit before decoder. The bit rate could be adjusted continuously with the change of the gain vector index s and the interpolation coefÔ¨Åcient l. The asymmetric Gaussian entropy model estimates entropy of the gained and quantized latent representation accurately. an optimal solution for different evaluation indicators.
Recently, deep learning has achieved signiÔ¨Åcant break-throughs in various learning problems such as image clas-siÔ¨Åcation, object detection. Thanks to variational autoen-coder (VAE) and scalar quantization assumption [5, 4, 6, 25, 28, 1], end-to-end learned image compression frame-work also has derived signiÔ¨Åcant interests. With the opti-mization of entropy estimation modules [7, 20, 16] and au-toencoder network structure [26, 3, 17, 19, 31, 42, 18, 9], the
VAE-based image compression methods have achieved bet-ter rate-distortion (R-D) performance than the classical im-age compression codecs on common metrics, such as Peak
Signal-to-Noise Ratio (PSNR) and Multi-Scale-Structural
Similarity Index measure (MS-SSIM) [33].
Some VAE-based image compression methods need to train multiple Ô¨Åxed-rate models to realize rate adaption, each model for one rate. Therefore, the training cost and memory requirement increase dramatically with the growth and reÔ¨Ånement of the desired rate range. Instead of using multiply models, some other methods achieve the rate adap-tation using one single model. The RNN-based schemes
[29, 30, 13] encode the input image progressively, but they suffer from bad R-D performance. The conditional autoen-10532
coder [10, 34] incorporates fully connected layers into the convolution unit to achieve discrete rate adaptation while in-creasing the network‚Äôs computational complexity and mem-ory requirement. Mixed bin sizes [10] are introduced to extend the range coverage from Ô¨Ånite discrete points to a broad rate range, but they induce R-D performance degra-dation. Bottleneck scaling scheme [28, 2] ignores compati-bility between autoencoder and scaling parameters and has a poor performance in low bit rate range. Although provid-ing feasible solutions to rate adaptation in a single model, the methods mentioned above have various practical prob-lems such as performance degradation, computational com-plexity increase and memory increase.
As shown in Figure 1, we propose a novel image com-pression framework, AG-VAE. It can continuously adjust the bit rate in one single model and achieves comparable
R-D performance with SOTA learned image compression methods in quantitative metrics and qualitative visual qual-ity. Based on the unevenness of channel redundancy, we design a plug-and-play variable-rate block, gain unit. By simple channel-wise multiplication, the gain unit rescales the latent representation. The degree of information loss is then controlled in the quantization process.
We address two critical challenges of the proposed framework. First, the inverse-gain unit is introduced to avoid performance degradation. Second, we study the re-construction assumption of gain units to deduce the expo-nent interpolation formula, enabling continuous rate adap-tation without extra training. To avoid entropy estimation error for the samples with the asymmetric distribution, we also introduce the asymmetric Gaussian entropy model to achieve good R-D performance. To demonstrate the uni-versality of gain units, we further integrate them into other backbone architectures [7, 20]. Besides, we also compare gain units with previous rate adaptation methods from addi-tional computation and performance degradation.
Our contributions can be summarized as follows:
‚Ä¢ We introduce gain units to achieve discrete rate adap-tation in one single model. With negligible additional computational cost, our method has a similar perfor-mance with SOTA learned image compression method.
‚Ä¢ We propose the exponent interpolation, which can gen-erate gain vectors at the arbitrary bit rate. The expo-nent interpolation formula extends the rate‚Äôs coverage from Ô¨Ånite discrete points to a broad continuous range without an extra training process.
‚Ä¢ Gain units with exponent interpolation can be eas-ily generalized to all VAE-based image compression methods while avoiding performance degradation.
‚Ä¢ We propose the asymmetric Gaussian entropy model to achieve more accurate entropy estimation. Less bit rate is required to reach the same distortion level. 2.