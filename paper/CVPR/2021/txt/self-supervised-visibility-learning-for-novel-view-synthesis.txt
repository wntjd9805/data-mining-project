Abstract
We address the problem of novel view synthesis (NVS) from a few sparse source view images. Conventional image-based rendering methods estimate scene geometry and syn-thesize novel views in two separate steps. However, erro-neous geometry estimation will decrease NVS performance as view synthesis highly depends on the quality of estimated scene geometry. In this paper, we propose an end-to-end
NVS framework to eliminate the error propagation issue.
To be speciﬁc, we construct a volume under the target view and design a source-view visibility estimation (SVE) module to determine the visibility of the target-view voxels in each source view. Next, we aggregate the visibility of all source views to achieve a consensus volume. Each voxel in the consensus volume indicates a surface existence probability.
Then, we present a soft ray-casting (SRC) mechanism to ﬁnd the most front surface in the target view (i.e., depth). Specif-ically, our SRC traverses the consensus volume along view-ing rays and then estimates a depth probability distribution.
We then warp and aggregate source view pixels to synthe-size a novel view based on the estimated source-view visi-bility and target-view depth. At last, our network is trained in an end-to-end self-supervised fashion, thus signiﬁcantly alleviating error accumulation in view synthesis. Experi-mental results demonstrate that our method generates novel views in higher quality compared to the state-of-the-art. 1.

Introduction
Suppose after taking a few snapshots of a famous sculp-ture, we wish to look at the sculpture from some other dif-ferent viewpoints. This task would require us to generate novel-view images from the captured ones and is generally referred to as “NVS”. However, compared with previous so-lutions, our setting is more challenging, because the num-ber of available real views is very limited, and the underly-ing 3D geometry is not available. Moreover, the occlusion along target viewing rays and the visibility of target pixels in source views are hard to infer.
Conventional image-based rendering (IBR) methods [4, 24, 10, 42, 23] ﬁrst reconstruct a proxy geometry by a multi-view stereo (MVS) algorithm [12, 47, 48, 46]. They then aggregate source views to generate the new view according 9675
to the estimated geometry. Since the two steps are separated from each other, their generated image quality is affected by the accuracy of the reconstructed 3D geometry.
However, developing an end-to-end framework that combines geometry estimation and image synthesis is non-trivial.
It requires addressing the following challenges.
First, estimating target view depth by an MVS method will be no longer suitable for end-to-end training because they need to infer depth maps for all source views. It is time-and memory-consuming. Second, when source view depths are not available, the visibility of target pixels in each source view is hard to infer. A naive aggregation of warped input images would cause severe image ghosting artifacts.
To tackle the above challenges, we propose to estimate target-view depth and source-view visibility directly from source view images, without estimating depths for source views. Speciﬁcally, we construct a volume under the target view camera frustum. For each voxel in this volume, when its projected pixel in a source view is similar to the projected pixels in other source views, it is likely that the voxel is visible in this source view. Motivated by this, we design a source-view visibility estimation module (SVE). For each source view, our SVE takes the warped source view features as input, compares their similarity with other source views, and outputs visibility of the voxels in this source view.
Then, we aggregate the estimated visibility of the vox-els in all source views, obtaining a consensus volume. The value in each voxel denotes a surface existence probability.
Next, we design a soft ray-casting (SRC) mechanism that traverses through the consensus volume along viewing rays and ﬁnds the most front surfaces (i.e., depth). Since we do not have ground truth target-view depth as supervision, our
SRC outputs a depth probability instead of a depth map to model uncertainty.
Using the estimated target-view depth and source-view visibility, we warp and aggregate source view pixels to gen-erate the novel view. Since the 3D data acquisition is ex-pensive to achieve in practice, we do not have any explicit supervision on the depth or visibility. Their training signals are provided implicitly by the ﬁnal image synthesis error.
We then employ a reﬁnement network to further reduce ar-tifacts and synthesize realistic images. To tolerate the visi-bility estimation error, we feed our reﬁnement network the aggregated images along with warped source view images. 2.