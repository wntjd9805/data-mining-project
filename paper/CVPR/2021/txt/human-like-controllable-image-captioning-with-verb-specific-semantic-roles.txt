Abstract
Controllable Image Captioning (CIC) — generating im-age descriptions following designated control signals — has received unprecedented attention over the last few years.
To emulate the human ability in controlling caption gener-ation, current CIC studies focus exclusively on control sig-nals concerning objective properties, such as contents of interest or descriptive patterns. However, we argue that al-most all existing objective control signals have overlooked two indispensable characteristics of an ideal control sig-nal: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. 2) Sample-suitable: the control signals should be suitable for a speciﬁc image sample. To this end, we pro-pose a new control signal for CIC: Verb-speciﬁc Seman-tic Roles (VSR). VSR consists of a verb and some semantic
∗ denotes equal contributions, † denotes the corresponding author. roles, which represents a targeted activity and the roles of entities involved in this activity. Given a designated VSR, we
ﬁrst train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to learn human-like descriptive semantic structures. Lastly, we use a role-shift captioning model to generate the captions. Extensive experiments and ablations demonstrate that our framework can achieve better controllability than several strong base-lines on two challenging CIC benchmarks. Besides, we can generate multi-level diverse captions easily. The code is available at: https://github.com/mad-red/VSR-guided-CIC. 1.

Introduction
Image captioning, i.e., generating ﬂuent and meaningful descriptions to summarize the salient contents of an image, is a classic proxy task for comprehensive scene understand-16846
ing [21]. With the release of several large scale datasets and advanced encoder-decoder frameworks, current captioning models plausibly have already achieved “super-human” per-formance in all accuracy-based evaluation metrics. How-ever, many studies have indicated that these models tend to produce generic descriptions, and fail to control the caption generation process as humans, e.g., referring to different contents of interest or descriptive patterns. In order to en-dow the captioning models with human-like controllability, a recent surge of efforts [16, 10, 19, 76, 46, 75, 27, 20] re-sort to introducing extra control signals as constraints of the generated captions, called Controllable Image Captioning (CIC). As a byproduct, the CIC models can easily generate diverse descriptions by feeding different control signals.
Early CIC works mainly focus on subjective control sig-nals, such as sentiments [40], emotions [41, 22], and per-sonality [14, 52], i.e., the linguistic styles of sentences. Al-though these stylized captioning models can eventually pro-duce style-related captions, they remain hard to control the generation process effectively and precisely. To further im-prove the controllability, recent CIC works gradually put a more emphasis on objective control signals. More speciﬁ-cally, they can be coarsely classiﬁed into two categories: 1)
Content-controlled: the control signals are about the con-tents of interest which need to be described. As the example
) as a shown in Figure 1 (a), given the region set ( control signal, we hope that the generated caption can cover all regions (i.e., man, wave, and surfboard). So far, various types of content-controlled signals have been pro-posed, such as visual relations [27], object regions [16, 34], scene graphs [10, 76], and mouse trace [46]. 2) Structure-controlled: the control signals are about the semantic struc-tures of sentences. For instance, the length-level [19], part-of-speech tags [20], or attributes [77] of the sentence (cf.
Figure 1 (b)) are some typical structure-controlled signals.
,
,
Nevertheless, all existing objective control signals (i.e., both content-controlled and structure-controlled) have over-looked two indispensable characteristics of an ideal control signal towards “human-like” controllable image captioning: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described ac-tivity. Imaging how humans describe images — our brains always quickly structure a descriptive pattern like “STH DO
STH AT SOMEPLACE” ﬁrst, and then ﬁll in the detailed de-scription [54, 44, 29, 69], i.e., we have subconsciously made sure that all the mentioned entities are event-compatible (e.g., man, wave, surfboard are all involved in activity riding in Figure 1 (a)). To further see the negative impact of dissatisfying this requirement, suppose that we deliber-ately utilize two more objects (hand and sky, i.e.,
) as part of the control signal, and the model generates an incoherent and illogical caption. 2) Sample-suitable: the control signals should be suitable for the speciﬁc image
,
Figure 2: Two image examples of a verb and its semantic roles. The verb eating captures the scope of the activ-ity, and agent, food, container, tool are all reasonable semantic roles for this activity.
EATING
Agent
Food
Container child pancake plate
Tool fork
Agent women
Food salad
Container bowl
Tool fork sample. By “suitable”, we mean that there do exist rea-sonable descriptions satisfying the control signals, e.g., a large length-level may not be suitable for an image with a very simple scene. Unfortunately, it is always very difﬁ-cult to decide whether a control signal is sample-suitable in advance. For example in Figure 1 (b), although the two con-trol signals (i.e., length-levels 3 and 4) are quite close, the quality of respectively generated captions varies greatly.
In this paper, we propose a new event-oriented objective control signal, Verb-speciﬁc Semantic Roles (VSR), to meet both event-compatible and sample-suitable requirements si-multaneously. VSR consists of a verb (i.e., predicate [8]) and some user-interested semantic roles [30]. As shown in
Figure 2, the verb captures the scope of a salient activity in the image (e.g., eating), and the corresponding semantic roles1 (e.g., agent, food, container, and tool) cat-egorize how objects participate in this activity, i.e., a child (agent) is eating (activity) a pancake (food) from a plate (container) with a fork (tool). Thus, VSR is designed to guarantee that all the mentioned entities are event-compatible. Meanwhile, unlike the existing structure-controlled signals which directly impose constraints on the generated captions, VSR only restricts the involved seman-tic roles, which is theoretically suitable for all the images with the activity, i.e., sample-suitable.
In order to generate sentences with respect to the desig-nated VSRs, we ﬁrst train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to rank the given verb and semantic roles, and output some human-like descriptive semantic structures, e.g., Arg0reader – read – Arg1thing – LOC in Figure 1 (c). Finally, we com-bine the grounded entities and semantic structures, and use an RNN-based role-shift captioning model to generate the captions by sequentially focusing on different roles.
Although these are no available captioning datasets with the VSR annotations, they can be easily obtained by off-the-shelf semantic role parsing toolkits [51]. Extensive ex-periments on two challenging CIC benchmarks (i.e., COCO 1We use PropBank-style annotations of semantic roles (e.g., Arg0,
Arg1) in all experiments (cf. Figure 1). The FrameNet-style annotations of semantic roles (e.g., Agent) here are just for a more intuitive illustra-tion. In the PropBank-style annotations, Arg denotes “argument”, MNR de-notes “manner”, DIR denotes “directional”, and LOC denotes “location”.
We leave more details in the supplementary material. 16847
Entities [16] and Flickr30K Entities [45]) demonstrate that our framework can achieve better controllability given des-ignated VSRs than several strong baselines. Moreover, our framework can also realize diverse image captioning and achieve a better trade-off between quality and diversity.
In summary, we make three contributions in this paper: 1. We propose a new control signal for CIC: Verb-speciﬁc
Semantic Roles (VSR). To the best of our knowledge,
VSR is the ﬁrst control signal to consider both event-compatible and sample-suitable requirements2. 2. We can learn human-like verb-speciﬁc semantic struc-tures automatically, and abundant visualization exam-ples demonstrate that these patterns are reasonable. 3. We achieve state-of-the-art controllability on two chal-lenging benchmarks, and generate diverse captions by using different verbs, semantic roles, or structures. 2.