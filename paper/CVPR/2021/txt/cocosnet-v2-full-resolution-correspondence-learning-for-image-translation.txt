Abstract
We present the full-resolution correspondence learning for cross-domain images, which aids image translation. We adopt a hierarchical strategy that uses the correspondence from coarse level to guide the ﬁne levels. At each hier-archy, the correspondence can be efﬁciently computed via
PatchMatch that iteratively leverages the matchings from the neighborhood. Within each PatchMatch iteration, the
ConvGRU module is employed to reﬁne the current cor-respondence considering not only the matchings of larger context but also the historic estimates. The proposed Co-CosNet v2, a GRU-assisted PatchMatch approach, is fully differentiable and highly efﬁcient. When jointly trained with image translation, full-resolution semantic correspondence can be established in an unsupervised manner, which in turn facilitates the exemplar-based image translation. Experi-ments on diverse translation tasks show that CoCosNet v2 performs considerably better than state-of-the-art literature on producing high-resolution images. 1.

Introduction
Image-to-image translation learns the mapping between image domains and has shown success in a wide range of applications [28, 10, 38, 45, 58]. Particularly, exemplar based image translation allows more ﬂexible user control by conditioning the translation on a speciﬁc exemplar with the desired style. However, simultaneously producing high quality while being faithful to the exemplar is non-trivial, whereas it becomes rather challenging for producing high-resolution images.
Early studies [9, 19, 55, 54, 47, 5] directly learn the map-ping through generative adversarial networks [14, 35], yet they fail to leverage the information in the exemplar. Later, a series of methods [12, 17, 39] propose to refer to the exem-plar image during the translation, by modulating the feature normalization according to the style of the exemplar image.
However, as the modulation is applied uniformly, only the global style can be transferred whereas the detailed textures are washed out in the ﬁnal output.
*Author did this work during his internship at Microsoft Research Asia.
Figure 1: Image translation at resolution 512×512 for pose-to-body (DeepFashion) and at resolution 1024×1024 for edge-to-face (MetFaces). For each task, the 1st row shows the exemplar images, and the 2nd row shows the translation outputs.
Very recently, CoCosNet [56] established the dense se-mantic correspondence between cross-domain images. In this way the network could make use of the ﬁne textures from the exemplar, which eases the hallucination for the local textures. However, prohibitive memory footprint oc-curs when estimating high-resolution correspondence, as the matching requires to compute the pairwise similarities among all locations of the input feature maps, while low-resolution correspondences (e.g., 64×64) cannot guide the network to leverage the ﬁne structures from the exemplar.
In this paper, we propose the cross-domain correspon-dence learning, in full-resolution for the ﬁrst time, which leads to high-resolution translated images in photo-realistic quality, as the network can leverage the meticulous details 11465
from the exemplar. To achieve that, we draw inspiration from PatchMatch [3] which is advantageous in computa-tional efﬁciency and texture coherency as it iteratively prop-agates the correspondence from the neighborhood rather than searching globally. Nonetheless, directly applying
PatchMatch to high-resolution feature maps for training is infeasible and the reasons are threefold. First of all, this algorithm is not efﬁcient enough for high-resolution images when the correspondence is initialized randomly. Second, at the early training phase, the correspondence is chaotic and the backward gradient ﬂows to the incorrectly corresponded patches, making the feature learning difﬁcult. Moreover,
PatchMatch fails to consider a larger context when prop-agating the correspondence estimate and requires a large number of iterations to converge.
To tackle these limitations, we propose the following techniques to learn the full-resolution correspondence. 1)
We adopt a hierarchical strategy that makes use of the matchings from the coarse level to guide the subsequent,
ﬁner levels so that the searching at the ﬁne levels may start with a good initialization. 2) Enlightened by the recent suc-cess of recurrent reﬁnement [41, 7, 44], we employ convo-lutional gated recurrent unit (ConvGRU) to reﬁne the cor-respondence within each PatchMatch iteration. The GRU-assisted PatchMatch considers a larger context as well as the historic correspondence estimates, which considerably im-proves the correspondence quality. Besides, it greatly ben-eﬁts the feature learning as the gradient can now ﬂow to a larger context than just a few corresponded patches. 3) Last but not least, the proposed hierarchical GRU-assisted Patch-Match is fully differentiable, and learns the cross-domain correspondence in an unsupervised manner, which is very challenging especially in high-resolution.
We show that our method, called CocosNet v2, achieves signiﬁcantly higher quality images than the state-of-the-art litearture due to the full-resolution cross-domain correspon-dence. More importantly, our approach is able to gen-erate visually appealing image translation results in high-resolution, e.g., images at 512×512 and 1024×1024 (Fig-ure 1). We summarize our major contributions as follows:
• We propose to learn full-resolution correspondence from different domains in order to capture meticulously realis-tic details from an exemplar image for image translation.
• To achieve that, we propose CoCosNet v2, a hierarchi-cal GRU-assisted PatchMatch method, for efﬁcient corre-spondence computation, which is simultaneously learned with image translation.
• We show that the full-resolution correspondence leads to signiﬁcantly ﬁner textures in the translation output. The translated images demonstrate unprecedented quality at large resolutions. 2.