Abstract
We propose D-RISE, a method for generating visual ex-planations for the predictions of object detectors. Utiliz-ing the proposed similarity metric that accounts for both localization and categorization aspects of object detection allows our method to produce saliency maps that show im-age areas that most affect the prediction. D-RISE can be considered “black-box” in the software testing sense, as it only needs access to the inputs and outputs of an object detector. Compared to gradient-based methods, D-RISE is more general and agnostic to the particular type of object detector being tested, and does not need knowledge of the inner workings of the model. We show that D-RISE can be easily applied to different object detectors including one-stage detectors such as YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed analysis of the generated visual explanations to highlight the utilization of context and possible biases learned by object detectors. 1.

Introduction
The ﬁeld of object detection has experienced signiﬁcant gains in performance since the adoption of deep neural net-works (DNNs) [9]. However, DNNs remain opaque tools with a complex and unintuitive process of decision-making, resulting in them being hard to understand, debug and im-prove. A number of different explanation techniques of-fer potential solutions to these issues. They have already been shown to ﬁnd biases in trained models [39], help debug them [13] and increase user’s trust [34]. A popular approach to explanation involves the use of attribution techniques which produce saliency maps [20, 35], i.e., heatmaps rep-resenting the inﬂuence different pixels have on the model’s
*Work completed while an intern at Adobe Research.
This work was partially supported by the DARPA XAI program.
Figure 1: D-RISE can highlight which regions of an image were used by an object detector. Here we show outputs for a few cor-responding images where importance increases from blue to red.
In these examples, D-RISE reveals things such as detectors often looking outside bounding boxes to detect objects e.g., looking at the ski poles to predict skis, or looking to a subset of regions within the object e.g., looking at the Apple logo to predict laptop. decision. Hitherto, these techniques have primarily focused on the image classiﬁcation task [27, ?, 34, 40, 44, 2, 42], with few addressing other problems such as visual question answering [25], video captioning [28, 3] and video activity recognition [3]. In this work, we address the relatively un-derexplored direction of generating saliency maps for object detectors.
Unlike methods that explain the emerging patterns in the learned weights or activations [4, 42, 22], attribution tech-niques are usually tightly connected to the model’s design and they rely on a number of assumptions about the model’s architecture. For example, Grad-CAM [34] assumes that each feature map correlates with some concept, and there-fore, feature maps can be weighted with respect to the im-portance of their concept for the output category. We show that these assumptions might not hold for object detec-tion models, resulting in failure to produce quality saliency maps. Additionally, object detectors require explanations 11443
Figure 2: Our method D-RISE attempts to explain the detections (bounding-box+category) produced for this image by an object detector.
We convert target detections that need to be explained into detection vectors dt. We sample N binary masks, Mi, and run the detector on the masked images to obtain proposals Dp. We compute pairwise similarities between targets and proposals to obtain weights for each mask. Finally, the weighted sum of masks is computed to produce saliency maps. In classiﬁcation, the ouput of the black-box model can be directly used as mask weights. not just for the categorization of a bounding box but also for the location of the bounding box itself. For these rea-sons, direct application of existing attribution techniques to object detectors is infeasible.
We propose Detector Randomized Input Sampling for
Explanation, or D-RISE, the ﬁrst method to produce saliency maps for object detectors that is capable of explain-ing both the localization and classiﬁcation aspects of the detection. D-RISE uses an input masking technique ﬁrst proposed by RISE [27], which enables explanation of the more complex detection networks because it does not rely on gradients or the inner workings of the underlying object detector. However, the method in [27] is only applicable to classiﬁcation, not detection. D-RISE is a black-box method and can be in principle applied to any object detector.
Explaining visual classiﬁers with saliency maps has al-lowed researchers to investigate the localization abilities implicitly learned by these models. Moreover, some works have used explanations of visual classiﬁers for weakly-supervised object localization [14, 24]. In object detection, however, the localization decisions of the model are explicit as they are expressed directly in the outputs of the model.
Therefore, one might assume that exploring spatial impor-tance in this case is redundant, and that the model has al-ready predicted bounding boxes around everything it deems important.
In our experiments with D-RISE, we observe that DNN based object detectors also learn to utilize con-textual regions outside of the box to detect objects. For in-stance the last column in Fig. 1 shows how the tap helps to localize the sink even when it is clearly outside the de-tected box. In fact, the importance of contextual informa-tion for object detection has long been established for both humans [5, 23] and machines [36, 21]. Another reason for studying an object detector’s saliency is the fact that not all sub-regions within the object’s bounding box are equally important. Some object parts are more discriminant, while others may occur with objects of different categories, e.g., cat faces are highlighted as more important by the network than its body (Fig. 1).
Our contributions can be summarized as follows:
• We propose D-RISE, a black-box attribution technique for explaining object detectors via saliency maps, by deﬁning a detection similarity metric.
• We demonstrate generalizability of D-RISE by ex-plaining two commonly used object detectors with different architectural designs, namely one-stage
YOLOv3 [29] and two-stage Faster R-CNN [30].
• Using D-RISE, we systematically analyze potential sources of errors and bias in commonly used object de-tectors trained on the MS-COCO [17] dataset and dis-cover common patterns in data learned by the model.
• We evaluate our method using automated metrics from classiﬁcation saliency and a user study. Additionally, we propose an evaluation procedure that measures how well the saliency method can discover deliberately in-troduced biases in the model via synthetic markers.
Our method surpasses the classiﬁcation baselines. 11444
2.