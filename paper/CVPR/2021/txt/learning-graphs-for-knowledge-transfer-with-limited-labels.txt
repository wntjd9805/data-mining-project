Abstract
Fixed input graphs are a mainstay in approaches that utilize Graph Convolution Networks (GCNs) for knowledge transfer. The standard paradigm is to utilize relationships in the input graph to transfer information using GCNs from training to testing nodes in the graph; for example, the semi-supervised, zero-shot, and few-shot learning setups.
We propose a generalized framework for learning and im-proving the input graph as part of the standard GCN-based learning setup. Moreover, we use additional constraints be-tween similar and dissimilar neighbors for each node in the graph by applying triplet loss on the intermediate layer output. We present results of semi-supervised learning on
Citeseer, Cora, and Pubmed benchmarking datasets, and zero/few-shot action recognition on UCF101 and HMDB51 datasets, signiﬁcantly outperforming current approaches.
We also present qualitative results visualizing the graph connections that our approach learns to update. 1.

Introduction
Graph Convolution Network (GCN) based techniques have been used widely in transfer learning for tasks where labeled data is limited, e.g., semi-supervised learning [24, 55] and zero-shot/few-shot learning [57, 9, 12] with zero or few samples from test classes. These approaches rely on an input graph that captures the relationships between the nodes in the graph. Given this input graph, a GCN is then used to propagate and assimilate information across the graph’s nodes, obeying the relationships expressed in the graph connections. The goal of this framework is to trans-fer information from the training nodes to the test nodes.
This is a fairly generic framework which has been adapted for a wide variety of tasks, with diverse node representa-tions and input graphs. For example, for semi-supervised learning [24, 55], the knowledge is transferred from train-ing samples to test samples; and the nodes represent each sample data point in the dataset and the input graph rep-resents how these samples are related. Zero-shot learn-ing [57, 9, 12] transfer knowledge from training classes to
Project : https://pallabig.github.io/LearningGraphsForGCN/
Figure 1: We use a GCN to update the input graph connec-tions and show results for “Mixing Batter” class in zero-shot action recognition. Language based models associates “bat-ter” to “baseball” which is rectiﬁed in the updated graph. test classes; and the nodes represent semantic embeddings for classes (e.g., word2vec [34], sentence2vec [40]) and in-put graphs can come from a variety of sources (e.g., Word-Net [35], NELL [2], NEIL [6]). Few-shot learning transfer knowledge between class or sample based nodes [12, 23].
One of the key limitations of these GCN-based tech-niques discussed thus far is that the input graph structure, as captured by the adjacency matrix, is ﬁxed. By design, the GCN-based approaches rely heavily on the input graphs, and noisy or low-quality graphs have an outsized impact on performance. In this work, we explore the adaptive learn-ing of the input adjacency matrix over time, in conjunction with the rest of the GCN training; i.e., the losses used to train the underlying tasks (e.g., semi-supervised learning or zero-/few-shot learning) are also used to update the struc-ture of the input adjacency matrix. We show empirically that our learned graph yields better results for downstream tasks. Our proposed approach is a straightforward algo-rithm to update the graph’s structure by learning better node representations and using these to recompute the adjacency matrix. Note that we do not add any new network weights to learn. This is in stark contrast with other related graph learning works [21, 10], which have a separate dedicated network and special loss functions to update the adjacency matrix. Since the learned node representations, via a GCN, capture better correlations with respect to the downstream task, the resulting graph tends to be better than the input graph from an external source. One such update is illus-trated in Figure 1, where we learn better connections for the class “Mixing batter”. A language-based knowledge graph 11151
(KG) associates “batter” with the verb “batting” (shown as
‘input’), and our approach rectiﬁes this mistake across up-dates and results in more meaningful connections.
Operationalizing the straightforward approach described above has two key issues. First, updating a densely or fully-connected graph, in the absence of any other constraints, of-ten tends to provide arbitrary updates to the structure, even-tually leading to degenerate solutions (e.g., same weights for all edges). Second, if the graph connections are sparse (as is generally the case), there is no mechanism to learn to add or drop connections in the learned graph. Simple heuristics, such as ﬁxed degree for each node can be a so-lution, but they tend to be sub-optimal as different nodes might have a different number of related nodes that they should be connected to. In addition, each downstream task can have domain-speciﬁc constraints on the degree of the nodes; e.g., for zero-shot action recognition, [12] observed that a fully-connected graph is detrimental to the perfor-mance and empirically determine the suitable degree. To address both the drawbacks discussed above, while obey-ing the domain-speciﬁc constraints, we propose to utilize a triplet loss formulation on the intermediate output nodes – i.e., the node features after our graph learning step but before the graph is passed to the GCN framework for the downstream task. Our formulation selects positive and neg-ative neighbors for each node, and uses them to add con-straints on its degree. Degenerate solutions are avoided by ensuring that negative neighbors are farther than the posi-tive ones. Therefore, the graph learning step is trained using both the downstream task losses and the triplet loss.
In summary, our contributions are a simple learning ap-proach that can update the input graphs for the GCN-based transfer learning framework and a triplet loss formulation that avoids degenerate solutions and allows the ﬂexibility of degree-constraints. We demonstrate the effectiveness of our approach on semi-supervised, zero-shot, and few-shot learning setups. For semi-supervised learning, we use the generic framework [24] built on citation network datasets, like Cora, Citeseer, and Pubmed, with accompanying well-deﬁned input graphs. For zero-shot/few-shot learning, we focus on the action recognition pipeline [12] with input KG built from sentence2vec [40] embeddings. 2.