Abstract 1.

Introduction
We present a new application direction named Parei-dolia Face Reenactment, which is deﬁned as animating a static illusory face to move in tandem with a human face in the video. For the large differences between pareido-lia face reenactment and traditional human face reenact-ment, two main challenges are introduced, i.e., shape vari-ance and texture variance. In this work, we propose a novel
Parametric Unsupervised Reenactment Algorithm to tackle these two challenges. Speciﬁcally, we propose to decom-pose the reenactment into three catenate processes: shape modeling, motion transfer and texture synthesis. With the decomposition, we introduce three crucial components, i.e.,
Parametric Shape Modeling, Expansionary Motion Trans-fer and Unsupervised Texture Synthesizer, to overcome the problems brought by the remarkably variances on pareido-lia faces. Extensive experiments show the superior perfor-mance of our method both qualitatively and quantitatively.
Code, model and data are available on our project page1.
*Equal contribution.
†Corresponding author. 1https://wywu.github.io/projects/ETT/ETT.html
It’s not often that you look at your meal to ﬁnd it staring back at you. But when Diane Duyser picked up her cheese toastie, she was in for a shock. “I went to take a bite out of it, and then I saw this lady looking back at me,” she told the
Chicago Tribune. “It scared me at ﬁrst.” [1].
The phenomenon described in this BBC news is called face pareidolia, a natural inclination of the human brain to perceive illusory faces that do not actually exist [19, 50]. In this work, we attempt to bring this interesting imagination into reality by animating. As shown in Fig. 1 (b), we pro-pose a new application direction named “Pareidolia Face”
Reenactment, which is deﬁned as animating illusory faces by the motion extracted from human faces automatically.
Pareidolia face reenactment, has large potential usages in ﬁlmmaking [46, 24], cartoon production [53, 56] and mixed reality [47, 55], which always requires a massive la-bor of professional animators. Mostly related, face reen-actment [13, 46, 25, 52] is becoming an emerging topic in recent years. However, all of these methods are designed speciﬁcally for human faces, of which rich priors like facial 2236
landmarks [54, 18] or 3D face models [46, 24] can be uti-lized. But, all of these priors are unachievable for pareidolia faces. Moreover, large-scale face datasets [5, 35] with mas-sive annotations are sufﬁcient for human faces, which are also unreachable for pareidolia faces. Reenacting pareido-lia faces by human portrait videos is still an open question.
The main challenges for pareidolia face reenactment can be summarized into two large variances, i.e., shape variance and texture variance. Shape variance means that the bound-ary shapes of facial parts are remarkably diverse, such as circular, square and moon-shape mouths as shown in Fig. 1 (a). For human faces, landmarks are always used as the in-termediary to transfer motions [7, 18, 56]. However, land-mark suffers from the tightly coupling with the shape/size of facial parts. It cannot be used as the intermediary to per-form a precise motion transfer from the source human face to the target pareidolia face. The shapes of target faces will be affected by the source ones’ easily. Also, it is difﬁcult to deﬁne the meaning of landmarks’ annotation for com-plex shapes, e.g., the tree’s mouth in Fig. 1 (b). Thus, it is challenging to design a universal shape representation to transfer motion from human faces to pareidolia faces.
Texture variance means the textures of pareidolia faces are remarkably diverse, such as wood, downy and metal tex-tures as shown in Fig. 1 (a). Also, the texture distribution of pareidolia faces is extremely discrete, since there is even no two faces with a similar texture. For human face, pre-vious works always deployed 3D facial models [46, 24] or
GAN-based generator [18, 54] in texture synthesis. How-ever, there is no 3D face model that can be leveraged to model pareidolia face. Also, for the GAN-based synthesis, large-scale labeled datasets with landmark-image pairs are always needed to train a generator [32, 52]. But, there exists no dataset or annotation for pareidolia faces, which makes the strong supervision with paired data out of action for tex-ture synthesis. Thus, synthesizing the textures of pareidolia faces is challenging, without a 3D model or annotated data.
In this work, we propose a novel Parametric Unsuper-vised Reenactment Algorithm, to tackle the pareidolia face reenactment problem. First, to solve the shape variance challenge, we propose a Parametric Shape Modeling tech-nique, in which we introduce B´ezier Curve [9], a classic parametric technology in computer graphics, to represent the boundary shapes of facial parts of both the source and target faces with a set of control points. With the para-metric modeling of boundaries on target pareidolia face, control points of the B´ezier Curve can locally modify the curve while keeping its global structure unchanged, even with large shape variance.
With the robust shape representation, a na¨ıve solution to transfer motion is to directly adapt the human face’ control points to the pareidolia face. However, the transferred mo-tion so far only decides the movement of the facial bound-aries of the pareidolia face, which is a local motion and cannot be used to drive the whole face. Thus, we pro-pose an Expansionary Motion Transfer technique to get a global motion representation named motion ﬁeld for a nat-ural animation, in which a Motion Spread strategy is de-signed to propagate the transferred motion from boundary to the whole face and a First-order Motion Approximation strategy is designed to reﬁne the motion ﬁeld further.
While the motion has been successfully transferred, the next step is to use the motion ﬁeld to deduce an image with high-quality textures. Reviewing the challenge of texture variance, we propose an Unsupervised Texture Synthesizer to address it in an AutoEncoder framework with a carefully designed Feature Deforming Layer. High-quality textures can be synthesized successfully, while neither 3D model, nor large-scale face datasets with annotations are needed.
We summarize our contributions as follows: 1) We make the ﬁrst attempt to animate pareidolia faces by the facial motion derived from the human faces. 2) We propose a novel Parametric Unsupervised Reenactment Algorithm to tackle pareidolia face reenactment, with three crucial com-ponents, i.e., Parametric Shape Modeling, Expansionary
Motion Transfer and Unsupervised Texture Synthesizer. 3)
Extensive experiments present the superior performance of our method and the effectiveness of each component. 2.