Abstract
Temporal modeling still remains challenging for action recognition in videos. To mitigate this issue, this paper presents a new video architecture, termed as Temporal Dif-ference Network (TDN), with a focus on capturing multi-scale temporal information for efﬁcient action recognition.
The core of our TDN is to devise an efﬁcient temporal mod-ule (TDM) by explicitly leveraging a temporal difference operator, and systematically assess its effect on short-term and long-term motion modeling. To fully capture temporal information over the entire video, our TDN is established with a two-level difference modeling paradigm. Speciﬁcally, for local motion modeling, temporal difference over consec-utive frames is used to supply 2D CNNs with ﬁner motion pattern, while for global motion modeling, temporal differ-ence across segments is incorporated to capture long-range structure for motion feature excitation. TDN provides a sim-ple and principled temporal modeling framework and could be instantiated with the existing CNNs at a small extra com-putational cost. Our TDN presents a new state of the art on the Something-Something V1 & V2 datasets and is on par with the best performance on the Kinetics-400 dataset.
In addition, we conduct in-depth ablation studies and plot the visualization results of our TDN, hopefully providing in-sightful analysis on temporal difference modeling. We re-lease the code at https://github.com/MCG-NJU/TDN. 1.

Introduction
Deep neural networks have witnessed great progress for action recognition in videos [14, 29, 38, 31, 6, 26, 37]. Tem-poral modeling is crucial for capturing motion information in videos for action recognition, and this is usually achieved by two kinds of mechanisms in the current deep learning approaches. One common method is to use a two-stream network [29], where one stream is on RGB frames to ex-tract appearance information, and the other is to leverage optical ﬂow as an input to capture movement information.
This method turns out to be effective for improving action recognition accuracy but requires high computational con-56 54 52 50 48 46
)
% ( y c a r u c c
A
TDNEN
TDN16F
TDN8F
TEINetEN
TEINet16F
TSMEN
TEINet8F
TSM16F
TSM8F 30M 100M 150M
# Parameters
ECOEnLite
NL 13 D 44 0 50 100 150 250 200
FLOPs/Video (G) 300 350 400
Figure 1. Video classiﬁcation performance comparison on
Something-Something V1 [8] in terms of Top1 accuracy, computa-tional cost, and model size. Our proposed TDN achieves the best trade-off between accuracy and efﬁciency, when compared with previous methods such as NL I3D [40], ECO [46], TSM [19] and
TEINet [20]. sumption for optical ﬂow calculation. Another alternative approach is to use 3D convolutions [12, 31] or temporal convolutions [33, 41, 25] to implicitly learn motion features from RGB frames. However, 3D convolutions often lack speciﬁc consideration in the temporal dimension and might bring higher computational cost as well. Therefore, design-ing an effective temporal module of high motion modeling power and low computational consumption is still a chal-lenging problem for video recognition.
This paper aims to present a new temporal modeling mechanism by introducing a temporal difference based module (TDM). Temporal derivative (difference) is highly relevant with optical ﬂow [11], and has shown effectiveness in action recognition by using RGB difference as an ap-proximate motion representation [38, 43]. However, these approaches simply treat RGB difference as another video modality and train a different network to fuse with the RGB network. Instead, we aim to present a uniﬁed framework to capture appearance and motion information jointly, by generalizing the idea of temporal difference into a princi-pled and efﬁcient temporal module for end-to-end network 1895  
design.
In addition, we argue that both short-term and long-term temporal information are crucial for action recognition, in the sense that they are able to capture the distinctive and complementary properties of an action instance. Therefore, in our proposed temporal modeling mechanism, we present a unique two-level temporal modeling framework based on a holistic and sparse sampling strategy [38], termed as Tem-poral Difference Network (TDN). Speciﬁcally, in TDN, we consider two efﬁcient forms of TDMs for motion modeling at different scales. For local motion modeling, we present a light-weight and low-resolution difference module to sup-ply a single RGB with motion patterns via lateral connec-tions, while for long-range motion modeling, we propose a multi-scale and bidirectional difference module to cap-ture cross-segment variations for motion excitation. These two TDMs are systematically studied as modular building blocks for short-term and long-rang temporal structure ex-traction.
Our TDN provides a simple and general video-level mo-tion modeling framework and could be instantiated with existing CNNs at a small extra computational cost. To demonstrate the effectiveness of TDN, we implement it with
ResNets and perform experiments on two datasets: Kinet-ics and Something-Something. The evaluation results show that our TDN is able to yield a new state-of-the-art per-formance on both motion relevant Something-Something dataset and scene relevant Kinetics dataset, under the set-ting of using similar backbones. As shown in Figure 1, our best result is signiﬁcantly better than previous methods on the dataset of Something-Something V1. We also perform detailed ablation studies to demonstrate the importance of temporal difference operation and investigate the effect of a speciﬁc design of TDM. In summary, our main contribution lies in the following three aspects:
• We generalize the idea of RGB difference to devise an efﬁcient temporal difference module (TDM) for mo-tion modeling in videos and provide an alternative to 3D convolutions by systematically presenting effective and detailed module design.
• Our TDN presents a video-level motion modeling framework with the proposed temporal difference module, with a focus on capturing both short-term and long-term temporal structure for video recognition.
• Our TDN obtains the new state-of-the-art performance on the datasets of Kinetics and Something-Something under the setting of using the same backbones. We also perform in-depth ablation study on TDM to provide some insights on our temporal difference modeling. 2.