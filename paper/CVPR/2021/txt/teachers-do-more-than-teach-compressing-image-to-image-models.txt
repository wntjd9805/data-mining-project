Abstract
Generative Adversarial Networks (GANs) have achieved huge success in generating high-ﬁdelity images, however, they suffer from low efﬁciency due to tremendous compu-tational cost and bulky memory usage. Recent efforts on compression GANs show noticeable progress in obtaining smaller generators by sacriﬁcing image quality or involving a time-consuming searching process. In this work, we aim to address these issues by introducing a teacher network that provides a search space in which efﬁcient network ar-chitectures can be found, in addition to performing knowl-edge distillation. First, we revisit the search space of gener-ative models, introducing an inception-based residual block into generators. Second, to achieve target computation cost, we propose a one-step pruning algorithm that searches a student architecture from the teacher model and substan-tially reduces searching cost. It requires no ℓ1 sparsity regu-larization and its associated hyper-parameters, simplifying the training procedure. Finally, we propose to distill knowl-edge through maximizing feature similarity between teacher and student via an index named Global Kernel Alignment (GKA). Our compressed networks achieve similar or even better image ﬁdelity (FID, mIoU) than the original models with much-reduced computational cost, e.g., MACs. Code will be released at https://github.com/snap-research/CAT. 1.

Introduction
Generative adversarial networks (GANs), which syn-thesize images by adversarial training [21], have wit-nessed tremendous progress in generating high-quality, high-resolution, and photo-realistic images and videos [4, 33, 66]. In conditional setting [54], the generation process is controlled via additional input signals, such as segmen-tation information [7, 57, 59, 69, 70], class labels [81], and sketches [29, 83]. These techniques have seen applications in commercial image editing tools. However, due to their massive computation complexity and bulky size, applying generative models at scale is less practical, especially on resource-constrained platforms, where low memory foot-*Work done while at Snap Inc.
Figure 1: Performance comparison between our and exist-ing GAN compression techniques [11, 20, 36, 63, 68] on
CycleGAN [83] for Horse(cid:1)Zebra dataset. Smaller MACs indicates more efﬁcient models. Lower FID indicates mod-els can generate more realistic images. Our method (red star) achieves the state-of-the-art performance-efﬁciency trade-off as it has the lowest FID with the smallest MACs. print, power consumption, and real-time execution are as, and often more, important than performance [36].
To accelerate inference and save storage space for huge models without sacriﬁcing performance, previous works propose to compress models with techniques including weight pruning [24], channel slimming [43, 44], layer skip-ping [3, 71], patterned or block pruning [17, 35, 40, 42, 49, 50, 51, 52, 55, 56, 80, 82], and network quantiza-tion [12, 18, 30, 31, 32, 38, 73]. Speciﬁcally, these studies elaborate on compressing discriminative models for image classiﬁcation, detection, or segmentation tasks. The prob-lem of compressing generative models, on the other hand, is less investigated, despite that typical generators are bulky in memory usage and inefﬁcient during inference. Up till now, only a handful of attempts exist [20, 36, 63, 68], all of which degenerate the quality of synthetic images compared to the original model (Fig. 1). 13600
In this work, we focus on compressing image-to-image translation networks, such as CycleGAN [83] and Gau-GAN [57]. Existing compression method [36] obtains an ef-ﬁcient student model and employs two additional networks: teacher and supernet, where the former is for knowledge distillation and the latter for architecture search. However, we argue that the supernet is not necessary, as the teacher can play its role. Speciﬁcally, in our proposed framework, the teacher does more than teaching the student (i.e. knowl-edge distillation)—it plays a central role in all aspects of the framework through three key contributions: 1. We introduce a new network design that can be ap-plied to both encoder-decoder architectures such as
Pix2pix [29], and decoder-style networks such as Gau-GAN [57]. It serves as both the teacher network de-sign, and the architecture search space of the student. 2. We directly prune the trained teacher network using an efﬁcient, one-step technique that removes certain channels in its generators to achieve a target computa-tion budget, e.g., the number of Multiply-Accumulate
Operations (MACs). This reduces architecture search costs by at least 10, 000× than the state-of-the-art compression method for generative models. Further-more, our pruning method only involves one hyper-parameter, making its application straightforward. 3. We introduce a knowledge distillation technique based on the similarity between teacher and student mod-els’ feature spaces, which we call global kernel align-ment (GKA). GKA directly forces feature representa-tions from the two models to be similar, and avoids extra learnable layers [36] to match the different di-mensions of teacher and student feature spaces, which could otherwise lead to information leakage.
We name our method as CAT as we show teacher model can and should do Compression And Teaching (distillation) jointly, which we ﬁnd is beneﬁcial for ﬁnding generative networks with smaller MACs, using much lower compu-tational resource than prior work. More importantly, our compressed networks can achieve similar or even better per-formance than their original counterparts (Tab. 1). 2.