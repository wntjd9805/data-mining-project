Abstract
Emerging interests have been brought to recognize previously unseen objects given very few training exam-ples, known as few-shot object detection (FSOD). Recent researches demonstrate that good feature embedding is the key to reach favorable few-shot learning performance.
We observe object proposals with different Intersection-of-Union (IoU) scores are analogous to the intra-image aug-mentation used in contrastive visual representation learn-ing. And we exploit this analogy and incorporate super-vised contrastive learning to achieve more robust objects representations in FSOD. We present Few-Shot object de-tection via Contrastive proposals Encoding (FSCE), a sim-ple yet effective approach to learning contrastive-aware ob-ject proposal encodings that facilitate the classiﬁcation of detected objects. We notice the degradation of average pre-cision (AP) for rare objects mainly comes from misclassify-ing novel instances as confusable classes. And we ease the misclassiﬁcation issues by promoting instance level intra-class compactness and inter-class variance via our con-trastive proposal encoding loss (CPE loss). Our design outperforms current state-of-the-art works in any shot and all data splits, with up to +8.8% on standard benchmark
PASCAL VOC and +2.7% on challenging COCO bench-mark. Code is available at: https://github.com/
MegviiDetection/FSCE. 1.

Introduction
Development of modern convolutional neural networks (CNNs) [1, 2, 3] give rise to great advances in general object detection [4, 5, 6]. Deep detectors demand a large amount of annotated training data to saturate its performance [7, 8].
In few-shot learning scenarios, deep detectors suffer sev-erer over-ﬁtting and the gap between few-shot detection and general object detection is larger than the corresponding gap
∗Corresponding author: libanghuai@megvii.com
Figure 1. Conceptualization of our contrastive object proposals en-coding. We introduce a score function which measures the seman-tic similarity between region proposals. Positive proposals (x+) refer to region proposals from the same category or the same ob-ject. Negative proposals (x−) refer to proposals from different categories. We encourage the object encodings to have the prop-erty that score(f (x), f (x+)) >> score(f (x), f (x−)), such that our contrastively learned object proposals have smaller intra-class variance and larger inter-class difference in few-shot image classiﬁcation [9, 10, 11]. On the contrary, a child can rapidly comprehend new visual concepts and recognize objects from a newly learned category given very few examples. Closing such gap is therefore an important step towards more successful machine perception [12].
Precedented by few-shot image classiﬁcation, earlier at-tempts in few-shot object detection utilize meta-learning strategy [13, 14, 15]. Meta-learners are trained with an episode of individual tasks, meta-task samples from com-mon objects (base class) to pair with rare objects (novel class) to simulate few-shot detection tasks. Recently, the two-stage ﬁne-tune based approach (TFA) reveals more po-tential in improving few-shot detection. Baseline TFA [16] simply freeze all base class trained parameters and ﬁne-tune 7352
Figure 2. We ﬁnd in ﬁne-tuning based few-shot object detector, classiﬁcation is more error-prone than localization. In the ﬁne-tuning stage, RPN is able to make good enough foreground proposals for novel instances, hence novel objects are often accurately localized but mis-classiﬁed as confusable base classes. Here shows 20 top-scoring RPN proposals and example detection results from PASCAL VOC
Split 1, wherein bird, sofa and cow are novel categories. The left panel shows the pair-wise cosine similarity between the class prototypes learned in the bounding box classiﬁer. For example, the similarity between bus and bird is -0.10, but the similarity between cow and horse is 0.39. Our goal is to decrease the instance-level similarity between similar objects that are from different categories. only box classiﬁer and box regressor with novel data, yet outperforms previous meta-learners. MPSR [17] improves upon TFA by alleviating the scale bias inherent to few-shot dataset, but their positive reﬁnement branch demands man-ual selection, which is somewhat less neat. In this work, we observe and address the essential weakness of the ﬁne-tuning based approach – constantly mislabeling novel in-stances as confusable categories, and improve the few-shot detection performance to the new state-of-the-art (SOTA).
Object detection involves localization and classiﬁcation of
In few-shot detection, one might nat-appeared objects. urally conjecture the localization of novel objects is go-ing to under-perform its base categories counterpart, with the concern that rare objects would be deemed as back-ground [14, 13, 18]. However, based on our experiments with Faster R-CNN [4], the commonly adopted detector in few-shot detection, class-agonistic region proposal network (RPN) is able to make foreground proposals for novel in-stances, and the ﬁnal box regressor can localize novel in-stances quite accurately.
In comparison, as demonstrated in Figure 2, misclassifying detected novel instances as con-fusable base classes is indeed the main source of error. We visualize the pairwise cosine similarity between class proto-types [19, 20, 21] of a Faster R-CNN box classiﬁer trained with PASCAL VOC [22, 23]. The cosine similarity between prototypes from resembled categories can be 0.39, whereas the similarity between objects and background is on aver-age −0.21. In few-shot setting, the similarity between clus-ter centers can go as high as 0.59, e.g., between sheep and cow, bicycle and motorbike, making classiﬁcation for sim-ilar objects error-prone. We make a calculation upon base-line TFA, manually correcting misclassiﬁed yet accurately localized box predictions can increase novel class average precision (nAP) by over 20 points.
A common approach to learn well-separated decision boundary is to use a large margin classiﬁer [24], but with our trials, category-level positive-margin based classiﬁers does not work in this data-hunger setting [20, 25]. To learn instance-level discriminative feature representations, contrastive learning [26, 27] has demonstrated its effective-ness in tasks including recognition [28], identiﬁcation [29] and the recent successful self-supervised models [30, 31, 32, 33]. In supervised contrastive learning for image clas-siﬁcation [34], intra-image augmentations of images from the same class are used to enrich the positive example pairs.
We think region proposals with different Intersection-over-Union (IoU) for an object are naturally analogous to the intra-image augmentation cropping, as illustrated in Fig-ure 1. Therefore in this work, we explore to extend the supervised batch contrastive approach [34] to few-shot ob-ject detection. We believe the contrastively learned object representations aware of the intra-class compactness and the inter-class difference can ease the misclassiﬁcation of un-seen objects as similar categories.
We present Few-Shot object detection via Contrastive pro-posals Encoding (FSCE), a simple yet effective ﬁne-tune based approach for few-shot object detection. When trans-fer the base detector to few-shot novel data, we augment the primary Region-of-Interest (RoI) head with a contrastive branch, the contrastive branch measures the similarity be-tween object proposal encodings. A supervised contrastive objective with speciﬁc considerations for detection will be optimized to reduce the variance of object proposal em-beddings from the same category, while pushing different-category instances away from each other. The proposed 7353
contrastive objective, contrastive proposal encoding (CPE) loss, is employed to the original classiﬁcation and localiza-tion objective in a multi-task fashion. The end-to-end train-ing of our proposed method is identical to vanilla Faster
R-CNN.
To our best knowledge, we are the ﬁrst to bring contrastive learning into few-shot object detection. Our simple design sets the new state-of-the-art in any shot (1, 2, 3, 5, 10, and 30), with up to +8.8% on the standard PASCAL VOC benchmark and +2.7% on the challenging COCO bench-mark. 2.