Abstract ages are available.
Image extrapolation extends an input image beyond the originally-captured ﬁeld of view. Existing methods strug-gle to extrapolate images with salient objects in the fore-ground or are limited to very speciﬁc objects such as hu-mans, but tend to work well on indoor/outdoor scenes. We introduce OCONet (Object COmpletion Networks) to extrap-olate foreground objects, with an object completion network conditioned on its class. OCONet uses an encoder-decoder architecture trained with adversarial loss to predict the ob-ject’s texture as well as its extent, represented as a predicted signed-distance ﬁeld. An independent step extends the back-ground, and the object is composited on top using the pre-dicted mask. Both qualitative and quantitative results show that we improve on state-of-the-art image extrapolation re-sults for challenging examples. 1.

Introduction
Image extrapolation, which extends pixels beyond image borders, is an important technique for computational photog-raphy. It is related to image interpolation techniques such as [4,5,6], which also infer missing pixels, and allow users to change image dimensions/aspect ratios without changing the content of the original images. Extrapolation, however, is a much more challenging problem since there is much less in-formation available; while inpainting methods are given the entire boundary of the missing region, in image extrapolation we only know one border. This less constrained problem means the the method needs to extrapolate both textures and structures in a convincing manner.
Image extrapolation methods include both classical [5, 7, 8, 9, 10] and learning-based approaches [1, 3, 11, 12]. Classi-cal methods often use guide images, for example [13] ﬁnds similar images on the Internet and stitches them together to expand the input image. This method makes strong assump-tions, and is only applicable for pictures taken at locations such famous landmarks, where a large set of reference im-Learning-based approaches for image extrapolation have only recently emerged, notably including Boundless [1],
Wide-Context [3], Panorama Synthesis [11], and Pluralistic
Image Completion [12]. The success of generative adver-sarial networks (GAN’s) [14, 15] motivated these methods.
Although similar methods have existed for interpolation for several years, the difﬁculty of extrapolation required more specialized and more powerful generative models.
Despite the recent progress in image extrapolation by methods such as [1,3] on textural images, the problem is still far from being solved for objects. While domain-speciﬁc image interpolation exists for a few important classes (e.g. for people [16]), the generic problem for images with salient objects remains unsolved.
The complexity of natural scene composition makes it challenging for a generic encoder-decoder network trained with adversarial losses to uncover the diverse shapes and
ﬁnal details of foreground object shapes given an input. It is easier to model the shape and appearances of each object class independently, e.g. cars, airplanes, people and dogs, as suggested by [15].
In this paper we introduce OCONet (Object COmple-tion Networks) to address the image extrapolation problem for a broad set of images with general object classes. Re-cent advances in high-quality instance segmentation, e.g.
ShapeMask [17], allow us to obtain object class and accu-rate foreground object shape masks even when only a small fraction of the object is visible inside the image boundary.
Using this information, we trained a class-conditioned ob-ject model to infer both the shape and pixels of foreground objects, as well as a background model to extrapolate the background. The completed object is simply composited on top of the extrapolated background to obtain the ﬁnal result. As shown in ﬁgure 1, we produce signiﬁcantly better results on the object of interest. Extensive quantitative and qualitative experiments show that our model signiﬁcantly outperforms the prior state-of-the-art.
To summarize, our contributions are as follows:
†Work performed while author was at Google Research
• We introduce object completion networks – OCONet– 2307
Input
Ours
BL
SSSD
WC
GT
Figure 1: Examples of our method on 4 different object categories: cars, trains, dogs, and apples. Comparisons include
BL=Boundless [1], SSSD=Self-Supervised Scene De-Occlusion [2] , WC=Wide Context [3], GT=Ground Truth. which complete a single object independent from the rest of the extrapolation problem.
• We show that the sign-distance ﬁeld (SDF) is effective as an internal representation of the segmentation mask for 2D shape completion (extrapolating the mask).
• We demonstrate substantially improved quantitative and qualitative extrapolation results for a number of impor-tant object classes on OpenImages [18]. 2.