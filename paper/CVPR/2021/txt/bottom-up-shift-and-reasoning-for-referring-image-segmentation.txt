Abstract
Referring image segmentation aims to segment the ref-erent that is the corresponding object or stuff referred by a natural language expression in an image. Its main chal-lenge lies in how to effectively and efﬁciently differentiate between the referent and other objects of the same category as the referent. In this paper, we tackle the challenge by jointly performing compositional visual reasoning and ac-curate segmentation in a single stage via the proposed novel
Bottom-Up Shift (BUS) and Bidirectional Attentive Reﬁne-ment (BIAR) modules. Speciﬁcally, BUS progressively lo-cates the referent along hierarchical reasoning steps im-plied by the expression. At each step, it locates the corre-sponding visual region by disambiguating between similar regions, where the disambiguation bases on the relation-ships between regions. By the explainable visual reasoning,
BUS explicitly aligns linguistic components with visual re-gions so that it can identify all the mentioned entities in the expression. BIAR fuses multi-level features via a two-way attentive message passing, which captures the visual details relevant to the referent to reﬁne segmentation re-sults. Experimental results demonstrate that the proposed method consisting of BUS and BIAR modules, can not only consistently surpass all existing state-of-the-art algorithms across common benchmark datasets but also visualize inter-pretable reasoning steps for stepwise segmentation. Code is available at https://github.com/incredibleXM/BUSNet. 1.

Introduction
The intersection of vision and language has attracted growing interests in academia, where many methods [1, 3, 25] have been proposed to promote a better understanding
∗Equal contribution.
†Corresponding authors.
This work was partially supported by National Key Research and Development Program of China (No.2020YFC2003902), National Natural Science Foundation of China (No.61976250 and No.U1811463), and the Guangdong Basic and Applied
Basic Research Foundation (No.2020B1515020048).
Figure 1. The Bottom-Up Shift (BUS) for referring image segmen-tation. BUS performs stepwise visual reasoning from the entity
“broccolis” to “female” to “person”. At each step, it ﬁrst identi-ﬁes the objects corresponding to the entity and then differentiates between the identiﬁed objects by the relational reasoning. of these two modalities. Existing vision-and-language ap-proaches can be roughly divided into two types based on their designing principles, i.e., multimodal fusion and rep-resentation learning, and language-conditioned visual rea-soning. In contrast to the former which focuses more on how to learn joint representations from multiple modalities, the latter reasoning based approaches usually are not only more effective in complex scenes but also can provide an explainable decision-making process.
However, as one of the most fundamental vision-and-language tasks, Referring Image Segmentation (RIS) [8] has not been well addressed in previous research works from the second perspective (i.e., reasoning). Existing vi-sual reasoning based methods [19, 46] for RIS mainly re-sort to a two-stage pipeline, where they ﬁrst detect and segment the object instances and then perform reasoning over feature vectors of both object instances and their rela-tionships. However, the two-stage solution inevitably faces the problems of slow inference speed and has poor gen-eralization [23]. What is worse, the relational and spa-tial priors in images that are essential for visual reasoning are lost when conducting reasoning over feature vectors of those object instances. On the other hand, most existing works [9, 10, 15] on RIS mainly focus on learning multi-modal contextual representations in a single stage. Gener-ally, one-stage RIS methods have fast inference speed but are inferior in handling complex visual scenes and expres-11266
sions because they lack sufﬁcient visual reasoning capabil-ity [23]. For example (see Figure 1), without visual rea-soning, the model can not distinguish the referred “person” from others in the image.
In this paper, we aim to empower the one-stage RIS with the ability to conduct visual reasoning and take advantages of both one-stage and two-stage methods. The two-stage methods rely on explicit object instances and their relation-ships to conduct visual reasoning; however, there is no ex-plicit object-level information in one-stage RIS. Therefore, we propose that capturing visual scenes’ constituents and their relationships is the key to perform visual reasoning in one-stage RIS. In Figure 1(a), given the linguistic struc-ture (“female”-“holding”-“broccolis”) of the referring ex-pression (“A female is holding broccolis”), we can ﬁrst align visual regions A and B with the nouns “female” and “broc-colis” respectively, and then shift region A to A1 by consid-ering its relationship “holding” to region B. By the process, the referred “female” is located with interpretable reasoning steps. Moreover, we can perform bottom-up shift and rea-soning to identify the referent hierarchically for complex expressions. As shown in Figure 1(b), we further segment the referred “person” by the following two steps. First, we
ﬁnd region C with respect to the noun “person”. Then, we shift region C to region C1 by considering its relationships
“stands beside” to the identiﬁed region A1. Also, we can reﬁne the visual region B by considering its inverse rela-tionship “be held” with A1. In addition to ﬁnding the refer-ent, bidirectional shifts for a pair of relationship and inverse relationship help to segment other mentioned objects.
To realize the above concepts and operations, we pro-pose a Bottom-Up Shift (BUS) module to introduce visual reasoning to one-stage RIS. Speciﬁcally, BUS ﬁrst parses the expression as a language graph and then analyzes hier-archical reasoning steps from the graph.
In the language graph, each node and directed edge represent a speciﬁc noun phrase and the type of semantic relationship from the object node to the subject node, respectively. Then, BUS conducts bottom-up visual reasoning on the entire image following the reasoning steps. Particularly, we decompose the compositional visual reasoning process into pairwise re-lational shifts on edges and integration on nodes. The pair-wise relational shift performs visual reasoning for a single edge by passing messages between its two nodes according to the type of this edge, where relationship-based convolu-tional operations implement the message passing.
Moreover, how to accurately segment the referent from a coarse localization also plays a vital role in RIS. Previous works [9, 10, 15] usually incorporate multi-level features to reﬁne the details of segmentation results. However, these approaches either neglect the low-level visual details or cap-ture incomplete interactions between multiple levels via a one-way fusion. In this paper, we propose a Bidirectional
Attentive Reﬁnement (BIAR) module to integrate low-level visual features and high-level semantic ones. Speciﬁcally, the top-down branch is responsible for capturing semantic-related visual details, while the bottom-up pathway helps to equip multi-level semantic features with the captured de-tails. However, directly incorporating the low-level visual features into high-level semantic ones may bring irrelevant noise, because low-level visual features contain visual de-tails of the entire image. Thus, we propose an attention mechanism to incorporate the details relevant to the refer-ent selectively.
In summary, this paper has following contributions:
• A Bottom-Up Shift (BUS) module is proposed to em-power one-stage referring image segmentation with the ability to perform explainable visual reasoning. The
BUS can not only distinguish the referent from other objects of the same category as the referent but also segment other mentioned entities in the expression.
• A Bidirectional Attentive Reﬁnement (BIAR) module is proposed to segment the referent from a coarse lo-calization accurately. BIAR integrates low-level visual features and high-level semantic ones via a two-way attentive message passing, which improves the seg-mentation accuracy.
• BUS and BIAR are integrated into a Bottom-Up Shift and Reasoning Network (BUSNet). Experimental re-sults demonstrate that BUSNet not only outperforms existing state-of-the-art methods and achieves signiﬁ-cant performance gains over referring expression rea-soning models, but also generates interpretable visual-izations for stepwise reasoning and segmentation. 2.