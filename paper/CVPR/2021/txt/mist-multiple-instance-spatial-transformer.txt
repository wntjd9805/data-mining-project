Abstract
We propose a deep network that can be trained to tackle image reconstruction and classiﬁcation problems that in-volve detection of multiple object instances, without any su-pervision regarding their whereabouts. The network learns to extract the most signiﬁcant K patches, and feeds these patches to a task-speciﬁc network – e.g., auto-encoder or classiﬁer – to solve a domain speciﬁc problem. The chal-lenge in training such a network is the non-differentiable top-K selection process. To address this issue, we lift the training optimization problem by treating the result of top-K selection as a slack variable, resulting in a simple, yet ef-fective, multi-stage training. Our method is able to learn to detect recurring structures in the training dataset by learning to reconstruct images. It can also learn to localize struc-tures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state-of-the-art.
Code available at https://github.com/ubc-vision/mist 1.

Introduction
Finding and processing multiple instances of character-istic entities in a scene is core to many computer vision applications, including object detection [42, 20, 41], pedes-trian detection [11, 46, 60], and keypoint localization [34, 2].
In traditional pipelines, it is common to localize entities by selecting the top-K responses in a heatmap and use their lo-cations [34, 2, 14]. However, this type of approach does not provide a gradient with respect to the heatmap, and cannot be directly integrated into neural network-based systems.
To overcome this challenge, previous work proposed to use grids [40, 20, 10] to simplify the formulation by isolating each instance [58], or to optimize over multiple branches [38]. While effective, these approaches require additional supervision to localize instances, and do not gen-eralize well outside their intended application domain. Other formulations, such as sequential attention [1, 18, 12] and channel-wise approaches [62] are problematic to apply when the number of instances of the same object is large, as we show later through experiments.
Here, we introduce a novel way to tackle this problem, which we term Multiple Instance Spatial Transformer, or
MIST for brevity. As illustrated in Figure 1 for the image syn-thesis task, given an image, we ﬁrst compute a heatmap via a deep network whose local maxima correspond to locations of interest. From this heatmap, we gather the parameters of the top-K local maxima, and then extract the corresponding collection of image patches via an image sampling process.
We process each patch independently with a task-speciﬁc network, e.g., an image decoder, and aggregate the network’s output across patches.
Training a pipeline that includes a non-differentiable se-lection/gather operation is non-trivial. Thus, we propose to lift the problem to a higher dimensional one by treating the parameters deﬁning the interest points as slack variables, and introduce a hard constraint that they must correspond to the output of the heatmap network. This constraint is realized by introducing an auxiliary function that generates a heatmap given a set of interest point parameters. We then solve for the relaxed version of this problem, where the hard constraint is turned into a soft one, and the slack variables are also optimized within the training process. Critically, our training strategy allows the network to incorporate both non-maximum suppression and top-K selection. We evaluate the performance of our approach for 1� recovering the basis functions that created a given texture, 2� detection and clas-siﬁcation of handwritten digits in cluttered scenes, and 3� object detection on natural images, all without any location supervision.
Contributions In summary, in this paper we:
• propose an end-to-end training method that allows the use of top-K selection;
• show that our framework can reconstruct images as parts, as well as detect/classify instances without any location supervision;
• outperform the state of the art in various scenarios, includ-ing on natural images. 2412
2.