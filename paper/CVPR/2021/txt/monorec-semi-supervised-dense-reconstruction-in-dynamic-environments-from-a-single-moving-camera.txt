Abstract
In this paper, we propose MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic en-vironments. MonoRec is based on a multi-view stereo set-ting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts mov-ing object masks by leveraging the photometric inconsisten-cies encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is able to reconstruct both static and moving objects by leveraging the predicted masks. Fur-thermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not re-quire LiDAR depth values. We carefully evaluate MonoRec on the KITTI dataset and show that it achieves state-of-the-art performance compared to both multi-view and single-view methods. With the model trained on KITTI, we further-more demonstrate that MonoRec is able to generalize well to both the Oxford RobotCar dataset and the more chal-lenging TUM-Mono dataset recorded by a handheld cam-era. Code and related materials are available at https:
//vision.in.tum.de/research/monorec. 1.

Introduction 1.1. Real world Scene Capture from Video
Obtaining a 3D understanding of the entire static and dy-namic environment can be seen as one of the key-challenges in robotics, AR/VR, and autonomous driving. State of to-day, this is achieved based on the fusion of multiple sen-sor sources (incl. cameras, LiDARs, RADARs and IMUs).
This guarantees dense coverage of the vehicle’s surround-ings and accurate ego-motion estimation. However, driven by the high cost as well as the challenge to maintain cross-calibration of such a complex sensor suite, there is an in-⋆ Indicates equal contribution.
Figure 1: MonoRec can deliver high-quality dense recon-struction from a single moving camera. The ﬁgure shows an example of a large-scale outdoor point cloud reconstruc-tion (KITTI Odometry sequence 07) by simply accumulat-ing predicted depth maps. Please refer to our project page for the video of the entire reconstruction of the sequence. creasing demand of reducing the total number of sensors.
Over the past years, researchers have therefore put a lot of effort into solving the problem of perception with only a sin-gle monocular camera. Considering recent achievements in monocular visual odometry (VO) [8, 58, 51], with respect to ego-motion estimation, this was certainly successful. Nev-ertheless, reliable dense 3D mapping of the static environ-ment and moving objects is still an open research topic.
To tackle the problem of dense 3D reconstruction based on a single moving camera, there are basically two paral-6112
lel lines of research. On one side, there are dense multi-view stereo (MVS) methods, which evolved over the last decade [39, 45, 2] and saw a great improvement through the use of convolutional neural networks (CNNs) [23, 61, 57].
On the other side, there are monocular depth prediction methods which purely rely on deep learning [7, 16, 58].
Though all these methods show impressive performance, both types have also their respective shortcomings. For
MVS the overall assumption is a stationary environment to be reconstructed, so the presence of dynamic objects deteri-orate their performance. Monocular depth prediction meth-ods, in contrast, perform very well in reconstructing mov-ing objects, as predictions are made only based on individ-ual images. At the same time, due to their use of a single image only, they strongly rely on the perspective appear-ance of objects as observed with speciﬁc camera intrinsics and extrinsics and therefore do not generalize well to other datasets. 1.2. Contribution
To combine the advantage of both deep MVS and monocular depth prediction, we propose MonoRec, a novel monocular dense reconstruction architecture that consists of a MaskModule and a DepthModule. We encode the infor-mation from multiple consecutive images using cost vol-umes which are constructed based on structural similarity index measure (SSIM) [54] instead of sum of absolute dif-ferences (SAD) like prior works. The MaskModule is able to identify moving pixels and downweights the correspond-ing voxels in the cost volume. Thereby, in contrast to other
MVS methods, MonoRec does not suffer from artifacts on moving objects and therefore delivers depth estimations on both static and dynamic objects.
With the proposed multi-stage training scheme,
MonoRec achieves state-of-the-art performance compared to other MVS and monocular depth prediction methods on the KITTI dataset [14]. Furthermore, we validate the generalization capabilities of our network on the Oxford
RobotCar dataset [35] and the TUM-Mono dataset [9].
Figure 1 shows a dense point cloud reconstructed by our method on one of our test sequences of KITTI. 2.