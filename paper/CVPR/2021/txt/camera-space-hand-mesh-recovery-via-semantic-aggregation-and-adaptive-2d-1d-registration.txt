Abstract
Recent years have witnessed signiﬁcant progress in 3D hand mesh recovery. Nevertheless, because of the intrinsic 2D-to-3D ambiguity, recovering camera-space 3D informa-tion from a single RGB image remains challenging. To tackle this problem, we divide camera-space mesh recovery into two sub-tasks, i.e., root-relative mesh recovery and root recovery. First, joint landmarks and silhouette are extracted from a single input image to provide 2D cues for the 3D tasks. In the root-relative mesh recovery task, we exploit semantic relations among joints to generate a 3D mesh from the extracted 2D cues. Such generated 3D mesh coordinates are expressed relative to a root position, i.e., wrist of the hand. In the root recovery task, the root position is registered to the camera space by aligning the generated 3D mesh back to 2D cues, thereby completing camera-space 3D mesh recovery. Our pipeline is novel in that (1) it explicitly makes use of known semantic relations among joints and (2) it exploits 1D projections of the silhouette and mesh to achieve robust registration. Extensive exper-iments on popular datasets such as FreiHAND, RHD, and
Human3.6M demonstrate that our approach achieves state-of-the-art performance on both root-relative mesh recovery and root recovery. Our code is publicly available at https://github.com/SeanChenxy/HandMesh. 1.

Introduction
Monocular 3D mesh recovery has attracted tremendous attention due to its extensive applications in AR/VR, hu-man–machine interaction, etc. The task is to estimate 3D locations of mesh vertices from a single RGB image. It is particularly challenging owing to highly articulated struc-tures, 2D-to-3D ambiguity, and self-occlusion. Signiﬁcant efforts have been made recently for accurate 2D-to-3D
*Corresponding author, chenxingyu@kuaishou.com x z y x z y x z y x z y x z y
Figure 1. Qualitative results of the proposed CMR. We show silhouette, 2D pose, projection of mesh, side-view mesh, camera-space mesh and pose (unit: meter). The red rectangles indicate the camera. Our method is robust enough to handle cases of occlusion, truncation, and challenging poses. reconstruction, including [16, 17, 20, 21, 23, 27, 30, 31, 32, 33, 36, 39], to name a few.
Most of the aforementioned methods [6, 9, 16, 20, 21, 22, 27, 40, 43] have difﬁculty in predicting absolute camera-Instead, they deﬁne a root (i.e., wrist space coordinates. of the hand) and estimate root-relative coordinates of the 3D mesh. In this aspect, these methods cannot be applied to many high-level tasks, e.g., hand-object interaction, that requires camera-space mesh information. To this end, we propose to jointly solve root-relative mesh recovery and root recovery by integrating these two sub-tasks into a uniﬁed framework, thereby bridging the gap between root-relative predictions and camera-space estimation.
RGB images consist of 2D patterns that are indirect cues of the underlying 3D structure. Therefore, 2D cues have long been leveraged to assist 3D tasks. For example, 2D 13274
pose and silhouette have been used to facilitate 3D pose regression [9, 20, 27, 33, 42, 43, 44].
However, the relationship between 2D cues and 3D structure remains unclear. We observe that 2D joint land-marks together with their semantic relations describe the 2D pose, while the silhouette indicates the holistic 3D-to-2D projection of the hand. They have different 2D properties and should be treated in different manners in the 3D task. Inspired by these observations, we set to explore the following aspects of the 2D-to-3D task: (1) different roles of 2D cues, (2) the reason for their different effects, and (3) how to construct more effective 2D cues.
In this paper, we propose a camera-space mesh recovery (CMR) framework to integrate the tasks of 3D hand mesh and root recovery into a uniﬁed framework. CMR consists of three phases, i.e., 2D cue extraction, 3D mesh recovery, and global mesh registration. For 2D cue extraction, we predict joint landmarks and silhouette from a single RGB image. For mesh recovery, we introduce an Inception
Spiral Module for robust 3D decoding. Moreover, an ag-gregation method is designed for composing more effective 2D cues. Speciﬁcally, instead of implicitly learning the relations among joints, we exploit their known relations by aggregating landmark heatmaps in groups, which proves to be effective for the subsequent 3D task. Finally, camera-space root location is obtained by a global mesh registration step that aligns the generated 3D mesh with the extracted 2D landmarks and silhouette. This step is carried out via an adaptive 2D-1D registration method that achieves robustness by leveraging matching objectives in different dimensions. Our full pipeline surpasses state-of-the-art methods in the 3D mesh and root recovery tasks. While our approach is mainly described for hand mesh, it can be readily applied to full-body mesh as shown in the experiments. Figure 1 demonstrates several example results of our CMR for camera-space mesh recovery.
Our main contributions are summarized as follows:
• We propose a novel aggregation method to collect effec-tive 2D cues and exploit high-level semantic relations for root-relative mesh recovery.
• We design an adaptive 2D-1D registration method to sufﬁciently leverage both joint landmarks and silhouette in different dimensions for robust root recovery.
• We present a uniﬁed pipeline CMR for camera-space mesh recovery and demonstrate state-of-the-art perfor-mance on both mesh and root recovery tasks via extensive experiments on FreiHAND, RHD, and Human3.6M. 2.