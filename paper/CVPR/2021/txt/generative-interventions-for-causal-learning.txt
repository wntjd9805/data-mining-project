Abstract
We introduce a framework for learning robust visual representations that generalize to new viewpoints, back-grounds, and scene contexts. Discriminative models of-ten learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distri-bution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and the-oretical results show this method learns robust representa-tions more consistent with the underlying causal relation-ships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset. 1.

Introduction
Visual recognition today is governed by empirical risk minimization (ERM), which bounds the generalization er-ror when the training and testing distributions match [47].
When training sets cover all factors of variation, such as background context or camera viewpoints, discriminative models learn invariances and predict object category labels with the right cause [33]. However, the visual world is vast and naturally open. Collecting a representative, balanced dataset is difﬁcult and, in some cases, impossible because the world can unpredictably change after learning.
Directly optimizing the empirical risk is prone to learn-ing unstable spurious correlations that do not respect the underlying causal structure [11, 8, 24, 44, 4, 35]. Figure 1 illustrates the issue succinctly. In natural images, the ob-ject of interest and the scene context have confounding fac-tors, creating spurious correlations. For example, ladle (the object of interest) often has a hand holding it (the scene context), but there is no causal relation between them. Sev-eral studies have exposed this challenge by demonstrating substantial performance degradation when the confounding bias no longer holds at testing time [41, 19]. For example,
Ladle
Shovel
Television
Figure 1. Top predictions from a state-of-the-art ImageNet classi-ﬁer [21]. The model uses spurious correlations (scene contexts, viewpoints, and backgrounds), leading to incorrect predictions.1
In this paper, we introduce a method to learn causal visual fea-tures that improve robustness of visual recognition models. The predictions of our model are in Figure 7. the ObjectNet [6] dataset removes several common spuri-ous correlations from the test set, causing the performance of state-of-the-art models to deteriorate by 40% compared to the ImageNet validation set.
A promising direction for fortifying visual recognition is to learn causal representations (see [43] for an excellent overview). If representations are able to identify the causal mechanism between the image features and the category la-bels, then robust generalization is possible. While the tradi-tional approach to establish causality is through randomized control trials or interventions, natural images are passively collected, preventing the use of such procedures.
This paper introduces a framework for learning causal visual representations with natural images. Our approach is based on the observation that generative models quan-tify nuisance variables [23, 26], such as viewpoint or back-ground. We present a causal graph that models both ro-bust features and spurious features during image recogni-tion. Crucially, our formulation shows how to learn causal features by steering generative models to perform interven-tions on realistic images, simulating manipulations to the camera and scene that remove spurious correlations. As our approach is model-agnostic, we are able to learn robust rep-resentations for any state-of-the-art computer vision model.
Our empirical and theoretical results show that our ap-*Equal Contribution. Order by coin ﬂip. 1The correct categories are clearly a broom, a tray, and a shoe. 3947
Rotation  (Principal Component 5)
Scale  (Principal Component 10)