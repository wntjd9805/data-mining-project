Abstract
Generating and representing human behavior are of ma-jor importance for various computer vision applications.
Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the de-picted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In con-trast, controlled behavior synthesis and transfer across indi-viduals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of speciﬁc postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics inde-pendent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a condi-tional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, trans-ferring, and sampling ﬁne-grained, diverse behavior, both quantitatively and qualitatively. Project page is available at https://cutt.ly/5l7rXEp 1.

Introduction
Understanding human appearance, posture and behav-ior are key problems of computer vision with numerous applications in autonomous driving [39, 41, 23], surveil-lance [12, 50, 60], medical treatment [6, 54] and be-yond. While there has been major progress on represen-tation [59, 48] and - with the advent of deep generative mod-els [34, 24] - synthesis [7, 30] and manipulation [20, 13, 17] of posture and appearance, the understanding of representa-tion and synthesis of behavior is an open problem.
Human motor behavior is deﬁned by the distinct dynam-ics of our limbs and the entire body. Take for example a person raising their arm. This is fully determined by the upward movement of the arm. Since the remaining body
*Indicates equal contribution. posture is mostly unaffected, the behavior can be directly performed independently of a particular initial body conﬁg-uration such as a sitting or standing posture (cf. Fig. 1).
Moreover, rather complex behavior like running involves an interplay between certain body limbs, e.g. arms swinging synchronously with the movement of legs, and, thus, is natu-rally limited to certain postures to start with. To nevertheless enact such behavior from arbitrary starting poses, ﬁrst a tran-sition to ﬁtting initial body conﬁgurations may be required -for instance, a sitting person needs to stand up before being able to walk. Finally, speciﬁc body features like size or build do not affect the ability to perform a walking behavior.
While behavior is eventually instantiated as a sequence of individual postures that can be observed in a video, this would be a suboptimal representation: We want the overall behavior to be the same, e.g. raising arm or walking, re-gardless of the initial posture it starts with. Although we are looking at different realizations it should still be represented as being the same behavior. Consequently, understanding, controlling, and synthesizing behavior calls for separate dis-entangled representations of the characteristic behavior and of individual (in particular the initial) posture. In contrast, present work on human motion synthesis typically represents behavior directly by means of the observed sequence of pos-tures [3, 40, 63, 57]. Thus, as no explicit understanding and representation of behavior is developed, synthesizing human behavior has been limited to only changing person appear-ance [57, 9, 56] or forecasting the most likely continuation of the depicted posture sequence [3, 40, 63, 11]. However, controlling such sequences, e.g. to re-enact a novel behavior by an observed person, asks for a posture independent repre-sentation which captures only the behavior dynamics to be transferred. Moreover, instantiating the re-enacted behavior requires combining these dynamics with the, potentially sig-niﬁcantly different, posture of the target person.
In this paper we propose a conditional variational generative model for controlled human behavior synthesis which only requires a collection of sequences without any class labels provided. Our models learns to understand the characteristic motor dynamics of behavior, which enables us to transfer behavior between videos. We learn a dedicated representa-12236
Figure 1. Our Approach for Behavior Transfer. Given a source sequence of human dynamics our model infers a behavior encoding which is independent of posture. We can re-enact the behavior by combining it with an unrelated target posture and thus control the synthesis process.
The resulting sequence is combined with an appearance to synthesize a video sequence. tion extracting these dynamics from pose sequences while factorizing out posture information. To this end, we propose an explicit disentanglement framework for behavior and pos-ture based on an alternating optimization procedure while simultaneously controlling the information ﬂow through our model. In particular, the explicit disentanglement allows our model to re-enact extracted behavior from arbitrary tar-get postures and, if needed, to infer required corresponding transitions itself. Our experiments demonstrate qualitatively and quantitatively that our model meaningfully transfers be-havior between sequences and is also able to sample novel and diverse behavior. Quantitative comparison against cur-rent approaches for human motion synthesis conﬁrms the competitive performance of our approach. 2.