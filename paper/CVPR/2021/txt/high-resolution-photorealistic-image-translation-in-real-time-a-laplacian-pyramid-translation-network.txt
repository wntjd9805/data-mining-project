Abstract
Existing image-to-image translation (I2IT) methods are either constrained to low-resolution images or long infer-ence time due to their heavy computational burden on the convolution of high-resolution feature maps.
In this pa-per, we focus on speeding-up the high-resolution photore-alistic I2IT tasks based on closed-form Laplacian pyramid decomposition and reconstruction. Speciﬁcally, we reveal that the attribute transformations, such as illumination and color manipulation, relate more to the low-frequency com-ponent, while the content details can be adaptively reﬁned on high-frequency components. We consequently propose a Laplacian Pyramid Translation Network (LPTN) to si-multaneously perform these two tasks, where we design a lightweight network for translating the low-frequency com-ponent with reduced resolution and a progressive masking strategy to efﬁciently reﬁne the high-frequency ones. Our model avoids most of the heavy computation consumed by processing high-resolution feature maps and faithfully pre-serves the image details. Extensive experimental results on various tasks demonstrate that the proposed method can translate 4K images in real-time using one normal GPU while achieving comparable transformation performance against existing methods. Datasets and codes are available: https://github.com/csjliang/LPTN. 1.

Introduction
Image-to-image translation (I2IT,
[11, 26, 31]), which aims to translate images from a source domain to a target one, has gained signiﬁcant attention. Recently, photore-alistic I2IT has been attracting increasing interest in vari-ous practical tasks, e.g., transferring images among differ-ent daytimes or seasons [11] or retouching the illumination and color of images to improve their aesthetic quality [4].
∗Equal contribution.
†Corresponding author. This work is supported by the Hong Kong RGC
RIF grant (R5001-18). (a) Original Images, MSE=7853.9 (b) High Frequencies, Level=1, MSE=97.5 (c) High Frequencies, Level=2, MSE=107.7 (d) Low Frequencies, Level=3, MSE=6969.4
Figure 1. (a) Images of a scene captured at different daytimes and (b∼d) the Laplacian pyramids (ﬁgures in (c∼d) are resized for better visualization). As shown by the MSE and the Histograms, the differences between the day and night images are dominated in the low-frequency components (d).
Different from the general I2IT problem, the key challenge of the practical photorealistic I2IT task is to keep efﬁciency and avoid content distortions when handling high-resolution images.
To achieve faithful translations, most traditional meth-ods [16, 29, 33] employ an encoding-decoding paradigm which maps the input image into a low-dimensional latent space, followed by reconstructing the output from a trans-lated latent code. However, these methods are naturally lim-19392
ited to low-resolution applications or time-consuming in-ference models [16, 19, 21, 25, 29, 33], which is far from practical. The main reason is that the model needs to ma-nipulate the image globally using deep networks, yet di-rectly convolving a high-resolution image with sufﬁcient channels and large kernels demands heavy computational cost. There are some developments in pruning and boost-ing the inference models [13, 17, 20], yet a shallow network can hardly fulﬁll the requirements of reconstructing com-plex content details from a low-dimensional latent space to a high-resolution image. To generate a photorealistic translation, recent researches [10, 14, 15] have also been fo-cusing on disentangling the contents and attributes of both domains in a data-driven manner. Nevertheless, the irre-versible down- and up-sampling operations in these models still involves heavy convolutions on high-resolution feature maps, sacriﬁcing the efﬁciency of the inference model.
Inspired by the reversible and closed-form frequency-band decomposition framework of a Laplacian pyramid (LP, [1]), we reveal that the domain-speciﬁc attributes, e.g., illuminations or colors, of a photorealistic I2IT task are mainly exhibited on the low-frequency component. In con-trast, the content details relate more to higher-frequency components, which can be adaptively reﬁned according to the transformation of the visual attributes. As shown in Fig-ure 1, for a pair of images with the same scene yet captured at different daytimes, the mean squared errors (MSE) be-tween the high-frequency components (b-c) of the two do-mains are much smaller (about 1/71 and 1/65) than that be-tween the low-frequency components (d). Similar ﬁndings can be observed from the histograms and visual appearance.
Figure 1 (b-c) also demonstrate that the higher-frequency subimages are with tapering resolutions, while different lev-els show pixel-wise correlations and exhibit similar tex-tures. Such properties allow an efﬁcient masking strategy for adjusting the content details accordingly.
Based on the above observations, in this paper, we pro-pose a fast yet effective method termed the Laplacian Pyra-mid Translation Network (LPTN) to improve efﬁciency while keeping the transformation performance for photo-realistic I2IT tasks.
In speciﬁc, we build a lightweight network with cascaded residual blocks on top of the low-frequency component to translate the domain-speciﬁc at-tributes. To ﬁt the manipulation of the low-frequency com-ponent and reconstruct the image from an LP faithfully, we reﬁne the high-frequency components adaptively yet avoid heavy convolutions on high-resolution feature maps to im-prove the efﬁciency. Therefore, we build another tiny net-work to calculate a mask on the smallest high-frequency component of the LP and then progressively upsample it to
ﬁt the others. The framework is trained end-to-end in an unsupervised manner via adversarial training strategy.
The proposed method offers multiple advantages.
Firstly, we are the ﬁrst to enable photorealistic I2IT on 4K resolution images in real-time. Secondly, given the lightweight and fast inference model, we still achieve com-parable or superior performance on photorealistic I2IT ap-plications in terms of transformation capacity and photore-alism. Both qualitative and quantitative results demonstrate that the proposed method performs favorably against state-of-the-art methods. 2.