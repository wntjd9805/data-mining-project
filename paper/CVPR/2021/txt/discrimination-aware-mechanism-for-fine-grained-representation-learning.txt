Abstract
Feature Embedding
Original Feature Space
Recently, with the emergence of retrieval requirements for certain individual in the same superclass, e.g., birds, persons, cars, ﬁne-grained recognition task has attracted a signiﬁcant amount of attention from academia and industry.
In ﬁne-grained recognition scenario, the inter-class differ-ences are quite diverse and subtle, which makes it challeng-ing to extract all the discriminative cues. Traditional train-ing mechanism optimizes the overall discriminativeness of the whole feature. It may stop early when some feature ele-ments has been trained to distinguish training samples well, leaving other elements insufﬁciently trained for a feature.
This would result in a less generalizable feature extractor that only captures major discriminative cues and ignores subtle ones. Therefore, there is a need for a training mech-anism that enforces the discriminativeness of all the ele-ments in the feature to capture more the subtle visual cues.
In this paper, we propose a Discrimination-Aware Mecha-nism (DAM) that iteratively identiﬁes insufﬁciently trained elements and improves them. DAM is able to increase the number of well learned elements, which captures more vi-sual cues by the feature extractor. In this way, a more infor-mative representation is learned, which brings better gen-eralization performance. We show that DAM can be easily applied to both proxy-based and pair-based loss functions, and thus can be used in most existing ﬁne-grained recog-nition paradigms. Comprehensive experiments on CUB-200-2011, Cars196, Market-1501, and MSMT17 datasets demonstrate the advantages of our DAM based loss over the related state-of-the-art approaches. 1.

Introduction
Fine-grained recognition aims at distinguishing each subclasses on a speciﬁc superclass dataset, such as birds, persons, cars. There are only a few effective cues that can distinguish samples of different classes due to the samples belong to a superclass. Meanwhile, images are usually cap-tured at different times/places, resulting in various visual differences appear in the same subclass. The extremely low
𝑥!
𝑥#
𝑥"
𝑓!
𝑓#
𝑓"
Optimization
Proxy-based 
Loss
Pair-based 
Loss
𝑓#
𝑤!
𝑓!
𝑓"
𝑤"
DAM Feature Space
𝐹#
𝑤!
𝐹!
𝐹"
𝑤"
Figure 1. Illustration of DAM for representation learning. On top left, a triplet (xa, xp, xn) and their features fa, fp, fn are shown in different colors. On top right, black vectors indicate class centers, and the dotted circles indicate class distributions. On bottom right, fa, fp and fn are mapped to a harder DAM feature space as Fa,
Fp and Fn. Inter-class samples (Fa and Fn) are pushed close and intra-class samples (Fa and Fp) are pushed away, as well as the distributions of intra-class are enlarged. Then the new features by
DAM are inputed to loss function. The pipeline runs iteratively and leads to a better generalization performance. intra-class similarity and high inter-class similarity make
ﬁne-grained recognition very challenging. Recently, with the popularity of retrieving examples of a speciﬁc subclass from the superclass database, this topic has been attracting a signiﬁcant amount of attention from academia and industry.
Current representation learning methods mainly focus on three directions: (1) Designing powerful loss function to extract robust feature embeddings [18, 29, 52, 53, 44], such as proxy-based loss and pair-based loss. (2) Constructing attention module to resolve local regions [15, 3, 24, 62, 66]. (3) Randomly erasing parameters or feature values during training for better generalization performance [67, 10, 43, 13, 9]. With the powerful deep networks and large-scale labeled benchmarks, these methods can obtain a relevant 813
feature representation for the image.
Despite the signiﬁcant progress in ﬁne-grained recogni-tion, most existing methods focus on optimizing the over-all discriminativeness of the whole feature elements to dis-tinguish it from other classes. But in ﬁne-grained recog-nition scenario, the inter-class differences are quite diverse and subtle. As shown in top-right subﬁgure of Figure 1, in original feature space, traditional optimization mecha-nism may stop early when some elements has been trained to distinguish training samples well, while other elements are insufﬁciently trained. This would result in a less gener-alizable feature extractor that only captures major discrim-inative cues and ignores subtle ones. But these subtle cues may really discriminative for ﬁne-grained recognition. e.g., shoes and glasses of person. Therefore, there is a need for a training mechanism that enforces the discriminativeness of all the elements in the feature to capture more the subtle visual cues.
In this paper, we propose a Discrimination-Aware Mech-anism (DAM) to iteratively learn elements of features more disciminative. As shown in Figure 1, we encourage the model to keep learning by mapping low-discriminative fea-ture to a harder space, and then further optimize the new fea-tures. Speciﬁcally, by exploiting difference between various speciﬁc inter-classes, we retain elements of features with less diferences to other classes and erase rest elements for the harder features. For example, fa, fp and fn are mapped to DAM feature space as Fa, Fp and Fn respectively. In
DAM feature space, inter-class samples are pushed close (e.g., distance between Fa and Fn) and intra-class samples are pushed away (e.g., distance between Fa and Fp), as well as the distributions of intra-class are enlarged. In our DAM, whether the feature element needs to be retained depends on the difference of the feature value between different classes.
For a certain element of the features, the smaller difference it is from other classes, the more it should be retained to improve its discriminativeness. By using DAM to the exist-ing proxy-based and pair-based loss, only the selected ele-ments are used for gradient update, and remaining elements of feature are erased during training. Finally, by iteratively mining discriminative cues, all elements of a feature are dis-criminative after convergence. In this way, more diverse and subtle cues are extracted, thus improving the discriminative-ness of the overall feature representation.
In summary, the main contributions of our work are listed as follows:
• We propose a discrimination-aware mechanism to ex-tract more discriminative visual cues. Compared with previous attention-based methods, our method does not need to modify network architecture. And we use differences between classes to guide feature mapping in constrast to previous erasing-based methods,
• We design two feature mapping mechanism to proxy-based loss and pair-based loss. For proxy-based loss, we selelet low-discriminativeness elements for each feature.
For pair-based loss, we also select low-discriminativeness elements for paired-negative fea-tures, but high-discriminativeness elements for paired-postive features to enhance the effectiveness of triplets.
• We achievie better performance on ﬁne-grained recog-nition tasks compared with the related state-of-the-art methods. i.e, CUB-200-2011, Cars196, Market-1501, and MSMT17. 2.