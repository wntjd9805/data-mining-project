Abstract 1.

Introduction
Inverted bottleneck layers, which are built upon depth-wise convolutions, have been the predominant building blocks in state-of-the-art object detection models on mobile devices. In this work, we investigate the optimality of this design pattern over a broad range of mobile accelerators by revisiting the usefulness of regular convolutions. We dis-cover that regular convolutions are a potent component to boost the latency-accuracy trade-off for object detection on accelerators, provided that they are placed strategically in the network via neural architecture search. By incorporat-ing regular convolutions in the search space and directly optimizing the network architectures for object detection, we obtain a family of object detection models, MobileDets, that achieve state-of-the-art results across mobile acceler-ators. On the COCO object detection task, MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at compa-rable mobile CPU inference latencies. MobileDets also outperform MobileNetV2+SSDLite by 1.9 mAP on mobile
CPUs, 3.7 mAP on Google EdgeTPU, 3.4 mAP on Qual-comm Hexagon DSP and 2.7 mAP on Nvidia Jetson GPU without increasing latency. Moreover, MobileDets are com-parable with the state-of-the-art MnasFPN on mobile CPUs even without using the feature pyramid, and achieve better mAP scores on both EdgeTPUs and DSPs with up to 2× speedup. Code and models are available in the TensorFlow
Object Detection API [16]: https://github.com/ tensorflow/models/tree/master/research/ object_detection.
∗Equal contribution.
In many computer vision applications it can be observed that higher capacity networks lead to superior performance
[29, 44, 15, 24]. However, they are often more resource-consuming. This makes it challenging to ﬁnd models with the right quality-compute trade-off for deployment on edge devices with limited inference budgets.
A lot of effort has been devoted to the manual design of lightweight neural architectures for edge devices [14, 27, 42, 39]. Unfortunately, relying on human expertise is time-consuming and can be sub-optimal. This problem is made worse by the speed at which new hardware platforms are re-leased. In many cases, these newer platforms have differing performance characteristics which make a previously devel-oped model sub-optimal.
To address the need for automated tuning of neural net-work architectures, many methods have been proposed.
In particular, neural architecture search (NAS) methods
[5, 32, 30, 13, 10] have demonstrated a superior ability in
ﬁnding models that are not only accurate but also efﬁcient on a speciﬁc hardware platform.
Despite many advancements in NAS algorithms [43, 24, 1, 19, 5, 32, 40, 30, 13, 10], it is remarkable that inverted bottlenecks (IBN) [27] remain the predominant building block in state-of-the-art mobile models. IBN-only search spaces have also been the go-to setup in a majority of the related NAS publications [30, 5, 32, 13]. IBN layers rely heavily on depthwise and depthwise separable convolutions
[28]. The resulting models have relatively low FLOPS and parameter counts, and can be executed efﬁciently on CPUs.
However, the advantage of depthwise convolutions for 13825
mobile inference is less clear for hardware accelerators such as DSPs or EdgeTPUs which are becoming increasingly popular on mobile devices. For example, for certain ten-sor shapes and kernel dimensions, a regular convolution can run 3× as fast as the depthwise variation on an EdgeTPU despite having 7× as many FLOPS. This observation leads us to question the exclusive use of IBN-only search spaces in most current state-of-the-art mobile architectures.
Our work seeks to rethink the use of IBN-only search space on modern mobile accelerators. To this end, we pro-pose the MobileDet search space family, which includes not only IBNs but also ﬂexible full convolution sequences mo-tivated by the structure of tensor decomposition [33, 6, 21].
Using the task of object detection as an example (one of the most popular mobile vision applications), we show the Mo-bileDet search space family enables NAS methods to iden-tify models with substantial better latency-accuracy trade-offs on mobile CPUs, DSPs, EdgeTPUs and edge GPUs.
To evaluate our proposed MobileDet search space, we perform latency-aware NAS for object detection, target-ing a diverse set of mobile platforms. Experimental re-sults show that by using our MobileDet search space family and directly searching on detection tasks, we can consis-tently improve the performance across all hardware plat-forms. By leveraging full convolutions at selected posi-tions in the network, our method outperforms IBN-only models by a signiﬁcant margin. Our searched models, Mo-bileDets, outperform MobileNetV2 classiﬁcation backbone by 1.9mAP on CPU, 3.7mAP on EdgeTPU, 3.4mAP on
DSP, and 2.7mAP on edge GPU at comparable inference latencies. MobileDets also outperform the state-of-the-art
MobileNetV3 classiﬁcation backbone by 1.7mAP at simi-lar CPU inference efﬁciency. Further, the searched models achieved comparable performance with the state-of-the-art mobile CPU detector, MnasFPN [7], without leveraging the
NAS-FPN head which may complicate the deployment. On both EdgeTPUs and DSPs, MobileDets are more accurate than MnasFPN while being more than twice as fast. 2.