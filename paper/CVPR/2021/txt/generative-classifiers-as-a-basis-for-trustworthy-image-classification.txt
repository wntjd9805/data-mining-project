Abstract
With the maturing of deep learning systems, trustworthiness is becoming increasingly important for model assessment.
We understand trustworthiness as the combination of ex-plainability and robustness. Generative classiﬁers (GCs) are a promising class of models that are said to naturally accomplish these qualities. However, this has mostly been demonstrated on simple datasets such as MNIST and CIFAR in the past. In this work, we ﬁrstly develop an architecture and training scheme that allows GCs to operate on a more relevant level of complexity for practical computer vision, namely the ImageNet challenge. Secondly, we demonstrate the immense potential of GCs for trustworthy image clas-siﬁcation. Explainability and some aspects of robustness are vastly improved compared to feed-forward models, even when the GCs are just applied naively. While not all trust-worthiness problems are solved completely, we observe that
GCs are a highly promising basis for further algorithms and modiﬁcations. We release our trained model for download in the hope that it serves as a starting point for other gen-erative classiﬁcation tasks, in much the same way as pre-trained ResNet architectures do for discriminative classiﬁ-cation.
Code: github.com/VLL-HD/trustworthy GCs 1.

Introduction
Generative classiﬁers (GCs) and discriminative classi-ﬁers (DCs) represent two contrasting ways of solving classi-ﬁcation tasks. In short, while standard DCs model the class probability given an input directly, p(class | image) (e.g. softmax classiﬁcation), generative classiﬁers (GCs) take the opposite approach: They model the likelihood of the input image, conditioned on each class, p(image | class). The actual classiﬁcation is then performed by ﬁnding the class under which the image has the highest likelihood.
The application of GCs has so far been limited to very
Discriminative Classiﬁer q(class Y | image X) sums to 1 over classes
Generative Classiﬁer q(image X | class Y ) integrates to 1 over images (a) Normal input 0.55
Cat 0.45
Dog 6.3
Cat 6.1
Dog (b) Unrelated input 0.42
Cat 0.58
Dog 0.014
Cat 0.017
Dog
Figure 1: Example of one advantage of generative classi-ﬁers: The class posterior of a DC always sums up to 1, while the likelihoods of the GC do not have this restriction, constituting inherently more informative outputs. E.g. the
GC can show if a prediction is uncertain because the input agrees with both classes, or with neither. simple datasets such as MNIST, SVHN and CIFAR-10/100.
For any practical image classiﬁcation tasks, DCs are used exclusively, due to their excellent discriminative perfor-mance. In principle, GCs are said to have various advan-tages over DCs, which align with the term trustworthiness.
In general agreement with [24], we understand trustworthi-ness as the combination of explainability and robustness.
Explainability: DCs based on deep neural networks are notorious for being ‘black boxes’, prompting many devel-opments in the ﬁeld of explainable AI. In the taxonomy laid out in [18], most commonly used algorithms fall into categories I or II: post-hoc methods that visualize how a network processes information (I), or that show its internal representations (II). The explanations can vary depending on the chosen method, and there is no guarantee that the results faithfully reﬂect what the DC is doing internally.
In contrast, GCs bring to mind Feynman’s mantra “What
I cannot create, I do not understand”. As GCs are able to model the input data itself, not just the class posteriors, they 2971
have fundamentally more informative outputs. For instance,
GCs allow us to tell if a decision between two classes is uncertain because the input agrees well with both classes, or with neither (see Fig. 1).
In addition, most GCs have interpretable latent spaces with meaningful features, allow-ing for the actual decision process to be directly visualized without post-hoc techniques. Therefore, it could be argued that GCs belong to category III of the explainability taxon-omy [18], i.e. methods that intrinsically work in an explain-able way, without relying on additional algorithms.
Robustness: A second large concern about the practical use of deep learning based classiﬁcation systems is their robust-ness, which can have different meanings, depending on the context. In particular, GCs have been assumed to be supe-rior to DCs in terms of generalization under dataset shifts
[51, 39] and accurately calibrated posteriors [3]. In addi-tion, a big advantage of GCs is their capability to explicitly identify abnormal inputs in a natural way, thus indicating when a decision should not be trusted. Furthermore, GCs were found to be more robust towards adversarial attacks
[33] and allow for their explicit detection [17].
It is still unclear if GCs can also manifest these ad-vantages in more complex tasks while remaining compet-itive to DCs in task performance. For example, the authors of [15] ﬁnd while GCs can successfully detect adversarially attacked MNIST images, this already fails for the CIFAR-10 dataset. The authors of [34, 30] observe that detection of other forms of OoD data also fails in various ways for natural images. In [16], the authors cast doubt on whether
GCs can be used for high-dimensional input data at all.
In light of this background, our work makes the follow-ing contributions: (i) We design and train a GC that per-forms at a level relevant to practical image classiﬁcation, demonstrated on the ImageNet dataset. (ii) We show vari-ous native explainability techniques unique to GCs. (iii) We examine the model in terms of robustness.
Overall, we ﬁnd our GC to work better than a compa-rable DC in terms of trustworthiness. However, we do ob-serve that previous ﬁndings on superior generalization un-der dataset shift [51] and immunity to adversarial attacks
[41] do not hold for the ImageNet dataset. For other aspects of robustness, our GC shows some great beneﬁts, such as naturally detecting OoD inputs and adversarial attacks. 2.