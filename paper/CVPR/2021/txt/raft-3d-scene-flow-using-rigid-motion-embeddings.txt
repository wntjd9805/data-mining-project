Abstract
We address the problem of scene ﬂow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D mo-tion. We introduce RAFT-3D, a new deep architecture for scene ﬂow. RAFT-3D is based on the RAFT model devel-oped for optical ﬂow but iteratively updates a dense ﬁeld of pixelwise SE3 motion instead of 2D motion. A key inno-vation of RAFT-3D is rigid-motion embeddings, which rep-resent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embed-dings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (δ < 0.05) from 34.3% to 83.7%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. 1.

Introduction
Scene ﬂow is the task of estimating pixelwise 3D mo-tion between a pair of video frames[43]. Detailed 3D mo-tion is requisite for many downstream applications includ-ing path planning, collision avoidance, virtual reality, and motion modeling. In this paper, we focus on stereo scene
ﬂow and RGB-D scene ﬂow, which address stereo video and RGB-D video respectively.
Many scenes can be well approximated as a collection of rigidly moving objects. The motion of driving scenes, for example, can be modeled as a variable number of cars, buses, and trucks. The most successful scene ﬂow ap-proaches have exploited this structure by decomposing a scene into its rigidly moving components[32, 45, 45, 47, 30, 1, 20, 19, 23]. This introduces a powerful prior which can be used to guide inference. While optical ﬂow approaches typically assume piecewise smooth motion, a scene contain-ing rigid objects will exhibit piecewise constant 3D motion
ﬁelds (Fig. 1).
Recently, many works have proposed integrating deep learning into scene ﬂow estimation pipelines. A common approach has been to use object detection[1, 3] or segmen-tation [1, 30, 29, 37] networks to decompose the scene into a collection of potentially rigidly moving objects. Once the scene has been segmented into its rigidly moving compo-nents, more traditional optimization can be used to ﬁt a mo-tion model to each of the objects. One limitation of this approach is that the networks require instance segmenta-tions to be trained and cannot recover the motion of new un-known objects. Object detection and instance segmentation introduce non-differentiable components into the network, making end-to-end training difﬁcult without bounding box or instance level supervision.
We introduce RAFT-3D, an end-to-end differentiable ar-chitecture which estimates pixelwise 3D motion from stereo or RGB-D video. RAFT-3D is built on top of RAFT [41], a state-of-the-art optical ﬂow architecture that builds all-pairs correlation volumes and uses a recurrent unit to iteratively reﬁne a 2D ﬂow ﬁeld. We retain the basic iterative structure of RAFT but introduce a number of novel designs.
The main innovation we introduce is rigid-motion em-beddings, which are per-pixel vectors that represent a soft grouping of pixels into rigid objects. During inference,
RAFT-3D iteratively updates the rigid-motion embeddings such that pixels with similar embeddings belong to the same rigid object and follow the same SE3 motion.
Integral to rigid-motion embeddings is Dense-SE3, a dif-ferentiable layer that seeks to ensure that the embeddings are geometrically meaningful. Dense-SE3 iteratively up-dates a dense ﬁeld of per-pixel SE3 motion by perform-ing unrolled Gauss-Newton iterations such that the per-pixel
SE3 motion is geometrically consistent with the current es-timates of rigid-motion embeddings and pixel correspon-dence. Because of Dense-SE3, the rigid-motion embed-dings can be indirectly supervised from only ground truth 3D scene ﬂow, and our approach does not need any super-vision of object boxes or masks.
Fig. 1 provides an overview of our approach. RAFT-3D take a pair of RGB-D images as input. It extracts features from the input images and builds a 4D correlation volume by computing the visual similarity between all pairs of pix-els. RAFT-3D maintains and updates a dense ﬁeld of pix-8375
1 h t p e
D
-e g a m
I 2 h t p e
D
-e g a m
I
R e s n e t 5 0 w o l
F l a c i t p
O n o i t a t o
R n o i t a l s n a r
T
Figure 1. Overview of our approach. Features extracted from the input images are used to construct a 4D correlation volume. We initialize the SE3 motion ﬁeld, T, to be the identity at every pixel. During each iteration, the update operator uses the current SE3 motion estimate to index from the correlation volume, using the correlation features and hidden state to produce estimates of pixel correspondence and rigid-motion embeddings. These estimates are plugged into Dense-SE3, a least-squares optimization layer which uses geometric constraints to produce an update to the SE3 ﬁeld. After successive iterations we recover a dense SE3 ﬁeld, which can be decomposed into a rotational and translation component. The SE3 ﬁeld can be projected onto the image to recover optical ﬂow. elwise SE3 motion. During each iteration, it uses the cur-rent estimate of SE3 motion to index from the correlation volume. A recurrent GRU-based update operator takes the correlation features and produces an estimate of pixel cor-respondence, which is then used by Dense-SE3 to generate updates to the SE3 motion ﬁeld.
RAFT-3D achieves state-of-the-art accuracy. On Fly-ingThings3D, under the two-view evaluation [25], RAFT-3D improves the best published accuracy (δ < 0.05) from 34.3% to 83.7%. On KITTI, RAFT-3D achieves an error of 5.77, outperforming the best published method (6.31), de-spite using no object instance supervision. 2.