Abstract
Weakly supervised phrase grounding aims at learning region-phrase correspondences using only image-sentence pairs. A major challenge thus lies in the missing links be-tween image regions and sentence phrases during training.
To address this challenge, we leverage a generic object de-tector at training time, and propose a contrastive learning framework that accounts for both region-phrase and image-sentence matching. Our core innovation is the learning of a region-phrase score function, based on which an image-sentence score function is further constructed. Importantly, our region-phrase score function is learned by distilling from soft matching scores between the detected object names and candidate phrases within an image-sentence pair, while the image-sentence score function is supervised by ground-truth image-sentence pairs. The design of such score functions removes the need of object detection at test time, thereby signiﬁcantly reducing the inference cost. Without bells and whistles, our approach achieves state-of-the-art results on visual phrase grounding, surpassing previous methods that require expensive object detectors at test time. 1.

Introduction
Visual phrase grounding — ﬁnding regions associated with phrases in a sentence description of the image, is an important problem at the intersection of computer vision and natural language processing. Most of the existing ap-proaches [15, 39, 46] follow a fully supervised paradigm that requires the labeling of bounding boxes for each phrase.
These ﬁne-grained annotations are unfortunately expensive to obtain and thus difﬁcult to scale. Consequently, weakly supervised grounding has recently received considerable at-tention [42, 51, 50, 56, 6, 53, 54, 14, 45]. In this setting, only images and their sentence descriptions are given at training time. At inference time, given an image sentence pair, a method is asked to link regions to sentence phrases.
∗equal contribution. Contact: lwwang@cse.cuhk.edu.hk
Figure 1: Our method uses object detector predictions to guide the learning of region-phrase matching in training.
At the inference time, our method no longer requires object detectors and directly predicts the box with the highest score.
A major challenge of weakly supervised grounding is to distinguish among many “concurrent” visual concepts.
For example, the region of a dog and that of its head are likely to co-occur in images associated with the phrase “a running puppy.” Without knowing the ground-truth region-phrase matching, learning to link the region of dog (but not dog head) to its corresponding phrase becomes very challenging. To address this challenge, recent methods [6, 14, 45, 18] leverage generic object detectors for training and/or inference. A detector provides high quality object regions, as well as their category labels that can be further matched to candidate phrases, thereby bringing in external knowledge about region-phrase matching and thus helping to disambiguate those “concurrent” concepts. However, it remains unclear about the best practices of using an external object detector for weakly supervised grounding.
In this paper, we focus on developing a principled ap-proach to distill knowledge from a generic object detector for weakly supervised phrase grounding. To this end, we present a simple method under the framework of contrastive learning. Speciﬁcally, our model learns a score function between region-phrase pairs, guided by two levels of simi-larity constraints encoded using noise-contrastive estimation (NCE) loss [19] during training. The ﬁrst level of region-14090
phrase similarity is distilled from object detection outputs.
This is done by aligning predicted region-phrase scores to a set of soft targets, computed by matching object names and candidate phrases. The second level of image-sentence similarity is computed from a greedy matching between all region-phrase pairs, and supervised by ground-truth image-sentence pairs. During inference, our method compares each image region to candidate phrases using the learned score function, without the need of object detection. Our training and inference stages are shown in Fig. 1.
To evaluate our method, we conduct extensive exper-iments on Flickr30K Entities [39] and ReferItGame [26] datasets. We compare our results to the latest methods of weakly supervised phrase grounding. Our experiments show that our method establishes new state-of-the-art results and outperforms all previous methods, including those using strong object detectors at test time [14, 46, 50] or using a similar contrastive loss [18]. For example, on Flickr30K
Entities, without additional training data, our method outper-forms the best reported results by a large margin. On Refer-ItGame, our method signiﬁcantly beats the best reported results. Moreover, we systematically vary the components of our model and demonstrate several best practices for weakly supervised phrase grounding. We hope that our simple yet strong method will shed light on new ideas and practices for weakly supervised image-text grounding. 2.