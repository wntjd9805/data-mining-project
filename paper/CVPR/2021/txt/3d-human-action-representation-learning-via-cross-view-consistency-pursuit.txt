Abstract
In this work, we propose a Cross-view Contrastive
Learning framework for unsupervised 3D skeleton-based action Representation (CrosSCLR), by leveraging multi-view complementary supervision signal. CrosSCLR con-sists of both single-view contrastive learning (Skeleton-CLR) and cross-view consistent knowledge mining (CVC-KM) modules, integrated in a collaborative learning man-ner.
It is noted that CVC-KM works in such a way that high-conﬁdence positive/negative samples and their dis-tributions are exchanged among views according to their embedding similarity, ensuring cross-view consistency in terms of contrastive context, i.e., similar distributions. Ex-tensive experiments show that CrosSCLR achieves remark-able action recognition results on NTU-60 and NTU-120 datasets under unsupervised settings, with observed higher-quality action representations. Our code is available at https://github.com/LinguoLi/CrosSCLR. 1.

Introduction
Human action recognition is an important but challeng-ing task in computer vision research. Due to the light-weight and robust estimation algorithms [3, 56], 3D skele-ton has become a popular feature representation to study hu-man action dynamics. Many 3D action recognition works
[6, 60, 17, 25, 44, 28, 16] use a fully-supervised manner and require massive labeled 3D skeleton data. However, annotating data is expensive and time-consuming, which prompts people to explore unsupervised methods [65, 26, 39, 47] on skeleton data. Some unsupervised methods exploit structure completeness within each sample based on pretext tasks, including reconstruction [9, 65], auto-regression [20, 47] and jigsaw puzzles [35, 54], but it is unsure that the designed pretext tasks generalize well for downstream tasks. Other unsupervised methods are based
∗ Equal Contribution
∗∗ Corresponding Author: Bingbing Ni
Figure 1. Hand waving in joint and motion form. Two samples are from the same action class. (a) usual contrastive learning methods regard them as negative pairs. (b) in a multi-view situation, con-sidering their similar motion patterns, they can be positive pairs.
This motivates us to introduce cross-view contrastive learning for skeleton representation. on contrastive learning [55, 4, 11, 18], aiming to leverage the instance discrimination of samples in latent space.
Although the above approaches improve the skeleton representation capability to some extent, it is believed that the power of unsupervised methods is by far from fully explored. On the one hand, traditional contrastive learn-ing uses only one positive pair generated by data augmen-tation and even similar samples are regarded as negative samples. Despite the high similarity, the negative samples are forced away in embedding space, which is unreason-able for clustering. On the other hand, current unsupervised methods [1, 65, 20, 26, 47] have not yet explored the rich intra-supervision information provided by different skele-ton modalities. Considering that it is easy to obtain skeleton data in multiple “views”, e.g., joint, motion and bone, com-plementary information preserved in different views can as-sist the operation to mine positive pairs from similar nega-tive samples. As shown in Figure 1, the same hand waving actions are different in pose (joint), but similar in motion.
Usual contrastive learning methods regard them as nega-4741
tive pairs, keeping them away in embedding space. If such complementary information, i.e., different in joint but simi-lar in motion, could be fully utilized and explored, the size of hidden positive pairs in joint can be boosted, enhancing training ﬁdelity. Thus, the cross-view contrastive learning strategy takes advantage of multi-view knowledge, resulting in better-extracted skeleton features.
To this end, we propose a Cross-view Contrastive Learn-ing framework for Skeleton-based action Representation (CrosSCLR), which exploits multi-view information for mining positive samples and pursuing cross-view consis-tency in unsupervised contrastive learning, enabling the model to extract more comprehensive cross-view features.
First, parallel Contrastive Learning is evoked for each single-view Skeleton action Representation (SkeletonCLR), yielding multiple single-view embedding features. Second, inspired by the fact that the distance of samples in embed-ding space reﬂects the similarity of the samples in the orig-inal space, we refer to the extreme similarity of samples in one view to guide the learning process in another view, as shown in Figure 1. More speciﬁcally, Cross-View Consis-tent Knowledge Mining (CVC-KM) module is developed to exam the similarity of samples, and select the most similar pairs as positive ones to boost the positive set in comple-mentary views, i.e., embedding distance/similarity (conﬁ-dence score) serves as the weight of corresponding mined sample in the contrastive loss. In other words, CVC-KM conveys the most prominent knowledge from one view to others, introduces complementary pseudo-supervised con-straint and promotes information sharing among views. The entire framework excavates positive pairs across views ac-cording to the distance between samples in the embedding space to promote knowledge exchange among views, so that the extracted skeleton features will contain multi-view knowledge and are more competitive for various down-stream tasks. Extensive results on NTU-RGB+D [40, 27] datasets demonstrate that our method indeed boosts the 3D action representation learning beneﬁting from cross-view consistency. We summarize our contributions as follows:
• We propose CrosSCLR, a cross-view contrastive learn-ing framework for skeleton-based action representation.
• We develop Contrastive Learning for Skeleton-based ac-tion Representation (SkeletonCLR) to learn the single-view representations of skeleton data.
• We use parallel SkeletonCLR models and CVC-KM to excavate useful samples across views, enabling the model to capture more comprehensive representation unsupervisedly.
• We evaluate our model on 3D skeleton datasets, e.g.,
NTU-RGB+D 60/120, and achieve remarkable results under unsupervised settings. 2.