Abstract
Transfer learning with pre-training on large-scale datasets has played an increasingly signiﬁcant role in computer vision and natural language processing recently.
However, as there exist numerous application scenarios that have distinctive demands such as certain latency con-straints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. In this paper, we focus on the area of object detection and present a transfer learning system named GAIA, which could automatically and efﬁciently give birth to customized solutions according to heterogeneous downstream needs. GAIA is capable of providing power-ful pre-trained weights, selecting models that conform to downstream demands such as latency constraints and spec-iﬁed data domains, and collecting relevant data for practi-tioners who have very few datapoints for their tasks. With
GAIA, we achieve promising results on COCO, Objects365,
Open Images, Caltech, CityPersons, and UODB which is a collection of datasets including KITTI, VOC, WiderFace,
DOTA, Clipart, Comic, and more. Taking COCO as an ex-ample, GAIA is able to efﬁciently produce models covering a wide range of latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and bells. To beneﬁt ev-ery practitioner in the community of object detection, GAIA is released at https://github.com/GAIA-vision. 1.

Introduction
Transfer learning has been one of the most powerful techniques throughout the history of deep learning. The paradigm of pre-training on large-scale source datasets and
ﬁnetuning on target datasets or tasks is applied over a wide range of tasks in both computer vision (CV) and natural lan-guage processing (NLP). The motivation behind is to endow
*Equal contributions.
†Corresponding author. (a) Comparison of models produced by GAIA, and
ResNet50 pre-trained on ImageNet and COCO to various downstream tasks. All models share the same latency. For
M R−2 in Caltech and CityPersons, lower is better. (b) Comparison of models with different latency on COCO.
Figure 1: Transfer performance of GAIA models that adapt to various downstream needs including speciﬁed domains and latency constraints. No whistles and bells are used. models the general-purpose abilities and transfer the knowl-edge to downstream tasks, in order to acquire higher perfor-mance or faster convergence. Until recently, the inﬂuence 274
of transfer learning has moved onto the next level as the scales of data are growing exponentially. In BiT [33] and
WSL [43], pre-training on enormous data like JFT-300M and Instagram (3.5B images) has been proved to yield ex-traordinary improvements over the conventional ImageNet pre-training on downstream tasks. Similar trends are going on in NLP, as shown in BERT [17], T5 [49] and GPT-3 [6].
Although transfer learning with huge scale of pre-training data has shown the great success, it is severely af-ﬂicted by its inﬂexibility on model architecture. As stated in the “no free lunch” theorem [57], no single algorithm is best suited for all possible scenarios and datasets. Dif-ferent datasets may request for different model architec-tures, and different application scenarios may request for model of different scales. To take advantage of the transfer learning, these customized models are obliged to be trained from scratch on the whole upstream datasets, which is pro-hibitively expensive.
This issue is even more serious in object detection, as it is one of the most signiﬁcant computer vision tasks and covers an wide range of deployment scenarios. In practice, detectors are always supposed to work across various de-vices with distinctive resource constraints, which requires detectors to have corresponding input sizes and architec-tures. In addition, the correlation between distributions of object scales and the adapted network architectures are very close in object detection. Therefore, the demand for task-speciﬁc architecture adaptation is much stronger in object detection than other tasks such as image classiﬁcation or semantic segmentation.
In this paper, we introduce a transfer learning system in object detection that aims to harmonize the gap between large-scale upstream training and downstream customiza-tion. We name this system “GAIA” as it could swiftly give birth to a variety of specialized solutions to hetero-geneous demands, including task-speciﬁc architectures and well-prepared pre-trained weights. For users who have very few datapoints for their tasks, GAIA is able to collect rele-vant data to fuel their needs. Our objective is not to propose a speciﬁc method, but more to present an integrated system that brings convenience to practitioners in the community of object detection. There are two components in GAIA: task-agnostic uniﬁcation and task-speciﬁc adaptation.
To begin with, we conduct the task-agnostic uniﬁcation on data and architectures respectively. In order to unleash the potential of transfer learning, we collect data from mul-tiple sources and build a huge data pool with a uniﬁed label space. The uniﬁed label space is formulated based on the word2vec similarity, which prevents knowledge conﬂicts among duplicated categories from distinctive sources and enables data of relevant categories to jointly boost the per-formance of detector. Besides, covering a wide range of cat-egories provides indicators for task-speciﬁc adaptation. To realize the purpose of training plenty of models efﬁciently on huge upstream data, we adopt the weight sharing scheme which has been widely used in [39, 5, 48, 30, 64, 8, 65], which enables models of different widths and depths to be optimized together. As models may interfere with each other during collective training, we propose an “anchor-based progressive shrinking” to alleviate the problem.
In the task-speciﬁc adaptation procedure, GAIA needs to ﬁnd adapted model architectures according to the given tasks. We quantitatively assess the ranking ability of differ-ent search methods based on the Kendall Tau measure [32] and propose an efﬁcient and reliable method of selecting models that surprisingly ﬁt the downstream task, regardless of data domains or latency constraints (Figure 1). To extend the utility of GAIA on the ubiquitous data-scarce scenar-ios, we develop GAIA an ability of collecting relevant data to downstream tasks from data pool, which yields further improvements.
The contributions of our paper are as follows:
• We demonstrate how transfer learning and weight shar-ing learning could be well combined, to produce pow-erful pre-trained models over a variety of architectures simultaneously.
• We propose an efﬁcient and reliable approach of ﬁnd-ing the adapted network architectures to speciﬁed downstream tasks. Powered by pre-training and task-speciﬁc architecture selection, GAIA achieves surpris-ingly good results over 10 downstream tasks without exclusive tuning on hyper-parameters.
• GAIA has the capability of ﬁnding relevant data based on 2 images per category in the downstream tasks to support ﬁnetuning. This further extends the utility of
GAIA in data-scarce settings. 2.