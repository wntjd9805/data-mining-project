Abstract
We propose a simple, intuitive yet powerful method for human-object interaction (HOI) detection. HOIs are so diverse in spatial distribution in an image that existing
CNN-based methods face the following three major draw-backs; they cannot leverage image-wide features due to
CNN’s locality, they rely on a manually deﬁned location-of-interest for the feature aggregation, which sometimes does not cover contextually important regions, and they cannot help but mix up the features for multiple HOI in-stances if they are located closely. To overcome these draw-backs, we propose a transformer-based feature extractor, in which an attention mechanism and query-based detec-tion play key roles. The attention mechanism is effective in aggregating contextually important information image-wide, while the queries, which we design in such a way that each query captures at most one human-object pair, can avoid mixing up the features from multiple instances.
This transformer-based feature extractor produces so effec-tive embeddings that the subsequent detection heads may be fairly simple and intuitive. The extensive analysis re-veals that the proposed method successfully extracts con-textually important features, and thus outperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.6 mAP on V-COCO). The source codes are available at https://github.com/hitachi-rd-cv/qpic. 1.

Introduction
Human-object interaction (HOI) detection has attracted enormous interest in recent years for its potential in deeper scene understanding [3–6,8,11–13,15–17,19–21,24,27,29– 37]. Given an image, the task of HOI detection is to localize a human and object, and identify the interactions between them, typically represented as hhuman bounding box, object bounding box, object class, action classi.
Conventional HOI detection methods can be roughly di-(a) (c) (b) (d)
Figure 1. Observed failure cases of conventional methods. The ground-truth human bounding boxes, object bounding boxes, ob-ject classes, and action classes are drawn with red boxes, blue boxes, blue characters, and yellow characters, respectively. vided into two types: two-stage methods [3–6, 8, 11, 13, 15, 16, 19–21, 24, 27, 29–31, 33–37] and single-stage meth-ods [12, 17, 32]. In the two-stage methods, humans and ob-jects are ﬁrst individually localized by off-the-shelf object detectors, and then the region features from the localized area are used to predict action classes. To incorporate con-textual information, auxiliary features such as the features from the union region of a human and object bounding box, and locations of the bounding boxes in an image are often utilized. The single-stage methods predict interactions us-ing the features of a heuristically-deﬁned position such as a midpoint between a human and object center [17].
While both two- and single-stage methods have shown signiﬁcant improvement, they often suffer from errors at-tributed to the nature of convolutional neural networks (CNNs) and the heuristic way of using CNN features. Fig-ure 1 shows observed failure cases of conventional methods.
In Fig. 1a, we can easily recognize from an entire image 10410
that a boy is washing a car. It is difﬁcult, however, for two-stage methods to predict the action class “wash” since they typically use only the cropped bounding-box regions. The regions sometimes miss contextually important cues located outside the human and object bounding box such as the hose in Fig. 1a. Even though the features of union regions may contain such cues, these regions are frequently dominated by disturbing contents such as background and irrelevant humans and objects. Figure 1b shows an example where multiple HOI instances are overlapped. In this example, the region features of the catching person inevitably contain the features of the blocking person because the hand of the lat-ter, which is important to predict “block”, is in the bounding box of the former, ending up in contaminated features. The detection based on the contaminated features easily results in failures. The single-stage methods attempt to capture the contextual information by pairing a target human and object from an early stage in feature extraction and extracting inte-grated features rather than individually treating the targets.
To determine the regions from which integrated features are extracted, they rely on heuristically-designed location-of-interest such as a midpoint between a human and object center [17]. However, such reliance sometimes causes a problem. Fig. 1c shows an example where a target human and object are located distantly. In this example, the mid-point is located close to the man in the middle, who is not relevant to the target HOI instance. Therefore, it is difﬁ-cult to detect the target on the basis of the features around the midpoint. Fig. 1d is an example where the midpoints of multiple HOI instances are close to each other. In this case,
CNN-based methods tend to make mis-detection due to the contaminated features as they do in the case of Fig. 1b.
To overcome these drawbacks, we propose QPIC, a query-based HOI detector that detects a human and object in a pairwise manner with image-wide contextual information.
QPIC is extended from a recently proposed object detec-tor, DETR [2], and has the attention mechanism and query-based detection in a transformer [28] as key components.
The attention mechanism scans through the entire area of an image and is expected to selectively aggregate contextually important features according to the contents of an image.
Moreover, we design QPIC’s queries in such a way that they can separately extract features of multiple HOI instances without contaminating them even when the instances are lo-cated closely. We realize this by making each query capture at most one human-object pair. This enables to calculate attentions query-wise as opposed to location-wise, and to clarify each query’s target human-object-pair through the self-attention mechanism. These key designs of the atten-tion mechanism and query-based pairwise detection make
QPIC robust even under the difﬁcult conditions such as the case where contextually important information appears out-side the human and object bounding box (Fig. 1a), the target human and object are located distantly (Fig. 1c), and multi-ple instances are close to each other (Fig. 1b and 1d). The key designs produce so effective embeddings that the sub-sequent detection heads may be fairly simple and intuitive.
To summarize, our contributions are three-fold: (1) We propose a simple yet effective query-based HOI detector,
QPIC. To the best of our knowledge, this is the ﬁrst work to introduce an attention- and query-based method to HOI detection. (2) We achieve signiﬁcantly better performance than state-of-the-art methods on two challenging HOI de-tection benchmarks. (3) We conduct detailed analysis on the behavior of QPIC, and reveal some of the important charac-teristics of HOI detection tasks that conventional methods could not capture, but QPIC does relatively well. 2.