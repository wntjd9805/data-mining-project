Abstract
Face recognition is greatly improved by deep convolu-tional neural networks (CNNs). Recently, these face recog-nition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically re-alizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the at-tacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we ob-serve that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indi-cating the overﬁtting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face im-ages. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transfer-ability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well. 1.

Introduction
Deep convolutional neural networks (CNNs) have led to substantial performance improvements on many com-*Equal contributions.
†Corresponding authors.
‡Work done as an intern at RealAI.
Figure 1. Demonstration of adversarial patches against face recog-nition models. (a) The attacker who wants to impersonate the tar-get identity. (b) An image of the target identity. (c) The adversarial patch and the corresponding adversarial example generated by the
TAP-TIDIM algorithm. (d) The adversarial patch and the corre-sponding adversarial example generated by the proposed GenAP-DI algorithm. The proposed GenAP algorithms use face-like fea-tures as perturbations to improve the transferability. puter vision tasks. As an important task, face recognition is also greatly facilitated by deep CNNs [17, 23, 4]. Due to their excellent recognition performance, deep face recog-nition models have been used for identity authentication in security-sensitive applications, e.g., ﬁnance/payment, pub-lic access, face unlock on smart phones, etc.
However, deep CNNs are shown to be vulnerable to ad-versarial examples at test time [21, 5]. Adversarial exam-ples are elaborately perturbed images that can fool mod-els to make wrong predictions. Early adversarial examples on deep CNNs are indistinguishable from legitimate ones for human observers by slightly perturbing every pixel in an image. Later, [18] proposes adversarial patches, which only perturb a small cluster of pixels. Several works have 11845
shown that the adversarial patches can be made into phys-ical objects to fool deep CNNs in the wild. For example,
[9, 19, 24] use adversarial stickers or T-shirts to fool spe-cial purpose object detectors. [18] proposes an adversar-ial eyeglass frame to impersonate another identity against face recognition models. These works show that adversar-ial patches are physically realizable and stealthy. Using the adversarial patches in the physical world, the attacker can fool a recognition model without accessing the digital in-put to it, making them an emerging threat to deep learn-ing applications, especially to face recognition systems in security-sensitive scenarios.
Previous works on adversarial patches are developed un-der the white-box setting [9, 19, 3, 18], where the attacker knows the parameters of the target model, or under the query-based setting [18, 27], where the attacker can make many queries against the target model. But for a black-box model deployed in the wild, both the white-box information and the excessive queries are not easily attainable. In this paper, we focus on evaluating the robustness of face recog-nition models under the query-free black-box setting, which is a more severe and realistic threat model.
Under the query-free black-box setting, the adversarial attacks based on transferability are widely used. Transfer-based attacks [10] leverage that the adversarial examples for the white-box substitute models are also effective at the black-box target models. Speciﬁcally, most adversarial al-gorithms perform optimization on an adversarial objective speciﬁed by the substitute models as a surrogate, to approx-imate the true (but unknown) adversarial objective on the black-box target models. Existing techniques on improv-ing the transferability of adversarial examples focus on us-ing advanced non-convex optimization [6], data augmenta-tions [26, 7], etc. These techniques are originally proposed to generate Lp-norm (p > 0) constrained adversarial exam-ples, and we show that they can be extended to improve the transferability of adversarial patches as well.
However, even though these techniques are extended and applied in the patch setting, we still observe it easy for the optimization to be trapped into local optima with unsatis-factory transferability. First, the transferability is sensitive to initialization of the algorithms. Second, if the perturba-tion magnitude increases, the transferability ﬁrst rises and then falls, exhibiting an overﬁtting phenomenon. The difﬁ-culties in escaping solutions of unsatisfactory transferability indicate that the optimization is prone to overﬁtting the sub-stitute model and new regularization methods are required.
We propose to regularize the adversarial patch by opti-mizing it on a low-dimensional manifold. Speciﬁcally, the manifold is represented by a generative model and the op-timization is conducted in its latent space. The generative model is pre-trained on legitimate human face data and can generate diverse and unseen human face images by manipu-lating the latent vectors to assemble different face features.
By optimizing the adversarial objective on this latent space, the adversarial perturbations resemble human face features (see Fig. 1, (d)), on which the predictions of the white-box substitute and the black-box target model are more related.
Consequently, the overﬁtting problem is relieved and the transferability is improved.
Extensive experiments are conducted to show the superi-ority of the proposed method for black-box attacks on face recognition. We show its effectiveness in the physical world as well. Finally, we extend the proposed method to other tasks, e.g., image classiﬁcation. 2.