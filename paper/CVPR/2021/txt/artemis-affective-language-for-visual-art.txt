Abstract
We present a novel large-scale dataset and accompa-nying machine learning models aimed at providing a de-tailed understanding of the interplay between visual con-tent, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, we focus on the affective experience triggered by visual artworks and ask the annotators to in-dicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explana-tion for their emotion choice. As we demonstrate below, this leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., “freedom” or “love”), or ref-erences that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. We focus on visual art (e.g., paint-ings, artistic photographs) as it is a prime example of im-agery created to elicit emotional responses from its viewers.
Our dataset, termed ArtEmis, contains 455K emotion attri-butions and explanations from humans, on 80K artworks from WikiArt. Building on this data, we train and demon-strate a series of captioning systems capable of expressing and explaining emotions from visual stimuli. Remarkably, the captions produced by these systems often succeed in re-ﬂecting the semantic and abstract content of the image, go-ing well beyond systems trained on existing datasets. The collected dataset and developed methods are available at https://artemisdataset.org. 1.

Introduction
Emotions are among the most pervasive aspects of hu-man experience. While emotions are not themselves lin-guistic constructs, the most robust and permanent access we have to them is through language [44].
In this work, we focus on collecting and analyzing at scale language that explains emotions generated by observing visual artworks.
Speciﬁcally, we seek to better understand the link between the visual properties of an artwork, the possibly subjective affective experience that it produces, and the way such emo-tions are explained via language. Building on this data and recent machine learning approaches, we also design and test neural-based speakers that aim to emulate human emotional responses to visual art and provide associated explanations.
Why visual art? We focus on visual artworks for two rea-sons. First and foremost because art is often created with the intent of provoking emotional reactions from its viewers. In the words of Leo Tolstoy,“art is a human activity consisting in that one human consciously hands on to others feelings they have lived through, and that other people are infected by these feelings, and also experience them” [55]. Second, artworks, and abstract forms of art in particular, often defy simple explanations and might not have a single, easily-identiﬁable subject or label. Therefore, an affective re-sponse may require a more detailed analysis integrating the image content as well as its effect on the viewer. This is un-like most natural images that are commonly labeled through purely objective content-based labeling mechanisms captur-ing the objects or actions they include [14, 13]. Instead, by focusing on art, we aim to initiate a more nuanced percep-tual image understanding which, downstream, can also be applied to richer understanding of ordinary images.
We begin this effort by introducing a large-scale dataset termed ArtEmis [Art Emotions] that associates human emo-tions with artworks and contains explanations in natural lan-guage of the rationale behind each triggered emotion. 111569
Amusement 
“His mustache looks   like a bird soaring   through the clouds.”
Awe 
“The woman’s ability to handle   the bird so calmly inspires  a sense  of bewilderment.”
Contentment 
“The pale color palette of this painting is  very relaxing. I can imagine myself sitting  by the water listening to the birds.”
Excitement 
“The brushstrokes of blues   resemble an exotic   bird that is nested in the ocean.” s i m
E t r
A
Fear 
“This looks like a bird that has been   injured and is bleeding taking a flight.”
Sadness 
“This woman of higher   status looks sad, like a bird   who lives in a golden cage.”
Anger 
“The large black bird   has stolen the life   from the helpless rabbit.”
Something Else 
“The white bird stands out in the dark  background giving a sense of hope.”
Figure 1. Examples of affective explanations mentioning the word ‘bird’. In ArtEmis the annotators expose a wide range of abstract semantics and emotional states associated with the concept of a bird when attempting to explain their primary emotion (shown in boldface).
The exposed semantics include properties that are not directly visible: birds can be listened to, they ﬂy, they can bring hope, but also can be sad when they are in ‘golden cages’.
Novelty of ArtEmis. Our dataset is novel as it concerns an underexplored problem in computer vision: the forma-tion of linguistic affective explanations grounded on visual stimuli. Speciﬁcally, ArtEmis exposes moods, feelings, personal attitudes, but also abstract concepts like freedom or love, grounded over a wide variety of complex visual stimuli (see Section 3.2). The annotators typically explain and link visual attributes to psychological interpretations e.g., ‘her youthful face accentuates her innocence’, high-light peculiarities of displayed subjects, e.g., ‘her neck is too long, this seems unnatural’; and include imaginative or metaphorical descriptions of objects that do not directly ap-pear in the image but may relate to the subject’s experience;
‘it reminds me of my grandmother’ or ‘it looks like blood’ (over 20% of our corpus contains such similes).
Subjectivity of responses. Unlike existing captioning datasets, ArtEmis welcomes the subjective and personal an-gle that an emotional explanation (in the form of a caption) might have. Even a single person can have a range of emo-tional reactions to a given stimulus [41, 50, 10, 51] and, as shown in Fig. 2, this is ampliﬁed across different annota-tors. The subjectivity and rich semantic content distinguish
ArtEmis from, e.g., the widely used COCO dataset [14].
Figure 1 shows different images from ArtEmis with cap-tions including the word bird, where the imaginative and metaphorical nature of ArtEmis is apparent (e.g., ‘bird gives hope’ and ‘life as a caged bird’). Interestingly, despite this phenomenon, as we show later (Section 3.2), (1) there is of-ten substantial agreement among annotators regarding their dominant emotional reactions, and (2) our collected expla-nations are often pragmatic – i.e., they also contain refer-ences to visual elements present in the image (see Section 3.3).
Difﬁculty of emotional explanations. There is debate within the neuroscience community on whether human emotions are innate, generated by patterns of neural activity, or learned [53, 4, 8, 9]. There may be intrinsic difﬁculties with producing emotion explanations in language – thus the task can be challenging for annotators in ways that tradi-tional image captioning is not. Our approach is informed by signiﬁcant research that argues for the central role of language in capturing and even helping to form emotions
[36], including the Theory of Constructed Emotions [6, 7] by Lisa Feldman Barrett. Nevertheless, this debate suggests that caution is needed when comparing, under various stan-dard metrics, ArtEmis with other captioning datasets.
Affective neural speakers. To further demonstrate the potential of ArtEmis, we experimented with building a 211570
emotions, and amusement, awe, contentment, and excite-ment as positive emotions. The four negative emotions are considered universal and basic (as proposed by Ekman in
[22]) and have been shown to capture well the discrete emo-tions of the International Affective Picture System [11]. The four positive emotions are ﬁner grained versions of happi-ness [21]. We note that while awe can be associated with a negative state, following previous works ([41, 48]), we treat awe as a positive emotion in our analyses.
Deep learning, emotions, and art. Most existing works in Computer Vision treat emotions as an image classiﬁ-cation problem, and build systems that try to deduce the main/dominant emotion a given image will elicit [39, 62, 67, 33]. An interesting work linking paintings to textual de-scriptions of their historical and social intricacies is given in
[24]. Also, the work of [30] attempts to make captions for paintings in the prose of Shakespeare using language style transfer. Last, the work of [59] introduces a large scale dataset of artistic imagery with multiple attribute annota-tions. Unlike these works, we focus on developing machine learning tools for analyzing and generating explanations of emotions as evoked by artworks.
Captioning models and data. There is a lot of work and corresponding captioning datasets [64, 31, 54, 34, 40, 47] that focus on different aspects of human cognition. For in-stance COCO-captions [14] concern descriptions of com-mon objects in natural images, the data of Monroe et al. [42] include discriminative references for 2D monochromatic colors, Achlioptas et al. [1, 2] collects discriminative ut-terances for 3D objects, etc. There is correspondingly also a large volume on deep-net based captioning approaches
[38, 40, 56, 66, 43, 65, 43]. The seminal works of [58, 29] opened this path by capitalizing on advancements done in deep recurrent networks (LSTMs [27]), along with other classic ideas like training with Teacher Forcing [60]. Our neural speakers build on these ‘standard’ techniques, and
ArtEmis adds a new dimension to image-based captioning reﬂecting emotions.
Sentiment-driven captions. There exists signiﬁcantly less captioning work concerning sentiments (positive vs. negative emotions). Radford and colleagues [49] discov-ered that a single unit in recurrent language models trained without sentiment labels, is automatically learning concepts of sentiment; enabling sentiment-oriented manipulation by
ﬁxing the sign of that unit. Other early work like Senti-Cap [46] and follow-ups like [63], provided explicit sen-timent supervision to enable sentiment-ﬂavored language generation grounded on real-world images. These studies focus on the visual cues that are responsible for two emo-tional reactions (positive and negative) and, most impor-tantly, they do not produce emotion-explaining language. 311571
Figure 2. Examples of different emotional reactions for the same stimulus. The emotions experienced (in bold font) for the shown painting vary across annotators and are reasonably justiﬁed (next to each emotion, the annotator’s explanation is given). We note that 61% of all annotated artworks have at least one positive and one negative emotional reaction. See Section 3.2 for details. number of neural speakers, using deep learning language generation techniques trained on our dataset. The best of our speakers often produce well-grounded affective expla-nations, respond to abstract visual stimuli, and fare reason-ably well in emotional Turing tests (Section 6).
In summary, we make the following key contributions:
• We introduce ArtEmis, a large scale dataset of emo-tional reactions to visual artwork coupled with expla-nations of these emotions in language (Section 3).
• We show how the collected corpus contains utter-ances that are signiﬁcantly more affective, abstract, and rich with metaphors and similes, compared to ex-isting datasets (Sections 3.1-3.2).
• Using ArtEmis, we develop machine learning mod-els for dominant emotion prediction from images or text, and neural speakers that can produce plausible grounded emotion explanations (Sections 4 and 6). 2.