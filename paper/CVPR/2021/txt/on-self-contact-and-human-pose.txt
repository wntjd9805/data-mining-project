Abstract
People touch their face 23 times an hour, they cross their arms and legs, put their hands on their hips, etc.
While many images of people contain some form of self-contact, current 3D human pose and shape (HPS) regres-sion methods typically fail to estimate this contact. To ad-dress this, we develop new datasets and methods that sig-niﬁcantly improve human pose estimation with self-contact.
First, we create a dataset of 3D Contact Poses (3DCP) con-taining SMPL-X bodies ﬁt to 3D scans as well as poses from AMASS, which we reﬁne to ensure good contact. Sec-ond, we leverage this to create the Mimic-The-Pose (MTP) dataset of images, collected via Amazon Mechanical Turk, containing people mimicking the 3DCP poses with self-contact.
Third, we develop a novel HPS optimization method, SMPLify-XMC, that includes contact constraints and uses the known 3DCP body pose during ﬁtting to cre-ate near ground-truth poses for MTP images. Fourth, for more image variety, we label a dataset of in-the-wild im-ages with Discrete Self-Contact (DSC) information and use another new optimization method, SMPLify-DC, that ex-ploits discrete contacts during pose optimization. Finally, we use our datasets during SPIN training to learn a new 3D human pose regressor, called TUCH (Towards Under-standing Contact in Humans). We show that the new self-contact training data signiﬁcantly improves 3D human pose estimates on withheld test data and existing datasets like 3DPW. Not only does our method improve results for self-contact poses, but it also improves accuracy for non-contact poses. The code and data are available for research pur-poses at https://tuch.is.tue.mpg.de. 1.

Introduction
Self-contact takes many forms. We touch our bodies both consciously and unconsciously [24]. For the major limbs, contact can provide physical support, whereas we touch our faces in ways that convey our emotional state.
We perform self-grooming, we have nervous gestures, and
Figure 1. The ﬁrst column shows images containing self-contact.
In blue (left), results of TUCH, compared to SPIN results in violet (right). When rendered from the camera view, the estimated pose may look ﬁne (column two vs. four). However, when rotated, it is clear that training TUCH with self-contact information improves 3D pose estimation (column three vs. ﬁve). we communicate with each other through combined face and hand motions (e.g. “shh”). We may wring our hands when worried, cross our arms when defensive, or put our hands behind our head when conﬁdent. A Google search for “sitting person” or “thinking pose” for example, will re-turn images, the majority of which, contain self-contact.
Although self-contact is ubiquitous in human behavior, it is rarely explicitly studied in computer vision. For our purposes, self-contact comprises “self touch” (where the hands touch the body) and contact between other body parts (e.g. crossed legs). We ignore body parts that are frequently in contact (e.g. at the crotch or armpits) and focus on contact that is communicative or functional. Our goal is to estimate 3D human pose and shape (HPS) accurately for any pose.
When self-contact is present, the estimated pose should re-ﬂect the true 3D contact.
Unfortunately, existing methods that compute 3D bodies from images perform poorly on images with self-contact; see Fig. 1. Body parts that should be touching generally are not. Recovering human meshes from images typically involves either learning a regressor from pixels to 3D pose and shape [20, 23], or ﬁtting a 3D model to image features using an optimization method [4, 34, 45, 46]. The learn-ing approaches rely on labeled training data. Unfortunately, current 2D datasets typically contain labeled keypoints or segmentation masks but do not provide any information 9990
about 3D contact. Similarly, existing 3D datasets typically avoid capturing scenarios with self-contact because it com-plicates mesh processing. What is missing is a dataset with in-the-wild images and reliable data about 3D self-contact.
To address this limitation, we introduce three new datasets that focus on self-contact at different levels of de-tail. Additionally, we introduce two new optimization-based methods that ﬁt 3D bodies to images with contact information. We leverage these to estimate pseudo ground-truth 3D poses with self-contact. To make reasoning about contact between body parts, the hands, and the face pos-sible, we represent pose and shape with the SMPL-X [34] body model, which realistically captures the body surface details, including the hands and face. Our new datasets then let us train neural networks to regress 3D HPS from images of people with self-contact more accurately than state-of-the-art methods.
To begin, we ﬁrst construct a 3D Contact Pose (3DCP) dataset of 3D meshes where body parts are in contact. We do so using two methods. First, we use high-quality 3D scans of subjects performing self-contact poses. We ex-tend previous mesh registration methods to cope with self-contact and register the SMPL-X mesh to the scans. To gain more variety of poses, we search the AMASS dataset [28] for poses with self-contact or “near” self-contact. We then optimize these poses to bring nearby parts into full contact while resolving interpenetration. This provides a dataset of valid, realistic, self-contact poses in SMPL-X format.
Second, we use these poses to collect a novel dataset of images with near ground-truth 3D pose. To do so, we show rendered 3DCP meshes to workers on Amazon Mechanical
Turk (AMT). Their task is to Mimic The Pose (MTP) as ac-curately as possible, including the contacts, and submit a photograph. We then use the “true” pose as a strong prior and optimize the pose in the image by extending SMPLify-X [34] to enforce contact. A key observation is that, if we know about self-contact (even approximately), this greatly reduces pose ambiguity by removing degrees of freedom.
Thus, knowing contact makes the estimation of 3D human pose from 2D images more accurate. The resulting method,
SMPLify-XMC (for SMPLify-X with Mimicked Contact), produces high-quality 3D reference poses and body shapes in correspondence with the images.
Third, to gain even more image variety, we take images from three public datasets [16, 17, 27] and have them la-beled with discrete body-part contacts. This results in the
Discrete Self-Contact (DSC) dataset. To enable this, we de-ﬁne a partitioning of the body into regions that can be in contact. Given labeled discrete contacts, we extend SM-PLify to optimize body shape using image features and the discrete contact labels. We call this method SMPLify-DC, for SMPLify with Discrete Self-Contact.
Given the MTP and DSC datasets, we ﬁnetune a re-cent HPS regression network, SPIN [23]. When we have 3D reference poses, i.e. for MTP images, we use these as though they were ground truth and do not optimize them in SPIN. When discrete contact annotations are available, i.e. for DSC images, we use SMPLify-DC to optimize the
ﬁt in the SPIN training loop. Fine-tuning SPIN on MTP and
DSC signiﬁcantly improves accuracy of the regressed poses when there is contact (evaluated on 3DPW [43]). Surpris-ingly, the results on non-self-contact poses also improve, suggesting that (1) gathering accurate 3D poses for in-the-wild images is beneﬁcial, and (2) that self-contact can pro-vide valuable constraints that simplify pose estimation.
We call our regression method TUCH (Towards Under-standing Contact in Humans). Figure 1 illustrates the effect of exploiting self-contact in 3D HPS estimation. By training with self-contact, TUCH signiﬁcantly improves the physi-cal plausibility.
In summary, the key contributions of this paper are: (1)
We introduce TUCH, the ﬁrst HPS regressor for self-contact poses, trained end-to-end. (2) We create a novel dataset of 3D human meshes with realistic contact (3DCP). (3) We de-ﬁne a “Mimic The Pose” MTP task and a new optimization method to create a novel dataset of in-the-wild images with accurate 3D reference data. (4) We create a large dataset of images with reference poses that use discrete contact la-bels. (5) We show in experiments that taking self-contact information into account improves pose estimation in two ways (data and losses), and in turn achieves state-of-the-art results on 3D pose estimation benchmarks. (6) The data and code are available for research purposes. 2.