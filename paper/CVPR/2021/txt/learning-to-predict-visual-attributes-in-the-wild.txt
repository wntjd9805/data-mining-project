Abstract
Visual attributes constitute a large portion of informa-tion contained in a scene. Objects can be described using a wide variety of attributes which portray their visual ap-pearance (color, texture), geometry (shape, size, posture), and other intrinsic properties (state, action). Existing work is mostly limited to study of attribute prediction in speciﬁc domains. In this paper, we introduce a large-scale in-the-wild visual attribute prediction dataset consisting of over 927K attribute annotations for over 260K object instances.
Formally, object attribute prediction is a multi-label clas-siﬁcation problem where all attributes that apply to an ob-ject must be predicted. Our dataset poses signiﬁcant chal-lenges to existing methods due to large number of attributes, label sparsity, data imbalance, and object occlusion. To this end, we propose several techniques that systematically tackle these challenges, including a base model that uti-lizes both low- and high-level CNN features with multi-hop attention, reweighting and resampling techniques, a novel negative label expansion scheme, and a novel super-vised attribute-aware contrastive learning algorithm. Us-ing these techniques, we achieve near 3.7 mAP and 5.7 overall F1 points improvement over the current state of the art. Further details about the VAW dataset can be found at https://vawdataset.com/ 1.

Introduction
Learning to predict visual attributes of objects is one of the most important problems in computer vision and image understanding. Grounding objects with their correct visual attribute plays a central role in a variety of computer vision tasks, such as image retrieval and search [32], tagging, re-ferring expression [35], visual question answering (VQA)
[4, 33], and image captioning [7].
While several existing works address attribute predic-tion, they are limited in many ways. Objects in a visual
*A portion of this work was done during Khoi Pham’s internship at
Adobe Research.
Positive ✔	
Brown, Wooden,
Curved, Clean
Negative ✘
White,	Metallic,
Square
Table
Unlabeled ? 
Large, Flat,
Painted, Indoors...
Positive ✔	
Yellow, Round,
Ceramic, Full
Negative ✘
White,	Square,
Glass,	Empty
Plate
Unlabeled ?
Red, Colorful,
Shallow, Dirty, ...
Positive ✔	
Pink, Leaning,
Floral
Negative ✘
Yellow,	Held,
Dried
Flower
Unlabeled ?
Bright, Cut, Light
Red, Patterned, ...
Positive ✔	
Brown, Yellow,
Colorful
Negative ✘
Chocolate,
Circular,	Burnt
Cookie
Unlabeled ?
White, Dark, Big
Frosted, ...
Figure 1. Example annotations in our dataset. Each possible attribute-object category pair is annotated with at least 50 exam-ples consisting of explicit positive and negative labels. scene can be described using a vast number of attributes, many of which can exist independently of each other. Due to the variety of possible object and attribute combinations, it is a daunting task to curate a large-scale visual attribute prediction dataset. Existing works have largely ignored large-scale visual attribute prediction in-the-wild and have instead focused only on domain-speciﬁc attributes [19, 43], datasets consisting of very small number of attribute-object pairs [51], or are rife with label noise, ambiguity and la-bel sparsity [38]. Similarly, while attributes can form an important part of related tasks such as VQA, captioning, re-ferring expression, these works do not address the unique challenges of attribute prediction. Existing work also fails to address the issue of partial labels, where only a small sub-set of all possible attributes are annotated. Partial labels and the lack of explicit negative labels make it challenging to train or evaluate models for large-scale attribute prediction.
To address these problems, we propose a new large-scale vi-sual attribute prediction dataset for images in the wild that includes both positive and negative annotations.
Our dataset, called visual attributes in the wild (VAW), consists of over 927K explicitly labeled positive and neg-ative attribute annotations applied to over 260K object in-stances (with 620 unique attributes and 2,260 unique ob-13018
ject phrases). Due to the number of combinations possi-ble, it is prohibitively expensive to collect exhaustive at-tribute annotations for each instance. However, we ensure that every attribute-object phrase pair in the dataset has a minimum of 50 positive and negative annotations. With density of 3.56 annotations per instance, our dataset is 4.9 times denser compared to Visual Genome while also pro-viding negative labels. Additionally, annotations in VAW are visually-grounded with segmentation masks available for 92% of the instances. Formally, our VAW dataset pro-poses attribute prediction as a long-tailed, partially-labeled, multi-label classiﬁcation problem.
We explore various state-of-the-art methods in attribute prediction and multi-label learning and show that the VAW dataset poses signiﬁcant challenges to existing work. To this end, we ﬁrst propose a strong baseline model that consid-ers both low- and high-level features to address the hetero-geneity in features required for different classes of attributes (e.g., color vs. action), and is modeled with multi-hop at-tention and an ability to localize the region of the object of interest by using partially available segmentation masks.
We also propose a series of techniques that are uniquely suited for our problem. Firstly, we explore existing works that address label imbalance between positive and negative labels. Next, we describe a simple yet powerful scheme that exploits linguistic knowledge to expand the number of neg-ative labels 15-fold. Finally, we propose a supervised con-trastive learning approach that allows our model to learn more attribute discriminative features. Through extensive ablations, we show that most of our proposed techniques are model-agnostic, producing improvements not only on our baseline but also other methods. Our ﬁnal model is called Supervised Contrastive learning with Negative-label
Expansion (SCoNE), which surpasses state-of-the-art mod-els by 3.5 mAP and 5.7 overall F1 points.
Our paper makes the following major contributions:
• We create a new large-scale dataset for visual attribute prediction in the wild (VAW) that addresses many short-comings in existing literature and demonstrate that VAW poses considerable difﬁculty to existing algorithms.
• We design a strong baseline model for attribute predic-tion using existing visual attention technique. We fur-ther extend this baseline to our novel attribute learn-ing paradigm called Supervised Contrastive learning with
Negative-label Expansion (SCoNE) that considerably ad-vances the state of the art.
• Through extensive experimentation, we show the efﬁcacy of both our proposed model and our proposed techniques. 2.