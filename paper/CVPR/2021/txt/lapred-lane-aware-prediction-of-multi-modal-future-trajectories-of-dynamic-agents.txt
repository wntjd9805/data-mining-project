Abstract
In this paper, we address the problem of predicting the future motion of a dynamic agent (called a target agent) given its current and past states as well as the informa-tion on its environment. It is paramount to develop a pre-diction model that can exploit the contextual information in both static and dynamic environments surrounding the target agent and generate diverse trajectory samples that are meaningful in a trafﬁc context. We propose a novel prediction model, referred to as the lane-aware prediction (LaPred) network, which uses the instance-level lane en-tities extracted from a semantic map to predict the multi-modal future trajectories. For each lane candidate found in the neighborhood of the target agent, LaPred extracts the joint features relating the lane and the trajectories of the neighboring agents. Then, the features for all lane candi-dates are fused with the attention weights learned through a self-supervised learning task that identiﬁes the lane candi-date likely to be followed by the target agent. Using the instance-level lane information, LaPred can produce the trajectories compliant with the surroundings better than 2D raster image-based methods and generate the diverse fu-ture trajectories given multiple lane candidates. The exper-iments conducted on the public nuScenes dataset and Argo-verse dataset demonstrate that the proposed LaPred method signiﬁcantly outperforms the existing prediction models, achieving state-of-the-art performance in the benchmarks. 1.

Introduction
Predicting the future motion of a dynamic agent given its past trajectory is crucial for self-driving robots and vehicles
∗Corresponding author: Jun Won Choi to conduct path planning and collision avoidance. However, predicting motion in realistic environments is challenging because the motion of the dynamic agent is determined by various indirectly observed factors, including the agent’s in-tention, the static environment around the agent, and the in-teraction with other agents. Such uncertainties in prediction tasks entail multiple plausible trajectories that an agent can take to reach its intended goals. Speciﬁcally, the distribution of an agent’s future trajectory will be multi-modal in that the agent can exhibit different maneuvers (e.g., right turn, left turn, or straight) for given speciﬁc scenes (e.g., four-way crossroad), or change the lanes and adjust its speed in-teracting with other agents. Therefore, a prediction model should suggest more than one plausible trajectory samples that follow the distribution for the given situation.
To predict such a diverse set of trajectories, the model must understand the environmental context consisting of social patterns in temporal motion as well as observa-tions for static scene environment via sensors or seman-tic maps. Therefore, the model’s ability to extract and meaningfully represent such multiple cues is crucial. Deep neural networks (DNN) suit this task well, owing to their large capacity and capability of end-to-end learning rep-resentation. Previous art has presented sophisticated ar-chitectures for interaction modeling [1, 6, 8, 10, 12, 15, 16, 22, 23, 26, 28, 29, 30, 34], static-scene processing
[3, 5, 14, 16, 17, 24, 25, 26, 27, 32, 34], and multi-modal trajectory modeling [3, 5, 10, 11, 16, 22, 24, 25, 26, 33].
However, intermediate logic in DNN models is often not interpretable, and thus human designers have limited space to intervene with a prediction instance. For example, most prediction models do not allow to explicitly condition on a particular set of lanes on the road, while such decisions im-plicitly stem from random input noise. This is a practical drawback when the models are to be used in self-driving robots, causing them to sample inefﬁciently many trajecto-114636
ries to cover all plausible modes in the future. 2.