Abstract
New results suggest strong limits to the feasibility of ob-ject classiﬁcation from human brain activity evoked by im-age stimuli, as measured through EEG. Considerable prior work suffers from a confound between the stimulus class and the time since the start of the experiment. A prior at-tempt to avoid this confound using randomized trials was unable to achieve results above chance in a statistically sig-niﬁcant fashion when the data sets were of the same size as the original experiments. Here, we attempt object classiﬁ-cation from EEG using an array of methods that are rep-resentative of the state-of-the-art, with a far larger (20×) dataset of randomized EEG trials, 1,000 stimulus presenta-tions of each of forty classes, all from a single subject. To our knowledge, this is the largest such EEG data-collection effort from a single subject and is at the bounds of feasi-bility. We obtain classiﬁcation accuracy that is marginally above chance and above chance in a statistically signiﬁcant fashion, and further assess how accuracy depends on the classiﬁer used, the amount of training data used, and the number of classes. Reaching the limits of data collection with only marginally above-chance performance suggests that the prevailing literature substantially exaggerates the feasibility of object classiﬁcation from EEG. 1.

Introduction
There has been considerable recent interest in applying deep learning to electroencephalography (EEG). Two re-cent survey papers [7, 33] collectively contain 372 refer-ences. Much of this work attempts to classify human brain activity evoked from visual stimuli. A recent CVPR oral
[35] claims to decode one of forty object classes when sub-jects view images from ImageNet [9] with 82.9% accu-racy. Considerable follow-on work uses the same dataset
[5, 10, 11, 12, 13, 15, 16, 17, 19, 21, 25, 26, 27, 28, 29, 30, 43, 46, 47, 48, 49], often claiming even higher accu-racy. Li et al. [22] demonstrate that this classiﬁcation ac-curacy is severely overinﬂated due to ﬂawed experimental design. All stimuli of the same class were presented to sub-jects as a single block (Fig. 1a). Further, training and test samples were taken from the same block. Because all EEG data contain long-term temporal correlations that are unre-lated to stimulus processing and their design confounded block-effects with class label, Spampinato et al. [35] were classifying these long-term temporal patterns, not the stim-ulus class. Because the training and test samples were taken in close temporal proximity from the same block, the tem-poral correlations in the EEG introduced label leakage be-tween the training and test data sets. When the experiment of Spampinato et al. [35] is repeated with randomized trials, where stimuli of different classes are randomly intermixed, classiﬁcation accuracy drops to chance [22].
Another recent paper [8] attempts to remedy the short-comings of a block design by recording two different ses-sions for the same subject, each organized as a block de-sign, one to be used as training data and one to be used as test data. However, both sessions used the same stimulus presentation order (Fig. 1b). Li et al. [22] demonstrate that classiﬁcation accuracy can even be severely inﬂated with such a cross-session design that employs the same stim-ulus presentation order in both sessions due to the same long-term transients that are unrelated to stimulus process-ing. While an analysis of training and test sets coming from different sessions with the same stimulus presentation or-der yields lower accuracy than an analysis where they come from the same session, accuracy drops to chance when the two sessions have different stimulus presentation order.
All this prior work is fundamentally ﬂawed due to im-proper experimental design. Essentially, the EEG signal en-codes a clock and any experimental design where stimulus class correlates with time since beginning of experiment al-lows classifying the clock instead of the stimuli. This means that all data collected in this fashion is irreparably contami-nated.
Li et al. [22] attempted to replicate the experiment of
Spampinato et al. [35] six times with nine different classi-ﬁers, including the LSTM employed by them, with random-ized trials (Fig. 1c) instead of a block design. All attempts failed, yielding chance performance.
Given that considerable prior work suffers from this 3845
training test training test g g a e   1 c l a s s  1  i m e   2 c l a s s  1  i m e   3 c l a s s  1  i m a a g c l a s s  1  i m g g a e   1 c l a s s  1  i m e   2 c l a s s  1  i m e   3 c l a s s  1  i m a a g c l a s s  1  i m e   4 g a e   4 g a g g a e   1 c l a s s  2  i m e   2 c l a s s  2  i m e   3 c l a s s  2  i m a a g c l a s s  2  i m e   4 g a training test g g a e   1 c l a s s  2  i m e   2 c l a s s  2  i m e   3 c l a s s  2  i m a a g c l a s s  2  i m e   4 g a later g g a e   1 c l a s s  1  i m e   2 c l a s s  1  i m e   3 c l a s s  1  i m a a g c l a s s  1  i m e   4 g a g g a e   1 c l a s s  2  i m e   2 c l a s s  2  i m e   3 c l a s s  2  i m a a g c l a s s  2  i m e   4 g a g 3  tr a i n i n e   6  tr a i n i n e   2 e   1 4  i m c l a s s  3  i m c l a s s  1  i m g g e   4  tr a i n i n 1  t e st 4  t e st 5  t e st e   1 e   1 g g g a 9  i m 4  i m a a c l a s s  7  i m c l a s s  2 c l a s s  1 c l a s s  2 a a a g g g a g 9  i m c l a s s  1 c l a s s  1 g g e   4  tr a i n i n e   3  tr a i n i n 7  i m a g (a) (b) (c)
Figure 1. Stimulus presentation order and training/test splits em-ployed by (a) Spampinato et al. [35], (b) Cudlenco et al. [8], and (c) randomized trials. (a) and (b) confound stimulus class with time since beginning of experiment. block confound, here, we sought to systematically re-assess the performance of state-of-the-art approaches to object classiﬁcation from EEG, using a large EEG dataset that does not suffer from this confound. Speciﬁcally, we ask the following seven questions: 1. Is it possible to decode object class from EEG data recorded from subjects viewing image stimuli with ran-domized stimulus presentation order? 2. If so, how many distinct classes can one decode? 3. If so, how much training data is needed? 4. If so, which classiﬁcation architectures that are cur-rently standard in the literature allow such decoding? 5. Does the inclusion of EEG data contaminated with ar-tifacts (e.g. from subject movement and tightening of facial muscles) limit the decoding ability? 6. Does use of a classiﬁcation method that lacks inherent ability to model temporal variation in the EEG signal limit decoding ability? 7. Can one perform such decoding across subjects?
To answer these questions, we collected EEG recordings from 40,000 stimulus presentations to a single subject (and reanalyze data from Li et al. [22] for cross-subject clas-siﬁcation). To our knowledge, this is by far the largest recording effort of its kind. Moreover, we argue that col-lecting such a large corpus is at the bounds of feasibility; it is infeasible to collect any appreciably larger corpus. With this corpus we achieve a modest ability to decode stimu-lus classes with accuracy above chance in a statistically sig-niﬁcant fashion. By using a greedy method to determine the most discriminable n classes for 2 ≤ n ≤ 40, and de-termining the classiﬁcation accuracy for each such set, we show that forty classes is at the limit of feasibility. Further, by repeating the experiments with successively larger frac-tions of the dataset, we determine that at least half of this large dataset is needed to achieve this accuracy. Finally, we show that two architectures previously claimed to yield high accuracy on this kind of task, namely the LSTM archi-tecture evaluated in Spampinato et al. [35], and the LSTM architecture [35] and the EEGChannelNet architecture [28] evaluated in Palazzo et al. [28], are unable to achieve classi-ﬁcation accuracy above chance in a statistically signiﬁcant fashion. The only four classiﬁers that we tried that achieve classiﬁcation accuracy above chance in a statistically signif-icant fashion are a support vector machine (SVM [6]), the one-dimensional convolutional neural network (1D CNN) previously reported by Li et al. [22], the EEGNet architec-ture [20], and the SyncNet architecture [23].
Before proceeding, we stress that we are solely con-cerned with forced-choice one-out-of-n classiﬁcation of ob-jects from EEG evoked by single-trial image stimuli, be-cause this is precisely the paradigm employed by consid-erable recent work [5, 10, 11, 12, 13, 15, 16, 17, 19, 21, 25, 26, 27, 28, 29, 30, 35, 43, 46, 47, 48, 49]. Further-more, carefully calibrating the performance achievable for this EEG paradigm is particularly important, because visual object classiﬁcation is a canonical problem in computer vi-sion where there is increasing interest in incorporating in-sights from brain data. We do not comment on work that involves other paradigms [18, 31, 41].
Summary of contribution: While Li et al. [22] showed that all this recent work [5, 10, 11, 12, 13, 15, 16, 17, 19, 21, 25, 26, 27, 28, 29, 30, 35, 43, 46, 47, 48, 49] hadn’t done what they claimed to have done, we go far beyond that here and show that they couldn’t have done what they claimed to have done. I.e. object classiﬁcation for single trial EEG is infeasible, at least with methods that are currently standard in the literature. 2. Data Collection
Spampinato et al. [35] selected ﬁfty ImageNet images from each of forty ImageNet synsets as stimuli. With one exception, we employed the same ImageNet synsets as classes (Table 1). Since we sought 1,000 images from each class, and one class, n03197337, digital watch, contained insufﬁcient images at time of download, we replaced that class with n04555897, watch.
We downloaded all ImageNet images of each of the forty classes that were available on 28 July 2019, ran-domly selected 1,000 images for each class, resized them to 1920×1080, preserving aspect ratio by padding them with black pixels either on the left and right or top and bottom, but not both, to center the image. All but one such image was either RGB or grayscale. One image, n02492035 15739, was in the CMYK color space so was transcoded to RGB for compatibility with our stimulus pre-sentation software.
The 40,000 images were partitioned into 100 sets of 400 images each. Each set of 400 images contained exactly ten images of each of the forty classes. Each set of 400 im-ages was randomly permuted. The order of the 100 sets of images was also randomly permuted.
A single adult male subject viewed all 100 sets of im-ages while recording EEG. Recording was conducted over 3846
n02106662 German shepherd n02504458 African elephant n02951358 canoe n04555897 watch n03445777 golf ball n03773504 missile n03888257 parachute n04120489 running shoe n02281787 lycaenid n02607072 anemone ﬁsh n02124075 Egyptian cat n02510455 giant panda n02992529 cellular telephone n03063599 coffee mug n03272010 electric guitar n03452741 grand piano n03775071 mitten n03982430 pool table n07753592 banana n03272562 electric locomotive n03297495 espresso maker n03590841 jack-o-lantern n03584829 iron n03792972 mountain tent n03792782 mountain bike n04069434 reﬂex camera n04044716 radio telescope n11939491 daisy n07873807 pizza n02389026 sorrel n02690373 airliner n03100240 convertible n02492035 capuchin n02906734 broom n03180011 desktop computer n03376595 folding chair n03709823 mailbag n03877472 pajama n04086273 revolver n13054560 bolete
Table 1. ImageNet synsets employed as classes in our experiment. ten sessions. Each session nominally recorded data from ten sets of images, though some sessions contained fewer sets, some sessions contained more sets, and some sets were re-peated due to experimenter error. (Runs per session: 10, 8, 10, 11, 11, 10, 10, 10, 10, 10. Run 19 was repeated after run 20 because one image was discovered to be in CYMK.
Run 43 was repeated because one earlobe electrode was off.) When sets were repeated, only one error-free set was retained. Each recording session was nominally about six hours in duration. The subject typically took breaks after every three or so sets of images. As the EEG lab was be-ing used for other experiments as well, recording was con-ducted over roughly a half-year period. (Session dates: 21, 28 Aug 2019, 3, 10, 16, 17 Sep 2019, 13, 14, 20, 21 Jan 2020.)
Our design is counterbalanced at the whole-experiment level, the session level, and the run level. Each unit (ex-periment, session, or run) has the same number of stimulus presentations for each class with no duplicates. Thus at any level, the baseline performance is chance. This allows par-tial analyses of arbitrary combinations of individual runs or sessions with simple calculation of statistical signiﬁcance.
Each set of 400 images was presented in a single EEG run lasting 20 minutes and 20 seconds. Each run started with 10 s of blanking, followed by 400 stimulus presenta-tions, each lasting 2 s, with 1 s of blanking between adjacent stimulus presentations, followed by 10 s of blanking at the end of the run. There was no block structure within each run.1
EEG data was recorded from 96 channels at 4,096 Hz with 24-bit resolution using a BioSemi ActiveTwo recorder and a BioSemi gel electrode cap. Two additional channels were used to record the signal from the earlobes for rerefer-encing. The BioSemi system uses the so called driven-right-leg circuit design to improve the common-mode rejection ratio of the ampliﬁer beyond conventional differential am-pliﬁers [37]. Within this design, a large DC offset at an elec-trode indicates scalp contact problems; this DC offset was monitored in real time to ensure good electrode-scalp con-tact by adding extra gel as needed. A trigger was recorded in the EEG data to indicate stimulus onset. Preprocessing 1Spampinato et al. [35] employed a design where stimuli were pre-sented in blocks of ﬁfty images. Each stimulus was presented for 0.5 s with no blanking between images, but with 10 s blanking between blocks.
During a pilot run of our experiment with this design, the subject reported that it was difﬁcult and tedious to attend to the stimuli when presented rapidly without pause, thus motivating adoption of our modiﬁed design.
Our longer trials with pauses attempt to reduce the potential of cross-stimulus contamination. software veriﬁes that there are exactly 400 triggers in each recording.2
The current analysis uses only the ﬁrst 500 ms after stim-ulus onset for each stimulus presentation, even though 2 s of data were recorded. Further, the current analysis decimated the data from 4,096 Hz to 1,024 Hz. This was done to speed the analysis. The full dataset is available for potential future enhanced analysis.
Each session was recorded with a single capping with the cap remaining in place when the subject took breaks between runs. With fMRI data, the anatomical informa-tion captured can be used to align volumes within a run to compensate for subject motion, between runs to com-pensate for subjects exiting and reentering the scanner (co-registration), and between subjects to compensate for vari-ations in brain anatomy (spatial normalization).
In con-trast, for EEG data, there are no established methods to ad-just for differing brain/scalp anatomy when combining data from multiple subjects; often approximately corresponding scalp locations are treated as equivalent. For this reason, we recorded data from a single subject to eliminate the need to align across subjects. By performing capping only once per session and choosing a cap size to yield a snug ﬁt, any within-session alignment issues are obviated. To minimize across-session misalignment, the same cap with pre-cut ear holes was used across sessions with the vertex marking on the cap (location Cz) positioned to be geodesically equidis-tant from the the nasion and inion in the front-back direc-tion, and equidistant from the left and right pre-auricular points in the left-right direction. Furthermore, visual in-spection was done from vantage points directly in front and at the back of the subject to check that the FPz, Fz, Cz, Pz, and Oz markings on the cap fell on the geodesic connecting the nasion and inion.
To check whether the subject consistently viewed the im-ages presented, online trial averaging of the EEG data was performed in every session to obtain evoked responses that are phase-locked to the onset of the images. Data from two occipital channels (C31 and C32) were bandpass ﬁltered in the 1–40 Hz range and epochs of 800 ms duration were segmented out synchronously following the onset of each image. Epochs with peak-to-trough ﬂuctuations exceeding 100 µV were discarded and the remaining epochs were av-eraged together to yield an 800 ms-long evoked response.
A clear and robust N1-P2 onset response pattern was dis-2Due to experimenter error, one recording, run 14, continued beyond 400 stimulus presentations. The recordings for the extra stimulus presen-tations were harmlessly discarded. 3847
cernible in the evoked response traces obtained in each of the 100 runs, consistent with the subject viewing the im-ages as instructed. Note that all online averaging proce-dures (e.g. ﬁltering) were done to data in a separate buffer; the raw unprocessed data from 96 channels was saved for ofﬂine analysis. 3. Preprocessing
The raw EEG data was recorded in bdf ﬁle format, a sin-gle ﬁle for each of the 100 runs.3 We performed minimal preprocessing on this data, independently for each run, ﬁrst rereferencing the data to the earlobes, then extracting ex-actly 0.5 s of data starting at each trigger, then z-scoring each channel of the extracted samples for each run inde-pendently, so that the extracted samples for each channel of each run have zero mean and unit variance, and then ﬁnally decimating the signal from 4,096 Hz to 1,024 Hz. No ﬁlter-ing was performed. After rereferencing, there is no appre-ciable line noise to ﬁlter. Randomized trials preclude classi-fying long-term transients, thus there is no need to ﬁlter out such transients. Note that this preprocessing is minimal; we discuss below the prospects of improving the SNR of the neural signals by removing movement and facial muscle ar-tifacts.
The data was then randomly partitioned into ﬁve equal-sized folds, each containing the same number of samples of each class. All analyses below report average across ﬁve-fold round-robin leave-one-fold-out cross validation, taking four folds in each split as training data and the remaining fold as test data. When performing analyses on subsets of the data, the sizes of the folds, and thus the sizes of the training and test sets, varied proportionally. 4. Classiﬁers
The analyses below employ eight different classiﬁers, an LSTM [14], a nearest neighbor classiﬁer (k-NN), an
SVM, a two-layer fully-connected neural network (MLP), 1D CNN, EEGNet [20], SyncNet [23], and EEGChannel-Net [28]. The LSTM is the same as Spampinato et al.
[35] with the modiﬁcations discussed previously by Li et al. [22]. The k-NN, SVM, MLP, and 1D CNN are as de-scribed previously by Li et al. [22], with minor differences resulting from the fact that here the signals contain 512 tem-poral samples instead of 440. Two of the classiﬁers (k-NN and SVM) are classical baseline machine-learning methods.
The remaining six classiﬁers are all neural networks, one (MLP) being shallow and ﬁve (LSTM, 1D CNN, EEGNet,
SyncNet, and EEGChannelNet) being deep-learning meth-ods. 3All code and raw data discussed in this manuscript are available at http://dx.doi.org/10.21227/bc7e-6j47.
LSTM k-NN 2.2% 2.1% 5.0%∗
SVM MLP 2.5% 1D CNN EEGNet 7.0%∗
SyncNet EEGChannelNet 2.5% 2.5% 5.1%∗
Table 2. Classiﬁcation accuracy on the validation set, averaged across all ﬁve folds, for each classiﬁer. Here and throughout, starred values indicate statistical signiﬁcance above chance (p < 0.005) by a binomial cmf. 5. Analyses
To answer the ﬁrst question, Is it possible to decode ob-ject class from EEG data recorded from subjects viewing image stimuli with randomized stimulus presentation or-der?, we trained and tested each of the eight classiﬁers on the entire dataset of 1,000 stimulus presentations of each of forty classes, using ﬁve-fold cross validation (Table 2). All analyses here and below test statistical signiﬁcance above chance using p < 0.005 against a null hypothesis by a bi-nomial cmf with a Bonferroni [4] correction.4 Only three classiﬁers, SVM, 1D CNN, and EEGNet, yield statistically signiﬁcant above-chance accuracy.5
To answer the second question, How many distinct classes can one decode?, we performed a greedy analysis, independently for each classiﬁer. We ﬁrst trained and tested a classiﬁer for each pair of distinct classes. Fig. 2 depicts the resulting average validation accuracies. Only one clas-siﬁer, SVM, yielded a statistically signiﬁcant above-chance accuracy for some pair. It did so for a large number of pairs.
We then selected the pair with the highest average validation accuracy, independently for each classiﬁer, and selected the
ﬁrst element of this pair as the seed for a class sequence for that classiﬁer. Then for each n between two and forty, we greedily and incrementally added one more class to the class sequence for each classiﬁer. This class was selected by trying each unused class, adding it to the class sequence, training and testing a classiﬁer with that addition, and se-lecting the added class that led to the highest classiﬁcation accuracy. This yielded a distinct class sequence of next-most-discriminable classes for each classiﬁer, along with an average validation accuracy on each initial preﬁx of that se-t 4A binomial pmf(k, t, q) = (cid:0) k(cid:1)qk(1 − q)t−k denotes the probabil-ity that exactly k out of t trials succeed where each trial has success prob-n k′=k pmf(k′, t, q) denotes the ability q. A binomial cmf(k, t, q) = P probability that k or more out of t trials succeed. We deem a classiﬁcation analysis with t trials, n classes, and computed accuracy of a to be above chance in a statistically signiﬁcant fashion when cmf(⌊at⌋, t, 1 c ) < 0.005.
All claims of statistical signiﬁcance, i.e. SVM, 1D CNN, and EEGNet in
Table 2, Fig. 3(right), and Table 3(b), and SVM, 1D CNN, EEGNet, and
SyncNet in Fig. 3(left) and Table 3(a), correct for m multiple comparisons by requiring cmf(⌊at⌋, t, 1 m , where m = 3 (SVM, 1D CNN, and EEGNet) for Table 2, Fig. 3(right), and Table 3(b) and m = 4 (SVM, 1D CNN, EEGNet, and SyncNet) for Fig. 3(left) and Table 3(a). Claims of lack of statistical signiﬁcance need no correction. c ) < 0.005 5All analyses reported here report classiﬁcation accuracy, as appropri-ate for a forced-choice one-out-of-n classiﬁcation task. All relevant work that employs this task [5, 10, 11, 12, 13, 15, 16, 17, 19, 21, 25, 26, 27, 28, 29, 30, 35, 43, 46, 47, 48, 49] similarly reports classiﬁcation accuracy.
Other metrics such as AUC and F1 would be inappropriate for this task, as it is a classiﬁcation task, not a detection task. 3848
quence (Fig. 3left and Table 3b).6 With the exception of a single data point, the MLP classiﬁer achieving marginally signiﬁcant above-chance classiﬁcation accuracy for n = 29, only four classiﬁers, SVM, 1D CNN, EEGNet, and Sync-Net, yielded statistically signiﬁcant above-chance accuracy for any number of classes. SVM and 1D CNN yielded sta-tistically signiﬁcant above-chance accuracy for all numbers of classes, EEGNet yielded statistically signiﬁcant above-chance accuracy for n ≥ 4, and SyncNet yielded statisti-cally signiﬁcant above-chance accuracy for 3 ≤ n ≤ 27.
To answer the third question, How much training data is needed?, we performed an analysis where classiﬁers were trained and tested on progressively larger portions of the dataset, starting with 10%, incrementing by 10%, until the full dataset was tested. This was done by taking the ﬁrst ten runs and incrementally adding the next ten runs. This was done only for SVM, 1D CNN, and EEGNet, as only these had statistically signiﬁcant above-chance accuracy for the full set of classes (Fig. 3right and Table 3b). Validation accuracy generally increases with the availability of more training data, though growth tapers off demonstrating di-minishing returns.
The fourth question, Which classiﬁcation architectures that are currently standard in the literature allow such de-coding?, was implicitly answered by the above three anal-yses. Only SVM, 1D CNN, EEGNet, and SyncNet answer any of the above three questions in the afﬁrmative. SVM, 1D CNN, and EEGNet answer all of the above three ques-tions in the afﬁrmative.
To answer the ﬁfth question, Does the inclusion of EEG data contaminated with artifacts limit the decoding abil-ity?, we conducted an additional analysis. While we had a very cooperative subject, the task of watching 40,000 im-age stimuli can be tedious. It is conceivable that the EEG recordings suffer from artifacts that reduce classiﬁcation ac-curacy. To assess this, we repeated the analyses from Ta-ble 2 for the three classiﬁers (SVM, 1D CNN, and EEG-Net) for which we have observed statistically signiﬁcant above-chance classiﬁcation accuracy, after performing ar-tifact removal. We computed the swing for each time point in each trial, i.e. the value of the maximal channel minus the value of the minimal channel, computed the overall swing for each trial as the maximal swing over all time points in that trial, and discarded trials with greater than 600 micro-Volt swing. A total of 852 out of 40,000 trials (2.13%) were discarded, maintaining the same splits. This procedure eliminates trials contaminated by appreciable artifacts from subject movement and tightening of facial muscles. As a result, the splits were no longer perfectly counterbalanced. 6The analyses reported in this manuscript require about a year of com-pute time on a cluster with 144 cores and 54 Titan V GPUs. The results for EEGChannelNet for n ≥ 26 in Fig. 3(left) and Table 3(a) are being computed but were not available in time for publication.
Table 3(c) shows the results. While there is improvement for 1D CNN (5.1% to 5.3%) and EEGNet (7.0% to 7.3%), the improvement is not statistically signiﬁcant, suggesting that artifacts are not the limiting factor in classiﬁcation ac-curacy.
To answer the sixth question, Does use of a classiﬁca-tion method that lacks inherent ability to model temporal variation in the EEG signal limit decoding ability?, we con-ducted an additional analysis. The LSTM, 1D CNN, EEG-Net, SyncNet, and EEGChannelNet classiﬁers all provide an inherent ability to compensate for temporal variation in the signal, both in the onset time of brain processing and its rate. The k-NN, SVM and MLP classiﬁers lack such an in-herent ability. We asked whether such ability materially af-fects classiﬁcation accuracy. To this end, we computed 257-point power spectral density [36] of the raw EEG signal on a per trial and per channel basis and repeated the analysis with the k-NN, SVM, and MLP classiﬁers on this frequency-domain signal instead of the original time-domain signal (Table 3d). Such frequency-domain analyses appears not to improve upon the time-domain analyses. We hypothesize two reasons for this. First, we recorded the stimulus on-set time as a trigger in the EEG signal and synchronize our analyses to this. This eliminates variation in onset time of the availability of visual information to the brain. Second, it appears that there is not much variation in brain processing rate for this task, and that the phase content of the EEG re-sponse is relatively uninformative for object classiﬁcation.
Finally, to answer the seventh question, Can one perform such decoding across subjects?, we performed an additional analysis.
It appears that to achieve even modest statisti-cally signiﬁcant above-chance classiﬁcation accuracy, one needs enormous amounts of data.
It is taxing to collect this data from a single subject. Perhaps, one could spread the burden by collecting data from many subjects, perhaps even across many sites. Doing this, however, would require cross-subject analyses, i.e. training classiﬁers on one set of subjects and testing on a different set of subjects. We con-ducted an analysis to assess the ability to do so. We reana-lyzed data from six subjects on a smaller set of ﬁfty shared stimuli for each of the same forty classes, all collected with randomized trials [22] using a leave-one-subject-out six-fold cross-validation paradigm with the three classiﬁers (SVM, 1D CNN, and EEGNet) for which we have observed statistically signiﬁcant above-chance classiﬁcation accuracy (Table 3e). While this analysis (12,000 trials) is not as small as the analyses in Li et al. [22] (2,000 trials) it is also not as large as the above analyses (40,000 trials). It corresponds to the 30% mark in Fig. 3(right) and Table 3(b). Note that while 1D CNN performs marginally above chance in a sta-tistically signiﬁcant fashion, the cross-subject analysis is far worse (2.9% vs. 4.0% for SVM, 3.6% vs. 5.3% for 1D CNN, and 2.7% vs. 5.1% for EEGNet). This suggests that per-3849
LSTM k-NN
SVM
MLP
Figure 2. Two-class classiﬁcation accuracy on the validation set, averaged across all ﬁve folds, for each pair of classes and each classiﬁer.
Green denotes statistical signiﬁcance above chance (p < 0.005) by a binomial cmf. Red denotes above chance but not statistically signiﬁcant. Blank denotes at or below chance. 1D CNN
EEGNet
SyncNet
EEGChannelNet
Figure 3. (left) Classiﬁcation accuracy on the validation set, averaged across all ﬁve folds, as a function of the number of classes, for each classiﬁer, for the most discriminable subset of classes as determined by the greedy algorithm. (right) Classiﬁcation accuracy on the validation set, for all forty classes as a function of the fraction of the dataset used for train and test, for the three classiﬁers for which accuracy is above chance in a statistically signiﬁcant fashion. The grey signiﬁcance curves denotes the accuracy threshold as a function of (left) number of classes and (right) fraction of dataset for which analyses above this threshold are above chance in a statistically signiﬁcant fashion (p < 0.005) by a binomial cmf with a Bonferroni correction. Tabular versions of these plots are in Table 3(a, b). forming cross-subject training and testing of EEG classi-ﬁers cannot lead to high classiﬁcation accuracy and crowd-sourcing the collection of a large dataset across subjects and sites is not likely to be fruitful. 6. Signiﬁcance
With our data collection, each run lasted 20:20. The recording alone for each session nominally took 3:23:20.
Including capping, uncapping, subject breaks, setup, tear-down, and data transfer, each session took more than six hours, i.e. most of a full business day. The ten sessions re-quired to collect our dataset took more than sixty hours, i.e. most of two full business weeks. Few subjects would con-sent to, and complete, such an extensive and tedious data-collection effort. Consider what it would take to collect a larger dataset. Collecting EEG recordings of a single sub-ject viewing all 1,431,167 images of ILSVRC 2012 [34] would take more than a full business year with the protocol employed in this manuscript. Doing so for all 14,197,122 images and 21,841 synsets currently included in ImageNet (3 Feb 2020) would take more than a full business decade.
We doubt that any subject would consent to, and complete, such an extensive and tedious data-collection effort. More-over, we doubt that any EEG lab would dedicate the re-sources needed to do so. 7.