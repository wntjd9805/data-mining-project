Abstract
We present Meta Pseudo Labels, a semi-supervised learn-ing method that achieves a new state-of-the-art top-1 ac-curacy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art [16]. Like Pseudo Labels, Meta
Pseudo Labels has a teacher network to generate pseudo la-bels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is ﬁxed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student’s performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student.1 1.

Introduction
The methods of Pseudo Labels or self-training [57, 81, 55, 36] have been applied successfully to improve state-of-the-art models in many computer vision tasks such as image classiﬁcation (e.g., [79, 77]), object detection, and semantic segmentation (e.g., [89, 51]). Pseudo Labels methods work by having a pair of networks, one as a teacher and one as a student. The teacher generates pseudo labels on unlabeled images. These pseudo labeled images are then combined with labeled images to train the student. Thanks to the abun-dance of pseudo labeled data and the use of regularization methods such as data augmentation, the student learns to become better than the teacher [77].
Despite the strong performance of Pseudo Labels meth-ods, they have one main drawback: if the pseudo labels are inaccurate, the student will learn from inaccurate data. As a result, the student may not get signiﬁcantly better than the teacher. This drawback is also known as the problem of conﬁrmation bias in pseudo-labeling [2].
In this paper, we design a systematic mechanism for the teacher to correct the bias by observing how its pseudo labels would affect the student. Speciﬁcally, we propose Meta
Pseudo Labels, which utilizes the feedback from the student 1Code is available at https : / / github . com / google -research/google-research/tree/master/meta_pseudo_ labels. to inform the teacher to generate better pseudo labels. In our implementation, the feedback signal is the performance of the student on the labeled dataset. This feedback signal is used as a reward to train the teacher throughout the course of the student’s learning. In summary, the teacher and student of Meta Pseudo Labels are trained in parallel: (1) the student learns from a minibatch of pseudo labeled data annotated by the teacher, and (2) the teacher learns from the reward signal of how well the student performs on a minibatch drawn from the labeled dataset.
We experiment with Meta Pseudo Labels, using the
ImageNet [56] dataset as labeled data and the JFT-300M dataset [26, 60] as unlabeled data. We train a pair of
EfﬁcientNet-L2 networks, one as a teacher and one as a student, using Meta Pseudo Labels. The resulting student network achieves the top-1 accuracy of 90.2% on the Im-ageNet ILSVRC 2012 validation set [56], which is 1.6% better than the previous record of 88.6% [16]. This student model also generalizes to the ImageNet-ReaL test set [6], as summarized in Table 1. Small scale semi-supervised learn-ing experiments with standard ResNet models on CIFAR-10-4K, SVHN-1K, and ImageNet-10% also show that Meta
Pseudo Labels outperforms a range of other recently pro-posed methods such as FixMatch [58] and Unsupervised
Data Augmentation [76].
Datasets
ImageNet
Top-1 Accuracy
ImageNet-ReaL
Precision@1
Previous SOTA [16, 14]
Ours 88.6 90.2 90.72 91.02
Table 1: Summary of our key results on ImageNet ILSVRC 2012 validation set [56] and the ImageNet-ReaL test set [6]. 2. Meta Pseudo Labels
An overview of the contrast between Pseudo Labels and
Meta Pseudo Labels is presented in Figure 1. The main difference is that in Meta Pseudo Labels, the teacher receives feedback of the student’s performance on a labeled dataset. 111557
Figure 1: The difference between Pseudo Labels and Meta Pseudo Labels. Left: Pseudo Labels, where a ﬁxed pre-trained teacher generates pseudo labels for the student to learn from. Right: Meta Pseudo Labels, where the teacher is trained along with the student. The student is trained based on the pseudo labels generated by the teacher (top arrow). The teacher is trained based on the performance of the student on labeled data (bottom arrow).
Notations. Let T and S respectively be the teacher net-work and the student network in Meta Pseudo Labels. Let their corresponding parameters be θT and θS. We use (xl, yl) to refer to a batch of images and their corresponding labels, e.g., ImageNet training images and their labels, and use xu to refer to a batch of unlabeled images, e.g., images from the internet. We denote by T (xu; θT ) the soft predictions of the teacher network on the batch xu of unlabeled images and likewise for the student, e.g. S(xl; θS) and S(xu; θS).
We use CE(q, p) to denote the cross-entropy loss between two distributions q and p; if q is a label then it is understood as a one-hot distribution; if q and p have multiple instances in them then CE(q, p) is understood as the average of all yl, S(xl; θS) instances in the batch. For example, CE is (cid:0) the canonical cross-entropy loss in supervised learning. (cid:1)
Pseudo Labels as an optimization problem. To intro-duce Meta Pseudo Labels, let’s ﬁrst review Pseudo Labels.
Speciﬁcally, Pseudo Labels (PL) trains the student model to minimize the cross-entropy loss on unlabeled data: mize Ll with respect to θT : min
θT where
Ll
θPL
S (θT )
, (cid:0)
θPL
S (θT ) = argmin (cid:1)
θS
Lu (cid:0)
θT , θS
. (cid:1) (2)
Intuitively, by optimizing the teacher’s parameter accord-ing to the performance of the student on labeled data, the pseudo labels can be adjusted accordingly to further improve student’s performance. As we are effectively trying to op-timize the teacher on a meta level, we name our method
Meta Pseudo Labels. However, the dependency of θPL
S (θT ) on θT is extremely complicated, as computing the gradient
∇θT θPL
S (θT ) requires unrolling the entire student training process (i.e. argminθS ).
Practical approximation. To make Meta Pseudo Labels feasible, we borrow ideas from previous work in meta learn-ing [40, 15] and approximate the multi-step argminθS with the one-step gradient update of θS:
θPL
S = argmin
θS
Exu hCE (cid:0)
T (xu; θT ), S(xu; θS)
|
:=Lu
θT ,θS
{z (cid:0) (cid:1) (1) (cid:1)i
}
S (θT ) ≈ θS − ηS · ∇θS Lu
θPL
θT , θS
, (cid:1) (cid:0) where ηS is the learning rate. Plugging this approximation into the optimization problem in Equation 2 leads to the practical teacher objective in Meta Pseudo Labels: (cid:0) yl, S(xl; θPL
S ) where the pseudo target T (xu; θT ) is produced by a well pre-trained teacher model with ﬁxed parameter θT . Given a good teacher, the hope of Pseudo Labels is that the obtained
θPL
S would ultimately achieve a low loss on labeled data, i.e.
Exl,yl hCE (cid:1)i := Ll
Under the framework of Pseudo Labels, notice that the optimal student parameter θPL
S always depends on the teacher parameter θT via the pseudo targets T (xu; θT ). To facili-tate the discussion of Meta Pseudo Labels, we can explicitly express the dependency as θPL
S (θT ). As an immediate obser-vation, the ultimate student loss on labeled data Ll (cid:1) is also a “function” of θT . Therefore, we could further opti-θPL
S (θT )
θPL
S (cid:1) (cid:0) (cid:0)
. min
θT
Ll(cid:16)θS − ηS · ∇θS Lu
θT , θS (cid:1)(cid:17). (cid:0) (3)
Note that, if soft pseudo labels are used, i.e. T (xu; θT ) is the full distribution predicted by teacher, the objective above is fully differentiable with respect to θT and we can perform standard back-propagation to get the gradient.2 However, in this work, we sample the hard pseudo labels from the teacher distribution to train the student. We use hard pseudo labels because they result in smaller computational graphs which 2When optimizing Equation (3), we always treat θS as ﬁxed parameters and ignore its higher-order dependency on θT . 211558
are necessary for our large-scale experiments in Section 4.
For smaller experiments where we can use either soft pseudo labels or hard pseudo labels, we do not ﬁnd signiﬁcant per-formance difference between them. A caveat of using hard pseudo labels is that we need to rely on a slightly modiﬁed version of REINFORCE to obtain the approximated gradient of Ll in Equation 3 with respect to θT . We defer the detailed derivation to Appendix A.
On the other hand, the student’s training still relies on the objective in Equation 1, except that the teacher parameter is not ﬁxed anymore. Instead, θT is constantly changing due to the teacher’s optimization. More interestingly, the student’s parameter update can be reused in the one-step approximation of the teacher’s objective, which naturally gives rise to an alternating optimization procedure between the student update and the teacher update:
• Student: draw a batch of unlabeled data xu, then sample
T (xu; θT ) from teacher’s prediction, and optimize objec-tive 1 with SGD: θ′
S = θS − ηS∇θS Lu(θT , θS),
• Teacher: draw a batch of labeled data (xl, yl), and “reuse” the student’s update to optimize objective 3 with SGD:
θ′
T = θT − ηT ∇θT Ll
θS − ∇θS Lu
θT , θS
. (cid:0) (cid:1)
S reused from student’s update
}
{z (cid:0)
= θ′
| (cid:1)
Teacher’s auxiliary losses. We empirically observe that
Meta Pseudo Labels works well on its own. Moreover, it works even better if the teacher is jointly trained with other auxiliary objectives. Therefore, in our implementation, we augment the teacher’s training with a supervised learning objective and a semi-supervised learning objective. For the supervised objective, we train the teacher on labeled data.
For the semi-supervised objective, we additionally train the teacher on unlabeled data using the UDA objective [76].
For the full pseudo code of Meta Pseudo Labels when it is combined with supervised and UDA objectives for the teacher, please see Appendix B, Algorithm 1.
Finally, as the student in Meta Pseudo Labels only learns from unlabeled data with pseudo labels generated by the teacher, we can take a student model that has converged after training with Meta Pseudo Labels and ﬁnetune it on labeled data to improve its accuracy. Details of the student’s
ﬁnetuning are reported in our experiments.
Next, we will present the experimental results of Meta
Pseudo Labels, and organize them as follows:
• Section 3 presents small scale experiments where we com-pare Meta Pseudo Labels against other state-of-the-art semi-supervised learning methods on widely used bench-marks.
• Section 4 presents large scale experiments of Meta Pseudo
Labels where we push the limits of ImageNet accuracy. 3. Small Scale Experiments
In this section, we present our empirical studies of Meta
Pseudo Labels at small scales. We ﬁrst study the role of feedback in Meta Pseudo Labels on the simple TwoMoon dataset [7]. This study visually illustrates Meta Pseudo
Labels’ behaviors and beneﬁts. We then compare Meta
Pseudo Labels against state-of-the-art semi-supervised learn-ing methods on standard benchmarks such as CIFAR-10-4K,
SVHN-1K, and ImageNet-10%. We conclude the section with experiments on the standard ResNet-50 architecture with the full ImageNet dataset. 3.1. TwoMoon Experiment
To understand the role of feedback in Meta Pseudo Labels, we conduct an experiment on the simple and classic TwoM-oon dataset [7]. The 2D nature of the TwoMoon dataset allows us to visualize how Meta Pseudo Labels behaves compared to Supervised Learning and Pseudo Labels.
Dataset. For this experiment, we generate our own version of the TwoMoon dataset. In our version, there are 2,000 ex-amples forming two clusters each with 1,000 examples. Only 6 examples are labeled, 3 examples for each cluster, while the remaining examples are unlabeled. Semi-supervised learn-ing algorithms are asked to use these 6 labeled examples and the clustering assumption to separate the two clusters into correct classes.
Training details. Our model architecture is a feed-forward fully-connected neural network with two hidden layers, each has 8 units. The sigmoid non-linearity is used at each layer.
In Meta Pseudo Labels, both the teacher and the student share this architecture but have independent weights. All networks are trained with SGD using a constant learning rate of 0.1. The networks’ weights are initialized with the uniform distribution between -0.1 and 0.1. We do not apply any regularization.
Results. We randomly generate the TwoMoon dataset for a few times and repeat the three methods: Supervised Learn-ing, Pseudo Labels, and Meta Pseudo Labels. We observe that Meta Pseudo Labels has a much higher success rate of
ﬁnding the correct classiﬁer than Supervised Learning and
Pseudo Labels. Figure 2 presents a typical outcome of our experiment, where the red and green regions correspond to the classiﬁers’ decisions. As can be seen from the ﬁgure, Su-pervised Learning ﬁnds a bad classiﬁer which classiﬁes the labeled instances correctly but fails to take advantage of the clustering assumption to separate the two “moons”. Pseudo
Labels uses the bad classiﬁer from Supervised Learning and hence receives incorrect pseudo labels on the unlabeled data.
As a result, Pseudo Labels ﬁnds a classiﬁer that misclassiﬁes 311559
Figure 2: An illustration of the importance of feedback in Meta Pseudo Labels (right). In this example, Meta Pseudo Labels works better than Supervised Learning (left) and Pseudo Labels (middle) on the simple TwoMoon dataset. More details are in Section 3.1. half of the data, including a few labeled instances. Meta
Pseudo Labels, on the other hand, uses the feedback from the student model’s loss on the labeled instances to adjust the teacher to generate better pseudo labels. As a result, Meta
Pseudo Labels ﬁnds a good classiﬁer for this dataset. In other words, Meta Pseudo Labels can address the problem of conﬁrmation bias [2] of Pseudo Labels in this experiment. 3.2. CIFAR 10 4K, SVHN 1K, and ImageNet 10%
Experiments
Datasets. We consider three standard benchmarks:
CIFAR-10-4K, SVHN-1K, and ImageNet-10%, which have been widely used in the literature to fairly benchmark semi-supervised learning algorithms. These benchmarks were created by keeping a small fraction of the training set as labeled data while using the rest as unlabeled data. For
CIFAR-10 [34], 4,000 labeled examples are kept as labeled data while 41,000 examples are used as unlabeled data. The test set for CIFAR-10 is standard and consists of 10,000 examples. For SVHN [46], 1,000 examples are used as labeled data whereas about 603,000 examples are used as unlabeled data. The test set for SVHN is also standard, and has 26,032 examples. Finally, for ImageNet [56], 128,000 examples are used as labeled data which is approximately 10% of the whole ImageNet training set while the rest of 1.28 million examples are used as unlabeled data. The test set for ImageNet is the standard ILSVRC 2012 version that has 50,000 examples. We use the image resolution of 32x32 for CIFAR-10 and SVHN, and 224x224 for ImageNet.
Training details.
In our experiments, our teacher and our student share the same architecture but have indepen-dent weights. For CIFAR-10-4K and SVHN-1K, we use a
WideResNet-28-2 [84] which has 1.45 million parameters.
For ImageNet, we use a ResNet-50 [24] which has 25.5 million parameters. These architectures are also commonly used by previous works in this area. During the Meta Pseudo
Labels training phase where we train both the teacher and the student, we use the default hyper-parameters from previ-ous work for all our models, except for a few modiﬁcations in RandAugment [13] which we detail in Appendix C.2.
All hyper-parameters are reported in Appendix C.4. After training both the teacher and student with Meta Pseudo La-bels, we ﬁnetune the student on the labeled dataset. For this
ﬁnetuning phase, we use SGD with a ﬁxed learning rate of 10−5 and a batch size of 512, running for 2,000 steps for
ImageNet-10% and 1,000 steps for CIFAR-10 and SVHN.
Since the amount of labeled examples is limited for all three datasets, we do not use any heldout validation set. Instead, we return the model at the ﬁnal checkpoint.
Baselines. To ensure a fair comparison, we only compare
Meta Pseudo Labels against methods that use the same archi-tectures and do not compare against methods that use larger architectures such as Larger-WideResNet-28-2 and Pyramid-Net+ShakeDrop for CIFAR-10 and SVHN [5, 4, 72, 76], or ResNet-50×{2,3,4}, ResNet-101, ResNet-152, etc. for
ImageNet-10% [25, 23, 10, 8, 9]. We also do not compare
Meta Pseudo Labels with training procedures that include self-distillation or distillation from a larger teacher [8, 9].
We enforce these restrictions on our baselines since it is known that larger architectures and distillation can improve any method, possibly including Meta Pseudo Labels.
We directly compare Meta Pseudo Labels against two baselines: Supervised Learning with full dataset and Un-supervised Data Augmentation (UDA [76]). Supervised
Learning with full dataset represents the headroom because it unfairly makes use of all labeled data (e.g., for CIFAR-10, it uses all 50,000 labeled examples). We also compare against UDA because our implementation of Meta Pseudo
Labels uses UDA in training the teacher. Both of these base-lines use the same experimental protocols and hence ensure a fair comparison. We follow [48]’s train/eval/test splitting, and we use the same amount of resources to tune hyper-parameters for our baselines as well as for Meta Pseudo
Labels. More details are in Appendix C.
Additional baselines.
In addition to these two baselines, we also include a range of other semi-supervised baselines in two categories: Label Propagation and Self-Supervised.
Since these methods do not share the same controlled envi-ronment, the comparison to them is not direct, and should be contextualized as suggested by [48]. More controlled ex-periments comparing Meta Pseudo Labels to other baselines 411560
Label Propagation Methods
Self-Supervised Methods
Method
Temporal Ensemble [35]
Mean Teacher [64]
VAT + EntMin [44]
LGA + VAT [30]
ICT [71]
MixMatch [5]
ReMixMatch [4]
EnAET [72]
FixMatch [58]
UDA∗ [76]
SimCLR [8, 9]
MOCOv2 [10]
PCL [38]
PIRL [43]
BYOL [21]
Meta Pseudo Labels
Supervised Learning with full dataset∗
CIFAR-10-4K (mean ± std) 83.63 ± 0.63 84.13 ± 0.28 86.87 ± 0.39 87.94 ± 0.19 92.71 ± 0.02 93.76 ± 0.06 94.86 ± 0.04 94.65 95.74 ± 0.05 94.53 ± 0.18
−
−
−
−
− 96.11 ± 0.07 94.92 ± 0.17
SVHN-1K (mean ± std) 92.81 ± 0.27 94.35 ± 0.47 94.65 ± 0.19 93.42 ± 0.36 96.11 ± 0.04 96.73 ± 0.31 97.17 ± 0.30 97.08 97.72 ± 0.38 97.11 ± 0.17
−
−
−
−
− 98.01 ± 0.07 97.41 ± 0.16
ImageNet-10%
Top-5
Top-1
−
−
−
−
−
−
− 83.39 89.1 88.19 90.4
− 85.6 84.9 89.0 91.38 93.27
− 71.5 68.07 71.7 71.1
−
− 68.8 73.89 76.89
Table 2: Image classiﬁcation accuracy on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. Higher is better. For CIFAR-10-4K and SVHN-1K, we report mean ± std over 10 runs, while for ImageNet-10%, we report Top-1/Top-5 accuracy of a single run. For fair comparison, we only include results that share the same model architecture: WideResNet-28-2 for CIFAR-10-4K and SVHN-1K, and ResNet-50 for
ImageNet-10%. ∗ indicates our implementation which uses the same experimental protocols. Except for UDA, results in the ﬁrst two blocks are from representative important papers, and hence do not share the same controlled environment with ours. are presented in Appendix D.
Results. Table 2 presents our results with Meta Pseudo
Labels in comparison with other methods. The results show that under strictly fair comparisons (as argued by [48]),
Meta Pseudo Labels signiﬁcantly improves over UDA. In-terestingly, on CIFAR-10-4K, Meta Pseudo Labels even exceeds the headroom supervised learning on full dataset.
On ImageNet-10%, Meta Pseudo Labels outperforms the
UDA teacher by more than 5% in top-1 accuracy, going from 68.07% to 73.89%. For ImageNet, such relative im-provement is very signiﬁcant.
Comparing to existing state-of-the-art methods. Com-pared to results reported from past papers, Meta Pseudo
Labels has achieved the best accuracies among the same model architectures on all the three datasets: CIFAR-10-4K, SVHN-1K, and ImageNet-10%. On CIFAR-10-4K and
SVHN-1K, Meta Pseudo Labels leads to almost 10% rela-tive error reduction compared to the highest reported base-lines [58]. On ImageNet-10%, Meta Pseudo Labels outper-forms SimCLR [8, 9] by 2.19% top-1 accuracy.
While better results on these datasets exist, to our knowl-edge, such results are all obtained with larger models, stronger regularization techniques, or extra distillation pro-cedures. For example, the best reported accuracy on CIFAR-10-4K is 97.3% [76] but this accuracy is achieved with a PyramidNet which has 17x more parameters than our
WideResNet-28-2 and uses the complex ShakeDrop reg-ularization [80]. On the other hand, the best reported top-1 accuracy for ImageNet-10% is 80.9%, achieved by Sim-CLRv2 [9] using a self-distillation training phase and a
ResNet-152×3 which has 32x more parameters than our
ResNet-50. Such enhancements on architectures, regular-ization, and distillation can also be applied to Meta Pseudo
Labels to further improve our results. 3.3. ResNet 50 Experiment
The previous experiments show that Meta Pseudo La-bels outperforms other semi-supervised learning methods on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. In this experiment, we benchmark Meta Pseudo Labels on the en-tire ImageNet dataset plus unlabeled images from the JFT dataset. The purpose of this experiment is to verify if Meta
Pseudo Labels works well on the widely used ResNet-50 architecture [24] before we conduct more large scale experi-ments on EfﬁcientNet (Section 4).
Datasets. As mentioned, we experiment with all labeled examples from the ImageNet dataset. We reserve 25,000 examples from the ImageNet dataset for hyper-parameter tuning and model selection. Our test set is the ILSVRC 2012 validation set. Additionally, we take 12.8 million unlabeled images from the JFT dataset. To obtain these 12.8 million 511561
unlabeled images, we ﬁrst train a ResNet-50 on the entire
ImageNet training set and then use the resulting ResNet-50 to assign class probabilities to images in the JFT dataset. We then select 12,800 images of highest probability for each of the 1,000 classes of ImageNet. This selection results in 12.8 million images. We also make sure that none of the 12.8 million images that we use overlaps with the ILSVRC 2012 validation set of ImageNet. This procedure of ﬁltering extra unlabeled data has been used by UDA [76] and Noisy
Student [77].
Implementation details. We implement Meta Pseudo La-bels the same as in Section 3.2 but we use a larger batch size and more training steps, as the datasets are much larger for this experiment. Speciﬁcally, for both the student and the teacher, we use the batch size of 4,096 for labeled images and the batch size of 32,768 for unlabeled images. We train for 500,000 steps which equals to about 160 epochs on the unlabeled dataset. After training the Meta Pseudo Labels phase on ImageNet+JFT, we ﬁnetune the resulting student on
ImageNet for 10,000 SGD steps, using a ﬁxed learning rate of 10−4. Using 512 TPUv2 cores, our training procedure takes about 2 days.
Method
Supervised [24]
AutoAugment [12]
DropBlock [18]
FixRes [68]
FixRes+CutMix [83]
NoisyStudent [77]
UDA [76]
Billion-scale SSL [68, 79]
Meta Pseudo Labels
Unlabeled
Images
Accuracy (top-1/top-5)
None
None
None
None
None
JFT
JFT
YFCC
JFT 76.9/93.3 77.6/93.8 78.4/94.2 79.1/94.6 79.8/94.9 78.9/94.3 79.0/94.5 82.5/96.6 83.2/96.5
Table 3: Top-1 and Top-5 accuracy of Meta Pseudo Labels and other representative supervised and semi-supervised methods on
ImageNet with ResNet-50.
Baselines. We compare Meta Pseudo Labels against two groups of baselines. The ﬁrst group contains supervised learning methods with data augmentation or regularization methods such as AutoAugment [12], DropBlock[18], and
CutMix [83]. These baselines represent state-of-the-art su-pervised learning methods on ResNet-50. The second group of baselines consists of three recent semi-supervised learn-ing methods that leverage the labeled training images from
ImageNet and unlabeled images elsewhere. Speciﬁcally, billion-scale semi-supervised learning [79] uses unlabeled data from the YFCC100M dataset [65], while UDA [76] and Noisy Student [77] both use JFT as unlabeled data like
Meta Pseudo Labels. Similar to Section 3.2, we only com-pare Meta Pseudo Labels to results that are obtained with
ResNet-50 and without distillation.
Results. Table 3 presents the results. As can be seen from the table, Meta Pseudo Labels boosts the top-1 accuracy of
ResNet-50 from 76.9% to 83.2%, which is a large margin of improvement for ImageNet, outperforming both UDA and Noisy Student. Meta Pseudo Labels also outperforms
Billion-scale SSL [68, 79] in top-1 accuracy. This is par-ticularly impressive since Billion-scale SSL pre-trains their
ResNet-50 on weakly-supervised images from Instagram. 4. Large Scale Experiment: Pushing the Limits of ImageNet Accuracy
In this section, we scale up Meta Pseudo Labels to train on a large model and a large dataset to push the limits of
ImageNet accuracy. Speciﬁcally, we use the EfﬁcientNet-L2 architecture because it has a higher capacity than ResNets.
EfﬁcientNet-L2 was also used by Noisy Student [77] to achieve the top-1 accuracy of 88.4% on ImageNet.
Datasets. For this experiment, we use the entire ImageNet training set as labeled data, and use the JFT dataset as un-labeled data. The JFT dataset has 300 million images, and then is ﬁltered down to 130 million images by Noisy Student using conﬁdence thresholds and up-sampling [77]. We use the same 130 million images as Noisy Student.
Model architecture. We experiment with EfﬁcientNet-L2 since it has the state-of-the-art performance on Ima-geNet [77] without extra labeled data. We use the same hyper-parameters with Noisy Student, except that we use the training image resolution of 512x512 instead of 475x475.
We increase the input image resolution to be compatible with our model parallelism implementation which we discuss in the next paragraph. In addition to EfﬁcientNet-L2, we also experiment with a smaller model, which has the same depth with EfﬁcientNet-B6 [63] but has the width factor increased from 2.1 to 5.0. This model, termed EfﬁcientNet-B6-Wide, has 390 million parameters. We adopt all hyper-parameters of EfﬁcientNet-L2 for EfﬁcientNet-B6-Wide. We ﬁnd that
EfﬁcientNet-B6-Wide has almost the same performance with
EfﬁcientNet-L2, but is faster to compile and train.
Model parallelism. Due to the memory footprint of our networks, keeping two networks in memory for the teacher and the student vastly exceeds the available memory of our accelerators. We thus design a hybrid model-data parallelism framework to run Meta Pseudo Labels. Speciﬁcally, we use a cluster of 2,048 TPUv3 cores. We divide these cores into 128 identical replicas to run with standard data parallelism with 611562
Method
# Params
Extra Data
ImageNet
Top-1 Top-5
ImageNet-ReaL [6]
Precision@1
ResNet-50 [24]
ResNet-152 [24]
DenseNet-264 [28]
Inception-v3 [62]
Xception [11]
Inception-v4 [61]
Inception-resnet-v2 [61]
ResNeXt-101 [78]
PolyNet [87]
SENet [27]
NASNet-A [90]
AmoebaNet-A [52]
PNASNet [39]
AmoebaNet-C + AutoAugment [12]
GPipe [29]
EfﬁcientNet-B7 [63]
EfﬁcientNet-B7 + FixRes [70]
EfﬁcientNet-L2 [63]
ResNet-50 Billion-scale SSL [79]
ResNeXt-101 Billion-scale SSL [79]
ResNeXt-101 WSL [42]
FixRes ResNeXt-101 WSL [69]
Big Transfer (BiT-L) [33]
Noisy Student (EfﬁcientNet-L2) [77]
Noisy Student + FixRes [70]
Vision Transformer (ViT-H) [14]
EfﬁcientNet-L2-NoisyStudent + SAM [16]
Meta Pseudo Labels (EfﬁcientNet-B6-Wide)
Meta Pseudo Labels (EfﬁcientNet-L2) 26M 60M 34M 24M 23M 48M 56M 84M 92M 146M 89M 87M 86M 155M 557M 66M 66M 480M 26M 193M 829M 829M 928M 480M 480M 632M 480M 390M 480M
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
− 76.0 77.8 77.9 78.8 79.0 80.0 80.1 80.9 81.3 82.7 82.7 82.8 82.9 83.5 84.3 85.0 85.3 85.5 93.0 93.8 93.9 94.4 94.5 95.0 95.1 95.6 95.8 96.2 96.2 96.1 96.2 96.5 97.0 97.2 97.4 97.5 3.5B labeled Instagram 81.2 3.5B labeled Instagram 84.8 3.5B labeled Instagram 85.4 3.5B labeled Instagram 86.4 87.5 88.4 88.5 88.55 − 88.6 300M labeled JFT 300M unlabeled JFT 300M unlabeled JFT 300M labeled JFT 300M unlabeled JFT 96.0
− 97.6 98.0 98.5 98.7 98.7 98.6 300M unlabeled JFT 300M unlabeled JFT 90.0 90.2 98.7 98.8 82.94 84.79
− 83.58
−
−
− 85.18
−
− 82.56
−
−
−
−
−
−
−
−
− 88.19 89.73 90.54 90.55
− 90.72
− 91.12 91.02
Table 4: Top-1 and Top-5 accuracy of Meta Pseudo Labels and previous state-of-the-art methods on ImageNet. With EfﬁcientNet-L2 and
EfﬁcientNet-B6-Wide, Meta Pseudo Labels achieves an improvement of 1.6% on top of the state-of-the-art [16], despite the fact that the latter uses 300 million labeled training examples from JFT. synchronized gradients. Within each replica, which runs on 2,048/128=16 cores, we implement two types of model parallelism. First, each input image of resolution 512x512 is split along the width dimension into 16 patches of equal size 512x32 and is distributed to 16 cores to process. Note that we choose the input resolution of 512x512 because 512 is close to the resolution 475x475 used by Noisy Student and 512 keeps the dimensions of the network’s intermediate out-puts divisible by 16. Second, each weight tensor is also split equally into 16 parts that are assigned to the 16 cores. We implement our hybrid data-model parallelism in the XLA-Sharding framework [37]. With this parallelism, we can ﬁt a batch size of 2,048 labeled images and 16,384 unlabeled im-ages into each training step. We train the model for 1 million steps in total, which takes about 11 days for EfﬁcientNet-L2 and 10 days for EfﬁcientNet-B6-Wide. After ﬁnishing the
Meta Pseudo Labels training phase, we ﬁnetune the mod-els on our labeled dataset for 20,000 steps. Details of the
ﬁnetuning procedures are in Appendix C.4.
Results. Our results are presented in Table 4. From the table, it can be seen that Meta Pseudo Labels achieves 90.2% top-1 accuracy on ImageNet, which is a new state-of-the-art on this dataset. This result is 1.8% better than the same
EfﬁcientNet-L2 architecture trained with Noisy Student [77] and FixRes [69, 70]. Meta Pseudo Labels also outperforms the recent results by BiT-L [33] and the previous state-of-the-art by Vision Transformer [14]. The important contrast here is that both Bit-L and Vision Transformer pre-train on 300 million labeled images from JFT, while our method only uses unlabeled images from this dataset. At this level of accuracy, our gain of 1.6% over [16] is a very signiﬁcant margin of improvement compared to recent gains. For instance, the gain of Vision Transformer [14] over Noisy Student + FixRes was only 0.05%, and the gain of FixRes over Noisy Student was only 0.1%.
Finally, to verify that our model does not simply overﬁt to the ImageNet ILSVRC 2012 validation set, we test it on the ImageNet-ReaL test set [6]. On this test set, our model also works well and achieves 91.02% Precision@1 which is 711563
0.4% better than Vision Transformer [14]. This gap is also bigger than the gap between Vision Transformer and Noisy
Student which is only 0.17%. from the student’s performance on labeled data is a novel way of utilizing labeled data.
A lite version of Meta Pseudo Labels. Given the expen-sive training cost of Meta Pseudo Labels, we design a lite ver-sion of Meta Pseudo Labels, termed Reduced Meta Pseudo
Labels. We describe this lite version in Appendix E, where we achieve 86.9% top-1 accuracy on the ImageNet ILSRVC 2012 validation set with EfﬁcentNet-B7. To avoid using pro-prietary data like JFT, we use the ImageNet training set as labeled data and the YFCC100M dataset [65] as unlabeled data. Reduced Meta Pseudo Labels allows us to implement the feedback mechanism of Meta Pseudo Labels while avoid-ing the need to keep two networks in memory. 5.