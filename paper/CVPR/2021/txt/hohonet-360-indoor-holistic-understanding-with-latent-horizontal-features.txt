Abstract
We present HoHoNet, a versatile and efﬁcient frame-work for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat ﬂattens the features along the vertical direc-tion and has shown success in modeling per-column modal-ity for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is re-designed to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 back-bones respectively, for modeling dense modalities from a high-resolution 512 × 1024 panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with cur-rent state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin. Code is available at https:// github.com/ sunset1995/ HoHoNet. 1.

Introduction
Panoramic images can capture the complete 360° FOVs in one shot to provide a wide range of context that facil-itates scene understanding [29]. As omnidirectional cam-eras become more easily accessible and several large-scale panorama datasets have been released, a growing number of techniques are developed for tasks of panoramic scene modeling such as semantic segmentation [9, 16, 28], depth estimation [13, 24, 27], layout reconstruction [21, 26, 33], and indoor real-time navigation [3].
This paper aims to address the problem of holistic scene modeling from a single high-resolution equirectangular pro-jection (ERP) image that captures the 360° panorama. We present HoHoNet as an efﬁcient, effective, and versatile framework to achieve this goal (Fig. 1). The input ERP 1National Tsing Hua University 2ASUS AICS Department 3Joint Research Center for AI Technology and All Vista Healthcare 4Aeolus Robotics (cid:43)(cid:82)(cid:79)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:56)(cid:81)(cid:71)(cid:72)(cid:85)(cid:86)(cid:87)(cid:68)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74) (cid:47)(cid:68)(cid:92)(cid:82)(cid:88)(cid:87)(cid:3)(cid:85)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81) (cid:47)(cid:43)(cid:41)(cid:72)(cid:68)(cid:87) (cid:75)(cid:21)(cid:71) (cid:75)(cid:21)(cid:71) (cid:11)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:75)(cid:82)(cid:85)(cid:76)(cid:93)(cid:82)(cid:81)(cid:87)(cid:68)(cid:79)(cid:3)(cid:86)(cid:75)(cid:68)(cid:83)(cid:72)(cid:12) (cid:39)(cid:72)(cid:83)(cid:87)(cid:75)(cid:3)(cid:72)(cid:86)(cid:87)(cid:76)(cid:80)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:54)(cid:72)(cid:80)(cid:68)(cid:81)(cid:87)(cid:76)(cid:70)(cid:3)(cid:86)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
Figure 1: One framework for all: HoHoNet is a novel deep learning framework for modeling layout structure, dense depth, and semantic segmentation through a Latent Horizon-tal Feature representation (LHFeat) whose height dimension is ﬂattened. The proposed horizon-to-dense (h2d) module can produce dense predictions from the compact LHFeat. image is ﬁrst passed through a CNN backbone for feature pyramid extraction, and then a proposed efﬁcient height com-pression module encodes the feature pyramid into a Latent
Horizontal Feature representation (LHFeat) whose height dimension is ﬂattened. Finally, from LHFeat, the HoHoNet framework can yield both per-column and per-pixel modali-ties with state-of-the-art quality.
Our way of encoding ERP images into LHFeat is inspired by Sun et al. [21]. However, their model is only applicable to tasks of predicting per-column modalities (e.g., corners or boundaries of layout), which constrains its feasibility in other scenarios requiring per-pixel predictions. We show that
LHFeat can ﬂexibly encode latent features for recovering the target 2D per-pixel modalities, based on our observation of the strong regularity between human-made structures and gravity aligned y-axis of ERP images (Fig. 2).
In HoHoNet we introduce a new horizon-to-dense (h2d) module for recovering 2D per-pixel modalities while main-taining the efﬁciency of overall framework (Fig. 1). A naive method is to treat the channel dimension of horizon-tal prediction as height and apply a linear interpolation if 2573
required. However, this requires the shallow Conv1D lay-ers to disentangle the row-dependent information from the row-independent LHFeat. The spatial (the row) blended essence of LHFeat motivates us to model dense information in the frequency domain, and we resort to the discrete cosine transform (DCT) for its long-standing applications in data compression. By replacing linear interpolation with IDCT, we are able to improve the dense prediction results. With our horizon-to-dense module, the efﬁciently encoded LHFeat can now model dense modalities.
We summarize the key merits and contributions of Ho-HoNet for holistic scene modeling from a 360° image.
• Fast. HoHoNet can yield dense modalities for a high-resolution 512 × 1024 panorama at 52 FPS and 110
FPS with ResNet-50 and ResNet-34 respectively.
• Versatile. Our method relaxes the ﬁnal prediction space upon the compact LHFeat from O(W ) to the most common O(HW ), capable of modeling layout, dense depth, and semantic segmentation.
• Accurate. The performances of HoHoNet on semantic segmentation and layout reconstruction are on par with the recent state-of-the-art. On dense depth estimation,
HoHoNet outperforms prior arts by a margin. 2.