Abstract
Event cameras are novel vision sensors that sample, in an asynchronous fashion, brightness increments with low latency and high temporal resolution. The resulting streams of events are of high value by themselves, especially for high speed motion estimation. However, a growing body of work has also focused on the reconstruction of intensity frames from the events, as this allows bridging the gap with the ex-isting literature on appearance- and frame-based computer vision. Recent work has mostly approached this problem using neural networks trained with synthetic, ground-truth data. In this work we approach, for the ﬁrst time, the inten-sity reconstruction problem from a self-supervised learning perspective. Our method, which leverages the knowledge of the inner workings of event cameras, combines estimated optical ﬂow and the event-based photometric constancy to train neural networks without the need for any ground-truth or synthetic data. Results across multiple datasets show that the performance of the proposed self-supervised ap-proach is in line with the state-of-the-art. Additionally, we propose a novel, lightweight neural network for optical ﬂow estimation that achieves high speed inference with only a minor drop in performance. 1.

Introduction
Unlike conventional cameras recording intensity frames at ﬁxed time intervals, event cameras sample light based on scene dynamics by asynchronously measuring per-pixel brightness1 changes at the time they occur [1]. This results in streams of sparse events encoding the polarity of the per-ceived changes. Because of this paradigm shift, event cam-eras offer several advantages over their frame-based coun-terparts, namely low power consumption, high dynamic range (HDR), low latency and high temporal resolution.
Despite the advantages, the novel output format of event cameras poses new challenges in terms of algorithm design.
Unless working with spiking neural networks [2], events are 1Deﬁned as the logarithm of the pixel intensity, i.e., L
.
= log(I).
E
FlowNet
Events
ReconNet
Event accumulation piC
P i∈ε
Contrast
Maximization
Photometric
Constancy
ˆu
ˆL
∆L
Figure 1: Overview of the proposed framework. Our model is trained in a self-supervised fashion to perform optical
ﬂow estimation and image reconstruction from event data using the contrast maximization proxy loss and the event-based photometric constancy, respectively. Colored reverse arrows indicate error propagation for each loss. usually converted into intermediate representations that fa-cilitate the extraction of information [1]. Among others, in-tensity frames are an example of a powerful representation since they allow the evaluation of the appearance of a visual scene, thus bridging the gap between event cameras and the existing frame-based computer vision literature [3, 4]. For this reason, there has been a signiﬁcant research drive to de-velop new methods to reconstruct images from events with similar statistics to those captured by standard cameras.
Recent work has mostly approached this problem from a machine learning perspective. With their E2VID artiﬁcial neural network, Rebecq et al. [3, 4] were the ﬁrst to show that learning-based methods trained to maximize perceptual similarity via supervised learning outperform hand-crafted techniques by a large margin in terms of image quality.
Later, Scheerlinck et al. [5] achieved high speed inference with FireNet, a simpliﬁed model of E2VID. Despite the high levels of accuracy reported, these architectures were trained with large sets of synthetic data from event cam-era simulators [6], which adds extra complexity to the re-construction problem due to the simulator-to-reality gap.
In fact, Stoffregen, Scheerlinck et al. [7] recently showed 3446
that if the statistics of the synthetic training datasets do not closely resemble those seen during inference, image qual-ity degrades and the generalizability of these architectures remains limited.
In this work, we propose to come back to the theoretical basics of event cameras to relax the dependency of learning-based reconstruction methods on ground-truth and synthetic data. Speciﬁcally, we introduce the self-supervised learn-ing (SSL) framework in Fig. 1, which consists of two ar-tiﬁcial neural networks, FlowNet and ReconNet, for opti-cal ﬂow estimation and image reconstruction, respectively.
FlowNet is trained through the contrast maximization proxy loss from Zhu et al. [8], while ReconNet makes use of the
ﬂow-intensity relation in the event-based photometric con-stancy [9] to reconstruct the frames that best satisfy the in-put events and the estimated ﬂow. Using our method, we re-train several networks from the image reconstruction [3, 5] and optical ﬂow [10] literature. In terms of accuracy, results show that the reconstructed images are in line with those generated by most learning-based approaches despite the lack of ground-truth data during training. Additionally, we propose FireFlowNet, a lightweight architecture for optical
ﬂow estimation that, inspired by [5], achieves high speed inference with only a minor drop in performance.
In summary, this paper contains two main contributions.
First, a novel SSL framework to train artiﬁcial neural net-works to perform event-based image reconstruction that, with the aid of optical ﬂow, does not require ground truth of any kind and can learn directly on real event data. Sec-ond, we introduce FireFlowNet: a novel, lightweight neural network architecture that performs fast optical ﬂow estima-tion from events. We validate our self-supervised method and optical ﬂow network through extensive quantitative and qualitative evaluations on multiple datasets. 2.