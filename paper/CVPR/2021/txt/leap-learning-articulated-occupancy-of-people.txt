Abstract
Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extend-ing these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for un-seen subjects in unseen, highly-articulated, poses. To ad-dress this challenge, we introduce LEAP (LEarning Articu-lated occupancy of People), a novel neural occupancy rep-resentation of the human body. Given a set of bone trans-formations (i.e. joint locations and rotations) and a query point in space, LEAP ﬁrst maps the query point to a canoni-cal space via learned linear blend skinning (LBS) functions and then efﬁciently queries the occupancy value via an oc-cupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experi-ments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the gen-eralization capability of the learned occupancy representa-tion across various human shapes and poses, outperforming existing solutions in all settings. 1.

Introduction
Parametric 3D human body models [37, 61] are often represented by polygonal meshes and have been widely used to estimate human pose and shape from images and videos [17, 28, 33], create training data for machine learn-ing algorithms [22, 49] and synthesize realistic human bod-ies in 3D digital environments [68, 69]. However, the mesh-based representation often requires a ﬁxed topology and lacks ﬂexibility when combined with deep neural networks where back-propagation through the 3D geometry represen-tation is desired.
Neural implicit representations [39, 45, 46] have been proposed recently to model rigid 3D objects. Such rep-resentations have several advantages. For instance, they
Figure 1. LEAP successfully represents unseen people in various challenging poses by learning the occupancy of people in a canon-ical space. Shape- and pose-dependent deformations are mod-eled through carefully designed neural network encoders. Pose-dependent deformations are best observed around the elbows in the canonical pose. are continuous and do not require a ﬁxed topology. The 3D geometry representation is differentiable, making in-terpenetration tests with the environment efﬁcient. How-ever, these methods perform well only on static scenes and objects, their generalization to deformable objects is lim-ited, making them unsuitable for representing articulated 3D human bodies. One special case is NASA [14] which takes a set of bone transformations of a human body as in-10461
put and represents the 3D shape of the subject with neural occupancy networks. While demonstrating promising re-sults, their occupancy representation only works for a single subject and does not generalize well across different body shapes. Therefore, the widespread use of their approach is limited due to the per-subject training.
In this work, we aim to learn articulated neural occu-pancy representations for various human body shapes and poses. We take inspiration from the traditional mesh-based parametric human body models [37, 61], where identity-and pose-dependent body deformations are modeled in a canonical space, and then Linear Blend Skinning (LBS) functions are applied to deform the body mesh from the canonical space to a posed space. Analogously, given a set of bone transformations that represent the joint locations and rotations of a human body in a posed space, we ﬁrst map 3D query points from the posed space to the canonical space via learned inverse linear blend skinning (LBS) functions and then compute the occupancy values via an occupancy network that expresses differentiable 3D body deformations in the canonical space. We name it LEAP (LEarning Artic-ulated occupancy of People).
The key idea of LEAP is to model accurate identity- and pose-dependent occupancy of human bodies in a canoni-cal space (in analogy to the Shape Blend Shapes and Pose
Blend Shapes in SMPL [37]). This circumvents the chal-lenging tasks of learning occupancy functions in various posed spaces. Although conceptually simple, learning the canonicalized occupancy representation for a large variety of human shapes and poses is a highly non-trivial task.
The ﬁrst challenge we encounter is that the conventional
LBS weights are only deﬁned on the body surface. In order to convert a query point from a posed space to the canonical space and perform the occupancy check, a valid skinning weight for every point in the posed spaces is required. To that end, we parameterize both forward and inverse LBS functions using neural networks and learn them from data.
To account for the undeﬁned skinning weights for the points that are not on the surface of a human body, we introduce a cycle-distance feature for every query point, which models the consistency between the forward and the inverse LBS operations on that point.
Second, a high ﬁdelity human body model should be able to express accurate body shapes that vary across individu-als and capture the subtle surface deformations when the body is posed differently. To that end, we propose novel en-coding schemes for the bone transformations by exploiting prior knowledge about the kinematic structure and plausi-ble shapes of a human body. Furthermore, inspired by the recent advances of learning pixel-aligned local features for 3D surface reconstruction [51, 52], for every query point, we use the learned LBS weights to construct a locally aware bone transformation encoding that captures accurate local shape deformations. As demonstrated in our experiments, the proposed local feature is an effective and expressive rep-resentation that captures detailed pose and shape-dependent deformations.
We demonstrate the efﬁcacy of LEAP on the task of plac-ing people in 3D scenes [68]. With the proposed occupancy representation, LEAP is able to effectively prevent person-person and person-scene interpenetration and outperforms the recent baseline [68].
Our contributions are summarized as follows: 1) we in-troduce LEAP, a novel neural occupancy representation of people, which generalizes well across various body shapes and poses; 2) we propose a canonicalized occupancy esti-mation framework and learn the forward and the inverse lin-ear blend skinning weights for every point in space via deep neural networks; 3) we conduct novel encoding schemes for the input bone transformations, which effectively model accurate identity- and pose-dependent shape deformations; 4) experiments show that our method largely improves the generalization capability of the learned neural occupancy representation to unseen subjects and poses. 2.