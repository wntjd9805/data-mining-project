Abstract
We present a framework for automatically reconﬁguring images of street scenes by populating, depopulating, or re-populating them with objects such as pedestrians or vehicles.
Applications of this method include anonymizing images to enhance privacy, generating data augmentations for percep-tion tasks like autonomous driving, and composing scenes to achieve a certain ambiance, such as empty streets in the early morning. At a technical level, our work has three primary contributions: (1) a method for clearing images of objects, (2) a method for estimating sun direction from a single image, and (3) a way to compose objects in scenes that respects scene geometry and illumination. Each component is learned from data with minimal ground truth annotations, by making creative use of large-numbers of short image bursts of street scenes. We demonstrate convincing results on a range of street scenes and illustrate potential applications. 1.

Introduction
Websites such as Google Street View enable users to explore places around the world through street-level im-agery. These sites can provide a rich sense of what different locales—neighborhoods, parks, tourist sites, etc—are really like. However, the imagery provided by such sites also has key limitations. A given image might be full of cars and pedestrians, making it difﬁcult to observe the environment.
Alternatively, a user might want to see how a scene appears at a certain time of day, e.g., lunchtime, but only have access to a morning image. And, importantly, the fact that the imagery records real people and vehicles may require anonymization efforts to protect privacy, e.g., by blurring faces and license plates [10, 1] or by removing pedestrians from images by leveraging multiple views [9].
We propose learning-based tools that mitigate these limi-tations by removing objects from a scene and then repopu-lating that scene with, for instance, anonymized images and vehicles. Our method could thus be used to enhance privacy
∗This work was done while Yifan was an intern at Google. of imagery, while also increase ﬂexibility to compose new scenarios (e.g., an empty street or lunchtime scene). These capabilities could also be useful in other applications, such as automatic generation of novel scene conﬁgurations as a way to augment data for training autonomous driving—especially emergency scenarios that might be rare in real data. In order for such reconﬁgured images to look realistic, they must re-spect the illumination and geometry of the underlying scene.
Our learning-based method takes such factors into account.
As shown in Fig. 1, our framework takes as input a single street image, and can realistically remove all objects and generate repopulated images. To remove objects, we use nearby patches to inpaint not only the objects themselves, but also the shadows they cast onto the scene. To repopulate with new objects, our framework can automatically select objects that match the lighting of the scene, and compose them into the scene with proper scale, occlusion, and cast shadows consistent with the scene’s geometry and lighting.
Our method consists of four main components: 1. a removal network that removes all existing objects (cars, pedestrians, and bicycles) – along with their shad-ows – from a street image and realistically ﬁlls the re-sulting holes, rendering an empty version of the scene; 2. a sun estimation network that takes a street image, and estimates a dominant lighting (sun) direction; 3. a method to compose the inserted object into the scene with proper scale, occlusion based on its placement in the scene; and 4. an insertion network that takes a segmented object, e.g., drawn from an anonymized collection, and inserts it into a scene, generating a realistic shadow.
Our method yields realistic results that improve upon prior work. In particular, unlike standard methods such as image inpainting [27, 38, 39, 37], our approach can realistically remove and render object shadows. Instead of learning to cast shadows into one speciﬁc scene captured with a long video sequence [35], our approach learns from short image bursts, then generalizes to any single image of a street scene.
Our three networks are learned in a novel way that in-volves observing large numbers of street scenes, with no 5110
(a) Object Removal 
Network (Fig. 3) (c) Geometry-Aware 
Composition
Selected objects (d) Object Insertion 
Network (Fig. 4)
Street image