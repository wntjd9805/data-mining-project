Abstract    study 
Robust  principal  component  analysis  (RPCA)  and  its  variants have gained wide applications in computer vision. 
However,  these methods either involve manual adjustment  of  some  parameters,  or  require  the  rank  of  a  low-rank  matrix to be known a prior. In this paper, an adaptive rank  estimate  based  RPCA  (ARE-RPCA)  is  proposed,  which  adaptively assigns weights on different singular values via  rank  estimation.  More  specifically,  we  the  characteristics  of  the  low-rank  matrix,  and  develop  an  improved Gerschgorin disk theorem to estimate the rank of  the low-rank matrix accurately. Furthermore in view of the  issue  occurred  in  the  Gerschgorin  disk  theorem  that  adjustment  factor  need  to  be  manually  pre-defined,  an  adaptive  setting  method,  which  greatly  facilitates  the  practical  is  presented.  Then,  the  weights  of  singular  values  in  the  nuclear norm are updated adaptively based on iteratively  estimated rank, and the resultant low-rank matrix is close  to the target. Experimental results show that the proposed 
ARE-RPCA  outperforms  the  state-of-the-art  methods  in  various complex scenarios.  the  rank  estimation,  implementation  of  1.

Introduction  is  computationally  expensive  and 
The real world is full of high-dimensional data such as  images  and  videos.  The  processing  in  high-dimensional  space  intractable. 
Fortunately, most data are not unstructured and randomly  distributed  over  the  high-dimensional  space,  and  usually  have  patterns  and  distributed  over  low-dimensional  manifolds. Principal component analysis (PCA) effectively  proves  this  phenomenon,  where  most  high-dimensional  data lie around a low-dimensional subspace spanned by the  principal components [1]. Hence PCA can be viewed as a  low-rank  modeling  technique,  and  works  well  when  the  matrix  data  has  no  missing  entries  with    normal  errors/noise.  However,  the  PCA  often  produces  an  undesired  model  when  the  data  assumptions  do  not  hold. 
Among these issues, the outlier is one of the most important  limitations. 
Robust  PCA  (RPCA)  was  proposed  in  [2]  to  address  these issues by decomposing a data matrix into a low-rank  matrix and a sparse matrix containing outliers as follows:  
,       (1)  (cid:24)  and   is  the  measurement  matrix,  (cid:11)(cid:12)(cid:13)(cid:14)((cid:8)) + (cid:18)‖(cid:10)‖(cid:20)     (cid:21). (cid:23).    (cid:24) = (cid:8) + (cid:10) low-rank  matrix  and  (cid:8) arg min where  (cid:8),(cid:10) decomposed  respectively, 
-norm  which  is  the  number  of  non-zero  indicates  the 
‖(cid:10)‖(cid:20) (cid:11)(cid:12)(cid:13)(cid:14)((cid:8))
, λ is a parameter balancing the rankness and  elements of 
ℓ(cid:20) sparsity.  Hence,  RPCA  is  more  robust  than  PCA  as  the  former considers outliers by employing a sparse term.   denote  the  sparse  matrix,   denotes  the  rank  of  matrix  (cid:10) (cid:10) (cid:8)
, 
However, the above optimization problem is intractable 
-norm  are  nonconvex.  because  the  rank  operator  and 
Fortunately,  it  can  be  relaxed  to  the  following  convex  problem: 
ℓ(cid:20)
,         (2) 
!
"  and 
|| ∙ ||∗
|| ∙ ||(cid:28) arg min (cid:8),(cid:10)  are  the  nuclear  norm  and  the  where  norm,  respectively,  and  the   ‖(cid:8)‖∗ + (cid:18)‖(cid:10)‖(cid:28)     (cid:21). (cid:23).    (cid:24) = (cid:8) + (cid:10)
-,  where   is 
ℓ(cid:28)
. As the nuclear norm and 
-norm are the convex surrogates of the rank function and 
-norm, respectively, a perfect recovery can be achieved 
‖(cid:8)‖∗ = ∑  !((cid:8)) (cid:8)
-th singular value of matrix 
ℓ(cid:28) by solving the above convex optimization [3]. 
ℓ(cid:20)
RPCA is an efficient way to find the sparsity and low-rankness  and  has  been  gained  wide  applications  [4-14],  such as face recognition[4, 5], audio processing [6],  depth  image repair [7], background subtraction [8], and recovery  models in vision process etc.    !((cid:8)) (cid:8)  and 
The optimization problem (2) is not solved immediately  since  the  matrices   are  coupled.  Alternating  direction method of multipliers (ADMM) algorithm is often  (cid:10) employed to solve the RPCA problem for obtaining sparse  and  low-rank  decomposition  [2].  The  key  ingredient  of 
ADMM  based  RPCA  algorithm  is  the  nuclear  norm  minimization (NNM) sub-problem, which is related to low-rank matrix recovery. The solution to NNM problem is the  so-called singular value soft-thresholding operator [15, 16]: 
,       (3)  where  (cid:8)# = arg min(cid:8)  is  a  parameter  controlling  the  rankness,   is  the  singular  value  decomposition  (SVD)  of 
&
‖(cid:24) − (cid:8)‖% + &‖(cid:8)‖∗ = ’Σ)[+]-.
. with 
’+-,  and  (cid:24) = (cid:24) (Σ)[+])!! =
+ = diag({ !}(cid:28)2!2345 (6,7)) 6577                      
 with parameter   denotes the soft-thresholding on 
. One can observe that the nuclear norm  sgn( !) ∙ max (| !| − &, 0) minimization reduces the singular values on the same scale 
+ for those larger than τ. Thus this approach is insensitive to  outliers [17]. 
&
To  improve  the  adaptation  of  NNM,  Hu  et  al.  [18]  proposed a truncated nuclear norm regularization (TNNR)  method,  where  only  some  special  singular  values  are  regularized. Later, Gu et al. [19, 20] proposed a weighted  nuclear  norm  minimization  (WNNM)  that  replaces  the  nuclear norm with a weighted one defined by 
<!
,                           (4)  where   are non-negative weights. The weighting strategy 
!
‖(cid:8)‖; = ∑ <! !((cid:8)) greatly improves the performance of NNM method, but the  weights  are  dependent  on  one  constant  regularization  parameter that is empirically chosen [19, 20]. On the other  hand,  assuming  that  the  rank  of   is  known  in  some  practical applications, Oh et al. [21, 22] proposed a partial  sum of singular values (PSSV) minimization defined by  (cid:8)
,  (5)  345 (6,7)
!=>?(cid:28) arg min  !((cid:8)) + (cid:18)‖(cid:10)‖(cid:28) (cid:8),(cid:10) ∑  (cid:21). (cid:23).  (cid:24) = (cid:8) + (cid:10) where  N  is  the  known  rank  of  a  low-rank  matrix.  For   for  background  subtraction,  instance,  one  can  set  and  cannot  be  known  in  most  practical  applications,  PSSV  is  (cid:8) unable to recover the low-rank structure correctly in these  cases.    for photometric stereo. However, the rank of 
@ = 3
@ = 1
To address this problem, a new method to estimate the  rank  of  a  low-rank  matrix  is  presented  in  this  paper. 
Inspired  by  the  source  number  estimation  in  array  signal  processing  [23],  we  propose  an  improved  estimation  method via Gerschgorin disks to estimate the rank of a low-rank  matrix.  Furthermore,  an  adaptive  weighting  strategy  based  on  the  iteratively  estimated  rank  is  developed  to  improve  the  performance  of  low-rank  matrix  recovery. 
Therefore, we can not only get an accurate approximation  to  the  rank  function,  but  also  faultlessly  recover  the  low-rank  matrix.  In  summary,  the  main  contributions  of  this  paper are as follows: 
•  An  improved  method  based  on  Gerschgorin  disks  is  presented to estimate the rank of a low-rank matrix. 
•  A novel RPCA method with weight updating based on  the iteratively estimated rank is proposed to recover the  low-rank  structure  of  a  data  matrix  and  the  sparse  representation from corrupted data.  
•  The proposed algorithm is applied to various scenarios  to  demonstrate  the  superior  performance  over  the  existing methods. 
The  organization  of  the  paper  is  as  follow.  Section  2  presents  the  modified  robust  PCA  which  consists  of  rank  estimation  and  rank-estimation  based  adaptive  weighting.  
Section  3  reports  the  experimental  results,  and  some  conclusions are drawn in Section 4.  2. Modified Robust PCA  2.1. Rank Estimation of Low-Rank Matrix 
Since the rank of a low-rank matrix is a very important  parameter in weighted NNM problem, we borrow the idea  of Gerschgorin disk theorem [23] to identify the rank when  it is unknown. 
Assuming   observations  with  there  are  that 
@  observation  matrix  elements, and each observation is transformed  into a row 
C  samples thus form a  through the stretching process, the 
.  If  each  row  of  the  observation matrix is regarded as a one-dimensional signal  (@ × C)  samples  are   signal  sensors.  In  this  way,  the  multiple  viewed  as 
F((cid:23)) sample  processing  is  transformed  into  an  array  signal 
@ processing.  
E  with  the  snapshot  number 
,  then 
C
@
@
Given  a  low-rank  matrix   with  rank 
,  it  can  be  regarded as an array signal  which  includes  from  be  defined  as 
@ rank matrix  (cid:11)  signal sensors. Thus the  (cid:11) (cid:8)G (cid:11)  independent  signals  with 
. (cid:8)G((cid:23)) = [H(cid:28)((cid:23)), HI((cid:23)), ⋯ , H>((cid:23))]  snapshots 
,  independent signals can 
C
.  The  low-.  is decomposed by SVD as follows: 
KL((cid:23)) = [M(cid:28)((cid:23)), MI((cid:23)), ⋯ , MN((cid:23))] (cid:8)G
O (cid:8)G = ’(cid:8)G +(cid:8)G -(cid:8)G
N where 
!=(cid:28) left  singular  value  vectors  of 
’(cid:8)G = (P(cid:8)G(cid:28), P(cid:8)GI, ⋯ , P(cid:8)GN)
O
= ∑ P(cid:8)G! (cid:8)G!Q(cid:8)G!  with                           (6)   is  the  matrix  consisting  of  
>×(cid:28)
,
.  (cid:8)G
Let 
P(cid:8)G! ∈ ℝ  is  the  singular  value  matrix, 
+(cid:8)G =  is the matrix consisting of right singular 
T"(cid:12)U( (cid:8)G(cid:28),  (cid:8)GI, ⋯ ,  (cid:8)GN)
-(cid:8)G =  with  value vectors of  (Q(cid:8)G(cid:28), Q(cid:8)GI, ⋯ , Q(cid:8)GN)
, and  (cid:8)G
V×(cid:28)
, the observation matrix is   if  the  environment  is  noise-free.  If  outliers  or   occur  in  the  environment,  the  observation  (cid:10)G
W(cid:8)G = ’(cid:8)G corrupt  noise  (cid:24) = W(cid:8)G KL(cid:8)G matrix can be defined as 
Q(cid:8)G! ∈ ℝ
O
KL(cid:8)G = +(cid:8)G -(cid:8)G
In view of Eq. (6), we define   independent  signal,  and 
.                           (7)   to correspond to the  (cid:24) = W(cid:8)G KL(cid:8)G + (cid:10)G
O th  signal’s  array   to  the   (cid:8)G!Q(cid:8)G! manifold  of   signal  sensors.  It  is  known  from  SVD 
"th principle that   are independent each other. It can be seen 
@  is often sparse matrix and independent of  from [13] that 
Q(cid:8)G! the low-rank matrix, and each sample, i.e. each row in 
, is  (cid:10)G also independent each other. Thus Eq. (7) is equivalent to  (cid:10)G the array output signal in the array signal processing [24]  given by 
P(cid:8)G!
"
.                    (8) 
In  this  way,  the  rank  estimation  problem  of  low-rank 
Z((cid:23)) = [\((cid:23)) + ]((cid:23)) matrix  is  transformed  into  the  problem  of  estimating  the  number  of  sources  in  the  array  signal  processing.  The  information  corresponding  to  each  rank  in  the  low-rank  matrix  can  be  equivalent  to  the  information  of  the  signal  sent by each source in the array signal processing. 
The  covariance  matrix  of  the  observation  matrix   with rank   can be defined as   (cid:24) =
.                                 (9) 
[^(cid:28), ^I, ⋯ , ^V] (cid:11)
_(cid:24) = (cid:24)(cid:24)
. 6578                                                       
Eigenvalue decomposition of   is  b
_(cid:24) = ’_(cid:24)‘_(cid:24)’_a where  and 
’_(cid:24) = [c(cid:28), cI, ⋯ , c>] there is no noise, the eigenvalues of 
‘_(cid:24) = T"(cid:12)U( (cid:28),  I, … ,  >)  are 
,                         (10) 
_(cid:24)  is  the  eigenvector  matrix,   is the eigenvalue matrix. If 
_(cid:24)
.     (11) 
Due  to  the  interference  of  the  sparse  matrix  in  the  real   (cid:28) ≥  I ≥ ⋯ ≥  N >  N?(cid:28) = ⋯ =  > = 0 environment, the eigenvalues of the covariance matrix  are 
_(cid:24)
.         (12) 
In order to accurately identify the rank of the low-rank   (cid:28) ≥  I ≥ ⋯ ≥  N ≥  N?(cid:28) ≥ ⋯ ≥  > matrix,  the  idea  of  the  Gerschgorin’s  disk  theorem  is   is partitioned as  employed. First the covariance matrix 
_(cid:24) hii hij ⋯ hi> hji hjj ⋯ hj>
⋮
_(cid:24) = g where  matrix  the last column and row of  in Eq. (7) as a vector, it can be rewritten as  h>i h>j ⋯ h>> (>q(cid:28))×(>q(cid:28))
_(cid:24)i ∈ ℝ
⋱
⋮
⋮
_(cid:24)
,   (13) 
_(cid:24)i o
_ m = n  is obtained by  deleting 
_ h>>p
. By defining each row of 
W(cid:8)G
.                        (14) 
It is noted that 
.  in Eq. (13) can be expressed by 
W(cid:8)G = [r(cid:28), rj, ⋯ , r>]
_
_ = sh(cid:28)>, hI>, ⋯ , h(>q(cid:28))>t
∗
_Kr>
= [u(cid:28), uj, ⋯ , u>q(cid:28)]
∗  and 
= W(cid:28)_Kr>
.
.
.
,                                          (15)  where 
=
Next  the  eigenvalue  decomposition  of  the  covariance 
.
W(cid:28) = [r(cid:28), rj, ⋯ , r>q(cid:28)]
_K
.
KL(cid:8)G KL(cid:8)G  can be given by  matrix 
_(cid:24)i  is  an  where  composed of the eigenvectors of 
_(cid:24)i = ’(cid:24)i+i’(cid:24)i  as  (@ − 1) × (@ − 1)
’(cid:24)i
,                        (16)   unitary  matrix  b
,                 (17) 
_(cid:24)i
,  is  a  diagonal  matrix  of  and 
’(cid:24)i = [vi
, ⋯ , v>q(cid:28)
,
,
,
.  Similar  to  Eq.  (12),  the  eigenvalues  eigenvalues  of 
+i = T"(cid:12)U{ (cid:28)
,  I can be expressed as 
_(cid:24)i
,
, vj
}
, ⋯ ,  >q(cid:28)        (18) 
Following  the  idea  in  [25]  that  the  eigenvalues  in  (12) 
]
,
,
,  (cid:28)
,
≥  I
,
≥ ⋯ ≥  N
≥ ⋯ ≥  >q(cid:28)
,
≥  N?(cid:28) and (18) satisfy the interlacing property:  
,  (cid:28) ≥  (cid:28)
One  be defined as 
@ × @
,
,
≥  I ≥  I
≥ ⋯ ≥  N ≥  N  unitary  transformed  matrix
≥ ⋯ ≥  >q(cid:28) ≥  >q(cid:28)
,
≥  N?(cid:28) ≥  N?(cid:28)
,                  (19)   can 
≥  >  ’ b (’’
= w)
’(cid:24)i y
Thus, the transformed covariance matrix is obtained by  i y
’ = x z
.
.                            (20)  b
_{ = ’
_(cid:24)’ = |
= |
+i o
’(cid:24)i
_ b
’(cid:24)i
_ b
’(cid:24)i
_ h>> b
_(cid:24)i’(cid:24)i ’(cid:24)i
_ o h>>
’(cid:24)i
}
,  (cid:28) 0 0
⋮ 0
∗ (cid:129)(cid:28)
⎛
⎜
⎜
⎝
= 0
,  I 0
⋮ 0
∗ (cid:129)I 0 0 0
⋮ 0 ⋯ 0 ⋯
,
⋯  (cid:130)
⋱
⋮ 0 ⋯  >q(cid:28)
∗
⋯ (cid:129)>q(cid:28) (cid:129)(cid:130)
∗
,
, b
.  (cid:129)! = v!
_ = v!
, b
∗
W(cid:28)_Kr> (cid:129)(cid:28) (cid:129)I (cid:129)(cid:130)
⋮ (cid:129)>q(cid:28) h>> ⎠
⎞    (21) 
⎟
⎟                    (22)  where  for 
The  eigenvalues  of 
" = 1,2, ⋯ , @ − 1  can  be  estimated  by 
Gerschgorin’s  disk  theorem  [23].  The  radii  of  the  first 
_{  Gerschgorin’s disks can be expressed as   (@ − 1) for  (cid:11)! = |(cid:129)!| = (cid:135)v!
. 
, b
∗
W(cid:28)_Kr>
, b (cid:135) = (cid:135)v!
_(cid:135)
By Cauchy-Schwartz inequality, we can obtain that 
" = 1,2, ⋯ , @ − 1           (23) 
,      (24)  where  of the  of 
, b (cid:11)! = |(cid:129)!| = (cid:135)v!
, b
∗
W(cid:28)_Kr>
∗  is  independent  of 
W(cid:28)(cid:135) ∙ |_Kr>
≤ (cid:135)v!
| (cid:135)
| = (cid:137)(cid:135)v!
"
, b
.  Then  the  radius 
W(cid:28)(cid:135) (cid:142)(V)
>q(cid:28) ∑
, b
.  
If  v!
∗ th Gerschgorin’s disk actually depends on the size  (cid:137) = |_Kr> (cid:11)!
"  is  the  eigenvector  of  noise,  the  radius  of  the  th 
W(cid:28)
,
Gerschgorin’s disk will be significantly small and close to  v! zero. If   is the eigenvector of the low-rank part, the radius 
, of the  th Gerschgorin’s disk will be far from zero. In this  v! work, the rank is identified by the heuristic decision rule as                 （25） 
"
" (cid:14) (cid:11)! (cid:11)(cid:141)  when  (cid:14) = 1,2, ⋯ , @ − 2
C (cid:138)(cid:139)(cid:140)((cid:14))
>q(cid:28) where 
, and the adjustment factor  (cid:138)(cid:139)(cid:140)((cid:14)) = (cid:11)(cid:141) −
!=(cid:28)
. The rank of the  (between 0 to 1) is a constant related to  (cid:143)(C) low-rank  matrix  is   is  negative  for the first time. This implies that the rank can be estimated  by  comparing  the   with  a  threshold, which is equal to the product of the adjustment  factor   and the arithmetic mean of all Gerschgorin’s  disk radius.   (cid:143)(C) (cid:11) = (cid:14) − 1 th  Gerschgorin’s  disk  radius 
To further improve the accuracy of the rank estimation,  we  propose  a  new  method  to  shrink  the  radius  of  the 
Gerschgorin’s  disk.  The  idea  is  to  compress  the  radii  of  low-rank  Gerschgorin’s  disks  and  sparse  Gerschgorin’s  disks  to  different  degrees,  which  benefits  to  discriminate  between low-rank Gerschgorin’s disks and sparse ones. In  light  of  Eq.  (18),   of  the  sparse  Gerschgorin’s  disk  is  low-rank  significantly   can  be 
Gerschgorin’s  disk.  Thus,  the  diagonal  matrix  constructed as follows: 
, smaller   ! that  of  than  the 
, (cid:139) = T"(cid:12)U( (cid:28)
,
,  I
. The new transformed matrix 
, ⋯ ,  >q(cid:28)
)
,
,
,  > where 
,
>q(cid:28)  >
!=(cid:28) can be obtained by 
= (cid:144)∑
,I  ! (cid:139)                (26) 
_{(cid:139)
}
_{(cid:139) = (cid:139)_{(cid:139) q(cid:28) 6579                                                                                                            
,  (cid:28) 0 0
,  I
⎛
⎜
⎜
⎜
⎜
=
⋮ 0
, (cid:145)(cid:146)
∗
, (cid:129)(cid:28) (cid:145)(cid:147)
⎝
It is noted that 
⋮ 0
, (cid:145)(cid:148)
∗
, (cid:129)I (cid:145)(cid:147)
⋯  and 
⋯
⋯
⋱
⋯ 0 0
, (cid:145)(cid:146)
, (cid:129)(cid:28) (cid:145)(cid:147)
, (cid:145)(cid:148)
, (cid:129)I (cid:145)(cid:147)
⋮
,  >q(cid:28)
, (cid:145)(cid:147)(cid:149)(cid:146)
∗
, (cid:129)>q(cid:28) (cid:145)(cid:147)
⋮
, (cid:145)(cid:147)(cid:149)(cid:146)
, (cid:129)>q(cid:28) (cid:145)(cid:147) h>>  (27) 
⎞
⎟
⎟
⎟
⎟
⎠
_{
_{(cid:139)  are similar matrices and their  eigenvalues are the same. Given Eq. (27), the centers of the 
Gerschgorin’s  disks  are  not  changed,  but  the  radii  are  compressed  to  various  degrees.  The  radii  of  the  sparse 
Gerschgorin’s disks are compressed more than those of the  low-rank  Gerschgorin’s  disks.  Then,  we  can  estimate  the  rank by the improved heuristic decision rule as  (cid:28)
_W(cid:150)(cid:151)((cid:14)) = where  (cid:144)∑ (cid:147)(cid:149)(cid:146) (cid:152)(cid:153)(cid:146)
,(cid:148) (cid:145)(cid:152) (cid:142) (V)
,
>q(cid:28) ∑ n(cid:135) (cid:141)
,  and  the  adjustment  factor  (cid:135)(cid:11)(cid:141) −
>q(cid:28)
!=(cid:28)
, (cid:135) ! (cid:135)(cid:11)! p ((cid:154))   (28)   if  the  first  negative  0 <
.  The  rank  (cid:14) = 1,2, ⋯ , @ − 2 value of (28) is reached at  (cid:143) (C) < 1 ((cid:141))
.  (cid:11) = (cid:14) − 1 (cid:14)
It is highlighted that the parameter   in Eq. (25) is  manually  set  according  to
.  A  very  high  or  a  very  low  adjustment factor yields inaccurate rank estimation. To this  end, we define a new adjustment factor  by using  the center of the Gerschgorin’s disk, which is given by   (cid:143)(C)  C ((cid:141)) (cid:143) (C)   .                        (29)  ((cid:141)) (cid:143) (C) = (cid:135)
,
I(cid:135)(cid:145)(cid:154)(cid:156)(cid:146) (cid:147)(cid:149)(cid:146) (cid:152)(cid:153)(cid:154)
,(cid:148) (cid:145)(cid:152) (cid:144)∑  is  completely  determined  by  the 
The  value  of   itself without heuristic selection. By using Eqs.  matrix  (28)  and  (29),  an  automatic  and  improved  method  is  accordingly developed for rank identification.   (C)
_{(cid:139) ((cid:141)) (cid:143) 2.2. Adaptive RPCA based on Iterative Rank Estimate  (cid:8)
In this subsection, a new adaptive RPCA, which updates  the weights of singular values via iterative rank estimate, is  proposed to recover the low-rank matrix from the corrupted  measurements.  Specifically,  the  low-rank  recovery  is  achieved  by  following  optimization  the  formulation:  solving      (30)   and 
!
‖(cid:8)‖; = ∑ <! !((cid:8))  ||(cid:8)||; + (cid:18)‖(cid:10)‖(cid:28)    (cid:21). (cid:23).   (cid:24) = (cid:8) + (cid:10) arg min  are  non-negative  where  (cid:8),(cid:10) weights. Given the rank   estimated in the previous section,  the proposed idea, different from the existing solutions [20],  is to preserve the singular values within the target rank, i.e.   while  minimizing  the  singular  values  outside  the   obtained by  target rank, i.e.   (cid:28)2!2N
Eq. (30) is better close to the target low-rank matrix. Hence,   N?(cid:28)2!2> we define the weight as 
, such that the matrix 
<! (cid:8) (cid:11) 0, " ≤ (cid:11) where   is  the  rank  of  the  low-rank  matrix,  which  is  1, F(cid:23)ℎ(cid:159)(cid:11)(cid:160)"(cid:21)(cid:159) estimated by Eq. (28). Thus only residual singular values 
<! = (cid:157) (cid:11)                        (31)  6580 are minimized, such that the recovered low-rank matrix has  rank close to the estimated rank r.  
In  general,  the  solution  to  Problem  (30)  has  to  been  performed  via  iterative  technique  by  fixing  rank 
. 
Different  from  such  processing,  the  proposed  menthod  updates  the  rank  r  according  to  Eq.  (28)  in  the  iterative  procedure. This reveals that the weights shown in Eq. (31)  updates  the  optimization  via  Eqs  (28)-(31)  is  called  adaptive  rank  estimate  based  RPCA  (ARE-RPCA).  In  other  word, 
Eq.(28) provides an initial estimation of rank 
, but the rank  r is updated iteratively in solving Eq.(30).  iteration  accordingly.  Hence,  in  each  (cid:11)
In  this  work,  the  alternating  direction  method  of  multipliers  (ADMM)  is  employed  to  solve  Problem  (30). 
The  augmented  Lagrangian  function  of  Eq.  (30)  can  be  written as  (cid:11)
〈∙,∙〉
ℒ((cid:8), (cid:10), ¢) = ‖(cid:8)‖; + (cid:18)‖(cid:10)‖(cid:28) + 〈¢, (cid:24) − (cid:8) − (cid:10)〉
¥
I  represents  matrix  inner  product,  (cid:148)‖(cid:24) − (cid:8) − (cid:10)‖%
+                   (32)   is  a  positive  where  penalty scalar, and   is the Lagrangian multiplier. As it is  difficult  to  solve  the  minimization  of  Eq.  (32),  an  alternativesolution is to optimize one variable  while fixing  the others. Accordingly, the optimization is divided into the  following three sub-problems.   sub-problem:  While  both   are  fixed,  Eq.  (32)  is   and 
¢
ƒ equal to the following optimization problem:  (cid:10)
¢ (cid:8)
∗ (cid:10)
= arg min(cid:10)  (cid:18)‖(cid:10)‖(cid:28) + 〈¢, (cid:24) − (cid:8) − (cid:10)〉
¥
I (cid:148)‖(cid:24) − (cid:8) − (cid:10)‖%
+ (cid:146) (cid:148)‖(cid:10) − ((cid:24) − (cid:8) + ƒ  and 
§
¤ ‖(cid:10)‖(cid:28) +  sub-problem:  Given 
= arg min(cid:10) following optimization problem:  (cid:8)
¢ (cid:10)      (33) 
,  Eq.  (32)  leads  to  the  q(cid:28)
I
¢)‖%
∗
= arg min(cid:8)  ‖(cid:8)‖; + 〈¢, (cid:24) − (cid:8) − (cid:10)〉 + (cid:28)
¤ ‖(cid:8)‖; +
= arg min(cid:8)  sub-problem: 
¥
I (cid:148)‖(cid:24) − (cid:8) − (cid:10)‖%
I
¢)‖%
.              (35) 
In  order  to  solve  the  three  sub-problems,  a  soft-(cid:146) (cid:148)‖(cid:8) − ((cid:24) − (cid:10) + ƒ  is updated by 
¢(cid:141)?(cid:28) = ¢(cid:141) + ƒ((cid:24) − (cid:8)(cid:141)?(cid:28) − (cid:10)(cid:141)?(cid:28))      (34)  q(cid:28)
¢
¢ thresholding operator is introduced:                      (36) 
'“[«] ≐ ›  and  where  the well-known analysis [26]: 
. 
« ∈ ℝ
ﬁ > 0
∗ (cid:10)
« − ﬁ,       "M « > ﬁ
« + ﬁ,      "M « < −ﬁ 0,             F(cid:23)ℎ(cid:159)(cid:11)(cid:160)"(cid:21)(cid:159)  in Eq. (33) can be obtained by                        (37)  q(cid:28)
∗ (cid:10)
= 'ﬂ
¥[(cid:24) − (cid:8) + ƒ with the operation being element-wise. In order to solve the  optimization  (34),  we  first  give  the  following  lemma  and  theorems 
¢]
Lemma 1 If   satisfy 
, we have  6×7 (cid:176), (cid:139) ∈ ℝ
.                            (38)  (cid:176) (cid:139) = 0                              (39) 
‖(cid:176) + (cid:139)‖; ≥ ‖(cid:176)‖;  is defined in Eq. (4). Detailed proof of Lemma 
‖(cid:176) + (cid:139)‖% ≥ ‖(cid:176)‖% where  1 is demonstrated in Supplementary Materials.  
‖∙‖;                    
Theorem  1  Given   where solution to the minimization problem   6×7 – ∈ ℝ
,  the 
.  – = ’–(cid:139)–-–                 (40)  (cid:28)
I
I ‖† − –‖%
, where   is the solution of the following 
+ &‖†‖; arg min† is 
. optimization problem: 
†‡ = ’–‘‡†-–
‘‡†
I (cid:28)
I ·‘‡† − (cid:139)–·%
Based on Theorem 1, we obtain the following important 
‘‡† = arg min‘‡†
+ &·‘‡†·;
.        (41)  result. 
Theorem  2  Given
,   where 6×7
,   and   – =
†, – ∈ ℝ  & > 0
.  We  can  define  and 
,  (cid:139)– = T"(cid:12)U(cid:181)¶–(cid:28), ⋯ , ¶–N, ¶–(•?(cid:28)), ⋯ , ¶–ℓ‚
.
’–(cid:139)–-–
,  where –(cid:28) =  – = –(cid:28) + –I
ℓ = „"(cid:13)(„, (cid:13))
.
.  are  the  singular   and 
’–(cid:28)(cid:139)–(cid:28)-–(cid:28) –I = ’–I(cid:139)–I-–I (cid:139)–(cid:28) = th  largest  singular  vector  matrices  corresponding  to  the 
T"(cid:12)U(cid:181)¶–(cid:28), ⋯ , ¶–N, 0, ⋯ ,0‚
’–(cid:28) values,  and  th to   is defined as shown in Eq. (30) and Eq. (31).   corresponding to the singular values from   (cid:139)–I = T"(cid:12)U(cid:181)0, ⋯ ,0, ¶–(•?(cid:28)), ⋯ , ¶–ℓ‚ the last. 
-–I
The optimal solution to the minimization problem 
’–I ((cid:11) + 1)
-–(cid:28) (cid:11)
, 
, 
‖∙‖;                  (42)  arg min† can be expressed as  (cid:146)
I (cid:148) ‖† − –‖%
+ &‖†‖;
∗
†
.  .    (43) 
= ”),;[–] = ’–(cid:181)(cid:139)–(cid:28) + ')s(cid:139)–It‚-–
.
Refer to Supplementary Materials for detailed proofs of 
= –(cid:28) + ’–I')s(cid:139)–It-–I
Theorem 1 and Theorem 2 due to space limitation.  
In light of Theorem 2,   in Eq. (34) can be obtained by 
.                    (44) 
∗
∗ (cid:8) (cid:146)
¥,;[(cid:24) − (cid:10) + ƒ (cid:8)
The entire procedure to solve problem (30) is summarized  in Algorithm 1. 
= ”
¢] q(cid:28)
Algorithm 1 Adaptive rank estimate based RPCA (ARE-RPCA): 
Input: 
;  6×7 1:  Initialization: 
⁄ (cid:24) ∈ ℝ
, and  by Eq. (28),  2: while not converged do  3:   compute 
ƒ = 1  N⁄
<!
, (cid:18) = 1 »max („, (cid:13)) (cid:10)(cid:20) = ¢(cid:20) = y ∈ ℝ
,   is defined by Eq. (31);  (cid:11)  is  estimated  6×7 (cid:146)
¥,;[(cid:24) − (cid:10)(cid:141) + ƒ 4:   compute  (cid:8)(cid:141)?(cid:28) = ” (cid:10)(cid:141)?(cid:28) = Σﬂ  and  5:   compute  update  6:   (31), respectively; 
¥[(cid:24) − (cid:8)(cid:141)?(cid:28) + ƒ  according  to  Eq.  (28)  and  Eq. 
¢(cid:141)]
; 
¢(cid:141)?(cid:28) = ¢(cid:141) + ƒ((cid:24) − (cid:8)(cid:141)?(cid:28) − (cid:10)(cid:141)?(cid:28)) (cid:11)
<!
;   q(cid:28)
¢(cid:141)]
;  q(cid:28) 7: end while  8: output: 
. 
Remark 1: In Algorithm 1, the parameter  (cid:8), (cid:10)  is set as 
Figure 1 The value of 
Assuming  that  and the corrupted rate  (cid:24) ∈ _
,   and 
,  (cid:190)(cid:10) (cid:190)(cid:8) 6×7 (cid:190)(cid:24)
.   
„ = 10000 with the number of iterations. 
, 
, 
,  (cid:13) = 20 (cid:11)(cid:12)(cid:13)(cid:14)((cid:8)) = 3
Table 1 Number of iterations, CPU time,   and   reconstruction 
¿ = 0.05 error for LSD, LRSD, RPCA, SRPCP, WNNM, ARE-RPCA  (cid:8) (cid:10)
Table 2: The rank of low-rank matrix   decomposed by LSD, 
LRSD, RPCA, SRPCP, WNNM and ARE-RPCA for different  value of corrupt rate  (cid:8)
¿
,  which  is  recommended  in  RPCA.  The  (cid:18) = (cid:18) terminated  when  iteration  1 »max („, (cid:13))
⁄ is 
.  q‰
Remark  2:  It  is  noted  that  10
||(cid:24)||% q(cid:28)
ƒ
||(cid:24) − (cid:8) − (cid:10)||% ≤  occurs  in  singular  value  6581                
Figure 2 PSNR for various algorithms with different sample sizes 
, matrix ranks 
, and corrupt rate 
.  (cid:8) thresholding  operators.  When  proportion  of  singular  values  of 
ƒ exceed the threshold and make the rank of  (cid:24) − (cid:10)(cid:141) + ƒ
∗ is  simply  chosen  as
In  classical  RPCA,   is  small,  a  large   would  q(cid:28)  be too large. 
¢(cid:141) it  q(cid:28) (cid:11)
. 
ƒ  by 
ƒ = 1  N⁄ th singular value of 
,  which is not related  with singular values. 
ƒ =
,  where 
In  this  work,  we  select  the  size  of  m × n 4‖(cid:24)‖(cid:28)
⁄ is the 
Remark  3:  It  should  be  noted  that  the  proposed   N  (cid:24) alternative Algorithm 1 follows the framework of inexact  augmented Lagrangian multiplier (IALM) [27]. However,  the  weights  in  Eq.  (30)  are  given  by  Eq.  (31)  and  the  underlying  problem  is  usually  nonconvex.  Although  mathematical proof of the convergence is challenging, the  following empirical claim is provided.   and 
Claim  1:  The  sequences   generated  by 
Algorithm 1 satisfy: 
{(cid:8)(cid:141)}
{(cid:10)(cid:141)} (cid:190)(cid:8) = lim(cid:141)→˜‖(cid:8)(cid:141)?(cid:28) − (cid:8)(cid:141)‖% = 0 (cid:190)(cid:10) = lim(cid:141)→˜‖(cid:10)(cid:141)?(cid:28) − (cid:10)(cid:141)‖% = 0 6582 (cid:13) (cid:11)(cid:20)
¿
Claim 1 has been proved by the experiment shown in Fig.  (cid:190)(cid:24) = lim(cid:141)→˜‖(cid:24) − (cid:8)(cid:141)?(cid:28) − (cid:10)(cid:141)?(cid:28)‖% = 0 1.  3. Experimental Results 
In this section, we report the experimental results of our  adaptive  rank  estimate  based  RPCA  (ARE-RPCA),  and  compare  it  with  the  state-of-the-art  RPCA  algorithms  (RPCA  [2],  WNNM  [20],  SRPCP  [28],  LSD  [29]  and 
LRSD [30]). All the experiments are conducted on a laptop  equipped with Windows 10, AMD Ryzen 7 4800H (8 Cores  at 2.9 GHz) and 16GB DDR4-3200Mhz RAM, and running  in MATLAB R2018b.  3.1. Synthetic Datasets 
In  this  subsection,  we  test  the  algorithms  on  synthetic   is  generated  by   with  data.  A  matrix  sampling  two  matrices,   with  rank  6×7  and  (cid:11)(cid:20) (cid:8) ∈ ℝ 6×N˘
†¯ ∈ ℝ
N˘×7 –¯ ∈ ℝ            
entries belonging to normal distribution 
.  In  this  experiment,  we  set  6×7
,  which  has  ground  truth  matrix  (cid:8) = †¯–¯  is  corrupted  by  sparse  noise 
˙(0, 1)
„ = 10000  non-zero  elements.  The  (cid:10) ∈  are randomly selected,  positions of non-zero elements in 
¿ × („ × (cid:13))
ℝ and  its  value  is  generated  from  a  Gaussian  distribution 
.  Therefore ， we  generate  synthetic  data   as  (cid:10) (cid:8)
, namely, 
.  The  follows: 
˙(0, 1)
.                                       
We first evaluate the peak signal to noise ratio (PSNR)  (cid:24)!,¨ = (cid:8)!,¨ + (cid:10)!,¨ under different settings such as different sample sizes  (cid:24)
,  different  matrix  ranks 
,  and  (cid:13) ∈  from 0.05 to 0.3. The simulation  different corrupt rate of 
{20, 40, 60} results are shown in Fig. 2. It is observed from Fig. 2 (a - c)  that  when  the  sample  sizes   is  small,  both  our  proposed 
ARE-RPCA  and  WNNM  outperform  others,  but  ARE-(cid:13)
RPCA is better than WNNM in the case of a lower corrupt  rate ( (cid:11)(cid:20) ∈ {1, 3, 5}
). 
¿
¿ ≤ 0.05
By comparing Fig. 2 (a, d, g), Fig. 2 (b, e, h) and Fig. 2  (c, f, i), it can be seen that the performance of all algorithms  is gradually enhanced with the increase of   under the same  rank.  However,  the  performance  of  the  proposed  ARE-RPCA algorithm is better than others in most cases except  for the large number of samples and large rank where ARE-RPCA is slightly weaker than WNNM. However, WNNM  algorithm has the disadvantage that it needs to adjust one  regularization  parameter  C  in  weight  updating  (cid:13)
⁄ (see  the  details  in  [20],  this  parameter  is 
<! = manually  adjusted  to  the  optimal  values  according  to  (cid:190) ( !((cid:8)) different  actual  environments  in  this  paper).  ARE-RPCA  algorithm has strong adaptability because it does not need  to be adjusted for different application scenarios. 
+ ˚)  (cid:8)¯¸(cid:204)  and 
Next, we evaluate the number of iterations, running time,   and sparse matrix   in  a  certain  reconstruction error of low-rank matrix   and  (cid:8) (cid:10)¯¸(cid:204) algorithm  and  define  the  reconstruction  error  as  (cid:10)
.  Denote  the  solution  as  (cid:10)•˝ = ||(cid:10)¯¸(cid:204) − (cid:10)||% ||(cid:10)||%
.  The  (cid:8)•˝ = test  results  are  presented  in  Table  1.  It  can  be  seen  that 
||(cid:8)¯¸(cid:204) − (cid:8)||% ||(cid:8)||%
Although  our  ARE-RPCA  is  the  fastest  among  all  algorithms although it has more iterations than SRPCP and 
LRSD. It also can be seen that there is almost no difference 
,  but  the  in  the  reconstruction  error  of  sparse  matrix  reconstruction  error  of  low-rank  matrix   of  our  ARE-RPCA is obviously less than others.  (cid:10)
⁄
⁄
Finally,  we  consider  the  rank  of  the  low-rank  matrix  decomposed  by  different  algorithms  under  different  (cid:8) corruption rates. Setting 
, the test results  are  shown  in  Table  2.  It  can  be  observed  that  our  ARE-(cid:13) = 20
RPCA  and  WNNM  can  obtain  correct  rank  of  low-rank  matrix in all cases even with large corruption rates. Other  algorithms  can  estimate  the  rank  correctly  only  if  the  corruption rate is low. Overall, our proposed ARE-RPCA is  (cid:11)(cid:20) = 5  and  (cid:8) 1 The Street dataset is provided in the supplementary materials.  6583
Figure 3 Video background subtraction: the top row corresponds  to one frame from the video. The second to last rows are the  separated background and foreground of FFP, MoG-RPCA, 
RPCA, WNNM and ARE-RPCA, respectively.  fast and highly accurate.  3.2. Real Datasets 
In this subsection, we compare the performance of ARE-RPCA, WNNM, RPCA, FFP [31], MoG-RPCA [32] on two  real  world  benchmark  problems:  video  background  subtraction and low dynamic range imaging.  
The  task  of  background  subtraction  is  to  separate  the  moving foreground object from the static background. We  choose  a  Street  dataset 1  that  has  a  relatively  static  background and a walking person as a dynamic foreground. 
The size of each frame of the Street dataset is 
. 
The total number of frames are 48 in the Street dataset. The  1920 × 1080 dataset can be represented by a matrix, where each column  of the matrix is a vectorized frame of the video. Then we  apply  each  algorithm  to  decompose  the  matrix  into  low-rank parts representing the static background of the video  and  sparse  parts  representing  the  moving  objects  in  the  video. The results are shown in Fig. 3. RPCA cannot well  separate the foreground from the background as shown in 
Fig. 3 (c). It can be seen from Fig. 3 (a) that FFP has better                                                                      
on  the  blue  and  green  components,  but  it  has  poorer  performance on the red component. As shown in Fig. 3(d,  e),  one  can  see  that  both  WNNM  and  ARE-RPCA  can  effectively  separate  the  foreground  and  background,  and  our proposed ARE-RPCA is better than WNNM. 
In  order  to  obtain  high-contrast  scene  images,  low  dynamic range (LDR) imaging technology needs to be used  to remove out-of-focus blur and dynamic objects in pictures  captured  by  low  dynamic  range  cameras.  We  select  the 
Arch dataset1 [33] and stack each image as a column into a  matrix. The size of each frame of the Arch dataset is 
. The total number of frames are 5 in the Arch dataset.  669 ×
Then we can use each algorithm to decompose the matrix  1024 into a low-rank part representing the scene and sparse part  representing dynamic objects. The experimental results are  shown in Fig. 4. It can be seen from Fig. 4 (c) that RPCA  cannot remove moving objects well. From Fig. 4. (a), FFP  has a better effect, but still has a larger ghost image. It can  be seen from Fig. 4(b) that MoG-RPCA cannot effectively  remove the moving objects in the red component. As shown  in Fig. 4 (d, e), one can observe that both WNNM and ARE-RPCA  can  effectively  remove  moving  objects,  and  our  proposed ARE-RPCA handles ghosting better.  4. Conclusions 
Robust principal component analysis (RPCA), due to its  powerful  capability  in  dealing  with  outliers,  has  been  gained wide applications in computer vision. To cope with  the issues that some RPCA variants need predefine the rank  of low-rank matrix and manually adjust some parameters,  an  adaptive  rank  estimate  based  RPCA  (ARE-RPCA)  is  proposed in this paper. Specifically, the rank of a low-rank  matrix is identified via Gerschgorin disk method. To avoid  setting  adjustment  factor  in  Gerschgorin  disk  method,  an  improved  rank  estimation  algorithm  is  proposed.  On  the  other  hand,  a  novel  RPCA  method  with  weight  updating  based  on  the  iteratively  estimated  rank  is  proposed  to  recover  the  low-rank  structure  of  a  data  matrix  and  the  sparse representation from corrupted data, which makes our  improved  algorithm  accurate  and  effective.  Experimental  results on synthetic data demonstrate that the identified rank  is  close  to  the  ground  truth,  and  the  results  on  real  data  indicate  that  the  proposed  ARE-RPCA  outperforms  the  state-of-the-art methods in terms of efficiency and accuracy. 
The proposed method will greatly facilitate RPCA in real  applications.  5. Acknowledgement 
This work was supported in part by the National Natural 
Science  Foundation  of  China  under  Grants  61775172,  61371190,  and  Hubei  Key  Technical  Innovation  Project  under Grant ZDCX2019000025. 
Figure 4 Low-Dynamic Range Imaging: the top row corresponds  to one frame of a sequence with differently exposed changes. 
The second to last rows are the separated static part and dynamic  part of FFP, MoG-RPCA, RPCA, WNNM and ARE-RPCA,  respectively.  performance  than  RPCA,  but  it  still  cannot  completely  separate the foreground from the background. From Fig. 3  (b), it can be seen that MoG-RPCA has better performance  1 http://alumni.soe.ucsc.edu/~orazio/deghost.html  6584                                                                   
References 
[1]  I. Jolliffe. Principal Component Analysis. Springer, 2002. 
[2]  Emmanuel  J.  Candès,  Xiaodong  Li,  Yi  Ma,  and  John  A 
Wright. Robust principal component analysis? Journal of the 
ACM, 58(3):1-37, 2011. 
[3]  Chengjin  Li.  A  new  approximation  of  the  matrix  rank  function  and  its  application  to  matrix  rank  minimization. 
Journal of Optimization Theory & Applications, 163(2):569-594, 2014. 
[4]  Niannan  Xue,  Jiankang  Deng,  Shiyang  Cheng,  Yannis 
Panagakis, and Stefanos Zafeiriou. Side information for face  completion: a robust PCA approach. IEEE Trans. on Pattern 
Analysis and Machine Intelligence, 41(1):2349-2364, 2019. 
[5]  Lei Wang, Bangjun Wang, Zhao Zhang, Qiaolin Ye, Liyong 
Fu, Guangcan Liu, and Meng Wang. Robust auto-weighted  projective  for  visual  representation. Neural Networks, 117(1):201-215, 2019. 
[6]  Sait  Melih  Doğan,  and  Özgül  Salor.  Music/Singing  voice  separation  based  on  repeating  pattern  extraction  technique  and  robust  principal  component  analysis.  In  2018  5th 
International  Conference  on  Electrical  and  Electronic 
Engineering (ICEEE), pages 482-487, 2018.  low-rank  and  sparse  recovery 
[7]  Hongyang  Xue,  Shengming  Zhang,  and  Deng  Cai.  Depth  image  inpainting:  Improving  low  rank  matrix  completion  with  low  gradient  regularization.  IEEE  Trans.  on  Image 
Processing, 26(9):4311-4320, 2017. 
[8]  Zhuorui  Yang,  Marco  F.  Duarte,  and  Aura  Ganz.  A  novel  crowd-resilient visual localization algorithm via robust PCA  background extraction. in 2018 International Conference on 
Acoustics, Speech, and Signal Processing (ICASSP), 2018.  
[9]  Paris  V  Giampouras,  Athanasios  A.  Rontogiannis,  and 
Konstantinos D. Koutroumbas. Robust PCA via alternating  iteratively reweighted low-rank matrix factorization. in 2018 
International Conference on Image Processing (ICIP), 2018.  
[10] Linhao Li, Qinghua Hu, and Xin Li. Moving object detection  in  video  via  hierarchical  modeling  and  alternating  optimization. IEEE Trans. on Image Processing, 28(1):2021-2036, 2018. 
[11] Thierry  Bouwmans,  Sajid  Javed,  Hongyang  Zhang, 
Zhouchen  Lin,  and  Ricardo  Otazo.  On  the  applications  of  robust PCA in image and video Processing. Proceedings of  the IEEE, 106(1):1427-1457, 2018. 
[12] Namrata  Vaswani,  Thierry  Bouwmans,  Sajid  Javed,  and  learning: 
Praneeth  Narayanamurthy.  Robust  subspace 
Robust PCA, robust subspace tracking and robust subspace  recovery.  IEEE  Signal  Processing  Magazine,  35(4):32-55,  2018. 
[13] Thierry Bouwmans, Andrews Sobral, Sajid Javed, Soon Ki 
Jung, and El Hadi Zahzah. Decomposition into low-rank plus  additive  matrices  for  background/foreground  separation:  A  review  for  a  comparative  evaluation  with  a  large-scale  dataset. Computer Science Review, 23(1):1-71, 2017. 
[14] Thierry  Bouwmans  and  El  Hadi  Zahzah.  Robust  PCA  via  principal  component  pursuit:  A  review  for  a  comparative  evaluation in video surveillance. Computer Vision and Image 
Understanding, 122(1):22–34, 2014. 
[15] Kim  Chuan  Toh  and  Sangwoon  Yun.  An  accelerated  proximal  gradient  algorithm  for  nuclear  norm  regularized  least  squares  problems.  Pacific  Journal  of  Optimization,  6(3):615-640, 2010. 
[16] Jian  Feng  Cai,  Emmanuel  J.  Candès,  and  Zuowei  Shen.  A  singular value thresholding algorithm for matrix completeon. 
SIAM Journal on Optimization, 20(4):1956-1982, 2010. 
[17] Paul  Rodriguez  and  Brendt  Wohlberg.  Performance  comparison  of  iterative  reweighting  methods  for  total  variation regularization. in 2014 International Conference on 
Image Processing (ICIP) 2014. 
[18] Yao Hu, Debing Zhang, Jieping Ye, Xuelong Li, and Xiaofei 
He.  Fast  and  accurate  matrix  completion  via  truncated  nuclear  norm  regularization.  IEEE  Trans.  on  Pattern 
Analysis and Machine Intelligence, 35(9):2117–2130, 2013. 
[19] Shuhang  Gu,  Lei  Zhang,  Wangmeng  Zuo,  and  Xiangchu 
Feng. Weighted nuclear norm minimization with application  to image denoising. in 2014 Conference on Computer Vision  and Pattern Recognition (CVPR), pages 2862-2869, 2014. 
[20] Shuhang  Gu,  Qi  Xie,  Deyu  Meng,  Wangmeng  Zuo,  and 
Xiangchu Feng. Weighted nuclear norm minimization and its  applications  to  low  level  vision.  International  Journal  of 
Computer Vision, 121(2):183-208, 2017. 
[21] Tae-Hyun  Oh,  Yu-Wing  Tai  Tai,  Jean-Charles  Bazin,  and 
Hyeongwoo  Kim.  Partial  sum  minimization  of  singular  values  in  robust  PCA:  algorithm  and  applications.  IEEE 
Trans.  on  Pattern  Analysis  and  Machine  Intelligence,  38(4):744-758, 2016. 
[22] Tae-Hyun Oh, Hyeongwoo Kim, Yu-Wing Tai, Jean-Charles 
Bazin,  and  In  So  Kweon.  Partial  sum  minimization  of  singular  values  in  RPCA  for  low-level  vision.  in  2013 
International Conference on Computer Vision (ICCV), pages  145-152, 2013. 
[23] Yuanming  Guo,  Wei  Li,  Junyuan  Shen,  Jinjun  Zhang,  and 
Chengguang  Shao.  Source  number  estimation  with 
Gerschgorin  radii  by  reduced-rank  covariance  matrix.  in  2014 
Information  & 
Communications Technologies (ICT), 2014. 
International  Conference  on 
[24] Payal Gupta and Monika Agrawal. Design and analysis of the  sparse array for DOA estimation of noncircular signals. IEEE 
Trans. on Signal Processing, 67(2):460-473, 2019. 
[25] Zhouchen  Lin,  Risheng.  Liu,  and  Zhixun  Su.  Linearized  alternating  direction  method  with  adaptive  penalty  for  low  rank  representation.  in  Neural  Information  Processing 
Systems (NIPS), pages 612-620, 2011. 
[26] Elaine  T.  Hale,  Wotao  Yin,  and  Yin  Zhang.  Fixed-point  continuation 
-minimization:  Methodology  and  convergence.  SIAM  Journal  on  Optimization,  19(3):1107– 1130, 2008. 
[27] Fanhua  Shang,  Yuanyuan  Liu,  James  Cheng,  and  Hong 
Cheng. Recovering low-rank and sparse matrices via robust  bilateral factorization. in 2014 International Conference on 
Data Mining (ICDM), pages 965-970, 2014.  for 
ℓ(cid:28)
[28] Jing  Liu  and  Bhaskar  D.  Rao.  Robust  PCA  via 
-ℓ(cid:20) regularization. IEEE Trans. on Signal Processing, pages 1-ℓ(cid:28) 15, 2018. 
[29] Xin  Liu,  Guoying  Zhao,  Jiawen  Yao,  and  Chun  Qi.