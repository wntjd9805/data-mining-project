Abstract
Modern deep convolutional networks (CNNs) are often criticized for not generalizing under distributional shifts.
However, several recent breakthroughs in transfer learning suggest that these networks can cope with severe distribution shifts and successfully adapt to new tasks from a few training examples. In this work we study the interplay between out-of-distribution and transfer performance of modern image classiﬁcation CNNs for the ﬁrst time and investigate the impact of the pre-training data size, the model scale, and the data preprocessing pipeline. We ﬁnd that increasing both the training set and model sizes signiﬁcantly improve the distributional shift robustness. Furthermore, we show that, perhaps surprisingly, simple changes in the preprocessing such as modifying the image resolution can signiﬁcantly mitigate robustness issues in some cases. Finally, we outline the shortcomings of existing robustness evaluation datasets and introduce a synthetic dataset SI-SCORE we use for a systematic analysis across factors of variation common in visual data such as object size and position. 1.

Introduction
Deep convolutional networks have attained impressive results across a plethora of visual classiﬁcation benchmarks
[36, 60] where the training and testing distributions match.
In the real world, however, the conditions in which the mod-els are deployed can often differ signiﬁcantly from the con-ditions in which the model was trained. It is thus imperative to understand the impact dataset shifts [50] have on the per-formance of these models. This problem has gained a lot of traction and several systematic investigations have shown unexpectedly high sensitivity of image classiﬁers to various dimensions, including photometric perturbations [27], natu-ral perturbations obtained from video data [54], as well as model-speciﬁc adversarial perturbations [23].
The problem of dataset shift, or out-of-distribution (OOD) generalization, is closely related to a learning paradigm
∗Shared ﬁrst authorship.
Please send e-mail correspondence to
{josipd,lucic}@google.com.
Figure 1: We explore the fundamental interplay between in-distribution performance, out-of-distribution (OOD) performance, and transfer learning performance (red arrows in the graph on the right), with respect to the major design choices listed on the left. The relationship between in-distribution and OOD performance is highly under-explored along these axes, whereas that between OOD and transfer performance has not been studied before to the best of our knowledge. known as transfer learning [56, §13]. In transfer learning we are interested in constructing models that can improve their performance on some target task by leveraging data from different related problems. In contrast, under dataset shift one assumes that there are two environments, namely training and testing [56], with the constraint that the model cannot be adapted using data from the target environment.
As a consequence, the two environments typically have to be more similar and their differences more structured than in the transfer setting (c.f. Section 2).
In the context of transfer learning, detailed scaling laws characterizing the interplay between the in-distribution and transfer performance as a function of pre-training data set size, model size, architectural choices such as normaliza-tion, and transfer strategy have been established recently
[37, 72, 36]. Model and dataset scale were identiﬁed as key factors for transfer performance. The similarities be-tween transfer learning and OOD generalization suggests that these axes are also relevant for OOD generalization and raises the question of what the corresponding scaling laws are. While some axes have been partially explored by prior work [27, 70], the big picture is largely unknown. Even more importantly, is in-distribution performance enough to charac-terize OOD performance, or can transfer performance give a more ﬁne-grained characterization of OOD performance 16458
of a population of models than in-distribution performance?
To the best of our knowledge, this question has not been systematically explored before in the literature.
Contributions We systematically investigate the interplay between the in-distribution accuracy of image classiﬁcation models on the training distribution, their generalization to
OOD data (without adaptation), and their transfer learning performance with adaptation in the low-data regime (see
Fig. 1 for an illustration). Speciﬁcally: (i) We present the ﬁrst meta-analysis of existing OOD met-rics and transfer learning benchmarks across a wide variety of models, ranging from self-supervised to fully supervised models with up to 900M parameters. We show that increasing the model and data scale dispro-portionately improves transfer and OOD performance, while only marginally improving the performance on the IMAGENET validation set. (ii) Focusing on OOD robustness, we analyze the effects of the training set size, model scale, and the training regime and testing resolution, and ﬁnd that the effect of scale overshadows all other dimensions. (iii) We introduce a novel dataset for ﬁne-grained OOD analysis to quantify the robustness to object size, object location, and object orientation (rotation angle). We be-lieve that this is a ﬁrst systematic study to show that the models become less sensitive (and hence more robust) to each of these factors of variation as the dataset size and model size increase. 2.