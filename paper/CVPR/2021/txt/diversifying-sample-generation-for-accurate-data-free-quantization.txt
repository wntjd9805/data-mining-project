Abstract
Quantization has emerged as one of the most prevalent approaches to compress and accelerate neural networks. Re-cently, data-free quantization has been widely studied as a practical and promising solution. It synthesizes data for calibrating the quantized model according to the batch nor-malization (BN) statistics of FP32 ones and signiﬁcantly relieves the heavy dependency on real training data in tra-ditional quantization methods. Unfortunately, we ﬁnd that in practice, the synthetic data identically constrained by BN statistics suffers serious homogenization at both distribution level and sample level and further causes a signiﬁcant per-formance drop of the quantized model. We propose Diverse
Sample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization. Speciﬁcally, we slack the alignment of feature statistics in the BN layer to relax the constraint at the distribution level and design a layerwise enhancement to reinforce speciﬁc layers for different data samples. Our DSG scheme is versatile and even able to be applied to the state-of-the-art post-training quantization method like AdaRound. We evaluate the DSG scheme on the large-scale image classiﬁcation task and consistently obtain signiﬁcant improvements over various network archi-tectures and quantization methods, especially when quan-tized to lower bits (e.g., up to 22% improvement on W4A4).
Moreover, beneﬁting from the enhanced diversity, models calibrated with synthetic data perform close to those cali-brated with real data and even outperform them on W4A4. 1.

Introduction
Recently, Deep Neural Networks (DNNs), especially Con-volutional Neural Networks (CNNs) achieve great success in a variety of domains, such as image classiﬁcation [16, 29,
*Equal contribution.
†Corresponding author (a) (b)
Figure 1: Comparison between real data and synthetic data (gener-ated by DSG and ZeroQ [2]) with 256 samples of each. (a) shows the activation distribution of one channel in ResNet-18 [13]. ZeroQ data mostly ﬁts the normal distribution of BN statistics (the red line), while real data and DSG data have an offset. (b) is a chart of the mean and standard deviation of one channel. ZeroQ data gathers near BN statistics (the red dot) but real data and DSG data are more scattered. 30, 33, 34, 37, 36, 32], object detection [9, 8, 21, 27, 17, 35], semantic segmentation [6, 42], etc. Nevertheless, deploying state-of-the-art models on resource-constrained devices is still challenging due to massive parameters and high com-putational complexity. With more and more hardware sup-port low-precision computations [41, 26, 25], quantization has emerged as one of the most promising approaches to obtain efﬁcient neural networks. Since the whole training stage is required, quantization-aware training methods are considered to be time-consuming and computationally in-tensive [10, 14, 24]. Therefore, post-training quantization methods are proposed, which directly quantize the FP32 models without retraining or ﬁne-tuning [1, 4, 40, 19, 18].
Nevertheless, they still require real training data to calibrate quantized models that is not often ready-to-use for privacy or security concerns, such as medical data and user data.
Fortunately, recent work have proposed data-free quan-tization to quantize models without any access to real data.
Existing data-free quantization methods [20, 2, 11, 3], such as ZeroQ [2], generate "optimal synthetic data", which learns 15658
an input data distribution to best match the Batch Normal-ization statistics of the FP32 model. However, models cal-ibrated with real data perform better than those calibrated with synthetic data, though the synthetic data matches BN statistics better. Our study reveals that the data generation process in typical data-free quantization methods has signiﬁ-cant homogenization issues at both distribution and sample levels, which prevent models from higher accuracy. First, since the synthetic data is constrained to match the BN statis-tics, the feature distribution in each layer might overﬁt the
BN statistics when data is fed forward in neural networks. As shown in Figure 1(a), the distribution of the synthetic sam-ples generated by ZeroQ almost ﬁts the normal distribution obeying the corresponding BN statistics, while those of real data have an obvious offset leading to more diverse distribu-tion. We call the phenomenon of the synthetic samples as the distribution level homogenization. Second, all samples of synthetic data are optimized by the same objective function in existing generative data-free quantization methods. For instance, ZeroQ and GDFQ [28] apply the same constraint to all data samples, which directly sums the loss objective (KL loss or statistic loss) of each layer. Therefore, as shown in Figure 1(b), the feature distribution statistics of these sam-ples are similar. Speciﬁcally, the distribution statistics of synthetic data generated by existing methods are centralized while those of real data are dispersed, as so-called sample level homogenization.
To mitigate the adverse effects caused by these issues, we propose a novel Diverse Sample Generation (DSG) scheme, a simple yet effective data generation method for data-free quantization to enhance the diversity of data. Our DSG scheme consists of two technical contributions: (1) Slack
Distribution Alignment (SDA): slack the alignment of the feature statistics in BN layers to relax the constraint of dis-tribution; (2) Layerwise Sample Enhancement (LSE): apply the layerwise enhancement to reinforce speciﬁc layers for different data samples.
Our DSG scheme presents a novel perspective of data diversity for data-free quantization. We evaluate the DSG scheme on the large-scale image classiﬁcation task, and the results show that DSG performs remarkably well across var-ious network architectures such as ResNet-18 [13], ResNet-50, SqueezeNext [7], ShufﬂeNet [39], and InceptionV3 [31], and surpasses previous methods by a wide margin, even out-performs models calibrated with real data. Moreover, we show that the synthetic data generated by DSG scheme can be applied to the most advanced post-training quantization methods, such as AdaRound [19].
We summarize our main contributions as: 1. We revisit the data generation process of data-free quanti-zation methods from the diversity perspective. Our study reveals the homogenization problems of synthetic data existing at two levels that harm the performance of the quantized models. 2. We propose Diverse Sample Generation (DSG) scheme, a novel sample generation method for accurate data-free quantization that enhances the diversity of data by com-bining two practical techniques: Slack Distribution Align-ment and Layerwise Sample Enhancement to solve the homogenization at distribution and sample levels. 3. We evaluate the proposed method on the large-scale im-age classiﬁcation task and consistently obtain signiﬁcant improvements over various base models and state-of-the-art (SOTA) post-training quantization methods. 2.