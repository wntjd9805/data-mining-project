Abstract
Most online multi-object trackers perform object detec-tion stand-alone in a neural net without any input from tracking.
In this paper, we present a new online joint detection and tracking model, TraDeS (TRAck to DEtect and Segment), exploiting tracking clues to assist detection end-to-end. TraDeS infers object tracking offset by a cost volume, which is used to propagate previous object fea-tures for improving current object detection and segmen-tation. Effectiveness and superiority of TraDeS are shown on 4 datasets, including MOT (2D tracking), nuScenes (3D tracking), MOTS and Youtube-VIS (instance segmentation tracking). Project page: https://jialianwu.com/ projects/TraDeS.html. 1.

Introduction
Advanced online multi-object tracking methods follow two major paradigms: tracking-by-detection [5, 38, 27, 52, 30, 49] and joint detection and tracking [26, 63, 1, 29, 45, 25, 43, 44]. The tracking-by-detection (TBD) paradigm treats detection and tracking as two independent tasks (Fig. 1 (a)).
It usually applies an off-the-shelf object detector to produce detections and employs another separate network for data association. The TBD system is inefﬁcient and not optimized end-to-end due to the two-stage processing. To address this problem, recent solutions favor a joint detection and tracking (JDT) paradigm that simultaneously performs detection and tracking in a single forward-pass (Fig. 1 (b)).
The JDT methods, however, are confronted with two is-sues: (i) Although in most JDT works [29, 45, 25, 50] the backbone network is shared, detection is usually performed standalone without exploring tracking cues. We argue that detection is the cornerstone for a stable and consistent track-let, and in turn tracking cues shall help detection, especially in tough scenarios like partial occlusion and motion blur. (ii)
As studied by [9] and our experiment (Tab. 1b), common re-ID tracking loss [45, 25, 32, 51] is not that compatible with detection loss in jointly training a single backbone network, which could even hurt detection performance to some extent.
Figure 1. Comparison of different online MOT pipelines. Our method follows the joint detection and tracking (JDT) paradigm.
Different from most JDT methods, the proposed TraDeS tracker deeply couples tracking and detection within an end-to-end and uni-ﬁed framework, where the motion clue from tracking is exploited to enhance detection or segmentation (omitted in the ﬁgure).
The reason is that re-ID focuses on intra-class variance, but detection aims to enlarge inter-class difference and minimize intra-class variance.
In this paper, we propose a new online joint detection and tracking model, coined as TraDeS (TRAck to DEtect and
Segment). In TraDeS, each point on the feature map repre-sents either an object center or a background region, similar as in CenterNet [64]. TraDeS addresses the above two is-sues by tightly incorporating tracking into detection as well as a dedicatedly designed re-ID learning scheme. Speciﬁ-cally, we propose a cost volume based association (CVA) module and a motion-guided feature warper (MFW) module, respectively. The CVA extracts point-wise re-ID embedding features by the backbone to construct a cost volume that stores matching similarities between the embedding pairs in two frames. Then, we infer the tracking offsets from the cost volume, which are the spatio-temporal displacements of all the points, i.e., potential object centers, in two frames. The tracking offsets together with the embeddings are utilized to conduct a simple two-round long-term data association. Af-terwards, the MFW takes the tracking offsets as motion cues to propagate object features from the previous frames to the current one. Finally, the propagated feature and the current feature are aggregated to derive detection and segmentation.
In the CVA module, the cost volume is employed to su-12352
pervise the re-ID embedding, where different object classes and background regions are implicitly taken into account.
This is being said, our re-ID objective involves the inter-class variance. This way not only learns an effective embedding as common re-ID loss [45, 25, 32, 51], but also is well com-patible with the detection loss and does not hurt detection performance as shown in Tab. 1b. Moreover, because the tracking offset is predicted based on appearance embedding similarities, it can match an object with very large motion or in low frame rate as shown in Fig. 3, or even accurately track objects in different datasets with unseen large motion as shown in Fig. 4. Thus, the predicted tracking offset of an object can serve as a robust motion clue to guide our feature propagation in the MFW module. The occluded and blurred objects in the current frame may be legible in early frames, so the propagated features from previous frames may support the current feature to recover potentially missed objects by our MFW module.
In summary, we propose a novel online multi-object tracker, TraDeS, that deeply integrates tracking cues to assist detection in an end-to-end framework and in return beneﬁts tracking as shown in Fig. 1 (c). TraDeS is a general tracker, which is readily extended to instance segmentation tracking by adding a simple instance segmentation branch. Exten-sive experiments are conducted on 4 datasets, i.e., MOT, nuScenes, MOTS, and Youtube-VIS datasets, across 3 tasks including 2D object tracking, 3D object tracking, and in-stance segmentation tracking. TraDeS achieves state-of-the-art performance with an efﬁcient inference time as shown in § 5.3. Additionally, thorough ablation studies are per-formed to demonstrate the effectiveness of our approach as shown in § 5.2. 2.