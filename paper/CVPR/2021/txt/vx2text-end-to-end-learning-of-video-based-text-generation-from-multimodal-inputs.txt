Abstract
We present VX2TEXT, a framework for text generation from multimodal inputs consisting of video plus text, speech, or audio. In order to leverage transformer networks, which have been shown to be effective at modeling language, each modality is ﬁrst converted into a set of language embed-dings by a learnable tokenizer. This allows our approach to perform multimodal fusion in the language space, thus eliminating the need for ad-hoc cross-modal fusion mod-ules. To address the non-differentiability of tokenization on continuous inputs (e.g., video or audio), we utilize a relaxation scheme that enables end-to-end training. Fur-thermore, unlike prior encoder-only models, our network includes an autoregressive decoder to generate open-ended text from the multimodal embeddings fused by the language encoder. This renders our approach fully generative and makes it directly applicable to different “video+x to text” problems without the need to design specialized network heads for each task. The proposed framework is not only conceptually simple but also remarkably effective: experi-ments demonstrate that our approach based on a single ar-chitecture outperforms the state-of-the-art on three video-based text-generation tasks—captioning, question answer-ing and audio-visual scene-aware dialog. 1.

Introduction
Among the fundamental goals of AI is the development of conversational multimodal systems that can reliably per-ceive the real-world and communicate with humans in nat-ural language. Progress in this area has been dramatically advanced in recent years by the introduction of large-scale benchmarks assessing the ability to interpret audiovisual in-formation and translate this understanding to natural lan-guage. Prime examples include datasets for image or video captioning [10, 38, 51, 24, 56, 28], question answering (QA) [5, 13, 54, 58, 19, 36, 46, 26], as well as audio-visual
In order to perform well on such bench-dialog [11, 1]. marks, the model must accomplish several goals: (1) ex-tract salient information from each individual modality, (2) effectively combine the different cues to address the given query, and (3) generate and present the results in human-comprehensible text.
In this paper, we present VX2TEXT, a simple video-based approach that embeds these three steps in a uniﬁed, end-to-end trainable framework. Objectives (1) and (2) are accomplished by utilizing modality-speciﬁc classiﬁers to convert the semantics from each input signal into a common semantic language space, which enables the application of powerful language models to directly interpret multimodal content. Speciﬁcally, our approach takes the textual labels of the top classes predicted by each classiﬁer pretrained on existing datasets [9, 14] and transforms them into word em-beddings, using a pretrained language model [12, 40]. The beneﬁt of this solution is that it opens up the possibility to carry out multimodal fusion by means of powerful language encoders such as T5 [40] without the need to design spe-cialized cross-modal network modules [33, 29, 57, 35] or to resort to pretext tasks to learn to combine the different input signals [44, 55, 29]. Not only is such a design much simpler but it also leads to better performance compared to prior approaches.
In order to fulﬁll objective (3), we employ a generative text decoder [40], which transforms the multimodal features computed by the encoder into text, thus realizing the goal of generating results in human-comprehensible language.
While prior multimodal works based on encoder-only ar-chitectures [44, 45, 33] are limited to operate in settings in-volving selection from a ﬁxed set of text candidates, our generative approach can be used for open-ended sentence generation as, e.g., required in dialog applications. In addi-tion, the use of a text decoder allows us to tackle different
“video+x to text” problems (e.g., answering and generat-ing questions, dialog, as well as captioning) with the same architecture, without having to design specialized network heads for each task.
We integrate these conceptually-distinct steps into a sin-gle architecture, which we train end-to-end. To achieve this, 7005
we adopt a differential tokenization on continuous modali-ties (e.g., audio or video) which renders the entire model— including the modality-speciﬁc classiﬁers—trainable with respect to the ﬁnal objective. Our experiments demonstrate that our uniﬁed framework trained end-to-end produces sig-niﬁcant performance gains over separately learned modules.
Our VX2TEXT based on a single architecture trained in a generative fashion without any multimodal pretext pretrain-ing outperforms the state-of-the-art on three different text-generation tasks—captioning, QA and dialog. 2.