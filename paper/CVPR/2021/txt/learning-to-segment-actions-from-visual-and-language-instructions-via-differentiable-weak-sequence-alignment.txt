Abstract
We address the problem of unsupervised localization of task-relevant actions (key-steps) and feature learning in instructional videos using both visual and language instructions. Our key observation is that the sequences of visual and linguistic key-steps are weakly aligned: there is an ordered one-to-one correspondence between most visual and language key-steps, while some key-steps in one modality are absent in the other.
To recover the two sequences, we develop an ordered prototype learning module, which extracts visual and linguistic prototypes representing key-steps. To ﬁnd weak alignment and perform feature learning, we develop a differentiable weak sequence alignment (DWSA) method that ﬁnds ordered one-to-one matching between sequences while allowing some items in a sequence to stay unmatched. We develop an efﬁcient for-ward and backward algorithm for computing the alignment and the loss derivative with respect to parameters of visual and language feature learning modules. By experiments on two instructional video datasets, we show that our method signiﬁcantly improves the state of the art. 1.

Introduction
Learning to perform procedural tasks by watching visual demonstrations or reading manuals is one of the complex capabilities of humans. Bringing this capability to machines allows us to design intelligent agents that autonomously learn to perform tasks or help humans/agents to achieve complex tasks and enables building massive instructional knowledge bases for education and autonomy. The explo-sion of data, on the other hand, has provided invaluable resources for automatic procedural task learning: there exist tens or hundreds of thousands of instructional videos on the web about how to cook different recipes, how to assemble or repair different devices, etc. [1, 49, 50, 32, 17, 45, 41, 25].
Given instructional videos of one or multiple tasks, the goal of procedure learning is to localize the key-steps (actions required to accomplish a task) in videos. Over
Figure 1: Key-steps in visual data and narrations for three videos from the task ‘change tire’. Each color represents one key-step or background. the past several years, we have seen great advances on different aspects of learning from instructions [42, 1, 40, 21, 30, 36, 12, 34, 11, 31]. The majority of works address learning from weakly annotated videos [22, 3, 37, 38, 9, 50, 27, 7, 28], i.e., videos with ground-truth lists/sequences of key-steps or ground-truth summaries [47]. On the other hand, understanding instructional videos at the scale necessary to build large knowledge bases or assistive agents that respond to many instructions, requires unsupervised learning that does not require costly video annotations. This has motivated several works on unsupervised procedure learning [42, 40, 17, 26, 19, 16, 51, 1, 18, 15], which mostly rely on learning from visual data alone.
Learning from Visual and Language Instructions.
In-structional videos are often accompanied with narrations, where visual demonstrations of many steps have language descriptions, see Figure 1.
Indeed, these two modalities contain rich information that can be leveraged to more effectively discover key-steps. However, there are multiple challenges that we need to address to leverage this shared information. First, while the majority of key-steps appear in both modalities, some may only appear in visual data or narrations. For example, ‘read manual’ appears in narration 1 but does not occur in the video, and ‘screw wheel’ occurs in video 1 while being absent in the narration. Second, the two modalities are not necessarily aligned: for a visual demonstration of a key-step, the associated narration could happen before, during or after performing it (e.g., one may review one or a few steps using language before or after demonstrating them). Third, visual data and narrations 10156
often contain substantial amount of background not related to the task, which do not necessarily occur at the same time1 (see grey temporal regions in Figure 1). While few works have addressed unsupervised learning from both modalities
[1, 42, 18], they rely on narration as the main modality or assume that visual and language descriptions have close temporal alignment.
This limits their applicability to general cases where visual data and narrations are unaligned or when some key-steps are missing in one modality.
Paper Contributions. We address task-relevant action (key-step) localization and multimodal feature learning in instructional videos using visual and language data.
Our key observation is that the ‘sequences’ of visual and linguistic key-steps are weakly aligned. More speciﬁcally, there is a one-to-one correspondence between most visual and language key-steps, while some key-steps in one modality are absent in the other. Moreover, the ordering of the common key-steps in two modalities are similar.
Thus, instead of assuming temporal alignment of key-steps in language and visual data, we assume weak alignment of key-step sequences once recovered, which allows for some steps to appear in only one modality and for the visual and language demonstration of a key-step to be temporally far.
To recover the sequences, we develop an ordered proto-type learning module, which extracts visual and linguistic prototypes representing key-steps. On the other hand, to ﬁnd weak alignment and perform feature learning, we develop a differentiable weak sequence alignment (DWSA) method that ﬁnds ordered one-to-one matching between sequences while allowing some items in a sequence to stay unmatched. We derive an efﬁcient dynamic programming-based algorithm for computing the loss and alignment as well as an efﬁcient backpropagation method for computing to parameters of visual and the gradient with respect language feature learning modules. By experiments on two instructional video datasets, we show that our method improves the state of the art by about 4.7% in F1 score. 2.