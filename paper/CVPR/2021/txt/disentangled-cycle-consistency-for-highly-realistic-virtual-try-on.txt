Abstract
Image virtual try-on replaces the clothes on a person
It is chal-image with a desired in-shop clothes image. lenging because the person and the in-shop clothes are un-paired. Existing methods formulate virtual try-on as either in-painting or cycle consistency. Both of these two formula-tions encourage the generation networks to reconstruct the input image in a self-supervised manner. However, exist-ing methods do not differentiate clothing and non-clothing regions. A straightforward generation impedes the virtual try-on quality because of the heavily coupled image con-tents.
In this paper, we propose a Disentangled Cycle-consistency Try-On Network (DCTON). The DCTON is able to produce highly-realistic try-on images by disentangling important components of virtual try-on including clothes warping, skin synthesis, and image composition. Moreover,
DCTON can be naturally trained in a self-supervised man-ner following cycle consistency learning. Extensive exper-iments on challenging benchmarks show that DCTON out-performs state-of-the-art approaches favorably. 1.

Introduction
Virtual try-on of fashion images aims at changing the clothes of a person with other in-shop clothes. There are wide applications including costume matching, fashion im-age editing, and clothes retrieval for e-commerce. Exist-ing methods mainly focus on a direct try-on based on 2D images because of the available person images and in-shop clothes images online. However, these images are unpaired since the collection of images with multiple models, of which each model wears different and pixel-wise aligned clothes is infeasible.
To handle unpaired images, existing methods such as VITON [15], CP-VTON [35], CP-VTON+ [24], and
*Y. Song is the corresponding author. This work is done when C. Ge is an intern in Tencent AI Lab. The code is available at https://github. com/ChongjianGE/DCTON.
Figure 1. Comparison of virtual try-on pipelines. The inpainting methods (e.g., CP-VTON [35] and ACGPN [40]) shown in the top row use one in-shop clothes to replace the same input clothes. The vanilla CycleGAN [18] shown in the middle row introduces two in-shop clothes for cycle consistency at the expense of generating coupled image contents (i.e., clothes, skin, and human poses). In the last row, we propose DCTON to disentangle virtual try-on as clothes warping and non-clothes generation, which is built upon vanilla cycle consistency for self-supervised learning.
ACGPN [40] formulate virtual try-on as an inpainting prob-lem. They ﬁrst mask the clothes region of a person im-age, and then recover the clothes region by using the same in-shop clothes for self-supervised network training. The pipeline is shown in the top row of Fig. 1. It is regarded as a one-way reconstruction from the corrupted input im-age to its original image. Since these methods only use one clothes during training (i.e., clothes 1 is matched to input 1), they are not effective when the person image and the target 16928
(a) Input (b) Target (c) ACGPN (d) CP-VTON (e) CA-GAN (f) DCTON
Figure 2. Virtual try-on comparisons. Inpainting based methods (ACGPN [40] and CP-VTON [35]) are not effective to establish an accurate correspondence in (c) and (d) when the target clothes are signiﬁcantly different from that in input images. Meanwhile, a heavily coupled content generation (CA-GAN [18]) brings salient artifacts as shown in (e). Different from existing methods, our DCTON disentangles virtual try-on as clothes warping, skin synthesis, and image composition in a cycle consistency training conﬁguration. The network is learned to produce highly-realistic try-on results as shown in (f). in-shop clothes are signiﬁcantly visually different. Exam-ples are shown in Fig. 2(c) and (d), where clothes with long sleeves will be changed to those with short sleeves. The arm region is not accurately generated as shown in the ﬁrst row. Meanwhile, there are large artifacts on the skirts in the second row. Besides these observations, these methods utilize separate modules for virtual try-on such as thin plate splines (TPS) [9] warping and semantic prediction. Their performance is limited due to a lack of end-to-end training for network potential exploitation.
Apart from the above inpainting-based methods, CA-GAN [18] incorporates cycle consistency for end-to-end network training. As shown in the middle row of Fig. 1,
CA-GAN substitutes the clothes of an input person image (i.e., input 1) with an arbitrary target in-shop image (i.e., clothes 2). This network design improves correspondence matching between the person image and arbitrary target clothes. Nevertheless, it is still challenging to simultane-ously generate the shape and the texture of clothes, the hu-man skin, and the non-clothing contents in a cycle gener-ative adversarial network (GAN). As shown in Fig. 2(e), artifacts appear around the arms and the logo region. This indicates a straightforward generation via cycle consistency training is insufﬁcient for high quality virtual try-on.
In this paper, we address aforementioned limitations by proposing a disentangled cycle-consistency try-on network (DCTON). It disentangles virtual try-on into three sub-modules. The ﬁrst one is clothes warping module that pre-serves clothes design (e.g., collar style, sleeve cutting, and logo). The second one is skin synthesis module for oc-cluded human body part generation (e.g., the arm of the blouse and vest in Fig. 2). The third one is image com-position module for output image generation. During train-ing, DCTON disentangles these three components from in-put images to constitute a try-on cycle for self-supervised learning. Extensive experiments on the benchmark datasets show that DCTON performs favorably against state-of-the-art virtual try-on approaches. 2.