Abstract
An essential prerequisite for unleashing the potential of supervised deep learning algorithms in the area of 3D scene understanding is the availability of large-scale and richly annotated datasets. However, publicly available datasets are either in relative small spatial scales or have limited semantic annotations due to the expensive cost of data ac-quisition and data annotation, which severely limits the de-velopment of ﬁne-grained semantic understanding in the context of 3D point clouds.
In this paper, we present an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is three times the number of labeled points than the existing largest photogrammetric point cloud dataset. Our dataset con-sists of large areas from three UK cities, covering about 7.6 km2 of the city landscape.
In the dataset, each 3D point is labeled as one of 13 semantic classes. We ex-tensively evaluate the performance of state-of-the-art algo-rithms on our dataset and provide a comprehensive anal-In particular, we identify several key ysis of the results. challenges towards urban-scale point cloud understanding.
The dataset is available at https://github.com/
QingyongHu/SensatUrban. 1.

Introduction
The three-dimensional world around us is composed of a rich variety of objects: buildings, trees, cars, and so forth, each with distinct appearance, morphology, and function.
Giving machines the ability to precisely segment and label these diverse objects is of key importance to allow them to interact competently within our physical world, for applica-tions such as object-level robotic grasping [39], scene-level robot navigation [54] and autonomous driving [16], or even large-scale urban 3D modeling, which is critical for the fu-ture of smart city planning and management [11, 4].
The ongoing revolution in data-driven deep networks has
*Corresponding author
Figure 1: An urban-scale point cloud collected from a re-gion on the perimeter of the city of York, UK. It covers a contiguous area of more than 3 square kilometer and repre-sents a typical urban suburb. led to a radical boost in the performance of 3D point cloud segmentation. A series of neural pipelines proposed to ad-dress the core problem of semantic segmentation, including: 1) 3D voxel-based methods such as SparseConvNet [19] and MinkowskiNet [10], 2) 2D projection-based approaches such as RangeNet++ [33] and SqueezeSeg [59], and 3) re-cent point-based architectures e.g. PointNet/PointNet++
[37, 38], KPConv [51] and RandLA-Net [23].
To a large degree, these techniques have been driven for-ward by the availability of open datasets which act as bench-marks for objective comparison of algorithms and their per-formance. These existing 3D repositories can be generally classiﬁed as 1) object-level 3D models such as ModelNet
[60] and ShapeNet [8], 2) indoor scene-level 3D scans, e.g.,
S3DIS [3], ScanNet [13], and SceneNN [70], and 3) outdoor roadway-level 3D point clouds including SemanticKITTI
[5] and Semantic3D [21].
However, there remain a number of key open questions as to whether these techniques are capable of learning ac-curate semantics over urban-scale 3D point clouds. Firstly, unlike the existing datasets for objects, rooms or roadways which are usually less than 200m in scale, the urban-scale datasets are expected to be collected by aerial platforms, spanning over extremely wide areas. How to efﬁciently 14977
and effectively preprocess massive points to feed into neural networks? Secondly, the real-world urban space is usually dominated by large-sized buildings or ground, and there-fore the urban-scale datasets demonstrate extreme class im-balance - a majority of points fall into a few categories with sparse, yet important classes being under represented.
How to overcome this data imbalance in neural networks?
Thirdly, with the advancement of aerial mapping systems, the urban-scale point clouds can not only capture the depth information, but also true color for the scene appearance. (How) does color information, in addition to depth, aid in semantic segmentation of urban areas? Lastly, and po-tentially most importantly, how are the existing networks trained on one urban area able to generalize to a novel area?
To this end, we aim to establish a new paradigm for urban-scale 3D semantic segmentation, enabled by UAV photogrammetry. Our dataset, called SensatUrban, repre-sents sub-sections of three large cities in the UK, i.e., Birm-ingham, Cambridge, and York.
It consists of nearly four billion 3D points covering more than 7.6 square kilome-ters urban area in these cities (as shown in Figure 1). The 3D point clouds are generated from high-quality aerial im-ages captured by a professional-grade UAV mapping sys-tem. Details of data acquisition are presented in Section 3. We manually labeled each point in the Birmingham and
Cambridge city as one of 13 semantic categories such as ground, vegetation, car, etc.. Compared with exiting 3D datasets, our SensatUrban is unique in two-fold.
• Unlike existing datasets for objects [60, 8], rooms [70, 3, 13] and roadways [21, 5] which are usually less than two hundred meters in scale, the SensatUrban point clouds continuously occupy kilometers in real-world urban ar-eas, opening up new opportunities towards urban-scale applications such as smart cities, and large national in-frastructure planning and management.
• Being reconstructed from high-resolution aerial images, our point clouds provide unique top-down and oblique perspectives for the entire landscape of cities. Inherently, the geometric patterns, textures, natural colours and dis-tributions are distinct from the existing datasets.
On the basis of SensatUrban, we further identify a num-ber of key challenges and empirically investigate them from various aspects in Section 5. In particular, we ﬁrstly study how the large-scale urban point clouds can be pre-processed, to adapt to existing approaches without losing segmentation accuracy. Secondly, we explore the neces-sity of colorful appearance for better semantic learning of several key categories, highlighting the advantage of pho-togrammetric point clouds over LiDAR-based point clouds.
Thirdly, we examine the imbalance of semantic categories in the urban-scale scenarios. Lastly, the difﬁculty of cross-city semantic learning is analysed. Note that, this paper does not aim to thoroughly tackle these challenges, but ex-pose them to the community for future research.
Overall, our primary contributions are: 1) a unique urban-scale 3D dataset for semantic learning, and 2) an in-depth study of generalizing existing algorithms to the large-scale urban point clouds and an outlook on future directions for 3D point cloud segmentation at massive scale and reso-lution. We aspire to highlight the challenges faced in the 3D semantic learning on large and dense point clouds of urban environments, sparking innovation in applications such as smart cities, digital twins, autonomous vehicles, automated asset management of large national infrastructures, and in-telligent construction sites. 2.