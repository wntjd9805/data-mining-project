Abstract
Multiple Object Tracking (MOT) has witnessed remark-able advances in recent years. However, existing studies dominantly request prior knowledge of the tracking tar-get (eg, pedestrians), and hence may not generalize well to unseen categories.
In contrast, Generic Multiple Ob-ject Tracking (GMOT), which requires little prior informa-tion about the target, is largely under-explored. In this pa-per, we make contributions to boost the study of GMOT in three aspects. First, we construct the ﬁrst publicly avail-able dense GMOT dataset, dubbed GMOT-40, which con-tains 40 carefully annotated sequences evenly distributed among 10 object categories. In addition, two tracking pro-tocols are adopted to evaluate different characteristics of tracking algorithms. Second, by noting the lack of de-voted tracking algorithms, we have designed a series of baseline GMOT algorithms. Third, we perform a thor-ough evaluations on GMOT-40, involving popular MOT al-gorithms (with necessary modiﬁcations) and the proposed baselines. The GMOT-40 benchmark is publicly available at https://github.com/Spritea/GMOT40. 1.

Introduction
Multiple Object Tracking (MOT) has long been stud-ied in the computer vision community [13, 39], due to its wide range of applications such as in robotics, surveil-lance, autonomous driving, cell tracking, etc. Remarkable advances have been made recently in MOT, partly due to the progress of major components such as detection, sin-gle object tracking, association, etc. Another driving force 1Equal contribution 2Corresponding author comes from the popularization of MOT benchmarks (e.g.,
[22, 33, 41, 52, 62]). Despite the achievement, previous studies in MOT mostly focus on a speciﬁc object category of interest (pedestrian, car, cell, etc.) and rely on models of such objects. For example, detectors of such objects are of-ten pre-trained ofﬂine, and motion patterns for speciﬁc ob-jects are sometimes utilized as well. It remains unclear how well existing MOT algorithms generalize to unseen objects and hence constrains the expansion of MOT to new applica-tions, especially those with limited data for training object detectors.
By contrast, Generic Multiple Object Tracking (GMOT), which requests no prior knowledge of the objects to be tracked, aims to deal with these issues. Hence GMOT could be applied in video editing, animal behaviour analysis, and vision based object counting. Despite its wide applications, it is however seriously under-explored, except for some early investigations [37, 38]. Comparing the progress in
GMOT with that in MOT, we see a clear lack of GMOT benchmark, and the absence of GMOT baselines with ef-fective deep learning ingredients. Note that we follow the deﬁnition of GMOT in [38], i.e., tracking multiple objects of a generic object class.
Addressing the above issues, in this paper, we contribute to the study of GMOT in three aspects: dataset, baseline, and evaluation. First, we construct the ﬁrst publicly avail-able dense GMOT dataset, dubbed GMOT-40, for system-atical study of GMOT. GMOT-40 contains 40 carefully se-lected sequences, which cover ten categories (e.g., insect and balloon) with four sequences per category. Each se-quence contains multiple objects of same category, and the average number of objects per frame is around 22.
All sequences are manually annotated with careful valida-tion/correction. The sequences involve many challenging factors such as heavy blur, occlusion, etc. A tracking Proto-6719
Input
Output (cid:150)(cid:3) (cid:3404)(cid:882) (cid:150)(cid:3) (cid:3404)(cid:882) (cid:150)(cid:3) (cid:3404)(cid:883) (cid:150)(cid:3) (cid:3404)(cid:17) (cid:150)(cid:3) (cid:3404)(cid:882) (cid:150)(cid:3) (cid:3404)(cid:883) (cid:150)(cid:3) (cid:3404)(cid:17)
Target 
Template (a)
Target Candidates 
Proposal (b)
Multiple Object 
Tracking (c)
Figure 1. One-shot generic multiple object tracking (GMOT). (a):
The input of one-shot generic MOT is a single bounding box to in-dicate a target template in the ﬁrst frame. (b): The target template is used to discover and propose all other target candidates of same category, which is different than model-based MOT where a pre-trained detector (typically class-speciﬁc) is required. (c): MOT then can be performed on the proposed candidates in either an online or ofﬂine manner. Yellow rectangles are zoomed-in local views of targets. col is adopted to evaluate different characteristics of track-ing algorithms. The one-shot GMOT [37, 38], takes as input the bounding box of one target object in the ﬁrst frame, and aims to detect and track all objects of the same category.
Figure 1 illustrates the one-shot GMOT Protocol.
Second, we design a series of baseline tracking algo-rithms dedicated to one-shot GMOT. These baselines con-sist of a one-shot detection stage and a target association stage. The one-shot detection stage is adapted from the recently proposed GlobalTrack algorithm [28]. The target association stage comes from several typical MOT algo-rithms. For each baseline, the one-shot detection algorithm plays the role of public detector.
Third, we conduct thorough evaluations on GMOT-40.
The evaluation involves both classic tracking algorithms (e.g., [8, 53, 54]) and recently proposed one (e.g., [12]), with necessary modiﬁcations. The results show that, as an important tracking problem, GMOT has a large room for improvement.
To summarize, we make three contributions in this paper:
• the ﬁrst publicly available dense GMOT dataset,
GMOT-40, which is carefully designed and annotated, along with evaluation Protocol,
• a series of GMOT baselines adapted from modern deep-learning enhanced MOT algorithm, and
• thorough evaluations and analysis on GMOT-40.
Table 1. Comparison of densely annotated data used in GMOT studies. # seq: number of sequence, # cat: number of categories,
# tgt: average number of targets per frame. ⋆: Estimated from samples in the paper.
Publication
Luo et al. [37]
Zhang et al. [59]
Luo et al. [38]
Zhu et al. [61]
Liu et al. [36]
GMOT-40
Year 2013 2014 2014 2017 2020 2021
# seq. 4 9 8 3 24 40
# cat. 4 9 8 1 9 10
# tgt.
≈15⋆
≈3⋆
≈15⋆ 13.13 3.375 26.58 2.