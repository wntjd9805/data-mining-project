Abstract
We present the ﬁrst method for real-time full body cap-ture that estimates shape and motion of body and hands to-gether with a dynamic 3D face model from a single color image. Our approach uses a new neural network architec-ture that exploits correlations between body and hands at high computational efﬁciency. Unlike previous works, our approach is jointly trained on multiple datasets focusing on hand, body or face separately, without requiring data where all the parts are annotated at the same time, which is much more difﬁcult to create at sufﬁcient variety. The possibility of such multi-dataset training enables superior generalization ability. In contrast to earlier monocular full body methods, our approach captures more expressive 3D face geometry and color by estimating the shape, expres-sion, albedo and illumination parameters of a statistical face model. Our method achieves competitive accuracy on public benchmarks, while being signiﬁcantly faster and pro-viding more complete face reconstructions. 1.

Introduction
Human motion capture from a single color image is an important and widely studied topic in computer vision.
Most solutions are unable to capture local motions of hands and faces together with full body motions. This renders them unsuitable for a variety of applications, e.g. AR, VR, or tele-presence, where capturing full human body pose and shape, including hands and face, is highly important.
In these applications, monocular approaches should ide-ally recover the full body pose (including facial expres-sion) as well as a render-ready dense surface which con-tains person-speciﬁc information, such as facial identity and body shape. Moreover, they should run at real-time fram-erates. Much progress has been made on relevant subtasks, i.e. body pose estimation [33, 31, 45, 40], hand pose estima-tion [78, 42, 80], and face capture [14, 61, 60, 53, 81]. How-*This work was supported by the National Key R&D Program of
China 2018YFA0704000, the NSFC (No.61822111, 61727808), Beijing
Natural Science Foundation (JQ19015), and the ERC Consolidator Grant 4DRepLy (770784). Feng Xu is the corresponding author.
Figure 1: We present the ﬁrst real-time monocular approach that jointly captures shape and pose of body and hands to-gether with facial geometry and color. Top: results on in-the-wild sequences. Bottom: real-time demo. Our approach predicts facial color while the body color is set manually. ever, joint full body capture, let alone in real-time, is still an open problem. Several recent works [9, 68, 28, 46, 38] have demonstrated promising results on capturing the full body. Nevertheless, they either only recover sparse 2D key-points [38, 28], require speciﬁc training data [9, 28] where body, hands, and face are annotated altogether which is expensive to collect, or cannot achieve real-time perfor-mance [9, 68, 46, 38].
We therefore introduce the ﬁrst real-time monocular ap-proach that estimates: 1) 2D and 3D keypoint positions of body and hands; 2) 3D joint angles and shape pa-rameters of body and hands; and 3) shape, expression, albedo, and illumination parameters of a 3D morphable face model [61, 14]. To recover the dense mesh, we use the SM-PLH model [49] for body and hands surface, and replace its face area with a more expressive face model.
To achieve real-time performance without the loss of ac-curacy, we rigorously design our new network architecture to exploit inter-part correlations by streaming body features into the hand pose estimation branch. Speciﬁcally, the sub-network for hand keypoint detection takes in two sources 4811
of features: one comes from the body keypoint detection branch as low-frequency global features, whereas the other is extracted from the hand area in the input image as high-frequency local features. This feature composition utilizes body information for hand keypoint detection, and saves the computation of extracting high-level features for the hands, resulting in reduced runtime and improved accuracy.
Further, we do not require a dataset where ground truth body, hands, and face reconstructions are all available at the same time: creating such data at sufﬁcient variety is very difﬁcult. Instead, we only require existing part-speciﬁc datasets. Our network features four task-speciﬁc modules that are trained individually with different types of data, while being end-to-end at inference. The ﬁrst module, Det-Net, takes a color image as input, estimates 3D body and hand keypoint coordinates, and detects the face location in the input image. The second and third module, namely
BodyIKNet and HandIKNet, take in body and hand keypoint positions and regress joint rotations along with shape pa-rameters. The last module, called FaceNet, takes in a face image and predicts the shape, expression, albedo, and il-lumination parameters of the 3DMM face model [61]. This modular network design enables us to jointly use the follow-ing data types: 1) images with only body or hand keypoint annotations; 2) images with body and hand keypoint anno-tations; 3) images annotated with body joint angles; 4) mo-tion capture (MoCap) data with only body or hand joint an-gles but without corresponding images; and 5) face images with 2D landmarks. To train with so many data modalities, we propose an attention mechanism to handle various data types in the same mini-batch during training, which guides the model to utilize the features selectively. We also in-troduce a 2-stage body keypoint detection structure to cope with the keypoint discrepancy between different datasets.
The above multi-modal training enables our superior gener-alization across different benchmarks.
Our contribution can be summarized as follows:
• The ﬁrst real-time approach that jointly captures 3D body, hands and face from a single color image.
• A novel network structure that combines local and global features and exploits inter-part correlations for hand keypoint detection, resulting in high computa-tional efﬁciency and improved accuracy.
• The utilization of various data modalities supported by decoupled modules, an attention mechanism, and a 2-stage body keypoint detection structure, resulting in superior generalization. 2.