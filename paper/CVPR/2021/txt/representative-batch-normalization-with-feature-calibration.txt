Abstract
Batch Normalization (BatchNorm) has become the de-fault component in modern neural networks to stabilize training. In BatchNorm, centering and scaling operations, along with mean and variance statistics, are utilized for feature standardization over the batch dimension. The batch dependency of BatchNorm enables stable training and better representation of the network, while inevitably ignores the representation differences among instances. We propose to add a simple yet effective feature calibration scheme into the centering and scaling operations of Batch-Norm, enhancing the instance-speciﬁc representations with the negligible computational cost. The centering calibra-tion strengthens informative features and reduces noisy fea-tures. The scaling calibration restricts the feature inten-sity to form a more stable feature distribution. Our pro-posed variant of BatchNorm, namely Representative Batch-Norm, can be plugged into existing methods to boost the performance of various tasks such as classiﬁcation, detec-tion, and segmentation. The source code is available in http://mmcheng.net/rbn. 1.

Introduction
Convolutional Neural Networks (CNNs) [19, 30, 52] have boosted the performance of various computer vision tasks [10, 18, 29, 48] with its powerful representation abil-ity. While with the growth of structural complexity and model parameters, CNNs are facing more training difﬁcul-ties. Batch normalization (BatchNorm) [24] eases the train-ing difﬁculty by constraining intermediate features within the normalized distribution with mini-batch statistical in-In BatchNorm, the reliance on mini-batch in-formation. formation is built on the assumption that features gener-ated from different instances ﬁt into the same distribution within a channel [24, 65]. However, this assumption can not always hold on two cases [7, 32, 54, 68]: i) the possible inconsistency between the mini-batch statistics in training
*M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author.
Training Loss
Testing Error
BN
RBN
BN
RBN
Epochs
Epochs
Figure 1. Image classiﬁcation results on the ImageNet dataset.
MobileNet v2 [50] equipped with Representative BatchNorm achieves smaller training loss and testing error than using Batch-Norm. Representative BatchNorm enhances the instance-speciﬁc representations and maintains the beneﬁt of the BatchNorm. and the running statistics in testing; ii) the instances in the testing set may not always fall into the distribution of the training set. To avoid the side effects introduced by these two kinds of inconsistencies, some works [2, 59, 63] utilize instance-speciﬁc statistics instead of mini-batch statistics to normalize intermediate features. However, due to the lack of batch information, the training instability makes their performance inferior to BatchNorm in many cases [37, 55].
Other works utilize mini-batch and instance statistics by combining multiple normalization techniques [36,37,51] or introducing attention mechanisms [25,31,32,39]. However, these methods usually introduce more overheads, making them not friendly in practical usage. A question has raised, can we maintain the mini-batch beneﬁts of BatchNorm and enhance the instance-speciﬁc representations with some mi-nor adjustments? To answer this question, we propose a simple yet effective feature calibration scheme to calibrate the feature standardization operation of BatchNorm with a negligible cost.
BatchNorm is composed of the feature standardization and afﬁne transformation operation. In this paper, we fo-cus on the standardization operation composed of feature centering and scaling operations. During training, based on mini-batch statistics, the centering operation ensures fea-tures to have the zero-mean property, and the scaling opera-tion makes features to have unit-variance. The zero-mean 8669
and unit-variance property of features cannot always be maintained during testing due to the statistics inconsistency and the instance inconsistency. Centering with inappropri-ate running mean values makes centered features contain extra noises or lose some informative representations after activation. When the mean value of testing instance fea-tures is below the running mean value, as shown in Fig. 2(a), some representative features are mistakenly removed by the activation. In contrast, as shown in Fig. 2(b), noises with small value should be ﬁltered out by the activation are kept when the mean value of features is greater than the running mean value. Also, inappropriate running variance causes the scaling operation to produce scaled features with too small or too large intensity, as shown in Fig. 2(c) and (d), resulting in unstable feature distribution among channels.
For example, suppose a scaled feature from one channel is much larger than the other channels in the same layer dur-ing testing. The feature in this channel would dominate the features produced by the next convolutional layer.
To reduce the side effect introduced by some inappro-priate running statistics while maintaining the beneﬁts of
BatchNorm, we utilize instance-speciﬁc statistics to cali-brate the centering and scaling operations with a negligi-ble cost. We propose the centering calibration to strengthen representative features and reduce noisy features by mov-ing features with instance-speciﬁc statistics. The scaling calibration accordingly scales the intensity of features with statistics of instances to produce a more stable feature distri-bution. These two calibrations only introduce three weights in each channel and require a negligible computational cost.
We propose the Representative Batch Normalization (RBN) by adding the centering and scaling calibrations to the BatchNorm, to make the intermediate features nor-malized by BatchNorm to be more representative. Fig. 1 shows the model equipped with Representative BatchNorm achieves smaller training loss and testing error than using
BatchNorm. We show that Representative BatchNorm can replace the BatchNorm in existing methods to boost the per-formance of various tasks such as classiﬁcation, detection, and segmentation with the negligible cost and parameters. 2.