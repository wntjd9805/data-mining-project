Abstract
Support Mask
Prototype learning is extensively used for few-shot seg-mentation. Typically, a single prototype is obtained from the support feature by averaging the global object information.
However, using one prototype to represent all the informa-tion may lead to ambiguities. In this paper, we propose two novel modules, named superpixel-guided clustering (SGC) and guided prototype allocation (GPA), for multiple pro-totype extraction and allocation. Speciﬁcally, SGC is a parameter-free and training-free approach, which extracts more representative prototypes by aggregating similar fea-ture vectors, while GPA is able to select matched prototypes to provide more accurate guidance. By integrating the SGC and GPA together, we propose the Adaptive Superpixel-guided Network (ASGNet), which is a lightweight model and adapts to object scale and shape variation. In addition, our network can easily generalize to k-shot segmentation with substantial improvement and no additional computa-tional cost. In particular, our evaluations on COCO demon-strate that ASGNet surpasses the state-of-the-art method by 5% in 5-shot segmentation.1 1.

Introduction
Humans have a remarkable ability to learn how to recog-nize novel objects after seeing only a handful of exemplars.
On the other hand, deep learning based computer vision sys-tems have made tremendous progress, but have largely de-pended on large-scale training sets. Also, deep networks mostly work with predeﬁned classes and are incapable of generalizing to new ones. The ﬁeld of few-shot learning studies the development of such learning ability in artiﬁcial learning systems, where only a few examples of the new category are available.
In this work, we tackle the few-shot segmentation prob-lem, where the target is learning to segment objects in a given query image while only a few support images with ground-truth segmentation masks are available. This is a
*Corresponding author 1Code is available at https://git.io/ASGNet.
Pool
Expand (a) Single prototype learning
Support Mask
Cluster
…
Allocate
Support
Query
Support
Query (b) Adaptive prototype learning and allocation
Figure 1. Comparison between (a) single prototype learning and (b) proposed adaptive prototype learning and allocation. We uti-lize superpixel-guided clustering to generate multiple prototypes and then allocate them pixel-wise to query feature. challenging problem as the test data are novel categories which do not exist in the training set, and there are usually large variations in appearance and shape between the sup-port and query images.
Current few-shot segmentation networks usually extract features from both query and support images, and then pro-pose different approaches for feature matching and object mask transfer from support to query image. This feature matching and mask transfer are usually performed in one of two ways: prototypical feature learning or afﬁnity learn-ing. Prototypical learning techniques condense the masked object features in a support image into a single or few proto-typical feature vectors. Then these techniques ﬁnd the pixel locations of similar features in the query image to segment the desired object. A key advantage of prototype learning is that the prototypical features are more robust to noise than pixel features. However, prototypical features inevitably drop spatial information, which is important when there is a large variation in the object appearance between the support and query images. In addition, most prototypical learning 8334
networks [40, 42, 32, 29, 4] merely generate a single pro-totype by masked average pooling as shown in Figure 1(a), thus losing information as well as discriminability.
Afﬁnity learning techniques [39, 36, 31] on the other hand, directly try to match object pixels in a support image to query image pixels thereby transferring the object mask.
These techniques predict cross-image pixel afﬁnities (also called connection strengths) using learned features, which perform feature matching while preserving spatial informa-tion better than prototypical learning approaches. How-ever, afﬁnity learning techniques are prone to over-ﬁtting on training data as they try to solve an under-constrained pixel-matching problem with dense afﬁnity matrices.
In this work, we propose a novel prototypical learning technique that addresses some of the main shortcomings of existing ones. In particular, we want to adaptively change the number of prototypes and their spatial extent based on the image content, making the prototypes content-adaptive and spatially-aware. This adaptive, multi-prototype strategy is important to deal with large variations in object scales and shapes across different images. Intuitively, when an ob-ject occupies a large portion of the image, it carries more information and thus requires more prototypes to represent all the necessary information. On the contrary, if the ob-ject is fairly small and the proportion of the background is large, then a single or few prototypes are sufﬁcient. In ad-dition, we want the support region (spatial extent) for each of the prototypes to be adaptive to object information that is present in the support image. Concretely, we aim to di-vide the support feature into several representative areas ac-cording to the feature similarity. We also want to adaptively choose more important prototypes while ﬁnding similar fea-tures in a query image. As different object parts are visible in different image regions and in different query images, we want to dynamically allocate different prototypes across query image for feature matching. For example, some parts of the object can be occluded in a query image and we want to dynamically choose the prototypes that are correspond-ing to the visible parts in the query image.
We achieve this adaptive, multi-prototype learning and allocation with our Adaptive Superpixel-guided Network leverages superpixels for adapting both (ASGNet) that the number and support regions of the prototypes. The schematic illustration is presented in Figure 1(b). In particu-lar, we propose two modules, Superpixel-guided Clustering (SGC) and Guided Prototype Allocation (GPA), which form the core of ASGNet. The SGC module does fast feature-based superpixel extraction on the support image and the resulting superpixel centroids are considered as prototypical features. Since superpixel shapes and numbers are adaptive to the image content, the resulting prototypes also become adaptive. The GPA module uses an attention-like mecha-nism to allocate most relevant support prototype features to each pixel in a query image. In summary, the SGC module provides adaptive prototype learning both in terms of the number of prototypes and their spatial extents, and the GPA module provides adaptive allocation of the learned proto-types when processing query features. These two modules make ASGNet highly ﬂexible and adaptive to varying object shapes and sizes, allowing it to generalize better to unseen object categories. We make the following contributions:
• We propose the Adaptive Superpixel-guided Network (ASGNet), a ﬂexible prototypical learning approach for few-shot segmentation that is adaptive to different object scales, shapes and occlusions.
• We introduce two novel modules, namely Superpixel-guided Clustering (SGC) and Guided prototype alloca-tion (GPA), for adaptive prototype extraction and allo-cation respectively. They can serve as effective plug-and-play components on feature matching.
• ASGNet achieves top-performing results with fewer parameters and less computation. Speciﬁcally, the pro-posed method obtains mIoUs of 64.36%/42.48% in the 5-shot setting on Pascal-5i/COCO-20i, exceeding the state-of-the-art by 2.40%/5.08%. 2.