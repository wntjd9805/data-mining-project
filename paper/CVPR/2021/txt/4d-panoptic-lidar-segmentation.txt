Abstract
Temporal semantic scene understanding is critical for self-driving cars or robots operating in dynamic environ-ments. In this paper, we propose 4D panoptic LiDAR seg-mentation to assign a semantic class and a temporally-consistent instance ID to a sequence of 3D points. To this end, we present an approach and a point-centric evalua-tion metric. Our approach determines a semantic class for every point while modeling object instances as proba-bility distributions in the 4D spatio-temporal domain. We process multiple point clouds in parallel and resolve point-to-instance associations, effectively alleviating the need for explicit temporal data association. Inspired by recent ad-vances in benchmarking of multi-object tracking, we pro-pose to adopt a new evaluation metric that separates the semantic and point-to-instance association aspects of the task. With this work, we aim at paving the road for future developments of temporal LiDAR panoptic perception. 1.

Introduction
Spatio-temporal interpretation of raw sensory data is im-portant for autonomous vehicles to understand how to inter-act with the environment and perceive how trajectories of moving agents evolve in 3D space and time.
* Authors contributed equally.
In the past, different aspects of dynamic scene under-standing such as semantic segmentation [22, 18, 48, 72, 85, 69], object detection [23, 62, 40, 65, 64, 66], instance seg-mentation [28], and multi-object tracking [43, 8, 53, 10, 76, 61, 58] have been tackled independently. The devel-opments in these ﬁelds were largely fueled by the rapid progress in deep learning-based image [37] and point-set representation learning [59, 60, 72], together with contribu-tions of large-scale datasets, benchmarks, and uniﬁed eval-uation metrics [44, 22, 25, 19, 17, 74, 26, 5, 18, 13, 68].
In the pursuit of image-based holistic scene understanding, recent community efforts have been moving towards con-vergence of tasks, such as multi-object tracking (MOT) and segmentation [74, 83], and semantic and instance segmen-tation, i.e., panoptic segmentation [35]. Recently, panop-tic segmentation was extended to the video domain [34].
Here, the dataset, task formalization, and evaluation met-rics focused on interpreting short and sparsely labeled video snippets in 3D (2D image+time) in an ofﬂine setting. Au-tonomous vehicles, however, need to continuously interpret sensory data and localize objects in a 4D continuum.
Tackling sequence-level LiDAR panoptic segmentation is a challenging problem, since state-of-the-art meth-ods [72] usually need to downsample even single-scan point clouds to satisfy the memory constraints. Therefore, the common approach in (3D) multi-object tracking is detect-5527
ing objects in individual scans, followed by temporal as-sociation [24, 76, 77], often guided by a hand-crafted mo-tion model. In this paper, we take a substantially different approach, inspired by the uniﬁed space-time treatment phi-losophy. We form overlapping 4D volumes of scans (see
Fig. 1) and, in parallel, assign to 4D points a semantic in-terpretation while grouping object instances jointly in 4D space-time.
Importantly, these 4D volumes can be processed in a sin-gle network pass, and the temporal association is resolved implicitly via clustering. This way, we retain inference efﬁ-ciency while resolving long-term association between over-lapping volumes based on the point overlap, alleviating the need for explicit data association.
For the evaluation, we introduce a point-centric higher-order tracking metric, inspired by recent metrics for multi-object tracking [45] and concurrent work on video panop-tic segmentation [75] which differ from the available met-rics [35, 9] that overemphasize the recognition part of the tasks. Our metric consist of two intuitive terms, one mea-suring the semantic aspect and second the spatio-temporal association of the task. Together with the recently proposed
SemanticKITTI [5, 6] dataset, this gives us a test bed to an-alyze our method and compare it with existing LiDAR se-mantic/instance segmentation [40, 72, 76, 47] approaches, adapted to the sequence-level domain.
In summary, our contributions are: (i) we propose a uni-ﬁed space-time perspective to the task of 4D LiDAR panop-tic segmentation, and pose detection/segmentation/tracking jointly as point clustering which can effectively leverage the sequential nature of the data and process several LiDAR scans while maintaining memory efﬁciency; (ii) we adopt a point-centric evaluation protocol that fairly weights seman-tic and association aspects of this task and summarizes the
ﬁnal performance with a single number; (iii) we establish a test bed for this task, which we use to thoroughly analyze our model’s performance and the existing LiDAR panoptic segmentation methods used in conjunction with a tracking-by-detection mechanism. Our code, experimental data1 and benchmark2 are publicly available. 2.