Abstract
Image super-resolution, which is often regarded as a pre-processing procedure of scene text recognition, aims to re-cover the realistic features from a low-resolution text im-age. It has always been challenging due to large variations in text shapes, fonts, backgrounds, etc. However, most exist-ing methods employ generic super-resolution frameworks to handle scene text images while ignoring text-speciﬁc prop-erties such as text-level layouts and character-level details.
In this paper, we establish a text-focused super-resolution framework, called Scene Text Telescope (STT). In terms of text-level layouts, we propose a Transformer-Based Super-Resolution Network (TBSRN) containing a Self-Attention
Module to extract sequential information, which is robust to tackle the texts in arbitrary orientations.
In terms of character-level details, we propose a Position-Aware Mod-ule and a Content-Aware Module to highlight the position and the content of each character. By observing that some characters look indistinguishable in low-resolution condi-tions, we use a weighted cross-entropy loss to tackle this problem. We conduct extensive experiments, including text recognition with pre-trained recognizers and image qual-ity evaluation, on TextZoom and several scene text recog-nition benchmarks to assess the super-resolution images.
The experimental results show that our STT can indeed gen-erate text-focused super-resolution images and outperform the existing methods in terms of recognition accuracy. 1.

Introduction
Scene text recognition (STR) has drawn much research interest of the computer vision community due to its various applications such as license plate recognition and ID card recognition [17, 23, 39]. While STR has made a big step forward with the development of deep learning, recognition performance on low-resolution (LR) text images is still sub-par [44]. LR text images exist in many situations, e.g., a
*Corresponding author
Interpolation
Generic SR method
LR
The proposed STT 
HR (ground truth)
Figure 1. The proposed STT generates a relatively clearer text im-age and pays more attention to character details compared with interpolation methods and generic SR methods. “SR” and “HR” denote super-resolution and high-resolution, respectively. photo taken with a low-focal camera or a document image compressed to reduce disk usages. When handling LR text images, existing recognition or spotting methods usually employ interpolation methods, like bicubic and bilinear in-terpolations, to upsample the original images [4, 27, 37, 38].
As shown in Figure 1, the image upsampled by the interpo-lation method is still blurred, which indeed brings difﬁcul-ties to existing recognition models.
In recent years, several works employ generic super-resolution methods for text image super-resolution. For ex-ample, in [8], SRCNN [7] with a shallow network is used as the backbone. In [42], a Laplacian-pyramid backbone is employed to combine features from several middle layers to upsample low-resolution images. However, these methods are not suitable for processing text images [44] since they see text images as general ones without taking text-speciﬁc properties (e.g. text-level layouts and character-level de-tails) into consideration. In contrast, there are few methods that take a part of these properties into account. For exam-ple, PlugNet [30] designs a multi-task framework, aiming to recognize and upsample text images in one model.
In
[44], a Text Super-Resolution Network (TSRN) containing a horizontal and a vertical BLSTMs [11] is proposed to cap-ture sequential information of text images. However, the
BLSTMs are not suitable for capturing sequential informa-tion of inclined or curved text images. 12026
In this paper, we propose a text-focused super-resolution framework, called Scene Text Telescope.
To tackle texts in arbitrary orientations, we propose a novel back-bone, namely Transformer-Based Super-Resolution Net-work (TBSRN) to capture sequential information. We no-tice that the previous methods usually employ loss functions that focus on every pixel of the image, which may suffer great disturbances from backgrounds. According to Inat-tentional Blindness [28, 29], when humans observe a text image, they will naturally pay more attention to text regions rather than backgrounds, i.e., there is no need to improve the quality of the whole image in the super-resolution task.
Based on this fact, we put forward a Position-Aware Mod-ule and a Content-Aware Module to focus on the position and the content of each character. By observing that there are some confusable characters in the low-resolution situa-tion (e.g. in Figure 1, “c” and “e” look similar), we employ a weighted cross-entropy loss in the Content-Aware Module to address this problem. Since these two modules are only used as text-speciﬁc guidance when training, they will not bring additional time overhead in the test stage.
We mainly evaluate our method on TextZoom [44], which contains LR-HR pairs captured from digital cameras.
Furthermore, we conduct several experiments on scene text recognition benchmarks to further verify the capabilities
In this work, we employ of our STT as a preprocessor. some widely used recognition models (e.g. ASTER [38],
MORAN [26], and CRNN [37]) and image quality met-rics, to evaluate the generated SR images. The experimen-tal results show that the proposed STT can indeed generate text-focused super-resolution images and outperform exist-ing methods in terms of recognition accuracy. Contributions of the proposed STT can be concluded in three-fold:
• We propose TBSRN to capture sequential information, which is more robust on texts in arbitrary orientations.
• A Position-Aware Module and a Content-Aware Mod-ule with a weighted cross-entropy loss are proposed to highlight the position and content of characters with-out bringing additional time overhead when testing.
• The proposed STT generates text-focused SR im-ages and achieves higher recognition accuracy on pre-trained recognizers than other existing methods. 2.