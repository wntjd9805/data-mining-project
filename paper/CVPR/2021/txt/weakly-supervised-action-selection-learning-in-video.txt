Abstract
Localizing actions in video is a core task in computer vi-sion. The weakly supervised temporal localization problem investigates whether this task can be adequately solved with only video-level labels, signiﬁcantly reducing the amount of expensive and error-prone annotation that is required.
A common approach is to train a frame-level classiﬁer where frames with the highest class probability are se-lected to make a video-level prediction. Frame-level acti-vations are then used for localization. However, the ab-sence of frame-level annotations cause the classiﬁer to im-part class bias on every frame. To address this, we pro-pose the Action Selection Learning (ASL) approach to cap-ture the general concept of action, a property we refer to as “actionness”. Under ASL, the model is trained with a novel class-agnostic task to predict which frames will be selected by the classiﬁer. Empirically, we show that
ASL outperforms leading baselines on two popular bench-marks THUMOS-14 and ActivityNet-1.2, with 10.3% and 5.7% relative improvement respectively. We further ana-lyze the properties of ASL and demonstrate the importance of actionness. Full code for this work is available here: https://github.com/layer6ai-labs/ASL. 1.

Introduction
Temporal action localization is a fundamental task in computer vision with important applications in video un-derstanding and modelling. The weakly supervised lo-calization problem investigates whether this task can be adequately solved with only video-level labels instead of frame-level annotations. This signiﬁcantly reduces the ex-pensive and error-prone labelling required in the fully su-pervised setting [39, 29], but considerably increases the difﬁculty of the problem. A standard approach is to ap-ply multiple instance learning to learn a classiﬁer over in-stances, where an instance is typically a frame or a short segment [23, 26, 14]. The classiﬁer is trained using the top-*Authors contributed equally to this work. (a) Context error (b) Actionness error
Figure 1: (a) Context error for the “Cricket Shot” action due to the presence of all cricket artifacts but absence of action. (b) Actionness error for the “Cricket Bowling” action due to the atypical scene despite the presence of action. k aggregation over the instance class activation sequence to generate video probabilities. Localization is then done by leveraging the class activation sequence to generate start and end predictions. However, in many cases, the instances that are selected in the top-k contain useful information for prediction but not the actual action. Furthermore, with top-k selection the classiﬁcation loss encourages the classiﬁer to ignore action instances that are difﬁcult to classify. Both of these problems can signiﬁcantly hamper localization ac-curacy and stem from the general inability of the classi-ﬁer to capture the intrinsic property of action in instances.
This property is known as “actionness” in the existing liter-ature [7, 21].
Ignoring actionness can lead to two major types of error: context error and actionness error. Context error occurs when the classiﬁer activates on instances that do not contain actions but contain context indicative of the overall video class [19, 14]. Figure 1 (a) shows an example of context er-ror. Here, cricket players are inspecting a cricket pitch. The instance clearly indicates that the video is about cricket and the classiﬁer predicts “Cricket Shot” with high conﬁdence.
However, no shot happens in this particular instance and in-cluding it in the localization for “Cricket Shot” would lead to an error. Actionness error occurs when the classiﬁer fails to activate on instances that contain actions. This generally occurs in difﬁcult instances that have signiﬁcant occlusion or uncommon settings. An example of this is shown in Fig-ure 1 (b). The action is “Cricket Bowling”, but the classiﬁer 7587
fails to activate as the scene is indoors and differs from the usual cricket setting.
Leading recent work [25, 14, 24] in this area propose an attention model to ﬁlter out background and then train a classiﬁer on the ﬁltered instances to predict class probabil-ities. This has the drawback of making it more challenging for the classiﬁer to learn as important context is potentially removed as background.
Our motivation is to design a learning framework that can use the context information for class prediction and at the same time learn to identify action instances for lo-calization. We have seen from the supervised setting that leading object detection [9, 27, 4] and temporal localiza-tion [17, 18, 16] methods leverage class-agnostic proposal networks to generate highly accurate predictions. This demonstrates that a general objectness/actionness property can be successfully learned across a diverse set of classes.
To this end, we propose a new approach called Action
Selection Learning (ASL) where the class-agnostic action-ness model learns to predict which frames will be selected in the top-k sets by the classiﬁer. During inference, we combine predictions from the actionness model with class activation sequence and show considerable improvement in localization accuracy. Speciﬁcally, ASL achieves new state-of-the-art on two popular benchmarks THUMOS-14 and
ActivityNet-1.2, where we improve over leading baselines by 10.3% and 5.7% in mAP respectively. We further an-alyze the performance of our model and demonstrate the advantages of the actionness approach. 2.