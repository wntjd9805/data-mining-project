Abstract
Continual learning tackles the setting of learning differ-ent tasks sequentially. Despite the lots of previous solutions, most of them still suffer signiﬁcant forgetting or expensive memory cost. In this work, targeted at these problems, we
ﬁrst study the continual learning process through the lens of information theory and observe that forgetting of a model stems from the loss of information gain on its parameters from the previous tasks when learning a new task. From this viewpoint, we then propose a novel continual learning approach called Bit-Level Information Preserving (BLIP) that preserves the information gain on model parameters through updating the parameters at the bit level, which can be conveniently implemented with parameter quantization.
More speciﬁcally, BLIP ﬁrst trains a neural network with weight quantization on the new incoming task and then es-timates information gain on each parameter provided by the task data to determine the bits to be frozen to prevent forgetting. We conduct extensive experiments ranging from classiﬁcation tasks to reinforcement learning tasks, and the results show that our method produces better or on par re-sults comparing to previous state-of-the-arts. Indeed, BLIP achieves close to zero forgetting while only requiring con-stant memory overheads throughout continual learning1. 1.

Introduction
Continual learning tackles the setting where an agent
It is challenging since learns different tasks sequentially. the agent is usually not allowed to refer to the previously learned tasks when learning a new one. Current artiﬁcial neural networks generally fail as they tend to suffer severe performance degradation on previously learned tasks after learning new ones, which is known as catastrophic forget-ting [23, 11]. A commonly acknowledged reason for this problem is that model parameters drift when ﬁtting the new incoming task data.
Two types of methods have been developed to address 1Code: https://github.com/Yujun-Shi/BLIP this hazard. The ﬁrst type of methods, which are based on model pruning technique [10, 29, 22, 21], ﬁrst iden-tify model parameters important for a task and then store a task-speciﬁc mask to mark these parameters and prohibit them from changing in subsequent learning to prevent for-getting. During the evaluation stage, only the parameters marked by the task-speciﬁc mask are applied on the given task. While these methods usually enjoy relatively low for-getting rate, they suffer from the drawback of linearly grow-ing memory overheads resulted from task-speciﬁc masks as continual learning proceeds. The other type, known as
“regularization-based methods” [14, 12, 36, 26, 16], impose model regularization terms when training on the new task to prevent model parameters from deviating away from previ-ously learned ones and thus alleviate forgetting issue. They do not involve any task-speciﬁc masks but usually suffer more severe forgetting comparing to pruning-based ones.
The hard trade-off between forgetting prevention and mem-ory efﬁciency for these two types of methods is because they only consider preventing forgetting on the parameter-level.
In this paper, we dive into the ﬁner granularity of bit-level instead of parameter-level to investigate and address the forgetting problem in continual learning. To start with, we study continual learning from information theory per-spective, which is developed from the Bayesian inference framework that interprets supervised learning as an infer-ence process over the model parameters from the training data. For a continual learning (inference) process, where data seen by model accumulate as the model experiences more tasks, the inference on each model parameter should become increasingly certain. To quantify this expected in-crease of certainty on model parameters given the streaming data, we exploit information gain, which corresponds to the reduction of entropy on parameters estimated after experi-encing new task data. In this way, we consider continual learning as a recursive process of gaining information on model parameters, driven by the sequentially incoming task data. From this viewpoint, forgetting can be understood as loss of information provided by previous task data on model parameters. 16674
Figure 1. Workﬂow of BLIP. Best viewed in color. We consider a simple scenario with one single parameter θ quantized to 10 bits to illustrate our method. θt denotes the parameter after learning on task 1 to t, and θ0 is a randomly initialized value before training on any task. IGt denotes information gain on θ after learning the task t. Bit representation of θ after learning each task is shown below. From the higher bit positions to lower ones is more signiﬁcant bits to less signiﬁcant ones. Frozen bits are ﬁlled with color and the rest bits are free bits. After learning each task, the information gain is calculated and then ⌈IGt⌉ bits are to be frozen in the bit representation. By repeating this process, the information on previous tasks can be preserved, enabling continual learning for neural networks.
To more intuitively study information gain in continual learning context, we quantize model parameters (e.g. to 20 bits) and view them in their bit representation, where values are represented by series of bits. Initially, before learning on any tasks, a model parameter is free to update, which corresponds to all bits of the parameter being free-to-ﬂip.
Shannon entropy, which is equivalent to the number of free-to-ﬂip bits, is 20 at this point. Next, after training on the
ﬁrst incoming task, information gain brought by the ﬁrst task data, which is the reduction of Shannon entropy after learning the ﬁrst task, thus corresponds to how many bits are now becoming certain.2 Leaving these bits to continue to ﬂip in the subsequent learning process means discarding the information provided by this task, and forgetting thus happens. This motivates us to freeze these certain bits and prohibiting them from ﬂipping in subsequent task iterations to preserve information gain provided by the ﬁrst task. By applying the same information gain-guided bit freezing af-ter learning each subsequent task, information provided by each task can be preserved and forgetting can thus be pre-vented in continual learning.
We accordingly develop a Bit-Level Information Pre-serving (BLIP) method to tackle the forgetting issue in con-tinual learning. Given an incoming task, BLIP ﬁrst trains a weight quantized neural network. Then, BLIP estimates the information gain on the parameters brought by this task’s data, which suggests how many bits to freeze. The frozen bits are not allowed to ﬂip in subsequent learning, preserv-ing the information provided by this task. This process is applied recursively with each new task to enable continual 2As will be explained in Sec. 4.4, bits becoming certain from more signiﬁcant bits (with higher bit positions) to less signiﬁcant ones (with lower bit positions). learning without forgetting. We provide a simple overview of its workﬂow in Fig. 1.
Unlike previous pruning-based methods, the memory overheads of our method is constant as BLIP only keeps track of how many bits to freeze in total for each parame-ter, without requiring any task-speciﬁc masks. On the other hand, our method is a more precise and stronger way of reg-ularizing parameters compared to previous regularization-based methods, which effectively prevents forgetting. We validate our method across diverse settings of classiﬁcation tasks and reinforcement learning tasks, and well prove that it performs on par with or better than previous state-of-the-arts.
This work makes following contributions: 1) We study continual learning through the lens of informa-tion theory and provide a new perspective on forgetting from information gain. 2) We propose a novel approach called Bit-Level Informa-tion Preserving (BLIP), which quantizes model param-eters and directly preserves information gain on model parameters brought by previously learned tasks via bit freezing. Comparing to prior works, our method enjoys advantages of both low forgetting rate and reasonable memory overheads. 3) We conduct extensive experiments ranging from classi-ﬁcation tasks to reinforcement learning tasks to demon-strate the effectiveness of our method. 4) To the best of our knowledge, our work is the ﬁrst to explore weight quantization for continual learning. 16675
2.