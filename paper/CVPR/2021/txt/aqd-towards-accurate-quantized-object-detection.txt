Abstract
Table 1 – Energy and Area cost for different precision opera-tions (on 45nm CMOS technology) [9, 14, 25].
Network quantization allows inference to be conducted using low-precision arithmetic for improved inference efﬁ-ciency of deep neural networks on edge devices. However, designing aggressively low-bit (e.g., 2-bit) quantization schemes on complex tasks, such as object detection, still remains challenging in terms of severe performance degra-dation and unveriﬁable efﬁciency on common hardware. In this paper, we propose an Accurate Quantized object Detec-tion solution, termed AQD, to fully get rid of ﬂoating-point computation. To this end, we target using ﬁxed-point op-erations in all kinds of layers, including the convolutional layers, normalization layers, and skip connections, allow-ing the inference to be executed using integer-only arith-metic. To demonstrate the improved latency-vs-accuracy trade-off, we apply the proposed methods on RetinaNet and
FCOS. In particular, experimental results on MS-COCO dataset show that our AQD achieves comparable or even better performance compared with the full-precision coun-terpart under extremely low-bit schemes, which is of great practical value. Source code and models are available at: https://github.com/aim-uofa/model-quantization 1.

Introduction
Deep neural networks (DNNs) have achieved great suc-cess in many computer vision tasks, such as image classiﬁ-cation [11, 21, 12], semantic segmentation [31, 10, 5], ob-ject detection [37, 27, 26], etc. However, DNNs are always equipped with a large number of parameters and consume heavy computational resources, which hinders their appli-cations, especially on resource-constrained devices such as smartphones and drones. To reduce the memory foot-print and computational burden, several network compres-sion methods have been proposed, such as channel prun-ing [13, 32, 53], efﬁcient architecture design [15, 39, 35] and network quantization [49, 52, 52].
In particular, network quantization aims to project
*First two authors contributed equally.
†Corresponding author. E-mail: bohan.zhuang@monash.edu
Operation 16-bit Floating-point Add 16-bit Floating-point Mult 32-bit Floating-point Add 32-bit Floating-point Mult 8-bit Fixed-point Add 8-bit Fixed-point Mult 32-bit Fixed-point Add 32-bit Fixed-point Mult
Energy (pJ) Area (µm2) 0.4 1.1 0.9 3.7 0.03 0.2 0.1 3.1 1360 1640 4184 7700 36 282 137 3495
ﬂoating-point values onto a spaced grid, where the origi-nal ﬂoating-point values can be approximated by a set of discrete values. In this way, the compute-intensive ﬂoating-point operations can be replaced by power-efﬁcient ﬁxed-point or bitwise operations, which greatly reduces the com-putational cost of the networks.
Recently, many quantization methods [52, 20, 8] have been proposed and achieved promising results on some tasks such as image classiﬁcation. However, using aggres-sively low-bit quantized networks for more complex tasks such as object detection still remains a challenge. Devel-oping quantized object detectors is a challenging task since a detector not only performs object classiﬁcation, but also needs to predict other rich information, such as the loca-tions of bounding boxes for regression. Some existing quan-tized object detection methods [18, 50] reduce the preci-sion of detectors to 4 or 8 bits and achieve promising per-formance. However, when it comes to aggressively low bitwidth (e.g., 2-bit) quantization, directly quantizing the detector incurs a signiﬁcant performance drop compared to their full-precision counterpart. Moreover, some layers (e.g., batch normalization, and skip connections) in the net-work still require ﬂoating-point arithmetic units for infer-ence. This means both integer and ﬂoating-point arithmetic units are needed for inference. As shown in Table 1, com-pared with ﬁxed-point operations, ﬂoating-point operations consume much higher energy and area cost. Besides, data exchange between different types of arithmetic units may further hamper the energy efﬁciency of the network. 104
In this paper, we propose an Accurate Quantized object
Detection (AQD) method to fully get rid of ﬂoating-point computation while maintaining performance. To this end, we propose to replace ﬂoating-point operations with ﬁxed-point operations in all kinds of layers, including the convo-lutional layers, normalization layers and skip connections.
In this way, only integer arithmetic is required during infer-ence, which signiﬁcantly reduces the computational over-heads. To reduce the performance drop from quantiza-tion while ensuring pure ﬁxed-point operations, we further propose a new variant of batch normalization (BN) called multi-level BN. Our proposed method is based on the obser-vation there is a large divergence of batch statistics across different feature pyramid levels, where batch statistics are computed using aggressively quantized activations. There-fore, using shared BN statistics in conventional detection frameworks [41, 27] will result in highly poor estimates of statistical quantities. To capture accurate batch statis-tics, multi-level BN privatizes batch normalization layers for each pyramid level of the head.
Our main contributions are summarized as follows:
• We propose an Accurate Quantized object Detection (AQD) method to fully get rid of ﬂoating-point com-putation in each layer of the network, including con-volutional layers, normalization layers and skip con-nections. As a result, only integer arithmetic is re-quired during inference, which greatly improves the on-device efﬁciency to carry out inference.
• We highlight that the degraded performance of the quantized detectors is largely due to the inaccurate batch statistics in the network. We therefore pro-pose multi-level batch normalization to capture accu-rate batch statistics of different levels of feature pyra-mid.
• We evaluate the proposed methods on the COCO de-tection benchmark with multiple precisions. Experi-mental results show that our low bit AQD can achieve comparable or even better performance with its full-precision counterpart. 2.