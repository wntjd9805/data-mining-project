Abstract
Human pose estimation has achieved signiﬁcant progress in recent years. However, most of the recent meth-ods focus on improving accuracy using complicated mod-els and ignoring real-time efﬁciency. To achieve a better trade-off between accuracy and efﬁciency, we propose a novel neural architecture search (NAS) method, termed ViP-NAS, to search networks in both spatial and temporal levels for fast online video pose estimation. In the spatial level, we carefully design the search space with ﬁve different di-mensions including network depth, width, kernel size, group number, and attentions.
In the temporal level, we search from a series of temporal feature fusions to optimize the to-tal accuracy and speed across multiple video frames. To the best of our knowledge, we are the ﬁrst to search for the temporal feature fusion and automatic computation alloca-tion in videos. Extensive experiments demonstrate the ef-fectiveness of our approach on the challenging COCO2017 and PoseTrack2018 datasets. Our discovered model fam-ily, S-ViPNAS and T-ViPNAS, achieve signiﬁcantly higher inference speed (CPU real-time) without sacriﬁcing the ac-curacy compared to the previous state-of-the-art methods. 1.

Introduction
Human pose estimation has made impressive progress in recent years with the development of stronger neural net-works. Most state-of-the-art models [36, 49, 56] only focus on improving the accuracy, but ignore the computational complexity and real-time performance. However, both ac-curacy and efﬁciency are critical for real-world applications of video pose estimation.
In this paper, we aim to build a lightweight pose estimator that achieves state-of-the-art performance with signiﬁcant model complexity reduction.
For video pose estimation, there is commonly consider-Figure 1. Speed-accuracy trade-off on PoseTrack2018 [1] val-idation set. Methods involve SBL [56], LightTrack [38] and our
ViPNAS with various backbones. With accuracy comparable to state-of-the-art networks, ViPNAS achieves CPU real-time with signiﬁcantly lower computation. able temporal redundancy that leads to superﬂuous compu-tation, i.e. adjacent frames in a video share similar global context information. The temporal contextual information can be used for improving pose estimation. Therefore, it is critical to fuse features from adjacent frames to the current frame in order to effectively utilize the temporal contextual information for balancing accuracy and efﬁciency. How-ever, there are still several open questions: 1. Low-level local features are important for accurate localization, while higher-level global features are robust to occlusion and large pose variations. Which stage of features should be fused? 2. For temporal feature fusion, various fusion operations (e.g. addition, multiplication, or concatenation) are chosen by trial-and-error. How to choose the optimal operation? 3. The goal is to optimize the total accuracy subject to the total computation complexity (Flops) constraints over 16072
the whole video. Previous works generally explicitly en-force different frames to apply the same model, which will result in sub-optimal performance. How to efﬁciently allo-cate computation across different video frames?
Manually exploring the design choices regarding the above questions via trial-and-error can be tedious. We in-stead apply neural architecture search (NAS) to give a uni-ﬁed solution to them. We propose a novel spatial-temporal
NAS framework for efﬁcient video pose estimation, termed
ViPNAS. For spatial-level search, we optimize the neural architecture by a wide spectrum of ﬁve dimensions (depth, width, kernel size, group number, and attention). For temporal-level search, we jointly search three aspects of de-signs: 1) the stage of features to be fused, 2) the feature fu-sion operation, and 3) the allocation of computation across video frames. The spatial-level and temporal-level search are jointly optimized through a single framework. Given the total Flops over multiple frames as constraints, we can ef-ﬁciently allocate computation across different video frames for optimizing performance. Experiments show that ViP-NAS signiﬁcantly improves over the state-of-the-art meth-ods, such as SBL [56] and LightTrack [38], with vari-ous well-known backbones (ResNet [13], CPN [6], Mo-bileNets [15, 14], ShufﬂeNet [35] and EfﬁcientNet [51]).
Our main contributions can be summarized as follows:
• We propose the novel spatial-temporal neural architec-ture search (NAS) framework for efﬁcient video pose estimation, termed ViPNAS.
• ViPNAS learns to allocate computational resources (e.g. Flops) for different frames under the total com-putation complexity constraints across frames.
• ViPNAS automatically searches temporal connections, i.e. the fusion module and positions.
In the task of video pose estimation, we achieve the state-of-the-art accuracy with CPU real-time performance (> 25 FPS). 2.