Abstract
Lossy compression optimises the objective
Lossy image compression is often limited by the sim-plicity of the chosen loss measure. Recent research sug-gests that generative adversarial networks have the ability to overcome this limitation and serve as a multi-modal loss, especially for textures. Together with learned image com-pression, these two techniques can be used to great effect when relaxing the commonly employed tight measures of distortion. However, convolutional neural network-based algorithms have a large computational footprint. Ideally, an existing conventional codec should stay in place, ensur-ing faster adoption and adherence to a balanced computa-tional envelope.
As a possible avenue to this goal, we propose and investi-gate how learned image coding can be used as a surrogate to optimise an image for encoding. A learned Ô¨Ålter alters the image to optimise a different performance measure or a particular task. Extending this idea with a generative ad-versarial network, we show how entire textures are replaced by ones that are less costly to encode but preserve a sense of detail.
Our approach can remodel a conventional codec to ad-just for the MS-SSIM distortion with over 20% rate im-provement without any decoding overhead. On task-aware image compression, we perform favourably against a simi-lar but codec-speciÔ¨Åc approach. 1.

Introduction
Forming the basis for video compression, image com-pression‚Äôs efÔ¨Åciency is vital to many data transmission and storage applications. Most compression algorithms are lossy, i.e. they do not reproduce the original content ex-actly but allow for deviations that reduce the coding rate.
L = R + ŒªD (1) where R and D stand for rate and distortion, respectively, and Œª controls their weight relative to each other. In prac-tice, computational efÔ¨Åciency is another constraint as at least the decoder needs to process high resolutions in real-time under a limited power envelope, typically necessitating dedicated hardware implementations. Requirements for the encoder are more relaxed, often allowing even ofÔ¨Çine en-coding without demanding real-time capability.
Recent research has developed along two lines: evolu-tion of exiting coding technologies, such as H264 [41] or
H265 [35], culminating in the most recent AV1 codec, on the one hand. On the other hand, inspired by the success of deep learning in computer vision, based on the vari-ational autoencoder [20], several learned image compres-sion approaches have been developed. Through careful modelling of the latent code‚Äôs symbol probabilities, those learned codecs have recently shown to outperform the x265 codec on various performance measures [30]. This trades engineering effort for complexity: training a neural network image codec is simple compared to designing a conven-tional codec with all its bells and whistles. However, the resulting neural decoder requires on the order of 105 oper-ations per pixel, where a conventional codec has at most a few hundred operations, being several orders of magnitude more efÔ¨Åcient. This is an important factor, given how much video and imagery is consumed on mobile devices.
Another advantage of learned compression is the ability to be easily adapted to different distributions or distortion measures. By merely collecting data and choosing an ap-propriate distortion loss, a new codec can quickly be trained without having to manually re-engineer coding tools or in-ternal parameters. 16165
In this work, we take a step towards leveraging the adapt-ability of learned codecs for conventional codecs without changing the conventional codec itself. In doing so, we en-able a higher compression performance without touching the decoder, meaning that existing endpoint decoder imple-mentations do not require changes.
In lossy compression, adaptability has two aspects: drop-ping information that is hard to encode and of little impor-tance and, secondly, exploiting expensive redundancies in the data. The latter would require either reorganising data or changing the coding tools a codec can apply at encod-ing time. The former, though, is feasible as the data can be changed prior to letting the encoder map it to its coded representation.
We demonstrate three possible applications of this scheme: 1. We apply a Ô¨Åltering mechanism to siphon out informa-tion that would drive the coding cost but is only of little concern with respect to the chosen loss function. Intro-ducing a learned codec as a surrogate gradient provider conventional codecs can be reÔ¨Åtted to the MS-SSIM distortion measure at over 20% rate improvement. 2. We apply the approach to task-speciÔ¨Åc compression where we perform favourably on image classiÔ¨Åcation without having to model the target codec speciÔ¨Åcally. 3. We pair the surrogate-induced Ô¨Ålter with a generative adversarial network to alter an image beyond optimi-sation for simple distortion measures. Textures are re-placed with perceptually similar alternatives that have a shorter code representation, thereby survive the cod-ing process and preserve the crispness of the original impression.
By simply manipulating the input data, we alter the way the Ô¨Åxed-function codec behaves. We present evaluations across different datasets and codecs and show examples of how introducing GANs can lead to distinctly sharper-looking images at low coding rates. 2. Learning to Filter with a Surrogate Codec
Current coding mechanisms are most often instances of transform coding, where the data is Ô¨Årst transformed, then quantised in the transform domain and Ô¨Ånally encoded losslessly using a variant of arithmetic coding. Conven-tional codecs use a cosine- or sine-transform, while learned codecs replace them with a convolutional neural network.
This has the distinct advantage that the Ô¨Ålters of the neural network can be adapted to leave out information that has low marginal coding gain, i.e. costly to encode and with only little beneÔ¨Åt as measured by a given loss function. Ta-ble 1 shows which functionality is invoked by different cod-ing tools.
Table 1. Comparing different coding mechanism as to which part of a lossy coding pipeline they inÔ¨Çuence.
Approach
Encoder
Decoder
Remove
Inform.
Capture
Redund.
Code Rec.
Conv. Codec
Learn. Codec
Denoising
Inpainting
[12, 34]
Our‚Äôs / [37]
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
Post-Filter
‚úó/‚úì
‚úì
‚úì
‚úì
In codecs with Ô¨Åxed transforms, this process needs to be emulated manually, which typically happens by varying the quantisation granularity so that high-frequency transform coefÔ¨Åcients are quantised more coarsely than low-frequency ones. Different rate-distortion trade-offs are then achieved by adjusting the granularity for all coefÔ¨Åcients simultane-ously. Omitting information happens implicitly through the quantisation process. This, however, requires the trans-forms to separate information according to its importance so that varying quantisation granularity has the desired ef-fect. The general underlying idea is that low-frequency con-tent is the most important and importance decreases towards higher frequencies. While this adequately distinguishes noise from actual information for some content, it does not hold in general. Hence, more adaptive Ô¨Ålters are desirable, however difÔ¨Åcult to adjust as their inÔ¨Çuence on the coding behaviour is unknown.
Omitting unimportant information helps to signiÔ¨Åcantly reduce the size of compressed multimedia data. For an opti-mal codec, it would be desirable to design the data Ô¨Åltering process with the distortion measure on the one hand and the transformation and coding procedure on the other in mind.
Machine-learning based codecs can do this implicitly be-cause all elements required are differentiable. Their opti-misation procedure allows the Ô¨Ålters to adapt to the input data, the distortion measure, and the arithmetic coding pro-cedure. For conventional codecs, this process is realised through extensive engineering and by relying on local opti-misation of the code structure, for example, how to partition the image into transform blocks or the choice of prediction modes, which makes it difÔ¨Åcult to turn a codec into a differ-entiable function and obtain a gradient with respect to the input image.
In the machine learning literature this has been addressed as black-box function optimisation, for example through the
REINFORCE learning algorithm [42] or the learning-to-learn approaches [15, 3, 11]. These methods rely on an approximation of the unknown function to be optimised, though. Filtering of less important information is likely 16166
Surrogate Gradient
ùúïùëîùëÖ ·àòùêº; ùúÉùê∂
ùúï ·àòùêº
+ ùúÜùëá
ùúïùê∑ùëá ùêº, ùëîùê∑ ·àòùêº; ùúÉùê∂
ùúï ·àòùêº
Filter
·àòùêº = ùëìùêπ ùêº; ùúÉùêπ
ùúÜùëá ùúïùê∑ùëá ùêº, ·àòùêº
ùúï ·àòùêº
Image 
ùêº
Gradient
Forward
Surrogate Codec
Rate Estimator 
ùëîùëÖ ·àòùêº; ùúÉùê∂
Encoder
Reconstruction 
ùëîùê∑ ·àòùêº; ùúÉùê∂
Switch
Reconstruction 
‚Ñéùê∑ ·àòùêº
Encoder
Rate Estimator 
Target Codec (not diff.)
‚ÑéùëÖ ·àòùêº
Target Loss
Rate
ùëÖùëá
Distortion Measure 
ùê∑ùëá ùêº, ·àòùêº
Image 
ùêº
Figure 1. Structural overview of our method. The goal is to obtain a trained Ô¨Ålter fF (I; Œ∏F ) to optimise the input image I for encoding by a target codec. This target is typically not differentiable. A surrogate codec is used instead. It provides a differentiable rate estimate.
For the reconstruction there are two options as indicated by the switch. The Ô¨Årst is to take the surrogate‚Äôs reconstruction, the second to invoke the target during the forward pass. The gradient Ô¨Çows back accordingly either through the surrogate‚Äôs reconstruction or directly into the Ô¨Ålter. The target distortion measure DT can be chosen freely. The surrogate codec is pre-trained with a distortion measure similar to the one of the target codec so as to imitate its behaviour. At testing time, the Ô¨Ålter is applied to the image before it‚Äôs encoded by the target codec. to require such an approximation to be very precise, as the changes applied to the input image are expected to be small in magnitude. In preliminary experiments, we found that even a complex, deep model does not approximate a codec‚Äôs rate and its decoded image accurately. This is most likely due to the complexity of the non-stationary error distribu-tion that codecs produce, which varies between adjacent blocks.
To address this, [12] present an approach for predicting an optimised quantisation matrix for the JPEG codec. [34] adds a pre-editing network to this, similar to our approach.
[37] also proposes image editing prior to encoding for better coding performance. All three approaches demonstrate ef-fectiveness, but lean on modelling the JPEG codec as a dif-ferentiable function. Hence, they are limited to this rather
In the case of [12], their exclusive use of simple codec. quantisation tables imposes restrictions on the corrections they can perform.
In contrast, we present an approach that does not rely on approximating the codec directly but exploits a (differen-tiable) model that optimises the same objective, i.e. Eq. 1.
The structure of this approach is depicted in Figure 1. Our objective is to approximate the gradient
‚àÇLh
‚àÇI
=
‚àÇhR(I)
‚àÇI
+ Œª
‚àÇD(hD(I), I)
‚àÇI (2) of the loss function Lh of a codec h with a rate hR(I), a de-coded reconstruction ÀúI = hD(I), and a distortion measure
D( ÀÜI, I) between the decoded and the original image I. Im-age compression models like [30] provide a differentiable codec g with rate gR(I; Œ∏C) and decoded reconstruction gD(I; Œ∏C) both utilising some parameter vector Œ∏C. Using this, an image I can be manipulated by adding Œ∂ ‚àó where
Œ∂ ‚àó
= arg min
Œ∂
Lg = arg min
Œ∂ gR (I + Œ∂)+ŒªT DT (gD (I + Œ∂) , I) (3) can be optimised using a gradient descent based algorithm.
The original codec then operates on the modiÔ¨Åed input im-age ÀÜI = I + Œ∂ ‚àó with the objective of achieving
Lh > ÀÜLh = hR (cid:16)
ÀÜI(cid:17) + ŒªT DT (cid:16)hD (cid:16)
ÀÜI(cid:17) , x(cid:17) . (4) for a chosen target distortion DT . The assumption here is that, though the losses differ (Lg 6= Lh), the provided gra-dients are similar:
‚àÇLg
‚àÇI
‚âà
‚àÇLh
‚àÇI
. (5)
We introduce a Ô¨Ålter ÀÜI = fF (I; Œ∏F ) to predict the Ô¨Åltered image ÀÜI directly from the original image I. Its parameters
Œ∏F can be trained using the gradient of the loss ÀÜLg where the surrogate codec provides the rate estimation. The parameter
ŒªT controls the trade-off between rate and distortion for the
Ô¨Ålter fF .
Most codecs are designed for a particular intrinsic distor-tion measure DI . For common codecs like H265 or H264, coding tools and mechanisms were evaluated using PSNR during standardisation. This suggests that a codec may perform differently in terms of the rate-distortion trade-off 16167
when subjected to a different distortion measure. It is there-fore likely that the measure used to optimise the parameters
Œ∏C of the learned codec g should be the same as that for which h was designed. This will be conÔ¨Årmed by experi-ments. We will denote that measure by DS as it is the one to be approximated. When optimising Equation 3 on the other hand, the distortion DT can be freely chosen so long as it is differentiable.
With this framework in place, we are set to optimise the
Ô¨Ålter fF for a chosen particular target distortion DT while observing rate constraints as given by the surrogate codec‚Äôs rate estimation. 2.1. Network Architectures 2.1.1 Filter 16 √ó w
The Ô¨Ålter needs to remove information that is too costly to encode and has little inÔ¨Çuence on the distortion DT . The chosen network architecture is depicted in Fig. 2. The Ô¨Årst stage is identical to the encoder and the rate estimation of the chosen surrogate codec. The general idea is that the
Ô¨Ålter Ô¨Årst needs to estimate which regions of the image have a higher concentration of information. This is con-tained in the entropy estimates Hi,j,k. Hi,j,k is of dimen-sion h 16 √ó C where C is the number of channels of the code tensor and h and w the dimensions of the image. For the surrogate model from [30] chosen here, this is C = 320.
The next step is to successively scale the code up to the size of the image using four layers of transposed convolutions with a stride of two. We keep reducing the number of Ô¨Ålters in each layer so as to have 16 output features per pixel. The idea behind not using a single value at each pixel is that we would like to represent a more complex distribution as the allocation of coding cost to single pixels is obviously chal-lenging and a distribution can be interpreted as being condi-tional on the input image that is fused in the next step. The resulting pixel-wise entropy estimate Hpixel ‚àà Rh√ów√ó16 is concatenated with the input image I to serve as input to the actual Ô¨Ålter. The Ô¨Ålter itself consists of four residual blocks followed by a convolutional layer at the end. We use 64 channels and kernels of size 3 √ó 3 throughout the entire Ô¨Ål-ter. 2.1.2 Discriminator
We choose a simple discriminator, as its only task is to recognise artefacts in the Ô¨Åltered image ÀÜI and simple pat-terns in texture as opposed to larger structures or seman-tically coherent objects when images are generated from scratch. Our discriminator has three consecutive stages to capture artefacts at different scales, as previously described by [32]. Each stage consists of two residual blocks. Subse-quent stages apply 2 √ó 2 average pooling prior to the resid-ual blocks. Each stage s is followed by a single convolu-tional Ô¨Ålter that computes the discriminator‚Äôs prediction ps.
To fuse those predictions, we Ô¨Årst apply a sigmoid to con-vert them into probabilities before computing the mean over each stage:
ÀÜyGAN(I) = 1 3 3
X s=1
E [sigmoid (ps(I))] (6)
We noticed that only computing high-level outputs at the discriminator does lead to low-level artefacts like blurriness in the image, an observation shared previously by Rippel and Bourdev [32]. Similar to [2], we used a least-squares objective to train the discriminator, as previously introduced in [27]. Discriminator and Ô¨Ålter are optimised alternatingly, i.e. not using the same batch, for stability. 3. Experiments 3.1. Experimental Conditions 3.1.1 Datasets used for evaluation are the Kodak dataset (24 images, about 0.35MP) and the ‚Äùprofessional‚Äù validation dataset of the
CLIC competition (41 images at more than 2MP). Our train-ing data is sampled from the ImageNet dataset and CLIC‚Äôs provided training data. We randomly resize an image so that its shorter side is between 512 and 1024px and then sample a 256 √ó 256 crop. 3.1.2 Surrogate Model
We use the deep convolutional neural network model pro-posed by Minnen et al. [30]. Besides a 4-layer encoder and decoder with generalised divisive normalisation [7], it uses context modelling to minimise the entropy of the code sym-bols. It has been shown to work well for both the MS-SSIM as well as the PSNR distortion metrics and achieves state-of-the-art performance on the Kodak test set. In addition, there is a reference implementation available [5, 9]. 3.1.3 Optimization
Our models are implemented using PyTorch [31] and trained with the Adam [19] optimizer. A batch size of 8 is used with a learning rate of 1e-4 for 400.000 iterations, which is reduced to 1e-5 for another 100.000 iterations af-terwards. 3.1.4 Codecs
We use JPEG, WebP, BPG (H265/HEVC) [10] and the re-cently released AV1 to validate our hypothesis and demon-strate coding improvements. For all codecs, we choose maximum gain over encoding speed and encode in the codec‚Äôs native YCbCr colour space. 16168
Rate-Tensor
Residual Block
Image 
ùêº
Encoder
Pixelwise Rate 
Estimator
Concatenate
Filter
Filtered Image 
·àòùêº
Figure 2. Architecture of the Ô¨Ålter component. The encoder (grey) is taken from the surrogate codec. However, we do not use its latent representation but only its rate estimate, i.e. the function hR. Its weights are Ô¨Åxed. The latent code‚Äôs rate estimate is transformed into a pixel-wise rate estimate via four successive transposed convolutions. The result is concatenated with the input image and fed to a Ô¨Ålter consisting of four residual blocks. sentation at encoding time. Not only can the MSE be com-puted faster, but it also doesn‚Äôt transcend block boundaries, making it a computationally suitable objective. Hence, ‚Äùre-targeting‚Äù a codec to MS-SSIM without having to change its encoder is appealing if it can be done by simply adding a Ô¨Ålter at encoding time.
We train three models at different rate-distortion trade-offs ŒªT to cover different parts of the rate range. We found in preliminary experiments that, when optimising Œ∂ ‚àó di-rectly, i.e. not training a Ô¨Ålter fF (I; Œ∏F ), learned surrogate codecs perform well around their operating point as deter-mined by ŒªS, however, quickly diminish in performance once the trade-off point Œª is changed signiÔ¨Åcantly. For this set of experiments, we chose ŒªS ‚àà {0.1, 0.2, 0.4} for the
MSE-optimized surrogate. When optimising Eq. 4 using the surrogate codec, the rate-distortion trade-off ŒªT for the target MS-SSIM loss needs to be chosen appropriately. We found ŒªT = 500ŒªS to work well by comparing at which pairs of ŒªS,MSE, ŒªS,MS-SSIM surrogate codecs optimised for each of the two distortions would yield similar rates.
There are limitations for very high bit rates as our retar-geting process takes out information, and hence the recon-struction error cannot be arbitrarily small. This is reÔ¨Çected in the graphs in Fig. 3. Towards higher rates, where the error is very small, the Ô¨Åltering effect fades, with the Ô¨Ål-tered version eventually having a worse distortion than the original one. For the more advanced codecs AV1, BPG and
WebP, the rate savings peak for low rates. Only for JPEG, the rate savings are much less dynamic. The reason may lie in the simplicity of the JPEG codec, with little space for op-timisation as the coding and quantisation process is static, only the quantisation tables can be optimised (over the en-tire image). Interestingly, for both datasets, the BPG codec initially performs worse than AV1. However, it has higher adaptation coding gains so that the Ô¨Åltered variants of both codecs are almost on par.
Comparing the rate savings of the Kodak dataset in Fig. 3 to those of the validation part of the CLIC Professional dataset in Fig. 4, one notices that the three advanced codecs 16169
Figure 3. Rate-Distortion characteristic and relative rate savings for different codecs when surrogate-induced Ô¨Åltering is optimised for MS-SSIM and tested on the Kodak dataset. The dashed lines represent the rate-distortion characteristic of the codecs without
Ô¨Åltering, the solid lines with Ô¨Åltering. The dotted line (cyan) is cre-ated using a surrogate codec trained on the MS-SSIM loss instead of PSNR. It shows no improvement over the original BPG (H265) codec as the surrogate distortion does not match the codec‚Äôs. The bottom Ô¨Ågure shows the rate savings over different rates, i.e. how much less rate the Ô¨Åltered codecs require relative to the original ones. 3.2. Adaptation to MS SSIM
We train a Ô¨Ålter to minimise the rate and the impact on the MS-SSIM distortion measure. The idea is that most codecs were designed to minimise the MSE and also opti-mise that measure internally when choosing the best repre-where yGAN (cid:16) loss optimizes the opposite objective of LGAN:
ÀÜI(cid:17) is deÔ¨Åned by Eq. 6. The discriminator‚Äôs 2
LDiscriminator = EI h(1 ‚àí yGAN (I)) i+EI hyGAN (fF (I; Œ∏F )) (9)
LDiscriminator and LFilter are optimized in turn every other it-eration. 2 i
Figure 4. Relative rate savings for different codecs when surrogate-induced Ô¨Åltering is optimized for MS-SSIM and tested on the CLIC Professional (validation) dataset. show similar behaviour in terms of rate savings. There‚Äôs a peak early on, followed by a steady, parallel decline. This indicates that, at higher rates, the induced error from the Ô¨Ål-tering process is too high relative to the coding error. When optimising only for numeric measures instead of percep-tual ones, this limitation cannot be overcome. This is an-other reason to also look at perceptual losses like generative adversarial networks, as discussed in Section 3.3. Beyond this, the higher resolution images of the CLIC Professional dataset are more difÔ¨Åcult to optimise yet still have coding gains up to 15%. Higher resolutions often have a lower in-formation density, making it harder to Ô¨Ålter out irrelevant parts.
Lastly, we also test with a surrogate codec optimised for
DS = MS-SSIM. This is shown as a dotted cyan coloured line in Fig. 3. There is no improvement over the raw codec, showing that the surrogate‚Äôs rate estimation needs to be based on the correct loss function to model a codec‚Äôs rate allocation properly. 3.3. Generative Adversarial Network
Generative Adversarial Networks have led to remarkable results in various tasks where high-resolution natural im-ages are to be reconstructed, such as super-resolution [23].
In the context of codec adaptation, the GAN serves as an (additional) loss measure.
Following [2], we employ MSE LMSE and the VGG-based perceptual loss LVGG proposed in [40] in addition to the GAN loss LGAN to ensure that the image content re-mains similar and only textural information changes. The
Ô¨Ånal loss function for the Ô¨Ålter adds these three distortion terms to the rate term hR of the surrogate codec:
LFilter = Œ≥GANLGAN + Œ≥VGGLVGG + Œ≥MSELMSE + hR (7) where the different loss components can be weighted ac-cording to one‚Äôs objectives. The Ô¨Ålter‚Äôs GAN loss LGAN( ÀÜI) conditioned on the Ô¨Åltered image ÀÜI = fF (I; Œ∏F ) is given by the least squares objective from [27]:
LGAN (I)) = EI h(1 ‚àí yGAN (fF (I; Œ∏F ))) 2 i (8)
The choice of weights in Eq. 7 determines what kind of sensitivity the GAN discriminator should apply. A lower weight Œ≥GAN allows only for small changes in the image so that smaller artefacts are being corrected. However, tex-tures will not be exchanged for ones that have a shorter code length yet look crisper when coded at low rates.
Setting the GAN‚Äôs weight to Œ≥GAN = 5.0 and Œ≥VGG = 0.01, Œ≥MSE = 0.001, the discriminator forces the Ô¨Ålter to replace textures that are indistinguishably sharp yet have a shorter code length. The MSE and the VGG-based loss en-sure that the texture is similar to the original. Examples of such a transformation are shown in Table 2. Due to the low rate, the codec‚Äôs aggressive quantisation causes most details to wash out. At even lower coding rates, our pre-Ô¨Åltered images are able to preserve perceived details throughout the coding process.
This way, without changing the mechanisms a conven-tional codec employs to represent an image, we can manip-ulate the codec through its input into optimising for a rather perception-based objective. The beneÔ¨Åt is that the shallow decoder that operates with only about 100 operations per pixel can be reused, and the encoder is simply augmented and otherwise remains unchanged. 3.4. Task Aware Compression
Task-aware compression takes a non-perceptual task into
In this work we follow account when compressing data.
[12] and use the ImageNet [17] object recognition task. To learn the Ô¨Ålter, we use the pre-trained ResNet-18 [14] avail-able from PyTorch. For this task, the Ô¨Ålter receives the clas-siÔ¨Åer‚Äôs feedback on which features are important. This ne-cessitates two changes to our algorithm.
The Ô¨Årst is to initialise the Ô¨Ålter with a very low variance so that the generated correction signal has a very low am-plitude at the beginning of the training. If one omits this, the (initially random) correction signal will still be adjusted to strengthen certain features. However, the learned codec‚Äôs rate estimate doesn‚Äôt transfer well. Apparently, the reason for this is that there are certain high-frequency signals that are cheap for a learned but expensive for a conventional codec and the Ô¨Ålter tends to exploit those because they are closer to its own starting point, i.e. the high variance ran-dom initialisation.
The second measure is to use the target codec‚Äôs de-coded version of the Ô¨Åltered image ÀÜI, hD( ÀÜI), to obtain feed-back from the classiÔ¨Åer about which features are important. 16170
Table 2. Visual comparison, showing how our algorithm modiÔ¨Åes certain textures within the image to mimic the original image‚Äôs details with lower-code-length content as controlled by a generative adversarial network.
Original
H265 (unÔ¨Åltered)
Ours (H265 Ô¨Åltered)
Rate: 0.086 bit/pixel
Rate: 0.084 bit/pixel
Rate: 0.045 bit/pixel
Rate: 0.042 bit/pixel
Rate: 0.190 bit/pixel
Rate: 0.192 bit/pixel 16171
ing many-layered neural networks in the image formation process.
Similar to their work, we employ techniques from learned image compression. Recursive architectures have been proposed [38, 39, 18] as well as adoptions of varia-tional auto-encoders [7, 24, 4, 1, 32] with various extensions to the latent code model [6, 25, 22, 30, 8, 28]. Within a few years, these techniques have closed the gap to conventional codecs. However, their decoders require three to four orders of magnitude more operations per pixel. Again, this is an important difference to our approach, as we target an exist-ing coding pipeline. Furthermore, those works have previ-ously been extended into the video coding domain, typically by adding and encoding optical Ô¨Çow [26, 13, 33].
Other works have improved upon existing codecs by adding neural networks as denoising modules. Pretrained denoisers such as [16, 43] are effective but, like the afore-mentioned approaches, do add heavily to the decoder com-plexity. Using an online adaptation of a small neural net-work, [21] showed that the complexity could be reduced signiÔ¨Åcantly while delivering a coding gain similar to pre-trained networks. On the downside, these methods require changes to the codec architecture and target only numerical losses, i.e. optimising Equation 1, no perceptual ones. 5. Conclusion
We have demonstrated how machine learning-based im-age compression algorithms can be repurposed as surrogate gradient generators to train image Ô¨Ålters to alter the cod-ing characteristic of conventional codecs without explicitly modelling their behaviour. Experiments demonstrate retar-geting to the MS-SSIM distortion measure leads to over 20% coding gain, an advantage over a codec-speciÔ¨Åc op-timisation approach on task-aware image compression, and how adding a generative adversarial network can enable the preservation of sharp textures even in very low rate condi-tions.
Our approach opens up a number of possible directions for future research. Modelling existing or upcoming codecs to a higher accuracy could enable even better results. Tra-ditional, hand-optimised image signal processing pipelines, as they are found in smartphones, for example, could be op-timised end-to-end from acquisition to image formation and to coding. The introduced concepts extend to the motion picture domain as well, which could enable codec reÔ¨Åtting beyond still images.
Finally, the proposed technique does not require chang-ing the coding pipeline.
Encoder and decoder stay in place. im-to many other it can utilise existing ef-age coding improvements,
Ô¨Åcient hardware implementations of conventional de-coders. in contrast
Hence, 16172
Figure 5. Comparison of our work to Choi et al. ([12]) on the Im-ageNet image recognition task with Inception-v3. Baselines from
[12].
While this slows training down due to B calls to the con-ventional codec during the forward pass for batch size B, it resolves the same issue as in the previous paragraph. The information that goes missing is likely to be high-frequency signals involved in rather Ô¨Åne-grained recognition. If the surrogate codec‚Äôs decoded image, gD( ÀÜI), was used directly, the Ô¨Ålter may overÔ¨Åt and exploit low-cost, high-frequency signals.
For testing, in accordance with [12], we use a pre-trained
Inception-v3 architecture [36]. We train a single model us-ing JPEG at a quality level of 30 at training time. At test time, we Ô¨Ålter the images once and then encode them at different quality levels. The results are shown in Fig. 5. Al-though [12] has a more complicated model that mimics the
JPEG codec, our approach performs favourably. One reason may be that [12] only manipulates quantisation tables while our approach can apply arbitrary changes to the image. In this, our method, though simpler, is more powerful. 4.