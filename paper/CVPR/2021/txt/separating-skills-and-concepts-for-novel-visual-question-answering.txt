Abstract
Generalization to out-of-distribution data has been a problem for Visual Question Answering (VQA) models. To measure generalization to novel questions, we propose to separate them into “skills” and “concepts”. “Skills” are visual tasks, such as counting or attribute recognition, and are applied to “concepts” mentioned in the question, such as objects and people. VQA methods should be able to compose skills and concepts in novel ways, regardless of whether the speciﬁc composition has been seen in training, yet we demonstrate that existing models have much to im-prove upon towards handling new compositions. We present a novel method for learning to compose skills and concepts that separates these two factors implicitly within a model by learning grounded concept representations and disen-tangling the encoding of skills from that of concepts. We enforce these properties with a novel contrastive learning procedure that does not rely on external annotations and can be learned from unlabeled image-question pairs. Ex-periments demonstrate the effectiveness of our approach for improving compositional and grounding performance.1 1.

Introduction
When humans answer questions, such as in Visual Ques-tion Answering (VQA), we ﬁrst interpret the question, dis-secting its content into parts (like concepts, relations, ac-tions, question types), and then we select and execute the skill (or plan/program) necessary to produce an answer based on this information and the relevant knowledge base (e.g., the image) [25, 34, 54, 57]. The skills needed to pro-duce an answer are general and can be applied to (composed with) many types of question-speciﬁc content. For exam-ple, if one can answer questions about “colors” for a variety of objects as well as recognize and answer questions about
“cars”, then questions like “What color is the car?” should be straightforward to answer even if this speciﬁc compo-sition has yet to be seen (Fig. 1). This ability of seam-∗Work was partly done as an intern at the MIT-IBM Watson AI Lab. 1Code: https://github.com/SpencerWhitehead/novelvqa
Figure 1. We propose a new view of compositionality in VQA that explores the ability to answer questions about unseen composi-tions of skills (e.g., color) and concepts (e.g., car). We present a method that learns to separate skills and concepts that can utilize both labeled and unlabeled image-question pairs in order to gener-alize to novel questions with new skill-concept compositions and new concepts. lessly adapting and composing conceptual representations with skills is crucial to demonstrating true understanding of
VQA and learning to generalize from less labeled data.
Compositionality is recognized as one of the essential properties of human cognition [33], but more research is still needed on incorporating compositionality into models and developing data-efﬁcient, generalizable systems. While much progress has been made to achieve better performance on standard VQA test benchmarks [4, 16, 32, 56], most state-of-the-art models are still designed without any notion of built-in compositionality and tend to entangle skills and concepts in their learned representations. Some previous work has studied the lack of generalization ability of VQA models, and evaluated models using test splits with differ-ent answer distributions from the training data. However, this measurement only indirectly addresses the central issue 5632
(lack of compositionality), which manifests itself as poor generalization and over-reliance on language priors [2, 42].
To address these issues, our ﬁrst contribution is a new view of VQA compositionality, called skill-concept compo-sition, and a new evaluation setting that directly targets how
VQA models can generalize to novel compositions of skills and concepts. This view is motivated by our observation that, to answer a natural question on real images requires the understanding of two distinct elements: 1) the visual con-cept referred to by the question; and 2) what information we need to extract from the referred concept. We elucidate this in Sec. 3 and evaluate a number of VQA architectures using this setting and demonstrate that the existing models have much to improve upon to answer novel questions.
We propose a novel approach to improve generalization that utilizes contrastive learning to separate skills and con-cepts within the internal representations of a model, while jointly learning to answer questions. We use grounding as a proxy to separate concepts so that the model learns to iden-tify a concept in both the question and image, regardless of the speciﬁc context. Akin to weakly supervised ground-ing [3, 17], we train the model to recover a concept men-tioned in a given image-question pair by contrasting the multi-modal representation of the masked concept word to the multi-modal representations of words in other questions.
We utilize a new way to curate positive and negative exam-ples for the contrastive loss so that the model learns to pre-dict the concept based on relevant visual information rather than using superﬁcial contextual cues. Additionally, our ap-proach learns to separate skills from concepts by contrast-ing question representations that have the same or different skills. These properties are learned jointly alongside the
VQA objective, on top of state-of-the-art models, and are generalizable to new architectures.
Some advantages of our approach are: 1) We learn grounding in a self-supervised manner using the VQA data alone, without external annotations. This is in contrast to previous approaches with similar goals that incur large ex-penses due to annotation requirements [44, 55]. 2) Our method does not rely on answer labels to learn skill-concept separation, so we are able to use unlabeled image-question pairs to learn these properties. Consequently, we are able to acquire new concepts and learn to answer questions about them without having labeled data with these concepts, which is pivotal for generalizing to a new domain or novel instances. Moreover, we focus on data-efﬁcient methods and do not use prodigious amounts of data external to VQA, like pre-training approaches [39, 50, 8], which is expensive to obtain and can require prior knowledge of the domain and/or concepts in order to perform well [47, 19].
Our main contributions in this paper are: 1) We present a novel view and evaluation setting for compositionality in VQA, called skill-concept composition, which enables a more direct and interpretable evaluation of VQA mod-els on real-image question answering. 2) We propose a novel contrastive learning approach, which combines the supervised VQA objective with self-supervised learning, to achieve skill-concept disentanglement at no additional an-notation cost. 3) Our approach shows signiﬁcant improve-ments over existing models on novel skill-concept composi-tions as well as generalization to unlabeled image-question pairs containing unseen concepts. 2.