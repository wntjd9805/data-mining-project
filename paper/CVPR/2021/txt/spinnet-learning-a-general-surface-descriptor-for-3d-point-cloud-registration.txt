Abstract
Extracting robust and general 3D local features is key to downstream tasks such as point cloud registration and reconstruction. Existing learning-based local descriptors are either sensitive to rotation transformations, or rely on classical handcrafted features which are neither general nor representative. In this paper, we introduce a new, yet conceptually simple, neural architecture, termed SpinNet, to extract local features which are rotationally invariant whilst sufﬁciently informative to enable accurate registra-tion. A Spatial Point Transformer is ﬁrst introduced to map the input local surface into a carefully designed cylin-drical space, enabling end-to-end optimization with SO(2) equivariant representation. A Neural Feature Extractor which leverages the powerful point-based and 3D cylindri-cal convolutional neural layers is then utilized to derive a compact and representative descriptor for matching. Ex-tensive experiments on both indoor and outdoor datasets demonstrate that SpinNet outperforms existing state-of-the-art techniques by a large margin. More critically, it has the best generalization ability across unseen scenarios with dif-ferent sensor modalities. The code is available at https:
//github.com/QingyongHu/SpinNet. 1.

Introduction
Accurate matching of partial 3D surfaces is critical for point cloud registration [17, 38, 6, 21, 29], segmenta-tion [58, 28, 27], and recognition [24, 12, 47]. Given multiple partially overlapped 3D scans, the goal of surface matching is to align these fragments according to a set of point correspondences, thus obtaining a complete 3D scene structure. To achieve this, it is of key importance to identify general and robust local geometric patterns shared between two scans. However, this is challenging, primarily because 1) different scans usually have different viewing angles, 2) the raw 3D scans are typically incomplete, noisy, and have signiﬁcantly different point densities.
*Equal contribution
D3Feat-pred
[95.8%     61.6%]
Ours
[97.6%    92.8%]
D3Feat-rand
[95.3%    26.2%]
FCGF
[95.2%    16.1%] 3DMatch
[59.6%    16.9%]
CGF
[58.2%    20.2%]
Figure 1: The Feature Matching Recall (FMR) scores of dif-ferent approaches on the indoor 3DMatch [65] and outdoor
ETH [46] dataset. Note that, all methods are trained only on the 3DMatch dataset. Our method not only achieves the highest score on 3DMatch, but also has the best generaliza-tion ability across the unseen ETH dataset.
Early methods to extract local geometries include PS [9], 3DSC [19], ISS [67] and SHOT [40], which simply com-pute the low-level features such as faces [41, 64], cor-ners [52], and handcrafted statistical histograms [43]. Al-though they achieve encouraging results on high-quality 3D point clouds, they are not capable of generalizing to highly noisy and large-scale real-world 3D point clouds.
Recent learning-based approaches [65, 32, 14, 45] have yielded excellent results in extracting better local point fea-tures, thanks to the availability of large-scale labeled 3D datasets. However, they have two major limitations. First, many of these methods such as D3Feat [2] and FCGF [8] rely on kernel-based point convolution [53] or submani-fold sparse convolution [23] to extract per-point features, resulting in the learned point local patterns being rotation-ally variant. Consequently, their performance drops dra-matically when across datasets with signiﬁcantly different rotation distributions. Second, although a number of re-cent approaches [14, 13, 22, 37] introduce rotation-invariant descriptors, they simply integrate the handcrafted features such as point-pairs [48, 16] and point density [50, 62, 51], or external local reference frames (LRFs) [37, 33, 66] into 11753
the pipeline, fundamentally limiting the representational power of the framework [25]. As a result, the extracted point features, albeit being rotation invariant, are not robust and general when being applied to unseen 3D scans with noise and different point densities.
In this paper, we aim to design a new neural architecture, which is able to learn descriptive local features and general-ize well to unseen scenarios. This network clearly satisﬁes three key properties: 1) It is rotation invariant. Particularly, it learns consistent local features from 3D scans with differ-ent rotation angles; 2) It is descriptive. In essence, it pre-serves the prominent local patterns despite the noise, possi-ble surface incompleteness, or different point densities; 3)
It does not include any handcrafted features. Instead, it only consists of multiple point transformations and simple neu-ral layers coupled with true end-to-end optimization. This allows the learned descriptor to be extremely representative and general for complex real-world 3D surfaces.
Our network, named SpinNet, mainly consists of two modules, 1) a Spatial Point Transformer1, which ex-plicitly transforms the input 3D scans into a carefully de-signed cylindrical space, driving the transformed scans to be
SO(2) equivariant, whilst retaining point local information; 2) a Neural Feature Extractor, which leverages powerful point-based and convolutional neural layers to learn repre-sentative and general local patterns.
The Spatial Point Transformer ﬁrstly aligns the input 3D surface by a reference axis, eliminating the rotational variance along the Z-axis. This is followed by a coordinate transformation over the XY-plane with the aid of spherical voxelization, further removing the rotation variance of each spherical voxel. Lastly, the transformed local surface is for-mulated as a simple yet novel 3D cylindrical volume, which is amenable to consumption by the subsequent point-based and convolutional neural layers. The Neural Feature Ex-tractor ﬁrstly uses simple point-based MLPs to extract a unique signature for each voxel within the cylindrical vol-ume, generating an initial set of cylindrical feature maps.
These maps are further fed into a series of novel 3D cylin-drical convolutional layers, which fully exploit the rich spa-tial and contextual information and generate a compact and representative feature vector for the input 3D surface.
These two modules enable our SpinNet to learn remark-ably robust and general local features for accurate 3D point cloud registration. It achieves state-of-the-art performance both on the indoor 3DMatch [65] dataset and the outdoor
ETH [46] dataset. Notably, it shows superior generalization ability across unseen scenarios. As shown in Figure 1, being trained only on the 3DMatch dataset, the learned descriptor of our SpinNet can achieve an average recall score of 92.8% on the unseen outdoor ETH dataset for feature matching, signiﬁcantly surpassing the state of the art by nearly 13%. 1This is different from the Transformer for natural language processing.
Overall, our contributions are three-fold:
• We propose a new neural feature learner for 3D surface matching. It is rotation invariant, representative, and has superior generalization ability across unseen scenarios.
• By formulating the transformed 3D surface into a cylin-drical volume, we introduce a powerful 3D cylindrical convolution to learn rich and general features.
• We conduct extensive experiments and ablation stud-ies, demonstrating the remarkable generalization of our method and providing the intuition behind our choices. 2.