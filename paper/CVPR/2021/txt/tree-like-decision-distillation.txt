Abstract
Knowledge distillation pursues a diminutive yet well-behaved student network by harnessing the knowledge learned by a cumbersome teacher model. Prior methods achieve this by making the student imitate shallow behav-iors, such as soft targets, features, or attention, of the teacher.
In this paper, we argue that what really matters for distillation is the intrinsic problem-solving process cap-tured by the teacher. By dissecting the decision process in a layer-wise manner, we found that the decision-making pro-cedure in the teacher model is conducted in a coarse-to-ﬁne manner, where coarse-grained discrimination (e.g., animal vs vehicle) is attained in early layers, and ﬁne-grained dis-crimination (e.g., dog vs cat, car vs truck) in latter layers.
Motivated by this observation, we propose a new distillation method, dubbed as Tree-like Decision Distillation (TDD), to endow the student with the same problem-solving mech-anism as that of the teacher. Extensive experiments demon-strated that TDD yields competitive performance compared to state of the arts. More importantly, it enjoys better inter-pretability due to its interpretable decision distillation in-stead of dark knowledge distillation. 1.

Introduction
Knowledge Distillation (KD), whose ultimate goal is to craft a lightweight student model with the aid of a capa-ble yet cumbersome teacher model, has become one of the most ﬂourishing research topic in deep learning since the pi-oneering work of [8]. Its success is largely attributed to the dark knowledge learned by the over-parameterized teacher model, which is exploited to regularize the learning of a low-capacity student model without sacriﬁcing too much performance.
*Equal contribution
†Corresponding author
ResNet56
CONV
GROUP 1
Decision Space
Decision Process 2-way classification 2-way: 80.1% 10-way: 34.0% 4-way: 73.0% 10-way: 50.2% 10-way: 94.9%
GROUP 2 4-way classification
GROUP 3 output 10-way classification r a
C e n a l p r i
A p i h
S k c u r
T g o r
F d r i
B g o
D t a
C e s r o
H r e e
D
Figure 1. An illustrative diagram of the coarse-to-ﬁne decision process on CIFAR10. After the ﬁrst group of ResNet56, vehicles and animals can be distinguished with accuracy of 80%, while the 10-way classiﬁcation here reaches only 34%.
Although remarkable progress has been made in the last several years, most existing KD methods are still stuck in the stage of mimicking shallow behaviors, such as soft tar-gets [8], features [21], or attentions [41], of the teacher. Few of them attempt to ﬁgure out the problem-solving process underlying the pre-trained teacher model, which leaves the student produced by KD entirely a black box. Moreover, as some behaviors are partially dependent on the network ar-chitecture, directly copying these behaviors yields inferior performance especially when the architecture gap between the teacher and the student is signiﬁcant [16, 31, 19].
In this paper, we argue that what really matters for dis-tillation is the intrinsic problem-solving process captured by the teacher. By dissecting the decision process in the teacher model in a layer-wise manner, we found that al-though most multi-layered neural networks are designed to make the classiﬁcation predictions in the last classiﬁcation layer, the decision-making procedure is in fact learned to be conducted in a coarse-to-ﬁne way and distributed over many layers, as shown in Figure 1. The early layers tend to capture the salient visual cues, and thus be capable of distinguishing between categories that are visually diverse enough, e.g., the human-made categories (vehicles) versus 13488
the natural categories (animals) in Figure 1. The latter lay-ers, on the other hand, are amenable to make the ﬁnal clas-siﬁcation and thus able to conduct more ﬁne-grained recog-nition. The overall decision process in the deep network is executed progressively in such an increasingly coarse-to-ﬁne manner, rather than being concentrated on the very last classiﬁcation layer.
We propose a new method, dubbed as Tree-like Deci-sion Distillation (TDD), to endow the student with the same problem-solving process as that of the teacher. TDD ﬁrst empirically analyzes the decisions made in different layers of the teacher model, and then imposes the same decision constraints on the student model, which impels the student to master the same problem-solving solution. As the stu-dent in TDD does not need to explore the solution ﬂow again during the training phase, it converges much faster and achieves higher ﬁnal accuracy. Furthermore, as TDD explores the underlying decision process rather than simply imitating some dark knowledge, it possesses better inter-pretability than prior methods.
Our main contributions are therefore summarized as fol-(1) we empirically demonstrated that the decision lows: process underlying deep networks is executed in a coarse-to-ﬁne manner which is somewhat similar to that of a deci-sion tree; (2) we propose TDD to distill the decision process from teacher into the student, which relieves the student of the burden of searching in the solution space; (3) extensive experiments are conducted to demonstrate that TDD yields competitive accuracy to the state of the arts. Meanwhile, it enjoys better interpretability. 2.