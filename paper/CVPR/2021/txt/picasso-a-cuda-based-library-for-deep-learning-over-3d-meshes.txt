Abstract
We present Picasso, a CUDA-based library comprising novel modules for deep learning over complex real-world 3D meshes. Hierarchical neural architectures have proved effective in multi-scale feature extraction which signiﬁes the need for fast mesh decimation. However, existing meth-ods rely on CPU-based implementations to obtain multi-resolution meshes. We design GPU-accelerated mesh deci-mation to facilitate network resolution reduction efﬁciently on-the-ﬂy. Pooling and unpooling modules are deﬁned on the vertex clusters gathered during decimation. For fea-ture learning over meshes, Picasso contains three types of novel convolutions namely, facet2vertex, vertex2facet, and facet2facet convolution. Hence, it treats a mesh as a geo-metric structure comprising vertices and facets, rather than a spatial graph with edges as previous methods do. Picasso also incorporates a fuzzy mechanism in its ﬁlters for robust-ness to mesh sampling (vertex density). It exploits Gaussian mixtures to deﬁne fuzzy coefﬁcients for the facet2vertex con-volution, and barycentric interpolation to deﬁne the coefﬁ-cients for the remaining two convolutions. In this release, we demonstrate the effectiveness of the proposed modules with competitive segmentation results on S3DIS. The library will be made public through github. 1.

Introduction
Data in computer vision vary commonly from homoge-neous format in 2D projective space (e.g. images, videos) to heterogeneous format in 3D Euclidean space (e.g. point clouds, meshes). The success of convolutional feature learning on homogeneous data [21, 28, 37, 38, 46, 47, 50, 55] has sparked research interest in geometric deep learn-ing [6, 7, 14, 26, 54], which aims for equally effective fea-ture learning on heterogeneous data. Due to the rise of au-tonomous driving and robotics, 3D deep learning has now become an important branch of the geometric research di-rection. Compared to 3D point clouds, 3D meshes convey richer geometric information about the object surface and topology. Yet, the heterogeneous facet shapes and sizes combined with unstructured vertex locations make its adap-tion to deep learning more difﬁcult as compared to point clouds. This is why most approaches address real-world 3D scene understanding via convolutions on point clouds
[31, 32, 33, 35, 43, 44]. However, point clouds still lack in preserving the structural details that are easily represented by meshes.
There are a few works that learn features from meshes, but they are largely constrained to shape analysis on small synthetic models [5, 20, 39, 41, 45, 68]. These methods either apply convolutions throughout the network to a sin-gle mesh resolution (the input), or exploit inefﬁcient CPU-based algorithms to decimate the mesh [16, 17, 51, 71].
However, non-hierarchical network conﬁgurations and slow network coarsening are both problematic while dealing with real-world meshes because of their large-scale nature. This calls for mesh simpliﬁcation methods that are fast and amenable to deep learning for the real-world applications.
We present a GPU-accelerated mesh simpliﬁcation algo-rithm to facilitate the exploration of hierarchical architec-tures on meshes. The proposed method is not only fast in decimating small-scale watertight meshes from CAD mod-elling [4, 9, 11, 36], but is also efﬁcient in simplifying large-scale unstructured real-wold meshes [2, 8, 12, 22]. We per-form all computations in parallel on GPU, except for the grouping of vertex pairs to be contracted. Meanwhile, to in-crease its compatibility with modern deep learning modules such as normalization [3, 24, 60, 66], we contract vertex clusters by controlling the desired vertex size of the deci-mated mesh. This also advances mesh-based modules to be exploited in conjunction with the various point cloud based modules [52]. Our algorithm is able to reduce the number of mesh vertices by half in each iteration. Figure 1 com-pares the runtime of our method with two well-founded dec-imation methods, VC [51] and QEM [17]. Notice that our method is 30× faster than QEM. During the simpliﬁcation, we record all vertex clustering information into a 1-D ten-sor. Based on this tensor, we also deﬁne max, average and weighted poolings, as well as unpooling.
Earlier attempts for convolution on meshes [5, 39, 41] explored local patch operators in hand-crafted coordinate systems. The development of spatial graph convolutions has led recent methods [20, 45, 52] to predominantly consider (triangular) mesh as a special graph and convolve features of each vertex from their geodesic k-ring neighborhood. 13854
(a) The input mesh (b) VC: 177 ms (c) QEM: 2160 ms (d) Ours: 65 ms
Figure 1: Runtime comparison of mesh simpliﬁcation. The input mesh consists of 115, 114 vertices and 231, 293 facets.
Different methods simplify it to similar mesh sizes. The number of vertices and facets in the decimated meshs are 41, 483 and 85, 424 for vertex clustering (VC) [51], 41, 133 and 83, 050 for the quadric error metrics (QEM) [16, 17]; and 41, 449 and 83, 051 for our GPU-accelerated algorithm. The runtime of VC, QEM, and our method are respectively 177 ms, 2160 ms, and 65 ms. For VC and QEM, we utilize their popular implementations in Open3D [71]. For better visualization, we show wireframes of the meshes only. Best viewed in color and enlarged.
In contrast, we study mesh as a set of vertices and facets, following its natural geometric structure. To learn features of each vertex, we aggregate their context information from the adjacent facets. We refer to the resulting operation as facet2vertex convolution. Lei et al. [32] showed that fuzzy mechanism makes the convolutional network robust to point density variation. Hence, we further exploit fuzzy coefﬁ-cients in the facet2vertex convolution. Due to the fact that facet normals are distributed strictly on the surface of a unit sphere, i.e. S3 = {x ∈ R3 : kxk = 1}, we associate learn-able ﬁlter weights to Gaussian mixtures deﬁned on S3. The parameters of the Gaussian components can optionally be kept ﬁxed or trainable within the network in our library. On the other hand, to learn features of the mesh facets, we intro-duce vertex2facet and facet2facet convolutions. The former propagates features from the vertices of a facet to the facet itself, while the latter is applied when facets of the input mesh are rendered with textures. We incorporate fuzziness into these two convolutions using barycentric interpolation.
The three proposed convolutions altogether enable ﬂexible vertex-wise and facet-wise feature learning on the mesh.
We provide CUDA implementations for all the above mentioned modules and organize them into a self-contained library, named Picasso1, to facilitate deep learning over the unstructured real-world 3D meshes. We note that meshes and point clouds are tightly bonded together, and it is more desirable to extract features from the two data modali-ties cooperatively rather than individually or competitively.
DCM-Net [52] also validates this argument. For this rea-son, we additionally incorporate all the point cloud modules from Lei et al. [32, 33] in our library (with author permis-sion). In this maiden release of our library, we demonstrate promise of its proposed modules with competitive segmen-tation results on S3DIS [2]. The segmentation network is 1Paying homage to Pablo Picasso for cubism in paintings. abbreviated as PicassoNet for consistency. We summarize the main contributions of our work below:
• We present a fast GPU-accelerated 3D mesh decima-tion technique to reduce mesh resolution on-the-ﬂy.
A public implementation with complementary CUDA-based pooling and unpooling operations is provided.
• We propose three novel convolution modules, i.e. facet2vertex, vertex2facet, facet2facet, to alternatively learn vertex-wise and facet-wise features on a mesh.
Diverging from existing methods, we do not rely on re-strictive treatment of mesh as an undirected graph with edges. Instead, it’s a geometric structure composed of vertices and facets for our modules, which is also a more conducive representation for the digital devices.
• With this paper, we release Picasso — a self-contained library for deep learning over unstructured real-world 3D meshes, along with synthetic watertight meshes.
The provided anonymous github link will be made public for the broader research community. 2.