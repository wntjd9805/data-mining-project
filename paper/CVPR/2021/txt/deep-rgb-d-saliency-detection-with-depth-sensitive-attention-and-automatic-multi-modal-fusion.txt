Abstract
Depth Decomposition
RGB-D salient object detection (SOD) is usually formu-lated as a problem of classiﬁcation or regression over two modalities, i.e., RGB and depth. Hence, effective RGB-D feature modeling and multi-modal feature fusion both play a vital role in RGB-D SOD. In this paper, we pro-pose a depth-sensitive RGB feature modeling scheme us-In ing the depth-wise geometric prior of salient objects. principle, the feature modeling scheme is carried out in a depth-sensitive attention module, which leads to the RGB feature enhancement as well as the background distraction reduction by capturing the depth geometry prior. More-over, to perform effective multi-modal feature fusion, we further present an automatic architecture search approach for RGB-D SOD, which does well in ﬁnding out a feasi-ble architecture from our specially designed multi-modal multi-scale search space. Extensive experiments on seven standard benchmarks demonstrate the effectiveness of the proposed approach against the state-of-the-art. 1.

Introduction
Recent years have witnessed a great development of
RGB-D salient object detection (SOD) due to its diverse applications, e.g., image retrieval [25, 36], video segmen-tation [20, 55], person re-identiﬁcation [62], visual track-ing [27, 41]. With the multi-modal input (i.e., RGB and depth channels), RGB-D SOD aims to localize and segment the visually salient regions in a scene, and is typically cast as an image-to-mask mapping problem within an end-to-end deep learning pipeline [22, 23, 45, 49].
In RGB-D SOD, depth maps, which provide useful cues such as spatial structure, 3D layout, and object boundary, are important complementary information to RGB chan-nels. For the sake of effective learning, there are usually two key issues to solve for RGB-D SOD: 1) how to fully exploit the rich depth geometry information for saliency analysis,
*Corresponding Author
Depth
Salient Object
RGB-D Visualization
RGB
Depth-Sensitive Attention
Figure 1. Left: Salient objects are often distributed within different depth intervals. Right: We decompose the raw depth map into multiple regions and extract the depth-sensitive RGB features. and 2) how to carry out the multi-modal feature fusion ef-fectively between RGB and depth features. In this paper, we focus on building a depth-sensitive SOD model that is capa-ble of learning the RGB-D feature interaction architecture automatically.
In the recent literature, RGB-D SOD methods usually treat the depth channel as an auxiliary input channel, which is directly fed into a convolutional neural network (CNN) for feature extraction [7, 21, 31, 43, 59]. As a result, they are incapable of well utilizing the depth prior knowledge to capture the corresponding geometric layouts of salient objects. As shown in Fig. 1, salient objects are often dis-tributed within several particular depth intervals, and thus can be roughly detected by regularly sliding the depth in-terval window.
Inspired by this observation, we have an intuitive idea that we can extract RGB features w.r.t. depth for effectively capturing the depth-wise geometric prior on salient objects while reducing the background distraction (e.g. cluttered objects or similar texture). With this motiva-tion, we propose to decompose the raw depth map into mul-tiple regions, and each region contains a set of pixels from the same depth interval. Then, we propose a depth-sensitive attention module (DSAM) to perform RGB feature extrac-tion in different regions, thereby leading to the RGB feature enhancement with depth-wise geometric prior.
Furthermore, designing an effective feature interaction architecture between RGB and depth branches is crucial 1407
for multi-modal feature fusion in RGB-D SOD. In gen-eral, the existing literature relies heavily on human ex-pertise knowledge through enormous trial and error, e.g.,
ﬂow ladder module [59] and ﬂuid pyramid integration mod-ule [61]. Moreover, the multi-source information on RGB and depth channels is extremely heterogeneous, making the feature fusion design rather difﬁcult and heuristic. Based on this observation, we leverage neural architecture search (NAS) [3, 13, 37] to automatically explore an effective fea-ture fusion module. However, simply porting existing NAS ideas from image classiﬁcation/segmentation to RGB-D
SOD would not sufﬁce, as the task requires nested combi-nations of multi-modal multi-scale features. To this end, we construct a new search space tailored for the multi-modal feature fusion across multiple scales for RGB-D SOD. As a result, the automatically-found feature fusion architecture equipped with the commonly used backbone VGG-19 [53] achieves the state-of-the-art performance.
Our contributions can be summarized as follows:
• We propose a depth-sensitive attention module to ex-plicitly eliminate the background distraction and en-hance the RGB features by depth prior knowledge.
• We design a new search space tailored for the hetero-geneous feature fusion in RGB-D SOD and present the
ﬁrst attempt to introduce NAS for RGB-D SOD.
• Finally, we conduct extensive experiments on seven benchmarks, which demonstrates that our method out-performs other state-of-the-art approaches. 2.