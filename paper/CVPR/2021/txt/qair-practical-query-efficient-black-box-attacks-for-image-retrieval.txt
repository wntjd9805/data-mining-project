Abstract
We study the query-based attack against image retrieval to evaluate its robustness against adversarial examples un-der the black-box setting, where the adversary only has query access to the top-k ranked unlabeled images from the database. Compared with query attacks in image clas-siﬁcation, which produce adversaries according to the re-turned labels or conﬁdence score, the challenge becomes even more prominent due to the difﬁculty in quantifying the attack effectiveness on the partial retrieved list. In this pa-per, we make the ﬁrst attempt in Query-based Attack against
Image Retrieval (QAIR), to completely subvert the top-k re-trieval results. Speciﬁcally, a new relevance-based loss is designed to quantify the attack effects by measuring the set similarity on the top-k retrieval results before and after at-tacks and guide the gradient optimization. To further boost the attack efﬁciency, a recursive model stealing method is proposed to acquire transferable priors on the target model and generate the prior-guided gradients. Comprehensive experiments show that the proposed attack achieves a high attack success rate with few queries against the image re-trieval systems under the black-box setting. The attack eval-uations on the real-world visual search engine show that it successfully deceives a commercial system such as Bing Vi-sual Search with 98% attack success rate by only 33 queries on average. 1.

Introduction
Despite of its impressive performance in many tasks such as image classiﬁcation [17], object detection [7] and image retrieval (IR) [37], deep neural network (DNN) has been shown to be vulnerable to adversarial examples that can trigger the misbehavior with human-imperceptible per-turbations [14, 10, 13]. Such vulnerability has raised great concerns about the robustness and real-world deployment of
DNNs for image retrieval [24, 39] and object detection [7],
*Corresponding author.
White-box
Target model
Transfer-based Query-based
Query access
Training data less information
FGSM, MIM
FGSM transfer
BA, HSJA, RGF
Cat, 0.9999
UAP, ODFA
DeepMisR
Ours
!
Cls
IR
"
Original
Queries
A t t a c k w i t h
Q u e r y 1 2
+
+
…
…
N
+
T a r g e t
I
R s y s t e m
Initialize a perturbation 
Update according to the last state
…
Update according to the last state
Figure 1. (a) (Left)Taxonomy of adversarial attacks. Different from existing attacks for IR, our attack is applicable to real-world scenarios since it only needs query access to the target model. (Right) The output of image classiﬁcation(Cls) is a label or conﬁ-dence score while it is a list of unlabeled images in IR. (b) Demonstration of the query-based black-box attack on IR.
Given a target model, the attackers use queries to update and gen-erate adversarial perturbations. et al.. For example, in digital right management, the origi-nal graphic designs are protected by checking if there exists a same design in the top-k similar ones retrieved from the whole graphic design database. By adding adversarial per-turbations on protected designs, attackers can deceive the target IR system into retrieving some irrelevant images for evading the censorship of professional monitors. There-fore, it is crucial to develop a practical robustness evalua-tion technology to explore the vulnerability of IR systems against adversarial attacks, and then facilitate the develop-ment of the corresponding countermeasures.
A general idea of adversarial attack is to generate ad-versarial examples with human-imperceptible perturbations along the gradient direction by maximizing a certain loss function, e.g., a classiﬁcation loss [14, 11]. However, as shown in Fig. 1, the IR system produces a list of images for a query input. This makes it hard to deﬁne the objective 3330        
function for indicating the attack effectiveness only with the retrieved list. Under this circumstance, the gradients can hardly be estimated for deriving effective attacks. Though there exist some decision-based methods [3, 5] in attacking classiﬁcation models which only rely on the ﬁnal decision to indicate whether the attack succeeds, they usually require a signiﬁcant attack cost by a tremendous number of queries to cross the decision boundary via greedy search [10].
Furthermore, in adversarial attacks, the gradients for guiding the attack process are usually calculated based on the knowledge of the target model, e.g., the model struc-ture and parameters. Existing studies on adversarial attacks against IR systems mainly focus on the white-box attacks in which attackers are assumed to have complete knowledge about the target model [41, 24, 36, 33], so the gradients can be directly acquired. However, the underlying white-box assumption does not hold in reality. Some studies try to use an approximate gradient instead for crafting adversarial examples [27, 36]. The approximate gradient could be ei-ther the gradient of a surrogate model (a.k.a. transfer-based attacks) or numerically estimated by methods (a.k.a. query-based attacks) such as the zero-order optimization [6]. The transfer-based methods attack the target model by leverag-ing adversarial examples generated against a white-box sub-stitute model [36, 24], requiring training data that are usu-ally protected. Besides, their attack success rate is still un-satisfactory due to the lack of adaptation procedure when the generated adversarial examples fail to attack the target model [10]. The query-based attacks produce the gradient with methods such as ﬁnite difference [6, 2], random gradi-ent estimation [27]. However, they are not efﬁcient enough due to the lack of knowledge about the target model.
To address the aforementioned challenges, we propose the ﬁrst attempt on practical Query-efﬁcient Attack against
Image Retrieval (QAIR) under the black-box setting. First of all, we formulate the problem of black-box attacks on IR systems, and propose a new relevance-based loss to quan-tify the attack effects on target models with probabilistic in-terpretation. In this way, the structural output of IR systems can help to guide the gradient estimation during attacks. Be-sides, considering the fact that retrieved images are ranked based on similarities with the input image, which can gen-erate plenty of labeled triplets, a recursive model stealing method is constructed on the ranking list to acquire transfer-based priors and generate the prior-guided gradients. Ex-tensive experiments show that the proposed method can achieve a high attack success rate against IR systems with a remarkable Recall@K drop. We also evaluate our attack efﬁcacy on the real visual search system1, which demon-strates its practicability in real-world scenarios.
Our main contributions can be summarized as follows:
• We formulate the problem of black-box attacks against image retrieval systems, and propose a new relevance-based loss to quantify the attack effects.
• We develop a recursive model stealing method to ac-quire transfer-based priors of target model for boosting the query-attack efﬁciency.
• We demonstrate the efﬁcacy of our attack through ex-tensive experiments on simulated environments and real-world commercial systems. 2.