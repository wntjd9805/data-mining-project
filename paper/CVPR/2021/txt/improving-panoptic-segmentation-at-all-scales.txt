Abstract
Crop-based training strategies decouple training reso-lution from GPU memory consumption, allowing the use of large-capacity panoptic segmentation networks on multi-megapixel images. Using crops, however, can introduce a bias towards truncating or missing large objects. To ad-dress this, we propose a novel crop-aware bounding box regression loss (CABB loss), which promotes predictions to be consistent with the visible parts of the cropped objects, while not over-penalizing them for extending outside of the crop. We further introduce a novel data sampling and aug-mentation strategy which improves generalization across scales by counteracting the imbalanced distribution of ob-ject sizes. Combining these two contributions with a care-fully designed, top-down panoptic segmentation architec-ture, we obtain new state-of-the-art results on the challeng-ing Mapillary Vistas (MVD), Indian Driving and Cityscapes datasets, surpassing the previously best approach on MVD by +4.5% PQ and +5.2% mAP. 1.

Introduction
Panoptic segmentation [16] is the task of generating per-pixel, semantic labels for an image, together with object-speciﬁc segmentation masks.
It is thus a combination of semantic segmentation and instance segmentation, i.e. two long-standing tasks in computer vision that have been tradi-tionally tackled separately. Due to its importance for tasks like autonomous driving or scene understanding it has re-cently attracted a lot of interest in the research community.
The majority of deep-learning based panoptic segmenta-tion architectures [15, 23, 17, 29, 21] proposed a combina-tion of specialized segmentation branches – one for con-ventional semantic segmentation and another one for in-stance segmentation – followed by a combination strategy to generate a ﬁnal panoptic segmentation result. Instance segmentation branches in top-down panoptic architectures are dominantly designed on top of Mask R-CNN [12], i.e. a segmentation extension of Faster R-CNN [24] generating state-of-the-art mask predictions for given bounding boxes.
In contrast and more recently, bottom-up panoptic architec-tures [6, 26] have emerged but still lag behind in terms of instance segmentation performance.
Panoptic segmentation networks are typically solving multiple tasks (object detection, instance segmentation and semantic segmentation), and are trained on batches of full-sized images. However, with increasing complexity of tasks and growing capacity of the network backbone, full-image training is quickly inhibited by available GPU memory, de-spite availability of memory-saving strategies during train-ing like [25, 20, 11, 14]. Obvious mitigation strategies include a reduction of training batch size, downsizing of high-resolution training images, or building on backbones with lower capacity. These workarounds unfortunately in-troduce other limitations: i) Small batch sizes can lead to higher variance in the gradients which will reduce the ef-fectiveness of Batch Normalization [13] and consequently ii) Reducing the the performance of the resulting model. image resolution leads to a loss of ﬁne structures which are known to strongly correlate with objects belonging to the 7302
long tail of the label distribution. Downsampling the images is consequently amplifying already existing performance is-sues on small and usually underrepresented classes. iii)
A number of recent works [28, 5, 31] have shown that larger backbones with sophisticated strategies of maintain-ing high-resolution features are boosting panoptic segmen-tation results in comparison to those with reduced capacity.
A possible strategy to overcome the aforementioned is-sues is to move from full-image-based training to crop-based training. This was successfully used for conventional semantic segmentation [25, 3, 2], which is however an eas-ier problem as the task is limited to a per-pixel classiﬁcation problem. By ﬁxing a certain crop size the details of ﬁne structures can be preserved, and at a given memory bud-get, multiple crops can be stacked to form reasonably sized training batches. For more complex tasks like panoptic seg-mentation, the simple cropping strategy also affects the per-formance on object detection and consequently on instance segmentation. In particular, extracting ﬁxed-size crops from images during training introduces a bias towards truncating large objects, with the likely consequence of underestimat-ing their actual bounding box sizes during inference on full images (see, e.g. Fig. 1 left). Indeed, Fig. 2 (left) shows that the distribution of box sizes during crop-based training on the high-resolution Mapillary Vistas [22] dataset does not match with the one derived from full-image training data.
In addition, Fig. 2 (right) shows that large objects (based on
# pixels) are drastically underrepresented, which may lead to over-ﬁtting and thus further harming generalization.
In this paper we overcome these issue by introducing two novel contributions: 1) A crop-based training strategy ex-ploiting a crop-aware loss function (CABB) to address the problem of cropping large objects, and 2) Instance scale-uniform (ISUS) sampling as data augmentation strategy to combat the imbalance of object scales in the training data.
Our solution enjoys all beneﬁts from crop-based training as discussed above.
In addition, our crop-aware loss incen-tivizes the model to predict bounding boxes to be consistent with the visible parts of cropped objects, while not over-penalizing predictions outside of the crop. The underlying intuition is simple: Even if an object bounding box size was modiﬁed through cropping, the actual object bound-ing boxes may be larger than what is visible to the net-work during training. By not penalizing hypothetical pre-dictions beyond the visible area of a crop but still within their actual sizes, we can better model the bounding box size distribution given by the original training data. With
ISUS we introduce an effective data augmentation strategy to improve feature-pyramid like representations as used for object detection at multiple scales. It aims at more evenly distributing supervision of object instances during training across pyramid scales, leading to improved recognition per-In formance of instances at all scales during inference. the experimental analyses we ﬁnd that our crop-aware loss function is particularly effective on high-resolution images as available in the challenging Mapillary Vistas [22], Indian
Driving [27], or Cityscapes [8] datasets.
Contributions. We summarize our contributions to the panoptic segmentation research community as follows.
• We introduce a novel, crop-aware training loss appli-cable to improving bounding box detection in panop-tic segmentation networks when training them in a crop-based way. At negligible computational over-head (∼10ms per batch) we show how our new loss addresses issues of crop-based training, considerably improving the performance on disproportionately of-ten truncated bounding boxes.
• We describe a novel Instance Scale-Uniform Sampling approach to smooth the distribution of object sizes ob-served by a network at training time, improving its generalization across scales.
• We signiﬁcantly push the state-of-the-art results on the high-resolution Mapillary Vistas dataset, improving on multiple evaluation metrics like Panoptic Quality [16] (+4.5%) and mean average precision (mAP) for mask segmentation (+5.2%). We also obtain remarkable per-formance gains on IDD and Cityscapes, improving PQ by +0.6% and mAP by +4.1% and +1.5%, respectively. 2. Technical Contributions
In this section we present our main methodological con-In particular, in Sec. 2.1 we describe a novel tributions.
Instance-Scale Uniform Sampling (ISUS) approach aimed at reducing the object scale imbalance inherent in high-resolution panoptic datasets. Sections 2.2, 2.3 and 2.4 de-scribe the Crop-Aware Bounding Box (CABB) loss, which we propose as a mitigation to the bias imposed by crop-based training on the detection of large objects. 2.1. Instance Scale Uniform Sampling (ISUS)
Most top-down panoptic segmentation networks build on top of backbones that produce a “pyramid” of features at multiple scales. At training time, some heuristic rule [15] is applied to split the ground truth instances across the available scales, such that the network is trained to detect small objects using high-resolution features and large ob-jects using low-resolution features. By sharing the parame-ters of the prediction modules (e.g. the RPN and ROI heads of [23]) across all scales, the network is incentivized to learn scale-invariant features. When dealing with high-resolution images, however, this approach encounters two major is-sues: i) the range of object scales can greatly exceed the range of scales available in the feature pyramid, and ii) the 7303
Cropped vs. original IoU
U o
I 1 0.6 0.2
Number of objects by scale
·105 s t c e j b o
# 4 2 0 25 – 0 75 – 25 90 – 75 99 – 90 100 – 99
≤ 16 32 64 128 scale (px) 256 ≥ 512
Figure 2: Left: average intersection over union of cropped bounding boxes w.r.t. their original extent, computed using the
Mapillary Vistas training settings in Sec. 4.1. Right: distribution of object scales in the Mapillary Vistas training set. distribution of object scales is markedly non-uniform (see
Fig. 2). While (i) can be partially addressed by adding more feature scales, at the cost of increased memory and compu-tation, (ii) will lead to a strong imbalance in the amount of supervision received by each level of the feature pyramid.
In order to mitigate this imbalance, we propose an ex-tension to the Class-Uniform Sampling (CUS) approach in-troduced in [25] we coin Instance Scale-Uniform Sampling (ISUS). The standard CUS data preparation process follows four steps: 1) sample a semantic class with uniform proba-bility; 2) load an image that contains that class and re-scale it such that its shortest side matches a predeﬁned size s0; 3) apply any data augmentation (e.g. ﬂipping, random scal-ing); and 4) produce a random crop from an area of the im-age where the selected class is visible. In ISUS, we follow the same steps as in CUS, except that the scale augmenta-tion procedure is made instance-aware. In particular, when a “thing” class is selected in step 1 and after completing step 2, we also sample a random instance of that class from the image and a random feature pyramid level. Then, in step 3 we compute a scaling factor σ such that the selected in-stance will be assigned to the selected level according to the heuristic adopted by the network being trained. In order to avoid excessively large or small scale factors, we clamp σ to a limited range rth. Conversely, when a “stuff” class is se-lected in step 1, we follow the standard scale augmentation procedure, i.e. uniformly sample σ from a range rst. In the long run, ISUS will have the effect of smoothing out the ob-ject scale distribution, providing more uniform supervision across all scales. 2.2. Bounding box regression
Most top-down panoptic segmentation approaches en-code object bounding boxes in terms of offsets with respect to a set of reference boxes [17, 23, 21]. These reference boxes can be ﬁxed, e.g. the “anchors” in the region proposal stage, or be the output of a different network section, e.g. the “proposals” in the detection stage. The goal of a net-work component that predicts bounding boxes is to regress these offset values given the input image (or derived fea-tures thereof).
A ground-truth bounding box G is encoded in terms of a center cG ∈ R2 and dimensions dG ∈ R2. Each ground-truth box is assigned a reference (or anchor) bounding box A with center cA ∈ R2 and dimensions dA ∈ R2. The ground truth for the training procedure is then encoded in relative terms and speciﬁcally given by ∆G = (δG, ωG) where
δG = cG − cA dA
∈ R2 and
ωG = dG dA
∈ R2 .
Here and later, we implicitly assume for notational conve-nience that operations and functions applied to vectors work element-wise unless otherwise stated. We will also use the notation ⊖ to denote the operation above that returns ∆G given bounding boxes G and A, i.e. ∆G = G ⊖ A.
Similarly, given an anchor bounding box A and ∆P = (δP, ωP), we can recover the predicted bounding box P with center cP and dimensions dP as cP = cA + δPdA and dP = ωPdA .
Standard bounding box loss [24]. To train the network, the following per-box loss is minimized over the training dataset:
LBB(∆P; ∆G) = kℓβ(δP−δG)+ℓβ(log ωP−log ωG)k1 , (1) where k · k1 is the 1-norm and ℓβ denotes the Huber (a.k.a. smooth-L1) norm with parameter β > 0, i.e.
ℓβ(z) = 1 2β z2
|z| − β 2 (
|z| ≤ β otherwise, and |z| gives the absolute value of z. 2.3. Crop Aware Bounding Box (CABB)
In a standard crop-based training, a ground-truth bound-ing box G from the original image that overlaps with the cropping area C is typically cropped yielding a new bound-ing box denoted by G|C. 1 Accordingly, the actual ground-truth ∆G that is used in the loss (1) is the result of ∆G = 1When masks are available like in instance or panoptic segmentation, the cropping operation is performed at the mask level and the bounding box is recomputed a posteriori. We implicitly assume that this is the case if a ground-truth mask is available for G. 7304
a valid prediction. To disambiguate, our new loss favours the solution closer to the actual prediction from the network in order to enforce a smoother training dynamic. Since the ground-truth box that is typically adopted for the standard loss in (1) belongs to the feasible set of the minimization in our new loss, we have that LCABB lower bounds LBB. 2.4. Computational Aspects
This section focuses on the computational aspects of our new loss. In particular, we will address the problem of eval-uating it by solving the internal minimization as well as computing the gradient.
The minimization problem that is nested into our new loss has no straightforward solution, since it is neither con-vex nor quasi-convex and in general, local, non-global so-lutions might exist. Its feasible set is convex in ∆ = (δ, ω) since it can be written in terms of linear equalities and in-equalities. Each dimension gives rise to an independent set of constraints and since also the objective function is separable with respect to dimension-speciﬁc variables, we have that the whole minimization problem can be separated into two independent minimization problems involving only dimension-speciﬁc variables.
Feasible set. Assume without loss of generality that the cropping area C is a box with top-left coordinate (0, 0) and bottom-right coordinate dC ∈ R2. Then the feasible set of each dimension-speciﬁc minimization problem can be writ-ten as:
• δ − ω 2 ≤ − cA dA if cG ≤ dG 2 else δ − ω 2 = δG − ωG 2 and dA
• δ + ω 2 ≥ dC−cA if cG ≥ dC − dG 2 = δG + ωG 2 , where we dropped the boldface style from the vector-valued variables to emphasize that the constraint is speciﬁed for a single dimension. 2 else δ + ω
Optimization problem. We will now enumerate the dif-ferent cases characterizing the feasible set and for each of them we will provide the dimension-speciﬁc optimization problem that should be solved. Akin to the feasible set above, all variables involved from here on refer implicitly to a single dimension.
• If dG 2 < cG < dC− dG 2 then ∆⋆ = (δG, ωG) is the solution to the minimization problem in (2) for the dimension under consideration, since the feasible set is singleton in this case.
• If cG > dG 2 and cG ≥ dC − dG problem in the variable ω of the form 2 , we obtain an optimization min
ω
ℓβ
ω − ˆω 2 (cid:19) (cid:18) s.t. ω ≥ b1 − a1 ,
+ ℓβ(log(ω) − log(ωP)) (O1) 7305
Figure 3: Example of Crop-Aware Bounding Boxes (CABB). We show 4 ground-truth boxes, three of which fall partially outside the crop area. The corresponding set
ρ(G, C), a.k.a. CABB, consists of all rectangular bounding boxes that can be formed by moving the white-bordered corners within the feasible areas (depicted in blue). Note that the areas extend to inﬁnity but are truncated here.
G|C ⊖ A. Training with this modiﬁed ground-truth, however, poses some issues, namely a bias towards cutting or missing big objects at inference time (see, e.g., Fig. 1 and 6).
The solution we propose in this work consists in relax-ing the notion of ground-truth bounding box G into a set of ground-truth boxes that coincide with G|C after the cropping operation. We denote by ρ(G, C) the function that computes this set for given ground-truth box G and cropping area C, i.e.
ρ(G, C) = {X ∈ B : X|C = G|C} , where X runs over all possible bounding boxes B. We re-fer to ρ(G, C) as a Crop-Aware Bounding Box (CABB) that in fact is a set of bounding boxes (see also Fig. 3). If the ground-truth bounding box G is strictly contained in the crop area then our CABB boils down to the original ground truth, for ρ(G, C) = {G} in that case.2 Since we will use a rep-resentation for bounding boxes relative to some anchor box
A we introduce also the notation ρA(G, C), which returns the same set as above but with elements expressed relative to A, i.e. ρA(G, C) = {X ⊖ A : X ∈ ρ(G, C)}.
Crop-aware bounding box loss.
In order to exploit the proposed, relaxed notion of ground-truth bounding box, we introduce the following new loss function for a given ground-truth box G, anchor box A and crop area C:
LCABB(∆P) = min
∆
LBB(∆P; ∆) , s.t ∆ ∈ ρA(G, C) . (2)
Any bounding box in ρ(G, C) is compatible with the cropped ground-truth box we observe and thus could be potentially 2To simplify the description, we deliberately neglect the fact that a bounding box strictly contained in the original image and touching the boundary of the crop area should not be extended beyond the crop. How-ever, our approach can be easily adapted to address these edge cases.
2 , b1 = dC−cA where a1 = δG − ωG and ˆω = 2(δP − a1).
If w⋆ is a solution to (O1) then ∆⋆ = (a1 + ω⋆ 2 , ω⋆) is a solution to the minimization problem in (2) for the dimension under consideration. dA
• If cG ≤ dG 2 and cG < dC − dG 2 , we obtain an optimization problem like (O1) but with a1 = − cA dA , b1 = δG +
ωG 2 and ˆω = 2(b1 − δP). If w⋆ is a solution to (O1) under this parametrization then ∆⋆ = (b1 − ω⋆ 2 , ω⋆) is a solution to the minimization problem in (2) for the dimension under consideration.
• If dC − dG 2 ≤ cG ≤ dG problem of the form 2 then we obtain an optimization yielding the lowest objective. See Alg. 2 in supplementary material for further details.
Gradient. For the sake of training a neural network, we are interested in computing gradients of the new loss func-tion, which exhibits a nested optimization problem. The following result shows that the derivative of the new loss function is equivalent to the derivative of the original one, with the ground-truth box replaced (as a constant) by the so-lution to the internal minimization problem. In general the solution to the internal minimization problem is a function of ∆P but the following result states that no gradient term is originated from this dependency. This is indeed a direct consequence of the envelope theorem [1]. min
δ,ω
ℓβ(δ − δP) + ℓβ(log(ω) − log(ωP)) s.t.
δ −
ω 2
≤ a2 ,
δ +
ω 2
≥ b2 , (O2)
Proposition 1. Let φ be a function returning the minimizer in (2) given ∆P, i.e. LCABB(∆P) = LBB(∆P, φ(∆P)) holds for any ∆P. Then dA and b2 = dC−cA where a2 = − cA
. Solutions to (O2) map directly to solutions to (2) for the dimension under consideration. dA
We focus now on ﬁnding the solution to the optimization problems (O1) and (O2).
Solution to (O1). As mentioned before, the optimization problem in (2) is in general non-convex and might have multiple local minima. The same holds true for the prob-lem in (O1) despite having a single variable. Nonetheless, we devised an ad-hoc solver for this problem that allows to quickly converge to a global solution under the desired precision. We skip the details due to lack of space, but we provide them in the supplementary material (see Alg. 1).
Solution to (O2). To solve this problem we break it down into cases. We start by noting that the solution to the uncon-strained optimization problem is trivially given by δ⋆ = δP and ω⋆ = ωP, because 0 is the minimizer of ℓβ. The solu-tion ∆⋆ = (δ⋆, ω⋆) is valid for (O2) if it satisﬁes the con-straints, but this is easy to check by substitution. If this is the case, we found the solution, otherwise no solution exists in the interior of the feasible set (see Prop. 1 in supplemen-tary material), but lies along the boundary of the feasible set. Accordingly, we start by forcing the ﬁrst constraint to be active. This yields an instance of (O1) with a1 = a2, b1 = b2 and ˆω = 2(δP − a2), which can be solved using the algorithm from the supplementary material, yielding ω⋆ 1.
By substituting it into the activated constraint we obtain the other variable δ⋆ 2 . Next, we move to activating the second constraint. This yields again an instance of the same optimization problem with the only difference being
ˆω = 2(b2 − δP). Again we solve it obtaining ω⋆ 2 and by sub-2 = b2 − ω⋆ stitution into the activated constraint we get δ⋆ 2 . 2, ω⋆ 1) and (δ⋆
We ﬁnally retain the solution among (δ⋆ 2) 1 = a2 + ω⋆ 1, ω⋆ 2 1 d d∆P
LCABB(∆P) =
∂
∂∆P 3.