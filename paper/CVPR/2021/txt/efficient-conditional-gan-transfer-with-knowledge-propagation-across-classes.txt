Abstract
Generative adversarial networks (GANs) have shown impressive results in both unconditional and conditional im-age generation. In recent literature, it is shown that pre-trained GANs, on a different dataset, can be transferred to improve the image generation from a small target data. The same, however, has not been well-studied in the case of con-ditional GANs (cGANs), which provides new opportunities for knowledge transfer compared to unconditional setup. In particular, the new classes may borrow knowledge from the related old classes, or share knowledge among themselves to improve the training. This motivates us to study the prob-lem of efﬁcient conditional GAN transfer with knowledge propagation across classes. To address this problem, we in-troduce a new GAN transfer method to explicitly propagate the knowledge from the old classes to the new classes. The key idea is to enforce the popularly used conditional batch normalization (BN) to learn the class-speciﬁc information of the new classes from that of the old classes, with implicit knowledge sharing among the new ones. This allows for an efﬁcient knowledge propagation from the old classes to the new ones, with the BN parameters increasing linearly with the number of new classes. The extensive evaluation demonstrates the clear superiority of the proposed method over state-of-the-art competitors for efﬁcient conditional
GAN transfer tasks. The code is available at: https:
//github.com/mshahbazi72/cGANTransfer 1.

Introduction
Generative adversarial networks (GANs) [10, 1] are the most common models used for image and video genera-tion, showing very promising results both [21] in uncon-ditional [14, 41, 16] and conditional [3, 34, 4] setups.
Learning from limited data is a well-studied problem in the discriminative setup, where the concept of knowledge transfer [31] between two different but related tasks [42] or domains [9] is ubiquitous. In contrast, the literature on transfer learning for generative adversarial models is fairly limited. One may ﬁnd this unexpected, since many popular knowledge transfer methods in discriminative setup, in turn, use generative schemes [22, 37]. However, the limited liter-ature is less surprising when the complexity of adversarial training and the mode collapse are taken into account.
A notable work by Wang et al. [40], ﬁrst addressed the problem of training GANs on limited data using a careful
ﬁne-tuning (FT) strategy. Following works [27, 39, 44] are the variants of [40] that focus on better ﬁne-tuning strate-gies. On the contrary, Noguchi & Harada [30] proposed the batch statistics adaptation (BSA) technique, by learning only the batch normalization parameters on a small target dataset. As most of the previous works [40, 39, 30] primar-ily focus on the case of unconditional GANs, we investigate in a different direction of conditional GANs (cGANs). In particular, we are interested in producing new classes given a pre-trained class-conditional cGAN. cGANs are strikingly interesting due to their capability of handling a large num-ber of classes with a single network. For example, Big-GAN [3] can generate images from all 1K classes of Ima-geNet [8]. In fact, BigGAN is exploited as the pre-trained network even by the unconditional methods [39, 30]. We refrain from ﬁne-tuning whenever possible, as we believe that new classes can be introduced within such powerful cGANs. Moreover, some powerful pre-trained cGANs, can potentially be used to add new classes in the lifelong learn-ing [19, 30, 33, 5] fashion, which however is beyond the focus of our paper.
In this work, we study how new classes with a limited amount of samples can be added to pre-trained cGANs us-ing knowledge transfer across classes. To do so, inspired by
BSA [30], we aim at learning only the batch normalization (BN) parameters that generally encode the class-speciﬁc in-formation. Our key idea, different from [30], relies on the assumption that the knowledge between old and new classes can also be transferred by searching for the similarity be-tween them. Our experimental setup, however, does not al-low us to access the old data used for the pre-trained model.
Therefore, the similarity is searched in the conditional space of the BN parameters, during the training of cGANs. In this process, we learn the similarity scores explicitly between old and new classes, and implicitly between new ones. The 12167
learned old-to-new similarity scores are then used to derive the batch statistics of new classes from that of old ones.
It is well-established in [2, 13, 38, 19, 36, 11, 6] and many other works, that learning algorithms can greatly ben-eﬁt from the shared knowledge between classes. Often, such similarity is either known or discovered when all the classes are accessible. In the context of domain generaliza-tion (or in some special case of adaptation), the source data is similarly inaccessible partially or completely [32, 47, 23].
However, the latter assumes that the new classes are either the same or largely overlap with the old ones. Note that in our case, new classes do not even overlap with the in-accessible old ones. In addition, almost all aforementioned works seek similarity in the feature space with an exception of [6]. However, [6] is primarily designed to serve the dis-criminative models. Our generative case, on the other hand, hinders us to access the feature space. We, therefore, rely on the conditional space of cGANs to establish the sought class similarities. Up to our knowledge, we learn the inter-class similarities in the conditional space of the generative models, for the ﬁrst time.
In summary, we utilize cross-class knowledge while in-troducing new classes in cGANs. While doing so, active searching of similarity scores between new and old classes with implicit knowledge sharing among new ones is sug-In this context, we propose a novel method for gested.
ﬁnding the similarity between new and old classes with-out requiring access to the old data. The proposed method is particularly suitable when transferring knowledge from pre-trained cGANs.
In summary, the key contributions of our work are as follows:
• We study the new problem of efﬁcient GAN transfer to new classes with explicit inter-class knowledge propa-gation in pre-trained cGANs.
• A novel method for learning similarity between old and new classes and knowledge sharing within the new classes is proposed using the batch normalization statistics of the old classes, in the conditional space of generative models.
• Our experiments on three benchmark datasets demon-strate the superiority of our method both in terms of generated image quality and the convergence speed. 2.