Abstract
Predicting future trajectories of trafﬁc agents in highly interactive environments is an essential and challenging problem for the safe operation of autonomous driving sys-tems. On the basis of the fact that self-driving vehicles are equipped with various types of sensors (e.g., LiDAR scan-ner, RGB camera, radar, etc.), we propose a Cross-Modal
Embedding framework that aims to beneﬁt from the use of multiple input modalities. At training time, our model learns to embed a set of complementary features in a shared latent space by jointly optimizing the objective functions across different types of input data. At test time, a single input modality (e.g., LiDAR data) is required to generate predictions from the input perspective (i.e., in the LiDAR space), while taking advantages from the model trained with multiple sensor modalities. An extensive evaluation is con-ducted to show the efﬁcacy of the proposed framework using two benchmark driving datasets. 1.

Introduction
Future trajectory prediction has become the central chal-lenge to succeed in the safe operation of autonomous ve-hicles designed to cooperate with interactive agents (i.e., pedestrians, cars, cyclists, etc.).
It can beneﬁt to the de-ployment of applications in autonomous navigation and driving assistance systems with advanced motion planning and decision making. Based on the fact that multi-modal sensors (e.g., LiDAR scanner, RGB cameras, radar, etc.) are equipped in autonomous vehicles, we propose a cross-modal embedding framework that demonstrates the efﬁcacy of the use of multiple sensor data for motion prediction.
Figure 1 illustrates an overview of the proposed ap-proach. At training time, we embed multiple feature repre-sentations encoded from individual sensor data into a single
∗Co-ﬁrst author.
†Work done during Jiachen’s internship at Honda Research Institute
Figure 1: Given a set of multi-modal data (e.g., LiDAR data, RGB images, etc.) obtained from an autonomous ve-hicle, the model is trained to embed complementary repre-sentations of different input modalities into a shared latent space. Output predictions are generated from different per-spectives using a latent variable sampled from the learned embedding space. At test time, the proposed method takes a single input modality (e.g., LiDAR data, red-dashed ar-row) and predicts the future motion in the same space (i.e.,
LiDAR-captured world space, red-solid arrow). shared latent space. Our model jointly optimizes the ob-jective functions across different input modalities, so that the evidence lower bound of multiple input data over the likelihood can be jointly maximized. We provide a deriva-tion of the objective of shared cross-modal embedding and its implementation using a CVAE-based generative model.
At test time, the model takes a single input modality (e.g.,
LiDAR data) and generates a future trajectory from the in-put perspective (i.e., top-down view) using a latent variable sampled from the shared embedding space. In this way, we can beneﬁt to the model training from the use of multiple input modalities1, while keeping the same computational time for trajectory generation as if the single modality had been used. To the best of our knowledge, we are the ﬁrst to employ multi-modal sensor data from a single framework 1For example, top-down view LiDAR data and frontal view RGB im-ages. However, the input modalities are not limited to these two types but also include stereo images, depth, radar, GPS, and many others equipped in autonomous vehicles, which can provide visual or locational information. 244
for trajectory prediction. Note that existing works solve the problem either in top-down view [23, 36, 9] with LiDAR data or in frontal view [46, 4, 30] with RGB images.
The proposed framework is clearly distinguishable to studies on a multi-modal pipeline for scene understanding such as detection [6, 22, 27], tracking [12, 50], and semantic segmentation [17, 41]. They have presented more accurate models by simply fusing different representations extracted from several sensor modalities. The generation of such joint representations, however, would not be desirable in driving automation systems due to the following issues: (i) during inference, it inherently increases the computation time pro-portional to the number of input modalities used; and (ii) with the anomalous LiDAR data, the model would fail in
ﬁnding a solution, which is critical to operate self-driving vehicles. For the former issue, our proposed cross-modal embedding takes only a single input data during inference and thus does not inﬂuence the computational time, while it still beneﬁts from the model trained with multiple input modalities. In the latter, our model provides alternative pre-diction solutions in frontal view using the RGB data, which will activate driving assistance functions (i.e., ADAS) for safe vehicle operation, even with a sensor failure.
To this end, we generate multiple modes of future trajec-tories by sampling several latent variables from the learned latent space. However, such random sampling-based strat-egy [23, 9] is likely to predict similar trajectories, ignoring the random variables while generating predictions from the decoder. This posterior collapse2 problem of VAEs is par-ticularly critical to future prediction as it mitigates the di-verse modes of system outputs. Therefore, we introduce a regularizer (i) that pushes the model to rely on the latent variables, predicting diverse modes of future motion; and (ii) that does not weaken the prediction capability of the de-coder while preventing the performance degradation.
We address the following ideas in the proposed method:
• The objective of shared cross-modal embedding to jointly approximate a real distribution using multi-ple input sources is mathematically derived using the
Kullback-Leibler divergence (Sec. 3.2).
• Shared cross-modal embedding is implemented based on our derivation to beneﬁt from the use of multiple in-put modalities, while keeping the same computational time as if the single modality had been used (Sec. 3.2).
• The regularizer is designed for future prediction to mit-igate posterior collapse of VAEs and to predict more diverse modes of motion behavior (Sec. 3.3).
In addition, we design an interaction graph with a graph-level target (Sec. 3.1), introduce a new evaluation metric to measure prediction success (Sec. 4.2), and propose to use absolute motions in frontal view (Sec. 4.1). 2We do not carry out any study on mode collapse of GANs or related problems other than posterior collapse of VAEs where our work is built on.
Throughout the paper, we use the word ‘multi-modality’ to denote two different sources. First, multi-modal input represents input data obtained from different types of sen-sors. Second, multi-modal prediction depicts predicted tra-jectory outputs with multiple variations. 2.