Abstract
High-resolution representations (HR) are essential for dense prediction tasks such as segmentation, detection, and pose estimation. Learning HR representations is typically ignored in previous Neural Architecture Search (NAS) meth-ods that focus on image classiﬁcation. This work proposes a novel NAS method, called HR-NAS, which is able to ﬁnd efﬁcient and accurate networks for different tasks, by ef-fectively encoding multiscale contextual information while
In HR-NAS, maintaining high-resolution representations. we renovate the NAS search space as well as its search-ing strategy. To better encode multiscale image contexts in the search space of HR-NAS, we ﬁrst carefully design a lightweight transformer, whose computational complexity can be dynamically changed with respect to different objec-tive functions and computation budgets. To maintain high-resolution representations of the learned networks, HR-NAS adopts a multi-branch architecture that provides convolu-tional encoding of multiple feature resolutions, inspired by
HRNet [73]. Last, we proposed an efﬁcient ﬁne-grained search strategy to train HR-NAS, which effectively explores the search space, and ﬁnds optimal architectures given var-ious tasks and computation resources. As shown in Fig.1 (a), HR-NAS is capable of achieving state-of-the-art trade-offs between performance and FLOPs for three dense pre-diction tasks and an image classiﬁcation task, given only small computational budgets. For example, HR-NAS sur-passes SqueezeNAS [63] that is specially designed for se-mantic segmentation while improving efﬁciency by 45.9%.
Code is available at https://github.com/dingmyu/HR-NAS. 1.

Introduction
Neural architecture search (NAS) has achieved remark-able success in automatically designing efﬁcient models for image classiﬁcation [76, 43, 47, 53, 6, 79, 5, 64, 92, 26].
*This work was done when Mingyu was a research intern at Bytedance.
Figure 1. Comparisons of the efﬁciency (i.e., FLOPs) and the per-formance (e.g., Acc, mIoU, AP) on 4 computer vision tasks, i.e., classiﬁcation (ImageNet), segmentation (CityScapes), pose esti-mation (COCO), and 3D detection (KITTI), between the proposed approach and existing SoTA methods. Each method is represented by a circle, whose size represents the number of parameters. ⋆ represents the optimal model with both high performance and low
FLOPs. Our approach achieves superior performance under simi-lar FLOPs compared to its counterparts on all four benchmarks.
NAS has also been applied to improve the efﬁciency of models for dense prediction tasks such as semantic segmen-tation [63, 8] and pose estimation [19]. However, existing
NAS methods for dense prediction either directly extend the search space designed for image classiﬁcation [19, 44], only search for a feature aggregation head [54, 8], organizing network cells in a chain-like single-branch manner [46, 63].
This lack of consideration to the speciﬁcity of dense predic-tion hinders the performance advancement of NAS methods compared to the best hand-crafted models [73, 23].
In principle, dense prediction tasks require the integrity of the global context and the high-resolution (HR) represen-tation; the former is critical to clarify ambiguous local fea-tures [87] at each pixel, and the latter is useful for the accu-2982
rate prediction of ﬁne details [39], such as semantic bound-aries and keypoint locations. However, these two aspects, especially the HR representations, have not got enough at-tention in existing NAS algorithms for classiﬁcation. The straightforward strategy to implement the principle is man-ually combining multi-scale features at the end of the net-work [46, 9, 41], while recent approaches [23, 73, 88] show the performance can be enhanced by putting multi-scale feature processing within the network backbone. Another observation from recent research is that multi-scale con-volutional representations can not guarantee a global out-look of the image since dense prediction tasks often come with high input resolution but a network often only covers a ﬁxed receptive ﬁeld. Therefore, global attention strate-gies such as SENet [34] and non-local network [75] have been proposed to enrich image convolutional features. Most recently, inspired by its success in natural language pro-cessing, Transformer architectures [72, 66], which contain global attention with spatial encoding, have also shown su-perior results when combined with convolutional neural net-work for image classiﬁcation [24] and object detection [7].
Motivated by the above observations, in this work, we propose a NAS algorithm, which incorporates these strate-gies, i.e. in-network multi-scale features and transformers, and enables their adaptive changing with respect to task objectives and resource constraints. In practice, it is non-trivial to put them together. Firstly, Transformer has a high computational cost that is quadratic w.r.t. image pixels and hence unfriendly to the NAS search space of efﬁcient archi-tectures. We solve this through a dynamic down projection strategy, yielding a lightweight and plug-and-play trans-former architecture that can be combined with other convo-lutional neural architectures. In addition, searching a fused space of multi-scale convolution and transformers needs proper feature normalization, selection of fusion strategies and balancing. We did extensive studies to calibrate various model choices that generalize to multiple tasks.
In summary, HR-NAS works as follows. We ﬁrst setup a super network, where each layer contains a multi-branch parallel module followed by a fusion module. The parallel module contains searching blocks with multiple resolutions, and the fusion module contains searching blocks of feature fusion determining how feature from different resolutions fuses. Then, based on the computational budget and the task objective, a ﬁne-grained progressive shrinking search strategy is introduced to prune redundant channels in convo-lutions and queries in transformers, resulting in an efﬁcient model that provides the best trade-off between performance and computational costs. With extensive experiments, HR-NAS achieves state-of-the-art on multiple dense prediction tasks and competitive results on image classiﬁcation under highly efﬁcient settings with a single search. Fig. 1 shows a comprehensive comparison of our proposed approach with previous NAS approaches as well as manually designed net-works on four different tasks.
Our main contributions are three-fold. (1) We introduce a novel lightweight and plug-and-play transformer, which is highly efﬁcient and can be easily combined with convo-lutional networks for computer vision tasks. (2) We pro-pose a well-designed multi-resolution search space contain-ing both convolutions and transformers to model in-network multi-scale information and global contexts for dense pre-diction tasks. To our best knowledge, we are the ﬁrst to in-tegrate transformers in a resource-constrained NAS search space for computer vision. (3) A resource-aware search strategy allows us to customize efﬁcient architectures for different tasks. Extensive experiments show models pro-duced by our NAS algorithm achieve state-of-the-art on three dense prediction tasks and four widely used bench-marks with lower computational costs. 2.