Abstract
Explainability of deep learning methods is imperative to facilitate their clinical adoption in digital pathology.
However, popular deep learning methods and explainabil-ity techniques (explainers) based on pixel-wise process-ing disregard biological entities’ notion, thus complicating comprehension by pathologists. In this work, we address this by adopting biological entity-based graph processing and graph explainers enabling explanations accessible to
In this context, a major challenge becomes pathologists. to discern meaningful explainers, particularly in a stan-dardized and quantiﬁable fashion. To this end, we propose herein a set of novel quantitative metrics based on statis-tics of class separability using pathologically measurable concepts to characterize graph explainers. We employ the proposed metrics to evaluate three types of graph explain-ers, namely the layer-wise relevance propagation, gradient-based saliency, and graph pruning approaches, to explain
Cell-Graph representations for Breast Cancer Subtyping.
The proposed metrics are also applicable in other domains by using domain-speciﬁc intuitive concepts. We validate the qualitative and quantitative ﬁndings on the BRACS dataset, a large cohort of breast cancer RoIs, by expert pathologists.
The code, data, and models can be accessed here1. 1.

Introduction
Histopathological image understanding has been revolu-tionized by recent machine learning advancements, espe-cially deep learning (DL) [8, 53]. DL has catered to increas-ing diagnostic throughput as well as a need for high predic-tive performance, reproducibility and objectivity. However, such advantages come at the cost of a reduced transparency in decision-making processes [28, 61, 23]. Considering the
∗denotes equal contribution 1https://github.com/histocartography/ patho-quant-explainer
Figure 1. Sample explanations produced by pixel- and entity-based explainability techniques for a ductal carcinoma in situ RoI. need for reasoning any clinical decision, it is imperative to enable the explainability of DL decisions to pathologists.
Inspired by the explainability techniques (explainers) for
DL model decisions on natural images [57, 68, 67, 6, 40, 52, 33, 72, 11, 32], several explainers have been implemented in digital pathology, such as feature attribution [10, 9, 23], concept attribution [21], and attention-based learning [38].
However, pixel-level explanations, exempliﬁed in Figure 1, (1) a pixel-wise pose several notable issues, including: analysis disregards the notion of biological tissue entities, their topological distribution, and inter-entity interactions; (2) a typical patch-based DL processing and explainer fail to accommodate complete tumor macro-environment infor-mation; and (3) pixel-wise visual explanations tend to be blurry. Explainability in entity space is thus a natural choice to address the above issues. To that end, an entity graph rep-resentation is built for a histology image, where nodes and edges denote biological entities and inter-entity interactions followed by a Graph Neural Network (GNN) [35, 64]. The choice of entities, such as cells [22, 71, 44], tissues [44] or others, can be task-dependent. Subsequently, explainers for graph-structured data [7, 46, 65] applied to the entity graphs highlight responsible entities for the concluded diagnosis, thereby generating intuitive explanations for pathologists. 18106
In the presence of various graph explainers producing distinct explanations for an input, it is crucial to discern the explainer that best ﬁts the explainability deﬁnition [4].
In the context of computational pathology, explainability is deﬁned as making the DL decisions understandable to pathologists [28]. To this end, the qualitative evaluation of explainers’ explanations by pathologists is the candid measure. However, it requires evaluations by task-speciﬁc expert pathologists, which is subjective, time-consuming, cumbersome, and expensive. Additionally, though the ex-planations are intuitive, they do not relate to pathologist-understandable terminologies, e.g. “How big are the im-portant nuclei?”, “How irregular are their shape?” etc., which toughens the comprehensive analysis. These bottle-necks undermine not only any qualitative assessment but also quantitative metrics requiring user interactions [39].
Furthermore, expressing the quantitative metrics in user-understandable terminologies [4] is fundamental to achieve interpretability [16, 41]. To this end, the most popular quan-titative metric, explainer ﬁdelity [48, 15, 49, 27, 39, 46], is not satisfactory. Moreover, explainers intrinsically maintain high-ﬁdelity, e.g. GNNEXPLAINER [65] produces an expla-nation to match the GNN’s prediction on the original graph.
Thus, we propose a set of novel user-independent quan-titative metrics expressing pathologically-understandable concepts. The proposed metrics are based on class sepa-rability statistics using such concepts, and they are applica-ble in other domains by incorporating domain-speciﬁc con-cepts. We use the proposed metrics to evaluate three types of graph-explainers, (1) graph pruning: GNNEXPLAINER
[65, 29], (2) gradient-based saliency: GRAPHGRAD-CAM
[52, 46], GRAPHGRAD-CAM++ [11], (3) layer-wise rele-vance propagation: GRAPHLRP [6, 40, 51], for explaining
Cell-Graphs [22] in Breast Cancer Subtyping as shown in
Figure 1. Our speciﬁc contributions in this work are:
• A set of novel quantitative metrics based on the statis-tics of class separability using domain-speciﬁc con-cepts to characterize graph explainability techniques.
To the best of our knowledge, our metrics are the
ﬁrst of their kind to quantify explainability based on domain-understandable terminologies;
• Explainability in computational pathology using pathologically intuitive entity graphs;
• Extensive qualitative and quantitative assessment of various graph explainability techniques in computa-tional pathology, with a validation of the ﬁndings by expert pathologists. 2.