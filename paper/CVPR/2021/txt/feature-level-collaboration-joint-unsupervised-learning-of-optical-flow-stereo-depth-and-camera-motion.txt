Abstract
Precise estimation of optical ﬂow, stereo depth and cam-era motion are important for the real-world 3D scene un-derstanding and visual perception. Since the three tasks are tightly coupled with the inherent 3D geometric con-straints, current studies have demonstrated that the three tasks can be improved through jointly optimizing geometric
In this pa-loss functions of several individual networks. per, we show that effective feature-level collaboration of the networks for the three respective tasks could achieve much greater performance improvement for all three tasks than only loss-level joint optimization. Speciﬁcally, we propose a single network to combine and improve the three tasks.
The network extracts the features of two consecutive stereo images, and simultaneously estimates optical ﬂow, stereo depth and camera motion. The whole network mainly con-tains four parts: (I) a feature-sharing encoder to extract features of input images, which can enhance features’ rep-resentation ability; (II) a pooled decoder to estimate both optical ﬂow and stereo depth; (III) a camera pose estima-tion module which fuses optical ﬂow and stereo depth infor-mation; (IV) a cost volume complement module to improve the performance of optical ﬂow in static and occluded re-gions. Our method achieves state-of-the-art performance among the joint unsupervised methods, including optical
ﬂow and stereo depth estimation on KITTI 2012 and 2015 benchmarks, and camera motion estimation on KITTI VO dataset. 1.

Introduction
Optical ﬂow, depth and camera motion estimation are three fundamental tasks in the ﬁeld of computer vision.
Deep learning methods have greatly advanced the state-of-the-art in optical ﬂow and stereo depth estimation [32, 13, 7]. Meanwhile, learning-based camera ego-motion predic-tion [53, 52] has also made signiﬁcant progress recently.
∗ represents the corresponding author
Jointly estimating optical ﬂow, stereo depth and camera mo-tion can be applied in a wide range of applications, such as autonomous navigation [9, 42], 3D scene reconstruc-tion [38] and robot control [1]. Many uniﬁed unsupervised framework [6, 44, 45, 27, 2] have been proposed to jointly optimize two or three tasks concurrently. These joint meth-ods demonstrate that jointly tackling these tasks has a posi-tive effect on each of them.
There is a tight geometric relationship among optical
ﬂow, stereo depth and camera pose, due to that each one of the three tasks can be calculated by the other two. There-fore, previous joint methods [4, 30, 21, 45] usually estimate them by several individual networks, and construct various geometric consistency constraint losses to mutually guide each other. Recently, some works [24, 27] have tried to share the same network for stereo depth and optical ﬂow estimation. However, based on the epipolar constraint, the stereo depth estimation network only needs to search pixel correspondences in horizontal lines, while the optical ﬂow estimation network demands a more comprehensive search in both horizontal and vertical directions. So these meth-ods only treat the stereo depth and optical ﬂow estimation as exactly the same task, but fail to allow full play the ad-vantage of sharing features between stereo depth and opti-cal ﬂow estimation. That is, most if not all, existing joint methods [4, 30, 45, 21, 24, 27] do not take full advantage of the feature-level collaboration to constrain each other in the learning process of the three tasks.
In this paper, we demonstrate that effective feature-level collaboration for the three respective tasks could achieve much greater performance improvement for all three tasks than loss-level joint optimization. The intuition behind this idea is that both stereo depth and optical ﬂow estimation networks ﬁnd pixel correspondences between two images, sharing features between the two tasks is reasonable and meanwhile constrain the training process of the two tasks.
In addition, camera motion can be directly calculated by the optical ﬂow and stereo depth, so utilizing the feature-level information of optical ﬂow and stereo depth could add further geometric constraint for feature training. To this 12463
end, we design a single network to integrate all the three tasks. We obtain the features of stereo images and consecu-tive frames by a feature-sharing encoder, and then take full advantage of the features to predict both optical ﬂow and stereo depth by a pooled decoder. After that, the extracted image features are used to predict the camera motion. In this way, we achieve the feature-level mutual leaning of the three tasks. And to our best knowledge, our method is the
ﬁrst feature-level collaboration of the three tasks. And our experiments verify that the collaboration at feature level can signiﬁcantly improve the performance of the three tasks.
Occlusion is also a primary problem in unsupervised op-tical ﬂow estimation, because the occluded pixels in the for-mer frame can not ﬁnd the corresponding matching pixels in the next frame.
In terms of image, the occluded pix-els do not obey photometric consistency hypothesis. Sev-eral methods [27, 44, 40, 23] utilize various geometric loss terms instead of photometric loss to guide the optical ﬂow estimation of the occluded pixels. In terms of features, the cost volume stores the matching costs of corresponding pix-els in different images, so it is obvious that the part of the cost volume corresponding to the occlusion pixels is inac-curate, which will degrade the performance of the network.
However, existing joint unsupervised methods hardly han-dle the inaccurate cost volume. In this study, we ﬁnd that in the real-world scenes, there are fewer occluded pixels be-tween the left and right image captured by the stereo cam-era. Based on the fact, we propose to leverage the cost vol-ume of stereo images to complement the cost volume of two consecutive frames, which can achieve a better performance on optical ﬂow estimation in occluded regions.
In summary, we propose a single unsupervised network to jointly estimate optical ﬂow, stereo depth and camera motion, and achieve feature-level collaboration of the three tasks. Our main contributions include: (I) We use a feature-sharing encoder for optical ﬂow, stereo depth and camera motion estimation; (II) We design a pooled decoder to es-timate both the optical ﬂow and stereo depth; (III) We pro-pose a novel method to estimate camera motion by using image features which are shared with optical ﬂow and stereo depth. Furthermore, we also design a pose reﬁnement mod-ule to further improve the camera motion accuracy; (IV) We explore the cost volumes complementary method to solve the occlusion problem at feature level. (V) Our method outperforms existing unsupervised joint methods. Remark-ably, our method in optical ﬂow estimation even outper-forms some classic supervised methods [43, 18]. 2.