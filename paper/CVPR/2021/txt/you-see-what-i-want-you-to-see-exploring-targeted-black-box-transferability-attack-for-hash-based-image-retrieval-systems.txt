Abstract
With the large multimedia content online, deep hashing has become a popular method for efﬁcient image retrieval and storage. However, by inheriting the algorithmic back-end from softmax classiﬁcation, these techniques are vul-nerable to the well-known adversarial examples as well.
The massive collection of online images into the database also opens up new attack vectors. Attackers can embed ad-versarial images into the database and target speciﬁc cat-egories to be retrieved by user queries. In this paper, we start from an adversarial standpoint to explore and enhance the capacity of targeted black-box transferability attack for deep hashing. We motivate this work by a series of empiri-cal studies to see the unique challenges in image retrieval.
We study the relations between adversarial subspace and black-box transferability via utilizing random noise as a proxy. Then we develop a new attack that is simultane-ously adversarial and robust to noise to enhance transfer-ability. Our experimental results demonstrate about 1.2-3× improvements of black-box transferability compared with the state-of-the-art mechanisms. The code is available at: https://github.com/SugarRuy/CVPR21 Transferred Hash. 1.

Introduction
With the exponential growth of visual content on the In-ternet, deep learning to hash (deep hashing) [46, 9, 25] has emerged as a leading technique in content-based image re-trieval. By mapping semantically similar images into close proximity in the Hamming space, it enables efﬁcient nearest neighbor search and storage of large-scale multimedia data.
Powered by deep hashing, from a photo of a product taken in the real world, without knowing its name, customers could extract similar products online. Service providers, such as search engines (Google [2], Bing [1]), social net-works (Pinterest [6], e-commerce (Taobao [5]) and fashion designers([16]), are investing largely into this technology to complement the traditional text query.
Unfortunately, by inheriting the backend from classiﬁca-tion networks, deep hashing is also vulnerable to the well-known adversarial examples [27, 44, 38, 42], that purposely crafted perturbations with minimal perceptual difference can cause misclassiﬁcation into any other label (untargeted attack) or a speciﬁc label (targeted attack). Targeted attacks are strictly more difﬁcult given the complex inter-class se-mantics [8, 26]. While white-box attacks almost guarantee success, service providers do not reveal their models pub-licly, which remain a black box to the attacker. Because of the resemblance of decision boundaries, adversarial exam-ples can still transfer to the black-box models, but at a much less chance to accomplish targeted attacks [26].
Rather than causing a wrong decision, system design-ers face a slightly different attack surface in image re-trieval systems, in which images from the database are
For better results, a returned to match user’s query. growing database is typically maintained via automated crawling, indexing of online images [4] and caching user queries [3]. However, this may also inadvertently in-clude private/inappropriate/upsetting content such as pro-tected copyright, violence, pornography, racism or advertis-ing spam into the database. By designing adversarial pertur-bations into the inappropriate images, attackers can launch targeted attacks against benign search queries, and visually display those images to the victims. To exploit this vulnera-bility, competitors can override the product search results in online shopping; advertisers can make customers view their advertisements for free; conspirators can divert images of political banners into racism or violence. Attackers can fur-ther target the content in the top searching list to reap high visibility.
The previous works have shown high success rate of un-targeted white-box attacks for image retrieval [27, 44, 38].
E.g., [44] shows that by maximizing the hamming distance of a perturbed image to its original category in the hash space, the network retrieves an irrelevant image. Never-theless, the most challenging targeted attacks are yet to be fully explored in the black-box setting and they also carry higher practical value as attackers can mislead the re-sults into speciﬁc categories. A trivial way to accomplish black-box transferability is to increase the level of perturba-tion [26], at the cost of degrading visual quality and being detected. In fact, our preliminary experiment indicates dras-tically small transferability under 1%, even the state-of-the-1934
art mechanism [42] is implemented for deep hashing. How-ever, such low transferability does not necessarily translate into a blessing in security before we fully understand the attacker’s capacity.
In this paper, we explore and improve targeted transfer-able attack in deep hashing. Similar to susceptible classes in classiﬁcation [32], our ﬁrst discovery is the existence of vulnerable pairs that transfer more easily than the rest. They could be explicitly mined based on the hamming distance from the white-box model, where attackers can utilize these pairs to enhance the success rate. Then we look into dif-ferent attacks to ﬁnd implications of their transferrable ca-pacity. We design an algorithm to utilize additive Gaussian random noise as a proxy to estimate the generated adversar-ial region, and show that it is indeed related to black-box transferability, i.e., an adversarial example with higher tol-erance to random noise is more prone to transfer to black-box models. Based on this ﬁnding, we further devise a new attack to look for perturbations that are simultaneously ad-versarial and robust to random noise, i.e., both adversarial and noise-corrupted adversarial images are retrievable by querying the target images.
The main contributions are summarized below. First, this work aims to bridge the two areas of adversarial attacks and image retrieval. By studying the most challenging targeted black-box transferability attack, it opens up a new dimen-sion to realize an array of realistic attacks in image retrieval systems. Second, we point out useful information from the white-box model that implies black-box transferability: a) the existence of vulnerable pairs; b) the relation between transferability and white-box adversarial region. We pro-pose an algorithm to estimate the adversarial region by in-troducing random noise, which is used to assess the capacity of different attacks. Then we design a new attack to search for a perturbation for potentially higher transferability. Fi-nally, we conduct extensive experiments and demonstrate that the proposed attack can boost the black-box transfer-ability by 1.2 − 3×, compared to PGD [29], and 1.5× com-pared to the diversity techniques [42]. We also demonstrate case studies of crafting out-of-distribution images to target normal queries with high successful rates. 2.