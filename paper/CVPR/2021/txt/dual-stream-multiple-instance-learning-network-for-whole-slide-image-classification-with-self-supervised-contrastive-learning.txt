Abstract
We address the challenging problem of whole slide image (WSI) classiﬁcation. WSIs have very high resolutions and usually lack localized annotations. WSI classiﬁcation can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MIL-based method for WSI classiﬁcation and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel
MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measure-ment. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good rep-resentations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classiﬁcation and localization. Our model is evaluated on two representative WSI datasets. The classiﬁcation accuracy of our model compares favorably to fully-supervised methods, with less than 2% accuracy gap across datasets. Our results also outperform all previous
MIL-based methods. Additional benchmark results on stan-dard MIL datasets further demonstrate the superior perfor-mance of our MIL aggregator on general MIL problems. 1.

Introduction
Whole slide scanning is a powerful and widely used tool to visualize tissue sections in disease diagnosis, medical ed-ucation, and pathological research [10, 38]. The scanning
* Co-corresponding authors.
Figure 1. Decision boundary learned in MIL. Left: Max pooling delineates the decision boundary according to the highest-score instances in each bag. Right: DSMIL measures the distance be-tween each instance and the highest-score instance. converts tissues on glass slides into digital whole slide im-ages (WSIs) for assessment, sharing, and analysis. Auto-mated disease detection in WSIs has been a long-standing challenge for computer aided diagnostic systems. We have begun to see some recent success from computer vision and medical image analysis communities [6, 44, 3, 24, 27, 29], fueled by the advances in deep learning.
WSIs have extremely high resolutions — a typical pathology image has a size of 40, 000 × 40, 000. Conse-quently, the most widely used paradigm for WSI classiﬁ-cation is patch-based processing — a WSI is divided into thousands of small patches and further examined by a clas-siﬁer e.g., a convolutional neural network (CNN) [21, 53, 35, 11, 33].
In clinics, a disease-positive tissue section might only take a small portion (e.g., less than 20%) of the whole tissue, leading to a large number of disease-negative patches. Unfortunately, with gigapixel resolution, patch-level labeling by expert pathologists is very time consum-ing and difﬁcult to scale. To address this challenge, several recent studies [21, 3, 18] have demonstrated the promise of weakly supervised WSI classiﬁcation, where only slide-level labels are used to train a patch-based classiﬁer. 14318
The majority of previous approaches [21, 53, 35, 11, 18, 8] on weakly supervised WSI classiﬁcation follows a mul-tiple instance learning (MIL) problem formulation [14, 34], where each WSI is considered as a bag that contains many instances of patches. A WSI (bag) is labeled as disease-positive if any of its patches (instances) is disease-positive (e.g., with lesions). Patch-level features or scores are ex-tracted, aggregated, and examined by a classiﬁer that pre-dicts slide-level labels. Recent MIL based approaches have greatly beneﬁted from using deep neural networks for fea-ture extraction and feature aggregation [22, 50, 37].
Two major challenges exist in developing deep MIL models for weakly supervised WSI classiﬁcation. First, when patches (instances) in positive images (bags) are highly unbalanced, i.e., only a small portion of patches are positive, the models are likely to misclassify those positive instances [22] when using a simple aggregation operation, such as the widely adopted max-pooling. This is because, under the assumptions of MIL, max-pooling can lead to a shift of the decision boundary compared to fully-supervised training (Figure 1). Besides, the model can easily suffer from overﬁtting and unable to learn rich feature representa-tions due to the weak supervisory signal [12, 32, 1]. Second, current models either use ﬁxed patch features extracted by a CNN or only update the feature extractor using a few high score patches, as the end-to-end training of the feature ex-tractor and aggregator is prohibitively expensive for large bags [12, 3, 32]. Such a simpliﬁed learning scheme might lead to sub-optimal patch features for WSI classiﬁcation.
To address these challenges, we propose a novel deep
MIL model, dubbed dual-stream multiple instance learn-ing network (DSMIL). Speciﬁcally, DSMIL jointly learns a patch (instance) and an image (bag) classiﬁer, using a two-stream architecture. The ﬁrst stream deploys a stan-dard max-pooling to identify the highest scored instance (referred to as critical instance), while the second stream computes an attention score for each instance by measur-ing its distance to the critical instance. DSMIL further ap-plies a soft selection of instances using the attention scores, leading to a decision boundary that better delineates the in-stances in positive bags, as shown in Figure 1. Importantly,
DSMIL makes use of self-supervised contrastive learning for training the feature extractor for WSI, producing strong patch representations. In addition, DSMIL incorporates a multiscale feature fusion mechanism that can leverage tis-sue features ranging from millimeter-scale (e.g., vessels and glands) to cellular-scale (tissue microenvironment).
We evaluate DSMIL for weakly supervised WSI classi-ﬁcation on two public WSI datasets including Camelyon16 and TCGA lung cancer. The results show that DSMIL out-performs other recent MIL models in classiﬁcation accuracy by at least 2.3%. More importantly, our classiﬁcation accu-racy compares favorably to fully-supervised methods, with less than 2% accuracy gap. Moreover, DSMIL also has su-perior localization accuracy, outperforming previous MIL models by a signiﬁcant margin. Finally, we demonstrate the state-of-the-art performance of DSMIL on general MIL problems beyond weakly supervised WSI classiﬁcation. 2.