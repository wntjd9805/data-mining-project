Abstract
Inspired by the fact that human eyes continue to develop tracking ability in early and middle childhood, we propose to use tracking as a proxy task for a computer vision sys-tem to learn the visual representations. Modelled on the
Catch game played by the children, we design a Catch-the-Patch (CtP) game for a 3D-CNN model to learn visual rep-resentations that would help with video-related tasks.
In the proposed pretraining framework, we cut an image patch from a given video and let it scale and move according to a pre-set trajectory. The proxy task is to estimate the po-sition and size of the image patch in a sequence of video frames, given only the target bounding box in the ﬁrst frame.
We discover that using multiple image patches simultane-ously brings clear beneﬁts. We further increase the dif-ﬁculty of the game by randomly making patches invisible.
Extensive experiments on mainstream benchmarks demon-strate the superior performance of CtP against other video pretraining methods. In addition, CtP-pretrained features are less sensitive to domain gaps than those trained by a supervised action recognition task. When both trained on
Kinetics-400, we are pleasantly surprised to ﬁnd that CtP-pretrained representation achieves much higher action clas-siﬁcation accuracy than its fully supervised counterpart on
Something-Something dataset. 1.

Introduction
During the development of artiﬁcial intelligence, we can always take inspiration from the way human brain learns, and computer vision is no exception. For instance, the in-sight behind building the ImageNet dataset was “to give the algorithms the kind of training data that a child was given through experiences in both quantity and quality.”1 In this work, we intend to address the visual representation learn-ing problem in computer vision, so we look for clues from what developing eyes learn to do in childhood. Our intu-ition is that once a computer vision system learns what de-1Fei-fei Li’s TED talk ”How we teach computers to understand pic-tures,” 2005.
Figure 1: Illustration of the Catch-the-Patch game we de-signed to train a computer vision system. We randomly crop one or multiple patches from a video clip, let them scale and move in a smooth way, and then train the neural network to predict the positions and sizes of the patches in each frame. veloping eyes are capable of, the visual features it extracts should contain the most important information needed by downstream vision tasks.
It is not surprising that the ability to track, or to follow a moving target, caught our attention. It is not only an impor-tant capability of human eyes, but it has also been regarded as an important technology in computer vision and the ba-sis of video analysis. In this work, however, we do not treat tracking as an ultimate task. Instead, we want to use it as a proxy task for a computer vision system to learn feature representations of visual signals. Here, the visual signals need to be videos, or moving pictures, instead of static im-ages. Ideally, the learning process does not require human annotation, or should be self-supervised. Only in this way can we make full use of the large amount of video data on the Internet. This falls into an active area of research called self-supervised video representation learning, which aims to learn video understanding models [37, 51, 38, 50] with-out access to human annotation.
The research progress in this area lags far behind a closely related area called self-supervised image representa-tion learning, where several ground-breaking works [14, 3] emerged in recent years. A possible reason is that videos are much larger in size and more redundant in its original 2563
representation than images. It is therefore more critical to design an efﬁcient proxy task which could guide the neural network to acquire the core capability or to distill the most important information. Some existing proxy tasks propose to estimate the orientation of video frames [20], to predict the spatial-temporal order [22, 44], or to estimate the play-back speed of the input video clip [1, 47, 4]. These tasks may fail to capture the ﬁne-grained information as they only care about the coarse global attributes.
Our work focuses on helping the network develop the ability to follow a moving target. The proxy task used to pretrain the network is inspired by the training of human vi-sion system. It is well-known that the Catch game can help children develop their visual tracking abilities. For comput-ers, we want to design a similar game. It would be ideal if we could throw all kinds of realistic objects with various appearance into the videos, but it is hard to implement. So, we step back and cut a patch from the existing video and let it change and move in the way we have pre-set it. The pre-training objective is to predict the location and size of this patch in all input frames given only the patch information in the initial frame. We call this game Catch-the-Patch (CtP).
Although the concept of the game is simple, it is not an easy task to design the details for the best pretraining re-sults. We ﬁnd that throwing more than one image patch, changing and moving in different patterns, in a video at the same time brings clear beneﬁts. In addition, making image patches invisible, or disappear, from time to time can further exercise the network’s ability to associate adjacent frames.
We call this masked region model (MRM). We train an R3D network [38] and an R(2+1)D network [38] using our in-vented CtP game and apply CtP-pretrained video represen-tation to two downstream tasks, namely action recognition and video clip retrieval.
Experimental results show that CtP signiﬁcantly outper-forms existing proxy tasks in video representation learn-ing. On UCF-101 dataset, our CtP-pretrained R3D model
[38] achieves 86.2% top-1 classiﬁcation accuracy. It out-performs the most advanced method TempTrans [19] by 6% absolute gains. Furthermore, for datasets like Something-something-V1 which require more temporal relationship mining, CtP-pretraining leads to a 48.3% top-1 accuracy.
Surprisingly, it even surpasses the fully supervised coun-terpart (44.1%) by a notable margin. To summarize, the contributions of this work are three-fold:
• Inspired by the Catch game which helps children de-velop their eyes, we design a Catch-the-Patch game for neural networks to learn visual features from videos.
• We scientiﬁcally design the details of the game, includ-ing using more than one patches for training and intro-ducing the MRM. These designs have been carefully validated by ablations studies.
• We carry out comprehensive evaluation of the pro-posed method. CtP pretraining not only achieves state-of-the-art results for standard downstream tasks, but also closes the performance gap between unsupervised and supervised video representation learning. 2.