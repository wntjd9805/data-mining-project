Abstract 1.

Introduction
We present a new framework for semantic segmentation without annotations via clustering. Off-the-shelf clustering methods are limited to curated, single-label, and object-centric images yet real-world data are dominantly uncu-rated, multi-label, and scene-centric. We extend cluster-ing from images to pixels and assign separate cluster mem-bership to different instances within each image. How-ever, solely relying on pixel-wise feature similarity fails to learn high-level semantic concepts and overﬁts to low-level visual cues. We propose a method to incorporate geometric consistency as an inductive bias to learn in-variance and equivariance for photometric and geometric variations. With our novel learning objective, our frame-work can learn high-level semantic concepts. Our method,
PiCIE (Pixel-level feature Clustering using Invariance and
Equivariance), is the ﬁrst method capable of segmenting both things and stuff categories without any hyperparam-eter tuning or task-speciﬁc pre-processing. Our method largely outperforms existing baselines on COCO [31] and
Cityscapes [8] with +17.5 Acc. and +4.5 mIoU. We show that PiCIE gives a better initialization for standard su-pervised training. The code is available at https:// github.com/janghyuncho/PiCIE.
Unsupervised learning from a set of unlabelled images has gained large popularity, but still is mostly limited to single-class, object-centric images. Consider the images shown in Figure 1 (top). Given a collection of these and other unlabeled images, can a machine discover the con-cepts of “grass”, “sky”, “house” and “trees” from each im-age? Going further, can it identify where in each image each concept appears, and segment it out?
A system that is capable of such unsupervised seman-tic segmentation can then automatically discover classes of objects with their precise boundaries, thus removing the substantial cost of collecting and labeling datasets such as
COCO. It might even discover objects, materials and tex-tures that an annotator may not know of a priori. This can be particularly useful for analyzing novel domains: for ex-ample, discovering new kinds of visual structures in satellite imagery. The ability of the system to discover and segment out unknown objects may also prove useful for robots trying to manipulate these objects in the wild.
However, while unsupervised semantic segmentation might be useful, it is also challenging. This is because it combines the problem of class discovery with the chal-lenge of exhaustive pixel labeling. Recent progress in self-432116794
Figure 2: PiCIE overview (left) and illustration of multi-view feature computation (right). More details in Sec. 3.3. supervised and unsupervised learning suggests that recog-nition systems can certainly discover image-level classes.
However, image-level labeling is easier since the network can simply rely on just a few distinctive, stable features and discard the rest of the image. For example, a recognition system might be able to group all four images of Figure 1 together simply by detecting the presence of roof tiles in each image, and ignoring everything else in the images. In contrast, when segmenting the image, no pixel can be ig-nored; whether it is a distinct object (thing) or a background entity (stuff ), each and every pixel must be recognized and accurately characterized in spite of potentially large intra-class variation. As such, very little prior work has tried to tackle this problem of discovering semantic segmentations, with results limited to extremely coarse stuff segmentation.
In this paper, we take a step towards a practically use-ful unsupervised semantic segmentation system: we present an approach that is able to segment out all pixels, be they things or stuff, at a much ﬁner granularity than prior art.
Our approach is based on a straightforward objective that codiﬁes only two common-sense constraints. First, pixels that have a similar appearance (i.e., they cluster together in a learned feature space) should be labeled similarly and vice versa. Second, pixel labels should be invariant to color space transformations and equivariant to geometric trans-formations. Our results show that using these two objectives alone, we can train a ConvNet based semantic segmentation system end-to-end without any labels.
We ﬁnd that in spite of its simplicity, our approach far outperforms prior work on this task, more than doubling the accuracy of prior art (Figure 1, bottom). Our clustering-based loss function (the ﬁrst objective above) leads to a much simpler and easier learning problem compared to prior work, which instead tries to learn parametric pixel classiﬁers. But the invariance and equivariance objectives are key. They allow the convolutional network to connect together pixels across scale, pose and color variation, some-thing that prior systems are unable to do. This increased robustness to invariance also allows our approach to ef-fectively segment objects. We vindicate these intuitions through an ablation study, where we ﬁnd that each of these contributes signiﬁcant improvements in performance.
In sum, our results show that convolutional networks can learn to not only discover image-level concepts, but also semantically parse images without any supervision. This opens the door to true large-scale discovery, where such a trained network can automatically surface new classes of objects, materials or textures from only an unlabeled, uncu-rated dataset. 2.