Abstract
In medical image analysis, it is typical to collect multiple annotations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. Meanwhile, from the computer vision practi-tioner viewpoint, it has been a common practice to adopt the ground-truth labels obtained via either the majority-vote or simply one annotation from a preferred rater. This process, however, tends to overlook the rich information of agreement or disagreement ingrained in the raw multi-rater annotations. To address this issue, we propose to ex-plicitly model the multi-rater (dis-)agreement, dubbed MR-Net, which has two main contributions. First, an expertise-aware inferring module or EIM is devised to embed the expertise level of individual raters as prior knowledge, to form high-level semantic features. Second, our approach is capable of reconstructing multi-rater gradings from coarse predictions, with the multi-rater (dis-)agreement cues being further exploited to improve the segmentation performance.
To our knowledge, our work is the ﬁrst in producing cali-brated predictions under different expertise levels for med-ical image segmentation. Extensive empirical experiments are conducted across ﬁve medical segmentation tasks of di-verse imaging modalities.
In these experiments, superior performance of our MRNet is observed comparing to the state-of-the-arts, indicating the effectiveness and applica-bility of our MRNet toward a wide range of medical seg-mentation tasks. Source code is publicly available. 1.

Introduction
Accurate anatomy and lesion segmentation is crucial in clinical assessment of various diseases, including for exam-Wei Ji, Shuang Yu and Junde Wu have equal contributions. Wei Ji contributes to this work during internship at Tencent Jarvis Lab.
Shuang Yu and Li Cheng are the corresponding authors.
Rater2
Rater5
Grade
Rater6
Rater1
Rater4
Rater3
Fundus image
Optic cup annotations
Optic disc annotations
Figure 1. Top: an exemplar medical image grading scenario con-ducted by multiple raters with different expertise levels. Bottom: visualization of optic cup and disc annotations of the above raters. ple glaucoma [28, 36, 43], prostate diseases [30, 52], and brain tumors [11, 17, 44].
It has been increasingly pop-ular to develop automated segmentation systems, to facil-itate a reliable reference for the quantiﬁcation of disease progression, which is especially accelerated by the excit-ing breakthroughs of deep convolutional neural networks (CNNs) [7, 20, 34, 35, 49, 55, 56, 59] over the past decade.
Different from labelling natural images, medical images are often independently annotated by a group of experts or raters, to mitigate the subjective bias of a particular rater due to factors such as the level of expertise, or possible neg-ligence of subtle symptoms [13, 39, 23, 28]. Inter-observer variability, as frequently reported by relevant research in the clinical ﬁeld, often leads to challenges in segmenting highly uncertain regions [3, 23, 37]. Fig. 1 provides a rep-resentative illustration of the multi-rater grading process in 12341
annotating optic cups and discs from fundus images, with notable uncertainties or disputed regions presented among graders. It is thus necessary for automated systems to con-sider a proper segmentation strategy that reﬂects the un-derlying (dis-)agreement among multiple experts. Existing works typically require unique ground-truth annotations, each pairing with one of the input images to train the deep learning models. It is a common practice to take majority vote, STAPLE [50] or other label fusion strategies to obtain the ground-truth labels [5, 29, 30, 34, 57, 59]. Being sim-ple and easy to implement, this strategy, however, comes at the cost of ignoring altogether the underlying uncertainty information among multiple experts. Very recently, several efforts start to explore the inﬂuence of multi-rater labels by label sampling [19, 24] or multi-head [16] strategies.
It is reported that models trained with multi-rater labels are better calibrated than those with the typical ground-truth label via, e.g. majority vote, which are prone to be over-conﬁdent [19, 24].
Meanwhile, there still lacks a principled approach to in-corporate in training the rich uncertainty information from multiple raters. Speciﬁcally, we focus on the following questions: 1) how to integrate varied expertise-level, or ex-pertness, of individual raters into the network architecture? 2) how to exploit the uncertainty information among differ-ent experts to produce probability maps that better reﬂect the underlying graders’ (dis-)agreement? This inspires us to propose a multi-rater agreement modeling framework, MR-Net. To our knowledge, it is the ﬁrst in explicitly addressing the above-mentioned questions. Our framework has the fol-lowing three main contributions:
• The notion of expertness is explicitly introduced as prior knowledge about the expertise levels of the in-volved multi-raters. It is embedded in the high-level semantic features through the proposed Expertise-aware Inferring Module (EIM), enabling the represen-tation capability to accommodate the multi-rater set-tings.
• A Multi-rater Reconstruction Module (MRM) is de-signed to reconstruct the raw multi-rater gradings from the the expertness prior and the soft prediction of the model. This enables the estimation of an uncertainty map that reﬂects the inter-rater variability, by exploit-ing the intrinsic correlations between the fused soft la-bel and the raw multi-rater annotations.
• To better utilize the rich cues among multi-rater (dis-)agreements, we further incorporate in our framework a Multi-rater Perception Module (MPM), which em-pirically leads to noticeable performance boost.
Extensive experiments are performed on ﬁve different med-ical image segmentation tasks of diverse image modali-ties, including color fundus imaging, computed tomogra-phy (CT), and magnetic resonance imaging (MRI). Overall, our MRNet framework consistently outperforms the state-of-the-art methods as well as existing multi-rater strategies.
In addition, our MRNet runs in real-time (29 frame per sec-ond) at inference stage, making it practically appealing for many real-world applications. 2.