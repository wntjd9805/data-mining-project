Abstract
This paper addresses the challenging unsupervised scene ﬂow estimation problem by jointly learning four low-level vision sub-tasks: optical ﬂow F, stereo-depth D, cam-era pose P and motion segmentation S. Our key insight is that the rigidity of the scene shares the same inherent geo-metrical structure with object movements and scene depth.
Hence, rigidity from S can be inferred by jointly coupling
F, D and P to achieve more robust estimation. To this end, we propose a novel scene ﬂow framework named EfﬁScene with efﬁcient joint rigidity learning, going beyond the ex-In isting pipeline with independent auxiliary structures.
EfﬁScene, we ﬁrst estimate optical ﬂow and depth at the coarse level and then compute camera pose by Perspective-n-Points method. To jointly learn local rigidity, we design a novel Rigidity From Motion (RfM) layer with three prin-cipal components: (i) correlation extraction; (ii) boundary learning; and (iii) outlier exclusion. Final outputs are fused based on the rigid map MR from RfM at ﬁner levels. To efﬁciently train EfﬁScene, two new losses Lbnd and Lunc are designed to prevent trivial solutions and to regularize the ﬂow boundary discontinuity. Extensive experiments on scene ﬂow benchmark KITTI show that our method is ef-fective and signiﬁcantly improves the state-of-the-art ap-proaches for all sub-tasks, i.e. optical ﬂow (5.19 → 4.20), depth estimation (3.78 → 3.46), visual odometry (0.012 → 0.011) and motion segmentation (0.57 → 0.62). 1.

Introduction
Scene ﬂow [38, 37] describes the 3D motion of a dy-namic scene by 2D optical ﬂow and scene depth, provid-ing essential geometrical clues for numerous practical ap-plications such as self-driving [28] and robotics naviga-tion [1, 31]. However, acquiring dense ground truth for both sub-tasks in real applications are usually expensive or impractical. To overcome this, learning scene ﬂow in an unsupervised way has attracted much attention in recent years, by minimizing the photometric differences between the original-synthesized pixel pairs.
Optimizing pixel-wise photometric error for low-level scene ﬂow task without supervision is not a trivial task.
One of the most critical reason is that the pixel correspon-dence between consecutive frames is highly ambiguous, es-pecially in unstructured or texture-less regions. For exam-ple, one pixel from a mountain or a highway surface in frame t can be projected to various surrounding pixels in frame t + 1 with very low photometric error, often leading to the failure of local scene ﬂow estimation. Unfortunately, this issue always happens in outdoor scenarios due to miss-ing small details due to motion blur. Therefore, additional constraints are strongly needed to eliminate the ambiguities for successful unsupervised scene ﬂow estimation.
Figure 1. Main idea of our method. Different from independently estimating rigid pixels from the auxiliary instance segmentation in existing pipeline, we jointly learn per-pixel rigidity from optical
ﬂow, depth and camera pose for more accurate rigid constraint.
Problems.
In recent approaches, rigid constraint is widely employed to separate the scene into static (rigid) and moving (non-rigid) areas. It also restricts the ego-motion of the rigid pixels which obey the rigid scene assumption [25].
To achieve this, current methods [33, 22, 43, 14, 9, 27] fol-low a popular scene ﬂow pipeline as shown in Fig. 1 (left), where the auxiliary instance segmentation network is de-signed to predict the rigid pixels that will be constrained by local rigidity. Though impressive scene ﬂow results can be achieved, the performance of the segmentation is often poor, indicating an inaccurate estimation of rigid pixels (static 5538
area), and it could, in turn, harm the rigid constraint. One reason is that the independent rigidity inference in existing pipeline limits the learning of pixel rigidity. More specif-ically, the segmentation task in existing pipeline is jointly optimized with scene ﬂow sub-tasks in back-propagation, but it is independently launched in forward inference. This independent structure makes inference inefﬁcient, resulting in networks that can only learn pixel-wise rigidity from raw
RGB images, but difﬁcult to extract extra geometrical in-formation from ﬂow and depth. Besides, optimizing both deep segmentation network and scene ﬂow multi-networks in current pipeline without ground truth might be very difﬁ-cult, requiring sophisticated training strategies such as [33].
Inspired from recent works, our key insight is that the rigidity of the scene shares the same inherent geometrical structure with optical ﬂow and depth, hence they are highly correlated and can be mutually ben-eﬁcial. Based on this observation, instead of designing the auxiliary segmentation structure, we jointly consider opti-cal ﬂow, depth and camera pose for rigidity learning as il-lustrated in Fig. 1 (right), and propose a novel framework called EfﬁScene. With the new pipeline, we can go beyond the existing methods by providing: (i) more effective rigid constraint via jointly considering scene ﬂow sub-tasks for learning accurate rigid pixels; and (ii) more efﬁcient scene
ﬂow framework optimization via eliminating the very deep instance segmentation network.
Motivation & Idea.
Approach. EfﬁScene aims to solve the following four unsupervised sub-tasks: (i) optical ﬂow F estimation; (ii) stereo-depth D prediction; (iii) camera pose P for visual odometry; and (iv) motion segmentation S. We ﬁrst esti-mate the optical ﬂow F o and depth D at the coarse level, then compute the relative camera pose P from time t to t+1 by minimizing the reprojection error between the observed coordinates (from F o) and the projected 3D points (from D) via a Perspective-n-Points (PnP) solver. Next, we propose a novel Rigidity From Motion (RfM) layer to estimate pixel rigidity by explicitly modeling the correlation between opti-cal ﬂow F o and rigid ﬂow F r. Our RfM includes three main steps: (i) correlation extraction; (ii) boundary learning; and (iii) outlier exclusion. The rigid map MR from RfM can be interpreted as motion segmentation. Finally, ﬂows from
F o and F r are fused to form the ﬁnal ﬂow, guided by the rigid map MR at the ﬁne level. In training, two new losses – Lbnd and Lunc – are designed to optimize RfM and reg-ularize the ﬂow boundary discontinuity, respectively. Dif-ferent from existing methods [41, 16], there are no sensitive thresholds needed to be set manually in EfﬁScene.
Contributions are summarized as follows.
• We introduce a new structure for unsupervised scene
ﬂow estimation, and demonstrate that per-pixel rigidity can be efﬁciently predicted by jointly learning optical
ﬂow, depth and camera pose.
• We design a novel Rigidity from Motion (RfM) layer to recognize rigid regions via explicitly modeling mo-tion correlations. To the best of our knowledge, this is the ﬁrst deep model for joint rigidity learning.
• We optimize scene ﬂow training by two new losses:
Lbnd prevents the trivial solution of RfM whereas
Lunc regularizes the optical ﬂow discontinuity in un-covered boundary.
Extensive experiments on KITTI benchmarks [4, 5, 29] show that our method outperforms existing state-of-the-art (SOTA) approaches for all four sub-tasks with highly efﬁcient rigidity inference (RfM with size 0.0032Mb vs. 5.22Mb [33]), i.e. optical ﬂow (5.19 → 4.20) by a signif-icant 19% improvement, depth estimation (3.78 → 3.46), visual odometry (0.012 → 0.011), and motion segmenta-tion (0.57 → 0.62). 2.