Abstract
Not all video frames are equally informative for recog-nizing an action. It is computationally infeasible to train deep networks on all video frames when actions develop over hundreds of frames. A common heuristic is uniformly sampling a small number of video frames and using these to recognize the action. Instead, here we propose full video action recognition and consider all video frames. To make this computational tractable, we ﬁrst cluster all frame acti-vations along the temporal dimension based on their simi-larity with respect to the classiﬁcation task, and then tem-porally aggregate the frames in the clusters into a smaller number of representations. Our method is end-to-end train-able and computationally efﬁcient as it relies on tempo-rally localized clustering in combination with fast Ham-ming distances in feature space. We evaluate on UCF101,
HMDB51, Breakfast, and Something-Something V1 and V2, where we compare favorably to existing heuristic frame sampling methods. 1.

Introduction
Videos have arbitrary length with actions occurring at arbitrary moments. Current video recognition methods use
CNNs on coarsely sub-sampled frames [2, 24, 28, 31, 38, 41, 45, 47, 48, 50] because using all frames is computa-tionally infeasible. Sub-sampling, however, can miss cru-cial frames for action recognition. For example, as shown in Fig. 1, sampling the frame with the dish in the pan is cru-cial for correct recognition. We propose to do away with sub-sampling heuristics and argue for leveraging all video frames: Full video action recognition.
It is worth analyzing why training CNNs on full videos is computationally infeasible in terms of memory and calcula-tions. The calculations in the forward pass yield activations, while the backward pass calculations give gradients which are summed over all frames to update the weights. Many of these calculations can be done in parallel and thus are well-suited for modern GPUs. When treating videos as a large collection of image frames, the amount of calculations are not too different from those on large image datasets [5].
Figure 1. Sub-sampling can miss crucial frames in videos and may cause confusion for action recognition: e.g. compare the two sub-samplings heuristics in row 1 and row 2: Without sampling the dish in the pan it is difﬁcult to classify.
Instead, as shown in row 3, we propose to efﬁciently use all frames during training by clustering frame activations along the temporal dimension and aggregating each cluster to a single representation. The temporal clustering is based on Hamming distances over frame activations, which is computationally fast. With the assumption that similar activations have similar gradients, the aggregated representations approximate the individual frame activations. We efﬁciently uti-lize all frames for training without missing important information.
Regarding memory, however, there is a crucial difference between videos and images: The video loss function is not per-frame but on the full video. Hence, to do the backward pass, all activations for each frame, for each ﬁlter in each layer need to be stored in memory. This even doubles for storing their gradients. With 10-30 frames per second, this quickly becomes infeasible for even just a few minutes of video. Existing approaches can trade off memory for com-pute [3, 4, 13] by not storing all intermediate layers, yet they do not scale to video as they would still need to store each frame. The main computational bottleneck for training video CNNs is memory for frame activations.
Here, we propose an efﬁcient method to use all video frames during training. The forward pass computes frame 14892
activations and the backward pass sums the gradients over the frames to update the weights. Now, if only the network was linear, then a huge memory reduction could be gained by ﬁrst summing all frame activations in the forward pass, which would reduce to just a single update in the back-ward pass. Yet, deep networks are infamously non-linear, and have non-linearities in the activation function and in the loss function. Thus, if all frames were independent, treating the non-linear network as linear would introduce considerable approximation errors. However, subsequent frames in a video are strongly correlated, and it’s this corre-lation that makes it possible for existing approaches to use sub-sampling. Instead of sub-sampling, we propose to pro-cess all frames and exploit the frame correlations to create groups of frames where the network is approximately linear.
We use the ReLU (Rectiﬁed Linear Unit) activation func-tion, which is linear when the signs of two activations agree, to estimate which parts of the video are approximately lin-ear. This allows us to develop an efﬁcient clustering algo-rithm based on Hamming distances of frame activations as illustrated in Fig. 1. By then aggregating the approximately linear parts in a video in the forward pass, we make large memory savings in the backward pass while still approxi-mating the full video gradient.
We summarize the contributions of our work as follows:
• We propose a method that allows us to use most or even all video frames for training action recognition by approximated individual frame gradients with the gradients of temporally aggregated frame activations;
• We devise an end-to-end trainable approach for efﬁ-cient grouping of video frames based on temporally localized clustering and Hamming distances;
• Extensive experiments demonstrate that our method compares well to state-of-the-art methods on sev-eral benchmark datasets such as UCF101, HMDB51,
Breakfast, and Something-Something V1 and V2. 2.