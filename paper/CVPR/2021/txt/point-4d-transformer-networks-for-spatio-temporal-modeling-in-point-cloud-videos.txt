Abstract
Point cloud videos exhibit irregularities and lack of or-der along the spatial dimension where points emerge incon-sistently across different frames. To capture the dynamics in point cloud videos, point tracking is usually employed.
However, as points may ﬂow in and out across frames, computing accurate point trajectories is extremely difﬁcult.
Moreover, tracking usually relies on point colors and thus may fail to handle colorless point clouds. In this paper, to avoid point tracking, we propose a novel Point 4D Trans-former (P4Transformer) network to model raw point cloud videos. Speciﬁcally, P4Transformer consists of (i) a point 4D convolution to embed the spatio-temporal local struc-tures presented in a point cloud video and (ii) a transformer to capture the appearance and motion information across the entire video by performing self-attention on the embed-ded local features. In this fashion, related or similar local areas are merged with attention weight rather than by ex-plicit tracking. Extensive experiments, including 3D action recognition and 4D semantic segmentation, on four bench-marks demonstrate the effectiveness of our P4Transformer for point cloud video modeling.
Figure 1. Illustration of point cloud video modeling by our Point 4D Transformer (P4Transformer) network. Color encodes depth.
A point cloud video is a sequence of irregular and unordered 3D coordinate sets. Points in different frames are not consistent. Our
P4Transformer consists of a point 4D convolution and a trans-N ), former. The convolution encodes a point cloud video (3
× where L and N denote the number of frames and the number of
N ′) and points in each frame, to a coordinate tensor (3
N ′). The transformer performs self-L′ a feature tensor (C attention on the embedded tensors to capture the global spatio-temporal structure across the entire point cloud video.
L′
×
×
×
×
×
L 1.

Introduction
Point cloud videos are a rich source of visual informa-tion and can be seen as a window into the dynamics of the 3D world we live in, showing how objects move against backgrounds and what happens when we perform an ac-tion. Moreover, point cloud videos provide more ﬂexibility for action recognition in poor visibility environments, and covers more precise geometry dynamics than conventional videos. Therefore, understanding point cloud videos is im-portant for intelligent systems to interact with the world.
Essentially, a point cloud video is a sequence of 3D coordi-nate sets. When point colors are available, they are often ap-pended as additional features. However, because coordinate sets are irregular and unordered, and points emerge incon-sistently across different sets/frames, modeling the spatio-temporal structure in point cloud videos is extremely chal-lenging.
In order to capture the dynamics from point clouds, one solution is to ﬁrst convert a point cloud video into a se-quence of regular and ordered voxels and then apply con-ventional grid based convolutions to these voxels. However, as points are usually sparse, directly performing convolu-14204
tions on the entire space along the time dimension is compu-tationally inefﬁcient. Therefore, special engineering efforts, e.g., sparse convolution [6], are usually needed. Moreover, voxelization requires additional computation [59], which restricts applications that require real-time processing. An-other solution is to directly model raw point cloud videos by grouping local points, in which point tracking is em-ployed to preserve the temporal structure [36]. However, as points may ﬂow in and out across frames, accurately track-ing points is extremely difﬁcult. In particular, when videos become long, the tracking error increases. Moreover, point tracking usually requires point colors. It may fail to handle colorless point clouds when point colors are not available.
In this paper, to avoid tracking points, we propose a novel Point 4D Transformer Network (P4Transformer) to model the spatio-temporal structure in raw point cloud videos. First, we develop a point 4D convolution to en-code the spatio-temporal local structures in a point cloud video. Our point 4D convolution is directly performed on raw points without voxelization and therefore saves compu-tation time. Moreover, by merging local points along the spatial and temporal dimensions, point 4D convolution re-duces the number of points to be processed by the subse-quent transformer. Second, instead of grouping these em-bedded local areas with tracking [36], we propose to uti-lize the transformer to capture the global appearance and motion information across the entire video. By performing self-attention [53], related local areas are adaptively merged based on the attention weight.
We evaluate our P4Transformer on a video-level classi-ﬁcation task, i.e., 3D action recognition, and a point-level prediction task, i.e., 4D semantic segmentation. Experi-ments on the MSR-Action3D [28], NTU RGB+D 60 [45],
NTU RGB+D 120 [30] and Synthia 4D [6] datasets demon-strate the effectiveness of our method. The contributions of this paper are threefold:
• To avoid point tracking, we propose a transformer for spatio-based network, named P4Transformer, temporal modeling of raw point cloud videos. To the best of our knowledge, we are the ﬁrst to apply trans-former in point cloud video modeling.
• To embed spatio-temporal local structures and reduce the number of points to be processed by transformers, we propose a point 4D convolution.
• Extensive experiments on four datasets show that the proposed P4Transformer effectively improves the ac-curacy of 3D action recognition and 4D semantic seg-mentation. 2.