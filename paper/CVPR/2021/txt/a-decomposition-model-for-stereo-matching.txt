Abstract
In this paper, we present a decomposition model for stereo matching to solve the problem of excessive growth in computational cost (time and memory cost) as the reso-lution increases. In order to reduce the huge cost of stereo matching at the original resolution, our model only runs dense matching at a very low resolution and uses sparse matching at different higher resolutions to recover the dis-parity of lost details scale-by-scale. After the decompo-sition of stereo matching, our model iteratively fuses the sparse and dense disparity maps from adjacent scales with an occlusion-aware mask. A reﬁnement network is also ap-plied to improving the fusion result. Compared with high-performance methods like PSMNet and GANet, our method achieves 10 − 100× speed increase while obtaining compa-rable disparity estimation results. 1.

Introduction
Stereo matching aims to estimate the disparity from a pair of images.
It has various downstream applications, such as 3D reconstruction, AR, autonomous driving, robot navigation, etc. Despite years of research on stereo match-ing, many state-of-the-art methods still face the problem of excessive growth in computational cost and memory con-sumption as the resolution increases. This problem limits the ability of existing methods to process high-resolution images, and restricts the use of stereo matching methods in practical situations with memory/speed constraints.
In this paper, we propose a decomposition model for stereo matching. Compared with the excessive growth of many state-of-the-art methods, our model reduces the growth rate by several orders of magnitude as shown in Fig-ure 1. The design of our model is inspired by the following two observations: (1) It is not necessary to estimate the disparity of all pix-els at the highest resolution, such as the disparities on the
∗Corresponding author 8000 6000
) s m (
Ours
DeepPruner-fast 
PSMNet
GANet-deep 
GWCNet
DSMNet 4000 e m
T i 3000 2000 1500 1000 500 100 500 × 320  1000 × 650  1500 × 1000  2000 × 1300  4500 × 3000  5000 × 3500
Resolution
Figure 1: As the resolution increases, the growth in time cost of state-of-the-art methods on one 1080 Ti GPU with 11GB of memory. The stopped growth of some curves is because the corresponding method cannot run at the ex-pected resolution on the GPU. Compared to GANet [48], our model is 100 times faster. Compared to PSMNet [5], our model achieves almost 15× speed increase. Compared to DeepPruner [8], our model achieves almost twice the run-ning speed and lower memory consumption. wall and the ﬂoor. As long as the content is not signiﬁcantly lost during downsampling, the disparity of most areas can be efﬁciently estimated at low resolution, and then reﬁned at high resolution. (2) It only needs to consider the disparity estimation of some image details that are lost during downsampling.
Fortunately, those lost details are sparse, and their stereo matching is also sparse (i.e., the lost details in the left image mostly only match the lost details in the right image). The sparse matching means less time and memory cost com-pared to the dense matching.
Based on the ﬁrst observation, our model only runs dense matching at a very low resolution (such as 20 × 36, called reference resolution), ensuring disparity estimation for most regions that are not lost during downsampling. Based on the second observation, our model uses a series of sparse 6091  
matching, each at a suitable higher resolution, to recover the disparity of lost details scale-by-scale. By decomposing the original stereo matching into a dense matching at the lowest resolution and a series of sparse matching at higher resolutions, the huge cost of original stereo matching can be signiﬁcantly reduced.
The speciﬁc pipeline of our model is shown in Figure 2.
Our model uses a full cost volume and a cost regularization for dense matching at the reference resolution. From the reference resolution, a series of operations are performed scale-by-scale, until the original input resolution is reached.
These operations include four modules: detail loss detec-tion, sparse matching, disparity upsampling, and disparity fusion. The corresponding implementation of the four mod-ules are as follows: (1) In the detail loss detection module, the lost image details are learned unsupervised based on the square difference between deep features from adjacent scales. (2) In the sparse matching module, the sparse dispar-ity map is estimated via cross-correlation and soft-max un-der the guidance of detected lost details. (3) In the disparity upsampling module, the estimated disparity map from the previous scale is upsampled to the resolution at the current scale via content-aware weights. (4) In the disparity fusion module, the results of the disparity upsampling module and sparse matching module are fused via an occlusion-aware soft mask. A reﬁnement network is also used in this mod-ule to improve the fused disparity map.
We analyze the complexity of stereo matching in this pa-per. For convenience, we deﬁne the complexity of matching as the size of the search space [21]. We prove that the com-plexity of original dense stereo matching grows cubically as the input resolution increases, while the complexity of sparse matching in our model only grows logarithmically.
In our model, the complexity of dense matching at the ref-erence resolution is ﬁxed, and independent of the input res-olution. At the same time, the three other operations, i.e., detail loss detection, disparity upsampling, and disparity fu-sion, could be efﬁciently implemented.
In the experiment, we compare our model with state-of-the-art methods over the growth in computation cost and memory consumption. The results show that our model re-duces the growth rate by several orders of magnitude. We also compare our model with state-of-the-art methods on
Scene Flow dataset [23], KITTI 2015 dataset [24, 25, 26], and Middlebury dataset [34]. The results show that our model is comparable to or even better than state-of-the-art methods with much faster running time and much lower memory cost. 2.