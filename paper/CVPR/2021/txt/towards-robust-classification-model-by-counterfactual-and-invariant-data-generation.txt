Abstract
Despite the success of machine learning applications in science, industry, and society in general, many approaches are known to be non-robust, often relying on spurious cor-relations to make predictions. Spuriousness occurs when some features correlate with labels but are not causal; re-lying on such features prevents models from generalizing to unseen environments where such correlations break. In this work, we focus on image classiﬁcation and propose two data generation processes to reduce spuriousness. Given human annotations of the subset of the features responsible (causal) for the labels (e.g. bounding boxes), we modify this causal set to generate a surrogate image that no longer has the same label (i.e. a counterfactual image). We also alter non-causal features to generate images still recognized as the original labels, which helps to learn a model invariant to these features. In several challenging datasets, our data generations outperform state-of-the-art methods in accuracy when spurious correlations break, and increase the saliency focus on causal features providing better explanations. 1.

Introduction
What makes an image be labeled as a cat? What makes a doctor think there is a tumor in a CT scan? What makes a human label a movie review as positive or negative? These questions are inherently causal, but typical machine learn-ing models rely on associations between features and labels rather than causation. Especially in high-dimensional fea-ture spaces with strong correlations, learning which sets of features are right (causal) associations to predict targets becomes difﬁcult, as different sets can result in the same best training accuracy. Because of this, we see issues such as spurious correlations [12], artifacts [15], lack of robust-ness [2, 5], and discrimination [18] happening across many machine learning ﬁelds.
Spurious associations happen when factors correlate with labels but are not causal. We might consider factors as spu-rious associations if intervening on such factors would not change the resulting labels. In the context of images, the backgrounds of images can be a source of spurious correla-tions with labels (e.g. a forest background correlates with bird label) because changing (intervening on) backgrounds should not affect the labels of the foreground classiﬁcation.
In this paper, we aim to address such spurious associations in the typical ML classiﬁcation framework by incorporating human causal knowledge. Given a human rationale behind a labeling process (e.g. this part of the image is cat-like), we augment our datasets to break the correlations between backgrounds and labels in two ways. First, we generate counterfactuals that ask "how can we modify the image such that a human would no longer label it as a cat?" That is, by removing the causal features (foreground region containing the cat), and imputing it in a way that is consistent with the background, we generate the counterfactual image that would not be labeled by humans as a cat. Second, we in-tervene on the non-causal factors (i.e. image backgrounds) to generate new images still containing the cat but with a modiﬁed background. This helps the model be invariant to such factors. We experiment on several large-scale datasets and show our methods consistently improve the accuracy and saliency focus on causal features. Our contributions can be summarized as follows:
• We use various counterfactual and invariant data genera-tions to augment training datasets which makes models more robust to spurious correlations.
• We show that our augmentations lead to similar or better accuracy than state-of-the-art saliency regularization and other robustness baselines on challenging datasets in the presence of background shifts. We also ﬁnd combining our augmentations with saliency regularization can further improve performance.
• Our methods have stronger salience focus on causal fea-tures that provide better explanations, although we ﬁnd strong salience on causal features only correlates weakly with good generalization. 15212
2.