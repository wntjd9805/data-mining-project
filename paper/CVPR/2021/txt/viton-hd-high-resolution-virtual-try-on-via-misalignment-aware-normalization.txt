Abstract
The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by ﬁtting the item to the desired body part and fusing the warped item with the person. While an increasing number of studies have been conducted, the resolution of synthesized images is still lim-ited to low (e.g., 256×192), which acts as the critical lim-itation against satisfying online consumers. We argue that the limitation stems from several challenges: as the resolu-tion increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the ﬁnal results; the architectures used in ex-* These authors contributed equally. isting methods have low performance in generating high-quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024×768 virtual try-on images. Speciﬁcally, we ﬁrst prepare the segmentation map to guide our virtual try-on synthesis, and then roughly ﬁt the target clothing item to a given person’s body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS genera-tor to handle the misaligned areas and preserve the details of 1024×768 inputs. Through rigorous comparison with ex-isting methods, we demonstrate that VITON-HD highly sur-passes the baselines in terms of synthesized image quality both qualitatively and quantitatively. 14131
1.

Introduction
Image-based virtual try-on refers to the image generation task of changing the clothing item on a person into a differ-ent item, given in a separate product image. With a growing trend toward online shopping, virtually wearing the clothes can enrich a customer’s experience, as it gives an idea about how these items would look on them.
Virtual try-on is similar to image synthesis, but it has unique and challenging aspects. Given images of a person and a clothing product, the synthetic image should meet the following criteria: (1) The person’s pose, body shape, and identity should be preserved. (2) The clothing prod-uct should be naturally deformed to the desired clothing re-gion of the given person, by reﬂecting his/her pose and body shape. (3) Details of the clothing product should be kept in-tact. (4) The body parts initially occluded by the person’s clothes in the original image should be properly rendered.
Since the given clothing image is not initially ﬁtted to the person image, fulﬁlling these requirements is challenging, which leaves the development of virtual try-on still far be-hind the expectations of online consumers. In particular, the resolution of virtual try-on images is low compared to the one of normal pictures on online shopping websites.
After Han et al. [10] proposed VITON, various image-based virtual try-on methods have been proposed [27, 32, 31, 6]. These methods follow two processes in common: (1) warping the clothing image initially to ﬁt the human body; (2) fusing the warped clothing image and the image of the person that includes pixel-level reﬁnement. Also, several recent methods [9, 32, 31] add a module that generates seg-mentation maps and determine the person’s layout from the
ﬁnal image in advance.
However, the resolution of the synthetic images from the previous methods is low (e.g., 256×192) due to the follow-ing reasons. First, the misalignment between the warped clothes and a person’s body results in the artifacts in the misaligned regions, which become noticeable as the image size increases. It is difﬁcult to warp clothing images to ﬁt the body perfectly, so the misalignment occurs as shown in
Fig. 2. Most of previous approaches utilize the thin-plate spline (TPS) transformation to deform clothing images. To accurately deform clothes, ClothFlow [9] predicts the op-tical ﬂow maps of the clothes and the desired clothing re-gions. However, the optical ﬂow maps does not remove the misalignment completely on account of the regulariza-tion. In addition, the process requires more computational costs than other methods due to the need of predicting the movement of clothes at a pixel level. (The detailed analysis of ClothFlow is included in the supplementary.) Second, a simple U-Net architecture [22] used in existing approaches is insufﬁcient in synthesizing initially occluded body parts in ﬁnal high-resolution (e.g., 1024×768) images. As noted in Wang et al. [28], applying a simple U-Net-based archi-Figure 2: An example of misaligned regions. tecture to generate high-resolution images leads to unstable training as well as unsatisfactory quality of generated im-ages. Also, reﬁning the images once at the pixel level is in-sufﬁcient in preserving the details of high-resolution cloth-ing images.
To address the above-mentioned challenges, we pro-pose a novel high-resolution virtual try-on method, called
VITON-HD. In particular, we introduce a new clothing-agnostic person representation that leverages the pose in-formation and the segmentation map so that the clothing information is eliminated thoroughly. Afterwards, we feed the segmentation map and the clothing item deformed to
ﬁt the given human body to the model. Using the addi-tional information, our novel ALIgnment-Aware Segment (ALIAS) normalization removes information irrelevant to the clothing texture in the misaligned regions and propa-gates the semantic information throughout the network. The normalization separately standardizes the activations corre-sponding to the misaligned regions and the other regions, and modulates the standardized activations using the seg-mentation map. Our ALIAS generator employing ALIAS normalization synthesizes the person image wearing the tar-get product while ﬁlling the misaligned regions with the clothing texture and preserving the details of the clothing item through the multi-scale reﬁnement at a feature level.
To validate the performance of our framework, we col-lected a 1024×768 dataset that consists of pairs of a per-son and a clothing item for our research purpose. Our ex-periments demonstrate that VITON-HD signiﬁcantly out-performs the existing methods in generating 1024×768 im-ages, both quantitatively and qualitatively. We also conﬁrm the superior capability of our novel ALIAS normalization module in dealing with the misaligned regions.
We summarize our contributions as follows:
• We propose a novel image-based virtual try-on ap-proach called VITON-HD, which is, to the best of our knowledge, the ﬁrst model to successfully synthesize 1024×768 images.
• We introduce a clothing-agnostic person representa-tion that allows our model to remove the dependency on the clothing item originally worn by the person. 14132
• To address the misalignment between the warped clothes and the desired clothing regions, we propose
ALIAS normalization and ALIAS generator, which is effective in maintaining the details of clothes.
• We demonstrate the superior performance of our method through experiments with baselines on the newly collected dataset. 2.