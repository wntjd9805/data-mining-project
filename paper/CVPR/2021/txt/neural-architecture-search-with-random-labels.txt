Abstract
In this paper, we investigate a new variant of neural architecture search (NAS) paradigm – searching with ran-dom labels (RLNAS). The task sounds counter-intuitive for most existing NAS algorithms since random label provides few information on the performance of each candidate ar-chitecture.
Instead, we propose a novel NAS framework based on ease-of-convergence hypothesis, which requires only random labels during searching. The algorithm in-volves two steps: ﬁrst, we train a SuperNet using ran-dom labels; second, from the SuperNet we extract the sub-network whose weights change most signiﬁcantly during the training. Extensive experiments are evaluated on multiple datasets (e.g. NAS-Bench-201 and ImageNet) and multiple search spaces (e.g. DARTS-like and MobileNet-like). Very surprisingly, RLNAS achieves comparable or even better re-sults compared with state-of-the-art NAS methods such as
PC-DARTS, Single Path One-Shot, even though the coun-terparts utilize full ground truth labels for searching. We hope our ﬁnding could inspire new understandings on the essential of NAS. 1.

Introduction
Recent years Neural Architecture Search [49, 2, 50, 47, 48, 29, 35, 38, 10] (NAS) has received much attention in the community as its superior performances over human-designed architectures on a variety of tasks such as image classiﬁcation [38, 39, 19], object detection [10, 16] and se-mantic segmentation [27]. In general, most existing NAS frameworks can be summarized as a nested bilevel opti-mization, formulated as follows:
⋆ a
= argmax a∈A
Score (a, W⋆ a) s.t. W⋆ a = argmin
W
L (a, W) , (1) (2)
*Corresponding author. This work is supported by The National Key
Research and Development Program of China (No.2017YFA0700800) and
Beijing Academy of Artiﬁcial Intelligence (BAAI). where a is a candidate architecture with weights Wa sam-pled from the search space A; L(·) represents the training loss; Score(·) means the performance indicator (e.g. accu-racy in supervised NAS algorithms or pretext task scores in unsupervised NAS frameworks [28]) evaluated on the val-idation set. Brieﬂy speaking, the NAS paradigm aims to search for the architecture which obtains the best validation performance, thus we name it performance-based NAS in the remaining text.
Despite the great success, to understand why and how performance-based NAS works is still an open question.
Especially, the mechanism how NAS algorithms discover good architectures from the huge search space is well worth study. A recent literature [37] analyzes the searching re-sults under cell-based search spaces and reveals that ex-isting performance-based methods tend to favor architec-tures with fast convergence. Although Shu et al. [37] fur-ther empirically ﬁnd that architectures with fast conver-gence can not achieve the highest generalization perfor-mance, the fast convergence connection pattern still implies that there may exist high correlations between architec-tures with fast convergence and the ones with high perfor-mance (named ease-of-convergence hypothesis for short).
Inspired by the hypothesis, we propose an alternative NAS paradigm, convergence-based NAS, as follows:
Convergence (a, W⋆ a)
= argmax (3) a
⋆ a∈A s.t. W⋆ a = argmin
L (a, W) , (4)
W where Convergence(·) is a certain indicator to measure the speed of convergence; other notations follow the same deﬁ-nitions as in Eq. 1, 2.
In this paper we mainly investigate convergence-based
NAS frameworks, which is rarely explicitly explored in pre-vious works to our knowledge. First of all, we study the role of labels in both frameworks. In performance-based
NAS, we notice that feasible labels are critical in both search steps: for Eq. 1 step, since we need to select the architecture with the highest validation performance, reasonable labels such as ground truths or at least carefully-designed pretext task (e.g. rotation prediction [17]) labels in unsupervised 10907
NAS [28] are required for evaluation. For Eq. 2 step such corresponding labels are also necessary in the training set to optimize the weights. While in convergence-based NAS,
Eq. 3 only depends on a metric to estimate the convergence speed, which is free of labels. Though the optimization in Eq. 4 still needs labels, the purpose of the training is just to provide the evidence for the benchmark in Eq. 3 rather than accurate representations. So, we conclude that in convergence-based NAS the requirement of labels is much weaker than that in performance-based NAS.
The observation motivates us to take a further step: in convergence-based NAS, can we use only random labels for search, instead of any feasible labels like ground truths or pretext task labels entirely? To demonstrate it, we propose a novel convergence-based NAS framework, called Ran-dom Label NAS (RLNAS), which only requires random la-bels to search. RLNAS follows the paradigm of Eq. 3, 4.
In Eq. 4 step, random labels are adopted to optimize the weight for each sampled architecture a; while in Eq. 3 step, a customized angle metric [21] is introduced to measure the distance between trained and initialized weights, which es-timates the convergence speed of the corresponding archi-tecture. To speed up the search procedure, RLNAS further utilizes the mechanism of One-Shot NAS [3, 19] to decou-ple the nested optimization of Eq. 3 and Eq. 4 into a two-step pipeline: ﬁrst training a SuperNet with random labels, then extracting the sub-network with the fastest conver-gence speed from the SuperNet using evolutionary search.
We evaluate our RLNAS in popular search spaces like
NAS-Bench-201 [15], DARTS [30] and MobileNet-like search space [5]. Very surprisingly, though RLNAS does not use any feasible labels, it still achieves comparable or even better performances on multiple benchmarks than many supervised/unsupervised methods, including state-of-the-art NAS frameworks such as PC-DARTS [43], Single-Path One-Shot [19], FairDARTS [13], FBNet [40] and Un-NAS [28]. Moreover, networks discovered by RLNAS are also demonstrated to transfer well in the downstream tasks such as object detection and semantic segmentation.
In conclusion, the major contribution of the paper is that we propose a new convergence-based NAS framework RL-NAS, which makes it possible to search with only random labels. We believe the potential of RLNAS may includes:
A simple but stronger baseline. Compared with the widely used random search [24] baseline, RLNAS is much more powerful, which can provide a stricter validation for future NAS algorithms.
Inspiring new understandings on NAS. Since the per-formance of RLNAS is as good as many supervised NAS frameworks, on one hand, it further validates the effective-ness of ease-of-convergence hypothesis. On the other hand, however, it suggests that the ground truth labels or NAS on speciﬁed tasks do not help much for current NAS algo-rithms, which implies that architectures found by existing
NAS methods may still be suboptimal. 2.