Abstract
Scalable sensor simulation is an important yet challeng-ing open problem for safety-critical domains such as self-driving. Current works in image simulation either fail to be photorealistic or do not model the 3D environment and the dynamic objects within, losing high-level control and physical realism. In this paper, we present GeoSim, a geometry-aware image composition process which syn-thesizes novel urban driving scenarios by augmenting ex-isting images with dynamic objects extracted from other scenes and rendered at novel poses. Towards this goal, we
ﬁrst build a diverse bank of 3D objects with both realis-tic geometry and appearance from sensor data. During simulation, we perform a novel geometry-aware simulation-by-composition procedure which 1) proposes plausible and realistic object placements into a given scene, 2) renders novel views of dynamic objects from the asset bank, and 3) composes and blends the rendered image segments. The resulting synthetic images are realistic, trafﬁc-aware, and geometrically consistent, allowing our approach to scale to complex use cases. We demonstrate two such impor-tant applications: long-range realistic video simulation across multiple camera sensors, and synthetic data gen-eration for data augmentation on downstream segmentation tasks. Please check https://tmux.top/publication/geosim/ for high-resolution video results. 1.

Introduction
Walking along an empty pavement on a silent Sunday morning, one can easily fantasize how busy it could look dur-ing rush hour on a weekday, or how a parked car might look when driving on a different street. Humans are capable of
*Equal Contribution recreating the experience of visually perceiving objects and scenes to generate new visual data in their minds. Such an ability allows us to formulate novel scenarios and synthesize events in our heads without experiencing it directly.
Researchers have devoted signiﬁcant effort towards en-hancing computers with the capability of creating pictures by replicating visual content [68]. This brings immense value to many industries, such as ﬁlm making, robot simulation, augmented reality, and teleconferencing. In the literature, two main paradigms exist: computer graphics approaches and image editing methods. Computer graphics models the image generation process with physics, by ﬁrst creating a virtual 3D world and then mimicking how light is transmitted within the world to produce a realistic scene rendering.
To produce visually appealing results, physics-based rendering requires a signiﬁcant amount of computing re-sources, costly manual asset creation, and physical modeling
[1]. Images produced by existing real-time rendering en-gines [22, 53, 19], still have a signiﬁcant realism gap, reduc-ing their impact in robot simulation and data augmentation for training. Data-driven image editing methods such as image composition [38, 43, 16, 20, 5] and generative image synthesis [72, 75, 71, 11, 35, 56] have received signiﬁcant attention over the past few years. They focus on pushing realism through generative models trained from large-scale visual data. However, most of the efforts do not correspond to an underlying realistic 3D world, and as a consequence, the generated 2D contents are not directly useful for applica-tions such as 3D gaming and robot simulation.
In this paper, we propose GeoSim, a realistic image manip-ulation framework that inserts dynamic objects into existing videos. GeoSim exploits a combination of both data-driven approaches and computer graphics to generate assets in-expensively while maintaining high visual quality through physically grounded simulation. In particular, by leveraging low-cost bounding box annotations and sensor data captured 7230
Figure 1: Realistic video simulation via geometry-aware composition for self-driving. We proposed a novel data-driven image manipulation approach that inserts dynamic objects into existing videos. Our resulting synthetic video footages are highly realistic, layout-aware, and geometrically consistent, allowing image simulation to scale to complex use cases. by a self-driving ﬂeet driving around multiple U.S. cities,
GeoSim builds a fully-textured large-scale 3D assets bank.
While self-driving data is widely available [25, 9, 7, 64], it is non-trivial to automatically build these assets due to the sparsity of the 3D observations, occlusions, shadows, limited viewpoints, and lighting changes. Our asset reconstruction is robust to these challenges, as we ensure consistency across multiple observations in time and learn a strong shape prior to regularize our assets. GeoSim then exploits the 3D scene layout (from high-deﬁnition (HD) maps and LiDAR data) to add vehicles in plausible locations and make them behave realistically by considering the full scene. Finally, using this new 3D scene, GeoSim performs image-based rendering to properly handle occlusions, and neural network-based image in-painting to ensure the inserted object seamlessly blends in by ﬁlling holes, adjusting color inconsistencies due to lighting changes, and removing sharp boundaries.
Using GeoSim, our resulting synthetic images and video footages are realistic, dynamically plausible, and geometri-cally consistent. We showcase two important applications: long-range realistic video simulation across multiple camera sensors and synthetic labeled data generation for training self-driving perception algorithms. Our approach outper-forms prior work on both qualitative and quantitative realism metrics. We also see signiﬁcant gains on perception perfor-mance when leveraging GeoSim images. These experiments suggest the potential of GeoSim for a plethora of applica-tions, such as realistic safety veriﬁcation, data augmentation,
Sim2Real, augmented reality, and automatic video editing. 2.