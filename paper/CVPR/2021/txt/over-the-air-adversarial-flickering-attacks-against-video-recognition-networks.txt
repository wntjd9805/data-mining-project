Abstract
Deep neural networks for video classiﬁcation, just like image classiﬁcation networks, may be subjected to adver-sarial manipulation. The main difference between image classiﬁers and video classiﬁers is that the latter usually use temporal information contained within the video.
In this work we present a manipulation scheme for fooling video classiﬁers by introducing a ﬂickering temporal per-turbation that in some cases may be unnoticeable by hu-man observers and is implementable in the real world. Af-ter demonstrating the manipulation of action classiﬁcation of single videos, we generalize the procedure to make uni-versal adversarial perturbation, achieving high fooling ra-tio. In addition, we generalize the universal perturbation and produce a temporal-invariant perturbation, which can be applied to the video without synchronizing the pertur-bation to the input. The attack was implemented on sev-eral target models and the transferability of the attack was demonstrated. These properties allow us to bridge the gap between simulated environment and real-world application, as will be demonstrated in this paper for the ﬁrst time for an over-the-air ﬂickering attack. (a) Diagram of a Flickering Adversarial Attack in a simulated en-vironment (digital). (b) Diagram of an Over-the-Air Flickering Adversarial Attack in the real-world (physical).
Figure 1: Top ﬁgure shows the attack diagram in the digital domain performed by adding a uniform RGB perturbation to the attacked video. Bottom ﬁgure shows the modeling of the digitally-developed attack into the real-world by trans-mitting the perturbation in the scene using a smart RGB led bulb. 1.

Introduction
In recent years, Deep Neural Networks (DNNs) have shown phenomenal performance in a wide range of tasks, such as image classiﬁcation [12], object detection [18], se-mantic segmentation [22] etc. Despite their success, DNNs have been found vulnerable to adversarial attacks. Many works [27, 5, 17] have shown that a small (sometimes im-perceptible) perturbation added to an image, can make a given DNNs prediction false. These ﬁndings have raised
*Equal contribution many concerns, particularly for critical systems such as face recognition systems [25], surveillance cameras [24], au-tonomous vehicles, and medical applications [16]. In recent years most of the attention was given to the study of adver-sarial patterns in images and less in video action recogni-tion. Only in the past two years works on adversarial video attacks were published [33, 8, 34, 10], even though DNNs have been applied to video-based tasks for several years, in particular video action recognition [2, 32, 4]. In video action recognition networks temporal information is of the essence in categorizing actions, in addition to per-frame im-515
age classiﬁcation. In some of the proposed attacks the em-phasis was, beyond adversarial categorization, the sparsity of the perturbation. In our work, we consider adversarial attacks against video action recognition under a white-box setting, with an emphasis on the imperceptible nature of the perturbation in the spatio-temporal domain to the hu-man observer and implementability of the generalized ad-versarial perturbation in the real-world. We introduce ﬂick-ering perturbations by applying a uniform RGB perturba-tion to each frame, thus constructing a temporal adversar-ial pattern. Unlike previous works, in our case sparsity of the pattern is undesirable, because it helps the adversarial perturbation to be detectable by human observers for its un-natural pattern, and to image based adversarial perturbation detectors for the exact same reason. The adversarial pertur-bation presented in this work does not contain any spatial information on a single frame other than a constant offset.
This type of perturbation often occurs in natural videos by changing lighting conditions, scene changes, etc.
In this paper, we aim to attack the video action recognition task
[11]. For the targeted model we focus on the I3D [2] model (Speciﬁcally we attack the RGB stream of the model, rather than on the easier to inﬂuence optical ﬂow stream) based on InceptionV1 [26] and we expand our experiments to ad-ditional models from [29]. The attacked models trained on the Kinetics-400 Human Action Video Dataset [11].
In order to make the adversarial perturbation unnotice-able by human observers, we reduce the thickness and tem-poral roughness of the adversarial perturbation, which will be deﬁned later in this paper. In order to do so we apply two regularization terms during the optimization process, each corresponds to a different effect of the perceptibly of the adversarial pattern. In addition, we introduce a modi-ﬁed adversarial-loss function that allows better integration of these regularization terms with the adversarial loss.
We will ﬁrst focus on the I3D [2] network and introduce a ﬂickering attack on a single video and present the trade-off between the different regularization terms. We con-struct universal perturbations that generalize over classes and achieve 93% fooling ratio. Another signiﬁcant fea-ture of our proposed method is time invariant perturbations that can be applied to the classiﬁer without synchroniza-tion. This makes the perturbation relevant for real world scenarios, since frame synchronization is rarely possible.
We show the effectiveness of the ﬂickering attack on other models [29] and the inter-model transferability, and ﬁnally demonstrate the over-the-air ﬂickering attack in a real world scenario for the ﬁrst time. A diagram of the digital attack and the over-the-air attack pipelines are shown in Figure 1.
The main contributions of this work are:
• A methodology for developing ﬂickering adversarial attacks against video action recognition networks that incorporates a new type of regularization for affecting the visibility of the adversarial pattern.
• A universal time-invariant adversarial perturbation that does not require frame synchronization.
• Adversarial attacks that are transferable between dif-ferent networks.
• Adversarial attacks that are implementable using tem-poral perturbations.
The paper is organized as follows: We brieﬂy review related work and present the ﬂickering adversarial attack.
Then we show experimental results and the generalization of the attack. Finally, we present real world examples of the
ﬂickering adversarial attacks, followed by conclusions and future work. We encourage the readers to view the attack videos1, over-the-air scene-based attack videos2, and over-the-air universal attack videos3. Our code can be found in the following repository4. 2.