Abstract
Recent studies propose membership inference (MI) at-tacks on deep models, where the goal is to infer if a sample has been used in the training process. Despite their apparent success, these studies only report accuracy, precision, and recall of the positive class (member class). Hence, the per-formance of these attacks have not been clearly reported on negative class (non-member class). In this paper, we show that the way the MI attack performance has been reported is often misleading because they suffer from high false positive rate or false alarm rate (FAR) that has not been reported.
FAR shows how often the attack model mislabel non-training samples (non-member) as training (member) ones. The high
FAR makes MI attacks fundamentally impractical, which is particularly more signiﬁcant for tasks such as membership inference where the majority of samples in reality belong to the negative (non-training) class. Moreover, we show that the current MI attack models can only identify the mem-bership of misclassiﬁed samples with mediocre accuracy at best, which only constitute a very small portion of training samples.
We analyze several new features that have not been com-prehensively explored for membership inference before, in-cluding distance to the decision boundary and gradient norms, and conclude that deep models’ responses are mostly similar among train and non-train samples. We conduct several experiments on image classiﬁcation tasks, including
MNIST, CIFAR-10, CIFAR-100, and ImageNet, using various model architecture, including LeNet, AlexNet, ResNet, etc.
We show that the current state-of-the-art MI attacks cannot achieve high accuracy and low FAR at the same time, even when the attacker is given several advantages. The source code is available at https://github.com/shrezaei/MI-Attack. 1.

Introduction
Table 1: Complete evaluation of CIFAR-100 with three dif-ferent target models. Almost all papers report the third sec-tion that includes accuracy, precision, recall, and F1 score.
The second section, including train and test accuracy of the target victim model, is missing in many papers in literature despite its usefulness in evaluating the generalization gap (and degree of overﬁtting). The last section that includes balanced accuracy and FAR has never been reported, but it is of paramount importance for understanding the performance of attack models in practice.
Dataset
Model
Cifar-100
AlexNet
Cifar-100
ResNet
Cifar-100
DenseNet
Target Model Train Acc.
Target Model Test Acc.
Attack Acc.
Attack Precision
Attack Recall
Attack F1
Attack Bal. Acc.
Attack FAR 92.48% 43.87% 82.62% 91.90% 86.92% 89.23% 74.02% 38.89% 95.80% 74.14% 79.13% 87.3% 87.85% 87.45% 61.70% 64.45% 99.98% 82.83% 87.74% 86.97% 98.29% 92.26% 66.65% 65.00%
These MI attack models often use conﬁdence values of the target model to infer the membership of an input sample.
High MI attack accuracy is often justiﬁed by claiming that deep learning models are more conﬁdent towards the training (member) samples than the samples they have not seen dur-ing training1. Consequently, MI attack accuracy is reported to be highly correlated to model’s overﬁtting or generaliza-tion gap [19, 20, 22] because an overﬁtted model should perhaps behave even more conﬁdent towards training sam-ples.
In this paper, we show that the way the previous papers re-port the attack performance do not reveal how exactly these attacks perform in practice and can be misleading. First, many papers do not provide the train and test accuracy of the target victim models. Hence, it is not clear whether the target
There is an extensive recent literature on membership inference (MI) attacks on deep learning models that achieve high MI attack accuracy [20, 19, 12, 22, 13, 24, 14, 26]. 1In this paper, we use training samples, member samples, and positive samples interchangeably. Likewise, we also use non-training, test, non-member, and negative samples interchangeably. 7892
model is well-trained. We only ﬁnd a handful of papers that report such metrics [20, 13, 17, 8, 9]. Even in these cases, one can clearly spot impractical target models where general-ization gap is sometimes larger than 35% [20, 13], 50% [17], or 80% [8]. Clearly, such extremely overﬁtted models have no practical use and the results on such models should not be generalized to well-trained models. Second, all papers we have examined limit their reporting to accuracy, precision, and recall. Such a reporting does not reveal the performance of attack models on negative samples (non-member), es-pecially how many negative samples are misclassiﬁed as positive (false positive). For binary classiﬁcation tasks, this is of crucial importance, especially when the negative class can signiﬁcantly outnumber the positive class (e.g., all possi-ble images vs. the limited number of images used to train an image classiﬁcation model). A good practice, which is also common in other ﬁelds such as intrusion detection systems
[25], is to report false positive rate (FPR) or false alarm rate (FAR) alongside the other metrics. A good attack model should have a low FAR.
To evaluate the feasibility of MI attack, we tried to repro-duce the results in [17] for CIFAR-100 dataset, presented in
Table 12. Although the generalization gap of AlexNet is high (∼ 48%), we keep the results for the sake of comparison.
As it is shown, commonly used metrics, including accuracy, precision, and recall, do not reveal how attack models really perform on non-member samples. The high FAR of these at-tacks make them unreliable. Interestingly, even the attack on an extremely overﬁtted model such as AlexNet still suffers from high FAR.
In this paper, we ﬁrst elaborate on why previous reporting practices are misleading in membership inference research.
Second, we provide a comprehensive evaluation of mem-bership inference attacks on deep learning models. We give as much advantage as possible to an attacker and we show that a reliable MI attack with high accuracy and low FAR is hard to achieve. We show that the reason MI attacks often fail is not because attack models are trained poorly. The reason is that the statistical properties of the features used in
MI attacks are not clearly different and distinguishable for training and non-training sample.
To provide an insight on why membership inference of some samples are possible, we separate datasets into two parts: correctly classiﬁed samples (by the target victim model) and misclassiﬁed samples. In general, we ﬁnd that membership inference of correctly classiﬁed samples, inde-pendent of what dataset or model is used, is a more difﬁcult task than the membership inference of misclassiﬁed samples.
This is because deep learning models often behave simi-2In literature, we have only found two papers with public source codes
[22, 19]. We run their implementation on CIFAR-10 and observed the same problem. They both suffer from high FAR, which has not been reported before. larly on train and non-train samples when they are correctly classiﬁed. This observation sheds light on the difﬁculty of membership inference on deep models.
Our contributions are summarized as follows:
• We show that attack accuracy, precision, and recall are not enough to show the performance of MI attacks, particularly on negative (non-member) samples. In-stead, we should also report FAR (or other substitutes explained in Sec.3) and train/test accuracy of target models to better demonstrate the performance of MI at-tacks. Moreover, we study the performance of correctly classiﬁed samples and misclassiﬁed samples separately.
We show that membership inference of correctly classi-ﬁed samples, to which the majority of training samples belong, is a very difﬁcult task.
• We perform MI attack on various image datasets (including MNIST, CIFAR-10, CIFAR-100, and Im-ageNet), and models (LeNet, AlexNet, ResNet,
DenseNet, InceptionV3, Xception, etc), some of which are studied for the ﬁrst time in the MI context. We conduct experiments such that they give a lot more advantages to the attacker than in any previous work.
Even in this case, we show that a meaningful member-ship inference attack with high accuracy and low FAR is often not achievable.
• In addition to conﬁdence values of the target (victim) model, we extensively analyze and use other informa-tion available from the target model, including values from intermediate layers, the gradient w.r.t input, gradi-ent w.r.t to model weights, and distance to the decision boundary. In some cases, these types of information slightly leak more membership status than conﬁdence values, but they are still not sufﬁcient for a reliable MI attack in practice. Surprisingly, all evidence suggests that deep models often behave similarly on train and non-train samples across all these metrics. The only considerable difference appears between correctly clas-siﬁed samples and misclassiﬁed samples, not between the train and non-train samples.
In summary, our experiments, including the reproduction of results in the literature, suggest membership inference of correctly classiﬁed samples, to which the majority of training samples belong, is a difﬁcult task. Clearly more research is needed and we are hesitant to generalize our results to all scenarios, some of which are as follows:
• We mainly focus on vision tasks with high dimensional input. Membership inference for other tasks with low dimensional input may culminate in a different result.
• We do not extend our conclusion to generative models.
High capacity generative models can often memorize 7893
Table 2: Performance evaluation of an MI attack model when balancedness of the evaluation set is changed.
Balancedness
Attack Model
Positive (member) Class
Negative (non-member) Class
Balanced Accuracy
FAR
-5:1 5:1 1:1 1:1 1:5 1:5
-Precision
Recall
Precision
Recall
MI Attack
ZeroR
MI Attack
ZeroR
MI Attack
ZeroR 87.30% 83.33% 57.68% 50.0% 21.41% 16.66% 87.85% 100.0% 87.42% 100.0% 87.82% 100.0% 38.18% 0.0% 74.49% 0.0% 93.57% 0.0% 35.55% 0.0% 35.42% 0.0% 35.73% 0.0%
-61.70% 50.0% 61.22% 50.0% 61.28% 50.0%
-64.45% 100.0% 64.82% 100.0% 64.42% 100.0% training samples, which can be retrieved at inference time, as shown in [2]. However, there is no trivial method to retrieve memorized samples from a discrimi-native model even if it memorizes training samples. attacks that rely on conﬁdence values either suffer from high
FAR or low accuracy. Moreover, we show that the output of deep learning models are often similar for correctly labeled samples, whether they were used in training or not.
• We do not extend the conclusion to any attack that can be launched during the training phase, such as data poisoning, model/training manipulation [21], etc. We only study membership inference on naturally trained models and natural datasets. Deep models may behave very differently if any unnatural manipulation appears during the training phase.
• In each dataset, there often exists outliers. The MI attack maybe more successful on these samples [14], whether they are classiﬁed correctly or not. 2.