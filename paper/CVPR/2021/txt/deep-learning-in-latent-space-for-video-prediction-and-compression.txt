Abstract
Learning-based video compression has achieved substan-tial progress during recent years. The most inﬂuential ap-proaches adopt deep neural networks (DNNs) to remove spatial and temporal redundancies by ﬁnding the appropri-ate lower-dimensional representations of frames in the video.
We propose a novel DNN based framework that predicts and compresses video sequences in the latent vector space. The proposed method ﬁrst learns the efﬁcient lower-dimensional latent space representation of each video frame and then performs inter-frame prediction in that latent domain. The proposed latent domain compression of individual frames is obtained by a deep autoencoder trained with a generative ad-versarial network (GAN). To exploit the temporal correlation within the video frame sequence, we employ a convolutional long short-term memory (ConvLSTM) network to predict the latent vector representation of the future frame. We demon-strate our method with two applications; video compression and abnormal event detection that share the identical latent frame prediction network. The proposed method exhibits superior or competitive performance compared to the state-of-the-art algorithms speciﬁcally designed for either video compression or anomaly detection. 1 1.

Introduction
Video data transmission occupies the majority of the inter-net data trafﬁc nowadays. With the trend of extensive mobile devices usage worldwide, video data streaming is extensively used for productivity tools and entertainment platforms that assist people’s work and life in various aspects. On top of the ubiquitous video engagement, superior video quality stan-dards such as 4k UHD, and VR 360 became more widely available, which makes high performance video compres-sion even more critical. Traditional video coding standards such as MPEG, AVC/H.264 [49], HEVC/H.265 [43], and
VP9 [38] have achieved impressive performance on video compression tasks. However, as their primary applications 1Code available at: https://github.com/BowenL0218/Video-compression
Figure 1. Reconstructed frame with the conventional codecs (H.264,
H.265) and our approach. Information and details are well pre-served in the frame generated from a purely prediction-based latent representation (top right). Compared with H.264, our result yields less block artifacts and preserves ﬁner details. Our method achieves a higher compression ratio than H.265 with similar quality. are human perception driven, those hand-crafted codecs are likely suboptimal for machine-related tasks such as deep learning based video analytic.
During recent years, a growing trend of employing deep neural networks (DNNs) for image compression tasks has been witnessed. Prior works [46, 7, 36] have provided theo-retical basis for application of deep autoencoders (AEs) on image codecs that attempt to optimize the rate-distortion trade-off, and they have showed the feasibility of latent representation as a format of compressed signal. While image compression reduces the redundancy only in spatial domain, video compression exploits the temporal correla-tion among consecutive frames as well. Using learned video prediction to substitute traditional block-based motion pre-diction/estimation methods has become a critical part of deep learning based video compression. Related recent works [14, 25, 27] address the uncertainties of real-world videos with stochastic video prediction networks using autoencoders and/or generative adversarial network (GAN) structures in recurrent settings. Learned video compression is a relatively recent topic. Early works [11, 50] either directly interpo-late the key-frames or emulate the functional units in hand-crafted codecs with neural networks. Later proposed deep neural video codecs [31, 28, 19, 15, 41, 3, 51, 17, 30, 21] mainly target on learning data-driven algorithms that take advantage of the end-to-end trainability of DNNs. Most of them [28, 19, 15, 41, 3, 51, 17] adopt autoencoder style structures that encode frame and residual representations in 701
latent space. 2.