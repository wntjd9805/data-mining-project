Abstract 1.

Introduction
Modern 3D object detectors have immensely beneﬁted from the end-to-end learning idea. However, most of them use a post-processing algorithm called Non-Maximal Sup-pression (NMS) only during inference. While there were attempts to include NMS in the training pipeline for tasks such as 2D object detection, they have been less widely adopted due to a non-mathematical expression of the NMS.
In this paper, we present and integrate GrooMeD-NMS – a novel Grouped Mathematically Differentiable NMS for monocular 3D object detection, such that the network is trained end-to-end with a loss on the boxes after NMS. We
ﬁrst formulate NMS as a matrix operation and then group and mask the boxes in an unsupervised manner to obtain a simple closed-form expression of the NMS. GrooMeD-NMS addresses the mismatch between training and infer-ence pipelines and, therefore, forces the network to select the best 3D box in a differentiable manner. As a result,
GrooMeD-NMS achieves state-of-the-art monocular 3D ob-ject detection results on the KITTI benchmark dataset per-forming comparably to monocular video-based methods. 3D object detection is one of the fundamental problems in computer vision, where the task is to infer 3D informa-tion of the object. Its applications include augmented re-ality [2, 68], robotics [43, 74], medical surgery [70], and, more recently path planning and scene understanding in au-tonomous driving [17, 35, 46, 77]. Most of the 3D object detectors [17, 35, 44, 46, 77] are extensions of the 2D object detector Faster R-CNN [69], which relies on the end-to-end learning idea to achieve State-of-the-Art (SoTA) object de-tection. Some of these methods have proposed changing ar-chitectures [46, 76, 77] or losses [10, 18]. Others have tried incorporating conﬁdence [12, 76, 77] or temporal cues [12].
Almost all of them output a massive number of boxes for each object and, thus, rely on post-processing with a greedy [65] clustering algorithm called Non-Maximal Sup-pression (NMS) during inference to reduce the number of false positives and increase performance. However, these works have largely overlooked NMS’s inclusion in training leading to an apparent mismatch between training and in-ference pipelines as the losses are applied on all boxes be-fore NMS but not on ﬁnal boxes after NMS (see Fig. 1(a)). 8973
We also ﬁnd that 3D object detection suffers a greater mis-match between classiﬁcation and 3D localization compared to that of 2D localization, as discussed further in Sec. A3.2 of the supplementary and observed in [12, 35, 76]. Hence, our focus is 3D object detection.
Earlier attempts to include NMS in the training pipeline [31,32,65] have been made for 2D object detection where the improvements are less visible. Recent efforts to improve the correlation in 3D object detection involve cal-culating [77, 79] or predicting [12, 76] the scores via like-lihood estimation [40] or enforcing the correlation explic-itly [35]. Although this improves the 3D detection perfor-mance, improvements are limited as their training pipeline is not end to end in the absence of a differentiable NMS.
To address the mismatch between training and inference pipelines as well as the mismatch between classiﬁcation and 3D localization, we propose including the NMS in the train-ing pipeline, which gives a useful gradient to the network so that it ﬁgures out which boxes are the best-localized in 3D and, therefore, should be ranked higher (see Fig. 1(b)).
An ideal NMS for inclusion in the training pipeline should be not only differentiable but also parallelizable.
Unfortunately, the inference-based classical NMS and Soft-NMS [8] are greedy, set-based and, therefore, not paralleliz-able [65]. To make the NMS parallelizable, we ﬁrst for-mulate the classical NMS as matrix operation and then ob-tain a closed-form mathematical expression using elemen-tary matrix operations such as matrix multiplication, ma-trix inversion, and clipping. We then replace the threshold pruning in the classical NMS with its softer version [8] to get useful gradients. These two changes make the NMS
GPU-friendly, and the gradients are backpropagated. We next group and mask the boxes in an unsupervised man-ner, which removes the matrix inversion and simpliﬁes our proposed differentiable NMS expression further. We call this NMS as Grouped Mathematically Differentiable Non-Maximal Suppression (GrooMeD-NMS).
In summary, the main contributions of this work include:
• This is the ﬁrst work to propose and integrate a closed-form mathematically differentiable NMS for object de-tection, such that the network is trained end-to-end with a loss on the boxes after NMS.
• We propose an unsupervised grouping and masking on the boxes to remove the matrix inversion in the closed-form NMS expression.
• We achieve SoTA monocular 3D object detection per-formance on the KITTI dataset performing compara-bly to monocular video-based methods. 2.