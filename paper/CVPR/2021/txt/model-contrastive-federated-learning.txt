Abstract
Federated learning enables multiple parties to collab-oratively train a machine learning model without commu-nicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data dis-tribution across parties. Although many studies have been proposed to address this challenge, we ﬁnd that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: model-contrastive federated learning. MOON is a simple and effective federated learning framework. The key idea of
MOON is to utilize the similarity between model represen-tations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON signiﬁcantly out-performs the other state-of-the-art federated learning algo-rithms on various image classiﬁcation tasks. 1.

Introduction
Deep learning is data hungry. Model training can beneﬁt a lot from a large and representative dataset (e.g., ImageNet
[6] and COCO [31]). However, data are usually dispersed among different parties in practice (e.g., mobile devices and companies). Due to the increasing privacy concerns and data protection regulations [40], parties cannot send their private data to a centralized server to train a model.
To address the above challenge, federated learning [20, 44, 27, 26] enables multiple parties to jointly learn a ma-chine learning model without exchanging their local data.
A popular federated learning algorithm is FedAvg [34]. In each round of FedAvg, the updated local models of the par-ties are transferred to the server, which further aggregates the local models to update the global model. The raw data is not exchanged during the learning process. Federated learn-ing has emerged as an important machine learning area and attracted many research interests [34, 28, 22, 25, 41, 5, 16, 2, 11]. Moreover, it has been applied in many applications such as medical imaging [21, 23], object detection [32], and landmark classiﬁcation [15].
A key challenge in federated learning is the hetero-geneity of data distribution on different parties [20]. The data can be non-identically distributed among the parties in many real-world applications, which can degrade the per-formance of federated learning [22, 29, 24]. When each party updates its local model, its local objective may be far from the global objective. Thus, the averaged global model is away from the global optima. There have been some studies trying to address the non-IID issue in the lo-cal training phase [28, 22]. FedProx [28] directly limits the local updates by ℓ2-norm distance, while SCAFFOLD
[22] corrects the local updates via variance reduction [19].
However, as we show in the experiments (see Section 4), these approaches fail to achieve good performance on im-age datasets with deep learning models, which can be as bad as FedAvg.
In this work, we address the non-IID issue from a novel the global perspective based on an intuitive observation: model trained on a whole dataset is able to learn a bet-ter representation than the local model trained on a skewed subset. Speciﬁcally, we propose model-contrastive learn-ing (MOON), which corrects the local updates by max-imizing the agreement of representation learned by the current local model and the representation learned by the global model. Unlike the traditional contrastive learning approaches [3, 4, 12, 35], which achieve state-of-the-art re-sults on learning visual representations by comparing the representations of different images, MOON conducts con-trastive learning in model-level by comparing the represen-tations learned by different models. Overall, MOON is a simple and effective federated learning framework, and addresses the non-IID data issue with the novel design of model-based contrastive learning.
We conduct extensive experiments to evaluate the ef-fectiveness of MOON. MOON signiﬁcantly outperforms the other state-of-the-art federated learning algorithms [34, 28, 22] on various image classiﬁcation datasets including
CIFAR-10, CIFAR-100, and Tiny-Imagenet. With only lightweight modiﬁcations to FedAvg, MOON outperforms existing approaches by at least 2% accuracy in most cases.
Moreover, the improvement of MOON is very signiﬁcant 10713
SCAFFOLD only shows experiments on EMNIST with lo-gistic regression and 2-layer fully connected layer. The ef-fectiveness of FedProx and SCAFFOLD on image datasets with deep learning models has not been well explored. As shown in our experiments, those studies have little or even no advantage over FedAvg, which motivates this study for a new approach of handling non-IID image datasets with deep learning models. We also notice that there are other related contemporary work [1, 30, 43] when preparing this paper. We leave the comparison between MOON and these contemporary work as future studies.
As for studies on improving the aggregation phase,
FedMA [41] utilizes Bayesian non-parametric methods to match and average weights in a layer-wise manner. Fe-dAvgM [14] applies momentum when updating the global model on the server. Another recent study, FedNova [42], normalizes the local updates before averaging. Our study is orthogonal to them and potentially can be combined with these techniques as we work on the local training phase.
Another research direction is personalized federated learning [8, 7, 10, 47, 17], which tries to learn personal-ized local models for each party. In this paper, we study the typical federated learning, which tries to learn a single global model for all parties.
Figure 1. The FedAvg framework. In this paper, we focus on the second step, i.e., the local training phase. on some settings. For example, on CIFAR-100 dataset with 100 parties, MOON achieves 61.8% top-1 accuracy, while the best top-1 accuracy of existing studies is 55%. 2.