Abstract loss loss out_v1 out_v2 out_v1 out_v2
We present a plug-in replacement for batch normaliza-tion (BN) called exponential moving average normaliza-tion (EMAN), which improves the performance of exist-ing student-teacher based self- and semi-supervised learn-ing techniques. Unlike the standard BN, where the statis-tics are computed within each batch, EMAN, used in the teacher, updates its statistics by exponential moving aver-age from the BN statistics of the student. This design re-duces the intrinsic cross-sample dependency of BN and en-hances the generalization of the teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2 points and semi-supervised learning by about 7/2 points, when 1%/10% supervised labels are available on ImageNet.
These improvements are consistent across methods, network architectures, training duration, and datasets, demonstrat-ing the general effectiveness of this technique. The code will be made available online. 1.

Introduction
Supervised learning has achieved remarkable success on a variety of visual tasks, beneﬁting from the availability of large-scale annotated datasets such as ImageNet [37], MS-COCO [31], and ShapeNet [6]. However, in some domains such as medical imaging, large amounts of annotations are expensive or time-consuming to collect. Learning effective representations with small amounts (semi-supervised) or no (unsupervised or self-supervised) manual annotation is thus an important problem in computer vision [3, 8, 9, 17, 19, 28, 29, 39, 41, 45].
Although many choices exist for semi- and self-supervised learning [3, 15, 29, 34, 50], an effective approach is the family of student-teacher models [9, 17, 19, 22, 28, 41, 47], where the outputs of the teacher are used to guide the learning of the student on the unlabeled data. Within this family, a common approach is to update the teacher using exponential moving average (EMA) of the student
BN
BN
BN
EMAN student
EMA teacher student teacher
BN
BN
BN
EMAN
EMA im_v1 im_v2 im_v1 im_v2
Figure 1. The EMA-teacher framework with standard BN (left) and the proposed EMAN (right). θ are the model parameters, and
µ and σ2 BN statistics. EMA denotes exponential moving average updates. im v1 and im v2 are two different views of the same image. No gradient is backpropagated through the teacher model. parameters over its training trajectory [41], which we call
EMA-teacher, as shown in Figure 1 (left). As discussed in [1,24,26], the temporally averaged teacher, as interpreted as the temporal ensembling of the student checkpoints, can improve generalization. Due to this property, it has been adopted in recent self-supervised learning methods [17,19].
While the objective and the update mechanisms are dif-ferent for the student and the teacher, both networks use the standard batch normalization (BN) [25], as in the early
EMA-teacher frameworks [41]. However, this can lead to two potential problems: 1. Cross-sample dependency. This is an intrinsic prop-erty of BN where the output of a sample is dependent on all other samples in the same batch. This cross-sample information leakage may allow the model to
“cheat” in semi- or self-supervised learning. To avoid this, some special designs on normalization were ap-plied in [8, 17, 19, 21]. For example, [21] switched to layer normalization [2]; MoCo [19] designed Shuf-ﬂeBN where a mini-batch uses BN statistics from other randomly sampled mini-batch; and SimCLR [8] and
BYOL [17] used Synchronized BN (SyncBN). 2. Model parameter mismatch. In the teacher network, its parameters are averaged from the student parameters 194
of previous iterations, but the batch-wise BN statistics are instantly collected at current iteration. This could lead to potential mismatch between the model param-eters and the BN statistics in the parameter space.
We present a simple replacement for standard BN used in the EMA-teacher framework, called exponential mov-ing average normalization (EMAN). As shown in Figure 1 (right), the EMAN statistics (mean µ′ and variance σ′2) in the teacher are exponentially moving averaged from the student BN statistics, similar to the other parameters. The
EMAN is simply a linear transform, without batch-wise statistics computation, and thus has removed cross-sample dependency presented in BN in the teacher. Since the nor-malization statistics and model parameters are both updated using EMA, we expect this to improve stability of train-ing by reducing the potential model parameter mismatches when using BN. This simple design requires only a few lines of code, and can replace other complex normaliza-tion schemes (e.g. ShufﬂeBN, SyncBN, etc.) within various semi- and self-supervised learning techniques.
We have evaluated EMAN within various EMA-teacher frameworks, including recent state-of-the-art semi-supervised learning (FixMatch [39]) and self-supervised learning (MoCo [19] and BYOL [17]) techniques. On self-supervised learning, EMAN improves the performance of
MoCo/BYOL by 4-6/1-2 points when 1%/10% labels are available on ImageNet [37]. On semi-supervised learning,
EMAN improves the performance of FixMatch by about 7/2 points for 1%/10% labels, leading to the new state-of-the-art performances of 63.0/74.0 top-1 accuracy for 1%/10% labels on ImageNet. These improvements are consistent across methods, network architectures, training duration, and datasets, demonstrating the effectiveness of EMAN as a general technique. In addition, EMAN is just as efﬁcient as standard BN, and does not require cross-GPU commu-nication or synchronization of ShufﬂeBN or SyncBN. We thus believe that EMAN can be of interest for other future student-teacher variants. 2.