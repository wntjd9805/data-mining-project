Abstract
The current evaluation protocol of long-tailed visual recognition trains the classiﬁcation model on the long-tailed source label distribution and evaluates its perfor-mance on the uniform target label distribution. Such pro-tocol has questionable practicality since the target may also be long-tailed. Therefore, we formulate long-tailed visual recognition as a label shift problem where the tar-get and source label distributions are different. One of the signiﬁcant hurdles in dealing with the label shift prob-lem is the entanglement between the source label distri-bution and the model prediction.
In this paper, we fo-cus on disentangling the source label distribution from the model prediction. We ﬁrst introduce a simple but over-looked baseline method that matches the target label dis-tribution by post-processing the model prediction trained by the cross-entropy loss and the Softmax function. Al-though this method surpasses state-of-the-art methods on benchmark datasets, it can be further improved by di-rectly disentangling the source label distribution from the model prediction in the training phase. Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan rep-resentation. LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LT, Places-LT,
ImageNet-LT, and iNaturalist 2018. Moreover, LADE out-performs existing methods on various shifted target label distributions, showing the general adaptability of our pro-posed method. 1.

Introduction
Based on large-scale datasets such as ImageNet [50],
COCO [34], and Places [64], deep neural networks have achieved signiﬁcant progress in various visual recogni-tion tasks, including classiﬁcation [22, 52], object detec-tion [47, 18], and segmentation [48]. In contrast to these relatively balanced datasets, real-world data often exhibit
∗Equal contribution. Author ordering determined by coin ﬂip.
†Corresponding author. v
𝒑𝒔(𝒚|𝒙) v
𝒑𝒔(𝒚|𝒙)
𝒑𝒔(𝒚)
𝒑𝒕(𝒚)
𝒑(𝒙|𝒚)
𝒑(𝒙)
𝒑𝒔(𝒚|𝒙)
𝒑(𝒙|𝒚)
𝒑(𝒙)
𝒑𝒕(𝒚|𝒙)
𝒑𝒔(𝒚)
𝒑𝒕(𝒚)
Figure 1: A comparison between the cross-entropy loss and our proposed LADE loss in long-tailed visual recognition.
After training with the cross-entropy loss, the model predic-tion gets entangled with the source label distribution ps(y), which causes a discrepancy with the target label distribu-tion pt(y) during the inference phase. Our proposed LADE disentangles ps(y) from the model prediction so that it can adapt to the arbitrary target probability by injecting pt(y). long-tailed distribution where head (major) classes occupy most of the data, while tail (minor) classes have a hand-ful of samples [58, 38]. Unfortunately, the performance of state-of-the-art classiﬁcation models degrades on datasets following the long-tailed distribution [7, 21, 60].
To tackle this problem, many long-tailed visual recogni-tion methods [7, 21, 25, 8, 51, 62, 9] have been proposed.
These methods compare their effectiveness by (1) training on the long-tailed source label distribution ps(y) and (2) evaluating on the uniform target label distribution pt(y).
However, we argue that this evaluation protocol is often im-practical as it is natural to assume that pt(y) could be the arbitrary distribution such as uniform distribution [50] and long-tailed distribution [17, 10].
From this perspective, we are motivated to explore a new method that adapts the model to the arbitrary pt(y). In this paper, we borrow the concept of the label distribution shift problems [16, 36, 56] to the long-tailed visual recognition 6626
1.0 y t i l i 0.8 b a b o r
P 0.6
Avg. Prob. ps(y) pt(y) 1.0 y t i l i 0.8 b a b o r
P 0.6
Avg. Prob. ps(y) pt(y)
) e g a r e v
A ( 0.4 0.2
) e g a r e v
A ( 0.4 0.2 0.0 0 20 80
Class index (Head to tail) 40 60 100 0.0 0 20 80
Class index (Head to tail) 40 60 100 (a) Cross-entropy (b) LADE
Figure 2: The average probability for each class calculated on the balanced test set. The model is ResNet-32 [22] trained on CIFAR-100-LT [9], which has uniform target label distribution pt(y) while the source dataset has long-tailed distribution ps(y). (a) depicts the average probabil-ity when trained with the cross-entropy loss, which shows average probability correlates with ps(y), resulting in the discrepancy with pt(y). (b) depicts the average probabil-ity when trained and inferred with LADE, which shows the ability of LADE on adapting to pt(y). task.
However, it is problematic to directly use the model pre-diction p(y|x; θ) which is ﬁtted to the source probability ps(y|x), as the target probability pt(y|x) is shifted from ps(y) to pt(y) (Figure 1). Figure 2a shows the entanglement between the model prediction and ps(y) when the model is trained by the cross-entropy (CE) loss and the Softmax function. To alleviate this problem, we focus on disentan-gling ps(y) from the model outputs so that the shifted target label distribution pt(y) can be injected to estimate the target probability.
We shed light on a simple yet strong baseline, called Post-Compensated Softmax (PC Softmax) that post-processes the model prediction to disentangle ps(y) from p(y|x; θ) and then incorporate pt(y) to the disentangled model output probability. Despite the simplicity of the method, PC Softmax outperforms state-of-the-art methods in long-tailed visual recognition (will be described in Sec-tion 4). Although this observation demonstrates the effec-tiveness of the disentanglement in the inference phase, PC
Softmax can be further improved by directly disentangling ps(y) in the training phase.
Thus, we propose a novel method, LAbel distribution
DisEntangling (LADE) loss. LADE utilizes the Donsker-Varadhan (DV) representation [15] to directly disentangle ps(y) from p(y|x; θ). Figure 2b shows that LADE disentan-gles ps(y) from p(y|x; θ). We claim that the disentangle-ment in the training phase shows even better performance on adapting to arbitrary target label distributions.
We conduct several experiments to compare our pro-posed method with existing long-tailed visual recognition methods, and show that LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LT [30], Places-LT [38], ImageNet-LT [38], and iNaturalist 2018 [57]. Moreover, we demonstrate that the classiﬁcation model trained with LADE can cope with arbitrary pt(y) by evaluating the performance on datasets with various shifted pt(y). We further show that our proposed LADE can also be effective in terms of conﬁdence calibration. Our contri-butions in this paper are summarized as follows:
• We introduce a simple yet strong baseline method, PC
Softmax, which outperforms state-of-the-art methods in long-tailed visual recognition benchmark datasets.
• We propose a novel loss called LADE that directly dis-entangles the source label distribution in the training phase so that the model effectively adapts to arbitrary target label distributions.
• We show that LADE achieves state-of-the-art perfor-mance in long-tailed visual recognition on various tar-get label distributions. 2.