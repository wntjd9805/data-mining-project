Abstract
Video-based human motion transfer creates video ani-mations of humans following a source motion. Current methods show remarkable results for tightly-clad subjects.
However, the lack of temporally consistent handling of plau-sible clothing dynamics, including ﬁne and high-frequency details, signiﬁcantly limits the attainable visual quality. We address these limitations for the ﬁrst time in the literature and present a new framework which performs high-ﬁdelity and temporally-consistent human motion transfer with nat-ural pose-dependent non-rigid deformations, for several types of loose garments. In contrast to the previous tech-niques, we perform image generation in three subsequent stages: synthesizing human shape, structure, and appear-ance. Given a monocular RGB video of an actor, we train a stack of recurrent deep neural networks that generate these intermediate representations from 2D poses and their temporal derivatives. Splitting the difﬁcult motion trans-fer problem into subtasks that are aware of the temporal motion context helps us to synthesize results with plausible dynamics and pose-dependent detail. It also allows artistic control of results by manipulation of individual framework stages.
In the experimental results, we signiﬁcantly out-perform the state-of-the-art in terms of video realism. The source code is available at https://graphics.tu-bs. de/publications/kappel2020high-fidelity. 1.

Introduction
Human motion transfer methods, also known as perfor-mance cloning or reenactment methods, can generate real-istic video animations of an actor following a target mo-tion speciﬁed by a user. This has several applications in
AR/VR and video editing. Building upon new advances in machine learning, current motion transfer methods tackle this challenging problem by learning a direct mapping be-tween an actor-independent motion space and the resulting target actor’s appearance space. These methods often re-quire a training video of an actor performing a rich set of motions [2, 5, 17, 19, 27, 33].
Some recent motion transfer approaches parameterize motion as skeletal pose sequences that can be computed from videos with off-the-shelf pose detectors [5, 27]. Others use pre-captured template meshes or parameterized body models to provide additional guidance to the synthesis step [17, 19]. Acquisition of such templates [17], however, requires an extensive structure-from-motion reconstruction of the static target actor under constant lighting. Further-more, existing human motion transfer approaches are likely to produce notable temporal and spatial artifacts when ac-tors wear loose clothing, such as dresses, skirts and hood-ies [2, 5, 19, 33]. On such garments, they struggle to real-istically reproduce the appearance of ﬁne-scale details like folds and wrinkles, as well as plausible dynamics.
In this paper, we present a new human motion transfer framework that generates visually plausible video anima-tions of humans that are spatially and temporally coherent, and show natural dynamics, even for actors wearing loose garments (see Fig. 1). Given a single monocular video of an actor performing a rich set of motions, we train a stack of deep generative networks to learn a mapping from 2D pose to a silhouette with semantic part labels, and per-pixel 1541
appearance of the actor. We model the person’s shape as a dense foreground silhouette mask with per-pixel labels en-coding assignment to limbs and garments. We further en-code the structure of wrinkles and texture patterns of gar-ments as the orientation and strength of local image gradi-ents. We extract this structure from images using a bank of oriented ﬁlter kernels [23, 31]. Encoding the actor’s ap-pearance with these explicitly decoupled intermediate rep-resentations of silhouette and structure is key to enhance the temporal and spatial quality of synthesized videos compris-ing human actors in loose clothing.
Our method improves over current motion transfer ap-proaches in terms of visual ﬁdelity using a single RGB cam-era. Furthermore, our representation provides an additional level of control over the ﬁnal image generation. For exam-ple, for the same overall dynamic geometric outline (i.e., the same garment geometry), color and appearance, includ-ing fold and wrinkle style, can be manipulated in a purely image-based way. Overall, our contributions can be sum-marized as follows: (1) A new motion transfer framework with an emphasis on visually-plausible ﬁne-scale deforma-tions and dynamics in the actor’s clothing. (2) For this, we propose to decompose the pose-to-image translation task into better conditioned cascaded processes, where the ﬁ-nal appearance is conditioned on the predicted shape out-line and internal structure of the clothing. (3) We show that our intermediate representations do not only help to provide more temporally coherent conditioning resulting in more appealing image synthesis, but also allow controlling indi-vidual aspects of the ﬁnal rendering (e.g., enhance wrinkles and transfer the clothing style). 2.