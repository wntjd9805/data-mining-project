Abstract
We propose a new framework for understanding and rep-resenting related salient events in a video using visual se-mantic role labeling. We represent videos as a set of re-lated events, wherein each event consists of a verb and mul-tiple entities that fulﬁll various roles relevant to that event.
To study the challenging task of semantic role labeling in videos or VidSRL, we introduce the VidSitu benchmark, a large scale video understanding data source with 29K 10-second movie clips richly annotated with a verb and
†Part of the work was done during Arka’s internship at PRIOR@AI2 semantic-roles every 2 seconds. Entities are co-referenced across events within a movie clip and events are connected to each other via event-event relations. Clips in VidSitu are drawn from a large collection of movies (∼3K) and have been chosen to be both complex (∼4.2 unique verbs within a video) as well as diverse (∼200 verbs have more than 100 annotations each). We provide a comprehensive analysis of the dataset in comparison to other publicly available video understanding benchmarks, several illustrative baselines and evaluate a range of standard video recognition models.
Our code and dataset is available at vidsitu.org. 5589
1.

Introduction
Videos record events in our lives with both short and long temporal horizons. These recordings frequently relate mul-tiple events separated geographically and temporally and capture a wide variety of situations involving human be-ings interacting with other humans, objects and their en-vironment. Extracting such rich and complex information from videos can drive numerous downstream applications such as describing videos [35, 82, 77], answering queries about them [85, 81], retrieving visual content [50], building knowledge graphs [48] and even teaching embodied agents to act and interact with the real world [84].
Parsing video content is an active area of research with much of the focus centered around tasks such as action clas-siﬁcation [31], localization [24] and spatio-temporal detec-tion [21]. Although parsing human actions is a critical com-ponent of understanding videos, actions by themselves paint an incomplete picture, missing critical pieces such as the agent performing the action, the object being acted upon, the tool or instrument used to perform the action, location where the action is performed and more. Expository tasks such as video captioning and story-telling provide a more holistic understanding of the visual content; but akin to their counterparts in the image domain, they lack a clear deﬁni-tion of the type of information being extracted making them notoriously hard to evaluate [32, 74].
Recent work in the image domain [83, 58, 22] has at-tempted to move beyond action classiﬁcation via the task of visual semantic role labeling - producing not just the pri-mary activity in an image or region, but also the entities participating in that activity via different roles. Building upon this line of research, we propose VidSRL – the task of recognizing spatio-temporal situations in video content.
As illustrated in Figure. 1, VidSRL involves recognizing and temporally localizing salient events across the video, identifying participating actors, objects, and locations in-volved within these events, co-referencing these entities across events over the duration of the video, and relating how events affect each other over time. We posit that Vid-SRL, a considerably more detailed and involved task than action classiﬁcation with more precise deﬁnitions of the ex-tracted information than video captioning, is a step towards obtaining a holistic understanding of complex videos.
To study VidSRL, we present VidSitu, a large video un-derstanding dataset of over 29K videos drawn from a di-verse set of 3K movies. Videos in VidSitu are exactly 10 seconds long and are annotated with 5 verbs, corresponding to the most salient event taking place within the ﬁve 2 sec-ond intervals in the video. Each verb annotation is accom-panied with a set of roles whose values 1 are annotated using free form text. In contrast to verb annotations which are de-rived from a ﬁxed vocabulary, the free form role annotations allow the use of referring expressions (e.g. boy wearing a blue jacket) to disambiguate entities in the video. An entity that occurs in any of the ﬁve clips within a video is consis-tently referred to using the same expression, allowing us to develop and evaluate models with co-referencing capabil-ity. Finally, the dataset also contains event relation annota-tions capturing causation (Event Y is Caused By/Reaction
To Event X) and contingency (Event X is a pre-condition for Event Y). The key highlights of VidSitu include: (i)
Diverse Situations: VidSitu enjoys a large vocabulary of verbs (1500 unique verbs curated from PropBank [54] with 200 verbs having at least 100 event annotations) and entities (5600 unique nouns with 350 nouns occurring in at least 100 videos); (ii) Complex Situations: Each video is annotated with 5 inter-related events and has an average of 4.2 unique verbs, 6.5 unique entities and; (iii) Rich Annotations: Vid-Situ provides structured event representations (3.8 roles per event) with entity co-referencing and event-relation labels.
To facilitate further research on VidSRL, we provide a comprehensive benchmark that supports partwise evalua-tion of various capabilities required for solving VidSRL and create baselines for each capability using state-of-art archi-tectural components to serve as a point of reference for fu-ture work. We also carefully choose metrics that provide a meaningful signal of progress towards achieving compe-tency on each capability. Finally, we perform a human-agreement analysis that reveals a signiﬁcant room for im-provement on the VidSitu benchmark.
Our main contributions are: (i) the VidSRL task formal-ism for understanding complex situations in videos; (ii) cu-rating the richly annotated VidSitu dataset that consists of diverse and complex situations for studying VidSRL; (iii) establishing an evaluation methodology for assessing cru-cial capabilities needed for VidSRL and establishing base-lines for each using state-of-art components. The dataset and code are publicly available at vidsitu.org. 2.