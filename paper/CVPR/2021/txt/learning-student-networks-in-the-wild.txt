Abstract
Data-free learning for student networks is a new paradigm for solving users’ anxiety caused by the privacy problem of using original training data. Since the archi-tectures of modern convolutional neural networks (CNNs) are compact and sophisticated, the alternative images or meta-data generated from the teacher network are often broken. Thus, the student network cannot achieve the compa-rable performance to that of the pre-trained teacher network especially on the large-scale image dataset. Different to previous works, we present to maximally utilize the massive available unlabeled data in the wild. Speciﬁcally, we ﬁrst thoroughly analyze the output differences between teacher and student network on the original data and develop a data collection method. Then, a noisy knowledge distilla-tion algorithm is proposed for achieving the performance of the student network. In practice, an adaptation matrix is learned with the student network for correcting the la-bel noise produced by the teacher network on the collected unlabeled images. The effectiveness of our DFND (Data-Free Noisy Distillation) method is then veriﬁed on several benchmarks to demonstrate its superiority over state-of-the-art data-free distillation methods. Experiments on various datasets demonstrate that the student networks learned by the proposed method can achieve comparable performance with those using the original dataset. Code is available at https://github.com/huawei- noah/Data-Efficient-Model-Compression 1.

Introduction
Deep convolutional neural networks have been widely used in various computer vision tasks such as image recog-nition [14, 18], object detection [29, 11, 42, 41] and image segmentation [23]. However, these networks usually consist of enormous number of parameters and requires heavy com-putation cost, which prevent their usage in edge devices such
∗Corresponding author canine canine craft craft sailboat sailboat vehicle vehicle mammal mammal dog dog
Original Data  (Unavailable)
Selected Data with 
Noisy Labels dog vehicle sailboat craft dog
Predictions
Knowledge Distillation: (cid:1830)(cid:3012)(cid:3013)(cid:4666)(cid:2280)(cid:3021)(cid:4666)(cid:1850) (cid:932)(cid:4667) (cid:1846) (cid:1510) (cid:2280)(cid:3020)(cid:4666)(cid:1850) (cid:932)(cid:4667) (cid:1846) (cid:4667) dog vehicle fish panda carnivore 0.78 0.04 0.12 0.06 0.02 0.91 0.05 0.02 0.03 0.13 0.83 0.01 0.22 0.03 0.21 0.54
Unlabeled Data  (Available)
Outputs of Student 
Network
Noisy Adaptation 
Matrix
Figure 1. The diagram of the proposed method for learning student networks in the wild. Useful data will be ﬁrst selected from the external unlabeled data and then utilized for training the desired stu-dent network. The noise adaption matrix is exploited for correcting labels of unlabeled data estimated by the teacher network. as mobile phones and autonomous cars. For example, VGG-net [31] requires 548MB memory for saving parameters and 20G ﬂoating point operations for processing a single image.
To this end, a great number of techniques including quan-tization [13], pruning [20] and distillation [15] have been proposed to accelerate and compress convolutional neural networks.
Admittedly, we can obtain considerable compression ra-tios on benchmark datasets and models using these method when we can access the original training data of the pre-trained network. However, the training data is often unavail-able in some practice constrains such as privacy or trans-mission. For example, we want to compress a deep model trained on millions of images, while the dataset is difﬁcult to transfer and restore. Furthermore, people are willing to share their trained models to public, while they are very anxious about the training data especially some private data, e.g., face, voice and ﬁngerprint. Thus, a recent trend for 6428
model compression algorithms is to develop data-free tech-niques that can reduce the computational complexities of pre-trained networks without original training data.
To this end, Lopes et al. [24] ﬁrst formulated the data-free learning problem and use the “meta-data” to reconstruct the original images. However, its performance is limited since the useful knowledge information in the teacher network has not been fully investigated. Chen et al. [2] developed a
GAN (Generate Adversarial Network [10]) based method, which used a generator network to approximate the training samples from the given teacher network. Besides using generators to obtain training data, other methods [27, 1] synthesized training data by directly optimizing the input random images on the pre-trained network.
However, it is hard to generate images which have enough information for training the compressed network, since the size of training data is usually much larger than the given network. For example, ImageNet dataset consists of over 10 million numbers of images with 224 × 224 size and requires over 138GB storage, while a ResNet-50 model contains only
∼100MB parameters. Therefore, the quality of these gener-ated images cannot be ensured. Moreover, the running time and overhead for generating enough images for large scale dataset (e.g., ImageNet) are expensive. Thus, an efﬁcient and effective method for learning portable student network without training data is urgently required.
In this paper, we present to utilize the large amount of unlabeled data in the wild to address the data-free knowledge distillation problem. Instead of generating images from the teacher network with a series of priori, images most relevant to the given pre-trained network and tasks will be identiﬁed from a large unlabeled dataset (e.g., Flickr [17]) to conduct the knowledge distillation task. We ﬁrst analyze the bound of distance between the outputs of the teacher and the stu-dent networks, and then explore a data selection method for searching useful unlabeled data. Then, these data with the noisy labels derived from the teacher network is collected.
To further improve the performance of the student network, a noise adaptation matrix is exploited for reﬁning the la-bels provided by the teacher network. The portable student network is supervised by the conventional knowledge distil-lation approach on the collected data and the proposed noisy distillation using the adaptation matrix, as shown in Figure 1.
Experiments conducted on several benchmarks demonstrate that the proposed DFND (Data-Free Noisy Distillation) al-gorithm can surpass all data-free distillation methods and achieve the state-of-the-art performance, the accuracy of the resulting student is comparable to that of the student network trained using original data. 2.