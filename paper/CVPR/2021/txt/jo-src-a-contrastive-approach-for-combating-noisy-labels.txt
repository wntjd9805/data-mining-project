Abstract
Due to the memorization effect in Deep Neural Networks (DNNs), training with noisy labels usually results in in-ferior model performance. Existing state-of-the-art meth-ods primarily adopt a sample selection strategy, which se-lects small-loss samples for subsequent training. However, prior literature tends to perform sample selection within each mini-batch, neglecting the imbalance of noise ratios in different mini-batches. Moreover, valuable knowledge within high-loss samples is wasted. To this end, we pro-pose a noise-robust approach named Jo-SRC (Joint Sample
Selection and Model Regularization based on Consistency).
Speciﬁcally, we train the network in a contrastive learning manner. Predictions from two different views of each sample are used to estimate its “likelihood” of being clean or out-of-distribution. Furthermore, we propose a joint loss to ad-vance the model generalization performance by introducing consistency regularization. Extensive experiments have val-idated the superiority of our approach over existing state-of-the-art methods. The source code and models have been made available at https : / / github . com / NUST -Machine- Intelligence- Laboratory/Jo- SRC. 1.

Introduction
DNNs have recently lead to tremendous progress in var-ious computer vision tasks [14, 28, 25, 40, 21]. These suc-cesses largely attribute to large-scale datasets with reliable annotations (e.g., ImageNet [4]). However, collecting well-annotated datasets is extremely labor-intensive and time-consuming, especially in domains where expert knowledge is required (e.g., ﬁne-grained categorization [37, 36]). The high cost of acquiring large-scale well-labeled data poses a bottleneck in employing DNNs in real-world scenarios.
∗Equal contribution.
†Corresponding author. fixed rate r
Clean samples
… …
Clean samples
Unclean samples
… …
Unclean samples
Clean samples
ID samples
OOD samples
… …
… …
Clean samples
ID samples
OOD samples i h c t a b
-i n i m
… j h c t a b
-i n i m i h c t a b
-i n i m
… j h c t a b
-i n i m
Figure 1. Existing small-loss based sample selection methods (upper) tend to regard a human-deﬁned proportion of samples within each mini-batch as clean ones. They ignore the ﬂuctu-ation of noise ratios in different mini-batches. On the contrary, our proposed method (bottom) selects clean samples in a global manner. Moreover, in-distribution (ID) noisy samples and out-of-distribution (OOD) ones are also selected and leveraged for en-hancing the model generalization performance.
As an alternative, employing web images to train DNNs has received increasing attention recently [20, 41, 42, 34, 45, 44, 51, 52, 32]. Unfortunately, whereas web images are cheaper and easier to obtain via image search engines
[5, 29, 46, 43], they usually yield inevitable noisy labels due to the error-prone automatic tagging system or non-expert annotations [23, 32, 45, 47]. A recent study has suggested that samples with noisy labels would be unavoidably over-ﬁtted by DNNs and consequently cause performance degra-dation [15, 50].
To alleviate this issue, many methods have been pro-posed for learning with noisy labels. Early approaches primarily attempt to correct losses during training. Some methods correct losses by introducing a noise transition ma-trix [31, 24, 6, 11]. However, estimating the noise transition 5192
matrix is challenging, requiring either prior knowledge or a subset of well-labeled data. Some methods design noise-robust loss functions which correct losses according to pre-dictions of DNNs [26, 54, 34]. However, these methods are prone to fail when the noise ratio is high.
Another active research direction in mitigating the nega-tive effect of noisy labels is training DNNs with selected or reweighted training samples [12, 27, 22, 8, 49, 38, 32]. The challenge is to design a proper criterion for identifying clean samples. It has been recently observed that DNNs have a memorization effect and tend to learn clean and simple pat-terns before overﬁtting noisy labels [15, 50]. Thus, state-of-the-art methods (e.g., Co-teaching [49], Co-teaching+ [49], and JoCoR [38]) propose to select a human-deﬁned propor-tion of small-loss samples as clean ones. Although promis-ing performance gains have been witnessed by employ-ing the small-loss sample selection strategy, these meth-ods tend to assume that noise ratios are identical among all mini-batches. Hence, they perform sample selection within each mini-batch based on an estimated noise rate. How-ever, this assumption may not hold true in real-world cases, and the noise rate is also challenging to estimate accurately (e.g., Clothing1M [39]). Furthermore, existing literature mainly focuses on closed-set scenarios, in which only in-distribution (ID) noisy samples are considered. In open-set cases (i.e., real-world cases), both in-distribution (ID) and out-of-distribution (OOD) noisy samples exist. High-loss samples do not necessarily have noisy labels. In fact, hard samples, ID noisy ones, and OOD noisy ones all produce large loss values, but the former two are potentially beneﬁ-cial for making DNNs more robust [32].
Motivated by the self-supervised contrastive learning
[3, 7], we propose a simple yet effective approach named
Jo-SRC (Joint Sample Selection and Model Regularization based on Consistency) to address aforementioned issues.
Speciﬁcally, we ﬁrst feed two different views of an im-age into a backbone network and predict two corresponding softmax probabilities accordingly. Then we divide samples based on two likelihood metrics. We measure the likelihood of a sample being clean using the Jensen-Shannon diver-gence between its predicted probability distribution and its label distribution. We measure the likelihood of a sample being OOD based on the prediction disagreement between its two views. Subsequently, clean samples are trained con-ventionally to ﬁt their given labels. ID and OOD noisy sam-ples are re-labeled by a mean-teacher model before they are back-propagated for updating network parameters. Finally, we propose a joint loss, including a classiﬁcation term and a consistency regularization term, to further advance model performance. A comparison between Jo-SRC and existing sample selection methods is provided in Figure 1. The ma-jor contributions of this work are: (1) We propose a simple yet effective contrastive ap-proach named Jo-SRC to alleviate the negative effect of noisy labels. Jo-SRC trains the network with a joint loss, including a cross-entropy term and a consistency term, to obtain higher classiﬁcation and generalization performance. (2) Our proposed Jo-SRC selects clean samples globally by adopting the Jensen-Shannon divergence to measure the likelihood of each sample being clean. We also propose to distinguish ID noisy samples and OOD noisy ones based on the prediction consistency between samples’ different views. ID and OOD noisy samples are relabeled by a mean-teacher network before being used for network update. (3) By providing comprehensive experimental results, we show that Jo-SRC signiﬁcantly outperforms state-of-the-art methods on both synthetic and real-world noisy datasets. Furthermore, extensive ablation studies are con-ducted to validate the effectiveness of our approach. 2.