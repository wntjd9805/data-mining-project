Abstract
Recent deep learning models have shown remarkable performance in image classiﬁcation. While these deep learning systems are getting closer to practical deploy-ment, the common assumption made about data is that it does not carry any sensitive information. This assump-tion may not hold for many practical cases, especially in the domain where an individual’s personal information is involved, like healthcare and facial recognition systems.
We posit that selectively removing features in this latent space can protect the sensitive information and provide better privacy-utility trade-off. Consequently, we propose
DISCO which learns a dynamic and data driven pruning
ﬁlter to selectively obfuscate sensitive information in the feature space. We propose diverse attack schemes for sensi-tive inputs & attributes and demonstrate the effectiveness of
DISCO against state-of-the-art methods through quantita-tive and qualitative evaluation. Finally, we also release an evaluation benchmark dataset of 1 million sensitive repre-sentations to encourage rigorous exploration of novel at-tack and defense schemes at https://github.com/ splitlearning/InferenceBenchmark. 1.

Introduction
Large deep neural network have resulted in break-throughs across computer vision [1], speech recognition [2] and reinforcement learning [3] with their success largely attributed to their ability to efﬁciently learn complex pat-terns from data. The deployment of these algorithms in critical application domains such as healthcare and face-recognition has motivated a research focus on learning cen-sored, unbiased and fair data representations to mitigate misuse by adversarial agents. Alternately, there can also be sensitive information in data which the user would like to keep private but the learned representations may inadver-tently encode. This sensitive information may manifest as sensitive inputs or attributes. Consider a setup where citi-zens consent to usage of face recognition in public spaces for identifying criminals. During inference, feature repre-sentations are extracted for faces and identiﬁcation is per-formed by matching in the feature space over an indexed database. While this may be a well-intended initiative, a malicious adversary may seek to intercept the feature repre-sentations to i) reconstruct the input face image or ii) extract personal attributes such as race, age, gender etc. The citi-zens did not consent to sharing this sensitive information which could be used to compromise their privacy and in a way that is biased or unfair to them. Exploring methods of improving privacy of the sensitive information (image, race, age, gender etc.) while preserving utility (identifying criminals) is the focus of this work.
Conventionally, research in privacy-aware machine learning has primarily focused on protecting training data from membership inference [4] and model inversion at-tacks [5], when i) training data is distributed over clients and ii) computation of training the model is out-sourced. For the former, distributed learning techniques such as feder-ated learning [6, 7] and split learning [8, 9] are used, where clients communicate with a centralized server using weights and activations and the latter relies on homomorphic en-cryption [10, 11] and secure enclaves [12, 13]. Additionally, techniques such as multi-party computation [14, 15] and differential privacy [16, 17, 18, 19] have been employed to improve the privacy in federated-learning. While effective for training, scaling these methods for deployment at infer-ence is a challenge for a variety of reasons. First, in several cases computational limitations and intellectual property considerations limit keeping the entire model on a client device. Secondly, cryptographic methods for training deep networks [20, 21, 22] are computationally very expensive operations which makes deploying models on the server in-feasible when working with sensitive data. We posit that collaborative inference, where the inference network is 12125
distributed between client devices (client network) and a server (server network) which communicate via the split ac-tivations, presents a viable alternative. While amenable to scalability, it is important to encode explicit measures of se-curity in the intermediate activations to protect privacy of the sensitive inputs and attributes.
While not motivating private collaborative inference, a few recent works [23, 24] have attempted the related prob-lem of attribute leakage [24, 25, 26, 27, 28] by focusing on adversarial representation learning (ARL). This couples to-gether two entities, i) an adversarial network that seeks to extract a sensitive attribute from a given activation and, ii) a predictor network that intends to extract compact activa-tions for accurate prediction of a task attribute (utility) while preventing the adversary from leaking the sensitive attribute (privacy). To balance this privacy-utility, [24] designed an objective to maximize entropy of the adversary network and
[29, 23] to minimize likelihood of the predictor on the sen-sitive attributes.
Motivated by the above observations, in this work, we
ﬁrst examine existing ARL methods which reveals the pres-ence of high redundancy in learned representations. We posit that selectively removing features in this latent space can protect the sensitive information and provide better privacy-utility trade-off. Consequently, we propose DISCO which learns a dynamic and data driven pruning ﬁlter to selectively to obfuscate sensitive information in the fea-ture space. We validate DISCO and other baseline with multiple attacks on inputs and attributes. We observe that
DISCO consistently achieves superior performance by dis-entangling representation learning from privacy using the pruning ﬁlter.
To this end, the contributions of this work can be sum-marized as follows:
• We introduce DISCO, a dynamic scheme for obfus-cation of sensitive channels to protect sensitive infor-mation in collaborative inference. DISCO provides a steerable and transferable privacy-utility trade-off at inference.
• We propose diverse attack schemes for sensitive inputs and attributes and achieve signiﬁcant performance gain over existing state-of-the-art methods across multiple datasets.
• To encourage rigorous exploration of attack schemes for private collaborative inference, we release a bench-mark dataset of 1 million sensitive representations. 2.