Abstract
Heatmap regression has become the most prevalent choice for nowadays human pose estimation methods. The ground-truth heatmaps are usually constructed via cover-ing all skeletal keypoints by 2D gaussian kernels. The stan-dard deviations of these kernels are ﬁxed. However, for bottom-up methods, which need to handle a large vari-ance of human scales and labeling ambiguities, the cur-rent practice seems unreasonable. To better cope with these problems, we propose the scale-adaptive heatmap regres-sion (SAHR) method, which can adaptively adjust the stan-In this way, SAHR is dard deviation for each keypoint. more tolerant of various human scales and labeling am-biguities. However, SAHR may aggravate the imbalance between fore-background samples, which potentially hurts the improvement of SAHR. Thus, we further introduce the weight-adaptive heatmap regression (WAHR) to help bal-ance the fore-background samples. Extensive experiments show that SAHR together with WAHR largely improves the accuracy of bottom-up human pose estimation. As a result, we ﬁnally outperform the state-of-the-art model by +1.5AP and achieve 72.0AP on COCO test-dev2017, which is com-parable with the performances of most top-down methods.
Source codes are available at https://github.com/ greatlog/SWAHR-HumanPose. 1.

Introduction
Multi-person human pose estimation (HPE) aims to lo-cate skeletal keypoints of all persons in a given RGB image.
It has been widely applied in human activity recognition, human computer interaction, animation etc. Current human pose estimation methods fall into two categories: top-down
∗This work is done when Zhengxiong is an intern at MEGVII Research. (a) (b) (c) (d) (e) (f)
Figure 1. Top row: the noses of different persons are covered by gaussian kernels with the same standard deviation. Bottom row: the standard deviations for keypoints of different persons are adap-tively adjusted in SAHR. and bottom-up. In top-down methods, all persons are ﬁrstly cropped out by a human detector and then resized to the same size before they are input to the keypoints detector.
Oppositely, bottom-up methods directly detect keypoints of all persons simultaneously. It is more light-weight fast but suffers from various human scales.
Heatmap regression is widely used in both top-down and bottom-up HPE methods. The ground-truth heatmaps are constructed by putting 2D Gaussian kernels on all key-points. They are used to supervise the predicted heatmaps via L2 loss. This method is easy to be implemented and has much higher accuracy than traditional coordinate re-gression [35, 34, 30]. However, in current practice, different keypoints are covered by gaussian kernels with the same standard deviation [36, 6, 25], which means that different keypoints are supervised by the same constructed heatmaps. 13264
We argue that this is unreasonable in two aspects. Firstly, keypoints of different scales are semantically discriminative in regions of different spatial sizes. It may cause confusion to put the same gaussian kernel on all keypoints. As shown in the top row of Figure 1, the noses of different persons are covered by gaussian kernels with the same deviation (σ = 2). In (a), the covered region is restricted on the top of the nose. But in (b), the Gaussian kernel could cover the face, and in (c), the whole head is even covered. The various covered regions for the same keypoint may cause semantic confusion. Secondly, even humans could not label the key-points with pixel-wise accuracy, and the ground-truth co-ordinates may have inherent ambiguities [13, 8]. Thus the keypoints could be treated as distributions (instead of dis-crete points) centered around the labeled coordinates. Their standard deviations represent their uncertainties and should be proportion to the labeling ambiguities. However, current
It seems to practice keeps the standard deviations ﬁxed. have ignored the inﬂuence of various labeling ambiguities of different keypoints.
From the above discussion, the standard deviation for each keypoint should be related to its scale and uncertainty.
A straightforward way to solve these issues is manually labeling different keypoints with different standard devia-tions. However, this work is extremely labor-intensive and time-consuming. Besides, it is difﬁcult to deﬁne customized standard deviations for different keypoints. Towards this problem, we propose the scale-adaptive heatmap regression (SAHR), which can adaptively adjust the standard deviation for each keypoint by itself.
Speciﬁcally, we ﬁrstly cover all keypoints by Gaussian kernels of the same base standard deviation σ0. We add a new branch to predict scale maps s, which are of the same shape as ground-truth heatmaps. Then we modify s the original standard deviation for each keypoint to σ0 · by a point-wise operation. Thus to some extent, s rep-resents the scales and uncertainties of corresponding key-points.
In this way, the suitable standard deviations for different keypoints could be adaptively learned, and thus
SAHR may be more tolerant of various human scales and labeling ambiguities. However, as shown in the bottom row of Figure 1, SAHR may aggravate the imbalance be-tween fore-background samples, which potentially restricts the improvements of SAHR [21, 19]. Motivated by focal loss for classiﬁcation [21], we further introduce the weight-adaptive heatmap regression (WAHR), which can automati-cally down-weight the loss of relatively easier samples, and focus more on relatively harder samples. Experiments show that the improvements brought by SAHR can be further ad-vanced by WAHR.
Our contributions can be summarized as four points: 1. To the best of our knowledge, this is the ﬁrst paper that focuses on the problems in heatmap regression when tackling large variance of human scales and labeling ambiguities. We attempt to alleviate these problems by scale and uncertainty prediction. 2. We propose a scale-adaptive heatmap regression (SAHR), which can adaptively adjust the standard de-viation of the Gaussian kernel for each keypoint, en-abling the model to be more tolerant of various human scales and labeling ambiguities. 3. We propose a weight-adaptive heatmap regression (WAHR) to alleviate the severe imbalance between foreground and background samples. It could automat-ically focus more on relatively harder examples and fully exploit the superiority of SAHR. 4. Our model outperforms the state-of-the-art model by 1.5AP and achieves 72.0AP on COCO test-dev2017, which is comparable with the performances of most top-down methods. 2.