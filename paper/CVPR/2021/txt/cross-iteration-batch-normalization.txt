Abstract
A well-known issue of Batch Normalization is its signiﬁ-cantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statis-tics upon which the normalization is deﬁned cannot be re-liably estimated from it during a training iteration. To ad-dress this problem, we present Cross-Iteration Batch Nor-malization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation qual-ity. A challenge of computing statistics over multiple itera-tions is that the network activations from different iterations are not comparable to each other due to changes in net-work weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polyno-mials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classiﬁcation with small mini-batch sizes, CBN is found to outperform the original batch nor-malization and a direct calculation of statistics over pre-vious iterations without the proposed compensation tech-nique. Code is available at https://aka.ms/cbn. 1.

Introduction
Batch Normalization (BN) [10] has played a signiﬁcant role in the success of deep neural networks. It was intro-duced to address the issue of internal covariate shift, where the distribution of network activations changes during train-ing iterations due to the updates of network parameters.
This shift is commonly believed to be disruptive to network training, and BN alleviates this problem through normaliza-tion of the network activations by their mean and variance, computed over the examples within the mini-batch at each iteration. With this normalization, network training can be performed at much higher learning rates and with less sen-sitivity to weight initialization.
In BN, it is assumed that the distribution statistics for the examples within each mini-batch reﬂect the statistics over
*This work is done when Zhuliang Yao is an intern at Microsoft Re-search Asia. Correspondence to: Yue Cao (yuecao@microsoft.com).
{yuecao,Shuxin.Zheng,stevelin}@microsoft.com the full training set. While this assumption is generally valid for large batch sizes, it breaks down in the small batch size regime [18, 31, 9], where noisy statistics computed from small sets of examples can lead to a dramatic drop in per-formance. This problem hinders the application of BN to memory-consuming tasks such as object detection [20, 3], semantic segmentation [15, 2] and action recognition [30], where batch sizes are limited due to memory constraints.
Towards improving estimation of statistics in the small batch size regime, alternative normalizers have been pro-posed. Several of them, including Layer Normalization (LN) [1], Instance Normalization (IN) [28], and Group Nor-malization (GN) [31], compute the mean and variance over the channel dimension, independent of batch size. Differ-ent channel-wise normalization techniques, however, tend to be suitable for different tasks, depending on the set of channels involved. Although GN is designed for detec-tion task, the slow inference speed limits its practical usage.
On the other hand, synchronized BN (SyncBN) [18] yields consistent improvements by processing larger batch sizes across multiple GPUs. These gains in performance come at the cost of additional overhead needed for synchronization across the devices.
A seldom explored direction for estimating better statis-tics is to compute them over the examples from multiple re-cent training iterations, instead of from only the current iter-ation as done in previous techniques. This can substantially enlarge the pool of data from which the mean and variance are obtained. However, there exists an obvious drawback to this approach, in that the activation values from differ-ent iterations are not comparable to each other due to the changes in network weights. As shown in Figure 1, directly calculating the statistics over multiple iterations, which we refer to as Naive CBN, results in lower accuracy.
In this paper, we present a method that compensates for the network weight changes among iterations, so that ex-amples from preceding iterations can be effectively used to improve batch normalization. Our method, called Cross-Iteration Batch Normalization (CBN), is motivated by the observation that network weights change gradually, in-stead of abruptly, between consecutive training iterations, 12331
normalization of activations has become nearly as preva-lent. By normalizing hidden activations by their statis-tics within each mini-batch, BN effectively alleviates the vanishing gradient problem and signiﬁcantly speeds up the training of deep networks. To mitigate the mini-batch size dependency of BN, a number of variants have been pro-posed, including Layer Normalization (LN) [1], Instance
Normalization (IN) [28], Group Normalization (GN) [31], and Batch Instance Normalization (BIN) [17]. The motiva-tion of LN is to explore more suitable statistics for sequen-tial models, while IN performs normalization in a manner similar to BN but with statistics only for each instance. GN achieves a balance between IN and LN, by dividing features into multiple groups along the channel dimension and com-puting the mean and variance within each group for nor-malization. BIN introduces a learnable method for auto-matically switching between normalizing and maintaining style information, enjoying the advantages of both BN and
IN on style transfer tasks. Cross-GPU Batch Normaliza-tion (CGBN or SyncBN) [18] extends BN across multiple
GPUs for the purpose of increasing the effective batch size.
Though providing higher accuracy, it introduces synchro-nization overhead to the training process. Kalman Normal-ization (KN) [29] presents a Kalman ﬁltering procedure for estimating the statistics for a network layer from the layer’s observed statistics and the computed statistics of previous layers.
Batch Renormalization (BRN) [9] is the ﬁrst attempt to utilize the statistics of recent iterations for normalization. It does not compensate for the statistics from recent iterations, but rather it down-weights the importance of statistics from distant iterations. This down-weighting heuristic, however, does not make the resulting statistics “correct”, as the statis-tics from recent iterations are not of the current network weights. BRN can be deemed as a special version of our
Naive CBN baseline (without Taylor polynomial approxi-mation), where distant iterations are down-weighted.
Recent work have also investigated the normalization of network parameters. In Weight Normalization (WN) [22], the optimization of network weights is improved through a reparameterization of weight vectors into their length and direction. Weight Standardization (WS) [19] instead repa-rameterizes weights based on their ﬁrst and second mo-ments for the purpose of smoothing the loss landscape of the optimization problem. To combine the advantages of mul-tiple normalization techniques, Switchable Normalization (SN) [16] and Sparse Switchable Normalization (SSN) [24] make use of differentiable learning to switch among differ-ent normalization methods.
The proposed CBN takes an activation normalization ap-proach that aims to mitigate the mini-batch dependency of
BN. Different from existing techniques, it provides a way to effectively aggregate statistics across multiple training iter-12332
Figure 1. Top-1 classiﬁcation accuracy vs. batch sizes per it-eration. The base model is a ResNet-18 [6] trained on Ima-geNet [21]. The accuracy of BN [10] drops rapidly when the batch size is reduced. BRN [9] stabilizes BN a little but still has trou-ble with smaller batch sizes. GN [31] exhibits stable performance but underperforms BN on adequate batch sizes. CBN compen-sates for the reduced batch size per GPU by exploiting approxi-mated statistics from recent iterations (Temporal window size de-notes how many recent iterationss are utilized for statistics com-putation). CBN shows relatively stable performance over different batch sizes. Naive CBN, which directly calculates statistics from recent iterations without compensation, is shown not to work well. thanks to the iterative nature of Stochastic Gradient De-scent (SGD). As a result, the mean and variance of exam-ples from recent iterations can be well approximated for the current network weights via a low-order Taylor polynomial, deﬁned on gradients of the statistics with respect to the net-work weights. The compensated means and variances from multiple recent iterations are averaged with those of the cur-rent iteration to produce better estimates of the statistics.
In the small batch size regime, CBN leads to appreciable performance improvements over the original BN, as exhib-ited in Figure 1. The superiority of our proposed approach is further demonstrated through more extensive experiments on ImageNet classiﬁcation and object detection on COCO.
These gains are obtained with negligible overhead, as the statistics from previous iterations have already been com-puted and Taylor polynomials are simple to evaluate. With this work, it is shown that cues for batch normalization can successfully be extracted along the time dimension, open-ing a new direction for investigation. 2.