Abstract
The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from ofﬂine-extracted dense video features from vi-sion models and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these
ﬁxed features sub-optimal for downstream tasks. Moreover, due to the high computational overload of dense video fea-tures, it is often difﬁcult (or infeasible) to plug feature ex-tractors directly into existing approaches for easy ﬁnetun-ing. To provide a remedy to this dilemma, we propose a generic framework CLIPBERT that enables affordable end-to-end learning for video-and-language tasks, by employ-ing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each train-ing step. Experiments on text-to-video retrieval and video question answering on six datasets demonstrate that CLIP-BERT outperforms (or is on par with) existing methods that exploit full-length videos, suggesting that end-to-end learn-ing with just a few sparsely sampled clips is often more accurate than using densely extracted ofﬂine features from full-length videos, proving the proverbial less-is-more prin-ciple. Videos in the datasets are from considerably differ-ent domains and lengths, ranging from 3-second generic-domain GIF videos to 180-second YouTube human activity videos, showing the generalization ability of our approach.
Comprehensive ablation studies and thorough analyses are provided to dissect what factors lead to this success. Our code is publicly available.1 1.

Introduction
Humans communicate with each other in this interactive and dynamic visual world via languages, signs, and ges-tures. The ability to jointly understand both visual and
* Equal contribution. 1https://github.com/jayleicn/ClipBERT o e d
V i g n i l p m a s e s n e
D
Text o e d
V i g n i l p m a s e s r a p
S
Vision encoder
Language encoder
Vision encoder
Text
Language encoder
Existing	mehods
Cross-modal modeling
Video-level prediction i t n e d a r g
/ p o t s
/
Cross-modal modeling
Cross-modal modeling
Cross-modal modeling
Prediction
Prediction
Prediction
Ours
Video-level prediction
Text	feature
Clip	feature
Figure 1: Comparison between popular video-and-language learning paradigm (top) and CLIPBERT (bottom). In contrast to most existing methods that utilize ofﬂine (stop gradient) extracted dense video features and text features, CLIPBERT uses sparsely sampled clips and raw text tokens for end-to-end modeling. textual clues is an essential ability for intelligent agents to interpret multimodal signals in the physical world. A wide range of tasks based on real-life videos have been designed to test such ability, including text-to-video re-trieval [72, 26, 51], video captioning [51, 72, 79], video question answering [71, 21, 31, 32], and video moment re-trieval [1, 17, 33]. The de facto paradigm to tackle these cross-modal tasks is to ﬁrst extract dense video features from pre-trained vision models [19, 3] and text features from pre-trained language models [47, 10], then apply mul-timodal fusion to wrangle together these ﬁxed representa-tions in a shared embedding space (Figure 1 (top)).
Existing approaches [21, 31, 80, 29] following this paradigm have achieved strong success, yet suffer from two main drawbacks: (i) Disconnection in tasks/domains: of-ﬂine feature extractors are often trained on tasks and do-mains different from the target task, e.g., features learned for action recognition on human activity videos [24] are in-congruently applied to downstream video question answer-ing on generic-domain GIF videos [21]. (ii) Disconnection 7331 	 	
in multimodal features: features from different modalities are learned independent of each other, e.g., action recogni-tion models [59, 63, 3] are typically trained on pure videos without textual input, yet are applied to video+language tasks. End-to-end task-speciﬁc ﬁnetuning offers a way to mitigate these inherent disconnections. However, extract-ing features from the full sequence of video frames, as in most existing work, casts excessive demand on memory and computation, rendering it difﬁcult or even infeasible to di-rectly plug feature extractors into a video+language learn-ing framework for efﬁcient end-to-end ﬁnetuning.
Motivated by this, we propose CLIPBERT, a generic and efﬁcient framework for end-to-end video-and-language learning (Figure 1 (bottom)). Two aspects distinguish CLIP-BERT from previous work. First, in contrast to densely extracting video features (adopted by most existing meth-ods), CLIPBERT sparsely samples only one single or a few short clips from the full-length videos at each training step.
The hypothesis is that sparse clips already capture key vi-sual and semantic information in the video, as consecutive clips usually contain similar semantics from a continuous scene. Thus, a handful of clips are sufﬁcient for training, in-stead of using the full video. Then, predictions from multi-ple densely-sampled clips are aggregated to obtain the ﬁnal video-level prediction during inference, which is less com-putational demanding. This sparse-training-then-dense-inference strategy greatly reduces memory needs and com-putations, allowing economical end-to-end learning from raw video frame pixels and language tokens.
The second differentiating aspect concerns the initializa-tion of model weights (i.e., transfer through pre-training). image-text pre-training (e.g., using
In recent literature,
COCO Captions [4] or Visual Genome Captions [27]) has been applied to image-text tasks [58, 41, 5, 55, 20, 34, 78], and video-text pre-training (e.g., using HowTo100M [43]) to video-related tasks [56, 80, 14, 35]. There has been no study to cross-examine the effect of image-text pre-training
Intuitively, visual features learned on video-text tasks. through pre-training from large-scale image datasets should also help video understanding tasks that rely on visual clues in static video frames. To investigate this, we use 2D (e.g.,
ResNet-50 [19]) instead of 3D architectures [59, 3, 70] as our visual backbone for video encoding, allowing us to har-ness the power of image-text pre-training for video-text un-derstanding along with the advantages of low memory cost and runtime efﬁciency. Empirically, we observe that the knowledge learned in image-text pre-training indeed helps video-text tasks; this simple strategy helps us achieve better or comparable performance to previous state of the art on text-to-video retrieval and video question answering tasks. (i) We propose
CLIPBERT, a new end-to-end learning framework for
Experiments show that CLIP-video+language tasks.
Our contributions are three-fold:
BERT achieves superior (or on par) performance than ex-isting methods across diverse video-text tasks, where the average video length ranges from a few seconds to three minutes. (ii) Our work suggests “less is more”: the pro-posed end-to-end training strategy with a single or a few (less) sparsely sampled clips is often more accurate than traditional approaches that employ densely extracted video features. (iii) We demonstrate that image-text pre-training beneﬁts video-text tasks. We also provide comprehensive ablation studies to reveal the key factors that lead to the suc-cess of CLIPBERT, in hope of inspiring more future work. 2.