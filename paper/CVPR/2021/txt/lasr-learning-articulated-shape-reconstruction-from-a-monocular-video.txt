Abstract 1.

Introduction
Remarkable progress has been made in 3D reconstruc-tion of rigid structures from a video or a collection of im-ages. However, it is still challenging to reconstruct nonrigid structures from RGB inputs, due to its under-constrained nature. While template-based approaches, such as para-metric shape models, have achieved great success in mod-eling the “closed world” of known object categories, they cannot well handle the “open-world” of novel object cat-egories or outlier shapes.
In this work, we introduce a template-free approach to learn 3D shapes from a sin-gle video. It adopts an analysis-by-synthesis strategy that forward-renders object silhouette, optical ﬂow, and pixel values to compare with video observations, which generates gradients to adjust the camera, shape and motion parame-ters. Without using a category-speciﬁc shape template, our method faithfully reconstructs nonrigid 3D structures from videos of human, animals, and objects of unknown classes.
Our code is available at lasr-google.github.io.
∗Work done in an internship at Google.
Perceiving and modeling the geometry and dynamics of 3D entities is an open research problem in computer vision and has numerous applications. One fundamental challenge is the under-constrained nature of the problem: from lim-ited 2D image measurements, there exist multiple interpre-tations of the geometry and motion of the 3D world.
A recent and promising trend for addressing this chal-lenge is exploiting data priors, which have proven quite suc-cessful for high-level vision tasks, such as image classiﬁca-tion and object detection [13, 31]. However, in contrast to high-level vision tasks, it is often costly to obtain 3D an-notations for real-world entities. For example, SMPL [33] is learned from thousands of registered 3D scans of human.
SMAL [58] is learned from scans of animal toys and a man-ually rigged mesh model. It involves nontrivial efforts to collect such data for an arbitrary object category. There-fore, existing methods often fail to capture objects of novel or unknown classes, and hallucinate an average 3D structure based on the category shape prior, as shown in Fig. 1.
Interestingly, remarkable progress has been made in the
ﬁeld of SLAM and structure-from-motion without relying 15980
on strong shape priors by taking advantage of multiview data recordings. However, such results are limited to static scenes. We explore an intermediate regime between these two extremes: Can one reconstruct an articulated shape from video data without relying on template priors?
Why videos? To reconstruct 3D object shape from im-ages, prior work learns category-speciﬁc shape models ei-ther from 3D data [17, 41] or from 2D supervision, such as object silhouette and keypoints in a large image collec-tion [10, 18, 23, 30]. However, 3D data are generally dif-ﬁcult to acquire at a large scale due to sensor design. Al-though it is easier to collect images of the same category, enforcing multiview constraints is often challenging, due to ambiguities of associating 2D observations across instances and under different viewpoints [11, 39]. Video serves as an alternative to depth scans and image collections – videos are easier to acquire, and provide well-deﬁned multiview constraints on the 3D shape of the same instance.
Why optical ﬂow? To solve the inverse problem, prior work discussed various forms of 2D constraints or super-vision, such as object silhouette, texture, 2D keypoints, and semantic parts [4, 18, 23, 30]. Why should motion be treated as a ﬁrst-class citizen? Besides that optical
ﬂow naturally encodes correspondences, it provides more
ﬁne-grained information than keypoints as well as semantic parts. Different from long-range point tracks, which is the classic input for NRSfM [42], optical ﬂow can be obtained more reliably [47, 52] over two consecutive frames.
Why not nonrigid SfM? NRSfM deals with a problem similar to ours: given a set of 2D point trajectories depict-ing a deformable object in a collection of images, the goal is to recover the 3D object shape and pose (i.e., relative camera position) in each view. Usually, trajectories of 2D points are factorized into low-rank shape and motion matri-ces [8, 19, 26] without using 3D shape templates. Although
NRSfM is able to deal with generic shapes, it requires reli-able long-term point tracks or keypoint annotations, which are challenging to acquire densely in practice [42, 44, 46].
Proposed approach: Instead of inferring 3D shapes from category-speciﬁc image collections or point trajectories, we build an articulated shape model from a monocular video of an object. Recent progress in differentiable rendering al-lows one to recast the problem as an analysis-by-synthesis task: we solve the inverse graphics problem of recover-ing the 3D object shape (including spacetime deformations) and camera trajectories (including intrinsics) to ﬁt video observations, such as object silhouette, raw pixels, and op-tical ﬂow. An overview of the pipeline is shown in Fig. 2.
Contributions: We propose a method for articulated shape reconstruction from a monocular video that does not require a prior template or category information. It takes advantage of dense two-frame optical ﬂow to overcome the inherent ambiguity in the nonrigid structure and motion estimation
Table 1.