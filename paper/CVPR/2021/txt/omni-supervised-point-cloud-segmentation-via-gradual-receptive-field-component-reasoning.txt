Abstract
Global Context Recognition
Hidden features in neural network usually fail to learn informative representation for 3D segmentation as supervi-sions are only given on output prediction, while this can be solved by omni-scale supervision on intermediate layers. In this paper, we bring the ﬁrst omni-scale supervision method to point cloud segmentation via the proposed gradual Re-ceptive Field Component Reasoning (RFCR), where target
Receptive Field Component Codes (RFCCs) are designed to record categories within receptive ﬁelds for hidden units in the encoder. Then, target RFCCs will supervise the de-coder to gradually infer the RFCCs in a coarse-to-ﬁne cat-egories reasoning manner, and ﬁnally obtain the semantic labels. Because many hidden features are inactive with tiny magnitude and make minor contributions to RFCC predic-tion, we propose a Feature Densiﬁcation with a centrifu-gal potential to obtain more unambiguous features, and it is in effect equivalent to entropy regularization over features.
More active features can further unleash the potential of our omni-supervision method. We embed our method into four prevailing backbones and test on three challenging bench-marks. Our method can signiﬁcantly improve the backbones in all three datasets. Speciﬁcally, our method brings new state-of-the-art performances for S3DIS as well as Seman-tic3D and ranks the 1st in the ScanNet benchmark among all the point-based methods. Code is publicly available at https://github.com/azuki-miho/RFCR. i
R e c e p t i v e
F e d
C o m p o n e n t l
RFCC
.  .  .
R e a s o n n g i
.  .  .
.  .  . c
.  .  .
.  .  . floor chair table wall bookshelf unlabeled
Figure 1: Illustration of Receptive Field Component Rea-soning for a point cloud in ScanNet v2 from top to bottom.
The Receptive Field Component Code (RFCC) indicates the category components in the receptive ﬁeld. In the decod-ing stage, the segmentation problem is decomposed into a much easier global context recognition problem (predict-ing the global RFCCs, see the top of ﬁgure) and a series of receptive ﬁeld component reasoning problems. During rea-soning, the target RFCCs generated in the encoder are used as the groundtruth in the decoder to guide the network to gradually reason the RFCCs in a coarse-to-ﬁne manner, and
ﬁnally obtain the semantic labels. 1.

Introduction
Semantic segmentation of point cloud in which we need to infer the point-level labels is a typical but still challenging
*Corresponding Author task in 3D vision. Meanwhile, this technique can be widely used in many applications like robotics, autonomous driv-ing, and virtual/augmented reality.
To handle point cloud segmentation, previous works usu-11673      
ally introduced well-designed encoder-decoder architecture to hierarchically extract global context features in the en-coding stage, and distribute contextual features to points in the decoding stage to achieve point-wise labeling [5, 31, 38]. However, in the typical encoder-decoder framework, network is merely supervised by labels of points in the ﬁ-nal layer [36, 31, 9], while ignoring a critical fact that, hid-den units in other layers lack direct supervision to extract features with informative representation.
In other words, multi-scale/omni-scale supervision is indeed necessary.
In 2D vision, CVAE [28] attempted to give a multi-scale prediction and supervision to extract useful features in seg-mentation task. CPM [35] and MSS-net [14] tried to add intermediate supervision periodically and layer-wise loss, respectively. PointRend [16] proposed to segment image in low-resolution, and iteratively up-sample the coarse predic-tion and ﬁne-tune it to obtain ﬁnal result, thus prediction at different scales can be supervised together.
However, so far, no one succeed in applying multi-scale, let alone omni-scale supervision to 3D semantic segmen-tation, due to the irregularity of point cloud. Unlike in image domain, it is hard to up-sample the hidden features to the original resolution through simple tiling or interpo-lation, because there is no ﬁxed mapping relationship be-tween sampled point cloud and original point cloud espe-cially when the sampling is random [36, 9]. Additionally, the common up-sampling methods using nearest neighbors cannot trace the encoding relationship, thus introducing im-proper supervisions to the intermediate features (referring
Sec 4.4 for discussion). More recently, SceneEncoder [37] provided a method to supervise the center-most layer to ex-tract meaningful global features, but lots of other layers re-main unhandled.
To solve this problem, we propose an omni-scale super-vision method via gradual Receptive Field Component Rea-soning. Instead of up-sampling the hidden features to the original resolution, we design a Receptive Field Compo-nent Code (RFCC) to effectively trace the encoding rela-tionship and represent the categories within receptive ﬁeld for each hidden unit. Based upon this, we generate the target
RFCCs at different layers from semantic labels in the en-coding stage to supervise the network at all scales. Specif-ically, in the decoding stage, the target RFCCs will super-vise the network to predict the RFCCs at different scales, and the features (hints) from skip link can help further de-duce RFCCs within more local and speciﬁc receptive ﬁelds.
In this way, the decoding stage is transferred into a gradual reasoning procedure, as shown in Figure 1.
Inspired by SceneEncoder [37], for each sampled point in any layer of encoder, according to the existence of cat-egories in its receptive ﬁeld, a multi-hot binary code can be built, designated as target Receptive Field Component
Code (RFCC). The target RFCCs at different layers are gen-erated alongside the convolution and down-sampling, thus they can precisely record the existing categories in corre-sponding receptive ﬁelds without any extra annotations. In
Figure 1, we show the target RFCCs at various layers for a point cloud in the decoding stage, where the network will
ﬁrst recognize the global context (inferring the categories of objects existing in the whole point cloud). Then, contextual features will be up-sampled iteratively to gradually reason the RFCCs in a coarse-to-ﬁne manner. By comparing the target RFCCs and the predicted RFCCs, the omni-scale su-pervision can be realized.
It is noteworthy that even the network reasons the RFCCs gradually, the training and in-ference of network is implemented in a end-to-end manner.
Additionally, to further unleash the potential of omni-scale supervision, more active features (features with large magnitude) are required to make unambiguous contribu-tion to the RFCC prediction. Contrarily, in traditional net-works [36, 31, 37], lots of units are inactive with tiny magnitude, such that having minor contribution to the ﬁ-nal prediction. The principle underlying the above ob-servations comes from entropy regularization [6, 18] over features, where greater number of active dimensionalities would bring low-density separation between positive fea-tures and negative features, generating more unambiguous features with certain signals. Consequently, in point cloud scenario, more certainty in features can help the training of the network to better reason the RFCCs at various scales and ﬁnally predict the semantic labels. Motivated by this, we proposed a Feature Densiﬁcation method with a well-deigned potential function to push hidden features away from 0. Moreover, this potential is in effect equivalent to a entropy loss over features (detailed deduction is shown in
Sec 3.4), leading to a simple but highly effective regulariza-tion for intermediate features.
To evaluate the performance and versatility of our method in point cloud semantic segmentation task, we embed our method into four prevailing backbones (de-formable KPConv, rigid KPConv [31], RandLA [9], and
SceneEncoder [37]), and test on three challenging point cloud datasets (ScanNet v2 [2] for indoor cluttered rooms,
S3DIS [1] for large indoor space, and Semantic3D [7] for large-scale outdoor space). In all the three datasets, we out-perform the backbone methods and almost all the state-of-the-art point-based competitors. What’s more, we also push the state-of-the-art of S3DIS [1] and Semantic3D [7] ahead. 2.