Abstract (a).  Decoder fusion for referring image segmentation
Recently, referring image segmentation has aroused widespread interest. Previous methods perform the multi-modal fusion between language and vision at the decoding side of the network. And, linguistic feature interacts with visual feature of each scale separately, which ignores the continuous guidance of language to multi-scale visual fea-tures. In this work, we propose an encoder fusion network (EFN), which transforms the visual encoder into a multi-modal feature learning network, and uses language to re-ﬁne the multi-modal features progressively. Moreover, a co-attention mechanism is embedded in the EFN to realize the parallel update of multi-modal features, which can promote the consistent of the cross-modal information representa-tion in the semantic space. Finally, we propose a bound-ary enhancement module (BEM) to make the network pay more attention to the ﬁne structure. The experiment results on four benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance under different evaluation metrics without any post-processing. 1.

Introduction
Referring image segmentation aims to extract the most relevant visual region (object or stuff) in an image based on the referring expression. Unlike the traditional semantic and instance segmentation, which require to correctly segment each semantic category or each object in an image, refer-ring image segmentation needs to ﬁnd a certain part of the image according to the understanding of the given language query. Therefore, it can be regarded as a pixel-wise fore-ground/background segmentation problem, and the output result is not limited by the predeﬁned semantic categories or object classes. This task has a wide range of potential applications in language-based human-robot interaction.
†Corresponding Author
Query: child in red (b).  Encoder fusion for referring image segmentation
Query: child in red
Visual feature
Multi-modal feature
Linguistic progressive guidance
Figure 1: Two multi-modal fusion mechanisms. Existing meth-ods achieve the fusion between language and vision in the decoder, while the proposed method does it in the encoder.
The key of this task is to realize the cross-modal match-ing between visual and linguistic features. The deep-learning community has rapidly improved the results of vision-language tasks over a short period of time. The rapid development of convolutional neural network (CN-N) and recurrent neural network (RNN) have made a qual-itative leap in the ability of understanding vision and lan-guage, thereby they can solve more complex pixel-level cross-modal prediction tasks. Early referring image seg-mentation methods [14, 26, 23, 33] mainly rely on the pow-erful learning ability of deep learning model. They directly concatenate linguistic features with visual features of each region, and then use the combined multi-modal features to generate the segmentation mask. Due to the lack of suf-ﬁcient interaction between two modalities, such solutions can not meet the requirements of real-world applications.
Recently, some works [36, 38, 1, 16, 17, 19] began to con-sider the linguistic and visual attention mechanisms to bet-ter aggregate these two kinds of features.
Although some referring image segmentation methods have been proposed in the last few years, there are still many problems that have not been explored. On the one hand, for the cross-modal fusion of vision and language. Previ-15506
ous methods usually adopt the decoder fusion strategy, in which the RGB image and the referring expression are fed into CNN or RNN to generate their own feature representa-tions separately, and then fuse these features in the decod-ing stage. However, this fusion strategy at the output side of the network either only considers the interaction between linguistic and highest-level visual features [26, 23] or com-bines the linguistic features with the visual features of each level independently (as shown in Fig. 1 (a)) [38, 16, 19].
They do not investigate the deep guidance of language to multi-modal fused features. Besides, some works utilize visual and linguistic attention mechanisms for cross-modal feature matching. But they update the linguistic and visual features in a serial mode [36, 1, 16, 17, 19], that is, they only update the feature of one modality at a speciﬁc time, which will lead to the update delay of the features between different modalities and eventually weaken the consistency of the representation of multi-modal information. On the other hand, in CNNs, the repeated stride and pooling oper-ations may lead to the loss of some important ﬁne-structure information, but few referring image segmentation methods explicitly consider the problem of detail recovery.
To resolve the aforementioned problems, we propose an encoder fusion network with co-attention embedding (CEFNet) for referring image segmentation. Instead of the cross-modal information fusion at the output side, we adopt the encoder fusion strategy for the ﬁrst time to progressive-ly guide the multi-level cross-modal features by language.
The original visual feature encoder (e.g., ResNet) is trans-formed into a multi-modal feature encoder (as shown in
Fig. 1 (b)). The features of two modalities are deeply in-terleaved in the CNN encoder. Furthermore, to effectively play the guiding role of language, we adopt the co-attention mechanism to simultaneously update the features of differ-ent modalities. It utilizes the same afﬁnity matrix to project different features to the common feature subspace in a par-allel mode and better achieve the cross-modal matching to bridge the gap between coarse-grained referring expression and highly localized visual segmentation. We implement two simple and effective co-attention mechanisms such as vanilla co-attention and asymmetric co-attention, which of-fer a more insightful glimpse into the task of referring image segmentation. Finally, we design a boundary enhancement module (BEM), which captures and exploits boundary cues as guidance to gradually recover the details of the targeted region in the decoding stage of the network.
Our main contributions are as follows:
• We propose an encoder fusion network (EFN) that us-es language to guide the multi-modal feature learning, thereby realizing deep interweaving between multi-modal features. In the EFN, the co-attention mecha-nism is embedded to guarantee the semantic alignment of different modalities, which promotes the represen-tation ability of the language-targeted visual features.
• We introduce a boundary enhancement module (BE-M) to emphasize the attention of the network to the contour representation, which can help the network to gradually recover the ﬁner details.
• The proposed method achieves the state-of-the-art per-formance on four large-scale datasets including the
UNC, UNC+, Google-Ref and ReferIt with the speed of 50 FPS on an Nvidia GTX 1080Ti GPU. 2.