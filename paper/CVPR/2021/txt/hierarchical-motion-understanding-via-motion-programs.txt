Abstract
Current approaches to video analysis of human motion focus on raw pixels or keypoints as the basic units of rea-soning. We posit that adding higher-level motion primi-tives, which can capture natural coarser units of motion such as “backswing” or “follow-through”, can be used to improve downstream analysis tasks. This higher level of abstraction can also capture key features, such as loops of repeated primitives, that are currently inaccessible at lower levels of representation. We therefore introduce Motion Pro-grams, a neuro-symbolic, program-like representation that expresses motions as a composition of high-level primitives.
We also present a system for automatically inducing motion programs from videos of human motion and for leveraging motion programs in video synthesis. Experiments show that motion programs can accurately describe a diverse set of human motions and the inferred programs contain semanti-cally meaningful motion primitives, such as arm swings and jumping jacks. Our representation also beneﬁts downstream tasks such as video interpolation and video prediction and outperforms off-the-shelf models. We further demonstrate how these programs can detect diverse kinds of repetitive motion and facilitate interactive video editing. 1.

Introduction
Most current video analysis architectures operate on ei-ther pixel-level or keypoint-level transitions that are good at local and short-term motions of human bodies but do not handle higher-level spatial and longer-lasting temporal structures well. We posit that incorporating higher-level rea-soning of motion primitives and their relationships enables better video understanding and improved downstream task performance: after seeing a person perform jumping jacks a few times, it’s reasonable to predict that they may repeat the same action for a few more iterations.
We thus introduce a hierarchical motion understanding framework with three levels of abstraction, as shown in
Figure 1:
∗Equal contribution. Email: sumith@cs.stanford.edu.
Webpage: https://sumith1896.github.io/motion2prog.
• Level 1 (Figure 1b) is Keypoints, which are the basis of most current computer vision systems. This level provides low-level priors such as temporal smoothing.
• Level 2 (Figure 1c) is Concrete Motion Programs, a programmatic representation of motion consisting of a sequence of atomic actions or motion primitives, each describing a set of frames for a single action. This level provides priors on primitive level motion.
• Level 3 (Figure 1d) is Abstract Motion Programs, which describe higher-level groupings of the concrete primitives, such as patterns of repetitive motion. This level provides priors on sequences of primitives and their patterns.
We describe the concrete and abstract levels with sim-ple domain speciﬁc languages (DSLs). The Concrete DSL consists of motion primitives such as stationary, linear, and circular movement with necessary spatial and temporal mo-tion parameters. The Abstract DSL includes for-loops for capturing higher-level groupings of primitives. Along with the DSLs, we propose an algorithm to automatically synthe-size motion programs from videos. For the sake of brevity, we often refer to motion programs as simply programs.
To demonstrate the advantages of motion programs, we apply our methods for synthesizing motion programs from videos to three applications: video interpolation (Sec-tion 4.2), repetitive segment detection (Section 4.3) and video prediction (Section 4.4). For video interpolation, we demonstrate that when combined with state-of-the-art image synthesis models, such as SepConv [23], motion programs can achieve higher quality interpolations with bigger gaps between frames than previous approaches. For repetitive segment detection, we demonstrate that motion programs can detect and extract repetitive segments of human motion.
For video prediction, we show that motion programs can perform longer-range video prediction than HierchVid [29] and HistryRep [32] by identifying repetitive motions.
To summarize, our contributions are:
• We introduce a hierarchical motion understanding framework using motion programs.
• We present an algorithm for automatically synthesizing motion programs from raw videos. 6568
(a) Input Video (b) Detected Keypoints
...
Frame #
... (c) Concrete Motion Program (R Wrist
) circular_motion( circular_motion( center=(249.46, 134.96), radius=36.63, angle=13.29*t + 99.70, duration=14) center=(247.26, 137.07), radius=38.71, angle=-13.25*t + -93.46, duration=14)
... (d) Abstract Motion Program (R Wrist
) circular_motion(center=(249.46, 134.96), radius=36.63, angle= 13.29*t +  99.70, duration=14) circular_motion(center=(247.26, 137.07), radius=38.71, angle=-13.25*t + -93.46, duration=14)
... similar motion repeated for 24 times
I. Keypoint
Detection
II. Concrete
Program
Synthesis (Sec. 3.1)
III.  Abstract
Program
Synthesis (Sec. 3.2)
VI. 
Video
Synthesis
V. Motion
Program
Execution (Sec. 3.1 & 3.2) (e) Motion Program Manipulation
IV. Motion
Program
Manipulation (Sec. 4)
Ex1. Interpolation circular_motion(center=(249.46, 134.96), radius=36.63, angle=  6.65*t +  99.70, duration=28) circular_motion(center=(247.26, 137.07), radius=38.71, angle= -6.65*t + -93.46, duration=28)
... similar motion repeated for 24 times
Ex2. Prediction circular_motion(center=(249.46, 134.96), radius=36.63, angle= 13.29*t +  99.70, duration=14) circular_motion(center=(247.26, 137.07), radius=38.71, angle=-13.25*t + -93.46, duration=14)
... similar motion repeated for 24 times
Figure 1. Overview of our hierarchical motion understanding framework. We start with low-level keypoints extracted from raw videos and build higher level motion understanding. Higher levels of abstraction consist of a sequence of motion primitives and their repetitive patterns.
Similar motion in repetitive patterns are captured using a probabilistic for-loop body (refer Figure 3).
• By incorporating motion programs into video synthesis pipelines, we demonstrate higher performance in video interpolation, video prediction and also show how to detect and extract repetitive motion segments. 2.