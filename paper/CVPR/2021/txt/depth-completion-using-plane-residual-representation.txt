Abstract
The basic framework of depth completion is to predict a pixel-wise dense depth map using very sparse input data.
In this paper, we try to solve this problem in a more effec-tive way, by reformulating the regression-based depth esti-mation problem into a combination of depth plane classi-ﬁcation and residual regression. Our proposed approach is to initially densify sparse depth information by ﬁguring out which plane a pixel should lie among a number of dis-cretized depth planes, and then calculate the ﬁnal depth value by predicting the distance from the speciﬁed plane.
This will help the network to lessen the burden of directly regressing the absolute depth information from none, and to effectively obtain more accurate depth prediction result with less computation power and inference time. To do so, we ﬁrstly introduce a novel way of interpreting depth in-formation with the closest depth plane label p and a resid-ual value r, as we call it, Plane-Residual (PR) representa-tion. We also propose a depth completion network utilizing
PR representation consisting of a shared encoder and two decoders, where one classiﬁes the pixel’s depth plane la-bel, while the other one regresses the normalized distance from the classiﬁed depth plane. By interpreting depth in-formation in PR representation and using our correspond-ing depth completion network, we were able to acquire im-proved depth completion performance with faster computa-tion, compared to previous approaches. 1.

Introduction
Many different computer vision algorithms are becom-ing more reachable in our everyday life, starting from a smartphone camera and augmented reality (AR) / vir-tual reality (VR) applications to autonomous driving and even more complicated robotics tasks.
In order to solve these problems efﬁciently, obtaining precise and reliable 3D scene information is crucial. 3D reconstruction has been studied for many years and certainly can be categorized as one of the traditional computer vision tasks, but it is still a core technology. There are various ways of obtaining 3D information, such as monocular depth prediction, structure from motion, and multi-view stereo. However, 3D recon-struction using additional sensors alongside an RGB camera like secondary camera, depth sensors, or radar will be more effective, since it can utilize more accurate depth measure-ments as prior information. Given that most of the recent mobile devices have more than one camera and even a Li-DAR sensor, it is widely acceptable that 3D reconstruction by integrating multiple sensor inputs is a more efﬁcient and practical approach.
While stereo matching being one of the most conven-tional and reliable ways in 3D reconstruction, it shows its weakness in depth prediction on the farther area due to the physical restriction for large baseline between stereo cam-eras. Therefore, using a depth sensor to obtain more ac-curate and absolute initial depth information is also pre-ferred. However, one of the major downsides of commer-cialized depth sensors, such as 3D LiDAR, Kinect, and Re-alSense, is the sparsity of the measurement. Addressing this problem, various approaches emerged trying to densify, i.e.,
‘complete’ the sparse depth measurement into a dense depth map, namely, ‘depth completion’.
In recent years, there have been many different meth-ods trying to solve depth completion using deep learn-ing, starting from Ma and Karaman [18]. The challeng-ing part of these algorithms is that regression-based ap-proaches have difﬁculties in maintaining the information of the object boundary and may show mixed depth results [14].
A few early works addressed these difﬁculties by maxi-mizing the information from the RGB input and reﬁning the initial depth regression output to get better ﬁnal re-sults [3, 17, 16, 26, 4, 2, 21]. Other algorithms tried to utilize additional information that can be inferred from the depth map, such as surface normal, to give more geomet-rical guidance to the training process [16, 28, 22]. While these algorithms showed some promising outcomes, they still lack in preserving edge information and often require a large amount of computation power, as in heavy network memory and longer inference time, which are not suitable 13916
for real-time applications.
To tackle these problems, we introduce a Plane-Residual (PR) representation, a novel way of interpreting depth in-formation, and an end-to-end depth completion deep learn-ing network. PR representation expresses an absolute depth value of a pixel with two parameters (p, r), where p refers to the closest depth plane among a number of pre-deﬁned discretized depth planes, and r refers to the normalized dis-tance from the selected plane. With Plane-Residual rep-resentation, we can factorize the direct depth regression problem into a combination of discrete depth plane clas-siﬁcation and plane-by-plane residual regression. We also propose an end-to-end depth completion network using PR representation to execute this idea and present our results with good performance and fast computation. We did ex-tensive ablation studies and compared our algorithm with some state-of-the-art methods, both quantitatively and qual-itatively, to prove the effectiveness and the validity of our design choices. 2.