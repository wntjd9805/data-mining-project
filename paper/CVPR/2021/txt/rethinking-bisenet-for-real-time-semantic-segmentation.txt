Abstract
BiSeNet [28, 27] has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial informa-tion is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classiﬁcation, may be inefﬁ-cient for image segmentation due to the deﬁciency of task-speciﬁc design. To handle these problems, we propose a novel and efﬁcient structure named Short-Term Dense Con-catenate network (STDC network) by removing structure re-dundancy. Speciﬁcally, we gradually reduce the dimension of feature maps and use the aggregation of them for im-age representation, which forms the basic module of STDC network.
In the decoder, we propose a Detail Aggrega-tion module by integrating the learning of spatial informa-tion into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to pre-dict the ﬁnal segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effec-tiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On
Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images. Code is available at https://github.com/
MichaelFan01/STDC-Seg. 1.

Introduction
Semantic segmentation is a classic and fundamental topic in computer vision, which aims to assign pixel-level labels in images. The prosperity of deep learning greatly promotes the performance of semantic segmentation by making various breakthroughs [18, 27, 22, 4], coming with fast-growing demands in many applications, e.g., au-tonomous driving, video surveillance, robot sensing, and so
*Equal contribution.
†Co-corresponding author. 78 76 74 72 70 68 66
) (
%
U o
I n a e
M
STDC2-Seg75
BiSeNetV2-L
DF2-Seg2
DF2-Seg1
HMSeg
STDC1-Seg75
SFNet
STDC2-Seg50
BiSeNetV1 B
DF1-Seg
DFANet A
DABNet
ICNet
GAS
CAS
BiSeNetV2
FasterSeg
STDC1-Seg50
DF1-Seg-d8
TinyHMSeg
DFANet A'
BiSeNetV1 A
Fast-SCNN
DFANet B 0 20 40 60 80 100 120 140 160 180 200 220 240 260 280
Inference Speed (FPS)
Figure 1. Speed-Accuracy performance comparison on the
Cityscapes test set. Our methods are presented in red dots while other methods are presented in blue dots. Our approaches achieve state-of-the-art speed-accuracy trade-off. on. These applications motivate researchers to explore ef-fective and efﬁcient segmentation networks, particularly for mobile ﬁeld.
To fulﬁll those demands, many researchers propose to design low-latency, high-efﬁciency CNN models with sat-isfactory segmentation accuracy. These real-time seman-tic segmentation methods have achieved promising perfor-mance on various benchmarks. For real-time inference, some works, e.g., DFANet [18] and BiSeNetV1 [28] choose the lightweight backbones and investigate ways of feature fusion or aggregation modules to compensate for the drop of accuracy. However, these lightweight backbones bor-rowed from image classiﬁcation task may not be perfect for image segmentation problem due to the deﬁciency of task-speciﬁc design. Besides the choice of lightweight back-bones, restricting the input image size is another commonly used method to promote the inference speed. Smaller in-put resolution seems to be effective, but it can easily ne-glect the detailed appearance around boundaries and small objects. To tackle this problem, as shown in Figure 2(a),
BiSeNet [28, 27] adopt multi-path framework to combine the low-level details and high-level semantics. However, 9716
Figure 2. Illustration of architectures of BiSeNet [28] and our proposed approach. (a) presents Bilateral Segmentation Network (BiSeNet [28]), which use an extra Spatial Path to encode spatial information. (b) demonstrates our proposed method, which use a
Detail Guidance module to encode spatial information in the low-level features without an extra time-cosuming path. adding an additional path to get low-level features is time-consuming, and the auxiliary path is always lack of low-level information guidance.
To this end, we propose a novel hand-craft network for the purpose of faster inference speed, explainable structure, and competitive performance to that of existing methods.
First, we design a novel structure, called Short-Term Dense
Concatenate module (STDC module), to get variant scal-able receptive ﬁelds with a few parameters. Then, the STDC modules are seamlessly integrated into U-net architecture to form the STDC network, which greatly promote network performance in semantic segmentation task.
In details, as shown in Figure 3, we concatenate response maps from multiple continuous layers, each of which en-codes input image/feature in different scales and respec-tive ﬁelds, leading to multi-scale feature representation. To speed up, the ﬁlter size of layers is gradually reduced with negligible loss in segmentation performance. The details structure of STDC networks can be found in Table 2.
In the phase of decoding, as shown in Figure 2(b), in-stead of utilizing an extra time-consuming path, Detail
Guidance are adopted to guide the low-level layers for the learning of spatial details. We ﬁrst utilize Detail Aggre-gation module to generate detail ground-truth. Then, the binary cross-entropy loss and dice loss are employed to op-timize the learning task of detail information, which is con-sidered as one type of side-information learning. It should be noted that this side-information is not required in the in-ference time. Finally, the spatial details from low-level lay-ers and semantic information from deep layers are fused to predict the semantic segmentation results. The whole archi-tecture of our method is shown in Figure 4.
Our main contributions can be summarized as follows:
• We design a Short-Term Dense Concatenate module (STDC module) to extract deep features with scalable receptive ﬁeld and multi-scale information. This mod-ule promotes the performance of our STDC network with affordable computational cost.
• We propose the Detail Aggregation module to learn the decoder, leading to more precise preservation of spatial details in low-level layers without extra computation cost in the inference time.
• We conduct extensive experiments to present the ef-fectiveness of our methods. The experiment results present that STDC networks achieve new state-of-the-art results on ImageNet, Cityscapes and CamVid.
Speciﬁcally, our STDC1-Seg50 achieves 71.9% mIoU on the Cityscapes test set at a speed of 250.4 FPS on one NVIDIA GTX 1080Ti card. Under the same ex-periment setting, our STDC2-Seg75 achieves 76.8% mIoU at a speed of 97.0 FPS. 2.