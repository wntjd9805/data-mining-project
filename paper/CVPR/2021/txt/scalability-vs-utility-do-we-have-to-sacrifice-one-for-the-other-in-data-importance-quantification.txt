Abstract 1.

Introduction
Quantifying the importance of each training point to a learning task is a fundamental problem in machine learning and the estimated importance scores have been leveraged to guide a range of data workﬂows such as data summa-rization and domain adaption. One simple idea is to use the leave-one-out error of each training point to indicate its importance. Recent work has also proposed to use the Shap-ley value, as it deﬁnes a unique value distribution scheme that satisﬁes a set of appealing properties. However, cal-culating Shapley values is often expensive, which limits its applicability in real-world applications at scale. Multiple heuristics to improve the scalability of calculating Shapley values have been proposed recently, with the potential risk of compromising their utility in real-world applications.
How well do existing data quantiﬁcation methods per-form on existing workﬂows? How do these methods compare with each other, empirically and theoretically? Must we sacriﬁce scalability for the utility in these workﬂows when using these methods? In this paper, we conduct a novel theo-retical analysis comparing the utility of different importance quantiﬁcation methods, and report extensive experimental studies on existing and proposed workﬂows such as noisy la-bel detection, watermark removal, data summarization, data acquisition, and domain adaptation. We show that Shapley value approximation based on a KNN surrogate over pre-trained feature embeddings obtains comparable utility with existing algorithms while achieving signiﬁcant scalability improvement, often by orders of magnitude. Our theoretical analysis also justiﬁes its advantage over the leave-one-out error.
The code is available at https://github.com/AI-secure/Shapley-Study.
∗Equal contribution.
Understanding the importance of a single training exam-ple, relative to other training examples, to a learning task is a fundamental problem in machine learning (ML) which could have profound impact on a range of applications including interpretability, robustness, data acquisition, data valuation, among others [12, 7, 14].
In this paper, we are driven by two questions around this fundamental problem. Our contribution is a novel theoretical analysis and thorough experimental studies towards under-standing both questions.
Q1: Leave-one-out vs. Shapley? Given a training set D, a validation set Dval and a learning algorithm A, let the utility UA,Dval (D) be the validation accuracy of the model trained on D using A, recently there have been two lines of work in assigning relative importance to a data point z ∈ D.
A. Leave-one-out (LOO)-based Method & Inﬂuence
Function. One natural way to assign importance to z is by calculating its contribution to the rest of training data: vloo(z) ∝ UA,Dval (D) − UA,Dval (D\{z})
When we need to assign such an importance score to all data points in the training set, we need to train a large number of models. Thus researchers have proposed efﬁcient techniques to approximate this score, e.g., via inﬂuence function [14].
B. Shapley-based Method. Another natural way to as-sign importance to z is inspired by cooperative game theory and to use the Shapley value [12, 7]:
νshap(z) ∝ 1
N X
S⊆D\{z} 1
N −1 (cid:0)
|S| (cid:1) (cid:2)UA,Dval (S ∪ {z}) − UA,Dval (S)(cid:3)
Both approaches have recently been explored by researchers and have been applied to a range of ML tasks including noisy label detection, watermark removal, data summarization, active data acquisition, and domain adaptation [12, 7, 14]. 8239
However, one question remains: What’s the relationships and differences, both theoretically and empirically, between these two lines of approaches?
Q2: Exact Shapley vs. Shapley over Surrogates? As we will show in this paper, Shapley-based methods often outperforms leave-one-out-based methods, both theoretically and empirically. However, Shapley-based approaches can be expensive as one needs to train, for general classiﬁers, exponentially many models. Thus, many state-of-the-art approaches resort to a sampling-based approach [12, 7] to approximate this score. On the other hand, a recent work by
Jia et al. [11] has shown that for a certain family of classiﬁers, i.e., K-Nearest Neighbor (KNN), calculating this score can be done efﬁciently, in O(|D| log |D|) time for all data points in D.
Despite this, there still remains a question: Can we use a K-Nearest Neighbor classiﬁer as a surrogate model to calculate the Shapley value, and how does it perform on real-world applications compared with the vanilla exact Shapley value?
Technical Contributions. In this paper, we take the ﬁrst step towards understanding the above questions. We make contributions on both theoretical and empirical fronts.
• We conduct a novel theoretical analysis aiming at rigor-ously analyzing the differences between the leave-one-out-based and the Shapley-based methods. Speciﬁcally, we formalize two performance metrics speciﬁc to data importance: one focuses on the predictive power of data importance, studying whether it is indicative of a training point’s contribution to a random set; the other focuses on the ability of a data to discriminate “good” training points from “bad” ones. We show that for both performance met-rics, under certain technical conditions, the Shapley-based method can outperform leave-one-out–based approaches.
To our best knowledge, this is the ﬁrst theoretical analy-sis reasoning the relative performances of different data importance quantiﬁcation techniques.
• We conduct a thorough empirical study on a range of ML tasks, including noisy label detection, watermark removal, data summarization, active data acquisition, and domain adaptation on different benchmark datasets. Some have been used by previous work as a use case of data valu-ation methods, and some are proposed by us. On these tasks, we empirically investigate the relative performance between (1) leave-one-out–based methods and Shapley-based methods, and (2) exact Shapley-based methods and
Shapley over KNN Surrogates.
• Our empirical study suggests that the Shapley-over-KNN-Surrogates method performs well and achieves compara-ble results with, and often outperforms, all other meth-ods in quality while being orders of magnitude faster.
This gives us the ﬁrst practical algorithm over large-scale datasets that returns useful data importance scores for a range of important ML tasks. 2.