Abstract
We deﬁne the concept of CompositeTasking as the fusion of multiple, spatially distributed tasks, for various aspects of image understanding. Learning to perform spatially dis-tributed tasks is motivated by the frequent availability of only sparse labels across tasks, and the desire for a com-pact multi-tasking network. To facilitate CompositeTask-ing, we introduce a novel task conditioning model – a sin-gle encoder-decoder network that performs multiple, spa-tially varying tasks at once. The proposed network takes an image and a set of pixel-wise dense task requests as in-puts, and performs the requested prediction task for each pixel. Moreover, we also learn the composition of tasks that needs to be performed according to some CompositeTasking rules, which includes the decision of where to apply which
It not only offers us a compact network for multi-task. tasking, but also allows for task-editing. Another strength of the proposed method is demonstrated by only having to supply sparse supervision per task. The obtained results are on par with our baselines that use dense supervision and a multi-headed multi-tasking design. The source code will be made publicly available at www.github.com/ nikola3794/composite-tasking. 1.

Introduction
Intuitively, different image understanding tasks offer complementary information for scene understanding and reasoning [24, 53, 2, 62, 57, 61, 11, 50]. Therefore, net-works that can perform multiple visual tasks on the same image are of very high interest [10, 34, 22, 51, 55]. A key aspect – effectively serving the ultimate goal of scene un-derstanding and reasoning – is often not part of their design.
This paper is about this utility question: can we determine where in the image it is necessary (or even meaningful) to perform a task? For example, the task of recognizing hu-man body parts is meaningful only in the presence of hu-mans. Similarly, any attempt to estimate the normals of the sky is absurd.
One may argue that we cannot know beforehand whether some task is necessary to be performed, without recogniz-ing the image content. The content of the image may then reveal the task necessity. This begs the question whether we can know what task needs to be performed where, while bypassing the content-task pairing altogether? When the an-swer to where is known – either by learning or not – we aim to design an algorithm that executes the given multi-task instructions in an efﬁcient manner. For example, some ap-plications of Augmented Reality may require human poses and the normals of the interacting surfaces. We show that such ﬂexibility to locally activate some tasks allows us to design more compact multi-tasking networks.
The task speciﬁc annotations of images are often sparse, either by deﬁnition or due to missing annotations. Take, for example, facial landmarks or image salience. Sometimes, the annotations may be missing simply because of being fu-tile. Even the well-curated PASCAL-MT [38, 34] dataset has the sparsity of 30.4%, 7.5%, 41.9%, 60.0%, for se-mantic segmentation, human body parts, surface normals, and salience, respectively. Such label sparsity only tends to get worse, if the image annotations are crowd-sourced.
In fact, it is simply impractical to expect pixel-wise dense annotations for large datasets, even at locations where the annotations are well deﬁned. The case of merging datasets, by cross-label intersection, follows the same behaviour of sparsity. Under such circumstances, it may be unnecessary to waste computational resources during learning, for im-age pixels without labels. This calls for an efﬁcient learning paradigm for multi-tasking from sparse labels. In this work, we show that efﬁcient learning from sparse multi-task la-bels and executing spatially chosen multi-task instructions go hand-in-hand.
The key idea of this paper is rather simple. We design a convolutional neural network that performs multiple, pixel-wise tasks. We feed every image along with a composi-tion of spatially distributed multiple task requests – which we call Task Palette – to execute pixel-speciﬁc tasks. This process we call CompositeTasking. The proposed network uses a single encoder-decoder architecture to perform all 6870
Figure 1: CompositeTasking. Given an RGB image and a Task
Palette as inputs, our CompositeTasking performs locally request-speciﬁc tasks to compute the output. the tasks in one forward pass. The simplicity of such archi-tecture allows us to perform multiple tasks in an efﬁcient and compact manner, thanks to the proposed method. An overview of our network is presented in Fig. 1.
The proposed method for CompositeTasking learns by task-speciﬁc batch normalization. Each task is performed by predicting layer-wise (only on the decoder side) afﬁne batch normalization (BN) parameters, using a small task-conditioned network. Such design choice dedicates the encoder towards a compact visual representation shared among tasks. On the output side, each task is represented in an image format – thereby performing the conditional image-to-image translation. The image format chooses some arbitrary embedding for every task. We aim to map images to such embedding, conditioned upon the spatially distributed task requests. The task speciﬁc losses are then computed by mapping the predicted embedding to the task-appropriate label representations. For example, the pre-dicted 3-channel values are mapped to class probabilities for segmentation task to compute cross-entropy, whereas, pixel normals are directly regressed by minimizing the angular distance between the prediction and the normal’s label. This design choice enforces tasks to share network parameters even on the decoder side. Surprisingly, such simple design already offers us very competitive results.
During inference, only a small part of the embedding network performs computation. This allows our Compos-iteTasking network to use an efﬁcient single encoder - single decoder architecture for all tasks. Furthermore, our training strategy enables users to request any task at any pixel. In fact, we also propose to learn the Task Palette, in case it is missing. The inferred palette follows some hand-crafted rules for task requests. It is then fed back to our network to execute the spatially distributed, rule-based tasks.
Learning pixel-speciﬁc tasking has several beneﬁts, which may be obvious when a parallel to the image segmen-tation is drawn. In this work, we demonstrate the beneﬁts in regard to a couple of chosen applications, namely, learn-ing the Task Palette, task editing, and rule transfer. In the following, we summarize key contributions of our work.
• We introduce the new problem of CompositeTasking which we demonstrate to be useful for images.
• A novel method for CompositeTasking is also pro-posed. It is signiﬁcantly superior in terms of compu-tational efﬁciency, and competitive in terms of perfor-mance for image understanding tasks.
• Applications of the proposed CompositeTasking net-work, namely on predicting with an estimated Task
Palette, task editing, and rule transfer, are also demon-strated in this paper. 2.