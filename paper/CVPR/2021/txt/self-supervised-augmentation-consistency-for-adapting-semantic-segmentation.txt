Abstract
We propose an approach to domain adaptation for se-mantic segmentation that is both practical and highly ac-curate. In contrast to previous work, we abandon the use of computationally involved adversarial objectives, network ensembles and style transfer. Instead, we employ standard data augmentation techniques – photometric noise, ﬂipping and scaling – and ensure consistency of the semantic pre-dictions across these image transformations. We develop this principle in a lightweight self-supervised framework trained on co-evolving pseudo labels without the need for cumbersome extra training rounds. Simple in training from a practitioner’s standpoint, our approach is remarkably ef-fective. We achieve signiﬁcant improvements of the state-of-the-art segmentation accuracy after adaptation, consistent both across different choices of the backbone architecture and adaptation scenarios. 1.

Introduction
Unsupervised domain adaptation (UDA) is a variant of semi-supervised learning [6], where the available unla-belled data comes from a different distribution than the an-notated dataset [4]. A case in point is to exploit synthetic data, where annotation is more accessible compared to the costly labelling of real-world images [59, 60]. Along with some success in addressing UDA for semantic segmenta-tion [67, 69, 80, 91], the developed methods are growing increasingly sophisticated and often combine style trans-fer networks, adversarial training or network ensembles
[39, 46, 68, 77]. This increase in model complexity impedes reproducibility, potentially slowing further progress.
In this work, we propose a UDA framework reaching state-of-the-art segmentation accuracy (measured by the
Intersection-over-Union, IoU) without incurring substantial training efforts. Toward this goal, we adopt a simple semi-supervised approach, self-training [12, 42, 91], used in re-cent works only in conjunction with adversarial training or
Code is available at https://github.com/visinf/da-sac. mIoU
Mean IoU on Cityscapes (val) after adaptation from GTA5 with VGG-16 49.9 50 48 46 44 42 40 38 36
Self-training
# number of rounds
Adversarial training
Style transfer
Ensemble 43.6 43.8 1
LDR
[77]
FADA
[70] 2019 39.0 37.2 4 42.2 1
PIT
[53] 42.3 3
FDA
[80] 1
LSE [65]
PyCDA [47] 42.4 2
TIR
[39] 2020 1
Ours 46.5 6
SA-I2I
[55] 44.9
CD-AM
[78] 2021
Figure 1. Results preview. Unlike much recent work that com-bines multiple training paradigms, such as adversarial training and style transfer, our approach retains the modest single-round train-ing complexity of self-training, yet improves the state of the art for adapting semantic segmentation by a signiﬁcant margin. network ensembles [17, 39, 54, 70, 80, 86, 87]. By contrast, we use self-training standalone. Compared to previous self-training methods [9, 43, 65, 91, 92], our approach also sidesteps the inconvenience of multiple training rounds, as they often require expert intervention between consecutive rounds. We train our model using co-evolving pseudo labels end-to-end without such need.
Our method leverages the ubiquitous data augmentation techniques from fully supervised learning [11, 85]: photo-metric jitter, ﬂipping and multi-scale cropping. We enforce consistency of the semantic maps produced by the model across these image perturbations. The following assump-tion formalises the key premise:
Assumption 1. Let f : I → M represent a pixelwise mapping from images I to semantic output M. Denote
ρǫ : I → I a photometric image transform and, sim-ilarly, τǫ′
: I → I a spatial similarity transformation, where ǫ, ǫ′ ∼ p(·) are control variables following some pre-deﬁned density (e.g., p ≡ N (0, 1)). Then, for any image
I ∈ I, f is invariant under ρǫ and equivariant under τǫ′ , i.e. f (ρǫ(I)) = f (I) and f (τǫ′ (I)) = τǫ′ (f (I)).
Next, we introduce a training framework using a momentum network – a slowly advancing copy of the original model. 15384
The momentum network provides stable, yet recent targets for model updates, as opposed to the ﬁxed supervision in model distillation [15, 86, 87]. We also re-visit the problem of long-tail recognition in the context of generating pseudo
In particular, we maintain an labels for self-supervision. exponentially moving class prior used to discount the con-ﬁdence thresholds for those classes with few samples and increase their relative contribution to the training loss. Our framework is simple to train, adds moderate computational overhead compared to a fully supervised setup, yet sets a new state of the art on established benchmarks (cf. Fig. 1).
Features
Adversarial training 1-round training
SOTA-VGG
SOTA-ResNet
PIT
[53]
LDR
[77]
SA-I2I
[55]
IAST
[54]
RPT
[83]
Ours
✓
✓
✓
✓
✓ (6)
✓
✓ (3)
✓
✓ (3)
✓
✓
✓
✓
Table 1. Relation to state of the art. Previous work reaches the state of the art in terms of IoU either with VGG-16 (SOTA-VGG) or ResNet-101 (SOTA-ResNet). Our framework uses neither ad-versarial training nor multiple training rounds (given in parenthe-ses), yet outperforms the state of the art consistently in both cases. 2.