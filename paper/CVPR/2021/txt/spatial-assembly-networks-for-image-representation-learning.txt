Abstract
It has been long recognized that deep neural networks are sensitive to changes in spatial conﬁgurations or scene structures. Image augmentations, such as random transla-tion, cropping, and resizing, can be used to improve the ro-bustness of deep neural networks under spatial transforms.
However, changes in object part conﬁgurations, spatial lay-out of object, and scene structures of the images may still result in major changes in the their feature representations generated by the network, creating signiﬁcant challenges for various visual learning tasks, including representation or metric learning, image classiﬁcation and retrieval.
In this work, we introduce a new learnable module, called spa-tial assembly network (SAN), to address this important is-sue. This SAN module examines the input image and per-forms a learned re-organization and assembly of feature points from different spatial locations conditioned by fea-ture maps from previous network layers so as to maximize the discriminative power of the ﬁnal feature representation.
This differentiable module can be ﬂexibly incorporated into existing network architectures, improving their capabilities in handling spatial variations and structural changes of the image scene. We demonstrate that the proposed SAN mod-ule is able to signiﬁcantly improve the performance of var-ious metric / representation learning, image retrieval and classiﬁcation tasks, in both supervised and unsupervised learning scenarios. 1.

Introduction
A key challenge in computer vision and machine learn-ing is to construct or learn discriminative representations for the semantic content of images, which should be in-variant to changes in camera positions, perspective trans-forms, object scales, poses, part deformations, spatial dis-placement, and scene conﬁgurations [8, 25]. Recently, deep
*Corresponding author: Zhihai He, e-mail: hezhi@missouri.edu.
Figure 1. Illustration of invariant image representation learning un-der generic spatial variations. neural networks have emerged as a powerful approach for visual learning and representation. With its shared weights for convolution across different spatial locations, average or maximum pooling, coupled with sufﬁcient training im-age augmentations, they are able to generate relatively in-variant features or decisions under small spatial variations or transforms. However, researchers have recognized that deep neural networks are still vulnerable to relatively large geometric transformations and spatial variations [23]. This limitation originates from the ﬁxed geometric structures of deep neural modules. For example, the convolution pro-cesses the input image or feature images on a ﬁxed grid structure with a small reception ﬁeld. The pooling layers then process the outputs from the convolution layers with a
ﬁxed spatial mapping or channel structures. There is lack of internal mechanisms to handle the ﬂexible spatial vari-ations, including spatial transforms and changes in object poses, spatial layout, and scene structures [22]. In their re-cent study [17], Kayhan and Gemert even found out that deep neural networks are exploiting the absolution spatial locations and image boundary conditions for object recog-nition and image classiﬁcation, challenging the common as-sumption that convolution layers in modern CNNs are trans-lation invariant.
Learning invariant features and visual representation with deep neural networks has become an important yet 13876
challenging research problem. Recent research has been fo-cusing on developing various methods on transform-aware data augmentations [4, 11], geometry adversarial training
[16], and transform-invariant network modules and struc-tures [14, 33, 35] to improve the robustness of deep neural networks under spatial transforms of images and objects, such as afﬁne or perspective transforms. In this work, we aim to address the challenging problem of invariant fea-ture representation learning under more generic spatial vari-ations, include changes in object poses, part conﬁgurations, and scene structures. For example, Figure 1 shows three example images with different spatial conﬁgurations of ob-jects due to object motion. Semantically, they should be the same or belong to the same class. However, existing deep neural networks will generate different features for them. Our goal is to design a new spatial assembly net-work (SAN), which is able to examine the input image and perform a learned re-organization or optimized assembly of feature points from different spatial locations so as to gen-erate invariant features for these three images. This learned spatial assembly is conditioned by feature maps of previous network layers. This differentiable module can be ﬂexibly incorporated into existing network architectures, improving their capabilities in handling spatial variations and struc-tural changes of the image scene, and maximizing the dis-criminative power of the ﬁnal feature representation. We will demonstrate that the proposed SAN module is able to improve the performance of various metric / representation learning, image retrieval and classiﬁcation tasks, in both su-pervised and unsupervised learning settings. 2.