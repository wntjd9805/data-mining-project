Abstract
Non-visual imaging sensors are widely used in the indus-try for different purposes. Those sensors are more expen-sive than visual (RGB) sensors, and usually produce images with lower resolution. To this end, Cross-Modality Super-Resolution methods were introduced, where an RGB image of a high-resolution assists in increasing the resolution of a low-resolution modality. However, fusing images from different modalities is not a trivial task, since each multi-modal pair varies greatly in its internal correlations. For this reason, traditional state-of-the-arts which are trained on external datasets often struggle with yielding an artifact-free result that is still loyal to the target modality character-istics.
We present CMSR, a single-pair approach for Cross-Modality Super-Resolution.
The network is internally trained on the two input images only, in a self-supervised manner, learns their internal statistics and correlations, and applies them to up-sample the target modality. CMSR con-tains an internal transformer which is trained on-the-ﬂy to-gether with the up-sampling process itself and without su-pervision, to allow dealing with pairs that are only weakly aligned. We show that CMSR produces state-of-the-art su-per resolved images, yet without introducing artifacts or ir-relevant details that originate from the RGB image only. 1.

Introduction
Super-Resolution (SR) methods are used to increase the spatial resolution and improve the level-of-detail of digital images, while preserving the image content. Such methods have important applications for multiple industries, such as health-care, agriculture, defense and ﬁlm. ([26]) In recent years, more advanced methods of SR have been heavily based on Deep Learning. ([12, 21, 4])
The need for super-resolution becomes even more prominent when dealing with sensors that capture other modalities, different than the visible light spectrum, since those sensors typically produce images with substantially ([18, 25]) For example, Infra-Red (IR) lower resolution. camera sensors are more expensive than classical camera sensors, and their output images commonly have much lower spatial resolution. To bridge that gap in level-of-detail, Joint Cross-Modality methods were developed. The idea is to use the higher-resolution RGB modality to guide the process of super-resolution on images taken by the lower resolution sensor, taking advantage of the ﬁner details found in the RGB images. The challenge is to remain loyal to the target modality characteristics and to avoid adding redun-dant artifacts or textures that may be present only in the
RGB modality.
In this work, learning is performed internally, relying solely on the input pair of images. This approach does not require any training data, and therefore avoids the need for a modal-speciﬁc dataset, relying solely on the internal ([6]) Using an internal image-speciﬁc statistics instead. super-resolution method is particularly strong in the con-text of cross-modality, since it allows the network to ﬁt to the unique properties and the modality characteristics of the speciﬁc input pair. This feature stands in contrast to the somewhat impractical task of generalizing to a large cross-modal image dataset; each multi-modal pair is inherently unique in its internal correlations, and therefore must be treated differently.
State-of-the-art Joint Cross-Modality SR methods rely on the assumption that their multiple inputs are well aligned. ([1, 2, 36, 8, 27, 24, 22]) Thus, they perform well only when the input images were captured by different sen-sors placed in the exact same position, and taken at the ex-act same time. As to be shown, in real-life scenarios perfect alignment of multiple sensors is often hard to achieve. In our work, we present new means to allow the two modal-ities to be moderately misaligned, namely weakly aligned.
Our network contains a learnable deformation component that implicitly aligns details in the two images together.
More speciﬁcally, our architecture includes a deformation model that aligns details from the RGB image to the target modality in a coarse-to-ﬁne manner, before they are fused together. The network does not use any explicit supervision for the deformation sub-task, but rather optimizes the de-formation parameters to adhere to the super-resolution goal.
Figures 1 and 8 present cases where a weakly aligned pair 6378
causes state-of-the-arts methods to fail, whereas our method produces high-quality super-resolved output.
Another notable advantage of our single-pair approach is the avoidance of over-transferal of information. Previous approaches which train on external cross-modal datasets are often limited in their ability to adjust to the speciﬁc nature of the input pair. For this reason, they often struggle in cases where the guiding modality should be only minimally used, or even completely ignored; they tend to fuse redun-dant details from the guiding modality anyway, resulting in the addition of textures and artifacts to the lower resolution modality. Our method is designed to adjust to the speciﬁc input pair and therefore transfers details from the higher res-olution image carefully and conservatively, learning only the details which aid improving the super-resolution task.
Figure 5 presents an example with cross-modality ambigu-ity. Namely, the RGB modality contains an object which does not exist in the target modality; this object should ide-ally be ignored. Our network successfully avoids transfer-ring it, whereas a competing cross-modality method results in unwanted artifacts and textures. We show that our net-work achieves state-of-the-art results on different modali-ties (Thermal, NIR, Depth), while being generic in support-ing any modality as input and requiring no pre-training (and thus, no training data). 2.