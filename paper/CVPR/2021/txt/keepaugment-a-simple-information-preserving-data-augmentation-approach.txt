Abstract
Data augmentation (DA) is an essential technique for training state-of-the-art deep learning systems. In this pa-per, we empirically show that the standard data augmen-tation methods may introduce distribution shift and conse-quently hurt the performance on unaugmented data during inference. To alleviate this issue, we propose a simple yet effective approach, dubbed KeepAugment, to increase the
ﬁdelity of augmented images. The idea is to use the saliency map to detect important regions on the original images and preserve these informative regions during augmentation.
This information-preserving strategy allows us to generate more faithful training examples. Empirically, we demon-strate that our method signiﬁcantly improves upon a num-ber of prior art data augmentation schemes, e.g. AutoAug-ment, Cutout, random erasing, achieving promising results on image classiﬁcation, semi-supervised image classiﬁca-tion, multi-view multi-camera tracking and object detection. 1.

Introduction
Recently, data augmentation is proven to be a crucial technique for solving various challenging deep learning tasks, including image classiﬁcation [e.g. 8, 39, 4, 5], natu-ral language understanding [e.g. 7], speech recognition [25] and semi-supervised learning [e.g. 36, 29, 1]. Notable ex-amples include regional-level augmentation methods, such
[8] and CutMix [39], which mask or modify as Cutout randomly selected rectangular regions of the images; and image-level augmentation approaches, such as AutoAug-ment [4] and Fast Augmentation [18]), which leverage re-inforcement learning to ﬁnd optimal policies for selecting and combining different label-invariant transforms (e.g., ro-tation, color-inverting, ﬂipping).
Although data augmentation increases the effective data size and promotes diversity in training examples, it in-evitably introduces noise and ambiguity into the training process. Hence the overall performance would deteriorate if the augmentation is not properly modulated. For example, as shown in Figure 1, random Cutout (Figure 1 (a2) and (b2)) or RandAugment (Figure 1 (a3) and (b3)) may de-stroy the key characteristic information of original images that is responsible for classiﬁcation, creating augmented im-ages to have wrong or ambiguous labels.
In this work, we propose KeepAugment, a simple yet powerful adaptive data augmentation approach that aims to increase the ﬁdelity of data augmentation by always keep-ing important regions untouched during augmentation. The idea is very simple: at each training step, we ﬁrst score the importance of different regions of the original images using attribution methods such as saliency-map [28]; then we perform data augmentation in an adaptive way, such that regions with high importance scores always remain in-tact. This is achieved by either avoiding cutting critical high-score areas (see Figure 1(a5) and (b5)), or pasting the patches with high importance scores to the augmented im-ages (see Figure 1(a6) and (b6)).
Although KeepAugment is very simple and computa-tionally efﬁcient, the empirical results on a variety of vi-sion tasks show that it can signiﬁcantly improve the prior art data augmentation (DA) baselines. Speciﬁcally, for im-age classiﬁcation, we achieve improvements on existing DA techniques, including Cutout [8], AutoAugment [4], and CutMix [39], boosting the performance on CIFAR-10 and ImageNet across various neural architectures. In par-ticular, we achieve 98.7% test accuracy on CIFAR-10 us-ing PyramidNet-ShakeDrop [38] by applying our method on top of AutoAugment. When applied to multi-view multi-camera tracking, we improve upon the recent state-of-the-art results on the Market1501 [44] dataset. In addition, we demonstrate that our method can be applied to semi-supervised learning and the model trained on ImageNet us-ing our method can be transferred to COCO 2017 objec-tive detection tasks [21] and allows us to improve the strong
Detectron2 baselines [35]. 1055
(a1) Red fox (a2) Cutout (a3) RandAugment (b1) Dog (b2) Cutout (b3) RandAugment (a4) Saliency map (a5) Keep+Cutout (a6) Keep+RandAugment (b4) Saliency map (b5) Keep+Cutout (b6) Keep+RandAugment
Figure 1. KeepAugment improves existing data augmentation by always keeping the important regions (measured using saliency map) of the image untouched during augmentation. This is achieved by either avoiding to cut important regions (see KeepCutout), or pasting important regions on top of the transformed images (see KeepRandAugment). Images are from ImageNet [6]. 2. Data Augmentation
In this work, we focus on label-invariant data augmen-tation due to their popularity and signiﬁcance in boosting empirical performance in practice. Let x be an input im-age, data augmentation techniques allow us to generate new images x′ = A(x) that are expected to have the same la-bel as x, where A denotes a label-invariant image trans-form, which is typically a stochastic function. Two classes of augmentation techniques are widely used for achieving state-of-the-art results on computer vision tasks:
Region-Level Augmentation Region-level augmentation schemes, including Cutout [8] and random erasing [45], work by randomly masking out or modifying rectangu-lar regions of the input images, thus creating partially occluded data examples outside the span of the training data. This procedure could be conveniently formulated as applying randomly generated binary masks to the origi-nal inputs. Precisely, consider an input image x of size
H × W , and a rectangular region S of the image domain.
Let M (S) = [Mij(S)]ij be the binary mask of S with
Mij(S) = I((i, j) ∈ S). Then the augmented data can be generated by modifying the image on region S, yielding images of form x′ = (1 − M (S)) ⊙ x + M (S) ⊙ δ, where
⊙ is element-wise multiplication, and δ can be either zeros (for Cutout) or random numbers (for random erasing). See
Figure 1(a2) and (b2) for examples.
Image-Level Augmentation Exploiting the invariance properties of natural image-level augmentation methods apply label-invariant transformations on the whole image, such as solarization, sharpness, posterization, and color normalization. Traditionally, image-level transforma-images, tions are often manually designed and heuristically chosen.
Recently, AutoAugment [4] applies reinforcement learning to automatically search optimal compositions of transfor-mations. Several subsequent works, including RandAug-ment [5], Fast AutoAugment [18], alleviate the heavy com-putational burden of searching on the space of transforma-tion policies by designing more compact search spaces. See
Figure 1(b3) and Figure 1(a3) for examples of transforms used by RandAugment.
Data Augmentation and its Trade-offs Although data augmentation increases the effective size of data, it may in-evitably cause loss of information and introduce noise and ambiguity if the augmentation is not controlled properly
[e.g. 34, 12]. To study this phenomenon empirically, we plot the train and testing accuracy on CIFAR-10 [16] when we apply Cutout with increasingly large cutout length in
Figure 2(a), and RandAugment with increasing distortion magnitude (see [5] for the deﬁnition) in Figure 2(b). As typically expected, the generalization (the gap between the training and testing accuracy on clean data) improves as the magnitude of the transform increases in both cases. How-ever, when the magnitudes of the transform are too large (≥ 16 for Cutout and ≥ 12 for RandAugment ), the training accuracy (blue line), and hence the testing accuracy (red line), starts to degenerate, indicating that augmented data no longer faithfully represent the clean training data in this case, such that the training loss on augmented data no longer forms a good surrogate of the training loss on the clean data. 3. Our Method
We introduce our method for controlling the ﬁdelity of data augmentation and hence decreasing harmful misinfor-1056
y c a r u c c
A 0 1
-r a f i
C y c a r u c c
A 0 1
-r a f i
C (a) Cutout length in CutOut (b) Distortion magnitude in RandAugment
Figure 2. The training and testing accuracy of Wide ResNet-28-10 trained on CIFAR-10 with Cutout and RandAugment, when we vary the cutout length of Cutout (a), and the distortion magnitude of RandAugment (b). We follow the same implementation details as in [8] and [5]. For RandAugment, we ﬁx the number of transformations to be 3 as suggested in [5]. mation. Our idea is to measure the importance of the rectan-gular regions in the image by saliency map, and ensure that the regions with the highest scores are always presented af-ter the data augmentation: for Cutout , we achieve this by avoiding to cut the important regions (see Figure 1(a5) and (b5)); for image-level transforms such as RandAugment, we achieve this by pasting the important regions on the top of the transformed images (see Figure 1 (a6) and (b6)).
Speciﬁcally, let gij(x, y) be saliency map of an image x on pixel (i, j) with the given label y. For a region S on the image, its importance score is deﬁned by
Algorithm 1 KeepAugment: An information-preserving data augmentation approach
Input: given a network, an input image and label pair (x, y), threshold τ (a) if use Selective-Cut repeat randomly select a mask region S until region score I(S, x, y) < τ
˜x = (1 − M (S)) ⊙ x (see Eq. 2) (b) if use Selective-paste x′ = A(x) repeat randomly select a mask region S until region
//apply data augmentation
I(S, x, y) = gij(x, y). (1) score I(S, x, y) > τ
˜x = M (S) ⊙ x + (1 − M (S)) ⊙ x′ (see Eq. 3)
X(ij)∈S
Return ˜x
In our work, we use the standard saliency map based on vanilla gradient [28]. Speciﬁcally, given an image x and its corresponding label logit value ℓy(x), we take gij(x, y) to be the absolute value of vanilla gradients |∇xℓy(x)|. For
RBG-images, we take channel-wise maximum to get a sin-gle saliency value for each pixel (i, j).
Selective-Cut For region-level (e.g. cutout-based) aug-mentation that masks or modiﬁes randomly selected rectan-gle regions, we control the ﬁdelity of data augmentation by ensuring that the regions being cut can not have large impor-tance scores. This is achieved in practice by Algorithm 1(a), in which we randomly sample regions S to be cut until its importance score I(S, x, y) is smaller than a given thresh-old τ . The corresponding augmented example is deﬁned as follows,
˜x = (1 − M (S)) ⊙ x, (2) where M (S) = [Mij(S)]ij is the binary mask for S, with
Mij = I((i, j) ∈ S).
Selective-Paste Because image-level transforms modify the whole images jointly, we ensure the ﬁdelity of the trans-form by pasting a random region with high importance score (see Figure 1(a6) and (b6) for an example). Algo-rithm 1(b) shows how we achieve this in practice, in which we draw an image-level augmented data x′ = A(x), uni-formly sample a region S that satisﬁes I(S, x, y) > τ for a threshold τ , and and paste the region S of the original image x to x′, which yields
˜x = M (S) ⊙ x + (1 − M (S)) ⊙ x′. (3)
Similarly, Mij(S) = I((i, j) ∈ S) is the binary mask of region S.
Remark In practice, we choose our threshold τ in an adaptive way. Technically, given an image and consider an region size h × w of interest, we ﬁrst calculate the im-portance scores of all possible candidate regions, following
Eq. 1; then we set our threshold to be the τ -quantile value of all the importance scores I(S, x, y) of all candidate regions.
For selective-cut, we uniformly keep sampling a mask re-gion S until its corresponding score I(S, x, y) is smaller than the threshold. For selective-paste, we uniformly sam-ple a region S with importance score is greater than the threshold.
We empirically study the effect of our threshold τ on
CIFAR-10, illustrated in Figure 3. Intuitively, for selective-1057
y c a r u c c
A 0 1
-r a f i
C y c a r u c c
A 0 1
-r a f i
C (a) threshold τ in CutOut (b) threshold τ in RandAugment
Figure 3. Analysis of the effect of threshold τ of our algorithm for Cutout (a) and RandAugment (b). In (a), we ﬁx the cutout length 20. In (b), We ﬁx the number of transformation to be 3 and distortion magnitude to be 15 and the paste back region size to be 8 × 8. We plot how the accuracy changes with respect to different choices of τ . We use Wide ResNet-28-10 and train on CIFAR-10. The dash line (baseline) in (a) represents test accuracy achieved by CutOut without selective-cut; the dash line baseline in (b) is the test accuracy achieved by
RandAugment without selective-paste. cut, it’s more likely to cut out important regions as we use an increasingly larger threshold τ ; on the contrary, a larger τ corresponds to copy back more critical regions for selective-paste. As we can see from Figure 3, for Cutout (Figure 3 (a)), we improve on the standard Cutout baseline (dash line) signiﬁcantly when the threshold τ is relative small (e.g. τ ≤ 0.6) since we would always avoid cutting impor-tant regions. As expected, the performance drops sharply when important regions are removed with a relative large threshold τ (e.g. τ = 0.8); for RandAugment (Figure 3 (b)), using a lower threshold (e.g., τ = 0.2) tends to yield similar performance as the standard RandAugment base-line (dash line). Increasing the threshold τ ( τ = 0.6 or 0.8) yields better results. We notice that further increasing τ (τ = 0.8) may hurt the performance slightly, likely be-cause a large threshold yields too restrictive selection and may miss other informative regions. we further evaluated
τ = 0, such that the saliency map information would be ignored. With τ = 0, we achieved 97.3% accuracy, which is worse compared to the result of our default setting (i.e., 97.8% accuracy with τ = 0.6). 3.1. Efﬁcient Implementation of KeepAugment
Note that our KeepAugment requires to calculate the saliency maps via back-propagation at each training step.
Naive implementation leads to roughy twice of the compu-tational cost. In this part, we propose two computational efﬁcient strategies for calculating saliency maps that over-come this weakness.
Low resolution based approximation we proceed as fol-lows: a) for a given image x, we ﬁrst generate a low-resolution copy and then calculate its saliency map; b) we map the low-resolution saliency maps to their correspond-ing original resolution. This allows us to speed up the saliency maps calculation signiﬁcantly, e.g., on ImageNet, we achieve roughly 3× computation cost reduction by re-ducing the resolution from 224 to 112.
Early classiﬁcation head based approximation Our sec-Low-Resolution
Loss
Original
Calculate Saliency Map
Augment (a) Low resolution based approximation
Loss
Original
Calculate Saliency Map
Augment (b) Early classiﬁcation head based approximation
Figure 4. We demonstrate two different approaches for using
KeepAugment with less training time. Using Cutout as an ex-ample, Figure (a) shows that we can use a low resolution copy to calculate the saliency map, and then generate the augmented im-age. Figure (b) shows that when calculating the saliency map, we can use an additional loss at early layer of a given neural network. ond idea is to introduce an early loss head in the network, then we approximate saliency maps with this loss. In prac-tice, we add an additional average pooling layer and a lin-ear head after the ﬁrst block of our networks evaluated. Our training objective is the same as the Inception Network [31].
The neural network is trained with the standard loss together with the auxiliary loss. We achieve about 3× computation cost reduction in computing saliency maps.
Furthermore, in section 4, we show that both approxi-mation strategies do not lead to any performance drop. In the following, we denote our low resolution based approxi-mation as low resolution and early classiﬁcation head based approximation as early loss for presentation clarity. 4. Experiments
In this section, we show our adaptive augmentation strategy KeepAugment signiﬁcantly improves on exist-ing state-of-the-art data augmentation baselines on a va-1058
Model
Cutout
KeepCutout
KeepCutout (low resolution)
KeepCutout (early loss)
Model
AutoAugment
KeepAutoAugment
KeepAutoAugment (low resolution)
KeepAutoAugment (early loss)
ResNet-18 95.6±0.1 96.1±0.1 96.2±0.1 96.0±0.1
ResNet-110 94.8±0.1 95.5±0.1 95.5±0.1 95.3±0.1
Wide ResNet-28-10 96.9±0.1 97.3±0.1 97.3±0.1 97.2±0.1
Wide ResNet-28-10 Shake-Shake PyramidNet+ShakeDrop 97.3±0.1 97.8±0.1 97.8±0.1 97.8±0.1 97.4±0.1 97.8±0.1 97.9±0.1 97.7±0.1 98.5 98.7±0.0 98.7±0.0 98.6±0.0
Table 1. Test accuracy (%) on CIFAR-10 using various models architectures. riety of challenging deep learning tasks, including image classiﬁcation, semi-supervised image classiﬁcation, multi-view multi-camera tracking, and object detection. For semi-supervised image classiﬁcation and multi-view multi-camera tracking, we use low resolution images to calculate saliency maps as discussed above.
Settings We apply our method to improve prior art region-level augmentation methods, including [8], CutMix
[39], Random Erasing [45] and image-level augmentation approach, such as AutoAugment [4]. To sample the re-gion of interest, for each image, we rank the absolute saliency values measured on all candidate regions and take our threshold to be the τ -th percentile value. We set τ to 0.6 for all our experiments. Additionally, we set the cutout paste-back length to be 16 on CIFAR-10 and 40 on Ima-geNet, which is the default setting used by Cutout [8].
For our low resolution based efﬁcient training strategy, we reduce the image width and height by half with bicubic in-terpolation. For the early loss based approach, we use an additional head (linear transform and loss) with a coefﬁcient of 0.3 after the ﬁrst block of each network. 4.1. CIFAR 10 Classiﬁcation
We apply of our adaptive selective strategy to improve two state-of-the-art augmentation schemes, Cutout and
RandAugment , on the CIFAR-10 1 [15] dataset. We experiment with various of backbone architectures, such as ResNets [13], Wide ResNets [40], PyramidNet Shake-Drop [38] and Shake-Shake [9]. We closely follow the training settings suggested in [8] and [5]. Speciﬁcally, we train 1,800 epochs with cosine learning rate deacy [22] for
PyramidNet-ShakeDrop and 300 epochs for all other net-works, We report the test test accuracy in Table 1. All results are averaged over three random trials, except for
PyramidNet-ShakeDrop [38], on which only one random trial is reported. 1https://www.cs.toronto.edu/˜kriz/cifar.html
From Table 1, we observe a consistent improvement on test accuracy by applying our information-preserving aug-mentation strategy.
Improve on CutOut We study the relative improvements on Cutout across various cutout lengths. We use ResNet-18 and train on CIFAR-10. We experiment with a variety of cutout length from 8 to 24. As shown in Table 2, we observe that our KeepCutout achieves increasingly signiﬁ-cant improvements over Cutout when the cutout regions become larger. This is likely because that with large cutout length, Cutout is more likely to remove the most infor-mative region and hence introducing misinformation, which in turn hurts the network performance. On the other hand, with a small cutout length, e.g. 8, those informative regions are likely to be preserved during augmentation; standard
Cutout strategy achieves better performance by taking advantage of more diversiﬁed training examples.
Cutout length 8 12 16 20 24
Cutout 95.3±0.0 95.4±0.0 95.6±0.0 95.5±0.1 94.9±0.1
KeepCutout 95.1±0.0 95.5±0.0 96.1±0.0 96.0±0.1 95.6±0.1
Table 2. Test accuracy (%) of ResNet-18 on CIFAR-10. All re-sults are averaged over 5 random trials.
Improve on AutoAugment
In this case, we use the Au-toAugment policy space, apply our selective-paste and study the empirical gain over AutoAugment for four dis-tortion augmentation magnitude (6, 12, 18 and 24). We train Wide ResNet-28-10 on CIFAR-10 and closely follow the training setting suggested in [4]. As we can see from
Table 3, our method yields better performance in all set-tings consistently, and our improvements is more signiﬁcant when the transformation distortion magnitude is large. 1059
Magnitude AutoAugment KeepAutoAugment 6 12 18 24 96.9±0.1 97.1±0.1 97.1±0.1 97.3±0.1 97.3±0.1 97.5±0.1 97.6±0.1 97.8±0.1
Table 3. Test accuracy (%) of wide ResNet-28-10 on CIFAR-10 across varying distortion augmentation magnitudes. All results are averaged over 5 random trials.
Wide ResNet-28-10
GridMask
AugMix
Attentive CutMix
KeepAutoAugment+L
ShakeShake
GridMask
AugMix
Attentive CutMix
KeepAutoAugment+L
Accuracy (%) Time (s) 97.5±0.1 97.5±0.0 97.3±0.1 97.8±0.1 92 92 127 111
Accuracy (%) Time (s) 97.4±0.1 97.5±0.0 97.4±0.1 97.9±0.1 124 124 166 142
Table 4. Results on CIFAR-10 using various models architectures and various baselines. ‘Time’ reports the per epoch training time on one TITAN X GPU. ‘Accuracy’ reports the accuracy on test set, which is averaged over 5 trials. ‘L’ denotes low resolution.
We use Wide ResNet-28-10, and the corresponding AutoAugment baseline result is presented above.
Additional Comparisons on CIFAR-10 Recently, some researchers [3, 33, 14] also mix the clean image and aug-mented image together to achieve higher performance.
Girdmask [3], AugMix [14] and Attentive CutMix are popular methods among these approaches. Here, we con-duct experiments on CIFAR-10 to show the accuracy and training cost of each method. Note that we implement all the baselines by ourselves, and the results of our imple-mentation are comparable or even better than the results re-ported in the original papers.
Table 4 shows that our proposed algorithm can achieve clear improvements on accuracy over all other baselines.
Moreover, Gridmask only implements upon Cutout and
Attentive CutMix only implements upon CutMix by pasting the most important region. But our approach is more
ﬂexible and can be easily applied to improve a large variety of data augmentation schemes. 4.2. ImageNet Classiﬁcation
We conduct experiments on large-scale challenging Im-ageNet dataset, on which our adaptive augmentation algo-rithm again shows clear advantage over existing methods.
Dataset and Settings We use ILSVRC2012, a subset of ImageNet classiﬁcation dataset [6], which contains
Method
ResNet-50
ResNet-101
Top-1 Top-5 Top-1 Top-5 93.6 76.3 93.9 76.8
-77.1
-77.5 94.4 77.9 94.3 78.3 94.4 77.6 94.7 77.3 94.4 77.6 94.6 78.0 94.6 78.1 94.5 77.9 94.6 78.6 95.1 79.0 95.2 79.1 79.0 95.1
Table 5. Validation accuracy (%) on ImageNet using ResNet-50 and ResNet-101.
Vanilla [13]
Dropout [30]
DropPath [17]
Manifold Mixup [32]
Mixup [41]
DropBlock [10]
RandAugment [5]
Random Erasing [45]
AutoAugment [4]
KeepAutoAugment
+ low resolution
+ early loss
CutMix [39]
KeepCutMix
+ low resolution
+ early loss 92.9 93.4 93.5 93.8 93.9 94.1 93.8 93.3 93.8 93.9 93.9 93.8 94.0 94.4 94.4 94.3 77.4 77.7
--79.2 79.0 79.2 79.6 79.3 79.7 79.7 79.6 79.9 80.3 80.3 80.2 around 1.28 million training images and 50,000 valida-tion images from 1,000 classes. We apply our adaptive data augmentation strategy to improve CutMix [39] and
AutoAugment [4], respectively.
CutMix randomly mixes images and labels. To aug-ment an image x with label y, CutMix removes a ran-domly selected region from x and replace it with a patch of the same size copied from another random image x′ with la-bel y′. Meanwhile, the new label is mixed as λy +(1−λ)y′, where λ equals the uncorrupted percentage of image x. We improve on CutMix by using selective-cut. In practice, we found it is often quite effective to simply avoiding cut-ting informative region from x. We denote our adaptive
CutMix method as KeepCutMix. We further improve on
AutoAugment by pasting-backing randomly selected re-gions with important score greater than τ = 0.6.
For a fair comparison, we closely follow the training set-tings in CutMix [39] and AutoAugment [5]. We test our method on both ResNet50 and ResNet101 [13]. Our models are trained for 300 epochs, and the experiment is implemented based on the open-source code 2.
Results We report the single-crop top-1 and top-5 accu-racy on the validation set in table 5. Compared to CutMix, we method KeepCutMix achieves 0.5% improvements on top1 accuracy using ResNet-50 and 0.4% higher top1 ac-curacy using ResNet101; compared to AutoAugment [4], our method improves top-1 accuracy from 77.6% to 78.1% and 79.3% to 79.7% using ResNet-50 and ResNet-101, re-spectively. Again, we also notice that our accelerated ap-2https://github.com/clovaai/CutMix-PyTorch 1060
Model
R-18
Cutout 19
KeepCutout 38 +100.0% 54 +92.8% 185 +101.1%
+ low resolution 24 +26.3% 35 +25.0% 111 +20.6%
+ early loss 23 +13.0% 34 +21.1% 104 +13.0%
Wide ResNet 92
R-110 28
Table 6. Per epoch training time (seconds) on CIFAR-10. Here R-18 and R-110 represents ResNet-18 and ResNet-110, respectively.
Model
CutMix
CutMix + low resolution
CutMix + early loss
AutoAugment
AutoAugment + low resolution 49.5 +20.4% 83.2 +21.9%
AutoAugment + early loss 48.9 +19.0% 82.4 +20.8%
ResNet-50 ResNet-101 41.5 49.8 +20.0% 83.5 +21.7% 49.1 +18.3% 82.7 +20.6 % 41.1 68.2 68.6
Table 7. Average training time (minutes) per epoch. proaches do not hurt the performance of the model. We also notice that, similar to the results on CIFAR-10, the proposed accelerating approach can speed up KeepAugment without loss of accuracy on ImageNet.
Training time cost We provide additional training cost comparisons on both CIFAR-10 and ImageNet in Table 6 and Table 7, respectively. On CIFAR-10, all models are trained on one TITAN X GPU with batch size 128; On
ImageNet, we train all models on 8 TITAN X GPUs with batch size 384. As we can see from Figure 6, our low reso-lution and early loss based approximation signiﬁcantly ac-celerates the computation of salience maps. Meanwhile, in general, our KeepAugment equipped with low resolution or early loss based saliency map approximation leads to ∼20% increase in per epoch training time compared to the cor-responding baselines. To factor in this training overhead, in Table 8, we increase the training budget of CutMix and
AutoAgument for additional 20%, from 300 epochs to 360 epochs. Comparing with our results in Table 5, our method still yields the best performance.
Method
CutMix
AutoAugment
Epochs 360 360
Table 8. Validation accuracy (%) on ImageNet using ResNet-50.
Top-1 78.72 77.63
Top-5 94.15 93.85 4.3. Semi Supervised Learning
Semi-supervised learning (SSL) is a key approach to-ward more data-efﬁcient machine learning by jointly lever-age both labeled and unlabeled data. Recently, data aug-mentation has been shown a powerful tool for developing state-of-the-art SSL methods. Here, we apply the proposed method to unsupervised data augmentation [37] (UDA) on
UDA + RandAug
UDA + KeepRandAug 4000 labels 95.1±0.2 95.4±0.2 2500 labels 91.2±1.0 92.4±0.8
Table 9. Result on CIFAR-10 semi-supervised learning. ‘4000 la-bels’ denotes that 4,000 images have labels.
CIFAR-10 to verify whether our approach can be applied to more general applications.
UDA minimizes the following loss on unlabelled data:
KL(pθ(· | x) k pθ(· | x′))
Ex∼Du, x′∼Px (cid:20)
, where P de-notes the randomized augmentation distribution, x′ denotes an augmented image and θ denotes the neural network pa-rameters. Notice that for semi-supervised learning, we do not have labels to calculate the saliency map. Instead, we use the max logit of pθ(· | x) to calculate the saliency map.
We simply replace the RandAug [5] in UDA with our pro-posed approach, and use the WideResNet-28-2. (cid:21)
In Table 9, we show that our approach improves on Ran-dAug and leads to improved semi-supervised learning per-formance on CIFAR-10. 4.4. Multi View Multi Camera Tracking
We apply our adaptive data augmentation to improve a state-of-the-art multi-view multi-camera tracking approach
[23]. Recent works [e.g. 23, 45, 44] have shown that data augmentation is an effective technique for improving the performance on this task.
[23] builds a strong baseline based on Random
Settings
Erasing [45] data augmentation. Random Erasing is similar to Cutout , except ﬁlling the region dropped with random values instead of zeros. We improve over [23] by only cutting out regions with importance score smaller than
τ = 0.6. We denote the widely-used open-source baseline open-ReID 3 as the standard baseline in table 10. To ab-late the role of our selective cutting-out strategy, we pursue minima changes made to the baseline code base. We follow all the training settings reported in [23], except using our adaptive data augmentation strategy. We use ResNet-101 as the backbone network.
We evaluate our method on a benchmark dataset, Mar-ket1501 [44]. Market1501 contains 32,668 bounding boxes of 1,501 identities, in which images of each identity are cap-tured by at most six cameras.
Results We report test accuracy and mean average preci-sion (mAP) of different methods in Table 10. Our method achieves the best performance. In particular, we achieve a 95.0% accuracy and 87.4 mAP on Market1501. 3https://github.com/Cysu/open-reid 1061
Method
Standard Baseline
+ Bag of Tricks [23]
+ Ours
Market1501
Accuracy 88.1±0.2 94.5±0.1 95.0±0.1 mAP 74.6±0.2 87.1±0.0 87.4±0.0
Table 10. We compare our method with the standard and [23] on two benchmark datasets. mAP represents mean average precision.
Backbone
Model
ResNet50-C4
Faster R-CNN
ResNet50-FPN
Faster R-CNN
ResNet50-FPN
RetinaNet
Faster R-CNN
ResNet101-C4
Faster R-CNN ResNet101-FPN
RetinaNet
ResNet101-FPN
Detectron2 Ours 39.5 40.7 39.1 42.2 42.9 41.2 38.4 40.2 37.9 41.1 42.0 39.9
Table 11. Detection mean Average Precision (mAP) Results on
COCO 2017 validation set. (mAP%) is reported for comparsion. 4.5. Transfer Learning: Object Detection
We demonstrate the transferability of our ImageNet pre-trained models on the COCO 2017 [21] object detection task, on which we observe signiﬁcant improvements over strong Detectron2 [35] baselines by simply applying our pre-trained models as backbones.
Dataset and Settings
COCO 2017 consists of 118,000 training images and 5,000 validation images. To verify that our trained models can be widely useful for different de-tector systems, we test several popular structures, including
Faster RCNN [26], feature pyramid networks [19] (FPN) and RetinaNet [20]. We use the codebase provided by
Detectron2 [35], follow almost all the hyper-parameters except changing the backbone networks from PyTorch provided models to our models. For our method, we test the ResNet-50 and ResNet-101 models trained with our
KeepCutMix.
Results
We report mean average precision (mAP) on the COCO 2017 validation set [21]. As we can see from Ta-ble 11, our method consistently improves over baseline ap-proaches. Simply replacing the backbone network with our pre-trained model gives performance gains for the COCO 2017 object detection tasks with no additional cost. In par-ticular, on the single-stage detector RetinaNet, we improve the 37.9 mAP to 39.1, and 39.9 mAP to 41.2 for ResNet-50 and ResNet-101, respectively. 5.