Abstract
We introduce the task of dense captioning in 3D scans from commodity RGB-D sensors. As input, we assume a point cloud of a 3D scene; the expected output is the bound-ing boxes along with the descriptions for the underlying objects. To address the 3D object detection and descrip-tion problems, we propose Scan2Cap, an end-to-end trained method, to detect objects in the input scene and describe them in natural language. We use an attention mechanism that generates descriptive tokens while referring to the re-lated components in the local context. To reﬂect object re-lations (i.e. relative spatial relations) in the generated cap-tions, we use a message passing graph module to facilitate learning object relation features. Our method can effec-tively localize and describe 3D objects in scenes from the
ScanRefer dataset, outperforming 2D baseline methods by a signiﬁcant margin (27.61% CiDEr@0.5IoU improvement). 1.

Introduction
The intersection of visual scene understanding [45, 20] and natural language processing [49, 13] is a rich and ac-tive area of research. Speciﬁcally, there has been a lot of work on image captioning [51, 27, 52, 33, 2] and the re-lated task of dense captioning [27, 26, 53, 56, 28, 31]. In dense captioning, individual objects are localized in an im-age and each object is described using natural language. So far, dense captioning work has operated purely on 2D visual data, most commonly single-view images that are limited by the ﬁeld of view. Images are inherently viewpoint speciﬁc and scale agnostic, and fail to capture the physical extent of 3D objects (i.e. the actual size of the objects) and their locations in the environment.
In this work, we introduce the new task of dense caption-ing in 3D scenes. We aim to jointly localize and describe each object in a 3D scene. We show that leveraging the 3D information of an object such as actual object size or object location results in more accurate descriptions.
Apart from the 2D constraints in images, even seminal work on dense captioning suffers from aperture issues [56].
Object relations are often neglected while describing scene objects, which makes the task more challenging. We ad-dress this problem with a graph-based attentive captioning architecture that jointly learns object features and object re-lation features on the instance level, and generates descrip-tive tokens. Speciﬁcally, our proposed method (referred to as Scan2Cap) consists of two critical components: 1) Re-lational Graph facilitates learning the object features and object relation features using a message passing neural net-work; 2) Context-aware Attention Captioning generates the descriptive tokens while attending to the object and object relation features. In summary, our contribution is fourfold:
• We introduce the 3D dense captioning task to densely 3193
detect and describe 3D objects in RGB-D scans.
• We propose a novel message passing graph module that facilitates learning of the 3D object features and 3D object relation features.
• We propose an end-to-end trained method that can take 3D object features and 3D object relation features into account when describing the 3D object in a single for-ward pass.
• We show that our method outperforms 2D-3D back-projected results of 2D captioning baselines by a sig-niﬁcant margin (27.61%). 2.