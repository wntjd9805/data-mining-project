Abstract
Current dynamic networks and dynamic pruning methods have shown their promising capability in reducing theo-retical computation complexity. However, dynamic sparse patterns on convolutional ﬁlters fail to achieve actual ac-celeration in real-world implementation, due to the extra burden of indexing, weight-copying, or zero-masking. Here, we explore a dynamic network slimming regime, named Dy-namic Slimmable Network (DS-Net), which aims to achieve good hardware-efﬁciency via dynamically adjusting ﬁlter numbers of networks at test time with respect to different inputs, while keeping ﬁlters stored statically and contigu-ously in hardware to prevent the extra burden. Our DS-Net is empowered with the ability of dynamic inference by the proposed double-headed dynamic gate that comprises an attention head and a slimming head to predictively adjust network width with negligible extra computation cost. To ensure generality of each candidate architecture and the fairness of gate, we propose a disentangled two-stage train-ing scheme inspired by one-shot NAS. In the ﬁrst stage, a novel training technique for weight-sharing networks named
In-place Ensemble Bootstrapping is proposed to improve the supernet training efﬁcacy. In the second stage, Sandwich
Gate Sparsiﬁcation is proposed to assist the gate training by identifying easy and hard samples in an online way. Ex-tensive experiments demonstrate our DS-Net consistently outperforms its static counterparts as well as state-of-the-art static and dynamic model compression methods by a large margin (up to 5.9%). Typically, DS-Net achieves 2-4× com-putation reduction and 1.62× real-world acceleration over
ResNet-50 and MobileNet with minimal accuracy drops on
ImageNet.1 1.

Introduction
As deep neural networks are becoming deeper and wider to achieve higher performance, there is an urgent need to
∗Corresponding author. 1Code release: https://github.com/changlin31/DS-Net
Figure 1. Universally accuracy-complexity comparison of our DS-Net and Universally Slimmable Network (US-Net) [66] (based on
MobileNetV1 [23]).
Table 1. Latency comparison of ResNet-50 with 25% channels (on GeForce RTX 2080 Ti). Both masking and indexing lead to inefﬁcient computation waste, while slicing achieves comparable acceleration with ideal (the individual ResNet-50 0.25×). method full masking indexing slicing (ours) ideal latency 12.2 ms 12.4ms 16.6 ms 7.9 ms 7.2 ms explore efﬁcient models for common mobile platforms, such as self-driving cars, smartphones, drones and robots. In recent years, many different approaches have been pro-posed to improve the inference efﬁciency of neural net-works, including network pruning [40, 47, 19, 20, 48, 50], weight quantization [30], knowledge distillation [2, 52, 21], manually and automatically designing of efﬁcient net-works [56, 53, 71, 51, 3, 69, 10, 16, 38, 70] and dynamic inference [4, 27, 59, 58, 41, 26, 13, 9].
Among the above approaches, dynamic inference meth-ods have attracted increasing attention because of their promising capability of reducing computational redundancy by automatically adjusting their architecture with respect to different inputs. As illustrated in Fig. 2 right, the dynamic network learns to conﬁgure different architecture routing adaptively for each input, instead of optimizing the archi-tecture among the whole dataset like Neural Architecture
Search (NAS) or Pruning. A performance-complexity trade-8607
Accuracy 1 0 easy & hard  static nets dynamic nets
MAdds (B)
Figure 2.
The motivation for designing dynamic networks to achieve efﬁcient inference. Left: A simulation diagram of accuracy-complexity comparing a series of static networks (searched by NAS) with 20 dynamic inference schemes of dif-ferent resource allocate proportion for easy and hard samples on a hypothetical classiﬁcation dataset with evenly distributed easy and hard samples. Right: Illustration of dynamic networks on efﬁcient inference. Input images are routed to use different architectures regarding their classiﬁcation difﬁculty. off simulated with exponential functions is shown in Fig. 2 left, the optimal solution of dynamic networks is superior to the static NAS or pruning solution. Ideally, dynamic network routing can signiﬁcantly improve model performance under certain complexity constraints.
However, networks with dynamic width, i.e., dynamic pruning methods [13, 26, 9], unlike its orthogonal counter-parts with dynamic depth, have never achieved actual acceler-ation in a real-world implementation. As natural extensions of network pruning, dynamic pruning methods predictively prune the convolution ﬁlters with regard to different input at runtime. The varying sparse patterns are incompatible with computation on hardware. Actually, many of them are implemented as zero masking or inefﬁcient path indexing, resulting in a massive gap between the theoretical analysis and the practical acceleration. As shown in Tab. 1, both masking and indexing lead to inefﬁcient computation waste.
To address the aforementioned issues in dynamic net-works, we propose Dynamic Slimmable Network (DS-Net), which achieves good hardware-efﬁciency via dynamically adjusting ﬁlter numbers of networks at test time with respect to different inputs. To avoid the extra burden on hardware caused by dynamic sparsity, we adopt a scheme named dy-namic slicing to keep ﬁlters static and contiguous when adjusting the network width. Speciﬁcally, we propose a double-headed dynamic gate with an attention head and a slimming head upon slimmable networks to predictively adjust the network width with negligible extra computation cost. The training of dynamic networks is a highly entangled bilevel optimization problem. To ensure generality of each candidate’s architecture and the fairness of gate, a disentan-gled two-stage training scheme inspired by one-shot NAS is proposed to optimize the supernet and the gates separately.
In the ﬁrst stage, the slimmable supernet is optimized with a novel training method for weight-sharing networks, named
In-place Ensemble Bootstrapping (IEB). IEB trains the smaller sub-networks in the online network to ﬁt the output logits of an ensemble of larger sub-networks in the momen-tum target network. Learning from the ensemble of different sub-networks will reduce the conﬂict among sub-networks and increase their generality. Using the exponential mov-ing average of the online network as the momentum target network can provide a stable and accurate historical repre-sentation, and bootstrap the online network and the target network itself to achieve higher overall performance. In the second stage, to prevent dynamic gates from collapsing into static ones in the multiobjective optimization problem, a technique named Sandwich Gate Sparsiﬁcation (SGS) is proposed to assist the gate training. During training, SGS identiﬁes easy and hard samples online and further generates the ground truth label for the dynamic gates.
Overall, our contributions are three-fold as follows:
• We propose a new dynamic network routing regime, achieving good hardware-efﬁciency by predictively ad-justing ﬁlter numbers of networks at test time with respect to different inputs. Unlike dynamic pruning methods, we dynamically slice the network parameters while keeping them stored statically and contiguously in hardware to prevent the extra burden of masking, indexing, and weight-copying. The dynamic routing is achieved by our proposed double-headed dynamic gate with negligible extra computation cost.
• We propose a two-stage training scheme with IEB and
SGS techniques for DS-Net. Proved experimentally,
IEB stabilizes the training of slimmable networks and boosts its accuracy by 1.8% and 0.6% in the slimmest and widest sub-networks respectively. Moreover, we empirically show that the SGS technique can effectively sparsify the dynamic gate and improves the ﬁnal perfor-mance of DS-Net by 2%.
• Extensive experiments demonstrate our DS-Net outper-forms its static counterparts [65, 66] as well as state-of-the-art static and dynamic model compression methods by a large margin (up to 5.9%, Fig. 1). Typically, DS-Net achieves 2-4× computation reduction and 1.62× real-world acceleration over ResNet-50 and MobileNet with minimal accuracy drops on ImageNet. Gate visu-alization proves the high dynamic diversity of DS-Net. 2.