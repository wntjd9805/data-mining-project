Abstract
GuessWhat?! is a visual dialog guessing game which in-corporates a Questioner agent that generates a sequence of questions, while an Oracle agent answers the respec-tive questions about a target object in an image. Based on this dialog history between the Questioner and the Ora-cle, a Guesser agent makes a ﬁnal guess of the target ob-ject. While previous work has focused on dialogue pol-icy optimization and visual-linguistic information fusion, most work learns the vision-linguistic encoding for the three agents solely on the GuessWhat?! dataset without shared and prior knowledge of vision-linguistic representation. To bridge these gaps, this paper proposes new Oracle, Guesser and Questioner models that take advantage of a pretrained vision-linguistic model, VilBERT. For Oracle model, we introduce a two-way background/target fusion mechanism to understand both intra and inter-object questions. For
Guesser model, we introduce a state-estimator that best uti-lizes VilBERT’s strength in single-turn referring expression comprehension. For the Questioner, we share the state-estimator from pretrained Guesser with Questioner to guide the question generator. Experimental results show that our proposed models outperform state-of-the-art models signif-icantly by 7%, 10%, 12% for Oracle, Guesser and End-to-End Questioner respectively. 1.

Introduction
Multi-modal dialog tasks have gained increasing pop-[8], Guess-ularity in recent years such as GuessWhat?!
Which?! [5], VisDial [7], VDQG [16], vision-and-language navigation R2R [3], ImageChat [27], Alfred [25] and so on. Multi-modal dialog tasks are challenging as the models need to perform high-level image understanding and visual grounding, and such visual grounding should be properly combined with understanding and tracking of multi-turn di-*Corresponding author
Figure 1: Example of the GuessWhat?! dataset alogues in the meanwhile.
The GuessWhat?! dataset is a challenging dataset for a two-player game, where one player will ask a sequence of binary questions and make a ﬁnal guess for an object in an image designated by another player. The ﬁrst player per-forms two sub-tasks, namely as a Questioner to ask ques-tions, and as a Guesser to make the ﬁnal guess. The sec-ond player serves as the Oracle to give Yes/No answer to
ﬁrst player’s questions. An example of the GuessWhat?! game can be seen in Figure-1. The GuessWhat?! game is a good test-bed for such multi-modal tasks such as VQA, referring expression comprehension and generation, and it is also organized in a multi-turn multi-agent dialog. This paper focuses on the three agents for the two players in the
GuessWhat?! dataset, namely the Oracle model, Guesser model and the Questioner model.
The Oracle task can be considered as an object-aware
Visual Question Answering task (VQA), where the inputs are an image, a question, and a pre-deﬁned target object, and the output is an answer of Yes/No/NA depending on whether the question matches the target object. The base-line Oracle model [8] encodes the target object with only category and spatial information but no visual informa-tion, which may be insufﬁcient for answering more complex questions about color, shape, relation, actions of an object.
To bridge this gap, we introduce VilBERT-Oracle, which takes advantage of the VilBERT model’s ability to achieve state-of-the-art performance on VQA tasks [17, 18]. We also introduce a two-way background/target fusion mecha-nism on top of the VilBERT encoder to learn how to predict 5622
correct binary answers with respective to a target object.
The Guesser model can be considered as a special case of referring expression comprehension problem. Given an im-age and an entire dialog history of questions (referring ex-pression) and its corresponding answers, the Guesser model has to look at the entire dialog and make a ﬁnal guess. One intuitive solution is to simply concatenate the entire dia-log and feed them to the model [9, 29, 26, 18]. However, this might be inadequate if the dialog history is not properly dissected in a way to promote/demote objects according to question and answer in each turn. Recent work [21] intro-duces object state tracking mechanism, where the belief of all objects is dynamically updated after each turn. Another issue is that almost all existing Guesser models learn the vision-linguistic associations between object and question from scratch on the GuessWhat?! dataset, which may be sparse in coverage of referring expressions for new objects.
To bridge this gap, we propose VilBERT-Guesser, which is built on top of VilBERT’s strength in single-turn referring expression comprehension, and introduces the object state tracking mechanism into VilBERT encoder to learn to up-date the belief of object states throughout the dialog. To our best knowledge, this is the ﬁrst work that brings dia-log state tracking to large-scale pre-trained vision-linguistic model, which is meant to work only on single-turn text de-scriptions.
The Questioner model can be considered as a special case of referring expression generation problem. Previous works for Questioner model intuitively encode the image feature and dialog history information to a fused representa-tion, and utilize a language decoder to generate the question
[29, 24, 39, 37, 1, 2]. The multi-modal fusion modules are mostly learned from scratch on the GuessWhat?! dataset, which may be insufﬁcient for similar reasons as the Guesser models. Moreover, the encoding of the dialog history as a whole, poses challenges for language generator which tends to forget long-term history and generates repeated ques-tions. Recent work introduces state-tracking to Questioner, which dynamically feeds the updated beliefs over objects into Questioner, so that the language generator could gen-erate more targeted questions in each turn[21]. Inspired by this work, we introduce object state estimation mechanism to our VilBERT-Questioner. Moreover, once the VilBERT-Guesser is trained, we load its weights to the state-estimator of VilBERT-Questioner, so that the later could take advan-tage of the Guesser’s ability to make reliable predictions for estimating object states.
Our major contribution of the paper are as follows. First, we propose novel Oracle, Guesser and Questioner models that are built on top of a state-of-the-art vision-linguistic pre-trained model. The proposed models outperform ex-isting state-of-the-art models with signiﬁcant margins. Sec-ond, we propose a uniﬁed framework for Guesser and Ques-tioner so that Questioner can take advantage of the ro-bust state-estimator learned from VilBERT-Guesser. Third, we conduct thorough ablation-study and analysis and ﬁnd that a shared vision-linguistic representation cross the three agents may be beneﬁcial for mutual-understanding and end-game success. Our code is made publicly available. 1 2.