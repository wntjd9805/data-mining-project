Abstract
It is well acknowledged that person re-identiﬁcation (person ReID) highly relies on visual texture information like clothing. Despite signiﬁcant progress has been made in recent years, texture-confusing situations like clothing changing and persons wearing the same clothes receive lit-tle attention from most existing ReID methods. In this pa-per, rather than relying on texture based information, we propose to improve the robustness of person ReID against clothing texture by exploiting the information of a person’s 3D shape. Existing shape learning schemas for person
ReID either ignore the 3D information of a person, or re-quire extra physical devices to collect 3D source data. Dif-ferently, we propose a novel ReID learning framework that directly extracts a texture-insensitive 3D shape embedding from a 2D image by adding 3D body reconstruction as an auxiliary task and regularization, called 3D Shape Learn-ing (3DSL). The 3D reconstruction based regularization forces the ReID model to decouple the 3D shape informa-tion from the visual texture, and acquire discriminative 3D shape ReID features. To solve the problem of lacking 3D ground truth, we design an adversarial self-supervised pro-jection (ASSP) model, performing 3D reconstruction with-out ground truth. Extensive experiments on common ReID datasets and texture-confusing datasets validate the effec-tiveness of our model. 1.

Introduction
The aim of person ReID is to ﬁnd the target person among an existing set of persons captured by a distributed camera system. Some works [7, 34, 44, 46] have demon-strated that person ReID largely depends on clothing ap-# Equal Contribution.
* Corresponding Author.
Figure 1. Illustration of texture-confusing person ReID. Human shape information is crucial when clothing texture is misleading.
However, modeling shape upon 2D measurement could not cap-ture the intact shape perception and collection of 3D source data relies on auxiliary devices. Single-view 3D human reconstruction could help to learn 3D shape feature in surveillance environments. pearance textures, and most of existing methods decline a lot when clothing texture is confusing. Considering cloth-ing texture-confusing situations (see Figure 1) that people might change their clothing [44] or different people wear very similar clothing [46], clothing texture would become unreliable for ReID. Situations like suspects wearing differ-ent clothes, or different people wearing similar uniforms in hospitals or schools are ubiquitous.
To extend the scalability of real-world person ReID, in this paper, we explicitly model discriminative clues beyond human clothing textures, i.e., human shape representations.
Existing works try to learn shape-related features in two ways: 2D image space [7, 34, 44] and 3D source data
[21, 31, 41]. 2D-based methods mainly make attempt to ex-tract shape feature based on visual statistics, such as contour
[44] and keypoint [34], or via adversarial feature disentan-glement [21]. These methods only utilize the structure and shape information in 2D space while 3D information like 18146
depth or relative 3D position is ignored. 3D-based source data could be collected from kinect cameras [41] or ratio signals [7], which has the potential to capture the integrated shape from an all-around horizon. However, collecting 3D data might be infeasible in a surveillance environment.
In order to learn 3D shape representation without extra 3D devices, we propose a novel feature learning schema combining 3D human reconstruction from a single image
[1, 2, 4, 5, 14, 18, 32].
Instead of extracting ReID fea-tures from imprecise reconstructed 3D meshes [50], we train a ReID model that extracts texture-insensitive 3D fea-tures directly from the original 2D images by adding 3D reconstruction as an auxiliary task and regularization to the
ReID feature learning. The 3D reconstruction based regu-larization is able to force the ReID model to decouple the 3D shape information from the visual texture, and acquire discriminative 3D shape ReID features that are more reli-able for texture-confusing persons. In practice, a multi-task framework is adopted, and ReID feature is supervised by both identiﬁcation losses (e.g., softmax loss and triplet loss) and 3D reconstruction losses.
One of the troublesome obstacles for training 3D hu-man reconstruction lies in the lack of 3D ground truth. To overcome the data limitation, following the literature of 3D reconstruction [14, 32], we design a purely unsupervised framework called Adversarial Self-Supervised Projection (ASSP). We ﬁrst utilize external unlabeled 3D data [26] to train a discriminator distinguishing the reconstruction re-sults from real 3D parameters in an adversarial way. This could prevent abnormal poses and shapes in a coarse level.
Then, we introduce a self-supervised learning loop that re-projects 3D reconstruction results back to the 2D plane and minimize the reconstruction error compared with 2D obser-vations (e.g., keypoints and silhouettes). This could further
ﬁt personalized 3D bodies in a ﬁne level. 3D human reconstruction tends to obtain a mean shape representation, and thus a global 3D shape feature is not discriminative enough. To enhance the discriminative abil-ity of ReID, we propose the Multi-Granularity Shape fea-ture (MGS) learning to combine both global and part shape features. In MGS, the global 3D shape feature corresponds to the global shape parameter estimation and part 3D shape features are used to estimate subtle local shape displace-ments. This could help to capture 3D shape features in dif-ferent scopes and enrich the diversity of features.
We summarize our contributions as follows:
- We propose a novel end-to-end architecture combin-ing person ReID and 3D human reconstruction to learn texture-insensitive 3D shape embedding. We further propose a multi-granularity shape (MGS) learning to enhance discriminative ability for person ReID.
- To address the problem of lacking 3D ground truth, we design the Adversarial Self-supervised Projec-tion (ASSP) combining adversarial learning and self-supervised projection, validating that 3D reconstruc-tion is capable to promote ReID in a uniﬁed training schema even without 3D ground truth.
The experimental results in common person ReID datasets (Market1501 [48], DukeMTMC-ReID [51]) and texture-confusing datasets (PRCC [44], VC-Clothes [39],
LTCC [34], FGPR [46]) have demonstrated the effective-ness of the proposed model. 2.