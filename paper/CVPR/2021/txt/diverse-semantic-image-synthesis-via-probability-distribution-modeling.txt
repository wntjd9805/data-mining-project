Abstract
Semantic image synthesis, translating semantic lay-outs to photo-realistic images, is a one-to-many mapping problem. Though impressive progress has been recently made, diverse semantic synthesis that can efﬁciently pro-duce semantic-level multimodal results, still remains a chal-lenge.
In this paper, we propose a novel diverse seman-tic image synthesis framework from the perspective of se-mantic class distributions, which naturally supports diverse generation at semantic or even instance level. We achieve this by modeling class-level conditional modulation param-eters as continuous probability distributions instead of dis-crete values, and sampling per-instance modulation param-eters through instance-adaptive stochastic sampling that is consistent across the network. Moreover, we propose prior noise remapping, through linear perturbation param-eters encoded from paired references, to facilitate super-vised training and exemplar-based instance style control at test time. Extensive experiments on multiple datasets show that our method can achieve superior diversity and compa-rable quality compared to state-of-the-art methods. Code will be available at https://github.com/tzt101/
INADE.git 1.

Introduction
Image synthesis has recently seen impressive progress, particularly with the help of generative adversarial networks (GANs) [8]. Besides stochastic approaches that generate high-quality images from random latent variables [18, 19], conditional image synthesis is attracting equal or even more attention due to the practical advantages of its controllabil-ity. The conditional input, to guide the synthesis, can be of various forms, including RGB images, edge/gradient maps, semantic labels, etc. In this work, semantic image synthesis
*Corresponding author. is one particular task that aims to generate a photo-realistic image from a semantic label mask. In particular, we fur-ther explore its diversity and controllability without loss of generation quality. Some samples are shown in Figure 1.
Previous works [15, 41] propose solutions within the general image-to-image translation framework, which di-rectly feeds the semantic mask into the encoder-decoder network. For higher quality, some recent methods [30, 50, 36] adopt spatially-varying conditional normalization to avoid the loss of semantic information due to conventional normalization layers [40]. Although proven successful in synthesizing certain types of content, these methods lack controllability over the generation diversity, which is par-ticularly important for such a one-to-many problem. Some methods [49, 43] attempt to yield multimodal results by in-corporating variational auto-encoder (VAE) or introducing noises. However, these methods only support global image-level diversity. To obtain ﬁner-grained controllability, a re-cent work [51] proposes to use group convolution for differ-ent semantics to achieve semantic-level diversity. However, it is computationally expensive and difﬁcult to be extended to support diversity at the instance level.
In this paper, we attempt to achieve controllable diversity in semantic image synthesis from the perspective of seman-tic probability distributions. The intuition is to treat each semantic class as one distribution, so that each instance of this class could be drawn from this distribution as a discrete sample. Following this idea, we propose a novel semantic image synthesis framework, which is naturally capable of producing diverse results at semantic or even instance level.
Speciﬁcally, our method contains three key ingredients.
Firstly, we propose variational modulation models (§ 3.2) that extend discrete modulation parameters to class-wise continuous probability distributions, which embed diverse styles of each semantic category in a class-adaptive manner.
Secondly, based on the variational models built per normal-ization layer, we further develop an instance-adaptive sam-pling method (§ 3.3) that achieves instance-level diversity 7962
to-ﬁne generator and discriminators. Subsequent meth-ods [32, 27, 39, 45, 37, 50] further explore how to synthe-size high quality images from semantic masks and achieve signiﬁcant improvements. Besides using class-level seman-tic masks, some works also consider instance-level informa-tion for image synthesis, since the semantic mask itself does not provide sufﬁcient information to synthesize instances especially in complex environments with multiple of them interacting with each other. Some works [41, 30, 37] ex-tract boundary information from the instance map and con-catenate it with the semantic mask. While recent work [6] proposes to use the instance map to guide convolution and upsampling layers for better exploiting both semantic and instance information. Different from these methods, we are interested in taking full advantage of information from in-stance maps to achieve instance-level diversity control. 2.2. Diversity in Image Synthesis
Diversity is a core target for image synthesis, which aims to generate multiple possible outputs from a sin-gle input image. Early conditional image synthesis net-works either trained with paired data, like Pix2Pix [15] and Pix2pixHD [41], or with unpaired data, like Cycle-GAN [48], DiscoGAN [20] and UNIT [26], are single-modal. Later, some multimodal unpaired image synthesis networks [13, 24, 1] are proposed. However, constrained by the reconstruction loss, the semantic image synthesis task trained with paired data is more difﬁcult to support diversity. To tackle this problem, BicycleGAN [49] en-forces the bijection mapping between the noise vector and target domain, and DSCGAN [43] proposes a simple reg-ularization method. More recently, a variational autoen-coder architecture is used to handle multimodal synthesis by [30, 37, 27]. However, these multimodal image synthesis networks only support diversity at the global level. To fur-ther control the diversity at the semantic level, the method proposed by [9] builds several auto-encoders for each face component to extract different component representations.
GroupDNet [51] uniﬁes the generation process in only one model, but still requires high computing resources, and the use of group convolution layer makes it difﬁcult to extend to the instance level. In contrast, we propose a novel instance-aware conditional normalization framework that allows di-verse instance-level generation with less overhead. 3. Method
We are interested in the task of semantic image synthesis, which is deﬁned as to map a semantic mask m ∈ LH×W to a photo-realistic image o ∈ R3×H×W . Here, m is a class-level label map with each pixel representing an integer index to a pre-deﬁned set of semantic categories
Lm = {1, 2, . . . , Lm}. Each pair of input m and output o is spatially-aligned and of the same dimension H × W , m 7963
Figure 1. Semantic-level (left three columns) and Instance-level (right two columns) multimodal images generated by the proposed method. Text of each column indicates which semantic class or instance will be changed in the following results. by stochastically sampling modulation parameters from the variational models. We harmonize the sampling across the network via consistent randomness and a learnable trans-formation function for each normalization layer. Finally, to more efﬁciently embed the instance diversity to the mod-ulation models, we propose prior noise remapping (§ 3.4) that transforms the noise samples with perturbation param-eters encoded from arbitrary references. We adopt this step to facilitate supervised training and enable test-time reference-based style guidance. Inspired by [30, 38, 37], the proposed method is called INADE (INstance-Adaptive
DEnormalization).
To evaluate the proposed method, we conduct extensive experiments on multiple datasets, including Cityscapes [3],
ADE20K [47], CelebAMask-HQ [23, 17, 29], and Deep-Fashion [28]. Both quantitative and qualitative results show that our method signiﬁcantly outperforms state-of-the-art methods by achieving much better instance-level diversity while keeping comparable generation quality. 2.