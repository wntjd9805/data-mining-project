Abstract
We propose a new approach to determine correspon-dences between image pairs in the wild under large changes in illumination, viewpoint, context, and material. While other approaches ﬁnd correspondences between pairs of images by treating the images independently, we instead condition on both images to implicitly take account of the differences between them. To achieve this, we introduce (i) a spatial attention mechanism (a co-attention module, CoAM) for conditioning the learned features on both images, and (ii) a distinctiveness score used to choose the best matches at test time. CoAM can be added to standard architectures and trained using self-supervision or supervised data, and achieves a signiﬁcant performance improvement under hard conditions, e.g. large viewpoint changes. We demonstrate that models using CoAM achieve state of the art or competi-tive results on a wide range of tasks: local matching, camera localization, 3D reconstruction, and image stylization. 1.

Introduction
Determining correspondence between two images of the same scene or object is a fundamental challenge of computer vision, important for many applications ranging from optical
ﬂow and image manipulation, to 3D reconstruction and cam-era localization. This task is challenging due to scene-shift: two images of the same scene can differ dramatically due to variations in illumination (e.g. day to night), viewpoint, texture, and season (e.g. snow in winter versus ﬂowering trees in spring).
Methods that solve the correspondence task typically fol-low a detect-and-describe approach: ﬁrst they detect dis-tinctive regions [5, 15, 30, 34, 49] and then describe these regions using descriptors [5, 6, 21, 26, 30, 49] with varying degrees of invariance to scale, illumination, rotation, and afﬁne transformations. These descriptors are then matched between images by comparing descriptors exhaustively, of-ten using additional geometric constraints [16]. Recent ap-proaches have sought to learn either or both of these com-ponents [3, 8, 9, 10, 13, 27, 39, 46, 47, 54, 59, 60, 73, 74].
These methods typically only ﬁnd matches at textured lo-cations, and do not ﬁnd matches over smooth regions of an object. Additionally, ﬁnding these repeatable detections with invariance to scene-shift is challenging [2, 51, 56].
If prior knowledge is assumed, in terms of limited cam-era or temporal change (as in optical ﬂow computation in videos), then a dense-to-dense approach can be used for pairs that have limited scene shift. In this case, methods typically obtain a dense feature map which is compared from one image to another by restricting the correspondence search to a small support region in the other image (based on the prior knowledge). Spatial and smoothness constraints can addi-tionally be imposed to improve results [8, 11, 29, 53, 66, 71].
We focus on the cases where there is potentially signiﬁ-cant scene shift (and no prior knowledge is available), and introduce a new approach for obtaining correspondences be-tween a pair of images. Previous methods learn descriptors for each image without knowledge of the other image. Thus, their descriptors must be invariant to changes – e.g. to scale and illumination changes. However, as descriptors become increasingly invariant, they become increasingly ambiguous to match (e.g. a constant descriptor is invariant to everything but also confused for everything). We forsake this invariance and instead condition the descriptors on both images. This al-lows the descriptors to be modiﬁed based on the differences between the images (e.g. a change in global illumination).
Traditionally, this was infeasible, but we can learn such a model efﬁciently using neural networks.
To achieve this we introduce a network (CD-UNet), which consists of two important components. First, a new spatial
Co-Attention Module (CoAM) that can be ‘plugged into’ a
UNet, or similar architectures developed for single image de-scriptors, in order to generate descriptors conditioned on the pair of images. Second, we introduce a Distinctiveness score in order to select the best matches from these descriptors.
We further investigate the utility of the CoAM under both supervised and self-supervised training. In the latter case, we augment the recent self-supervised approach of learning camera pose of [70] by using CoAMs in a plug-and-play fashion. We evaluate these trained models on a variety of 115920
(a) (b) (c) (d)
Figure 1: Correspondences obtained with the CoAM model, which is augmented with an attention mechanism. These demonstrate the model’s robustness in the face of challenging scene shift: changes in illumination (a,d), viewpoint (a-d), context (a,d), or style (b). tasks: local matching, camera localization, 3D reconstruc-tion, and style transfer. We improve over state-of-the-art (sota) models, especially under challenging conditions, and achieve sota or comparable on all tasks.
In summary, we present a key insight: that condition-ing learned descriptors on both images should allow for improved correspondence matching under challenging con-ditions. As will be seen, CD-UNet is simple and scalable and eschews a number of techniques used by other methods to improve matching performance: high dimensional descrip-tors (we use a 64D descriptor, half the size of the current smallest descriptor), and multiple scales (we only operate at a single scale, whereas other methods use multiple scales). 2.