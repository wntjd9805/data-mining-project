Abstract
Convolutional Neural Networks (CNNs) are now a well-established tool for solving computational imaging problems.
Modern CNN-based algorithms obtain state-of-the-art per-formance in diverse image restoration problems. Further-more, it has been recently shown that, despite being highly overparameterized, networks trained with a single corrupted image can still perform as well as fully trained networks.
We introduce a formal link between such networks through their neural tangent kernel (NTK), and well-known non-local
ﬁltering techniques, such as non-local means or BM3D. The
ﬁltering function associated with a given network architec-ture can be obtained in closed form without need to train the network, being fully characterized by the random initial-ization of the network weights. While the NTK theory ac-curately predicts the ﬁlter associated with networks trained using standard gradient descent, our analysis shows that it falls short to explain the behaviour of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local
ﬁltering function during training. We evaluate our ﬁndings via extensive image denoising experiments1. 1.

Introduction
Convolutional neural networks are now ubiquitous in deep learning solutions for computational imaging and com-puter vision, ranging from image restoration tasks such as denoising, deblurring, inpainting and super-resolution, to im-age reconstruction tasks such as computed tomography [19] and magnetic resonance imaging [18]. However, the empiri-cal success of CNNs is in stark contrast with our theoretical understanding. Contrary to traditional sparse models [8], there is little understanding of the implicit assumptions on the set of plausible signals imposed by CNNs.
Perhaps surprisingly, Ulyanov et al. [29] discovered that 1Code available at https://gitlab.com/Tachella/neural_ tangent_denoiser training a CNN only with a single corrupted image (the one being restored) could still achieve competitive recon-structions in comparison to fully trained networks, naming this phenomenon the deep image prior (DIP). This discov-ery challenges traditional wisdom that networks should be trained with large amounts of data and illustrates the power-ful bias of CNN architectures towards natural images. Simi-lar ideas have also been explored in Noise2Self [4] and other variants [14]. In this setting, the number of weights (e.g., 2,000,000 for a U-Net CNN [29, 26]) is much larger than the number of pixels in the training image (e.g., 50,000 pixels 128 color image). The clean version of a standard 128 of the corrupted image is obtained by early-stopping the optimization process before the network fully matches the noisy image or by considering a loss that does not allow the network to learn the corrupted image exactly [4]. These sur-prising results raise the following questions: how, amongst all possible optimization trajectories towards the multiple global minima of the training loss, the procedure consistently provides close to state-of-the-art reconstructions? What is the role of the optimization algorithm on the trajectory to-wards the global minima, and how does it affect the bias towards clean images?
×
Despite their surprisingly good performance, these meth-ods provide comparable or slightly worse denoising results than classical patch-based non-local ﬁltering techniques, such as non-local means (NLM) [5] or BM3D [7], which also only have access to the corrupted image. Moreover, training a large neural network is more computationally intensive.
Subsequent questions then arise: is the neural network per-forming a similar ﬁltering process? Can we avoid the slow training, and apply this ﬁlter in a more direct way? These in-sights are important to build a better framework in which we can optimize and design new denoisers and other low-level computer vision algorithms.
Denoising is generally considered as the fundamental building block of any image restoration problem. In many applications, CNNs are used to perform denoising steps, either in unrolled schemes [19] or in the context of plug-and-play methods [25, 31]. Hence, understanding better the bias 8618
corrupted    image 
CNN clean eigenvectors input output random features l s e u a v n e g e i noisy eigenvectors index
Figure 1: A convolutional neural network z trained with gradient descent on a single corrupted image can achieve powerful denoising. The left singular vectors of the Jacobian offer a representation based on patch similarities which is robust to noise. of CNNs towards clean images is the ﬁrst step towards more general imaging problems.
On another line of work, researchers have also observed that increasing the amount of overparameterization does not necessarily harm the generalization of the network [38] in the context of classiﬁcation. Recently, Jacot et al. showed that overparameterized neural networks trained with (stochastic) gradient descent (GD) converge to a Gaussian process as the number of weights tends to inﬁnity, with a kernel that depends only on the architecture and variance of the random initialization, named the neural tangent kernel (NTK) [12].
While the properties and accuracy of the kernel were ana-lyzed for image classiﬁcation [1], to the best of our knowl-edge, little is known in the context of high-dimensional image restoration with no clean data. Can this theory explain the good denoising performance of networks trained with a single corrupted image?
In this paper, we study overparameterized convolutional networks and their associated neural tangent kernel in the context of the image denoising, formalizing strong links with classical non-local ﬁltering techniques, but also analyzing the short-comings of this theory to fully explain the results obtained by the DIP. The main contributions of this paper are as follows: 1. We show that GD trained CNN denoisers with a sin-gle corrupted image (placed both at the input and as a target) in the overparameterized regime equate to performing an existing iterative non-local ﬁltering tech-nique known as twicing [20], where the non-local ﬁlter is characterized by the architectural properties of the network. Moreover, these ﬁlters impose a form of low-dimensionality due to their fast eigenvalue decay, and efﬁcient ﬁltering can be performed directly without the
CNN, using the Nyström approximation [33]. 2. Departing from previous explanations [6, 11], we show that the DIP cannot be solely understood as a prior pro-moting low-pass images. We link this short-coming to the choice of the optimization algorithm. When trained with GD, the DIP has poor performance as predicted by the NTK theory, and maintains a ﬁxed low-pass ﬁlter throughout training. However, training with the popular
Adam optimizer as in the original DIP is able to adapt the ﬁlter with non-local information from the target image. 3. We evaluate our ﬁndings with a series of denoising experiments, showing that the ﬁxed non-local ﬁlter as-sociated with gradient descent performs signiﬁcantly better when the corrupted image is placed at the input, whereas the Adam optimizer adapts the ﬁlter during training, providing good results for both scenarios. 2.