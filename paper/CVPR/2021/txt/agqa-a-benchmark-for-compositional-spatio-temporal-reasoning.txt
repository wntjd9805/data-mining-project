Abstract
Visual events are a composition of temporal actions in-volving actors spatially interacting with objects. When developing computer vision models that can reason about compositional spatio-temporal events, we need benchmarks that can analyze progress and uncover shortcomings. Ex-isting video question answering benchmarks are useful, but they often conﬂate multiple sources of error into one accu-racy metric and have strong biases that models can exploit, making it difﬁcult to pinpoint model weaknesses. We present
Action Genome Question Answering (AGQA), a new bench-mark for compositional spatio-temporal reasoning. AGQA contains 192M unbalanced question answer pairs for 9.6K videos. We also provide a balanced subset of 3.9M question answer pairs, 3 orders of magnitude larger than existing benchmarks, that minimizes bias by balancing the answer distributions and types of question structures. Although human evaluators marked 86.02% of our question-answer pairs as correct, the best model achieves only 47.74% ac-curacy. In addition, AGQA introduces multiple training/test splits to test for various reasoning abilities, including gen-eralization to novel compositions, to indirect references, and to more compositional steps. Using AGQA, we eval-uate modern visual reasoning systems, demonstrating that the best models barely perform better than non-visual base-lines exploiting linguistic biases and that none of the exist-ing models generalize to novel compositions unseen during training. 1.

Introduction
People represent visual events as a composition of tem-poral actions, where each action encodes how an actor’s re-lationships with surrounding objects evolves over time [44, 46, 30, 37]. For instance, people can encode the video in
Figure 1 as a set of actions like putting a phone down and holding a bottle. The action holding a bottle can be further decomposed into how the actor’s relationship with the bot-tle evolves – initially the actor may be twisting the bottle and then later shift to holding it. This ability to decompose
Figure 1. We introduce AGQA: a new benchmark to test for com-positional spatio-temporal reasoning. AGQA contains a balanced 3.9M and an unbalanced 192M question answer pairs associated with 9.6K videos. We design handcrafted programs that operate over spatio-temporal scene graphs to generate questions. These questions explicitly test how well models generalize to novel com-positions unseen during training, to indirect references of con-cepts, and to more compositional steps. events is reﬂected in the language people use to communi-cate with one another [8, 42], so tasks involving both vision and language comprehension, such as answering questions about visual input, can test models’ compositional reason-ing capability. We can ask questions like “What did the per-son hold after putting a phone down?” and expect a model capable of compositional spatio-temporal reasoning to an-swer “bottle.” While such behavior seems fundamental to developing vision models that can reason over events, the vision community has only developed compositional ques-tion answering benchmarks using static images [17] or syn-thetic worlds [31, 56] which either are not spatio-temporal or do not reﬂect the diversity of real-world events.
Although questions and visual events are composed of multiple reasoning steps, existing video question answer-ing benchmarks conﬂate multiple sources of model errors 11287
Table 1. AGQA is 3 orders of magnitude larger than all existing VideoQA benchmarks. It contains real-world videos and compositional open-answer questions with action, object, and relationship grounding. AGQA’s questions focus on visual comprehension and do not require common sense or dialogue understanding.
Dataset
MarioQA [43]
CLEVRER [56]
Pororo-QA [27]
MovieQA [48]
SocialIQ [59]
TVQA [33]
TVQA+ [34]
MovieFIB[40]
TGIF-QA [18]
MSVD-QA [53]
Video-QA [61]
MSRVTT-QA [53]
ActivityNet-QA [58]
AGQA
Avg. length (s) 3-6 5 1.4 202.7 99 76.2 7.2 4.9 3.1
<10 45 10-30 180 30
Video
# videos (K) 188 20 16.1 6.77 1.25 21.8 4.2 118.5 71.7 1.97 18.1 10 5.8 9.6
Question answers
Real-world
Not dialogue related
X
X
Open answer
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Compositional
X
X
X
X
# questions 188K 282K 9K 6.4K 7.5K 152.5K 29.4K 349K 165.2K 50.5K 175K 243K 58K 192M objects
Grounding relationships actions
X
X
X
X
X
X
X
X
X
X into a single accuracy metric [18, 53, 40, 61, 58]. Consider this stereotypical question-answer pair, Q: “ What does the bear on the right do after sitting?” A: “stand up” [18]. A model’s inability to answer such questions does not afford any deeper insights into the model’s capabilities. Did the model fail because it is unable to identify objects like bear or relationships like sitting or does it fail to reason over the temporal ordering implied by the word after? Or did the model fail for a combination of these reasons?
Not only are failure cases difﬁcult to analyze, but the inputs where the model correctly guesses the answer are equally difﬁcult to dissect. Due to biases in answer distri-butions and the non-uniform distribution of occurrences of visual events, models may develop “cheating” approaches that can superﬁcially guess answers without learning the underlying compositional reasoning process [36, 54]. To ef-fectively measure how well models jointly compose spatio-temporal reasoning over objects, their relationships, and temporal actions, we need newer benchmarks with more granular control over question composition and the distri-bution of concepts in questions and answers.
To measure whether models exhibit compositional spatio-temporal reasoning, we introduce Action Genome
Question Answering (AGQA)1. AGQA presents a bench-mark of 3.9M balanced and 192M unbalanced question an-swer pairs associated with 9.6K videos. We validate the ac-curacy of the questions and answers in AGQA using human annotators for at least 50 questions per category and ﬁnd that annotators agree with 86.02% of our answers. Each question is generated by a handcrafted program that outlines the necessary reasoning steps required to answer a question.
The programs that create questions operate over Charades’ action annotations and Action Genome’s spatio-temporal scene graphs, which ground all objects with bounding boxes and actions with time stamps in the video [19, 45]. These programs also provide us with granular control over which reasoning abilities are required to answer each question. 1Project page: https://tinyurl.com/agqavideo
For example, some questions in AGQA only require under-standing the temporal ordering of actions (e.g. “Did they take a picture before or after they did the longest action?”) while some others require understanding actions in tandem with relationships (e.g. “What did the person hold after putting a phone somewhere?”). We control bias using re-jection sampling on skewed answer distributions and across families of different compositional structures.
With our granular control over the question genera-tion process, we also introduce a set of new training/test splits that test for particular forms of compositional spatio-temporal desiderata: generalization to novel compositions, to indirect references, and to more compositional steps. We test whether models (PSAC, HME, and HRCN [11, 32, 35]) generalize to novel compositions unseen during training — the training set can contain the relationship twist and the object bottle separately while the test set requires reason-ing over questions such as “Did the person twist the bottle after taking a picture?” with both concepts paired together in a novel composition. Similarly, we test whether models generalize to indirect references of objects by replacing ob-jects like bottle in “Did the person twist the bottle?” with an indirect reference to make the question “Did the person twist the object they were holding last?” Finally, we test whether models generalize to questions with more reason-ing steps by constraining the test set to questions with more reasoning steps than those in the training set (e.g. “What did they touch last before holding the bottle but after taking a picture, a phone or a bottle?”).
Using AGQA, we evaluate modern visual reasoning sys-tems (PSAC, HME, and HRCN [11, 32, 35]), and ﬁnd that they barely perform better than models that purely exploit linguistic bias. The highest performing model achieves only 47.74% accuracy, and HCRN performs only 0.42% better than a linguistic-only version. While there is some evidence that models generalize to indirect references, all of them de-crease in accuracy when the number of compositional steps increase and none of them generalize to novel compositions. 11288
2.