Abstract 3D object detection is vital for many robotics applica-tions. For tasks where a 2D perspective range image exists, we propose to learn a 3D representation directly from this range image view. To this end, we designed a 2D convo-lutional network architecture that carries the 3D spherical coordinates of each pixel throughout the network. Its layers can consume any arbitrary convolution kernel in place of the default inner product kernel and exploit the underlying local geometry around each pixel. We outline four such kernels: a dense kernel according to the bag-of-words paradigm, and three graph kernels inspired by recent graph neural network advances: the Transformer, the PointNet, and the Edge Con-volution. We also explore cross-modality fusion with the camera image, facilitated by operating in the perspective range image view. Our method performs competitively on the Waymo Open Dataset and improves the state-of-the-art
AP for pedestrian detection from 69.7% to 75.5%. It is also efﬁcient in that our smallest model, which still outperforms the popular PointPillars in quality, requires 180 times fewer
FLOPS and model parameters. 1.

Introduction
Deep-learning-based point cloud understanding has in-creased in popularity in recent years. Numerous architec-tures [9, 11, 14, 19, 17, 22, 21, 28, 30, 33] have been pro-posed to handle the sparse nature of point clouds, with successful applications ranging from 3D object recognition
[4, 25, 29], to indoor scene understanding [6, 23] and au-tonomous driving [2, 8, 24].
Point clouds may have different properties based on the way they are acquired. For example, point clouds for 3D object recognition are often generated by taking one or many depth images from multiple views around a single object. In other applications such as robotics and autonomous driving, a device such as a LiDAR continuously scans its surround-ings in a rotating pattern, producing a 2D scan pattern called the range image. Each pixel in this image contains a range value and other features, such as each laser return’s intensity.
The operating range of these sensors has signiﬁcantly improved over the past few years. As a result, state-of-the-art methods [11, 21, 30, 33] that require projecting points into a dense 3D grid have become less efﬁcient as their complexity scales quadratically with the range. In this work, we propose a new point cloud representation that directly operates on the perspective 2D range image without ever projecting the pixels to the 3D world coordinates. Therefore, it does not suffer from the efﬁciency scaling problem as mentioned earlier. We coin this new representation perspective point cloud, or PPC for short. We are not the ﬁrst to attempt to do so. [12, 14] have proposed a similar idea by applying a convolutional neural network to the range image. However, they showed that these models, despite being more efﬁcient, are not as powerful as their 3D counterparts, i.e. 3D grid methods [9, 11, 21, 30, 33] and 3D graph methods [19, 22].
We believe that this quality difference traces its root to the traditional 2D convolution layers that cannot easily exploit the range image’s underlying 3D structure. To counter this deﬁciency, we propose four alternative kernels (Fig. 1: c, d) that can replace the scalar product kernel at the heart of the 2D convolution. These kernels inject much needed 3D information to the perspective model, and are inspired by recent advances in graph operations, including transformers
[26], PointNet [18] and Edge Convolutions [28].
We summarize the contributions of this paper as follows: 1) We propose a perspective range-image-based 3D model which allows the core of the 2D convolution operation to harness the underlying 3D structure; 2) We validate our model on the 3D detection problem and show that the re-sulting model sets a new state-of-the-art for pedestrians on the Waymo Open Dataset, while also matching the SOTA on vehicles; 3) We provide a detailed complexity/model-size-16000
3D Detector 3D Detector 3D Detector
Output Feature
Voxel
Point
Pixel 2D TD / 3D
Conv
... 2D TD / 3D
Conv
Voxelization
Sparse
Conv
...
Sparse
Conv
Perspective
Point Set
Aggr.
...
Perspective
Point Set
Aggr.
Perspective
Point Set
Aggr. 2D Conv
Range-quantized 2D Conv
Self-Attention
Kernel
PointNet
Kernel
EdgeConv
Kernel
W e g n a r n i e z i t n a u
Q
W
Q • K
Choose one
Asymmetric positional encoding
Range Image
Range Image
Range Image
Perspective Point-Set
Aggregation Layer a) b) c) d)
Choose one
Input Feature
Figure 1: Overview of existing 3D detectors and our proposed perspective point cloud representation. a) 3D grid-based methods [9, 11, 21, 33]
ﬁrst voxelizes the 3D space, feeds the 3D dense structure to a 3D convolution network or a 2D top-down network, and make the ﬁnal prediction based on 3D voxels. b) 3D graph models [19, 22] builds a graph neural network on top of the sparse point cloud and makes predictions based on points. c) Our method, PPC, operates directly on the perspective range image view and predicts from pixels. d) It utilizes a set of specialized 2D convolution layers in the perspective 2D view. We propose four improved kernels in addition to the traditional inner product kernel (2D conv). vs.-accuracy analysis, and show that we can maintain the efﬁciency beneﬁts from operating on the 2D range image.
Our smallest model with only 24k parameters has higher accuracy than the popular PointPillars [11] model with over 4M parameters. 2.