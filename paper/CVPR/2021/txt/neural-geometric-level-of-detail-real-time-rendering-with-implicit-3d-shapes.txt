Abstract
Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, ﬁxed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics.
We introduce an efﬁcient neural representation that, for the ﬁrst time, enables real-time rendering of high-ﬁdelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively
ﬁts shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efﬁcient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efﬁcient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics. 1.

Introduction
Advanced geometric modeling and rendering techniques in computer graphics use 3D shapes with complex details, arbitrary topology, and quality, usually leveraging polygon meshes. However, it is non-trivial to adapt those represen-tations to learning-based approaches since they lack differ-entiability, and thus cannot easily be used in computer vi-sion applications such as learned image-based 3D recon-struction. Recently, neural approximations of signed dis-tance functions (neural SDFs) have emerged as an attrac-*Authors contributed equally.
Figure 1: Levels of Detail. Our representation pools features from multiple scales to adaptively reconstruct high-ﬁdelity geometry with continuous level of detail (LOD). The subﬁgures show sur-faces (blue) at varying LODs, superimposed on the corresponding coarse, sparse octrees (orange) which contain the features of the learned signed distance functions. These were directly rendered in real-time using our efﬁcient sparse sphere tracing algorithm. tive choice to scale up computer vision and graphics appli-cations. Prior works [36, 31, 6, 8] have shown that neural networks can encode accurate 3D geometry without restric-tions on topology or resolution by learning the SDF, which deﬁnes a surface by its zero level-set. These works com-monly use a large, ﬁxed-size multi-layer perceptron (MLP) as the learned distance function.
Directly rendering and probing neural SDFs typically re-lies on sphere tracing [17], a root-ﬁnding algorithm that can 11358
Figure 2: We are able to ﬁt shapes of varying complexity, style, scale, with consistently good quality, while being able to leverage the geometry for shading, ambient occlusion [11], and even shadows with secondary rays. Best viewed zoomed in. require hundreds of SDF evaluations per pixel to converge.
As a single forward pass through a large MLP-based SDF can require millions of operations, neural SDFs quickly be-come impractical for real-time graphics applications as the cost of computing a single pixel inﬂates to hundreds of mil-lions of operations. Works such as Davies et al. [8] circum-vent this issue by using a small neural network to overﬁt single shapes, but this comes at the cost of generality and reconstruction quality. Previous approaches also use ﬁxed-size neural networks, making them unable to express geom-etry with complexity exceeding the capacity of the network.
In this paper, we present a novel representation for neu-ral SDFs that can adaptively scale to different levels of de-tail (LODs) and reconstruct highly detailed geometry. Our method can smoothly interpolate between different scales of geometry (see Figure 1) and can be rendered in real-time with a reasonable memory footprint. Similar to Davies et al. [8], we also use a small MLP to make sphere tracing practical, but without sacriﬁcing quality or generality.
We take inspiration from classic surface extraction mechanisms [26, 12] which use quadrature and spatial data structures storing distance values to ﬁnely discretize the Eu-clidean space such that simple, linear basis functions can re-construct the geometry. In such works, the resolution or tree depth determines the geometric level of detail (LOD) and different LODs can be blended with interpolation. How-ever, they usually require high tree depths to recreate a so-lution with satisfying quality.
In contrast, we discretize the space by using a sparse voxel octree (SVO) and we store learned feature vectors in-stead of signed distance values. These vectors can be de-coded into scalar distances using a shallow MLP, allowing us to truncate the tree depth while inheriting the advantages of classic approaches (e.g., LOD). We additionally develop a ray traversal algorithm tailored to our architecture, which allows us to render geometry close to 100× faster than
DeepSDF [36]. Although direct comparisons with neural volumetric rendering methods are not possible, we report frametimes over 500× faster than NeRF [32] and 50× faster than NSVF [24] in similar experimental settings.
In summary, our contributions are as follows:
• We introduce the ﬁrst real-time rendering approach for complex geometry with neural SDFs.
• We propose a neural SDF representation that can ef-ﬁciently capture multiple LODs, and reconstruct 3D geometry with state-of-the-art quality (see Figure 2).
• We show that our architecture can represent 3D shapes in a compressed format with higher visual ﬁdelity than traditional methods, and generalizes across different geometries even from a single learned example.
Due to the real-time nature of our approach, we en-vision this as a modular building block for many down-stream applications, such as scene reconstruction from im-ages, robotics navigation, and shape analysis. 2.