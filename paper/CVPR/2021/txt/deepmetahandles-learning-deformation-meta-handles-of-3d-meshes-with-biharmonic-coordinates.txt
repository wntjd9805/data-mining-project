Abstract
We propose DeepMetaHandles, a 3D conditional gen-erative model based on mesh deformation. Given a col-lection of 3D meshes of a category and their deformation handles (control points), our method learns a set of meta-handles for each shape, which are represented as combina-tions of the given handles. The disentangled meta-handles factorize all the plausible deformations of the shape, while each of them corresponds to an intuitive deformation. A new deformation can then be generated by sampling the co-efﬁcients of the meta-handles in a speciﬁc range. We em-ploy biharmonic coordinates as the deformation function, which can smoothly propagate the control points’ transla-tions to the entire mesh. To avoid learning zero deforma-tion as meta-handles, we incorporate a target-ﬁtting mod-ule which deforms the input mesh to match a random tar-get. To enhance deformations’ plausibility, we employ a soft-rasterizer-based discriminator that projects the meshes to a 2D space. Our experiments demonstrate the superiority of the generated deformations as well as the interpretabil-ity and consistency of the learned meta-handles. The code is available at https://github.com/Colin97/
DeepMetaHandles. 1.

Introduction 3D Meshes can store sharp edges and smooth surfaces compactly. However, Learning to generate 3D meshes is much more challenging than 2D images due to the irregu-larity of mesh data structures and the difﬁculty in designing loss functions to measure geometrical and topological prop-erties. For such reasons, to create new meshes, instead of generating a mesh from scratch, recent work assumes that the connectivity structure of geometries is known so that the creation space is restricted to changing the geometry with-out altering the structure. For example, [37, 36, 48] create new shapes by deformations of one template mesh. They, however, limit the scope of the shape generation to possible variants of the template mesh. We thus propose a 3D condi-tional generative model that can take any existing mesh as input and produce its plausible variants. Our approach in-tegrates a target-driven ﬁtting component and a conditional generative model. At test time, it allows both deforming the input shape to ﬁt the given target shape and exploring plausible variants of the input shape without a target.
Our main design goals are two-fold: improving the plau-sibility of the output shapes and enhancing the interpretabil-ity of the learned latent spaces. To achieve the goals, the key is to choose a suitable parameterization of deformations.
One option is to follow the recent target-driven deformation network [39, 9, 46, 35], which parameterizes the deforma-tion as new positions of all the mesh vertices. However, such a large degree of freedom often results in the loss of
ﬁne-grained geometric details and tends to cause undesir-able distortions. Instead of following the above works, we leverage a classical idea in computational geometry, named 12
deformation handles, to parameterize smooth deformations with a low degree of freedom. Speciﬁcally, we propose to take a small set of control points as deformation handles and utilize a deformation function deﬁned on the control points and their biharmonic coordinates [41].
Not all the translations of the control points lead to plau-sible deformations. Based on the control-point handles, we aim to learn a low-dimensional deformation subspace for each shape, and we expect the structure of this subspace to exhibit interpretability.
In contrast to typical genera-tive models, where shape variations are embedded into a la-tent space implicitly, our method explicitly factorizes all the plausible deformations of a shape with a small number of interpretable deformation functions. Speciﬁcally, for each axis of our input-dependent latent space, we assign a defor-mation function deﬁned with the given set of control points and offset vectors on them so that each axis corresponds to an intuitive deformation direction. Since each axis is ex-plicitly linked to multiple control-point handles, we thus call them meta-handles. We enforce the network to learn disentangled meta-handles, in the sense that a meta-handle should not only leverage the correlations of the control-point handles, but also correspond to a group of parts that tend to deform altogether according to the dataset. We hope that the disentangled meta-handles allow us to deform each part group independently in downstream applications.
Beyond choosing the parameterization of deformations, we have to overcome the challenge of examining the plau-sibility.
In the popular adversarial learning framework, a straightforward approach would be converting the output mesh to voxels or point clouds and exploiting voxel or point cloud based discriminators. The conversions, however, may discard some important geometric details. In our method, we instead project the shapes into a 2D space with a differ-entiable soft rasterizer [25] and employ a 2D discriminator.
We found that this architecture can be trained more robustly, and it captures local details of plausible shapes.
Our deformation-based conditional generative model, named DeepMetaHandles, takes random pairs of source and target shapes as input during training. For the source shape, the control points are sampled from its mesh ver-tices by farthest point sampling, and the biharmonic coorid-nates [41] for control-point handles are pre-computed. Our network consists of two main modules: MetaHandleNet and DeformNet. The MetaHandleNet ﬁrst predicts a set of meta-handles for the source shape, where each meta-handle is represented as a combination of control-point offsets. A deformation range is also predicted for each meta-handle, describing the scope of plausible deformations along that direction. The learned meta-handles, together with the cor-responding ranges, deﬁne a deformation subspace for the source shape. Then, DeformNet predicts coefﬁcients multi-plied to the meta-handles, within the predicted ranges, so that the source shape deformed with the coefﬁcients can match the target shape. To ensure the plausibility of varia-tions within the learned subspace, we then randomly sample coefﬁcients within the predicted ranges and apply both geo-metric and adversarial regularizations to the corresponding deformations.
Fig. 1 shows examples of the learned meta-handles, which interestingly resemble natural deformations of se-mantic parts, such as lifting the armrests or bending the back of a chair. Our experiments also show that the learned meta-handles are consistent across various shapes and well disentangle the shape variation space. Finally, we compare our approach with other target-driven deformation tech-niques [13, 39, 9, 46] and demonstrate that our method pro-duces superior ﬁtting results.
Key contributions:
• We propose DeepMetaHandles, a 3D conditional genera-tive model based on mesh deformation.
• We employ a few control points as deformation handles.
Together with their biharmonic coordinates, we can pro-duce smooth but ﬂexible enough deformations.
• We propose to factorize the deformation space with a small number of disentangled meta-handles, each of which provides an intuitive deformation by leveraging the correlations between the control points.
• We improve the plausibility of the deformations by ex-ploiting a differentiable renderer and a 2D discriminator. 2.