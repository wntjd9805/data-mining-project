Abstract
This paper strives for repetitive activity counting in videos. Different from existing works, which all analyze the visual video content only, we incorporate for the ﬁrst time the corresponding sound into the repetition counting process.
This beneﬁts accuracy in challenging vision conditions such as occlusion, dramatic camera view changes, low resolution, etc. We propose a model that starts with analyzing the sight and sound streams separately. Then an audiovisual temporal stride decision module and a reliability estimation module are introduced to exploit cross-modal temporal interaction.
For learning and evaluation, an existing dataset is repur-posed and reorganized to allow for repetition counting with sight and sound. We also introduce a variant of this dataset for repetition counting under challenging vision conditions.
Experiments demonstrate the beneﬁt of sound, as well as the other introduced modules, for repetition counting. Our sight-only model already outperforms the state-of-the-art by itself, when we add sound, results improve notably, especially under harsh vision conditions. The code and datasets are available at https://github.com/xiaobai1217/
RepetitionCounting. 1.

Introduction
Sight
Sound
Convolutional 
Network
Count
Count
Convolutional 
Network
Reliable?
✓ 2 5
Figure 1: From sight and sound, as well as their cross-modal in-teraction, we predict the number of repetitions for an (unknown) activity happening in a video. This is especially beneﬁcial in chal-lenging vision conditions with occlusions and low illumination.
The goal of this paper is to count in video the repetitions of (unknown) activities, like bouncing on a trampoline, slic-ing an onion or playing ping pong. The computer vision solutions to this challenging problem have a long tradition.
Early work emphasized on repetitive motion estimation by
Fourier analysis, e.g., [4, 10, 24, 36], and more recently by a continuous wavelet transform [28, 29]. State-of-the-art solu-tions rely on convolutional neural networks [11, 21, 42] and large-scale count-annotated datasets [11, 42] to learn to pre-dict the number of repetitions in a video. Albeit successful, all existing works focus exclusively on the visual modality, and could fail in poor sight conditions such as low illumi-nation, occlusion, camera view changes, etc. Different from existing works, we propose in this paper the ﬁrst repetitive activity counting method based on sight and sound.
Analyzing sound has recently proven advantageous in a variety of computer vision challenges, such as representation learning by audio-visual synchronization [1, 3, 19, 23], video captioning [25, 34, 39], sound source localization [27, 30], to name a few. Correspondingly, several mechanisms for fusing both modalities have been introduced. In works for action recognition by previewing the audio track [20] and talking-face generation [31, 38], the audio network usually works independently and the predictions guide the inference process of the visual counterpart. In contrast, feature multipli-cation and concatenation operations, as well as cross-modal attention mechanisms, are widely adopted for fusion in tasks like audio-visual synchronization [2, 19, 27] and video cap-tioning [25, 34, 39]. We also combine sight and sound, but observe that for some activities, like playing ping pong, hu-mans can count the number of repetitions by just listening.
This gives us an incentive that sound could be an important 14070
cue by itself. Hence, an intelligent repetition counting system should be able to judge when the sight condition is poor and therefore utilize complementary information from sound.
The ﬁrst and foremost contribution of this paper is ad-dressing video repetition estimation from a new perspective based on not only the sight but also the sound signals. As a second contribution, we propose an audiovisual model with a sight and a sound stream, where each stream facilitates each modality to predict the number of repetitions. As the repetition cycle lengths may vary in different videos, we further propose a temporal stride decision module to select the best sample rate for each video based on both visual and audio features. Our reliability estimation module ﬁnally exploits cross-modal temporal interaction to decide which modality-speciﬁc prediction is more reliable. Since existing works focus on visual repetition counting only, our third con-tribution entails two sight and sound datasets that we derive from Countix [11] and VGGsound [8]. One of our datasets is for supervised learning and evaluation and the other for assessing audiovisual counting in various challenging vision conditions. Finally, our experiments demonstrate the beneﬁt of sound, as well as the other introduced network modules, for repetition counting. Our sight-only model already outper-forms the state-of-the-art by itself, and when we add sound, the results improve further, especially under harsh vision conditions. Before detailing our model, as summarized in
Figure 1, we ﬁrst discuss related work. 2.