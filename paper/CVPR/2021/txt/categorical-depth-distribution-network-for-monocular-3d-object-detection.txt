Abstract
Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with sim-ple conﬁguration compared to typical multi-sensor systems.
The main challenge in monocular 3D detection lies in accu-rately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range mea-surement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categor-ical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth in-terval in 3D space. We then use the computationally efﬁ-cient bird’s-eye-view projection and single-stage detector to produce the ﬁnal output detections. We design CaDDN as a fully differentiable end-to-end approach for joint depth es-timation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1st among published monocular methods. We also provide the ﬁrst monocular 3D detection results on the newly re-leased Waymo Open Dataset. We provide a code release for
CaDDN which is made available. 1.

Introduction
Perception in 3D space is a key component in ﬁelds such as autonomous vehicles and robotics, enabling systems to understand their environment and react accordingly. Li-DAR [21, 50, 51] and stereo [46, 45, 28, 11] sensors have a long history of use for 3D perception tasks, showing ex-cellent results on 3D object detection benchmarks such as the KITTI 3D object detection benchmark [16] due to their ability to generate precise 3D measurements.
Monocular based 3D perception has been pursued simul-taneously, motivated by the potential for a low-cost, easy-to-deploy solution with a single camera [9, 40, 5, 22]. Per-formance on the same 3D object detection benchmarks lags signiﬁcantly relative to LiDAR and stereo methods, due to the loss of depth information when scene information is pro-jected onto the image plane.
Figure 1. (a) Input image. (b) Without depth distribution super-vision, BEV features from CaDDN suffer from smearing effects. (c) Depth distribution supervision encourages BEV features from
CaDDN to encode meaningful depth conﬁdence, in which objects can be accurately detected.
To combat this effect, monocular object detection meth-ods [13, 36, 37, 59] often learn depth explicitly, by train-ing a monocular depth estimation network in a separate stage. However, depth estimates are consumed directly in the 3D object detection stage without an understanding of depth conﬁdence, leading to networks that tend to be over-conﬁdent in depth predictions. Over-conﬁdence in depth is particularly an issue at long range [59], leading to poor lo-calization. Further, depth estimation is separated from 3D detection during the training phase, preventing depth map estimates from being optimized for the detection task.
Depth information in image data can also be learned im-plicitly, by directly transforming features from images to 3D space and ﬁnally to bird’s-eye-view (BEV) grids [48,
Implicit methods, however, tend to suffer from fea-44]. ture smearing, wherein similar image features can exist at multiple locations in the projected space. Feature smearing increases the difﬁculty of localizing objects in the scene.
To resolve the identiﬁed issues, we propose a monocular 3D object detection method, CaDDN, that enables accurate 3D detection by learning categorical depth distributions. By 8555
leveraging probabilistic depth estimation, CaDDN is able to generate high quality bird’s-eye-view feature representa-tions from images in an end-to-end fashion. We summarize our approach with three contributions. (1) Categorical Depth Distributions. In order to perform 3D detection, we predict pixel-wise categorical depth distri-butions to accurately locate image information in 3D space.
Each predicted distribution describes the probabilities that a pixel belongs to a set of predeﬁned depth bins. We en-courage our distributions to be as sharp as possible around the correct depth bins, in order to encourage our network to focus more on image information where depth estima-tion is both accurate and conﬁdent [23]. By doing so, our network is able to produce sharper and more accurate fea-tures that are useful for 3D detection (see Figure 1). On the other hand, our network retains the ability to produce less sharp distributions when depth estimation conﬁdence is low. Using categorical distributions allows our feature en-coding to capture the inherent depth estimation uncertainty to reduce the impact of erroneous depth estimates, a prop-erty shown to be key to CaDDN’s improved performance in
Section 4.3. Sharpness in our predicted depth distributions is encouraged through supervision with one-hot encodings of the correct depth bin, which can be generated by project-ing LiDAR depth data into the camera frame. (2) End-To-End Depth Reasoning. We learn depth dis-tributions in an end-to-end fashion, jointly optimizing for accurate depth prediction as well as accurate 3D object de-tection. We argue that joint depth estimation and 3D detec-tion reasoning encourages depth estimates to be optimized for the 3D detection task, leading to increased performance as shown in Section 4.3. (3) BEV Scene Representation. We introduce a novel method to generate high quality bird’s-eye-view scene rep-resentations from single images using categorical depth dis-tributions and projective geometry. We select the bird’s-eye-view representation due to its ability to produce excel-lent 3D detection performance with high computational ef-ﬁciency [26]. The generated bird’s-eye-view representation is used as input to a bird’s-eye-view based detector to pro-duce the ﬁnal output.
CaDDN is shown to rank ﬁrst among all previously pub-lished monocular methods on the Car and Pedestrian cate-gories of the KITTI 3D object detection test benchmark [1], with margins of 1.69% and 1.46% AP|R40 respectively. We are the ﬁrst to report monocular 3D object detection results on the Waymo Open Dataset [56]. 2.