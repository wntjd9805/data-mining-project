Abstract
Video deblurring models exploit consecutive frames to remove blurs from camera shakes and object motions. In order to utilize neighboring sharp patches, typical meth-ods rely mainly on homography or optical ﬂows to spatially align neighboring blurry frames. However, such explicit ap-proaches are less effective in the presence of fast motions with large pixel displacements. In this work, we propose a novel implicit method to learn spatial correspondence among blurry frames in the feature space. To construct dis-tant pixel correspondences, our model builds a correlation volume pyramid among all the pixel-pairs between neigh-boring frames. To enhance the features of the reference frame, we design a correlative aggregation module that maximizes the pixel-pair correlations with its neighbors based on the volume pyramid. Finally, we feed the aggre-gated features into a reconstruction module to obtain the re-stored frame. We design a generative adversarial paradigm to optimize the model progressively. Our proposed method is evaluated on the widely-adopted DVD dataset, along with
∗ Authors contributed equally.
† Corresponding author: kaihao.zhang@anu.edu.au. a newly collected High-Frame-Rate (1000 fps) Dataset for
Video Deblurring (HFR-DVD). Quantitative and qualita-tive experiments show that our model performs favorably on both datasets against previous state-of-the-art methods, conﬁrming the beneﬁt of modeling all-range spatial corre-spondence for video deblurring. 1.

Introduction
Hand-held devices are popular in capturing videos of dynamic scenes, where prevalent high-speed object move-ments and abrupt camera shakes lead to undesirable blurs in videos. To this end, the task of video deblurring aims to improve the video quality by restoring sharp frames from blurry video sequences and beneﬁts a wide range of vision applications [18, 14, 13, 20, 19, 21, 53, 57].
Video deblurring methods remove blurs by taking advan-tage of sharper scene patches from neighboring frames [23, 3]. However, since neighboring frames are usually not spa-tially aligned with the reference frame due to motions, it is non-trivial to construct visual correspondence between frames. Existing works mainly tackle the misalignment be-tween frames using homography or optical ﬂows. Some 7721
previous works [36, 7, 5, 38, 45, 26] ﬁrst estimate the rel-ative motions using optical ﬂows, then warp neighboring frames to the reference frame. However, in occurrence of fast object motions with large displacements, the constraint on the velocity smoothness may not always hold. As a re-sult, estimating optical ﬂow from blurry images remains a challenging research problem by itself [33, 39].
In addi-tion, these methods are also notoriously less effective in the presence of occlusions and severe depth variations. Re-cent works [9, 56] use dynamic ﬁlters to restore videos in the feature domain. However, their ability to address large pixel displacement is restricted by the local receptive ﬁelds.
Therefore, it remains a challenge to design a ﬂexible yet ef-fective visual correspondence method with abrupt motions.
To tackle this problem, as a complement to the existing pixel based warping methods, we propose an implicit ap-proach that estimates the pairwise image correspondence in the feature space. In particular, we ﬁrst extract visual fea-tures from a sequence of warped consecutive frames. Then, we pair up the reference frame and its neighboring frame, and compute their pixel-wise correlations by matching their features in the embedding space. Speciﬁcally, in order to account for distant pixel correspondence, we compute cor-relations for pixel pairs in all the spatial range. In this way, we enable the model to better capture the visual dependen-cies at various lengths, thus being more effective in handling blurs to fast motions.
To further increase the receptive ﬁelds while maintain-ing the ﬁne-grained visual details, we propose to build a pyramid of correlation volumes in order to achieve feature matching at different spatial scales. Particularly, rather than subsampling all the frames to the same scale as in typical feature pyramid modules [6], we keep the reference frame feature maps while subsampling only those of neighboring frames. By maintaining the spatial scale of the reference frame, we keep high resolution information, allowing the model to learn correspondence taking into account the ﬁne-grained visual details.
We optimize our model progressively in a generative ad-versarial paradigm with a new temporal consistency loss.
The training proceeds in stages, where later stages take as input the restored frames from the previous stage. In this way, we ease the optimization by allowing the model to re-store details gradually. We propose an adversarial loss to encourage the temporal consistency between the restored frames. Distinct from the previous adversarial video de-blurring method [51] that takes as input a single restored frame, we use a discriminator to distinguish between re-stored frame sequence and the ground-truth sharp frame se-quence. We show the two training strategies combined im-prove the restoration quality quantitatively and visually.
To validate the effectiveness of the proposed method, we ﬁrst conduct evaluations on the widely-adopted DVD dataset [36]. In addition, since the DVD dataset uses auto-matic frame interpolation to increase the frame rate, it ex-hibits artefacts in the synthetic blurry frames. To this end, we contribute a new large-scale dataset for video deblur-ring research, called HFR-DVD. The HFR-DVD dataset is captured using a high-speed camera in 1,000 fps, featuring sharper frames and more realistic motion blurs. Our exper-iments show that the proposed feature correlation methods help to improve the restoration on both datasets.
Our contributions are summarized as follows:
• we introduce a novel video deblurring method by con-structing spatial correspondence between pixel pairs in the feature space. To account for distant pixel displace-ments, we match pixel pairs in all the spatial range be-tween the reference frame and neighboring frames;
• we propose a correlative aggregation module to en-hance the reference frame feature based on the corre-lation volume pyramid;
• in order to encourage the temporal consistency in the restored frames, we develop a adversarial loss for op-timizing the model;
• we benchmark existing video deblurring methods on a new large-scale high-frame-rate dataset HFR-DVD with sharper frames and more realistic blurs;
• our video deblurring model ARVo outperforms pre-vious methods quantitatively and qualitatively on the
DVD and HFR-DVD datasets, establishing a new state-of-the-art for video deblurring. 2.