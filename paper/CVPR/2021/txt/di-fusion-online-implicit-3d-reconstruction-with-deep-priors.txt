Abstract
Previous online 3D dense reconstruction methods strug-gle to achieve the balance between memory storage and sur-face quality, largely due to the usage of stagnant underlying geometry representation, such as TSDF (truncated signed distance functions) or surfels, without any knowledge of the scene priors.
In this paper, we present DI-Fusion (Deep
Implicit Fusion), based on a novel 3D representation, i.e.
Probabilistic Local Implicit Voxels (PLIVoxs), for online 3D reconstruction with a commodity RGB-D camera. Our
PLIVox encodes scene priors considering both the local geometry and uncertainty parameterized by a deep neu-ral network. With such deep priors, we are able to per-form online implicit 3D reconstruction achieving state-of-the-art camera trajectory estimation accuracy and mapping quality, while achieving better storage efﬁciency compared with previous online 3D reconstruction approaches. Our implementation is available at https://www.github. com/huangjh-pub/di-fusion. 1.

Introduction
Online 3D dense reconstruction has made great progress in the past ten years [3, 6, 12, 25, 38, 39, 51], enabling a wide range of applications including augmented reality, robotic navigation and games. Technically most of the pre-vious depth fusion approaches focus on the globally con-sistent 3D reconstruction with bundle adjustment [3, 12] or loop closure [51] techniques. However, the underlying representation for the 3D scene itself has seldom changed ever since the success of VoxelHashing [39] with Signed
Distance Function (SDF) integration on a sparse set of vox-els [10]. This leads to the drawback of previous depth fusion systems that often costs a huge amount of memory storage even for moderate-sized 3D scenes. Besides, the geometric quality could be unsatisfactory with non-complete regions or objects [13] due to the uncertainties caused by sensor noise or scan ambiguities such as view occlusions.
On the other hand, recent efforts from the deep geome-try learning community have demonstrated the power of im-*corresponding author.
Figure 1. DI-Fusion incrementally builds up a continuous 3D scene from an RGB-D sequence. The tracking and mapping al-gorithm are fully based on our novel local deep implicit scene rep-resentation incorporating learned priors, where both the geometry and its uncertainty are estimated. plicit geometric representation parameterized by neural net-works [8, 34, 40]. By representing the geometry as a con-tinuous implicit function, the underlying shape can be ex-tracted at arbitrary resolution, which introduces more ﬂexi-bilities. These methods are also efﬁcient since the network structure used to regress implicit function only consists of simple fully connected layers. Another important feature of such deep implicit representation is the capability to encode geometric priors, which enables many applications such as shape interpolation or reconstruction [40]. This capability can be generalized to scene-level by decomposing and en-coding the implicit ﬁelds in local voxels, leading to high-quality reconstruction agnostic to semantics [4, 24].
The power of such deep implicit representation moti-vates us to incorporate it into online 3D dense reconstruc-tion systems. By encoding meaningful scene priors with a continuous function, we can achieve improved surface re-construction as well as accurate camera trajectory. How-ever, several challenges need to be overcome before this new representation can be successfully applied in an online fusion scenario: (1) geometric uncertainty need to be ex-plicitly modeled against sensor noise or view occlusion, (2) an accurate camera tracking formulation based on such an 8932
implicit representation, which is essential for depth fusion, remains unknown yet, and (3) an efﬁcient surface mapping strategy that incrementally integrates new observations di-rectly is also missing.
We hence respond with DI-Fusion, the ﬁrst online 3D reconstruction system with tracking and mapping modules fully supported by deep implicit representations. To address the above challenges, we ﬁrst extend the original local im-plicit grids [4, 24] and adapt it into a novel Probabilistic
Local Implicit Voxel (PLIVox), which encodes not only scene geometry but also the uncertainty with one deep neu-ral network. We show that such an additional uncertainty encoding is extremely useful during the online depth fusion.
Based on our PLIVox representation, we devise an approx-imate gradient for solving the camera tracking problem ef-ﬁciently. Moreover, thanks to our tailored encoder-decoder network design, we are able to perform geometry integra-tion on the domain of latent vectors, achieving high quality surface mapping in an efﬁcient way. We evaluated our ap-proach on public 3D RGB-D benchmark (ICL-NUIM [20],
ScanNet dataset [11]), showing state-of-the-art or improved tracking and mapping quality compared to previous repre-sentations. We make our implementation publicly available. 2.