Abstract
Scene text recognition is a challenging task due to di-verse variations of text instances in natural scene im-ages. Conventional methods based on CNN-RNN-CTC or encoder-decoder with attention mechanism may not fully investigate stable and efﬁcient feature representations for multi-oriented scene texts. In this paper, we propose a prim-itive representation learning method that aims to exploit intrinsic representations of scene text images. We model elements in feature maps as the nodes of an undirected graph. A pooling aggregator and a weighted aggregator are proposed to learn primitive representations, which are transformed into high-level visual text representations by graph convolutional networks. A Primitive REpresentation learning Network (PREN) is constructed to use the visual text representations for parallel decoding. Furthermore, by integrating visual text representations into an encoder-decoder model with the 2D attention mechanism, we pro-pose a framework called PREN2D to alleviate the misalign-ment problem in attention-based methods. Experimental re-sults on both English and Chinese scene text recognition tasks demonstrate that PREN keeps a balance between ac-curacy and efﬁciency, while PREN2D achieves state-of-the-art performance. 1.

Introduction
In recent years, there have been increasing demands for scene text recognition in various real-world applications, such as image search, instant translation, and robot navi-gation. With the emergence of deep learning, there are two main scene text recognition frameworks. One is the CRNN framework [48, 14, 15, 44, 31, 17] that encodes images into hidden representations by CNNs and RNNs, and uses the connectionist temporal classiﬁcation (CTC) [10] for decod-ing, as shown in Fig. 1 (a). The other is the attention-based encoder-decoder framework [2, 44, 24, 7, 3, 32, 43, 54, 60, 29, 40] that can learn to align output texts with feature maps, as shown in Fig. 1 (b).
Figure 1. Illustrations of different scene text recognition frame-works. (a) CTC-based methods, where “ ” denotes the blank sym-bol; (b) attention-based methods; (c) the proposed PREN.
However, the above methods still have room for im-provement. On the one hand, for CTC-based methods, the extracted feature sequences contain redundant information that may degrade the performance on irregular text images.
On the other hand, attention-based encoder-decoder meth-ods usually suffer from the misalignment problem [7, 54], because the alignment between feature maps and texts is highly sensitive to previous decoded results, which lack global visual information. Therefore, to handle the diversity of texts in natural scenes, it is important to exploit intrinsic representations of scene text images.
In this paper, we propose a novel scene text recog-nition framework that learns primitive representations of scene text images. Inspired by graph representation learn-ing methods [22, 12, 38], we model the elements in feature maps as nodes of an undirected graph. Primitive represen-tations are learned by globally aggregating features over the coordinate space and are then projected into the visual text representation space, as shown in Fig. 1 (c).
The “primitive” representations refer to a set of base vectors that can be transformed into character-by-character vector representations in the so-called visual text represen-tation space. The visual text representations are generated 284
from original feature maps, which are different from char-acter embeddings generated from ground truth or predicted texts used in an encoder-decoder model.
For the global feature aggregation, a pooling aggregator and a weighted aggregator are proposed. For the pooling ag-gregator, each primitive representation is learned from input feature maps through two convolutions followed by a global average pooling layer. In this way, aggregating weights are shared by all samples to learn intrinsic structural informa-tion from various scene text instances. For the weighted aggregator, input feature maps are transformed into sample-speciﬁc heatmaps, which are used as aggregating weights.
Visual text representations are generated from prim-itive representations by graph convolutional networks (GCNs) [22, 6]. Each visual text representation is used to represent a character to be recognized.
A primitive representation learning network (PREN) is constructed. PREN consists of a feature extraction mod-ule that extracts multiscale feature maps from input images and a primitive representation learning module that learns primitive representations and generates visual text represen-tations. Texts are generated from visual text representations with parallel decoding.
Moreover, since visual text representations are purely learned from visual features, they can mitigate the misalign-ment problem [7, 54] of attention-based methods. We fur-ther construct a framework called PREN2D by integrating
PREN into a 2D-attention-based encoder-decoder model with a modiﬁed self-attention network.
We conduct experiments on seven public English scene text recognition datasets (IIIT5k, SVT, IC03, IC13, IC15,
SVTP, and CUTE) and a subset of the RCTW Chinese scene text dataset. Experimental results show that PREN keeps a balance between accuracy and speed, while PREN2D achieves state-of-the-art model performance.
In summary, the main contributions of the paper are as follows.
• Different from commonly used CTC-based and attention-based methods, we provide a novel scene text recognition framework by learning primitive represen-tations and forming visual text representations that can be used for parallel decoding.
• We propose a pooling aggregator and a weighted ag-gregator to learn primitive representations from fea-ture maps output by a CNN, and use GCNs to trans-form primitive representations into visual text repre-sentations.
• The proposed primitive representation learning method can be integrated into attention-based frame-works. Experimental results on both English and
Chinese scene text recognition tasks demonstrate the effectiveness and efﬁciency of our method. 2.