Abstract
Source
Animated
We propose novel motion representations for animating articulated objects consisting of distinct parts. In a com-pletely unsupervised manner, our method identiﬁes object parts, tracks them in a driving video, and infers their motions by considering their principal axes. In contrast to the previ-ous keypoint-based works, our method extracts meaningful and consistent regions, describing locations, shape, and pose. The regions correspond to semantically relevant and distinct object parts, that are more easily detected in frames of the driving video. To force decoupling of foreground from background, we model non-object related global motion with an additional afﬁne transformation. To facilitate animation and prevent the leakage of the shape of the driving object, we disentangle shape and pose of objects in the region space.
Our model1 can animate a variety of objects, surpassing pre-vious methods by a large margin on existing benchmarks. We present a challenging new benchmark with high-resolution videos and show that the improvement is particularly pro-nounced when articulated objects are considered, reaching 96.6% user preference vs. the state of the art. 1.

Introduction
Animation—bringing static objects to life—has broad applications across education and entertainment. Animated characters and objects, such as those in Fig. 1, increase the creativity and appeal of content, improve the clarity of material through storytelling, and enhance user experiences.
Until very recently, animation techniques necessary for achieving such results required a trained professional, spe-cialized hardware, software, and a great deal of effort. Qual-ity results generally still do, but vision and graphics commu-nities have attempted to address some of these limitations by training data-driven methods [36, 6, 25, 11, 10] on object classes for which prior knowledge of object shape and pose can be learned. This, however, requires ground truth pose and shape data to be available during training.
Recent works have sought to avoid the need for ground 1Our source code is publicly available at https://github.com/snap-research/articulated-animation.
Figure 1: Our method animates still source images via unsu-pervised region detection (inset). truth data through unsupervised motion transfer [39, 27, 28].
Signiﬁcant progress has been made on several key chal-lenges, including training using image reconstruction as a loss [39, 27, 28], and disentangling motion from appear-ance [19]. This has created the potential to animate a broader range of object categories, without any domain knowledge or labelled data, requiring only videos of objects in motion during training [27]. However, two key problems remain open. The ﬁrst is how to represent the parts of an articulated or non-rigid moving object, including their shapes and poses.
The second is given the object parts, how to animate them using the sequence of motions in a driving video.
Initial attempts used end-to-end frameworks [39, 28] to
ﬁrst extract unsupervised keypoints [19, 17], then warp a feature embedding of a source image to align its keypoints with those of a driving video. Follow on work [27] further modelled the motion around each keypoint with local, afﬁne transformations, and introduced a generation module that both composites warped source image regions and inpaints occluded regions, to render the ﬁnal image. This enabled a variety of creative applications,2 for example needing only one source face image to generate a near photo-realistic animation, driven by a video of a different face.
However, the resulting unsupervised keypoints are de-2E.g. a music video in which images are animated using prior work [27]. 13653
tected on the boundary of the objects. While points on edges are easier to identify, tracking such keypoints between frames is problematic, as any point on the boundary is a valid candidate, making it hard to establish correspondences between frames. A further problem is that the unsupervised keypoints do not correspond to semantically meaningful ob-ject parts, representing location and direction, but not shape.
Due to this limitation, animating articulated objects, such as bodies, remains challenging. Furthermore, these methods assume static backgrounds, i.e. no camera motion, leading to leakage of background motion information into one or several of the detected keypoints. Finally, absolute motion transfer, as in [27], transfers the shape of the driving object into the generated sequence, decreasing the ﬁdelity of the source identity. These remaining deﬁciencies limit the scope of previous works [27, 28] to more trivial object categories and motions, especially when objects are articulated.
This work introduces three contributions to address these challenges. First, we redeﬁne the underlying motion rep-resentation, using regions from which ﬁrst-order motion is measured, rather than regressed. This enables improved convergence, more stable, robust object and motion repre-sentations, and also empirically captures the shape of the underpinning object parts, leading to better motion segmen-tation. This motion representation is inspired by Hu mo-ments [8]. Fig. 3(a) contains several examples of region vs. keypoint-based motion representation.
Secondly, we explicitly model background or camera mo-tion between training frames by predicting the parameters of a global, afﬁne transformation explaining non-object re-lated motions. This enables the model to focus solely on the foreground object, making the identiﬁed points more stable, and further improves convergence. Finally, to prevent shape transfer and improve animation, we disentangle the shape and pose of objects in the space of unsupervised regions. Our framework is self-supervised, does not require any labels, and is optimized using reconstruction losses.
These contributions further improve unsupervised motion transfer methods, resulting in higher ﬁdelity animation of ar-ticulated objects in particular. To create a more challenging benchmark for such objects, we present a newly collected dataset of TED talk speakers. Our framework scales better in the number of unsupervised regions, resulting in more detailed motion. Our method outperforms previous unsuper-vised animation methods on a variety of datasets, including talking faces, taichi videos and animated pixel art being pre-ferred by 96.6% of independent raters when compared with the state of the art [27] on our most challenging benchmark. 2.