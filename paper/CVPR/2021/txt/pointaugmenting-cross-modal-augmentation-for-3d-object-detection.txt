Abstract
Camera and LiDAR are two complementary sensors for 3D object detection in the autonomous driving con-text. Camera provides rich texture and color cues while
LiDAR specializes in relative distance sensing. The chal-lenge of 3D object detection lies in effectively fusing 2D camera images with 3D LiDAR points.
In this paper, we present a novel cross-modal 3D object detection algorithm, named PointAugmenting. On one hand, PointAugment-ing decorates point clouds with corresponding point-wise
CNN features extracted by pretrained 2D detection mod-els, and then performs 3D object detection over the deco-rated point clouds. In comparison with highly abstract se-mantic segmentation scores to decorate point clouds, CNN features from detection networks adapt to object appear-ance variations, achieving signiﬁcant improvement. On the other hand, PointAugmenting beneﬁts from a novel cross-modal data augmentation algorithm, which consistently pastes virtual objects into images and point clouds dur-ing network training. Extensive experiments on the large-scale nuScenes and Waymo datasets demonstrate the effec-tiveness and efﬁciency of our PointAugmenting. Notably,
PointAugmenting outperforms the LiDAR-only baseline de-tector by +6.5% mAP and achieves the new state-of-the-art results on the nuScenes leaderboard to date. 1.

Introduction 3D object detection plays a crucial role in 3D scene un-derstanding for autonomous driving. Existing 3D object de-tection algorithms mainly use LiDAR and cameras to per-ceive environments. LiDAR grasps depth information in the form of sparse point clouds, while cameras capture images in the form of dense intensity array with rich color and tex-tures. The challenge of 3D object detection lies in the mis-alignment between images and point clouds. In this work, we aim to advance 3D object detection by means of effec-tive cross-modal data fusion and augmentation.
*Corresponding author.
Figure 1. Performance comparison between using segmentation scores and CNN features to fuse with LiDAR points for 3D ob-ject detection. We replace the segmentation scores in PointPaint-ing [19] with middle CNN features extracted from the same seg-mentation network [20]. We use the LiDAR-only detector Center-Point [27] as baseline. The improvement of +4.9% mAP on the 1/8 nuScenes dataset suggests that CNN features from images are better at fusing with point clouds. Abbreviations stand for con-struction vehicle (C.V.), motorcycle (Motor.), pedestrian (Ped.), and trafﬁc cone (T.C.).
Previous arts have explored a variety of cross-modal fu-sion schemes, which fall into three categories: result-level fusion, proposal-level fusion, and point-level fusion. The result-level fusion approaches [13, 21] adopt off-the-shelf 2D object detectors, thus their performances are limited by the upper bound of 2D detectors. The proposal-level fusion methods, such as MV3D [3] and AVOD [8], perform fusion at the region proposal level, resulting in heavy computation load. Recent approaches [11, 10, 29, 16, 7, 19] attempt to fetch point-wise image features by projecting point clouds onto image plane.
[11, 10, 29] construct birds-eye-view (BEV) camera features before fusing with LiDAR BEV fea-tures to mitigate the viewpoint inconsistency. However, the cross-view transformation readily causes feature blurring.
Instead, MVX-Net [16], EPNet [7] and PointPainting [19] directly exploit point-wise correspondence to augment each
LiDAR point with CNN features or segmentation scores from image segmentation. We note that prior fusion meth-11794
Method
CenterPoint w/o GT-Paste
CenterPoint w/ GT-Paste
Gains of GT-Paste
Car 74.2 78.6
+4.4
Truck 30.9 39.2
+8.3
C.V. 3.7 2.0
-1.7
Bus 27.0 33.5
+6.5
Trailer 12.5 13.5
+1.0
Barrier Motor. Bicycle 30.3 32.2
+1.9 1.7 8.6
+6.9 37.2 46.8
+9.6
Ped. 68.2 74.2
+6.0
T.C. mAP NDS 42.3 32.8 42.4 49.5 37.6 47.5
+7.2
+4.8
+5.1
Table 1. Effectiveness of the GT-Paste data augmentation scheme. Applying GT-Paste data augmentation for LiDAR points achieves an improvement of +4.8% mAP. We use CenterPoint as baseline with 1/8 training data on the nuScenes dataset. mantics on the Conv5 layer of VGG-16. High-level se-mantics often cause blurred image features for neighbor-ing LiDAR points. We thus take the output activation from the DLA34 layer of CenterNet [33, 32] as image features, putting emphasis on ﬁne-grained details to strengthen the distinction between point clouds. Moreover, considering the modality gap between LiDAR and cameras, we employ a late fusion mechanism across modalities. With our fusion scheme, we achieve remarkable improvements of +10.1% and +5.2% mAP respectively over the LiDAR-only and
PointPainting methods on the 1/8 nuScenes dataset. The ex-ample in Figure 2 illustrates the superiority of our method.
When training 3D detectors, one of the bottlenecks lies in cross-modal data augmentation. Existing LiDAR-only detectors widely use GT-Paste [22], a data augmentation scheme to augment point clouds. GT-Paste pastes virtual objects in the forms of ground-truth boxes and LiDAR points from other scenes to the training scenes. Table 1 shows that GT-Paste improves the LiDAR-only detector by
+4.8% mAP. However, directly applying GT-Paste to cross-modal detectors would destruct the consistency between Li-DAR points and camera images. To address this issue, we propose a simple yet effective cross-modal augmentation method to make GT-Paste applicable to both point clouds and images. Speciﬁcally, we ﬁrst follow an observer’s per-spective to ﬁlter occluded LiDAR points according to the geometrical unanimity rule. We then take hold of all the ob-jects in current scene and paste their corresponding patches onto images in a far-to-near order. With the help of the cross-modal data augmentation, our proposed 3D detector achieves competitive results over the state-of-the-art meth-ods.
In brief, our contributions are summarized as follow.
• We explore effective CNN features from 2D object de-tection networks as image representation to fuse with
LiDAR points for 3D object detection.
• We propose a simple yet effective cross-modal data augmentation method for training 3D object detectors, considering the modality consistency between cameras and LiDAR.
• We extensively validate the effectiveness of cross-modal fusion and data augmentation on large-scale nuScenes and Waymo datasets. The proposed 3D de-tector PointAugmenting achieves the new state-of-the-art results on the nuScenes leaderboard to date. 11795
Figure 2. Comparison of different detectors. The far-away pedes-trian in the scene is missed by the LiDAR-only baseline detector due to the sparse point clouds. PointPainting also fails as a re-sult of segmentation failures on small objects. Beneﬁting from the abundant semantics provided by image features, our method suc-cessfully detects the pedestrian. ods before PointPainting have limited generalization and performance, as concluded by PointPainting, “despite re-cent fusion research, the top methods on the popular KITTI leaderboard are lidar only”. With the help of segmentation scores, PointPainting has served as a popular baseline of fusion with large gains over the LiDAR-only detectors on large-scale datasets.
Despite the impressive improvements, segmentation scores are sub-optimal to cover color and textures in im-ages. Intuitively, high dimensional CNN features of images contain richer appearance cues and larger receptive ﬁeld than segmentation scores, therefore are more complemen-tary to fuse with point clouds. To validate this intuition, we conduct a preliminary experiment on the basis of Point-Painting. Speciﬁcally, we replace the segmentation scores with CNN features extracted by the same segmentation net-work [20]. Figure 1 shows that CNN features help Point-Painting to achieve a signiﬁcant gain of +4.9% mAP on the 1/8 nuScenes [2] dataset. This manifests the effectiveness of CNN features to fuse with point clouds for 3D detectors.
Considering the lack of ground truth segmentation labels for most 3D detection tasks, we use pretrained 2D object de-tection networks rather than image segmentation networks as feature extractor. Our method differs from the prior 3D detector MVX-Net [16], which utilizes the high-level se-n o i t a z i l e x o
V r e d o c n
E e s i w
-l e x o
V
LiDAR 
Feature to BEV to BEV
Camera 
Feature
Camera Feature
+
LiDAR Feature
C
C
N
P
R d a e
H
Point-wise Feature Fetching 3D Backbone n o i t a c i f i s s a l
C n o i s s e r g e
R
Figure 3. PointAugmenting overview. The architecture consists of two stages. (1) Point-wise feature fetching: LiDAR points are projected onto image plane and then appended by the fetched point-wise CNN features. (2) 3D detection: we extend CenterPoint with an additional 3D sparse convolution stream for camera features and fuse different modalities via a simple skip and concatenation approach in BEV maps. 2.