Abstract 1.

Introduction
Understanding the 3D world from 2D projected natural images is a fundamental challenge in computer vision and graphics. Recently, an unsupervised learning approach has garnered considerable attention owing to its advantages in data collection. However, to mitigate training limitations, typical methods need to impose assumptions for viewpoint distribution (e.g., a dataset containing various viewpoint images) or object shape (e.g., symmetric objects). These as-sumptions often restrict applications; for instance, the ap-plication to non-rigid objects or images captured from sim-ilar viewpoints (e.g., ﬂower or bird images) remains a chal-lenge. To complement these approaches, we propose aper-ture rendering generative adversarial networks (AR-GANs), which equip aperture rendering on top of GANs, and adopt focus cues to learn the depth and depth-of-ﬁeld (DoF) ef-fect of unlabeled natural images. To address the ambigui-ties triggered by unsupervised setting (i.e., ambiguities be-tween smooth texture and out-of-focus blurs, and between foreground and background blurs), we develop DoF mixture learning, which enables the generator to learn real image distribution while generating diverse DoF images. In addi-tion, we devise a center focus prior to guiding the learning direction. In the experiments, we demonstrate the effective-ness of AR-GANs in various datasets, such as ﬂower, bird, and face images, demonstrate their portability by incorpo-rating them into other 3D representation learning GANs, and validate their applicability in shallow DoF rendering.
Natural images are 2D projections of a 3D world. Ad-dressing the inverse problem, i.e., understanding the 3D world from natural images, is a fundamental challenge in computer vision and graphics. Owing to its diverse applica-tions in various ﬁelds, such as in robotics, content creation, and photo editing, this challenge has been actively studied.
A direct solution to challenge is learning a 3D predic-tor in a supervised manner using 2D and 3D data pairs or multiview image sets. However, obtaining such data is often impractical or time-consuming. To eliminate this process, several studies have attempted to learn 3D rep-resentations from single-view images (i.e., with only a single view per training instance). However, owing to the ill-posed nature, several studies required auxiliary in-formation, such as 2D keypoints [49, 23] or 2D silhou-ettes [17, 6, 32, 13], to align object positions or extract a target object from the background. Other studies required predeﬁned category-speciﬁc shape models (e.g., 3DMM [3] and SMPL [36]) [22, 56, 11, 43, 44] to obtain clues for re-construction. Although they have exhibited promising re-sults, collecting auxiliary information still requires a labo-rious annotation process, and a shape model requires addi-tional preparation costs and restricts applicable objects.
To eliminate these disadvantages, fully unsupervised learning methods that enable 3D representation learning from single-view images without additional supervision and shape models have been devised. Although this is a se-15679
vere setting, previous studies have addressed this challenge by imposing assumptions for viewpoint distribution (e.g., a dataset including various viewpoint images) [38, 47, 40] or object shape (e.g., symmetric objects) [59]. The ﬁrst as-sumption is required to learn 3D representations by sam-pling diverse viewpoint images. The second assumption is required to perform stereo reconstruction using a pair of mirrored images. Although these assumptions are practical for objects of a speciﬁc class (e.g., human faces), several ob-jects do not satisfy these assumptions. For example, these methods are difﬁcult to apply to non-rigid objects or im-ages captured from similar viewpoints (e.g., ﬂower or bird images).
To broaden the application without contradicting previ-ous achievements, in this study, we consider complemen-tary cues inherent in photos that have not been actively used in previous deep generative models (including those above).
In particular, we focus on focus cues, in other words, we consider the learning depth1 and the depth-of-ﬁeld (DoF) effect in the defocus process. Speciﬁcally, instead of im-posing an assumption on the viewpoint distribution, we do so on the DoF distribution (i.e., a dataset including various
DoF images), and as shown in Figure 1, we attempt to learn 3D representations (particularly depth and DoF effect) from a collection of single-DoF images (i.e., images with solely a single DoF setting per training instance).
To achieve this, we propose a novel family of generative adversarial networks (GANs) [14], referred to as aperture rendering GAN (AR-GAN), which equips aperture rendering (e.g., light ﬁeld aperture rendering [46]) on top of GANs.
Speciﬁcally, AR-GAN initially generates a pair of a deep
DoF image and depth from a random noise, and then renders a shallow DoF image from the generated deep DoF image and depth via aperture rendering. With this mechanism, we can synthesize various DoF images using a virtual camera with an optical constraint on the light ﬁeld.
When AR-GAN is learned in an unsupervised manner using single-DoF images, two non-trivial challenges are ambiguity between the smooth texture and out-of-focus blurs and ambiguity between the foreground and back-ground blurs, as we cannot obtain explicit supervision of these relationships. For the ﬁrst problem, we introduce DoF mixture learning, which enables the generator to learn the real image distribution while generating various DoF im-ages. This learning ensures that the generated images (deep and shallow DoF images) are in a real distribution, and fa-cilitates the learning of the depth, which is a source of con-necting deep and shallow DoF images. For the second prob-lem, based on the observed tendency to focus on the center object when a focused image is considered, we impose a center focus prior, which facilitates the focusing of the cen-ter while guiding the surroundings to be behind the focal plane. In practice, we adopt prior solely at the beginning of training to guide the learning direction. 1In this study, we use depth and disparity interchangeably to indicate disparity across a camera aperture.
To evaluate the effectiveness of AR-GAN, we ﬁrst con-ducted experiments with comparative and ablation stud-ies on diverse datasets, including ﬂower (Oxford Flowers 102 [39]), bird (CUB-200-2011 [53]), and face (FFHQ [26]) datasets. A signiﬁcant property of AR-GAN is its porta-bility, which we validated by incorporating AR-GAN into other 3D representation learning GANs (i.e., Holo-GAN [38] and RGBD-GAN [40]). Another signiﬁcant property of AR-GAN is its ability to synthesize a tuple of deep and shallow DoF images and depth from a random noise, after training. We utilize this property to train a shal-low DoF renderer and empirically demonstrate its utility.
Overall, our contributions are summarized as follows:
• We provide unsupervised learning of depth and DoF effect from unlabeled natural images. This is note-worthy because it does not impose assumptions on the viewpoint distribution or object shape, which are re-quired in conventional unsupervised 3D representation learning.
• To achieve this, we propose a novel GAN family (AR-GAN), which generates a deep DoF image and depth from a random noise and renders a shallow DoF image from them via aperture rendering.
• To address ambiguities caused by a fully unsupervised setting, we devise DoF mixture learning to enable the generator to learn real image distribution using gener-ated diverse DoF images, and develop a center focus prior to determine the learning direction.
• We validate the effectiveness, portability, and applica-bility of AR-GANs via extensive experiments. The project page is available at http://www.kecl. ntt . co . jp / people / kaneko . takuhiro / projects/ar-gan/. 2.