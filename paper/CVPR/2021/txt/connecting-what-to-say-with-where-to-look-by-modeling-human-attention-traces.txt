Abstract
We introduce a uniﬁed framework to jointly model im-ages, text, and human attention traces. Our work is built on top of the recent Localized Narratives annotation frame-work [31], where each word of a given caption is paired with a mouse trace segment. We propose two novel tasks: (1) predict a trace given an image and caption (i.e., visual grounding), and (2) predict a caption and a trace given only an image. Learning the grounding of each word is challenging, due to noise in the human-provided traces and the presence of words that cannot be meaningfully visually grounded. We present a novel model architecture that is jointly trained on dual tasks (controlled trace generation and controlled caption generation). To evaluate the quality of the generated traces, we propose a local bipartite match-ing (LBM) distance metric which allows the comparison of two traces of different lengths. Extensive experiments show our model is robust to the imperfect training data and outperforms the baselines by a clear margin. More-over, we demonstrate that our model pre-trained on the pro-posed tasks can be also beneﬁcial to the downstream task of
COCO’s guided image captioning. Our code1 and project page2 are publicly available. 1.

Introduction
The development of powerful models and algorithms within computer vision and natural language processing proceeded along distinct trajectories with only occasional overlap until recently. However, ideas from these two ﬁelds are gradually converging, with a focus on building multi-modal models, particularly for aligning visual and language stimuli [25, 35, 34, 10]. The goal of these models is to 1Code: github.com/facebookresearch/connect-caption-and-trace 2Project page: http://pages.cs.wisc.edu/∼zihangm/connect caption trace
Figure 1: The three vision-and-language tasks, as illustrated on a single example from the Localized Narratives dataset.
The ﬁrst and third depicted tasks are novel. mimic humans’ extraordinary abilities to compress infor-mation and translate it across modalities. Several joint or combined visual recognition and natural language under-standing tasks have emerged as natural tests of these vision-and-language models’ capabilities. Image captioning asks a model to identify and localize the key scene elements in an image and describe them in natural language form. Vi-sual grounding, and speciﬁcally phrase localization, asks a model to solve the reverse problem: given a natural lan-guage query, identify the target object(s) of the query in the image. Controlled image captioning, ﬁrst introduced in [12], combines the two tasks. Here, an external user is asked to specify which parts of the image they want de-scribed and in what order (e.g., by providing an ordered se-quence of bounding boxes). The output captions are there-fore explicitly grounded in the image. One application of this line of work is automatically generating localized de-scriptions of images for visually impaired users on social 12679
media services. This removes the need to rely on human-written “alt” text, which is often missing in web images [6].
Vision-and-language models share common components and techniques.
Image captioning architectures are typi-cally composed of two modules: an image encoder, which ingests and interprets an image, and a language model de-coder, which generates a natural language caption [39, 17].
Visual grounding models ﬁrst identify the key components of the image (i.e., bounding box proposals) and query (i.e., which words or phrases to focus on), extract features from each, and then correlate them to predict the referred-to ob-ject [33, 16, 30, 42]. Architectures for both tasks often rely on attention [39, 15, 17, 25], a mechanism inspired by the human visual system [32, 11]. Researchers have also de-signed more complex models that can do both caption gen-eration and grounding. For example, [27] and [42] can both generate an unambiguous description of a speciﬁc object or region in an image and automatically select an object given a referring text expression.
Despite these advancements, existing image captioning and visual grounding models cannot jointly generate long-form, natural language captions and dense, word-level vi-sual groundings. This is because existing image captioning datasets only provide short captions with sparse ground-ings at the noun level (Flickr30k Entities [30]) or phrase level (Google RefEx [27], Flickr30k Entities [30] and Vi-sual Genome [22]). To address these limitations, [31] intro-duced the Localized Narratives dataset, in which annotators were asked to describe an image with their voice while si-multaneously drawing a mouse trace over the region they are describing. This annotation framework provides rich, longform image captions and dense visual grounding in the form of a mouse trace segment for each word. The work in [31] incorporates the annotated mouse trace to aid in standard image captioning and controlled image captioning tasks. However, it does not investigate the reverse problem of directly predicting the mouse trace or explore the con-nections between caption generation and trace generation.
In this paper, we take a step beyond [31] by requiring models to directly predict the trace, which is analogous to a ﬁne-grained and temporally grounded log of human at-tention. Besides controlled caption generation, where a model generates a caption guided by the given ordered trace from [31], we further introduce two challenging new tasks: controlled trace generation, where a model must densely lo-calize each word from a natural language caption in an im-age, and joint caption and trace generation, where a model is only given an image and must act as an annotator in the
Localized Narratives protocol. There tasks are shown in
Fig. 1. The task of predicting the trace is meaningful in two ways. First, a point-wise trace is a straightforward means for representing eye gaze. Learning the trace (independent of speciﬁc use cases) could be variously useful, and this is made possible by the efﬁcient collection scheme described in [31] which does not rely on expensive gaze trackers.
Second, this form of annotation yields “weakly-labeled” word-level grounding. We demonstrate that such “weak” word-to-trace alignment could offer beneﬁts for some im-portant vision and language tasks. Besides, the predicted trace can provide a better explanation than most attention-based image captioning approaches. To evaluate the gener-ated traces, we propose a novel evaluation metric, local bi-partite matching (LBM), to compare two traces of arbitrary length. We present a ﬂexible new transformer-based model architecture that is trained in parallel on controlled caption generation and controlled trace generation. The model also incorporates a symmetric cycle loss to improve the quality of the generated caption and trace. In addition to the three tasks mentioned above, we show that our approach can ben-eﬁt downstream tasks by pre-training on our proposed tasks before ﬁne-tuning for the downstream setting.
To summarize, we make the following contributions:
• We introduce two novel tasks: (i) controlled trace gen-eration and (ii) joint caption and trace generation.
• We present a novel mirrored transformer model archi-tecture (MITR), which is jointly trained and evaluated on three vision-and-language tasks.
• We design an evaluation metric to address the chal-lenge of computing the distance between two traces of different lengths.
• By jointly learning from the mirrored trace genera-tion task, our proposed method beneﬁts the down-stream task of guided caption generation on the COCO dataset. 2.