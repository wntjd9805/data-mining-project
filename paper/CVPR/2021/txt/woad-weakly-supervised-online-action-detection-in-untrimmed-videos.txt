Abstract video-level	anno. segment-level	anno.
Online action detection in untrimmed videos aims to iden-tify an action as it happens, which makes it very impor-tant for real-time applications. Previous methods rely on tedious annotations of temporal action boundaries for train-ing, which hinders the scalability of online action detection systems. We propose WOAD, a weakly supervised framework that can be trained using only video-class labels. WOAD contains two jointly-trained modules, i.e., temporal proposal generator (TPG) and online action recognizer (OAR). Super-vised by video-class labels, TPG works ofﬂine and targets at accurately mining pseudo frame-level labels for OAR. With the supervisory signals from TPG, OAR learns to conduct ac-tion detection in an online fashion. Experimental results on
THUMOS’14, ActivityNet1.2 and ActivityNet1.3 show that our weakly-supervised method largely outperforms weakly-supervised baselines and achieves comparable performance to the previous strongly-supervised methods. Beyond that,
WOAD is ﬂexible to leverage strong supervision when it is available. When strongly supervised, our method obtains the state-of-the-art results in the tasks of both online per-frame action recognition and online detection of action start. 1.

Introduction
Temporal Action Localization aims to detect temporal action boundaries in long, untrimmed videos. Most previous methods are under ofﬂine settings [2, 4, 5, 10, 25, 31], where they can observe the entire action before making decisions.
However, applications such as surveillance systems and au-tonomous cars, are required to interact with the world in real time based on their accumulative observations up to now. On-line Action Detection [6] is proposed to address this problem, where methods need to identify occurring actions moment-to-moment without access to future information. With different focuses, recent online action detectors consider two sub-tasks: (1) online per-frame action recognition [6, 9, 28] and (2) online detection of action start [11, 24]. The former task focuses on the general capability of recognizing the action category of each coming frame. On the other hand, detecting previous	 methods mixed
× video × segment √ our	method mixed √ segment √ video √
Figure 1. Comparison between previous methods and our approach.
Previous methods require segment-level annotations (start and end times of actions) during training, which leads to high human label-ing cost. In contrast, our method can be trained with video-level annotations (video-level action labels) and is ﬂexible to utilize (full/ partial) segment-level supervision when it is available. action starts in a timely manner is more important to some real-world applications. For example, an autonomous car needs to recognize “line merging” of another vehicle as soon as it starts. While, it is challenging to detect action starts due to the similar appearances near the start points and the lack of training data. The later task specially targets on this problem. Our method jointly addresses these two tasks.
Although previous methods have achieved promising progress, they rely on segment-level annotations of action boundaries for training (see Fig. 1). However, annotating action boundaries in long, untrimmed videos involves pos-sibly ambiguous decisions and requires signiﬁcant amount of human labor. This hinders the scalability of model learn-ing, particularly for videos embodying complex semantics.
Compared to the segment-level boundaries, video-level ac-tion classes are much easier to acquire. With the help of text-based video retrieval techniques, video-class labels may be obtained almost for free from the internet at a large scale.
To take advantage of the easy-to-obtain video-level an-notations, we propose WOAD, a Weakly supervised Online
Action Detection framework, that can be trained with video-class labels only. Detecting actions using weak supervision in an online scenario is challenging since (1) online action detectors generally require per-frame labels for training, so it is hard to utilize video-level labels as supervision and (2) it is not trivial for a model to be accurate for action recognition and sensitive to action starts without access to future infor-mation. As shown in Fig. 2, our proposed WOAD contains 1915
two jointly-trained modules, i.e., Temporal Proposal Gener-ator (TPG) and Online Action Recognizer (OAR), each of which focuses on handling one of the challenges. Supervised by video-class labels, TPG mines class-wise temporal action proposals that can be used as pseudo per-frame labels for
OAR. While, OAR aims at conducting both per-frame action recognition and start detection jointly in an online fashion.
The proposed design has the following beneﬁts: (1) TPG is used only during training for pseudo labels generation, so it can fully utilize temporal relation of frames (e.g. grouping nearby frames of the same class to improve proposal gen-eration) without online constraint; (2) the design of OAR directly targets at improving the online tasks without being distracted by the weakly supervised setting and (3) the joint training could help learn better representations.
Our contributions are summarized as follows: (1) we in-troduce a novel method for weakly supervised online action detection. To the best of our knowledge, this is the ﬁrst work that addresses the problem using weak supervision; (2) our method is ﬂexible to combine weak and strong supervision when only a part of videos have strong annotations and (3) experimental results show that our method largely outper-forms weakly-supervised baselines and achieves comparable performance to the previous strongly-supervised methods.
When strongly supervised, our method obtains the state-of-the-art results in the tasks of both online per-frame action recognition and online detection of action start. 2.