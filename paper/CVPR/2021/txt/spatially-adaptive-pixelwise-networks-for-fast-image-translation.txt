Abstract
We introduce a new generator architecture, aimed at fast and efﬁcient high-resolution image-to-image translation. We design the generator to be an extremely lightweight func-tion of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple afﬁne transforma-tions and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate ex-pressivity. First, the parameters of the pixel-wise networks are spatially varying, so they can represent a broader func-tion class than simple 1 × 1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input. Third, we augment the input image by concate-nating a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18× faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains. 1.

Introduction
Translating images from one domain to another has been extensively studied in recent years [18]. Current approaches usually train a conditional Generative Adversarial Network (GAN) to learn a direct mapping from one domain to the other [53, 5, 48, 32, 27]. Although these approaches have made rapid progress in terms of visual quality, model size and inference time have also grown signiﬁcantly. The com-putational cost of these algorithms becomes even more acute when operating on high resolution images, which is the most desirable setting in typical real-world applications.
In this work, we present a novel architecture, designed for fast image-to-image translation1. The key ingredient for an efﬁcient runtime is a new generator that operates pixel-wise: each pixel is processed independently from the others using a pixel-speciﬁc, lightweight Multi-Layer Perceptron 1https://tamarott.github.io/ASAPNet_web
Figure 1: Fast image translation. Our novel spatially adap-tive pixelwise design enables generating high-resolution images at signiﬁcantly lower runtimes than existing meth-ods, while maintaining high visual quality. Particularly, as seen in the plot our model is 2-18× faster than baselines
[27, 32, 34, 48, 5], depending on resolution. (MLP). At ﬁrst glance, the representation power of this gener-ator should appear limited. However, three key components make our network fully expressive. First, in contrast to tradi-tional convolutional networks, where network parameters are shared across spatial positions, the parameters of the MLPs vary spatially so each pixel is effectively transformed by a different function. Second, the spatially-varying parameters are predicted at low-resolution by a convolutional network that processes a drastically downsampled representation of the input. This makes the MLPs adaptive to the input image (i.e., the pixel-functions depend on the input image itself).
Third, in addition to the input pixel values, the local MLPs 14882
1024 × 1024 label map ground truth
SPADE [32], 359ms pix2pixHD [48], 166ms
ASAP-Net (ours), 35ms 512 × 1024 label map
CC-FPSE [27], 520ms
SPADE [32], 190ms ground truth pix2pixHD [48], 95ms
ASAP-Net (ours), 29ms
Figure 2: Fast image-to-image translation. Our model translates high-resolution images in short execution time comparing to baselines, while maintaining high visual quality. In this example, we translate 512 × 1024 and a 1024 × 1024 label images into equal-sized outputs in only 29 and 35 ms, respectively – up to 10× and 18× faster than the state-of-the-art methods
SPADE [32] and CC-FPSE [27], respectively – while maintaining the same visual quality (see § 4.1 for a user study and quantitative comparisons). Please note that CC-FPSE cannot be trained on 1024 × 1024 images due to memory constraints. consume a sinusoidal encoding of the pixel’s spatial posi-tion [46]. Together, these three components enable realistic output synthesis with coherent and detailed image structures.
As a result, our model, which we coin ASAP-Net (A
Spatially-Adaptive Pixelwise Network), generates images of high visual quality at very short execution times, even for high-resolution inputs (see Fig. 1). A visual compari-son to state-of-the-art methods can be seen in Fig. 2. Here, our model processes 512 × 1024 and 1024 × 1024 label maps in only 29 and 35 milliseconds on a GPU. This is over 4.5×, 10× and 18× faster than the high-performance pix2pixHD [48], SPADE [32] and CC-FPSE [27] models, respectively, on the same hardware.
We evaluate our model on several image domains and at various resolutions, including transferring facade labels to building images and generating realistic city scenery from semantic segmentation maps. Additionally, we show the
ﬂexibility of our method by testing on a markedly different translation task, predicting depth maps from indoor images.
In all cases, our model’s runtime is considerably lower than existing state-of-the-art methods, while its visual quality is comparable. We conﬁrm this with human perceptual studies, as well as by automatic metrics. 2.