Abstract
Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying dis-tribution of the observed data. However, how the features learned from solving the task of image generation are applicable to other vision tasks remains seldom explored.
In this work, we show that learning to synthesize images can bring remarkable hierarchical visual features that are generalizable across a wide range of applications. Specif-ically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise represen-tation to train a novel hierarchical encoder. The visual feature produced by our encoder, termed as Generative
Hierarchical Feature (GH-Feat), has strong transferabil-ity to both generative and discriminative tasks, including image editing, image harmonization, image classiﬁcation, face veriﬁcation, landmark detection, and layout prediction.
Extensive qualitative and quantitative experimental results demonstrate the appealing performance of GH-Feat.1 1.

Introduction
Representation learning plays an essential role in the rise of deep learning. The learned representation is able to express the variation factors of the complex visual world. Accordingly, the performance of a deep learning algorithm highly depends on the features extracted from the input data. As pointed out by Bengio et al. [4], a good representation is expected to have the following properties.
First, it should be able to capture multiple conﬁgurations from the input. Second, it should organize the explanatory factors of the input data as a hierarchy, where more abstract concepts are at a higher level. Third, it should have strong transferability, not only from datasets to datasets but also from tasks to tasks.
Deep neural networks supervisedly trained for image classiﬁcation on large-scale datasets (e.g., ImageNet [8] and
Places [66]) have resulted in expressive and discriminative
*denotes equal contribution. 1Project page is at https://genforce.github.io/ghfeat/. visual features [46]. However, the developed features are heavily dependent on the training objective. For example, prior work has shown that deep features trained for the object recognition task may mainly focus on the shapes and parts of the objects while remain invariant to rota-tion [1, 41], and the deep features from a scene classiﬁcation model may focus more on detecting the categorical objects (e.g., bed for bedroom and sofa for living room) [65].
Thus the discriminative features learned from solving high-level image classiﬁcation tasks might not be necessarily good for other mid-level and low-level tasks, limiting their transferability [57, 64]. Besides, it remains unknown how the discriminative features can be used in generative applications like image editing.
Generative Adversarial Network (GAN) [17] has re-cently made great process in synthesizing photo-realistic images.
It considers the image generation task as the training supervision to learn the underlying distribution of real data. Through adversarial training, the generator can capture the multi-level variations underlying the input data to the most extent, otherwise, the discrepancy between the real and synthesized data would be spotted by the discriminator. The recent state-of-the-art StyleGAN [31] has been shown to encode rich hierarchical semantics in its layer-wise representations [31, 55, 47]. However, the generator is primarily designed for image generation and hence lacks the inference ability of taking an image as the input and extracting its visual feature, which greatly limits the applications of GANs to real images.
To solve this problem, a common practice is to introduce an additional encoder into the two-player game described in GANs [12, 15, 13, 44]. Nevertheless, existing encoders typically choose the initial latent space (i.e., the most abstract level feature) as the target representation space, omitting the pre-layer information learned by the generator.
On the other hand, the transferability of the representation from GAN models is not fully veriﬁed in the literature.
Most prior work focuses on learning discriminative features for the high-level image classiﬁcation task [12, 15, 13] yet put little effort on other mid-level and low-level downstream tasks, such as landmark detection and layout prediction. 4432
In this work, we show that the pre-trained GAN genera-tor can be considered as a learned loss function. Training with it can bring highly competitive hierarchical visual features which are generalizable to various tasks. Based on the StyleGAN model, we tailor a novel hierarchical encoder whose outputs align with the layer-wise representations from the generator.
In particular, the generator takes the feature hierarchy produced by the encoder as the per-layer inputs and supervises the encoder via reconstructing the input image. We evaluate such visual features, termed as
Generative Hierarchical Features (GH-Feat), on both gen-erative and discriminative tasks, including image editing, image harmonization, image classiﬁcation, face veriﬁca-tion, landmark detection, layout prediction, etc. Extensive experiments validate that the generative feature learned from solving the image synthesis task has compelling hierarchical and transferable properties, facilitating many downstream applications. 2.