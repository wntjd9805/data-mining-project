Abstract
Accuracy of many visiolinguistic tasks has beneﬁted signiﬁcantly from the application of vision-and-language (V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One rea-son for this is the difﬁculty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Speciﬁcally, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through ex-tensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based ar-chitectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously. 1.

Introduction
Asking a robot to navigate in complex environments fol-lowing human instructions has been a long-term goal in AI research. Recently, a great variety of vision-and-language navigation (VLN) setups [3, 44, 52] have been introduced for relevant studies and a large number of works explore different methods to leverage visual and language clues to assist navigation. For example, in the popular R2R naviga-tion task [3], enhancing the learning of visual-textual cor-respondence is essential for the agent to correctly interpret the instruction and perceive the environment.
On the other hand, recent work on vision-and-language pre-training has achieved signiﬁcant improvement over a wide range of visiolinguistic problems. Instead of designing complex and monolithic models for different tasks, those  State 
𝑠𝑡  Observation      𝑉𝑡
𝑣3
𝑣2
[                                                                   ]
𝑡
𝑡
𝑣1
𝑡
𝑣𝑛
𝑡
...
Multi-Layer Transformer
Recurrence
𝑑𝑡
[        ]
Decision
Figure 1. Recurrent multi-layer Transformer for addressing par-tially observable inputs. A state token is deﬁned along with the input sequence. At each time step, a new state representation st will be generated based on the new observation. Meanwhile, the past information will help inferring a new decision dt. methods pre-train a multi-layer Transformer [53] on a large number of image-text pairs to learn generic cross-modal representations [7, 26, 28, 30, 33, 48, 50], known as V&L
BERT (Bidirectional Encoder Representations from Trans-formers [10]). Such advances have inspired us to employ
V&L BERT for VLN, replacing the complicated modules for modelling cross-modal relationships and allowing the learning of navigation to adequately beneﬁt from the pre-trained visual-textual knowledge. Unlike recent works on
VLN, which apply a pre-trained V&L BERT only for en-coding language [14, 29] or for measuring the instruction-path compatibility [37], we propose to use existing V&L
BERT models themselves for learning to navigate.
However, an essential difference between VLN and other vision-and-language tasks is that VLN can be considered as a partially observable Markov decision process, in which future observations are dependent on the agent’s current state and action. Meanwhile, at each navigational step, the visual observation only corresponds to partial instruction, requiring the agent to keep track of the navigation progress and correctly localise the relevant sub-instruction to gain useful information for decision making. Another difﬁculty of applying V&L BERT for VLN is the high demand on computational power; since the navigational episode could 1643
be very long, performing self-attention on a long visual and textual sequence at each time step will cost an excessive amount of (GPU) memory during training.
To address the aforementioned problems, we propose a recurrent vision-and-language BERT for navigation, or sim-ply VLNœBERT. Instead of employing large-scale datasets for pre-training which usually require thousands of GPU hours, the aim of this work is to allow the learning of
VLN to adequately beneﬁt from pre-trained V&L BERT.
Based on the previously proposed V&L BERT models, we implement a recurrent function in their original architec-ture (Fig. 1) to model and leverage the history-dependent state representations, without explicitly deﬁning a memory buffer [61] or applying any external recurrent modules such as an LSTM [16]. To reduce the memory consumption, we control the self-attention to consider the language tokens as keys and values but not queries during navigation, which is similar to the cross-modality encoder in LXMERT [50].
Such design greatly reduces the memory usage so that the entire model can be trained on a single GPU without perfor-mance degeneration. Furthermore, as in the original V&L
BERT, our proposed model has the potential of multi-task learning, it is able to address other vision and language problems along with the navigation task.
We employ two datasets to evaluate the performance of our VLNœBERT, R2R [3] and REVERIE [44]. The chosen datasets are different in terms of the provided vi-sual clues, the instructions and the goal. Our agent, ini-tialised from a pre-trained V&L BERT and ﬁne-tuned on the two datasets, achieves state-of-the-art results. We also ini-tialise our model with the PREVALENT [14], a LXMERT-like model pre-trained for VLN. On the test split of R2R
[3], it improves the Success Rate absolutely by 8% and achieves 57% Success weighted by Path Length (SPL). For the remote referring expression task in REVERIE [44], our agent obtains 23.99% navigation SPL and 13.51% Remote
Grounding SPL. These results indicate the strong generali-sation ability of our proposed VLNœBERT as well as the potential of using it for merging the learning of VLN with other vision and language tasks. 2.