Abstract
Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efﬁcient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efﬁciency and ac-curacy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the ﬁnal searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose At-tentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose al-gorithms to efﬁciently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large num-ber of networks across a wide range of FLOPs. Our dis-covered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outper-forms SOTA models, including BigNAS, Once-for-All net-works and FBNetV3. We also achieve ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models are available at https://github. com/facebookresearch/AttentiveNAS. 1.

Introduction
Deep neural networks (DNNs) have achieved remark-able empirical success. However, the rapid growth of net-work size and computation cost imposes a great challenge to bring DNNs to edge devices [16, 18, 38]. Designing net-works that are both accurate and efﬁcient becomes an im-portant but challenging problem.
Neural architecture search (NAS) [45] provides a pow-erful tool for automating efﬁcient DNN design. NAS re-quires optimizing both model architectures and model pa-rameters, creating a challenging nested optimization prob-lem. Conventional NAS algorithms leverage evolutionary y c a r u c c a n o i t a d i l a v 1
-p o
T
Figure 1. proaches [3, 10, 35, 36, 43] on ImageNet.
Comparison of AttentiveNAS with prior NAS ap-MFLOPs search [10, 11] or reinforcement learning [34], these NAS algorithms can be prohibitively expensive as thousands of models are required to be trained in a single experiment.
Recent NAS advancements decouple the parameter train-ing and architecture optimization into two separate stages
[3, 8, 15, 43]:
• The ﬁrst stage optimizes the parameters of all can-didate networks in the search space through weight-sharing, such that all networks simultaneously reach superior performance at the end of training.
• The second stage leverages typical search algorithms, such as evolutionary algorithms, to ﬁnd the best per-forming models under various resource constraints.
Such NAS paradigm has delivered state-of-the-art empirical results with great search efﬁciency [3, 37, 43].
The success of the two-stage NAS heavily relies on the candidate network training in the ﬁrst stage. To achieve su-perior performance for all candidates, candidate networks are sampled from the search space during training, followed by optimizing each sample via one-step stochastic gradient descent (SGD). The key aspect is to ﬁgure out which net-work to sample at each SGD step. Existing methods of-ten use a uniform sampling strategy to sample all networks 6418
with equal probabilities [8, 15, 37, 43]. Though promising results have been demonstrated, the uniform sampling strat-egy makes the training stage agnostic of the searching stage.
More speciﬁcally, while the searching stage focuses on the set of networks on the Pareto front of accuracy and infer-ence efﬁciency, the training stage is not tailored towards improving the Pareto front and regards each network can-didate with equal importance. This approach misses the op-portunity of further boosting the accuracy of the networks on the Pareto during the training stage.
In this work, we propose AttentiveNAS to improve the baseline uniform sampling by paying more attention to models that are more likely to produce a better Pareto front.
We speciﬁcally answer the following two questions:
• Which sets of candidate networks should we sample during the training?
• How should we sample these candidate networks ef-ﬁciently and effectively without introducing too much computational overhead to the training?
To answer the ﬁrst question, we explore two different sam-pling strategies. The ﬁrst strategy, denoted as BestUp, in-vestigates a best Pareto front aware sampling strategy fol-lowing the conventional Pareto-optimal NAS, e.g., [4, 6, 7, 23]. BestUp puts more training budgets on improving the current best Pareto front. The second strategy, denoted as WorstUp, focuses on improving candidate networks that yield the worst-case performance trade-offs. We refer to these candidate networks as the worst Pareto models. This sampling strategy is similar to hard example mining [14, 30] by viewing networks on the worst Pareto front as hard train-ing examples. Pushing the limits of the worst Pareto set could help update the least optimized parameters in the weight-sharing network, allowing all the parameters to be fully trained.
The second question is also non-trivial as determining the networks on both the best and the worst Pareto front is not straightforward. We propose two approaches to leverage 1) the training loss and 2) the accuracy predicted by a pre-trained predictor as the proxy for accuracy comparison. The overall contribution can be summarized as follows:
• We propose a new strategy, AttentiveNAS, to improve existing two-stage NAS with attentive sampling of net-works on the best or the worst Pareto front. Different sampling strategies, including BestUp and WorstUp, are explored and compared in detail.
• We propose two approaches to guide the sampling to the best or the worst Pareto front efﬁciently during training.
• We achieve state-of-the-art ImageNet accuracy given the FLOPs constraints for the searched Attentive-For example, AttentiveNAS-NAS model family.
A0 achieves 2.1% better accuracy compared to Mo-bileNetV3 with fewer FLOPs, while AttentiveNAS-A2 achieves 0.8% better accuracy compared to FBNetV3 with 10% fewer FLOPs. AttentiveNAS-A5 reaches 80.1% accuracy with only 491 MFLOPs. 2.