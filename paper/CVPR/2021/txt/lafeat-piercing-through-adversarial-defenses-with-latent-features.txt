Abstract
Deep convolutional neural networks are susceptible to adversarial attacks. They can be easily deceived to give an incorrect output by adding a tiny perturbation to the in-put. This presents a great challenge in making CNNs ro-bust against such attacks. An inﬂux of new defense tech-niques have been proposed to this end. In this paper, we show that latent features in certain “robust” models are surprisingly susceptible to adversarial attacks. On top of this, we introduce a uniﬁed ℓ∞-norm white-box attack algo-rithm which harnesses latent features in its gradient descent steps, namely LAFEAT. We show that not only is it computa-tionally much more efﬁcient for successful attacks, but it is also a stronger adversary than the current state-of-the-art across a wide range of defense mechanisms. This suggests that model robustness could be contingent on the effective use of the defender’s hidden components, and it should no longer be viewed from a holistic perspective. 1.

Introduction
Many safety-critical systems, such as aviation [12, 1, 45] medical diagnosis [54, 34], self-driving [6, 33, 52] have seen a large-scale deployment of deep convolutional neu-ral networks (CNNs). Yet CNNs are prone to adversarial attacks: a small specially-crafted perturbation impercepti-ble to human, when added to an input image, could result in a drastic change of the output of a CNN [51, 19, 7]. As a rapidly increasing number of safety-critical systems are automated by CNNs, it is now incumbent upon us to make them robust against adversarial attacks.
The strongest assumption commonly used for generat-ing adversarial inputs is known as white-box attacks, where the adversary have full knowledge of the model [8]. For in-stance, the model architecture, parameters, and training al-∗These authors contributed equally to this work.
†Corresponding author. gorithm and dataset are completely exposed to the attacker.
By leveraging the gradient of the output loss with respect to (w.r.t.) the input, gradient-based methods [37, 8, 35] have been shown to decimate the accuracy of CNNs when eval-uated on adversarial examples. Many new techniques to improve the robustness of CNNs have since been proposed to defend against such attacks. Recent years have therefore seen a tug of war between adversarial attack [19, 37, 8, 35, 16, 59, 14] and defense [35, 48, 9, 2, 64, 43, 55, 58, 57, 20] strategies. Attackers search for a perturbation that maxi-mizes the loss of the model output, typically through gra-dient ascent methods, e.g. one popular method is projected gradient descent (PGD) [35]; whereas defenders attempts to make the loss landscape smoother w.r.t. the perturbation via adversarial training, i.e. training with adversarial examples.
From a human perception perspective, as feature extrac-tors, shallow layers of CNNs extract simple local textures while neurons in deep layers specialize to differentiate com-plex objects [40, 10]. Intuitively, we expect incorrectly ex-tracted shallow features often cannot be pieced together to form correct high-level features. Moreover, this could have a cascading effect in subsequent layers. To illustrate, we equipped PGD with the ability to attack one of the inter-mediate layers by maximizing only the loss of an attacker-trained classiﬁer, which we call LPGD for now. In Figure 1, we scrambled the feature extracted by attacking an interme-diate layer with LPGD, and observed increasing discrepan-cies between the pairs of features extracted from the natural images and their associated adversary in deeper layers.
Nevertheless, existing attack and defense strategies ap-proach the challenge of evaluating or promoting the white-box model robustness in a model-holistic manner. Namely, for classiﬁers, they regard the model as a single non-linear differentiable function f that maps the input image to out-put logits. While these approaches generalize well across models, they tend to ignore the latent features extracted by the intermediate layers within the model.
Some recent defense strategies [4, 62, 63, 28, 27, 38, 39] reported that their models can achieve high robustness 5735
against PGD attacks. Understandably, these defenses are highly specialized to counter these conventional attacks. We speculate that one of the reasons why PGD failed to break through the defenses is because of its model-holistic nature.
This notion implores us to ask two important questions:
Can latent features be vulnerable to attacks; and subse-quently, can the falsely extracted features be cascaded to the remaining layers to make the model output incorrect?
It turns out that the new adversarial examples computed by LPGD can harm the accuracies of the “robust” models above (Figure 2). The experiment showed that while they are trained to be effective against PGD, they could fail spec-tacularly when faced attacks that simply target their latent features. This may also imply that a ﬂat model loss land-scape w.r.t. the input image does not necessarily entail ﬂat latent features w.r.t. the input. Existing attack methods that rely on a holistic view of the model therefore may fail to provide a reliable assessment of model robustness.
Motivated by the ﬁndings above, in this paper we pro-pose a new strategy, LAFEAT, which seeks to harness la-tent features in a generalized framework. To push the envelope of current state-of-the-art (SOTA) in adversarial robustness assessment, it draws inspiration from effective techniques discovered in recent years, such as the use of momentum [16, 14], surrogate loss [20, 15], step size sched-ule [14, 21], and multi-targeted attacks [21, 44, 43]. To summarize, our main contributions are as follows:
• We introduce how intermediate layers can be leveraged in adversarial attacks.
• We show that latent features provide faster conver-gence, and accelerate gradient-based attacks.
• By combining multiple effective attack tactics, we pro-pose LAFEAT. Empirical results show that it rivals competing methods in both the attack performance and computational efﬁciency. We perform extensive abla-tion analysis of its hyperparameters and components.
To the best of our knowledge, LAFEAT is currently the strongest against a wide variety of defense mechanisms and matches the current top-1 on the TRADES [64] CIFAR-10 white-box leaderboard (Section 4). Since latent features are vulnerable to adversarial attacks, which could in turn break robust models, we believe the future evaluation of model robustness could be contingent on how to make effective use of the hidden components of a defending model.
In short, model robustness should no longer be viewed from a holistic perspective. 2. Preliminaries &