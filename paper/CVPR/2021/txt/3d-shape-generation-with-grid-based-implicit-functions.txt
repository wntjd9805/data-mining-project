Abstract
Previous approaches to generate shapes in a 3D setting train a GAN on the latent space of an autoencoder (AE).
Even though this produces convincing results, it has two major shortcomings. As the GAN is limited to reproduce the dataset the AE was trained on, we cannot reuse a trained
AE for novel data. Furthermore, it is difﬁcult to add spa-tial supervision into the generation process, as the AE only gives us a global representation. To remedy these issues, we propose to train the GAN on grids (i.e. each cell cov-ers a part of a shape). In this representation each cell is equipped with a latent vector provided by an AE. This local-ized representation enables more expressiveness (since the cell-based latent vectors can be combined in novel ways) as well as spatial control of the generation process (e.g. via bounding boxes). Our method outperforms the current state of the art on all established evaluation measures, proposed for quantitatively evaluating the generative capabilities of
GANs. We show limitations of these measures and propose the adaptation of a robust criterion from statistical analysis as an alternative. 1.

Introduction
Training stable 3D GANs can be challenging and often better results are obtained by splitting the generation pro-cess into two parts. First an autoencoder (AE) is trained to obtain a compressed latent representation, then a GAN is trained to model the density of this global latent space. This simpliﬁes the task, as the AE typically generates reasonable output for a wide range of latent vectors. Training the AE can be seen as imposing a bias on the generation process to produce shapes similar to those seen during its training.
This procedure has obtained convincing results both for the generation of point clouds [1] and implicit functions
[5]. However, using the latent space of an AE as inter-mediate shape representation has some severe drawbacks.
Small variations of shapes are usually of local nature. As the latent representation used in prior work is of inherently global nature, such changes cannot be modeled easily. This makes it difﬁcult to add any localized modiﬁcations or con-straints into the generation process. Tasks such as condi-tional generation based on semantic information (e.g. seg-mentation/part labels) become hard to train. Another prob-lem is that the AE limits the space of shapes that can be generated. Indeed every AE, which does not represent the entire space of possible shapes in its latent space, limits the expressiveness of the GAN through the bias it imposes. E.g. if we train a GAN to create tables in such a manner, this is unlikely to work with the latent space of an AE trained for chairs.
A more natural choice of representation is to view a shape as a composition of many different local parts. This allows the network to choose parts that occur in different objects and arrange them to create new shapes. Recently,
Jiang et al. [13] learned local latent representations by sub-dividing the object space into a grid to reconstruct scenes from point clouds with high ﬁdelity. This can be seen as only imposing a localized bias on the shape generation pro-cess. Learning to capture the geometry within a single grid cell is much simpler than learning to represent the whole shape. Thus the reconstruction quality of the AE is much improved.
We argue that this representation has several further ad-vantages. Training a generative model on this localized la-tent space enables more variety in the generated shapes, as latent vectors belonging to different shapes or even different object classes can be mixed, giving the generator more de-grees of freedom. Its expressiveness is thus not limited by the AEs ability to generalize. In fact we do not even need to train an AE for each speciﬁc class. An AE trained on cells of e.g. tables can still be used to generate chairs, as on part level both classes share similarities. Furthermore, we can use convolutional architectures and thus build on exist-ing research in image generation. Although this argument applies to voxel based models as well [34], those are lim-ited by the grid resolution that can ﬁt into memory. As we can represent complex surfaces per grid cell, we can model realistic shapes while still keeping the grid resolution low.
Lastly, the spatial decomposition of the latent space allows us to perform conditional generation based on spatial infor-mation, such as bounding boxes or semantic labels.
When it comes to evaluating GANs there is no univer-13559
sally established measure to quantitatively rate, to what de-gree such networks are able to approximate a data distribu-tion. Image GANs are usually evaluated with the inception score [29] or Fr´echet inception distance [11]. The compu-tation of both measures involves the application of the in-ception network [32]. Since many different representations are used to encode 3D shapes (point clouds, voxel grids, implicit functions, etc.), this is not straightforward to apply for our use case. While previous quality scores for 3D data have been proposed, we show that they have some limita-tions. Therefore, we propose to apply a statistic measure from a two sample test for multivariate sets [4] to compute the statistical difference between a set of generated objects and a test set of unseen shapes from the data distribution.
Our key contributions are as follows:
• We show that localized grid-based implicit functions are better suited for 3D shape generation with GANs than global implicit functions for three reasons. First, they offer higher quality results, since each cell only has to represent fairly simple geometry. Second, lo-calized implicit functions can be combined together in novel ways and therefore offer more ﬂexibility. Third, localized grid-based implicit functions allow us to con-trol the generation process spatially, which is difﬁcult to accomplish with global implicit functions.
• We show that common evaluation techniques for 3D shape generation have several drawbacks. To allevi-ate this we propose a new robust score inspired from statistical analysis. 2.