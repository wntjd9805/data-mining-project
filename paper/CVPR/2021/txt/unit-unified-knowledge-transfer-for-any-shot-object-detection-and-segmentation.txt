Abstract
Methods for object detection and segmentation rely on large scale instance-level annotations for training, which are difﬁcult and time-consuming to collect. Efforts to allevi-ate this look at varying degrees and quality of supervision.
Weakly-supervised approaches draw on image-level labels to build detectors/segmentors, while zero/few-shot methods as-sume abundant instance-level data for a set of base classes, and none to a few examples for novel classes. This taxonomy has largely siloed algorithmic designs. In this work, we aim to bridge this divide by proposing an intuitive and uniﬁed semi-supervised model that is applicable to a range of super-vision: from zero to a few instance-level samples per novel class. For base classes, our model learns a mapping from weakly-supervised to fully-supervised detectors/segmentors.
By learning and leveraging visual and lingual similarities be-tween the novel and base classes, we transfer those mappings to obtain detectors/segmentors for novel classes; reﬁning them with a few novel class instance-level annotated sam-ples, if available. The overall model is end-to-end trainable and highly ﬂexible1. Through extensive experiments on MS-COCO [32] and Pascal VOC [14] benchmark datasets we show improved performance in a variety of settings. 1.

Introduction
Over the past decade CNNs have emerged as the dominant building blocks for various computer vision understanding tasks, including object classiﬁcation [21, 45, 52], detection
[33, 42, 43], and segmentation [8, 20]. Architectures based on Faster R-CNN [43], Mask R-CNN [20] and YOLO [42] have achieved impressive performance on a variety of core vi-sion tasks. However, traditional CNN-based approaches rely on lots of supervised data for which the annotation efforts can be time-consuming and expensive [22, 29]. While image-level class labels are easy to obtain, more structured labels such as bounding boxes or segmentations are difﬁcult and
Figure 1: Semi-supervised Any-shot Detection and Seg-mentation. The data used in our setting is categorized in two ways: (1) image-level classiﬁcation data for all ob-ject classes, and (2) abundant instance data for base object classes and limited (possibly zero) instance data for novel object classes, with the aim to obtain a model that learns to detect/segment both base and novel objects at test time. expensive2. Further, in certain domains (e.g., medical imag-ing) more detailed labels may require subject expertise. The growing need for efﬁcient learning has motivated develop-ment of various approaches and research sub-communities.
On one end of the spectrum, zero-shot learning methods require no visual data and use auxiliary information, such as attributes or class names, to form detectors for unseen classes from related seen category detectors [3, 16, 40, 65].
Weakly-supervised learning methods [2, 5, 12, 29, 34, 61] aim to utilize readily available coarse image-level labels for more granular downstream tasks, such as object detection
[3, 40] and segmentation [29, 71]. Most recently, few-shot learning [1, 41, 49, 60] has emerged as a learning-to-learn paradigm which either learns from few labels directly or by simulation of few-shot learning paradigm through meta-learning [15, 47, 57]. An interesting class of semi-supervised methods [17, 22, 26, 56, 58, 68] have emerged which aim 1Code is available at https://github.com/ubc-vision/UniT
∗Denotes equal contribution 2Segmentation annotations in PASCAL VOC take 239.7 seconds/image, on average, as compared to 20 seconds/image for image-level labels [4]. 5951
to transfer knowledge from abundant base classes to data-starved novel classes, especially for granular instance-level visual understanding tasks. However, to date, there isn’t a single, uniﬁed framework that can effectively leverage various forms and amounts of training data (zero-shot to fully supervised).
We make two fundamental observations that motivate our work. First, image-level supervision is abundant, while instance-level structured labels, such as bounding boxes and segmentation masks, are expensive and scarce. This is re-ﬂected in the scales of widely used datasets where classiﬁ-cation tasks have > 5K classes [28, 52] while the popular object detection/segmentation datasets, like MSCOCO [32], have annotations for only 80 classes. A similar observation was initially made by Hoffman et al. [22] and other semi-supervised [26, 56, 58] approaches. Second, the assumption of no instance-level supervision for target classes (as is the case for semi-supervised [22, 26, 56, 58] and zero-shot meth-ods [3, 16, 40, 65]) is artiﬁcial. In practice, it is often easy to collect few instance-level annotations and, in general, a good object detection/segmentation model should be robust and work with any amount of available instance-level su-pervision. Our motivation is to bridge weakly-supervised, zero- and few-shot learning paradigms to build an expres-sive, simple, and interpretable model that can operate across types (weak/strong) and amounts of instance-level supervi-sion (from 0 to 90+ instance-level samples per class).
We develop a uniﬁed semi-supervised framework (UniT) for object detection and segmentation that scales with dif-ferent levels of instance-level supervision (see Figure 1).
The data used in training our model is categorized in two ways, (1) image-level classiﬁcation data for all the object classes, and (2) abundant detection data for a set of base object classes and limited (possibly zero) detection data for a set of novel object classes, with the aim to obtain a model that learns to detect both base and novel objects at test time.
Our algorithm, illustrated in Figure 2, jointly learns weak-detectors for all the object classes, from image-level clas-siﬁcation data, and supervised regressors/segmentors on top of those for base classes (based on instance-level an-notations in a supervised manner). The classiﬁers, regres-sors and segmentors of the novel classes are expressed as a weighted linear combination of its base class counterparts.
The weights of the combination are determined by a multi-modal similarity measure: lingual and visual. The key in-sight of our approach is to utilize the multi-modal similarity measure between the novel and base classes to enable effec-tive knowledge transfer and adaptation. The adopted novel classiﬁer/regressors/segmentors can further be reﬁned based on instance-level supervision, if any available. We experi-ment with the widely-used detection/segmentation datasets
- Pascal VOC [13] and MSCOCO [32], and compare our method with state-of-the-art few-shot, weakly-supervised, and semi-supervised object detection/segmentation methods.
Contributions: Our contributions can be summarized as fol-lows: (1) We study the problem of semi-supervised object de-tection and segmentation in light of image-level supervision and limited instance-level annotations, ranging from no data (zero-shot) to a few (few-shot); (2) We propose a general, uniﬁed, interpretable, and ﬂexible end-to-end framework that, by leveraging a learned multi-modal (lingual + visual) similarity metric, can adopt classiﬁers/detectors/segmentors for novel classes by expressing them as linear combinations of their base class counterparts. (3) In the context of our model, we contrast the relative importance of weak image-level supervision with strong instance-level supervision, and highlight the importance of the former under a small ﬁxed annotation budget (4) We illustrate the ﬂexibility and effec-tiveness of our model by applying it to a variety of tasks (ob-ject detection and segmentation) and datasets (Pascal VOC
[13], MSCOCO [32]); showing state-of-the-art performance.
We get up to 23% relative improvement in mAP over the closest semi-supervised methods [17], and up to 16% gain over the best performing few-shot method [62] under a ﬁxed annotation budget. We conduct comprehensive comparisons across settings, tasks, types and levels of supervision. 2.