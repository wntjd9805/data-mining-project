Abstract
The Remote Embodied Referring Expression (REVERIE) is a recently raised task that requires an agent to navi-gate to and localise a referred remote object according to a high-level language instruction. Different from related
VLN tasks, the key to REVERIE is to conduct goal-oriented exploration instead of strict instruction-following, due to the lack of step-by-step navigation guidance. In this paper, we propose a novel Cross-modality Knowledge Reasoning (CKR) model to address the unique challenges of this task.
The CKR, based on a transformer-architecture, learns to generate scene memory tokens and utilise these informa-tive history clues for exploration. Particularly, a Room-and-Object Aware Attention (ROAA) mechanism is devised to explicitly perceive the room- and object-type informa-tion from both linguistic and visual observations. More-over, through incorporating commonsense knowledge, we propose a Knowledge-enabled Entity Relationship Reason-ing (KERR) module to learn the internal-external correla-tions among room- and object-entities for agent to make proper action at each viewpoint. Evaluation on REVERIE benchmark demonstrates the superiority of the CKR model, which signiﬁcantly boosts SPL and REVERIE-success rate by 64.67% and 46.05%, respectively. Code is available at: https://github.com/alloldman/CKR. 1.

Introduction
The Embodied-AI (E-AI), where embodied agents per-form various egocentric perception tasks, has attracted a surge of interest within both computer vision and natural language processing communities. In recent years, numer-ous datasets [1, 6, 16] and simulators [5, 26, 49] have been constructed to provide 3D assets with annotations and simu-late the agent respectively. These platforms support legions of tasks including Vision-Language Navigation (VLN) [1],
Embodied Question Answering [6], etc.
*Equal contribution
†Corresponding author (liusi@buaa.edu.cn)
Figure 1. At viewpoint A, our agent with commonsense turns right into the ‘meeting room’ through perceived ‘chair’ and ‘meeting-desk’. Then at viewpoint B, it seeks for easy-to-ﬁnd related ob-jects (e.g., ‘computer’) at ﬁrst for efﬁcient exploration, where tar-get ‘mouse’ is usually around. C is the ﬁnal viewpoint it arrived.
Most recently, a valuable task named Remote Embod-ied Visual referring Expression in Real Indoor Environ-ments (REVERIE) [37] was proposed to further facilitate the E-AI ﬁeld. The goal of REVERIE is for a robot in a photo-realistic 3D indoor environment to navigate closer to and localise a referred target object according to the given high-level instruction, which is similar to VLN task. How-ever, simply utilising approaches in VLN is not capable of completing REVERIE task satisfactorily, which has been proved in [37] through extensive experiments.
The REVERIE contains several challenges. Firstly, es-sentially different from previous VLN tasks (e.g., R2R [1]) that provide step-by-step navigation guidance, REVERIE towards practicability only annotates high/semantic-level instructions like ‘Go to the corner of meeting room, bring me the mouse on the table’, as shown in Fig. 1. This is more natural and closer to the human needs from a home assistance function perspective, but it is more challeng-ing. Therefore, instead of strict instruction-following, the agent needs to conduct goal-oriented exploration in an un-3064
seen environment. Speciﬁcally, efﬁcient exploration re-quires the agent to hold a long-term scene memory and extract informative memory clues for making a sequence of proper decisions. the goal in REVERIE
Secondly, can be abstractly expressed as ‘find XXX object in
XXX room’, which requires the cross-modal (vision-and-language) comprehension ability for the agent to be aware of the room/object-type at each viewpoint, and mine their relations with the goal. Thirdly, it is non-trivial to learn the internal-external correlations among rooms and objects from limited environments and apply it to the previously-unseen environments. Thus commonsense knowledge is re-quired, and an example is shown in Fig. 1. At last, due to the lack of detailed guidance, how to make the exploration more efﬁcient, i.e., complete the goal in a shortest trajec-tory, needs to be properly considered.
In this paper, we make multi-fold innovations to address the aforementioned challenges. Firstly, we design a Cross-modality Knowledge Reasoning (CKR) model to perform the REVERIE task, where the knowledge-enabled visual and linguistic clues constitute a scene memory token. Then all the memory tokens ordered by time sequence are fed into a decoder simultaneously to predict the next action.
The informative clues are effectively extracted from scene memory tokens for current decision through a learnable multi-layer attention. Secondly, we propose a Room-and-Object Aware Attention (ROAA) mechanism to explicitly recognise rooms and objects from both instruction and vi-sual input, bridging the cross-modal semantic gap between them. Thirdly, we bring external commonsense knowledge into the REVERIE task to improve capability of capturing the complicated relationships within rooms and objects ob-tained under the ROAA mechanism. Speciﬁcally, we pro-pose a Knowledge-enabled Entity Relationship Reasoning (KERR) module to incorporate prior knowledge from Con-ceptNet [41] for comprehensive room- and object-entity reasoning. For room-entity reasoning, we explicitly learn the room-to-room correlations to guide the action decision.
For object-entity reasoning, we perform internal and exter-nal knowledge graph reasoning complementarily, where the commonsense knowledge is dynamically learned and ex-tracted from the external graph to interact with the internal knowledge at each iteration of graph reasoning. Last but not least, we devise a Direction-Aware Loss (DAL) to pe-nalise the actions with more angle deviation from ground truth path, and a distance-aware policy to make agent prop-erly consider the moving distance during navigation to fur-ther improve the efﬁciency.
Experiments conducted on the REVERIE benchmark show our CKR model signiﬁcantly boosts the SPL and
REVERIE-success rate by 64.67% and 46.05% respectively on val-unseen set. Besides, extensive ablations and qualita-tive results verify the contribution of each component. 2.