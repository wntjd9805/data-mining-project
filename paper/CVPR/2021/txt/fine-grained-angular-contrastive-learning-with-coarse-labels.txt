Abstract
Training (coarse) (few-shot) Testing (fine) g o d t a c h s i f
Few-shot learning methods offer pre-training techniques optimized for easier later adaptation of the model to new classes (unseen during training) using one or a few ex-amples. This adaptivity to unseen classes is especially important for many practical applications where the pre-trained label space cannot remain ﬁxed for effective use and the model needs to be ”specialized” to support new categories on the ﬂy. One particularly interesting scenario, essentially overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where the training classes (e.g. animals) are of much ‘coarser granularity’ than the tar-get (test) classes (e.g. breeds). A very practical example of C2FS is when the target classes are sub-classes of the training classes. Intuitively, it is especially challenging as (both regular and few-shot) supervised pre-training tends to learn to ignore intra-class variability which is essential
In this paper, we introduce a for separating sub-classes. novel ’Angular normalization’ module that allows to effec-tively combine supervised and self-supervised contrastive pre-training to approach the proposed C2FS task, demon-strating signiﬁcant gains in a broad study over multiple baselines and datasets. We hope that this work will help to pave the way for future research on this new, challenging, and very practical topic of C2FS classiﬁcation. 1.

Introduction
In the most commonly encountered learning scenario, supervised learning, a set of target (class) labels is provided for a set of samples (images) using which we train a model (CNN [26, 18, 53] or Transformer [66, 40]) that casts these samples into some representation space from which predic-tions are made, e.g. using a linear classiﬁer. Neverthe-less, while supervised learning is a very common setting, in many practical applications the set of the target labels of interest is not static, and may change over time. One good example is few-shot learning [57, 49, 55], where a model is pre-trained in such a way that more classes could be added
∗Equal contribution beagle collie pug Maltese persian ragdoll scottish british d o g c a t f i s h siamese guppy goldfish sunfish i p g american rex ridgeback silkie d o p h n baji atlantic hourglass i l spinner s e e n i g u n e a u n s e e n
Figure 1. The Coarse-to-Fine Few-Shot (C2FS): During training we observe only coarse class labels (in red, e.g. animals), while at test time we are expected to adapt our model to support the ﬁne classes (in blue, e.g. breeds) using one or few samples. The ﬁne classes may be sub-classes of the train classes (seen) or sub-classes of classes unseen during training. later with only very few additional labeled examples used for adapting the model to support these new classes.
However, in previous few-shot learning works most (if not all) of the new classes: (i) are separate from the classes the model already knows, in the sense that they either be-long to a different branch of the class hierarchy or are sib-lings to the known classes; and (ii) are of same or sim-ilar level of granularity (same level of class hierarchy).
But what about the very practical situation when the new classes are ﬁne-grained sub-classes strictly included inside the known (coarse) classes being their descendants in the class taxonomy? This situation typically occurs during the lifespan of the model when the application requires sepa-rating some sub-classes of the current classes into separate classes and yet when the training dataset was created these (unknown in advance) sub-classes were not annotated. For example, this could occur in product specialization for prod-uct search, or during personalizing a generic model to a spe-ciﬁc customer. Naturally, going back to re-labeling each time this occurs is much too costly to be an option.
In this paper, we target the Coarse-to-Fine Few-Shot (C2FS) task (Fig. 1) where a model pre-trained on a set of base classes (denoted as the ‘coarse’ classes), needs to 18730
Coarse
Fine (intra-class) self-sup losses coarse-sup  losses i d e s v r e p u
S s r u
O
Figure 2. Learned embedding tSNE visualization: Top - coarse-supervised baseline, Bottom - ours (ANCOR). Left - coarse classes, right - ﬁne sub-classes of one arbitrary coarse class. Stars are em-beddings of the linear classiﬁer (class) weight vectors, black ar-rows point from the class weight to the ﬁne sub-classes centroids.
Clearly, ANCOR induces order on the sub-classes arranging them nicely around the class weight and making them separable. adapt on the ﬂy to an additional set of target (‘ﬁne’) classes of much ‘ﬁner granularity’ than the training classes. The target classes could be sub-classes of the base classes (a particularly interesting case), or they could be a separate set, yet requiring much stronger (than base classes) atten-tion to ﬁne-grained details in order to visually separate. To be efﬁcient, we want this adaptation to occur using only one
Intuitively, this or few samples of the ﬁne (sub-)classes. setup is particularly challenging for models pre-trained on the coarse classes in ‘the standard’ supervised manner, as: (a) standard supervised learning losses do not care about the intra-class arrangement of the samples belonging to the same class in the model’s feature space F, as long as these samples are close to each other and the regions associated with different classes are separable (Fig. 2 top-left) - po-tentially causing the sub-classes to spread arbitrarily inside same-class-associated regions of F thus hindering their sep-arability (Fig. 2 top-right); and (b) F is retaining the infor-mation on the attributes needed to predict the set of target
‘coarse’ labels, while at the same time reducing intra-class variance and suppressing attributes not relevant to the task for better generalization, which may eliminate the intra-class distinctions between sub-classes (Fig. 2 top-right).
In contrast to supervised learning, recently emerged con-trastive self-supervised methods [3, 17, 2, 13] were proven highly instrumental in learning good features without any labels. These methods are able to pre-train effectively at-taining almost the same representation (feature) quality as fully supervised counterparts, and even surpassing it when transferring to other tasks (e.g. detection [17]). Even more importantly, these methods are optimizing features for ’in-Angular 
Normalization images coarse labels
Figure 3. Angular Normalized COntrastive Regularization (ANCOR): our method jointly employs inter-class supervised and intra-class self-supervised contrastive losses that would pull to different directions without our proposed Angular normalization component that separates the forces applied by the two losses to different planes leading to signiﬁcant performance gains. stance recognition’, retaining the information for identi-fying the ﬁne details that separate instances between and within classes in the dataset, and thus likely also retaining features needed for effective sub-class recognition. That be-ing said, contrastive methods have so far been mostly eval-uated examining their ability for inter-class separation in a relatively favorable condition of an abundance of unlabeled data (e.g. ImageNet). And yet, naive use of these methods for the C2FS task is sub-optimal. On their own, they lack the use of coarse labels supervision. And when naively used jointly with coarse-supervised losses, their lack of synergy with those losses leads to lower gains (Sec. 4.5.1).
Building upon advances in contrastive self-supervised learning, we propose the Angular Normalized COntrastive
Regularization (ANCOR) approach for the C2FS task.
It enables few-shot adaptation to ﬁne-grained (sub-)classes using few examples, while pre-training using only coarse class labels. Our approach (Fig. 3) effectively combines, in a multi-task manner, the supervised pre-training on the coarse classes that ensures inter-class separation, with con-trastive self-supervised intra-class learning that facilitates the self-organization and separability of the ﬁne sub-classes in the resulting feature space (Fig. 2 bottom). Our method features a novel angular normalization component that en-hances the synergy between the supervised and contrastive self-supervised tasks, minimizing friction between them by separating their forces to different planes. We compare AN-COR to a diverse set of baselines and ablations, on multiple datasets, both underlining its effectiveness and providing a strong basis for future studies of the proposed C2FS task.
To summarize, our contribution is threefold: (i) we pro-pose the Coarse-to-Fine Few-Shot (C2FS) task of training using only coarse class labels and adapting to support ﬁner (sub-)classes with few (even one) examples; (ii) we pro-pose the ANCOR approach for C2FS task, based on effec-tive multi-task combination of supervised inter-class and self-supervised intra-class learning, featuring a novel an-gular normalization component to minimize friction and maximize the synergy between the two tasks; (iii) we of-fer extensive evaluation and analysis showing the strength of our proposed ANCOR approach on a variety of datasets 8731
and compared to a diverse set of baselines. 2.