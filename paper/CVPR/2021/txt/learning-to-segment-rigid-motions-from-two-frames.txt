Abstract
Appearance-based detectors achieve remarkable perfor-mance on common scenes, beneﬁting from high-capacity models and massive annotated data, but tend to fail for scenarios that lack training data. Geometric motion segmen-tation algorithms, however, generalize to novel scenes, but have yet to achieve comparable performance to appearance-based ones, due to noisy motion estimations and degenerate motion conﬁgurations. To combine the best of both worlds, we propose a modular network, whose architecture is mo-tivated by a geometric analysis of what independent object motions can be recovered from an egomotion ﬁeld. It takes two consecutive frames as input and predicts segmentation masks for the background and multiple rigidly moving ob-jects, which are then parameterized by 3D rigid transforma-⇤Code is available at github.com/gengshan-y/rigidmask. tions. Our method achieves state-of-the-art performance for rigid motion segmentation on KITTI and Sintel. The inferred rigid motions lead to a signiﬁcant improvement for depth and scene ﬂow estimation. 1.

Introduction
Autonomous agents such as self-driving cars need to be able to navigate safely in dynamic environments. Static en-vironments are far easier to process because one can make use of geometric constraints (SFM/SLAM) to infer scene structure [15]. Dynamic environments require the fundamen-tal ability to both segment moving obstacles and estimate their depth and speed [2]. Popular solutions include object detection or semantic segmentation [27]. While one can build accurate detectors for many categories of objects that are able to move, “being able to move” is not equivalent to 1266
“moving”. For example, there is a profound difference be-tween a parked car and an ever-so-slightly moving car (that is pulling out of parked location), in terms of the appropriate response needed from a nearby autonomous agent. Secondly, class-speciﬁc detectors rely heavily on appearance cues and categories present in a training set. Consider a trash can that falls on the street; current closed-world detectors will likely not be able to model all types of moving debris. This poses severe implications for safety in the open-world that a truly autonomous agent must operate [4].
Problem formulation: We follow historic work on motion-based perceptual grouping [23, 43, 50, 56, 61] and segment moving objects without relying on appearance cues. Speciﬁ-cally, we focus on segmenting rigid bodies from two frames.
We focus on two-frame because it is the minimal set of in-puts to study the problem of motion segmentation, and in practice, perception-for-autonomy needs to respond immedi-ately to dynamic scenes, e.g., an animal that appears from behind an occlusion. We focus on rigid body and its 3D motion parameterizations because it’s directly relevant for an autonomous agent acting in a 3D world. While dynamic scenes often contain nonrigid objects such as people, we ex-pect that deformable objects may be modeled as a rigid body over short time scales, or decomposed into rigidly-moving parts [1, 8].
Challenges: Earlier work on rigid motion segmentation of-ten makes use of geometric constraints arising from epipolar geometry and rigid transformations. However, there are sev-eral fundamental difﬁculties that plague geometric motion segmentation. First, epipolar constraints fail when cam-era motion is close to zero [61]. Second, points moving along epipolar lines cannot be distinguished from the rigid background [65], which we discuss at length in Sec. 3.1.
Third, geometric criteria are often not robust enough to noisy motion correspondences and camera egomotion estimates, which can lead to catastrophic failures in practice.
Method: We theoretically analyze ambiguities in 3D rigid motion segmentation, and resolve such ambiguities by ex-ploiting recent techniques for upgrading 2D motion observa-tion to 3D with optical expansion [63] and monocular depth cues [38]. To deal with noisy motion correspondences and degenerate scene motion, we design a convolutional archi-tecture that segments the rigid background and an arbitrary number of rigid bodies from a given motion ﬁeld. Finally, we parameterize the 3D motion of individual rigid bodies by
ﬁtting 3D rigid transformations.
Contributions: (1) We provide a geometric analysis for am-biguities in 3D rigid motion segmentation from 2D motion
ﬁelds, and introduce solutions to deal with such ambigui-ties. (2) We propose a geometry-aware architecture for 3D rigid motion segmentation from two RGB frames, which is generalizable to novel appearance, resilient to different motion types and robust to noisy motion observations. (3)
Our method achieves state-of-the-art (SOTA) performance of rigid motion segmentation on KITTI/Sintel. The inferred rigidity masks signiﬁcantly improve the performance of downstream depth and scene ﬂow estimation tasks. 2.