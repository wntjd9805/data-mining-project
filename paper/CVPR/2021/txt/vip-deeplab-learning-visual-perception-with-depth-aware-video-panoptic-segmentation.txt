Abstract
In this paper, we present ViP-DeepLab, a uniﬁed model attempting to tackle the long-standing and challenging in-verse projection problem in vision, which we model as restoring the point clouds from perspective image se-quences while providing each point with instance-level se-mantic interpretations. Solving this problem requires the vi-sion models to predict the spatial location, semantic class, and temporally consistent instance label for each 3D point.
ViP-DeepLab approaches it by jointly performing monocu-lar depth estimation and video panoptic segmentation. We name this joint task as Depth-aware Video Panoptic Seg-mentation, and propose a new evaluation metric along with two derived datasets for it, which will be made available to the public. On the individual sub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming previ-ous methods by 5.1% VPQ on Cityscapes-VPS, ranking 1st on the KITTI monocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The datasets and the eval-uation codes are made publicly available1. 1.

Introduction
The inverse projection problem, one of the most funda-mental problems in vision, refers to the ambiguous mapping from the retinal images to the sources of retinal stimula-tion. Such a mapping requires retrieving all the visual infor-mation about the 3D environment using the limited signals contained in the 2D images [60, 62]. Humans are able to easily establish this mapping by identifying objects, deter-mining their sizes, and reconstructing the 3D scene layout, etc. To endow machines with similar abilities to visually perceive the 3D world, we aim to develop a model to tackle the inverse projection problem.
As a step towards solving the inverse projection, the problem is simpliﬁed as restoring the 3D point clouds with semantic understandings from the perspective image se-*Work done while an intern at Google. 1https://github.com/joe-siyuan-qiao/ViP-DeepLab
Figure 1: Projecting 3D points to the image plane results in 2D images. We study the inverse projection problem: how to restore the 3D points from 2D image sequences while providing temporally consistent instance-level semantic in-terpretations for the 3D points. quences, which calls for vision models to predict the spatial location, semantic class, and temporally consistent instance label for each 3D point. Fig. 1 shows an example of the in-verse projection problem we study in this paper. This sim-pliﬁed problem can be formulated as Depth-aware Video
Panoptic Segmentation (DVPS) that contains two sub-tasks: (i) monocular depth estimation [68], which is used to esti-mate the spatial position of each 3D point that is projected to the image plane, and (ii) video panoptic segmentation [42], which associates the 3D points with temporally consistent instance-level semantic predictions.
For the new task DVPS, we present two derived datasets accompanied by a new evaluation metric named Depth-aware Video Panoptic Quality (DVPQ). DVPS datasets are hard to collect, as they need special depth sensors and a huge amount of labeling efforts. Existing datasets usually lack some annotations or are not in the format for DVPS.
Our solution is to augment and convert existing datasets for DVPS, producing two new datasets, Cityscapes-DVPS and SemKITTI-DVPS. Cityscapes-DVPS is derived from
Cityscapes-VPS [42] by adding depth annotations from
Cityscapes dataset [18], while SemKITTI-DVPS is derived from SemanticKITTI [6] by projecting its annotated 3D point clouds to the image plane. Additionally, the proposed metric DVPQ includes the metrics for depth estimation and 3997
video panoptic segmentation, requiring a vision model to simultaneously tackle the two sub-tasks. To this end, we present ViP-DeepLab, a uniﬁed model that jointly performs video panoptic segmentation and monocular depth estima-tion for each pixel on the image plane. In the following, we introduce how ViP-DeepLab tackles the two sub-tasks.
The ﬁrst sub-task of DVPS is video panoptic segmen-tation [42]. Panoptic segmentation [43] uniﬁes semantic segmentation [36] and instance segmentation [34] by as-signing every pixel a semantic label and an instance ID.
It has been recently extended to the video domain, result-ing in video panoptic segmentation [42], which further de-mands each instance to have the same instance ID through-out the video sequence. This poses additional challenges to panoptic segmentation as the model is now expected to be able to track objects in addiction to detecting and seg-menting them. Current approach VPSNet [42] adds a track-ing head to learn the correspondence between the instances from different frames based on their regional feature simi-larity. By contrast, our ViP-DeepLab takes a different ap-proach to tracking objects. Speciﬁcally, motivated by our
ﬁnding that video panoptic segmentation can be modeled as concatenated image panoptic segmentation, we extend
Panoptic-DeepLab [17] to perform center regression for two consecutive frames with respect to only the object centers that appear in the ﬁrst frame. During inference, this off-set prediction allows ViP-DeepLab to group all the pixels in the two frames to the same object that appears in the
ﬁrst frame. New instances emerge if they are not grouped to the previously detected instances. This inference pro-cess continues for every two consecutive frames (with one overlapping frame) in a video sequence, stitching panop-tic predictions together to form predictions with temporally consistent instance IDs. Based on this simple design, our
ViP-DeepLab outperforms VPSNet [42] by a large margin of 5.1% VPQ, setting a new record on the Cityscapes-VPS dataset [42]. Additionally, Multi-Object Tracking and Seg-mentation (MOTS) [77] is a similar task to video panop-tic segmentation, but only segments and tracks two classes: pedestrians and cars. We therefore also apply our ViP-DeepLab to MOTS. As a result, ViP-DeepLab outperforms the current state-of-the-art PointTrack [92] by 7.2% and 2.5% sMOTSA on pedestrians and cars, respectively, and ranks 1st on the leaderboard for KITTI MOTS pedestrian.
The second sub-task of DVPS is monocular depth es-timation, which is challenging for both computers [68] and humans [38]. The state-of-the-art methods are mostly based on deep networks trained in a fully-supervised way [20, 21, 22, 26]. Following the same direction, our ViP-DeepLab appends another depth prediction head on top of Panoptic-DeepLab [17]. Without using any additional depth training data, such a simple approach outperforms all the published and unpublished works on the KITTI benchmark [30].
Speciﬁcally, it outperforms DORN [26] by 0.97 SILog, and even outperforms MPSD that uses extra planet-scale depth data [2], breaking the long-standing record on the challeng-ing KITTI depth estimation [74]. Notably, the differences between top-performing methods are all around 0.1 SILog, while our method signiﬁcantly outperforms them.
To summarize, our contributions are listed as follows.
• We propose a new task Depth-aware Video Panop-tic Segmentation (DVPS), as a step towards solving the inverse projection problem by formulating it as joint video panoptic segmentation [42] and monocular depth estimation [68].
• We present two DVPS datasets along with an eval-uation metric Depth-aware Video Panoptic Quality (DVPQ). To facilitate future research, the datasets and the evaluation codes are made publicly available.
• We develop ViP-DeepLab, a uniﬁed model for DVPS.
On the individual sub-tasks, ViP-DeepLab ranks 1st on
Cityscapes-VPS [42], KITTI-MOTS pedestrian [77], and KITTI monocular depth estimation [30]. 2.