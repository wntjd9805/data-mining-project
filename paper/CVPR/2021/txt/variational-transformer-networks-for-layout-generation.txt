Abstract
Generative models able to synthesize layouts of different kinds (e.g. documents, user interfaces or furniture arrange-ments) are a useful tool to aid design processes and as a ﬁrst step in the generation of synthetic data, among other tasks.
We exploit the properties of self-attention layers to capture high level relationships between elements in a layout, and use these as the building blocks of the well-known Varia-tional Autoencoder (VAE) formulation. Our proposed Vari-ational Transformer Network (VTN) is capable of learning margins, alignments and other global design rules without explicit supervision. Layouts sampled from our model have a high degree of resemblance to the training data, while demonstrating appealing diversity. In an extensive evalua-tion on publicly available benchmarks for different layout types VTNs achieve state-of-the-art diversity and percep-tual quality. Additionally, we show the capabilities of this method as part of a document layout detection pipeline. 1.

Introduction
Layouts, i.e. the abstract positioning of elements in a scene or document, constitute an essential tool for various downstream tasks. Consequently, the ability to ﬂexibly ren-der novel, realistic layouts has the potential to yield sig-niﬁcant improvements in many tasks, such as neural scene synthesis [36], graphic design or in data synthesis pipelines.
Even though the task of synthesizing novel layouts has re-cently started to gain the attention of the deep learning com-munity [23, 16, 22, 28], it is still a sparsely explored area and provides unique challenges to generative models based on neural networks, namely a non-sequential data structure consisting of varying length samples with discrete (classes) and continuous (coordinates) elements simultaneously.
Generative models based on neural networks have re-ceived a signiﬁcant share of attention in recent years, as they proved capable of learning complex, high-dimensional distributions. Common formulations such as Generative
Adversarial Networks (GANs) [8] and Variational Autoen-Document
UI design Natural Scene Room layout
Figure 1: Given a random vector z, our novel transformer
VAE model produces layouts that follow the design con-straints of the training data. It can generate various layouts types, from documents to objects and scenes. coders (VAEs) [21] have shown impressive results in tasks such as image translation [43], image synthesis [17], and text generation [2]. A GAN is comprised of an arrangement of generator-discriminator neural networks in a zero-sum conﬁguration, while a VAE learns a lower bound of the data distribution using an encoder-decoder neural network with a regularized bottleneck. Since these are general frameworks, they leave room for adapting the underlying neural architec-tures to exploit the properties of the data. For example, the weight sharing strategy of Convolutional Neural Networks (CNNs) renders them the most common building block for image processing, while for sequential data (e.g., text), Re-current Neural Networks (RNNs) or attention modules are often the architecture of choice. In particular, the attention mechanism has recently demonstrated strong performance on a variety of tasks, such as language translation [35] and object detection [3], proving its superiority over RNNs re-garding modeling long-term relationships.
Prior work has built the foundation by proving the ef-13642
fectiveness of deep learning to generate novel documents
[22, 28, 9], natural scenes [16] and User Interface (UI) de-signs [22]. Mostly, the location and size of a given element depends not only on the particularities of its type (e.g. titles tend to be small and at the top of a document, while ﬁgures or tables usually occupy a signiﬁcant amount of space), but also on their relationship to other elements. One way to incorporate this knowledge into modeling a layout distri-bution is to deﬁne handcrafted rules, (e.g. enforcing mar-gins, alignment, the allowed number of elements in a docu-ment. . . ). However, such rules are subjective, hard to deﬁne unambiguously and certainly do not generalize to arbitrary layout distributions. Consequently, we refrain from mod-eling any prior knowledge by i.e. enforcing heuristics, and instead equip the neural architecture itself with an inherent bias towards learning the relationship between elements in a layout. This makes the attention mechanism a suitable fun-damental architectural component, since it naturally models many-to-many relationships and is, thus, particularly suit-able for discovering relationships in a given layout distribu-tion in an unsupervised manner.
By instantiating the VAE framework with an attention-based architecture, this work investigates an important gap in literature. We explore relevant design choices in great detail - e.g. autoregressive vs. non-autoregressive decoder, learned vs. non-learned prior. Furthermore, we tailor our novel approach to the yet under-explored task of layout generation, where we demonstrate state-of-the-art perfor-mance across various metrics on several publicly available datasets. To summarize, our main contributions are:
• A novel generative model specialized in layout gener-ation that incorporates an inductive bias towards high-level relationships between a large number of elements in a layout without annotations.
• Exploration of strategies for creating a variational bot-tleneck on sequences with varying lengths. 2.