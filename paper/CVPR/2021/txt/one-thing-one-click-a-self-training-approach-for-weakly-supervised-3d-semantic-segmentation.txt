Abstract
Point cloud semantic segmentation often requires large-scale annotated training data, but clearly, point-wise la-bels are too tedious to prepare. While some recent methods propose to train a 3D network with small percentages of point labels, we take the approach to an extreme and pro-pose “One Thing One Click,” meaning that the annotator only needs to label one point per object. To leverage these extremely sparse labels in network training, we design a novel self-training approach, in which we iteratively conduct the training and label propagation, facilitated by a graph propagation module. Also, we adopt a relation network to generate the per-category prototype and explicitly model the similarity among graph nodes to generate pseudo labels to guide the iterative training. Experimental results on both
ScanNet-v2 and S3DIS show that our self-training approach, with extremely-sparse annotations, outperforms all existing weakly supervised methods for 3D semantic segmentation by a large margin, and our results are also comparable to those of the fully supervised counterparts. 1.

Introduction
The success of 3D semantic segmentation beneﬁts a lot from the large annotated training data. However, annotating a large amount of point cloud data is exhausting and costly.
Taking ScanNet-v2[7] as an example, it takes 22.3 minutes to annotate one scene on average. It is a great burden to annotate the whole data set, which includes 1,513 scenes, thus potentially restricting further applications that require larger scale data. Thus, efﬁcient approaches to facilitate 3D data annotation are highly desirable.
Very recently, some methods [47, 46, 50] were proposed to reduce efforts to annotating 3D point clouds. Though they improve annotation efﬁciency, various issues remain. Scene-level annotation in [47] could impose negative effects on the model in the absence of localization information, whereas sub-cloud annotation in [47] requires an extra burden to ﬁrst
Figure 1. Comparing our approach of “One Thing One Click” (1T1C) with two recent weakly supervised methods MPRM [47] (CVPR 2020) and Xu’s [50] (CVPR 2020) and a fully supervised version of our method Fully-sup on 3D semantic segmentation of
ScanNet-v2 and S3DIS. Our approach achieves better performance by training on data with only one label per object. Note the anno-tation percentages under each method in the charts. If “One Thing
Three Clicks” (1T3C) is allowed, we can further raise our result.
Figure 2. We train our self-training approach using only our “One
Thing One Click” annotations (top). Yet, it can produce plausible segmentation results close to the ground truth (bottom). divide the input into subclouds and then repeatedly annotate semantic categories in individual subclouds. The 2D image annotation approach [46] requires extra labor to prepare a 2D image annotation, which is also a tedious task on its own. Xu et al. [50] presume that the labeled points follow a uniform distribution. Such a requirement can be achieved by 1726
subsampling from a fully-annotated dataset, but is hard for the annotators to follow in practice.
In this work, we also aim to reduce the amount of neces-sary annotations on point clouds, but we take the approach to an extreme by proposing “One Thing One Click,” so the annotator only needs to label one single point per object.
To further relieve the annotation burden, such a point can be randomly chosen, not necessarily at the center of the ob-ject. On average, it takes less than 2 minutes to annotate a
ScanNet-v2 scene with our “One Thing One Click” scheme (see an example annotation in Figure 2 (b), which contains only 13 clicks), which is more than 10x faster compared with the original ScanNet-v2 annotation scheme.
However, directly training a network on the extremely-sparse labels from our annotating scheme (less than 0.02% in ScanNet-v2 and S3DIS) will easily make the network overﬁt the limited data and restrict its generalization ability.
Hence, it raises a question that “can we achieve a perfor-mance comparable with a fully supervised baseline given the extremely-sparse annotations?” To meet such a challenge, we design a self-training approach with a label-propagation mechanism for weakly supervised semantic segmentation.
On the one hand, with the prediction result of the model, the pseudo labels can be expanded to unknown regions through our graph propagation module. On the other hand, with richer and higher quality labels being generated, the model performance can be further improved. Thus, we conduct the label propagation and network training iteratively, forming a closed loop to boost the performance of each other.
A core problem of label propagation is how to measure the similarity among nodes. Previous works [54, 5, 52] build a graph model upon 2D pixels and measure the similarity with low-level image features, e.g., coordinates and colors.
In contrast, our graph is built upon the 3D super-voxels with more complex geometric structures and a variable number of points in each group. Hence, existing hand-craft features can-not fully reveal the similarity among nodes in our case. To resolve this problem, we further propose a relation network to leverage 3D geometrical information for similarity learn-ing among the graph nodes in 3D. The geometrical similarity and learned similarity are integrated together to facilitate label propagation. To effectively train the relation network with the extremely-sparse and category-unbalanced data, we further propose to generate a category-wise prototype with a memory bank for better similarity measurement.
Experiments conducted on two public data sets ScanNet-v2 and S3DIS manifest the effectiveness of the proposed method. With just around 0.02% point annotations, our ap-proach surpasses all existing weakly supervised approaches (which employ far more labels) for 3D point cloud segmen-tation by a large margin, and our approach even achieves results that are comparable with a fully supervised counter-part; see Figure 1. These results manifest the high efﬁciency of our “One Thing One Click” scheme for 3D point cloud an-notation and the effectiveness of our self-training approach for weakly supervised 3D semantic segmentation. 2.