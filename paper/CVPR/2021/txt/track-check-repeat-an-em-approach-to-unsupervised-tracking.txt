Abstract
We propose an unsupervised method for detecting and tracking moving objects in 3D, in unlabelled RGB-D videos.
The method begins with classic handcrafted techniques for segmenting objects using motion cues: we estimate opti-cal ﬂow and camera motion, and conservatively segment regions that appear to be moving independently of the back-ground. Treating these initial segments as pseudo-labels, we learn an ensemble of appearance-based 2D and 3D detectors, under heavy data augmentation. We use this ensemble to detect new instances of the “moving” type, even if they are not moving, and add these as new pseudo-labels. Our method is an expectation-maximization algo-rithm, where in the expectation step we ﬁre all modules and look for agreement among them, and in the maximization step we re-train the modules to improve this agreement. The constraint of ensemble agreement helps combat contami-nation of the generated pseudo-labels (during the E step), and data augmentation helps the modules generalize to yet-unlabelled data (during the M step). We compare against existing unsupervised object discovery and tracking meth-ods, using challenging videos from CATER and KITTI, and show strong improvements over the state-of-the-art. 1.

Introduction
Humans can detect moving objects and delineate their approximate extent [54, 15], without ever having been sup-plied boxes or segmentation masks as supervision. Where does this remarkable ability come from? Psychology litera-ture points to a variety of perceptual grouping cues, which make some regions look more object-like than others [25].
These types of objectness cues have long been known in computer vision literature [1], yet this domain knowledge has not yet led to powerful self-supervised object detectors.
Work on integrating perceptual grouping cues into com-puter vision models stretches back decades [51], and still likely serves as inspiration for many of the design decisions
*Equal contribution. in modern computer vision architectures related to atten-tion, segmentation, and tracking. Much of the current work on recognition and tracking is fully-supervised, and relies on vast pools of human-provided annotations. On the un-supervised side, a variety of deep learning-based methods have been proposed, which hinge on reconstruction objec-tives and part-centric, object-centric, or scene-centric bot-tlenecks in the architecture [38, 9]. These methods are rapidly advancing, but so far only on toy worlds, made up of simple 2D or 3D shapes against simple backgrounds – a far cry from the complexity tackled in older works, based on perceptual grouping (e.g., [21]).
Classic methods of object discovery, such as center-surround saliency in color or ﬂow [2], are known to be brit-tle, but they need not be discarded entirely. In this paper, we propose to mine and exploit the admittedly rare suc-cess scenarios of these models, to bootstrap the learning of something more general. We hypothesize that if the suc-cessful vs. unsuccessful runs of the classic algorithms can be readily identiﬁed with automatic techniques, then we can self-supervise a learning-based module to mimic and out-perform the traditional methods. This is a kind of knowl-edge distillation [30], from traditional models to deep ones.
We propose an optimization algorithm for learning de-tectors of moving objects, based on expectation maximiza-tion [45]. We begin with a motion-based handcrafted de-tector, tuned to be very conservative (low recall, high pre-cision). We then convert each object proposal into thou-sands of training examples for learning-based 2D and 3D detectors, by randomizing properties like color, scale, and orientation. This forces the learned models to general-ize, and allows recall to expand. We then use the ensem-ble of models to obtain new high-conﬁdence estimates (E step), repeat the optimization (M step), and iterate. Our method outperforms not only the traditional methods, which only work under speciﬁc conditions, but also the current learning-based methods, which only work in toy environ-ments. We demonstrate success in a popular synthetic envi-ronment where recent deep models have already been de-ployed (CLEVR/CATER [34, 24]), and also on the real-16581
world urban scenes benchmark (KITTI [23]), where the ex-isting learned models fall ﬂat.
Our main contribution is not in any particular compo-nent, but rather in their combination. We demonstrate that by exploiting the successful outcomes of traditional methods for moving object segmentation, we can train a learning-based method to detect and track objects in a target domain, without requiring any annotations. 2.