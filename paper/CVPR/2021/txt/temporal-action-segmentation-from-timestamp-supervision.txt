Abstract
Temporal action segmentation approaches have been very successful recently. However, annotating videos with frame-wise labels to train such models is very expensive and time consuming. While weakly supervised methods trained using only ordered action lists require less annotation ef-fort, the performance is still worse than fully supervised approaches. In this paper, we propose to use timestamp su-pervision for the temporal action segmentation task. Time-stamps require a comparable annotation effort to weakly supervised approaches, and yet provide a more supervisory signal. To demonstrate the effectiveness of timestamp su-pervision, we propose an approach to train a segmentation model using only timestamps annotations. Our approach uses the model output and the annotated timestamps to gen-erate frame-wise labels by detecting the action changes. We further introduce a conﬁdence loss that forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This ensures that all and not only the most distinctive frames of an action are learned during training. The evaluation on four datasets shows that mod-els trained with timestamps annotations achieve compara-ble performance to the fully supervised approaches. 1.

Introduction
Analyzing and understanding video content is very im-portant for many applications, such as surveillance or in-telligent advertisement. Recently, several approaches have been very successful in analyzing and segmenting activities in videos [20, 24, 1, 29, 41]. Despite the success of the pre-vious approaches, they rely on fully annotated videos where the start and end frames of each action are annotated.
This level of supervision, however, is very time consum-ing and hard to obtain. Furthermore, as the boundaries be-tween action segments are usually ambiguous, this might result in inconsistencies between annotations obtained from different annotators. To alleviate these problems, many re-searchers start exploring weaker levels of supervision in the form of transcripts [3, 35, 25] or even sets [34, 11, 26]. For
Figure 1. For fully supervised action segmentation, each frame in the training videos is annotated with an action label (top). This process is time-consuming since it requires an accurate annotation of the start and end frame of each action. To reduce the annotation effort, we propose to use timestamps as supervision (bottom). In this case, only one arbitrary frame needs to be annotated for each action and the annotators do not need to search for the start and end frames, which is the most time-consuming annotation part. transcript-level supervision, the videos are annotated with an ordered list of actions occurring in the video without the starting and ending time of each action. Whereas for the set-level supervision, only the set of actions are provided without any information regarding the order or how many times each action occurs in the videos.
While transcript-level and set-level supervision signiﬁ-cantly reduce the annotation effort, the performance is not satisfying and there is still a gap compared to fully super-vised approaches. In this paper, inspired by the recently in-troduced timestamp supervision for action recognition [31], we propose to use timestamp supervision for the action segmentation task to address the limitations of the current weakly supervised approaches. For timestamp supervision, only one frame is annotated from each action segment as il-lustrated in Fig. 1. Such timestamps annotations can be ob-tained with comparable effort to transcripts, and yet it pro-vides more supervision. Besides the ordered list of actions occurring in the video, timestamps annotations give par-tial information about the location of the action segments, which can be utilized to further improve the performance. 18365
Given the timestamps annotations, the question is how to train a segmentation model with such level of supervision.
A naive approach takes only the sparsely annotated frames for training. This, however, ignores most of the informa-tion in the video and does not achieve good results as we will show in the experiments. Another strategy is to iter-ate the process and consider frames with high conﬁdence scores near the annotations as additional annotated frames and include them during training [31]. Furthermore, frames that are far away from the annotations can be considered as negative samples [28]. For temporal action segmentation, which is comparable to semantic image segmentation, how-ever, all frames need to be annotated and there are no large parts of the video that can be used to sample negative exam-ples. Furthermore, relying only on frames with high conﬁ-dence discards many of the video frames that occur during an action and focuses only on the most distinctive frames of an action, which can be sufﬁcient for action recognition or detection but not for action segmentation.
In this work, we therefore propose a different approach where all frames of the videos are used. Instead of detecting frames of high conﬁdences, we aim to identify changes of actions in order to divide the videos into segments. Since for each action change the frames before the change should be assigned to the previous timestamp and after the change to the next timestamp, we ﬁnd the action changes by mini-mizing the variations of the features within each of the two clusters of frames. While we can then train the model on all frames by assigning the label of the timestamp to the corresponding frames, it does not guarantee that all frames of an action are effectively used. We therefore introduce a loss function that enforces a monotonic decrease in the class probabilities as the distance to the timestamps increases.
This loss encourages the model to predict higher probabili-ties for low conﬁdent regions that are surrounded by high conﬁdent frames and therefore to use all frames and not only the most distinctive frames.
Our contribution is thus three folded. 1. We propose to use timestamp supervision for the tem-poral action segmentation task, where the goal is to predict frame-wise action labels for untrimmed videos. 2. We introduce an approach to train a temporal action segmentation model from timestamp supervision. The approach uses the model predictions and the annotated timestamps for estimating action changes. 3. We propose a novel conﬁdence loss that forces the model conﬁdence to decrease monotonically as the distance to the timestamp increases.
We evaluate our approach on four datasets: 50Sal-ads [38], Breakfast [16], BEOID [7], and Georgia Tech
Egocentric Activities (GTEA) [10]. We show that training an action segmentation model is feasible with only time-stamp supervision without compromising the performance compared to the fully supervised approaches. On the 50Sal-ads dataset, for instance, we achieve 97% of the accuracy compared to fully supervised learning, but at a tiny fraction of the annotation costs. 1 2.