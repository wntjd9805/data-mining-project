Abstract
We study the problem of quantizing N sorted, scalar dat-apoints with a ﬁxed codebook containing K entries that are allowed to be rescaled. The problem is deﬁned as ﬁnding the optimal scaling factor α and the datapoint assignments into the α-scaled codebook to minimize the squared er-ror between original and quantized points. Previously, the globally optimal algorithms for this problem were derived only for certain codebooks (binary and ternary) or under the assumption of certain distributions (Gaussian, Lapla-cian). By studying the properties of the optimal quantizer, we derive an O(N K log K) algorithm that is guaranteed to ﬁnd the optimal quantization parameters for any ﬁxed codebook regardless of data distribution. We apply our al-gorithm to synthetic and real-world neural network quan-tization problems and demonstrate the effectiveness of our approach. 1.

Introduction
The appealing efﬁcacy of modern deep neural networks in a wide spectrum of tasks generally comes with a seem-ingly sharp increase in network complexity. The result-ing computation burden hinges network deployment to real-world tasks that typically face stringent resources and con-straints, e.g., mobile applications or autonomous driving. A myriad of approaches has thus been proposed to reduce net-work redundancy, quantization being the most popular and simple-to-use approach. The central question behind quan-tization is around weight simpliﬁcation: how one could re-assign a large number of high-precision weights from the network, to a far less diverse set of points speciﬁed by a codebook, hence drastically cut down on required compute.
Despite remarkable insights into this problem, theoretical analysis lacks far behind, leaving no viable guarantees to optimality of any sort even in the substeps of this process.
In this work we consider a general formulation of the scaled codebook quantization problem: given a sorted1 data vector w with elements w1 ≤ w2 ≤ · · · ≤ wN and a ﬁxed codebook C = {c1, c2, . . . , cK} ⊂ R such that c1 < c2 <
· · · < cK we would like to learn the optimal rescaling of the codebook (by a scalar α > 0) and the corresponding assignments of datapoints into the codebook entries so that the ℓ2 quantization error (MSE) is minimized:
N
K min
α, z1,...,zN
LOSS(α, Z) = s.t. zT n 1 = 1, n=1
X k=1
X zn ∈ {0, 1}K znk(wn − α ck)2 (1)
Here, zn is a binary assignment vector deﬁned for each dat-apoint wn: if znk = 1 then the datapoint wn is assigned to the codebook entry ck.
The problems of this type arise in various settings of sig-nal processing and have recently gained a signiﬁcant inter-est in the ﬁeld of neural network compression. For instance, if we set C = {0, ±1, ±2... ± 27} we recover the symmet-ric variant of INT8 quantization scheme of Jacob et al. [1] which is particularly advantageous for efﬁcient inference, and currently has become a standard part of deep-learning frameworks with many accompanying studies and imple-mentations [2, 3]. With a codebook of C = {−1, 0, 1} we recover the scaled ternary quantization scheme, and if we set C = {0, ±21, ±22, . . . , ±2b} we recover a scaled powers-of-two quantization scheme.
Previously, it was believed that the general formulation of scaled quantization problem (1) is hard to optimize [4] and requires exponential-time algorithm [5]. Therefore, most of the approaches in the literature used heuristic search or alternating optimization without any optimality guaran-In this paper, we present an optimal algorithm that tees.
*Work performed during a Summer Internship at NVIDIA. 1We do not include the cost of sorting into our analysis. 12095
is guaranteed to ﬁnd the solution of (1) in O(N K log K) time using O(N ) memory. While optimal algorithms are known for special types of codebooks (see sec. 2), to the best of our knowledge, this is the ﬁrst result with a generic algorithm that can handle any codebook C. Our approach is based on studying the properties of the optimal quantizer of (1) and leveraging possible shortcuts which we present in sections 3–4. The numerical experiments are presented in section 5. 2.