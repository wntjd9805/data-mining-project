Abstract
Coordinate-based neural representations have shown signiﬁcant promise as an alternative to discrete, array-based representations for complex low dimensional signals.
However, optimizing a coordinate-based network from ran-domly initialized weights for each new signal is inefﬁcient.
We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being rep-resented (e.g., images of faces or 3D models of chairs). De-spite requiring only a minor change in implementation, us-ing these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better gener-alization when only partial observations of a given signal are available. We explore these beneﬁts across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations. 1.

Introduction
Figure 1. A coordinate-based MLP, illustrated on the left, takes a coordinate as input and outputs a value at that location. For exam-ple, the network could take in a pixel coordinate (x, y) and emit the (R, G, B) color at that pixel as output, thereby representing a 2D image. The network weights θ are typically optimized via gradient descent to produce the desired image, as depicted on the right. However, ﬁnding good parameters can be computationally expensive, and the full optimization process must be repeated for each new target. We propose using meta-learning to ﬁnd initial network weights θ∗ 0 that allow for faster convergence and better generalization.
Recent work has demonstrated the potential of repre-senting complex low-dimensional signals using deep fully-connected neural networks (typically referred to as multi-layer perceptrons, or MLPs). A coordinate-based neural representation fθ for a given signal is an MLP (with weights
θ) that is optimized to map from an input coordinate x to the signal’s value at that coordinate. For example, fθ could map from 2D pixel coordinates to RGB color values to encode an image. Unlike a signal stored as a discretely sampled array of values, a coordinate-based neural representation is continuous and is not constrained to have a ﬁxed spatial res-olution. This fact has recently been exploited to design rep-resentations for 3D shapes (which typically occupy a small 2D subset of 3D space) that do not require cubic storage complexity, in contrast to 3D voxel grids [22, 24, 27, 34].
* Authors contributed equally to this work.
However, one limitation of these neural representations is that computing network weights θ that reproduce a given signal typically requires solving an optimization problem by running many steps of gradient descent. This can take between seconds (when encoding a small image) and hours (when solving an inverse problem to recover a high resolu-tion radiance ﬁeld, as in NeRF [24]). Common approaches to address this issue include concatenating a latent vector to the input coordinate and supervising a single neural network to represent an entire class of signals [22, 27], or training a hypernetwork to map from signal observations (or a latent code) to MLP weights [33, 34]. However, each of these strategies is restricted to representing only signals within its learned latent space, potentially limiting its ability to ex-press previously unseen target signals.
Recent work [32] has shown that optimization-based meta-learning can dramatically reduce the number of gra-2846
dient descent steps required to optimize a neural represen-tation to encode a new signal in the case of signed dis-tance ﬁelds of 2D and 3D shapes.
In this work, we pro-pose learning the weight initialization for neural represen-tations across a wide variety of underlying signal types, such as images, volumetric data, and 3D scenes. We show that compared to a standard random initialization, using
ﬁxed, learned values for the initial network weights acts as a strong prior that enables both faster convergence dur-ing optimization and better generalization when only par-tial observations of the target signal are available. In the context of using neural representations for 3D reconstruc-tion from images, a learned initialization specialized to a particular ShapeNet [2] class allows the network to recover 3D shape from a single image over the course of optimiza-tion, whereas a standard randomly initialized network fails unless provided with multiple input views. Given a meta-training set consisting of observations of different signals sampled from a ﬁxed underlying class, our setup applies an optimization-based meta-learning algorithm (MAML [6] or
Reptile [25]) in order to produce initial weights better suited for representing that speciﬁc signal class (e.g., face images from CelebA [20] or 3D chairs from ShapeNet [2]).
The biggest advantage of our approach is its simplicity.
Given an existing framework for test-time optimization of a neural representation, implementing an outer loop with
MAML or Reptile update steps only requires a few extra lines of code and a dataset of training examples. Once the meta-learning phase is complete, the learned initial weights can be stored and later reloaded in place of a standard net-work initialization whenever a new signal needs to be en-coded. This minor implementation change can signiﬁcantly alter the behavior of the network during optimization. 2.