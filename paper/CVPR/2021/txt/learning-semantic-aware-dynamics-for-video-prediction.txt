Abstract
We propose an architecture and training scheme to pre-dict video frames by explicitly modeling dis-occlusions and capturing the evolution of semantically consistent regions in the video. The scene layout (semantic map) and motion (optical ﬂow) are decomposed into layers, which are pre-dicted and fused with their context to generate future lay-outs and motions. The appearance of the scene is warped from past frames using the predicted motion in co-visible regions; dis-occluded regions are synthesized with content-aware inpainting utilizing the predicted scene layout. The result is a predictive model that explicitly represents objects and learns their class-speciﬁc motion, which we evaluate on video prediction benchmarks. 1.

Introduction
Anticipating the future is critical for autonomous agents to operate intelligently in the environment, such as for nav-igation, manipulation, and other forms of physical interac-tion. We hypothesize that decomposing the scene into inde-pendent entities, each with its own attributes, is beneﬁcial to prediction. For example, in Fig. 1, different objects have different geometry and motion, which induces distinctive temporal changes in the video.
We propose a video prediction architecture that explicitly models the different dynamics of semantically consistent re-gions (Fig. 2). The model, described in detail in Sec. 3.1, decomposes the video into regions, corresponding to differ-ent semantic classes in the scene, and learns class-speciﬁc characteristics while ensuring that their re-composition can predict the image, along with class labels and ﬂow ﬁelds.
Unlike warping the past using globally predicted ﬂow
ﬁelds [20, 19, 27, 9], in our semantic-aware dynamic model (SADM), local regions are represented by binary semantic masks, whose evolution is simpler and easier to learn than the motion of the entire video frames (see Fig. 1). Each of the regions is predicted and then fused with its content to generate future semantic maps and ﬂow ﬁelds. The predic-tion in co-visible regions of future frames is warped from
Figure 1. Different representations (video frame, semantic map,
ﬂow ﬁeld) have dynamics with different complexity. Also, differ-ent classes have different dynamics within a given representation.
Top: a sequence of video frames (left), semantic maps (middle), and ﬂow ﬁelds (right). Bottom: dynamics or changes visualized in terms of their difference. The dynamics in video frames is much more complex than that in semantic maps and ﬂow ﬁelds. the past, with dis-occlusion detection mediated by the pre-dicted semantic maps. Furthermore, the dis-occluded re-gions are ﬁlled-in by a generative model or conditional ren-derer, trained with not only the warped images, but also the predicted semantic maps, enabling more structured and semantically-aware synthesis. Modeling dis-occlusions ex-plicitly spares the model the effort otherwise needed to learn this complex phenomenon.
We incorporate semantic segmentation (scene layout), optical ﬂow (scene motion) and synthesis (scene appear-ance) into a complete generative model for videos, which facilitates semantically and geometrically consistent predic-tion of complete video frames. SADM achieves state-of-the-art performance in video prediction benchmarks such as [7, 11, 10]. 2.