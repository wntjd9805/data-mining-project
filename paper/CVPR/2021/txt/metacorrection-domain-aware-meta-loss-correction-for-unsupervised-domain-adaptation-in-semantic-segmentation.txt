Abstract
Unsupervised domain adaptation (UDA) aims to trans-fer the knowledge from the labeled source domain to the unlabeled target domain. Existing self-training based UDA approaches assign pseudo labels for target data and treat them as ground truth labels to fully leverage unlabeled tar-get data for model adaptation. However, the generated pseudo labels from the model optimized on the source do-main inevitably contain noise due to the domain gap. To tackle this issue, we advance a MetaCorrection framework, where a Domain-aware Meta-learning strategy is devised to beneﬁt Loss Correction (DMLC) for UDA semantic seg-mentation. In particular, we model the noise distribution of pseudo labels in target domain by introducing a noise transition matrix (NTM) and construct meta data set with domain-invariant source data to guide the estimation of
NTM. Through the risk minimization on the meta data set, the optimized NTM thus can correct the noisy issues in pseudo labels and enhance the generalization ability of the model on the target data. Considering the capacity gap between shallow and deep features, we further employ the proposed DMLC strategy to provide matched and compat-ible supervision signals for different level features, thereby ensuring deep adaptation. Extensive experimental results highlight the effectiveness of our methoda against existing state-of-the-art methods on three benchmarks. 1.

Introduction
Unsupervised domain adaptation (UDA) aims to adapt a model for the unlabeled target domain through transfer-ring the knowledge from a labeled source domain with the same label space. UDA for semantic segmentation is a cru-cial practical problem since it may be beneﬁcial for various real-world applications, such as simulation for robots [18] and autonomous driving [44]. The main challenge of UDA
∗Xiaoqing Guo and Chen Yang contribute equally.
†Yixuan Yuan is the corresponding author.
This work is supported by Shenzhen-Hong Kong Innovation Circle
Category D Project SGDX2019081623300177 (CityU 9240008). ahttps://github.com/cyang-cityu/MetaCorrection (a) Unlabeled target domain image (b) Generated pseudo label  h t u r t d n u o r
G 0 road 1 sidewalk 2 building 3 wall 4 fence 5 pole 6 traffic lgt  7 traffic sgn  8 vegetation  9 terrain 10 sky 11 person 12 rider 13 car 14 truck 15 bus 16 train 17 motor 18 bike (c) Confusion matrix (d) Ground truth label  (e) Class distribution (log$)
Figure 1. Sample of the noisy pseudo labels on Cityscapes [10].
The generated pseudo labels suffer from the data distribution bi-ases in comparison to the ground truth. semantic segmentation lies in the divergence of data dis-tribution between two domains [5, 48]. Such domain gap often results in signiﬁcant performance degradation if the model learned on the labeled source data is directly applied to the target samples [46, 51].
There exist two major lines of approaches to tackle the domain gap problem. On one hand, adversarial learning based UDA methods as a dominant stream have been de-vised to bridge the domain gap by aligning the distribu-tions of two domains in the appearance [6, 7, 9, 22], feature
[2, 11, 39, 45] or output spaces [20, 26, 40, 41]. Despite the signiﬁcant progress of domain alignment, these works ignored the domain-speciﬁc knowledge and could not guar-antee the sufﬁcient discriminative capability of the classiﬁer for the speciﬁc task. On the other hand, self-training based methods [4, 5, 17, 23, 24, 28, 30, 35, 42, 46, 48, 49, 50, 51] have emerged as promising alternatives towards UDA, which enhanced the discrimination property of target fea-tures and implicitly encouraged cross-domain alignment by simultaneously training with pseudo-labeled target data and labeled source data. Speciﬁcally, self-training methods as-sign pixel-wise pseudo labels according to conﬁdence score
[17, 23, 30, 51] or uncertainty [24, 48], providing extra su-pervision for target data to optimize the model.
However, one issue with self-training based UDA meth-ods is that the generated pseudo labels usually suffer from 3927 	
the noise problem, as illustrated in Figure 1 (b). The pres-ence of noisy pseudo labels may severely hamper the gen-eralization ability of the adapted models, because deep neu-ral networks (DNN) may overﬁt due to these noisy labeled data. Although some existing works [14, 23, 51] manually deﬁne a threshold to eliminate the low-conﬁdence pseudo-labeled samples, it is still challenging in several aspects.
First, the threshold value is hard to predeﬁne manually. It may depend on many factors such as the stage of training procedure, the degree of discrepancy between two domains, the number of pixels in each class, the location of the pixel, etc. Secondly, those selected training samples may be mis-classiﬁed with high conﬁdence, leading to accumulated er-rors. In fact, the noisy pseudo labels tend to appear in under-represented minor classes or ambiguous classes. For in-stance, the minor category ‘trafﬁc sign’ is overwhelmed by major category ‘building’, and ‘road’ is usually connected to ‘sidewalk’, yielding noisy labels, as in Figure 1 (c). In this scenario, noisy pseudo labels can be theoretically con-verted from the ground truth labels via a NTM [37, 47], which encodes the inter-class misclassiﬁcation relationship.
To heuristically discover intrinsic inter-class noise tran-sition probabilities underlying target data, we model the noise distribution of pseudo labels by a NTM and devise a domain-aware meta-learning strategy to estimate the NTM in a learning-to-learn fashion. The key idea of domain-aware meta-learning is to obtain the meta-knowledge of un-derlying label distribution of clean data in the target domain, and we introduce a domain predictor to adaptively select domain-invariant source data with ground truth labels as meta data set, so as to guide the derivation of NTM. The domain-aware meta-learning strategy enables the gradient of empirical risk measured on meta data to update the NTM, thereby boosting the generalization capacity. Then the ap-proximated noise distribution can be utilized to explicitly correct the supervision signal for target data, aiming at solv-ing the noisy pseudo label problem in a self-training based
UDA method. An alternating optimization approach is fur-ther adopted to mutually improve the estimation of NTM and the UDA segmentation model. For simplicity, we re-fer to the whole Domain-aware Meta-learning strategy for
Loss Correction in the above process as DMLC. Moreover, we devise a MetaCorrection framework, which incorporates
DMLC to provide matched supervision signals for outputs of different levels, thereby enhancing the deep adaptation of model. In particular, we introduce the learnable NTMs for different layers, and adopt the proposed domain-aware meta-learning to estimate the corresponding noise distribu-tions and beneﬁt the loss correction.
We summarize our contributions in four aspects.
• We present a MetaCorrection framework, which incor-porates the proposed DMLC strategy for UDA seman-tic segmentation. To our best knowledge, it represents the ﬁrst effort to formally model the noise distribution of pseudo labels in target domain by a learnable NTM and further solve it in a meta-learning strategy.
• In the DMLC strategy, we formulate the misclassiﬁ-cation probability of inter-classes to model noise dis-tribution in target domain and devise a domain-aware meta-learning algorithm to estimate NTM for loss cor-rection in a data driven manner.
• Our MetaCorrection framework aims to provide matched and compatible supervision signals for differ-ent layers with the proposed DMLC strategy, boosting the adaptation performance of model.
• We conduct extensive comparison experiments and ab-lation studies to thoroughly verify the impact and ef-fect of the proposed MetaCorrection framework. 2.