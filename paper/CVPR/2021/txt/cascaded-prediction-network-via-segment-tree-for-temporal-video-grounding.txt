Abstract
Query: Several people begin chopping vegetables.
Temporal video grounding aims to localize the target segment which is semantically aligned with the given sen-tence in an untrimmed video. Existing methods can be di-vided into two main categories, including proposal-based approaches and proposal-free approaches. However, the former ones suffer from the extra cost of generating pro-posals and inﬂexibility in determining ﬁne-grained bound-aries, and the latter ones usually attempt to decide the start and end timestamps directly, which brings about much dif-ﬁculty and inaccuracy. In this paper, we convert this task into a multi-step decision problem and propose a novel Cas-caded Prediction Network (CPN) to generate the grounding result in a coarse-to-ﬁne manner. Concretely, we ﬁrst en-code video and query into the same latent space and fuse them into integrated representations. Afterwards, we con-struct a segment-tree-based structure and make predictions via decision navigation and signal decomposition in a cas-caded way. We evaluate our proposed method on three large-scale publicly available benchmarks, namely Activi-tyNet Caption, Charades-STA and TACoS, where our CPN surpasses the performance of the state-of-the-art methods. 1.

Introduction
With the rapid development of Internet technology, video has become a signiﬁcant medium for information com-munication and dissemination, which brings great appli-cation value and prospect for the ﬁeld of automatic video analysis. After the release of several large-scale datasets
[3, 4, 13, 36, 38], research in this area is gradually moving towards video-text understanding tasks, including video-text retrieval [39, 43], video captioning [21, 33], video ques-tion answering [30, 49] and so forth. Considering the appli-cation scenarios in video websites and search engines, an increasing number of researchers begin to focus on the task of temporal video grounding.
∗Zhou Zhao is the corresponding author.
GT: 41.91s 110.49s
Figure 1. An illustration of temporal video grounding.
Figure 2. The overall perspective of our proposed model.
As the example in Figure 1 illustrates, temporal video grounding aims to automatically determine when an ac-tion or event corresponding to a given text query occurs in the video. The previous approaches in this area can be mainly categorized into two groups, including proposal-based methods [5, 9, 34, 37, 41, 44, 46, 48] and proposal-free methods [12, 20, 24, 35, 42, 45]. The former ones mainly follow the paradigm to manually predeﬁne some proposals and select the best one by considering the corre-lation between proposal features and the given semantic in-formation. And the latter ones try to tackle this problem by utilizing fully integrated features to determine the start and end timestamps aligned to the given description directly.
However, for the proposal-based methods, the hand-crafted pre-deﬁnitions heavily rely on the prior knowledge to the length distribution of target segments for the speciﬁc datasets and bring much extra computational cost for pre-processing. Besides, the proposal boundaries are usually
ﬁxed, leading to the incapability to work out more ﬂexi-ble results. As for the proposal-free methods, the decision space for the ﬁnal prediction is always too large for the model to generate accurate results in a single-shot classi-ﬁcation or regression. Moreover, due to the lack of supervi-sion from the inside of segments, these methods are strongly dependent on the expression ability of fusion modules.
To alleviate these problems, we devise a novel Cas-14197
caded Prediction Network (CPN) for temporal grounding as shown in Figure 2. Contrary to the existing approaches, we perform multiple cascaded prediction subtasks in a coarse-to-ﬁne manner to generate ﬁne-grained and ﬂexible ground-ing results. Considering the effectiveness of segment tree in storing and representing segments of sequential data, we choose to use this data structure to maintain our CPN model, thus increasing computing speed and making the whole framework easy to maintain. Speciﬁcally, we ﬁrst ex-tract features from video and query and integrate them into fusion representations. Afterwards, we develop a segment-tree-based structure to generate segment features in differ-ent temporal scales and reﬁne them in a message-passing way via graph neural network. Finally, we perform deci-sion navigation and signal decomposition on each level to fully exploit the information from the boundary annotation and response signal associated with the sentence query.
Our main contributions can be summarized as follows:
• We consider the temporal video grounding task as a multi-step prediction problem and propose a novel
Cascaded Prediction Network (CPN) based on a segment-tree structure to address this problem in a coarse-to-ﬁne manner.
• We devise an effective representation learning method to generate discriminative segment features in different scales, thus enhancing the grounding performance.
• The extensive experiments conducted on three chal-lenging public benchmarks, namely ActivityNet Cap-tion, Charades-STA and TACoS, demonstrate the ef-fectiveness of our proposed CPN method. 2.