Abstract
When in a new situation or geographical location, hu-man drivers have an extraordinary ability to watch others and learn maneuvers that they themselves may have never performed. In contrast, existing techniques for learning to drive preclude such a possibility as they assume direct ac-cess to an instrumented ego-vehicle with fully known obser-vations and expert driver actions. However, such measure-ments cannot be directly accessed for the non-ego vehicles when learning by watching others. Therefore, in an applica-tion where data is regarded as a highly valuable asset, cur-rent approaches completely discard the vast portion of the training data that can be potentially obtained through indi-rect observation of surrounding vehicles. Motivated by this key insight, we propose the Learning by Watching (LbW) framework which enables learning a driving policy with-out requiring full knowledge of neither the state nor expert actions. To increase its data, i.e., with new perspectives and maneuvers, LbW makes use of the demonstrations of other vehicles in a given scene by (1) transforming the ego-vehicle’s observations to their points of view, and (2) in-ferring their expert actions. Our LbW agent learns more robust driving policies while enabling data-efﬁcient learn-ing, including quick adaptation of the policy to rare and novel scenarios.
In particular, LbW drives robustly even with a fraction of available driving data required by exist-ing methods, achieving an average success rate of 92% on the original CARLA benchmark with only 30 minutes of to-tal driving data and 82% with only 10 minutes. 1.

Introduction
Modern autonomous driving systems primarily rely on collecting vast amounts of data through a ﬂeet of instru-mented and operated vehicles in order to train imitation and machine learning algorithms [4, 7, 15]. This process gen-erally assumes direct knowledge of the sensory state of an ego-vehicle as well as the control actions of the expert op-erator. As driving algorithms largely depend on such costly training data to learn to drive safely, it is regarded as a highly valuable asset.
Figure 1: Learning to Drive by Watching Others. While existing imitation learning approaches solely utilize data from an ego-vehicle perspective, our proposed Learning by
Watching (LbW) framework learns a more robust and data-efﬁcient policy from all available demonstration sources in a scene.
Unfortunately, such data requirements have resulted in self-captured data being amassed in the hands of a few iso-lated organizations, thereby hindering the progress, accessi-bility, and utility of autonomous driving technologies. First, while technological giants such as Uber, Tesla, Waymo and other developers may spend signiﬁcant efforts instrument-ing and operating their own vehicles, it is difﬁcult to com-pletely capture all of the driving modes, environments, and events that the real-world presents. Consequently, learned agents may not be able to safely handle diverse cases, e.g., a new scenario or maneuver that is outside of their in-house collected dataset. Second, while small portions of driving data may be shared, i.e., for research purposes, the bulk of it is predominantly kept private due to its underlying worth.
Finally, we can also see how this current practice may lead to signiﬁcant redundancy and thus stagnation in develop-ment. How can we advance the underlying development process of safe and scalable autonomous vehicles?
In an application where the lack of a data point could mean the difference between a potential crash or a safe ma-neuver, we sought to develop a more efﬁcient and shared paradigm to ensure safe driving. Towards this goal, we pro-pose the LbW framework for learning to drive by watch-ing other vehicles in the ego-vehicle’s surroundings. Moti-vated by how human drivers are able to quickly learn from demonstrations provided by other drivers and vehicles, our 12711
approach enables leveraging data in many practical scenar-ios where watched vehicles may not have any instrumen-tation or direct capture means at all.
Towards more effective use of a collected dataset, LbW does not assume direct knowledge of either the state or ex-pert actions. For instance, in the scenario shown in Fig. 1, a non-instrumented human-driven vehicle or perhaps an-other company’s autonomous vehicle is observed by our ego-vehicle as they turn and negotiate an intersection. LbW infers the observed agent’s state and expert actions so that it may be used to teach our autonomous vehicle. By leverag-ing supervision from other drivers, our LbW agent can more efﬁciently learn to drive in varying perspectives and scenar-ios. The framework facilitates access to large amounts of driving data from human-driven vehicles that may not have been instrumented to directly measure and collect such data.
While offering several beneﬁts for scalability, learning a driving model via indirect means of watching surrounding vehicles in a scene also poses several challenges which we address in this work. Speciﬁcally, to advance the state-of-the-art of robust autonomous driving agents, we make the following three contributions: (1) We propose LbW, a new paradigm which can help facilitate a more efﬁcient develop-ment of driving agents, thus aiding real-world deployment, (2) we develop an effective two-step behavior cloning ap-proach which infers the states and actions of surrounding vehicles without direct access to such observations, and (3) we validate the impact of LbW on the resulting driving pol-icy through a set of novel experiments on the CARLA and
NoCrash benchmarks. While previous approaches tend to exclusively focus on driving policy performance, i.e., re-quiring many hours of collected ego-vehicle training data, we instead emphasize the beneﬁts of our approach by vary-ing and limiting the amount of available data. We also demonstrate LbW to enable adaption to novel driving sce-narios and maneuvers, without ever having direct access to an operator of an ego-vehicle performing such maneuvers. 2.