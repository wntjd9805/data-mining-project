Abstract
Multi-label activity recognition is designed for recogniz-ing multiple activities that are performed simultaneously or sequentially in each video. Most recent activity recogni-tion networks focus on single-activities, that assume only one activity in each video. These networks extract shared features for all the activities, which are not designed for multi-label activities. We introduce an approach to multi-label activity recognition that extracts independent feature descriptors for each activity and learns activity correla-tions. This structure can be trained end-to-end and plugged into any existing network structures for video classiﬁcation.
Our method outperformed state-of-the-art approaches on four multi-label activity recognition datasets. To better un-derstand the activity-speciﬁc features that the system gen-erated, we visualized these activity-speciﬁc features in the
Charades dataset. The code will be released later. 1.

Introduction
Activity recognition has been studied in recent years due to its great potential in real-world applications. Recent ac-tivity recognition researches [31, 46, 18, 33] focused on single-activity recognition assuming that each video con-tains only one activity, without considering a multi-label problem where a video may contain multiple activities (con-current or sequential). Multi-label activity recognition is an understudied ﬁeld but has more general real-world use cases (e.g., sports activity recognition [47, 5], or daily life activ-ity recognition [44]). Most of the recent multi-label activity recognition methods are derived from structures for single activities that generate a shared feature vector and apply sig-moid as the output activation function [35, 55, 7, 57, 12, 59].
Although these approaches enable the network to provide multi-label outputs, the features are not designed for multi-label activities.
We introduce our mechanism to recognize multi-label activities from another angle by generating independent fea-Figure 1: System overview using an example from Charades. The sys-tem ﬁrst generates k independent feature snippets (“observations”) that fo-cus on different key regions from the video (arms, blankets, and clothes).
The activity-speciﬁc features are then generated by independently com-bining these observations. The weights of the observations that contribute to activity-speciﬁc features are represented as lines with different colors (black, red, and blue). The thicker lines denote higher weights. For example, the activity-specif ic f eatures1 (holding a blanket) are ob-tained by combining information from observation1 (focuses on arms) and observation2 (focuses on clothes). The system ﬁnally learns corre-lations between activity-speciﬁc features and provide multi-label activity predictions. ture descriptors for different activities. We named these fea-ture descriptors “activity-speciﬁc features”. This mecha-nism generates activity-speciﬁc features in two stages. The
ﬁrst-stage network (Figure 1, middle) summarizes the fea-ture maps extracted by the backbone network (3D convo-lution layers) and generates a set of independent feature snippets by applying independent spatio-temporal attention for each snippet. We name these feature snippets “obser-vations”.
In the second stage (Figure 1, right), the net-work learns activity-speciﬁc features from different combi-nations of observations for different activities. In this way, each activity is represented as an independent set of fea-ture descriptors (activity-speciﬁc features). The multi-label activity predictions can then be made based on the activity-speciﬁc features. Unlike most of the previous approaches
[35, 55, 7, 57, 12, 59] that generate a shared feature vec-114625
tor to represent multiple activities by pooling feature maps globally, our network produces speciﬁc feature descriptors for each activity.
Label dependencies have proven important for multi-label image classiﬁcation [26, 20, 53], and we argue that it is also important to consider the label-wise correlation for activities. We generate an activity label-wise correlation map to model co-existing patterns (e.g., walking and talk-ing) and exclusive patterns (e.g., sitting and standing). The correlation map is applied on the activity-speciﬁc features to predict activities based on the highly correlated (or exclu-sive) activities’ descriptors. In multi-label activity videos, different activities might have different duration and need to be recognized using video clips with different lengths. To address this issue, we further introduced a speed-invariant tuning method for generating activity-speciﬁc features and recognizing multi-label activities using inputs with different downsampling rates.
We evaluated our model on both large-scale multi-label activity datasets (Charades [44] and AVA [19]), and real-world multi-label sports datasets (Volleyball [47] and
Hockey [5]) to show that our model performs well on multi-label datasets and is applicable to real-world tasks. Our introduced model outperformed the recent state-of-the-art networks on all four datasets without using additional in-formation (e.g., optical ﬂow [24] or object information [29]) other than RGB frames, which demonstrated the efﬁciency of our introduced method. We also provide detailed abla-tion experiments on the model structure to show that the introduced activity-speciﬁc features and activity correla-tion work as expected. We further visualized the activity-speciﬁc features by applying the learned attention maps on the backbone features (feature maps after the last 3D convo-lution layer) to represent the activity-speciﬁc feature maps.
Our contributions can be summarized as:
• A network structure that generates activity-speciﬁc features for multi-label activity recognition.
• An activity correlation map that learns correlations be-tween different activity-speciﬁc features.
• A speed-invariant tuning method that produces multi-temporal-label activity predictions using different resolution inputs. 2.