Abstract 1.

Introduction
Recent advances in 3D semantic scene understanding have shown impressive progress in 3D instance segmenta-tion, enabling object-level reasoning about 3D scenes; how-ever, a ﬁner-grained understanding is required to enable interactions with objects and their functional understanding.
Thus, we propose the task of part-based scene understand-ing of real-world 3D environments: from an RGB-D scan of a scene, we detect objects, and for each object predict its decomposition into geometric part masks, which com-posed together form the complete geometry of the observed object. We leverage an intermediary part graph represen-tation to enable robust completion as well as building of part priors, which we use to construct the ﬁnal part mask predictions. Our experiments demonstrate that guiding part understanding through part graph to part prior-based pre-dictions signiﬁcantly outperforms alternative approaches to the task of semantic part completion.
Recently, we have seen remarkable advances in 3D se-mantic scene understanding, driven by efforts in large-scale data collection and annotation of 3D reconstructions of RGB-D scanned environments [5, 2], coupled with exploration of 3D deep learning approaches across 3D representations such as sparse or dense volumetric grids [55, 39, 5, 15, 4], point clouds [38, 40], meshes [13, 23], and multi-view [7, 50].
This has led to signiﬁcant progress in both 3D semantic segmentation as well as 3D semantic instance segmentation
[16, 15, 4, 25]. These have enabled a basis for 3D percep-tion at the level of objects, which is essential for semantic understanding, but lacks ﬁner-grained understanding often critical for enabling interactions with objects and reasoning about functionality (e.g., the seat part of a chair is for sitting on, a knob or handle enables opening doors or drawers).
At the same time, notable progress has been made in part segmentation for shapes [33, 32, 18]. However, these methods have been developed on synthetic datasets such as ShapeNet [3], of objects in isolation; this scenario is 7484
much less complex than the objects observed in real-world environments. Thus, we aim to bring these two directions together and propose the task of semantic part completion, predicting the part decomposition of objects in real-world 3D environments, where observations are often cluttered and geometrically incomplete (e.g., due to occlusions, sensor limitations, etc). That is, from an RGB-D scan of a scene, we detect objects characterized by 3D bounding boxes and class labels, and for each object, we predict its complete part decomposition into binary part masks, with each part mask reﬂecting the part geometry of the complete object, including unobserved missing regions, to achieve a holistic understanding of the objects in an observed scene.
To achieve this part-based understanding of a scene, we propose to predict the full part graph for each detected object, and based on the predicted part graph, the geometric masks for each complete part. Predicting the part graph structure enables capturing the complete semantic structure of the object in a low-dimensional representation, allowing reliable prediction of missing and unobserved parts (e.g., for a four-legged table with one leg unobserved, the missing leg is easy to predict based on commonly observed table part patterns).
Furthermore, this enables us to build and exploit strong part geometry priors for each predicted part in the part graph. We can then predict the part masks by ﬁnding similar part priors and reﬁning them to produce ﬁnal part mask predictions.
This enables a robust decomposition of an RGB-D scan of a scene into its component objects and their constituent parts, including regions of objects that have been unobserved. We believe that this takes an important step towards enabling local interactions with objects and functionality analysis in real-world 3D scenes.
We formulate the task of semantic part completion for 3D scene understanding, informing comprehensive part-based object understanding of real-world scans. To address this part understanding, we propose an approach to decompose a 3D scan of a scene into its complete object parts, outperforming state-of-the-art alternative approaches for the task:
• We propose to predict part graph information for ob-jects in real-world scan scenes as an intermediary rep-resentation that enables robust, part-based completion of objects.
• We leverage the predicted part graphs to guide prior-based prediction for effective inference of geometric part mask decomposition for the objects of a scanned scene. 3D object localization and segmentation. Earlier approaches leveraging 3D convolutional neural networks developed methods operating on dense voxel grids using 3D region proposal techniques for detection and segmentation [47, 20].
Sparse volumetric backbones have also been leveraged to enable effective feature extraction on high-resolution in-puts for improved 3D detection and segmentation perfor-mance [10, 16]. Recently, VoteNet [37] introduced a Hough
Voting-inspired scheme for 3D object detection on point clouds. This was extended by MLCVNet [56] to incorporate multi-scale contextual information for improved detection performance. These approaches have now shown impressive performance for instance-level scene understanding; we aim to build upon this and propose to infer ﬁner-grained part decomposition for each object in a 3D scan. 3D Scan Completion. Repairing and completing holes or broken meshes has been well-studied for 3D shapes. Tra-ditional methods have mainly focused on repairing small holes by ﬁtting geometric primitives, continuous energy min-imization, or leveraging surface reconstruction for interpo-lation of missing regions [34, 59, 49, 27, 28]. Structural or symmetry priors have also been leveraged for shape comple-tion [52, 31, 36, 46, 49]. Recently, generative deep learning approaches have been developed, with signiﬁcant progress in 3D shape reconstruction and completion [55, 9, 17, 35].
In addition to operating on the limited spatial context of shapes, generative deep learning approaches have also been developed for completion of 3D scenes. Song et al. [48] developed a voxel-based approach to predict geometric occu-pancy of a single depth frame, leveraging a large-scale syn-thetic 3D dataset of scenes. Dai et al. [8] proposed an autore-gressive approach for scan completion, enabling very large scale completion. SG-NN [6] presented a self-supervised approach towards 3D scan completion, enabling training only on real scan data. These approaches operate on geomet-ric completion but without knowledge of individual object instances, which is fundamental to many perception-based tasks. RevealNet [21] introduced an approach to detect ob-jects in a 3D scan and infer each object’s complete geometry, joining together geometric reconstruction with object-based understanding. We similarly aim to infer each object’s com-plete geometry from a partial scan observation, but infer a part decomposition of the object structure, enabling both
ﬁner-grained understanding as well as more effective object completion through its part structure. 2.