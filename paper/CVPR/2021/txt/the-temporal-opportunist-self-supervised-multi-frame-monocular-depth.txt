Abstract (cid:44)(cid:87)(cid:16)(cid:20)
Self-supervised monocular depth estimation networks are trained to predict scene depth using nearby frames as a supervision signal during training. However, for many ap-plications, sequence information in the form of video frames is also available at test time. The vast majority of monoc-ular networks do not make use of this extra signal, thus ig-noring valuable information that could be used to improve the predicted depth. Those that do, either use computation-ally expensive test-time reﬁnement techniques or off-the-shelf recurrent networks, which only indirectly make use of the geometric information that is inherently available.
We propose ManyDepth, an adaptive approach to dense depth estimation that can make use of sequence information at test time, when it is available. Taking inspiration from multi-view stereo, we propose a deep end-to-end cost vol-ume based approach that is trained using self-supervision only. We present a novel consistency loss that encourages the network to ignore the cost volume when it is deemed unreliable, e.g. in the case of moving objects, and an aug-mentation scheme to cope with static cameras. Our detailed experiments on both KITTI and Cityscapes show that we outperform all published self-supervised baselines, includ-ing those that use single or multiple frames at test time. 1.

Introduction
Knowing the depth to each pixel in an image has proved to be a useful and versatile tool, with applications rang-ing from augmented reality [59], autonomous driving [22], through to 3D reconstruction [64]. While specialist hard-ware can give per-pixel depth, e.g. from structured light or Lidar sensors, a more attractive approach is to only re-quire a single RGB camera. Many recent monocular depth from RGB methods are trained using only self-supervision, which removes the need for expensive hardware to capture training depth data [21, 99, 23, 26]. While these approaches appear to be very promising, their test-time depth estima-tion performance is not yet on a par with specialist depth hardware or deep multi-view methods [73]. (cid:11)(cid:68)(cid:12)(cid:3)(cid:54)(cid:76)(cid:81)(cid:74)(cid:79)(cid:72)(cid:3)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:11)(cid:69)(cid:12)(cid:3)(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:3)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:54)(cid:68)(cid:80)(cid:72)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79) (cid:62)(cid:41)(cid:85)(cid:82)(cid:80)(cid:3)(cid:48)(cid:82)(cid:81)(cid:82)(cid:85)(cid:72)(cid:70)(cid:64) (cid:62)(cid:41)(cid:85)(cid:82)(cid:80)(cid:3)(cid:48)(cid:82)(cid:81)(cid:82)(cid:85)(cid:72)(cid:70)(cid:64)
Figure 1. Trained using only self-supervision, our model not only predicts depth from single frames (a) but can also utilize multiple frames, when they are available, using the same model (b). This results in superior depth predictions at test time. Error maps on the bottom row show large depth errors as red, small as blue. (cid:11)(cid:69)(cid:12)(cid:3)(cid:54)(cid:76)(cid:81)(cid:74)(cid:79)(cid:72)(cid:16)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:3)(cid:71)(cid:72)(cid:83)(cid:87)(cid:75)(cid:15)(cid:3)(cid:72)(cid:85)(cid:85)(cid:82)(cid:85)(cid:3) (cid:11)(cid:71)(cid:12)(cid:3)(cid:44)(cid:80)(cid:83)(cid:85)(cid:82)(cid:89)(cid:72)(cid:71)(cid:3)(cid:80)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:73)(cid:85)(cid:68)(cid:80)(cid:72)(cid:3) (cid:71)(cid:72)(cid:83)(cid:87)(cid:75)(cid:15)(cid:3)(cid:72)(cid:85)(cid:85)(cid:82)(cid:85)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:83)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:70)(cid:79)(cid:82)(cid:88)(cid:71) (cid:80)(cid:68)(cid:83)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:83)(cid:82)(cid:76)(cid:81)(cid:87)(cid:3)(cid:70)(cid:79)(cid:82)(cid:88)(cid:71)
In an attempt to close this performance gap, we observe that in most practical scenarios more than one frame is available at test time e.g. from a camera on a moving vehi-cle or micro-baseline frames from a phone camera [32, 41].
Yet these additional frames are typically not exploited by current monocular methods. In this work, we use these ad-ditional frames at both training and test time, when they are available, to self-supervise a multi-frame depth estima-tion system. We show that a straightforward application of self-supervised training to a multi-view plane-sweep stereo architecture produces poor results, signiﬁcantly worse than self-supervised single frame networks. To overcome this, we introduce several innovations to address issues caused by moving objects, scale ambiguity, and static cameras. We call our resultant multi-frame system ManyDepth.
We make the following three contributions: 1. A novel self-supervised multi-frame depth estimation model that combines the strengths of monocular and multi-view depth estimation by making use of multiple frames at test time, when they are available. 2. We show that moving objects and static scenes signiﬁ-cantly impact self-supervised multi-view matching ap-proaches, and we introduce efﬁcient losses and train-1164
ing solutions to alleviate this problem. 3. We propose an adaptive cost volume to overcome the scale ambiguity arising from self-supervised training on monocular sequences. To the best of our knowl-edge, this is the ﬁrst time cost volume extents have been learned from data rather than set as parameters.
Our ManyDepth model outperforms existing single and multi-frame approaches on the KITTI and Cityscapes datasets. 2.