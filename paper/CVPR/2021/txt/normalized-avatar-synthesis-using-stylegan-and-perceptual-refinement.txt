Abstract
We introduce a highly robust GAN-based framework for digitizing a normalized 3D avatar of a person from a sin-gle unconstrained photo. While the input image can be of a smiling person or taken in extreme lighting conditions, our method can reliably produce a high-quality textured model of a person’s face in neutral expression and skin textures under diffuse lighting condition. Cutting-edge 3D face re-construction methods use non-linear morphable face mod-els combined with GAN-based decoders to capture the like-ness and details of a person but fail to produce neutral head models with unshaded albedo textures which is critical for creating relightable and animation-friendly avatars for in-tegration in virtual environments. The key challenges for existing methods to work is the lack of training and ground truth data containing normalized 3D faces. We propose a
Hao Li is afﬁliated with Pinscreen and UC Berkeley; Koki Nagano is currently at NVIDIA. This work was fully conducted at Pinscreen. two-stage approach to address this problem. First, we adopt a highly robust normalized 3D face generator by embed-ding a non-linear morphable face model into a StyleGAN2 network. This allows us to generate detailed but normalized facial assets. This inference is then followed by a perceptual reﬁnement step that uses the generated assets as regulariza-tion to cope with the limited available training samples of normalized faces. We further introduce a Normalized Face
Dataset, which consists of a combination photogrammetry scans, carefully selected photographs, and generated fake people with neutral expressions in diffuse lighting condi-tions. While our prepared dataset contains two orders of magnitude less subjects than cutting edge GAN-based 3D facial reconstruction methods, we show that it is possible to produce high-quality normalized face models for very chal-lenging unconstrained input images, and demonstrate supe-rior performance to the current state-of-the-art. 11662
1.

Introduction
Figure 2: Automated digitization of normalized 3D avatars from a single photo.
The creation of high-ﬁdelity virtual avatars have been mostly reserved to professional production studios and typ-ically involves sophisticated equipment and controlled cap-ture environments. Automated 3D face digitization meth-ods that are based on unconstrained images such as selﬁes or downloaded internet pictures are gaining popularity for a wide range of consumer applications, such as immersive telepresence, video games, or social media apps based on personalized avatars.
Cutting-edge single-view avatar digitization solutions are based on non-linear 3D morphable face models (3DMM) generated from GANs [66, 65, 28, 45], outper-forming traditional linear models [10] which often lack fa-cial details and likeness of the subject. To successfully train these networks, hundreds of thousands of subjects in vari-ous lighting conditions, poses, and expressions are needed.
While highly detailed 3D face models can be recovered, the generated textures have the lighting of the environment baked in, and expressions are often difﬁcult to neutralize making these methods unsuitable for applications that re-quire relighting or facial animation. In particular, inconsis-tent textured models are obtained when images are taken under different lighting conditions.
Collecting the same volume of 3D face data with neu-tral expressions and controlled lighting condition is in-tractable. Hence, we introduce a GAN-based facial digitiza-tion framework that can generate a high-quality textured 3D face model with neutral expression and normalized lighting using only thousands of real world subjects. Our approach consists of dividing the problem into two stages. The ﬁrst stage uses a non-linear morphable face model embedded into a StyleGAN2 [40] network to robustly generate de-tailed and clean assets of a normalized face. The likeness of the person is then transferred from the input photograph using a perceptual reﬁnement stage based on iterative op-timization using a differentiable renderer. StyleGAN2 has proven to be highly expressive in generating and represent-ing real world images using an inversion step to convert im-age to latent vector [3, 60, 4, 33] and we are adopting the same two step GAN-inversion approach to learn facial ge-ometry and texture jointly. To enable 3D neutral face in-ference from an input image, we connect the image with the embedding space of our non-linear 3DMM using an identity regression network based on identity features from
FaceNet [58]. To train a sufﬁciently effective generator, we introduce a new Normalized Face Dataset which consists of a combination of high-ﬁdelity photogrammetry scans, frontal and neutral portraits in diffuse lighting conditions, as well as fake subjects generated using a pre-trained Style-GAN2 network with FFHQ dataset [39].
Despite our data augmentation effort, we show that our two-stage approach is still necessary to handle the large variation of possible facial appearances, expressions and lighting conditions. We demonstrate the robustness of our digitization framework on a wide range of extremely chal-lenging examples, and provide extensive evaluations and comparisons with current state-of-the-art methods. Our method outperforms existing techniques in terms of digitiz-ing textured 3D face models with neutral expressions and diffuse lighting conditions. Our normalized 3D avatars can be converted into parametric models with complete bodies and hair, and the solution is suitable for animation, relight-ing, and integration with game engines as shown in Fig. 2.
We summarize our key contributions as follows:
• We propose the ﬁrst StyleGAN2-based approach for digitizing a 3D face model with neutral expressions and diffusely lit textures from an unconstrained image.
• We present a two-stage digitization framework which consists of a robust normalized face model inference stage followed by a perception-based iterative face re-ﬁnement step.
• We introduce a new data generation approach and dataset based on a combination of photogrammetry scans, photographs of expression and lighting normal-ized subjects, and generated fake subjects.
• Our method outperforms existing single-view 3D face reconstruction techniques for generating normalized faces, and we also show that our digitization approach works using limited subjects for training. 2.