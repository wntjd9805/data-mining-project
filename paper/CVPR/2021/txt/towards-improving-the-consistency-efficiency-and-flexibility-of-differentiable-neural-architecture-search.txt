Abstract
Most differentiable neural architecture search methods construct a super-net for search and derive a target-net as its sub-graph for evaluation. There exists a signiﬁcant gap between the architectures in search and evaluation. As a result, current methods suffer from an inconsistent, inefﬁ-cient, and inﬂexible search process. In this paper, we in-troduce EnTranNAS that is composed of Engine-cells and
Transit-cells. The Engine-cell is differentiable for architec-ture search, while the Transit-cell only transits a sub-graph by architecture derivation. Consequently, the gap between the architectures in search and evaluation is signiﬁcantly reduced. Our method also spares much memory and com-putation cost, which speeds up the search process. A feature sharing strategy is introduced for more balanced optimiza-tion and more efﬁcient search. Furthermore, we develop an architecture derivation method to replace the traditional one that is based on a hand-crafted rule. Our method en-ables differentiable sparsiﬁcation, and keeps the derived ar-chitecture equivalent to that of Engine-cell, which further improves the consistency between search and evaluation.
More importantly, it supports the search for topology where a node can be connected to prior nodes with any number of connections, so that the searched architectures could be more ﬂexible. Our search on CIFAR-10 has an error rate of 2.22% with only 0.07 GPU-day. We can also directly per-form the search on ImageNet with topology learnable and achieve a top-1 error rate of 23.8% in 2.1 GPU-day. 1.

Introduction
Current neural architecture search (NAS) methods mainly include reinforcement learning-based NAS [1, 57], evolution-based NAS [41, 31], Bayesian optimization-based NAS [25, 56], and gradient-based NAS [33, 32], some of which have successfully been applied to related
∗Corresponding author. tasks for better architectures, such as semantic segmenta-tion [9, 29] and object detection [38, 12, 16, 45].
Among the NAS methods, gradient-based algorithms gain much attention because of the simplicity. Liu et al. ﬁrst propose the differentiable search framework, DARTS [32], based on continuous relaxation and weight sharing [39], and inspire the follow-up studies [48, 7, 8, 49, 11]. In DARTS, different architectures share their weights as sub-graphs of a super-net. The super-net is trained for search, after which a target-net is derived for evaluation by manually keeping the important paths according to their softmax activations.
Despite the simplicity, the architecture for evaluation only covers a small subset of the one for search, which causes a signiﬁcant gap of architectural difference. We point out that the gap causes the following problems:
• inconsistent: The super-net trained in the search phase is a summation among all candidate connections with a trainable distribution induced by softmax. It essen-tially optimizes a feature combination, instead of fea-ture selection, which is the real goal of architecture search. As noted by [8, 52], operations may be highly correlated. Even if the weight of some connection is small, the corresponding path may be indispensable for the performance. So the target-net derived from a high-performance super-net is not ensured to be a good one
[42, 50]. The search process is inconsistent.
• inefﬁcient: Because the super-net is a combination the whole graph among all candidate connections, needs to be stored in both forward and backward stages, which requires much memory and computa-tional consumption. As a result, the search can be per-formed only on a very limited number of candidate op-erations, and the super-net is inefﬁcient to train.
• inﬂexible: The gap between the architectures in search and evaluation does not allow the search for topology
In current methods [32, 48, in a differentiable way. 7, 49, 11], the target-net is derived based on a hand-crafted rule where each intermediate node keeps the 6667
Figure 1. A diagram of DARTS. The target-net is derived by keeping the top-2 strongest connections of each node and has a signiﬁcant gap with the architecture in search. The connections in different color represent candidate operations, with exemplar weights beside them. top-2 strongest connections to prior nodes. However, there is no theoretical or experimental evidence show-ing that this rule is optimal.
It limits the diversity of derived architectures in the topological sense [23].
Therefore, the search result is not ﬂexible as we have no access to other kinds of topologies.
Some studies adopt the Gumbel Softmax strategy [24, 35] to sample a target-net that approaches to the one in search so that the gap can be reduced [48, 46, 8, 13]. But still, the demand for computation and memory of the whole graph is not relieved. Chen et al. [11] propose a progres-sive shrinking method to bridge the depth gap between the super-net and target-net. NASP [52] and ProxylessNAS [7] only propagate the proximal or sampled paths in search, which effectively reduces the computational cost. A recent study [50] relies on sparse coding to improve consistency and efﬁciency. However, all these methods do not support the search for ﬂexible topologies in a differentiable way.
DenseNAS [15] and PC-DARTS [49] introduce another set of trainable parameters to model path probabilities, but the target-net is still derived based on a hand-crafted rule.
In this paper, we aim to close the gap between the archi-tectures in search and evaluation, and solve the problems mentioned above. Inspired by the observation that only one cell armed with learnable architecture parameters sufﬁces to enable differentiable search, we introduce EnTranNAS composed of Engine-cells and Transit-cells. The Engine-cell is differentiable for architecture search as an engine, while the Transit-cell only transits the derived architecture.
So the network in search is close to that in evaluation. We adopt a feature sharing strategy for more balanced param-eter training of Transit-cell. It also reduces the computa-tion and memory cost in search. Given that Engine-cell still has a gap with the derived architecture, we further develop an architecture derivation method that enables differentiable sparsiﬁcation. The connections with non-zero weights are active for evaluation, which keeps the derived architecture equivalent to the one in search, and meanwhile supports the differentiable search for ﬂexible topologies.
We list the contributions of this study as follows:
• We propose a new NAS method, named EnTranNAS, which effectively reduces the gap between the archi-tectures in search and evaluation. A feature sharing strategy is adopted for more balanced and efﬁcient training of the super-net in search.
• We develop a new architecture derivation method to re-place the hand-crafted rule widely adopted in studies.
The derived target-net has an equivalent architecture to the one in search, which closes the architecture gap between search and evaluation. It also makes topology learnable to explore more ﬂexible search results.
• Extensive experiments verify the validity of our pro-posed methods. We achieve an error rate of 2.22% on
CIFAR-10 with 0.07 GPU-day. Our method is able to efﬁciently search for ﬂexible architectures of different scales directly on ImageNet and achieve a state-of-the-art top-1 error rate of 23.8% in 2.1 GPU-day. 2. Methods
In this section, we ﬁrst brieﬂy review the gradient-based search method widely adopted in current studies, and then develop our proposed methods, EnTranNAS and
EnTranNAS-DST, respectively, showing that how they work to improve the consistency, efﬁciency, and ﬂexibility of differentiable neural architecture search. 2.1. Preliminaries
In [32, 48, 7, 11, 49, 8], the cell-based search space is represented by a directed acyclic graph (DAG) com-posed of n nodes {x1, x2, · · · , xn} and a set of edges
E = {e(i,j)|1 ≤ i < j ≤ n}. For each edge e(i,j), there are K connections in accordance with the candidate opera-tions O = {o1, · · · , oK}. The forward propagation of the super-net for search is formulated as:
K xj = p(i,j) k ok(w(i,j) k
, xi), (1) i<j
X k=1
X 6668
Figure 2. A diagram of our (a) EnTranNAS and (b) EnTranNAS-DST. Engine-cell and Transit-cell are in red and green boxes, respectively.
EnTranNAS reduces the gap between the super-net and target-net. EnTranNAS-DST derive the architecture by keeping the connections with non-zero weights, so the valid computation graph in search is equivalent to the one of derived architecture in evaluation, and is not subject to any hand-crafted topology. The consistency is further improved and a ﬂexible topology is supported. Zoom in to view better. k where p(i,j)
∈ {0, 1} is a binary variable that indicates whether the connection is active, ok denotes the k-th op-eration, and w(i,j) is its corresponding weight on this con-nection and becomes none for non-parametric operations, such as max pooling and identity. Since binary variables are not easy to optimize in a differentiable way, continuous relaxation is adopted and p(i,j) is relaxed as: k k p(i,j) k = exp(α(i,j)
) k exp(α(i,j) k k
,
) (2) k
P where α(i,j) is the introduced architecture parameter jointly optimized with the super-net weights. After search, as shown in Figure 1, a target-net is derived according to a hand-crafted rule based on p(i,j) as the importance of con-k nections. We let P ∈ R|E|×K denote the matrix formed by p(i,j)
, and the forward propagation of the target-net for k evaluation is formulated as: xj = ok(w(i,j) k
, xi), (3)
X(i,k)∈Sj
Sj = {(i, k)|A(i,j) k = 1, ∀i < j, 1 ≤ k ≤ K}, where A(i,j) is the element of A ∈ {0, 1}|E|×K and we have A = ProjΩ(P), where Ω denotes the hand-crafted rule by which only the top-2 strongest elements of each node j in P are projected onto 1 and others are 0. (4) k
It is shown that there is a gap between the super-net and target-net in DARTS. As mentioned in Section 1, the gap may cause inconsistency with target-net, and the super-net is inefﬁcient to train. Besides, the hand-crafted rule restricts the derived architecture to a ﬁxed topology. 2.2. Engine cell and Transit cell
Given that only one cell armed with learnable parame-ters sufﬁces to enable differentiable search, we aim to re-design the DARTS framework. First, at the super-net level, we introduce EnTranNAS composed of Engine-cells and
Transit-cells. As shown in Figure 2 (a), the architecture derivation is not a post-processing step as in DARTS, but is performed at each iteration of search. Engine-cell has the same role as the cell in DARTS and stores the whole DAG.
It performs architecture search as an engine by optimizing architecture parameters α(i,j)
. As a comparison, Transit-cell only transits the currently derived architecture as a sub-graph into later cells. By doing so, EnTranNAS keeps the differentiability for architecture search by Engine-cell, and effectively reduces the gap between the super-net and target-net using Transit-cells. At the ﬁnal layer of super-net, representation is output from a Transit-cell, which has the same architecture as the target-net. Thus, with more con-ﬁdence, a higher super-net performance indicates a better target-net architecture. Besides, a huge amount of computa-tion and memory overhead in Transit-cells is saved. We can k 6669
accordingly use a larger batchsize to speed up the search process, or adopt a larger search space with more candidate operations due to the memory relief.
By introducing a temperature parameter [48, 46], we cal-culate p(i,j) k in Engine-cell as: p(i,j) k = exp(α(i,j)
/τ ) k exp(α(i,j) k k
/τ )
, (5)
P where τ is a temperature parameter. As τ → 0, p(i,j) ap-proaches to a one-hot vector. We do not introduce the Gum-ble random variables as adopted in [48, 46] because our ar-chitecture is not derived by sampling. We anneal τ with epoch so that Engine-cell approximately performs feature selection after convergence and can be close to the derived architecture in Transit-cell. k 2.3. Feature Sharing Strategy
Since Transit-cell only conducts the derived sub-graph, only a small portion of super-net weights w(i,j) is optimized in Transit-cell at each update. It impedes the training ef-ﬁciency of super-net and may affect the search result due to the uneven optimization on candidate operations. In or-der to circumvent this issue and have a balanced parame-ter training for Transit-cells, we introduce a feature sharing strategy within the cell level. k
We notice that the non-parametric operation from a node to different nodes always produces the same features, which can be stored and computed only once. We extend it to parameterized operations, by making the simpliﬁcation that the same operation from node i to other nodes j > i always shares the same feature in one cell. The output of node xj in our EnTranNAS is thus formulated as: xj = i<j ok(w(i) k , xi),
K k=1 p(i,j) k ok(w(i) in Engine-cell, in Transit-cell, k , xi),
P (i,k)∈Sj ( P
P where w(i) is the parameter of operation k for node i, and k becomes none for non-parametric operations. In this way, the number of trainable connections in one cell is reduced from |E|× ¯K to (n−1)× ¯K, where ¯K denotes the number of parametrized operations and |E| = C 2 n. Consequently, the less learnable parameters have a more balanced opportunity to be optimized. In addition, the feature of one operation from the node i is calculated only once and is re-used for later nodes j > i in the same cell, which saves much com-putation and memory overhead and accelerates the search.
Note that the feature sharing strategy harms the represen-tation power of super-net. However, it does not affect the search validity as the features for selection are still produced by the same operations on the same nodes. What we search for is which operation performed on which node, instead of how their parameters are optimized. 2.4. Differentiable Search for Topology
Albeit EnTranNAS reduces the gap between super-net and target-net, the Engine-cell computes the whole graph and is still different from the derived cell for evaluation.
To this end, we further reduce the gap by proposing a new architecture derivation method that supports differentiable sparsiﬁcation and enables the search for topology, named
EnTranNAS-DST. As shown in Figure 2 (b), in Engine-cell, the non-derived connections always have zero weights, such that the valid propagation of Engine-cell is equivalent to that of the derived cell, which eliminates the gap between the architectures in search and evaluation.
In prior studies [32, 11, 49], connection coefﬁcients are induced as softmax activations and thus do not support zero values. A differentiable sparsiﬁcation method is proposed in [27] for network pruning. We combine both advantages to keep the softmax activations and also enable the differen-tiability for zero weights. Concretely, since we need to cut out connections for each intermediate node instead of edge, we compute p(i,j) by Eq. (5), and then perform a connection normalization for each intermediate node j > 1 as: k
ˆp(i,j) k = p(i,j) k max i<j,1≤k≤K
{p(i,j) k
}
, (7) k where ˆp(i,j) is the activation after connection normaliza-tion. We introduce another set of trainable parameters
{β(j)}n j=2 and have the threshold of each intermediate node by t(j) = sigmoid(β(j)). With the thresholds, we can prune these connections as: k = σ(ˆp(i,j) q(i,j) k − t(j)), (8) where σ denotes the ReLU function. Finally, if there ex-ists a k such that q(i,j) 6= 0 for edge (i, j), we perform an operation normalization by: k (6)
ˆq(i,j) k = q(i,j) k k q(i,j) k
, (9) k k where ˆq(i,j)
P is used as the coefﬁcients of connections. It enables sparsiﬁcation in a differentiable way. Given that maxi<j,1≤k≤K{ˆp(i,j)
} = 1 and t(j) < 1, there is at least one connection left for each intermediate node j by Eq. (8), so the cell structure will not be broken, and will keep valid along the training. An illustration of how do we compute
ˆq(i,j) k is shown in Figure 3.
In Engine-cell, we replace the p(i,j) in Eq. (6) with ˆq(i,j) for search. To derive the architecture in Transit-cell or for evaluation, the Sj in Eq. (6) is changed from Eq. (4) to the following form: k k
Sj = {(i, k)|ˆq(i,j) k > 0, ∀i < j, 1 ≤ k ≤ K}, (10) 6670
Figure 3. An illustration of the computation procedures of ˆq(i,j) 1 ≤ k ≤ K and i < j. There is at least one connection left for each intermediate node j since maxi<j,1≤k≤K {ˆp(i,j) as an example. The gray bin denotes the maximal element of p(i,j) for all k
} = 1 and t(j) < 1. k k
Figure 4. Our architecture for search. Engine-cell and Transit-cell are shown in red and green boxes, respectively. Normal and reduction cells are shown in solid and dotted boxes, respectively. by which we only keep the connections with non-zero coef-ﬁcients as the derived architecture, which eliminates its gap with the super-net architecture, and meanwhile does not re-strict the architecture to any ﬁxed topology.
In implementation, we enforce sparsiﬁcation by adding a regularization. Our optimization objective is in accordance with the bi-level manner introduced in [32]. The upper-level loss function of our super-net when optimizing the architec-ture parameters {α(i,j)
} and {β(j)} is formulated as: k n j=2
X (11) min
α,β
− log(t(j)),
Lval (α, w∗) + λ 1 n − 1 where Lval (α, w∗) is the validation loss with the current network parameters w∗, and λ is a hyper-parameter by which we can control the degree of sparsiﬁcation to obtain more ﬂexible topologies. We visualize our search process of EnTranNAS-DST (λ = 0.1) in the video attached in the supplementary material. Its corresponding description is shown in the Appendix ﬁle. 2.5. Implementations
For both EnTranNAS and EnTranNAS-DST, we set the
ﬁrst normal and reduction cells as Engine-cells, and the other cells as Transit-cells. The super-net with 8 cells for search on CIFAR-10 is shown in Figure 4. The ﬁrst cells of normal and reduction cells are set as Engine-cells, while the others are Transit-cells. In experiments, we compare it with other conﬁgurations in Table 1 to ablate our design choice.
Similar to the partial channel connection strategy in [49], we also try to reduce the number of channels to further save memory cost and reduce search time. Different from their method, we adopt the bottleneck technique that is popular in manually designed networks [19, 22, 51]. Concretely, we perform a 1 × 1 convolution to reduce the number of channels by a ratio before feeding a node into all candi-date operations. Another 1 × 1 convolution is appended to recover the number of channels to form each intermediate node. The reduction ratio is set as 4 in our experiments. 3.