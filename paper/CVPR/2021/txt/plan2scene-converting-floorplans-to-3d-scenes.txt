Abstract
We address the task of converting a ﬂoorplan and a set of associated photos of a residence into a textured 3D mesh model, a task which we call Plan2Scene. Our system 1) lifts a ﬂoorplan image to a 3D mesh model; 2) synthesizes sur-face textures based on the input photos; and 3) infers tex-tures for unobserved surfaces using a graph neural network architecture. To train and evaluate our system we create indoor surface texture datasets, and augment a dataset of
ﬂoorplans and photos from prior work with rectiﬁed surface crops and additional annotations. Our approach handles the challenge of producing tileable textures for dominant surfaces such as ﬂoors, walls, and ceilings from a sparse set of unaligned photos that only partially cover the resi-dence. Qualitative and quantitative evaluations show that our system produces realistic 3D interior models, outper-forming baseline approaches on a suite of texture quality metrics and as measured by a holistic user study. 1.

Introduction
Digital 3D scene representations of interiors are key to emerging application areas such as AI assistants, online product marketing, and augmented reality. Private resi-dences are predominantly designed with CAD software.
However, texture-mapped 3D scene models of the built interiors are rarely available. Despite recent progress in indoor reconstruction techniques and depth-sensing hard-ware, the state-of-the-art in room layout inference and photogrammetry-based 3D modeling is still far from reli-able and practical for non-expert users.
This paper explores a novel path for 3D interior digitiza-tion by utilizing a residential ﬂoorplan and a sparse set of photos without camera pose as input, which are prevalent in online real estate listings. More precisely, we present the Plan2Scene task: conversion of a residential ﬂoorplan and photos of a residence to a textured 3D scene model (see
Figure 1). In the context of this task, we focus on textur-ing of architectural surfaces. This task is challenging due to 1) the lack of camera poses for the photos; 2) the challenge of photometric calibration under varying lighting conditions
Output
Input
Bathroom
Bedroom
Bathroom
Bedroom
Corridor,
Entrance
Reception
Kitchen
Plan2Scene
Figure 1: Our system addresses the Plan2Scene task by con-verting a ﬂoorplan and set of photos to a textured 3D mesh. and 3) limited photo coverage, leaving many surfaces unob-served. The Plan2Scene task allows us to articulate these challenges for residential interiors and to identify suitable texture appropriateness metrics.
Our key idea is to model the architectural surfaces and identify appropriate textures for each surface. We treat photos as sparse and partial observations of surface tex-tures and formulate a texture inference task, instead of rely-ing on exact camera poses and texture-mapping as in prior work [21]. Textures for observed surfaces are generated us-ing an encoder-decoder architecture. Unobserved surfaces (i.e., surfaces for which a photo is not available) are handled by a graph neural network (GNN) that propagates informa-tion while learning inter/intra-room consistency.
The paper also makes three dataset contributions. First, we extend an existing database of ﬂoorplans and photos (Rent3D) [21] by annotating more room boundaries, assign-ing more photos to the rooms, and annotating object-icons indicated on the ﬂoorplans. We also curate two datasets of textures of common indoor substances (e.g., ‘painted walls’,
‘tiles’, ‘carpets’) from various online sources for training our texture synthesis method.
Through qualitative and quantitative evaluations using a suite of metrics characterizing texture appropriateness and quality, and a user study, we demonstrate that the proposed approach outperforms baselines including rectiﬁed image patch texturing, and direct texture retrieval. We release all our code, data, and pretrained models to the community.1 1https://3dlg-hcvc.github.io/plan2scene/ 10733
2.