Abstract
Panoptic segmentation is a challenging task aiming to simultaneously segment objects (things) at instance level and background contents (stuff) at semantic level. Exist-ing methods mostly utilize a two-stage detection network to attain instance segmentation results, and a fully convolu-tional network to produce a semantic segmentation predic-tion. Post-processing or additional modules are required to handle the conﬂicts between the outputs from these two nets, which makes such methods suffer from low efﬁciency, heavy memory consumption and complicated implementation. To simplify the pipeline and decrease computation/memory cost, we propose an one-stage approach called Lightweight
Panoptic Segmentation Network (LPSNet), which does not involve a proposal, anchor or mask head. Instead, we pre-dict a bounding box and semantic category at each pixel upon the feature map produced by an augmented feature pyramid, and design a parameter-free head to merge the per-pixel bounding box and semantic prediction into panop-tic segmentation output. Our LPSNet is not only efﬁcient in computation and memory, but also accurate in panop-tic segmentation. Comprehensive experiments on COCO,
Cityscapes and Mapillary Vistas datasets demonstrate the promising effectiveness and efﬁciency of the proposed LP-SNet. 1.

Introduction
Panoptic Segmentation (PS) is a challenging task aim-ing to assign each pixel a semantic category and segment each object in the input image [11]. Speciﬁcally, the goal of PS is to segment countable objects (things) at instance level and parse amorphous image regions (stuff) at seman-tic level. Therefore, compared with semantic segmentation or instance segmentation, PS provides more comprehensive scene information and can be broadly used in autonomous driving and scene parsing.
A straightforward solution to tackle PS is to merge the instance segmentation and semantic segmentation predic-tions, as is done by most existing methods [11, 3, 10, 35, (a) Input image. (b) Object detection. (c) Semantic segmentation. (d) Panoptic segmentation.
Figure 1. (Better viewed in color). Lightweight Panoptic Segmen-tation Network (LPSNet) predicts bounding boxes and semantic segmentation, and merge them into panoptic segmentation output with a parameter-free panoptic head. 34]. To achieve good panoptic quality, these methods are of-ten built upon large networks and complex pipelines, with-out concerning efﬁciency and computational resources. For example, Mask RCNN [7] has been widely used or in-tegrated to attain promising accuracy, despite that Mask
RCNN is heavy in resources consumption and slow in in-ference. Moreover, the utilization of Mask RCNN naturally introduces conﬂicts with the output from semantic segmen-tation branch, hence heuristic or complex post-processing like the pixel rank module [20] are required to deal with the outputs of both branches and obtain uniﬁed panoptic seg-mentation results. Nevertheless, we argue that an efﬁcient solution for PS is not only desired for practical usage like autonomous driving, but also of great importance for poten-tial performance gain by saving memory for larger image and batch size.
In this paper, we propose Lightweight Panoptic Segmen-tation Network (LPSNet) to tackle the drawback mentioned above. By introducing a parameter-free panoptic head, our
LPSNet decomposes panoptic segmentation into two inde-116746
pendent sub-tasks, i.e., object detection and semantic seg-mentation, as shown in Figure 1. Thanks to fully convo-lutional network [22] and the recent one-stage anchor-free detector [13, 12, 31], both sub-tasks can be solved in one pass, in a fully-convolutional style. Therefore, compared with existing two-stage PS methods like UPSNet[34] and
AUNet [16], our LPSNet can be around two times faster with less memory consumption. Moreover, by slightly in-creasing train/test image size, our LPSNet achieves superior accuracy while still maintaining the advantages in memory and computation.
Comprehensive benchmarks experiments
COCO [19], Cityscapes [2] and Mapillary Vistas [24] datasets evidently demonstrate that LPSNet achieves competitive performances with excellent efﬁciency. We summarize our main contributions as follows: on
• We present a novel panoptic segmentation approach
LPSNet, which is different from existing methods and produces panoptic segmentation in one pass. Our LP-SNet does not involve anchor, proposal or mask head, thus is efﬁcient in computation, memory and hyper-parameters usage. For examples, anchor settings such as scale, aspect ratio are sensitive to different applica-tions and datasets. The parameters of proposal ground truth generation, extraction and selecting strategy re-the quires sophistical tuning. For the mask head, weight of the loss function is subject to careful trial and error. In contrast, our LPSNet is easy to train and more generic to different scenarios.
• We decompose the PS task as object detection and se-mantic segmentation with a parameter-free PS head.
The head takes detection boxes, object center off-set prediction and semantic segmentation as inputs to obtain PS results, while existing approaches usually work on instance segmentation and semantic segmen-tation results. Our panoptic head is portable to other networks with detection and semantic segmentation branches.
• Additionally, overlapping or deformable objects often cause severe false positive in most one-stage detection
In our approach, we har-methods like FCOS [31]. ness mask information to determine whether a pixel is positive and central or not, thus provide more accurate learning targets and boost the ﬁnal performances. 2.