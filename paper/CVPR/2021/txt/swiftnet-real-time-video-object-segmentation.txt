Abstract
In this work we present SwiftNet for real-time semisu-pervised video object segmentation (one-shot VOS), which reports 77.8% J &F and 70 FPS on DAVIS 2017 val-idation dataset, leading all present solutions in over-all accuracy and speed performance. We achieve this by elaborately compressing spatiotemporal redundancy in matching-based VOS via Pixel-Adaptive Memory (PAM).
Temporally, PAM adaptively triggers memory updates on frames where objects display noteworthy inter-frame vari-ations. Spatially, PAM selectively performs memory up-date and match on dynamic pixels while ignoring the static ones, signiﬁcantly reducing redundant computations wasted on segmentation-irrelevant pixels.
To promote efﬁcient reference encoding, light-aggregation encoder is also in-troduced in SwiftNet deploying reversed sub-pixel. We hope SwiftNet could set a strong and efﬁcient baseline for real-time VOS and facilitate its application in mo-bile vision. The source code of SwiftNet can be found at https://github.com/haochenheheda/SwiftNet. 1.

Introduction
Given the ﬁrst frame annotation, semi-supervised video object segmentation (one-shot VOS) localizes the annotated object(s) on pixel-level throughout the video. One-shot
VOS generally adopts a matching-based strategy, where target objects are ﬁrst modeled from historical reference frames, then precisely matched against the incoming query frame for localization. Being a video-based task, VOS ﬁnds vast applications in surveillance, video editing, and mobile visions, most of which ask for real-time processing [41].
Nonetheless, although pursued in fruitful endeavors [2, 22, 35, 6, 17, 29, 11], real-time VOS remains unsolved, as object variation over-time poses heavy demands for so-phisticated object modeling and matching computations.
As a compromise, most existing methods solely focus on
∗These authors contribute equally.
Figure 1. Accuracy and speed performance of state-of-the-art methods on DAVIS2017 validation dataset, methods locate on the right side of the red vertical dotted line meet the real-time require-ment. Our solutions (ResNet-18 and 50 versions) are marked in red. improving segmentation accuracy while at the expense of speed. Amongst, memory-based methods [20, 46, 45] re-veal exceptional accuracy with comprehensively modeling object variations using all historical frames and expres-sive non-local [33] reference-query matching. Unfortu-nately, deploying more reference frames and complicated matching scheme inevitably slow down segmentation. Ac-cordingly, recent attempts seek to accelerate VOS with re-duced reference frames and light-weight matching scheme
[9, 26, 13, 3, 34, 29, 4, 35]. For the ﬁrst aspect, solu-tions proposed in [26, 13, 3, 34, 29, 4, 35] follow a mask-propagation strategy, where only the ﬁrst and last histori-cal frames are considered reference for current segmenta-tion. For the second aspect, efﬁcient pixel-wise matching
[13, 29, 34], region-wise distance measuring [26, 9, 4], and correlation ﬁltering [3, 32] are deployed to reduce compu-tations. However, as shown in Fig.1, although these accel-erated methods enjoy faster segmentation speed, they still barely meet real-time requirement, and more critically, they are far from state-of-the-art segmentation accuracy. 11296
Figure 2. An illustration of pixel-wise memory update in video clips. In each row, pixels updated into memory (red) are the ones previously invisible and remain static afterwards.
We argue that, the accurate solutions are less efﬁcient due to the spatiotemporal redundancy inherently resides in matching-based VOS, and the fast solutions suffer de-graded accuracy for reducing the redundancy indiscrimi-nately. Considering its pixel-wise modeling, matching, and estimating nature, matching-based VOS manifests positive correlation between processing time T and multiplied num-ber of pixels Nr and Nq in reference and query frames as described in Eqa.1. The spatiotemporal redundancy denotes that Nr is populated with pixels not beneﬁcial for accurate segmentation. Temporally, existing methods [20, 45] care-lessly involve all historical frames (mostly by periodic sam-pling) for reference modeling, resulting in the fact that static frames showing no object evolution are repeatedly mod-eled, while dynamic frames containing incremental object information are less attended. Spatially, full-frame model-ing and matching are adopted as defaults [20, 40], wherein most static pixels are redundant for segmentation. Illustra-tion in Fig. 2 vividly advocates the above point. From this standpoint, explicitly compressing pixel-wise spatiotempo-ral redundancy is the best way to yield accurate and fast one-shot VOS.
T ∝ O(Nr × Nq), (1)
Accordingly, we propose SwiftNet for real-time one-shot video object segmentation. Overall, as depicted in
Fig.3, SwiftNet instantiates matching-based segmentation with an encoder-decoder architecture, where spatiotempo-ral redundancy is compressed within the proposed Pixel-Adaptive Memory (PAM) component. Temporally, instead of involving all historical frames indiscriminately as ref-erence, PAM introduces a variation-aware trigger module, which computes inter-frame difference to adaptively ac-tivate memory update on temporally-varied frames while overlook the static ones. Spatially, we abolish full-frame operations and design pixel-wise update and match mod-ules in PAM. For pixel-wise memory update, we explicitly evaluate inter-frame pixel similarity to identify a subset of pixels beneﬁcial for memory, and incrementally add their feature representation into the memory while bypassing the redundant ones. For pixel-wise memory match, we com-press the time-consuming non-local computation to accom-modate the pixel-wise memory as reference, thus achieving efﬁcient matching without degradation of accuracy. To fur-ther accelerate segmentation, PAM is equipped with a novel light-aggregation encoder (LAE), which eschews redundant feature extraction and enables multi-scale mask-frame ag-gregation leveraging reversed sub-pixel operations.
In summary, we highlight three main contributions:
• We propose SwiftNet to set the new record in over-all segmentation accuracy and speed, thus providing a strong baseline for real-time VOS with publicized source code.
• We pinpoint spatiotemporal redundancy as the
Achilles heel of real-time VOS, and resolve it with
Pixel-Adaptive Memory (PAM) composing variation-aware trigger and pixel-wise update & matching.
Light-Aggregation Encoder (LAE) is also introduced for efﬁcient and thorough reference encoding.
• We conduct extensive experiments deploying various backbones on DAVIS 2016 & 2017 and YouTube-VOS datasets, reaching the best overall segmentation accu-racy and speed performance at 77.8% J &F and 70
FPS on DAVIS2017 validation set. 2.