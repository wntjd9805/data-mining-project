Abstract
We wish to detect speciﬁc categories of objects, for on-line vision systems that will run in the real world. Object de-tection is already very challenging. It is even harder when the images are blurred, from the camera being in a car or a hand-held phone. Most existing efforts either focused on sharp images, with easy to label ground truth, or they have treated motion blur as one of many generic corruptions.
Instead, we focus especially on the details of egomotion induced blur. We explore ﬁve classes of remedies, where each targets different potential causes for the performance gap between sharp and blurred images. For example, ﬁrst deblurring an image changes its human interpretability, but at present, only partly improves object detection. The other four classes of remedies address multi-scale texture, out-of-distribution testing, label generation, and conditioning by blur-type. Surprisingly, we discover that custom label gen-eration aimed at resolving spatial ambiguity, ahead of all others, markedly improves object detection. Also, in con-trast to ﬁndings from classiﬁcation, we see a noteworthy boost by conditioning our model on bespoke categories of motion blur.
We validate and cross-breed the different remedies ex-perimentally on blurred COCO images and real-world blur datasets, producing an easy and practical favorite model with superior detection rates. 1.

Introduction
A little motion blur is present in most hand-held pho-tography. Blur is ever harder to ignore because images are increasingly captured on the move, e.g. by a gimbaled robot or from an autonomous vehicle. Precisely these on-the-go situations prompt us to explore: how much does motion blur severity impact object detection? What can be done about it? Detection is important because it underpins many other tasks, such as tracking and re-identiﬁcation, and our initial scope is further narrowed to egomotion induced blur.
Unsurprisingly, the severity of the blur correlates with detection failure [2]. Fig. 1 shows an example. An ideal al-gorithm will make that degradation more gradual, and could
Figure 1. a) Original sharp MS COCO [26] image with object de-tections. b) Same image with signiﬁcant linear motion-blur, with
COCO ground-truth. c) Failed predictions from original Faster-RCNN. d) Predictions from network with our proposed model. someday enable a model that surpasses even a human’s abil-ity to see through blur. Instead of a single breakthrough, it is more likely that a combination of approaches is needed.
Much like the “devil in the details” papers [6, 7], the task speciﬁcs and pipeline likely make a difference.
Our main contribution is an empirical exploration of ﬁve classes of remedies. These remedies are selected to cope with ﬁve proposed causes for reduced detection accuracy.
The ﬁve cause/remedy pairs explored here are: 1) Is the en-tire image too blurry to be useful? Deblur test image ﬁrst. 2) Is texture mismatch along blur axes confusing the model?
Spatially transform image to compensate. 3) Does test-time blur differ from training data? Train model for out-of-distribution robustness, and/or perform test-time tuning of network. 4) Are the training labels incorrect? Customize labels to match detection-in-blur task and reconsider labels used for testing. 5) Are egomotion blur types too diverse?
Treat detection in blur as a multi-task problem.
Overall, we propose a new model that focuses on the remedies from (4) and (5), and set a new standard for online object detection in egomotion-induced blur. 2.