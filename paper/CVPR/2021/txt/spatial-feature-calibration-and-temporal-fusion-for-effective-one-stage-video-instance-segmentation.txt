Abstract
Modern one-stage video instance segmentation networks suffer from two limitations. First, convolutional features are neither aligned with anchor boxes nor with ground-truth bounding boxes, reducing the mask sensitivity to spa-tial location. Second, a video is directly divided into in-dividual frames for frame-level instance segmentation, ig-noring the temporal correlation between adjacent frames.
To address these issues, we propose a simple yet effective one-stage video instance segmentation framework by spa-tial calibration and temporal fusion, namely STMask. To ensure spatial feature calibration with ground-truth bound-ing boxes, we ﬁrst predict regressed bounding boxes around ground-truth bounding boxes, and extract features from them for frame-level instance segmentation. To further ex-plore temporal correlation among video frames, we aggre-gate a temporal fusion module to infer instance masks from each frame to its adjacent frames, which helps our frame-work to handle challenging videos such as motion blur, partial occlusion and unusual object-to-camera poses. Ex-periments on the YouTube-VIS valid set show that the pro-posed STMask with ResNet-50/-101 backbone obtains 33.5
% / 36.8 % mask AP, while achieving 28.6 / 23.4 FPS on video instance segmentation. The code is released online https://github.com/MinghanLi/STMask. 1.

Introduction
Video instance segmentation aims to obtain the pixel-level segmentation mask for individual instances of all classes over the entire frames of a video, which heavily depends on spatial position-sensitive features to localize frame-level objects and redundant temporal information to track instances across frames. Following object detection and image instance segmentation works, modern video in-stance segmentation approaches usually adopt the top-down
∗Corresponding author.This work is supported by the Hong Kong RGC
RIF grant (R5001-18). (a) Original anchors and features (b) Calibrated anchors and features (c) Adaptive features (d) Aligned features
Figure 1.
Spatial calibration for anchor boxes and bounding boxes. (a) and (b) display anchors and features in original and cal-ibrated one-stage networks respectively. (c) and (d) demonstrate adaptive and aligned features extracted from predicted bounding boxes. Purple, blue and green rectangles denote anchors, predicted and ground-truth bounding boxes respectively, where coloured ar-eas indicate the receptive ﬁled of their convolutional features. framework of ﬁrst detecting and segmenting objects frame by frame and then linking instance masks across frames.
Top-down video instance segmentation approaches can be divided into two-stage and one-stage methods. By adding a tracking branch to Mask R-CNN [18], two-stage video instance segmentation methods [43, 2, 16] ﬁrst pre-dict region-of-interests (RoIs) around ground-truth bound-ing boxes, and then feed aligned features via RoIPool-ing [32] or RoIAlign [18] to segment frame-level object masks and to track cross-frame instances. To obtain better location-sensitive features for mask predictor, many spatial feature calibration strategies for RoIs have been continu-ously proposed in recent years like Deformable RoI [12] and Hybrid Task Cascade [8]. For temporal information exploration, recently proposed MaskProp [2] utilizes tem-poral features propagated from all frames of a video clip for clip-level instance tracking. Obviously, two-stage meth-ods have recognized the importance of spatial feature lo-calisation and temporal feature tracking for video instance 11215
segmentation. One-stage instance segmentation networks
[11, 42, 5, 35, 6, 24, 25], which focus more on real-time speed, usually employ a fully convolutional network struc-ture to directly predict the ﬁnal mask for instances. Without
RoIs of two-stage methods for localisation, early one-stage methods have to introduce extra position-sensitive infor-mation to improve segmentation performance like position-sensitive score maps [11] or semantic features [9]. In image domain, recently proposed anchor-based one-stage meth-ods like Yolact [5] and CondInst [35] decompose instance segmentation as the linear combination between instance-speciﬁc mask coefﬁcients and instance-independent proto-types. Furthermore, SipMask [6], introducing a tracking branch in Yolact[5], achieves real-time speed but inferior performance in video instance segmentation task.
Analysing these anchor-based one-stage instance seg-mentation methods, we observe that, as shown in Fig.1 (a), multiple anchors of different shapes at each spatial position (purple rectangles) share same convolutional features (yel-low area), which are neither aligned with pre-deﬁned anchor boxes nor with ground-truth bounding boxes. This fact does violate that instances segmentation is a spatial location-sensitive task. On the other hand, one-stage video instance segmentation methods directly divide a video into separate frames to perform image instance segmentation frame by frame and then track them across frames, which completely ignores high temporal correlation between adjacent frames.
This may fail to handle those challenging videos with mo-tion blur, partial occlusion, or unconventional object-to-camera poses. In other words, modern one-stage video in-stance segmentation methods achieve real-time speed at the cost of discarding spatial feature calibration and temporal feature correlation.
To address the issues, we propose a simple yet effective one-stage video instance segmentation framework, named
STMask. Firstly, we design a feature calibration strategy for anchor boxes and ground-truth bounding boxes to ob-tain more precise spatial features. Speciﬁcally, as shown in Fig.1 (b), to enable each anchor box can extract its own speciﬁc features, we ﬁrst design multiple convolutional ker-nels at each spatial position, and then generate anchors ac-cording to the receptive ﬁeld of these convolutional ker-nels. To improve feature presentation for objects segmen-tation and tracking, we ﬁrst predict regressed bounding boxes around ground-truth bounding boxes by regression branch, and then extract features from them to segment and track instances. As shown in Fig.1 (c) and (d), we pro-vide two strategies to extract features from regressed bound-ing boxes, including adaptive features by a single 1 × 1 convolutional layer and aligned features by mathematical derivation. Finally, we explore temporal correlation be-tween video frames by adding a temporal fusion module to infer instance masks from adjacent frames, thereby improv-ing the performance of objects detection, segmentation and tracking for those challenging videos. 2.