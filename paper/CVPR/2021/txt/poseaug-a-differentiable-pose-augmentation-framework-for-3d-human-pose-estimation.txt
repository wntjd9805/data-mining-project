Abstract
Source dataset: H36M
Cross dataset: 3DHP
Existing 3D human pose estimators suffer poor gener-alization performance to new datasets, largely due to the limited diversity of 2D-3D pose pairs in the training data.
To address this problem, we present PoseAug, a new auto-augmentation framework that learns to augment the avail-able training poses towards a greater diversity and thus im-prove generalization of the trained 2D-to-3D pose estima-tor. Speciﬁcally, PoseAug introduces a novel pose augmen-tor that learns to adjust various geometry factors (e.g., pos-ture, body size, view point and position) of a pose through differentiable operations. With such differentiable capacity, the augmentor can be jointly optimized with the 3D pose estimator and take the estimation error as feedback to gen-erate more diverse and harder poses in an online manner.
Moreover, PoseAug introduces a novel part-aware Kine-matic Chain Space for evaluating local joint-angle plausi-bility and develops a discriminative module accordingly to ensure the plausibility of the augmented poses. These elab-orate designs enable PoseAug to generate more diverse yet plausible poses than existing ofﬂine augmentation methods, and thus yield better generalization of the pose estimator.
PoseAug is generic and easy to be applied to various 3D pose estimators. Extensive experiments demonstrate that
PoseAug brings clear improvements on both intra-scenario and cross-scenario datasets. Notably, it achieves 88.6% 3D PCK on MPI-INF-3DHP under cross-dataset evalua-tion setup, improving upon the previous best data augmen-tation based method [22] by 9.1%. Code can be found at: https://github.com/jfzhang95/PoseAug. 1.

Introduction 3D human pose estimation aims to estimate 3D body joints in images or videos.
It is a fundamental task with broad applications in action recognition [48, 40], human-*Equal contribution. †Work done during an internship at Huawei inter-national Pte Ltd.
Figure 1: Estimation error (in MPJPE) on H36M (intra-dataset evaluation) and 3DHP (cross-dataset evaluation) of four well established models [53, 26, 34, 3] trained with and without PoseAug. PoseAug signiﬁcantly improves their performance for both the intra- and cross-dataset settings. robot interaction [11], human tracking [29], etc. This task is typically solved using learning-based methods [26, 53, 3, 32] with ground truth annotations that are collected in the laboratorial environments [16]. Despite their success in in-door scenarios, these methods are hardly generalizable to cross-scenario datasets (e.g., an in-the-wild dataset). We ar-gue that their poor generalization is mainly due to the lim-ited diversity of training data, such as limited variations in human posture, body size, camera view point and position.
Recent works explore data augmentation to improve the training data diversity and enhance the generalization of their trained models. They either generate data through im-age composition [38, 29, 28] and synthesis [5, 43], or di-rectly generate 2D-3D pose pairs from the available training data by applying pre-deﬁned transformations [22]. How-ever, all of these works regard data augmentation and model training as two separate phases, and conduct data augmen-tation in an ofﬂine manner without interaction with model training. Consequently, they tend to generate ineffective augmented data that are too easy for model training, lead-ing to marginal boost to the model generalization. More-over, these methods heavily rely on pre-deﬁned rules such as joint angle limitations [1] and kinematics constraints [38] 8575
for data augmentation, which limit the diversity of the gen-erated data and make the resulting model hardly generalize to more challenging in-the-wild scenes.
To improve the diversity of augmented data, we propose
PoseAug, a novel auto-augmentation framework for 3D hu-man pose estimation. Instead of conducting data augmenta-tion and network training separately, PoseAug jointly opti-mizes the augmentation process with network training end-to-end in an online manner. Our main insight is that the feedback from the training process can be used as effec-tive guidance signals to adapt and improve the data aug-mentation. Speciﬁcally, PoseAug exploits a differentiable augmentation module (the ‘augmentor’) implemented by a neural network to directly augment 2D-3D pose pairs in the training data. Considering the potential domain shift with respective to geometry in pose pairs (e.g., postures, view points) [37, 22, 51], the augmentor learns to perform three types of augmentation operations to respectively control 1) the skeleton joint angle, 2) the body size, and 3) the view point and human position.
In this way, the augmentor is able to produce augmented poses with more diverse geo-metric features and thus relieves the diversity limitation is-sue. With its differentiable capacity, the augmentor can be optimized together with the pose estimator end-to-end via an error feedback strategy. Concretely, by taking increasing training loss of the estimator as the learning target, the aug-mentor can learn to enrich the input pose pairs via enlarging data variations and difﬁculties; in turn, through combating such increasing difﬁculties, the pose estimator can become increasingly more powerful during the training process.
To ensure the plausibility of the augmented poses, we use a pose discriminator module to guide the augmentation, to avoid generating implausible joint angles [1], unreasonable positions or view points that may hamper model training.
In particular, the module consists of a 3D pose discrimina-tor for enhancing the joint angle plausibility and a 2D pose discriminator for guiding the body size, view point and po-sition plausibility. The 3D pose discriminator adopts the
Kinematic Chain Space (KCS) [45] representation and ex-tends it into a part-aware KCS for local-wise supervision.
More concretely, it splits skeleton joints into several parts and focuses on joint angles in each part separately instead of the whole body pose, which yields greater ﬂexibility of the augmented poses. By jointly training the pose augmen-tor, estimator and discriminator in an end-to-end manner (Fig. 2), PoseAug can largely improve the training data di-versity, and thus boost model performance on both source and more challenging cross-scenario datasets.
Our PoseAug framework is ﬂexible regarding the choice of the 3D human pose estimator. This is demonstrated by the clear improvements made with PoseAug on four rep-resentative 3D pose estimation models [53, 26, 34, 3] over both source (H36M) [16] and cross-scenario (3DHP) [29]
Figure 2: Overview of our PoseAug framework. The aug-mentor, estimator and discriminator are jointly trained end-to-end with an error-feedback training strategy. As such, the augmentor learns to augment data with guidance from the estimator and discriminator. datasets (Fig. 1). Remarkably, it brings more than 13.1% average improvement w.r.t. MPJPE for all models on 3DHP. it achieves 88.6% 3D PCK on 3DHP under
Moreover, cross-dataset evaluation setup, improving upon the previous best data augmentation based method [22] by 9.1%.
Our contributions are three-fold. 1) To the best of our knowledge, we are the ﬁrst to investigate differentiable data augmentation on 3D human pose estimation. 2) We pro-pose a differentiable pose augmentor, together with the er-ror feedback design, which generates diverse and realistic 2D-3D pose pairs for training the 3D pose estimator, and largely enhances the model’s generalization ability. 3) We propose a new part-aware 3D discriminator, which enlarges the feasible region of augmented poses via local-wise super-vision, ensuring both data plausibility and diversity. 2.