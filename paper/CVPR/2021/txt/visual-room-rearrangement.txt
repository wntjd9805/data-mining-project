Abstract 1.

Introduction
There has been a signiﬁcant recent progress in the ﬁeld of Embodied AI with researchers developing models and algorithms enabling embodied agents to navigate and in-teract within completely unseen environments. In this pa-per, we propose a new dataset and baseline models for the task of Rearrangement. We particularly focus on the task of Room Rearrangement: an agent begins by exploring a room and recording objects’ initial conﬁgurations. We then remove the agent and change the poses and states (e.g., open/closed) of some objects in the room. The agent must restore the initial conﬁgurations of all objects in the room.
Our dataset, named RoomR, includes 6,000 distinct rear-rangement settings involving 72 different object types in 120 scenes. Our experiments show that solving this challenging interactive task that involves navigation and object interac-tion is beyond the capabilities of the current state-of-the-art techniques for embodied tasks and we are still very far from achieving perfect performance on these types of tasks.
One of the longstanding goals of Embodied AI is to build agents that interact with their surrounding world and per-form tasks. Recently, navigation and instruction following tasks have gained popularity [1, 2, 4] in the Embodied AI community. These tasks are the building blocks of inter-active embodied agents, and over the past few years, we have observed remarkable progress regarding the develop-ment of models and algorithms. However, a typical assump-tion for these tasks is that the environment is static; namely, the agent can move within the environment but cannot inter-act with objects or modify their state. The ability to interact with and change its environment is crucial for any artiﬁ-cial embodied agent and cannot be studied in static envi-ronments. There is a general trend towards interactive tasks
[50, 41, 49]. These tasks focus on speciﬁc aspects of in-teraction such as object manipulation, long-horizon plan-ning and understanding pre-condition and post-conditions of actions. In this paper, we address a more comprehensive task in a visually rich environment that can subsume each of these skills. 5922
We address an instantiation of the rearrangement prob-lem, an interactive task, recently introduced by Batra et al. [3]. The goal of the rearrangement task is to reach a goal room conﬁguration from an initial room conﬁguration through interaction. In our instantiation, an agent must re-cover a scene conﬁguration after we have randomly moved, or changed the state of, several objects (e.g. see Fig. 1).
This problem has two stages: walkthrough and unshuf-ﬂe. During the walkthrough stage, the agent may explore the scene and, through egocentric perception, record infor-mation regarding the goal conﬁguration. We then remove the agent from the room and move some objects to other locations or change their state (e.g. opening a closed mi-In the unshufﬂe stage, the agent must interact crowave). with objects in the room to recover the goal conﬁguration observed in the walkthrough stage.
Rearrangement poses several challenges such as infer-ring the visual differences between the initial and goal con-ﬁgurations, inferring the objects’ state, learning the post-conditions and pre-conditions of actions, maintaining a persistent and compact memory representation during the walkthrough stage, and successful navigation. To establish baseline performance for our task, we evaluate an actor-critic model akin to the state-of-the-art models used for long-horizon tasks such as navigation. We train our base-lines using decentralized distributed proximal policy op-timization (DD-PPO) [47, 40], a reward-based RL algo-rithm, as well as with DAgger [37], a behavioral cloning method. During the walkthrough stage, the agent uses a non-parametric mapping module to memorize its observa-tions along with any visible objects and their positions. In the unshufﬂe stage the agent compares images that it ob-serves against what it has observed in its map and may use this information to inform which objects it should move or open. As a proof-of-concept we also run experiments with a model that includes a semantic mapping component adapted from the Active Neural SLAM model [8].
To facilitate research in this challenging direction, we compiled the Room Rearrangement (RoomR) dataset.
RoomR is built upon AI2-THOR [29], a virtual interac-tive environment that enables interacting with objects and changing their state. The RoomR dataset includes 6,000 re-arrangement tasks that involve changing the pose and state of multiple objects within an episode. The level of the dif-ﬁculty of each episode varies depending on the differences between the initial and the goal object conﬁgurations. We have used 120 rooms and more than 70 unique object cate-gories to create the dataset.
We consider two variations of the room rearrangement task. In the ﬁrst setting, which we call the 1-Phase task, the agent completes the walkthrough and unshufﬂe stages in parallel so that it is given aligned images from the walk-through and unshufﬂe conﬁgurations at every step. In the second setting, the 2-Phase task, the agent must complete the walkthrough and unshufﬂe stages sequentially; this 2-Phase variant is more challenging as it requires the agent to reason over longer time spans. Highlighting the difﬁculty of the rearrangement, our evaluations show that our strong baselines struggle even in the easier 1-Phase task. Rear-rangement poses a new set of challenges for the embodied-AI community. Our code and dataset are publicly available.
A supplementary video1 provides the description of the task and some qualitative results. 2.