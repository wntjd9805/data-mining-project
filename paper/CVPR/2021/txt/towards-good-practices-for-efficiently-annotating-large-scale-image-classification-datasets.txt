Abstract
Efﬁciently Annotating Multi-Class Labels
Large Image Collection  Images to Label
Data is the engine of modern computer vision, which necessitates collecting large-scale datasets. This is expen-sive, and guaranteeing the quality of the labels is a ma-jor challenge. In this paper, we investigate efﬁcient anno-tation strategies for collecting multi-class classiﬁcation la-bels for a large collection of images. While methods that ex-ploit learnt models for labeling exist, a surprisingly preva-lent approach is to query humans for a ﬁxed number of labels per datum and aggregate them, which is expensive.
Building on prior work on online joint probabilistic mod-eling of human annotations and machine-generated beliefs, we propose modiﬁcations and best practices aimed at min-imizing human labeling effort. Speciﬁcally, we make use of advances in self-supervised learning, view annotation as a semi-supervised learning problem, identify and mitigate pitfalls and ablate several key design choices to propose ef-fective guidelines for labeling. Our analysis is done in a more realistic simulation that involves querying human la-belers, which uncovers issues with evaluation using exist-ing worker simulation methods. Simulated experiments on a 125k image subset of the ImageNet100 show that it can be annotated to 80% top-1 accuracy with 0.35 annotations per image on average, a 2.7x and 6.7x improvement over prior work and manual annotation, respectively. 1 1.

Introduction
Data, the basic unit of machine learning, has tremendous impact on the success of learning-based applications. Much of the recent A.I. revolution can be attributed to the cre-ation of the ImageNet dataset [12], which showed that im-age classiﬁcation with deep learning at scale [25] can result in learning strong feature extractors that transfer to domains and tasks beyond the original dataset. Using citations as a proxy, ImageNet has supported at least 40,000 research
It has been unmatched as a pre-training projects to date. dataset to downstream tasks, due to its size, diversity and the 1Code at: https://github.com/ﬁdler-lab/efﬁcient-annotation-cookbook y c a r u c c
A l e b a
L 100 90 80 70 60 50 40 30 20 10 0 w/ machine w/o machine 0.1 0.3 0.5 0.7 0.9 1.1 1.3
#Annotations per image
High Label Conﬁdence
Low Label Conﬁdence
+
Human Labelers w/ Estimated Skill
Machine Labeler
Figure 1: We tackle efﬁcient model-assisted annotation of multi-class labels at scale. We propose improvements to prior work by incorporating self- and semi-supervised learning and address asso-ciated challenges. Extensive ablation of common design choices in realistically simulated experiments leads us to provide best prac-tice recommendations to minimize human annotation effort. quality of labels. Since its conception, interest in creating large datasets serving diverse tasks and domains has sky-rocketed. Examples include object detection [47], action-recognition- [10], and 3D reconstruction [32, 6], in domains such as self-driving [15, 3], and medical imaging [44].
ImageNet and its successors such as OpenImages [26] collected their data using search engines on the web, fol-lowed by human veriﬁcation of either the search query term or automatically generated labels. Thus, their labeling is formulated as a veriﬁcation task, i.e., does this image really belong to the class, allowing efﬁcient annotation at scale.
In contrast to ImageNet labeling, in many practical use cases, the data and labels of interest are often known apriori.
This departs from the case above where arbitrary images could be used by querying keywords online. A common approach used in practice is to query humans to get a ﬁxed number of labels per datum and aggregate them [29, 22], presumably because of its simplicity and reliability. This can be prohibitively expensive and inefﬁcient in human re-source utilization for large datasets, as it assumes equal ef-fort needed per datum. We build on prior work and inves-tigate integration of modern learning methods to improve 4350  
annotation efﬁciency for multi-class classiﬁcation at scale.
Recent work [2] explored integrating a learnt classiﬁer into the DS model [11] in an online setting. Their method allows principled online estimation of worker skills and la-bel uncertainty. This is used to decide whether another hu-man should be queried for a datum. We follow this frame-work, while noting that directions such as design of user-interfaces [13], computing optimal task assignment [20] etc. can provide complementary beneﬁts.
Having a pool of workers that can be repeatably queried improves both skill estimation over time and reduces anno-tation noise typically found in crowdsourcing, where work-ers perform micro-tasks and their presence is ﬂeeting. Thus, in this work we choose to focus on a ﬁxed worker pool.2
We ﬁrst investigate integrating advances in self-supervised learning in our setting. Next, we view online labeling as a semi-supervised problem and show conse-quent efﬁciency gains. These additions can sometimes lead to negative feedback cycles, which we identify and rem-edy. Finally, to encourage adoption into a practitioner’s toolchain, we ablate several key design choices and pro-vide a set of good practices and guidelines. We avoid the expense of running large experiments with human workers by proposing a more realistic annotator simulation that in-volves collecting statistics from human annotators. Prior work [2, 41] collected a large number of human labels for all experiments, leading to 1) smaller individual experiment scale and 2) a barrier for further research since these labels are not available and expensive to collect. We note that [41] also look into efﬁcient multi-class annotation for large label sets, with a focus on efﬁcient factorization and learning of worker abilities. This is important and orthogonal to our ex-ploration into integration of learning methods. In summary, we make the following contributions:
• Explore the usage of advances in self-supervised learn-ing to efﬁcient annotation for multi-class classiﬁcation
• Propose to view the annotation process as a semi-supervised learning problem, identify resulting insta-bilities and provide remedies
• Ablate several key design choices for the annotation process providing a set of best practices and guidelines to facilitate adoption into a practitioner’s toolchain
• Provide a realistic annotator simulation to conduct such experiments at scale while avoiding the high cost of involving human annotators for every experiment
• Release a modular codebase to facilitate adoption and further research into efﬁcient human-in-the-loop multi-class labeling 2This is also a growing trend in the industry, with large companies using trained labelers over offering micro-tasks.
We experiment on subsets of varying difﬁculty from Im-ageNet [12]. We show 87% top-1 label accuracy on a 100 class subset of ImageNet, with only 0.98 annotations per image. 80% top-1 label accuracy needed 0.35 annotations per image, a 2.7x reduction with respect to prior work and a 6.7x reduction over manual annotation. On the small-scale experiment using human annotations, we achieve 91% label accuracy with 2x fewer annotations. 2.