Abstract
Semi-supervised learning on class-imbalanced data, al-though a realistic problem, has been under studied. While existing semi-supervised learning (SSL) methods are known to perform poorly on minority classes, we ﬁnd that they still generate high precision pseudo-labels on minority classes.
By exploiting this property, in this work, we propose Class-Rebalancing Self-Training (CReST), a simple yet effec-tive framework to improve existing SSL methods on class-imbalanced data. CReST iteratively retrains a baseline
SSL model with a labeled set expanded by adding pseudo-labeled samples from an unlabeled set, where pseudo-labeled samples from minority classes are selected more frequently according to an estimated class distribution. We also propose a progressive distribution alignment to adap-tively adjust the rebalancing strength dubbed CReST+. We show that CReST and CReST+ improve state-of-the-art SSL algorithms on various class-imbalanced datasets and con-sistently outperform other popular rebalancing methods.
Code has been made available at https://github. com/google-research/crest. 1.

Introduction
Semi-supervised learning (SSL) utilizes unlabeled data to improve model performance and has achieved promis-ing results on standard SSL image classiﬁcation bench-marks [34, 25, 43, 2, 39, 47]. A common assumption, which is often made implicitly during the construction of SSL benchmark datasets, is that the class distribution of labeled and/or unlabeled data are balanced. However, in many re-alistic scenarios, this assumption holds untrue and becomes the primary cause of poor SSL performance [5, 22].
Supervised learning on imbalanced data has been widely explored. It is commonly observed that models trained on imbalanced data are biased towards majority classes which have numerous examples, and away from minority classes which have few examples. Various solutions have been pro-posed to help alleviate bias, such as re-sampling [3, 4], re-∗Work done while an intern at Google. (a) (b) (c) (d)
Figure 1. Experimental results on CIFAR10-LT. (a) Both labeled and unlabeled sets are class-imbalanced, where the most major-ity class has 100× more samples than the most minority class.
The test set remains balanced. (b) Precision and recall of a Fix-Match [39] model. Although minority classes have low recall, they obtain high precision. (c) & (d) The proposed CReST and
CReST+ improve the quality of pseudo-labels (c) and thus the re-call on the balanced test set (d), especially on minority classes. weighting [9, 6], and two-stage training [20, 52]. All these methods rely on labels to re-balance the biased model.
In contrast, SSL on imbalanced data has been under-studied. In fact, data imbalance poses further challenges in
SSL where missing label information precludes rebalancing the unlabeled set. Pseudo-labels for unlabeled data gener-ated by a model trained on labeled data are commonly lever-aged in SSL algorithms. However, pseudo-labels can be problematic if they are generated by an initial model trained on imbalanced data and biased toward majority classes: subsequent training with such biased pseudo-labels intensi-ﬁes the bias and deteriorates the model quality. Apart from a few recent works [22, 48], the majority of existing SSL al-gorithms [2, 1, 46, 39] have not been thoroughly evaluated on imbalanced data distributions.
In this work, we investigate SSL in the context of class-110857
imbalanced data in which both labeled and unlabeled sets have roughly the same imbalanced class distributions, as il-lustrated in Fig. 1(a). We observe that the undesired per-formance of existing SSL algorithms on imbalanced data is mainly due to low recall on minority classes. Our method is motivated by the further observation that, despite this, pre-cision on minority classes is surprisingly high. In Fig. 1(b), we show predictions on a CIFAR10-LT dataset produced by FixMatch [39], a representative SSL algorithm with state-of-the-art performance on balanced benchmarks. The model obtains high recall on majority classes but suffers from low recall on minority classes, which results in low ac-curacy overall on the balanced test set. However, the model has almost perfect precision on minority classes, suggesting that the model is conservative in classifying samples into minority classes, but once it makes such a prediction we can be conﬁdent it is correct. Similar observations are made on other SSL methods, and on supervised learning [19].
With this in mind, we introduce a class-rebalancing self-training scheme (CReST) which re-trains a baseline SSL model after adaptively sampling pseudo-labeled data from the unlabeled set to supplement the original labeled set. We refer to each fully-trained baseline model as a generation.
After each generation, pseudo-labeled samples from the un-labeled set are added into the labeled set to retrain an SSL model. Rather than updating the labeled set with all pseudo-labeled samples, we instead use a stochastic update strategy in which samples are selected with higher probability if they are predicted as minority classes, as those are more likely to be correct predictions. The updating probability is a func-tion of the data distribution estimated from the labeled set.
In addition, we extend CReST to CReST+ by incorporating distribution alignment [1] with a temperature scaling factor to control its alignment strength over generations, so that predicted data distributions are more aggressively adjusted to alleviate model bias. As shown in Fig. 1(c) and 1(d), the proposed strategy reduces the bias of pseudo-labeling and improves the class-balanced test set accuracy as a result.
We show in experiments that CReST and CReST+ im-prove over baseline SSL methods by a large margin. On
CIFAR-LT [9, 6], our method outperforms FixMatch [39] under different imbalance ratios and label fractions by as much as 11.8% in accuracy. Our method also outperforms
DARP [22], a state-of-the-art SSL algorithm designed for learning from imbalanced data, on both MixMatch [2] and
FixMatch [39] by up to 4.0% in accuracy. To further test the efﬁcacy of the proposed method on large-scale data, we apply our method on ImageNet127 [17], a naturally im-balanced dataset created from ImageNet [11] by merging classes based on the semantic hierarchy, and get 7.9% gain on recall. Extensive ablation study further demonstrates that our method particularly helps improve recall on minority classes, making it a viable solution for imbalanced SSL. 2.