Abstract
Collaborative learning has gained great popularity due to its beneﬁt of data privacy protection: participants can jointly train a Deep Learning model without sharing their training sets. However, recent works discovered that an ad-versary can fully recover the sensitive training samples from the shared gradients. Such reconstruction attacks pose se-vere threats to collaborative learning. Hence, effective mit-igation solutions are urgently desired.
In this paper, we propose to leverage data augmentation to defeat reconstruction attacks: by preprocessing sensi-tive images with carefully-selected transformation policies, it becomes infeasible for the adversary to extract any use-ful information from the corresponding gradients. We de-sign a novel search method to automatically discover qual-iﬁed policies. We adopt two new metrics to quantify the impacts of transformations on data privacy and model us-ability, which can signiﬁcantly accelerate the search speed.
Comprehensive evaluations demonstrate that the policies discovered by our method can defeat existing reconstruc-tion attacks in collaborative learning, with high efﬁciency and negligible impact on the model performance. 1.

Introduction
A collaborative learning system enables multiple partic-ipants to jointly train a shared Deep Learning (DL) model for a common artiﬁcial intelligence task [36, 22, 9]. Typ-ical collaborative systems are distributed systems such as federated learning systems, where each participant itera-tively calculates the local gradients based on his own train-ing dataset and shares them with other participants to ap-proach the ideal model. This collaborative mode can signif-icantly improve the training speed, model performance and generalization. Besides, it can also protect the training data privacy, as participants do not need to release their sensitive
*Corresponding author. data during the training phase. Due to these advantages, collaborative learning has become promising in many sce-narios, e.g., smart manufacturing [10], autonomous driving
[25], digital health [3], etc.
Although each participant does not disclose the training dataset, he has to share with others the gradients, which can leak information of the sensitive data indirectly. Past works [14, 22, 23] demonstrated the possibility of member-ship inference and property inference attacks in collabora-tive learning. A more serious threat is the reconstruction attack [40, 38, 6], where an adversary can recover the exact values of samples from the shared gradients with high ﬁ-delity. This attack is very practical under realistic and com-plex circumstances (e.g., large-size images, batch training).
Due to the severity of this threat, an effective and practi-cal defense solution is desirable to protect the privacy of collaborative learning. Common privacy-aware solutions
[40, 33] attempt to increase the difﬁculty of input recon-struction by obfuscating the gradients. However, the obfus-cation magnitude is bounded by the performance require-ment of the DL task: a large-scale obfuscation can hide the input information, but also impair the model accuracy.
The effectiveness of various techniques (e.g., noise injec-tion, model pruning) against reconstruction attacks have been empirically evaluated [40]. Unfortunately, they can-not achieve a satisfactory trade-off between data privacy and model usability, and hence become less practical.
Motivated by the limitations of existing solutions, this paper aims to solve the privacy issue from a different per-spective: obfuscating the training data to make the recon-struction difﬁcult or infeasible. The key insight of our strat-egy is to repurpose data augmentation techniques for pri-vacy enhancement. A variety of transformation approaches have been designed to improve the model performance and generalization. We aim to leverage certain transformation functions to preprocess the training sets and then train the gradients, which can prevent malicious participants from re-constructing the transformed or original samples.
Mitigating reconstruction attacks via data augmentation 114
is challenging. First, existing image transformation func-tions are mainly used for performance and generalization improvement.
It is unknown which ones are effective in reducing information leakage. Second, conventional ap-proaches apply these transformations to augment the train-ing sets, where original data samples are still kept for model training. This is relatively easier to maintain the model per-formance. In contrast, to achieve our goal, we have to aban-don the original samples, and only use the transformed ones for training, which can impair the model accuracy.
We introduce a systematic approach to overcome these challenges. Our goal is to automatically discover an en-semble of effective transformations from a large collection of commonly-used data augmentation functions. This en-semble is then formed as a transformation policy, to pre-serve the privacy of collaborative learning. Due to the large search space and training overhead, it is computationally in-feasible to evaluate the privacy and performance impacts of each possible policy. Instead, we design two novel metrics to quantify the policies without training a complete model.
These metrics with our new search algorithm can identify the optimal policies within 2.5 GPU hours.
The identiﬁed transformation policies exhibit great ca-pability of preserving the privacy while maintaining the model performance. They also enjoy the following prop-erties: (1) the policies are general and able to defeat differ-ent variants of reconstruction attacks. (2) The input trans-formations are performed locally without modifying the training pipeline. They are applicable to any collaborative learning systems and algorithms. (3) The transformations are lightweight with negligible impact on the training efﬁ-ciency. (4) The policies have high transferability: the opti-mal policy searched from one dataset can be directly applied to other datasets as well. 2.