Abstract
A layout to image (L2I) generation model aims to gen-erate a complicated image containing multiple objects (things) against natural background (stuff), conditioned on a given layout. Built upon the recent advances in gen-erative adversarial networks (GANs), existing L2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) the object-to-object as well as object-to-stuff relations are often broken and (2) each object’s appearance is typically distorted lacking the key deﬁning characteristics associated with the object class. We argue that these are caused by the lack of context-aware object and stuff feature encod-ing in their generators, and location-sensitive appearance representation in their discriminators. To address these limitations, two new modules are proposed in this work.
First, a context-aware feature transformation module is in-troduced in the generator to ensure that the generated fea-ture encoding of either object or stuff is aware of other co-existing objects/stuff in the scene. Second, instead of feed-ing location-insensitive image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images to preserve location-sensitive information, resulting in much enhanced object appear-ance. Extensive experiments show that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks. Code available at: https://github.com/wtliao/layout2img. 1.

Introduction
Recent advances in generative adversarial networks (GANs) [11] have made it possible to generate photo-realistic images for a single object, e.g., faces, cars, cats
[4, 46, 20, 21]. However, generating complicated im-ages containing multiple objects (things) of different classes against natural backgrounds (stuff) still remains a challenge
[18, 3, 31, 30]. This is due to the large appearance variations
*Equal contribution
Figure 1. Illustration of the limitations of existing L2I models and how our model overcome them. From left to right: ground truth layout, images generated by the state-of-the-art LostGAN-v2 [39], and by our model with the layout as input.
In the middle and right column, regions with key differences in the generation qual-ity between LostGAN-v2 and our model are highlighted in dashed boxes. See text for more details. for objects of different classes, as well as the complicated relations between both object-to-object and object-to-stuff.
A generated object needs to be not only realistic on its own, but in harmony with surrounding objects and stuff.
Without any conditional input, the mode collapse [36, 6] problem is likely to be acute for GANs trained to gener-ate such complicated natural scenes. Consequently, various inputs have been introduced to provide some constraints on the image generation process. These include textual description of image content [31], scene graph represent-ing objects and their relationship [18], and semantic map providing pixel-level annotation [30]. This work focuses on the conditional image generation task using the layout
[48, 38, 40] that deﬁnes a set of bounding boxes with spec-iﬁed size, location and categories (see Fig. 1). Layout is a user-friendly input format on its own and can also be used as 15049
an intermediate input step of other tasks, e.g., scene graph and text to image generation [3, 15].
Since the seminal work [48] in 2019, the very recent layout to image (L2I) generation models [49, 40, 39] have made great progresses, thanks largely to the advances made in GANs [30, 20] as they are the key building blocks. From a distance, the generated images appear to be realistic and adhere to the input layout (see Fig. 1 and more in Fig. 3).
However, a closer inspection reveals two major limitations.
First, the relations between objects and object-to-stuff are often broken. This is evident from the food example in
Fig. 1 (Top-Middle) – the input layout clearly indicates that the four bowls are overlapping with each other. Using the state-of-the-art LostGAN-v2 [39], the occluded regions be-tween objects are poorly generated. Second, each gener-ated object’s appearance is typically distorted lacking class-deﬁning characteristics. For instance, the surﬁng example in Fig. 1 (Middle) and the giraffe example in Fig. 1 (Bot-tom) show that the object appearance has as if been touched by Picasso – one can still recognize the surﬁng person or giraffe, but key body parts are clearly misplaced.
We believe these limitations are caused by two major de-sign ﬂaws in existing L2I models in both their GAN gen-erators and discriminators. (1) Lack of context-aware mod-eling in the generator: Existing models generate the fea-ture for the object/stuff in each layout bounding box ﬁrst, and then feed the generated feature into a generator for im-age generation. However, the feature generation process for each object/stuff is completely independent of each other, therefore offering no chance for capturing the inter-object and object-to-stuff relations. (2) Lack of location-sensitive appearance representation in the discriminator: As in any
GAN model, existing L2I models deploy a discriminator that is trained to distinguish the generated whole image and individual object/stuff images from the real ones. Such a discriminator is essentially a CNN binary classiﬁer whereby globally pooled features extracted from the CNN are fed to a real-fake classiﬁer. The discriminator thus cares only about the presence/absence and strength of each semantic feature, rather than where they appear in the generated im-ages. This lack of location-sensitive appearance representa-tion thus contributes to the out-of-place object part problem in Fig. 1 (Middle).
In this paper, we provide solutions to overcome both lim-itations. First, to address the lack of context-aware mod-eling problem, we propose to introduce a context-aware feature transformation module in the generator of a L2I model. This module updates the generated feature for each object and stuff after each has examined its relations with all other objects/stuff co-existing in the image through self-attention. Second, instead of feeding location-insensitive globally pooled object image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images. The feature map Gram matrix captures the inter-feature correlations over the vectorized feature map, and is therefore locations sensitive. Adding it to the input of the real-fake classiﬁer in the discriminator, the generated images preserve both shape and texture char-acteristics of each object class, resulting in much enhanced object appearance (see Fig. 1 (Right)).
The contributions of this work are as follows: (1) For the ﬁrst time, we identify two major limitations of exist-ing L2I models for generating complicated multi-object im-ages. (2) Two novel components, namely a context-aware feature transformation module and a location-sensitive ob-ject appearance representation are introduced to address (3) The proposed modules can be these two limitations. easily integrated into any existing L2I generation mod-els and improve them signiﬁcantly. (4) Extensive exper-iments on both the COCO-Thing-Stuff [25, 5] and Vi-sual Genome [22] datasets show that state-of-the-art perfor-mance is achieved using our model. The code and trained models will be released soon. 2.