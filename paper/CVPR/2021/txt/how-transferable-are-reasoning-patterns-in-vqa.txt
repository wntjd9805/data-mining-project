Abstract
Appearance	shift	+	presence	shift
Since its inception, Visual Question Answering (VQA) is no-toriously known as a task, where models are prone to exploit biases in datasets to ﬁnd shortcuts instead of performing high-level reasoning. Classical methods address this by re-moving biases from training data, or adding branches to models to detect and remove biases. In this paper, we argue that uncertainty in vision is a dominating factor preventing the successful learning of reasoning in vision and language problems. We train a visual oracle and in a large scale study provide experimental evidence that it is much less prone to exploiting spurious dataset biases compared to standard models. We propose to study the attention mechanisms at work in the visual oracle and compare them with a SOTA
Transformer-based model. We provide an in-depth analy-sis and visualizations of reasoning patterns obtained with an online visualization tool which we make publicly avail-able1. We exploit these insights by transferring reasoning patterns from the oracle to a SOTA Transformer-based VQA model taking standard noisy visual inputs via ﬁne-tuning.
In experiments we report higher overall accuracy, as well as accuracy on infrequent answers for each question type, which provides evidence for improved generalization and a decrease of the dependency on dataset biases. 1.

Introduction
The high prediction performance obtained by high-capacity deep networks trained on large-scale data has led to ques-tions concerning the nature of these improvements. Vi-sual Question Answering (VQA) in particular has become a testbed for the evaluation of the reasoning and generaliza-tion capabilities of trained models, as it combines multiple modalities of heterogeneous nature (images and language) with open questions and large varieties. It has been shown,
*Both authors contributed equally. 1https://reasoningpatterns.github.io
GQA	data
GT	objects
GT	classes
GQA/Vis.	Gen./COCO/VQAv2
R-CNN	objects
R-CNN	embed.
GQA	data
R-CNN	objects
R-CNN	embed.
Classif.	loss
Classif.	loss
+BERT	losses
Classif.	loss
Training
Oracle  model
Fine-tuning
Model
Fine-tuning
Adapted
Model
Attention	 modes
Figure 1. We argue that noise and uncertainties in visual inputs are the main bottleneck in VQA preventing successful learning of reasoning capacities. In a deep analysis, we show that oracle models with perfect sight, trained on noiseless visual data, tend to depend signiﬁcantly less on bias exploitation. We exploit this by training models on data without visual noise, and then transfer the learned reasoning patterns to real data. We illustrate successful transfer by an analysis and visualization of attention modes. that current models are prone to exploiting harmful biases in the data, which can provide unwanted shortcuts to learning in the form of “Clever Hans” effects [36, 22].
In this work we study the capabilities of VQA models to
“reason”. An exact deﬁnition of this term is difﬁcult, we refer to [7, 22] and deﬁne it as “algebraically manipulating words and visual objects to answer a new question”. In par-ticular, we interpret reasoning as the opposite of exploiting spurious biases in training data. We argue, and in Section 3 will provide evidence for this, that learning to algebraically manipulate words and objects is difﬁcult when visual input is noisy and uncertain compared to learning from perfect in-formation about a scene. When objects are frequently miss-ing, detected multiple times or recognized with ambiguous visual embeddings wrongly overlapping with different cate-gories, relying on statistical shortcuts may be an easy short-14207
cut for the optimizer 2 We show, that a perfect-sighted ora-cle model learns to predict answers while signiﬁcantly less relying on biases in training data. We claim that once any noise has been removed from visual input, replacing object detection output by Ground Truth (GT) object annotations, a deep neural network can more easily learn the reasoning patterns required for prediction and for generalization.
In the line of recent work in AI explainability [28, 32], and data visualization [18, 38, 11], we propose an in-depth analysis of attention mechanisms in Transformer-based models and provide indications of the patterns of rea-soning employed by models of different strengths. We visu-alize different operating modes of attention and link them to different sub tasks (“functions”) required for solving VQA.
In particular, we use this analysis for a comparison between oracle models and standard models processing noisy and uncertain visual input, highlighting the presence of reason-ing patterns in the former and less so in the latter.
Drawing conclusions from this analysis, we propose to
ﬁne-tune the perfectly-sighted oracle model on the real noisy visual input (see Fig. 1). Using the same analysis and visualization techniques, we show that attention modes absent from noisy models are transferred successfully from oracle models to deployable 3 models, and we report im-provements in overall accuracy and generalization.
Contributions — (i) An in-depth analysis of reasoning patterns at work in Transformer-based models, comparing
Oracles vs. deployable models, including visualization of attention modes; an analysis of the relationships between attention modes and reasoning, and the impact of attention pruning on reasoning. (ii) We propose to transfer reasoning capabilities, learned by the oracle, to SOTA VQA methods with noisy input and improve overall performance and gen-eralization on the GQA [19] dataset (iii) We show that this transfer is complementary with self-supervised large-scale pre-training (LXMERT [35]/BERT-like). 2.