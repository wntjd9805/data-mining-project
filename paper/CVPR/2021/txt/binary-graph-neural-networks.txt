Abstract
Graph Neural Networks (GNNs) have emerged as a pow-erful and ﬂexible framework for representation learning on irregular data. As they generalize the operations of clas-sical CNNs on grids to arbitrary topologies, GNNs also bring much of the implementation challenges of their Eu-clidean counterparts. Model size, memory footprint, and energy consumption are common concerns for many real-world applications. Network binarization allocates a sin-gle bit to parameters and activations, thus dramatically re-ducing the memory requirements (up to 32x compared to single-precision ﬂoating-point numbers) and maximizing the beneﬁts of fast SIMD instructions on modern hardware for measurable speedups. However, in spite of the large body of work on binarization for classical CNNs, this area re-mains largely unexplored in geometric deep learning. In this paper, we present and evaluate different strategies for the binarization of graph neural networks. We show that through careful design of the models, and control of the training pro-cess, binary graph neural networks can be trained at only a moderate cost in accuracy on challenging benchmarks. In particular, we present the ﬁrst dynamic graph neural net-work in Hamming space, able to leverage efﬁcient k-NN search on binary vectors to speed-up the construction of the dynamic graph. We further verify that the binary models offer signiﬁcant savings on embedded devices. Our code is publicly available on Github1. 1.

Introduction
Standard CNNs assume their input to have a regular grid structure, and are therefore suitable for data that can be well-represented in an Euclidean space, such as images, sound, or videos. However, many increasingly relevant types of data do not ﬁt this framework [5]. Graph theory offers a broad mathematical formalism for modeling interactions, and is therefore commonly used in ﬁelds such as network sciences
[12], bioinformatics [24, 40], and recommender systems 1https://github.com/mbahri/binary gnn
Figure 1. Top: Test accuracy of different binarization schemes at all stages of our cascaded distillation protocol (baseline: 92.89%).
Bottom: The ”BF2” variant of our XorEdgeConv operator.
[37], as well as for studying discretisations of continuous mathematical structures such as in computer graphics [4].
This motivates the development of machine learning meth-ods able to deal with graph-supported data. Among them,
Graph Neural Networks (GNNs) generalize the operations of
CNNs to arbitrary topologies by extending the basic building blocks of CNNs such as convolutions and pooling to graphs.
Similarly to CNNs, GNNs learn deep representations of graphs or graph elements, and have emerged as the best per-forming models for learning on graphs as well as on 3D meshes with the development of advanced and increasingly deep architectures [33, 18].
As the computational complexity of the networks and the scale of graph datasets increase, so does the need for faster and smaller models. The motivations for resource-efﬁcient deep learning are numerous and also apply to deep learning on graphs and 3D shapes. Computer vision models are rou-tinely deployed on embedded devices, such as mobile phones or satellites [2, 32], where energy and storage constraints are important. The development of smart devices and IoT may bring about the need for power-efﬁcient graph learning models [27, 61, 9]. Finally, models that require GPUs for inference can be expensive to serve, whereas CPUs are typi-cally more affordable. This latter point is especially relevant 9492
to the applications of GNNs in large-scale data mining on relational datasets, such as those produced by popular social networks, or in bioinformatics [35].
While recent work has proposed algorithmic changes to make graph neural networks more scalable, such as the use of sampling [21, 60] or architectural improvements [15, 10] and simpliﬁcations [56], our approach is orthogonal to these advances and focuses on compressing existing architectures while preserving model performance. Model compression is a well-researched area for Euclidean neural networks, but has seen very little application in geometric deep learning. In this paper, we study different strategies for binarizing GNNs.
Our contributions are as follows:
• We present a binarization strategy inspired by the latest developments in binary neural networks for images
[7, 36] and knowledge distillation for graph networks
• We develop an efﬁcient dynamic graph neural network model that constructs the dynamic graph in Hamming space, thus paving the way for signiﬁcant speedups at inference time, with negligible loss of accuracy when using real-valued weights
• We conduct a thorough ablation study of the hyperpa-rameters and techniques used in our approach
• We demonstrate real-world acceleration of our models on a budget ARM device
Notations Matrices and vectors are denoted by upper and lowercase bold letters (e.g., X and x), respectively. I de-notes the identity matrix of compatible dimensions. The ith column of X is denoted as xi. The set of real numbers is denoted by R. A graph G = (V, E) consists of vertices
V = {1, . . . , n} and edges E ⊆ V × V. The neighborhood of vertex i, denoted by N (i) = {j : (i, j) ∈ E}, is the set of vertices adjacent to i. Other mathematical notations are summarized in Table 5 of the Supplementary Material. 2.