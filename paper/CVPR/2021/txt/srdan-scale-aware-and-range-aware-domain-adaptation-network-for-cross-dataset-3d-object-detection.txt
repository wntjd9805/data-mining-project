Abstract
Geometric characteristic plays an important role in the representation of an object in 3D point clouds. For example, large objects often contain more points, while small ones contain fewer points. The points from objects near the cap-ture device are denser, while those from far-range objects are sparser. These issues bring new challenges to 3D object detection, especially under the domain adaptation scenar-ios. In this work, we propose a new cross-dataset 3D ob-ject detection method named Scale-aware and Range-aware
Domain Adaptation Network (SRDAN). We take advantage of the geometric characteristics of 3D data (i.e., size and distance), and propose the scale-aware domain alignment and the range-aware domain alignment strategies to guide the distribution alignment between two domains. For scale-aware domain alignment, we design a 3D voxel-based fea-ture pyramid network to extract multi-scale semantic voxel features, and align the features and instances with simi-lar scales between two domains. For range-aware domain alignment, we introduce a range-guided domain alignment module to align the features of objects according to their distance to the capture device. Extensive experiments un-der three different scenarios demonstrate the effectiveness of our SRDAN approach, and comprehensive ablation study also validates the importance of geometric characteristics for cross-dataset 3D object detection. 1.

Introduction
With the advance of autonomous driving, increasing at-tention has been attracted to 3D object detection [63, 56, 8, 27, 69, 78, 68, 29, 43, 30, 54, 71, 42, 53]. Although signiﬁcant progress has been made, most of these works considered only the constrained setting, where the train-ing data used to learn the detection model and the test data for performance evaluation are collected from a sim-ilar scenario. However, in practical applications, we of-ten face the more challenging situation, where the train-ing data and the test data captured by various devices at different time/places often exhibit considerable distri-bution mismatch. This could lead to signiﬁcant perfor-mance drop when deploying the trained model in a new sce-(b) LiDAR Point Cloud
Large
Small
Far
Near (a) BEV Image (c) Scale Awareness (d) Range Awareness
Figure 1. Motivation of the Scale-aware and Range-aware Domain
Alignment strategies in our SRDAN method. (a) A Bird’s-Eye-View (BEV) image is presented for better illustration. (b) A point cloud map collected by the LiDAR sensor (i.e., the car with gray box in (a)) is presented to illustrate the 3D geometric characteris-tics. (c) At similar distances to the LiDAR sensor, large objects (e.g., the car with orange box) often contain more points, while small ones (e.g., the cyclist with purple box) contain fewer points. (d) The 3D points of objects (e.g., the car with blue box) near the
LiDAR sensor are denser, while those of far-range ones (e.g., the car with red box) are sparser. We incorporate these geometric characteristics (i.e., size and distance) into our method, and learn domain-invariant representation to ensure objects with similar ge-ometric characteristics are well aligned between two domains. nario, which is also known as the domain adaptation prob-lem [19, 35, 36, 37, 13, 61, 74, 58, 3, 33, 25].
While many works have been proposed to address the domain adaptation problem for 2D images, little attention was paid to the cross-dataset object detection task for 3D point clouds. Different from the traditional cross-dataset object detection task for 2D images [9, 49, 26, 80, 66], the texture information is absent in 3D point cloud data, which makes it more challenging to deal with the data distribu-tion mismatch issue. Consequently, the intrinsic geometric information becomes especially important when addressing the domain adaptation issue for the 3D object detection task.
However, it is non-trivial to capture intrinsic geometric information. As shown in Fig. 1, the point cloud of an ob-ject can vary signiﬁcantly when the geometric characteris-6769
tics (e.g., the object scale and the distance of an object to the capture device like LiDAR) changes. The large-scale ob-jects often contain more points, while the small-scale ones contain fewer points. The point clouds of objects near the capture device are denser, while those of far-range objects are sparser. Such geometric variance brings challenges to the 3D object detection task, especially in the domain adap-tation scenarios where objects from the source and target domains with different geometric characteristics may be in-correctly aligned, leading to poor object detection results.
In this work, we propose a new cross-dataset 3D ob-ject detection method named Scale-aware and Range-aware
Domain Adaptation Network (SRDAN). To deal with the issues related to geometric variance when performing dis-tribution alignment, we propose to learn domain-invariant representation by incorporating the object geometric char-acteristic into deep neural networks. Ideally, with a good domain-invariant representation, the objects with similar geometric characteristics (e.g., size and distance) should be well aligned between two domains. In other words, the large objects (resp., the small objects) from the source domain should be aligned to the large ones (resp., the small ones) from the target domain. The same criterion should also be applied to the objects in a similar range around the sensor.
To achieve this goal, in our SRDAN, we propose the scale-aware domain alignment and the range-aware do-main alignment approach to guide distribution alignment between the two domains. For scale-aware domain align-ment, we design a 3D voxel-based feature pyramid network, which can extract multiple feature maps at different scales for detecting the objects with different sizes. Then, we align the features and instances with similar scales between the two domains at each feature map. For range-aware domain alignment, we take advantage of range information in 3D data and further enhance domain alignment at both anchor-level and feature-level. For anchor-level alignment, we in-troduce a range-guided domain alignment (RLA) module to align the local features of objects with different sparsity lev-els. For feature-level alignment, we introduce a Location-related Global Alignment (LGA) module to learn a global weight map for ﬁner alignment, which can guide our model to focus more on aligning the foreground objects. on four datasets
Extensive experiments (i.e.,
Nuscenes [4], A*3D [39], PreSIL [22] and KITTI [14]) under three autonomous driving scenarios (i.e., cross-scene adaptation, day-to-night adaptation and synthetic-to-real adaptation) clearly demonstrate the effectiveness of our approach for the cross-dataset 3D object detection task. 2.