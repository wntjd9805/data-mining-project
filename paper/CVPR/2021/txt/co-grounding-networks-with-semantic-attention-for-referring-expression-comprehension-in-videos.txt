Abstract
In this paper, we address the problem of referring expres-sion comprehension in videos, which is challenging due to complex expression and scene dynamics. Unlike previous methods which solve the problem in multiple stages (i.e., tracking, proposal-based matching), we tackle the problem from a novel perspective, co-grounding, with an elegant one-stage framework. We enhance the single-frame ground-ing accuracy by semantic attention learning and improve the cross-frame grounding consistency with co-grounding feature learning.
Semantic attention learning explicitly parses referring cues in different attributes to reduce the ambiguity in the complex expression. Co-grounding feature learning boosts visual feature representations by integrat-ing temporal correlation to reduce the ambiguity caused by scene dynamics. Experiment results demonstrate the superi-ority of our framework on the video grounding datasets VID and LiOTB in generating accurate and stable results across frames. Our model is also applicable to referring expres-sion comprehension in images, illustrated by the improved performance on the RefCOCO dataset. Our project is available at https://sijiesong.github.io/co-grounding. 1.

Introduction
Referring expression comprehension has attracted much
It aims to localize a region of the im-attention recently. age/video described by the natural language. This top-ic is of great importance in computer vision to support a variety of research problems such as image/video caption-ing [2, 27], visual question answering [3] and image/video retrieval [31, 9].
It also plays a key role in machine in-telligence for a wide range of applications from human-computer interaction, robotics to early education.
In the past years, most of the previous work for referring expression comprehension focus on the grounding for static
∗Corresponding author.
Figure 1. Referring expression comprehension in videos. Due to dynamic scenes and ambiguity in the expression, per-frame infer-ence (in blue) with state-of-the-art grounding method [36] would lead to unstable results across frames, while our co-grounding networks achieve accurate and consistent predictions (in red).
Ground-truth annotations are denoted in green. images [32, 34, 38, 18, 36, 23, 24, 12, 1] and have achieved promising results. However, referring expression compre-hension for videos is less explored, which is challenging yet important. Different from several threads of referring expression comprehension in videos, such as referring al-l mentioned entities [42], we localize the spatio-temporal tube that semantically corresponds to the whole sentence.
That is, we output bounding box for each frame as shown in Figure 1.
The work in [17] treats referring expression comprehen-sion for videos as a tracking problem. We argue that it would suffer from template selection error, because it is hard to tell from the grounding results from multiple frames which is the right one to track. The other work [6] ﬁrst proposes spatio-temporal tube candidates and then matches them with textual features from expression. However, the performance is limited by the proposal quality. Inspired by the advance in one-stage image grounding methods [36] that get rid of proposal detectors, another solution is to conduct per-frame inference with [36], but there are still two prob-lems. Firstly, the entities in the expression (such as ‘boat’,
‘men’, ‘whale’, ‘sea’ in Figure 1) would cause ambiguity 1346
when encoded into textual features, making the model con-fused about which entity is the correct one to ground. Sec-ondly, the dynamic scenes across frames would also inter-rupt the grounding process (see the drifting blue bounding boxes in Figure 1). Therefore, the key challenge is to gen-erate robust textual and visual features to reduce ambigu-ity, then further achieve accurate and stable results across frames.
To tackle the aforementioned issues, we propose to solve referring expression comprehension in videos with a new perspective, i.e., co-grounding, with semantic atten-tion learning in an elegant one-stage framework. The ba-sic structure of our model is based on YOLO [28], which predicts the bounding box and conﬁdence simultaneously.
The conﬁdence reﬂects the matching score between the tex-tual and visual features. We design a semantic attention mechanism to obtain attribute-speciﬁc features both for vi-sion and language. Speciﬁcally, a proposal-free subject at-tention scheme is proposed to parse the words for subjec-t from the expression. An object-aware location attention scheme is developed to parse the words for location from the expression. The interaction between attribute-speciﬁc textual and visual features determines the subject score and location score for each visual region (see the visualization examples in Figure 6). Besides, to improve cross-frame prediction consistency, we develop the co-grounding fea-ture learning. Taking multiple frames as input, it utilizes the correlation across frames to enhance visual features and stabilize the grounding process in training and testing. A post-processing strategy is further employed to improve the temporal consistency during inference.
Our contributions are summarized as follows:
• We propose to solve referring expression comprehen-sion in videos by co-grounding in an one-stage framework.
• We propose semantic attention learning to parse refer-ring cues, including a proposal-free subject attention and object-aware location attention.
• Our networks are applicable to both video/image grounding, and achieve state-of-the-art performance on re-ferring expression comprehension benchmarks. 2.