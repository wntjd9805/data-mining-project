Abstract
Humans perceive and construct the surrounding world as an arrangement of simple parametric models. In partic-ular, man-made environments commonly consist of volumet-ric primitives such as cuboids or cylinders. Inferring these primitives is an important step to attain high-level, abstract scene descriptions. Previous approaches directly estimate shape parameters from a 2D or 3D input, and are only able to reproduce simple objects, yet unable to accurately parse more complex 3D scenes. In contrast, we propose a robust estimator for primitive ﬁtting, which can meaningfully ab-stract real-world environments using cuboids. A RANSAC estimator guided by a neural network ﬁts these primitives to 3D features, such as a depth map. We condition the net-work on previously detected parts of the scene, thus parsing it one-by-one. To obtain 3D features from a single RGB im-age, we additionally optimise a feature extraction CNN in an end-to-end manner. However, naively minimising point-to-primitive distances leads to large or spurious cuboids occluding parts of the scene behind. We thus propose an occlusion-aware distance metric correctly handling opaque scenes. The proposed algorithm does not require labour-intensive labels, such as cuboid annotations, for training.
Results on the challenging NYU Depth v2 dataset demon-strate that the proposed algorithm successfully abstracts cluttered real-world 3D scene layouts. 1.

Introduction
Humans tend to create using simple geometric forms.
For instance, a house is made from bricks and squared timber, and a book is a cuboidal assembly of rectangles.
Consequently, it appears that humans also visually abstract environments by decomposing them into arrangements of cuboids, cylinders, ellipsoids and other simple volumetric primitives [5]. Such an abstraction of the 3D world is also very useful for machines with visual perception. Scene rep-resentation based on geometric shape primitives has been an active topic since the very beginning of Computer Vi-sion. In 1963, Blocks World [50] from Larry Roberts was one of the earliest approaches for qualitative 3D shape re-covery from 2D images using generic shape primitives. In (a) Input images (b) Recovered superquadrics [45] (c) Recovered cuboids (ours)
Figure 1: Primitive-based Scene Abstractions: We parse images of real-world scenes (a) in order to generate abstrac-tions of their 3D structure using cuboids (c). Our method is capable of capturing scene structure more accurately than previous work [45] based on superquadrics (b). recent years, with rapid advances in the ﬁeld of deep learn-ing, high-quality 3D reconstruction from single images has become feasible. Most approaches recover 3D informa-tion such as depth [13] and meshes [56] from RGB im-ages. Fewer works consider more parsimonious 3D shape descriptions such as cuboids [54] or superquadrics [45, 44].
These 3D shape parsers work well for isolated objects, but do not generalise to complex real-world scenes (cf. Fig. 1).
Robust model ﬁtting algorithms such as RANSAC [14] and its many derivatives [2, 9, 47] have been used to ﬁt 13070
low-dimensional parametric models, such as plane homo-graphies, fundamental matrices or geometric primitives, to real-world noisy data. Trainable variants of RANSAC [6, 7, 28] use a neural network to predict sampling weights from data. They require fewer samples and are more accurate.
Leveraging advances in the ﬁelds of single image 3D re-construction and robust multi-model ﬁtting, we present a novel approach for robustly parsing real-world scenes using 3D shape primitives, such as cuboids. A trainable RANSAC estimator ﬁts these primitives to 3D features, such as a depth map. We build upon the estimator proposed in [28], and ex-tend it by predicting multiple sets of RANSAC sampling weights concurrently. This enables our method to distin-guish between different structures in a scene more easily.
We obtain 3D features from a single RGB image using a
CNN, and show how to optimise this CNN in an end-to-end manner. Our training objective is based on geometrical consistency with readily available 3D sensory data.
During primitive ﬁtting, a naive maximisation of inlier counts considering point-to-primitive distances causes the algorithm to detect few but excessively large models. We argue that this is due to parts of a primitive surface correctly representing some parts of a scene, while other parts of the same primitive wrongly occlude other parts of the scene.
Thus, points should not be assigned to primitive surfaces which cannot be seen by the camera due to occlusion or self-occlusion. We therefore propose an occlusion-aware distance and a corresponding occlusion-aware inlier count.
As no closed-form solution exists to calculate cuboid pa-rameters, we infer them by numerical optimisation. How-ever, backpropagation through this optimisation is numeri-cally unstable and computationally costly. We therefore an-alytically derive the gradient of primitive parameters w.r.t. the features used to compute them. Our gradient compu-tation allows for end-to-end training without backpropaga-tion through the minimal solver itself. We demonstrate the efﬁcacy of our method on the challenging real-world NYU
Depth v2 dataset [52].
In summary, our contributions1 are as follows:
• A 3D scene parser which can process more complex real-word scenes than previous works on 3D scene ab-straction.
• An occlusion-aware distance metric for opaque scenes.
• Analytical derivation of the gradient of cuboids w.r.t. input features, in order to circumvent infeasible back-propagation through our minimal solver, thus enabling end-to-end training.
• Our method does not require labour-intensive labels, such as cuboid or object annotations, and can be trained on readily available sensory data instead. 1Source code is available at: https://github.com/fkluger/ cuboids_revisited 2.