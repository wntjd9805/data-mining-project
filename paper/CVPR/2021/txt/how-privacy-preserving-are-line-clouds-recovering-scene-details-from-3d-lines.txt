Abstract
Visual localization is the problem of estimating the cam-era pose of a given image with respect to a known scene.
Visual localization algorithms are a fundamental building block in advanced computer vision applications, including
Mixed and Virtual Reality systems. Many algorithms used in practice represent the scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D matches between a query image and the 3D points for camera pose estima-tion. As recently shown, image details can be accurately recovered from SfM point clouds by translating renderings of the sparse point clouds to images. To address the re-sulting potential privacy risks for user-generated content, it was recently proposed to lift point clouds to line clouds by replacing 3D points by randomly oriented 3D lines passing through these points. The resulting representation is un-intelligible to humans and effectively prevents point cloud-to-image translation. This paper shows that a signiﬁcant amount of information about the 3D scene geometry is pre-served in these line clouds, allowing us to (approximately) recover the 3D point positions and thus to (approximately) recover image content. Our approach is based on the obser-vation that the closest points between lines can yield a good approximation to the original 3D points. Code is available at https://github.com/kunalchelani/Line2Point. 1.

Introduction
Visual localization is the problem of estimating the po-sition and orientation from which an image was taken in a known scene. Visual localization is a fundamental part of computer vision systems such as self-driving cars [29, 66], Augmented and Mixed Reality applications [2, 12],
Structure-from-Motion (SfM) [28, 31, 65, 69], and Simul-taneous Localization and Mapping (SLAM) [18, 51, 57].
Classical approaches to visual localization [16, 30, 40– 42, 56, 59, 62, 86, 87] are based on local features such as
SIFT [43]. They use SfM to construct a sparse 3D point cloud of the scene, where each point is associated with the local image features it was triangulated from. Descriptor matching between local features extracted in a test / query image and the 3D points then yields a set of 2D-3D matches that can be used for RANSAC-based camera pose estima-tion [9, 17, 23, 36–39].
Traditionally, work on visual localization has focused on accurate and scalable algorithms able to cover large ar-eas [15, 30, 40, 58, 74, 76, 86] or to run in real-time on mo-bile devices with limited memory and compute capabili-ties [2,42,46,47,50]. Thus, the underlying scene representa-tions have been designed to enable efﬁcient 2D-3D match-ing [30, 47, 58, 61, 63] and / or to limit memory consump-tion [10, 11, 41, 46, 47, 61]. Privacy aspects such as avoiding user generated content from being recovered either through 3D models stored in the cloud or through query images sent to a server have traditionally not been taken into account.
Recently, [55, 70] showed that it is possible to recover images from SfM point clouds. Given a rendering of the point cloud (and the feature descriptors associated with the 3D points), [55] uses a CNN to translate the rendering to a complete image. Their work clearly demonstrates that stor-ing SfM point clouds creates potential privacy risks as an attacker could recover details from user-uploaded content stored in the cloud. To prevent such attacks, [71] proposed to replace each SfM point through a random line passing through this point (cf . Fig. 1(left)). They showed that the re-sulting representation is unintelligible to humans, prevents a direct application of [55], and still enables accurate cam-era pose estimation. This idea of lifting points to lines was later adapted for privacy-perserving SLAM [67]. However, this paper shows that it is possible to (approximately) re-cover the original 3D point positions from a line cloud (cf .
Fig. 1(middle)), again enabling us to use [55, 70] to obtain images (cf . Fig. 1(right)).
In detail, this paper makes the following contributions: (i) in the case that the line directions are chosen uniformly at random, as is the case in [71], we show that knowledge about local neighborhoods allows us to (approximately) re-cover the original 3D points based on the closest points be-tween pairs of lines. (ii) based on this insight, we propose a two-stage approach that ﬁrst recovers these neighborhoods and then estimates the 3D points corresponding to the input lines. (iii) detailed experiments on both indoor and outdoor 15668
Figure 1. In order to preserve privacy, [71] proposed to store line clouds instead of point clouds for visual localization (left). While unintelligible to the human eye, we show that it is possible to recover the underlying 3D point clouds (middle). Applying a point cloud-to-image translation approach [55] then allows us to recover image details (right), showing that lifting point clouds to line clouds can still preserve privacy critical information that can later be extracted from the line clouds. datasets show that our approach allows us to faithfully re-produce the original point clouds. In addition, applying [55] on the resulting point clouds enables us to recover image details (cf . Fig. 1(right)). (iv) while using line clouds alone is not effective in obfuscating the underlying 3D scene ge-ometry, we show that using (very) sparse line clouds effec-tively prevents our approach from recovering image details.
Our results clearly show that further research on privacy-preserving scene representations is needed. 2.