Abstract
The existence of noisy labels in real-world data nega-tively impacts the performance of deep learning models. Al-though much research effort has been devoted to improving robustness to noisy labels in classiﬁcation tasks, the prob-lem of noisy labels in deep metric learning (DML) remains open. In this paper, we propose a noise-resistant training technique for DML, which we name Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM identiﬁes noisy data in a minibatch using average similarity against image features extracted by several previous ver-sions of the neural network. These features are stored in and retrieved from a memory bank. To alleviate the high computational cost brought by the memory bank, we intro-duce an acceleration method that replaces individual data points with the class centers. In extensive comparisons with 12 existing approaches under both synthetic and real-world label noise, PRISM demonstrates superior performance of up to 6.06% in Precision@1. 1.

Introduction
Commonly resulting from human annotation errors or imperfect automated data collection, noisy labels in training data degrade the predictive performance of models trained on them [11, 47, 16]. Manual inspection and correction of labels are labour-intensive and hence scale poorly to large datasets. Therefore, training techniques that are robust to incorrect labels in training data play an important role in real-world applications of machine learning.
To date, most works on noise-resistant neural networks
[11, 17, 31, 52, 47, 48, 16] focus on image classiﬁcation.
Little research effort has been devoted to noise-resistant deep metric learning (DML). The goal of DML is to learn a distance metric that maps similar pairs of data points close together and dissimilar pairs far apart, based on a predeﬁned notion for similarity. DML ﬁnds diverse applications such as image retrieval [18, 10, 33], landmark identiﬁcation [49], and self-supervised learning [25].
Pair-based loss functions encourages DML networks to distinguish a similar pair of data points from one or more dissimilar pairs. Large batch sizes often lead to improved performance [6, 46, 5], as larger batches are more likely to contain informative examples. Pushing the idea of large batches to an extreme, [46] collects all positive and negative data samples from a memory bank. However, in the pres-ence of substantial noise, indiscriminate use of all samples could lower performance. Alternatively, [26] uses learnable class centers to replace individual data samples in order to reduce computational complexity. Nonetheless, the cluster centers can also be sensitive to outliers and label noise.
We propose a noise-resistant deep metric learning algo-rithm, Probabilistic Ranking-based Instance Selection with
Memory (PRISM), which works with both the memory bank approach and the class-center approach. PRISM computes the probability that a label is clean based on the similarities between the data point and other data points using features extracted during the last several training iterations. This may be seen as modeling the posterior probability of the data label. For data points with high probability, we ex-tract their features and insert them into the memory bank, which is used in subsequent model updates. In addition, we develop a smooth top-R (sTRM) trick to adjust the thresh-old for noisy data identiﬁcation as well as an acceleration technique that replaces individual data points with the class centers in the probability calculation.
We perform extensive empirical evaluations on both syn-thetic and real datasets. Inspired by the the “noise cluster” phenomenon observed from real-world data, we introduce the Small Cluster noise model to mimic open-set noise in real data. Experimental results show that PRISM achieves superior performance compared to 12 existing DML and noise-resistant training techniques under symmetric noise,
Small Cluster noise, and real noise. In addition, the accel-6811
eration trick speeds up the algorithm by a factor of 6.9 on
SOP dataset. The code and data are available at https:
//github.com/alibaba-edu/Ranking-based-Instance-Selection. 2.