Abstract 1.

Introduction
We aim to infer 3D shape and pose of object from a single image and propose a learning-based approach that can train from unstructured image collections, supervised by only segmentation outputs from off-the-shelf recognition systems (i.e. ‘shelf-supervised’). We ﬁrst infer a volumetric representation in a canonical frame, along with the cam-era pose. We enforce the representation geometrically con-sistent with both appearance and masks, and also that the synthesized novel views are indistinguishable from image collections. The coarse volumetric prediction is then con-verted to a mesh-based representation, which is further re-ﬁned in the predicted camera frame. These two steps allow both shape-pose factorization from image collections and per-instance reconstruction in ﬁner details. We examine the method on both synthetic and the real-world datasets and demonstrate its scalability on 50 categories in the wild, an order of magnitude more classes than existing works.
We live in a 3D world where 3D understanding plays a crucial role in our visual perception. Yet most computer vision systems in the wild still perform 2D semantic recog-nition (classiﬁcation/detection). Why is that? We believe the key reason is the lack of 3D supervision in the wild.
Most recent advances in 2D recognition have come from supervised learning but unlike 2D semantic tasks, obtaining supervision for 3D understanding is still not scalable.
While some recent approaches [10, 49] have attempted to build supervised 3D counterpart of 2D approaches, the concerns about scalability still remain.
Instead, a more promising direction is to learn models of single image 3D reconstruction by minimizing the amount of manual super-vision needed. Early approaches in this direction focused on using multi-view supervision [48, 53]. However, ob-taining multiple views of the same objects/scene is still not easy for the data in the wild. Therefore, recent ap-8843
proaches [17, 23, 33] have attempted to learn single-image 3D reconstruction models from image collections. These approaches have targeted use of category templates, pose supervision and keypoints to provide supervision (See Ta-ble 1). However, such supervision still limits the scalability to hundreds of categories.
Our work is inspired by recent approaches that forgo su-pervision by exploiting meta-supervision from the category structure and geometric nature of the task. More speciﬁ-cally, the two common supervisions used are: (a) render-ing supervision ([23, 26]): any given image of an instance in a category is merely a rendering of a 3D structure un-der a particular viewpoint. We can therefore enforce that the inferred 3D shape be consistent with the available im-age evidence when rendered; (b) adversarial supervision ([33]): in addition, the availability of an image collection also allows us to understand what renderings of 3D struc-tures should look like in general. This enables us to derive supervisory signal not just from renderings of predictions in the input view, but also from novel views, by encouraging the novel-view renderings to look realistic. Prior work has exploited these supervisions but individually they pose sev-eral limitations for scaling 3D reconstruction models. For example, [11, 23] still requires template models. Similarly,
[33] exploits the adversarial supervision and ignores the ex-plicit geometric supervision. Therefore, such an approach only works on categories with strong structure and curated image collections.
This paper attempts to build upon the very recent suc-cesses in meta-supervision and provide an approach to scale learning of single image 3D reconstruction in the wild. We present a two-step approach: the ﬁrst step re-lies on category-level understanding for coarse 3D inference (learned via meta-supervision). The second step specializes coarse models to match the details in the input image. Our approach can learn using only unannotated image collec-tions, without requiring any ground-truth 3D [3, 10, 46], multi-view [44, 53], category templates [11, 22], or pose supervision [18, 48]. This not only allows our approach to infer accurate 3D, but also enables it to do so beyond the synthetic settings, using in-the-wild image collections in a
‘shelf-supervised’ manner: with only approximate instance segmentation masks obtained using off-the-shelf recogni-tion systems as supervision. Yet our biggest contribution is the demonstration of scalability – we show results on order of magnitude more classes than existing papers. 2.