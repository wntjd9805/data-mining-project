Abstract
Deep models have shown their vulnerability when pro-cessing adversarial samples. As for the black-box attack, without access to the architecture and weights of the at-tacked model, training a substitute model for adversarial attacks has attracted wide attention. Previous substitute training approaches focus on stealing the knowledge of the target model based on real training data or synthetic data, without exploring what kind of data can further improve the transferability between the substitute and target mod-els. In this paper, we propose a novel perspective substitute training that focuses on designing the distribution of data used in the knowledge stealing process. More speciﬁcally, a diverse data generation module is proposed to synthe-size large-scale data with wide distribution. And adversar-ial substitute training strategy is introduced to focus on the data distributed near the decision boundary. The combina-tion of these two modules can further boost the consistency of the substitute model and target model, which greatly im-proves the effectiveness of adversarial attack. Extensive ex-periments demonstrate the efﬁcacy of our method against state-of-the-art competitors under non-target and target at-tack settings. Detailed visualization and analysis are also provided to help understand the advantage of our method. 1.

Introduction
Despite achieved impressive performance in most com-puter vision tasks, deep neural networks (DNNs) have been
*indicates equal contributions.
†indicates corresponding author.
‡Wenxuan Wang and Xiangyang Xue are with School of Computer Sci-ence, and Shanghai Key Lab of Intelligent Information Processing, Fudan
University. Email: {wxwang19, xyxue}@fudan.edu.cn.
§Li Zhang and Yanwei Fu are with School of Data Science, MOE
Frontiers Center for Brain Science, and Shanghai Key Lab of Intel-ligent Information Processing, Fudan University. Email: {yanweifu, lizhangfd}@fudan.edu.cn.
¶Bangjie Yin, Taiping Yao, Shouhong Ding, Jilin Li and Feiyue Huang are with Youtu Lab, Tencent. Email: {bangjieyin, taipingyao, ericshding, jerolinli, garyhuang}@tencent.com.
Figure 1. Differences between applying real data and synthetic data for substitute training. The ‘T’/‘S’ means the target/substitute model, the blue (+)/(-) in (b) indicates the adversarial examples, and the dashed green/red lines represent the decision boundary.
Comparing (a) and (b), synthetic data generated in our way can train a substitute model with a more similar decision boundary to the target model. Best viewed in color and zoomed in. shown to be vulnerable to even imperceptible adversarial noise/perturbations [28, 18]. The existence of adversar-ial examples reveals important security risks in deploying
DNNs to real-world applications. The community studies the adversarial attacks in the settings of white-box or black-box attack, by whether or not fully access to the target at-tack model. Practically, as the information of the full target model for white-box attack is unavailable to real-world de-ployment, this paper particularly focuses on the black-box attack, which normally produces the adversarial examples only replying on hard-labels or output scores of the target model. Typically, the black-box attack includes the score-based [3, 12, 11, 7] or decision-based methods [5, 1]. Nev-ertheless, it is required to make an avalanche of queries to the target model in such attacks, still potentially limiting their usability to attack DNNs in real situations.
Recently, the idea of substitute training has been exten-sively explored in the black-box attack [8, 26, 16, 23, 29].
Normally, rather than directly learning to synthesize adver-sarial examples, a substitute model is trained to make sim-ilar predictions as the target model, queried by the same 4761
input data. Within a certain amount of queries, this type of method is usually capable of learning a substitute model from the target model. Attack can thus be conducted on substitute model, and then transferable to the target model.
Fundamentally, substitute model tries to gain knowledge from the target model, by giving the input data and cor-responding queried labels. Critically, shall the input data come from the training data for the target model? By as-suming the ‘yes’ answer, it indeed simpliﬁes the substitute training. However, it is even non-trivial to collect real in-put data in many real-world vision tasks. For example, the data of person images and videos are under very strict con-trol, and the privacy of personal data has been well pro-tected by the laws in many countries. Moreover, are the real images the most effective data for substitute training? The training data of the target model indeed help to get a well-performing substitute model on original task, but it cannot guarantee the transferability of the attack from the substitute model to target model, which has been proved in Tab. 1 and
Tab. 2. For improving the attack performance in substitute training, it is necessary to minimize the decision boundary distance between the substitute and target models, which needs not only large-scale and diverse training data, but es-pecially the data distributed near the decision boundary.
To address the limitation of real data and explore a better distribution of substitute training data, we propose a novel task-driven uniﬁed framework, which only uses specially-designed generated data for substitute training and achieves high attack performance. As shown in Fig. 1, compared with using the training data of the target model to conduct substitute training, diverse synthetic data combined with ad-versarial examples will promote the substitute model to fur-ther approach the target. More speciﬁcally, in our frame-work, we ﬁrst propose a novel Diverse Data Generation module (DDG), which samples noise combined with label-embedded information to generate diverse training data.
Such distributed generated data can basically guarantee the substitute model to learn knowledge from the target. More-over, to further encourage the substitute model with simi-lar decision boundary as the target, Adversarial Substitute
Training strategy (AST) is proposed to introduce adversar-ial examples as boundary data into the training process.
Overall, the jointly learning of DDG and AST ensures the consistency between the substitute and target model, which greatly improves attack success rate in substitute training for black-box attack without any real data beforehand.
The main contributions of this work are summarized as, (1) We propose a novel effective generation-based substi-tute training paradigm to boost data-free black-box attack-ing performance, for the ﬁrst time, by delving into the essence of input generated substitute training data. (2) To achieve this goal, we ﬁrstly propose a diverse data gener-ation module with multiple diverse constraints to broaden the distribution of synthetic data. And then further improve the consistency of decision boundaries between substitute model and target model by adversarial substitute training strategy. (3) The comprehensive experiments and visualiza-tions over the four datasets and one online machine learn-ing platform demonstrate the effectiveness of our method against the state-of-the-art attacks. 2.