Abstract
The task of motion transfer between a source dancer and a target person is a special case of the pose transfer prob-lem, in which the target person changes their pose in ac-cordance with the motions of the dancer. In this work, we propose a novel method that can reanimate a single im-age by arbitrary video sequences, unseen during training.
The method combines three networks: (i) a segmentation-mapping network, (ii) a realistic frame-rendering network, and (iii) a face reﬁnement network. By separating this task into three stages, we are able to attain a novel sequence of realistic frames, capturing natural motion and appearance.
Our method obtains signiﬁcantly better visual quality than previous methods and is able to animate diverse body types and appearances, which are captured in challenging poses. 1.

Introduction
The goal of this work is to animate a target person, who is speciﬁed by a single input image, to mimic the motion of a driving person, who is captured in a video sequence. This pair of inputs can be considered the easiest to obtain, and most minimalist and generic input for the given synthesis problem. Importantly: both the input image and the driving video are unseen during training.
The method we propose extends the envelope of the cur-rent possibilities in multiple ways: (i) the target person can vary in body shape, age, ethnicity, gender, pose, and view-point (ii) the sequence of poses that form the motion can be unconstrained, which is why we emphasize freestyle dance, (iii) the background can vary arbitrarily and is not limited to the source image or the background of the driving video.
This general setting contrasts with the limitations of ex-isting methods, which often struggle to maintain the tar-get person’s appearance and avoid mixing elements from the driving video. The existing methods also often require an input video of the target person, have difﬁculty produc-ing natural motion, and are limited to speciﬁc backgrounds.
This is true, even for methods that train to map between speciﬁc persons seen during training.
To achieve this novel set of capabilities, we make ex-tensive use of the latest achievements of neural networks for human capturing. Two pre-trained pose recognition net-works are used to analyze the input video, a pre-trained hu-man parsing network is used to segment the input image (of 882
the target person), a pre-trained face embedding network is used to improve the face, and an inpainting network is uti-lized to extract the background of each training image. This maximal use of existing tools is an enabler for our method: using just one of the pose networks, or using pose in lieu of human parsing fails to deliver the desired results.
In addition to these components, for which there exist previous works that include a subset of it, we further employ speciﬁc representations. In order to ensure that the clothing and face appearance are captured realistically, we employ a ﬁve-part human encoder to the realistic frame-rendering network, consisting of four ImageNet-trained classiﬁers, and a trained face embedding network. These provide a rich embedding of the target, later enforced by a set of relevant perceptual losses. To ensure that ﬁnger motion is natural and the rendered hands do not suffer from missing parts, hand training data is augmented.
The method separates the pose and frame generation parts, performing each by a different network. The pose is provided in the space of a part-based segmentation map and is conditioned on both the target person and the motion frame. The second network transforms the generated pose and the target person’s details to a masked frame, which is blended with an arbitrary background. The frame is further improved by applying a face reﬁnement network based on an appearance preserving perceptual loss.
An extensive set of experiments is provided to estab-lish the visual and numerical validity of the method. Com-pared to previous methods, our method provides consider-ably more accurate and visually pleasing results, as evalu-ated by a set of numerical metrics, a user study, and visual examples. Contrary to most previous work, we emphasize the ability to handle diversity in the target and generated in-dividuals, promoting inclusion, which is generally lacking in this line of work. 2.