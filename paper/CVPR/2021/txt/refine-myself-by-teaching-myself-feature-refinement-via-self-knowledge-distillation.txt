Abstract
Knowledge distillation is a method of transferring the knowledge from a pretrained complex teacher model to a student model, so a smaller network can replace a large teacher network at the deployment stage. To reduce the ne-cessity of training a large teacher model, the recent litera-tures introduced a self-knowledge distillation, which trains a student network progressively to distill its own knowl-edge without a pretrained teacher network. While Self-knowledge distillation is largely divided into a data aug-mentation based approach and an auxiliary network based approach, the data augmentation approach looses its lo-cal information in the augmentation process, which hin-ders its applicability to diverse vision tasks, such as se-mantic segmentation. Moreover, these knowledge distilla-tion approaches do not receive the reﬁned feature maps, which are prevalent in the object detection and seman-tic segmentation community. This paper proposes a novel self-knowledge distillation method, Feature Reﬁnement via
Self-Knowledge Distillation (FRSKD), which utilizes an auxiliary self-teacher network to transfer a reﬁned knowl-edge for the classiﬁer network. Our proposed method,
FRSKD, can utilize both soft label and feature-map distilla-tions for the self-knowledge distillation. Therefore, FRSKD can be applied to classiﬁcation, and semantic segmenta-tion, which emphasize preserving the local information.
We demonstrate the effectiveness of FRSKD by enumer-ating its performance improvements in diverse tasks and benchmark datasets. The implemented code is available at https://github.com/MingiJi/FRSKD. 1.

Introduction
Deep neural networks (DNNs) have been applied to var-ious ﬁelds of computer vision due to the exponential ad-vancement of convolutional neural networks [7, 27, 12].
To distribute the success at the mobile devices, the vi-sion task needs to overcome the limited computing re-(cid:39) (cid:36)(cid:1) (cid:39) (cid:36)(cid:1)
Parameter 
Shared  (cid:39) (cid:36)(cid:1) (cid:39) (cid:36)(cid:1) (a) Knowledge Distillation  (b) Self-Knowledge Distillation via 
Data-augmentation 
Augmentation  (cid:39)(cid:1)(cid:36)(cid:1) (cid:39)(cid:1)(cid:36)(cid:1) (cid:39)(cid:1)(cid:36)(cid:1) (cid:39) (cid:36)(cid:1) (cid:39) (cid:36)(cid:1) (cid:39) (cid:36)(cid:1) (c) Self-Knowledge Distillation  via Auxiliary Classifiers  (d) Feature Refinement   via Self-Knowledge Distillation  
Figure 1: Comparison of various distillation methods. The black line is the forward path; the green line is the soft la-bel distillation; and the orange line is the feature distilla-tion. (a) Conventional knowledge distillation method with pretrained teacher [9, 26, 36, 14, 1]. (b) Self-knowledge distillation method via data augmentation [32, 35, 18]. (c)
Auxiliary weak classiﬁer based self-knowledge distillation, which creates a set of layer-wise classiﬁers to generate a backpropagation signal at each layer, and the layer-wise classiﬁer produce its estimation from the layer’s feature dis-tillations of the orange line and the logit distillations of the (d) Our proposed method. The original green line [39]. classiﬁer provides original feature as an input to the aux-iliary self-teacher network (blue blocks). Afterwards, the self-teacher network distills the reﬁned feature-map to the original classiﬁer (orange lines). sources [11, 41]. To solve this problem, the model com-pression has been a crucial research task, and knowledge distillation has been a prominent technology with a good compression and equivalent performances [9].
Knowledge distillation is a method of transferring the knowledge from a pretrained teacher network to a student network, so a smaller network can replace a large teacher network at the deployment stage. Knowledge distillation utilizes the teacher’s knowledge through receiving either 1) class predictions as soft labels [9]; 2) penultimate layer out-puts [29, 21, 23], or 3) feature-maps including spatial in-10664
formation at the intermediate layer [26, 2, 8]. Whereas the knowledge distillation enables utilizing the larger network in a condensed manner, the inference on such large network, a.k.a. the teacher network, becomes an ultimate burden of its practical usages. In addition, pretraining the large net-work requires substantial computational resources to pre-pare the teacher network.
To reduce such necessity of training a large network, the recent literatures introduce an alternative knowledge dis-tillation [6, 42]; which is a distillation from a pretrained network with the same architecture of the student network.
This knowledge distillation is still known to be informative to the student network with the same scale. Moreover, there are literatures on a self-knowledge distillation, which trains the student network progressively to distill and to regular-ize its own knowledge without the pretrained teacher net-work [43, 35, 18, 32, 39]. The self-knowledge distillation is different from the previous knowledge distillation because it does not require a prior preparation of the teacher network.
Self-knowledge distillation is largely divided into a data augmentation based approach and an auxiliary net-work based approach. The data augmentation based self-knowledge distillation induces a consistent prediction of relevant data, i.e. the different distorted versions of a sin-gle instance or a pair of instances from the same class [32, 35, 18]. The auxiliary network based approach utilizes ad-ditional branches in the middle of the classiﬁer network, and the additional branches are induced to make similar outputs via knowledge transfer [43, 39]. However, these approaches depend on the auxiliary network, which has the same or less complexity than classiﬁer network; so it is hard to generate a reﬁned knowledge, either by features, which are the output of the convolutional layers, or soft labels, for the classiﬁer network [32, 35, 18, 43, 39]. Furthermore, the data aug-mentation based approaches are vulnerable to lose the local information between instances, such as differently distorted instances or rotated instances. Therefore, it is hard to utilize the feature distillation, which is well known technique for improving the performances in general knowledge distilla-tions [32, 35, 18].
To deal with the limitation of existing self-knowledge distillation, this paper proposes a novel self-knowledge dis-tillation method, Feature Reﬁnement via Self-Knowledge
Distillation (FRSKD), which introduces an auxiliary self-teacher network to enable the transfer of a reﬁned knowl-edge to the classiﬁer network. Figure 1 shows the difference between FRSKD and existing knowledge distillation meth-ods. Our proposed method, FRSKD, can utilize both soft label and feature-map distillations for the self-knowledge distillation.
Therefore, FRSKD can be applied to classiﬁcation and semantic segmentation, which emphasize preservation of
FRSKD shows the state-of-the-the local information. art performances in image classiﬁcation task on various datasets, compared to other self-knowledge distillation methods. In addition, FRSKD improves the performances on semantic segmentation. Besides, FRSKD is compati-ble with other self-knowledge distillation methods as well as data augmentation. We demonstrate the compatibility of
FRSKD with large performance improvements through var-ious experiments. 2.