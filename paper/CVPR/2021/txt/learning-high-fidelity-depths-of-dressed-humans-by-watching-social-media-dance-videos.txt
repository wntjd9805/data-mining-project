Abstract
A key challenge of learning the geometry of dressed hu-mans lies in the limited availability of the ground truth data (e.g., 3D scanned models), which results in the perfor-mance degradation of 3D human reconstruction when ap-plying to real-world imagery. We address this challenge by leveraging a new data resource: a number of social me-dia dance videos that span diverse appearance, clothing styles, performances, and identities. Each video depicts dy-namic movements of the body and clothes of a single person while lacking the 3D ground truth geometry. To utilize these videos, we present a new method to use the local transfor-mation that warps the predicted local geometry of the per-son from an image to that of another image at a different time instant. This allows self-supervision as enforcing a temporal coherence over the predictions. In addition, we jointly learn the depth along with the surface normals that are highly responsive to local texture, wrinkle, and shade by maximizing their geometric consistency. Our method is end-to-end trainable, resulting in high ﬁdelity depth esti-mation that predicts ﬁne geometry faithful to the input real image. We demonstrate that our method outperforms the state-of-the-art human depth estimation and human shape recovery approaches on both real and rendered images. 1.

Introduction
Clothes are an integral part of our everyday life to func-tion, express, and protect ourselves. With the increasing prevalence of VR and AR, the ability to precisely model the complex geometry of dressed humans is becoming the key to authentic social tele-presence. To capture the local geometry, e.g., wrinkle and fabric texture, photogrammetry based on massive camera infrastructure (e.g., 40-500 cam-eras to cover full body shape) [12, 25, 55] has been used, resulting in production-level rendering [9, 32] and 3D fabri-cation [3, 5]. Despite its promise, the practical deployment of such massive camera systems in our daily environment is still challenging because of its hardware requirements and computational complexity. Single view reconstruction is an immediate remedy to address this challenge where 3D rep-resentation of humans can be learned from the scanned hu-man 3D models [1–3]. Nonetheless, the amount of these 12753
data is limited (e.g., a few hundreds of static models), which do not span diverse poses, appearance, and complex cloth geometry resulting in the performance degradation of 3D human reconstruction when applying to real-world imagery.
In this paper, we present a method to reconstruct high ﬁ-delity 3D geometry of dressed humans in the form of depths and surface normals from a single view image by exploiting hundreds of dance videos shared in social media (e.g., Tik-Tok mobile application) as shown in Figure 1.
The main characteristics of these dance videos are that 1) each video depicts a sequence of diverse poses of a single person; and 2) 3D ground truth is not available, i.e., exist-ing fully supervised methods are not applicable. We con-jecture that since the geometry of dressed humans is an in-herent semi-rigid structure, the local geometry of the same person approximately remains constant up to some trans-formations. For instance, the cloth movement on the left upper arm region undergoes, by large, a rigid transforma-tion when its pose changes. Therefore, it is possible that the geometric consistency over different poses can be applied to learn from the real dance videos. We estimate a trans-formation for each body part that can warp its 3D geometry from one image to another image at a different time instant.
This allows us to self-supervise the predicted geometry of the dressed humans without 3D supervision.
While modern learning based depth estimators are capa-ble of recovering holistic scene geometry, it is shown [30] that it often fails to encode ﬁne local geometry such as com-plex cloth wrinkles and face proﬁle features, which consti-tutes the dominant factor of realism. On the other hand, sur-face normals are highly responsive to ﬁne visual structures such as texture and wrinkles [54]. We exploit the geometric relationship of depths and surface normals to learn jointly (e.g. matching the surface normal to the curvature of the depth).
Our end-to-end trainable method takes as input an RGB image, corresponding human foreground, and human UV coordinates and outputs a high ﬁdelity depth that captures
ﬁne wrinkles and shapes that is faithful to the input image.
We design a network called HDNet that learns the spatial relationship between the image and UV coordinate to pro-duce an intermediate surface normals. These predicted sur-face normals are, in turn, used to predict the high ﬁdelity depths of dressed humans. We use a Siamese design of HD-Net to measure the self-consistency of a pair of geometric predictions. To that end, our method is semi-supervised by leveraging both 3D scanned models and real dance videos.
We demonstrate that our method outperforms the state-of-the-art human depth estimation approaches on both real and rendered images.
We present four core contributions: (1) a new dataset called TikTok dataset that consists of more than 300 se-quences of dance videos shared in a social media mobile platform, TikTok, totaling more than 100K images along with the human mask and human UV coordinates; (2) a novel formulation that warps the 3D geometry of dressed humans from one image to the other image at a different time instant to measure self-consistency, which allows us to utilize the real dance videos; (3) HDNet design that learns to predict ﬁne depths reﬂective of surface normal prediction by enforcing their geometric consistency; (4) strong quali-tative and quantitative prediction on real world imagery. 2.