Abstract
We tackle the challenging task of image change caption-ing. The goal is to describe the subtle difference between two very similar images by generating a sentence caption.
While the recent methods mainly focus on proposing new model architectures for this problem, we instead focus on an alternative training scheme. Inspired by the success of multi-task learning, we formulate a training scheme that uses an auxiliary task to improve the training of the change captioning network. We argue that the task of composed query image retrieval is a natural choice as the auxiliary task. Given two almost similar images as the input, the primary network generates a caption describing the ﬁne change between those two images. Next, the auxiliary net-work is provided with the generated caption and one of those two images.
It then tries to pick the second image among a set of candidates. This forces the primary net-work to generate detailed and precise captions via having an extra supervision loss by the auxiliary network. Further-more, we propose a new scheme for selecting a negative set of candidates for the retrieval task that can effectively im-prove the performance. We show that the proposed training strategy performs well on the task of change captioning on benchmark datasets. 1.

Introduction
Change is an inevitable part of a dynamic environment.
There has been much attention in the community for a vari-ety of change detection tasks [10, 27, 28, 13, 22, 23, 31].
While localizing the change has been the cornerstone of these works, it requires a deeper level of understanding to be able to semantically refer to the change. From a user’s per-spective, describing (captioning) the change between two images provides a more meaningful way of understanding the difference between images. The task of change cap-tioning aims to describe the change between two images by generating a detailed sentence about the change of objects in these images. Note that in this task, we are only inter-ested in changes at the object level (e.g. changes in terms of
What's the difference between these two images?
“ The big green shiny cube in front of the big purple object became purple. ”
Figure 1: (Best viewed in color) Given two very similar im-ages, the goal of change captioning is to describe the subtle difference between these two images. The difference can be in terms of objects’ color, texture, position, addition or removal, etc. object color, position, etc), i.e. we do not want to generate captions for changes in terms of viewpoints, light condition, etc. Some early works [13, 22, 23] in this area assume that there is always an object change between a pair of input im-ages. This assumption does not always hold in a real-world application. In many cases, no object has been changed be-tween the two images. Instead, it is only the viewpoint that is different (e.g. a moving robot can see a scene from two different viewpoints and should be able to distinguish that it is the same scene observed from different angles/lighting conditions). To address this limitation, recent work [25] proposes the task of robust changing captioning where not all pairs of images exhibit a change – some pairs can be distractors with similar scenes from different viewpoints. 2725
In this paper, we also consider the problem of change captioning with distractors. Our goal is to detect a change between a pair of images and generate a sentence describing the change. While previous works mainly focus on propos-ing new network architectures to better tackle the problem
[31, 25], we instead focus on improved learning strategies via multi-task learning. Our proposed method consists of networks for two complementary tasks, namely the primary task and the auxiliary task.
The primary task in our formulation is the task of change captioning. Given two images, the goal of the primary task is to describe the difference between these two images. The input images are similar to each other and only differ in very subtle ways (e.g. an object’s color, shape, or position is changed).
The auxiliary task in our formulation is the composed query image retrieval [37, 11, 8, 3]. This is an extension of the image retrieval task. The input to this task consists of a reference image and a sentence that deﬁnes the user’s desired modiﬁcation on the reference image. The model should then pick a candidate among a set of images. The candidate should look like the reference image, but differ from it according to the desired modiﬁcation.
We argue that the aforementioned two tasks are naturally complimentary to each other. As a result, we can use the auxiliary task to help the primary task during learning. We propose a joint learning scheme for these two tasks.
In-spired by the works on cycle-consistency [42, 29, 9], our proposed learning scheme sequentially performs the pri-mary and the auxiliary tasks in a way that the output of one task is the input to the other task. These two tasks can rein-force each other during training. For example, the learning scheme forces the primary task to generate a good caption to be used as the input to the auxiliary network. If the cap-tion generated by the primary network is not reliable, the auxiliary network would fail to retrieve the correct image.
This will produce an auxiliary loss which is then used to further train the primary network.
The contributions of this paper are manifold.
• First, we propose a new learning scheme for the task of change image captioning that involves using an aux-iliary task to improve the performance of the primary task.
• Second, we further improve our proposed learning scheme by deﬁning a new strategy for selecting hard negative samples for the auxiliary task of composed query image retrieval.
• Finally, we show that our proposed method can im-prove the performance of a change captioning task by performing empirical experiments on the CLEVR-Change dataset and the Spot-the-diff dataset. 2.