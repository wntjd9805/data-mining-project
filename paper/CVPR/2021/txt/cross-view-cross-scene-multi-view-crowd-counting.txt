Abstract
Synthetic training dataset
Multi-view crowd counting has been previously proposed to utilize multi-cameras to extend the ﬁeld-of-view of a sin-gle camera, capturing more people in the scene, and im-prove counting performance for occluded people or those in low resolution. However, the current multi-view paradigm trains and tests on the same single scene and camera-views, which limits its practical application.
In this paper, we propose a cross-view cross-scene (CVCS) multi-view crowd counting paradigm, where the training and testing occur on different scenes with arbitrary camera layouts. To dynam-ically handle the challenge of optimal view fusion under scene and camera layout change and non-correspondence noise due to camera calibration errors or erroneous fea-tures, we propose a CVCS model that attentively selects and fuses multiple views together using camera layout ge-ometry, and a noise view regularization method to train the model to handle non-correspondence errors. We also gener-ate a large synthetic multi-camera crowd counting dataset with a large number of scenes and camera views to capture many possible variations, which avoids the difﬁculty of col-lecting and annotating such a large real dataset. We then test our trained CVCS model on real multi-view counting datasets, by using unsupervised domain transfer. The pro-posed CVCS model trained on synthetic data outperforms the same model trained only on real data, and achieves promising performance compared to fully supervised meth-ods that train and test on the same single scene. 1.

Introduction
Deep neural network-based multi-view (MV) crowd counting [56, 57] was recently proposed to count people in wide-area scenes that cannot be covered by a single cam-era.
In these works, feature maps from multiple camera views are fused together and decoded to predict a scene-level crowd density map. However, one major disadvantage of the current MV paradigm is the models are trained and
PETS2009
DukeMTMC (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28581) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28582) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28583) i
V e w p o o l i n g (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28582) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28583) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28585) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28588) (cid:28614)(cid:28582) (cid:28614)(cid:28583) (cid:28614)(cid:28581)
Cross-View Cross-Scene
CityStreet (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28581) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28583) (cid:28599)(cid:28629)(cid:28641)(cid:28633)(cid:28646)(cid:28629)(cid:28564)(cid:28584)
Real test datasets
Synthetic validation dataset
Figure 1: Cross-view cross-scene (CVCS) multi-view crowd counting.
The CVCS model is trained and validated on synthetic multi-view crowd scenes, where the scenes and camera-views are different between the train-ing and validation sets. To test on a real scene, unsupervised domain adap-tation is applied to the trained CVCS model, where only the real images are used to ﬁne-tune the model. tested on the same single scene and a ﬁxed camera layout, and thus the trained models do not generalize well to other scenes or other camera layouts.
In this work, we propose a new paradigm of cross-view cross-scene multi-view counting (CVCS), where MV counting models are trained and tested on different scenes and arbitrary camera layouts. This paradigm is challeng-ing because both the scene and camera layout (including number of cameras) change at test time. In particular, in single-scene MV counting, the optimal selection of features from each camera and the handling of non-correspondence errors (caused by camera calibration errors or erroneous features) can be directly learned by the MV model (in its network parameters), since it is trained and tested on the
In contrast, for CVCS MV count-same scene/cameras. ing, because the camera positions, camera orders, and scenes are all varying, the MV counting model must learn to dynamically handle different camera layouts and non-correspondence noise. To address these two issues, we pro-pose a CVCS counting model, which attentively selects and fuses features from multiple cameras using the camera lay-557  
Distance  maps
View 1 (cid:2282)(cid:3036)
D i s t m e a s u r e
Camera  selection 3 x 3
, 6 4 3 x 3
, 6 4 p o o l i n g
, 2 x 2 3 x 3
, 1 2 8 3 x 3
, 1 2 8
P o o l i n g
, 2 x 2 3 x 3
, 2 5 6 3 x 3
, 2 5 6 3 x 3
, 2 5 6
Feature extraction (cid:2282)(cid:2869) (cid:2282)(cid:2870) (cid:2282)(cid:2871)
View-pooling
View 2
View 3
…
View K
CVCS model
Multi-view decoding 3 x 3
, 5 1 2 3 x 3
, 5 1 2 3 x 3
, 5 1 2 3 x 3
, 2 5 6 3 x 3
, 1 2 8 3 x 3
, 6 4 3 x 3
, 1
Prediction
Figure 2: The pipeline of the cross-view cross-scene multi-view counting model (CVCS): 1) Single-view feature extraction: The ﬁrst 7 layers of VGG-Net extracts the single-view features; 2) Feature projection: The extracted single-view features are projected to the average height-plane by camera projection; 3) Multi-camera selection and fusion: An adaptive CNN subnet selects among the multi-view feature maps, through an attention mechanism guided by the object-to-camera distance, and a max-pooling layer fuses projected camera-view features; 4) Multi-view decoding: The fused projected features are decoded to predict the scene-level density maps.
Camera parameters (cid:1844)(cid:3036)(cid:481) (cid:1846)(cid:3036)(cid:481) (cid:1837)(cid:3036) out (object-to-camera distances), and a noise-injection reg-ularization scheme, which simulates non-correspondence errors, improving model generalization.
Effectively training cross-scene counting models re-quires a large dataset of scenes in order to capture the many possible variations of camera poses and scenes. For ex-ample, cross-scene models for single-view counting [3, 58, 31, 37, 1] are trained on large single-view datasets, such as ShanghaiTech [58], UCF-QNRF [12], JHU-CROWD++
[44] or NWPU-CROWD [51], which contain thousands of images, each of a different scene. However, current
MV counting datasets, such as PETS2009 [8], DukeMTMC
[33], and CityStreet [56] only contain 2 to 4 views of a sin-gle scene, and combining these three datasets only yields 3 scenes and 3 camera layouts, which is not enough to train a
CVCS model. Collecting and annotating a large-scale MV dataset, comprising a large number of scenes taken with many synchronized cameras, is a time-consuming and la-borious task, and is further complicated due to personal pri-vacy issues and social-distancing in the current pandemic situation. To avoid such limitations, we generate a synthetic
CVCS multi-view dataset of 31 scenes containing around 100 camera views with 100 frames in each view. The large number of camera views for each scene is sufﬁcient for gen-eralizing across camera layouts.
We use the synthetic dataset to train our CVCS model, and directly applying the trained CVCS model to real-world multi-view counting datasets, yielding promising results.
The results are further improved by using unsupervised do-main adaptation to ﬁne-tune the trained model on only the test images (and not crowd labels).
In summary, the contributions of this paper are 3-fold: 1. We propose a cross-view cross-scene multi-view counting DNN model (CVCS), which adaptively se-lects and fuses multi-cameras, and a noise view regu-larization method to improve generalization. To our knowledge, this is the ﬁrst study on the cross-view cross-scene multi-view problem in crowd counting. 2. We propose a large synthetic multi-view crowd count-ing dataset, which contains a large number of camera views, scene variations and frames. This is the ﬁrst large synthetic dataset for multi-view counting, which enables research on cross-scene cross-view problems. 3. The proposed CVCS model outperforms existing state-of-the-art MV models in the cross-view cross-scene paradigm. Furthermore, the CVCS model, trained on synthetic scenes and adapted to a real-world test scene with unsupervised domain adaptation, achieves promising performance compared with MV models trained on single-scenes. 2.