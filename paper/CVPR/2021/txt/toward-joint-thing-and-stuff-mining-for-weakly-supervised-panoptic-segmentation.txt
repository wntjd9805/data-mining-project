Abstract
Panoptic segmentation aims to partition an image to ob-ject instances and semantic content for thing and stuff cat-egories, respectively. To date, learning weakly supervised panoptic segmentation (WSPS) with only image-level la-bels remains unexplored. In this paper, we propose an ef-ﬁcient jointly thing-and-stuff mining (JTSM) framework for
WSPS. To this end, we design a novel mask of interest pool-ing (MoIPool) to extract ﬁxed-size pixel-accurate feature maps of arbitrary-shape segmentations. MoIPool enables a panoptic mining branch to leverage multiple instance learning (MIL) to recognize things and stuff segmentation in a uniﬁed manner. We further reﬁne segmentation masks with parallel instance and semantic segmentation branches via self-training, which collaborates the mined masks from panoptic mining with bottom-up object evidence as pseudo-ground-truth labels to improve spatial coherence and con-tour localization. Experimental results demonstrate the ef-fectiveness of JTSM on PASCAL VOC and MS COCO. As a by-product, we achieve competitive results for weakly su-pervised object detection and instance segmentation. This work is a ﬁrst step towards tackling challenge panoptic seg-mentation task with only image-level labels. 1.

Introduction
Panoptic segmentation focuses on simultaneously seg-menting all object instances and semantic content in an image. It is one of the most important tasks in computer
*Corresponding author.
Figure 1: The overall ﬂowchart of our JTSM framework. vision due to its great academic values and industrial ap-plications. Recent rapid progress on panoptic segmenta-tion has been driven by combining the strength of instance segmentation and semantic segmentation tasks via a multi-branch scheme. However, these deep models heavily rely on a large amount of training data with expensive instance-level and pixel-wise annotations. Collecting such training data has been a particular bottleneck on the way of apply-ing panoptic segmentation to real-world applications, e.g., autonomous driving, robotics, and image editing, where la-belling each pixel for numerous images is particularly time-consuming. For example, fully annotating a single image in
Cityscapes [1] required more than 1.5 hours on average.
One way to reduce the requirement of strong supervision is the weakly supervised panoptic segmentation (WSPS), which seeks to use weak annotations for model training. To our best knowledge, the only previous work that attempted to address WSPS problem is that of
[2], which requires bounding boxes for thing categories and image-level tags for stuff during training. However, for applications needing very large-scale image sets and categories, bounding-box-level annotations still require enormous human effort. It is 16694
thus desirable to learn panoptic segmentation from large-scale datasets with weaker supervision.
We focus on the most extreme case of WSPS where only image-level labels are available, and no instance-level an-notations are involved during training. To date, none of the existing work further investigates the problem of learning panoptic segmentation with only image-level labels. An in-tuitive and strong baseline method is to perform weakly su-pervised instance segmentation (WSIS) and weakly super-vised semantic segmentation (WSSS) independently, and use heuristic post-processing method [3] to merge their re-sults. However, the straightforward combination of such two techniques disregards the underlying relationship and fails to borrow rich contextual cues between things and stuff. As context information is critical to recognize and localize the objects, and foreground objects provide com-plementary cues to assist background understanding [4, 5].
In this paper, we propose a Joint Thing-and-Stuff Min-ing (JTSM) framework to learn panoptic segmentation with only image-level labels, as illustrated in Fig. 1. Our motiva-tion is to consider foreground things and background stuff as uniform object instances in form of segmentation masks.
Particularly, each connected component of stuff content is viewed as an individual instance, which shares the same spirit as thing objects. Different to the baseline that frames the two related tasks at architectural level via a multi-branch scheme, the main advantage of JMTS is to model the corre-lations between objects and background at instance level.
To this end, we design a novel mask of interest pool-ing (MoIPool) to extract ﬁxed-size pixel-accurate feature maps for arbitrary-shape segmentations, which provides a uniform representational power for things and stuff. Thus, given a set of segment proposals, a panoptic mining branch leverages multiple instance learning (MIL) to mine all tar-get categories in a uniﬁed manner. We further introduce two schemes to reﬁne segmentation masks. First, we collaborate the mined results from panoptic mining with bottom-up ob-ject evidence to improve spatial coherence and contour lo-calization. Second, we introduce self-training to reﬁne seg-mentation masks with parallel instance and semantic seg-mentation branches. With pseudo-ground-truth masks from its preceding branch, the discriminatory power of the im-age segmentation can be enhanced. Experimental results demonstrate the effectiveness of our proposed JTSM com-pared to strong baselines on PASCAL VOC [6] and MS
COCO [7]. As a by-product, we also achieve competitive results for both weakly supervised object detection and in-stance segmentation tasks.
The contributions of this work are three folds:
• We propose JTSM to jointly segment things and stuff for weakly supervised panoptic segmentation in a uni-ﬁed framework. To our best knowledge, this work makes the ﬁrst attempt to tackle challenge panoptic segmentation task with only image-level labels.
• We design a novel mask of interest pooling (MoIPool) to compute ﬁxed-size pixel-accurate feature maps of arbitrary-shape segmentations, which enables JTSM to leverage multiple instance learning (MIL) to mine thing and stuff with a uniform representational power.
• Self-training is further introduced to reﬁne the image segmentation with two parallel instance and semantic segmentation branches, which are supervised by the mined results and bottom-up object evidence to im-prove spatial coherence and contour localization. 2.