Abstract
Self-attention has the promise of improving computer vi-sion systems due to parameter-independent scaling of recep-tive ﬁelds and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent inter-actions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efﬁcient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classiﬁcation benchmark. In preliminary transfer learning experiments, we ﬁnd that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efﬁcacy of self-attention models on settings traditionally dominated by convolutions. 1 1.

Introduction
Vision and natural language processing (NLP) systems divide the landscape of computational primitives. While self-attention is the primary workhorse in NLP, convolutions 1Please refer to https://arxiv.org/abs/2103.12731 for a longer version. are ubiquitous in nearly all vision models. Convolutions em-body the principle of local processing, to learn local spatial features such as edges and texture that are abundant in im-ages. On the other hand, the Transformer [53] showed that self-attention is an effective and computationally efﬁcient mechanism for capturing global interactions between words in a sentence. Self-attention has several properties that make it a good ﬁt for vision: (a) content-based interactions as opposed to content-independent interactions of convolution; (b) parameter-independent scaling of receptive ﬁeld size as opposed to parameter-dependent scaling of convolution; (c) empirical ability to capture long-range dependencies for use in larger images; (d) ﬂexibility to handle and integrate multiple types of data that appear in vision, such as pix-els [55, 2, 40, 62], point clouds [59], sequence conditioning information [58], and graphs [29]. Self-attention may also be regarded as an adaptive nonlinearity paralleling a long history of techniques in computer vision, such as bilateral
ﬁltering [36] and non-local means [3].
Several recent papers [2, 39, 10, 62, 46] have attempted using self-attention primitives to improve image classiﬁca-tion accuracy over the strong and commonly used ResNet backbones [14, 15]. Among them, the Stand-Alone Self-Attention (SASA) [39] is a fully self-attentive model that replaces every spatial convolution with local self-attention, which improves the performance of ResNet backbones while having fewer parameters and ﬂoating point opera-tions. While conceptually promising, these models lag be-hind state-of-the-art convolutional models in image classiﬁ-cation. State-of-the-art convolutional models [51, 63, 38] use a variety of scaling techniques to achieve strong performance across a range of computation and parameter regimes.
In this work, we aim to develop and understand tech-niques for scaling local self-attention models to outperform some of the best convolutional models. Scaling self-attention 12894
models presents a unique set of challenges. For example, convolutions have been very efﬁciently mapped to matrix accelerators such as TPUs and GPUs that drive most deep learning workloads, but fast implementations of local 2D self-attention do not currently exist. To bridge this gap, we introduce a non-centered version of local attention that efﬁ-ciently maps to existing hardware with haloing. While our formulation breaks translational equivariance, it improves both throughput and accuracies over the centered local self-attention used in SASA. We also introduce a strided attention downsampling operation for multi-scale feature extraction.
We leverage these techniques to develop a new local self-attention model family, HaloNet, which achieves state-of-the-art performance across different parameter regimes. The largest HaloNet achieves 84.9% top-1 accuracy on the Im-ageNet [43] classiﬁcation benchmark (Section 4.1). We perform a detailed study to uncover how self-attention and convolutional models scale differently. Our self-attention layers also show promising results on harder tasks such as object detection and instance segmentation (Section 4.5) us-ing the Mask R-CNN framework on the COCO benchmark.
Finally, we end with a discussion of current limitations and ideas for future work in applying self-attention to vision. 2. Models and Methods
Although our models use self-attention instead of convo-lutions for capturing spatial interactions between pixels, they adopt some important architectural features of modern convo-lutional neural networks (CNNs). Like CNNs, we compute multi-scale feature hierarchies [31] which enable detecting objects at multiple sizes in tasks such as localization and instance segmentation. For this, we develop a strided self-attention layer, a natural extension of strided convolutions (Section 2.2). To deal with the computational cost in larger resolutions where global attention is infeasible, we follow the fairly general principle of local processing, which is at the heart of convolutions and natural perceptual systems [22, 23], and use spatially restricted forms of self-attention. However, unlike the model of [39], that also use local self-attention, we abstain from enforcing translation equivariance in lieu of better hardware utilization, which improves the speed-accuracy tradeoff (Section 2.2). Also note that while we use local attention, our receptive ﬁelds per pixel are quite large (up to 18 × 18) and we show in Section 4.2.2 that larger receptive ﬁelds help with larger images. In the remainder of this section, we will motivate self-attention for vision tasks and describe how we relax translational equivariance to efﬁciently map local self-attention to hardware. 2.1. Self attention can generate spatially varying convolutional ﬁlters
Self-attention has been viewed as a method to directly capture relationships between distant pixels [39, 19, 54]. It has also been interpreted as a speciﬁc instantiation of the classic technique of non-local means [3, 55]. The perspective that we discuss in this section is one that views self-attention as generating spatially varying ﬁlters, in contrast to the reuse of the same ﬁlter across every spatial location in standard convolutions [12]. To observe this, we write self-attention and convolution as speciﬁc instances of a general spatial pooling function. Given an input x ∈ RH×W ×cin , where
H is the height, W is the width, and cin is the number of input channels, we deﬁne a local 2D pooling function that computes an output at location (i, j), yij ∈ Rcout as yij =
X a,b∈N (i,j) f (i, j, a, b)xab, where f (i, j, a, b) is a function that returns a weight ma-trix W ∈ Rcin×cout at every location in a 2D window
N (i, j) of size k × k centered at (i, j). Note that later in this section, we introduce non-centered windows for self-attention, but we use centering here for ease of explana-tion. This computation is repeated for every pixel (i, j). For a convolution, f (i, j, a, b) returns a different linear trans-formation for each relative distance in neighborhood, and these weights are shared across all (i, j). Weight sharing signiﬁcantly reduces parameters and encourages learning features that repeat spatially. In dot-product relative self-attention [44, 39, 2] (eqs. (2) and (3)), every pixel in the neighborhood shares the same linear transformation which is multiplied by a scalar probability that is a function of both content-content and content-geometry interactions resulting in weights that can vary spatially. As an example, for a ball and an orange at two different locations in an image, pixels inside the ball and the orange are likely to generate different pij a−i,b−j because of the different content around them, such as color or texture. f (i, j, a, b)conv = Wa−i,b−j (1) f (i, j, a, b)self −att = softmaxab(cid:16)(WQxij)⊤WKxab+ (WQxij)⊤ra−i,b−j(cid:17)WV
= pij a−i,b−jWv (2) (3)
For self-attention, WQ, WK, and WV are learned linear transformations that are shared across all spatial locations, and respectively produce queries, keys, and values when used to transform x. Spatial geometry is captured by ra−i,b−j, which is a learned relative position based embedding. The (WQxij)⊤WKxab component captures the content-content interaction between the query pixel and a key pixel in the window. The (WQxij)⊤ra−i,b−j component is the content-geometry interaction that captures the relationship between the query and the relative position of the key pixel [44]. Note 12895
Neighborhood Windows
[2, 2, 4, 4, c]
Image
[4, 4, c]
Blocked Image
[2, 2, 2, 2, c]
Blocking
Haloing (halo=1)
Query 
[2, 2, 2, 2, c]
Output
[2, 2, 2, 2, c]
Attention
Figure 1. HaloNet local self-attention architecture: The different stages of blocked local attention for a [4, 4, c] image, block size b = 2, and halo h = 1. The image is ﬁrst blocked into non-overlapping [2, 2, c] images from which the queries are computed. The subsequent haloing step then extracts a [4, 4, c] memory around each of the blocks which linearly transform to keys and values. The spatial dimensions after attention are the same as the queries.
Blocked 
Image
[2, 2, 2, 2, c]
Query 
[2, 2, 1, 1, c]
Subsampling
Haloing
Output
[2, 2, 1, 1, c]
Output 
[1, 1, 2, 2, c]
Attention
Merging
Same
Neighborhood as Stride=1
[2, 2, 4, 4, c] convolution). On the other hand, the computational cost of self-attention grows quadratically with k, preventing the use of very large values for k.
Figure 2. The attention downsampling layer subsamples the queries but keeps the neighborhood the same as the the stride=1 case. that this formulation preserves translational equivariance.
If an object translates in an image, for any pixel within the object, the content around it stays the same, generating the same pij a−i,b−j, thereby producing the same output after self-attention. To increase expressivity, multi-headed attention
[53] is used, which repeats this computation multiple times in parallel with different parameters, analogous to group convolutions [27, 57].
In the SASA model of [39], the local window N (i, j) is a k × k window centered around (i, j), just like a convolution.
The size of this local window k is an important setting to leverage in self-attention. Unlike dense convolutions, k can grow without signiﬁcantly increasing the number of param-eters. Since the projection parameters (WQ, WK, WV ) are independent of k, the only parameters that increase with k is ra−i,b−j. However, ra−i,b−j constitutes a trivial fraction of the parameters compared to the projection parameters 2 , so increasing k does not not impact the number of parameters of the layer signiﬁcantly. In contrast, the number of param-eters in a convolution layer scale quadratically with k (e.g., a 5 × 5 convolution has 25 9 times the parameters of a 3 × 3 2For a window size as large as 63, and 16 dimensions per attention head, ra−i,b−j would add only 63 ∗ 16 = 1008 parameters per layer because ra−i,b−j are shared among heads. In contrast, if the dimensions of the attention layer were 512, WQ, WK , WV would contribute 786432 parameters. We show details in the appendix. 2.2. Improving the speed memory tradeoff by re  laxing translational equivariance
Global self-attention, in which all locations attend to each other, is too expensive for most image scales due to the quadratic computation cost with respect to k. Thus, multi-scale visual backbones need to use local attention to limit the size of k. We follow the intuitive form of local attention developed in [39], which tries to mimic the square neighborhoods used by convolutions. This form of local attention requires extracting local 2D grids around each pixel. Unfortunately, while deep learning libraries auto-matically handle neighborhood gathering for convolutions, no such neighborhood gathering function exists for local self-attention (or any general local function). Thus, imple-menting local self-attention requires explicitly gathering the local neighborhoods before the actual self-attention opera-tion can be performed. While the implementation of this local neighborhood gathering function might initially appear to be a relatively minor implementation detail, in practice, it must actually be carefully designed to reduce memory usage while avoiding unnecessary extra computation. An unop-timized implementation can prevent self-attention models from scaling up due to either out-of-memory errors or exces-sive slowness. The following discussion frames the design considerations of this neighborhood gathering function.
A straightforward approach would gather k × k sized windows separately around each pixel. As summarized in
Table 1 (Row 1), this method blows up the memory used by a factor of k2 due to replicating the pixel contents for each of 12896
Method
Global
Per pixel windows
SASA [39]
Blocked local (ours)
Neighborhood
Memory
HW c
HW k2c
HW b2 (b + 2h)2c
HW b2 (b + 2h)2c
Receptive
Field
HW × HW k × k k × k, where h = ⌊ k 2 ⌋ (b + 2h) × (b + 2h)
FLOPs
Per Pixel 4(HW )2c 4k2c 4(b + 2h)2c 4(b + 2h)2c
Table 1. Scaling behavior of self-attention mechanisms. f is the number of heads, b is the size of the block, c is the total number of channels, and h is the size of the halo the k2 neighborhoods it participates in. This solution quickly leads to out-of-memory errors. Global attention (Row 4) is at the other end of the spectrum, where all pixels share the same neighborhood, lowering memory at the expense of considerably more FLOPs 3. This solution slows down models signiﬁcantly, while also imposing memory problems due the massize size of the attention matrix. A solution that lies in-between these two extremes should trade-off memory and compute appropriately, with the recognition that a small amount of waste is required.
A compromise solution can be achieved by leveraging the idea that neighboring pixels share most of their neigh-borhood. For example, two pixels that are right next to each other share k × (k − 1) pixels of their neighborhoods. Thus a local neighborhood for a block of pixels can be extracted once together, instead of extracting separate neighborhoods per pixel. The FLOPs can be controlled by varying the number of pixels that form a block. We name this strategy blocked local self-attention. The two extremes discussed above are a special case of blocked local self-attention.
Global attention corresponds to setting the block size to be the entire spatial extent, while the per-pixel extraction corresponds to setting the block size to be 1. b , W
Figure 1 depicts the different steps involved in executing blocked local self-attention for an image with height H = 4, width W = 4, and c channels with stride 1. Blocking chops up the image into a H b tensor of non-overlapping (b, b) blocks. Each block behaves as a group of query pixels and a haloing operation combines a band of h pixels around them (with padding at boundaries) to obtain the corresponding shared neighborhood block of shape ( H b , b + 2h, b + 2h, c) from which the keys and values are computed. H b × W b attention operations then run in parallel for each of the query blocks and their corresponding neighborhoods, illustrated with different colors in Figure 1. SASA [39] used the same blocking strategy4, setting h = ⌊ k 2 ⌋ and uses attention masks to emulate pixel-centered neighborhood windows of size k × k. Our approach For example, to achieve a 7 × 7 pixel b , W 3To illustrate this, on a 128 × 128 resolution with 64 channels, global self-attention would incur about 28 times more FLOPs than a 3 × 3 convo-lution with 64 input and output channels 4Code for both SASA and HaloNet will be made available, along with the checkpoints for HaloNet centered window, [39] set h = 3. The use of attention masks gives the operation translational equivariance, since each pixel only looks at a square window around it.
However, the downside of using attention masks is that it wastes computation that must happen regardless due to the implementation of this algorithm. If attention masks are not used, the receptive ﬁeld increases without any additional computation, as shown in Table 1 (Rows 2 and 3). However, pixel-level translational equivariance is lost because the non-square receptive ﬁelds means that the output of a pixel is dependent on which block it falls into. Take for example a pixel at the left edge of its block, which sees additional pixels that are to the right of its square receptive ﬁeld. If the entire image is shifted one pixel to the right, the pixel now falls into right edge of a neighboring block, and now sees additional pixels that are to the left of its square recep-tive ﬁeld. Thus the output of the pixel is dependent on its position in a block, which can change if the image shifts.
Another perspective is that blocked local self-attention is only translational equivariant to shifts of size b. While pixel-level translational equivariance is considered important for achieving good performance[61], we ﬁnd that empirically, using a non-masked block local self-attention actually im-proves the accuracy of the model (see Section 4.3). We suspect that the image shifting and cropping perturbations in common data augmentation strategies reduce the reliance on such inductive biases. Thus we adopt unmasked blocked local self-attention because it improves accuracy without sacriﬁcing performance.
Another difference with SASA is our implementation of downsampling. We replace attention followed by post-attention strided average pooling by a single strided attention layer that subsamples queries similar to strided convolutions, as shown in Figure 2. Note that we use the same neighbor-hood as is extracted in the stride 1 case (Figure 1). This change does not impact accuracy while also reducing the
FLOPs 4× in the downsampling layers. We also implement some important algorithmic optimizations that improve our throughput primarily by avoiding reshapes and data format-ting operations. In interest of space, we list them in the Ap-pendix D. Taken together, the speedups produced by these improvements are signiﬁcant as seen in Figure 3, with up to 2× improvements in step time. These improvements can be leveraged to train large self-attention models that were previously too expensive. We leave additional optimizations, such as fused operations and better pipelining of memory accesses with computation, to future work.
Note that in the deeper layers of multiscale architectures, smaller spatial dimensions and larger channels would shift the compute calculus in favor of global attention. The mod-els we introduce in Section 4, also take advantage of this, typically using local attention in the higher resolutions and global attention when the image resolutions are the smallest. 12897
× 3 3.