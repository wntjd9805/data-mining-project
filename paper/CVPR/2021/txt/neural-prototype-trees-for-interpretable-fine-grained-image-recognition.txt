Abstract 1.

Introduction
Prototype-based methods use interpretable representa-tions to address the black-box nature of deep learning mod-els, in contrast to post-hoc explanation methods that only approximate such models. We propose the Neural Prototype
Tree (ProtoTree), an intrinsically interpretable deep learn-ing method for ﬁne-grained image recognition. ProtoTree combines prototype learning with decision trees, and thus results in a globally interpretable model by design. Addi-tionally, ProtoTree can locally explain a single prediction by outlining a decision path through the tree. Each node in our binary tree contains a trainable prototypical part.
The presence or absence of this learned prototype in an im-age determines the routing through a node. Decision mak-ing is therefore similar to human reasoning: Does the bird have a red throat? And an elongated beak? Then it’s a hummingbird! We tune the accuracy-interpretability trade-off using ensemble methods, pruning and binarizing. We apply pruning without sacriﬁcing accuracy, resulting in a small tree with only 8 learned prototypes along a path to classify a bird from 200 species. An ensemble of 5 Pro-toTrees achieves competitive accuracy on the CUB-200-2011 and Stanford Cars data sets. Code is available at github.com/M-Nauta/ProtoTree.
There is an ongoing scientiﬁc dispute between simple, interpretable models and complex black boxes, such as
Deep Neural Networks (DNNs). DNNs have achieved su-perior performance, especially in computer vision, but their complex architectures and high-dimensional feature spaces has led to an increasing demand for transparency, inter-pretability and explainability [1], particularly in domains with high-stakes decisions [43]. In contrast, decision trees are easy to understand and interpret [14, 19], because they transparently arrange decision rules in a hierarchical struc-ture. Their predictive performance is however far from competitive for computer vision tasks. We address this so-called ‘accuracy-interpretability trade-off’ [1, 35] by com-bining the expressiveness of deep learning with the inter-pretability of decision trees.
We present the Neural Prototype Tree, ProtoTree in short, an intrinsically interpretable method for ﬁne-grained image recognition. A ProtoTree has the representational power of a neural network, and contains a built-in binary decision tree structure, as shown in Fig. 1 (left). Each in-ternal node in the tree contains a trainable prototype. Our prototypes are prototypical parts learned with backpropa-gation, as introduced in the Prototypical Part Network (Pro-14933
toPNet) [9] where a prototype is a trainable tensor that can be visualized as a patch of a training sample. The extent to which this prototype is present in an input image deter-mines the routing of the image through the corresponding node. Leaves of the ProtoTree learn class distributions. The paths from root to leaves represent the learned classiﬁca-tion rules. The reasoning of our model is thus similar to the
“Guess Who?” game where a player asks a series of binary questions related to visual properties to ﬁnd out which of the 24 displayed images the other player had in mind.
To this end, a ProtoTree consists of a Convolutional Neu-ral Network (CNN) followed by a binary tree structure and can be trained end-to-end with a standard cross-entropy loss function. We only require class labels and do not need any other annotations. To make the tree differentiable and back-propagation compatible, we utilize a soft decision tree, meaning that a sample is routed through both children, each with a certain weight. We present a novel routing procedure based on the similarity between the latent image embedding and a prototype. We show that a trained soft ProtoTree can be converted to a hard, and therefore more interpretable,
ProtoTree without loss of accuracy.
A ProtoTree approximates the accuracy of non-interpretable classiﬁers, while being interpretable-by-design and offering truthful global and local explanations.
This way it provides a novel take on interpretable machine
In contrast to post-hoc explanations, which ap-learning. proximate a trained model or its output [37, 31], a ProtoTree is inherently interpretable since it directly incorporates in-terpretability in the structure of the predictive model [37]. A
ProtoTree therefore faithfully shows its entire classiﬁcation behaviour, independent of its input, providing a global ex-planation (Fig. 1). As a consequence, our compact tree en-ables a human to convey, or even print out, the whole model.
In contrast to local explanations, which explain a single pre-diction and can be unstable and contradicting [3, 27], global explanations enable simulatability [35]. Additionally, our
ProtoTree can produce local explanations by showing the routing of a speciﬁc input image through the tree (Fig. 1, right). Hence, ProtoTree allows retraceable decisions in a human-comprehensible number of steps. In case of a mis-classiﬁcation, the responsible node can be easily identiﬁed by tracking down the series of decisions, which eases error analysis.
Scientiﬁc Contributions
• An intrinsically interpretable neural prototype tree ar-chitecture for ﬁne-grained image recognition.
• Outperforming ProtoPNet [9] while having roughly only 10% of the number of prototypes, included in a built-in hierarchical structure.
• An ensemble of 5 interpretable ProtoTrees achieves competitive performance on CUB-200-2011 [50] (CUB) and Stanford Cars [30].
Figure 2: Excerpt from classiﬁcation process of ProtoP-Net [9]. ProtoPNet learns 10 prototypes per class, resulting in 2000 prototypes for CUB, therefore making 2000 local decisions for one test image. 2.