Abstract
We introduce Position Adaptive Convolution (PAConv), a generic convolution operation for 3D point cloud process-ing. The key of PAConv is to construct the convolution ker-nel by dynamically assembling basic weight matrices stored in Weight Bank, where the coefﬁcients of these weight matri-ces are self-adaptively learned from point positions through
ScoreNet. In this way, the kernel is built in a data-driven manner, endowing PAConv with more ﬂexibility than 2D convolutions to better handle the irregular and unordered point cloud data. Besides, the complexity of the learning process is reduced by combining weight matrices instead of brutally predicting kernels from point positions.
Furthermore, different from the existing point convolu-tion operators whose network architectures are often heav-ily engineered, we integrate our PAConv into classical
MLP-based point cloud pipelines without changing net-work conﬁgurations. Even built on simple networks, our method still approaches or even surpasses the state-of-the-art models, and signiﬁcantly improves baseline perfor-mance on both classiﬁcation and segmentation tasks, yet with decent efﬁciency. Thorough ablation studies and vi-sualizations are provided to understand PAConv. Code is released on https://github.com/CVMI-Lab/PAConv. 1.

Introduction
In recent years, the rise of 3D scanning technologies has been promoting numerous applications that rely on 3D point cloud data, e.g., autonomous driving, robotic manip-ulation and virtual reality [35, 40]. Thus, the approaches to effectively and efﬁciently processing 3D point clouds are in critical needs. While remarkable advancements have been obtained in 3D point cloud processing with deep learn-ing [36, 37, 47, 25], it is yet a challenging task in view of the sparse, irregular and unordered structure of point clouds.
*M. Xu and R. Ding contribute equally.
†Corresponding author
𝑓#
𝑓$
…
𝑓-𝐹,-: 𝑁×𝐶,-MLP
SOP
𝑓./0
𝐹./0: 1×𝐶./0 (a) PointNet weights
⊚
Kernel
Points
Kernels position
𝑝KL − 𝑝,
𝑝KM − 𝑝,
…
𝑝KN − 𝑝,
𝑃,-: 𝑁×3
⊙
SOP
𝑓./0
𝐹./0: 1×𝐶./0 kernel scores
𝑓#
𝑓$
…
𝑓-𝐹,-: 𝑁×𝐶,-(c) KPConv
𝑝KL − 𝑝,
𝑝KM − 𝑝,
…
𝑝KN − 𝑝,
𝑃,-: 𝑁×3
𝑓#
𝑓$
…
𝑓-𝐹,-: 𝑁×𝐶,-Weight
Bank
𝑝KL − 𝑝,
𝑝KM − 𝑝,
…
𝑝KN − 𝑝,
𝑃,-: 𝑁×3
MLP1
Kernels
MLP2
⊚
⊚
SOP
𝑓./0
𝐹./0: 1×𝐶./0
‘𝑓#
‘𝑓$
…
‘𝑓-O𝐹,- : 𝑁×(𝐶,-× 𝐶./0) (b) PointConv
Kernels
⊙ SOP
𝑓./0
𝐹./0: 1×𝐶./0
⊚ kernel scores
Score
Net
𝑓#
𝑓$
…
𝑓-𝐹,-: 𝑁×𝐶,-(d) PAConv
Figure 1. Overview about convolutional sturctures of PointNet
[36], PointConv [52], KPConv [47] and our PAConv. It illustrates the differences of these point-based convolutions. SOP denotes symmetric operations, like MAX.
To tackle these difﬁculties, previous research can be coarsely cast into two categories. The ﬁrst line attempts to voxelize the 3D point clouds to form regular grids such that 3D grid convolutions can be adopted [33, 43, 39]. How-ever, important geometric information might be lost due to quantization, and voxels typically bring extra memory and computational costs [10, 7].
Another stream is to directly process point cloud data.
The pioneering work [36] proposes to learn the spatial encodings of points by combing Multi-Layer Perceptron (MLP) [13] and global aggregation as illustrated in Fig. 1 (a). Follow-up works [37, 38, 48, 20, 51] exploit local ag-gregation schemes to improve the network. Nonetheless, all the points are processed by the same MLP, which limits the capabilities in representing spatial-variant relationships.
Beyond MLP, most recent works design convolution-like operations on point clouds to exploit spatial correla-tions. To handle the irregularity of 3D point clouds, some works [58, 50, 29] propose to directly predict the kernel weights based on relative location information, which is 3173
further used to transform features just like 2D convolu-tions. One representative architecture [52] in this line of research is shown in Fig. 1 (b). Albeit conceptually effec-tive, the methods severely suffer from heavy computation and memory costs caused by spatial-variant kernel predic-tion in practice. The efﬁcient implementation also trade-offs its design ﬂexibility, leading to inferior performance.
Another group of works relate kernel weights with ﬁxed kernel points [2, 47, 32] and use a correlation (or interpo-lation) function to adjust the weight of kernels when they are applied to process point clouds. Fig. 1 (c) illustrates one representative architecture [47]. However, the hand-crafted combination of kernels may not be optimal and sufﬁcient to model the complicated 3D location variations.
In this paper, we present Position Adaptive Convolution, namely PAConv, which is a plug-and-play convolutional op-eration for deep representation learning on 3D point clouds.
PAConv (shown in Fig. 1 (d)) constructs its convolutional kernels by dynamically assembling basic weight matrices in Weight Bank. The assembling coefﬁcients are self-adaptively learned from relative point positions by MLPs (i.e. ScoreNet). Our PAConv is ﬂexible to model the com-plicated spatial variations and geometric structures of 3D point clouds while being efﬁcient. Speciﬁcally, instead of inferring kernels from point positions [52] in a brute-force way, PAConv bypasses the huge memory and com-putational burden via a dynamic kernel assembling strategy with ScoreNet. Besides, unlike kernel point methods [47], our PAConv gains ﬂexibility to model spatial variations in a data-driven manner and is much simpler without requiring sophisticated designs for kernel points.
We conduct extensive experiments on three challenging benchmarks on top of three generic network backbones.
Speciﬁcally, we adopt the simple MLP-based point net-works PointNet [36], PointNet++ [37] and DGCNN [51] as the backbones, and replace their MLPs with PAConv without changing other network conﬁgurations. With these simple backbones, our method still achieves the state-of-the-art performance on ModelNet40 [53] and considerably improves the baseline by 2.3% on ShapeNet Part [61] and 9.31% on S3DIS [1] with decent model efﬁciency. It’s also worth noting that recent point convolution methods often use complicated architectures and data augmentations tai-lored to their operators [47, 25, 30] for evaluation, making it difﬁcult to measure the progress made by the convolu-tional operator. Here, we adopt simple baselines and aim to minimize the inﬂuence of network architectures to better assess the performance gain from the operator – PAConv. 2.