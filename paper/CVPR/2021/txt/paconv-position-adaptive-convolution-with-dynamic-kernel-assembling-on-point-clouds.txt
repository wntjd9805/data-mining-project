Abstract
We introduce Position Adaptive Convolution (PAConv), a generic convolution operation for 3D point cloud process-ing. The key of PAConv is to construct the convolution ker-nel by dynamically assembling basic weight matrices stored in Weight Bank, where the coefï¬cients of these weight matri-ces are self-adaptively learned from point positions through
ScoreNet. In this way, the kernel is built in a data-driven manner, endowing PAConv with more ï¬‚exibility than 2D convolutions to better handle the irregular and unordered point cloud data. Besides, the complexity of the learning process is reduced by combining weight matrices instead of brutally predicting kernels from point positions.
Furthermore, different from the existing point convolu-tion operators whose network architectures are often heav-ily engineered, we integrate our PAConv into classical
MLP-based point cloud pipelines without changing net-work conï¬gurations. Even built on simple networks, our method still approaches or even surpasses the state-of-the-art models, and signiï¬cantly improves baseline perfor-mance on both classiï¬cation and segmentation tasks, yet with decent efï¬ciency. Thorough ablation studies and vi-sualizations are provided to understand PAConv. Code is released on https://github.com/CVMI-Lab/PAConv. 1.

Introduction
In recent years, the rise of 3D scanning technologies has been promoting numerous applications that rely on 3D point cloud data, e.g., autonomous driving, robotic manip-ulation and virtual reality [35, 40]. Thus, the approaches to effectively and efï¬ciently processing 3D point clouds are in critical needs. While remarkable advancements have been obtained in 3D point cloud processing with deep learn-ing [36, 37, 47, 25], it is yet a challenging task in view of the sparse, irregular and unordered structure of point clouds.
*M. Xu and R. Ding contribute equally.
â€ Corresponding author
ğ‘“#
ğ‘“$
â€¦
ğ‘“-ğ¹,-: ğ‘Ã—ğ¶,-MLP
SOP
ğ‘“./0
ğ¹./0: 1Ã—ğ¶./0 (a) PointNet weights
âŠš
Kernel
Points
Kernels position
ğ‘KL âˆ’ ğ‘,
ğ‘KM âˆ’ ğ‘,
â€¦
ğ‘KN âˆ’ ğ‘,
ğ‘ƒ,-: ğ‘Ã—3
âŠ™
SOP
ğ‘“./0
ğ¹./0: 1Ã—ğ¶./0 kernel scores
ğ‘“#
ğ‘“$
â€¦
ğ‘“-ğ¹,-: ğ‘Ã—ğ¶,-(c) KPConv
ğ‘KL âˆ’ ğ‘,
ğ‘KM âˆ’ ğ‘,
â€¦
ğ‘KN âˆ’ ğ‘,
ğ‘ƒ,-: ğ‘Ã—3
ğ‘“#
ğ‘“$
â€¦
ğ‘“-ğ¹,-: ğ‘Ã—ğ¶,-Weight
Bank
ğ‘KL âˆ’ ğ‘,
ğ‘KM âˆ’ ğ‘,
â€¦
ğ‘KN âˆ’ ğ‘,
ğ‘ƒ,-: ğ‘Ã—3
MLP1
Kernels
MLP2
âŠš
âŠš
SOP
ğ‘“./0
ğ¹./0: 1Ã—ğ¶./0
â€˜ğ‘“#
â€˜ğ‘“$
â€¦
â€˜ğ‘“-Oğ¹,- : ğ‘Ã—(ğ¶,-Ã— ğ¶./0) (b) PointConv
Kernels
âŠ™ SOP
ğ‘“./0
ğ¹./0: 1Ã—ğ¶./0
âŠš kernel scores
Score
Net
ğ‘“#
ğ‘“$
â€¦
ğ‘“-ğ¹,-: ğ‘Ã—ğ¶,-(d) PAConv
Figure 1. Overview about convolutional sturctures of PointNet
[36], PointConv [52], KPConv [47] and our PAConv. It illustrates the differences of these point-based convolutions. SOP denotes symmetric operations, like MAX.
To tackle these difï¬culties, previous research can be coarsely cast into two categories. The ï¬rst line attempts to voxelize the 3D point clouds to form regular grids such that 3D grid convolutions can be adopted [33, 43, 39]. How-ever, important geometric information might be lost due to quantization, and voxels typically bring extra memory and computational costs [10, 7].
Another stream is to directly process point cloud data.
The pioneering work [36] proposes to learn the spatial encodings of points by combing Multi-Layer Perceptron (MLP) [13] and global aggregation as illustrated in Fig. 1 (a). Follow-up works [37, 38, 48, 20, 51] exploit local ag-gregation schemes to improve the network. Nonetheless, all the points are processed by the same MLP, which limits the capabilities in representing spatial-variant relationships.
Beyond MLP, most recent works design convolution-like operations on point clouds to exploit spatial correla-tions. To handle the irregularity of 3D point clouds, some works [58, 50, 29] propose to directly predict the kernel weights based on relative location information, which is 3173
further used to transform features just like 2D convolu-tions. One representative architecture [52] in this line of research is shown in Fig. 1 (b). Albeit conceptually effec-tive, the methods severely suffer from heavy computation and memory costs caused by spatial-variant kernel predic-tion in practice. The efï¬cient implementation also trade-offs its design ï¬‚exibility, leading to inferior performance.
Another group of works relate kernel weights with ï¬xed kernel points [2, 47, 32] and use a correlation (or interpo-lation) function to adjust the weight of kernels when they are applied to process point clouds. Fig. 1 (c) illustrates one representative architecture [47]. However, the hand-crafted combination of kernels may not be optimal and sufï¬cient to model the complicated 3D location variations.
In this paper, we present Position Adaptive Convolution, namely PAConv, which is a plug-and-play convolutional op-eration for deep representation learning on 3D point clouds.
PAConv (shown in Fig. 1 (d)) constructs its convolutional kernels by dynamically assembling basic weight matrices in Weight Bank. The assembling coefï¬cients are self-adaptively learned from relative point positions by MLPs (i.e. ScoreNet). Our PAConv is ï¬‚exible to model the com-plicated spatial variations and geometric structures of 3D point clouds while being efï¬cient. Speciï¬cally, instead of inferring kernels from point positions [52] in a brute-force way, PAConv bypasses the huge memory and com-putational burden via a dynamic kernel assembling strategy with ScoreNet. Besides, unlike kernel point methods [47], our PAConv gains ï¬‚exibility to model spatial variations in a data-driven manner and is much simpler without requiring sophisticated designs for kernel points.
We conduct extensive experiments on three challenging benchmarks on top of three generic network backbones.
Speciï¬cally, we adopt the simple MLP-based point net-works PointNet [36], PointNet++ [37] and DGCNN [51] as the backbones, and replace their MLPs with PAConv without changing other network conï¬gurations. With these simple backbones, our method still achieves the state-of-the-art performance on ModelNet40 [53] and considerably improves the baseline by 2.3% on ShapeNet Part [61] and 9.31% on S3DIS [1] with decent model efï¬ciency. Itâ€™s also worth noting that recent point convolution methods often use complicated architectures and data augmentations tai-lored to their operators [47, 25, 30] for evaluation, making it difï¬cult to measure the progress made by the convolu-tional operator. Here, we adopt simple baselines and aim to minimize the inï¬‚uence of network architectures to better assess the performance gain from the operator â€“ PAConv. 2.