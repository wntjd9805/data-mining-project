Abstract
With the development of deep learning, neural networks tend to be deeper and larger to achieve good performance.
Trained models are more compute-intensive and memory-intensive, which lead to the big challenges on memory band-width, storage, latency, and throughput. In this paper, we propose the neural network compression method named minimally invasive surgery. Different from traditional mod-el compression and knowledge distillation methods, the pro-posed method refers to the minimally invasive surgery prin-ciple. It learns the principal features from a pair of dense and compressed models in a contrastive manner. It also op-timizes the neural networks to meet the speciﬁc hardware acceleration requirements. Through qualitative, quantita-tive, and ablation experiments, the proposed method shows a compelling performance, acceleration, and generaliza-tion in various tasks. 1.

Introduction
Deep learning technologies promote performance in var-ious applications like computer vision, natural language processing, autonomous driving, recommendation system, etc. The promising performance is achieved by deeper and larger neural networks. For example, the classical architec-tures in convolutional neural networks like VGG-19 [41],
ResNeXt-101 [49], SENet-154 [17] has 143.67, 83.46, and 115.09 million parameters, respectively. Google’s neu-ral machine translation model [48] has about 210 million parameters. The popular language understanding model
BERT [8] has about 340 million parameters. The deep learning recommendation model (DLRM) [29] has about 540 million parameters.
Neural networks with a huge amount of parameters have some shortcomings [56]. First of all, the large neural net-work is very compute-intensive.
In the network evalua-tion process, inference costs a lot of time even the net-work is running on dedicated acceleration hardware like G-PU [31] [32] or TPU [40]. We can enlarge the batch size to help improve the throughput of large neural networks. But the latency is still a problem. In fact, whenever we inter-act with phones or computers, we are very sensitive to the latency of the interaction. We don’t like to wait for an appli-cation to launch or for the web-page to load search results.
Moreover, we are especially sensitive in realtime interac-tions such as speech recognition and autonomous driving systems. Secondly, the large neural network is memory-intensive on mobile devices as well as in the server envi-ronment. Storage and loading the large neural network to compute inference results consume a large amount of ener-gy. Due to the limitations on application sizes, download time and launch speed, transfer and storage of large models is especially a challenge in the mobile environment.
Compressing the large neural network to a smaller ver-sion can bring beneﬁts to more efﬁcient computation, mem-ory, and energy consumptions. But at the meanwhile, how to keep the accuracy of the original neural network during compression needs to be investigated. A com-mon method of neural model compression is network prun-ing [12]: setting the weights with small magnitude values of a pre-trained network to zero and ﬁne-tuning the remain-ing weights to try to recover accuracy. For the aggressive network pruning tasks, knowledge distillation [15] is often used as the auxiliary method to improve the accuracy of the pruned network. A complementary method of neural mod-el compression is quantization. Changing fundamental data types adds the ability to accelerate the arithmetic operations, both in training [28] and inference processes [20].
In this work, we explore a neural network compression method based on the knowledge extracted from a pair of dense and compressed models. We named this method as
Minimally Invasive Surgery(MIS) because it is inspired by the principle and process of the real minimally invasive surgery. We apply the MIS technique to several networks and tasks to show generality in supervised and unsupervised learning. Our main contributions include:
• We prove that MIS has better performance than knowl-edge distillation and network compression methods.
• We provide the theoretical demonstration of MIS from information entropy and Bayes perspectives.
• We show that MIS technique can apply to various net-3589
works and tasks. It can work even without ground truth label info in an unsupervised learning style.
• MIS provides end-to-end compression for neural net-works to meet the hardware acceleration requirements. 2.