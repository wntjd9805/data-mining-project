Abstract
Generative models for 3D point clouds are extremely im-portant for scene/object reconstruction applications in au-tonomous driving and robotics. Despite recent success of deep learning-based representation learning, it remains a great challenge for deep neural networks to synthesize or reconstruct high-ﬁdelity point clouds, because of the dif-ﬁculties in 1) learning effective pointwise representations; and 2) generating realistic point clouds from complex dis-tributions. In this paper, we devise a dual-generators frame-work for point cloud generation, which generalizes vanilla generative adversarial learning framework in a progressive manner. Speciﬁcally, the ﬁrst generator aims to learn effec-tive point embeddings in a breadth-ﬁrst manner, while the second generator is used to reﬁne the generated point cloud based on a depth-ﬁrst point embedding to generate a robust and uniform point cloud. The proposed dual-generators framework thus is able to progressively learn effective point embeddings for accurate point cloud generation. Exper-imental results on a variety of object categories from the most popular point cloud generation dataset, ShapeNet, demonstrate the state-of-the-art performance of the pro-posed method for accurate point cloud generation. 1.

Introduction 3D data representation has received increasing attention from the community due to the rapid development of robot perception and scene/object reconstruction technologies, especially in autonomous driving, robotics, and augmented reality applications [29, 24]. As a simple yet effective 3D data format, 3D point clouds can capture much more so-phisticated surface geometry of different objects than voxel grids and are suitable for large-scale rendering [13]. How-ever, the surface-centric nature of point clouds also poses challenges to scanning the objects, such as the issues of occlusion and distance. Therefore, high-ﬁdelity 3D point cloud generation is of great importance for a variety of 3D applications. For example, the generated large-scale 3D
Figure 1. The proposed dual-generators framework for 3D point cloud generation. The overall pipeline mainly consists of two generators accompanying with a shared discriminator to generate point clouds in a progressive manner. data can be used in the learning of different 3D tasks, such as segmentation [29, 21], volumetric shape estimation [44], object detection [28], and scene understanding [26].
With the great success of deep learning in 2D image data, deep learning-based 3D data generation has attracted increasing attention, and a wide range of generative ap-proaches have been intensively investigated from different perspectives, such as image to point cloud [7, 16], im-age to voxel [42, 45], image to mesh [39, 41], image to
SDF [27], point cloud to voxel [51], and point cloud to point cloud [47, 48, 35, 14]. With a better prior of point clouds, a point cloud generation model can beneﬁt a va-riety of synthesis tasks such as reconstruction and super-resolution. Recently, several typical deep architectures such as auto-encoders [19] and generative adversarial networks (GANs) [11] have been very successful in learning effective representations and generating realistic samples from com-plex distributions. For example, GAN-based methods have been explored in transforming random latent codes into 3D point clouds [1, 37]. In this paper, we focus on the learn-ing of point cloud generation models under the generative adversarial learning framework: given a sparse, noisy, and non-uniform latent code, the target is then to generate point clouds that are dense, complete, and uniform, as a faithful representation of the underlying 3D object surface.
To generate high-quality point clouds, we progressively learn effective point embeddings and generate point clouds in a coarse-to-ﬁne manner. Speciﬁcally, the idea of pro-10266
gressive generation/reﬁnement has been widely used in very challenging 2D image and video tasks, such as image gen-eration, object detection, and semantic landmark localiza-tion. Motivated by this, we devise a dual-generators frame-work for point cloud generation. As shown in Fig. 1, a stack of two generators sequentially transform the input la-tent variables into a suitable 3D representation, such that the discriminator cannot distinguish between the ground truth and the generated point clouds. The overall dual-generators framework for point cloud generation contains two main steps: (1) the ﬁrst generator generates a dense point cloud to sketch the primitive geometry of the underlying object using an oversampling operation; and (2) the second gener-ator is an encoder-decoder network, which reﬁnes the point cloud of the ﬁrst step to obtain the ﬁnal high-ﬁdelity point cloud. The main contribution of the proposed method for point cloud generation is threefold: 1. We propose the dual-generators architecture to learn effective point embeddings and generate high-quality point clouds in a progressive manner; 2. The discriminator simultaneously considers two kinds of discriminative cues on point cloud generation: shapewise and pointwise, which facilitates the genera-tor to generate more realistic point clouds; 3. We devise an oversampling conception in regard to point cloud generation. Different from previous meth-ods, the proposed method generates point clouds via a dense-to-sparse process.
Extensive qualitative and quantitative experiments on the
ShapeNet dataset demonstrate the effectiveness of the pro-posed method as well as each individual components, pro-viding valuable insights on how to design effective point cloud generation models. 2.