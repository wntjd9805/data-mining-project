Abstract
A video prediction model that generalizes to diverse scenes would enable intelligent agents such as robots to perform a variety of tasks via planning with the model.
However, while existing video prediction models have produced promising results on small datasets, they suf-fer from severe underﬁtting when trained on large and diverse datasets. To address this underﬁtting challenge, we ﬁrst observe that the ability to train larger video prediction models is often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep hierar-chical latent variable models can produce higher quality predictions by capturing the multi-level stochasticity of future observations, but end-to-end optimization of such models is notably diﬃcult. Our key insight is that greedy and modular optimization of hierarchical autoencoders can simultaneously address both the memory constraints and the optimization challenges of large-scale video pre-diction. We introduce Greedy Hierarchical Variational
Autoencoders (GHVAEs), a method that learns high-ﬁdelity video predictions by greedily training each level of a hierarchical autoencoder. In comparison to state-of-the-art models, GHVAEs provide 17-55% gains in prediction performance on four video datasets, a 35-40% higher success rate on real robot tasks, and can improve performance monotonically by simply adding more modules. Visualization and more details are at https: // sites. google. com/ view/ ghvae . 1.

Introduction
A core aspect of intelligence is the ability to predict the future. Indeed, if equipped with an accurate video prediction model, an intelligent agent such as a robot may be able to perform a variety of tasks using raw pixel inputs. For example, algorithms such as visual foresight [1] can leverage an action-conditioned video
† Equal advising and ordered alphabetically.
Figure 1: Greedy Hierarchical Variational Autoen-coders (GHVAEs). Unlike traditional hierarchical varia-tional autoencoders (VAEs), a GHVAE model trains each encoder-decoder module greedily using the frozen weights of the previously-trained modules. Greedy training circum-vents ﬁtting the entire model into memory and enables larger models to be trained within the same GPU or TPU memory.
Further, greedy training improves the optimization stability of such a hierarchical model by breaking the bidirectional dependencies among individual latent variables. As a re-sult, given the current image, xt, GHVAE predicts a more accurate next image, ˆxt+1, than a hierarchical VAE. Each module is optimized sequentially, and all modules are used at test time. prediction model to plan a sequence of actions that ac-complish the desired task objective. Importantly, such video prediction models can in principle be trained with broad, unlabeled datasets, and building methods that can learn from large, diverse oﬄine data is a recipe that has seen substantial success in visual [2] and language [3] 12318
understanding. However, learning an accurate video prediction model from large and diverse data remains a signiﬁcant challenge. The future visual observations of the world are hierarchical [4], high-dimensional, and uncertain, demanding the model to accurately repre-sent the multi-level stochasticity of future pixels, which can include both low-level features (e.g. the texture of a table as it becomes unoccluded by an object) and higher-level attributes (e.g. how an object will move when touched), such as the top images in Fig. 1.
To capture the stochasticity of the future, prior works have proposed a variety of stochastic latent variable models [5, 6, 7]. While these methods generated rea-sonable predictions for relatively small video prediction datasets such as the BAIR robot pushing dataset [8], they suﬀer from severe underﬁtting in larger datasets in the face of practical GPU or TPU memory con-straints [9]. On the other hand, while hierarchical vari-ational autoencoders (VAEs) can in principle produce higher-quality predictions by capturing multiple levels of stochasticity, the bidirectional dependency between individual hierarchical latent variables (higher-level vari-ables inﬂuence the lower level and vice versa) potentially creates an unsolved problem of optimization instability as the number of hierarchical latent variables in the network increases [10, 11].
The key insight of this work is that greedy and mod-ular optimization of hierarchical autoencoders can si-multaneously address both the memory constraints and the optimization challenges of learning accurate large-scale video prediction. On one hand, by circumventing end-to-end training, greedy machine learning allows sequential training of sub-modules of the entire video prediction model, enabling much larger models to be learned within the same amount of GPU or TPU mem-ory. On the other hand, optimizing hierarchical VAEs in a greedy and modular fashion breaks the bidirectional dependency among individual latent variables. As a result, these variables can remain stable throughout the entire training process, resolving the typical instability of training deep hierarchical VAEs.
With this key insight, this paper introduces Greedy
Hierarchical VAEs (“GHVAEs” hereafter) (Fig. 1)– a set of local latent VAE modules that can be sequentially stacked and trained in a greedy, module-by-module fashion, leading to a deep hierarchical variational video prediction model that in practice admits a stable op-timization and in principle can scale to large video datasets. As evaluated in Section 4, GHVAEs outper-form state-of-the-art video prediction models by 17-55% in FVD score [12] on four diﬀerent datasets, and by 35-40% success rate on two real robotic manipulation tasks when used for planning. In addition, our empirical and theoretical analyses ﬁnd that GHVAE’s performance can improve monotonically as the number of GHVAE modules in the network increases. In summary, the core contribution of this work is the use of greedy machine learning to improve both the optimization stability and the memory eﬃciency of hierarchical VAEs, leading to signiﬁcant gains in both large-scale video prediction accuracy and real robotic task success rates. 2.