Abstract
Leveraging datasets available to learn a model with high generalization ability to unseen domains is important for computer vision, especially when the unseen domain’s anno-tated data are unavailable. We study a novel and practical problem of Open Domain Generalization (OpenDG), which learns from different source domains to achieve high perfor-mance on an unknown target domain, where the distributions and label sets of each individual source domain and the tar-get domain can be different. The problem can be generally applied to diverse source domains and widely applicable to real-world applications. We propose a Domain-Augmented
Meta-Learning framework to learn open-domain generaliz-able representations. We augment domains on both feature-level by a new Dirichlet mixup and label-level by distilled soft-labeling, which complements each domain with miss-ing classes and other domain knowledge. We conduct meta-learning over domains by designing new meta-learning tasks and losses to preserve domain unique knowledge and gener-alize knowledge across domains simultaneously. Experiment results on various multi-domain datasets demonstrate that the proposed Domain-Augmented Meta-Learning (DAML) outperforms prior methods for unseen domain recognition. 1.

Introduction
Deep convolutional neural networks have achieved state-of-the-art performance on wide ranges of computer vision applications with access to large-scale labeled data [23, 20, 39, 19]. However, for a target domain of interest, collecting enough training data is prohibitive. A practical solution is to generalize the model learned on the existing data to the un-seen domain. Since the existing source datasets for training may be from different resources, they may fall into different domains and hold different label sets, e.g., ImageNet [8] and DomainNet [36]. Besides, the target domain is totally unknown, and may also have a distribution shift and a differ-ent label set from the source domains. We call the valuable and challenging problem as Open Domain Generalization
*Equal contribution. (OpenDG), where we need to learn generalizable represen-tation from disparate source domains that generalizes well to any unseen target domain, as illustrated in Figure 1.
Unseen target domain
Known classes
Open classes
Source domain 1
Source domain 2
Figure 1. Open Domain Generalization (OpenDG). Different source domains hold disparate label sets. The goal is to learn generalizable representations from these source domains to help classify the known classes and detect open classes in the unseen target domain.
There are two key challenges for open domain general-ization. (1) Distinct source domains and the unseen target domain are drawn from different distributions with a large distribution shift. (2) The different label sets of distinct source domains cause some classes to exist in many more domains than other classes. The data of minor classes exist-ing in few domains are lacking in diversity. This makes the problem extremely difﬁcult for existing methods [25, 29].
To address the ﬁrst challenge, previous works minimize the distribution distance between domains by adversarial learning [34, 29], which successfully closes the domain gap when all source domains share the same label set. However, according to the second challenge, the different label sets between domains cause these distribution alignment methods to suffer from severe mismatch of classes. For the second challenge, a straightforward way is to manually sample data of minor classes existing in few domains, but the diversity in domains of the class is still limited. The generalization on the minor class is still inferior to other classes.
To generalize from arbitrary source domains to an unseen target domain, we propose a Domain-Augmented Meta-9624
Table 1. Comparison of the proposed generalization setting with the previous settings related to cross-domain learning. The columns list assumptions made by the problem settings. Note that more “✗” means the method needs less assumption and thus is more widely-applicable. We can observe that the proposed open domain generalization problem requires no assumptions on the label set, no target data, and no post-training on target data, which is the most general problem setting. S means source while T means target. Note that “Same between S&T Domains” means the union of all source domain label sets equals the target label set, i.e., whether there are open classes.
Post-Training on
Target Labeled Data
Target Data for Training
Problem Setting
Label Set
Same for S Domains
Same between S&T Domains
Labeled Data Unlabeled Data
Domain Adaptation [31, 32]
Domain Adaptation with Category Shift [35, 2, 51]
Multi-Source Domain Adaptation [55]
Multi-Source Domain Adaptation with Category Shift [50]
Domain Generalization [34]
Heterogeneous Domain Generalization [30]
The Proposed Open Domain Generalization
✓
✓
✓
✗
✓
✗
✗
✓
✗
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✓
✗
Learning (DAML) framework. To close the domain gap between disparate source domains, we avoid distribution matching but learn generalizable representations across do-mains by meta-learning. To overcome the disparate label sets of open domain generalization, we propose two domain augmentation methods at both feature-level and label-level.
At feature-level, we design a novel Dirichlet mixup (Dir-mixup) to compensate for the missing labels. At label-level, we utilize the soft-labeling distilled from other domains’ networks to transfer the knowledge of other domains to the current network. DAML learns a representation that embeds the knowledge of all source domains and is highly generaliz-able to the unseen target domain. We use the ensemble of all source domain network outputs as the ﬁnal prediction, which naturally calibrates the predictive uncertainty. In summary:
• We propose a new and practical problem: Open Do-main Generalization (OpenDG), which learns from arbitrary source domains with disparate distributions and label sets to generalize to an unseen target domain.
• We propose a principled Domain-Augmented Meta-Learning (DAML) framework to address open domain generalization. We augment each domain with novel
Dir-mixup and distilled soft-labeling to overcome the disparate label sets of source domains and conduct meta-learning across augmented domains to learn open-domain generalizable representations.
• Experiment results on several multi-domain datasets show that compared to previous generalization methods,
DAML achieves higher classiﬁcation accuracy on both known classes and open classes in an unseen target domain even with extremely diverse source domains. 2.