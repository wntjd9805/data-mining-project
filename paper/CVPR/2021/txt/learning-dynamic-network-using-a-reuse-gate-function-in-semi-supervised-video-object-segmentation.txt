Abstract
Current state-of-the-art approaches for Semi-supervised
Video Object Segmentation (Semi-VOS) propagates infor-mation from previous frames to generate segmentation mask for the current frame. This results in high-quality seg-mentation across challenging scenarios such as changes in appearance and occlusion. But it also leads to un-necessary computations for stationary or slow-moving ob-jects where the change across frames is minimal. In this work, we exploit this observation by using temporal in-formation to quickly identify frames with minimal change and skip the heavyweight mask generation step. To re-alize this efﬁciency, we propose a novel dynamic net-work that estimates change across frames and decides which path – computing a full network or reusing pre-vious frame’s feature – to choose depending on the ex-pected similarity. Experimental results show that our ap-proach signiﬁcantly improves inference speed without much accuracy degradation on challenging Semi-VOS datasets – DAVIS 16, DAVIS 17, and YouTube-VOS. Furthermore, our approach can be applied to multiple Semi-VOS meth-ods demonstrating its generality. The code is available in https://github.com/HYOJINPARK/Reuse VOS . 1.

Introduction
Semi-VOS tracks an object of interest across all the frames in a video given the ground truth mask of the ini-tial frame. VOS classiﬁes each pixel as belonging to back-ground or a tracked object. This task has wide applicability to many real-world use cases including autonomous driving, surveillance, video editing as well as to the emerging class of augmented reality/mixed reality devices. VOS is a chal-lenging task because it needs to distinguish the target object from other similar objects in the scene even as target’s ap-pearance changes over time as well as through occlusions.
A variety of methods have been proposed for solving
*Indicate same equal contribution as second authors
†This work was done when Seohyeong Jeong was with SNU (a) (b)
Figure 1. (a) Histogram of a range of IoU between the previous and the current ground truth masks. X-axis is a range of IoU and y-axis denotes frequency corresponding to the range of IoU. (b) FPS and accuracy (J&F ) comparison between the baseline model (FRTM
[30]) and ours on videos having high IoU between the previous and current ground truth masks. The proposed method preserves the original accuracy while improving the speed a lot. video object segmentation including online learning [30, 19], mask propagation [17, 3], and template matching [40, 22]. A common theme across most of these previous meth-ods is to use information from previous frames – either just the ﬁrst frame, some of the previous frames (ﬁrst and last being a popular option) or all the previous frames – to pro-duce high quality segmentation mask.
In this work, we ask a different question
Q: Can we use temporal information to identify when the object appearance and position has not changed across frames?
The motivation for doing so would be to skip much of the expensive computation needed to produce a high-quality mask for the current frame if that mask is almost the same as the mask we computed in the previous frame. Instead, we can produce current frame’s mask using a cheap model that just makes minor edits to the previous frame’s feature. As 8405
shown in Fig. 1(a), for a signiﬁcant fraction of the frames in the popular DAVIS and YouTube-VOS dataset, object masks are very similar to their previous frame’s masks (73.3% of consecutive frames in DAVIS 17 dataset have IoU greater than 0.7).
We build on the above observation by constructing a cheap temporal matching module to quickly quantify the similarity of the current frame with the previous frame. We use the similarity to gate the computation of high-quality mask for the current frame – if the similarity is high we reuse previous features with minor reﬁnements and avoid the expensive mask generation step. This allows us to avoid majority of computations for the current frame without compromising on accuracy.
Our approach compliments the existing video object seg-mentation approaches and to demonstrate its generality, we integrate our proposal into multiple prior video object seg-mentation models – FRTM [30] and TTVOS [25]. To the best of our knowledge, we are the ﬁrst to propose skipping computation of segmentation masks dynamically based on the object movement. We believe this is a signiﬁcant con-tribution that will enable high-quality video object segmen-tation models to run on mobile devices in real time with minimal battery impact.
We make the following contributions in this paper:
• We make the case for exploiting temporal information to skip mask generation for frames with little or no movement.
• We develop a general framework to skip mask com-putation consisting of sub-networks to estimate move-ment across frames, dynamic selection between pro-cessing full-network or reusing previous frame’s fea-ture for generating mask and a novel loss function to train this dynamic architecture.
• We evaluate our approach on multiple video object segmentation models (FRTM, TTVOS) as well as multiple challenging datasets (DAVIS 16, DAVIS 17,
Youtube-VOS) and demonstrate that we can save up to 47.5% computation and speedup FPS by 1.45× with minimal accuracy impact on DAVIS 16 (within around 0.4 % of baseline). 2.