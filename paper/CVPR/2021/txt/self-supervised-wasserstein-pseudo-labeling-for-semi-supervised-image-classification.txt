Abstract
The goal is to use Wasserstein metric to provide pseudo labels for the unlabeled images to train a Convolutional Neu-ral Networks (CNN) in a Semi-Supervised Learning (SSL) manner for the classiﬁcation task. The basic premise in our method is that the discrepancy between two discrete empiri-cal measures (e.g., clusters) which come from the same or similar distribution is expected to be less than the case where these measures come from completely two different distribu-tions. In our proposed method, we ﬁrst pre-train our CNN using a self-supervised learning method to make a cluster assumption on the unlabeled images. Next, inspired by the
Wasserstein metric which considers the geometry of the met-ric space to provide a natural notion of similarity between discrete empirical measures, we leverage it to cluster the unlabeled images and then match the clusters to their similar class of labeled images to provide a pseudo label for the data within each cluster. We have evaluated and compared our method with state-of-the-art SSL methods on the standard datasets to demonstrate its effectiveness. 1.

Introduction
CNN models have enabled breakthroughs in computer vi-sion and machine learning. However, training a CNN model relies on a large-scale annotated datasets which are usually tedious and labor intensive to collect [38]. Considering the vast amounts of unlabeled data available on the web, the idea to use the unlabeled data without human effort to annotate them has become very appealing [77, 11]. In this work, we tackle the challenge of deep SSL, the task of which is to use the unlabeled data in conjunction with the labeled data to train a better CNN classiﬁer. Conventionally, we are given a dataset D = Dl ∪ Du where the data in Dl are annotated by labels while the data in Du are not. The goal is to train a CNN classiﬁer on the known categories in Dl using the data in D. The test data involves only the classes that are present in Dl. The main challenge in SSL is to efﬁciently leverage the unlabeled Du to help learning on Dl. To make use of unlabeled data in the general setting of SSL challenge, there are two fundamental assumptions that must be taken into the consideration [11]: 1) We assume that labeled and unlabeled data come from the same or similar underlying distribution and there is no class distribution mismatch be-tween the labeled and unlabeled sets. 2) We presume that the underlying distribution of data has some structure. SSL algo-rithms considers at least one of these structural assumptions: consistency, manifold and cluster.
In consistency assumption [5, 8, 9, 60, 66], data samples in a small neighbourhood have the same class label. In cluster assumption [53, 12, 75, 62], data tends to construct discrete clusters in some geometric sense, and data within the same cluster are more probably to have the same class label. In manifold assumption [49, 59, 70], data lie in the neighbour-hood of a low-dimensional and well-deﬁned manifold which can be classiﬁed by meaningful distances on the manifold.
For all of these assumptions, it is important to consider the geometry of the data when designing an SSL method. For example, popular mean teacher [63] and π-model [39] lever-age different data augmentations approaches, each of which uses a different strategy to explore the local geometry of the labeled data for generating new data.
Recently, the theory of Optimal Transport (OT) [57, 64] is used as a tool in machine learning algorithms to consider the geometry of the data. For example, the Wasserstein distance in OT uses the geometry of the metric space to provide a meaningful distance between two distributions even if the supports of these distributions do not overlap. This property of the Wasserstein distance has made it useful and practical for many computer vision and machine learning applications such as clustering [18, 37, 31, 46], generative models [4, 27], loss function [22], semi-supervised learning [61, 23, 69, 43, 62], and domain adaptation [16, 36, 58, 67, 19, 40].
In this work, we propose a new SSL method based on the
Wasserstein metric which follows the general assumptions in SSL. Inspired by the effectiveness of Self-Supervised learning in many tasks including SSL [72, 35, 32], we ﬁrst 12267
pre-train our CNN using a self-supervised learning method,
MoCo v2 [28, 13, 14]. This process potentially enforces a clustered structure in the feature space for the unlabeled data which motivates us to perform a clustering on the feature of unlabeled data and then infer a pseudo-label for them.
Speciﬁcally, using the self-supervised pre-training on the
CNN, we make a cluster assumption about the unlabeled data in which clusters are identiﬁed by the Wasserstein barycen-ter of the unlabeled data. Then, we leverage the Wasserstein metric to match the clusters of unlabeled data to their most similar classes of labeled data to provide pseudo-labels for the unlabeled data. Here, the Wasserstein distance is a mea-sure of similarity between two sets of data points where one of them contains labeled data while the other one consists of unlabeled data. This matching is based on the assumption that the labeled and unlabeled data within the same class have the same or similar distribution. Therefore, we would expect that the similarity between two sets of data which come from the same or similar distribution is more than the case where these sets of data come from completely two different distributions. Finally, depending on the matching, we infer a pseudo label for the unlabeled data within each cluster, which are used along with the initially labeled data to train our CNN classiﬁer. 2.