Abstract
Pose-guided person image generation usually involves using paired source-target images to supervise the train-ing, which signiﬁcantly increases the data preparation ef-fort and limits the application of the models. To deal with this problem, we propose a novel multi-level statistics trans-fer model, which disentangles and transfers multi-level ap-pearance features from person images and merges them with pose features to reconstruct the source person images themselves. So that the source images can be used as super-vision for self-driven person image generation. Speciﬁcally, our model extracts multi-level features from the appearance encoder and learns the optimal appearance representation through attention mechanism and attributes statistics. Then we transfer them to a pose-guided generator for re-fusion of appearance and pose. Our approach allows for ﬂexible manipulation of person appearance and pose properties to perform pose transfer and clothes style transfer tasks. Ex-perimental results on the DeepFashion dataset demonstrate our method’s superiority compared with state-of-the-art su-pervised and unsupervised methods. In addition, our ap-proach also performs well in the wild. 1.

Introduction
Person image generation has been gaining attention in recent years, which aims to generate the person image as realistic as possible, and at the same time, to transfer the source person image to a target pose. It has great potential for applications, like virtual try-on, clothing texture editing, controllable person manipulation, and so on.
Recently, many researchers have contributed to this topic, with most of the work focusing on pose-guided per-son image generation [24, 35, 29, 45, 37, 18, 27, 31]. Many person image generation models [24, 35, 45, 18, 27, 31] use
* Corresponding author.
Pa
Ia
Pb
Ia
Self-driven
Source A Source B 
Target
Pose
Ours
Loss
MUST-GAN (a) Training
MUST-GAN (b) Inference of pose transfer (c) Results of pose transfer and clothes style transfer
Figure 1. Self-driven person image generation and visualization of pose transfer with clothes style transfer. Our model can be trained in a self-driven way without paired source-target images and ﬂex-ibly controls the appearance and pose attributes to achieve pose transfer and clothes style transfer in inference. The images in (c) show the generated results using this model for simultaneous pose and cloths style transfer. Source A is transferred to the target pose, and its clothes are replaced by source B’s. paired source-target images for supervised training. How-ever, the paired images require a lot of time and workforce to collect, limiting the usage scenarios of these models.
There are also some unsupervised person image generation methods [29, 3, 37], but the quality of their generated im-ages is not ﬁne.
In this paper, we propose a self-driven approach for person image generation without using any paired source-target images during training, as shown in Figure 1(a).
Speciﬁcally, we propose a novel multi-level statistics trans-fer model, which can disentangle and transfer multi-level appearance features for person images. The source image can be used as supervision information for person image generation without paired source-target training data. Our method allows for ﬂexible manipulation of pose and appear-13622
ance properties to achieve pose transfer and clothes style transfer, as shown in Figure 1(c).
Since there is no need to pair data during training for our approach, how to extract the person appearance features from input images and transfer them to a pose-guided gener-ator for reconstruction is the key to our proposed method. It is also important to prevent the model from learning trivial solutions that directly copy all input information to gener-ate output image. To deal with these problems, we propose a multi-level statistics transfer model, which extracts the multi-level features from the appearance encoder, and uti-lizes attention mechanism and attributes statistics to learn optimal representations of appearance features. Finally, the model fuses appearance features into a pose-guided gener-ator to reconstruct the source person image.
Speciﬁcally, our method uses an appearance encoder and a pose encoder to extract features from person im-age and pose image, respectively. Then we introduce the
MUST module to obtain multi-level statistics from the ap-pearance encoder and use the channel attention mechanism
[8] to learn the weights of each channel in multi-level fea-ture maps. After that, we calculate the statistics of feature maps and apply a multi-layer fully connected network to learn the corresponding relationship when the statistics are transferred to the generator branch. In addition we propose a multi-level statistics matching network for pose-guided generator, which is composed of statistics matching residual blocks with AdaIN [9] and learnable skip connection. This generator module can match the scale and channel number of multi-level statistics and generate realistic person image.
Our method is self-driven by source person images throughout the training process, and no paired source-target images are used. We compare our approach with state-of-the-art supervised and unsupervised methods. Both quanti-tative and qualitative comparisons prove the superiority of our method. Our model allows for ﬂexible manipulation of person appearance and pose properties to perform pose transfer and clothes style transfer tasks in the inference, and it also shows good performance in the wild.
To summarize, the main contributions of this paper are as follows:
• We propose a fully self-driven person image genera-tion method which requires no paired source-target im-ages for training.
• We propose a multi-level statistics transfer model that can effectively disentangle the rich appearance fea-tures from person images and allow for ﬂexible ma-nipulation of person appearance and pose properties.
• Our proposed model performs well compared with the state-of-the-art methods on pose transfer and clothes style transfer and also is tested in the wild for its po-tential applications. 2.