Abstract
An image is worth a thousand words, conveying infor-mation that goes beyond the mere visual content therein. In this paper, we study the intent behind social media images with an aim to analyze how visual information can facil-itate recognition of human intent. Towards this goal, we introduce an intent dataset, Intentonomy, comprising 14K images covering a wide range of everyday scenes. These images are manually annotated with 28 intent categories derived from a social psychology taxonomy. We then sys-tematically study whether, and to what extent, commonly used visual information, i.e., object and context, contribute to human motive understanding. Based on our ﬁndings, we conduct further study to quantify the effect of attending to object and context classes as well as textual information in the form of hashtags when training an intent classiﬁer. Our results quantitatively and qualitatively shed light on how vi-sual and textual information can produce observable effects when predicting intent.1 1.

Introduction
Why do we post images on social media platforms like
Facebook or Instagram? Are we expressing our feelings to friends and family? Are we seeking to entertain a wide au-dience? Or is it purely out of habit, or perhaps out of fear of missing out? Images on social media embody more than their explicit visual information, and they tend to be persua-sive in commercial ads and even manipulative in the context of political campaigns. Therefore, in the deluge of social media, understanding the intent behind images is critical, especially for tasks like ﬁghting fake news and misinforma-tion [16, 32] on social platforms.
However, understanding human intent behind images from a computer vision point of view is particularly chal-lenging, since it goes beyond standard visual recognition— predicting a set of stuff and thing categories that physi-cally exist in images such as objects [25, 12, 56, 51, 18] and scenes [34, 58, 67]. Additionally, it is a psychological task [41] inherent to human cognition and behavior. It is 1Intentonomy project page: github.com/kmnp/intentonomy harmony in love harmony (a) (b) annoying natural  beauty in love same class different classes
Figure 1.
Intent behind images: while (b) shows that the visual motif of holding hands aligns with the common intent of “in love”, (a) illustrates that similarity based on visual appearance alone of-ten would lead to an incorrect match with respect to intent. similar in spirit to visual commonsense reasoning [62, 43] to derive an answer conditioned on the objects and scenes present in images. In certain cases, intent can be inferred rather directly from representative objects and scenes. For example, a couple holding hands or making a heart symbol clearly have the same motive “in love”(Fig. 1(b)). However, the mapping from visual cue to intent is not always one-to-one. Fig. 1(a) shows that two images with completely different contents (a girl facing the ocean vs. a person relax-ing on a rocky surface, with face covered) can represent the same intent (“harmony”). This goes beyond the usual vari-ability (pose, color, illumination, and other nuisances) tradi-tionally addressed in object recognition pipelines [11, 37].
This brings us to the question: are objects and their image context sufﬁcient for recognizing the intent behind images?
In this paper, we introduce a human intent dataset, In-tentonomy, containing 14K images that are manually an-notated with 28 intent categories, organized in a hierar-chy by psychology experts. To investigate the intangible and subtle connection between visual content and intent, we present a systematic study to evaluate how the perfor-12986
mance of intent recognition changes as a a function of (a) the amount of object/context information; (b) the properties of object/context, including geometry, resolution and tex-ture. Our study suggests that (1) different intent categories rely on different sets of objects and scenes for recognition; (2) however, for some classes that we observed to have large intra-class variations, visual content provides negli-gible boost to the performance. Furthermore, our study also reveals that attending to relevant object and scene classes brings beneﬁcial effects for recognizing intent.
In light of this, we further study a multimodal frame-work for intent recognition. In particular, given an intent category, the framework localizes, in a weakly-supervised manner, salient regions in images that are important for rec-ognizing the class-of-interest. These discovered regions are further reinforced during training using a localization loss to guide the network to focus.
In addition, we leverage hashtags as a modality complementary to visual informa-tion. We demonstrate through extensive evaluations that properly ingesting visual and textual information helps to boost the performance of intent prediction signiﬁcantly.
Our work makes the following key contributions: (1) A novel dataset of 14,455 high-quality images, each labeled with one or more human perceived intent. This dataset, which we call Intentonomy, offers a total of 28 intent la-bels supported by a systematic social psychological taxon-omy [41] proposed by experts; (2) A systematic study to show how commonly used object and context information, as well as textual information, contribute to intent recogni-tion; (3) We introduce a framework with the help of weakly-supervised localization and an auxiliary hashtag modality that is able to narrow the gap between human and machine understanding of images. 2.