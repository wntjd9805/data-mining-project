Abstract
One of the key steps in Neural Architecture Search (NAS) is to estimate the performance of candidate architectures.
Existing methods either directly use the validation perfor-mance or learn a predictor to estimate the performance.
However, these methods can be either computationally ex-pensive or very inaccurate, which may severely affect the search efﬁciency and performance. Moreover, as it is very difﬁcult to annotate architectures with accurate performance on speciﬁc tasks, learning a promising performance predic-tor is often non-trivial due to the lack of labeled data. In this paper, we argue that it may not be necessary to esti-mate the absolute performance for NAS. On the contrary, we may need only to understand whether an architecture is better than a baseline one. However, how to exploit this comparison information as the reward and how to well use the limited labeled data remains two great challenges. In this paper, we propose a novel Contrastive Neural Architecture
Search (CTNAS) method which performs architecture search by taking the comparison results between architectures as the reward. Speciﬁcally, we design and learn a Neural Ar-chitecture Comparator (NAC) to compute the probability of candidate architectures being better than a baseline one.
Moreover, we present a baseline updating scheme to improve the baseline iteratively in a curriculum learning manner.
More critically, we theoretically show that learning NAC is equivalent to optimizing the ranking over architectures.
Extensive experiments in three search spaces demonstrate the superiority of our CTNAS over existing methods. 1.

Introduction
Deep neural networks (DNNs) have made signiﬁcant progress in various challenging tasks, including image clas-siﬁcation [11, 34, 37], face recognition [25, 40], and many
*Authors contributed equally.
†Corresponding author.
Figure 1. Comparison between the standard performance estimator (top) and our NAC (bottom). Unlike the standard estimator that predicts the absolute performance, our NAC takes two architectures as inputs and outputs the comparison probability of the sampled architectures being better than the baseline architecture. other areas [5, 6, 48, 49]. One of the key factors behind the progress lies in the innovation of effective neural architec-tures, such as ResNet [17] and MobileNet [19]. However, designing effective architectures is often labor-intensive and relies heavily on human expertise. Besides designing archi-tectures manually, Neural Architecture Search (NAS) seeks to design architectures automatically and outperforms the hand-crafted architectures in various tasks [1, 31].
Existing NAS methods seek to ﬁnd the optimal architec-ture by maximizing the expectation of the performance of the sampled architectures. Thus, how to estimate the per-formance of architectures is a key step in NAS. In practice, the searched architectures can be evaluated by the absolute performance provided by a supernet [3, 8, 31] or a predic-tor [22, 27]. However, using the absolute performance as the training signal may suffer from two limitations.
First, it is non-trivial to obtain stable and accurate ab-solute performance for all the candidate architectures. In practice, the performance of architectures may ﬂuctuate a lot under the training with different random seeds [21, 24].
Thus, there would be a large performance deviation if we evaluate the architecture only with a single value w.r.t. ab-9502
solute performance. As a result, using the absolute perfor-mance as the training signals may greatly hamper the search performance. Based on such signals, a randomly searched architecture may even outperform the architectures obtained by existing NAS methods [21, 47] in practice. Thus, how to obtain stable and accurate training signals to guide the search is an important problem.
Second, it is time-consuming to obtain the absolute per-formance from the supernet. Speciﬁcally, one can evaluate an architecture by feeding in the validation data on a spe-ciﬁc task to obtain the accuracy. However, given a large number of validation data, obtaining the validation accu-racy for candidate architectures via forward propagation can be computationally expensive. To address this issue, one can learn a regression model to predict the performance of architectures [22, 27]. However, the training of predictor models still requires plenty of architectures with the ground-truth performance as the training architecture data, which are very expensive to obtain in practice. Thus, how to efﬁ-ciently evaluate architectures with limited architectures with ground-truth performance becomes an important problem.
In this paper, we propose a Contrastive Neural Architec-ture Search (CTNAS) method that searches by architecture comparisons. To address the ﬁrst limitation, we devise a
Neural Architecture Comparator (NAC) to perform pairwise architecture comparisons. Unlike existing methods that rely on the absolute performance, we use the comparison results between the searched architectures and a baseline one as the reward (See Figure 1). In practice, the pairwise comparison results are easier to obtain and more stable than the absolute performance (See analysis in Section 3.1). To constantly
ﬁnd better architectures, we propose to improve the baseline gradually via a curriculum learning manner. To address the second limitation, the proposed NAC evaluates architectures via pairwise comparisons and avoid performing forward propagation on task data. Thus, the evaluation can be much more efﬁcient and greatly accelerate the search process (See
Table 5.2). Moreover, we also propose a data exploration method that exploits the architectures without ground-truth performance to improve the generalization ability of NAC to unseen architectures. In this way, we are able to effectively reduce the requirement of the training data for NAC.
Our contributions are summarized as follows.
• We propose a Contrastive Neural Architecture Search (CTNAS) method that searches for promising architec-tures by taking the comparison results between archi-tectures as the reward.
• To guarantee that CTNAS can constantly ﬁnd better ar-chitectures, we propose a curriculum updating scheme to gradually improve the baseline architecture. In this way, CTNAS has a more stable search process and thus greatly improves the search performance.
• Extensive experiments on three search spaces demon-strate that the searched architectures of our CTNAS outperform the architectures searched/designed by state-of-the-art methods. 2.