Abstract
A practical low-light enhancement solution must be com-putationally fast, memory-efﬁcient, and achieve a visually appealing restoration. Most of the existing methods tar-get restoration quality and thus compromise on speed and memory requirements, raising concerns about their real-world deployability. We propose a new deep learning ar-chitecture for extreme low-light single image restoration, which despite its fast & lightweight inference, produces a restoration that is perceptually at par with state-of-the-art computationally intense models. To achieve this, we do most of the processing in the higher scale-spaces, skipping the intermediate-scales wherever possible. Also unique to our model is the potential to process all the scale-spaces concurrently, offering an additional 30% speedup without compromising the restoration quality. Pre-ampliﬁcation of the dark raw-image is an important step in extreme low-light image enhancement. Most of the existing state of the art methods need GT exposure value to estimate the pre-ampliﬁcation factor, which is not practically feasible. Thus, we propose an ampliﬁer module that estimates the ampli-ﬁcation factor using only the input raw image and can be used “off-the-shelf” with pre-trained models without any
ﬁne-tuning. We show that our model can restore an ultra-high-deﬁnition 4K resolution image in just 1 sec. on a CPU and at 32 f ps on a GPU and yet maintain a competitive restoration quality. We also show that our proposed model, without any ﬁne-tuning, generalizes well to cameras not seen during training and to subsequent tasks such as object detection. 1.

Introduction
The Computer Vision community has witnessed excel-lent methods in the last two decades for low-light enhance-ment [32, 62, 20, 42, 10, 63, 61]. Especially noteworthy is
SID’s [10] recent success in restoring extreme low-light im-ages captured in near zero lux conditions (0.1−5 lux). Since then several deep learning architectures have been proposed for enhancing dark images [44, 18, 34, 65].
Dark input: 20× ampliﬁed for visualization Restored video + Object Detection
Figure 1. (One frame of the restored video) We can restore ultra-high-deﬁnition 4K resolution night-time images at 32 fps on a
GPU. This enables real-time visualization and subsequent infer-ence such as object detection.
In spite of these advances, such solutions may not be appropriate for real-world deployment due to their pro-hibitively high computational cost. In fact, a practical solu-tion must have low network latency, less memory footprint, fewer model parameters, smaller operations count and yet maintain a pleasing restoration. Conventionally, however, these qualities are mutually contradictory. This is especially true for extreme low-light restoration where colors are hard to recover and noise suppression is signiﬁcantly challeng-ing. Thus, the predominant trend is to forsake model speed and computational efﬁciency for better restoration, raising concerns for real-world deployment [9, 24, 26, 75, 60].
For example, two recent methods, SID [10] and SGN [18], require 562 GMAC (Giga multiply-accumulate) and 2474
GMAC ﬂoating-point operations, respectively, to restore a single 4K resolution raw image. This demands staggering levels of computations, which is unlikely to be available with the deployed edge devices and will cause enormous network latency. This may be frustrating for a casual user and impractical for critical tasks. Thus, our goal is to de-sign a network which achieves similar restoration quality but with drastically low operations.
We propose a deep learning architecture that extracts most of the information required for restoration from higher scale-spaces (which operate at lower resolution) and skip in-termediate scales, as shown in Fig. 2. At the highest scale, we use the widely used Residual Dense Block (RDB) [72], 3487
Input 1 1
Output
Input 1 1 Output 1/2 1/4 1/8 1/2 1/4 1/8 1/4 1/4 1/16 1/16 scale at   resolution
Process
ﬂow 
Feature
Concatenation (a) U-net style encoder-decoder (b) Proposed architecture
Figure 2. (a) Almost all methods rely on sequential processing. (b) We propose a parallel architecture for high inference speed. which uses non-linear rectiﬁcation after each convolutional layer. But the excessive use of non-linear rectiﬁcation has been criticized in recent works for decreasing the accu-racy, and the naive approach of limiting their usage has worked successfully [21, 56, 67, 71]. However, ours is al-ready a lightweight model with comparatively less number of convolutions and non-linear activations. Thus to throw away some of the non-linearity from our design deprives it to model complex functions and the restoration degrades.
Consequently, we modify the RDB to reduce the side ef-fects of negative clipping while maintaining sufﬁcient non-linearity.
Most architectures rely on sequential data processing by making the previous convolution block’s output as the input to the current convolution block. By this approach, the only way to limit network latency is to either accelerate hard-ware operations — a technology that has started to plateau, or reduce the network operations and compromise with the restoration quality. To break this conundrum, we imbue our design with architectural parallelism that allows concurrent processing of various scale-spaces (see Fig. 2), offering an additional 30% speedup but with no effect on the restoration quality.
SID [10], a landmark work on extreme low-light restora-tion, pre-ampliﬁed raw images for a successful recov-ery. But, the pre-ampliﬁcation required the knowledge of
Ground-Truth (GT) exposure even during inference (see
Eq. (3)). Although this is now a standard practice to re-cover dark raw images [34, 18, 44, 64], a real-world solution will be beneﬁted if the ampliﬁcation is computed using only the input raw image during inference. To this end, we pro-pose a new ampliﬁer module that directly uses the intensity values of the dark raw image to estimate the ampliﬁcation.
Thus, our ampliﬁer can automatically adapt to varying light levels, allowing it sometimes to achieve a perceptual score even better than the GT image. Finally, we show that our ampliﬁer can be augmented as-it-is to existing pre-trained models without any ﬁne-tuning for a successful restoration.
Our contributions. In summary, the contributions of this work are: (1) A new deep learning architecture for extreme low-light single image restoration, which compared to state-of-the-art [10, 18, 65] is 5 − 100× faster, 6 − 20× computa-tionally cheaper, uses 3 − 11× fewer model parameters and has MAC operations lower by an order. (2) A systematic strategy to enable architectural parallelism for additional speedup with no effect on restoration. (3) A modiﬁcation to the popular Residual Dense Block for better restoration. (4) A novel ampliﬁer module useful in a real-world sce-nario where the ampliﬁcation factor is estimated only from the input image.
It can be used directly with pre-trained models with no ﬁne-tuning. (5) Our model generalizes well to cameras not seen during training and also to subsequent tasks such as object detection without any ﬁne-tuning. Our code is available at the mohitlamba94.github.io/Restoring-Extremely-Dark-Images-In-Real-Time. 2.