Abstract
We describe an unsupervised method to detect and seg-ment portions of images of live scenes that, at some point in time, are seen moving as a coherent whole, which we refer to as objects. Our method ﬁrst partitions the motion ﬁeld by minimizing the mutual information between segments. Then, it uses the segments to learn object models that can be used for detection in a static image. Static and dynamic models are represented by deep neural networks trained jointly in a bootstrapping strategy, which enables extrapolation to pre-viously unseen objects. While the training process requires motion, the resulting object segmentation network can be used on either static images or videos at inference time. As the volume of seen videos grows, more and more objects are seen moving, priming their detection, which then serves as a regularizer for new objects, turning our method into unsupervised continual learning to segment objects. Our models are compared to the state of the art in both video object segmentation and salient object detection. In the six benchmark datasets tested, our models compare favorably even to those using pixel-level supervision, despite requiring no manual annotation. 1.

Introduction
The ability to segment the visual ﬁeld into coherently moving regions is among the traits most broadly shared among visual animals [2, 17, 18]. Even camouﬂaged, mov-ing objects are easy to spot (Fig. 1). During early develop-ment, humans spend considerable amounts of time interact-ing with a single moving object before losing interest [4], which may help prime object models and learn invariances
[59]. In contrast, the mature visual system can learn an ob-ject with a few static examples; objects do not need to move in order to be detected. This suggests using motion as a cue to bootstrap object models that can be used for detection in static images, with no need for explicit supervision. Ob-∗Also ArXiv:2008.07012, August 16, 2020
†Equal contributions. Our implementation and trained models are avail-able at: https://github.com/blai88/unsupervised_segmentation
Figure 1. Dynamic-static bootstrapping. (a) A lizard is hard to detect when still thanks to camouﬂage (top left). However, it is easy to see once it moves (bottom left; optical ﬂow visualized using the inset color wheel). Once learned the lizard, a never-before-seen sloth (b) can be easily detected in a static image, exploiting the static model learned from the moving lizard. jects that have never been seen moving are considered part of whatever background they are part of, at least until they move. As time goes by, more and more objects are seen moving, thus improving one’s ability to detect and segment objects in static images (Tab. 6). The more objects are boot-strapped in a bottom-up fashion, the easier they are to detect top-down, priming better motion discrimination, which in turn results in more accurate object detection. This synergis-tic loop gradually improves both the diversity of objects that can be detected and the accuracy of the detection.
We present a method to learn object segmentation using unlabeled videos, that at test time can be used for both mov-ing objects in videos and static objects in single images. The method uses a motion segmentation module that performs temporally consistent region separation. The resulting mo-tion segmentation primes a detector that operates on static images, and feeds back to the motion segmentation module, reinforcing it. We call this method Dynamic-Static Boot-strapping, or DyStaB. During training, the dynamic model minimizes the mutual information between partitions of the motion ﬁeld, while enforcing temporal consistency, which yields a state-of-the-art unsupervised motion segmentation method. Putative regions, along with their uncertainty ap-proximated during the computation of mutual information in the dynamic model, are used to train a static model. The 2826
forward/backward flow motion segmentation
Dynamic
Static u21 u12
I2
I1 m2 m1 dynamic model image pair masked flow current static prediction previous static  prediction adversarial inpainting static model
Figure 2. System overview. Dynamic: motion segmentation model described in Sec. 3.1; Static: object model described in Sec. 3.2. The training (“dynamic-static bootstrapping” in Sec. 3.3) iterates between these two models. In particular, once χ (static object model) is trained, it can be used as a “top-down” object prior to bias the motion segmentation network φ (dynamic model) in a feedback loop. ψ is the adversarial inpainting network that enforces minimal mutual information between motion ﬁeld partitions (two ψ’s are identical), and losses are represented using dashed lines/boxes. static model is then fed back as a regularizer in a top-down fashion, completing the synergistic loop.
One might argue that every pixel in the image back-projects to something in space that we could call an object.
However, the training data biases the process towards objects that exist at a scale that is detectable relative to the size of the pixel and the magnitude of the motion. For instance, individual leaves in an outdoor video might not be seen at a resolution that allows detecting them as independent objects.
Instead, the tree may be detected as moving coherently. So, the deﬁnition of objects is conditioned on the training data and, in particular, the scale and distribution of their size and relative motion.
With this caveat, our contribution is two-fold: First, a deep neural network trained with unlabeled videos that achieves state-of-the-art performance in motion segmenta-tion. It exploits mutual information separation and temporal consistency to identify candidate objects. Second, a deep neural network to perform object segmentation in single im-ages, bootstrapped from the ﬁrst. The static model uses as input both the output of the dynamic model and its uncer-tainty, to avoid self-learning. The two models are trained jointly in a synergistic loop. The resulting object segmenta-tion models outperform the state of the art by 10% in average precision in both video and static object segmentation across six standard benchmarks. Despite not requiring any manual annotation, our method also outperforms recent supervised ones by almost 5% on average. 2.