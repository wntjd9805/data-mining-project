Abstract 4D reconstruction and rendering of human activities is critical for immersive VR/AR experience. Recent advances still fail to recover ﬁne geometry and texture results with the level of detail present in the input images from sparse multi-view RGB cameras. In this paper, we propose Neural-HumanFVV, a real-time neural human performance capture and rendering system to generate both high-quality geom-etry and photo-realistic texture of human activities in ar-bitrary novel views. We propose a neural geometry gen-eration scheme with a hierarchical sampling strategy for real-time implicit geometry inference, as well as a novel neural blending scheme to generate high resolution (e.g., 1k) and photo-realistic texture results in the novel views.
Furthermore, we adopt neural normal blending to enhance geometry details and formulate our neural geometry and texture rendering into a multi-task learning framework. Ex-tensive experiments demonstrate the effectiveness of our ap-proach to achieve high-quality geometry and photo-realistic free view-point reconstruction for challenging human per-formances. 1.

Introduction
The rise of virtual and augmented reality (VR and AR) to present information in an immersive way has increased the demand of the 4D (3D spatial plus 1D time) content gener-ation. Further reconstructing human activities and provid-ing photo-realistic rendering from a free viewpoint conve-niently evolves as a cutting-edge yet bottleneck technique.
Early solutions [27, 28, 58, 9] require pre-scanned tem-plates or two to four orders of magnitude more time than is available for daily usages such as immersive tele-presence.
Recently, volumetric approaches have enabled real-time human performance reconstruction and eliminated the re-liance of a pre-scanned template model, by leveraging the
RGBD sensors and modern GPUs. The high-end solu-tions [12, 11, 23, 68] rely on multi-view studio setup to
Figure 1. Our NeuralHumanFVV achieves real-time and photo-realistic reconstruction results of human performance in novel views, using only 6 RGB cameras. achieve high-ﬁdelity reconstruction and rendering in a novel view but are expensive and difﬁcult to be deployed, while the low-end approaches [39, 53, 66, 72, 55] adopt the most handy monocular setup with a temporal fusion pipeline [40] but suffer from inherent self-occlusion constraint. More-over, these approaches above rely on depth cameras which are not as cheap and ubiquitous as color cameras.
The recent learning-based techniques enable robust hu-man attribute reconstruction [35, 48, 73, 29] using only
RGB input.
In particular, the approaches PIFu [48] and
PIFuHD [49] utilize pixel-aligned implicit function to re-construct clothed humans with ﬁne geometry details, while
MonoPort [29] further enables real-time inference in a novel view. However, these methods fail to generate compelling photo-realistic texture due to the reliance of implicit texture representation. On the other hand, neural rendering tech-niques [32, 7, 64, 38, 25, 46] bring huge potential for photo-realistic novel view synthesis. However, existing solutions rely on per-scene training or are hard to achieve real-time performance due to the heavy network and the complicated 3D representation. Moreover, few researchers explore to combine volumetric geometry modeling and photo-realistic novel view synthesis of human performance in a data-driven manner simultaneously, especially under the light-weight multi-RGB and real-time setting.
In this paper, we attack the above challenges and present 6226
NeuralHumanFVV – a real-time human neural volumetric rendering system using only light-weight and sparse RGB cameras surrounding the performer. As illustrated in Fig. 1, our novel approach generates both high-quality geometry and photo-realistic texture of human activities in arbitrary novel views, whilst still maintaining real-time computation and light-weight setup.
Generating such a human free-viewpoint video by com-bining volumetric geometry modeling and neural texture synthesis in a data-driven manner is non-trivial. Our key idea is to encode the local ﬁne-detailed geometry and tex-ture information of the adjacent input views into the novel target view, besides utilizing the inherent global informa-tion from our multi-view setting. To this end, we ﬁrst in-troduce a neural geometry generation scheme to implicitly reason about the underlying geometry in a novel view. With a hierarchical sampling strategy along the camera rays in a coarse-to-ﬁne manner, we achieve real-time detailed geom-etry inference. Then, based on the geometry proxy above, a novel neural blending scheme is proposed to map the input adjacent images into a photo-realistic texture output in the target view, through efﬁcient occlusion analysis and blend-ing weight learning. A boundary-aware upsampling strat-egy is further adopted to generate high resolution (e.g., 1k) novel view synthesis result without sacriﬁcing the real-time performance. Finally, we recover the normal information in the target view using the same neural blending strategy, which not only enhances the output ﬁne-grained geome-try details but also combines our neural geometry genera-tion and texturing blending into a multi-task learning frame-work. To summarize, our main contributions include:
• We present a real-time human performance rendering approach, which is the ﬁrst to reconstruct high quality geometry and photo-realistic texture results in a novel view using sparse multiple RGB cameras, achieving signiﬁcant superiority to existing state-of-the-arts.
• We propose an efﬁcient neural implicit generation scheme to recover ﬁne geometry details in the novel view via a hierarchical and coarse-to-ﬁne strategy.
• We propose a novel neural blending scheme to pro-vide high-resolution and photo-realistic texture result as well as normal result to further reﬁne the geometry. 2.