Abstract
Neural rendering techniques combining machine learn-ing with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance ﬁelds (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achiev-ing an unprecedented level of photorealism on the gener-ated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different im-ages.
In this paper we introduce D-NeRF, a method that extends neural radiance ﬁelds to a dynamic domain, allow-ing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canon-ical space and another that maps this canonical represen-tation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can ren-der novel images, controlling both the camera view and the time variable, and thus, the object movement. We demon-strate the effectiveness of our approach on scenes with ob-jects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be avail-able at [1]. 1.

Introduction
Rendering novel photo-realistic views of a scene from a sparse set of input images is necessary for many appli-cations in e.g. augmented reality, virtual reality, 3D con-tent production, games and the movie industry. Recent advances in the emerging ﬁeld of neural rendering, which learn scene representations encoding both geometry and appearance [31, 28, 24, 58, 34, 41], have achieved re-sults that largely surpass those of traditional Structure-10318
from-Motion [18, 48, 44], light-ﬁeld photography [22] and image-based rendering approaches [6]. For instance, the
Neural Radiance Fields (NeRF) [31] have shown that sim-ple multilayer perceptron networks can encode the mapping from 5D inputs (representing spatial locations (x, y, z) and camera views (θ, φ)) to emitted radiance values and volume density. This learned mapping allows then free-viewpoint rendering with extraordinary realism. Subsequent works have extended Neural Radiance Fields to images in the wild undergoing severe lighting changes [28] and have proposed sparse voxel ﬁelds for rapid inference [24]. Similar schemes have also been recently used for multi-view surface recon-struction [58] and learning surface light ﬁelds [35].
Nevertheless, all these approaches assume a static scene without moving objects. In this paper we relax this assump-tion and propose, to the best of our knowledge, the ﬁrst end-to-end neural rendering system that is applicable to dynamic scenes, made of both still and moving/deforming objects.
While there exist approaches for 4D view synthesis [3], our approach is different in that: 1) we only require a single camera; 2) we do not need to pre-compute a 3D reconstruc-tion; and 3) our approach can be trained end-to-end.
Our idea is to represent the input of our system with a continuous 6D function, which besides 3D location and it also considers the time component t. camera view,
Naively extending NeRF to learn a mapping from (x, y, z, t) to density and radiance does not produce satisfying results, as the temporal redundancy in the scene is not effectively exploited. Our observation is that objects can move and deform, but typically do not appear or disappear. Inspired by classical 3D scene ﬂow [51], the core idea to build our method, denoted Dynamic-NeRF (D-NeRF in short), is to decompose learning in two modules. The ﬁrst one learns a spatial mapping (x, y, z, t) → (∆x, ∆y, ∆z) between each point of the scene at time t and a canonical scene conﬁg-uration. The second module regresses the scene radiance emitted in each direction and volume density given the tu-ple (x + ∆x, y + ∆y, z + ∆z, θ, φ). Both mappings are learned with deep fully connected networks without convo-lutional layers. The learned model then allows to synthesize novel images, providing control in the continuum (θ, φ, t) of the camera views and time component, or equivalently, the dynamic state of the scene (see Fig. 1).
We thoroughly evaluate D-NeRF on scenes undergoing very different types of deformation, from articulated mo-tion to humans performing complex body poses. We show that by decomposing learning into a canonical scene and scene ﬂow D-NeRF is able to render high-quality images while controlling both camera view and time components.
As a side-product, our method is also able to produce com-plete 3D meshes that capture the time-varying geometry and which remarkably are obtained by observing the scene un-der a speciﬁc deformation only from one single viewpoint. 2.