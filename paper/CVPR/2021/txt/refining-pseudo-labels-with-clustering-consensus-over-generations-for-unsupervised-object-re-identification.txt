Abstract (cid:43)(cid:104)(cid:73)(cid:107)(cid:71)(cid:93)(cid:3)(cid:89)(cid:60)(cid:68)(cid:73)(cid:89)(cid:104)(cid:3)(cid:60)(cid:106)(cid:3) (cid:43)(cid:104)(cid:73)(cid:107)(cid:71)(cid:93)(cid:3)(cid:89)(cid:60)(cid:68)(cid:73)(cid:89)(cid:104)(cid:3)(cid:60)(cid:106)
Unsupervised object re-identiﬁcation targets at learning discriminative representations for object retrieval without any annotations. Clustering-based methods [27, 46, 10] conduct training with the generated pseudo labels and cur-rently dominate this research direction. However, they still suffer from the issue of pseudo label noise. To tackle the challenge, we propose to properly estimate pseudo label similarities between consecutive training generations with clustering consensus and reﬁne pseudo labels with tem-porally propagated and ensembled pseudo labels. To the best of our knowledge, this is the ﬁrst attempt to leverage the spirit of temporal ensembling [25] to improve classi-ﬁcation with dynamically changing classes over genera-tions. The proposed pseudo label reﬁnery strategy is sim-ple yet effective and can be seamlessly integrated into ex-isting clustering-based unsupervised re-identiﬁcation meth-ods. With our proposed approach, state-of-the-art method
[10] can be further boosted with up to 8.8% mAP improve-ments on the challenging MSMT17 [39] dataset. 1.

Introduction
Recent years witnessed the remarkable progresses of employing unsupervised representation learning in various downstream visual recognition tasks, such as image classi-ﬁcation [1, 13, 14, 20], object detection [23, 22, 17, 33], and object re-identiﬁcation (re-ID) [27, 28, 37, 46, 10]. Ob-ject re-ID aims at retrieving objects of interest in large-scale gallery images given an object’s query images. The task of unsupervised object re-ID further requires learning dis-criminative representations to properly model inter-/intra-identity afﬁnities without any annotations, which is a more
*The ﬁrst two authors contribute equally. (cid:48)(cid:103)(cid:60)(cid:81)(cid:91)(cid:81)(cid:91)(cid:79) (cid:104)(cid:60)(cid:90)(cid:100)(cid:89)(cid:73)(cid:104) (cid:43)(cid:104)(cid:73)(cid:107)(cid:71)(cid:93)(cid:159)(cid:89)(cid:60)(cid:68)(cid:73)(cid:89) (cid:69)(cid:93)(cid:91)(cid:78)(cid:81)(cid:71)(cid:73)(cid:91)(cid:69)(cid:73)(cid:104) (cid:43)(cid:104)(cid:73)(cid:107)(cid:71)(cid:93)(cid:3)(cid:89)(cid:60)(cid:68)(cid:73)(cid:89) (cid:100)(cid:103)(cid:93)(cid:100)(cid:60)(cid:79)(cid:60)(cid:106)(cid:81)(cid:93)(cid:91) (cid:47)(cid:107)(cid:100)(cid:73)(cid:103)(cid:112)(cid:81)(cid:104)(cid:73)(cid:71)(cid:3)(cid:68)(cid:115) (cid:100)(cid:104)(cid:73)(cid:107)(cid:71)(cid:93)(cid:3)(cid:89)(cid:60)(cid:68)(cid:73)(cid:89)(cid:104) (cid:13)(cid:89)(cid:107)(cid:104)(cid:106)(cid:73)(cid:103)(cid:81)(cid:91)(cid:79)(cid:3) (cid:13)(cid:93)(cid:91)(cid:104)(cid:73)(cid:91)(cid:104)(cid:107)(cid:104) (cid:47)(cid:107)(cid:100)(cid:73)(cid:103)(cid:112)(cid:81)(cid:104)(cid:73)(cid:71)(cid:3)(cid:68)(cid:115)(cid:3) (cid:103)(cid:73)(cid:78)(cid:81)(cid:91)(cid:73)(cid:71)(cid:3)(cid:100)(cid:104)(cid:73)(cid:107)(cid:71)(cid:93)(cid:3)(cid:89)(cid:60)(cid:68)(cid:73)(cid:89)(cid:104) (cid:34)(cid:73)(cid:106)(cid:113)(cid:93)(cid:103)(cid:88) (cid:34)(cid:73)(cid:106)(cid:113)(cid:93)(cid:103)(cid:88)
Figure 1: Illustration of the proposed Reﬁning pseudo La-bel with Clustering Consensus (RLCC) framework. Hard pseudo labels or soft pseudo-label conﬁdences from the pre-vious generation t − 1 can be temporally propagated to gen-eration t to effectively reﬁne the pseudo labels at generation t to signiﬁcantly improve the performance of unsupervised object re-identiﬁcation. practical setup in real-world applications.
Pseudo-label-based methods with a clustering-based la-bel generation scheme were found effective in state-of-the-art semi-supervised/unsupervised object re-ID approaches
[27, 46, 10, 9, 47, 8, 49]. An iterative and alternative two-stage pipeline is adopted in each training generation (epoch), i.e., creating pseudo labels and training the net-work with the generated pseudo labels. Although multi-ple attempts on improving the quality of the pseudo la-bels have been investigated, the training is still substantially hindered by the inevitable label noise, showing noticeable performance gaps compared to the oracle experiments with ground-truth identities [10]. We argue that properly reﬁning the pseudo labels is at the core of further improving unsu-pervised re-ID algorithms.
To tackle the challenge, we propose a simple yet ef-fective pseudo label reﬁnery strategy following the similar 13436
spirit of temporal ensembling [25], i.e., the pseudo labels from the past generation (epoch) also carry valuable super-vision information and can help mitigate the pseudo label noise by smoothing the pseudo label variations.
The temporal ensembling technique has been widely adopted in semi-supervised learning [25, 35, 9] and self-supervised learning [15, 12] tasks.
It aims at generating more robust supervision signals via aggregating models or predictions with a moving average strategy over previous generations (epochs). However, it is non-trivial to improve the pseudo labels in unsupervised object re-ID tasks with off-the-shelf label temporal ensembling methods [25, 35], since they assume that the class deﬁnitions of the recogni-tion tasks remain ﬁxed over training generations. In con-trast, pseudo labels in different training generations for un-supervised re-ID vary much as the pseudo labels are always updated after each generation.
Towards this end, we introduce Reﬁning pseudo Labels with Clustering Consensus over consecutive training gener-ations (RLCC). Speciﬁcally, we estimate the pseudo-label similarities over two consecutive generations with an In-tersection over Union (IoU) criterion over the sample-label assignments, where a larger value indicates higher consen-sus between two pseudo classes in two consecutive genera-tions. To exploit the valuable temporal knowledge encoded by the pseudo labels, we propose to propagate hard or soft pseudo labels from the previous generation to the current generation. The propagation is conducted via a random walk over the pseudo labels, guided by the cross-generation pseudo-label similarities. Given the temporally propagated labels, the noisy pseudo labels at the current generation can be properly reﬁned via a momentum averaging formula-tion. Our proposed reﬁned pseudo labels can be readily in-tegrated into existing clustering-based unsupervised re-ID approaches [27, 46, 10] with marginal modiﬁcations, i.e. replacing the conventional hard pseudo labels with the pro-posed temporally propagated and ensembled soft pseudo la-bels.
Our contributions can be summarized into three-fold. (1)
We introduce to leverage the spirit of temporal ensembling to regularize the noisy pseudo labels in unsupervised object re-ID. Note that existing temporal ensembling techniques
[25, 35] are all designed for close-set classiﬁcation mod-els, which are not applicable in our task. (2) We propose a simple yet effective pseudo label reﬁnery strategy: reﬁning pseudo labels with clustering consensus over training gen-erations (epochs). Our proposed strategy is well compati-ble with existing pseudo-label-based methods [27, 46, 10] and leads to further improvements on the already high-performance baseline. (3) Our method outperforms state-of-the-arts on multiple benchmarks for unsupervised ob-ject re-ID, surpassing state-of-the-art unsupervised method
SpCL [10] with up to 8.8% mAP improvements. 2.