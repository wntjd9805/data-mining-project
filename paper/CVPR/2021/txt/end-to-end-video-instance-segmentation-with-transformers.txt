Abstract
Video instance segmentation (VIS) is the task that re-quires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typ-ically develop sophisticated pipelines to tackle this task.
Here, we propose a new video instance segmentation frame-work built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence de-coding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly.
At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspec-tive of similarity learning, thus considerably simplifying the overall pipeline and is signiﬁcantly different from existing approaches.
Without bells and whistles, VisTR achieves the highest
*Corresponding author. speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the ﬁrst time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy.
We hope that VisTR can motivate future research for more video understanding tasks.
Code is available at: https://git.io/VisTR 1.

Introduction
Instance segmentation is one of the fundamental tasks in computer vision. While signiﬁcant progress has been wit-nessed in instance segmentation of images [5, 9, 22, 25–27], much less effort was spent on segmenting instances in videos. Here we propose a new video instance segmentation framework built upon Transformers. Video instance seg-mentation (VIS), recently proposed in [30], requires one to simultaneously classify, segment and track object instances of interest in a video sequence. It is more challenging in that one needs to perform instance segmentation for each indi-8741
vidual frame and at the same time to establish data associa-tion of instances across consecutive frames, a.k.a., tracking.
State-of-the-art methods typically develop sophisticated pipelines to tackle this task. Top-down approaches [2, 30] follow the tracking-by-detection paradigm, relying heav-ily on image-level instance segmentation models [6, 9] and complex human-designed rules to associate the instances.
Bottom-up approaches [1] separate object instances by clus-tering learned pixel embeddings. Due to heavy reliance on the dense prediction quality, these methods often need mul-tiple steps to generate the masks iteratively, which makes them slow. Thus, a simple, end-to-end trainable VIS frame-work is highly desirable.
Here, we take a deeper look at the video instance seg-mentation task. Video frames contain richer information than single images such as motion patterns and temporal consistency of instances, offering useful cues for instance segmentation, and classiﬁcation. At the same time, the bet-ter learned instance features can help tracking of instances.
In essence, the instance segmentation and instance tracking are both concerned with similarity learning: instance seg-mentation is to learn the pixel-level similarity and instance tracking is to learn the similarity between instances. Thus, it is natural to solve these two sub-tasks in a single frame-work and beneﬁt each other. Here we aim to develop such an end-to-end VIS framework. The framework needs to be simple and achieves strong performance without whistles and bells. To this end, we propose to employ the Trans-formers [23].
Importantly, for the ﬁrst time we demon-strate that, as the Transformers provide building blocks, it enables one to design a simple and clean framework for
VIS, and possibly for a much wider range of video process-ing tasks in computer vision. Thus potentially, it is possible to unify most vision tasks of different input modalities— such as image, video and point clouds processing—into the
Transformer framework. Transformers are widely used for sequence to sequence learning in NLP [23], and start to show promises in vision [4, 8]. Transformers are capable of modeling long-range dependencies, and thus can be nat-urally applied to video for learning temporal information across multiple frames. In particular, the core mechanism of Transformers, self-attention, is designed to learn and up-date the features based on all pairwise similarities between them. The above characteristics of Transformers make them great candidates for the VIS task.
In this paper, we propose the Video Instance Segmen-tation TRansformer (VisTR), which views the VIS task as a parallel sequence decoding/prediction problem. Given a video clip that consists of multiple image frames as input, the VisTR outputs the sequence of masks for each instance in the video in order directly. The output sequence for each instance is referred to as instance sequence in this paper.
The overall VisTR pipeline is illustrated in Fig. 1. In the ﬁrst stage, given a sequence of video frames, a standard CNN module extracts features of individual image frames, then the multiple image features are concatenated in the frame order to form the clip-level feature sequence. In the second stage, the Transformer takes the clip-level feature sequence as input, and outputs a sequence of object predictions in or-der. In Fig. 1 same shapes represent predictions for the same image, and the same colors represent the same instance of different images. The sequence of predictions follow the order of input images, and the predictions of each image follows the same instance order. Thus, instance tracking is achieved seamlessly and naturally in the same framework of instance segmentation.
To achieve this goal, there are two main challenges: 1) how to maintain the order of outputs and 2) how to obtain the mask sequence for each instance out of the Transformer network. Correspondingly, we introduce the instance se-quence matching strategy and the instance sequence seg-mentation module. The instance sequence matching per-forms bipartite graph matching between the output instance sequence and the ground-truth instance sequence, and su-pervises the sequence as a whole. Thus, the order can be maintained directly. The instance sequence segmentation accumulates the mask features for each instance across mul-tiple frames through self-attention and segments the mask sequence for each instance through 3D convolutions.
Our main contributions are summarized as follows.
• We propose a new video instance segmentation frame-work built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel se-quence decoding/prediction problem. The framework is signiﬁcantly different from existing approaches, considerably simplifying the overall pipeline.
• VisTR solves the VIS from a new perspective of sim-ilarity learning. Instance segmentation is to learn the pixel-level similarity and instance tracking is to learn the similarity between instances. Thus, instance track-ing is achieved seamlessly and naturally in the same framework of instance segmentation.
• The key to the success of VisTR is a new strategy for instance sequence matching and segmentation, which is tailored for our framework. This carefully-designed strategy enables us to supervise and segment instances at the sequence level as a whole.
• VisTR achieves strong results on the YouTube-VIS dataset, achieving 38.6% in mask mAP at the speed of 57.7 FPS , which is the best and fastest among methods that use a single model. 2.