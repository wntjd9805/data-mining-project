Abstract
No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem for social and streaming media applications. Efﬁcient and ac-curate video quality predictors are needed to monitor and guide the processing of billions of shared, often imper-fect, user-generated content (UGC). Unfortunately, current
NR models are limited in their prediction capabilities on real-world, “in-the-wild” UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 38, 811 real-world distorted videos and 116, 433 space-time localized video patches (‘v-patches’), and 5.5M human perceptual quality annotations. Using this, we created two unique
NR-VQA models: (a) a local-to-global region-based NR
VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a ﬁrst-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. The entire dataset and prediction models are freely available at https://live.ece.utexas.edu/ research.php.. 1.

Introduction
User-generated content (UGC) and video streaming have exploded on social media platforms such as Facebook, In-stagram, YouTube, and TikTok, each supporting millions and billions of users [63]. It has been estimated that each day, about 4 billion video views occur on Facebook [60] and 1 billion hours are viewed on YouTube [62]. Given the tremendous prevalence of Internet videos, it would be of great value to measure and control the quality of UGC videos, both on capture devices and at social media sites where they are uploaded, encoded, processed, and analyzed.
Full-reference (FR) video quality assessment (VQA) models perceptually compare quality against pristine videos, while no-reference (NR) models involve no such
∗†Equal contribution
Fig. 1: Modeling local to global perceptual quality: From each video, we ex-tract three spatio-temporal video patches (Sec. 3.1), which along with their subjective scores, are fed to the proposed video quality model. By integrating spatial (2D) and spatio-temporal (3D) quality-sensitive features, our model learns spatial and temporal distortions, and can robustly predict both global and local quality, a temporal quality series, as well as space-time quality maps (Sec. 5.2). Best viewed in color. comparison. Thus, NR video quality monitoring could transform the processing and interpretation of videos on smartphones, social media, telemedicine, surveillance, and vision-guided robotics, in ways that FR models are un-able to. Unfortunately, measuring video quality without a pristine reference is very hard. Hence, though FR mod-els are successfully deployed at the largest scales [78], NR video quality prediction on UGC content remains largely unsolved, for several reasons.
First, UGC video distortions arise from highly diverse capture conditions, unsteady hands of content creators, im-perfect camera devices, processing and editing artifacts, frame rates, compression and transmission artifacts, and the way they are perceived by viewers. Inter-mixing of distor-tions is common, creating complex, composite distortions that are harder to model in videos. Moreover, it is well-known that the technical degree of distortion (e.g. amount of blur, blocking, or noise) does not correlate well with perceptual quality [75], because of neurophysiological pro-cesses that induce masking [47]. Indeed, equal amounts of distortions may very differently affect the quality of two dif-ferent videos [52].
Second, most existing video quality resources are too 14019
small and unrepresentative of complex real-world distor-tions [12, 56, 30, 72, 73, 68, 69]. While three publicly avail-able databases of authentically distorted UGC videos are available [23, 57, 74], they are far too small to train modern, data-hungry deep neural networks. What is needed are very large databases of videos corrupted by real-world distor-tions, subjectively rated by large numbers of human view-ers. However, conducting large-scale psychometric studies is much harder and time-consuming (per video) than stan-dard object/action classiﬁcation tasks.
Finally, although a few NR algorithms achieve reason-able performance on small databases [42, 6, 28, 4, 35, 65, 37, 10], most of them fail to account for the complex space-time distortions common to UGC videos. UGC distortions are often transient (e.g., frame drops, focus changes, and transmission glitches) and yet may signiﬁcantly impact the overall perceived quality of a video [55]. Most existing models are frame-based, or use sample frame differences, and cannot capture diverse temporal impairments.
We have made recent progress towards addressing these challenges, by learning to model the relationships that ex-ist between local and global spatio-temporal distortions and perceptual quality. We built a large-scale public UGC video dataset of unprecedented size, comprising full videos and three kinds of spatio-temporal video patches (Fig. 1), and we conducted an online visual psychometric study to gather large numbers of human subjective quality scores on them.
This unique data collection allowed us to successfully learn to exploit interactions between local and global video qual-ity perception and to create algorithms that accurately pre-dict video quality and space-time quality maps. We sum-marize our contributions below:
• We built the largest video quality database in exis-tence. We sampled hundreds of thousands of open-source
Internet UGC digital videos to match the feature distribu-tions of social media UGC videos. Our ﬁnal collection includes 38, 811 real-world videos of diverse sizes, con-tents, and distortions, 26 times larger than the most re-cent UGC dataset [74]. We also extracted three types of v-patches from each video, yielding 116, 433 space-time video patches (“v-patches”) in total (Sec. 3.1).
• We conducted the largest subjective video quality study to date. Our ﬁnal dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6, 300 subjects, more than 9 times larger than any prior UGC video quality study (Sec. 3.2).
• We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D video features using PaQ2PiQ [76], in parallel with 3D features using ResNet3D [20]. The 2D and 3D fea-tures feed a time series regressor [13] that learns to accu-rately predict both global video quality, as well as local space-time v-patch quality, by exploiting the relations be-tween them. This new model, which we call Patch VQ (PVQ) achieves top performance on the new database as well as on smaller “in-the-wild” databases [57, 23], with-out ﬁnetuning (Secs. 4.1 and 5.3).
• We also create another unique prediction model that predicts ﬁrst-of-a-kind space-time maps of video qual-ity by learning global-to-local quality relationships. This second model, called the PVQ Mapper, helps localize, vi-sualize, and act on video distortions (Sec. 5.2). 2.