Abstract
ImageNet)
In contrast
This paper presents a detection-aware pre-training (DAP) approach, which leverages only weakly-labeled for pre-classiﬁcation-style datasets (e.g., training, but is speciﬁcally tailored to beneﬁt object de-tection tasks. to the widely used image classiﬁcation-based pre-training (e.g., on ImageNet), which does not include any location-related training tasks, we transform a classiﬁcation dataset into a detection dataset through a weakly supervised object localization method based on Class Activation Maps to directly pre-train a de-tector, making the pre-trained model location-aware and capable of predicting bounding boxes. We show that DAP can outperform the traditional classiﬁcation pre-training in terms of both sample efﬁciency and convergence speed in downstream detection tasks including VOC and COCO. In particular, DAP boosts the detection accuracy by a large margin when the number of examples in the downstream task is small. 1.

Introduction
Pre-training and ﬁne-tuning have been a dominant paradigm for deep learning-based object recognition in computer vision [14, 10, 29, 17]. In such a paradigm, neural network weights are typically pre-trained on a large dataset (e.g., through ImageNet [8] classiﬁcation training), and then transferred to initialize models in downstream tasks. Pre-training can presumably help improve downstream tasks in multiple ways. The low-level convolutional ﬁlters, such as edge, shape, and texture ﬁlters, are already well-learned in pre-training [42]. The pre-trained network is also capable of providing meaningful semantic representations. For ex-ample, in the case of ImageNet classiﬁcation pre-training, since the number of categories is large (1000 classes), the downstream object categories might be related to a subset of the pre-training categories and can reuse the pre-trained feature representations. Pre-training may also help the opti-mizer avoid bad local minima by providing a better initial-Figure 1. The DAP workﬂow. It consists of 4 steps: (1) Classiﬁer pre-training on a weak supervision dataset, (2) Pseudo box gener-ation by WSOL (e.g., through CAM as illustrated), (3) Detector pre-training with the generated pseudo boxes, (4) Downstream de-tection tasks. The traditional classiﬁcation pre-training and ﬁne-tuning directly go from Step (1) to (4) at the bottom, while DAP inserts the additional Steps (2) and (3) at the top. In both cases, the pre-trained weights are used to initialize the downstream models.
DAP gives the model a chance to learn how to perform explicit localization, and is able to pre-train detection-related components while classiﬁcation pre-training cannot, such as the FPN, RPN, and box regressor in a Faster RCNN detector. ization point than a completely random initialization [12].
Therefore, ﬁne-tuning would only require a relatively small number of gradient steps to achieve competitive accuracy.
However, the empirical gain for object detection brought by classiﬁcation pre-training is diminishing with succes-sively larger pre-training datasets, ranging from ImageNet-1M, ImageNet-5k [17], to ImageNet-21k (14M), JFT-300M
[36], and billion-scale Instagram images [25]. Meanwhile,
[16] shows that training from random initialization (i.e., from scratch) can work equally well with sufﬁciently large data (COCO [24]) and a sufﬁciently long training time, making the effect of classiﬁcation pre-training questionable.
We conjecture that the diminishing gain of classiﬁca-4537
tion pre-training for object detection is due to several mis-matches between the pre-training and the ﬁne-tuning tasks.
Firstly, the task objectives of classiﬁcation and detection are different. Existing classiﬁcation pre-training is typically unaware of downstream detection tasks. The pre-training adopts a single whole-image classiﬁcation loss which en-courages translation and scale-invariant features, while the detection ﬁne-tuning involves several different classiﬁca-tion and regression losses which are sensitive to object lo-cations and scales. Secondly, the data distributions are mis-aligned. The localization information required by detec-tion is not explicitly made available in classiﬁcation pre-training. Thirdly, the architectures are misaligned. The net-work used in pre-training is a bare backbone network such as a ResNet model [18] followed by an average pooling and a linear classiﬁcation layer. In contrast, the network in an object detector contains various additional architectural components such as the Region Proposal Network (RPN)
[29], the Feature Pyramid Network (FPN) [22], the ROI classiﬁcation heads and the bounding box regression heads
[29], etc. These unique architectural components in detec-tors are not pre-trained and are instead randomly initialized in detection ﬁne-tuning, which could be sub-optimal.
Aiming at bridging the gap between pre-training with classiﬁcation data and detection ﬁne-tuning, we introduce a Detection-Aware Pre-training (DAP) procedure as shown in Figure 1. There are two desired properties that are nec-essary to pre-train a detector: (1) Classiﬁcation should be done locally rather than globally; (2) Features should be capable of predicting bounding boxes and can be easily adapted to any desired object categories after ﬁne-tuning.
With the desired properties in mind, DAP starts from pre-training a classiﬁer on the classiﬁcation data, and extracts the localization information with existing tools developed in Weakly Supervised Object Localization (WSOL) based on Class Activation Maps (CAM) [47]. The next step is to treat the localized instances as pseudo bounding boxes to pre-train a detection model. Finally, the pre-trained weights are used for model initialization in downstream detection tasks such as VOC [13] and COCO [24]. DAP enables the pre-training of (almost) the entire detector architecture and offers the model the opportunity to adapt its representa-tion to perform localization explicitly. Our problem setting focuses on leveraging the weak image-level supervision in classiﬁcation-style data for pre-training (ImageNet-1M and
ImageNet-14M) [8], therefore makes a head-to-head com-parison to the traditional classiﬁcation pre-training. Note that our setting is different from unsupervised pre-training
[15, 4, 5] which is only based on unlabeled images, and is different from fully-supervised detection pre-training [32] which is hard to scale.
Comprehensive experiments demonstrate that adding the simple lightweight DAP steps in-between the traditional classiﬁcation pre-training and ﬁne-tuning stages yields con-sistent gains across different downstream detection tasks.
The improvement is especially signiﬁcant in the low-data regime. This is particularly useful in practice to save the an-notation effort. In the full-data setting, DAP leads to faster convergence than classiﬁcation pre-training and also im-proves the ﬁnal detection accuracy by a decent margin. Our work suggests that a carefully designed detection-speciﬁc pre-training strategy with classiﬁcation-style data can still beneﬁt object detection. We believe that this work makes the ﬁrst attempt towards detection-aware pre-training with weak supervision. 2.