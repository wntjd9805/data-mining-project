Abstract
Many computer vision tasks address the problem of scene understanding and are naturally interrelated e.g. ob-ject classiﬁcation, detection, scene segmentation, depth es-timation, etc. We show that we can leverage the inher-ent relationships among collections of tasks, as they are trained jointly, supervising each other through their known relationships via consistency losses. Furthermore, explic-itly utilizing the relationships between tasks allows improv-ing their performance while dramatically reducing the need for labeled data, and allows training with additional unsu-pervised or simulated data. We demonstrate a distributed joint training algorithm with task-level parallelism, which affords a high degree of asynchronicity and robustness. This allows learning across multiple tasks, or with large amounts of input data, at scale. We demonstrate our framework on subsets of the following collection of tasks: depth and nor-mal prediction, semantic segmentation, 3D motion and ego-motion estimation, and object tracking and 3D detection in point clouds. We observe improved performance across these tasks, especially in the low-label regime. 1.

Introduction
Many tasks in computer vision, such as depth and surface normal estimation, ﬂow prediction, pose estimation, seman-tic segmentation, or classiﬁcation, are inherently related as they describe the surrounding scene along with its dynam-ics. While solving for each of these tasks may require spe-cialized methods, most tasks are connected by the under-lying physics observed in the real world. A considerable amount of research aims to reveal the relationships between tasks [60, 5, 15, 56, 13, 54, 55], but only a few methods exploit these fundamental relationships. Some approaches rely on the unparalleled performance of deep networks to learn explicit mappings between tasks [55, 54]. However, while training tasks pairs leverages their relationships, it
∗Work done while at Robotics at Google
Figure 1. Illustration of our framework for the collective training of multiple tasks with a consistency loss (two tasks are shown). Each task is performed by a separate network, and trained on a its own dataset and a shared unlabeled mediator dataset. The consistency loss is imposed for samples from the mediator dataset. may lead to inconsistencies across multiple tasks, e.g. [55], and points to the alternative of training tasks jointly.
Multi-task learning targets the problem of training mul-tiple tasks jointly. Common to many approaches is a shared feature-extractor component with multiple “heads” that per-form separate tasks [15, 56, 13]. Training multiple tasks to-gether increases the coherency between them and – in some setups – also enables their self-supervision [60, 5]. How-ever, the joint training also has a few disadvantages. For one, a single model for multiple tasks is difﬁcult to design, maintain and improve, as any changes in the training data, losses, or hyperparameters associated with one of the tasks, also affects all others. Secondly, different modalities come with different architectures, which are difﬁcult to merge into a single model. For example, point clouds require sparse processing [43], while tensored images use CNNs. Thirdly, it can become intractable to process a single model – built to perform multiple tasks – on a single compute node.
In this paper we introduce a novel approach for dis-tributed collective training that explicitly leverages the in-herent connections between multiple tasks (Fig. 1). Con-8700
sistency losses are designed for related tasks, intended to enforce their logical or geometric structure. For example, given the two tasks of predicting surface normals and depth from RGB images, the consistency loss between them is based on the analytical relation between them – normals can be computed from the derivatives of a depth map. We show here that explicitly enforcing consistency between tasks can improve their individual performance, while their collec-tive training also establishes the correspondence among the tasks, which – in turn – leads to a more sound visual un-derstanding of the whole scene. We term the framework
‘Taskology’, as it connects tasks by their physical and logi-cal constraints.
Using consistency losses to collectively train tasks en-ables a modular design for training neural networks, which offers three major advantages: We train structurally dif-ferent tasks with entirely separate networks that are better suited for each individual task. This is also advantageous from a design, development, and maintainability point of view; each component can be replaced or improved sepa-rately from all others. Secondly, we beneﬁt from unsuper-vised or partially labeled data. For example, many datasets are labeled for either segmentation or scene depth; with consistency losses, we can use partially labeled datasets for training both tasks, where the consistency losses are active for the unlabeled portion of the data (Fig. 1). Fi-nally, we train multiple complex models jointly and asyn-chronously in a distributed manner, on different compute nodes. Each network is processed on a separate machine, while their training is tied together through consistency losses. The communication between collectively trained networks – through their predictions – is asynchronous.
Our experiments show that networks for different tasks can be trained with stale predictions from their peers; we do not observe a decrease in performance for up to 20 minute (∼ 2000 steps) old predictions. Unlike existing methods for distributed training [37] that mostly rely on data- or model parallelism to split training across multiple compute nodes, our framework separates training at task level; each model is trained independently and asynchronously from all other models, while all models are coherently trained together.
Distributed training allows scalability in multi-tasks learn-ing, both in the number of tasks and datasets sizes.
To summarize, the contributions are: (1) we present a framework that enables a modular design for training neu-ral networks by separating tasks into modules that can be combined and then trained collectively; (2) we propose con-sistency losses for coherently training multiple tasks jointly, which allows improving their overall performance; (3) we demonstrate distributed training of multiple tasks, which al-lows for scalability; (4) we show that collectively trained tasks supervise themselves, which reduces the need for la-beled data, and can leverage unsupervised or simulated data. 2.