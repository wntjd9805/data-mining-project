Abstract
Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detec-tors have difﬁculties enumerating all orientations or ﬁtting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, ﬁrst detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it reﬁnes these estimates using additional point features on the object. In CenterPoint, 3D object tracking simpliﬁes to greedy closest-point matching.
The resulting detection and tracking algorithm is simple, efﬁcient, and effective. CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, Center-Point outperforms all previous single model methods by a large margin and ranks ﬁrst among all Lidar-only submis-sions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint. 1.

Introduction
Strong 3D perception is a core ingredient in many state-of-the-art driving systems [1, 48]. Compared to the well-studied 2D detection problem, 3D detection on point-clouds offers a series of interesting challenges: First, point-clouds are sparse, and most parts of 3D objects are without mea-surements [22]. Second, the resulting output is a three-dimensional box that is often not well aligned with any global coordinate frame. Third, 3D objects come in a wide range of sizes, shapes, and aspect ratios, e.g., in the traf-ﬁc domain, bicycles are near planer, buses and limousines elongated, and pedestrians tall. These marked differences be-tween 2D and 3D detection made a transfer of ideas between a) Anchor-based t=1 b) Center-based t=1 c) Anchor-based t=2 d) Center-based t=2
Figure 1: We present a center-based framework to represent, detect and track objects. Previous anchor-based methods use axis-aligned anchors with respect to ego-vehicle coor-dinate. When the vehicle is driving on straight roads, both anchor-based (red boxes) and our center-based (red points) method can detect objects accurately (top). However, during a safety-critical left turn (bottom), anchor-based methods have difﬁculty ﬁtting axis-aligned bounding boxes to rotated objects. Our center-based model accurately detects objects through rotationally invariant points. Best viewed in color. the two domains harder [43, 45, 58]. An axis-aligned 2D box [16, 17] is a poor proxy of a free-form 3D object. One solution might be to classify a different template (anchor) for each object orientation [56, 57], but this unnecessarily in-creases the computational burden and may introduce a large number of potential false-positive detections. We argue that the main underlying challenge in linking up the 2D and 3D domains lies in this representation of objects.
In this paper, we show how representing objects as points (Figure 1) greatly simpliﬁes 3D recognition. Our two-stage 3D detector, CenterPoint, ﬁnds centers of ob-11784
jects and their properties using a keypoint detector [62], a second-stage reﬁnes all estimates. Speciﬁcally, CenterPoint uses a standard Lidar-based backbone network, i.e., Voxel-Net [54, 64] or PointPillars [27], to build a representation of the input point-cloud. It then ﬂattens this representation into an overhead map-view and uses a standard image-based keypoint detector to ﬁnd object centers [62]. For each de-tected center, it regresses to all other object properties such as 3D size, orientation, and velocity from a point-feature at the center location. Furthermore, we use a light-weighted second stage to reﬁne the object locations. This second stage extracts point-features at the 3D centers of each face of the estimated objects 3D bounding box. It recovers the lost local geometric information due to striding and a limited receptive
ﬁeld and brings a decent performance boost with minor cost.
The center-based representation has several key advan-tages: First, unlike bounding boxes, points have no intrinsic orientation. This dramatically reduces the object detector’s search space and allows the backbone to learn the rotational invariance and equivalence of objects. Second, a center-based representation simpliﬁes downstream tasks such as tracking. If objects are points, tracklets are paths in space and time. CenterPoint predicts the relative offset (veloc-ity) of objects between consecutive frames and links objects greedily. Thirdly, point-based feature extraction enables us to design an effective two-stage reﬁnement module that is much faster than the previous approaches [42–44].
We test our models on two popular large datasets: Waymo
Open [46], and nuScenes [6]. We show that a simple switch from the box representation to center-based representation yields a 3-4 mAP increase in 3D detection under different backbones [27, 54, 64, 65]. Two-stage reﬁnement further brings an additional 2 mAP boost with a small (< 10%) computation overhead. Our best single model achieves 71.8 and 66.4 level 2 mAPH for vehicle and pedestrian detec-tion on Waymo, 58.0 mAP and 65.5 NDS on nuScenes, outperforming all published methods on both datasets. No-tably, in NeurIPS 2020 nuScenes 3D Detection challenge,
CenterPoint forms the basis of 3 of the top 4 winning en-tries. For 3D tracking, our model performs at 63.8 AMOTA outperforming the prior state-of-the-art by 8.8 AMOTA on nuScenes. On Waymo 3D tracking benchmark, our model achieves 59.4 and 56.6 level 2 MOTA for vehicle and pedes-trian tracking, respectively, surpassing previous methods by up to 50%. Our end-to-end 3D detection and tracking system runs near real-time, with 11 FPS on Waymo and 16 FPS on nuScenes. 2.