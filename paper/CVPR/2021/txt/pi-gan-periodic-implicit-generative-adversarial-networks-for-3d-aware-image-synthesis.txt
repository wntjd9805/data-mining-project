Abstract
We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how-ever fall short in two ways: ﬁrst, they may lack an under-lying 3D representation or rely on view-inconsistent ren-dering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Ad-versarial Networks (π-GAN or pi-GAN), for high-quality 3D-aware image synthesis. π-GAN leverages neural repre-sentations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance
ﬁelds. The proposed approach obtains state-of-the-art re-sults for 3D-aware image synthesis with multiple real and synthetic datasets. 1.

Introduction
Generative Adversarial Networks (GANs) are capable of generating high-resolution, photorealistic images [24, 25, 26]. However, these GANs are often conﬁned to two di-mensions because of a lack of photorealistic 3D training data; therefore, they cannot support tasks such as synthe-sizing multiple views of a single object. 3D-aware image synthesis offers to learn neural scene representations unsu-pervised from 2D images. The learned representations can be used to render view-consistent images from new camera poses [41, 54, 18].
Current solutions have achieved impressive results in de-coupling identity from structure, allowing for the render-ing of a single instance from multiple poses. Nevertheless, these approaches either lack multi-view consistency or ﬁne detail. Voxel-based approaches [18] generate interpretable,
∗These authors contributed equally to this work. Project page: https://marcoamonteiro.github.io/pi-GAN-website/
Figure 1: Selected examples synthesized by π-GAN with
CelebA [32] and Cats [67] datasets. true 3D representations, but are limited by computational complexity to low resolutions and coarse detail. Convolu-tional approaches with deep-voxel representations [41, 42] take advantage of recent progress in convolutional GANs and can create ﬁnely detailed images. However, because of their reliance on learned black-box rendering, these ap-proaches fail to guarantee multi-view consistency and can-not easily generalize beyond the training distribution of camera poses at inference. Recent approaches that lever-age neural implicit representations [54] incorporate repre-sentations based on neural network–parameterized radiance
ﬁelds that ensure multi-view consistency and explicit cam-era control. Nonetheless, the implicit representations used by these approaches have so far been unable to effectively express ﬁne details, leading to compromised image quality.
We propose Periodic Implicit Generative Adversar-5799
ial Networks (π-GAN), a generative adversarial approach to unsupervised 3D representation learning from images.
Given input noise, π-GAN conditions an implicit radi-ance ﬁeld represented by a SIREN network [56], a fully-connected network with periodic activation functions. The conditioned radiance ﬁeld maps a 3D location and 2D viewing direction to a view-dependent radiance and view-independent volume density [22, 35]. Using a differentiable volume rendering approach that relies on classical volume rendering techniques, we can render the radiance ﬁeld from arbitrary camera poses [39].
π-GAN improves upon the image quality and view-consistency of previous approaches to 3D-aware image syn-thesis, as shown in Figure 1. The proposed method utilizes a
SIREN-based neural radiance ﬁeld representation to encour-age multi-view consistency, allowing rendering from a wide range of camera poses and providing an interpretable 3D structure. The SIREN implicit scene representation, which makes use of periodic activation functions, is more capa-ble than ReLU implicit representations at representing ﬁne details and enables π-GAN to render sharper images than previous works.
Beyond introducing π-GAN, we make two additional technical contributions. First, we observe that while ex-isting work has conditioned ReLU-based radiance ﬁelds through concatenation of the input noise to one or more layers, conditioning-by-concatenation is sub-optimal for implicit neural representations with period activations (SIRENs). We instead propose to use a mapping network to condition layers in the SIREN through feature-wise lin-ear modulation (FiLM) [48, 9]. This contribution can more generally be applied to SIREN architectures beyond
GANs. Second, we introduce a progressive growing strat-egy, inspired by previous successes in 2D convolutional
GANs [24], to accelerate training and offset the increased computational complexity of 3D GANs.
We obtain state-of-the-art 3D-aware image synthesis re-sults on real-world and synthetic datasets, demonstrate that our method generalizes to new viewpoints, and has appli-cations to novel view synthesis. Moreover, the 5D spatio-angular radiance ﬁeld representation used by π-GAN allows for an interpretable 3D proxy shape to be extracted via the marching cubes algorithm [33]. While these proxy shapes may not be as high quality as those estimated by single-view shape reconstruction methods tailored to this task [65], they often end up resulting in a fair approximation, all without explicit supervision.
Our contributions in this paper include the following:
• We introduce SIREN-based implicit GANs as a viable alternative to convolution GAN architectures.
• We propose a mapping network with FiLM condition-ing and a progressive growing discriminator as key components to achieve high quality results with our novel SIREN-based implicit GAN.
• We demonstrate view consistency and explicit camera control as advantages of approaches that rely on an un-derlying neural radiance ﬁeld representation and clas-sical rendering.
• We achieve state-of-the-art results on 3D-aware im-age synthesis from unsupervised 2D data on the
CelebA [32], Cats [67], and CARLA [8, 54] datasets. 2.