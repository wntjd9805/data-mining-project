Abstract
Knowledge transfer from large teacher models to smaller student models has recently been studied for metric learning, focusing on ﬁne-grained classiﬁcation. In this work, focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we intro-duce asymmetric metric learning, a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task.
We systematically evaluate different teacher and student models, metric learning and knowledge transfer loss func-tions on the new asymmetric testing as well as the standard symmetric testing task, where database and queries are rep-resented by the same model. We ﬁnd that plain regression is surprisingly effective compared to more complex knowledge transfer mechanisms, working best in asymmetric testing. In-terestingly, our asymmetric metric learning approach works best in symmetric testing, allowing the student to even out-perform the teacher.
Our implementation is publicly available,1 including trained student models for all loss functions and all pairs of teacher/student models.2 1.

Introduction
Originating in metric learning, loss functions based on pairwise distances or similarities [18, 71, 46, 72, 5] are paramount in representation learning. Their power is most notable in category-level tasks where classes at inference are different than classes at learning, for instance ﬁne-grained classiﬁcation [46, 72], few-shot learning [69, 62] local de-scriptor learning [19] and instance-level retrieval [16, 53].
There are different ways to use them without supervi-sion [31, 80, 6] and indeed, they form the basis for modern 1https://github.com/budnikm/aml 2This work was supported by Conseil Régional de Bretagne and
Rennes Métropole (Images & Réseaux AAP-PME-2017 grant MobilAI).
It was performed using HPC resources of GENCI–IDRIS (Grant 2019-AD011011245). unsupervised representation learning [44, 21, 8].
Powerful representations come traditionally with pow-erful network models [22, 28], which are expensive. The search for resource-efﬁcient architectures has lead to the de-sign of lightweight networks for mobile devices [26, 58, 84], neural architecture search [48, 41] and model scaling [64].
Training of small networks may be facilitated by knowledge transfer from larger networks [23]. However, both network design and knowledge transfer are commonly performed on classiﬁcation tasks, using standard cross-entropy.
Focusing on ﬁne-grained classiﬁcation and retrieval, sev-eral recent methods have extended metric learning loss func-tions to allow for knowledge transfer from teacher to student models [9, 38, 82, 47]. However, two questions are in order: (a) since transferring a representation from one model to another is inherently a continuous task, can’t we just use regression? (b) apart from knowledge transfer, is the original metric learning task still relevant and what is a simple way to combine the two?
In this work, we focus on the task of instance-level image retrieval [49, 52], which is at the core of metric learning in the sense of using pairwise distances or similarities. In its most well-known form [16, 53], the task is supervised, but the supervision is originating from automated data analysis rather than humans. As such, apart from noisy, supervision is often incomplete, in the sense that although class labels per example may exist, not all pairs of examples of the same class are labeled. Hence, one has to work with pairs rather than examples, unlike e.g. face recognition [11].
Our work is motivated by the scenario where a database (gallery) of images is represented and indexed according to a large model, while queries are captured from mobile devices, where a smaller model is the only option. In such scenario, rather than re-indexing the entire database, it is preferable to adapt different smaller models for different end-user devices.
In this case, knowledge transfer from the large (teacher) to the small (student) model is not just helping, but the student should really learn to map inputs to the same representation space. We call this task asymmetric testing.
More importantly, even if we consider the standard sym-metric testing task, where both queries and database exam-ples are represented by the same model at inference, we 8228
introduce a novel paradigm of using asymmetric representa-tions at training, as a knowledge transfer mechanism. We call this paradigm asymmetric metric learning. By repre-senting anchors by the student and positives/negatives by the teacher, one can apply any metric learning loss function.
This achieves both metric learning and knowledge trans-fer, without resorting to a linear combination of two loss functions.
In summary, we make the following contributions:
• We study the problem of knowledge transfer from a teacher to a student model for the ﬁrst time in pair-based metric learning for instance-level image retrieval.
• In this context, we study the asymmetric testing task, where the database is represented by the teacher and queries by the student.
• In both symmetric and asymmetric testing, we sys-tematically evaluate different teacher and student mod-els, metric learning loss functions (subsection 3.3) and knowledge transfer loss functions (subsection 3.4), serv-ing as a benchmark for future work.
• We introduce the asymmetric metric learning paradigm, an extremely simple mechanism to combine metric learning with knowledge transfer (subsection 3.2). 2.