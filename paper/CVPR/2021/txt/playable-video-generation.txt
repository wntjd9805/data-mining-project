Abstract
This paper introduces the unsupervised learning prob-lem of playable video generation (PVG). In PVG, we aim at allowing a user to control the generated video by select-ing a discrete action at every time step as when playing a video game. The difﬁculty of the task lies both in learn-ing semantically consistent actions and in generating re-alistic videos conditioned on the user input. We propose a novel framework for PVG that is trained in a self-supervised manner on a large dataset of unlabelled videos. We em-ploy an encoder-decoder architecture where the predicted action labels act as bottleneck. The network is constrained to learn a rich action space using, as main driving loss, a reconstruction loss on the generated video. We demon-strate the effectiveness of the proposed approach on several datasets with wide environment variety. Further details, code and examples are available on our project page willi-menapace.github.io/playable-video-generation-website. 1.

Introduction
Humans at a very early age can identify key objects and how each object can interact with its environment.
This ability is particularly notable when watching videos of sports or games. In tennis and football, for example, the skill is taken to the extreme. Spectators and sportscasters often argue which action or movement the player should have performed in the ﬁeld. We can understand and antici-pate actions in videos despite never being given an explicit list of plausible actions. We develop this skill in an unsu-pervised manner as we see actions live and on the screen.
We can further analyze the technique with which an action is performed as well as the “amount” of action, i.e. how much to the left. Furthermore, we can reason about what
*The second and third authors contributed equally to the work. (a) Playable Video Generation (b) Our results
Figure 1: We introduce the task of playable video genera-tion in an unsupervised setting (left). Given a set of unla-beled video sequences, a set of discrete actions are learned in order to condition video generation. At test time, using our method, named CADDY, the user can control the gen-erated video on-the-ﬂy providing action labels. happens if the player took a different action and how this would change the video.
From this observation, we propose a new task, Playable
Video Generation (PVG) illustrated in Fig 1a. In PVG, the goal is to learn a set of distinct actions from real-world video clips in an unsupervised manner (green block) in or-der to offer the user the possibility to interactively generate new videos (red block). As shown in Fig 1b, at test time, the user provides a discrete action label at every time step and can see live its impact on the generated video, similarly to video games. Introducing this novel problem paves the way toward methods that can automatically simulate real-world environments and provide a gaming-like experience.
PVG is related to the future frame prediction prob-lem [13, 25, 28, 40, 42, 44] and in particular to methods that condition future frames on action labels [20, 30, 31]. Given one or few initial frames and the labels of the performed ac-tions, such systems aim at predicting what happens next in 10061
the video. For example, this framework can be used to imi-tate a desired video game using a neural network with a re-markable generation quality [20, 31]. However, at training time, these methods require videos with their corresponding frame-level action at every time step. Consequently, these methods are limited to video game environments [20, 31] or robotic data [30] and cannot be employed in real-world en-vironments. As an alternative, the annotation effort required to address real-world videos can be reduced using a single action label to condition the whole video [45], but it lim-its interactivity since the user cannot control the generated video on-the-ﬂy. Conversely, in PVG, the user can control the generation process by providing an action at every time-step after observing the last generated frame.
This paper addresses these limitations introducing a novel framework for PVG named Clustering for Action
Decomposition and DiscoverY (CADDY). Our approach discovers a set of distinct actions after watching multiple videos through a clustering procedure blended with the gen-eration process that, at inference time, outputs playable videos. We adopt an encoder-decoder architecture where a discrete bottleneck layer is employed to obtain a dis-crete representation of the transitions between consecutive frames. A reconstruction loss is used as main driving loss avoiding the need for neither action label supervision nor even the precise number of actions. A major difﬁculty in PVG is that discrete action labels cannot capture the stochasticity typical of real-world videos. To address this difﬁculty, we introduce an action network that estimates the action label posterior distribution by decomposing actions in a discrete label and a continuous component. While the discrete action label captures the main semantics of the per-formed action, the continuous component captures how the action is performed. At test time, the user provides only the discrete actions to interact with the generation process.
Finally, we experimentally demonstrate that our ap-proach can learn consistent actions in varied environments.
We conduct experiments on three different datasets includ-ing both real-world (i.e. tennis and robotics [9]) and syn-thetic (i.e. video game [2]). Our experiments show that
CADDY generates high-quality videos while offering the user a better playability experience than existing future frame prediction methods. 2.