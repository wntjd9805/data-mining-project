Abstract
Although deep neural networks (DNNs) have achieved tremendous performance in diverse vision challenges, they are surprisingly susceptible to adversarial examples, which are born of intentionally perturbing benign samples in a human-imperceptible fashion.
It thus poses security con-cerns on the deployment of DNNs in practice, particularly in safety- and security-sensitive domains. To investigate the robustness of DNNs, transfer-based attacks have attracted a growing interest recently due to their high practical ap-plicability, where attackers craft adversarial samples with local models and employ the resultant samples to attack a remote black-box model. However, existing transfer-based attacks frequently suffer from low success rates due to over-ﬁtting to the adopted local model. To boost the transfer-ability of adversarial samples, we propose to improve the robustness of synthesized adversarial samples via adver-sarial transformations. Speciﬁcally, we employ an adver-sarial transformation network to model the most harmful distortions that can destroy adversarial noises and require the synthesized adversarial samples to become resistant to such adversarial transformations. Extensive experiments on the ImageNet benchmark showcase the superiority of our method to state-of-the-art baselines in attacking both unde-fended and defended models. 1.

Introduction
Deep neural networks (DNNs) have emerged as state-of-the-art solutions to a dizzying array of challenging vi-sion tasks [35, 22]. Despite their astonishing perfor-mance, DNNs are surprisingly vulnerable to adversarial samples, which are crafted by purposely attaching human-imperceptible noises to legitimate images and can mis-lead DNNs into wrong predictions [34, 38].
It poses a severe threat to the security of DNN-based systems, es-pecially in safety- and security-critical domains like self-driving [26, 39, 43]. Therefore, learning how to synthe-Figure 1: From left to right: An example of the clean image, the resultant image distorted by our adversarial transforma-tion network, and the corresponding adversarial image gen-erated by our method. size adversarial samples can serve as a crucial surrogate to evaluate the robustness of DNN-based systems before deployment [9] and spur the development of effective de-fenses [18, 36].
There are generally two lines of adversarial attacks stud-ied in the literature [2]. One focuses on the white-box set-ting, where the attackers possess perfect knowledge about the target model [9, 17, 25]. The other considers the black-box setting, where attackers do not know the speciﬁcs of the target model, such as its architecture and param-eters [28, 10]. Compared to the white-box counterpart, black-box attacks are recognized as a more realistic threat to DNN-based systems in practice [28]. Besides, among ex-isting black-box attacks, transfer-based attacks have gained increasing interest recently due to their high practical appli-cability, where attackers craft adversarial samples based on local source models and directly harness the resultant adver-sarial examples to fool the remote black-box victims [5, 37].
However, existing transfer-based attacks frequently man-ifest limited transferability due to overﬁtting to the em-ployed source model [9, 5, 44]. Concretely, although the generated adversarial samples can fool the source model with high success rates, they can hardly remain malicious to a different target model. Inspired by the data augmentation strategy [12, 16, 31], prior efforts have endeavored to im-prove the transferability of adversarial samples by training 9024
Update  contributions of this work:
Adversarial Transformation 
Network 
Target Model 
Figure 2: The diagram of our attack strategy. We proceed by ﬁrst training an adversarial transformation network that can characterize the most harmful image transformations to adversarial noises. We then manufacture adversarial sam-ples by additionally requiring them to be robust against the adversarial transformation network. them to become robust against common image transforma-tions, such as resizing [42], translation [6], and scaling [21].
Unfortunately, these works explicitly model the applied im-age distortions by employing only individual image trans-formations or their simple combination under a ﬁxed dis-tortion magnitude. Therefore, it makes the generated adver-sarial samples overﬁt to the applied image transformations and hardly resist against unknown distortions [4], leading to inferior transferability.
To mitigate the issue of poor transferability caused by employing a ﬁxed image transformation, a typical solution is to identify a rich collection of representative image trans-formations and then carefully tune a combination of them for each image. However, such a strategy can incur pro-hibitive computational costs. Therefore, we propose to ex-ploit an adversarial transformation network to automate this distortion tuning process, and Figure 1 illustrates an image manipulated by our adversarial transformation network.
Figure 2 depicts the diagram of our Adversarial
Transformation-enhanced Transfer Attack (ATTA). Specif-ically, motivated by the recent advance in applying convo-lutional neural networks (CNNs) to conduct diverse image manipulation tasks, like digital watermarking [45, 24] and style transfer [7], we propose to train a CNN as the adversar-ial transformation network by adversarial learning, which can capture the most harmful deformations to adversarial noises. After ﬁnishing the learning of the adversarial trans-formation network, we require the crafted adversarial sam-ples to be able to resist the distortions introduced by the adversarial transformation network. As such, we can make the generated adversarial samples more robust and improve their transferability.
In summary, we would like to highlight the following
• We propose a novel technique to improve the transfer-ability of adversarial samples with adversarial trans-formations.
• We conduct extensive experiments on the ImageNet benchmark to evaluate our approach. Experimental re-sults conﬁrm the superiority of our method over state-of-the-art baselines in attacking both undefended and defended models.
• We show that our technology is generally complemen-tary to other state-of-the-art schemes, suggesting it as a general strategy to boost adversarial transferability. 2.