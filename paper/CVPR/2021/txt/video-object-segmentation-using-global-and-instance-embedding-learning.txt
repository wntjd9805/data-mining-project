Abstract
Our
Once for All
In this paper, we propose a feature embedding based video object segmentation (VOS) method which is simple, fast and effective. The current VOS task involves two main challenges: object instance differentiation and cross-frame instance alignment. Most state-of-the-art matching based
VOS methods simplify this task into a binary segmentation task and tackle each instance independently.
In contrast, we decompose the VOS task into two subtasks: global em-bedding learning that segments foreground objects of each frame in a pixel-to-pixel manner, and instance feature em-bedding learning that separates instances. The outputs of these two subtasks are fused to obtain the ﬁnal instance masks quickly and accurately. Through using the rela-tion among different instances per-frame as well as tempo-ral relation across different frames, the proposed network learns to differentiate multiple instances and associate them properly in one feed-forward manner. Extensive experi-mental results on the challenging DAVIS [34] and Youtube-VOS [57] datasets show that our method achieves better performances than most counterparts in each case. 1.

Introduction
Video object segmentation (VOS) aims at segmenting out the class-agnostic object(s) in the video. It has various applications in video editing, autonomous driving, robotic-s, human-computer interaction, etc. According to whether providing the annotation of the initial frame, VOS can be di-vided into two settings: unsupervised and semi-supervised.
Unsupervised VOS needs to segment the primary objects automatically[11, 42, 64] while semi-supervised VOS need-s to segment the speciﬁed objects from human interaction or predeﬁned interesting target[33, 5, 52]. In this paper, we focus on the latter task, where the initial annotation of the object(s) are provided in the ﬁrst frame.
Most current successful semi-supervised VOS approach-es formulate VOS as a binary classiﬁcation task and seg-∗The ﬁrst two authors contribute equally to this work.
†Corresponding author: Xiankai Lu.
Support Set
VOS model
Query Set 0.95 0.90
STM
Forward three times (a)
Our_fast
Our
STM
J
M n a e 0.85 0.912077 0.805214 0.711917 0.715558 0.7192
Our_fast 0.913939 0.732332 0.701939 0.700188 0.698437 0.80 0.914694 0.798261 0.761261 0.748995 0.736729 0.75 6.249549 4.86043 4.175439 3.78411 3.419387 0.70 5.3001 5.974192 5.30453 4.93019 4.346354 4.005048 0.65 5
Our_fast 6.000541 5.792918 5.493308 5.394975 1 4 2 3
Instance Number (b) 1 6.5 6 5.5 5 4.5 4 3.5 3
S
P
F
Figure 1: Illustration of our intuition. (a) For multiple instances case, STM [30] tackles each instance independently while our method handles them simultaneously. (b) Comparison of accuracy (in the histogram) and computation speed (by the curve) regarding different instance numbers. Our method takes the instance relation into consideration and achieves better and faster performance than
STM. ment each instance separately [17, 55, 30] (Fig. 1(a)). This leads to two disadvantages: the computation efﬁciency is heavily affected by the instance number. Moreover, the s-patial and temporal relation between different instances can not be sufﬁciently mined. To capture multiple instances per frame, some methods employ region proposal network-s to generate instance masks [27, 58, 63]. Then, the re-identiﬁcation network is used to ﬁnd and associate multi-frame instances. However, the performance of these meth-ods heavily relies on the region proposal model that lacks the temporal information during proposal generation. To guide the network to differentiate multiple instances, some recent methods append the instance mask as prior knowl-edge during the input stream [23]. Nevertheless, this leads to the unclear segmentation boundary as well as ambiguity 16836  
accumulation due to unstable temporal mask propagation. 2.