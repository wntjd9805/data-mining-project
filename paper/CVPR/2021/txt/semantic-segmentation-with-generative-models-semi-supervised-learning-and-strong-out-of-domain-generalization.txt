Abstract
Training
Out-of-Domain Test Images
Training deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely avail-able unlabeled data to complement small labeled data sets.
In this paper, we propose a novel framework for discrim-inative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative ad-versarial network that captures the joint image-label dis-tribution and is trained efﬁciently using a large set of un-labeled images supplemented with only few labeled ones.
We build our architecture on top of StyleGAN2 [45], aug-mented with a label synthesis branch.
Image labeling at test time is achieved by ﬁrst embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the in-ferred embedding. We evaluate our approach in two impor-tant domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain per-formance compared to several baselines, and are the ﬁrst to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and pho-tographs of real faces to paintings, sculptures, and even cartoons and animal faces. Project Page: https://nv-tlabs.github.io/semanticGAN/ 1.

Introduction
Deep learning is now powering the majority of com-puter vision applications ranging from autonomous driv-ing [93, 73] and medical imaging [78, 38] to image edit-ing [69, 15, 98, 88, 70, 74]. However, deep networks are extremely data hungry, typically requiring training on large-scale datasets to achieve high accuracy. Even when large datasets are available, generalizing the network’s perfor-mance to out-of-distribution data, for example, on images captured by a different sensor, presents challenges, since deep networks tend to overﬁt to artiﬁcial statistics in the
*Correspondence to {daiqingl,sﬁdler}@nvidia.com
CT xray
MRI
Figure 1: Out-of-domain Generalization. Our model trained on real faces generalizes to paintings, sculptures, cartoons and even outputs plau-sible segmentations for animal faces. When trained on chest x-rays, it gen-eralizes to multiple hospitals, and even hallucinates lungs under clothed people. Our model also generalizes well from CT to MRI medical scans. training data. Labeling large datasets, particularly for dense pixel-level tasks such as semantic segmentation, is already very time consuming. Re-doing the annotation effort each time the sensor changes is especially undesirable. This is particularly true in the medical domain, where pixel-level annotations are expensive to obtain (require highly-skilled experts), and where imaging sensors vary across sites. In this paper, we aim to signiﬁcantly reduce the number of training data required for attaining successful performance, while achieving strong out-of-domain generalization.
Semi-supervised learning (SSL) facilitates learning with small labeled data sets by augmenting the training set with large amounts of unlabeled data. The literature on SSL is vast and some classical SSL techniques include pseudo-labeling [50, 2, 8, 83], consistency regularization [80, 49, 87, 23, 83], and various data augmentation techniques [7, 6, 91] (also see Sec. 2). State-of-the-art SSL performance is currently achieved by contrastive learning, which aims to train powerful image feature extractors using unsupervised contrastive losses on image transformations [13, 29, 61, 32].
Once the feature extractors are trained, a smaller amount of labels is needed, since the features already implicitly en-code semantic information. While SSL approaches have been more widely explored for classiﬁcation, recent meth-ods also tackle pixel-wise tasks [37, 62, 40, 47, 22, 68].
Although SSL techniques allow to train models with lit-tle labeled data, they usually do not explicitly model the dis-8300
tribution of the input data itself and therefore can still easily overﬁt to the training data, hampering their generalization capabilities. This is especially critical in semantic segmen-tation, where annotations are expensive and hence the avail-able amount of labeled data can be particularly small.
To address this, we propose a fully generative approach based on a generative adversarial network (GAN) that mod-els the joint image-label distribution and synthesizes both images and their semantic segmentation masks. We build on top of the StyleGAN2 [45] architecture and augment it with a label generation branch. Our model is trained on a large unlabeled image collection and a small labeled sub-set using only adversarial objectives. Test-time prediction is framed as ﬁrst optimizing for the latent code that recon-structs the input image, and then synthesizing the label by applying the generator on the inferred embedding.
We showcase our method in the medical domain and
It achieves competitive or better in-on human faces. domain performance even when compared to heavily engi-neered state-of-the-art approaches, and shows signiﬁcantly higher generalization ability on out-of-domain tests. We also demonstrate the ability to generalize to domains that are drastically different from the training domain, such as going from CT to MRI volumes, and natural photographs of faces to sculptures, paintings and cartoons, and even an-imal faces (see Figure 1).
In summary, we make the following contributions: (i)
We propose a novel generative model for semantic segmen-tation that builds on the state-of-the-art StyleGAN2 and naturally allows semi-supervised training. To the best of our knowledge, we are the ﬁrst work that tackles semantic segmentation with a purely generative method that directly models the joint image-label distribution. (ii) We exten-sively validate our model in the medical domain and on face images. In the semi-supervised setting, we demonstrate re-sults equal to or better than available competitive baselines. (iii) We show strong generalization capabilities and outper-form our baselines on out-of-domain segmentation tasks by a large margin. (iv) We qualitatively demonstrate reason-able performance even on extreme out-of-domain examples. 2.