Abstract
With the increasing demand to efﬁciently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the exe-cution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently, and do not fully con-sider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we ﬁrst propose (i) a gen-eral category of ﬁne-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting different
DNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware uniﬁed network pruning and architec-ture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learn-ing with fast evaluation and Bayesian optimization, ensur-ing the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, and 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mo-bile phone, consistently outperforming prior work. 1.

Introduction
The growing popularity of mobile AI applications and the demand for real-time Deep Neural Network (DNN) ex-ecutions raise signiﬁcant challenges for DNN accelerations.
However, the ever-growing size of DNN models causes in-tensive computation and memory cost, which impedes the
*⋆These authors contributed equally. deployment on resource limited mobile devices.
DNN weight pruning
[71, 21, 54, 27, 28] has been proved as an effective model compression technique that can remove redundant weights of the DNN models, thereby reducing storage and computation costs simultane-ously. Existing work mainly focus on unstructured pruning scheme [24, 21, 46] where arbitrary weight can be removed, and (coarse-grained) structured pruning scheme [54, 85, 84, 50, 82, 45] to eliminate whole ﬁlters/channels. The former results in high accuracy but limited hardware parallelism (and acceleration), while the latter is the opposite. Another active research area is the Neural Architecture Search (NAS) [86], which designs more efﬁcient DNN architec-tures using automatic searching algorithms. EfﬁcientNet
[69] and MobileNetV3 [30] are representative lightweight networks obtained by using NAS approaches. Recently, hardware-aware NAS [68, 73, 8, 33] has been investigated targeting acceleration on actual hardware platforms.
Different from the prior work on coarse-grained pruning and NAS that ﬁnd a smaller, yet regular, DNN structure, recent work [48, 58, 16] propose to prune the weights in a more ﬁne-grained manner, e.g., assigning potentially differ-ent patterns to kernels. Higher accuracy can be achieved as a result of the intra-kernel ﬂexibility, while high hard-ware parallelism (and mobile inference acceleration) can be achieved with the assist of compiler-level code generation techniques [58]. This work reveals a new dimension of opti-mization: With the aid of advanced compiler optimizations, it is possible to achieve high accuracy and high acceleration simultaneously by injecting a proper degree of ﬁne gran-ularity in weight pruning. Despite the promising results, pattern-based pruning [48, 58] is only applied to 3×3 con-volutional (CONV) layers, which limits the applicability.
As the ﬁrst contribution, we propose a general cate-gory of ﬁne-grained structured pruning schemes that can be 14255
applied to various DNN layers, i.e., block-punched prun-ing for CONV layers with different kernel sizes, and block-based pruning for FC layers. We develop a comprehen-sive, compiler-based automatic code generation framework supporting the proposed pruning schemes in a uniﬁed man-ner, supporting other types of pruning schemes, and differ-ent schemes for different layers. We show (i) the advan-tage of the proposed ﬁne-grained structured pruning in both accuracy and mobile acceleration, and (ii) the superior end-to-end acceleration performance of our compiler framework on both dense (before pruning) and sparse DNN models.
While our compiler optimizations provide notable mo-bile acceleration and support of various sparsity schemes, it introduces a much larger model optimization space: Differ-ent kernel sizes (1×1, 3×3, etc.) result in different accel-eration performances under compiler optimizations, so do different sparsity schemes. Thus, it is desirable to perform a compiler aware, joint network pruning and architecture search, determining the ﬁlter type and size, as well as prun-ing scheme and rate, for each individual layer. The objec-tive is to maximize accuracy satisfying a DNN latency con-straint on the target mobile device. The DNN latency will be actually measured on the target mobile device, thanks to the fast auto-tuning capability of our compiler for efﬁcient inference on different mobile devices.
We develop the compiler-aware NPAS framework to ful-ﬁll the above goal. It consists of three phases: (1) replace-ment of mobile-unfriendly operations, (2) the core search process, and (3) pruning algorithm search. The overall la-tency constraint is satisﬁed through the synergic efforts of (i) incorporating the overall DNN latency constraint into the automatic search in Phase 2, and (ii) the effective search of pruning algorithm and performing weight training/pruning accordingly. As Phase 2 exhibits a larger search space than prior NAS work, to perform efﬁcient search, we propose a meta-modeling procedure based on reinforcement learning (RL) with fast evaluation and Bayesian optimization. This will ensure the total number of training epochs comparable with representative NAS frameworks.
Our key contributions include:
• We propose a general category of ﬁne-grained struc-tured pruning applicable to various DNN layers, and a comprehensive, compiler code generation framework supporting different pruning schemes. We bridge the gap between model compression and NAS.
• We develop a compiler-aware framework of joint net-work pruning and architecture search, maximizing ac-curacy while satisfying inference latency constraint.
• We design a systematic search acceleration strategy, integrating pre-trained starting points, fast accuracy and latency evaluations, and Bayesian optimization.
• Our NPAS framework achieves by far the best mobile acceleration: 6.7ms, 5.9ms, and 3.9ms ImageNet infer-ence times with 78.2%, 75%, and 71% Top-1 accuracy, respectively, on an off-the-shelf mobile phone. 2.