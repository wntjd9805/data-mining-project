Abstract
Recent work on audio-visual navigation assumes a constantly-sounding target and restricts the role of audio to signaling the target’s position. We introduce semantic audio-visual navigation, where objects in the environment make sounds consistent with their semantic meaning (e.g., toilet ﬂushing, door creaking) and acoustic events are spo-radic or short in duration. We propose a transformer-based model to tackle this new semantic AudioGoal task, incorpo-rating an inferred goal descriptor that captures both spatial and semantic properties of the target. Our model’s persis-tent multimodal memory enables it to reach the goal even long after the acoustic event stops. In support of the new task, we also expand the SoundSpaces audio simulations to provide semantically grounded sounds for an array of objects in Matterport3D. Our method strongly outperforms existing audio-visual navigation methods by learning to as-sociate semantic, acoustic, and visual cues.1 1.

Introduction
An autonomous agent interacts with its environment in a continuous loop of action and perception. The agent needs to reason intelligently about all the senses available to it (sight, hearing, proprioception, touch) to select the proper sequence of actions in order to achieve its task. For exam-ple, a service robot of the future may need to locate and fetch an object for a user, go empty the dishwasher when it stops running, or travel to the front hall upon hearing a guest begin speaking there.
Towards such applications, recent progress in visual nav-igation builds agents that use egocentric vision to travel to a designated point in an unfamiliar environment [23, 38, 42, 10], search for a speciﬁed object [44, 9, 37, 8], or explore and map a new space [35, 34, 13, 10, 15, 10, 36]. Lim-ited new work further explores expanding the sensory suite of the navigating agent to include hearing as well. In par-ticular, the AudioGoal challenge [11] requires an agent to navigate to a sounding target (e.g., a ringing phone) using audio for key directional and distance cues [11, 19, 12]. 1Project page: http://vision.cs.utexas.edu/projects/ semantic-audio-visual-navigation
Figure 1: Semantic audio-visual navigation in 3D environ-ments: an agent must navigate to a sounding object. Since the sound may stop while the agent searches for the object, the agent is incentivized to learn the association between how objects look and sound, and to build contextual mod-els for where different semantic sounds are more likely to occur (e.g., water dripping in the bathroom).
While exciting ﬁrst steps, existing audio-visual naviga-tion work has two key limitations. First, prior work assumes the target object constantly makes a steady repeating sound (e.g., alarm chirping, phone ringing). While important, this corresponds to a narrow set of targets; in real-world sce-narios, an object may emit a sound only brieﬂy or start and stop dynamically. Second, in current models explored in re-alistic 3D environment simulators, the sound emitting target has neither a visual embodiment nor any semantic context.
Rather, target sound sources are placed arbitrarily in the en-vironment and without relation to the semantics of the scene and objects. As a result, the role of audio is limited to pro-viding a beacon of sound announcing where the object is.
In light of these limitations, we introduce a novel task: semantic audio-visual navigation.
In this task, the agent must navigate to an object situated contextually in an envi-ronment that only makes sound for a certain period of time.
Semantic audio-visual navigation widens the set of real-world scenarios to include acoustic events of short temporal duration that are semantically grounded in the environment.
It offers new learning challenges. The agent must learn not only how to associate sounds with visual objects, but also 115516
how to leverage the semantic priors of objects (along with any acoustic cues) to reason about where the object is likely located in the scene. For example, hearing the dishwasher stop running and issue its end of cycle chime should sug-gest both what visual object to search for as well as the likely paths for ﬁnding it, i.e., towards the kitchen rather than the bedroom. Notably, in the proposed task, the agent is not given any external information about the goal (such as a displacement vector or name of the object to search for).
Hence the agent must learn to leverage sporadic acoustic cues that may stop at any time as it searches for the source, inferring what visual object likely emitted the sound even after it is silent. See Figure 1.
To tackle semantic AudioGoal, we introduce a deep re-inforcement learning model that learns the association be-tween how objects look and how they sound. We develop a goal descriptor module that allows the agent to hypoth-esize the goal properties (i.e., location and object cate-gory) from the received acoustic cues before seeing the target object. Coupled with a transformer, it learns to at-tend to the previous visual and acoustic observations in its memory—conditioned on the predicted goal descriptor—to navigate to the audio source. Furthermore, to support this line of research, we instrument audio-visual simulations for real scanned environments such that semantically relevant sounds are attached to semantically relevant objects.
We evaluate our model on 85 large-scale real-world en-vironments with a variety of semantic objects and their sounds. Our approach outperforms state-of-the-art models in audio-visual navigation with up to an absolute 8.9% im-provement in SPL. Furthermore, our model is robust in han-dling short acoustic signals emitted by the goal with vary-ing temporal duration, and compared to the competitors, it more often reaches the goal after the acoustic observations end. In addition, our model maintains good performance in the presence of environment noise (distractor sounds) com-pared to baseline models. Overall, this work shows the po-tential for embodied agents to learn about how objects look and sound through interactions with a 3D environment. 2.