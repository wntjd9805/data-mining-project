Abstract
In this paper, we present a large scale unlabeled per-son re-identiﬁcation (Re-ID) dataset “LUPerson” and make the ﬁrst attempt of performing unsupervised pre-training for improving the generalization ability of the learned person
Re-ID feature representation. This is to address the prob-lem that all existing person Re-ID datasets are all of limited scale due to the costly effort required for data annotation.
Previous research tries to leverage models pre-trained on
ImageNet to mitigate the shortage of person Re-ID data but suffers from the large domain gap between ImageNet and person Re-ID data. LUPerson is an unlabeled dataset of 4M images of over 200K identities, which is 30× larger than the largest existing Re-ID dataset.
It also covers a much diverse range of capturing environments (e.g., cam-era settings, scenes, etc.). Based on this dataset, we system-atically study the key factors for learning Re-ID features from two perspectives: data augmentation and contrastive loss. Unsupervised pre-training performed on this large-scale dataset effectively leads to a generic Re-ID feature that can beneﬁt all existing person Re-ID methods. Using our pre-trained model in some basic frameworks, our meth-ods achieve state-of-the-art results without bells and whis-tles on four widely used Re-ID datasets: CUHK03, Mar-ket1501, DukeMTMC, and MSMT17. Our results also show that the performance improvement is more signiﬁcant on small-scale target datasets or under few-shot setting. 1.

Introduction
Model pre-training plays an indispensable role in person
Re-identiﬁcation (Re-ID). Compared to other vision tasks, as data collection and annotation for Re-ID is extremely dif-ﬁcult and expensive, existing public datasets all have a lim-ited scale in terms of image number (largest MSMT17 [39], 126K images), person identities (largest Airport [25], 9,651
*Corresponding author. (a) Market1501 (b) DukeMTMC
Figure 1: Person Re-ID performance comparison of apply-ing different pre-trained models on two methods: IDE [45] and MGN [38]. We report the results on different dataset scales for Market1501 and DukeMTC with small-scale set-ting. IN sup. and LUP unsup. are the supervised model trained on ImageNet and the unsupervised model trained on
LUPerson, respectively. identities) and captured environments (< 20 scenes, ﬁxed cameras and resolution). To mitigate the shortage of person
Re-ID data, previous research has tried to leverage models pre-trained on ImageNet and transfer the pre-trained feature to Re-ID tasks [22, 3, 34, 38, 13]. However, it is arguable if using ImageNet for pre-training is optimal, due to the large domain gap between ImageNet and person Re-ID data.
Inspired by the recent success of self-supervised learn-ing [6, 8, 19], we make the ﬁrst attempt towards large scale unsupervised pre-training for person Re-ID feature repre-sentation learning in this paper. Considering the limited scale of existing Re-ID datasets, we build a new Large-scale
Unlabeled Person Re-ID dataset “LUPerson”. It consists of 4M person images of over 200K identities extracted from 46K YouTube videos, which is 30× larger than the largest existing Re-ID dataset MSMT [39]. Moreover, the collected videos cover a wide range of capturing environments (e.g., using ﬁxed or moving cameras, under dynamic scenes, or having different resolutions), yielding a great data diversity which is essential for learning generic representation. We 14750
hope this study and the developed LUPerson dataset will serve as a solid baseline and motivate more feature repre-sentation learning researches for person re-identiﬁcation.
Based on the LUPerson dataset, we systematically study the problem of unsupervised Re-ID feature learning. We
ﬁnd that directly applying the commonly used contrastive learning method such as MoCo v2 [9] does not work well for the Re-ID task. After a careful investigation, we dis-cover some unique factors when applying unsupervised pre-training to the Re-ID task: 1) As a common data augmen-tation [19, 7, 1], the color distortion (e.g., color jitter) is harmful to Re-ID feature learning. This is because the color information is a crucial clue for Re-ID. 2) To prevent contrastive learning from degrading to a trivial solution, a strong task-speciﬁc augmentation operation RandomEras-ing [49] is proved as beneﬁcial as in supervised Re-ID train-ing. 3) How to use a proper temperature parameter in the contrastive loss plays an important role in ﬁnding a bal-ance between maintaining the discriminativity and mining the hard negatives.
We demonstrate the effectiveness of our pre-trained
Upon the model on various person Re-ID datasets. strong MGN [38] baseline, our pre-trained model can improve the mAP by 3.5% on Market1501 [44], 2.7% on DukeMTMC [47], and 2.0% on MSMT17[39]; while achieving 2.9% mAP gain on CUHK03 [44] based on an-other strong BDB [10] baseline. These results are superior to all the state-of-the-art methods. Our results show that the performance improvement is even more signiﬁcant with small-scale training samples for different datasets and base-lines, as shown in Fig.1. Besides, our pre-trained model is also general to unsupervised Re-ID methods. Based on the strongest baseline SpCL [15], our pre-trained model consistently achieves remarkable improvements on various datasets. To the best of our knowledge, this is the ﬁrst show-ing that large scale unsupervised pre-training can signiﬁ-cantly beneﬁt the person Re-ID task.
Our key contributions can be summarized as follows:
• We build a large-scale unlabeled dataset LUPerson, which consists of 4M images for over 200K identities, for unsupervised person Re-ID feature learning. This dataset is much larger than any existing public datasets and enables the ﬁrst unsupervised pre-training for per-son Re-ID tasks.
• We make generic unsupervised pre-training possible for Re-ID tasks, by carefully investigating the crucial factors, such as data augmentation strategies and the temperature usage in the contrastive learning frame-work.
• The unsupervised representation learning is general to not only supervised Re-ID methods, but also un-supervised Re-ID methods, and helps signiﬁcantly im-prove their performance on different datasets. 2.