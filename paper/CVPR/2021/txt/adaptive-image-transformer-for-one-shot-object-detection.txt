Abstract
One-shot object detection tackles a challenging task that aims at identifying within a target image all object instances of the same class, implied by a query image patch. The main difﬁculty lies in the situation that the class label of the query patch and its respective examples are not avail-able in the training data. Our main idea leverages the con-cept of language translation to boost metric-learning-based detection methods. Speciﬁcally, we emulate the language translation process to adaptively translate the feature of each object proposal to better correlate the given query feature for discriminating the class-similarity among the proposal-query pairs. To this end, we propose the Adaptive
Image Transformer (AIT) module that deploys an attention-based encoder-decoder architecture to simultaneously ex-plore intra-coder and inter-coder (i.e., each proposal-query pair) attention. The adaptive nature of our design turns out to be ﬂexible and effective in addressing the one-shot learning scenario. With the informative attention cues, the proposed model excels in predicting the class-similarity be-tween the target image proposals and the query image patch.
Though conceptually simple, our model signiﬁcantly outper-forms a state-of-the-art technique, improving the unseen-class object classiﬁcation from 63.8 mAP and 22.0 AP50 to 72.2 mAP and 24.3 AP50 on the PASCAL-VOC and MS-COCO benchmark datasets, respectively. 1.

Introduction
Object detection is considered one of the core techniques in computer vision. Learning such a system [24, 27, 28, 35] reliably often requires a large amount of labeled training data over a wide range of object categories . These days a state-of-the-art object detector is expected to perform well in localizing those objects in a scene, whose class labels have been seen in training, but it can still be easily confused by those of unseen classes. To alleviate the predicament, the task of One-Shot object Detection (OSD) [2, 12, 26]
Figure 1: Adaptive Image Transformer (AIT): The proposed
AIT module can adaptively represent each region proposal so that the similarity to the query patch can be properly eval-uated. Speciﬁcally, AIT translates the feature of a region proposal to match the query feature. The adaptive mecha-nism can improve the similarity measurement and boost the performance of our metric-learning based one-shot object detector. Here the illustrated visual features are selected from the 508th channel of ResNet-50 stage-4. is introduced to extend the system to detect objects of an arbitrary unseen class, which is implicitly hinted by a given inference query often in the form of an image patch.
Understandably, the OSD problem leads to a challeng-ing task as on-line model ﬁne-tuning is not performed and less feasible under the one-shot setting. In addition, even of the same object class, the query and the corresponding objects in a target image could differ substantially in size, shape, color, texture and appearance. To account for such unforeseen variations, we aim to develop a neural network approach that explores the multi-head attention mechanism to adaptively represent each potential region (i.e., a region proposal in our formulation) so that its relatedness to the query patch can be properly evaluated. Like in most of the attention-based architectures, the proposed network design may appear to be engineered solely for better performance. It 12247
Figure 2: Overview of our one-shot object detection model. (a) Multi-head Co-Attention (MCA). (b) Adaptive Image
Transformer (AIT). (c) Selective Channel Attention (SCA). In AIT, SPA: Selective Parallel Attention; FF: Feed Forward;
SEDA: Selective Encoder-Decoder Attention, which replaces the original encoder-decoder attention with the intra-coder SPA. indeed goes beyond this perspective and intends to establish a uniﬁed principle for solving the OSD problem based on the proposed adaptive image transformer (AIT), as shown in
Figure 1. To better convey our motivations, we describe the reasoning behind three key components of our method and their advantages over existing relevant SOTA techniques.
Our proposed OSD method is a two-stage technique that relies on the region proposal network (RPN) to generate candidate regions. Despite being class-agnostic, the original
RPN is trained without access to any examples from the unseen object classes, and thus could degrade the detection performance in inference due to excluding some legitimate region proposals for a given one-shot query. In dealing with this issue, we develop the ﬁrst key component of our method that uses multi-head co-attention (MCA) to correlate the tar-get image and query patch through various embeddings. The attention mechanism jointly considers the target image and the query by exploring different aspects of visual character-istics and spawns a corresponding feature map that encode such relatedness, upon which the RPN could generate more relevant region proposals to the query. Compared with the non-local scheme in Hseish et al. [12], the proposed MCA mechanism resolves the one-shot issue more effectively as supported by the improved detection accuracy.
The AIT module, illustrated in Figures 1 and 2(b), consti-tutes the second key component of our method. It is designed to explore how each proposal-query pair shares common se-mantic attributes over the deep visual features. Speciﬁcally, we employ a feature translation scheme, inspired by the attention-based paradigm [31] which shows the advantages of tackling the task of language translation by leveraging the intra-coder and inter-coder attention. In our formula-tion, AIT would adaptively transform the feature map of each proposal to match the query feature via employing the learned attention mechanisms. That is, given a query patch, the aspects of visual characteristics, e.g., shape, texture, and color, to be emphasized could vary among the region pro-posals. Note that AIT is more general than the co-excitation module in [12] where channel re-weighting is applied to the whole feature map rather than adapted to each proposal. In
Figure 1, we see that the two region proposals are adaptively translated to match the query patch by attending on different aspects of visual features, namely, color and texture.
The third key component in our formulation concerns the use of selective channel attention (SCA) to improve the effectiveness of optimizing with the ranking loss. Although the AIT module can transform the proposal feature to match the query feature, the similarity could differ signiﬁcantly over the respective channel dimension. Thus, it would be beneﬁcial to enhance the importance of those channels of high similarity before evaluating a proposal-query pair. The consideration prompts our design of the SCA module, which is implemented with the SK-Net [21] to make each neuron based on multi-scale input information to adjust its receptive
ﬁeld size. Figure 2(c) shows that SCA is separately applied to each proposal feature and the query feature before com-puting the losses. We characterize the main contributions of our method to one-shot object detection as follows:
• We introduce the Adaptive Image Transformer (AIT) to effectively address the OSD task. AIT can adaptively translate the feature of each object proposal to better correlate the given query feature. The region-aware attention mechanism is more general than the whole-image co-excitation scheme in [12].
• We develop a Multi-head Co-Attention (MCA) module to explore feature relatedness and then use the informa-tion to reﬁne both the feature maps of the target image and the query patch. As a result, the quality of the RPN region proposals can be signiﬁcantly enhanced.
• We propose a novel Selective Parallel Attention (SPA) mechanism to improve the performance of intra-coder or inter-coder multi-head attention, which is the corner stone of Transformer-based techniques.
• Our method for one-shot object detection achieves state-of-the-art experimental results over existing techniques on two standard benchmark datasets. 12248
2.