Abstract
Advanced tensor decomposition, such as tensor train (TT) and tensor ring (TR), has been widely studied for deep neural network (DNN) model compression, especially for recurrent neural networks (RNNs). However, compressing convolutional neural networks (CNNs) using TT/TR always suffers signiﬁcant accuracy loss. In this paper, we propose a systematic framework for tensor decomposition-based model compression using Alternating Direction Method of
Multipliers (ADMM). By formulating TT decomposition-based model compression to an optimization problem with constraints on tensor ranks, we leverage ADMM technique to systemically solve this optimization problem in an itera-tive way. During this procedure, the entire DNN model is trained in the original structure instead of TT format, but gradually enjoys the desired low tensor rank characteris-tics. We then decompose this uncompressed model to TT format, and ﬁne-tune it to ﬁnally obtain a high-accuracy TT-format DNN model. Our framework is very general, and it works for both CNNs and RNNs, and can be easily modiﬁed to ﬁt other tensor decomposition approaches. We evaluate our proposed framework on different DNN models for image classiﬁcation and video recognition tasks. Experimental re-sults show that our ADMM-based TT-format models demon-strate very high compression performance with high accu-racy. Notably, on CIFAR-100, with 2.3ˆ and 2.4ˆ com-pression ratios, our models have 1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and ResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model achieves 2.47ˆ FLOPs reduction without accuracy loss. 1.

Introduction
Deep Neural Network (DNNs) have already obtained widespread applications in many computer vision tasks, such as image classiﬁcation [19, 28], video recognition
:This work was done when the author was with Rutgers University.
[10, 2], objective detection [13, 41], and image caption
[45, 9]. Despite these unprecedented success and popular-ity, executing DNNs on the edge devices is still very chal-lenging. For most embedded and Internet-of-Things (IoT) systems, the sizes of many state-of-the-art DNN models are too large, thereby causing high storage and computational demands and severely hindering the practical deployment of DNNs. To mitigate this problem, to date many model compression approaches, such as pruning [16, 17, 35, 50] and quantization [17, 47, 40], have been proposed to reduce the sizes of DNN models with limited impact on accuracy.
Tensor Decomposition for Model Compression. Re-cently, tensor decomposition, as a mathematical tool that explores the low tensor rank characteristics of the large-scale tensor data, have become a very attractive DNN model compression technique. Different from other model com-pression methods, tensor decomposition, uniquely, can pro-vide ultra-high compression ratio, especially for recurrent neural network (RNN) models. As reported in [48, 38], the advanced tensor decomposition approaches, such as ten-sor train (TT) and tensor ring (TR), can bring more than 1,000ˆ parameter reduction to the input-to-hidden layers of RNN models, and meanwhile the corresponding clas-siﬁcation accuracy in the video recognition task can be even signiﬁcantly improved. Motivated by such strong compression performance, many prior research works have been conducted on tensor decomposition-based DNN mod-els [12, 37, 43]. In addition, to fully utilize the beneﬁts pro-vided by those models, several TT-format DNN hardware accelerators have been developed and implemented in dif-ferent chip formats, such as digital CMOS ASIC [7], mem-ristor ASIC [23] and IoT board [4].
Limitations of the State of the Art. Despite its promis-ing potentials, the performance of tensor decomposition is not satisﬁed enough as a mature model compression ap-proach. Currently all the reported success of tensor decom-position are narrowly limited to compressing RNN mod-els in video recognition tasks. For compressing convo-lutional neural network (CNN) in the image classiﬁcation task, which are the most commonly used and representa-10674
tive setting for evaluating model compression performance, all the state-of-the-art tensor decomposition approaches, in-cluding TT and TR, suffer very signiﬁcant accuracy loss.
For instance, even the very recent progress [32] using TR still has 1.0% accuracy loss when the compression ratio is only 2.7ˆ for ResNet-32 model on CIFAR-10 dataset. For the larger compression ratio as 5.8ˆ, the accuracy loss fur-ther increases to 1.9% .
Why Limited Performance? The above limitation of tensor decomposition is mainly due to the unique challenges involved in training the tensor decomposed DNN models.
In general, there are two ways to use tensor decomposition to obtain a compressed model: 1) Train from scratch in the decomposed format; and 2) Decompose a pre-trained un-compressed model and then retrain.
In the former case, when the required tensor decomposition-based, e.g. TT-format model, is directly trained from scratch, because the structure of the models are already pre-set to low tensor rank format before the training, the corresponding model capacity is typically limited as compared to the full-rank structure, thereby causing the training process being very sensitive to initialization and more challenging to achieve high accuracy. In the later scenario, though the pre-trained uncompressed model provides good initialization position, the straightforwardly decomposing full-rank uncompressed model into low tensor rank format causes inevitable and non-negligible approximation error, which is still very difﬁ-cult to be recovered even after long-time re-training period.
Besides, no matter which training strategy is adopted, ten-sor decomposition always brings linear increase in network depth, which implies training the tensor decomposition-format DNNs are typically more prone to gradient vanishing problem and hence being difﬁcult to be trained well.
Technical Preview and Contributions. To overcome the current limitations of tensor decomposition and fully unlock its potentials for model compression, in this pa-per we propose a systematic framework for tensor decom-position-based model compression using alternating direc-tion method of multipliers (ADMM). By formulating TT decomposition-based model compression to an optimiza-tion problem with constraints on tensor ranks, we leverage
ADMM technique [1] to systemically solve this optimiza-tion problem in an iterative way. During this procedure the entire DNN model is trained in the original structure instead of TT format, but gradually enjoys the desired low tensor rank characteristics. We then decompose this uncom-pressed model to TT format, and ﬁne-tune it to ﬁnally ob-tain a high-accuracy TT-format DNN model. In overall, the contributions of this paper are summarized as follows:
• We propose a systematic framework to formulate and solve the tensor decomposition-based model compres-sion problem. With formulating this problem to a con-strained non-convex optimization problem, our frame-work gradually restricts the DNN model to the target tensor ranks without explicitly training on the TT for-mat, thereby maintaining the model capacity as well as avoiding huge approximation error and increased net-work depth.
• We propose to use ADMM to efﬁciently solve this re-formulated optimization problem via separately solv-ing two sub-problems: one is to directly optimize the loss function with a regularization of the DNN by stochastic gradient descent, and the other is to use the introduced projection to constraint the tensor ranks an-alytically.
• We evaluate our proposed framework on different
DNN models for image classiﬁcation and video recog-nition tasks.
Experimental results show that our
ADMM-based TT-format models demonstrate very high compression performance with high accuracy.
Notably, on CIFAR-100, with 2.3ˆ and 2.4ˆ com-pression ratios, our models have 1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and
ResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model achieves 2.47ˆ FLOPs reduc-tion with no accuracy loss. 2.