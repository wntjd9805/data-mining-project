Abstract
We present M3P, a Multitask Multilingual Multimodal
Pre-trained model that combines multilingual pre-training and multimodal pre-training into a uniﬁed framework via multitask pre-training. Our goal is to learn universal repre-sentations that can map objects occurred in different modal-ities or texts expressed in different languages into a com-mon semantic space. In addition, to explicitly encourage
ﬁne-grained alignment between images and non-English lan-guages, we also propose Multimodal Code-switched Train-ing (MCT) to combine monolingual pre-training and multi-modal pre-training via a code-switch strategy. Experiments are performed on the multilingual image retrieval task across two benchmark datasets, including MSCOCO and Multi30K.
M3P can achieve comparable results for English and new state-of-the-art results for non-English languages. 1.

Introduction
Recently, we witness the rise of a new paradigm of natu-ral language processing (NLP), where general knowledge is learned from raw texts by self-supervised pre-training and then applied to downstream tasks by task-speciﬁc ﬁne-tuning.
Now, these state-of-the-art monolingual pre-trained language models, such as BERT [7], RoBERTa [23] and GPT-2 [28], have been expanded to multilingual scenarios, such as Mul-tilingual BERT [7], XLM/XLM-R [5, 4], Unicoder [13].
Moreover, some pre-training models under multimodal sce-*Work is done during an internship at Microsoft Research Asia.
†These authors contributed equally to this work.
‡Corresponding Author. narios, such as Unicoder-VL [19], UNITER [3], ERNIE-ViL
[36], VILLA [10] and Oscar [21], also come out.
However, it is still challenging to extend these pre-trained models to multilingual-multimodal scenarios. The multilin-gual pre-trained language models cannot handle vision data (e.g., images or videos) directly, whereas many pre-trained multimodal models are trained on English corpora thus can-not perform very well on non-English languages. Therefore, high quality multilingual multimodal training corpus is es-sential to combine multilingual pre-training and multimodal pre-training. However, there are only a few multilingual multimodal corpora exist, and they also have low language coverage. Moreover, relying on high-quality machine transla-tion engines to generate such data from English multimodal corpora is both time-consuming and computationally ex-pensive. Learning explicit alignments between vision and non-English languages during pre-training is lacking.
To address these challenges, this paper presents M3P, a Multitask Multilingual Multimodal Pre-trained model, which aims to learn universal representations that can map objects occurred in different modalities or texts expressed in different languages into a common semantic space. In order to alleviate the issue of lacking enough non-English labeled data for multimodal pre-training, we introduce Multimodal
Code-switched Training (MCT) to enforce the explicit align-ments between images and non-English languages. The goal is achieved by (i) learning to represent multilingual data us-ing multilingual corpora (e.g., sentences from Wikipedia cov-ering 100 languages) by multilingual pre-training, (ii) learn-ing multilingual-multimodal representations by randomly replacing some English words with their translations in other languages from multimodal corpora (e.g., image-caption pairs labeled in English), and (iii) generalizing these rep-13977
resentations to deal with multilingual-multimodal tasks by
Multitask learning.
In summary, the main contributions of this paper are:
• We present M3P, the ﬁrst known effort on combining multilingual pre-training and multimodal pre-training into a uniﬁed framework.
• We propose a novel Multimodal Code-switched Train-ing (MCT) method, an effective way to enhance the multilingual transfer ability of M3P in the zero-shot and few-shot settings.
• We achieve new state-of-the-art results for the multi-lingual image-text retrieval task on both Multi30K and
MSCOCO for non-English languages, outperforming existing multilingual methods by a large margin. The proposed model can also achieve comparable results for English on these two datasets, compared to the state-of-the-art monolingual multimodal models.
• Last but not least, we conduct extensive experiments and analysis to provide insights on the effectiveness of using Multimodal Code-switched Training (MCT) and each pre-training task. 2.