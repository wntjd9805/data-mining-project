Abstract
In the setting of continual learning, a network is trained on a sequence of tasks, and suffers from catastrophic for-getting. To balance plasticity and stability of network in continual learning, in this paper, we propose a novel net-work training algorithm Adam-NSCL which sequentially optimizes network parameters in the null space of all pre-vious tasks. We ﬁrst propose two mathematical conditions respectively for achieving network stability and plasticity in continual learning. Based on them, the network training for sequential tasks without forgetting can be simply achieved by projecting the candidate parameter update into the ap-proximate null space of all previous tasks in the network training process, where the candidate parameter update can be generated by Adam. The approximate null space can be derived by applying singular value decomposition to the un-centered covariance matrix of all input features of previous tasks for each linear layer. For efﬁciency, the uncentered co-variance matrix can be incrementally computed after learn-ing each task. We also empirically verify the rationality of the approximate null space at each linear layer. We apply our approach to training networks for continual learning on benchmark datasets of CIFAR-100 and TinyImageNet, and the results suggest that the proposed approach outper-forms or matches the state-ot-the-art continual learning ap-proaches. 1.

Introduction
Deep neural networks have achieved promising perfor-mance on various tasks in natural language processing, ma-chine intelligence, etc., [5, 9, 47, 48, 51]. However, the abil-ity of deep neural networks for continual learning is limited, where the network is expected to continually learn knowl-edge from sequential tasks [15]. The main challenge for continual learning is how to overcome catastrophic forget-ting [11, 32, 42], which has drawn much attention recently.
In the context of continual learning, a network is trained on a stream of tasks sequentially. The network is required to have plasticity to learn new knowledge from current task, and also stability to retain its performance on previous tasks. However, it is challenging to simultaneously achieve plasticity and stability in continual learning for neural net-works, and catastrophic forgetting always occurs. This phe-nomenon is called plasticity-stability dilemma [33].
Recently, various strategies for continual learning have been explored, including regularization-based, distillation-based, architecture-based, replay-based and algorithm-based strategies. The regularization-based strategy focuses on penalizing the variations of parameters across tasks, such as EWC [21]. The distillation-based strategy is inspired by knowledge distillation [14], such as LwF [26]. The architecture-based strategy modiﬁes the architecture of net-work on different tasks, such as [1, 25]. The replay-based strategy utilizes data from previous tasks or pseudo-data to maintain the network performance on previous tasks, such as [4, 36]. The algorithm-based strategy designs network parameter updating rule to alleviate performance degrada-tion on previous tasks, such as GEM [30], A-GEM [7] and
OWM [53].
In this paper, we focus on the setting of continual learn-ing where the datasets from previous tasks are inaccessible.
We ﬁrst propose two theoretical conditions respectively for stability and plasticity of neural networks in continual learn-ing. Based on them, we design a novel network training al-gorithm called Adam-NSCL for continual learning, which forces the network parameter update to lie in the null space of the input features of previous tasks at each network layer, as shown in Fig. 1. The layer-wise null space of input fea-tures can be modeled as the null space of the uncentered covariance of these features, which can be incrementally computed after learning each task. Since it is too strict to guarantee the existence of null space, we approximate the null space of each layer by the subspace spanned by sin-gular vectors corresponding to the smallest singular values of the uncentered covariance of input features. We embed this strategy into the Adam optimization algorithm by pro-184
gt−1
∆wt−1
Null space wt−1
∆wt−1 gt−1 wt
High loss contour on current task
Low loss contour on previous tasks
Low loss contour on current task
Figure 1. To avoid forgetting, we train network in the layer-wise null space of the corresponding uncentered covariance of all input features of previous tasks. jecting the candidate parameter update generated by Adam
[20] into the approximate null space layer by layer, which is ﬂexible and easy to implement.
We conduct various experiments on continual learning benchmarks in the setting that the datasets of previous tasks are unavailable, and results show that our Adam-NSCL is effective and outperforms the state-of-the-art continual learning methods. We also empirically verify the rational-ity of the approximate null space.
The paper is organized as follows. We ﬁrst introduce related works in Sec. 2. In Sec. 3, we present the mathemat-ical conditions and then propose network training algorithm for continual learning in Sec. 4. In Sec. 5, we conduct ex-periments to verify the efﬁcacy of our approach. 2.