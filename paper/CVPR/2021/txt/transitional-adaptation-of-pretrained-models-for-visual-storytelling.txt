Abstract
Previous models for vision-to-language generation tasks usually pretrain a visual encoder and a language generator in the respective domains and jointly ﬁnetune them with the target task. However, this direct transfer practice may suf-fer from the discord between visual speciﬁcity and language
ﬂuency since they are often separately trained from large corpora of visual and text data with no common ground. In this work, we claim that a transitional adaptation task is required between pretraining and ﬁnetuning to harmonize the visual encoder and the language model for challenging downstream target tasks like visual storytelling. We propose a novel approach named Transitional Adaptation of Pre-trained Model (TAPM) that adapts the multi-modal modules to each other with a simpler alignment task between visual inputs only with no need for text labels. Through extensive experiments, we show that the adaptation step signiﬁcantly improves the performance of multiple language models for sequential video and image captioning tasks. We achieve new state-of-the-art performance on both language metrics and human evaluation in the multi-sentence description task of LSMDC 2019 [50] and the image storytelling task of
VIST [18]. Our experiments reveal that this improvement in caption quality does not depend on the speciﬁc choice of language models. 1.

Introduction
Most models for vision-to-language generation tasks consist of a visual encoder to extract visual information from input images or videos, a language model to gener-ate text sentences, and a mechanism to weld the two mod-ules into one harmonized architecture. For example, recent models for visual captioning [7, 59] adopt a pretrained vi-sual encoder and a pretrained language generator and then optimize the target cross-modal generation objective with the downstream datasets [63, 40, 49, 67, 69, 74]. In this pro-∗Equal Contribution
Figure 1. Comparison between existing captioning models and our Transitional Adaptation of Pretrained Model (TAPM). (a) Pre-vious captioning models start from a pretrained visual encoder and a language generator and then directly ﬁnetune with the down-stream datasets. (b) TAPM includes a simple pretext task as an adaptation process that harmonizes the generator with the visual encoder before optimizing the target objective. cess, however, no transitional adaptation step has proposed to match the potentially substantial differences between the information stored in the visual encoder and the language generator, as they are separately trained from large sets of visual and text data with no common ground (e.g. images from ImageNet and text from Wikipedia).
This work is motivated by that this direct transfer of pre-trained models to a downstream task may suffer from the dissonance between visual speciﬁcity and language ﬂuency.
For example, ﬁnetuning pretrained language models on an-other target task may result in catastrophic forgetting of the language generation capability [8, 66]. Moreover, existing captioning models have often been criticized for not suf-ﬁciently conditioning on the visual context and thus lack visual discriminability [34, 36].
Considering the potentially vast gap between the nature of the information stored in the visual encoder and the lan-guage decoder, it would be difﬁcult for them to work in har-mony at once for another challenging objective of vision-12658
to-language generation. In this light, we believe a simpler objective dedicated to improving coordination between the two separately pretrained models could help the model get prepared for the target objective eventually better and faster.
Therefore, we present Transitional Adaptation of Pre-trained Model (TAPM) for visual storytelling as the ﬁrst approach that proposes an explicit visual adaptation step to harmonize the visual encoder with the pretrained language models as depicted in Fig. 1. Our adaptation step can be trained with only visual inputs, such as images or videos with no text label. We outline the contributions of this work as follows: 1. Our work is the ﬁrst attempt to demonstrate an auxil-iary adaptation loss’s effectiveness in welding a visual encoder with a pretrained language model. By exten-sive experiments, we show that this additional adap-tation between pretraining and ﬁnetuning consistently improves the captioning quality of various language models such as GPT-2 [45], XLM [14], and QRNN [5]. 2. We present the sequential coherence loss that can adapt the language generator using only sequential video/image inputs with no text label. We also intro-duce two recipes critical to TAPM’s success: (i) using the language model outputs for adaptation training and (ii) using the split-training process. 3. We evaluate TAPM in two storytelling tasks: sequen-tial video captioning in the LSMDC 2019 [50] and sequential image captioning in VIST [18]. TAPM achieves new state-of-the-art performance in both tasks in terms of automatic language metrics and hu-man evaluation. 2.