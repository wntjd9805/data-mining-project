Abstract
Label Supervision
Self Supervision
Proxy Tasks e.g. Bi-captioning
We develop an approach to learning visual representa-tions that embraces multimodal data, driven by a combi-nation of intra- and inter-modal similarity preservation ob-jectives. Unlike existing visual pre-training methods, which solve a proxy prediction task in a single domain, our method exploits intrinsic data properties within each modality and semantic information from cross-modal correlation simulta-neously, hence improving the quality of learned visual rep-resentations. By including multimodal training in a uni-ﬁed framework with different types of contrastive losses, our method can learn more powerful and generic visual features. We ﬁrst train our model on COCO and evaluate the learned visual representations on various downstream tasks including image classiﬁcation, object detection, and instance segmentation. For example, the visual represen-tations pre-trained on COCO by our method achieve state-of-the-art top-1 validation accuracy of 55.3% on ImageNet classiﬁcation, under the common transfer protocol. We also evaluate our method on the large-scale Stock images dataset and show its effectiveness on multi-label image tag-ging, and cross-modal retrieval tasks. 1.

Introduction
Visual representation learning is crucial for many com-puter vision tasks including image classiﬁcation [9, 50, 27, 30], tagging [16, 23], object detection [17, 47, 40], semantic and instance segmentation [41, 26]. Supervised pre-training over large-scale datasets [9] yields useful visual features which lead to state-of-the-art performance on those tasks.
Yet, ﬁne-grained class labeling efforts [9] are prohibitively heavy. Self-supervised learning methods [4, 12, 59, 25, 5, 6] do not require any annotations, but still require either ex-tremely large training sets or longer training epochs.
In addition to labels, image data usually comes with ad-ditional information including tags and captions, which is
*This work has been done during the ﬁrst author’s internship at Adobe.
Image
Encoder
Image (a) Sup.
Image
Encoder
Image
Image
Encoder
Textual
Texture 
Decoder
Image
Caption (b) Self-Sup (c) VirTex [10] 1 2 3 4 5
Image
Encoder
Textual
Encoder
Image
Caption (d) Our Method
Visual Feature
Texture Feature
Tag Supervision
Image Self-supervised Loss
Caption Self-supervised Loss
Image-Caption Loss
Caption-Image Loss 1 2 3 4 5
Figure 1. Main idea of the proposed method. Different from (c)
VirTex [10], our method not only learns the cross-modal correla-tion between images and captions, but also exploits intrinsic data properties in a self-supervised manner within each modality. typically generated by internet users and therefore easier to acquire. More importantly, such multimodal informa-tion comes with higher-level abstract concepts, which offer the potential for drawing useful connections across different modalities [22, 31, 34, 20, 15].
Our objective is to learn visual representation from mul-timodal data in a uniﬁed training framework. The frame-work design should have the following essential proper-ties: (1) fully exploits data potential within each unlabeled modality in a self-supervised manner; (2) bridges the het-erogeneity gap by comparing different modalities in a com-mon semantic space with similarity preservation objectives; (3) can be easily extended to take any new incoming modal-ity. We aim to learn high-quality visual features, which beneﬁt from not only the additional semantic information learned by cross-modal correlation modeling, but also the intrinsic data properties provided by each modality itself.
Some recently proposed methods [46, 35, 18, 19, 10, 49, 16995
2] also focus on generating high-quality visual representa-tions from scratch using multimodal data. For example, Vir-Tex [10] makes a trade-off between the data efﬁciency and annotation effort by relaxing the extremeness of the unsu-pervised setting and embracing caption annotations which are relatively easy to acquire. However, as shown in Fig-ure 1, VirTex is still trained in a single-path manner by solving a cross-modal proxy task, which is not sufﬁcient to exploit the full potential within each individual modality.
In this paper, we take a uniﬁed view of both intra-and inter-modal similarity preservation in multi-modal data, based on which we develop a new visual representation learning framework, as shown in Figure 1. To be speciﬁc, an intra-modal training path is used to capture the intrinsic patterns of augmented data examples in a prediction task.
A inter-modal training scheme is used to enhance the visual features by embracing the cross-modal interactions. With carefully designed contrastive losses, features in all modal-ities are adjusted via backpropagation in multiple training paths. We summarize our contributions as two-fold:
• Uniﬁed Multi-modal Training. Our multi-modal train-ing framework can exploit intrinsic data properties within each modality and extract semantic information from cross-modal correlation simultaneously. In addition, as shown in Figure 1 and 2, our framework is symmetric for all modalities, which suggests it has the ﬂexibility to incorporate any new modality.
• Broad Transferability. The visual representations pre-trained by our method can be transferred to many down-stream computer vision tasks and achieve excellent per-formance under the common transfer learning protocols.
We demonstrate these advantages through making exper-imental comparisons between supervised, self-supervised and learning from text methods through extensive experi-ments on classiﬁcation, tagging, cross-modal retrieval, ob-ject detection, and instance segmentation. 2.