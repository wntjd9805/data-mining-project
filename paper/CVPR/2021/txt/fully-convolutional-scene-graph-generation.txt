Abstract
This paper presents a fully convolutional scene graph generation (FCSGG) model that detects objects and re-lations simultaneously. Most of the scene graph genera-tion frameworks use a pre-trained two-stage object detector, like Faster R-CNN, and build scene graphs using bound-ing box features. Such pipeline usually has a large number of parameters and low inference speed. Unlike these ap-proaches, FCSGG is a conceptually elegant and efﬁcient bottom-up approach that encodes objects as bounding box center points, and relationships as 2D vector ﬁelds which are named as Relation Afﬁnity Fields (RAFs). RAFs encode both semantic and spatial features, and explicitly represent the relationship between a pair of objects by the integral on a sub-region that points from subject to object. FCSGG only utilizes visual features and still generates strong results for scene graph generation. Comprehensive experiments on the Visual Genome dataset demonstrate the efﬁcacy, efﬁ-ciency, and generalizability of the proposed method. FC-SGG achieves highly competitive results on recall and zero-shot recall with signiﬁcantly reduced inference time. 1.

Introduction
Philosophers, linguists and artists have long wondered about the semantic content of what the mind perceives in images and speech [1, 8, 29, 43]. Many have argued that images carry layers of meaning [2, 45]. Considered as an engineering problem, semantic content has been modeled either as latent representations [11, 17, 25, 38], or explicitly as structured representations [37, 54, 56]. For a computer vision system to explicitly represent and reason about the detailed semantics, Johnson [24] et al. adopt and formalize scene graphs from computer graphics community. A scene graph serves as a powerful representation that enables many down-stream high-level reasoning tasks such as image cap-tioning [59, 62], image retrieval [23, 24], Visual Question answering [20, 49] and image generation [23, 57].
*Work done in part as an intern at Futurewei Technologies Inc.
Figure 1: An example of scene graph generation. (a) The ground-truth scene graph of an image. (b) The ground-truth bounding boxes and their centers. (c) Our proposed rela-tionship representation called relation afﬁnity ﬁelds. (The image is 2353896.jpg from Visual Genome [27].)
A scene graph is considered as an explicit structural rep-resentation for describing the semantics of a visual scene.
The nodes in a scene graph represent the object classes and the edges represent the relationships between the objects.
Figure 1(a) shows a simple example of a scene graph that represents the underlying semantics of an image. Each re-lationship between two objects is denoted as a triplet of
<subject, PREDICATE, object> , i.e., banana ON
−→ chair and chair HAS
−−→ wheel in Figure 1(a). Most of the SGG work [5, 47, 48, 56, 66] is build as a two-stage pipeline: object detection then scene graph generation. For the ﬁrst stage of object detection, existing object detectors, i.e., Fast/Faster R-CNN [12, 41], are used for object fea-ture extraction from region proposals. For the second stage of scene graph generation, various approximation meth-ods [48, 56, 66] for graph inference have been used. Some work [5, 63, 64, 65] have also investigated how to utilize ex-ternal knowledge for improving the results. However, most previous work suffer from not only the long-tailed distri-bution of relationships [6, 10, 66], but also the highly bi-ased prediction conditioned on object labels [15, 21, 26, 47].
Consequently, frequent predicates will prevail over less fre-quent ones, and unseen relationships can not be identiﬁed.
Moreover, the extensibility and inference speed of a SGG 11546
framework is crucial for accelerating down-stream tasks.
Although few researchers have studied the efﬁciency and scalability in SGG [13, 31, 58], the high computational complexity impedes the practicality towards real-world ap-plications. A natural question that arises is: can we solve scene graph generation in a per-pixel prediction fashion?
Recently, anchor-free object detectors [28, 50, 61, 68] have become popular due to their simplicity and low cost. These methods treat an object as a single or many, pre-deﬁned or self-learned keypoints. Relating object detection to human pose estimation, if an object can be modeled as a point (hu-man “keypoint”), is it possible to represent a binary rela-tionship as vectors (human “limb”)?
In this paper, we propose a novel fully convolu-tional scene graph generation model, i.e., FCSGG, with state-of-the-art object detection results on Visual Genome dataset [27], as well as compelling SGG results compared with visual-only methods. We present a bottom-up repre-sentation of objects and relationships by modeling objects as points and relationships as vectors. Each relationship is encoded as a segment in a 2D vector ﬁeld called rela-tion afﬁnity ﬁeld (RAF). Figure 1(c) shows an illustration of RAFs for predicates ON and HAS. Both objects and rela-tionships are predicted as dense feature maps without los-ing spatial information. For the ﬁrst time, scene graphs can be generated from a single convolutional neural network (CNN) with signiﬁcantly reduced model size and inference speed. Speciﬁcally, we make the following contributions:
• We propose the ﬁrst fully convolutional scene graph generation model that is more compact and computa-tionally efﬁcient compared to previous SGG models.
• We introduce a novel relationship representation called relation afﬁnity ﬁelds that generalizes well on unseen visual relationships. FCSGG achieves strong results on zero-shot recall.
• Our proposed model outperforms most of the visual-only SGG methods, and achieves competitive results compared to methods boosted by external knowledge.
• We conduct comprehensive experiments and bench-mark our proposed method together with several pre-vious work on model efﬁciency, and FCSGG achieves near real-time inference. 2.