Abstract
Despite the signiﬁcant progress made by deep learning in natural image matting, there has been so far no repre-sentative work on deep learning for video matting due to the inherent technical challenges in reasoning temporal domain and lack of large-scale video matting datasets. In this paper, we propose a deep learning-based video matting framework which employs a novel and effective spatio-temporal fea-ture aggregation module (ST-FAM). As optical ﬂow estima-tion can be very unreliable within matting regions, ST-FAM is designed to effectively align and aggregate information across different spatial scales and temporal frames within the network decoder. To eliminate frame-by-frame trimap annotations, a lightweight interactive trimap propagation network is also introduced. The other contribution consists of a large-scale video matting dataset with groundtruth al-pha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation.
Quantitative and qualitative experimental results show that our framework signiﬁcantly outperforms conventional video matting and deep image matting methods applied to video in presence of multi-frame temporal information. Our dataset is available at https://github.com/nowsyn/DVM . 1.

Introduction
Video matting, or extracting from a given video high-quality alpha matte of a moving foreground object, has a wide range of applications in special effect and TV/movie production. Formally, given the color of a video frame I, foreground color F , background color B and alpha matte
α ∈ [0, 1], the video compositing equation Eq. 1 is
I = αF + (1 − α)B. (1)
∗The second and third authors contribute equally to this work.
This work was done when Yanan Sun was a student intern at Kuaishou
Technology. This work was supported by Kuaishou Technology and the
Research Grant Council of the Hong Kong SAR under grant no. 16201420.
Figure 1. A challenging video matting example where top and bot-tom are consecutive frames. Left: input frames with insets zoom-ing in complex hairs and showing the estimated optical ﬂow from
PWC-Net [38]. Note that the estimated optical ﬂow is unreliable within the hair regions. Middle: deep image matting [44] results.
Right: our results.
Compared to image matting, video matting poses two fur-ther challenges. First, video matting needs to preserve spa-tial and temporal coherence in the predicted alpha matte.
A straightforward solution applying image matting on in-dividual frames may inevitably cause severe ﬂickering ar-tifacts for moving ﬁne details. Using optical ﬂow to regu-larize output may help to alleviate these artifacts, but even with the most state-of-the-art optical ﬂow estimation meth-ods [20, 38, 46], optical ﬂow estimation within complex matting regions is still very unreliable. This is because mat-ting regions simultaneously contain both the foreground and background and there is so far no good optical ﬂow estima-tion that can handle large area of semi-transparency.
Traditional methods tackled the video matting problem by ﬁnding local or non-local afﬁnity among pixel colors and computing the motion of the foreground [9, 48] but their results are still far from satisfactory especially when dealing with complex cases, such as rapidly moving objects or complex backgrounds. Figure 1 shows an example with challenging motions. The other challenge for video mat-ting is the necessary input of a dense trimap for each frame, making it difﬁcult to generate high quality large-scale video 16975
matting benchmarks.
In this paper, we propose an encoder-decoder network consisting of a novel spatio-temporal feature aggregation module (ST-FAM) for extracting feature pyramids at differ-ent levels, which utilizes spatial and temporal information across multiple frames. Without optical ﬂow estimation, our network can effectively address the video matting problem and produce spatially and temporally coherent alpha mat-tes, and can generate good predictions of hard cases using the fused temporal information. To provide reliable frame-by-frame trimaps with minimum user inputs, a novel corre-lation layer is introduced to propagate trimaps across differ-ent frames. With our trimap propagation method, a user can edit and propagate trimaps at an interactive frame rate.
To support our and future video matting research, we have also contributed a high-quality video matting dataset with groundtruth alpha mattes. Furthermore, to verify the generalization capability of our method to real videos, we provide 10 high-resolution real-world videos with dense and frame-by-frame human annotated trimaps for evalua-tion. We evaluate our method on our composited test set as well as real-world high-resolution videos. Experimen-tal results demonstrate that our deep video matting method signiﬁcantly outperforms image-based deep matting meth-ods and conventional video matting approaches, capable of handling complex scenarios such as rapidly moving objects with fuzzy boundaries or complex backgrounds. 2.