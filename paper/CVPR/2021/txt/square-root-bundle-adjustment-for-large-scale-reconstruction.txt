Abstract
We propose a new formulation for the bundle adjustment problem which relies on nullspace marginalization of land-mark variables by QR decomposition. Our approach, which we call square root bundle adjustment, is algebraically equivalent to the commonly used Schur complement trick, improves the numeric stability of computations, and allows for solving large-scale bundle adjustment problems with single-precision ﬂoating-point numbers. We show in real-world experiments with the BAL datasets that even in single precision the proposed solver achieves on average equally accurate solutions compared to Schur complement solvers using double precision. It runs signiﬁcantly faster, but can require larger amounts of memory on dense problems. The proposed formulation relies on simple linear algebra op-erations and opens the way for efﬁcient implementations of bundle adjustment on hardware platforms optimized for single-precision linear algebra processing. 1.

Introduction
Bundle adjustment (BA) is a core component of many 3D reconstruction algorithms and structure-from-motion
It is one of the classical computer vi-(SfM) pipelines. sion problems and has been investigated by researchers for more than six decades [7]. While different formulations ex-ist, the underlying question is always the same: given a set of approximate point (landmark) positions that are ob-served from a number of cameras in different poses, what are the actual landmark positions and camera poses? One can already compute accurate 3D positions with only a few images; however, with more available images we will get a more complete reconstruction. With the emergence of large-scale internet photo collections has come the need to solve bundle adjustment on a large scale, i.e., for thousands of images and hundreds of thousands of landmarks. This
This work was supported by the Munich Center for Machine Learn-ing, by the ERC Advanced Grant SIMULACRON and by the DFG project
CR 250/20-1 “Splitting Methods for 3D Reconstruction and SLAM.”
Figure 1: Optimized 3D reconstruction of the largest venice
BAL dataset with 1778 cameras, around one million land-marks, and ﬁve million observations. For this problem, the proposed square root bundle adjustment (√BA) solver is 42% faster than the best competing method at reaching a cost tolerance of 1%. requires scalable solutions that are still efﬁcient for large problems and do not run into memory or runtime issues.
Modern BA solvers usually rely on the Schur comple-ment (SC) trick that computes the normal equations of the original BA problem and then breaks them down into two smaller subproblems, namely (1) the solution for camera poses and (2) ﬁnding optimal landmark positions. This results in the drastically smaller reduced camera system (RCS), which is also better-conditioned [2] than the origi-nal normal equations. The use of the Schur complement has become the de facto standard for solving large-scale bundle adjustment and is hardly ever questioned.
In this work, we challenge the widely accepted assump-11723
tion of SC being the best choice for solving bundle adjust-ment, and provide a detailed derivation and analysis of an alternative problem reduction technique based on QR de-composition. Inspired by similar approaches in the Kalman
ﬁlter literature [34], we factorize the landmark Jacobian
Jl such that we can project the original problem onto the nullspace of Jl. Thus, we circumvent the computation of the normal equations and their system matrix H, and are able to directly compute a matrix square root of the
RCS while still solving an algebraically equivalent prob-lem. This improves numerical stability of the reduction step, which is of particular importance on hardware opti-mized for single-precision ﬂoats. Following terminology for nullspace marginalization on Kalman ﬁlters, we call our method square root bundle adjustment, short √BA. In par-ticular, our contributions are as follows:
• We propose nullspace marginalization as an alternative to the traditional Schur complement trick and prove that it is algebraically equivalent.
• We closely link the very general theory of nullspace marginalization to an efﬁcient implementation strategy that maximally exploits the speciﬁc structure of bundle adjustment problems.
• We show that the algorithm is well parallelizable and that the favorable numerical properties admit compu-tation in single precision, resulting in an additional twofold speedup.
• We perform extensive evaluation of the proposed ap-proach on the Bundle Adjustment in the Large (BAL) datasets and compare to the state-of-the-art optimiza-tion framework Ceres.
• We release our implementation and complete evalua-tion pipeline as open source to make our experiments reproducible and facilitate further research: https://go.vision.in.tum.de/rootba. 2.