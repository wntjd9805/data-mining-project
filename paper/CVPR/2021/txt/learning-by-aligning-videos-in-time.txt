Abstract niﬁcant performance gain where labeled data is lacking.
We present a self-supervised approach for learning video representations using temporal video alignment as a pre-text task, while exploiting both frame-level and video-level information. We leverage a novel combination of tempo-ral alignment loss and temporal regularization terms, which can be used as supervision signals for training an encoder network. Speciﬁcally, the temporal alignment loss (i.e.,
Soft-DTW) aims for the minimum cost for temporally align-ing videos in the embedding space. However, optimizing solely for this term leads to trivial solutions, particularly, one where all frames get mapped to a small cluster in the embedding space. To overcome this problem, we pro-pose a temporal regularization term (i.e., Contrastive-IDM) which encourages different frames to be mapped to differ-ent points in the embedding space. Extensive evaluations on various tasks, including action phase classiﬁcation, ac-tion phase progression, and ﬁne-grained frame retrieval, on three datasets, namely Pouring, Penn Action, and IKEA
ASM, show superior performance of our approach over state-of-the-art methods for self-supervised representation learning from videos. In addition, our method provides sig-1.

Introduction
There are just three problems in computer vision: registration, registration, and registration.
Takeo Kanade
Lukas-Kanade and Iterative Closest Point have been amongst the most ubiquitous building blocks in artiﬁcial perception literature. Yet spatio-temporal registration has received little attention in the present deep learning renais-sance. Correspondingly, we add to a small number of recent approaches [14, 39] that have revived temporal alignment as a means of improving video representation learning. In order to learn perfect alignment of two videos, a learning algorithm must be able to disentangle phases of the activ-ity in time while simultaneously associating visually sim-ilar frames in the two different videos. We demonstrate that learning in this manner generates representations that are effective for downstream tasks that rely on ﬁne-grained
∗ indicates joint ﬁrst author.
{sanjay,sateesh,huseyin,shahram,andrey,zeeshan,huy}@retrocausal.ai 5548
temporal features.
In the context of using temporal alignment for learn-ing video representations, some recent works [14, 39] use cycle-consistency losses to perform local alignment be-tween individual frames. At the same time, some works have explored global alignment for video classiﬁcation and segmentation [8, 5]. We adapt such global alignment ideas for video representation learning in this work.
A few of approaches have been proposed for super-vised action recognition [45, 7, 49, 46] and action segmen-tation [15, 33]. Unfortunately, these approaches require
ﬁne-grained annotations which can be prohibitively expen-sive [40]. We note the seemingly inﬁnite supply of public video data, and contrast it with the high cost of ﬁne-grained annotation. This discrepancy emphasizes the importance of exploring self-supervised methods. We are further mo-tivated by datasets and downstream tasks that speciﬁcally beneﬁt from temporal alignment, such as video streams of semi-repetitive activities from manufacturing assembly lines to surgery rooms. It is desirable to measure the vari-ability and anomalies [44, 23] across such datasets, where representations that optimize for temporal alignment may be highly performant.
Our approach, Learning by Aligning Videos (LAV), uti-lizes the task of temporally aligning videos for learning self-supervised video representations. Speciﬁcally, we use a differentiable version of an alignment metric which has been widely used in the time series literature, namely Dy-namic Time Warping (DTW) [4]. DTW is a global align-ment metric, taking into account entire sequences while aligning. Unfortunately, in a self-supervised representation learning context, optimizing solely for DTW may converge to trivial solutions wherein the learned representations are not meaningful. To address this issue, we combine the above alignment metric with a regularization, as shown in
Fig. 1. In particular, we propose a regularization term that optimizes for temporally disentangled representations, i.e., frames that are close in time are mapped to spatially nearby points in the embedding space and vice versa.
In summary, our contributions include:
• We introduce a novel self-supervised method for learning video representations by temporally aligning videos as a whole, leveraging both frame-level and video-level cues.
• We adopt the classical DTW as our temporal alignment loss, while proposing a new temporal regularization.
The two components have mutual beneﬁts, i.e., the lat-ter prevents trivial solutions, whereas the former leads to better performance.
• Our approach performs on par with or better than the state-of-the-art on various temporal understand-ing tasks on Pouring, Penn Action, and IKEA ASM datasets. The best performance is sometimes achieved by combining our method with a recent work [14].
Further, our approach offers signiﬁcant accuracy gain when lacking labeled data.
• We have made our dense per-frame labels for 2123 videos of Penn Action publicly available at https:
//bit.ly/3f73e2W. 2.