Abstract
Classiﬁcation
Reasoning
Unreliable labels derived from large-scale dataset pre-vent neural networks from fully exploring the data. Exist-ing methods of learning with noisy labels primarily take noise-cleaning-based and sample-selection-based methods.
However, for numerous studies on account of the above two views, selected samples cannot take full advantage of all data points and cannot represent actual distribution of cat-egories, in particular if label annotation is corrupted. In this paper, we start from a different perspective and propose a robust learning algorithm called DualGraph, which aims to capture structural relations among labels at two different levels with graph neural networks including instance-level and distribution-level relations. Speciﬁcally, the instance-level relation utilizes instance similarity characterize sam-ple category, while the distribution-level relation describes instance similarity distribution from each sample to all other samples. Since the distribution-level relation is ro-bust to label noise, our network propagates it as super-vised signals to reﬁne instance-level similarity. Combin-ing two level relations, we design an end-to-end training paradigm to counteract noisy labels while generating re-liable predictions. We conduct extensive experiments on the noisy CIFAR-10 dataset, CIFAR-100 dataset, and the
Clothing1M dataset. The results demonstrate the advanta-geous performance of the proposed method in comparison to state-of-the-art baselines. 1.

Introduction
Deep learning has turned out to be excellent performance at discovering intricate structures in high-dimensional data
[14], particularly deep convolutional nets have brought about breakthroughs in processing image classiﬁcation [8], semantic segmentation [16], and object detection [23].
Most of these tasks require reliable and clean large-scale datasets to train Deep Neural Networks (DNNs), but it is time-consuming and expensive to collect such high-quality datasets like ImageNet [3]. To alleviate this problem, al-Distribution Graph
Aggregate
Prediction
Sample Similarity
Reconstruct
Instance Graph  
Difference extract sample  features
Embedding
Network
Figure 1. The concept of iterate optimization. Distribution graph forward propagation instance similarity distribution difference to reﬁne structural relationships among labels. In the classiﬁcation phase, we utilize the reconstructed instance graph to generate reli-able predictions. The two training processes are executed alterna-tively. ternatives such as crowdsourcing [34, 37] and web-crawlers
[5] are available to improve annotation efﬁciency. How-ever, those low-cost approaches introduce low-quality anno-tations, and these labels are unreliable due to various types of noise. Meanwhile, DNNs easily ﬁt a random labeling of the training data [38]. As noisy labels severely degrade the generalization performance [2, 30] of DNNs, learning from noisy labels has become a signiﬁcant task.
Existing methods of learning with noisy labels primar-ily take noise-cleaning-based and sample-selection-based methods [1, 27]. Noise-cleaning-based methods mainly re-move samples with suspicious labels or correct their noisy labels to corresponding true class [6, 29, 35].
[6] ob-tains correct label by building the prototype and compar-ing its similarity with the training data. However, the pro-posed heuristics algorithms [29, 35] have been criticized for removing too many instances or keeping mislabeled in-stances. Sample-selection-based methods aim to identify true-labeled samples from noisy training data [7, 17, 36].
Co-teaching [7] and Co-teaching+ [36] train models on small-loss instances. Decoupling [17] and Co-teaching+
[36] introduce the “Disagreement” strategy, where “when to update” depends on a disagreement between two dif-9654
ferent networks. However, there are only a part of train-ing examples that can be selected by the “Disagreement” strategy, and these examples cannot be guaranteed to have ground-truth labels. As described, studies based on these two ideas generally choose trusted examples to avoid the phenomenon of over-ﬁtting. Also, these methods are lim-ited by memorization effects [2], and examples with noisy labels are among the most forgotten examples [30]. Fur-thermore, previous methods lack of a global perspective to explore patterns in the relationship between samples, which naturally motivates us to improve them in our research.
In this paper, we start from a different perspective and propose a robust learning algorithm called DualGraph, which aims to capture structural relationships among la-bels at two different levels. Relevant references reveal that both noise and hard examples are the causes of classiﬁer forgetting [2, 30, 38], but the methods of probability selec-tion and small loss selection tend to confuse noise and hard examples. To this end, we design an end-to-end training paradigm called iterate optimization mechanism as Figure 1 illustrates. It consists of two alternate phases, i.e., rea-soning and classiﬁcation. In the reasoning phase, we gen-erate the similarity distribution for each sample by calcu-lating the similarity from one sample to all other samples.
In the classiﬁcation phase, the distribution features are ap-plied to reconstruct the instance graph nodes and generate reliable predictions via calculating node similarity. Such a cyclic operation is executed several times until convergence.
Speciﬁcally, we train two graph neural networks with a joint loss, including the example classiﬁer loss and the distribu-tion loss. Furthermore, we utilize the joint loss to weighted edge loss to enhance positive examples (clean examples) and weaken negative examples (noise and hard examples) in instance graph.
In summary, the main contributions of this paper are:
• To the best of our knowledge, we are the ﬁrst to ex-ploit the graph neural network that captures structural relations among labels at two different levels. Our approach reﬁnes instance-level relations via instance similarity distribution obtained from distribution-level relations to establish the robust label relations.
• We propose an iterate optimization mechanism that contains two phases to train two graph neural networks simultaneously. In the reasoning phase, our approach obtains the distribution feature for each sample and propagates distribution information in a graph neural network to correct corrupted labels.
In the classiﬁ-cation phase, our approach utilizes the reconstructed instance-level graph to generate reliable predictions.
• We experimentally show that our approach signif-icantly advances state-of-the-art results on multiple benchmarks with different types and levels of label noise. Especially on the Clothing1M dataset, our ap-proach outperforms existing methods by 6% ∼ 8%. 2.