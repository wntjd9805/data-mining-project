Abstract
Backpropagation image saliency aims at explaining model predictions by estimating model-centric impor-tance of individual pixels in the input. However, class-insensitivity of the earlier layers in a network only allows saliency computation with low resolution activation maps of the deeper layers, resulting in compromised image saliency.
Remedifying this can lead to sanity failures. We propose
CAMERAS, a technique to compute high-ﬁdelity backprop-agation saliency maps without requiring any external priors and preserving the map sanity. Our method systematically performs multi-scale accumulation and fusion of the acti-vation maps and backpropagated gradients to compute pre-cise saliency maps. From accurate image saliency to artic-ulation of relative importance of input features for different models, and precise discrimination between model percep-tion of visually similar objects, our high-resolution map-ping offers multiple novel insights into the black-box deep visual models, which are presented in the paper. We also demonstrate the utility of our saliency maps in adversarial setup by drastically reducing the norm of attack signals by focusing them on the precise regions identiﬁed by our maps.
Our method also inspires new evaluation metrics and a san-ity check for this developing research direction. 1.

Introduction
Deep visual models are fast surpassing human-level per-formance for various vision tasks, including image classiﬁ-cation [15], [28], object detection [22], [23], and semantic segmentation [17], [5]. However, they hardly offer any ex-planation of their decisions, and are rightfully considered black-boxes. This is problematic for their practical deploy-ment, especially in high-risk emerging applications where transparency is vital, e.g. in healthcare, self-driving vehicles and smart surveillance [21]. The problem is exacerbated by the push of ‘right to explanation’ by algorithmic regula-tory authorities and their objection to black-box models in safety-critical applications [1].
Addressing this issue for deep visual models, tech-niques are emerging to offer input-agnostic [11] and input-speciﬁc [8], [21], [25] explanation of model predictions.
This work subscribes to the latter, where the ultimate goal is to identify the contribution of each pixel in an input to the output prediction. The popular techniques to achieve this adopt one of two strategies. The ﬁrst, systematically modiﬁes the input image pixels (i.e. image regions) and an-alyzes the effects of those perturbations on the output pre-dictions [8], [9], [20], [33]. The underlying search nature of this perturbation-based formulation offers high-ﬁdelity model-centric importance attribution to the input pixels, al-beit at a high computational cost. Hence, tractability is achieved under heuristics or external priors over the com-puted importance maps. This is undesired because the eventual maps may be inﬂuenced by these external factors, which compromises the model-ﬁdelity of the maps.
The second strategy relies on the activation maps of the internal layers and gradients of the models. Commonly known as backpropagation saliency methods [25], [21],
[27], [30], [33], [34] approaches adopting this strategy are computationally efﬁcient, thereby offering the possibility of avoiding unnecessary heuristics or priors. However, for the visual neural models, the layers closer to the input are class-insensitive [21]. This limits the ammunition of back-propagation saliency methods to the deeper layers of the networks, where the size of activation maps is very small, e.g. 10−3× of the input size. Projecting the saliency com-puted with those maps onto the original image grid results in intrinsically low-resolution image saliency. On the other hand, using heuristics or priors to sharpen those projections inadvertently compromise the sanity of the maps [2]. Not to mention, employing activation signals of multiple internal layers for resolution enhancement takes us back to a combi-natorial search problem of choosing the best layers, under a pre-speciﬁed heuristic.
Addressing the above issues, we introduce CAMERAS
- an Enhanced Resolution And Sanity preserving Class
Activation Mapping for backpropagation image saliency. 16327
Figure 1. (Top) CAMERAS meticulously fuses activation maps and backpropagated gradients of a layer for multi-scale copies of an input.
After passing the resulting saliency map through ReLU and normalising it (f ), the map is embedded on the original image. (Bottom) By avoiding inﬂuence of external factors, CAMERAS easily passes the sanity checks for image saliency. Shown are the results of cascading randomization [2] on ResNet. Progressive randomising of layer weights randomises the output right from the logits layer which identiﬁes preservation of sanity. Thus, the CAMERAS maps do not sacriﬁce their sanity for high-resolution, achieving the best of both worlds.
The proposed technique (Fig. 1-top) systematically accu-mulates and fuses multi-scale activation maps and back-to construct precise propagated gradients of a model saliency maps. By avoiding the inﬂuence of any exter-nal factor, e.g. heuristics, priors, thresholds, the saliency maps of CAMERAS easily pass the sanity checks for im-age saliency (Fig. 1-bottom). Moreover, the technique al-lows saliency estimation with a single network layer, not requiring any layer search for map resolution enhancement.
Contributions of the paper are summarised below:
• We propose CAMERAS for precise backpropagation image saliency while preserving the sanity. Our method outperforms the state-of-the-art saliency meth-ods by a large margin, achieving up to 27.5% error re-duction for the popular pointing game metric [34].
• Exploring the newly found precision saliency map-ping with CAMERAS, we visualise differences in the semantic understanding of different architectures that govern their performance. We also highlight model-centric discrimination of input features for visually similar objects in never-before-seen details.
• Considering the equivalent treatment of deep mod-els as differentiable programs by the fast-developing parallel ﬁeld of adversarial learning, we enhance the widely considered strongest adversarial attack
PGD [18] with our saliency technique - drastically im-proving the efﬁcacy of the attack.
• The ability of precise saliency computation allowed by
CAMERAS calls for new quantitative metrics and san-ity checks. We contribute two new evaluation metrics and a sanity check to advance this research direction. 2.