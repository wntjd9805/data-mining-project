Abstract
While most conversational AI systems focus on textual dialogue only, conditioning utterances on visual context (when it’s available) can lead to more realistic conversations. Unfortunately, a major challenge for incorporating visual context into conversational dialogue is the lack of large-scale labeled datasets.
We provide a solution in the form of a new visually conditioned
Future Utterance Prediction task. Our task involves predicting the next utterance in a video, using both visual frames and transcribed speech as context. By exploiting the large number of instructional videos online, we train a model to solve this task at scale, without the need for manual annotations. Leveraging recent advances in multimodal learning, our model consists of a novel co-attentional multimodal video transformer, and when trained on both textual and visual context, outperforms baselines that use textual inputs alone. Further, we demonstrate that our model trained for this task on unlabelled videos achieves state-of-the-art performance on a number of downstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA. 1.

Introduction
Imagine that you are cooking an elaborate meal, but forget the next step in the recipe – or fixing your car and uncertain about which tool to pick up next. Developing an intelligent dialogue system1 that not only emulates human conversation, but also predicts and suggests future actions – not to mention is able to answer questions on complex tasks and topics – has long been a moonshot goal for the AI community. Conversational AI allows 1often used interchangeably with the term ‘conversational AI’ humans to interact with systems in free-form natural language, in the same way that we would communicate with one another.
This has led to an outpouring of research in NLP focused on conversational agents, ranging from goal-oriented systems for helping with reservations [6, 82] to chit-chat models [46, 69, 87], both of which are found in modern virtual assistants such as
Alexa, Google Assistant and Siri.
Such works, however, are limited to linguistic interactions only.
In contrast, human interaction in the physical world is facilitated through multiple modalities (e.g. verbal, visual, haptic), each modality often complementing the other seamlessly. While doing a task, it is often easier to show another person your progress, than to describe it verbally. Hence we argue that a truly intelligent dialogue system would have knowledge of both visual and textual contexts before making its next utterance. Unfortunately, a major challenge for incorporating visual context is a lack of suitable data. Most traditional conversational datasets [6, 10, 31, 71] are solely text based, and notoriously difficult to collect, relying on narrowly constructed ontologies [31, 55] and highly specific domains [6, 10]. More importantly, they do not contain visual information of the surrounding physical environment.
In an attempt to incorporate visual context to dialogue systems, the task of visual dialog [15] was proposed, which requires an
AI agent to hold a meaningful dialog with humans given an image [15] or a video [38]. In these works, a dialog history and question are artificially created for each image/video in a dataset, and the goal is then to infer context from history and answer the question accurately. Such datasets, while valuable, are created at great manual effort and contain artificially contrived scenarios, where the dialog history is not naturally present in video. Such datasets are also limited in size.
Unlike such works [15, 38], we propose to use online videos 116877
to learn from naturally co-occurring vision and dialogue in a scalable manner. We note that certain video domains such as narrated instructional videos [54, 79] and lifestyle vlogs [21, 34] are available in huge numbers (e.g. online on video sharing platforms) and are likely to contain narration explicitly linked to the visual content. Given the availability of high quality ASR, this gives us a large amount of readily available paired visual and textual data. We begin by proposing a future prediction task, where the goal is to predict the next utterance in an instructional video, given both visual and textual contexts (Figure 1). The labels for such a task are freely available from the video itself.
As we show in this work, solving such a task requires knowledge of both visual and textual contexts. Leveraging recent advances in multimodal learning, we do so with a two-stream co-attentional transformer based model, each stream encoding a different modality. Our co-attentional model effectively attends to features within each modality, as well as across modalities through lateral self-attention blocks. We demonstrate that using both visual and textual information leads to a large performance gain over using text alone, and additionally, our two-stream co-attentional model outperforms single stream multimodal models. In addition, we show that our model, trained for this future prediction task, can be transferred to other conversational tasks, achieving state-of-the-art performance on various VideoQA benchmarks.
Concretely, we make the following four contributions: (i) We formulate a future utterance prediction (FUP) task which uses both dialogue and vision; (ii) We re-purpose freely available online instructional video datasets to create training and testing benchmarks for this task (HowToFUP); (iii) We propose a new two-stream multimodal video transformer based architecture (CoMVT) which effectively attends jointly over words in text and visual objects and scenes to learn visual-dialogue context; and finally (iv) We show that our model trained on unlabelled instructional videos is also, perhaps surprisingly, able to achieve state-of-the-art performance on a number of downstream vision-language QA datasets, including MSRVTT-QA [85],
MSVD-QA [85], ActivityNet-QA [88], and How2QA [47]. 2.