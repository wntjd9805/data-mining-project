Abstract
Accuracy
Efficiency
We tackle the problem of visual search under resource constraints. Existing systems use the same embedding model to compute representations (embeddings) for the query and gallery images. Such systems inherently face a hard accuracy-efï¬ciency trade-off: the embedding model needs to be large enough to ensure high accuracy, yet small enough to enable query-embedding computation on resource-constrained platforms. This trade-off could be mitigated if gallery embeddings are generated from a large model and query embeddings are extracted using a com-pact model. The key to building such a system is to ensure representation compatibility between the query and gallery models. In this paper, we address two forms of compati-bility: One enforced by modifying the parameters of each model that computes the embeddings. The other by modify-ing the architectures that compute the embeddings, leading to compatibility-aware neural architecture search (CMP-NAS). We test CMP-NAS on challenging retrieval tasks for fashion images (DeepFashion2), and face images (IJB-C). Compared to ordinary (homogeneous) visual search us-ing the largest embedding model (paragon), CMP-NAS achieves 80-fold and 23-fold cost reduction while main-taining accuracy within 0.3% and 1.6% of the paragon on
DeepFashion2 and IJB-C respectively. 1.

Introduction
A visual search system in an â€œopen universeâ€ setting is often composed of a gallery model Ï†g and a query model
Ï†q, both mapping an input image to a vector representa-tion known as embedding. The gallery model Ï†g is typi-cally used to map a set of gallery images onto their embed-ding vectors, a process known as indexing, while the query
âˆ—Currently at the Georgia Institute of Technology. Work conducted during an internship with Amazon AI.
â€ Corresponding author
Compatible âˆš
Embedding space of ğœ™!
Compatible ?
Embedding space of ğœ™"
Compatible âˆš
Homogeneous  (ğœ™!, ğœ™!)
Heterogeneous  (ğœ™", ğœ™!)
Homogeneous  (ğœ™", ğœ™")
Gallery Set
Model ğœ™!
Query Image
Model ğœ™"
Figure 1: Homogeneous visual search uses the same embedding model, either large (orange) to meet performance speciï¬cations, or small (green) to meet cost constraints, forcing a dichotomy. Het-erogeneous Visual Search (blue) uses a large model to compute embeddings for the gallery, and a small model for the query im-ages. This allows high efï¬ciency without sacriï¬cing accuracy, pro-vided that the green and orange embedding models are designed and trained to be compatible. model extracts embeddings from query images to perform search against the indexed gallery. Most existing visual search approaches [24, 26, 38, 2, 31] use the same model architecture for both Ï†q and Ï†g. We refer to this setup as homogeneous visual search. An approach that uses differ-ent model architectures for Ï†q and Ï†g is referred to as het-erogeneous visual search (HVS).
The use of the same Ï†g = Ï†q trivially ensures that gallery and query images are mapped to the same vec-tor space where the search is conducted. However, this engenders a hard accuracy-efï¬ciency trade-off (Fig. 1)â€” choosing a large architecture Ï†g for both query and gallery achieves high-accuracy at a loss of efï¬ciency; choosing a small architecture Ï†q improves efï¬ciency to the detriment of accuracy, which is compounded since in practice, index-ing only happens sporadically while querying is performed continuously. This leads to efï¬ciency being driven mainly by the query model. HVS allows the use of a small model
Ï†q for querying, and a large model Ï†g for indexing, partly 10723
90 85 80 75 70 65 60 y c a r u c c
A l a v e i r t e
R e c a
F
N
: 1
Accuracy vs Flops 86.7 84.1 85.1 80.4 81.8 75.8 77.0 78.878.8 73.1 73.0 62.7 80 160 320 640 1280
Million Flops
ResNet-101
MobileNetV1
MobileNetV2
MobileNetV3
ShuffleNetV2
ProxyLessNAS
CMP-NAS(our) 2560 5120
Figure 2: The trade-off between accuracy and efï¬ciency for a het-erogeneous system performing 1:N Face retrieval on DeepFash-ion2. We use a ResNet-101 as the gallery model and compare dif-ferent architectures as query models. For MobileNetV1 and V2, we provide results with width 0.5Ã— and 1Ã—. mitigating the accuracy-complexity trade-off by enlarging the trade space. The challenge in HVS is to ensure that Ï†g and Ï†q live in the same metric (vector) space. This can be done for given architectures Ï†g, Ï†q, by training the weights so the resulting embeddings are metrically compatible [28].
However, one can also enlarge the trade space by including the architecture in the design of metrically compatible mod-els. Typically, Ï†g is chosen to match the best current state-of-the-art (paragon) while the designer can search among query architectures Ï†q to maximize efï¬ciency while ensur-ing that performance remains close to the paragon.
In this work, we pursue compatibility by optimizing both the model parameters (weights) as well as the model ar-chitecture. We show that (1) weight inheritance [19] and (2) backward-compatible training (BCT) [28] can achieve compatibility through weight optimization. Among these, the latter is more general in that it works with arbitrary em-bedding functions Ï†g and Ï†q. We expand beyond BCT to neural architecture search (NAS) [43, 4, 8, 30] with our proposed compatibility-aware NAS (CMP-NAS) strategy that searches for a query model Ï†q that is maximally ef-ï¬cient while being compatible with Ï†g. We hypothesize that CMP-NAS can simultaneously ï¬nd the architecture of query model and its weights that achieve efï¬ciency similar to that of the smallest (query) model, and accuracy close to that of the paragon (gallery model). Indeed the results in
Fig. 2 shows that CMP-NAS outperforms all of the state-of-the-art off-the-shelf architectures designed for mobile plat-forms with resource constraints. Compared with paragon (state-of-the-art high-compute homogeneous visual search) methods, HVS reduce query model ï¬‚ops by 23Ã— with only 1.6% in loss of search accuracy for the task of face retrieval.
Our contributions can be summarized as follows: 1) we demonstrate that an HVS system allows to better trade off accuracy and complexity, by optimizing over both model parameters and architecture. 2) We propose a novel CMP-NAS method combining weight-based compatibility with a novel reward function to achieve compatibility-aware archi-tecture search for HVS. 3) We show that our CMP-NAS can reduce model complexity many-fold with only a marginal drop in accuracy. For instance, we achieve 23Ã— reduction in ï¬‚ops with only 1.6% drop in retrieval accuracy on face retrieval using standard benchmarks. 2.