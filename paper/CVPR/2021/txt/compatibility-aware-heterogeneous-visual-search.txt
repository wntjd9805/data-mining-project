Abstract
Accuracy
Efficiency
We tackle the problem of visual search under resource constraints. Existing systems use the same embedding model to compute representations (embeddings) for the query and gallery images. Such systems inherently face a hard accuracy-efﬁciency trade-off: the embedding model needs to be large enough to ensure high accuracy, yet small enough to enable query-embedding computation on resource-constrained platforms. This trade-off could be mitigated if gallery embeddings are generated from a large model and query embeddings are extracted using a com-pact model. The key to building such a system is to ensure representation compatibility between the query and gallery models. In this paper, we address two forms of compati-bility: One enforced by modifying the parameters of each model that computes the embeddings. The other by modify-ing the architectures that compute the embeddings, leading to compatibility-aware neural architecture search (CMP-NAS). We test CMP-NAS on challenging retrieval tasks for fashion images (DeepFashion2), and face images (IJB-C). Compared to ordinary (homogeneous) visual search us-ing the largest embedding model (paragon), CMP-NAS achieves 80-fold and 23-fold cost reduction while main-taining accuracy within 0.3% and 1.6% of the paragon on
DeepFashion2 and IJB-C respectively. 1.

Introduction
A visual search system in an “open universe” setting is often composed of a gallery model φg and a query model
φq, both mapping an input image to a vector representa-tion known as embedding. The gallery model φg is typi-cally used to map a set of gallery images onto their embed-ding vectors, a process known as indexing, while the query
∗Currently at the Georgia Institute of Technology. Work conducted during an internship with Amazon AI.
†Corresponding author
Compatible √
Embedding space of 𝜙!
Compatible ?
Embedding space of 𝜙"
Compatible √
Homogeneous  (𝜙!, 𝜙!)
Heterogeneous  (𝜙", 𝜙!)
Homogeneous  (𝜙", 𝜙")
Gallery Set
Model 𝜙!
Query Image
Model 𝜙"
Figure 1: Homogeneous visual search uses the same embedding model, either large (orange) to meet performance speciﬁcations, or small (green) to meet cost constraints, forcing a dichotomy. Het-erogeneous Visual Search (blue) uses a large model to compute embeddings for the gallery, and a small model for the query im-ages. This allows high efﬁciency without sacriﬁcing accuracy, pro-vided that the green and orange embedding models are designed and trained to be compatible. model extracts embeddings from query images to perform search against the indexed gallery. Most existing visual search approaches [24, 26, 38, 2, 31] use the same model architecture for both φq and φg. We refer to this setup as homogeneous visual search. An approach that uses differ-ent model architectures for φq and φg is referred to as het-erogeneous visual search (HVS).
The use of the same φg = φq trivially ensures that gallery and query images are mapped to the same vec-tor space where the search is conducted. However, this engenders a hard accuracy-efﬁciency trade-off (Fig. 1)— choosing a large architecture φg for both query and gallery achieves high-accuracy at a loss of efﬁciency; choosing a small architecture φq improves efﬁciency to the detriment of accuracy, which is compounded since in practice, index-ing only happens sporadically while querying is performed continuously. This leads to efﬁciency being driven mainly by the query model. HVS allows the use of a small model
φq for querying, and a large model φg for indexing, partly 10723
90 85 80 75 70 65 60 y c a r u c c
A l a v e i r t e
R e c a
F
N
: 1
Accuracy vs Flops 86.7 84.1 85.1 80.4 81.8 75.8 77.0 78.878.8 73.1 73.0 62.7 80 160 320 640 1280
Million Flops
ResNet-101
MobileNetV1
MobileNetV2
MobileNetV3
ShuffleNetV2
ProxyLessNAS
CMP-NAS(our) 2560 5120
Figure 2: The trade-off between accuracy and efﬁciency for a het-erogeneous system performing 1:N Face retrieval on DeepFash-ion2. We use a ResNet-101 as the gallery model and compare dif-ferent architectures as query models. For MobileNetV1 and V2, we provide results with width 0.5× and 1×. mitigating the accuracy-complexity trade-off by enlarging the trade space. The challenge in HVS is to ensure that φg and φq live in the same metric (vector) space. This can be done for given architectures φg, φq, by training the weights so the resulting embeddings are metrically compatible [28].
However, one can also enlarge the trade space by including the architecture in the design of metrically compatible mod-els. Typically, φg is chosen to match the best current state-of-the-art (paragon) while the designer can search among query architectures φq to maximize efﬁciency while ensur-ing that performance remains close to the paragon.
In this work, we pursue compatibility by optimizing both the model parameters (weights) as well as the model ar-chitecture. We show that (1) weight inheritance [19] and (2) backward-compatible training (BCT) [28] can achieve compatibility through weight optimization. Among these, the latter is more general in that it works with arbitrary em-bedding functions φg and φq. We expand beyond BCT to neural architecture search (NAS) [43, 4, 8, 30] with our proposed compatibility-aware NAS (CMP-NAS) strategy that searches for a query model φq that is maximally ef-ﬁcient while being compatible with φg. We hypothesize that CMP-NAS can simultaneously ﬁnd the architecture of query model and its weights that achieve efﬁciency similar to that of the smallest (query) model, and accuracy close to that of the paragon (gallery model). Indeed the results in
Fig. 2 shows that CMP-NAS outperforms all of the state-of-the-art off-the-shelf architectures designed for mobile plat-forms with resource constraints. Compared with paragon (state-of-the-art high-compute homogeneous visual search) methods, HVS reduce query model ﬂops by 23× with only 1.6% in loss of search accuracy for the task of face retrieval.
Our contributions can be summarized as follows: 1) we demonstrate that an HVS system allows to better trade off accuracy and complexity, by optimizing over both model parameters and architecture. 2) We propose a novel CMP-NAS method combining weight-based compatibility with a novel reward function to achieve compatibility-aware archi-tecture search for HVS. 3) We show that our CMP-NAS can reduce model complexity many-fold with only a marginal drop in accuracy. For instance, we achieve 23× reduction in ﬂops with only 1.6% drop in retrieval accuracy on face retrieval using standard benchmarks. 2.