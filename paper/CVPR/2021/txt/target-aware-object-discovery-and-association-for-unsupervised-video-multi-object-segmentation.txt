Abstract
This paper addresses the task of unsupervised video multi-object segmentation. Current approaches follow a two-stage paradigm: 1) detect object proposals using pre-trained Mask R-CNN, and 2) conduct generic feature matching for temporal association using re-identiﬁcation techniques. However, the generic features, widely used in both stages, are not reliable for characterizing unseen ob-jects, leading to poor generalization. To address this, we introduce a novel approach for more accurate and efﬁcient spatio-temporal segmentation. In particular, to address in-stance discrimination, we propose to combine foreground region estimation and instance grouping together in one network, and additionally introduce temporal guidance for segmenting each frame, enabling more accurate object dis-covery. For temporal association, we complement current video object segmentation architectures with a discrimina-tive appearance model, capable of capturing more ﬁne-grained target-speciﬁc information. Given object propos-als from the instance discrimination network, three essen-tial strategies are adopted to achieve accurate segmenta-tion: 1) target-speciﬁc tracking using a memory-augmented appearance model; 2) target-agnostic veriﬁcation to trace possible tracklets for the proposal; 3) adaptive memory up-dating using the veriﬁed segments. We evaluate the pro-posed approach on DAVIS17 and YouTube-VIS, and the re-sults demonstrate that it outperforms state-of-the-art meth-ods both in segmentation accuracy and inference speed. 1.

Introduction
Unsupervised video object segmentation aims at au-tomatically segmenting primary object(s) from the back-ground in unconstrained videos, which is a fundamental vi-sion task. This task has become increasingly popular due to its potential values in a wide range of real-world ap-*Corresponding author: Jianwu Li. plications, e.g., video compression [17], autonomous driv-ing [14], and human-centric understanding [52, 67]. It also plays an essential role in collecting large-scale annotated dataset [34, 63]. However, the task is challenging due to the lack of prior knowledge about the target objects, as well as the challenging factors (e.g., occlusions, cluttered back-ground, diverse motion patterns) carried by video data.
Towards better segmenting the prominent foreground ob-jects, early studies typically exploit saliency cues [50, 11] or objectness priors [25, 64, 56, 66, 27] for identifying them. More recently, with the advent of deep neural net-works, many learning-based models have been proposed to learn more discriminative video object patterns, by lever-aging motion cues [43], addressing spatiotemporal fea-tures [23, 68], exploring multi-frame contextual informa-tion [32, 48, 61] or using recurrent networks to capture sequential information [42]. Though impressive results have been achieved, these approaches mainly focus on fore-ground/background separation, hindering their applications in more practical multi-object scenarios.
Unsupervised video multi-object segmentation, with an elegant and formal deﬁnition in [4], is more challenging as it requires not only discovering instance-agnostic, fore-ground regions automatically, but also discriminating dif-ferent object instances and associating the same identi-ties over the entire sequence. To tackle this task, exist-ing methods [33, 42, 49, 65] generally follow the conven-tional tracking-by-detection paradigm which performs in a top-down fashion to employ image-aware instance seg-mentation networks (e.g., Mask R-CNN [18], SOLO [54]) to detect object candidates in individual frames, and asso-ciate them over consecutive frames based on object track-to ing or proposal re-identiﬁcation (ReID). In addition, avoid the negative impact of background objects, many studies [42, 49, 65] also rely on a foreground/background separation step to remove background proposals. Even though these approaches demonstrate compelling perfor-mance, they still suffer several limitations. 1) Directly using image-level instance segmentation networks is insufﬁcient 6985
since they are trained on static images, neglecting the in-formative temporal context in videos. 2) Instance segmen-tation and foreground estimation are often separately con-sidered by different networks, incurring high computational expense. 3) ReID-based matching networks, trained com-pletely ofﬂine, focus more on general object appearance, while rarely capturing distinctive ﬁne-grained features of speciﬁc targets.
In this work, we propose a novel approach for unsuper-vised multi-object segmentation in unconstrained videos.
To address points 1) and 2), we introduce an instance dis-crimination network (D-Net) for video object proposal. The network performs in a bottom-up fashion and takes video temporal information into account to achieve better seg-mentation accuracy and efﬁciency. In particular, the D-Net includes two branches: a foreground estimation branch fol-lows the typical design of fully convolutional networks to segment attention-grabbing objects, and an instance seg-mentation branch learns to predict the instance center as well as the offset from each pixel to its corresponding cen-ter for instance grouping. Rather than processing each frame independently, we consider segmentation of previous frames as an important guidance for segmenting the cur-rent frame. In this way, we integrate instance-agnostic and instance-aware segmentation together into one network for discovering temporal coherent object proposal.
To address point 3), we design a target-aware track-ing network (T-Net) for associating object proposals of the same identities over each image sequence. Different from previous ReID-based matching techniques, we aim to learn target-speciﬁc appearance features for more robust object association. More speciﬁcally, the T-Net learns a discrimi-native appearance model for each object instance during the inference stage to predict a coarse but robust segmentation score of the target object. Note that the appearance model is more prone to drifting due to the lack of ground-truth an-notations. Therefore, we further propose a target-agnostic backward veriﬁcation module to examine the tracking re-sults. The veriﬁed results are used as new training samples to update the appearance model online.
With above efforts, our algorithm achieves state-of-the-art results on the DAVIS17 benchmark for video multi-object segmentation. It also demonstrates compelling per-formance for video instance segmentation on YouTube-VIS. In addition, our approach obtains a better trade-off be-tween segmentation accuracy and inference efﬁciency, run-ning at about 10 FPS on images with 480p resolution.
To sum up, the contributions of the proposed approach are three-fold: First, we propose a novel bottom-up in-stance discrimination network which takes advantage of temporal context information in videos for more accurate segmentation. The network couples foreground discov-ery and instance grouping together, beneﬁting from multi-tasking and improving the inference efﬁciency. Second, we introduce a target-aware tracking model for online match-ing of object proposals. Compared with target-agnostic ap-proaches, our method can better capture the appearance in-formation of the target objects, yielding more robust asso-ciation. Third, our approach achieves compelling perfor-mance on the popular DAVIS17 and YouTube-VIS bench-marks. Furthermore, its high inference speed enables our method to support a wide variety of practical applications. 2.