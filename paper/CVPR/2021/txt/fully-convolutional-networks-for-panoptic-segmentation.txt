Abstract
In this paper, we present a conceptually simple, strong, and efﬁcient framework for panoptic segmentation, called
Panoptic FCN. Our approach aims to represent and pre-dict foreground things and background stuff in a uniﬁed fully convolutional pipeline. In particular, Panoptic FCN encodes each object instance or stuff category into a spe-ciﬁc kernel weight with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent prosperties for things and stuff can be respectively satisﬁed in a simple generate-kernel-then-segment workﬂow. Without extra boxes for localization or instance separation, the proposed approach outperforms previous box-based and -free models with high efﬁciency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. Our code is made publicly available at https://github.com/Jia-Research-Lab/PanopticFCN.1 1.

Introduction
Panoptic segmentation, aiming to assign each pixel with a semantic label and unique identity, is regarded as a chal-lenging task. In panoptic segmentation [19], countable and uncountable instances (i.e., things and stuff) are expected to be represented and resolved in a uniﬁed workﬂow. One main difﬁculty impeding uniﬁed representation comes from conﬂicting properties requested by things and stuff. Specif-ically, to distinguish among various identities, countable things usually rely on instance-aware features, which vary with objects. In contrast, uncountable stuff would prefer se-mantically consistent characters, which ensures consistent predictions for pixels with the same semantic meaning. An example is given in Fig. 1, where embedding of individuals should be diverse for inter-class variations, while characters of grass should be similar for intra-class consistency. 1Part of the work was done in MEGVII Research. (a) Separate representation (b) Uniﬁed representation
Figure 1. Compared with traditional methods, which often utilize separate branches to handle things and stuff in 1(a), the proposed
Panoptic FCN 1(b) represents things and stuff uniformly with gen-erated kernels. Here, an example with box-based stream for things is given in 1(a). The shared backbone is omitted for concision.
For conﬂict at feature level, speciﬁc modules are usu-ally tailored for things and stuff separately, as presented in
Fig. 1(a). In particular, instance-aware demand of things is satisﬁed mainly from two streams, namely box-based [18, 50, 25] and box-free [51, 10, 6] methods. Meanwhile, the semantic-consistency of stuff is met in a pixel-by-pixel manner [33], where similar semantic features would bring identical predictions. A classic case is Panoptic FPN [18], which utilizes Mask R-CNN [12] and FCN [33] in sepa-rated branches to respectively classify things and stuff, sim-ilar to that of Fig. 1(a). Although attempt [51, 10, 6] has been made to predict things without boxes, extra predictions (e.g., afﬁnities [10], and offsets [51]) together with post-process procedures are still needed to distinguish among instances, which slow down the whole system and hinder it from being fully convolutional. Consequently, a uniﬁed representation is required to bridge this gap.
In this paper, we propose a fully convolutional frame-work for uniﬁed representation, called Panoptic FCN. In particular, Panoptic FCN encodes each instance into a spe-ciﬁc kernel and generates the prediction by convolutions di-rectly. Thus, both things and stuff can be predicted together 214
with a same resolution. In this way, instance-aware and se-mantically consistent properties for things and stuff can be respectively satisﬁed in a uniﬁed workﬂow, which is brieﬂy illustrated in Fig. 1(b). To sum up, the key idea of Panoptic
FCN is to represent and predict things and stuff uniformly with generated kernels in a fully convolutional pipeline.
To this end, kernel generator and feature encoder are respectively designed for kernel weights generation and shared feature encoding. Speciﬁcally, in kernel genera-tor, we draw inspirations from point-based object detec-tors [20, 55] and utilize the position head to locate as well as classify foreground objects and background stuff by object centers and stuff regions, respectively. Then, we select kernel weights [17] with the same positions from the kernel head to represent corresponding instances. For the instance-awareness and semantic-consistency described above, a kernel-level operation, called kernel fusion, is fur-ther proposed, which merges kernel weights that are pre-dicted to have the same identity or semantic category. With a naive feature encoder, which preserves the high-resolution feature with details, each prediction of things and stuff can be produced by convolving with generated kernels directly.
In general, the proposed method can be distinguished from two aspects. Firstly, different from previous work for things generation [12, 4, 45], which outputs dense pre-dictions and then utilizes NMS for overlaps removal, the deigned framework generates instance-aware kernels and produces each speciﬁc instance directly. Moreover, com-pared with traditional FCN-based methods for stuff predic-tion [53, 3, 9], which select the most likely category in a pixel-by-pixel manner, our approach aggregates global con-text into semantically consistent kernels and presents results of existing semantic classes in a whole-instance manner.
The overall approach, named Panoptic FCN, can be eas-ily instantiated for panoptic segmentation, which will be fully elaborated in Sec. 3. To demonstrate its superior-ity, we give extensive ablation studies in Sec. 4.2. Fur-thermore, experimental results are reported on COCO [29],
Cityscapes [8], and Mapillary Vistas [35] datasets. With-out bells-and-whistles, Panoptic FCN outperforms previous methods with efﬁciency, and respectively attains 44.3% PQ and 47.5% PQ on COCO val and test-dev set. Meanwhile, it surpasses all similar box-free methods by large margins and achieves leading performance on Cityscapes and Mapillary
Vistas val set with 61.4% PQ and 36.9% PQ, respectively. 2.