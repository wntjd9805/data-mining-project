Abstract
Panoptic segmentation presents a new challenge in ex-ploiting the merits of both detection and segmentation, with the aim of unifying instance segmentation and semantic seg-mentation in a single framework. However, an efﬁcient so-lution for panoptic segmentation in the emerging domain of
LiDAR point cloud is still an open research problem and is very much under-explored. In this paper, we present a fast and robust LiDAR point cloud panoptic segmentation framework, referred to as Panoptic-PolarNet. We learn both semantic segmentation and class-agnostic instance clustering in a single inference network using a polar Bird’s
Eye View (BEV) representation, enabling us to circum-vent the issue of occlusion among instances in urban street scenes. To improve our network’s learnability, we also pro-pose an adapted instance augmentation technique and a novel adversarial point cloud pruning method. Our exper-iments show that Panoptic-PolarNet outperforms the base-line methods on SemanticKITTI and nuScenes datasets with an almost real-time inference speed. Panoptic-PolarNet achieved 54.1% PQ in the public SemanticKITTI panoptic segmentation leaderboard and leading performance for the validation set of nuScenes. 1.

Introduction
As a crucial step in applications such as autonomous driving and robotics, processing and analyzing 3D scanning data have received increasing attention in recent years in computer vision and deep learning. Panoptic segmentation is a recently introduced problem in the image domain [20] that presents a new challenge in unifying instance segmen-tation and semantic segmentation in a single training archi-tecture. With the recent introduction of new LiDAR point cloud datasets [2, 5, 13] that include both pixel-wise seman-tic label annotation and object annotation, this problem can now be also explored for 3D scanning data as we propose
∗ Contributed equally.
† Now at Waymo LLC.
Code at: https://github.com/edwardzhou130/Panoptic-PolarNet.
Figure 1: SemanticKITTI [1] panoptic quality vs. single frame inference latency. The green line marks the sampling rate of the LiDAR scanner, which spins at 10 frames-per-second. Our proposed Panoptic-PolarNet outperforms other baselines in both speed and PQ. in this paper.
By deﬁnition, Panoptic segmentation requires that we identify both class labels and instance id’s for points in the
“thing” classes, and only the class labels for points in the
“stuff” classes. To solve this problem, the ﬁrst question to answer is: What information is needed in order to obtain a panoptic segmentation of data? It can be either the se-mantic label of all points and the instance clustering of the
“thing” classes, or the instance segmentation of the “thing” classes and the class labels of remaining “stuff” classes. As a consequence, these two alternative designs would lead to two different categories of panoptic segmentation, known as proposal-free and proposal-based, the former being adapted from a semantic segmentation network [27] and the latter adapted from an object detection network [16]. 2D image panoptic segmentation faces two main prob-lems. First, proposal-based ones segment instances inde-pendently within each individual object proposal. Such ap-proaches require extra architectural modiﬁcations [25, 50] to compensate for the impact of heavy object collision in the proposals. Second, semantic segmentation and instance segmentation are usually handled in two separate prediction 13194
heads in order to tailor the design of the dedicated network to each task. However, this may inevitably introduce either potential conﬂicts or redundant information since these two tasks clearly share common characteristics. For example, in the proposal-based methods, semantic and instance heads can yield different label predictions at the same pixel. And in the proposal-free methods, the features learned in the in-stance head have signiﬁcant correlations with class labels.
Both cases ultimately lead to inference inefﬁciency. 3D panoptic segmentation, on the other hand, is by and large at its infancy and still an open research problem. It is mainly motivated by LiDAR point cloud processing in ap-plications such as self-driving cars, autonomous robot nav-igation, and environment mapping, all of which generally require real-time processing. On the other hand, compared to conventional 3D data in computer vision, LiDAR point clouds are irregularly sampled in the 3D space. These dif-ferences in terms of the nature of the 3D data, the need for real-time processing, and the level of accuracy needed for safety and security (e.g., in self-driving cars) are clearly creating new challenges, encouraging new innovative solu-tions. These challenges motivated us to ﬁnd a more suitable architecture that takes into account the unique characteris-tics of LiDAR data, efﬁciently solves panoptic segmenta-tion with minimum conﬂicts in predictions (instance versus class), and achieves real-time or near real-time speed with-out compromising accuracy.
Given the speed limitation, proposal-free methods nat-urally seem to be a more favorable choice, since they are proven to perform better in computational time in the 2D case. Therefore, starting from a backbone semantic pre-diction network [55], our ﬁrst goal is to integrate it with a network for class-agnostic instance clustering. We hypoth-esize that most “thing” class objects in the LiDAR point cloud are separable when projected onto the XY-plane. In-stance separability implies that the discretized BEV rep-resentation [46] is highly suitable for LiDAR point cloud instance clustering. Therefore, we can use the same net-work of PolarNet also to generate discriminative features for separating instances in the BEV. Based on these obser-vations and assumptions, we propose a panoptic segmen-tation framework that simultaneously learns semantic and instance features on the discretized BEV map. Therefore, we follow the backbone network design of PolarNet [55] to generate the 3D semantic prediction and use a lightweight 2D instance head inspired by Panoptic-DeepLab [7] on top of it. Predictions from semantic and instance heads are then fused through a majority voting to create the ﬁnal panoptic segmentation. This results in a highly efﬁcient proposal-free panoptic segmentation network design, which we refer to as Panoptic-PolarNet.
We evaluated our approach on SemanticKITTI and nuScenes datasets. Panoptic-PolarNet achieves state-of-the-art performance. Compared to the PolarNet, our in-stance segmentation head only introduces 0.1M parameters and increases the inference time by only 0.027s.
Our contributions are summarized as follows:
• We propose a model taking into account the speciﬁc nature of LiDAR data and the applications in mind, to construct a proposal-free LiDAR panoptic segmenta-tion network that can efﬁciently cluster instances on top of the semantic segmentation.
• Unlike existing panoptic segmentation networks that generally use two entirely separate decoding modules for semantic and instance segmentation and rely on an attention module to connect the learned information, our networks share decoding layers among them, al-lowing for early fusion at feature extraction level. This early fusion strategy has two substantial impacts: (1) it reduces redundancy between the networks and there-fore improves computational efﬁciency; (2) increases the PQ measure despite a smaller computation load.
• Compared to existing proposal-based panoptic seg-mentation methods that suffer from class and instance prediction overlapping, we propose a proposal-free de-sign and train the instance head without bounding box annotation, which allows us to avoid the conﬂict of class prediction.
• We introduce two novel point cloud data augmentation methods that can apply to any other LiDAR segmenta-tion networks.
• Experiments show that our approach outperforms strong baselines on SemanticKITTI and nuScenes datasets with smaller and near-real-time latency, as shown in Figure 1. 2.