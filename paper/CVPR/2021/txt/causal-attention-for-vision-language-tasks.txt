Abstract
We present a novel attention mechanism: Causal Atten-tion (CATT), to remove the ever-elusive confounding effect in existing attention-based vision-language models. This ef-fect causes harmful bias that misleads the attention mod-ule to focus on the spurious correlations in training data, damaging the model generalization. As the confounder is unobserved in general, we use the front-door adjustment to realize the causal intervention, which does not require any knowledge on the confounder. Speciﬁcally, CATT is implemented as a combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention (CS-ATT), where the latter forcibly brings other samples into every IS-ATT, mimicking the causal intervention. CATT abides by the Q-K-V convention and hence can replace any attention mod-ule such as top-down attention and self-attention in Trans-formers. CATT improves various popular attention-based vision-language models by considerable margins. In partic-ular, we show that CATT has great potential in large-scale pre-training, e.g., it can promote the lighter LXMERT [57], which uses fewer data and less computational power, com-parable to the heavier UNITER [14]. Code is published in https://github.com/yangxuntu/lxmertcatt. 1.

Introduction
Stemming from the strong cognitive evidences in selec-tive signal processing [59, 50], the attention mechanism has arguably become the most indispensable module in vision and language models [66, 5, 3, 16, 11, 36]. Although its idiosyncratic formulation varies from task to task, its nature can be summarized as the following common Q-K-V nota-tion: given a query q, the attention mechanism associates q to each feature value vi by using the normalized atten-tive weight αi ∝ qT ki, where ki is the key function of the value; thus, the resultant selective feature value — attention
— is Pi αivi. In a modern view, the attention can be un-Figure 1. Top: an example of image captioner with a self-attention the corresponding and a top-down attention modules. Bottom: causal graph. The reason why the prediction is “riding” but not
“driving” is explained in Figure 3. derstood as a feature transformer that encodes input query q by using the given values V = {vi} [60].
Taking image captioning as an example in Figure 1, if q and V are both encoded from the input X, e.g., the
RoI features of an image, we call it self-attention; if q is changed to the sentence context, we call it top-down atten-tion. Intuitively, self-attention is usually viewed as a non-local [65] (or graph [7]) convolution network that enriches each local value with global relationship features; top down-attention is used to enrich the context with the cross-domain relationship features [3]. Both of them can be combined and stacked into deep networks, serving as powerful multi-modal encoder-decoder transformer networks [32, 12].
As a bridge connecting the input feature X and the out-put label Y , the quality of attention — how reasonable the attentive weight α is — plays a crucial role for the overall performance. However, due to the fact that the at-tention weights are unsupervised, e.g., there is no word-region grounding for the top-down attention or relationship dependency annotation for the self-attention, the weights will be inevitably misled by the dataset bias. For exam-ple, as shown in Figure 1, since there are many images captioned with “person riding horse” in the training data, self-attention learns to infer “riding” by building the depen-dency between “person” and “horse”. Then, given a test 9847
#“Sport+Man” / #“Sport+Screen”=213
#“Color+Girl” / #“Color+Necklace”=54
#“Board+Man” / #“Board+Woman”=20
Q: What sport is being shown on the  screen?
A: Dancing (Bowling)
Q: What color is the girl's necklace?
A: Black (White)
Q: What gender is the person  standing up?
A: Male (Female)
Figure 2. Before pre-training (e.g., LXMERT [57]), attentions are correct (blue). After pre-training, attentions are wrong (red). This is because the co-occurrences of some concepts appear much more often than others, e.g., “Sport+Man” appears 213 times more than
“Sport+Screen” in the pre-training data. image with “person driving carriage”, this self-attention still tends to relate “person” with “horse” to infer “riding”, but ignoring the “carriage”. Unfortunately, such bias cannot be mitigated by simply enlarging the dataset scale, as most of the bias abides by the data nature — Zipf’s law [48] and social conventions [19] — there are indeed more “red ap-ple” than “green apple” or “person standing” than “person dancing”. Therefore, as shown in Figure 2, large-scale pre-training may lead to even worse attentions. links
The dataset bias
Figure 3. This expands causal the of the confounding path
X L9999K Y in Figure 1
. is es-sentially caused by the con-founder, a common cause that makes X and Y correlated even if X and Y have no direct cau-sation. We illustrate this crucial idea in Figure 3. Suppose that the confounder C is the com-mon sense1 “person can ride horse”, C → X denotes that a visual scene is generated by such knowledge, e.g., the dataset curator observes and cap-tures the common sense; X → M denotes the fact that the objects M = {person, horse} can be detected (e.g.,
Faster R-CNN [49]), whose object inventory is determined by C → M ; M → Y denotes the language generation for “person riding horse”. Note that besides the legitimate causal path from image X via object M to Y , the “back-door” path X ← C → M → Y also contributes an effect to Y . Therefore, if we only train the model based on the correlation P (Y |X) without knowing the confounding ef-fect, no matter how large the amount of training data is, the model can never identify the true causal effect from X to Y [41, 52]. For example, if the confounder distribution varies from training to testing, e.g., the common sense “per-son can ride horse” is dominantly more often than the com-mon sense “person can drive carriage” in training, but the latter is more often than the former in testing, then P (Y |X) based on “person can ride horse” in training will be no longer applicable in testing [42].
In this paper, we propose a novel attention mechanism 1It is also well-known as the disentangled causal mechanism [56]. called: Causal Attention (CATT), which can help the mod-els identify the causal effect between X and Y , and thus mitigates the bias caused by confounders. It is based on the front-door adjustment principle that does not require the as-sumption of any observed confounder [40], and thus CATT can be applied in any domain where the attention resides.
In this way, CATT is fundamentally different from exist-ing deconfounding methods based on the backdoor adjust-ment [76, 64], which has to be domain-speciﬁc to comply with the observed-confounder assumption. Speciﬁcally, we
ﬁrst show that the conventional attention is indeed an im-proper approximation of the front-door principle, and then we show what is a proper one, which underpins CATT the-oretically (Section 3.1).
We build CATT on the proposed In-Sample attention (IS-ATT) and Cross-Sample attention (CS-ATT), which abides by the Q-K-V operations (Section 3.2). In particular, the parameters of the Q-K-V operations can also be shared be-tween both IS-ATT and CS-ATT to further improve the ef-ﬁciency in some architectures. We replace the the con-ventional attention with CATT in various vision-language including the clas-models to validate its effectiveness, sic Bottom-Up Top-Down LSTM [3], Transformer [60], and a large-scale vision-language pre-training (VLP) model
LXMERT [57]. The experimental results demonstrate that our CATT can achieve consistent improvements for all of them. Signiﬁcantly, our light LXMERT+CATT outper-forms the heavy UNITER [14] on VQA2.0, i.e., 73.04% vs. 72.91% on test-std split, and NLVR2, i.e., 76.0% vs. 75.80% on test-P split, while we require much fewer pre-training burdens: 624 vs. 882 V100 GPU hours. Such com-parisons show that our CATT has great potential in vision-language pre-training (VLP) tasks. 2.