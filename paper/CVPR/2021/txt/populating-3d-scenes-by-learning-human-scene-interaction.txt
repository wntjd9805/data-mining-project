Abstract
Humans live within a 3D space and constantly interact with it to perform tasks. Such interactions involve physi-cal contact between surfaces that is semantically meaning-ful. Our goal is to learn how humans interact with scenes and leverage this to enable virtual characters to do the same. To that end, we introduce a novel Human-Scene In-teraction (HSI) model that encodes proximal relationships, called POSA for “Pose with prOximitieS and contActs”.
The representation of interaction is body-centric, which en-ables it to generalize to new scenes. Speciﬁcally, POSA augments the SMPL-X parametric human body model such that, for every mesh vertex, it encodes (a) the contact prob-ability with the scene surface and (b) the corresponding semantic scene label. We learn POSA with a VAE con-ditioned on the SMPL-X vertices, and train on the PROX dataset, which contains SMPL-X meshes of people interact-ing with 3D scenes, and the corresponding scene seman-tics from the PROX-E dataset. We demonstrate the value of POSA with two applications. First, we automatically place 3D scans of people in scenes. We use a SMPL-X model ﬁt to the scan as a proxy and then ﬁnd its most likely placement in 3D. POSA provides an effective representa-tion to search for “affordances” in the scene that match the likely contact relationships for that pose. We perform a perceptual study that shows signiﬁcant improvement over the state of the art on this task. Second, we show that
POSA’s learned representation of body-scene interaction supports monocular human pose estimation that is consis-tent with a 3D scene, improving on the state of the art.
Our model and code are available for research purposes at https://posa.is.tue.mpg.de. 1.

Introduction
Humans constantly interact with the world around them.
We move by walking on the ground; we sleep lying on a bed; we rest sitting on a chair; we work using touchscreens and keyboards. Our bodies have evolved to exploit the af-fordances of the natural environment and we design objects 14708
to better “afford” our bodies. While obvious, it is worth stat-ing that these physical interactions involve contact. Despite the importance of such interactions, existing representations of the human body do not explicitly represent, support, or capture them.
In computer vision, human pose is typically estimated in isolation from the 3D scene, while in computer graphics 3D scenes are often scanned and reconstructed without people.
Both the recovery of humans in scenes and the automated synthesis of realistic people in scenes remain challenging problems. Automation of this latter case would reduce an-imation costs and open up new applications in augmented reality. Here we take a step towards automating the realistic placement of 3D people in 3D scenes with realistic con-tact and semantic interactions (Fig. 1). We develop a novel body-centric approach that relates 3D body shape and pose to possible world interactions. Learned parametric 3D hu-man models [2, 28, 39, 46] represent the shape and pose of people accurately. We employ the SMPL-X [46] model, which includes the hands and face, as it supports reasoning about contact between the body and the world.
While such body models are powerful, we make three key observations. First, human models like SMPL-X [46] do not explicitly model contact. Second, not all parts of the body surface are equally likely to be in contact with the scene. Third, the poses of our body and scene semantics are highly intertwined. Imagine a person sitting on a chair; body contact likely includes the buttocks, probably also the back, and maybe the arms. Think of someone opening a door; their feet are likely in contact with the ﬂoor, and their hand is in contact with the doorknob.
Based on these observations, we formulate a novel model, that makes human-scene interaction (HSI) an ex-plicit and integral part of the body model. The key idea is to encode HSI in an ego-centric representation built in
SMPL-X. This effectively extends the SMPL-X model to capture contact and the semantics of HSI in a body-centric representation. We call this POSA for “Pose with prOxim-itieS and contActs”. Speciﬁcally, for every vertex on the body and every pose, POSA deﬁnes a probabilistic feature map that encodes the probability that the vertex is in con-tact with the world and the distribution of semantic labels associated with that contact.
POSA is a conditional Variational Auto-Encoder (cVAE), conditioned on SMPL-X vertex positions. We train on the PROX dataset [22], which contains 20 subjects, ﬁt with SMPL-X meshes, interacting with 12 real 3D scenes.
We also train POSA using use the scene semantic annota-tions provided by the PROX-E dataset [65]. Once trained, given a posed body, we can sample likely contacts and se-mantic labels for all vertices. We show the value of this representation with two challenging applications.
First, we focus on automatic scene population as illus-trated in Fig. 1. That is, given a 3D scene and a body in a particular pose, where in the scene is this pose most likely?
As demonstrated in Fig. 1 we use SMPL-X bodies ﬁt to commercial 3D scans of people [45], and then, conditioned on the body, our cVAE generates a target POSA feature map. We then search over possible human placements while minimizing the discrepancy between the observed and tar-get feature maps. We quantitatively compare our approach to PLACE [64], which is SOTA on a similar task, and ﬁnd that POSA has higher perceptual realism.
Second, we use POSA for monocular 3D human pose estimation in a 3D scene. We build on the PROX method
[22] that hand-codes contact points, and replace these with our learned feature map, which functions as an HSI prior.
This automates a heuristic process, while producing lower pose estimation errors than the original PROX method.
To summarize, POSA is a novel model that intertwines
SMPL-X pose and scene semantics with contact. To the best of our knowledge, this is the ﬁrst learned human body model that incorporates HSI in the model. We think this is important because such a model can be used in all the same ways that models like SMPL-X are used but now with the addition of body-scene interaction. The key nov-elty is posing HSI as part of the body representation it-self. Like the original learned body models, POSA pro-vides a platform that people can build on. To facilitate this, our model and code are available for research purposes at https://posa.is.tue.mpg.de. 2.