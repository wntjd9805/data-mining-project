Abstract
We consider the problem of generating realistic trafﬁc scenes automatically. Existing methods typically insert ac-tors into the scene according to a set of hand-crafted heuris-tics and are limited in their ability to model the true com-plexity and diversity of real trafﬁc scenes, thus inducing a content gap between synthesized trafﬁc scenes versus real ones. As a result, existing simulators lack the ﬁdelity neces-sary to train and test self-driving vehicles. To address this limitation, we present SceneGen—a neural autoregressive model of trafﬁc scenes that eschews the need for rules and heuristics. In particular, given the ego-vehicle state and a high deﬁnition map of surrounding area, SceneGen inserts actors of various classes into the scene and synthesizes their sizes, orientations, and velocities. We demonstrate on two large-scale datasets SceneGen’s ability to faithfully model distributions of real trafﬁc scenes. Moreover, we show that
SceneGen coupled with sensor simulation can be used to train perception models that generalize to the real world. 1.

Introduction
The ability to simulate realistic trafﬁc scenarios is an important milestone on the path towards safe and scalable self-driving. It enables us to build rich virtual environments in which we can improve our self-driving vehicles (SDVs) and verify their safety and performance [9, 31, 32, 53].
This goal, however, is challenging to achieve. As a ﬁrst step, most large-scale self-driving programs simulate pre-recorded scenarios captured in the real world [32] or em-ploy teams of test engineers to design new scenarios [9, 31].
Although this approach can yield realistic simulations, it is ultimately not scalable. This motivates the search for a way to generate realistic trafﬁc scenarios automatically.
More concretely, we are interested in generating the lay-out of actors in a trafﬁc scene given the SDV’s current state and a high deﬁnition map (HD map) of the surround-*Indicates equal contribution. Work done at Uber ATG.
Figure 1: Given the SDV’s state and an HD map, SceneGen autoregressively inserts actors onto the map to compose a realistic trafﬁc scene. The ego SDV is shown in red; vehi-cles in blue; pedestrians in orange; and bicyclists in green. ing area. We call this task trafﬁc scene generation (see
Fig. 1). Here, each actor is parameterized by a class label, a bird’s eye view bounding box, and a velocity vector. Our lightweight scene parameterization is popular among exist-ing self-driving simulation stacks and can be readily used in downstream modules; e.g., to simulate LiDAR [9, 10, 32].
A popular approach to trafﬁc scene generation is to use procedural models to insert actors into the scene accord-ing to a set of rules [55, 31, 9, 37]. These rules encode reasonable heuristics such as “pedestrians should stay on the sidewalk” or “vehicles should drive along lane center-lines”, and their parameters can be manually tuned to give reasonable results. Still, these simplistic heuristics cannot fully capture the complexity and diversity of real world trafﬁc scenes, thus inducing a content gap between syn-thesized trafﬁc scenes and real ones [26]. Moreover, this approach requires signiﬁcant time and expertise to design good heuristics and tune their parameters.
To address these issues, recent methods use machine learning techniques to automatically tune model parame-892
ters [52, 51, 24, 26, 8]. These methods improve the realism and scalability of trafﬁc scene generation. However, they remain limited by their underlying hand-crafted heuristics and priors; e.g., pre-deﬁned scene grammars or assumptions about road topologies. As a result, they lack the capacity to model the true complexity and diversity of real trafﬁc scenes and, by extension, the ﬁdelity necessary to train and test SDVs in simulation. Alternatively, we can use a simple data-driven approach by sampling from map-speciﬁc em-pirical distributions [10]. But this cannot generalize to new maps and may yield scene-inconsistent samples.
In this paper, we propose SceneGen—a trafﬁc scene gen-eration model that eschews the need for hand-crafted rules and heuristics. Our approach is inspired by recent successes in deep generative modeling that have shown remarkable results in estimating distributions of a variety of data, with-out requiring complex rules and heuristics; e.g., handwrit-ing [18], images [49], text [39], etc. Speciﬁcally, SceneGen is a neural autoregressive model that, given the SDV’s cur-rent state and an HD map of the surrounding area, sequen-tially inserts actors into the scene—mimicking the process by which humans do this as well. As a result, we can sam-ple realistic trafﬁc scenes from SceneGen and compute the likelihood of existing ones as well.
We evaluate SceneGen on two large-scale self-driving datasets. The results show that SceneGen can better esti-mate the distribution over real trafﬁc scenes than compet-ing baselines and generate more realistic samples as well.
Furthermore, we show that SceneGen coupled with sensor simulation can generate realistic labeled data to train per-ception models that generalize to the real world. With Sce-neGen, we take an important step towards developing SDVs safely and scalably through large-scale simulation. We hope our work here inspires more research along this direction so that one day this goal will become a reality. 2.