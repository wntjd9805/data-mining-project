Abstract
Federated learning (FL) is a popular distributed learn-ing framework that can reduce privacy risks by not ex-plicitly sharing private data. However, recent works have demonstrated that sharing model updates makes FL vulner-able to inference attack. In this work, we show our key ob-servation that the data representation leakage from gradi-ents is the essential cause of privacy leakage in FL. We also provide an analysis of this observation to explain how the data presentation is leaked. Based on this observation, we propose a defense called Soteria against model inversion attack in FL. The key idea of our defense is learning to per-turb data representation such that the quality of the recon-structed data is severely degraded, while FL performance is maintained. In addition, we derive a certiﬁed robustness guarantee to FL and a convergence guarantee to FedAvg, after applying our defense. To evaluate our defense, we conduct experiments on MNIST and CIFAR10 for defend-ing against the DLG attack and GS attack. Without sacri-ﬁcing accuracy, the results demonstrate that our proposed defense can increase the mean squared error between the reconstructed data and the raw data by as much as 160× for both DLG attack and GS attack, compared with base-line defense methods. Therefore, the privacy of the FL sys-tem is signiﬁcantly improved. Our code can be found at https://github.com/jeremy313/Soteria. 1.

Introduction
Federated learning (FL) [16] is a popular distributed learning approach that enables a number of devices to train a shared model in a federated fashion without transferring their local data. A central server coordinates the FL pro-cess, where each participating device communicates only the model parameters on the central server while keeping local data private. Thus, FL becomes a natural choice for developing mobile deep learning applications, such as next-word prediction [10], emoji prediction [22], etc.
Privacy preservation is the major motivation for propos-ing FL. However, recent works demonstrated that shar-ing model updates or gradients also makes FL vulnera-ble to inference attack, e.g., property inference attack [18] and model inversion attack [5, 28, 7, 26]. Here prop-erty inference attack infers sensitive properties of training data using the model updates, and model inversion attack reconstructs training data using model gradients. How-ever, the essential causes of such privacy leakages have not been thoroughly investigated or explained. Some de-fense strategies have been presented to prevent the privacy leakage and can be categorized into three types: differen-tial privacy [21, 24, 9, 17, 8], secure multi-party computa-tion [4, 19, 3, 18], and data compression [28]. But these defensive approaches incur either signiﬁcant computational overheads or unignorable accuracy loss. The reason is that existing defenses are not speciﬁcally designed for the pri-vacy leakage from the communicated local updates. The privacy issues seriously hinder the development and deploy-ment of FL. There is an urgent necessity to unveil the es-sential cause of privacy leakage such that we can develop effective defenses to tackle the privacy issue of FL.
In this work, we assume that the server in FL is malicious and it aims to reconstruct the private training data from de-vices. Our key observation is: the class-wise data repre-sentations of each device’s data are embedded in shared local model updates, and such data representations can be inferred to perform model inversion attacks. There-fore, the information can be severely leaked through the model updates.
In particular, we provide an analysis to reveal how the data representations, e.g., in the fully con-nected (FC) layer, are embedded in the model updates. We then propose an algorithm to infer class-wise data represen-tation to perform model inversion attacks. Our empirical study demonstrates that the correlation between the inferred data representations using our algorithm and the real data representations is as high as 0.99 during local training, and thus proves that the representations leakage is the essential cause behind existing attacks. Note that the data is often non-IID (identically and independently distributed) across the devices in FL. We also show that the non-IID character-9311
istic aggravates the representation leakage.
Based on our observation of the representation leakage from local updates, we design a defense called Soteria.
Speciﬁcally, we present an algorithm to generate a perturba-tion added to the data representations, such that: 1) the per-turbed data representations are as similar as possible to the true data representations to maintain the FL performance; and 2) the reconstructed data using the perturbed data rep-resentations are as dissimilar as possible to the raw data.
Importantly, we also derive certiﬁed robustness guarantee to
FL and convergence guarantee to FedAvg, a popular FL al-gorithm, when applying our defense. To evaluate the effec-tiveness of our defense, we conduct experiments on MNIST and CIFAR10 for defending against the DLG attack [28] and GS attack [7]. The results demonstrate that without sac-riﬁcing accuracy, our proposed defense can increase mean squared error (MSE) between the reconstructed data and the raw data for both DLG attack and GS attack by as much as 160×, compared with baseline defense methods. Therefore, the privacy of the FL system is signiﬁcantly improved.
Our key contributions are summarized as follows:
• To the best of our knowledge, this is the ﬁrst work to explicitly reveal that data representations embedded in the model updates are the essential cause of leaking private information from the communicated local up-dates in FL. In addition, we develop an algorithm to effectively reconstruct the data from the local updates.
• We develop an effective defense by perturbing data representations. We also derive certiﬁed robustness guarantee to FL and convergence guarantee to FedAvg when applying our defense.
• We empirically evaluate our defense on MNIST and
CIFAR10 against DLG and GS attacks. The results show our defense can offer a signiﬁcantly stronger pri-vacy guarantee without sacriﬁcing accuracy. 2.