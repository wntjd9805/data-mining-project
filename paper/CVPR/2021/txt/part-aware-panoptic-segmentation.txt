Abstract
In this work, we introduce the new scene understand-ing task of Part-aware Panoptic Segmentation (PPS), which aims to understand a scene at multiple levels of abstrac-tion, and uniﬁes the tasks of scene parsing and part parsing.
For this novel task, we provide consistent annotations on two commonly used datasets: Cityscapes and Pascal VOC.
Moreover, we present a single metric to evaluate PPS, called
Part-aware Panoptic Quality (PartPQ). For this new task, using the metric and annotations, we set multiple baselines by merging results of existing state-of-the-art methods for panoptic segmentation and part segmentation. Finally, we conduct several experiments that evaluate the importance of the different levels of abstraction in this single task. 1.

Introduction
Humans perceive and understand a scene at multiple lev-els of abstraction. Concretely, when observing a scene, we do not only see a single semantic label for each visual entity, such as person or car. We also distinguish the parts of en-tities, such as person-leg and car-wheel, and we are able to group together the parts that belong to a single individual en-tity. Currently, there is no computer vision task that aims at simultaneously understanding a scene holistically on both of these levels of abstraction: scene parsing and part parsing.
Instead, most methods focus on solving a task at a single level of abstraction. On the one hand, scene parsing aims to recognize and semantically segment all foreground ob-jects (things) and background classes (stuff ) in an image.
Recently, this task has been formalized as panoptic segmen-tation [21], for which the goal is to predict 1) a class label and 2) an instance id for each pixel in an image. This for-malization has resulted in a boost in research interest that advanced the state-of-the-art [5, 20, 27, 42, 44, 49]. On the other hand, part parsing takes over where scene parsing stops, as it aims to segment an image based on part-level semantics, i.e., the parts constituting the scene-level classes. For this level of abstraction, there is a wide range of different task
∗Both authors contributed equally.
Semantic Segmentation
Part Segmentation
Instance Segmentation
Instance-aware Part Segmentation
Panoptic Segmentation
Part-aware Panoptic Segmentation
Figure 1. Evolution of scene understanding tasks: from semantic to panoptic (top to bottom) and from part-agnostic to part-aware (left to right). Colors indicate scene-level and part-level semantics.
Instance-level boundaries are emphasized with a white contour. deﬁnitions, and resulting methods. Most methods focus on a single object class and are instance-agnostic, while only a few are instance-aware [15, 26, 56], or focus on multiple object classes [41, 57]. A more comprehensive overview of related work is provided in Section 2.
To come closer to uniﬁed perception at multiple levels of abstraction, this work deﬁnes a task that combines scene pars-ing and part parsing in a single task. This task encompasses the ability to 1) apply per-pixel scene-level classiﬁcation, 2) segment things classes into individual instances, and 3) seg-ment stuff classes or things instances into their respective parts. We call this task part-aware panoptic segmentation (PPS); the conceptual differences with existing tasks are vi-sualized in Figure 1. Together with this task, we also deﬁne a metric to evaluate it. This metric, part-aware panoptic quality (PartPQ), extends the panoptic quality metric [21] to cover part segmentation performance per detected things instance or stuff class. More details on the task and metric can be found in Section 3.
To allow for research on the new task of PPS, we intro-5485
duce consistent part-aware panoptic annotations for two com-monly used datasets. For Cityscapes [6], we have labeled part classes for all 3.5k images of the train and validation set, which are consistent with the existing panoptic annota-tions. For Pascal VOC [13], we have combined the existing datasets for semantic segmentation [43] and instance-aware part segmentation [4] to generate a consistent annotation set for PPS. In Section 4, we provide further explanations and statistics on these datasets.
As there is no existing work on part-aware panoptic seg-mentation, we establish several benchmarks. We create base-lines by generating state-of-the-art results on panoptic seg-mentation and part segmentation, and merging these to the
PPS format using heuristics. As explained in Section 5, there are several design choices that need to be taken into account, when combining predictions at multiple levels of abstraction.
Speciﬁcally: should we opt for a top-down method, where we prioritize the scene-level predictions from panoptic seg-mentation, and complement these with part predictions, or is it better to use a bottom-up approach, where we com-bine parts to generate scene-level predictions? To evaluate this, we conduct experiments to research the beneﬁts of both types of strategies. Both these experiments and the baselines provide a direction for future research on multi-task training of PPS architectures where the different subtasks can beneﬁt from each other.
To summarize, this work contains the following contribu-tions:
• The introduction of the part-aware panoptic segmenta-tion (PPS) task, unifying perception at multiple levels of abstraction.
• The PartPQ metric to evaluate this task.
• Coherent PPS annotations for two commonly used datasets, which are made available to the public.
• Baselines for the PPS task on two datasets.
• An analysis of the design choices for the new PPS task.
All annotations and the code are available at https:// github.com/tue-mps/panoptic_parts. 2.