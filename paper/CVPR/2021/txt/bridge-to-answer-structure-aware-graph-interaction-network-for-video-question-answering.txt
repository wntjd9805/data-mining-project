Abstract
This paper presents a novel method, termed Bridge to
Answer, to infer correct answers for questions about a given video by leveraging adequate graph interactions of hetero-geneous crossmodal graphs. To realize this, we learn ques-tion conditioned visual graphs by exploiting the relation be-tween video and question to enable each visual node using question-to-visual interactions to encompass both visual and linguistic cues. In addition, we propose bridged visual-to-visual interactions to incorporate two complementary vi-sual information on appearance and motion by placing the question graph as an intermediate bridge. This bridged ar-chitecture allows reliable message passing through compo-sitional semantics of the question to generate an appropri-ate answer. As a result, our method can learn the question conditioned visual representations attributed to appearance and motion that show powerful capability for video question answering. Extensive experiments prove that the proposed method provides effective and superior performance than state-of-the-art methods on several benchmarks. 1.

Introduction
Video question answering (VideoQA) is a task to an-swer the question regarding a given video in a natural language form. Over the past few years, several meth-ods have been focused on manipulating spatio-temporal visual representations conditioned by linguistic cues for
VideoQA [20, 31, 32, 35]. However, because of its speci-ﬁcities such as dynamic spatiotemporal dependencies of the video and sophisticated compositional semantics of the question, the VideoQA still remains a challenging problem.
Recent works [11, 27, 6, 2, 5, 18] have adopted the encoder-decoder structure. Typically, LSTM-based en-coders [11, 6, 2, 5] are used to encode the representa-tions of video frames and a question into the visual and
*Corresponding author.
This research was supported by the Yonsei University Research Fund of 2021 (2021-22-0001). (a) Example for VideoQA (b) Q2V interactions (c) M2A interactions
Figure 1. (a) An example for VideoQA. (b) Question-to-Visual (Q2V) interactions that each question node are propagated to vi-sual nodes. (c) Visual-to-Visual (V2V) interactions that each vi-sual node are associated with the relative visual nodes using the question bridge. Only motion-to-appearance (M2A) interaction is shown. word sequence. The encoded representations are then in-corporated to provide the answer with an attention mech-anism. The several types of attention have shown promis-ing results by learning the temporal relations between video frames [38, 2], the spatial relations between regions in ev-ery single frame [15, 37, 27], or spatiotemporal relations using appearance and motion representations [11, 6]. Al-though these methods have suggested how to use the vi-sual relationship for VideoQA, they still rely on learning positional relationships, not on in-depth semantic meaning, which makes capturing sophisticated appearance-motion or visual-question relations difﬁcult.
Meanwhile, methods to understand cross-modal rela-tionships have been proposed for vision-language interac-15526
tion tasks, such as image-text matching [21, 26] or video-text matching [1], exploiting global [22, 29, 36] or lo-cal [19, 25] representations for visual and textual infor-mation. Similar approaches have also been adopted in
VideoQA. The global question representation has been used as a condition to learn a question-speciﬁc visual represen-tation [40, 6, 18]. For example, the global question fea-ture vector was used to update the memory network to learn attention that attributed to the question in [6]. Le et al. [18] proposed a hierarchical architecture that trans-forms a sequence of objects into a new array conditioned on the global question feature. The word-level features of the question have been treated as sequential data in the local ap-proach [11, 38, 5, 24, 10]. These approaches leveraged each word representation to learn visual attention [11, 38, 2] or co-attention [27, 5, 24, 10] by fusing visual and word rep-resentations. However, the global approaches have learned coarse relations that frequently fail to capture video-word relations. The local approaches associate visual and word information based on co-occurrence statistics, not compo-sitional semantics of the question. For instance, without semantic relations, the word “woman” of the question in
Fig. 1-(a) can incorrectly be correlated with all women in the video. On the other hand, compositional semantics clearly indicate from the phrase “in the red” that the ques-tion point to the left woman.
To address these limitations, the consideration of gram-matical dependencies between sentence words [3, 4] has been raised. For visual question answering (VQA), Teney et al. [33] proposed structured representations that the input image and question are encoded as graphs to leverage com-positional semantics of the question. For image-text match-ing, Liu et al. [26] proposed a graph-structured network that construct graphs for the image and corresponding captions to ﬁnd the ﬁne-grained image-text correspondences using node-level and structure-level matching. Although the ef-fectiveness of structured representations for image-text re-lations has been extensively demonstrated, it is still under-explored in VideoQA.
In this paper, we propose a novel method, called Bridge to Answer, that formulates structure-aware interaction for semantic relation modeling between crossmodal informa-tion, including appearance, motion, and question. Contrary to existing approaches [6, 10], we construct not only ap-pearance and motion graphs for video but also the question graph that represents compositional semantic relations be-tween words. We perform question-to-video (Q2V) inter-actions that propagate the question node to its relevant vi-sual nodes to learn question conditioned visual representa-tions with visual-question relations, as shown in Fig. 1-(b).
Also, we apply visual-to-visual (V2V) interactions to visual graphs delivering each visual node to nodes in the relative visual graph to model appearance-motion relations. To uti-lize compositional semantic structure of the question, we use the question graph as an intermediate bridge, as shown in Fig. 1-(c). We demonstrate the capability of the proposed method through extensive ablation studies and comparison with state-of-the-art methods on three datasets, including
TGIF-QA [11], MSVD-QA [38], and MSRVTT-QA [39]. 2.