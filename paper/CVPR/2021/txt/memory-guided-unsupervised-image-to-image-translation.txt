Abstract 1.

Introduction
We present a novel unsupervised framework for instance-level image-to-image translation. Although recent advances have been made by incorporating additional object anno-tations, existing methods often fail to handle images with multiple disparate objects. The main cause is that, dur-ing inference, they apply a global style to the whole image and do not consider the large style discrepancy between instance and background, or within instances. To address this problem, we propose a class-aware memory network that explicitly reasons about local style variations. A key-values memory structure, with a set of read/update opera-tions, is introduced to record class-wise style variations and access them without requiring an object detector at the test time. The key stores a domain-agnostic content representa-tion for allocating memory items, while the values encode domain-speciﬁc style representations. We also present a feature contrastive loss to boost the discriminative power of memory items. We show that by incorporating our memory, we can transfer class-aware and accurate style represen-tations across domains. Experimental results demonstrate that our model outperforms recent instance-level methods and achieves state-of-the-art performance.
This research was supported by the Agency for Defense Development under the grant UD2000008RD.
∗Corresponding author
Unsupervised image-to-image (I2I) translation is the task of learning a mapping between unpaired images in di-It can be applied to a variety of applica-verse domains. tions, including attribute manipulation [3, 21], style trans-fer [43, 12], data augmentation [25, 11], and domain adapta-tion [30, 10]. Recent methods [49, 23, 16, 42, 47] achieved impressive results based on a cycle-consistency constraint that forces translated images to be mapped back to their original domain. However, they usually assume a determin-istic one-to-one mapping between two domains, thus failing to capture the full distribution of possible outputs. Several methods [50, 13, 22, 8, 45] aim to model complex and mul-timodal distributions to generate diverse outputs. They pos-tulate that the image representation can be disentangled into domain-invariant content and domain-speciﬁc style. How-ever, they simply formulate I2I translation as a global trans-lation problem and apply a global content/style to entire images, which is problematic when handling complex im-ages with many disparate objects. Recently, INIT [38] and
DUNIT [1] alleviated this problem by separately treating object instances and background with additional object an-notations. During training, INIT [38] independently trans-lates the instances using a separate reconstruction loss along with the global translation module. At test time, however, it only uses the global module and discards the instance-level information. DUNIT [1] integrates an object detector 6558
within the I2I translation module and adds an instance-level encoder to extract instance-boosted features. Although it can leverage the object instances at test time, it is not ﬂexi-ble enough to model diverse local style variations. Further-more, both methods require an off-the-shelf computation-ally expensive object detection module at test time.
Motivated by the aforementioned problems, in this pa-per, we introduce a novel instance-level I2I translation framework with an external memory module. Speciﬁcally, we propose a class-aware memory network that can accu-rately store and propagate local-style information across different visual domains.
It comprises several class-wise memory matrices, and each matrix contains a set of key-values (items). The key is used to address relevant memory items with respect to queries, and covers a shared content space. Conversely, the values encode domain-speciﬁc style representations for its paired key. This memory module allows storing diverse styles for different object instances into memory items during training (update) and efﬁciently accessing them without an explicit object detector at test time (read). Furthermore, we present a feature contrastive loss to enhance the discriminative power of memory items.
We show that, by incorporating our memory, the proposed method can capture the object details and reconstruct real-istic images. Experimental results on standard benchmarks, including INIT [38], KITTI [7], and Cityscapes [4], demon-strate the effectiveness of our method, which outperforms state-of-the-art instance-level I2I translation methods. Fur-thermore, we demonstrate that our approach can be applied to domain adaptation detection tasks.
Our contributions can be summarized as follows:
• We propose a memory-guided unsupervised I2I trans-lation (MGUIT) framework that stores and propagates instance-level style information across visual domains.
To best of our knowledge, this is the ﬁrst work that ex-plores a memory network in I2I translation.
• We introduce a key-values memory structure to effec-tively record diverse style variations and access them during I2I translation. Our model does not require ex-plicit object detection modules at test time. We also propose a feature contrastive loss to improve the diver-sity and discriminative power of our memory items.
• Our method produces realistic translation results while preserving instance details well; it outperforms recent state-of-the-art methods on standard benchmarks. 2.