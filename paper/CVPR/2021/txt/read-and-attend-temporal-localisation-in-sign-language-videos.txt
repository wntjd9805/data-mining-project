Abstract
“The ﬁrst plants to choose when planning any garden”
The objective of this work is to annotate sign instances across a broad vocabulary in continuous sign language. We train a Transformer model to ingest a continuous signing stream and output a sequence of written tokens on a large-scale collection of signing footage with weakly-aligned sub-titles. We show that through this training it acquires the ability to attend to a large vocabulary of sign instances in the input sequence, enabling their localisation. Our con-tributions are as follows: (1) we demonstrate the ability to leverage large quantities of continuous signing videos with weakly-aligned subtitles to localise signs in continu-ous sign language; (2) we employ the learned attention to automatically generate hundreds of thousands of annota-tions for a large sign vocabulary; (3) we collect a set of 37K manually veriﬁed sign instances across a vocabulary of 950 sign classes to support our study of sign language recognition; (4) by training on the newly annotated data from our method, we outperform the prior state of the art on the BSL-1K sign language recognition benchmark. 1.

Introduction languages that,
Sign languages are visual for deaf communities, represent the natural means of communica-tion [43]. Our goal in this paper is to identify and tempo-rally localise instances of signs among sequences of con-tinuous sign language. Achieving automatic sign local-isation enables a diverse range of practical applications: construction of sign language dictionaries to support lan-guage learners, indexing of signing content to enable efﬁ-cient search and “intelligent fast-forward” to topics of inter-est, automatic sign language dataset construction, “wake-word” recognition for signers [34] and tools to assist lin-guistic analysis of large-scale signing corpora.
In recent years, there has been a great deal of progress garden e r o c s n o i t n e t t
A plant garden plant plant
Time frames
Figure 1. Sign localisation emerges from sequence prediction.
In this work, we show that the ability to localise instances of signs emerges naturally by training a Transformer model [45] to per-form a sequence prediction task on hundreds of hours of continu-ous signing videos with weakly-aligned subtitles. in temporally localising human actions within video streams [39, 51] and spotting words in spoken languages through aural [15] and visual [30, 40] keyword spotting methods. In both cases, a key driver of progress has been the availability of large-scale annotated datasets, enabling the powerful representation learning abilities of convolutional neural networks to be brought to bear on the task.
By contrast, annotated datasets for sign language are lim-ited in scale and typically orders of magnitude smaller than their spoken counterparts [5]. Widely used datasets such as RWTH-PHOENIX [9, 26] and the CSL dataset [23] pro-vide continuous sign annotations in the form of glosses1 or free-form sentences, but lack precise temporal annotations and are limited in content diversity, vocabulary, and scale.
Large-scale collections of continuous signing videos exist, but are limited to sparse annotation coverage [2, 36].
In the absence of large-scale annotated training data, in this work we turn to a readily available and large-scale source: sign-interpreted TV broadcast footage together with
∗Equal contribution 1Glosses are atomic lexical units used to annotate sign languages. 16857  
subtitles of the corresponding speech in English. We pro-pose to annotate this data with signs by training a Trans-former [45] to predict, given input streams of continu-ous signing, the corresponding subtitles, and then using its trained attention mechanism to perform alignment from En-glish words to signs.
This is a very challenging task: ﬁrst, subtitles are only weakly aligned to the signing content—a sign may ap-pear several seconds before or after its corresponding trans-lated word appears in the subtitles, thus subtitles provide a relatively imprecise cue about the temporal location of a sign. Second, sign interpreters produce a translation of the speech that appears in subtitles, rather than a tran-scription—words in the subtitle may not correspond di-rectly to individual signs produced by interpreters, and vice versa. Third, grammatical structures between sign lan-guages and spoken languages differ considerably [43], and consequently the ordering of words in the subtitle is typi-cally not preserved in the signing.
The core hypothesis motivating this approach is that in order to solve the sequence prediction task, the attention mechanism of the Transformer must be capable of localising sign instances. We demonstrate that by employing recent sign spotting techniques [2, 31] to coarsely align subtitles, sequence prediction is rendered tractable. One of the pri-mary ﬁndings of this work is that, when performed at large scale (across hundreds of hours of continuous signing con-tent), the ability to localise signs indeed emerges from the attention patterns of the sequence prediction model (Fig. 1).
We make the following four contributions: (1) by train-ing on an appropriate sequence prediction task, we show that the attention mechanism of the Transformer learns to attend to speciﬁc signs, enabling their localisation; (2) we employ the learned attention to automatically generate hun-dreds of thousands of annotations for a large sign vocab-ulary; (3) we collect a set of 37K manually veriﬁed sign instances across a vocabulary of 950 sign classes to sup-port our study of sign language recognition; (4) by training on the newly annotated data from our method, we outper-form the prior state of the art on the BSL-1K sign language recognition benchmark. 2.