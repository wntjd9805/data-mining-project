Abstract
To promote the developments of object detection, track-ing and counting algorithms in drone-captured videos, we construct a benchmark with a new drone-captured large-scale dataset, named as DroneCrowd, formed by 112 video clips with 33, 600 HD frames in various scenarios. Notably, we annotate 20, 800 people trajectories with 4.8 million heads and several video-level attributes. Meanwhile, we design the Space-Time Neighbor-Aware Network (STNNet) as a strong baseline to solve object detection, tracking and counting jointly in dense crowds. STNNet is formed by the feature extraction module, followed by the density map es-timation heads, and localization and association subnets.
To exploit the context information of neighboring objects, we design the neighboring context loss to guide the asso-ciation subnet training, which enforces consistent relative position of nearby objects in temporal domain. Extensive experiments on our DroneCrowd dataset demonstrate that
STNNet performs favorably against the state-of-the-arts. 1.

Introduction
Drones, or general unmanned aerial vehicles (UAVs), equipped with cameras have been fast deployed to a wide range of applications, such as video surveillance for crowd control [45] and public safety [22]. In recent years, many massive stampedes have taken place around the world that claimed many victims, making the automatic density map estimation, counting and tracking in crowds on drones im-portant tasks, which draw great attention from the computer vision community.
∗Both authors contributed equally to this work.
†Corresponding author.
This work was supported by the Na-tional Key Research and Development Program of China under Grant 2018AAA0102402, the National Natural Science Foundation of China un-der Grants 61732011, 61876127, and 61925602, Natural Science Founda-tion of Tianjin under Grant 17JCZDJC30800, the Applied Basic Research
Program of Qinghai (2019-ZJ-7017).
Despite signiﬁcant progress, crowd counting and track-ing algorithms still have room for improvement to deal with drone-captured videos due to various challenges, such as view point and scale variations, background clutter, and small scales. Developing and evaluating these algorithms for drones are impeded by the lack of publicly available large-scale benchmarks. Some recent efforts [40, 44, 10, 41, 5, 35] have devoted to construct datasets for crowd count-ing. However, the majority of them focus on crowd count-ing with still images or inconsistent frames by surveillance cameras, due to difﬁculties in data collection and annotation for drone-based crowd counting and tracking.
To ﬁll this gap, we collect a large-scale drone-based dataset for density map estimation, crowd localization and tracking. Our DroneCrowd dataset consists of 112 video clips formed by total 33, 600 frames, captured by various drone-mounted cameras, in 70 different scenarios across 4 different cities in China (i.e., Tianjin, Guangzhou, Daqing, and Hong Kong). These video clips are annotated with more than 4.8 million head annotations and several video-level attributes. To the best of our knowledge, this is the largest and most thoroughly annotated density map estimation, lo-calization, and tracking dataset to date, see Table 1.
To handle this challenging dataset, we design a Space-Time Neighbor-Aware Network (STNNet) as a strong base-line, which solves the density map estimation, localization, and tracking simultaneously. Speciﬁcally, the proposed
STNNet is formed by four modules, i.e., the feature extrac-tion subnetwork, followed by the density map estimation heads, the localization, and the association subnets. The feature extraction subnetwork ﬁrst uses two-branch CNNs to extract multi-scale features, and then computes the cor-relations between the extracted features in consecutive two frames to exploit the temporal relations. Using density map estimation heads, we can estimate the density of objects in video frames to perform crowd counting. Inspired by object detection [25, 26, 42], we introduce the localization subnet, formed by the classiﬁcation and regression branches, to out-put accurate locations of targets in each individual frames. 7812
Table 1. Comparison between the DroneCrowd dataset and existing datasets.
Dataset
UCF CC 50 [9]
Shanghaitech A [44]
Shanghaitech B [44]
AHU-Crowd [7]
CARPK [6]
Smart-City [41]
UCF-QNRF [10]
NWPU [35]
UCSD [3]
Mall [19]
WorldExpo [40]
FDST [5]
DroneCrowd
Type Trajectory image image image image image image image image video video video video video
X
Resolution 50
-482
-768 × 1024 716 576 × 720 107 1280 × 720 1, 448 1920 × 1080 50 1, 535
-2191 × 3209 5, 109 158 × 238 2, 000 640 × 480 2, 000 576 × 720 3, 980 1920 × 1080 15, 000 1920 × 1080 33, 600
Frames Max count Min count Ave count Total count Year 2013 2016 2016 2016 2017 2018 1, 251, 642 2018 2, 133, 375 2020 49, 885 2008 62, 316 2013 199, 923 2015 394, 081 2019 4, 864, 280 2021 1, 279.5 501.4 123.6 420.6 62.0 7.4 815.4 418.0 24.9 31.2 50.2 26.7 144.8 4, 543 3, 139 578 2, 201 188 14 12, 865 20, 033 46 53 253 57 455 63, 974 241, 677 88, 488 45, 000 89, 777 369 94 33 9 58 1 1 49 0 11 13 1 9 25
To exploit the temporal consistency, the association subnet is designed to predict motion offests of targets in consec-utive frames for tracking. Besides, we develop the neigh-boring context loss by integrating spatial-temporal context of neighboring targets to guide the training of association subnet. Speciﬁcally, the neighboring context loss penalizes large displacements of the relative positions of adjacent ob-jects in temporal domain, and guides the association sub-net to generate accurate motion offsets. The whole network is trained in an end-to-end manner with the multi-task loss and Adam optimizer [12]. After that, multi-object tracking methods [24, 1] are used to predict long trajectories of tar-gets. Compared with 12 state-of-the-art algorithms, exten-sive experiments on our DroneCrowd dataset demonstrate the effectiveness of the proposed STNNet method for den-sity map estimation, crowd localization and tracking tasks.
Contributions. (1) We collect a large-scale drone captured dataset for density map estimation, localization, and track-ing in dense crowd, which signiﬁcantly surpasses existing datasets in terms of data type and volume, annotation qual-ity, and difﬁculty. (2) We propose a space-time neighbor-aware network to solve the density map estimation, local-ization and tracking tasks simultaneously. (3) To exploit the spatial-temporal context, we design the neighboring context loss to penalize large displacements of the relative positions of adjacent objects in temporal domain for network training. 2.