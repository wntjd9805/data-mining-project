Abstract
Integrated Gradients (IG) [29] is a commonly used fea-ture attribution method for deep neural networks. While IG has many desirable properties, the method often produces spurious/noisy pixel attributions in regions that are not re-lated to the predicted class when applied to visual models.
While this has been previously noted [27], most existing so-lutions [25, 17] are aimed at addressing the symptoms by explicitly reducing the noise in the resulting attributions. In this work, we show that one of the causes of the problem is the accumulation of noise along the IG path. To mini-mize the effect of this source of noise, we propose adapting the attribution path itself - conditioning the path not just on the image but also on the model being explained. We intro-duce Adaptive Path Methods (APMs) as a generalization of path methods, and Guided IG as a speciﬁc instance of an
APM. Empirically, Guided IG creates saliency maps bet-ter aligned with the model’s prediction and the input image that is being explained. We show through qualitative and quantitative experiments that Guided IG outperforms other, related methods in nearly every experiment. 1.

Introduction
As deep neural network computer vision models are in-tegrated into critical applications such as healthcare and security, research on explaining these models has intensi-ﬁed. Feature attribution techniques strive to explain which inputs the model considers to be most important for a given prediction, making them useful tools in debugging models or understanding what they have likely learned.
However, while a plethora of techniques have been devel-oped [24, 29, 13, 9, 20], there are still behaviors of these
In attribution techniques that remain to be understood. this context, our work is focused on studying the source of noise in attributions produced by path-integral-based meth-ods [27].
Gradient-based feature attribution techniques [24, 22] are of particular interest in our work. The main idea behind these techniques is that the partial derivative of the output (c) Guided IG (a) Input image (b) IG attributions
Figure 1: Comparing feature attribution for Integrated Gradients and Guided Integrated Gradients. Both (b) and (c) use a black baseline to explain the “house ﬁnch” prediction. While (b) has attributions on the bird, there is substantial noise in the attribu-tions compared to (c). This work studies the source of noise, and presents Guided IG as a solution. with respect to the input is considered as a measure of the sensitivity of the network for each input dimension. While early methods [24] use the gradients multiplied by the input as a feature attribution technique, more recent methods ex-ploit gradients of the activation maps [22], or integrate gra-dients over a path [29]. This work studies Integrated Gra-dients (IG) [29], a commonly used method that is based on game-theoretic ideas in [1]. IG avoids the problem of di-minishing inﬂuences of features due to gradient saturation, and has desirable theoretical properties.
One commonly observed problem while calculating In-tegrated Gradients for vision models is the noise in pixel attribution (Figure 1) originating from gradient accumula-tion [11, 25, 27, 30] along the integration path. A few pos-sible explanations for this noise have been put forth: (a) high curvature of the output manifold [7]; (b) approxima-tion of the integration with Riemann sum; and (c) choice of baselines [30, 27]. Our experiments indicate that one source of attribution noise comes from regions of corre-lated, high-magnitude gradients on irrelevant pixels found along the straight line integration path. Our ﬁndings cor-relate with observations in [7], which state that the model surface plays a large role in determining the magnitude of attribution values.
Methods have been proposed to explicitly reduce the noise in attributions. SmoothGrad [25] averages attribu-tions over multiple samples of the input, created by adding
Gaussian noise to the original input. The aggregation im-5050
(a) Schematic (b) IG attributions (c) Guided IG attributions
Figure 2: Comparing IG and Guided IG’s paths and results. (a): For IG, a straight line path from baseline to input is followed (red dotted line), regardless of changes in gradients. For Guided IG, the path is chosen by selecting features that have the smallest absolute value of corresponding partial derivatives (cyan dotted line). Guided IG’s goal is to reduce the accumulation of gradients caused by nearby very high/very low prediction examples. (b) and (c): Snapshots of attributions for the ﬂower pot class for Integrated Gradients (center) and
Guided IG (right) at alpha values of 0.1, 0.5, and 1.0. The top rows show graphs of the softmax prediction for ﬂower pot as a function of alpha. The second row shows the input image produced by each technique at the three different alpha values. Note that IG’s straight line path affects all pixels equally (e.g., see α = 0.5), while Guided IG reveals the least important features, ﬁrst. The third row shows each technique’s attributions for each of the three alpha values, with Guided IG showing less noise outside the area of the image occupied by the ﬂower pot. proves the overall true signal in the attribution. XRAI [12] aggregates attributions within segments to reduce the out-lier effect. Sturmfels et al. suggest the choice in baseline is a contributing factor, and propose different baselines as a potential solution [27]. [30] integrate over the frequency di-mension by blurring the input image, thereby reducing per-turbation artifacts along the attribution path. Dombrowski et al. smooth the network output by converting ReLUs to softplus [7].
While the above methods address noise in the attri-butions by manipulating the input (or the baseline), ours examines the entire path of integration. As mentioned in [29], each path from the baseline to the input consti-tutes a different attribution method; the methods discussed above [29, 25, 12, 27] choose the straight line path, while
[30] choose the ‘’blur” path when integrating gradients. In this work, instead of determining the path based on the in-put and baseline alone, we propose adaptive path methods (APMs) that adapt the path based on the input, baseline, and the model being explained. Our intuition is that model-agnostic paths, such as the straight line, are susceptible to travel through regions that have irregular gradients, result-ing in noisy attributions. We posit that adapting the integra-tion path based on the model can avoid selecting samples from anomalous regions when determining attributions. model. Guided IG deﬁnes a path from the baseline towards the input, moving in the direction of features that have the lowest absolute value of partial derivatives. At each step,
Guided IG selects the features (pixel intensities) with the lowest absolute value of partial derivatives (e.g., bottom 10%), and moves only that subset closer to the intensity in the input image, leaving all others unchanged. As the in-tensity of speciﬁc pixels (features) becomes equal to that in the input image being explained, they are no longer candi-dates for selection. The attributions resulting from this ap-proach are considerably less noisy. Experiments highlight that Guided IG outperforms other, related methods in nearly every experiment. Our main contributions are as follows:
•
•
•
We propose Adaptive Path Methods (APMs) a gener-alization of path methods [29] that consider the model and input when determining the attribution path.
We introduce Guided IG, an attribution technique that is an instance of an adaptive path method, and describe its theoretical properties.
We present experimental results that show Guided IG outperforms other attribution methods quantitatively and reduces noise in the ﬁnal explanations. 2.