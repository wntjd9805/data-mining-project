Abstract 19.22 fps
Multiview shape-from-shading (SfS) has achieved high-detail geometry, but its computation is expensive for solv-ing a multiview registration and an ill-posed inverse ren-dering problem. Therefore, it has been mainly used for ofﬂine methods. Volumetric fusion enables real-time scan-ning using a conventional RGB-D camera, but its geom-etry resolution has been limited by the grid resolution of the volumetric distance ﬁeld and depth registration errors.
In this paper, we propose a real-time scanning method that can acquire high-detail geometry by bridging volumet-ric fusion and multiview SfS in two steps. First, we pro-pose the ﬁrst real-time acquisition of photometric normals stored in texture space to achieve high-detail geometry. We also introduce geometry-aware texture mapping, which pro-gressively reﬁnes geometric registration between the texture space and the volumetric distance ﬁeld by means of normal texture, achieving real-time multiview SfS. We demonstrate our scanning of high-detail geometry using an RGB-D cam-era at ∼20 fps. Results verify that the geometry quality of our method is strongly competitive with that of ofﬂine multi-view SfS methods. 1.

Introduction
Shape-from-shading (SfS) has been commonly used to enhance geometric details in 3D scanning. When surface reﬂectance and illumination are known, SfS factorizes re-ﬂected irradiance of camera signals to photometric normals in the camera’s resolution [12]. When a high-resolution camera is used, the geometry quality can be improved sig-niﬁcantly by combining base geometry and normals [21].
However, when reﬂectance and illumination are unavail-able, SfS becomes a very ill-posed problem. Multiview SfS estimates distributions of illumination and albedo by lever-aging multiview input [1, 10] and then obtains high-detail normals from shading. The geometry quality of these mul-tiview SfS methods is signiﬁcantly higher than that of real-time scanning methods using a conventional RGB-D cam-Ours s l a m r o
N y r t e m o e
G
Conventional fusion
Our normal fusion
Figure 1: Result of our real-time normal fusion method, compared with the conventional fusion method that accu-mulates TSDFs of depth maps in the canonical space. We decompose camera signals to photometric normals and ac-cumulate them in texture space associated with voxel grids of TSDFs, enabling high-resolution geometry in real-time.
Refer to the supplemental video for real-time demo. era. However, when a camera is unstructured and scenes are uncontrolled, multiview SfS becomes highly under-determined; therefore, the ill-posed multiview SfS prob-lem needs to be solved by expensive non-linear optimiza-tion with strong assumptions of scene and lighting condi-tions [29, 28] in addition to multiview registration.
Despite the strong beneﬁts of multiview SfS to high-resolution geometry, it has been hardly achieved in real-time RGB-D scanning due to several challenges. First, mul-tiview color and depth frames need to be registered by it-erative closest point (ICP) [26] in general. However, per-fect geometric registration by ICP is theoretically impossi-ble with real systems due to noise in depth frames. It results in blurry reconstruction [18].
Second, to handle noise and inaccurate registration of depth information, the truncated signed distance function 15970
(TSDF) [6] of depth maps has been accumulated in the canonical space. However, the TSDF-based algorithms in-troduce an inevitable tradeoff between spatial resolution of geometry and real-time performance [15, 22, 14, 11, 8]. A hashing technique was used to mitigate the tradeoff [23] by reducing memory footprint, but still, details of geometry of-ten need to be compromised for performance.
To mitigate these challenges, we propose a real-time scanning method that can capture high-detail geometry by integrating two different techniques, volumetric fusion, and multiview SfS via geometry-aware texture mapping. First, we introduce the ﬁrst real-time acquisition method of pho-tometric normals, enabling us to capture the ﬁne level of geometry stored in high-resolution texture space. Second, we propose geometry-aware texture mapping, which pro-gressively reﬁnes geometric registration between the tex-ture space and the canonical space of TSDFs so that we can solve multiview SfS with high accuracy.
We demonstrate that our method can acquire high-detail geometry, in addition to photometric normals and albedo textures in a high resolution at ∼20 fps using a conventional
RGB-D camera. In particular, the geometry quality of our method is strongly competitive with that of ofﬂine multi-view SfS methods. All codes and demo will be published online to ensure reproducibility. 2.