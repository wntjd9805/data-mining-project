Abstract
Unshifted image
Unshifted image
Thanks to the use of convolution and pooling layers, con-volutional neural networks were for a long time thought to be shift-invariant. However, recent works have shown that the output of a CNN can change signiﬁcantly with small shifts in input—a problem caused by the presence of down-sampling (stride) layers. The existing solutions rely either on data augmentation or on anti-aliasing, both of which have limitations and neither of which enables perfect shift invariance. Additionally, the gains obtained from these methods do not extend to image patterns not seen during training. To address these challenges, we propose adaptive polyphase sampling (APS), a simple sub-sampling scheme that allows convolutional neural networks to achieve 100% consistency in classiﬁcation performance under shifts, with-out any loss in accuracy. With APS, the networks exhibit perfect consistency to shifts even before training, making it the ﬁrst approach that makes convolutional neural networks truly shift-invariant. 1.

Introduction
The output of an image classiﬁer should be invariant to small shifts in the image. For a long time, convolutional neural networks (CNNs) were simply assumed to exhibit this desirable property [36, 37, 38, 39]. This was thanks to the use of convolutional layers which are shift equivariant, and non-linearities and pooling layers which progressively build stability to deformations [6, 42]. However, recent works have shown that CNNs are in fact not shift-invariant
[2, 58, 18, 34, 31]. Azulay and Weiss [2] show that the output of a CNN trained for classiﬁcation can change with a probability of 30% with merely a one-pixel shift in input images.