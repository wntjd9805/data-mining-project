Abstract
Thanks to the rapid advances in the deep learning tech-niques and the wide availability of large-scale training sets, the performances of video saliency detection models have been improving steadily and signiﬁcantly. However, the deep learning based visual-audio ﬁxation prediction is still in its infancy. At present, only a few visual-audio sequences have been furnished with real ﬁxations being recorded in the real visual-audio environment. Hence, it would be nei-ther efﬁciency nor necessary to re-collect real ﬁxations un-der the same visual-audio circumstance. To address the problem, this paper advocate a novel approach in a weakly-supervised manner to alleviating the demand of large-scale training sets for visual-audio model training. By using the video category tags only, we propose the selective class ac-tivation mapping (SCAM), which follows a coarse-to-ﬁne strategy to select the most discriminative regions in the spatial-temporal-audio circumstance. Moreover, these re-gions exhibit high consistency with the real human-eye ﬁxa-tions, which could subsequently be employed as the pseudo
GTs to train a new spatial-temporal-audio (STA) network.
Without resorting to any real ﬁxation, the performance of our STA network is comparable to that of the fully super-vised ones. Our code and results are publicly available at https://github.com/guotaowang/STANet. 1.

Introduction and Motivation
In the deep learning era, we have witnessed a growing development in video saliency detection techniques [53, 34, 29, 14], where the primary task is to locate the most dis-tinctive regions in a series of video sequences. At present, this ﬁeld consists of two parallel research directions, i.e., the video salient object detection and the video ﬁxation pre-diction. In practice, the former [19, 49, 41, 32, 13, 4, 5, 8] aims to segment the most salient objects with clear object
∗Corresponding Author
Figure 1. This paper mainly focuses on using a weakly-supervised approach to predicting spatial-temporal-audio (STA) ﬁxations, where the key innovation is that, as the ﬁrst attempt, we automat-ically convert semantic category tags to pseudo-ﬁxations via the newly-proposed selective class activation mapping (SCAM). boundaries (Fig. 1-A). The latter [35, 54, 12, 44, 18], as the main topic of this paper, predicts the real human-eye ﬁxa-tions in the form of scattered coordinates spreading over the entire scene without any clear boundaries (Fig. 1-B). In fact, this topic has long been investigated extensively in the past decades. Different from the previous works [39, 29, 51], this paper is interested in exploiting the deep learning tech-niques to predict ﬁxations under the visual and audio cir-cumstance, also known as visual-audio ﬁxation prediction, and this topic is still in its early exploration stage.
At present, almost all state-of-the-art (SOTA) visual-audio ﬁxation prediction approaches [47, 45] are devel-oped with the help of the deep learning techniques, using the vanilla encoder-decoder structure, facilitated with vari-ous attention mechanisms, and trained in a fully-supervised manner. Albeit making progress, these fully-supervised ap-proaches are plagued by one critical limitation (see below).
It is well known that a deep model’s performance is heavily dependent on the adopted training set, and large-15119
scale training sets equipped with real visual ﬁxations are al-ready accessible in our research community. However, it is time-consuming and laborious to re-collect real human-eye
ﬁxations in the visual-audio circumstance, thus, to our best knowledge, only a few visual-audio sequences are available for the visual-audio ﬁxation prediction task, where only a small part of them are recommended for the network train-ing, making the data shortage dilemma even worse. As a result, according to the extensive quantitative evaluation that we have done, almost all existing deep learning based visual-audio saliency prediction models [47, 45], though re-luctant to admit, might be overﬁtted in essence.
To solve this problem, we seek to realize the visual-audio
ﬁxation prediction using a weakly-supervised strategy. In-stead of using the labor-intensive frame-wise visual-audio ground truths (GTs), we devise a novel scheme to produce the GT-like visual-audio pseudo ﬁxations by using the video category tags only. Actually, there already exist plenty of visual-audio sequences with well labeled semantic category tags (e.g., AVE set [46]), where most of them are originally collected for the visual-audio classiﬁcation task.
Our approach is also inspired by the class activation mapping (CAM, [64]) that has been used in the image object localization [57, 50, 43] and video object location
[2, 3, 33]. The key rationale of CAM relies on the fact that image regions with the strongest discriminative power re-garding the classiﬁcation task should be the most salien-t ones, where these regions usually tend to have relatively larger classiﬁcation conﬁdences than others.
Considering that we aim at the ﬁxation prediction in the visual-audio circumstance, we propose the novel selective class activation mapping (SCAM), which relies on a coarse-to-ﬁne strategy to select the most discrimina-tive regions from multiple sources, where these regions ex-hibit high consistency with the real human-eye ﬁxation-s. This coarse-to-ﬁne methodology ensures the aforemen-tioned less-discriminative scattered regions to be ﬁltered completely, and the selection operation between different sources helps reveal the most discriminative regions, en-abling the pseudo-ﬁxations to be closer to the real ones.
Once the pseudo-ﬁxations have been obtained, a spatial-temporal-audio (STA) ﬁxation prediction network will be trained, and it learns the common consistency of all pseudo-ﬁxations. Consequently, it can predict ﬁxations accurately for videos without being assigned to any semantic category tag in advance.
It is worth mentioning that this paper is one of the ﬁrst at-tempts to explore the deep learning based visual-audio ﬁx-ation prediction in a weakly-supervised manner, which is expected to contribute to visual-audio information integra-tion and relevant applications in computer vision.
Figure 2. Existing SOTA approaches (e.g., Zeng et al. [57]) are mainly designed for locating salient objects rather than simulating human ﬁxations; thus their results tend to be large scatter regions (b), which are quite different from the real ﬁxations (d). 2.