Abstract
This work focuses on object goal visual navigation, aim-ing at ﬁnding the location of an object from a given class, where in each step the agent is provided with an egocen-tric RGB image of the scene. We propose to learn the agent’s policy using a reinforcement learning algorithm.
Our key contribution is a novel attention probability model for visual navigation tasks. This attention encodes seman-tic information about observed objects, as well as spa-tial information about their place. This combination of the “what” and the “where” allows the agent to navigate toward the sought-after object effectively. The attention model is shown to improve the agent’s policy and to achieve state-of-the-art results on commonly-used datasets. 1.

Introduction
Human and animals can navigate new environments rel-atively well. This adaption to new surroundings, although natural, is not trivial. It requires to ﬁnd parallels between the new observations and our past experience. This is largely possible due to our ability to sort through new visual in-formation and intelligently focus on the most relevant se-mantic cues. For instance, when looking for a toaster in a previously-unvisited kitchen, our intuition is to look for the refrigerator, while ignoring other ”irrelevant” information, since our past experience indicates that the toaster is usually located not far from the refrigerator.
Object goal visual navigation tasks include two basic components: semantic understanding of the scene and path planning [28, 3, 14]. With the increase of data and com-putation power, reinforcement learning algorithms excelled in learning policies for these two components jointly in an end-to-end manner [25, 21, 13, 6]. As a result, many exten-sions to visual navigation were presented, including tasks speciﬁed by natural language instructions [7], by a desired goal image [37], or by a target object [30, 10]. Reinforce-ment learning of spatial and semantic relations is a funda-mental challenge for these tasks [31].
This work focuses on object goal visual navigation, where the goal is to ﬁnd an instance of the target object class (Figure 1). Like previous works, we utilize reinforcement learning. We propose to improve the agent’s policy by en-coding semantic information about observed objects using a convolutional net, as well as spatial information about their place, using an attention probability model. This combina-tion of both semantic and spatial information, i.e., of “what” 116898
Step 1
Step 11
Step 16
Step 22
Step 24
Step 29
Figure 2. Path sampling. The ﬁrst row shows a top view of the scene, with the path thus far, along with the agent’s view (white triangle) at this step. The second row shows the image the agent views at this step. The third row shows the fused attention map per step, as well as the three maps that build it. The agent is looking for a toaster (red rectangle) and starts from the opposite side of the kitchen. In Step 1 the agent focuses on the refrigerator, which is an indicator to a nearby toaster; in Step 11 it moves toward the refrigerator; in Step 16 it decides to turns right and then in Step 22 the toaster becomes visible, at which point the agent’s focus switches from the refrigerator to the toaster and the agent turns right; in Step 24 it starts moving forward, toward the toaster; in Step 29 it is sufﬁciently close and declares Done. and of “where”, allows the agent to navigate towards the sought objects effectively. Our novel attention mechanism consists of three types of attention probability models for navigation: target attention that considers the target infor-mation in the image; action attention that takes into account the last action of the agent; memory attention that consid-ers the agent’s previous steps in the scene. Our attention probability model results in an attended embedding, which preserves the semantic and spatial information of objects.
We validate our approach using the AI2-THOR [37] en-vironment. We use Wortsman et al. [30] setup with their scenes from four room categories: kitchen, living room, bedroom and bathroom, where an agent is navigating to a given object using only visual observations. In our exper-imental validation we show that not only we outperform the state-of-the-art, but also our attention unit carries spa-tial information about the objects. This is achieved using a probability distribution over areas of the observed image that are represented by the spatial locations of the topmost convolutional neurons of a standard convolutional net (e.g.,
ResNet18). As this attention probability distribution pre-serves the spatial information that is fed to the reinforce-ment learner, it controls the areas of the image that the agent considers when improving its policy. Hence, this attention unit also carries the promise to explain the agent’s actions in visual navigation tasks.
Figure 2 illustrates this promise. For instance, in Step 1 the attention map suggests that the agent focuses on the refrigerator, which is a good indicator to the location of the toaster. Similarly, once the toaster becomes visible in
Step 22, the attention map switches from focusing on the refrigerator to focusing on the sought-after toaster, and in accordance with that, the agent turns right.
Hence, this paper makes three contributions: 1. We propose a novel attention mechanism that suits navigation. It consists of three types of attentions: tar-get, action, and memory. 2. We present an end-to-end reinforcement learning framework that realizes the attention mechanism and achieves state-of-the-art results. 3. An added beneﬁt of the different attention maps is be-ing able to explain the agent’s actions through visual-ization. 2.