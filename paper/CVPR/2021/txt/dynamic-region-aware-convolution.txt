Abstract
We propose a new convolution called Dynamic Region-Aware Convolution (DRConv), which can automatically as-sign multiple ﬁlters to corresponding spatial regions where
In this way, DR-features have similar representation.
Conv outperforms standard convolution in modeling seman-tic variations. Standard convolutional layer can increase the number of ﬁlers to extract more visual elements but re-sults in high computational cost. More gracefully, our DR-Conv transfers the increasing channel-wise ﬁlters to spa-tial dimension with learnable instructor, which not only im-prove representation ability of convolution, but also main-tains computational cost and the translation-invariance as standard convolution dose. DRConv is an effective and ele-gant method for handling complex and variable spatial in-formation distribution. It can substitute standard convolu-tion in any existing networks for its plug-and-play property, especially to power convolution layers in efﬁcient networks.
We evaluate DRConv on a wide range of models (MobileNet series, ShufﬂeNetV2, etc.) and tasks (Classiﬁcation, Face
Recognition, Detection and Segmentation). On ImageNet classiﬁcation, DRConv-based ShufﬂeNetV2-0.5× achieves state-of-the-art performance of 67.1% at 46M multiply-adds level with 6.3% relative improvement. 1.

Introduction
Beneﬁting from powerful representation ability, con-volutional neural networks (CNNs) have made signiﬁcant progress in image classiﬁcation, face recognition, object de-tection and many other applications. The powerful repre-sentation ability of CNNs stems from that different ﬁlters are responsible for extracting information at different level of abstraction. However, current mainstream convolutional operations perform in ﬁlter sharing manner across spatial domain, so that more effective information can only be cap-∗Equal contribution. This work is supported by The National Key Re-search and Development Program of China (No. 2017YFA0700800) and
Beijing Academy of Artiﬁcial Intelligence (BAAI).
†Corresponding author.
Figure 1. Illustration of DRConv with kernel size k × k and region number m. We get guided feature from X with standard k × k convolution and get m ﬁlters from ﬁlter generator module G(·).
The spatial dimension is divided into m regions as guided mask shows. Every region has individual ﬁlter Wi which is shared in this region and we execute k × k convolution with corresponding
ﬁlter in these regions of X to output Y . tured when these operations are applied repeatedly (e.g., in-creasing channels and depth with more ﬁlters). This repeat-ing manner has several limitations. First, it is computation-ally inefﬁcient. Second, it causes optimization difﬁculties that need to be carefully addressed [13, 33].
Different from the ﬁlter sharing methods, to model more visual elements, some studies focus on making use of the diversity of semantic information by using multiple ﬁlters in spatial dimension. [10, 29] came up with alternative con-volutions to possess individual ﬁlter at each pixel in spa-tial dimension, and we collectively call them local con-volution for convenience. Therefore, the feature of each position will be treated differently, which is more effec-tive to extract the spatial feature than standard convolution.
[29, 27, 26] have shown local convolution’s power on face recognition task. Although local convolution doesn’t in-crease the computation complexity compared with standard convolution, it has two fatal drawbacks. One is to bring 8064
a large amount of parameters, which is proportional to the spatial size. The other is that local convolution destroys translation-invariance, which is unfriendly to some tasks requiring translation-invariant features (e.g., local convo-lution doesn’t work for classiﬁcation task). Both of them make it hard to be widely used in neural networks. Be-sides, local convolution still shares ﬁlters across different samples, which makes it insensitive to the speciﬁc feature of each sample. For example, there are samples with different poses or viewpoints on face recognition and object detec-tion tasks. Therefore, shared ﬁlters across different samples can not be effective to extract customized features.
Considering above mentioned limitations, in this paper, we put forward a new convolution named Dynamic Region-Aware Convolution (DRConv), which can automatically as-sign ﬁlters to corresponding spatial-dimension regions with learnable instructor. As a consequence, DRConv has power-ful semantic representation ability and perfectly maintains translation-invariance property. In detail, we design a learn-able guided mask module to automatically generate the ﬁl-ters’ region-sharing-pattern for each input image accord-ing to their own characteristic. The region-sharing-pattern means that we divide spatial dimension into several regions and only one ﬁlter is shared within each region. The ﬁlters for different samples and different regions are dynamically generated based on the corresponding input features, which is more effective to focus on their own vital characteristic.
The structure of our DRConv is shown in Fig. 1. We apply standard convolution to generate guided feature from the input. According to the guided feature, the spatial di-mension is divided into several regions. As can be seen, pixels with same color in the guided mask are attached to the same region. In each shared region, we apply ﬁlter gen-erator module to produce a ﬁlter to execute 2D convolu-tion operation. So the parameters needed to be optimized are mainly in ﬁlter generator module, and its amount has nothing to do with spatial size. Therefore, apart from sig-niﬁcantly improving networks’ performance, our DRConv can greatly reduce the amount of parameters compared with local convolution, and nearly doesn’t increase the computa-tion complexity compared with standard convolution.
To verify the effectiveness of our method, we conduct a series of empirical studies on several different tasks, includ-ing image classiﬁcation, face recognition, object detection and segmentation by simply replacing standard convolution with our DRConv. The experimental results show that DR-Conv can achieve excellent performance on these tasks. We also offer adequate ablation studies for analyzing the effec-tiveness and robustness of our DRConv.
In brief, this work makes the following contributions, 1. We present a new Dynamic Region-Aware Convolu-tion, which not only has powerful semantic represen-tation ability but also perfectly maintains translation-invariance property. 2. We specially design the backward propagation process for learnable guided mask, so that our region-sharing-pattern is determined and updated according to the gra-dient of the overall task loss through backward propa-gation, which means our method can be optimized in an end-to-end manner. 3. Our DRConv can achieve excellent performance on image classiﬁcation, face recognition, detection and segmentation tasks by simply replacing standard con-volution without increasing much computation cost. 2.