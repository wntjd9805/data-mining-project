Abstract
Differential Neural Architecture Search (NAS) requires all layer choices to be held in memory simultaneously; this limits the size of both search space and ﬁnal architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a distri-bution over high-performing architectures, and uses only as much memory as needed to train a single model. Neverthe-less, it needs to sample many architectures, making it com-putationally expensive for searching in an extensive space.
To solve these problems, we propose a sampling method adaptive to the distribution entropy, drawing more samples to encourage explorations at the beginning, and reducing samples as learning proceeds. Furthermore, to search fast in the multi-variate space, we propose a coarse-to-ﬁne strategy by using a factorized distribution at the beginning which can reduce the number of architecture parameters by over an order of magnitude. We call this method Fast Probabilistic
NAS (FP-NAS). Compared with PARSEC, it can sample 64% fewer architectures and search 2.1× faster. Compared with
FBNetV2, FP-NAS is 1.9× - 3.5× faster, and the searched models outperform FBNetV2 models on ImageNet. FP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger channel choices) and deeper (i.e. more blocks), while adding Split-Attention block and enabling the search over the number of splits. When searching a model of size 0.4G FLOPS, FP-NAS is 132× faster than EfﬁcientNet, and the searched FP-NAS-L0 model outperforms EfﬁcientNet-B0 by 0.7% accuracy. Without using any architecture surrogate or scaling tricks, we directly search large models up to 1.0G
FLOPS. Our FP-NAS-L2 model with simple distillation out-performs BigNAS-XL with advanced inplace distillation by 0.7% accuracy using similar FLOPS. 1.

Introduction
Designing efﬁcient architectures for visual recognition requires extensive exploration in the search space; doing this by hand takes substantial human effort and computa-tional resources. Since the introduction of AlexNet [18], handed-crafted models like ResNet [10], Densenet [14] and
*Correspondence to Zhicheng Yan <zyan3@fb.com>.
InceptionV4 [28], have become increasingly deep with more complex connectivity. Exploring this space manually is difﬁ-cult and time-consuming. Neural Architecture Search (NAS) aims to automate the architecture design process. How-ever, early approaches based on evolution or reinforcement learning take hundreds or thousands of GPU days just for
CIFAR10 or Imagenet target datasets [42, 43, 31, 25].
Recently, differentiable neural architecture search (DNAS) [20] relaxed the discrete representation of archi-tectures to a continuous space, enabling efﬁcient search with gradient descent. This comes at a price: DNAS instantiates all layer choices in memory, and computes all feature maps.
Therefore, its memory footprint increases linearly with the number of layer choices, and greatly limits the size of both search space and ﬁnal architecture. PARSEC [4] presents a probabilistic version of DNAS which samples individual architectures from a distribution on search space. PARSEC’s sampling strategy uses much less memory, but it samples a large number of architectures to fully explore the space, which signiﬁcantly increases computational cost. Here we pose the following question: Can we reduce PARSEC’s com-pute cost and maintain a small memory footprint to support the fast search of both small and large models?
In this work, we accelerate PARSEC by proposing two novel techniques. First, we replace the ﬁxed architecture sampling with a dynamic sampling adaptive to the entropy of architecture distribution. In particular, we sample more architectures in the early stage to encourage exploration, and fewer later, as the distribution concentrates on a smaller set of promising architectures. Furthermore, in multi-variate space where several variables (e.g. block type, feature channel, ex-pansion ratio) are searched over, we propose a coarse-to-ﬁne search strategy. In the beginning stage, we adopt a factorized distribution representation to search at a coarse-grained gran-ularity, which uses much fewer architecture parameters and makes the learning much faster. In the following stage, we seamlessly convert the factorized distribution into the joint distribution to support ﬁne-grained search.
When searching in the FBNetV2 space [33], FP-NAS samples 64% fewer architectures and searches 2.1× faster compared with the PARSEC method. Compared with FB-NetV2, FP-NAS is 3.5× faster, and the searched mod-15139
)
% ( y c a r u c c a 1
-p o
T
)
% ( y c a r u c c a 1
-p o
T 77 75 73 71 69 67 82 81 80 79 78 77 76
S4++
S3++
S2++
S1++
FP-NAS	(ours)
FBNetV2
MobileNetV3
ProxylessNAS
MnasNet
MobileNetV2,	0.75x 50
FLOPS	(M) 100 200 400
L2
FP-NAS-L1
L2
L1
B2
B1
FP-NAS-L0
B0
FP-NAS		w/	distillation	(ours)
FP-NAS	w/o	distillation		(ours)
EfficientNet
ResNeSt
MobileNeXt
MobileNetV3,	1.25X 200 800
FLOPS	(M) 3200
Figure 1: Model FLOPS vs. ImageNet validation accu-racy. All numbers are obtained using a single-crop and a single model. Top: Small models using less than 350M
FLOPS. Bottom: Large models. Our searched models, in-cluding both small- (i.e. FP-NAS-S++) and large models (i.e.
FP-NAS-L), achieve better accuracy-to-complexity trade-off than other models. For example, when model is trained from scratch, our FP-NAS-L0 model achieves 0.7% higher accu-racy than EfﬁcientNet-B0 while searching it is over 132× faster (28.7 Vs. 3790+ GPU-days). We also use notation (+ distillation) to report the results of our large models with vanilla model distillation [11]. els outperform FBNetV2 models on ImageNet. To fur-ther demonstrate FP-NAS’ efﬁciency, we expand the large
FBNetV2 search space, and introduce searchable Split-Attention blocks [39] which increases the space size by over 103×. Our main results are shown in Fig 1. In total, we searched a family of FP-NAS models, including 3 large ar-chitectures (from L0 to L2) of over 350M FLOPS and 4 small ones (from S1++ to S4++). When searching models using 0.4G target FLOPS, FP-NAS only uses 28.7 GPU-days, and is at least 132× faster than the search of EfﬁcientNet-B0, which uses a similar amount of FLOPS. Moreover, our method also discovers more superior FP-NAS-L0 model which outperforms EfﬁcientNet-B0 by 0.7% top-1 accuracy.
The largest model FP-NAS-L2, found via direct search on a subset of ImageNet, uses 1.05G FLOPS for a single crop.
To our knowledge, this is the largest architecture obtained via direct search.
To summarize, this work makes the contributions below:
• An adaptive sampling method for fast probabilistic NAS, which can sample 60% fewer architectures and accelerate the search in FBNetV2 space by 1.8×.
• A coarse-to-ﬁne search method by adopting a factorized distribution representation with much fewer architecture parameters in the early coarse-grained search stage. It can further accelerate the overall search by 1.2×.
• For searching small models, comparing with FBNetV2,
FP-NAS is not only 3.5× faster, but also discovers models with substantially better accuracy-to-complexity trade-off.
• For searching large models, compared with EfﬁcientNet, when searching models of 0.4G FLOPS, FP-NAS is not only 132× faster, but also discovers a model with 0.7% higher accuracy on ImageNet. The largest model we searched is FP-NAS-L2, which uses 1.05G FLOPS, out-perform EfﬁcientNet-B2 by 0.4% top-1 accuracy, while the search cost is much lower.
• We expand FBNetV2 search space by replacing Squeeze-Excite module [13] with searchable Split-Attention (SA) module [39]. We demonstrate uniformly inserting SA modules to the model with a ﬁxed number of splits, as done in hand-crafted ResNeSt [39] models, is sub-optimal, and models with searched SA modules are more competitive. 2.