Abstract 1.

Introduction
We propose pixelNeRF, a learning framework that pre-dicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance ﬁelds [27] involves optimiz-ing the representation to every scene independently, requir-ing many calibrated views and signiﬁcant compute time.
We take a step towards resolving these shortcomings by in-troducing an architecture that conditions a NeRF on im-age inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no ex-plicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view syn-thesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the ﬂexibility of pixel-NeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pix-elNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction.
For the video and code, please visit the project website: https://alexyu.net/pixelnerf.
We study the problem of synthesizing novel views of a scene from a sparse set of input views. This long-standing problem has recently seen progress due to advances in dif-ferentiable neural rendering [27, 20, 24, 39]. Across these approaches, a 3D scene is represented with a neural net-work, which can then be rendered into 2D views. Notably, the recent method neural radiance ﬁelds (NeRF) [27] has shown impressive performance on novel view synthesis of a speciﬁc scene by implicitly encoding volumetric density and color through a neural network. While NeRF can ren-der photorealistic novel views, it is often impractical as it requires a large number of posed images and a lengthy per-scene optimization.
In this paper, we address these shortcomings by propos-ing pixelNeRF, a learning framework that enables predict-ing NeRFs from one or several images in a feed-forward manner. Unlike the original NeRF network, which does not make use of any image features, pixelNeRF takes spatial image features aligned to each pixel as an input. This im-age conditioning allows the framework to be trained on a set of multi-view images, where it can learn scene priors to perform view synthesis from one or few input views. In contrast, NeRF is unable to generalize and performs poorly when few input images are available, as shown in Fig. 1. 4578
Speciﬁcally, we condition NeRF on input images by ﬁrst computing a fully convolutional image feature grid from the input image. Then for each query spatial point x and view-ing direction d of interest in the view coordinate frame, we sample the corresponding image feature via projection and bilinear interpolation. The query speciﬁcation is sent along with the image features to the NeRF network that outputs density and color, where the spatial image features are fed to each layer as a residual. When more than one image is available, the inputs are ﬁrst encoded into a latent represen-tation in each camera’s coordinate frame, which are then pooled in an intermediate layer prior to predicting the color and density. The model is supervised with a reconstruction loss between a ground truth image and a view rendered us-ing conventional volume rendering techniques. This frame-work is illustrated in Fig. 2.
PixelNeRF has many desirable properties for few-view novel-view synthesis. First, pixelNeRF can be trained on a dataset of multi-view images without additional supervision such as ground truth 3D shape or object masks. Second, pixelNeRF predicts a NeRF representation in the camera coordinate system of the input image instead of a canoni-cal coordinate frame. This is not only integral for general-ization to unseen scenes and object categories [41, 37], but also for ﬂexibility, since no clear canonical coordinate sys-tem exists on scenes with multiple objects or real scenes.
Third, it is fully convolutional, allowing it to preserve the spatial alignment between the image and the output 3D rep-resentation. Lastly, pixelNeRF can incorporate a variable number of posed input views at test time without requiring any test-time optimization.
We conduct an extensive series of experiments on syn-thetic and real image datasets to evaluate the efﬁcacy of our framework, going beyond the usual set of ShapeNet experi-ments to demonstrate its ﬂexibility. Our experiments show that pixelNeRF can generate novel views from a single im-age input for both category-speciﬁc and category-agnostic settings, even in the case of unseen object categories. Fur-ther, we test the ﬂexibility of our framework, both with a new multi-object benchmark for ShapeNet, where pixel-NeRF outperforms prior approaches, and with simulation-to-real transfer demonstration on real car images. Lastly, we test capabilities of pixelNeRF on real images using the
DTU dataset [14], where despite being trained on under 100 scenes, it can generate plausible novel views of a real scene from three posed input views. 2.