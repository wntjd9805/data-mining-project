Abstract
#229
#815
#836
Boosting performance of the ofﬂine trained siamese trackers is getting harder nowadays since the ﬁxed infor-mation of the template cropped from the ﬁrst frame has been almost thoroughly mined, but they are poorly capable of re-sisting target appearance changes. Existing trackers with template updating mechanisms rely on time-consuming nu-merical optimization and complex hand-designed strategies to achieve competitive performance, hindering them from real-time tracking and practical applications. In this paper, we propose a novel tracking framework built on top of a space-time memory network that is competent to make full use of historical information related to the target for better adapting to appearance variations during tracking. Specif-ically, a novel memory mechanism is introduced, which stores the historical information of the target to guide the tracker to focus on the most informative regions in the cur-rent frame. Furthermore, the pixel-level similarity compu-tation of the memory network enables our tracker to gen-erate much more accurate bounding boxes of the target.
Extensive experiments and comparisons with many com-petitive trackers on challenging large-scale benchmarks,
OTB-2015, TrackingNet, GOT-10k, LaSOT, UAV123, and
VOT2018, show that, without bells and whistles, our tracker outperforms all previous state-of-the-art real-time methods while running at 37 FPS. The code is available at https:
//github.com/fzh0917/STMTrack. 1.

Introduction
Visual object tracking is an essential task in computer vision with applications in various ﬁelds such as human-computer interactions [29], video surveillance [54], and au-tonomous driving [22]. Signiﬁcant efforts have been de-voted to address this problem, yet there is still a great gap to the practical applications due to the challenging factors such as occlusions, fast motions, and non-rigid deforma-∗Corresponding author.
#214
#247
#519
#419
#568
#1312
Ours
SiamFC++
DiMP-50
Ground Truth
Figure 1: Visualized comparisons of our method with rep-resentative trackers SiamFC++ [56] and DiMP-50 [2]. Our method can estimate more accurate target state when targets suffer from partial occlusions and non-rigid deformations. tions [13, 53, 20], which urge us to develop trackers with strong adaptiveness and robustness.
The goal of visual tracking is to locate an object in the subsequent frames of a video given its initial annotation in the ﬁrst frame. In recent years, with the advancements of deep learning techniques, deep trackers have dominated the tracking ﬁeld, among which two methodologies are widely studied, and one popular methodology addresses object tracking as a similarity matching problem between the tar-get template and the search frames in an embedding space ofﬂine trained. The representative template-matching meth-ods are siamese trackers [1, 65, 61, 14, 50, 6, 24, 23, 56, 15].
These methods usually do not update the template and thus are hard to adapt to appearance changes caused by occlu-sions, non-rigid deformations, etc.
To solve this problem, some trackers [2, 11] are equipped with sophisticated template updating mechanisms and thus show stronger robustness than siamese trackers. How-ever, online template updating requires much more compu-tational resources, which impends trackers from real-time tracking. Furthermore, these customized updating strate-gies [16, 58, 65, 25, 59, 7] introduce hyper-parameters that require tricky tuning. 13774
Note that when tracking moving objects humans remem-ber their identities in visual working memory to main-tain temporal continuity in a constantly changing environ-ment [33]. This inspires us to develop a memory-based tracking model to take advantage of rich historical informa-tion of the object. In contrast to previous works that strive to design template updating mechanisms to capture appear-ance variations of the object, our model predicts the state of the object from its historical information stored in the mem-ory network, thus avoiding of using the template and up-dating it. Thus we call our model a template-free method.
Furthermore, our tracker computes pixel-level similarities to locate the target, making it more robust to partial occlu-sions and non-rigid deformations than those using feature-map-level cross correlation. Fig. 1 depicts this advantage of our tracker (Refer to the section 1.1 of the supplemental material for quantitative comparisons).
The proposed method is evaluated on six benchmarks:
OTB-2015, TrackingNet, LaSOT, GOT-10k, UAV123, and
VOT2018 and surpasses all state-of-the-art real-time ap-proaches while running at 37 FPS. Notably, it achieves 80.3 success (AUC) on the challenging TrackingNet dataset, outperforming the previous best real-time method by 4.5%. It also sets new state-of-the-art performance on the performance-saturated OTB-2015 dataset.
Summarily, the main contributions of this work are four-fold.
•
•
•
•
We propose a novel end-to-end memory-based track-ing framework, which not only is as simple and ef-ﬁcient as the ofﬂine trained siamese networks, but also has strong adaptiveness ability as the sophisticated template updating strategies.
The proposed tracking framework deviates from the original evolving path of template-based tracking, and it could inspire more space-time-memory-based track-ers to be developed in the future.
A novel memory mechanism based on pixel-level sim-ilarity computation is introduced for visual tracking, which enables our tracker to have stronger robustness and to generate much more accurate target bounding boxes than many previous high-performance methods that use feature-map-level cross correlation.
Our proposed tracker outperforms all state-of-the-art real-time approaches on OTB-2015, TrackingNet, La-SOT, and GOT-10k, while running in real-time at 37
FPS, which demonstrates the superiority of the frame-work. 2.