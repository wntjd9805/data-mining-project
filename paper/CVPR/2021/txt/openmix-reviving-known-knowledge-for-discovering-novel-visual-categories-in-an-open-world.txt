Abstract
In this paper, we tackle the problem of discovering new classes in unlabeled visual data given labeled data from disjoint classes. Existing methods typically ﬁrst pre-train a model with labeled data, and then identify new classes in unlabeled data via unsupervised clustering. However, the labeled data that provide essential knowledge are often un-derexplored in the second step. The challenge is that the labeled and unlabeled examples are from non-overlapping classes, which makes it difﬁcult to build a learning rela-tionship between them. In this work, we introduce Open-Mix to mix the unlabeled examples from an open set and the labeled examples from known classes, where their non-overlapping labels and pseudo-labels are simultaneously mixed into a joint label distribution. OpenMix dynamically compounds examples in two ways. First, we produce mixed training images by incorporating labeled examples with un-labeled examples. With the beneﬁt of unique prior knowl-edge in novel class discovery, the generated pseudo-labels will be more credible than the original unlabeled predic-tions. As a result, OpenMix helps preventing the model from overﬁtting on unlabeled samples that may be assigned with wrong pseudo-labels. Second, the ﬁrst way encour-ages the unlabeled examples with high class-probabilities to have considerable accuracy. We introduce these exam-ples as reliable anchors and further integrate them with un-labeled samples. This enables us to generate more combi-nations in unlabeled examples and exploit ﬁner object re-lations among the new classes. Experiments on three clas-siﬁcation datasets demonstrate the effectiveness of the pro-posed OpenMix, which is superior to state-of-the-art meth-ods in novel class discovery. 1.

Introduction
Recent advances in deep learning have witnessed great developments in visual recognition, especially image clas-siﬁcation [4]. It is reported that modern image classiﬁcation
*Corresponding author models [9, 18, 19] can identify thousands of classes with high accuracy, but require a large number of labeled train-ing samples. Although semi-supervised learning (SSL) [2] and noisy label learning (NLL) [6] can mitigate the need for annotations and maintain high performance, they still require some clean (SSL) or noisy (NLL) annotations for every class of interest. Furthermore, the generalization abil-ity of learned classiﬁers is far from the human ability. In fact, a human can easily identify samples of new classes that may appear in real applications. However, a learned classi-ﬁer can only recognize samples of the known classes, but is likely to fail handling the ones of unseen (new) classes.
That is, it is signiﬁcantly difﬁcult and still underexplored to identify new classes that are undeﬁned previously and do not have any annotated samples.
In this work, we attempt to address the recent proposed problem, called novel class discovery [8], where we are given labeled data of known (old) classes and unlabeled data of novel (new) classes. It is an open set problem where classes of unlabeled data are undeﬁned previously and an-notated samples of these novel classes are not available. The goal of novel class discovery is to identify new classes in unlabeled data with the support of knowledge of old classes.
To achieve this objective, existing methods [7, 8, 10, 11] commonly follow a two-step learning strategy: 1) pre-train the model with labeled data to obtain basic discriminative ability; 2) recognize new classes in unlabeled data via un-supervised learning upon the trained model. However, the labeled data are only used to learn off-the-shelf features in the ﬁrst step, but are largely ignored in the second step.
In this way, the model can only beneﬁt from the off-the-shelf knowledge of the labeled data, but fails to leverage the underlying relationship between the labeled and unlabeled data. In this work, we argue that the labeled data provide essential knowledge about underlying object structures and common visual patterns. However, the use of labeled data is much harder than in semi-supervised learning [2, 16], due to the fact that the labeled and unlabeled samples are from disjoint classes.
To this end, the question is how to effectively exploit the labeled data to promote the discovery of new classes? In 9462
Unlabeled
Labeled
Mixed
C4
C5
C4
C5
Unlabeled
C5
C4
New
Mixed
C1
C2
C 3
C 4
C 5
Clean-label
Pseudo-label
Label Confidence
High
Medium
Low
C1
C2
C 3
C 4
C 5
Unlabeled
C 3
C1
C2
Old
C 4
C 5
New (a) MixUp among unlabeled samples (b) OpenMix among labeled and unlabeled samples
Figure 1. Examples of (a) directly using MixUp among unlabeled samples and (b) the proposed OpenMix. Due to the uncertainty of pseudo-labels of unlabeled samples, their mixed labels may still have low conﬁdence. In OpenMix, the prior knowledge (area of high conﬁdence) leads the mixed label to have high (exactly true) conﬁdence in old classes and medium (reliable) conﬁdence in new classes. this work, we try to answer this question and propose a sim-ple but effective method, called OpenMix, for the open set problem considered in this paper. OpenMix is largely mo-tivated by MixUp [31], which is widely used in supervised learning [29, 31] and semi-supervised learning [1, 2]. How-ever, one premise of using MixUp is that there should be labeled samples for every class of interest, which is not ap-propriate for our task. This is because we only have pseudo-labels for unlabeled samples of new classes, and the accu-racy of these pseudo-labels can not be guaranteed. If we di-rectly apply MixUp on unlabeled samples along with their uncertain pseudo-labels, the generated pseudo-labels will still be unreliable (Fig. 1 (a)). Training with these unreliable pseudo-labels may further damage the model performance.
Therefore, it is non-trivial to adopt MixUp for novel class discovery.
Instead of readily using MixUp on unlabeled samples, during the unsupervised clustering, OpenMix generates training samples by incorporating both labeled and unla-beled samples. OpenMix compounds samples in two ways.
First, OpenMix mixes the labeled samples with unlabeled samples. Meanwhile, since the labeled and unlabeled sam-ples belong to different label spaces, we ﬁrst extend their labels/pseudo-labels to joint label distributions, and then mix them. OpenMix leverages two priors in novel class dis-covery: 1) labels of labeled samples of old classes are ex-actly clean, and 2) labeled and unlabeled samples belong to completely different classes. These two properties encour-age the pseudo-labels of mixed samples to have 1) exactly true conﬁdence in old classes and 2) higher conﬁdence in new classes (Fig. 1 (b)). That is, in the old class set, the pseudo-label of a mixed sample is correct, because the la-bel of the labeled counterpart is correct and the unlabeled counterpart does not belong to any old classes. On the other hand, in the new class set, the uncertainty of a pseudo-label will be partially eliminated by mixing with the labeled sam-ple. This is because the labeled counterpart does not belong to any new classes and its label distribution in the new class set is exactly true. With the above properties, the pseudo-labels of mixed samples will be more reliable than those of their unlabeled counterparts. As a result, OpenMix can help preventing the model from overﬁtting on unlabeled samples that may be assigned wrong pseudo-labels. Second, we ob-serve that the ﬁrst way of OpenMix encourages the model to keep high classiﬁcation accuracy for unlabeled samples having high class-probabilities. Therefore, we select these samples as reliable anchors of new classes and mix them with unlabeled samples for further improvement.
In summary, the contributions of this paper are: (1) This work proposes the OpenMix, which is tailor-made for effec-tively leveraging known knowledge in novel class discov-ery. OpenMix can prevent the model from ﬁtting on wrong pseudo-labels, thereby consistently improving the model performance. (2) OpenMix enables us to explore reliable anchors from unlabeled samples, which can be used to gen-erate diverse smooth samples of new classes towards a more (3) This paper presents a simple discriminative model. baseline for novel class discovery, which can achieve com-petitive results. Experiments conducted on three datasets show that our approach outperforms the state-of-the-art methods by a large margin in novel class discovery. 2.