Abstract
We introduce a framework that predicts the goals behind observable human action in video. Motivated by evidence in developmental psychology, we leverage video of uninten-tional action to learn video representations of goals without direct supervision. Our approach models videos as contex-tual trajectories that represent both low-level motion and high-level action features. Experiments and visualizations show our trained model is able to predict the underlying goals in video of unintentional action. We also propose a method to “automatically correct” unintentional action by leveraging gradient signals of our model to adjust latent trajectories. Although the model is trained with minimal supervision, it is competitive with or outperforms baselines trained on large (supervised) datasets of successfully exe-cuted goals, showing that observing unintentional action is crucial to learning about goals in video. 1.

Introduction
Goal-directed action is all around us. Even though Fig-ure 1 shows a person performing an unconventional action (heating a wine bottle with a blowtorch), we cannot help but to perceive the action as rational in the context of the goal (to open the bottle). cause future goals are not directly observable in video. How-ever, in a series of papers, developmental psychologists
Amanda Woodward and Michael Tomasello demonstrated that children reason about goals before their second birth-day [48, 56], and this reasoning plays a key role in rapid development of communicative skills [49] and mental repre-sentations of the world [2]. Despite the relative ease of this task for children, machine recognition of goals has remained challenging.
The hypothesis underlying this paper is that examples of failure are key missing pieces in action recognition systems.
Without observing unintentional action, we cannot expect models to discriminate goals from actions. Examples demon-strating unintentional action are necessary to decouple these two notions, separating between the visible action and the latent goals. As Efros has been telling us all along, it is all about the data [20], and negative data doubly so [59].
The main observation behind our approach is that natural video will contain abundant and rich examples of both inten-tional and unintentional action [9], which we can leverage for learning. In our model, video is represented as a trajectory, and goals are encoded as the path for the trajectory. Given ex-amples of videos with variable success, we present a model that learns goal-oriented video representations by discrimi-nating between success and failure. Our model captures both motion and relational features through an attention-based transformer architecture, allowing end-to-end training.
Predicting the goal of action may seem challenging be-Our experiments show that failure data is crucial for learn-Figure 1: What are they doing? While just the action is observable (heating the bottle), we still predict the goal behind the action (to open the bottle). In this paper, we learn from failure examples to learn representations of goals in video. 11194
Figure 2: Learning goal-oriented video rep-resentations: We show an overall view of our approach. First, we embed short clips using a 3D CNN to represent short-term motion features.
Then, we run the sequence of CNN embeddings through a stack of Transformers, where they in-teract with each other to ﬁnally form a context-adjusted latent action trajectory. The model is trained end-to-end from scratch, with intentional-ity and temporal coherence losses (depicted top-left). Points along the resultant trajectory are de-coded with linear projections into various spaces (top-middle). ing representations of goals. We evaluate our model on three goal prediction tasks. First, we experiment on detecting unintentional action in video, and we demonstrate strong performance over baselines on this task. Second, we eval-uate the representation at predicting goals with minimal supervision, which we characterize as structured categories consisting of subject, action, and object triplets. Lastly, we use our representation to automatically “correct” uninten-tional action and decode these corrections by retrieving from other videos or generating categorical descriptions.
Our main contribution is an approach that, training on data of unintentional action, learns a goal-directed represen-tation of videos. We show that our model often captures the latent goals behind observed action, performing on par with or better than supervised models trained on large la-beled datasets of only intentional action. We also introduce a method to ﬁnd minimal adjustments to the path and “automat-ically correct” unintentional action in video. The remainder of this paper will describe this approach in detail. Code, data, and models will be available. 2.