Abstract
In this paper, we propose a novel learning-based polyg-onal point set tracking method. Compared to existing video object segmentation (VOS) methods that propagate pixel-wise object mask information, we propagate a polygonal point set over frames. Speciﬁcally, the set is deﬁned as a subset of points in the target contour, and our goal is to track corresponding points on the target contour. Those outputs enable us to apply various visual effects such as motion tracking, part deformation, and texture mapping. To this end, we propose a new method to track the correspond-ing points between frames by the global-local alignment with delicately designed losses and regularization terms.
We also introduce a novel learning strategy using synthetic and VOS datasets that makes it possible to tackle the prob-lem without developing the point correspondence dataset.
Since the existing datasets are not suitable to validate our method, we build a new polygonal point set tracking dataset and demonstrate the superior performance of our method over the baselines and existing contour-based VOS meth-ods. In addition, we present visual-effects applications of our method on part distortion and text mapping. 1.

Introduction
Object mask tracking in a video is one of the most fre-quently required tasks in visual effects (VFX). However, the task (i.e. rotoscoping) is so painstaking and time-consuming that even a highly-skilled designer processes only a dozen frames on average per day [28]. Therefore, propagating ob-ject mask information through subsequent frames becomes a critical problem to reduce human labor for rotoscoping.
Propagation methods are categorized into four groups based on object representations: point, region, contour, and polyg-onal point set (Figure 2). Each representation carries differ-ent amount of information.
Patch tracking [14, 33, 13, 7] denotes a target object as point representation and tracks a target point over frames by matching the patch around the point. The tracking en-ables visual effects that require positional information such
∗Work mostly done during a M.S. student at Yonsei University
Before
After
Figure 1: Our method tracks a set of points in a polygon over frames. The output represents mask contour with point correspondences across frames. It allows multiple applica-tions, e.g., a non-rigid transformation of a speciﬁc part of an object over time as shown here. as motion tracking and texture mapping. These applications often require multiple patch tracking to compute point-to-point information. However, conducting each patch track-ing independently ignores strong correlations between tar-get points, thus multiple patch tracking are susceptible to a drift problem and are not suitable for mask propagation.
Meanwhile, video object segmentation (VOS) [44, 8, 22, 47, 35, 34] and contour tracking [20, 49, 11, 39] propagate target object information over subsequent frames by repre-senting the target as a region (i.e. mask) and a contour re-spectively. These representations can describe only the tar-get area without any pixel correspondences, therefore they are not applicable to complex VFX scenarios that require point-to-point relation information (e.g., Figure 1).
On the other hand, polygonal point set tracking com-bines the positional information with the target region. The polygonal point set is deﬁned as a subset of contour points that represent an object in a polygonal shape. By track-ing the point set, we can get both the object contour and the point-to-point matching information. Previous works in this category [2, 28, 32, 37] focus on making the user inter-5569
(a) Patch Tracking (b) Video Object Segmentation (c) Contour Tracking (d) Point Set Tracking (Ours)
Figure 2: Illustration of different approaches for object mask propagation. (a) Independent multi-patch tracking represents an object coarsely and drifts easily. (b) Region-based video object segmentation achieves high accuracy in pixel-level dense prediction, but it usually may yield cattery boundaries due to a high degree of freedom and does not provide point (c) In contour tracking, the constrained contour representation can give us clean boundary, but point correspondences. correspondence information is still missing. On the other hand, (d) Polygonal point set tracking provides both clean polygonal object mask and point correspondences across frames. action easier for highly customizable results rather than tak-ing the point-to-point matching (or tracking) into account.
Therefore, they assume heuristic shape priors of an object and often exhibit propagation failures for challenging object motions.
In this paper, we aim to track all points directly through a learning-based approach without assuming a heuristic shape prior and propose a novel point set tracking method. Ac-cording to the hypothesis that the target object state in adja-cent frames is highly correlated, we train a network to learn progressive alignment of a point set between frames. We
ﬁrst match the point set globally using a simple rigid trans-formation. Then, we further tune each point position in a coarse to ﬁne manner using a local alignment module. Our local alignment module (LAM) adopts recurrent neural net-works (RNN) to take into account the temporal history of each point and also uses multi-head attention (MHA) mod-ules for non-local communication among the points. In ad-dition, we regularize the alignments to avoid drifting and honor the original topology of a target polygon in challeng-ing situations.
We introduce a new learning strategy to train our model without fully annotated data. Currently available
VOS datasets [9, 48] only contain region information (i.e. masks), thus it cannot be directly applied to our point cor-respondence learning. To overcome the data issue, we pro-pose an unsupervised learning method based on cyclic con-sistency between predicted frames [45, 46]. We also obtain the supervision for point set tracking by synthesizing data from image instance segmentation data. To the best of our knowledge, this is the ﬁrst work on learning-based point set tracking that considers the point correspondence.
Figure 1 shows an example application that utilizes our network results. In this example, to exaggerate the upper body, the same effect is applied to the target across the entire frame, even if the user edits the points in the set individually only in the ﬁrst frame.
Popular evaluation datasets for video object segmenta-tion are not suitable for evaluating point-set tracking as they only provide mask annotations [9, 16, 27, 32]. While
CPC [32] provides annotations of object contours, it is also not sufﬁcient for the evaluation as there is no point corre-spondence annotation. To this end, we introduce an evalu-ation dataset for polygonal point set tracking, consisting of 30 sequences. To build the dataset, named PoST, we aug-ment video clips from existing VOS datasets with additional point set annotations with correspondence. We evaluate our method on PoST and the existing VOS datasets [9, 32], and we show that our method outperforms competing methods with a large margin.
Our contributions are summarized as follows:
• We propose a novel learning-based method for polyg-onal point set tracking with point correspondence for the ﬁrst time.
• We design a local alignment module for point tracking with taking temporal history and communication with other points into account.
• We present a learning strategy to train a deep network for point set tracking using unsupervised learning and synthesized data.
• We introduce a new dataset for evaluating performance of point set tracking. 2.