Abstract
Real-time and high-performance 3D object detection is an attractive research direction in autonomous driving. Re-cent studies prefer point based or voxel based convolution for achieving high performance. However, these methods suffer from the unsatisﬁed efﬁciency or complex customized convolution, making them unsuitable for applications with
In this paper, we present an efﬁ-real-time requirements. cient and effective 3D object detection framework, named
RangeIoUDet that uses the range image as input. Ben-eﬁting from the dense representation of the range image,
RangeIoUDet is entirely constructed based on 2D convo-lution, making it possible to have a fast inference speed.
This model learns pointwise features from the range im-age, which is then passed to a region proposal network for predicting 3D bounding boxes. We optimize the pointwise feature and the 3D box via the point-based IoU and box-based IoU supervision, respectively. The point-based IoU supervision is proposed to make the network better learn the implicit 3D information encoded in the range image.
The 3D Hybrid GIoU loss is introduced to generate high-quality boxes while providing an accurate quality evalua-tion. Through the point-based IoU and the box-based IoU,
RangeIoUDet outperforms all single-stage models on the
KITTI dataset, while running at 45 FPS for inference. Ex-periments on the self-built dataset further prove its effec-tiveness on different LIDAR sensors and object categories. 1.

Introduction
As the vital component of autonomous driving systems, 3D object detection from point clouds has attracted more and more attention. Many methods have been proposed for processing point clouds and achieved excellent perfor-mance. However, most of these methods are difﬁcult to ap-ply in practice, due to the complex framework, high mem-∗Corresponding author. This work is supported by National Key R&D
Program of China (Grant No.2020AAA010400X).
Figure 1. Speed (Hz) versus accuracy (AP) on the test set of KITTI 3D car detection. The single-stage methods are drawn as circles and the two-stage methods are drawn as squares. RangeIoUDet outperforms all methods except the top two-stage method PV-RCNN [24], and is much faster than PV-RCNN. ory complexity, and slow inference time (Fig. 1). The meth-ods preferred by practical applications generally meet the following characteristics: simple framework for easy de-ployment, fast inference time, and 2D convolution based model without extra customized operations.
In terms of the input representations of point clouds, most existing methods can be divided into three types: voxel based, point based, and range image based meth-ods. Voxel based [38, 32, 8, 26, 24] and point based meth-ods [20, 31, 25, 27, 33] are currently the popular methods, but they are difﬁcult to apply in practice due to the memory and time complexity issue. Range image based methods have been explored in early deep learning based 3D object detection [4]. As the raw data format of the LIDAR sensor, the range image is dense and compact, and retains almost all original information with minor loss. Operating on the range image enjoys the beneﬁt of applying mature 2D con-volution and does not suffer from the sparsity issue of point clouds, but this representation has been ignored for a long time for its unsatisﬁed performance [18]. Recently, several methods [18, 2, 29] rethink the advantages of this represen-tation and propose effective frameworks based on the range 7140
image. However, the inference speeds of these methods are still unsatisfactory due to the two-stage architecture [2] or multi-view fusion [29], making them still unable to meet the needs of practical applications. In this paper, we propose a high-performance and fast-speed single-stage 3D detection method based on the range image.
A simple idea utilizing the range image is to extract pointwise features from the range image [19] and then regress 3D bounding boxes from the bird’s eye view, illus-trated in the upper part of Fig. 2. Such a framework was proposed in our preliminary work [16] on the ArXiv.
It only needs 2D convolution thanks to the dense represen-tation of the range image. We introduce this framework in
Sec. 3.1 of this paper. Although the framework is elegant, its performance is not satisfactory. The main drawback of the range image representation is the lack of the 3D local relationship, which means that points far away in the 3D space may be adjacent in the range image plane. It causes that although the pixels around the boundary of the object and background are far away in the 3D space, their fea-tures extracted from the range image may be similar due to the blurry issue of 2D feature extraction, which leads to the inaccurate pointwise features. The range image stores 3D spatial coordinates in its input channels, which means that it has the potential to learn more accurate features. To this end, we propose a point-based module to allow the net-work to learn the hidden spatial information encoded in the range image by explicit loss supervision, thereby indirectly enhancing the pointwise features. Speciﬁcally, the point-wise features are aggregated within the receptive ﬁeld of 3D points, and then supervised by the point-based IoU [1].
It is worth noting that the point-based module is only used to supervise the learning of pointwise features during train-ing, and is not needed for inference, so it will not bring extra computation cost or customized convolution.
Apart from enhancing the pointwise features, it is neces-sary to design power supervision losses to force the network to learn high-quality 3D boxes, especially for the single-stage model without the reﬁnement stage. The performance of the 3D bounding box is mainly affected by the seven positioning parameters and the conﬁdence score of the 3D box. Most current methods use smooth L1 loss to indepen-dently optimize the seven positioning parameters, and use the classiﬁcation score to represent the conﬁdence of the box. However, the positioning parameters are usually cou-pled with each other [35, 23], and the classiﬁcation score cannot fully reﬂect the quality of the box. In order to ad-dress the above two challenges, we propose the 3D Hybrid
GIoU loss based on the implementation of the differentiable 3D IoU. The ”Hybrid” here consists of two meanings: the hybrid regression strategy and the combination of the re-gression and quality evaluation. On the one hand, we pro-pose to use the smooth L1 loss to supervise the location of the box center and use the 3D GIoU loss to indirectly super-vise the size of the 3D box. Such a combination avoids the local optimum of smooth L1 loss, and achieves better per-formance compared to the sole 3D GIoU. On the other hand, because most 3D objects have only partial point clouds, it is not easy to accurately regress the 3D bounding boxes, so a score that accurately measures the quality of the box is meaningful. 3D IoU is just a crucial indicator to measure the box quality, so we use the differentiable 3D IoU as the quality score to evaluate the quality of the 3D bounding box.
In summary, our contributions can be summarized into four-fold:
• We propose a single-stage 3D detection model
RangeIoUDet based on the range image, which is sim-ple, effective, fast, and only uses 2D convolution.
• We enhance pointwise features by supervising the point-based IoU, which makes the network better learn the implicit 3D information from the range image.
• We propose the 3D Hybrid GIoU (HyGIoU) loss for supervising the 3D bounding box with higher location accuracy and better quality evaluation.
• Our proposed single-stage model RangeIoUDet achieves state-of-the-art performance on the compet-itive KITTI 3D detection benchmark and the actual operation scenario dataset. 2.