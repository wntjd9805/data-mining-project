Abstract
We propose a universal building block of Convolutional
Neural Network (ConvNet) to improve the performance without any inference-time costs. The block is named Di-verse Branch Block (DBB), which enhances the representa-tional capacity of a single convolution by combining diverse branches of different scales and complexities to enrich the feature space, including sequences of convolutions, multi-scale convolutions, and average pooling. After training, a
DBB can be equivalently converted into a single conv layer for deployment. Unlike the advancements of novel Con-vNet architectures, DBB complicates the training-time mi-crostructure while maintaining the macro architecture, so that it can be used as a drop-in replacement for regular conv layers of any architecture. In this way, the model can be trained to reach a higher level of performance and then transformed into the original inference-time structure for inference. DBB improves ConvNets on image classiﬁcation (up to 1.9% higher top-1 accuracy on ImageNet), object detection and semantic segmentation. The PyTorch code and models are released at https://github.com/
DingXiaoH/DiverseBranchBlock. 1.

Introduction
Improving the performance of Convolutional Neural
Network (ConvNet) has always been a heated research topic. On one hand, the advancements in architecture de-sign, e.g., the Inception models [27, 28, 26, 15], have re-∗This work is supported by The National Key Research and Develop-ment Program of China (No. 2017YFA0700800), the National Natural
Science Foundation of China (No.61925107, No.U1936202) and Beijing
Academy of Artiﬁcial Intelligence (BAAI). Xiaohan Ding is funded by the
Baidu Scholarship Program 2019 (http://scholarship.baidu. com/). This work is done during Xiaohan Ding’s internship at MEGVII.
†Corresponding author. vealed that the multi-branch topology and combination of various paths with different scales and complexities can en-rich the feature space and improve the performance. How-ever, the complicated structure usually slows down the in-ference, as a combination of small operators (e.g., concate-nation of 1 × 1 conv and pooling) is not friendly to the de-vices with strong parallel computing powers like GPU [21].
On the other hand, more parameters and connections usually lead to higher performance, but the size of ConvNet we deploy cannot increase arbitrarily due to the business requirements and hardware constraints. Considering this, we usually judge the quality of a ConvNet by the trade-off between performance and inference-time costs such as the
In latency, memory footprint and number of parameters. the common cases, we train the models on powerful GPU workstations and deploy them onto efﬁciency-sensitive de-vices, so we consider it acceptable to improve the perfor-mance with the costs of more training resources, as long as the deployed model keeps the same size.
In this paper, we seek to insert complicated structures into numerous ConvNet architectures to improve the perfor-mance while keeping the original inference-time costs. To this end, we decouple the training-time and inference-time network structure by complicating the model only during training and converting it back into the original structure for inference. Naturally, we require such extra training-time structures to be 1) effective in improving the training-time model’s performance and 2) able to transform into the orig-inal inference-time structure.
For the usability and universality, we upgrade the basic
ConvNet component, K × K conv, into a powerful block named Diverse Branch Block (DBB) (Fig. 1). As a build-ing block, DBB is complementary to the other efforts to im-prove ConvNet, e.g., architecture design [12, 24, 13, 21, 23, 35], neural architecture search [2, 39, 22, 20, 19], data aug-mentation and training methods [25, 5, 33] and fulﬁlls the above two requirements by the following two properties: 10886
input input 1×1 1×1 1×1
K×K
K×K replace conv layers
K×K
AVG
+ with DBBs
+
+
K×K
K×K
K×K
K×K
+
+ output
Training-time DBB output
Inference-time DBB
Training-time model
Inference-time model conv
AVG average pooling batch norm nonlinearity
Figure 1: A representative design of Diverse Branch Block (DBB). The block can be equivalently transformed into a regular conv layer for deployment, so that we can complicate the training-time microstructures of ConvNet without affecting the macro architecture (e.g., ResNet) or the inference-time structure. Note that it is only an instance we used, and one may utilize the six transformations summarized in this paper (Sect. 3.2) to customize a DBB with more complicated structures.
• A DBB adopts a multi-branch topology with multi-scale convolutions, sequential 1 × 1 - K × K con-volutions, average pooling and branch addition. Such operations with various receptive ﬁelds and paths with different complexities can enrich the feature space, just like the Inception architectures.
• A DBB can be equivalently transformed into a sin-gle conv for inference. Given an architecture, we can replace some regular conv layers with DBB to build more complicated miscrostructures for training, and convert it back into the original structure so that there will be no extra inference-time costs.
More precisely, we do not derive the parameters for in-Instead, we convert the ference before each forwarding. model after training once for all, then we only save and use the resultant model, and the trained model can be dis-carded. The idea of converting a DBB into a conv can be categorized into structural re-parameterization, which means parameterizing a structure with the parameters trans-formed from another structure, together with a concurrent work [9]. Though a DBB and a regular conv layer have the same inference-time structure, the former has higher rep-resentational capacity. Through a series of ablation experi-ments, we attribute such effectiveness to the diverse connec-tions (paths with different scales and complexities) which resembles Inception units, and the training-time nonlinear-ity brought by batch normalization [15]. Compared to some counterparts with duplicate paths or purely linear branches (Fig. 6), DBB shows better performance (Table. 4).
We summarize our contributions as follows.
• We propose to incorporate abundant microstructures into various ConvNet architectures to improve the per-formance but keep the original macro architecture.
• We propose DBB, a universal building block, and sum-marize six transformations (Fig. 2) to convert a DBB into a single convolution, so it is cost-free for the users.
• We present a representative Inception-like DBB in-stance and show that it improves the performance on
ImageNet [7] (e.g., up to 1.9% higher top-1 accuracy),
COCO detection [18] and Cityscapes [4]. 2.