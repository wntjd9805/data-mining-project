Abstract
Single image low light enhancement is an important task and it has many practical applications. Most existing meth-ods adopt a single image approach. Although their per-formance is satisfying on a static single image, we found, however, they suffer serious temporal instability when han-dling low light videos. We notice the problem is because existing data-driven methods are trained from single image pairs where no temporal information is available. Unfortu-nately, training from real temporally consistent data is also problematic because it is impossible to collect pixel-wisely paired low and normal light videos under controlled envi-ronments in large scale and diversities with noise of iden-tical statistics. In this paper, we propose a novel method to enforce the temporal stability in low light video enhance-ment with only static images. The key idea is to learn and in-fer motion ﬁeld (optical ﬂow) from a single image and syn-thesize short range video sequences. Our strategy is gen-eral and can extend to large scale datasets directly. Based on this idea, we propose our method which can infer mo-tion prior for single image low light video enhancement and enforce temporal consistency. Rigorous experiments and user study demonstrate the state-of-the-art performance of our proposed method. Our code and model will be pub-licly available at https://github.com/zkawfanx/
StableLLVE. 1.

Introduction
Illumination in sunny day and low light night can vary more than 10 orders of magnitude. In low light scenes, sen-sor noise is not negligible due to the low signal-to-noise ratio (SNR). Therefore, low light image enhancement is an important task which improves the SNR and enhances the image after modeling the noise and the signal. It enables various computer vision algorithms to perform properly.
Rather than explicitly modeling the noise and the signal,
*Corresponding author: fuying@bit.edu.cn recent data-driven methods [3, 4, 9, 25, 26, 27] implicitly learn such models from image data and get satisfying re-sults on a single static image. And they require pixel-wisely paired images of low and high SNR for training. However, we notice that it is impossible to collect pixel-wisely paired low and normal light videos under controlled environments in large scale and diversities with noise of identical statis-tics. Therefore, existing single image methods use either synthetic data or temporally inconsistent single image data for training. Thus, no temporal consistency can be learned through existing data. One can perceive serious artifacts and
ﬂickering from existing single image methods when han-dling low light videos.
In this paper, we aim to enforce temporal consistency even when training from static images. We propose a novel method to enforce the temporal stability in low light video enhancement with only static images. The key idea is to learn and infer motion ﬁeld (optical ﬂow) from a single im-age and synthesize short range video sequences. Our strat-egy is general and can extend to large scale datasets directly.
Based on this idea, we propose our method which can in-fer motion prior for single image low light video enhance-ment and enforce temporal consistency. In particular, we present an image-based method to achieve low light video enhancement and tackle temporal inconsistency problem by imposing consistency on the network. Speciﬁcally, we choose optical ﬂow to mimic motions of dynamic scenes. It is more capable of representing both global and local mo-tions. We ﬁrst predict plausible optical ﬂow from static im-ages. Then we warp images with optical ﬂow to be adjacent frames and impose consistency on deep model.
We conduct rigorous experiments to validate the effec-tiveness of our method. Experimental results on both syn-thetic and real data show that our method outperforms the state-of-the-art single image methods and achieves compa-rable results to video-based ones, which means our method can alleviate ﬂickering problem without the need of videos.
Furthermore, we also conduct a user study on 26 volunteers, of whom 78.9% prefer our method, suggesting the better temporal stability of our method. 4967
Our main contributions are summarized as follows:
• We present a novel solution to solving temporal incon-sistency problem of low light video enhancement when using only single image data.
• We propose to use optical ﬂow prior to indicate po-tential motion from single image and thus enable us to model the temporal consistency.
• We demonstrate the state-of-the-art performance of our method from rigorous experiments and user study. 2.