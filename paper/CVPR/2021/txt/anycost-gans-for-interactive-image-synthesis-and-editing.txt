Abstract
Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing. However, due to the high computational cost of large-scale generators (e.g.,
StyleGAN2), it usually takes seconds to see the results of a single edit on edge devices, prohibiting interactive user experience. In this paper, inspired by quick preview features in modern rendering software, we propose Anycost GAN for interactive natural image editing. We train the Anycost
GAN to support elastic resolutions and channels for faster image generation at versatile speeds. Running subsets of the full generator produce outputs that are perceptually similar to the full generator, making them a good proxy for quick preview. By using sampling-based multi-resolution train-ing, adaptive-channel training, and a generator-conditioned discriminator, the anycost generator can be evaluated at various conﬁgurations while achieving better image quality compared to separately trained models. Furthermore, we develop new encoder training and latent code optimization techniques to encourage consistency between the different sub-generators during image projection. Anycost GAN can be executed at various cost budgets (up to 10× computation reduction) and adapt to a wide range of hardware and la-tency requirements. When deployed on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12× speedup, enabling interactive image edit-ing. The code and demo are publicly available. 1.

Introduction
Generative Adversarial Networks (GANs) [17] have ex-celled at synthesizing diverse and realistic images from ran-domly sampled latent codes [41, 42, 43, 57, 6]. Furthermore, a user can transform the generated outputs (e.g., add smiling to a portrait) by tweaking the latent code [61, 38, 42, 24, 65].
In real-world use cases, a user would often like to edit a natu-ral image rather than generating random samples. To achieve this, one can project the image into the image manifold of
GANs by ﬁnding a latent code that reconstructs the image, and then modify the code to produce ﬁnal outputs [79].
Despite its photorealistic results and versatile editing abil-ity, modern deep generative models incur huge computa-tional costs, prohibiting edge deployment. For example, the StyleGAN2 generator [43] consumes 144G MACs, 36× larger compared to ResNet-50 [26]. The expensive model often introduces a several-second delay for a single edit, leading to a sub-optimal user experience and shorter battery life when used on an edge device.
∗ Part of the work done during an internship at Adobe Research.
Modern 2D/3D content creation workﬂows, such as the 14986
preview rendering feature in Maya and Blender, as well as the playback feature in Adobe Premiere Pro, allow users to easily control the tradeoff between image quality and rendering speed. A user can turn off certain visual effects, reduce the resolution and ﬁdelity, or use a fast method dur-ing user interaction. Once the edit is ﬁnalized, a user can use an expensive method with additional visual effects at a higher resolution. In rendering literature, this tradeoff can be easily achieved by reducing the number of sampled rays in ray/path tracing [39], or early stopping of iterative solvers in progressive radiosity [18, 13]. In this work, we aim to bring a smooth tradeoff between visual quality and interactivity to deep generative models.
We propose “Anycost” GANs for interactive image syn-thesis and editing. Our goal is to train a generator that can be executed at a wide range of computational costs while producing visually consistent outputs: we can ﬁrst use a low-cost generator for fast, responsive previews during image editing, and then use the full-cost generator to render high-quality ﬁnal outputs. We train the anycost generators to sup-port multi-resolution outputs and adaptive-channel inference.
The smaller generators are nested inside the full generator via weight-sharing. Supporting different conﬁgurations in one single generator introduces new challenges for minimax optimization, as the adversarial optimization involves too many players (different sub-generators). Vanilla GAN train-ing methods fail catastrophically in this setting. To stabilize the training process, we propose to perform stage-wise train-ing: ﬁrst train sub-generators at multiple resolutions but with full channels, and then train sub-generators with reduced channels. To account for different sub-generators’ capacities and architectures, we train a weight-sharing discriminator that is conditioned on the architectural information of the speciﬁc sub-generator.
We train Anycost GAN to support two types of channel conﬁgurations: uniform channel reduction ratio and ﬂexi-ble ratios per layer. The combined architecture space leads to high ﬂexibility in terms of computation cost, containing sub-generators at > 10× computation difference. We can directly obtain low-cost generators by taking a subset of weights without any ﬁne-tuning, which allows us to easily switch between quality and efﬁciency. To handle diverse hardware capacities, we use evolutionary search to automat-ically ﬁnd the best sub-generator under different computa-tional budgets, while achieving the best output consistency w.r.t. the full-cost generator (Figure 1b).
To better maintain consistency during the image projec-tion process, we further propose consistency-aware encoder training and iterative optimization for image projection. We optimize the reconstruction loss, not only for the full-cost generator, but also for the sub-generators, which signiﬁcantly improves the consistency during both image projection and editing steps (Figure 1c).
Our single, anycost generator can provide visually con-sistent outputs at various computation budgets. Compared to small generators of the same architecture or existing com-pression methods based on distillation, our method can pro-vide better generation quality and higher consistency w.r.t. to the full generator. The generated outputs from generators at different costs also share high-level visual cues, for example, producing generated faces with consistent facial attributes (see arXiv version). Our method provides 12.2× speed-up on Xeon CPU and 8.5× speed up on Jetson Nano GPU for faster preview. Combined with consistency-aware image projection, our anycost generator maintains consistency after various editing operations (Figure 7) and offers an efﬁcient and interactive image editing experience. 2.