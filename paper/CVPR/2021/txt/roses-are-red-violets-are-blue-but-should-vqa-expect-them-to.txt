Abstract
Models for Visual Question Answering (VQA) are notori-ous for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to “rea-son”, leading them to perform “educated guesses” instead.
In this paper, we claim that the standard evaluation met-ric, which consists in measuring the overall in-domain ac-curacy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit sub-tle training set statistics. Alternatively, naively introducing artiﬁcial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not re-ﬂect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are speciﬁcally designed for this particular setting, and do not generalize to other conﬁgurations. We propose the GQA-OOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we ex-perimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA mod-els and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions in-volving infrequent concepts and provide recommendations for future directions of research. 1.

Introduction
Visual Question Answering (VQA), i.e. the task of an-swering a question posed over an image, is often seen as a testbed for the capability of learning-based systems to per-form high-level reasoning. This multimodal problem is no-torious for its diversity, meaning that VQA models are re-quired to learn various high-level general representations of concepts of the physical world as well as their interactions.
Efforts to learn the necessary high-level reasoning from large-scale datasets depend on the absence of harmful bi-ases in the data, which could provide unwanted shortcuts
Question	groups	 (context)
Group:	objects	on	walls (…) (…)
“What	is	on	 the	wall?”
”
P
“
M ic t ir r u r e
“
P
M ai
C
A n
V
I
S-ti n
O r
” o r
”:
”:	(
Q
“
S t a
“
S h (
G elf
”:
T	A
R
A
B
A
C
L
N 4
[
E,	L
M
“
C
“
L
B
U
T o tt o eft”: n	d
D
B
+
B
U
T n s
D w
+
B e s s
U
T
D
P
[ 9
] e r
+
L t”:	
N	[ 3
L
S
T
M sti o
[ 4 u e g 1
”:
],	
],	B ri o
M n	p
N
U
T
[ 8
]
],	
E
R e r
R
U 1 7
X
M
):	
B
I	[ 7
] r
)
D	[ 3
]
T	[ 2 6
]
M 	[ 9
]
Figure 1. We address bias exploitation in VQA and propose a new benchmark for Out-Of-Distribution evaluation containing distribu-tion shifts tailored to different question groups with highly imbal-anced distributions. A new evaluation metric based on rareness inside each question group, here shown for ”objects on walls”, is experimentally demonstrated to be less prone to bias exploitation.
We show that SOTA methods (7 VQA models and 3 bias reduction methods) reproduce biases in training data. to learning in the form of “Clever Hans” effects. Unfortu-nately, and in spite of recent efforts [11, 15], most VQA datasets remain very imbalanced. Common concepts are signiﬁcantly more frequent, e.g the presence of a “red rose”, compared to out of context concepts like the presence of a
“zebra in a city”. This causes the tendency of models to overly rely on biases, hindering generalisation [7, 9]. De-spite a general consensus on this diagnostic, systemic eval-uations of error distributions are rare. In particular, overall accuracy is still the major, and often unique, metric used to evaluate models and methods, although it is clearly insuf-ﬁcient. Several questions remain open. How is error dis-tributed? Are true positives due to reasoning or to exploita-tion of bias? What is the prediction accuracy on infrequent vs. frequent concepts? How can we validate models in Out
Of Distribution (OOD)-settings?
In this work we propose a new benchmark and a study of State-Of-The-Art (SOTA) VQA models, which allows to precisely answer these questions. The proposed new eval-uation protocol is complementary to existing ones, but al-12776
lows a better diagnostic of current VQA performance. In particular, our benchmark can be viewed as an alternative to the VQA-CP [2] dataset, which has lead to mixed results (see [25] for a recent comprehensive study). Our benchmark comprises (i) a new ﬁne-grained reorganization of the GQA dataset [15] introducing distribution shifts in both validation and test sets (see Figure 1-a); (ii) a set of evaluation metrics; (iii) new evaluation plots illustrating the generalisation be-havior of VQA models on different operating points. The choice of the GQA dataset is motivated by its useful struc-turing into question groups, which allows to capture biases precisely, to select groups with strong biases and to create distribution shifts tailored to the exact nature of each ques-tion (see Figure 1-b). It also makes it possible to analyze how errors are distributed over different associations of con-cepts according to their frequency in the dataset.
Our contributions — (i) We propose and make pub-lic1 a new ﬁne-grained re-organization of the GQA dataset and a set of the respective evaluation metrics allowing to precisely evaluate the reasoning behavior of VQA models and to characterise and visualise their generalisation behav-ior on different operating points w.r.t distribution shifts. (ii)
Compared to competing benchmarks, our dataset features distribution shifts for both, validation and test, allowing to validate models under OOD conditions. (iii) We experimen-tally evaluate the usefulness of the proposed metric showing its behavior on models trained to, more or less, exploit bi-ases. (iv) In a large study, we evaluate several recent VQA models and show that they struggle to generalise in OOD conditions; we also test several SOTA bias reduction meth-ods and show that there is still room for improvement in addressing bias in VQA. 2.