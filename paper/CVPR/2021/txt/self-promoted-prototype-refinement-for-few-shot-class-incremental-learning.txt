Abstract
Few-shot class-incremental learning is to recognize the new classes given few samples and not forget the old classes. It is a challenging task since representation opti-mization and prototype reorganization can only be achieved under little supervision. To address this problem, we pro-pose a novel incremental prototype learning scheme. Our scheme consists of a random episode selection strategy that adapts the feature representation to various generated in-cremental episodes to enhance the corresponding extensi-bility, and a self-promoted prototype reﬁnement mechanism which strengthens the expression ability of the new classes by explicitly considering the dependencies among different classes. Particularly, a dynamic relation projection module is proposed to calculate the relation matrix in a shared em-bedding space and leverage it as the factor for bootstrap-ping the update of prototypes. Extensive experiments on three benchmark datasets demonstrate the above-par incre-mental performance, outperforming state-of-the-art meth-ods by a margin of 13%, 17% and 11%, respectively. 1.

Introduction
Currently, deep convolutional neural networks [23, 11, 35] have made signiﬁcant breakthroughs in a large number of recognition tasks. When the class is given in advance and the sample is sufﬁcient, we can get a good recognition model by typical supervised learning. In practice, however, we are likely to encounter new classes that were not seen before in continual data stream, and need to add them into the recognition tasks, which forms the problem of class-incremental learning (CIL) [20].
In this case, it is both time consuming and computation-ally expensive to retrain the model on all the old and new data. And in many cases the old data may not be available, due to data privacy or limited storage. A common solu-tion is to ﬁne-tune old models with new data, but it may
∗Co-ﬁrst Author
†Corresponding Author
Figure 1. The t-SNE [19] results in different methods at two stages.
The initial deep representations obtained by typical incremental learning method [20, 1] and our proposed method are visualized in the upper row (60 classes used, and 5 classes visualized in color). In each colored class, deep-color points are learnable pro-totypes, and light-color ones show the distribution of real data.
The lower row shows the reﬁned representations and prototypes of each class after the increment of the gray class. Compared with previous method, (1) the representations of the incremental classes are more clustered (regions circled in violet dotted lines), (2) and their corresponding prototypes are more discriminative, where the incremental prototype visualized in black is representative and no longer confused with that in red color. arise the problem of catastrophic forgetting. To this end, re-cent learning-based approaches present to maintain the rep-resentation space for the old classes by preserving mem-ories of old classes (e.g., examplar [20]) and introducing various distillation losses, and then reconstruct classiﬁers (e.g., fully connected layer [31], learnable prototypes [12]) in different ways to correct their preference for new classes.
However, existing methods assume that new class sam-ples are available in large quantities, while the incremental classes are usually atypical and the sample size is small in practical applications. For example, in industrial visual in-spection tasks, with the continuous progress of production, new classes of defects often appear due to equipment wear and other reasons. These defect samples may not only be 6801
essentially different from the old samples, but also small in number. It brings great difﬁculties to the recognition task, as representation optimization and prototype reorganization brought by new classes are hard to complete under little su-pervision. This paper focuses on this ability of incremen-tally learning new classes from few samples, which is called few-shot class-incremental learning (FSCIL [27]).
A natural idea for FSCIL task is to directly apply ex-isting incremental learning methods to solve the problem, but experimental results show that this way results in a dra-matic drop in performance. Our analysis suggests that this is mainly due to the following two reasons. First, the initial deep representation space used for CIL is relatively com-pact, which is conducive to classiﬁcation of existing classes, but it lacks extensibility for FSCIL. In FSCIL, due to the in-sufﬁciency of incremental class samples, there is no enough supervision at each stage to participate in the classiﬁcation and distillation process. Therefore, it cannot promote the expansion of the representation space as the existing incre-mental learning methods do. In addition, the small number of new class samples is not sufﬁcient to learn discrimina-tive classiﬁer for new classes while maintaining the perfor-mance on old classes. As shown in Fig. 1, the represen-tation space extended by typical incremental learning ap-proach [20, 4] is underrepresented, such that the new classes usually exhibit insufﬁcient aggregation compared to the old ones. Also, due to the insufﬁciency of new class samples, the prototypes used for classiﬁcation are prone to be con-fused with other classes after incremental learning, which greatly deteriorates subsequent tasks.
To address this problem, we propose an incremental prototype learning scheme to explicitly learn an extensi-ble feature representation, and thus facilitate subsequent incremental tasks. The scheme is mainly manifested in two aspects. First, we adopt the random episode selec-tion strategy (RESS) to enhance the extensibility of feature representation by forcing features adaptive to various ran-domly simulated incremental processes. Secondly, we in-troduce a self-promoted prototype reﬁnement mechanism (SPPR) to update the existing prototypes by utilizing the relation matrix between representations of the new class samples and the old class prototypes. This enhances the expressiveness of the new classes while retaining the rela-tional characteristics among the old classes. Particularly, a novel module called dynamic relation projection is pro-posed to map the representation of the new class samples and the prototype of the old classes into the same embed-ding space, and calculate a projection matrix between them by using the distance metric of the two embeddings in the space. We take the matrix as the weight of prototype re-ﬁnement to guide the dynamic change of the prototype to-ward maintaining the existing knowledge and enhancing the discriminability of the new class. To demonstrate the supe-riority of our method, we conducted comparative experi-ments with existing few-shot class-incremental and typical class-incremental methods on three datasets CIFAR-100,
MiniImageNet and CUB200. We achieved the best results against the state-of-the-art methods, leading by 13% , 17%, and 11%, respectively.
Our main contributions are as follows: 1. An incremental prototype learning scheme is pro-posed for few-shot class-incremental learning, in which a randomly episodic training is accomplished by a self-promoted prototype reﬁnement mechanism, resulting in an extensible feature representation. 2. A novel dynamic relation projection module is pro-posed, which uses the relational metric between old class prototypes and new class samples to constrain the update of prototypes during training and test. 3. Extensive experiments on benchmark CIFAR-100,
MiniImageNet and CUB200 datasets demonstrate the su-periority of our proposed method over the state-of-the-art. 2.