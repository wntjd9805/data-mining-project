Abstract
Modern human-object interaction (HOI) detection ap-proaches can be divided into one-stage methods and two-stage ones. One-stage models are more efﬁcient due to their straightforward architectures, but the two-stage models are still advantageous in accuracy. Existing one-stage model-s usually begin by detecting predeﬁned interaction areas or points, and then attend to these areas only for interac-tion prediction; therefore, they lack reasoning steps that dynamically search for discriminative cues. In this paper, we propose a novel one-stage method, namely Glance and
Gaze Network (GGNet), which adaptively models a set of action-aware points (ActPoints) via glance and gaze step-s. The glance step quickly determines whether each pix-el in the feature maps is an interaction point. The gaze step leverages feature maps produced by the glance step to adaptively infer ActPoints around each pixel in a pro-gressive manner. Features of the reﬁned ActPoints are ag-gregated for interaction prediction. Moreover, we design an action-aware approach that effectively matches each detect-ed interaction with its associated human-object pair, along with a novel hard negative attentive loss to improve the opti-mization of GGNet. All the above operations are conducted simultaneously and efﬁciently for all pixels in the feature maps. Finally, GGNet outperforms state-of-the-art meth-ods by signiﬁcant margins on both V-COCO and HICO-DET benchmarks. Code of GGNet is available at https:
//github.com/SherlockHolmes221/GGNet. 1.

Introduction
Human-Object Interaction (HOI) detection is one of the fundamental tasks in human-centric scene understanding. It involves not only detecting persons and objects in an im-age, but also the interactions (verbs) between each human-object pair. The output of an HOI detection model can be represented as a set of triplets in the form of <human
∗Corresponding author (a) InteractNet (b) UnionDet (c) PPDM (d) GGNet
Figure 1. Comparisons of interaction area deﬁnition. Green box-es or points represent the interaction area for “hold tennis racket”, while the red ones stand for “hit sports ball”. (a) InteractNet [3] uses the same human bounding box to represent the interaction area for all interactions pertaining to the person. (b) UnionDet [1] adopts the union box of one human-object pair to represent their interaction area. (c) PPDM [10] leverages the middle point of one human-object pair to represent their interaction area. (d) GGNet employs a single set of dynamic points to adaptively capture infor-mative areas for the interaction between each human-object pair. interaction object>, and each triplet is also referred to as a single HOI category. For example, there are two HOI cat-egories in Figure 1, i.e. <human hold tennis racket> and <human hit sports ball>.
According to the order in which object detection and interaction detection is performed, modern HOI detection methods can be divided into one-stage and two-stage ap-proaches. Two-stage methods must perform the objec-t detection ﬁrst and then identify the interactions between each possible human-object pair. However, because the t-wo stages are separated in this approach, these methods are usually inefﬁcient. By contrast, one-stage methods can per-13234
form object detection and interaction detection in parallel by ﬁrst deﬁning interaction areas (e.g. a union box or a s-ingle point). Generally speaking, one-stage methods tend to be more efﬁcient and structurally elegant, but two-stage methods are more accurate at present.
One of the key issues for one-stage methods is the way to represent the “interaction area” for each human-object pair
[1]. Existing approaches usually deﬁne this area artiﬁcially and the interaction area often face the semantic ambiguity problem. For example, as shown in Figure 1(a), Interact-Net [3] utilizes a human bounding box to represent the area of all interactions involving the person, meaning that object-speciﬁc information is ignored. UnionDet [1] addresses this problem by utilizing the union box for each human-object pair as their interaction region. However, union boxes may overlap signiﬁcantly with each other (Figure 1(b)), which introduces ambiguity between pairs. For its part, PPDM
[10] utilizes the middle point of each human-object pair as the interaction point (Figure 1(c)). Although interaction points are less likely to overlap with each other under this approach, a single interaction point is often vague to repre-sent complex interactions between a human-object pair.
With the predeﬁned interaction areas discussed above, existing one-stage methods usually attend to the interaction area only once to predict the interaction categories. Recent works [24], [26] have revealed that the eyes of human be-ings usually move around an object to discover more cues regarding its location. Similarly, when it comes to HOI detection, people often ﬁrst glance at the scene to identi-fy possible human-object pairs with any interaction; they then search for cues around each pair, and ﬁnally gaze at discriminative areas to identify the interaction class.
Accordingly, inspired by the above observation, we here-in propose a novel model, named Glance and Gaze Network (GGNet), which adaptively infers a set of action-aware points (ActPoints) to represent the interaction area (Figure 1(d)). GGNet mimics the two steps taken by humans to i-dentify human-object interactions: Glance and Gaze. First,
GGNet quickly determines whether each pixel in the fea-ture maps is an interaction point; we call it the glance step.
Based on the feature maps in the glance step, the subsequent gaze step searches for a set of ActPoints around each pixel.
This step then progressively proceeds to reﬁne the location of these ActPoints. In brief, this step comprises two sub-steps, in which the coarse location and location residuals of
ActPoints are inferred, respectively. Finally, GGNet aggre-gates features of the reﬁned ActPoints to predict interaction categories at the interaction points.
We further propose an action-aware point matching (AP-M) approach designed to match each interaction with its as-sociated human-object pair. This matching process speci-ﬁes the location of both the human and object instances for each interaction. Existing interaction point-based methods tend to employ a single location regressor shared by all in-teraction categories [10], [20]; however, we observe that the interaction category affects the spatial layout of one human-object pair. We accordingly propose to assign each interac-tion category a unique location regressor, which is proven in the experimentation section to be a more effective approach.
Finally, we propose a novel focal loss, namely Hard Neg-ative Attentive (HNA) loss, to further promote the perfor-mance of GGNet. As there are massive numbers of nega-tive samples for each interaction classiﬁer of the interaction point-based methods [10], [20], a serious imbalance prob-lem exists between the positive and negative samples for each interaction category. We thus develop an efﬁcient ap-proach to address this problem by inferring and highlighting hard negative samples. Hard negatives are inferred between meaningful HOI categories containing the same object. For example, we can infer a hard negative sample “repair bicy-cle” according to the labeled positive sample “carry bicy-cle”, unless “repair bicycle” is labeled as positive; in this way, the decision boundary between easily confused inter-action categories can be clariﬁed.
Both APM and HNA loss can be readily applied to other interaction point-based HOI detection methods. We con-duct extensive experiments on the two most popular HOI detection databases, i.e. V-COCO [31] and HICO-DET
[30]. Experimental results demonstrate that our proposed
GGNet consistently outperforms start-of-the-art methods. 2.