Abstract
Model 1
Outfit A
Results A-1
Outfit B
Results B-1
Virtual try-on methods aim to generate images of fash-ion models wearing arbitrary combinations of garments.
This is a challenging task because the generated image must appear realistic and accurately display the interaction be-tween garments. Prior works produce images that are ﬁlled with artifacts and fail to capture important visual details necessary for commercial applications. We propose Outﬁt
Visualization Net (OVNet) to capture these important de-tails (e.g. buttons, shading, textures, realistic hemlines, and interactions between garments) and produce high quality multiple-garment virtual try-on images. OVNet consists of 1) a semantic layout generator and 2) an image genera-tion pipeline using multiple coordinated warps. We train the warper to output multiple warps using a cascade loss, which reﬁnes each successive warp to focus on poorly gen-erated regions of a previous warp and yields consistent im-provements in detail. In addition, we introduce a method for matching outﬁts with the most suitable model and pro-duce signiﬁcant improvements for both our and other previ-ous try-on methods. Through quantitative and qualitative analysis, we demonstrate our method generates substan-tially higher-quality studio images compared to prior works for multi-garment outﬁts. An interactive interface powered by this method has been deployed on fashion e-commerce websites and received overwhelmingly positive feedback. 1.

Introduction
While e-commerce has brought convenience to many as-pects of our lives, shopping online is difﬁcult for fashion consumers who want to try-on garments and outﬁts before deciding to buy them [52].
In most online shopping ex-periences, we are only given a neutral product image of a garment or a single example of a model wearing the gar-ment, and users have to imagine how the garment would look in different settings (e.g. with different garments, on different models etc.). As a result, there has been a consid-erable amount of literature on synthesizing people wearing garments [18, 53, 46, 11, 16, 58, 41, 25, 23].
Model 2
Outfit C
Results C-2
Outfit D
Results D-2
Figure 1. Our method takes in a model image and multiple neutral garments images as inputs, and generates a high quality image of the selected model wearing the garments. Pay careful attention to details of the garment properties that are accurately portrayed (e.g., the patterns on the dress (A-1), the unicorn and the string (C-2), the hemline (C-2), buttons (B-1, D-2), and the lengths of the garments); the interaction between multiple garments has been captured (e.g., the collar and sleeve coming out of the sweater (A-1), the open outerwear cast shading (B-1, C-2) to the garment be-neath); the interaction between the garment and the person is nat-ural (e.g., the loose sleeves, the folds by the arm (D-2), and the shadows casted on the leg by the dresses); and skin is generated realistically (B-1). See image without bounding box in Appendix.
Three natural cases arise when shopping online. A user may want to see (a) any image of a model wearing a cho-sen set of garments (outﬁt) to visualize a combination; (b) any image of themselves wearing the outﬁt to see how the garments interact; and (c) an image of themselves wearing the outﬁt (the VITON case [18, 53, 46, 11, 16, 58, 25, 23]).
In all cases, users expect the image to capture the visual fea-tures of the garments and the physical interactions between 15546
them. However, current methods have problems capturing details of shading, texture, drape and folds. Getting these right is crucial for shoppers to make purchase decisions.
In this work, we introduce a variety of innovations that substantially improve upon the synthesis of details (Fig-ure 1). Our proposed method not only produces accurate textures, necklines, and hemlines, but also can drape multi-ple garments with realistic overlay and shading. The drape can adapt to the body pose and generate natural creases, folds, and shading. Skin and background are also generated, with appropriate shadows casted from the garments (Fig-ure 1). Our method signiﬁcantly outperforms prior work in multi-garment image synthesis as shown in Figure 9.
While other virtual try-on (VITON) methods [18, 53, 46, 11, 16, 58, 23] focused on single garment try-on, Neuberger et al. proposed O-VITON [41], which transfers multiple garments from model to model.
In comparison, our sys-tem takes garments from neutral garment photographs and transfers them to a model. This distinction is commercially important because it is easier and cheaper to obtain neutral pictures. The formatting is also consistent across different sites, meaning no extra work is required for the merchants.
Also, O-VITON [41] encodes garments into feature vec-tors and broadcasts the vectors onto a layout to produce the image. Such a formulation can handle complex garment shapes (a major difﬁculty for multi-garment try-on) but re-sults in a loss of spatial patterns (e.g., logos, prints, buttons), making it hard to synthesize texture details accurately. In contrast, other VITON literature [18, 53, 16, 58, 23] uses warping, which faithfully perseveres details. However, they only demonstrate success with warping single gar-ments of simple shapes (mostly). Warping multiple gar-ments with complicated shapes has not yet been achieved.
In this work, we directly address the challenge of warp-ing multiple garments, while also being able to accurately transfer textures between complicated garment shapes (Fig-ure 1). Our procedure uses multiple warps, which can han-dle (say) open jackets, and can generate buttons, zippers, logos, and collars correctly (Figure 2). The warpers are trained end-to-end with the generator and learn to coor-dinate through a cascading loss, which encourages subse-quent warps to address errors made by earlier warps. Us-ing multiple coordinated warps produces substantial quan-titative and qualitative improvements over prior single-warp methods [18, 53, 11, 16, 58, 25].
Finally, because publicly available try-on datasets do not contain rich garment categories, we test on a dataset with all available garment categories from multiple fashion e-commerce websites. Evaluation on this new dataset shows that using multiple warps consistently outperforms single warp baselines in this new setting, demonstrated both quan-titatively (Table 3) and qualitatively (Figure 8). Our try-on system also produces higher quality images compared
Outfit
Pose 1
Pose 2
Pose 3
Pose 4
Pose 5
Pose 6
Figure 2. We show a sequence of visualizations for the same outﬁt generated on different reference models. Our generation method is able to adapt to a diverse set of poses, skin-tones, and hand positions. When the hand is in the pocket, the jeans plump up and connect seamlessly with the jacket (Pose 2 & 5). to prior works on both single and multi-garment genera-tion (Table 1 and 2, and Figure 9). Furthermore, we intro-duce a procedure for matching garment-pose pairs, which yields signiﬁcant improvement for both our and previous image generation pipelines in scenarios (a) and (b) (Ta-ble 2). Lastly, we conduct a user study comparing our generated images with real commercial photos, simulating the effectiveness of e-commerce sites replacing real pho-tographs of models with our synthesized images. Results show over 50% of our synthesized images were thought to be real even with references to real images (Table 4). Fur-thermore, our method is fast enough to integrate with inter-active user-interfaces, where users can select garments and see generated visualizations in real-time. A live demo of an virtual try-on shopping interface powered by our method is publicly available 10.
As a summary of our contributions:
• We introduce OVNet - the ﬁrst multi-garment try-on framework that generates high quality images at laten-cies low enough to integrate with interactive software.
• We are the ﬁrst warping-based try-on method that sup-ports multi-garment synthesis on all garment types.
• We introduce a garment-pose matching procedure that signiﬁcantly enhances our method and prior methods.
• Our results strongly outperform prior works, both quantitatively and qualitatively.
• We evaluate on a dataset with all available garment cat-egories from multiple fashion e-commerce sites, and show that our method works with all categories. 2.