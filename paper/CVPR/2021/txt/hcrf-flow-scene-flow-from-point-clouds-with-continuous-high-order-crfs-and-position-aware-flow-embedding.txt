Abstract
Scene ﬂow in 3D point clouds plays an important role in understanding dynamic environments. Although signiﬁ-cant advances have been made by deep neural networks, the performance is far from satisfactory as only per-point trans-lational motion is considered, neglecting the constraints of the rigid motion in local regions. To address the issue, we propose to introduce the motion consistency to force the smoothness among neighboring points.
In addition, con-straints on the rigidity of the local transformation are also added by sharing unique rigid motion parameters for all points within each local region. To this end, a high-order
CRFs based relation module (Con-HCRFs) is deployed to explore both point-wise smoothness and region-wise rigid-ity. To empower the CRFs to have a discriminative unary term, we also introduce a position-aware ﬂow estimation module to be incorporated into the Con-HCRFs. Compre-hensive experiments on FlyingThings3D and KITTI show that our proposed framework (HCRF-Flow) achieves state-of-the-art performance and signiﬁcantly outperforms previ-ous approaches substantially. 1.

Introduction
Scene ﬂow estimation [37] aims to provide dense or semi-dense 3D vectors, representing the per-point 3D mo-tion in two consecutive frames. The information provided has proven invaluable in analyzing dynamic scenes. Al-though signiﬁcant advances have been made in the 2D opti-cal ﬂow, the counterpart in 3D point cloud is far more chal-lenging. This is partly due to the irregularity and sparsity of the data, but also due to the diversity of the scene.
As pointed out in [17], most of the structures in the vi-sual world are rigid or at least nearly so. Many previous top-performing approaches [13, 5, 45, 26] simplify this task as a
∗Corresponding author: G. Lin. (e-mail: gslin@ntu.edu.sg )
Figure 1. The warped point cloud at the next frame based on differ-ent scene ﬂow. Green points represent the point cloud at frame t.
Red points are the warped results at frame t + 1 by adding scene
ﬂow back to corresponding green points. (a) scene ﬂow pro-duced by FlowNet3D [13]; (b) scene ﬂow produced by FlowNet3D and reﬁned by a conventional CRF; (c) scene ﬂow produced by
FlowNet3D and reﬁned by our continuous high order CRFs; (d) ground truth scene ﬂow. The local structure of the warped point cloud is distorted in Flownet3d and the conventional CRF but pre-served in our method. regression problem by estimating a point-wise translational motion. Although promising results have been achieved, the performance is far from satisfactory as the potential rigid motion constraints existing in the local region are ig-nored. As shown in Fig. 1(a), the results generated by the
FlowNet3D [13] are deformed and fail to maintain the lo-cal geometric smoothness. A straightforward remedy is to utilize pair-wise regularization to smooth the ﬂow predic-tion. However, ignoring the potential rigid transformations 364
makes it hard to maintain the underlying spatial structure, as presented in Fig. 1(b).
To address this issue, we propose a novel framework termed HCRF-Flow, which consists of two components: a position-aware ﬂow estimation module (PAFE) for per-point translational motion regression and a continuous high-order CRFs module (Con-HCRFs) for the reﬁnement of the per-point predictions by considering both spatial smooth-ness and rigid transformation constraints. Speciﬁcally, in
Con-HCRFs, a pairwise term is designed to encourage neighboring points with similar local structure to have simi-lar motions. In addition, a novel high order term is designed to encourage each point in a local region to take a motion obeying the shared rigid motion parameters, i.e., translation and rotation parameters, in this region.
In point cloud scene ﬂow estimation, it is challenging to aggregate the matching costs, which are calculated by comparing one point with its softly corresponding points.
To encode this knowledge into the embedding features, we propose a position-aware ﬂow embedding layer in the PAFE module.
In the aggregation step, we introduce a pseudo matching pair that is applied to calculate the difference of the matching cost. For each softly corresponding pair, both its position information and the matching cost difference will be considered to output weights for aggregation.
Our main contributions can be summarized as follows:
• We propose a novel scene ﬂow estimation framework
HCRF-Flow by combining the strengths of DNNs and CRFs to perform a per-point translational motion regression and a reﬁnement with both pairwise and region-level regularization;
• Formulating the rigid motion constraints as a high order term, we propose continuous high-order CRFs (Con-HCRFs) to model the interaction of points by imposing point-level and region-level consistency con-straints.
• We present a novel position-aware ﬂow estimation layer to build reliable matching costs and aggregate them based on both position information and match-ing cost differences.
• Our proposed HCRF-Flow signiﬁcantly outperforms the state-of-the-art on both FlyingThing3D and KITTI
Scene Flow 2015 datasets. In particular, we achieve
Acc3DR scores of 95.07% and 94.44% on FlyingTh-ing3D and KITTI, respectively. 1.1.