Abstract
The perceptual loss has been widely used as an effective loss term in image synthesis tasks including image super-resolution [16], and style transfer [14]. It was believed that the success lies in the high-level perceptual feature repre-sentations extracted from CNNs pretrained with a large set of images. Here we reveal that, what matters is the net-work structure instead of the trained weights. Without any learning, the structure of a deep network is sufﬁcient to capture the dependencies between multiple levels of vari-able statistics using multiple layers of CNNs. This insight removes the requirements of pre-training and a particular network structure (commonly, VGG) that are previously as-sumed for the perceptual loss, thus enabling a signiﬁcantly wider range of applications. To this end, we demonstrate that a randomly-weighted deep CNN can be used to model the structured dependencies of outputs. On a few dense per-pixel prediction tasks such as semantic segmentation, depth estimation and instance segmentation, we show improved results of using the extended randomized perceptual loss, compared to the baselines using pixel-wise loss alone. We hope that this simple, extended perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks. 1.

Introduction
Dense pixel-wise prediction tasks represent the most important category of computer vision problems, ranging from low-level image processing such as denoising, super-resolution, through mid-level tasks such as stereo match-ing, to high-level understanding such as semantic/instance segmentation. These tasks are naturally structured output learning problems since the prediction variables often de-pend on each other. The pixel-wise loss serves as the unary term for these tasks. Besides, the perceptual loss [14] was introduced to capture perceptual information by measuring discrepancy in high-level convolutional features extracted from CNNs. It has been successfully used in various low-*C. Shen is the corresponding author. level image processing tasks, such as style transfer, and super-resolution [14].
Previous works assume that the perceptual loss bene-ﬁts from the high-level perceptual features extracted from
CNNs pretrained with a large set of images (e.g., VGG
[23] pretrained on the ImageNet dataset. Relying on this assumption, the perceptual loss is limited to a speciﬁc net-work structure (commonly, VGG) with pre-trained weights, which is not able to take arbitrary signals as the input. In this work, we reveal that, contrary to this belief, the success of the perceptual loss is not necessarily dependent on the ability of a pretrained CNN in extracting high-level percep-tual features. Instead, without any learning, the structure of a multi-layered CNN is sufﬁcient to capture a large amount of interaction statistics for various output forms. We ar-gue that what matters is the deep network architecture rather than the pretrained weights.
To verify the statement, we conduct a pilot experiment on image super-resolution. Apart from using the pretrained
VGG net for perceptual loss, we use a randomly-weighted network. The results with the randomly-weighted network are on par with that of the pretrained VGG, which are both visually improved than using the per-pixel loss alone (see Figure 1). This indicates that the pretrained weights— previously assumed for the perceptual loss—is not essential to the success of the perceptual loss. We may conclude from this experiment that it is the deep network structure, rather than learnt weights, plays the core role.
Given a target y or a prediction ˆy as an input, a randomly-weighted network f (·) can work as a function to explore hi-erarchical dependencies between variable statistics through the convolution operations in multiple layers. Thus, a generic perceptual loss for structured output learning can be computed by comparing the discrepancy between f j(y) and f j(ˆy). Here j indexes a particular layer of the network f (·). Thus, this enables the perceptual loss1 to be applied to a wider range of structured output learning tasks.
Structured information is important in dense per-pixel prediction problems, such as semantic segmentation [17], 1Here we still use the notion of ‘perceptual’ as it was ﬁrstly intro-duced in [14] even though broadly this loss is more about capturing inter-dependencies in variables, instead of extracting perceptual features. 5424
(a) Ground Truth (b) Bicubic
Pixel-wise (c) alone [16]
Loss (d) w. Pretrained VGG [16] (e) w. Random VGG
Figure 1 – Super-resolution results of the pilot experiments (super-resoled from 4× down-scaled images). (a) Ground truth high-resolution images. (b) Bi-cubic up-sampling. (c) SRResNet [16] trained with the per-pixel loss. (d) SRResNe trained with the per-pixel loss and perceptual loss with a pretrained VGGNet. (e) SRResNet trained with the per-pixel loss and perceptual loss with a randomly-weighted VGGNet. We can see that the perceptual loss improves image quality. Besides, formulating the perceptual loss with a pre-trained network and a randomly-weighted network produces on par results. depth estimation and instance segmentation [18]. For ex-ample, the pairwise term in Markov Random Filed is com-plementary to the unary term, which deﬁnes pairwise com-patibility and in general improves prediction accuracy espe-cially when the unary term alone is not sufﬁcient. The pro-posed generic perceptual loss can be easily applied to these dense prediction tasks, with no computation overhead dur-ing inference. Also, as now pre-training with labelled data is not required, it is straightforward to explore the effective-ness of using various network structures—not necessary the
VGG—to model the dependency between output variables.
Experimental results on various structured output learn-ing tasks with different network structures show that the generic perceptual loss beneﬁts the training, and consis-tently achieves improved performance compared to the baselines using pixel-wise loss term alone. We also pro-vide detailed comparisons and analysis on the impact of ini-tialization schemes and architectures of the perceptual loss network.
In summary, our main contributions are as follows.
• We reveal that the success of the perceptual loss is not dependent on the pretrained CNN weights. With-out any learning, the structure of a deep network is sufﬁcient to capture the dependencies between multi-ple levels of variable statistics using multiple layers of
CNNs.
• We apply the generic perceptual loss to a few struc-tured output learning tasks, including semantic seg-mentation, depth estimation and instance segmenta-tion. We consistently improve the performance of baseline models.
• We investigate how the initialization and the network structures may affect the performance of the proposed perceptual loss. A reliable initialization approach is designed based on the analysis.
• This proposed simple perceptual loss may serve as a generic structured-output loss that is applicable to most structured output learning tasks in computer vi-sion. 2.