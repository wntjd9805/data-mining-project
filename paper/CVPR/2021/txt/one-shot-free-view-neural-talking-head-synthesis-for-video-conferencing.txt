Abstract
We propose a neural talking-head video synthesis model and demonstrate its application to video conferencing. Our model learns to synthesize a talking-head video using a source image containing the target person’s appearance and a driving video that dictates the motion in the output. Our motion is encoded based on a novel keypoint representa-tion, where the identity-speciﬁc and motion-related informa-tion is decomposed unsupervisedly. Extensive experimental validation shows that our model outperforms competing methods on benchmark datasets. Moreover, our compact keypoint representation enables a video conferencing sys-tem that achieves the same visual quality as the commercial
H.264 standard while only using one-tenth of the bandwidth.
Besides, we show our keypoint representation allows the user to rotate the head during synthesis, which is useful for simulating face-to-face video conferencing experiences. 1.

Introduction
We study the task of generating a realistic talking-head video of a person using one source image of that person and a driving video, possibly derived from another person. The source image encodes the target person’s appearance, and the driving video dictates motions in the output video.
We propose a pure neural rendering approach, where we render a talking-head video using a deep network in the one-shot setting without using a graphics model of the 3D human head. Compared to 3D graphics-based models, 2D-based methods enjoy several advantages. First, it avoids 3D model acquisition, which is often laborious and expensive.
Second, 2D-based methods can better handle the synthesis of hair, beard, etc., while acquiring detailed 3D geometries of these regions is challenging. Finally, they can directly synthesize accessories present in the source image, including eyeglasses, hats, and scarves, without their 3D models.
However, existing 2D-based one-shot talking-head meth-ods [62, 75, 86] come with their own set of limitations. Due to the absence of 3D graphics models, they can only syn-thesize the talking-head from the original viewpoint. They cannot render the talking-head from a novel view.
Our approach addresses the ﬁxed viewpoint limitation and achieves local free-view synthesis. One can freely change the viewpoint of the talking-head within a large neighbor-hood of the original viewpoint, as shown in Fig. 1(c). Our model achieves this capability by representing a video using a novel 3D keypoint representation, where person-speciﬁc and motion-related information is decomposed. Both the key-points and their decomposition are learned unsupervisedly.
Using the decomposition, we can apply 3D transformations to the person-speciﬁc representation to simulate head pose 10039
changes such as rotating the talking-head in the output video.
Figure 2 gives an overview of our approach.
We conduct extensive experimental validation with com-parisons to state-of-the-art methods. We evaluate our method on several talking-head synthesis tasks, including video re-construction, motion transfer, and face redirection. We also show how our approach can be used to reduce the bandwidth of video conferencing, which has become an important plat-form for social networking and remote collaborations. By sending only the keypoint representation and reconstructing the source video on the receiver side, we can achieve a 10x bandwidth reduction as compared to the commercial H.264 standard without compromising the visual quality.
Contribution 1. A novel one-shot neural talking-head syn-thesis approach, which achieves better visual quality than state-of-the-art methods on the benchmark datasets.
Contribution 2. Local free-view control of the output video, without the need for a 3D graphics model. Our model allows changing the viewpoint of the talking-head during synthesis.
Contribution 3. Reduction in bandwidth for video stream-ing. We compare our approach to the commercial H.264 standard on a benchmark talking-head dataset and show that our approach can achieve 10ˆ bandwidth reduction. 2.