Abstract 1.

Introduction
Recent neural view synthesis methods have achieved im-pressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF [34], are designed to learn a sin-gle scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we intro-duce Stereo Radiance Fields (SRF), a neural view synthe-sis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by ﬁnding similar image regions in stereo images.
In SRF, we pre-dict color and density for each 3D point given an encod-ing of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities – emulating classical stereo. Experiments show that SRF learns structure instead of over-ﬁtting on a scene.
We train on multiple scenes of the DTU dataset and gen-eralize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of ﬁne-tuning further improve the results, achiev-ing signiﬁcantly sharper, more detailed results than scene-speciﬁc models. The code, model, and videos are available – https://virtualhumans.mpi-inf.mpg.de/ srf/.
We introduce a neural multi-view view synthesis ap-proach which is trained end-to-end, generalizes to novel scenes, and requires only sparse views at test time (Fig. 1-(b)). This is in stark contrast to State-of-the-Art (SOTA) view synthesis methods like NeRF [34], which are trained for a speciﬁc scene and require dense multi-views to pro-duce sharp results.
On one end of the view synthesis spectrum of meth-ods, we have pure data-driven methods such as NeRF [34], which have shown impressive results. NeRF takes a radical data-driven approach by learning a mapping from a loca-tion and direction to the emitted radiance. This mapping is speciﬁcally trained for a scene (Fig. 2-(a)). Generaliza-tion to a new scene requires retraining for 2 days and re-sults are blurry when trained on sparse and spread-out views (Fig. 1-(a)). On the other end of the spectrum, popular classical image-based rendering techniques [46] use geom-etry [8, 29, 44, 45]. These approaches warp pixels to the de-sired target view via correspondences [39, 41, 49] or multi-view 3D reconstruction [42, 43]. Consequently, these meth-ods rely on high-quality 3D reconstruction or dense per-pixel correspondence, which requires dense multi-views.
Recent work [5, 38] combines classical methods with data-driven approaches by learning to correct the warped views of classical methods. The sequential pipeline in these meth-ods [5, 38] do not allow end-to-end learning. 7911
Dense Input
Scene 
Representation
Novel View
Sparse Input
Image Pairs
SRF
Novel View (a)
Inference, scene memorization loop (time intensive) (b)
Inference, single forward pass
Figure 2. Pure data-driven view synthesis and SRF (ours). Existing methods achieve remarkable realism representing scenes with a neural network. A model is trained speciﬁcally for a scene to synthesize high-quality novel views. However, this requires dense views and 2 days of training per scene. In this work, we address the more challenging task of novel view synthesis from sparse and spread-out views, using a single forward pass through the network, to instantly obtain the result.
We take inspiration from both classical and pure data-driven methods. Like NeRF, we also learn a neural net-work to predict radiance (speciﬁcally color and density).
However, instead of memorizing the scene radiance at 3D locations, we use an image-based feature encoding, which allows the network to reason about scene geometry (Fig. 2-(b)). In classical stereo reconstruction [41, 49], correspon-dences across views are found by computing a similarity score. We devise an architecture, called Stereo Radiance
Fields (SRF), which mimics the classical approach with-out computing explicit correspondences, but can be trained end-to-end. A 3D point is projected to each available view to extract point-wise view features. Then view features are processed in pairs by a bank of ﬁlters, which emulate corre-spondence ﬁnding in classical methods (Fig. 3). The result-ing matrix of pair-wise scores is further processed with a
Convolutional Neural Network [21] (CNN), which agglom-erates information from the available views to predict the desired radiance at that point. that
Our experiments incorporating demonstrate multi-view reconstruction ideas within the architecture signiﬁcantly boosts generalization ability. When training on a single scene and testing on a new scene, SRF can produce reasonable results. This indicates that the network does not memorize the scene, but learns to reason about structure. When trained on multiple scenes (100 or more),
SRF can generalize to novel scenes, even when only 10 sparse and spread-out views are available as input. Further improvements can be obtained by ﬁne-tuning on the 10 views (Fig. 1-(c)), which typically takes about 15 minutes, which is much less than the 2 − 3 days required by methods that re-train from scratch [34, 47]. SRF results are sharper, validating that multi-view reconstruction structure not only helps to generalize but also constrains the learning problem. We encourage the reader to view our results as videos available on our project page. To summarize, our contributions are:
• We introduce Stereo Radiance Fields (SRF), an end-to-end, self-supervised architecture for multi-view view synthesis. We bring together insights from classical multi-view reconstruction pipelines and neural render-ing approaches.
• Experiments demonstrate that SRF generalize to novel scenes given sparse and spread-out views as input.
Further, ﬁne-tuning a pre-trained SRF for a few min-utes on test distribution improve results.
• We show how to combine recent paradigms into one model, often treated in isolation in novel view synthe-sis: SRF builds on classical multi-view 3D reconstruc-tion and learning from multiple scenes.
• In the sparse and spread-out view setting, SRF pro-duces much sharper results than SOTA baselines like
NeRF [34]. We achieve even better results when we
ﬁne-tune for only 15 minutes in contrast to NeRF trained on the 10 test views for 2 days. 2. Multi-View View Synthesis
Given N camera views, our goal is to synthesize a view for a new virtual camera. This is a long-standing prob-lem [17, 50]. Historically [46], the problem has been stud-ied under three possible directions depending on the ge-ometric information used: (1) rendering without geome-try [2, 16, 22, 30, 35] by modelling a plenoptic function to compute intensity of light rays for a given camera at every possible angle; (2) rendering using correspondences [8, 44] which requires knowledge of positional correspondences across multi-views; and (3) rendering with explicit geom-etry [29, 45] which requires explicit 3D information in the form of depth or point clouds. In this work, we bring to-gether insights from neural rendering with classical recon-struction pipelines. We encourage our network to reason about correspondences across pairs of views by computing 7912
an ensemble of pair-wise scores within the network. Al-though we never explicitly compute correspondences, this geometric reasoning, allow us to generalize to new scenes.
Correspondences across multi-views:
Classic ap-proaches [8, 13, 17, 44, 51] in multi-view stereo rely on correspondences across views. In this work, we bring to-gether the insights from classical multi-view stereo [17, 46] and contemporary learning-based approaches [10, 34, 40].
We use an encoder network that inputs 10 multi-views and extracts multi-scale features [4, 40]. We replace classi-cal block or feature matching with a multi-layer perceptron (MLP) which outputs an ensemble of similarity scores. Like us, recent work can do view synthesis from sparse views [5] incorporating explicit correspondences. However, explic-itly computing correspondences is hard due to differences in illumination, zoom, scale, and occlusion. A scene-speciﬁc model is trained to correct artifacts.
In our method, the network reasons about correspondences driven by the view synthesis loss, but they are never explicitly computed. Im-portantly, our model is not speciﬁc to a scene.
Neural Rendering and Plenoptic Modeling:
State-of-the-art neural rendering [52] approaches have enabled cre-ation of photo-realistic visual content using deep neural net-works [21]. There are three popular directions for multi-view view synthesis: (1) using plane-sweep stereo [13, 15] or multi-plane image (MPI) representation [61]. MPI-based approaches [7, 14, 15, 20, 33, 48] have shown remark-able results on continuous view synthesis for small base-line shifts, but fail for large ones as it assumes accurate multi-plane imaging; (2) explicitly incorporating 3D recon-struction using SfM [42, 43] or multi-view stereo [19] for view synthesis [3, 12, 18, 32, 38]. These approaches as-sume a reasonably dense 3D point cloud used in conjunc-tion with a neural network for view synthesis. The role of the neural network is to correct the imperfections in the 3D reconstruction. However, these approaches strug-gle when the views are sparse with small overlap be-cause explicit 3D reconstruction fails; and (3) recent ap-proaches [23, 24, 27, 34, 37, 47, 57, 58] learn a 3D represen-tation that can be combined with differentiable-ray march-ing operations to synthesize a new view. These approaches by design require scene-speciﬁc modeling. This restricts: (1) an instant and online visualization of a new capture be-cause it requires 2 − 3 days to train a model; and (2) uti-lizing large amounts of diverse visual data, which has been the driving force for progress in other areas of vision such as recognition, semantic segmentation, and detection.
Our work is deeply inspired by recent neural rendering approaches. Like NeRF [34], we predict radiance at con-tinuous locations and use volume rendering to generate the target image. Instead of predicting based on point coordi-nates and radiance, we predict based on point image fea-tures and an ensemble of similarity functions that emulate
≈
≠
Figure 3. Intuition of our Method: We structure our model in-spired by a geometric observation: 3D points in a scene that are on a surface will project to similar-looking regions when viewed from different perspectives (blue). We call this a photo-consistent point. A point in free space, however, will not be photo consistent (red). This holds for opaque, non-occluded surface points. classical stereo matching. Hence, our work brings together contemporary neural rendering with classical computer vi-sion within an end-to-end architecture. SRF is similar in spirit to previous work on 3D implicit shape reconstruc-tion, Implicit Feature Networks (IF-Nets) [6, 10] and Neural
Distance Fields (NDF) [11], where we decode occupancy or unsigned distances based on volumetric deep features computed from the input, instead of the originally proposed point coordinates [9, 31, 36]. Our work also shares insights with contemporary approaches [55, 56, 59]. Finally, our work is inspired by lifelong learning [53, 54] that aims to learn a generic representation that can be easily adapted to a new task with a few examples. We learn a generic view synthesis network that readily generalizes to new scenes.
Our results further improve when we adapt it to the new scene with simple ﬁne-tuning on test examples. 3. Method
In this section, we present our method Stereo Radiance
Fields (SRF) for novel view synthesis given sparse and spread-out input views of objects unseen during training.
We ﬁrst give a background in Section 3.1 and then build
SRF on these insights in Section 3.2. 3.1.