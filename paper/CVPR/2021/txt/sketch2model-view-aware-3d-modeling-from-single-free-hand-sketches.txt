Abstract
We investigate the problem of generating 3D meshes from single free-hand sketches, aiming at fast 3D modeling for novice users. It can be regarded as a single-view re-construction problem, but with unique challenges, brought by the variation and conciseness of sketches. Ambiguities in poorly-drawn sketches could make it hard to determine how the sketched object is posed. In this paper, we address the importance of viewpoint speciﬁcation for overcoming such ambiguities, and propose a novel view-aware generation approach. By explicitly conditioning the generation pro-cess on a given viewpoint, our method can generate plausi-ble shapes automatically with predicted viewpoints, or with speciﬁed viewpoints to help users better express their in-tentions. Extensive evaluations on various datasets demon-strate the effectiveness of our view-aware design in solving sketch ambiguities and improving reconstruction quality. 1.

Introduction
Sketch-based 3D modeling has been studied for decades, intended to free people from the tedious and time-consuming modeling process. With the widespread usage of portable touch screens and emergence of VR/AR technolo-gies, the need of 3D content creation for novice users is in-creasing [37]. However, existing sketch-based 3D modeling techniques either require precise line-drawings from multi-ple views or simply retrieve from existing models, both fail-ing to provide easy-to-use interface and customizability at the same time. We aim at fast and intuitive 3D modeling for people without professional drawing skills, and inves-tigate the problem of mesh generation from a single free-hand sketch. It is in general a single-view reconstruction problem, but has its unique challenges due to the character-istics of free-hand sketches. Unlike real images, free-hand sketches can be poorly-drawn with drastic simpliﬁcations and geometric distortions, and lack important visual cues like textures or shading. This makes it difﬁcult, sometimes
*corresponding author. airplane front or top? car left or right? table front or top?
Figure 1. Ambiguities in free-hand sketches. It could be hard to determine how the sketched object is posed. even impossible, for algorithms to understand sketches cor-rectly. One tricky scenario results from the “camera-shape ambiguity” [21], as shown in Fig. 1. Subjectivity of sketch creation could lead to different explanation of the same sketch, like which side being the front of the car (left). Fur-thermore, a recent study shows that existing single-view re-construction approaches mainly rely on the recognition of input images, rather than actually performing geometry re-construction. Therefore, they can be hard to generalize to unseen data and perform worse on hand-drawn sketches, which are created with barely any constraint, showing more diverse in appearance.
Recall how human cognitive system works for the task of 3D reconstruction. Human relies on visual memory and visual rules to recover 3D geometry from imagery [28]. Vi-sual memory contains priors gained from daily life, like what an object of a certain class might look like, while visual rules act like a regularizer, requiring that an object should match what it looks like from a certain viewpoint.
When seeing a sketch, we search our visual memory and try to ﬁnd a solution that also obeys visual rules. However, this search process may not go well when (1) we could not
ﬁgure out how the sketched object is posed, thus ﬁnding dif-ﬁculty applying visual rules; (2) multiple solutions exist due to the ambiguity of the sketch. Both problems result from the lack of details in sketches, and we address that view-point speciﬁcation can be essential to the above problems.
If we are told which viewpoint the object is sketched from, search space can be easily reduced, which lowers search difﬁculty and may eliminate ambiguities. 6012
Based on the above observations, we think that explic-itly speciﬁed viewpoints can directly guide the reconstruc-tion algorithm to ﬁnd the correct shape described by the user. Moreover, viewpoints offer extra information for the understanding of hand-drawn sketches, which can improve generalization ability for out-of-distribution data. To utilize view information, we propose a view-aware sketch-based 3D reconstruction method, which explicitly conditions the generation process on a given viewpoint. We extend the tra-ditional encoder-decoder architecture used in some recent works [20, 22]. In the encoder, image features are decom-posed to a latent view code and a latent shape code. The latent view code is used to predict viewpoint for the input sketch, and the latent shape code is combined with a latent view code to generate the output shape. Just like other dis-entangling problems, the network would take a short path and fail to decompose certain properties if no other con-straints are made. To ensure that only view information is contained in the latent view space, a view auto-encoder is learned to transform viewpoints to latent view space and vice versa. To force the decoder network to depend on the given view code, we propose a simple but effective ran-dom view reinforced training strategy, where random views along with ground truth views are fed to the decoder, forc-ing the network to learn how to match the input sketch from different viewpoints. We use synthetic sketch data for train-ing, due to the lack of paired sketch-3D dataset, and ap-ply domain adaptation technique to bridge the gap between synthetic sketches and real sketches. In the inference stage, our method can generate 3D mesh based on a single free-hand sketch automatically using the predicted viewpoint, or semi-automatically using a user-speciﬁed viewpoint.
Extensive experiments are conducted to demonstrate the effectiveness of our design. We ﬁrst perform case stud-ies to show characteristics of our view-aware architecture, and how speciﬁed views can be used to eliminate ambi-guities. To evaluate how our method performs on free-hand sketches and inspire further research, we collect a
ShapeNet-Sketch dataset based on the ShapeNet dataset, which contains 1,300 sketch-shape pairs. Results on various datasets show that our method can generate higher-quality shapes and better convey user’s intentions comparing to al-ternative baselines, which demonstrates the advantages of introducing viewpoint speciﬁcation. Ablation studies are performed to show the necessity of serveral parts of our method.
To summarize, our contributions are as follows:
• We are the ﬁrst to investigate the problem of gener-ating 3D mesh from a single free-hand sketch, which provides a fast and easy-to-use 3D content creation so-lution for novice users.
• We address the importance of viewpoint speciﬁca-tion in the sketch-based reconstruction task, and de-sign a view-aware generation architecture to condi-tion the generation process explicitly on viewpoints.
Quantitative and qualitative evaluations on various datasets demonstrate our method can generate promis-ing shapes that well convey user’s intentions and gen-eralize better on free-hand sketches. 2.