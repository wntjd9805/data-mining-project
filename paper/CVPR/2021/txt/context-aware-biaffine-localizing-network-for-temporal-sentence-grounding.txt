Abstract
This paper addresses the problem of temporal sen-tence grounding (TSG), which aims to identify the tem-poral boundary of a speciﬁc segment from an untrimmed video by a sentence query. Previous works either compare pre-deﬁned candidate segments with the query and select the best one by ranking, or directly regress the boundary timestamps of the target segment.
In this paper, we pro-pose a novel localization framework that scores all pairs of start and end indices within the video simultaneously with a biafﬁne mechanism. In particular, we present a Context-aware Biafﬁne Localizing Network (CBLN) which incorpo-rates both local and global contexts into features of each start/end position for biafﬁne-based localization. The lo-cal contexts from the adjacent frames help distinguish the visually similar appearance, and the global contexts from the entire video contribute to reasoning the temporal rela-tion. Besides, we also develop a multi-modal self-attention module to provide ﬁne-grained query-guided video repre-sentation for this biafﬁne strategy. Extensive experiments show that our CBLN signiﬁcantly outperforms state-of-the-arts on three public datasets (ActivityNet Captions, TACoS, and Charades-STA), demonstrating the effectiveness of the proposed localization framework. The code is available at https://github.com/liudaizong/CBLN. 1.

Introduction
Video understanding is a fundamental task in computer vision and has drawn increasing attention over the last years due to its various applications in video event detec-tion [8], video summarization [37, 9, 24], video captioning
[18, 6, 20] and temporal action localization [34, 52], etc.
Recently, temporal sentence grounding (TSG) [14, 1, 30] has been proposed as an important yet challenging task.
This task requires automatically determining the start and
∗Equal contributions.
†Corresponding author.
Query: The woman pulls on a rowing machine for a second time.
Global Context
Local Context
Start: 3.7s
Query-guided video representation
Context-aware biaffine localization
End: 6.6s
Local start start start
Global
Biaffine Multiplication
Scoring
Candidate Segment
Local end end end
Figure 1. An illustrative example of temporal sentence ground-ing task. From a new perspective, we propose a biafﬁne-based localization method that scores all pairs of start and end frames simultaneously with both local and global contexts. end timestamps of a target segment in an untrimmed video that contains an activity semantically corresponding to a given sentence description, as shown in Figure 1. It is sub-stantially more challenging as it needs to not only model the complex multi-modal interactions among vision and lan-guage features, but also capture complicated context infor-mation for their semantics alignment.
Most previous methods [1, 14, 15, 25, 4, 51, 26, 46] tackle TSG task following a multi-modal matching architec-ture, which generates multiple candidate proposals of dif-ferent time intervals and ranks them according to their sim-ilarities with the sentence query. These methods severely rely on the quality of proposals, and break the intrinsic tem-poral structure and global context of videos. Recently, sev-eral works [32, 47, 5, 42, 16, 28, 48] directly regress the temporal locations of the target segment. Speciﬁcally, they either regress the start/end timestamps based on the entire video representation [47, 28], or predict at each frame to determine whether this frame is a start or end boundary
[32, 48, 5]. However, in these methods, start and end fea-tures are never jointly considered. Given a video with two target segments that have the same starting action (open the 11235
door) but end with different actions (go in, go out), pre-dicting the start point independently may lead to timestamp confusion. Moreover, the ending point also tends to be inac-curate if it is predicted conditioned on the wrong start time.
Different from the aforementioned frameworks, we ad-dress the TSG from a new perspective: we reformulate this task by scoring all pairs of start and end indices simultane-ously with a biafﬁne mechanism which interacts character-istics of each pair of possible start and end frames. This biafﬁne-based architecture is inspired by the dependency parsing task [12] in natural language processing, in which the system predicts a dependency head for each child to-ken and assigns a relation to the head-child pairs. However, there are two main obstacles when the biafﬁne mechanism used in dependency parsing is applied to TSG task. First, different from adjacent words in a sentence which carry dif-ferent meaning, video is continuous and the adjacent frames naturally contain visually similar appearance. As shown in Figure 1, the adjacent frames near the segment bound-ary possess the similar semantics on “woman” and “pulls”.
Thus, it is difﬁcult to distinguish the speciﬁc boundary from adjacent frames without referring to these adjacent features as local context. Second, causalities between word pairs are usually indirect and can be far apart, but in videos, events in different intervals are directly correlated and rely on the whole contents to reason the precise semantics. For exam-ple, in Figure 1, without perceiving “a second time” from a global perspective, the ﬁrst and second time of “pull on a rowing machine” possess similar semantic boundaries but totally different temporal indices. Therefore, the causal re-lations between the video events which act as global context are essential for understanding the segment.
Based on the above considerations, we propose a novel
Context-aware Biafﬁne Localizing Network (CBLN), for temporal sentence grounding. Speciﬁcally, we develop a multi-context biafﬁne localization (MCBL) module which aggregates both local and global contexts to enrich the in-formation of each frame representation. For each frame, we span the entire video features with different window sizes to get multi-scale video events as global contexts, and extract different numbers of adjacent frame features as multi-scale local contexts. The multi-scale local and global contexts are then inter-modulated to produce more adaptive contexts, which serve as the input representation for further biafﬁne localization by concatenating with frame-wise feature. At last, we obtain the output scores from the biafﬁne model to identify the similarities of all possible start-end pairs ac-cording to the semantics of sentence query. Besides, to provide ﬁne-grained query-guided video representation for above biafﬁne localization, we also develop a multi-modal self attention (MMSA) module to sufﬁciently capture de-pendencies among video frames under the guidance of the sentence description. By jointly learning the overall model, our CBLN is able to localize query in video effectively.
Our main contributions are summarized as follows:
• From a new perspective, we adopt biafﬁne mechanism to the TSG task. The biafﬁne-based architecture si-multaneously scores all possible pairs of start and end frames for segment localization. Compared to previ-ous methods, it gets rid of complicated proposal design and interacts both start-end timestamps effectively.
• To alleviate the limitation of the biafﬁne localization, we further develop a multi-context biafﬁne localiza-tion module which utilizes multi-scale local and global contexts to enrich frame representations.
• We conduct extensive experiments to validate the ef-fectiveness of our proposed CBLN on three datasets (ActivityNet Captions, TACoS, and Charades-STA), and show that it signiﬁcantly outperforms the state-of-the-arts by a large margin. 2.