Abstract
Existing deep learning-based image deraining methods have achieved promising performance for synthetic rainy images, typically rely on the pairs of sharp images and simulated rainy counterparts. However, these methods suf-fer from signiﬁcant performance drop when facing the real rain, because of the huge gap between the simpliﬁed syn-thetic rain and the complex real rain. In this work, we ar-gue that the rain generation and removal are the two sides of the same coin and should be tightly coupled. To close the loop, we propose to jointly learn real rain generation and removal procedure within a uniﬁed disentangled image translation framework. Speciﬁcally, we propose a bidirec-tional disentangled translation network, in which each uni-directional network contains two loops of joint rain genera-tion and removal for both the real and synthetic rain image, respectively. Meanwhile, we enforce the disentanglement strategy by decomposing the rainy image into a clean back-ground and rain layer (rain removal), in order to better pre-serve the identity background via both the cycle-consistency loss and adversarial loss, and ease the rain layer translat-ing between the real and synthetic rainy image. A counter-part composition with the entanglement strategy is symmet-rically applied for rain generation. Extensive experiments on synthetic and real-world rain datasets show the superi-ority of proposed method compared to state-of-the-arts. 1.

Introduction
Rain is a common weather phenomenon which dramati-cally degrades the quality of images and affects many com-puter vision tasks such as detection [17] and segmentation
[1]. The forward rain generation procedure [19, 35, 8, 12, 16] is usually simpliﬁed as:
O = B + R, (1)
∗Corresponding Author r
S i m i l a
Dissimilar  
S y n t h e t i c
R e a l (a) Rain Generation (b) Rain Removal (c) Joint Rain Generation and Removal
Figure 1. Illustration of rain generation and removal. (a) The hand-crafted simpliﬁed rain generation model. (b) Although the simu-lated rain has been well removed, the huge gap between synthetic training images and real-world testing images leads a signiﬁcant performance drop in existing learning-based method JORDER (c) The proposed method learns the complex rain model
[35]. from the data-driven perspective. We propose to jointly learn the rain generation and removal within a uniﬁed framework, as so to better bridges the domain gap between the real and synthetic rain. where O, B, R denote the rainy image, clean background and rain layer [Fig. 1(a)]. Image deraining is formulated as an ill-posed inverse problem of the rain generation (1), aiming to recover the clean image B from rainy image O.
Recently, the deep learning-based deraining methods have achieved remarkable performance, beneﬁting from powerful representation of convolutional neural network.
Fu et al. [8] introduced the end-to-end residual CNN for rain streaks removal task. Latter, more sophisticated CNN architectures [35, 18, 20, 26, 16, 33, 27, 6] have sprung up with tremendous progress, and the visual deraining results are impressive. Unfortunately, it is widely recognized that although the trained models can achieve satisfactory results on the synthetic rain, they cannot well generalize to the real rain [Fig. 1(b)] because of the huge gap between the sim-pliﬁed synthetic rain and the complex real rain.
To address this problem, a number of works have been proposed for better real rain removal. The ﬁrst category 2053
in starts from the intuitive rain generation perspective, which the key idea of this research line is to make the sim-ulated rain as real as possible in supervised manner. They tend to incorporate more complicated visual appearance of rain into consideration, such as streaks, haze and occlu-sion degradation factors within a comprehensive rain model
[35, 20, 16, 12]. However, these hand-crafted generation models cannot well accommodate the complicated distribu-tion of the real rain, due to the varied angle, location, depth, intensity, density, length, width and so on. Also, researchers try to generate the pair of the real rain and ‘clean’ image from the videos [28] or the rendering technique [11]. On the one hand, the clean image generation is cumbersome; on the other hand, the generated clean image is still pseudo-label, not the oracle clean image. The second category di-rectly resorts to the real image from the domain adaptation perspective [30, 37]. The key idea is to enforce additional constraint between the real and simulated deraining results.
These semi-supervised/unsupervised methods can alleviate the domain gap by learning from the real rain, but they ne-glect the physical rain generation procedure.
The previous methods either focus on the rain generation or rain removal, few of them have noticed a simple yet im-portance problem that the rain generation and rain removal are of equal importance and should be tightly coupled. The rain removal is a typical inverse problem for the rain genera-tion. A better rain generation model would deﬁnitely beneﬁt for real rain removal, or vice versa shown in Fig. 1(c).
In this work, we bridge the gap between the rain genera-tion and rain removal in an end-to-end learning framework.
We bypass the difﬁculty of explicitly designing the sophis-ticated rain degradation model. Instead, our philosophy is to learn from real rainy image so as to approximate the real degradation implicitly. Speciﬁcally, we propose a bidirec-tional disentangled translation network [Fig. 2], in which each unidirectional network contains two stages of rain gen-eration and removal for both the real and synthetic rain im-age, respectively. We observe that, in the image translation between the real rain image and simulated rain image, the background clean image layer is consistent while only the rain layer is changed. Instead of directly translating images from synthetic to real domain, this motivates us to preserve the identity in the image background while focus on trans-forming the simpler rain layer between the real ones and simulated ones. We employ both the self-consistency loss and adversarial loss for the image background. We summa-rize the main contributions as follows:
• We propose a novel image deraining algorithm which jointly learns the rain generation (forward) and rain removal (inverse) in a uniﬁed framework. Compared with the hand-crafted rain generation model, learning physical degradation from real rainy image could of-fer better approximation to the real rain in an implicit manner. Moreover, the rain generation and removal would beneﬁt greatly from each other, thus improving the generalization for the real rainy images.
• We employ the disentanglement strategy in which the consistent background is well preserved by the self-consistency loss and adversarial loss.
In contrast to previous methods which directly transform the sim-ulated rain image to real rain image, the proposed method gets rid of the identity background and con-centrates on the simpler rain layer translating, which signiﬁcantly ease the difﬁculty of bridging the gap be-tween real rain and simulated rain image.
• We conduct extensive experiments on both synthetic and real-world datasets, which perform favorably against the state-of-the-art methods and consistently superior on real-world rainy images. 2.