Abstract
Cross-modal retrieval aims to learn discriminative and modal-invariant features for data from different modalities.
Unlike the existing methods which usually learn from the features extracted by ofﬂine networks, in this paper, we pro-pose an approach to jointly train the components of cross-modal retrieval framework with metadata, and enable the network to ﬁnd optimal features. The proposed end-to-end framework is updated with three loss functions: 1) a novel cross-modal center loss to eliminate cross-modal discrep-ancy, 2) cross-entropy loss to maximize inter-class varia-tions, and 3) mean-square-error loss to reduce modality
In particular, our proposed cross-modal cen-variations. ter loss minimizes the distances of features from objects belonging to the same class across all modalities. Exten-sive experiments have been conducted on the retrieval tasks across multi-modalities including 2D image, 3D point cloud and mesh data. The proposed framework signiﬁcantly out-performs the state-of-the-art methods for both cross-modal and in-domain retrieval for 3D objects on the ModelNet10 and ModelNet40 datasets. 1.

Introduction
With the stream of multimedia data ﬂourishing on the In-ternet in the format of videos, images, text, etc, cross-modal retrieval task has attracted more and more attention from the multimedia communities. Cross-modal retrieval is the task of retrieving data from one modality given a query from a different modality. Inspired by the representation power of deep learning, a series of deep learning-based methods have been proposed for cross-modal retrieval [27, 52, 51].
These methods operate by learning modal-invariant repre-sentations in a common space.
The features from different modalities generally have different distributions. Therefore, a fundamental require-ment for cross-modal retrieval task is to bridge the gap among different modalities which is commonly done by
∗ Equal contribution.
Figure 1. Traditional center loss vs. the proposed cross-modal center loss. Our proposed cross-modal center loss (right) ﬁnds a unique center for each class across all modalities. Traditional cen-ter loss (left) ﬁnds a center for each modality and each class and ignores the relation among centers of different modalities. Our proposed cross-modal center loss speciﬁcally eliminates the dis-crepancy across multiple modalities and thus is very effective for learning modal-invariant features. representation learning. The existing methods mainly ex-tract the features of each modality by ofﬂine pre-trained models, and apply a projection function to transfer the fea-tures into a common representation space. By this transfor-mation, the similarity of features from different modalities can be directly measured. Hence, the main challenge during this process is to learn discriminative and modal-invariant features.
By learning discriminative features, we ensure that data from the same class are mapped closely to each other in the feature space while different classes are separated as far as
In many studies, cross entropy or mean square possible. error loss in the label space are used to maximize the inter-class variations. In order to compare the features extracted from different modalities, the features need to be modal-invariant. Various methods are proposed to reduce the cross-domain discrepancy by using adversarial loss, sharing a projection network, using triplet loss with pairs/triplets of different modalities, maximizing cross-modal pairwise item correlation [29, 42, 34, 20, 10].
Even though the existing methods [42, 51] achieved promising results in the cross-modal retrieval tasks, they 3142
suffer from the following limitations: 1) Their core idea is to minimize the cross-modal discrepancy over the fea-tures from multiple modalities extracted by pre-trained neu-ral networks. For example, in the task of image-text re-trieval, image and text features are extracted by pre-trained models(VGG [37] and SentenceCNN [21]), and then learn-ing is performed on these extracted features instead of the metadata. Because these feature extractors (VGG, Sen-tenceCNN) are not trained or ﬁnetuned for cross-modal re-trieval task, they are not optimally representative. Instead, the network should be jointly trained with multimodal data to fully address the retrieval task. 2) The existing loss functions are mainly designed for two types of modalities, mainly image and text, and may not generalize well for cases when more than two modalities are available.
It is essential to develop a simple yet effective loss function that can be easily extended for multiple modalities.
In this paper, we propose a new loss function, called
Cross-modal Center Loss, speciﬁcally designed to mini-mize the intra-class variation across multiple modalities.
Our loss function is directly inspired by the traditional uni-modal center loss which learns a center for each class and minimizes the distance between objects and their corre-sponding centers in the feature space. Fig. 1 shows the com-parison between the traditional center loss and the newly proposed cross-modal center loss. Having multi-modal data, the traditional center loss minimizes the distance of objects and their centers in separate features spaces deﬁned for each modality. Instead, our proposed cross-modal center loss learns a unique center C for each class in the common space of all modalities. Speciﬁcally, it minimizes the dis-tance of multi-modal objects and their centers in the same common feature space for all modalities. When more multi-modal data is available, the cross-modal center loss will be able to learn more reliable centers for each class in the com-mon space.
With the proposed cross-modal center loss, the cross-modal discrepancy between different modalities of the data can be eliminated. The proposed cross-modal center loss can be employed in conjunction with other loss functions to jointly learn features for cross-modal retrieval task. To ver-ify the effectiveness of the proposed loss function, we fur-ther propose an end-to-end framework for cross-modal re-trieval task to learn discriminative and modal-invariant fea-tures. The proposed framework is optimized with three loss functions including the cross-entropy in the label space to learn discriminative features, the cross-modal center loss to speciﬁcally eliminate the cross-modal discrepancy in a uni-versal space, and the mean square error loss to minimize the cross-modal distance per object. Furthermore, a weight sharing strategy is applied to learn modal invariant features in the common space.
Different from the previous cross-modal retrieval meth-ods which extract the features of image or text by ofﬂine networks, we propose to jointly train the entire framework from the metadata without being limited by pre-trained models from other datasets. The effectiveness of the pro-posed framework is evaluated on a novel 3D cross-modal retrieval task which has not been explored by existing super-vised methods. Our method signiﬁcantly outperforms the recent state-of-the-art methods on 3D cross-modal retrieval task and in-domain retrieval task. The main contributions of this paper are summarized as follows:
• We propose a novel cross-modal center loss to map the representations of different modalities into a common feature space.
• We propose an end-to-end framework for cross-modal retrieval task by jointly training multiple modalities us-ing the proposed cross-modal center loss. The pro-posed framework can be extended to various cross-modal retrieval tasks.
• The proposed framework signiﬁcantly outperforms the state-of-the-art methods on cross-modal and in-domain retrieval tasks across images, point cloud, and mesh for 3D shapes. To the best of our knowledge, this is the ﬁrst supervised learning method for object re-trieval across 2D image and 3D point cloud and mesh data. 2.