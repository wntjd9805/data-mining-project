Abstract
Face clustering is a promising method for annotating un-labeled face images. Recent supervised approaches have boosted the face clustering accuracy greatly, however their performance is still far from satisfactory. These methods can be roughly divided into global-based and local-based ones. Global-based methods suffer from the limitation of training data scale, while local-based ones are difﬁcult to grasp the whole graph structure information and usually take a long time for inference. Previous approaches fail to tackle these two challenges simultaneously. To address the dilemma of large-scale training and efﬁcient inference, we propose the STructure-AwaRe Face Clustering (STAR-FC) method. Speciﬁcally, we design a structure-preserved sub-graph sampling strategy to explore the power of large-scale training data, which can increase the training data scale from 105 to 107. During inference, the STAR-FC performs efﬁcient full-graph clustering with two steps: graph parsing and graph reﬁnement. And the concept of node intimacy is introduced in the second step to mine the local structural information. The STAR-FC gets 91.97 pairwise F-score on partial MS1M within 310s which surpasses the state-of-the-arts. Furthermore, we are the ﬁrst to train on very large-scale graph with 20M nodes, and achieve superior infer-ence results on 12M testing data. Overall, as a simple and effective method, the proposed STAR-FC provides a strong baseline for large-scale face clustering. Code is available at https://sstzal.github.io/STAR-FC/. 1.

Introduction
Recent years have witnessed the great progress of face recognition [9, 28, 29, 38, 39, 41]. Large-scale datasets are an important factor in the success of face recognition and there is an increasing demand for larger-scale data. Face
* Corresponding author 105 96 100 94 92 95 90 90 88 85 86 80 75 70 70 65 65 60 60 55 55 e r o c s
-F e s i w r i a
P
DBSCAN
K-Means
GCN-D
GCN-V+E
STAR-FC 0 0.4M 10 4M
#
Training data 20 12M 30 20M
Figure 1: Method comparison when training with differ-ent scales of data and testing on 12M data from Web-Face42M [55]. The proposed STAR-FC can fully explore the power of large-scale training data. GCN-V+E fails to handle larger training graph while GCN-D’s performance is severely restricted due to the less consideration of the global structural information. clustering [22, 30, 42, 48, 50, 51, 52] is a natural way to solve the data annotation problem so as to make better use of massive unlabeled data. Face clustering is also one possi-ble approach to organize and ﬁle large volumes of real face images in social media or other application scenarios.
Recently a variety of efforts have been devoted to face clustering. Traditional unsupervised methods [16, 53] in-cluding K-Means [30] and DBSCAN [10] usually depend on some manually designed clustering strategies. They per-form well on small datasets, however they are less effec-tive when dealing with large-scale data as shown in Fig-ure 1. Recent research trends [12, 42, 47, 49, 50] turn to the
GCN-based supervised learning. These methods are per-formed based on the afﬁnity graph and can be roughly di-vided into global-based and local-based ones according to whether their GCN input is the whole graph or not. The rep-resentative global-based method GCN-V+E [47] uses the entire graph for GCN training. As shown in Figure 1, it boosts the face clustering performance greatly compared with unsupervised methods, however the training data scale 9085  
is limited by the GPU memory which makes it difﬁcult to further explore the power of larger-scale training sets. Al-though local-based methods such as GCN-D [49] shown in
Figure 1 don’t suffer from memory restrictions, its perfor-mance is limited since it lacks the comprehension of global graph structure. Besides, it organizes the data as overlapped local subgraphs which leads to inefﬁcient inference.
For many computer vision tasks [7, 8, 11, 15, 25, 34], large-scale training data is one of the most important engines to promote the performance. With the emer-gence of some new large-scale benchmarks such as Web-Face42M [55] whose data volume is ten times that of
MS1M [13], we have more data available for training.
Therefore, exploring the power of these rich training data is imperative. For testing, efﬁciency matters, we are there-fore eager to perform full graph inference. Based on the above motivations, we propose a structure-aware face clus-tering method STAR-FC to address the dilemma of large-scale training and efﬁcient inference. Speciﬁcally, we de-sign a GCN [20] based on the KNN [6] afﬁnity graph to estimate the edge conﬁdence. Furthermore, a structure pre-served subgraph sampling strategy is proposed for larger-scale GCN training. During inference, we perform face clustering with two steps: graph parsing and graph reﬁne-ment. In the second step, node intimacy is introduced to mine the local structural information for further reﬁnement.
In the inference process, the entire graph is taken as the in-put for efﬁciency.
The experiments demonstrate that with these structure-aware designs, the STAR-FC can not only perform sample-based training but also implement full-graph inference.
With sample-based training, the training data scale can be increased by two orders of magnitude from 105 to 107 and beyond. As shown in Figure 1, with the increase of training data, our method has been consistently improved and ﬁnally achieves 95.1 pairwise F-score. Interestingly, we ﬁnd that such sampling method does not lead to performance loss and brings some extra accuracy gain since it enhances the generalization of the model. In inference, with full-graph as input, the efﬁciency can be guaranteed. We achieve state-of-the-art face clustering results on partial MS1M within 310s. What’s more, we can complete inference on 12M data within 1.7h thus provide a strong baseline for face cluster-ing. To summarize, we make the following contributions:
• To fully explore the power of large-scale training dataset, we propose a structure-preserved subgraph sampling strategy which can break through the limi-tation of training data scale from 105 to 107.
• For inference, we take the entire graph as input to en-sure efﬁciency. We transform face clustering into two steps: graph parsing and graph reﬁnement. Node in-timacy is introduced in the second step to explore the local structure for further graph reﬁnement.
• The proposed STAR-FC achieves 91.97 F-score on partial MS1M within 310s. Moreover, we are the ﬁrst to conduct large-scale training on 20M data which pro-vides a strong baseline for large-scale face clustering. 2.