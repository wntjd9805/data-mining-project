Abstract
Most existing distance metric learning approaches use fully labeled data to learn the sample similarities in an embedding space. We present a self-training framework,
SLADE, to improve retrieval performance by leveraging ad-ditional unlabeled data. We ﬁrst train a teacher model on the labeled data and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate ﬁnal feature embed-dings. We use self-supervised representation learning to ini-tialize the teacher model. To better deal with noisy pseudo labels generated by the teacher network, we design a new feature basis learning component for the student network, which learns basis functions of feature representations for unlabeled data. The learned basis vectors better measure the pairwise similarity and are used to select high-conﬁdent samples for training the student network. We evaluate our method on standard retrieval benchmarks: CUB-200, Cars-196 and In-shop. Experimental results demonstrate that with additional unlabeled data, our approach signiﬁcantly improves the performance over the state-of-the-art methods. 1.

Introduction
Existing distance metric learning methods mainly learn sample similarities and image embeddings using labeled data [22, 17, 2, 29], which often require a large amount of data to perform well. A recent study [21] shows that most methods perform similarly when hyper-parameters are properly tuned despite employing various forms of losses.
The performance gains likely come from the choice of net-work architecture. In this work, we explore another direc-tion that uses unlabeled data to improve retrieval perfor-mance.
Recent methods in self-supervised learning [14, 6, 5] and self-training [32, 7] have shown promising results us-ing unlabeled data. Self-supervised learning leverages un-* Work done during an internship at Amazon.
Figure 1. A self-training framework for retrieval. In the training phase, we train the teacher and student networks using both la-beled and unlabeled data. In the testing phase, we use the learned student network to extract embeddings of query images for re-trieval. labeled data to learn general features in a task-agnostic manner. These features can be transferred to downstream tasks by ﬁne-tuning. Recent models show that the features produced by self-supervised learning achieve comparable performance to those produced by supervised learning for downstream tasks such as detection or classiﬁcation [5].
Self-training methods [32, 7] improve the performance of fully-supervised approaches by utilizing a teacher/student paradigm. However, existing methods for self-supervised learning or self-training mainly focus on classiﬁcation but not retrieval.
We present a SeLf-trAining framework for Distance mEtric learning (SLADE) by leveraging unlabeled data.
Figure 1 illustrates our method. We ﬁrst train a teacher model on the labeled dataset and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate a ﬁnal feature 9644
embedding. 2.