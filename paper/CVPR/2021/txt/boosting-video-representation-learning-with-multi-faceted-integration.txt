Abstract
Video content is multifaceted, consisting of objects, scenes, interactions or actions. The existing datasets mostly label only one of the facets for model training, resulting in the video representation that biases to only one facet depending on the training dataset. There is no study yet on how to learn a video representation from multifaceted labels, and whether multifaceted information is helpful for video representation learning.
In this paper, we pro-pose a new learning framework, MUlti-Faceted Integra-tion (MUFI), to aggregate facets from different datasets for learning a representation that could reﬂect the full spectrum of video content. Technically, MUFI formulates the problem as visual-semantic embedding learning, which explicitly maps video representation into a rich semantic embedding space, and jointly optimizes video representation from two perspectives. One is to capitalize on the intra-facet supervi-sion between each video and its own label descriptions, and the second predicts the “semantic representation” of each video from the facets of other datasets as the inter-facet su-pervision. Extensive experiments demonstrate that learn-ing 3D CNN via our MUFI framework on a union of four large-scale video datasets plus two image datasets leads to superior capability of video representation. The pre-learnt 3D CNN with MUFI also shows clear improvements over other approaches on several downstream video appli-cations. More remarkably, MUFI achieves 98.1%/80.9% on UCF101/HMDB51 for action recognition and 101.5% in terms of CIDEr-D score on MSVD for video captioning. 1.

Introduction
Deep Neural Networks have been proven to be highly effective for learning vision models on large-scale datasets.
To date in the literature, there are various image datasets (e.g., ImageNet [47], COCO [31], Visual Genome [25]) that include large amounts of expert labeled images for train-Figure 1. Example illustrating the four facets of video content: ob-ject (blue), scene (green), interaction (magenta), and action (red). ing deep models. The well-trained models, on one hand, manifest impressive classiﬁcation performances, and on the other, produce discriminative and generic representation for image understanding tasks. Compared to static 2D images, video has one more dimension (time) and is an information-intensive media with large variations and complexities. As a result, learning a powerful spatio-temporal video repre-sentation is yet a challenging problem.
Capitalizing on the high capability of deep models, one natural way to improve video representation is to acquire more video data. For example, Tran et al.
[50] devised a widely-adopted 3D CNN, namely C3D, optimized on a large-scale Sports1M dataset and a constructed I380K dataset. Carreira et al.
[2] built a popular pre-training dataset, i.e., Kinetics, consisting of around 300K well-annotated trimmed video clips. To expand the study in the regime of web videos which is multiple orders of magnitude larger, Ghadiyaram et al. [14] collected 65M web videos for pre-training 3D CNN in a weakly-supervised manner.
Despite the tremendous progresses, performing learning on a speciﬁc dataset usually focuses on a particular channel of videos (e.g., action) and seldom explores other facets of videos simultaneously. Taking a video of “making a cake” from Kinetics dataset as an example (Figure 1), there are a wide variety of facets, ranging from object, scene, inter-action, to action. Nevertheless, Kinetics, as human action dataset, mainly emphasizes the facet of action, making the learnt representation mostly aware of the action informa-14030
tion. A valid question is how to leverage multifaceted video information for representation learning.
This paper explores the integration of six facets, i.e., ac-tion (Kinetics [2]), event (Moments-In-Time [38]), interac-tion (Something-Something [15]), sport (Sports1M [14]), object (ImageNet [47]), and scene (Place365 [64]), and each facet corresponds to one dataset. We aim for a model that engages all the six facets (datasets) to learn video represen-tation, ideally making the representation more discrimina-tive and generic. The inherent difﬁculty of learning such a representation is: how to execute effective representation learning on various datasets with different labels in a uni-ﬁed framework? We propose to mitigate this issue through visual-semantic embedding learning. The basic idea is to learn a semantic space that bridges the labels from different datasets. The model that generates the space is pre-trained on a large-scale unannotated text data. The learning objec-tive is to model the semantic relationships between labels and embed the disjoint labels into semantically-meaningful feature vectors. We project video representation into the semantic space and optimize visual-semantic embedding to enhance representation learning.
To materialize our idea, we present a new MUlti-Faceted
Integration (MUFI) framework for video representation learning. Speciﬁcally, we employ off-the-shelf language models such as BERT [6] to extract textual features and build the semantic space, which is also taken as the em-bedding space. Each video is fed into a 3D CNN to obtain the video representation and then mapped into the embed-ding space. The learning of visual-semantic embedding is supervised by intra-facet embedding of a video and its class label within a dataset, and inter-facet label prediction and embedding across multiple datasets. Our MUFI framework capitalizes on the two types of supervision to jointly learn visual-semantic embedding and adjust 3D CNN through a multi-attention projection structure, and performs the whole training in an end-to-end manner. Note that the image datasets only offer the inter-facet supervision to the videos here and are not exploited as the network inputs.
The main contribution of this work is the exploration of multifaceted video content from various datasets to improve video representation learning. The novel idea leads to the elegant views of how to relate the facets of videos from different datasets, and how to consolidate various facets in a uniﬁed framework for learning, which are problems not yet fully understood. We demonstrate the effectiveness of our MUFI framework on a union of four large-scale video datasets plus two image datasets in the experiments. 2.