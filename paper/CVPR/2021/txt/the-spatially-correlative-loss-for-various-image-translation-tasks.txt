Abstract
We propose a novel spatially-correlative loss that is sim-ple, efﬁcient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-speciﬁc nature of these losses hinder translation across large do-main gaps. To address this, we exploit the spatial patterns of self-similarity as a means of deﬁning scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learn-ing method to explicitly learn spatially-correlative maps for each speciﬁc translation task. We show distinct im-provement over baseline models in all three modes of un-paired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can eas-ily be integrated into existing network architectures and thus allows wide applicability. The code is available at https://github.com/lyndonzheng/F-LSeSim. 1.

Introduction
I2I translation refers to the task of modifying an input image to ﬁt the style / appearance of the target domain, while preserving the original content / structure (as shown in Fig. 1: horse → zebra); learning to assess the content and style correctly is thus of central importance. While
GANs [15] have the ability to generate images that adhere to the overall dataset distribution, it is still difﬁcult to preserve scene structure during translation when image-conditional
GANs are optimized with purely adversarial loss.
To mitigate the issue of scene structure discrepancies, a few loss functions for comparing the content between input and output images have been proposed, including (a) pixel-level image reconstruction loss [23, 46, 6] and cycle-consistency loss [29, 56, 52]; (b) feature-level per-ceptual loss [12, 25] and PatchNCE loss [39]. However, (a) Input (b) Output (c) Structure map (d) Structure map
Figure 1. Our learned spatially-correlative representation en-codes local scene structure based on self-similarities. Despite vast appearance differences between the horse and zebra, when the scene structures are identical (i.e. same poses), the spatial patterns of self-similarities are as well. these losses still have several limitations. First, pixel-level losses do not explicitly decouple structure and appearance.
Second, feature-level losses help but continue to conﬂate domain-speciﬁc structure and appearance attributes. Fi-nally, most feature-level losses are calculated using a ﬁxed
ImageNet [11] pre-trained network (e.g. VGG16 [47]), which will not correctly adapt to arbitrary domains.
In this work, we aim to design a domain-invariant rep-resentation to precisely express scene structure, rather than using original pixels or features that couple both appearance and structure. To achieve this, we propose to revisit the idea of self-similarity. Classically, low-level self-similarity has been used for matching [43] and image segmentation [44], while feature-level self-similarity in deep learning mani-fests as self-attention maps [51]. We propose to go further, to advance an assumption that all regions within same cat-egories exhibit some form of self-similarity. For instance, while the horse and zebra in Fig. 1 appear very differ-ent, there is obvious visual self-similarity in their own re-16407
two-sided one-sided
𝒳𝒳
𝑥𝑥𝑖𝑖
�𝑥𝑥𝑖𝑖
𝐺𝐺𝑥𝑥→𝑦𝑦
�𝑦𝑦𝑖𝑖 𝒴𝒴 kxi − Gy→x(Gx→y(xi))kp
𝐺𝐺𝑦𝑦→𝑥𝑥 (a) DiscoGAN [29], CycleGAN [56],
DualGAN [52], MUNIT [22], StarGAN [9],
BicycleGAN [57], DRIT++ [32] . . . kf (xi) − f (Gx→y(xi))kp (b) DeePSiM [12], StyleNet [48],
PerceptualLoss [25], SimGAN [46],
ContextualLoss [34], TSIT [24] . . .
𝒳𝒳
𝐺𝐺𝑥𝑥→𝑦𝑦
𝑥𝑥𝑖𝑖 f (xi) ∝ f (Gx→y(xi))
�𝑦𝑦𝑖𝑖 𝒴𝒴 (c) DistanceGAN [5],
TraVelGAN [2], GcGAN [13],
CUT [39] . . . d(S(xi), S(Gx→y(xi))) (d) Ours
Figure 2. Comparison of unpaired I2I translation methods with various content losses. (a) The cycle-consistency loss [29, 56, 52] in a two-sided framework. (b) Pixel-level image reconstruction loss [46] and feature-level matching loss [25]. (c) Various indirect relation-ships [5, 13] between the input and output. (d) Our spatially-correlative loss based on a learned spatially-correlative map. gions. We believe a network can learn deeper representa-tions of self-similarities (beyond just visual ones) that can encode intact object shapes, even when there are variations in appearances within an object. Then through estimating such co-occurrence signals in self-similarity, we can explic-itly represent the structure as multiple spatially-correlative maps, visualized as heat maps in Fig.1 (c) and (d). Based on this within-shape self-similarity, we propose then that a structure-preserving image translation will retain the pat-terns of self-similarity in both the source and translated im-ages, even if appearances themselves change dramatically.
Our basic spatially-correlative map, called FSeSim, is obtained by computing the Fixed Self-Similarity of features extracted from a pre-trained network. While this basic ver-sion achieved comparative or even better results than state-of-the-art methods [56, 13, 39] on some tasks, the generality is limited because features extracted from an ImageNet pre-trained network are biased towards photorealistic imagery.
To obtain a more general spatially-correlative map, the
Learned Self-Similarity, called LSeSim, is presented by using a form of contrastive loss, in which we explicitly encourage homologous structures to be closer, regardless of their appearances, and reciprocally dissociate dissimilar structures even they have similar appearances. To do this, the model learns a domain-invariant spatially-correlative map, where having the same scene structure leads to sim-ilar maps, even if the images are from different domains. (a) In contrast
There are several advantages of using the proposed
F/LSeSim loss: to the existing losses that directly compare the loss on pixels [56] or features
[25], F/LSeSim captures the domain-invariant structure rep-resentation, regardless of the absolute pixel values; (b)
Through contrastive learning, the LSeSim learns a spatially-correlative map for a speciﬁc image translation task, rather than features extracted from a ﬁxed pre-trained network, as in e.g. perceptual loss [25], contextual loss [34]; (c)
The translation model is more efﬁcient and faster than the widely used cycle-consistency architectures, because our
F/LSeSim explicitly encodes the structure, bypassing the expensive multi-cycle looping; (d) As we show in Fig. 5, our F/LSeSim correctly measures the structural distance even when the two images are in completely different do-mains; (e) Finally, our F/LSeSim can easily be integrated into various frameworks. In our experiments, we directly used the generator and discriminator architectures of Cy-cleGAN [56], MUNIT [22] and StyleGAN [26, 27] for ex-tensive I2I translation tasks. The experimental results show that our model outperformed the existing both one-sided translation methods [5, 2, 13, 39] and two-sided translation methods [56, 52, 22]. 2.