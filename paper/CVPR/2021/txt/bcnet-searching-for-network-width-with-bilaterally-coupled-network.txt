Abstract
Searching for a more compact network width recently serves as an effective way of channel pruning for the de-ployment of convolutional neural networks (CNNs) under hardware constraints. To fulﬁll the searching, a one-shot supernet is usually leveraged to efﬁciently evaluate the per-formance w.r.t. different network widths. However, current methods mainly follow a unilaterally augmented (UA) prin-ciple for the evaluation of each width, which induces the training unfairness of channels in supernet.
In this pa-per, we introduce a new supernet called Bilaterally Cou-In BCNet, pled Network (BCNet) to address this issue. each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we leverage a stochas-tic complementary strategy for training the BCNet, and pro-pose a prior initial population sampling method to boost the performance of the evolutionary search. Extensive experi-ments on benchmark CIFAR-10 and ImageNet datasets in-dicate that our method can achieve state-of-the-art or com-peting performance over other baseline methods. Moreover, our method turns out to further boost the performance of
NAS models by reﬁning their network widths. For example, with the same FLOPs budget, our obtained EfﬁcientNet-B0 achieves 77.36% Top-1 accuracy on ImageNet dataset, sur-passing the performance of original setting by 0.48%. 1.

Introduction
For practical deployment of convolutional neural net-works (CNNs), it is important to consider different hard-ware budgets [12, 10, 11, 30], to name a few, ﬂoating point operations (FLOPs), latency, memory footprint and energy
*Corresponding authors. consumption. One way to simultaneously accommodate all these budgets is to prune the redundant channels of a model, so that a compact network width can be obtained. Typi-cal channel pruning usually leverages a pre-trained network and implement the pruning in an end-to-end [21, 24, 32] or layer-by-layer [16, 36] manner. After pruning, the struc-ture of the pre-trained model remains unchanged, so that the pruned network is friendly to off-the-shelf deep learn-ing frameworks and can be further boosted by other tech-niques, such as quantization [12] and knowledge distillation
[17, 41, 22].
Recently, [26] found the core of channel pruning is to learn a more compact network width instead of the re-tained weights. Other literature also uses number of chan-nels/ﬁlters to indicate the network width. Thus follow-up work resorts to neural architecture search (NAS) [38, 40, 39, 1] or other automl techniques for directly searching for an optimal network width, such as MetaPruning [25], Au-toSlim [43] and TAS [6]. In their methods, a one-shot super-net is usually leveraged for evaluation of different widths.
Concretely, for the width c at a certain layer, we need to as-sign c channels (ﬁlters) in the layer and all layers follow the same way. Then all these assigned channels in the supernet specify a sub-network with the supernet. As a result, the performance of a network width refers to the accuracy of the speciﬁed sub-network with shared weights of supernet.
For fair evaluation of different network widths, during the training of supernet, all network widths will be evenly sam-pled from the supernet and get optimized accordingly. For brevity, we use the name of layer width to indicate the width for a certain layer, while network width represents the set of widths for all layers.
In this way, how to specify the sub-network(s) for each network width matters for the performance evalua-tion. However, current methods [25, 43, 6] mainly follow a 2175
unilaterally augmented (UA) principle for the evaluation of network widths in supernet. Suppose we count channels in a layer from the left to the right as Figure 1. To evaluate the width c, UA principle simply assigns the leftmost c chan-nels to specify a sub-network for evaluation. In this way, channels within smaller width will also be used for evalua-tion of larger widths. Since we uniformly sample all widths during training the supernet, channels close to left side will be used more times than those close to the right side in the evaluation of widths as in Figure 1(a). For example, the leftmost channel will be used 6 times for evaluation while the rightmost channel is only used once. This causes train-ing unfairness among the channels and their corresponding kernels. Left channels will be trained more than right ones.
Nevertheless, this training unfairness will affect the accu-racy of evaluation, and thus hampers the ability of supernet to rank over all network widths.
In this paper, we introduce a new supernet called Bi-laterally Coupled Network (BCNet) to address the training and evaluation unfairness within UA principle. In BCNet, each channel is fairly trained and responsible for the same amount of widths. Speciﬁcally, both in training and evalua-tion, each width is determined symmetrically by the average performance of bilateral (i.e., both left and right) channels.
As shown in Figure 1(b), suppose a layer has 6 channels, then each channel of BCNet evenly corresponds to 7 layer widths from the left or right side. In this way, all channels will be trained equally; all widths are bilaterally coupled in
BCNet and will be evaluated more fairly.
To encourage a rigorous training fairness over channels, we adopt a complementary training strategy for training
BCNet as in Figure 2. As for the subsequent searching, since the evolutionary algorithm is empirically fairly sensi-tive to the initial population, we also propose a prior sam-pling method, which enables to generate a good and steady initial population instead of random initialization. Exten-sive experiments on the benchmark CIFAR-10 and Ima-geNet datasets show that our method outperforms the state-of-the-art methods under various FLOPs budget. For ex-ample, our searched EfﬁcientNet-B0 achieves 74.9% Top-1 accuracy on ImageNet dataset with 192M FLOPs (2 × ac-celeration). 2.