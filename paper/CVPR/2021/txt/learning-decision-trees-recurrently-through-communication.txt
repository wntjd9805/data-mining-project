Abstract
Integrated interpretability without sacriﬁcing the predic-tion accuracy of decision making algorithms has the poten-tial of greatly improving their value to the user. Instead of assigning a label to an image directly, we propose to learn iterative binary sub-decisions, inducing sparsity and trans-parency in the decision making process. The key aspect of our model is its ability to build a decision tree whose struc-ture is encoded into the memory representation of a Recur-rent Neural Network jointly learned by two models commu-nicating through message passing. In addition, our model assigns a semantic meaning to each decision in the form of binary attributes, providing concise, semantic and rele-vant rationalizations to the user. On three benchmark image classiﬁcation datasets, including the large-scale ImageNet, our model generates human interpretable binary decision sequences explaining the predictions of the network while maintaining state-of-the-art accuracy. 1.

Introduction
The decision mechanism of deep Convolutional Neural
Networks (CNNs) is often hidden from the user, hinder-ing their employment in critical applications such as health-care, where a thorough understanding of this mechanism may be required. The aim for analyzing the decision mech-introspection, is to reveal the internal process anism, i.e. of the decision maker to a machine learning practitioner or user [48]. However, models offering explanations through introspection may result in a performance loss [18, 45].
Incorporating recent advances in multi-agent communi-cation [15], we formulate the decision process as an itera-tive decision tree and embed its structure into the memory representation of a Recurrent Neural Network (RNN). Our model uses message-passing [20] with discrete symbols from a vocabulary. A tunable parameter controls whether to learn this vocabulary from scratch or to map it to human-understandable attributes assigning a meaning to every de-cision to improve its interpretability. Further, encoding the decision tree into the memory of an RNN retains the ﬂexi-bility and performance of CNNs while being scalable. In-Figure 1: Our Recurrent Decision Tree (RDT) (left) asks questions, Attribute-based Learner (AbL) (right) answers with a yes/no s.t. the accuracy improves after each step. stead of requiring an exponential number of tree nodes with increasing depth, our model learns orders of magnitude fewer nodes with a constant number of model parameters for an arbitrary tree depth. After training, our neural model can be converted exactly into a standard decision tree, being computationally efﬁcient at test time.
Our framework (see Figure 1) exposes a decision path in the form of an explainable decision chain by breaking down the decision process into multiple binary decisions. Our re-current decision-tree (RDT) (blue) does not see the image, and has to infer the image class, e.g. dog, by recurrently asking binary questions, e.g. does it have whiskers? Our attribute-based learner (AbL) (red) answers these questions with yes/no by looking at the image, allowing the RDT to update the class probabilities and the memory representa-tion of the previous questions and answers. This is repeated until the RDT reaches a ﬁnal decision and the decision tree becomes easily understandable as it associates the binary answers with semantic attributes, e.g. has whiskers.
Our contributions are: 1) We propose a recurrent deci-sion tree model (RDTC) with hard node splits and over-come current limitations of decision trees in terms of depth scalability and ﬂexibility; 2) We predict attributes in an end-to-end manner allowing human-interpretable explana-tions; 3) We showcase on three datasets that our model generates explainable decision trees more efﬁciently than related methods while retaining the performance of non-explainable CNNs. Our code is publicly available at: https://github.com/ExplainableML/rdtc. 13518
2.