Abstract
Deep neural networks achieve state-of-the-art and some-times super-human performance across various domains.
However, when learning tasks sequentially, the networks easily forget the knowledge of previous tasks, known as
“catastrophic forgetting”. To achieve the consistencies be-tween the old tasks and the new task, one effective solution is to modify the gradient for update. Previous methods en-force independent gradient constraints for different tasks, while we consider these gradients contain complex informa-tion, and propose to leverage inter-task information by gra-dient decomposition. In particular, the gradient of an old task is decomposed into a part shared by all old tasks and a part speciﬁc to that task. The gradient for update should be close to the gradient of the new task, consistent with the gra-dients shared by all old tasks, and orthogonal to the space spanned by the gradients speciﬁc to the old tasks. In this way, our approach encourages common knowledge consol-idation without impairing the task-speciﬁc knowledge. Fur-thermore, the optimization is performed for the gradients of each layer separately rather than the concatenation of all gradients as in previous works. This effectively avoids the inﬂuence of the magnitude variation of the gradients in dif-ferent layers. Extensive experiments validate the effective-ness of both gradient-decomposed optimization and layer-wise updates. Our proposed method achieves state-of-the-art results on various benchmarks of continual learning. 1.

Introduction
Recent years have witnessed the great progress in deep learning (DL) through training on large datasets. A typ-ical supervised DL task requires independent and identi-cally distributed (i.i.d) samples from a stationary distribu-tion [15, 39, 46]. However, many DL models deployed in the real-world are exposed to non-stationary situations where data is acquired sequentially and its distribution varies over time. In this scenario, DNNs trained by Stochas-2This work was done when Shixiang Tang was an intern at SenseTime. tic Gradient Descent (SGD) will easily forget the knowl-edge from previous tasks while adapting to the informa-tion from the incoming tasks. This phenomenon, known as catastrophic forgetting, invokes more effective algorithms for continual learning (CL), the goal of which is to learn consecutive tasks without severe performance degradation on previous tasks [5, 30, 34, 38, 44, 43, 57, 57, 50].
One of the popular attempts for continual learning relies on a set of episodic memories, where each episodic mem-ory stores representative data from an old task [5, 38, 30].
In particular, the network parameters are jointly optimized by the recorded samples that are regularly replayed and the samples drawn from the new task. An effective solution is to modify the gradients for updates. The methods like
GEM, A-GEM and S-GEM obtain gradient update by re-quiring the loss of each old task does not increase [7, 30].
In practice, the constraints are imposed by forcing the inner product between the gradient for update and the gradient of every old task non-negative.
In this work, we consider that the gradients of multi-ple tasks have mixed information and should be disentan-gled when used for learning. There are two parts mixed in the gradients, shared gradient and task-speciﬁc gradi-ents. Optimization along the shared gradient will be ben-eﬁcial for memorizing all old tasks, but optimization along task-speciﬁc gradients will fall into a dilemma that optimiz-ing one episodic memory will inevitably damage another.
Therefore, different constraints are imposed on two gradi-ents. We encourage the consistency between gradient for update and the shared gradient but expect that the gradient for update will be orthogonal to all task-speciﬁc gradients.
The ﬁrst constraint can be mathematically presented as the inner product of shared gradient and gradient for update non-negative. The second constraint requires the gradient for update should be orthogonal to the space spanned by all task-speciﬁc gradients. Our further analysis points out that the second constraints can be relaxed by PCA, which captures the most important gradient constraints.
Furthermore, we observe the large variation of magni-tudes for the gradients in different layers. However, the previous gradient modiﬁcation methods ignore the intrinsic 9634
magnitude variations of different layers. They concatenate the gradients from different layers into a vector and con-struct optimization problems under the constraints made by these concatenated vectors. We argue that during optimiza-tion, gradients from some layers have larger magnitudes and they will dominate the solution of the optimization problem.
However, no evidence shows that these layers are more im-portant for loss minimization than the others. To address this problem, we propose a layer-wise gradient update strat-egy, where unique gradient constraints are imposed by each layer for optimization and the solution is only speciﬁc to the parameters in that layer. Our further analysis manifests that the layer-wise optimization strategy increases the efﬁciency of reducing old task losses.
The contribution of this paper is two-fold: (1) Gradient decomposition is leveraged to specify the shared and task-speciﬁc information in the episodic memory. Different con-straints are imposed based on the shared gradient and tasks-speciﬁc gradient respectively. (2) Layer-wise gradient up-date strategy is proposed to deal with large magnitude vari-ations between gradients from different layers and thus it can reduce the losses of episodic memory more efﬁciently.
Extensive ablation studies validate the effectiveness of the two improvements. 2.