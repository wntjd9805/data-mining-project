Abstract
While radar and video data can be readily fused at the detection level, fusing them at the pixel level is poten-tially more beneﬁcial. This is also more challenging in part due to the sparsity of radar, but also because auto-motive radar beams are much wider than a typical pixel combined with a large baseline between camera and radar, which results in poor association between radar pixels and color pixel. A consequence is that depth completion meth-ods designed for LiDAR and video fare poorly for radar and video. Here we propose a radar-to-pixel association stage which learns a mapping from radar returns to pix-els. This mapping also serves to densify radar returns.
Using this as a ﬁrst stage, followed by a more traditional depth completion method, we are able to achieve image-guided depth completion with radar and video. We demon-strate performance superior to camera and radar alone on the nuScenes dataset. Our source code is available at https://github.com/longyunf/rc-pda. 1.

Introduction
We seek to incorporate automotive radar as a contribut-ing sensor to 3D scene estimation. While recent work fuses radar with video for the objective of achieving improved ob-ject detection [4, 33, 26, 32, 35], here we aim for pixel-level fusion of depth estimates, and ask if fusing video with radar can lead to improved dense depth estimation of a scene.
Up to the present, outdoor depth estimation has been dominated by LiDAR, stereo, and monocular techniques.
The fusion of LiDAR and video has lead to increasingly ac-curate dense depth completion [17]. At the same time, radar has been relegated to the task of object detection in vehicle’s
Advanced Driver Assistance Systems (ADAS) [30]. How-ever, phased array automotive radar technologies have been advancing in accuracy and discrimination [14]. Here we in-vestigate the suitability of using radar instead of LiDAR for the task of dense depth estimation. Unlike LiDAR, automo-tive radars are already ubiquitous, being integrated in most vehicles for collision warning and similar tasks. If success-fully fused with video, radar could provide an inexpensive alternative to LiDARs for 3D scene modeling and percep-tion. However, to achieve this, attentive algorithm design is required in order to overcome some of the limitations of radar, including coarser, lower resolution, and sparser depth measurements than typical LiDARs.
This paper proposes a method to fuse radar returns with image data and achieve depth completion; namely a dense depth map over pixels in a camera. We develop a two-stage algorithm. The ﬁrst stage builds an association between radar returns and image pixels, during which we resolve some of the uncertainty in projecting radar returns into a camera.
In addition, this stage is able to ﬁlter occluded radar returns and “densify” the projected radar depth map along with a conﬁdence measure for these associations (see
Fig. 1 (a,b)). Once a faithful association between radar hits and camera pixels is achieved, the second stage uses a more 12507
standard depth completion approach to combine radar and image data and estimate a dense depth map, as in Fig. 1(c).
A practical challenge to our fusion goal is the lack of public datasets with radar. KITTI [12], the dataset used most extensively for LiDAR depth completion, does not in-clude radar and nor do the Waymo [43] or ArgoVerse [5] datasets. The main exception is nuScenes [3] and the small Astyx [31] which have radar, but unfortunately do not include a dense, pixel-aligned depth map as created by
Uhrig et al. [45]. Similarly, the Oxford Radar Robot Car dataset [1] includes camera, LiDAR and raw radar data, but no annotations are available for scene understanding. As a result, all experiments of this work will use the nuScenes dataset along with its annotations. However, we ﬁnd single
LiDAR scans insufﬁcient to train depth completion, and so accumulate scans to build semi-dense depth maps for train-ing and evaluating depth completion.
The main contributions of this work include:
• Radar-camera pixel depth association that upgrades the projection of radar onto images and prepares a densi-ﬁed depth layer.
• Enhanced radar depth that improves radar-camera depth completion over raw radar depth.
• LiDAR ground truth accumulation that leverages op-tical ﬂow for occluded pixel elimination, leading to higher quality dense depth images. 2.