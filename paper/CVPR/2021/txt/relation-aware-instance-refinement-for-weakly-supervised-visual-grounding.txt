Abstract
Visual grounding, which aims to build a correspon-dence between visual objects and their language entities, plays a key role in cross-modal scene understanding. One promising and scalable strategy for learning visual ground-ing is to utilize weak supervision from only image-caption pairs. Previous methods typically rely on matching query phrases directly to a precomputed, ﬁxed object candidate pool, which leads to inaccurate localization and ambigu-ous matching due to lack of semantic relation constraints.
In our paper, we propose a novel context-aware weakly-supervised learning method that incorporates coarse-to-ﬁne object reﬁnement and entity relation modeling into a two-stage deep network, capable of producing more ac-curate object representation and matching. To effectively train our network, we introduce a self-taught regression loss for the proposal locations and a classiﬁcation loss based on parsed entity relations. Extensive experiments on two public benchmarks Flickr30K Entities and Refer-ItGame demonstrate the efﬁcacy of our weakly grounding framework. The results show that we outperform the previ-ous methods by a considerable margin, achieving 59.27% top-1 accuracy in Flickr30K Entities and 37.68% in the
ReferItGame dataset respectively1. 1.

Introduction
Cross-modal understanding of visual scene and natural language description plays a crucial role in bridging human and machine intelligence, and has attracted much interest from AI community [13]. Towards this goal, one core prob-*Both authors contributed equally. This work was done when Yongfei
Liu was a research intern at Tencent AI Lab, and Bo Wan was a master student in ShanghaiTech University. This work was supported by Shanghai
NSF Grant (No. 18ZR1425100). at https://github.com/youngﬂy11/ReIR-1Code is
WeaklyGrounding.pytorch.git available
Figure 1. Comparison of visual entities representation with exist-ing weakly-supervised grounding models. (a) Previous methods directly match between noun phrases and a precomputed, ﬁxed object proposals. (b) Our approach is capable of reﬁning the ini-tial object proposals and enriching their representation with visual relation context cues. lem is to establish instance-level correspondence between visual regions and its related language entities, which is commonly referred to as visual grounding [15]. Such corre-spondence serves as a fundamental building-block for many vision-language tasks, such as image captioning [6, 32], vi-sual question answering [43, 23], visual navigation [37, 45] and visual dialog[18, 7].
Much progress has been made recently in learning vi-sual grounding with strong supervision [22, 39], which requires costly annotations on region-phrase correspon-dence. A more scalable modeling strategy is to learn from only image-caption pairs, namely weakly-supervised visual grounding [29, 40, 2, 8, 20]. Nevertheless, learning from such weak supervision is particularly challenging mainly due to the severe ambiguity in visual object location and in correspondence between diverse noun phrases and object entities during cross-modal learning.
Most existing approaches tackle those challenges via the 5612
Multiple Instance Learning (MIL) framework [14] using object candidates generated by a pre-trained object detec-tor [29, 4, 2]. Despite their promising results, these learn-ing pipelines often suffer from the visual and matching am-biguity from several aspects. First, they usually rely on a precomputed object proposal set that contain many dis-tractor or background regions, making it difﬁcult to infer positive matches for learning. In addition, these proposals are typically kept ﬁxed during learning, which leads to in-accurate localization bounded by the external detectors (cf.
Fig. 1(a)). Furthermore, these methods often represent noun phrase or visual object context in an implicit manner, using attention-based feature aggregation or encoding predicate triples [20, 21]. Such representations are limited in cap-turing rich semantic constraints from relations in an image-sentence pair, resulting in cross-modal matching ambiguity in both learning and prediction.
To address the afore-mentioned limitations, we propose a
ﬂexible and context-aware object representation for weakly-supervised visual grounding in this work. Unlike previous work, our representation is capable of reﬁning the spatial locations of object proposals using a self-taught mecha-nism, and incorporates a relation-aware context model by exploiting the language prior (cf. Fig. 1(b)). Such enriched representation alleviates the impact from the inaccurate ob-ject detection and the cross-modal matching ambiguity. To achieve this, we develop a coarse-to-ﬁne matching strategy modeled as a two-stage deep network. The ﬁrst stage of our model consists of a backbone and a coarse-level match-ing network for proposal generation and reﬁnement, while the second stage builds a visual object graph network and a
ﬁne-level matching network for context modeling and ﬁnal matching prediction.
Speciﬁcally, given a pair of image and language descrip-tion, we ﬁrst use the backbone network to generate a set of object proposals with their visual features and compute the language embedding for the noun phrases. Then the coarse-level matching network selects a small set of relevant pro-posals for each phrase and reﬁnes their spatial locations. For the second stage, we construct the visual object graph net-work on the reﬁned proposals by exploiting parsed language structure, which enriches object features with their relations and context. Based on the context-aware representation, the
ﬁne-level matching network ﬁnally predicts instance-level correspondence between phrases and object proposals, as well as further reﬁned object locations.
To train our deep network in a weak supervision setting, we introduce a novel multi-task loss function to exploit both the model prediction and linguistic relation cues.
In par-ticular, we ﬁrst devise a self-taught regression loss for the proposal location reﬁnement, which employs highly conﬁ-dent proposal predictions as pseudo groundtruth for their neighboring proposals. Moreover, we develop a classiﬁca-tion loss on visual relation types based on the output of an external language parser. This enables us to generate effec-tive supervision from the noisy language parsing results for learning better entity representations.
We conduct extensive experiments on two public bench-marks: Flickr30K Entities [27] and ReferItGame [16]. The experiment results show that our method outperforms the prior state-of-the-art with a considerable margin. To val-idate the effectiveness of each model component, we also provide the detailed ablative study on Flickr30K Entities dataset. The main contributions of our work are three-folds:
• We adopt a coarse-to-ﬁne strategy to reﬁne object pro-posals and alleviate semantic ambiguities by enriching visual feature with relationship constraints.
• We propose a self-taught regression loss to supervise object proposal reﬁnement, and introduce an addi-tional visual relation loss that helps learn a context-aware object representation.
• Our method achieves new state-of-the-art performance on Flickr30K Entities and ReferItGame benchmarks. 2.