Abstract
Convolutional Networks (ConvNets) excel at seman-tic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cam-eras present themselves as a perfect ﬁt in such systems.
Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images.
Transferring these models from the domain they were de-signed for to 360◦ perception, their performance drops dra-matically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Ef-ﬁcient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omni-directional imagery. In addition to the learned attention-based contextual priors that can stretch across 360◦ im-ages, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both:
Densely labeled and unlabeled data originating from mul-tiple datasets. To foster progress in panoramic image seg-mentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks. 1 1.

Introduction
Convolutional Networks (ConvNets) reach striking per-formance on semantic segmentation [1, 34], a dense vi-sual recognition task that aims at transforming an image into its underlying semantic regions. Particularly, segment-ing images of road scenes automatically is of interest as it lies vital groundwork for scene understanding in an au-tonomous driving environment [26]. Most segmentation
Figure 1. Top: Examples from our Wild PAnoramic Semantic
Segmentation (WildPASS) dataset; bottom: Class distribution of pixel-class associations as unfolded over the angular direction. algorithms [8, 40, 73] are designed to work on pinhole-camera images whose Field of View (FoV) is rather narrow, capturing only a fraction of what is occurring on and aside the road. For a more holistic view on street-scenes, omnidi-rectional cameras are becoming ubiquitous in autonomous driving systems [71], as their capability of all-around sens-ing, gives rise to comprehensive 360◦ scene understanding.
Yet, rather than designing models based on the new imag-ing modality, predominantly, due to lack of sufﬁcient la-beled data, narrow FoV-trained models are applied to yield a segmentation [7, 15, 59]. Brought about by the large mismatch in FoV and structural distribution between imag-ing domains, this practice causes signiﬁcant performance degradation, even to a point of rendering the perception of surroundings completely unreliable [70]. 1WildPASS: https://github.com/elnino9ykl/WildPASS
Fig. 1 shows the distribution of semantic classes along 1376
the viewing angle, where green-tinted regions indicate the section visible to a front-facing narrow FoV camera. It be-comes apparent, that the positional priors in narrow- and omnidirectional images diverge severely, e.g., the road class occupies much of the center portion in narrow FoV images, which, in 360◦ images, is far smaller at viewing angles perpendicular to the front-facing direction (180◦). These observations suggest distinct contextual priors between the imaging techniques, which provide critical cues for seman-tic segmentation [78, 87]. Here, we address the challenging problem of leveraging inherent long-range contextual priors unique to omnidirectional images, that previous systems fail to harvest [69]. We lift the widely used, but computation-ally expensive non-local operations based on pixel-to-pixel correlations [17, 64] to the 360◦ FoV scenario in a computa-tionally efﬁcient fashion, i.e., modeling correlations of each position to omni-range regions.
By design, non-local attention [64] is meant to aggre-gate associations from highly correlated positions like re-gions of homogeneous classes, which often lie in the hori-zontal direction at a similar height, e.g., the sidewalks can be distributed across 360◦ but principally in the lower part of street-scenes. As preliminary investigation, we calcu-late the mean of the Pearson correlation coefﬁcient r(Xdir) of the probability distribution over the classes in either the horizontal r(Xhor) = 0.37 or vertical r(Xver) = 0.16 di-rection. In light of the substantially larger class-correlation of pixels observed horizontally, one can expect designing an attention module with emphasis in the large-FoV width-wise dimension to be advantageous, which reinforces the model’s discriminability while eliminating redundant com-putation on weakly-correlated pixels belonging to hetero-geneous semantics. With this rationale, we propose Efﬁ-cient Concurrent Attention Networks (ECANets) for captur-ing omni-range dependencies within wide-FoV imagery.
Aside from considering the inherent properties of 360◦ data in architecture design, training such models is not straightforward, as a lack of sufﬁcient annotated panoramic images impedes optimization. The labeling process is ex-tremely time- and labor-intensive, thus, only few surround-view datasets [52, 63, 76] are publicly accessible and none capture as diverse scenes as contemporary pinhole datasets [38, 58] do. This is why, to unlock the full po-tential of our novel ECANet architecture, we propose a multi-source omni-supervised learning regimen that inter-twines training on unlabeled, full-sized panoramic images and densely labeled pinhole images. By means of data dis-tillation [46] and our novel multi-source prediction fusion, we increase prediction certainty, ingrain long-range contex-tual priors while still beneﬁting from large-scale, narrow
FoV segmentation datasets.
To facilitate progress in omnidirectional semantic image segmentation, we put forward the Wild PAnoramic Seman-tic Segmentation (WildPASS) dataset for evaluating models in unconstrained environments, featuring scenes collected from all around the globe, reﬂecting the perception chal-lenge of autonomous driving in the real world. An exhaus-tive number of diverse experiments demonstrate the pro-posed ECANet model, learning- and fusion strategy make it possible to deploy highly efﬁcient ConvNets for panoramic image segmentation, surpassing the state-of-the-art on the public PASS [69] and novel WildPASS datasets. On a glance, we deliver the following contributions:
• Rethink omnidirectional semantic segmentation from the context-aware perspective and propose Efﬁcient
Concurrent Attention Networks, capturing inherent long-range dependencies across the 360◦.
• Introduce multi-source omni-supervised learning, inte-grating unlabeled panoramic images into training, em-powering models to learn rich contextual priors.
• Present the diverse WildPASS dataset: Collected from 6 continents and 65 cities, enabling evaluation of panoramic semantic segmentation in the wild.
• Our methods surpass previous and set new state-of-the-art results on the PASS and WildPASS datasets. 2.