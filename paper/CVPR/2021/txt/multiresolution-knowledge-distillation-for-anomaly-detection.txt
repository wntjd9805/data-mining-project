Abstract
Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through con-ventional techniques. Secondly, while only normal samples are available at training, the learned features should be dis-criminative of normal and anomalous samples. Here, we propose to use the “distillation” of features at various lay-ers of an expert network, which is pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We de-tect and localize anomalies using the discrepancy between the expert and cloner networks’ intermediate activation val-ues given an input sample. We show that considering mul-tiple intermediate hints in distillation leads to better ex-ploitation of the expert’s knowledge and a more distinctive discrepancy between the two networks, compared to utiliz-ing only the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorpo-rate interpretability algorithms in our novel framework to localize anomalous regions. Despite the striking difference between some test datasets and ImageNet, we achieve com-petitive or signiﬁcantly superior results compared to SOTA on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two other medical datasets on both anomaly detection and localization. 1.

Introduction
Anomaly detection (AD) aims for recognizing test-time inputs that look abnormal or novel to the model according to previously seen normal samples during training. AD has been a vital demanding task in computer vision with various applications, like in industrial image-based product quality control [27, 7], or health monitoring processes [26]. These
∗ Denotes equal contribution.
Figure 1: Our precise heatmaps are localizing anomalous features in MVTecAD (top two rows) and normal features in MNIST and CIFAR-10 (two bottom rows). tasks also require pixel-precise localization of anomalous regions, called defects. This is pivotal for comprehend-ing the dynamics of monitored procedures, triggering the apt antidotes, and providing pertinent data for downstream models in industrial settings.
Traditionally, the AD problem has been approached in a one-class setting, where anomalies represent a broadly dif-ferent class from normal samples. Recently, considering subtle anomalies has attracted attention. This new setting further necessitates precise anomaly localization. However, performing excellently in both settings on various datasets is highly appreciated but is not fully achieved.
Due to the unsupervised nature of the AD problem and the restricted data access, only having anomaly-free data in training, the majority of AD methods [36, 31, 40, 18, 34] model the normal data abstraction by extracting semanti-cally meaningful latent features. These methods perform 14902
well solely on either of the two mentioned cases. This problem, called the generality problem [39], highly declines trust in them on unseen future datasets. Moreover, anomaly localization is either impossible inadequate in most of them
[36, 31, 33] and leads to intensive computations that hurt their real-time performance. Additionally, many earlier works [33, 31] suffer from unstable training, requiring un-principled early stopping to achieve acceptable results.
Though not fully explored in the AD context, using pre-trained networks could potentially be an alternative track.
This is especially helpful when the sample size is small and the normal class shows signiﬁcant variations. Some ear-lier studies [4, 12, 28, 29] try to train their model based on pre-trained features of the normal data. These methods either miss anomaly localization [4, 12], or tackle the prob-lem in a region-based fashion [28, 53], i.e., splitting images into smaller patches to determine the sub-regional abnor-mality. This is computationally expensive and often leads to inaccurate localization. To evade this issue, Bergmann et al. [8] train an ensemble of student networks to mimic the last layer of a teacher network on the anomaly-free data.
However, performing a region-based approach in this work makes it heavily rely on the size of the cropped patches and hence susceptible to the changes in this size, and intensi-ﬁes the training cost severely. Furthermore, imitating only the last layer misses fully exploiting the knowledge of the teacher network [32]. This makes them complicate their model and employ other complementary techniques, such as self-supervised learning in parallel.
Lately, Zhang et al. [52] have demonstrated that activa-tion values of intermediate layers of neural networks are a solid perceptual representation of their input images. By this premise, we propose a novel knowledge distillation method for AD that is designed to distill the comprehen-sive knowledge of an ImageNet pre-trained source network, solely on the normal training data, into a simpler cloner net-work. This happens by forcing the cloner’s intermediate embedding of normal training data at several critical lay-ers to conform to those of the source. Consequently, the cloner learns the normal data manifold thoroughly and yet earns no knowledge from the source about other possible in-put data. Hence, the cloner will behave differently from the source when fed with the anomalous data. Furthermore, a simpler cloner architecture enables avoiding distraction by non-distinguishing features and enhances the discrepancy in the behavior of the two networks on anomalies.
Moreover, we derive precise anomaly localization heat maps without using region-based expensive training and testing through exploiting the concept of gradient. We eval-uate our method on a comprehensive set of datasets on vari-ous anomaly detection/localization tasks, where we exceed
SOTA in both localization and detection. Our training is highly stable and needs no dataset-dependent ﬁne-tuning.
As we only train the cloner’s parameters, we require just one more forward pass of inputs through the source com-pared to a standard network training on the normal data.
We also investigate our method through exhaustive ablation studies. Our main contributions are summarized as follows: 1. Enabling a more comprehensive transfer of the knowl-edge of the pre-trained expert network to the cloner one. Distilling the knowledge into a more compact net-work also helps to concentrate solely on the features that are distinguishing normal vs. anomalous. 2. Our method has a computationally inexpensive and stable training process compared to the earlier work. 3. Our method allows a real-time and precise anomaly localization based on computing gradients of the dis-crepancy loss concerning the input. 4. Conducting a considerable number of diverse experi-ments and outperforming previous SOTA models by a large margin on many datasets and yet staying com-petitive on the rest. 2.