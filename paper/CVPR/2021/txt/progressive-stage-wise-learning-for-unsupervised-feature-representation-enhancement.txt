Abstract
Unsupervised learning methods have recently shown their competitiveness against supervised training. Typi-cally, these methods use a single objective to train the en-tire network. But one distinct advantage of unsupervised over supervised learning is that the former possesses more
In this variety and freedom in designing the objective. work, we explore new dimensions of unsupervised learning by proposing the Progressive Stage-wise Learning (PSL) framework. For a given unsupervised task, we design multi-level tasks and deï¬ne different learning stages for the deep network. Early learning stages are forced to focus on low-level tasks while late stages are guided to extract deeper information through harder tasks. We discover that by pro-gressive stage-wise learning, unsupervised feature repre-sentation can be effectively enhanced. Our extensive ex-periments show that PSL consistently improves results for the leading unsupervised learning methods. 1.

Introduction
Aiming at learning features from label-free data, unsu-pervised representation learning, including self-supervised learning, is an important problem to study. Many efforts have been made, to bridge the performance gap between supervised and unsupervised learning algorithms. These methods can be roughly divided into two categories: i) handcrafted pretext tasks, that learns data-level invariant features (e.g., jigsaw puzzle [38], image rotation [21], im-age colorization [13]) and ii) contrastive visual representa-tion learning, which learns the similarity and dissimilarity
*Work done while visiting Johns Hopkins University.
â€ Now at Waymo.
â€¡Corresponding author.
Jigsaw Puzzle Task ğ’¢
Permutation Set ğ’
ğ’¢#
ğ’¢"
ğ’¢! i g n n r a e
L e s i w
-e g a t
S e v i s s e r g o r
P n g i s e
D k s a
T l e v e
L
-i t l u
M
Unsupervised/self-supervised Task
ğ‘†#
ğ‘†"
ğ‘†!
Figure 1. We present the framework of the proposed Progres-sive Stage-wise Learning (PSL) algorithm, aiming for improv-ing unsupervised/self-supervised task. We take the jigsaw puz-zle task G for example. We ï¬rst do multi-level task partition
G â†’ {G1, G2, G3} with an increased task complexity and perform progressive stage-wise training for different learning stages of the network. The black arrow denotes forward pass while colored ar-row represents the backward pass of each learning stage (i.e., S1,
S2, and S3). between data pairs [9, 10, 11, 26]. For approaches using pretext tasks, they usually generate pseudo labels based on some data attributes and learn visual features through cor-responding objective functions of the pretext tasks. There-fore, the ï¬nal performance of these approaches is highly re-lated to how the pretext tasks were initially designed. Most pretext tasks are designed heuristically, limiting the qual-ity of learned representation. Contrastive learning methods usually generate positive/negative sample pairs through a set of image transformations and learn visual representa-tion by bringing positive sample pairs closer while pushing negative sample pairs away from each other. The design of the contrastive loss and the conï¬guration of image trans-formations are essential to the quality of the resulting net-9767
works. Those methods have shown great promise in the area of unsupervised learning, achieving state-of-the-art re-sults [25, 16, 40, 2]. Some recent methods show it is possi-ble for unsupervised learned features to surpass supervised learning in some downstream applications [9]. However, performance gaps still exists between unsupervised and su-pervised learning methods in most cases [23]. Therefore, how to fully explore the potential of unsupervised learning and improve the learning quality is a valuable topic.
Instead of designing a new pretext task or a better con-trastive learning loss, we try to look into this problem from a new perspective. As curriculum learning [5] suggests, when dealing with a complex learning target, learning things pro-gressively can be very useful. Indeed, as humans, we learn visual concepts from easy to hard, and from elementary to
ï¬ne-grained. Similarly, learning high-quality feature repre-sentations in an unsupervised manner is a challenging task, and may beneï¬t from such ideas. In this paper, we propose
PSL, a progressive stage-wise learning framework for un-supervised visual representation learning.
As presented in Fig 1, for a given unsupervised learn-ing task G (e.g., the jigsaw puzzle pretext task), we ï¬rst do multi-level task design G â†’ {G1, G2, G3} with an in-creased task complexity. Then, a stage-wise network par-tition is performed to get early/mid/late stages (i.e., S1,
S2 and S3). Each learning stage is assigned with a task, following the principle of easy-to-hard. Then, a stage-wise training is performed. The training of lower stages become much easier as they focus on more simple tasks.
The feature representations learned in upper stages are of better quality because they are trained upon the learning experience of former tasks. Our starting point is to de-sign PSL as a plug-in learning method, which can be ap-plied in any unsupervised learning scenarios under proper multi-level task design. We validate the effectiveness of the proposed PSL framework by evaluating our method on several unsupervised/self-supervised tasks (e.g., the jiasaw puzzle [38] and image rotation [21] pretext task and con-trastive learning [10]) and present results on linear classiï¬-cation, semi-supervised learning and transfer learning.
In general, the contributions of this paper can be summa-rized as follows:
â€¢ PSL creates new dimensions for unsupervised learning research. Speciï¬cally, this includes task series, net-work partitions, and stage-wise training.
â€¢ PSL is design to be a general framework, that can be applied to multiple unsupervised learning tasks be-longing to either pretext tasks or contrastive learning (e.g., jigsaw puzzle, image rotation, and SimCLR).
â€¢ By experiments of downstream applications (e.g., semi-supervised learning, transfer learning), we show that the feature representations learned by PSL consis-tently achieve better quality than the original unsuper-vised task. 2.