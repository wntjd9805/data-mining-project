Abstract
Most existing video text detection methods track texts with appearance features, which are easily inﬂuenced by the change of perspective and illumination. Compared with ap-pearance features, semantic features are more robust cues for matching text instances. In this paper, we propose an end-to-end trainable video text detector that tracks texts based on semantic features. First, we introduce a new character center segmentation branch to extract semantic features, which encode the category and position of char-acters. Then we propose a novel appearance-semantic-geometry descriptor to track text instances, in which se-mantic features can improve the robustness against appear-ance changes. To overcome the lack of character-level an-notations, we propose a novel weakly-supervised character center detection module, which only uses word-level anno-tated real images to generate character-level labels. The proposed method achieves state-of-the-art performance on three video text benchmarks ICDAR 2013 Video, Minetto and RT-1K, and two Chinese scene text benchmarks CA-SIA10K and MSRA-TD500. 1.

Introduction
Video text detection aims to localize and track text in-stances in videos.
It has attracted much attention in re-cent years, due to its wide application in video analysis and multimedia information retrieval. Although previous meth-ods [34, 41, 52] have made signiﬁcant efforts in both text detection and tracking, it is still a challenging task because of motion blur and illumination changes.
Most existing methods [56, 46, 34] treat text detection and tracking separately, where a single frame is detected
ﬁrst, then text tracking methods are applied based on detec-tion results. However, these methods ignore the temporal contexts and the information interaction between detection (a)
A U L A
D LN F OR MATI C A
A U L A
LN FOR M A TI C A
D
A U LE S 1 0 A 1 5
A U LES 01
A 1 5 (b)
Figure 1. (a) The text appearance changes dramatically from dif-ferent perspectives, which makes the tracking branch failed to match text instances. (b) The category and position of characters can help the tracking branch to match text instances more accu-rately. Boxes with the same color belong to the same trajectory, and dots represent the character centers. and tracking. Recently, Yu et al. [52] proposed an end-to-end trainable framework to integrate text detection and tracking, in which the appearance-geometry descriptor is used to track text instances. However, the proposed descrip-tor is mainly based on the text appearance, which is easily inﬂuenced by the change of perspective and illumination.
In contrast to appearance features, semantic features are ro-bust cues for matching text instances. For example, most text instances fail to match due to large perspective changes as shown in Fig 1 (a). However, the character position and category of the same text instance from different perspec-tives are similar. When there are priori semantic features, wrong matching results could be corrected as shown in Fig 1 (b). Although both word-level and character-level annota-tions provide semantic information, character-level annota-tions contain more detailed structure information, which are 1695
more powerful references for text tracking. Unfortunately, character-level annotations of real datasets are too costly.
To generate character-level annotations of real datasets automatically, some methods [1, 44] were proposed in a weakly-supervised learning way. In these methods, a char-acter detector is ﬁrst trained on synthetic datasets, then the trained detector detects characters of real images. There are two main disadvantages of these methods. On one hand, there is a large domain gap between synthetic and real im-ages, which makes the performance of character detector on real images unsatisfactory. On the other hand, the widely used synthetic datasets only focus on English, so it is dif-ﬁcult to transfer these methods to other languages without synthetic datasets.
To exploit the semantic information in video text detec-tion while overcome the lacking of character-level anno-tated data, we propose a semantic-aware video text detec-tion framework as shown in Fig 2, in which character-level annotations are directly generated from word-level anno-tated real datasets. Speciﬁcally, a ConvLSTM [30] block is used to propagate frame-level information, which makes full use of the temporal contexts in videos. Then, a char-acter center segmentation task in the mask head of Mask
R-CNN [9] is designed to encode the position and category of characters as semantic features. Based on appearance features and newly added semantic features, appearance-semantic-geometry descriptors (ASGD) are introduced to robustly represent text instances, which are matched with stored ASGD of previous frames to achieve text track-ing. Although the proposed framework needs character-level annotations, we adopt a sliding-window based text rec-ognizer [38, 49] to detect character centers automatically, which only needs word-level annotated real images to train.
This makes our framework easy to apply to multiple lan-guages, such as Chinese which is typical of large character set. To the best of our knowledge, this is the ﬁrst video text detector to introduce semantic features into text detec-tion and tracking, which only uses word-level annotated real images to generate character-level labels.
Our contributions are in three folds: (1) We propose a novel end-to-end video text detector, which uniﬁes text and character detection, and text tracking. (2) An appearance-semantic-geometry descriptor is proposed, in which the se-mantic features help improve the robustness to appearance changes. (3) Character-level annotations are generated in a weakly-supervised way, which improves the practicabil-ity of our method. The proposed method is effective for both text detection and tracking, and has achieved state-of-the-art performance on three video text datasets ICDAR 2013 Video [16], Minetto [25] and RT-1K [27], and two
Chinese scene text datasets CASIA10K [11] and MSRA-TD500 [47]. 2.