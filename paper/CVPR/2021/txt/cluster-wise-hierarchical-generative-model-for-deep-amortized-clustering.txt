Abstract
ﬂexibility and interpretability.
In this paper, we propose Cluster-wise Hierarchical
Generative Model for deep amortized clustering (CHiGac).
It provides an efﬁcient neural clustering architecture by grouping data points in a cluster-wise view rather than point-wise view. CHiGac simultaneously learns what makes a cluster, how to group data points into clusters, and how to adaptively control the number of clusters. The dedicated cluster generative process is able to sufﬁciently exploit pair-wise or higher-order interactions between data points in both inter- and intra-cluster, which is useful to sufﬁciently mine the hidden structure among data. To efﬁciently min-imize the generalized lower bound of CHiGac, we design an Ergodic Amortized Inference (EAI) strategy by consider-ing the average behavior over sequence on an inner varia-tional parameter trajectory, which is theoretically proven to reduce the amortization gap. A series of experiments have been conducted on both synthetic and real-world data.
The experimental results demonstrated that CHiGac can ef-ﬁciently and accurately cluster datasets in terms of both in-ternal and external evaluation metrics (DBI and ACC). 1.

Introduction
Clustering is a fundamental task in unsupervised ma-chine learning to group similar data points into multiple clusters. Aside from its usefulness in many downstream tasks, clustering is an important tool for visualizing and un-derstanding the underlying structures of datasets, as well as a model for categorization in cognitive science. A plethora of clustering methods have been developed and successfully employed in various ﬁelds, including computer vision [13, 6], natural language processing [18, 26], so-cial network analysis [12], and medical informatics [35].
Among various clustering algorithms [47, 48], probabilistic clustering model has been widely concerned because of its
*Corresponding author
[44].
Probabilistic generative clustering models (or equiva-lently, mixture models) [8] are a staple of statistical mod-eling in which a discrete latent variable is introduced for each observation, indicating its cluster identity. These gen-erative clustering models can be roughly divided into two categories: ﬁnite mixture model [8] and inﬁnite mixture
In recent years, ﬁnite mixture models have model been increasingly applied in unsupervised learning prob-lems with the aid of deep neural networks. The most rel-evant research is that of deep generative clustering mod-els [21, 31, 49], where neural networks are trained to predict the states of latent variables given observations in a deep generative model or probabilistic program [23, 42]. Instead of using an arbitrary prior for the latent variable, these meth-ods adopted ﬁnite mixture prior, such as Gaussian Mixture
Model (GMM) [21, 49]. A ﬁnite mixture model with a ﬁxed number of clusters may ﬁt the given data set well, however, it may be sub-optimal to use the same number of clusters if more data comes under a slightly changed distribution. It would be ideal if the clustering models can ﬁgure out the unknown number of clusters automatically.
Alternatively, inﬁnite mixture model is the application of nonparametric Bayesian techniques to mixture modeling, which allows for the automatic determination of an appro-priate number of mixture components. The prior distribu-tion can be speciﬁed in terms of a data point sequential pro-cess called Dirichlet process (e.g., Chinese restaurant pro-cess (CRP)), where the number of clusters can arbitrarily grow to better accommodate data as needed. To approxi-mate the corresponding inﬁnite mixture posterior, recently, deep amortized clustering [37, 38, 29, 39, 50] methods are proposed in a black-box fashion [45]. Speciﬁcally, they make use of neural networks for amortized inferring the cluster assignments and parameters, which is ﬂexible to de-ﬁne the clusters. To adaptively determine the number of clusters, however, they have to construct the non-parametric prior, which largely depends on the sequential data point modeling. For large-scale dataset, this process will be time-15109
consuming and difﬁcult to converge.
In this paper, we build on these prior works and pro-pose a Cluster-wise Hierarchical Generative model for deep amortized clustering (CHiGac), which aims to learn non-parametric deep Bayesian posterior in a cluster-wise view.
In other words, CHiGac targets on generating clusters rather than generating data points (all existing generative cluster-ing methods adopted the later). This cluster generation pro-cess has two good facets. One is that the whole genera-tion process is efﬁcient, because it depends on the number of clusters (K) rather than the number of data points (N ), where K ≪ N . The other is that inter-cluster and intra-cluster structure can be sufﬁciently exploited during the learning process, because each cluster is generated accord-ing to the previous clusters and the current left data points.
To efﬁciently approximate the non-parametric Bayesian posterior for CHiGac, we propose an Ergodic Amortized
Inference (EAI) algorithm by considering the average be-havior over sequence on an inner variational parameter tra-jectory. EAI is theoretically proven to reduce the amortiza-tion gap and provide ﬂexible parameterization for the neu-ral amortized inference model. To illustrate the superior of the proposed method, we perform experiments on both syn-thetic data and real-world data in terms of clustering perfor-mance and inference optimization performance. 2.