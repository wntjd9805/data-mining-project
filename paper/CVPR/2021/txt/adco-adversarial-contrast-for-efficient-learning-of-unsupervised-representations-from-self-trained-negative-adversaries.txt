Abstract
Contrastive learning relies on constructing a collection of negative examples that are sufﬁciently hard to discrim-inate against positive queries when their representations are self-trained. Existing contrastive learning methods ei-ther maintain a queue of negative samples over minibatch-es while only a small portion of them are updated in an iteration, or only use the other examples from the curren-t minibatch as negatives. They could not closely track the change of the learned representation over iterations by up-dating the entire queue as a whole, or discard the useful information from the past minibatches. Alternatively, we present to directly learn a set of negative adversaries play-ing against the self-trained representation. Two players, the representation network and negative adversaries, are alter-nately updated to obtain the most challenging negative ex-amples against which the representation of positive queries will be trained to discriminate. We further show that the negative adversaries are updated towards a weighted com-bination of positive queries by maximizing the adversari-al contrastive loss, thereby allowing them to closely track the change of representations over time. Experiment results demonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves superior performances (a top-1 ac-curacy of 73.2% over 200 epochs and 75.7% over 800 e-pochs with linear evaluation on ImageNet), but also can be pre-trained more efﬁciently with much shorter GPU time and fewer epochs. The source code is available at https:
//github.com/maple-research-lab/AdCo. 1.

Introduction
Learning visual representations in an unsupervised fash-ion [1, 31, 10, 23, 24, 39] has attracted many attentions as it greatly reduces the cost of collecting a large volume of
∗Q. Hu and X. Wang made an equal contribution to performing experi-ments while being mentored by G.-J. Qi.
†Corresponding author: W. Hu and G.-J. Qi (guojunq@gmail.com).
G.-J. Qi conceived the idea, formulated the method, and wrote the paper. labeled data to train deep networks. Signiﬁcant progresses have also been made to reduce the performance gap with the fully supervised models. Among them are a family of con-trastive learning methods [19, 7, 38, 21, 29, 3, 41, 36, 20] that self-train deep networks by distinguishing the represen-tation of positive queries from their negative counterparts.
Depending on how the negative samples are constructed, t-wo large types of contrastive learning approaches have been proposed in literature [19, 7]. These negative samples play a critical role in contrastive learning since the success in self-training deep networks relies on how positive queries can be effectively distinguished from negative examples.
Speciﬁcally, one type of contrastive learning methods explicitly maintains a queue of negative examples from the past minibatches. For example, the Momentum Contrast (MoCo) [19] iteratively updates an underlying queue with the representations from the current minibatch in a First-In-First-Out (FIFO) fashion. However, only a small portion of oldest negative samples in the queue would be updated, which could not continuously track the rapid change of the feature representations over iterations. Even worse, the mo-mentum update of key encoders, which is necessary to sta-bilize the negative queue in MoCo, could further slow down the track of the representations. Consequently, this would inefﬁciently train the representation network, since partial-ly updated negatives may not cover all critically challenging samples thus far that ought to be distinguished from positive queries to train the network.
Alternatively, another type of contrastive learning meth-ods [7] abandons the use of such a separate queue of nega-tive examples. Instead, all negative examples come from the current minibatch, and a positive query would be retrieved by distinguishing it from the other examples in the mini-batch. However, it discards the negative examples from the past minibatches, and often requires a much larger size of minibatch so that a sufﬁcient number of negative samples are available to train the representation network by discrim-inating against positive queries. This incurs heavy memory 11074
and computing burden to train over each minibatch for this type of contrastive learning.
In this paper, we are motivated to address the aforemen-tioned drawbacks, and the objective is twofold. First, we wish to construct a set of negative examples that can contin-uously track the change of the learned representation rather than updating only a small portion of them. In particular, it will update negative samples as a whole by making them sufﬁciently challenging to train a representation network more efﬁciently with fewer epochs. On the other hand, it will retain the discriminative information from the past it-erations without depending on a much larger size of mini-batch to train the network [7]. We will show that in the pro-posed model, the negative examples are directly trainable so that they can be integrated as a part of the underlying network and trained end-to-end together with the represen-tation. Thus, the trainable negatives are analogous to the ad-ditional network component in other self-supervised models without involving negative examples, such as the prediction
MLP of the BYOL [17] that also needs to be trained end-to-end. More discussions on whether we still need negative examples can be found in Appendix C.
Particularly, we will present an Adversarial Contrast (AdCo) model consisting of two adversarial players. One is a backbone representation network that encodes the rep-resentation of input samples. The other is a collection of negative adversaries that are used to discriminate against positive queries over a minibatch. Two players are alter-nately updated. With a ﬁxed set of negative adversaries, the network backbone is trained by minimizing the con-trastive loss of mistakenly assigning positive queries to neg-ative samples as in the conventional contrastive learning.
On the other hand, the negative adversaries are updated by maximizing the contrastive loss, which pushes the negative samples to closely track the positive queries over the cur-rent minibatch. This results in a minimax problem to ﬁnd an equilibrium as its saddle point solution. Although there is no theoretical guarantee of convergence, iterative gradi-ent updates to the network backbone and the negative ad-versaries work well in practice, which has been observed in many other adversarial methods [16, 32, 35, 40]. We will also show that the derivative of the contrastive loss wrt the negative adversaries reveals how they are updated towards a weighted combination of positive queries, and gives us an insight into how the AdCo focuses on low-density queries compared with an alternative model.
The experiment results not only demonstrate the AdCo has the superior performance on downstream tasks, but also verify that it can train the unsupervised networks with fewer epochs by updating the negative adversaries more efﬁcient-ly. For example, with merely 10 epochs of pretraining, the
AdCo has a top-1 accuracy of 44.4%, which is almost 5% higher than that of the MoCo v2 pretrained over the same number of epochs with the linear evaluation on the ResNet-50 backbone on ImageNet. It also greatly outperforms the
MoCHi [22] by 4.1% in top-1 accuracy over 800 epochs that enhances the MoCo v2 with mixed hard negatives, showing its effectiveness in constructing more challenging negative adversaries in a principled fashion to pretrain the represen-tation network.
Moreover, the AdCo achieves a record top-1 accuracy of 75.7% over 800 epochs compared with the state-of-the-art BYOL (74.3%) and SWAV (75.3%) models pretrained for 800 ∼ 1, 000 epochs. This is obtained with the same or even smaller amount of GPU time than the two top-performing models. Indeed, the AdCo is computationally efﬁcient with a negligible cost of updating negative adver-saries, making it an attractive paradigm of contrast mod-el having higher accuracies over fewer epochs with no in-crease in the computing cost.
The remainder of the paper is organized as follows. We will review the related works in Section 2, and present the proposed approach in Section 3. By comparing the pro-posed AdCo with an alternative form of adversarial con-trastive loss, we will reveal how AdCo focuses on low-density queries whose representations have not been well captured in Section 4. We will conduct experiments in Sec-tion 5 to demonstrate its superior performance in multiple tasks. Finally, we will conclude in Section 6. 2.