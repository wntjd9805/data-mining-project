Abstract
The dual-pixel (DP) hardware works by splitting each pixel in half and creating an image pair in a single snap-shot. Several works estimate depth/inverse depth by treat-ing the DP pair as a stereo pair. However, dual-pixel dis-parity only occurs in image regions with the defocus blur.
The heavy defocus blur in DP pairs affects the performance of matching-based depth estimation approaches. Instead of removing the blur effect blindly, we study the formation of the DP pair which links the blur and the depth informa-tion. In this paper, we propose a mathematical DP model which can beneﬁt depth estimation by the blur. These ex-plorations motivate us to propose an end-to-end DDDNet (DP-based Depth and Deblur Network) to jointly estimate the depth and restore the image. Moreover, we deﬁne a re-blur loss, which reﬂects the relationship of the DP image formation process with depth information, to regularise our depth estimate in training. To meet the requirement of a large amount of data for learning, we propose the ﬁrst DP image simulator which allows us to create datasets with DP pairs from any existing RGBD dataset. As a side contri-bution, we collect a real dataset for further research. Ex-tensive experimental evaluation on both synthetic and real datasets shows that our approach achieves competitive per-formance compared to state-of-the-art approaches. 1.

Introduction
The Dual-Pixel (DP) sensor has been used by DSLR (digital single-lens reﬂex camera) and smartphone cameras to aid focusing. Though the DP sensor is designed for auto-focus [32, 16, 15], it is used in applications such as, depth estimation [10, 29, 43], defocus deblurring [1], reﬂection removal [30], and shallow Depth-of-Field (DoF) images synthesis [36]. In this paper, we model the imaging process of the DP sensor theoretically, and show its effectiveness for simultaneous depth estimation and image deblurring.
∗Equal contribution (b) Ours (a) Inputs (c) GT
Figure 1. The pipeline of our approach. (a) The input of our
DDDNet. A red line in each DP image is drawn to indicate a small shift between the left and right sub-aperture views. (b) Our de-blurred image and estimated inverse depth map. (c) The Ground-Truth (GT) sharp image and inverse depth map. The input DP pair is from a real-world DP sensor, or generated by our simulator.
A DP camera simultaneously captures two images, one formed from light rays passing through the right half of the aperture, and one from those passing through the left half of the aperture. Because of the displacement of these two half-apertures, the two images form a stereo pair. The primary role of DP cameras is to enable auto-focus. The set of points in the world that are exactly in focus forms a plane parallel to the lens (perpendicular to the axis of the camera), and points that lie on this plane are both in focus, and imaged with zero-disparity in the two images. Points that lie further away from the camera are imaged with positive disparity in the two images and those that are closer have negative disparity. By detecting and modelling these disparities, it is possible to compute scene depth, as in standard stereo imaging. (In auto-focus these disparities are used to move the sensor, or the lens, to focus on any part of the scene.)
At the same time, points other than those lying on the in-focus plane will be signiﬁcantly out of focus, which raises the possibility of using depth-from-defocus techniques to estimate depth in the scene. This out-of-focus effect is more signiﬁcant than with ordinary stereo pairs, making stereo matching potentially more challenging for DP cam-eras. However, because of the small base-line, between the 4340
two half-apertures, occlusion effects are minimal.
Thus, computing depth from a DP camera can be seen as an out-of-focus stereo estimation problem. This paper aims to combine depth-from-disparity and depth-from-defocus approaches in a single network, demonstrating the advan-tage of modelling them both simultaneously.
To accomplish this, we study the imaging process of a
DP pair and provide a mathematical model handling the in-trinsic problem of depth from defocus blur for a DP pair.
Our method can jointly recover an all-in-focus image and estimate the depth of a scene (see Fig. 1). Our contributions are summarised as follows: 1. We propose a theoretical DP model to explicitly deﬁne the relationship between depth, defocus blur and all-in-focus image; 2. We design an end-to-end DP-based Depth and Deblur
Network (DDDNet) to jointly estimate the depth map and restore the sharp image; 3. We formulate a reblur loss based on our DP model which is used to regularise depth estimate in training; 4. We create a DP image simulator, which enables us to create DP datasets from any RGB-D dataset; 5. We collect a real dataset to stimulate further research.
Extensive experimental evaluation on both synthetic and real data shows that our approach achieves competitive per-formance compared to state-of-the-art approaches. 2.