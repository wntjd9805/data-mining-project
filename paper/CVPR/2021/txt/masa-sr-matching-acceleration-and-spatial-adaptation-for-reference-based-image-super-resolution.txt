Abstract
Reference-based image super-resolution (RefSR) has shown promising success in recovering high-frequency de-tails by utilizing an external reference image (Ref). In this task, texture details are transferred from the Ref image to the low-resolution (LR) image according to their point- or patch-wise correspondence. Therefore, high-quality cor-respondence matching is critical. It is also desired to be computationally efﬁcient. Besides, existing RefSR methods tend to ignore the potential large disparity in distributions between the LR and Ref images, which hurts the effective-ness of the information utilization. In this paper, we pro-pose the MASA network for RefSR, where two novel mod-ules are designed to address these problems. The proposed
Match & Extraction Module signiﬁcantly reduces the com-putational cost by a coarse-to-ﬁne correspondence match-ing scheme. The Spatial Adaptation Module learns the dif-ference of distribution between the LR and Ref images, and remaps the distribution of Ref features to that of LR fea-tures in a spatially adaptive way. This scheme makes the network robust to handle different reference images. Exten-sive quantitative and qualitative experiments validate the effectiveness of our proposed model. 1.

Introduction
Single image super-resolution (SISR) is a fundamen-tal computer vision task that aims to restore a high-resolution image (HR) with high-frequency details from its low-resolution counterpart (LR). Progress of SISR in re-cent years is based on deep convolutional neural networks (CNN) [3, 11, 12, 13, 16, 31]. Nevertheless, the ill-posed nature of SISR problems makes it still challenging to re-cover high-quality details.
In this paper, we explore reference-based super-resolution (RefSR), which utilizes an external reference im-age (Ref) to help super-resolve the LR image. Reference images usually contain similar content and texture with the
*Equal contribution
LR
Bicubic
SRNTT
Ref
TTSR
MASA(Ours)
Figure 1: Visual comparison of ×4 SR results. Our
MASA method generates more appealing texture details than the two leading RefSR methods, i.e., SRNTT [33] and
TTSR [30].
LR image. They can be acquired from web image search or captured from different viewpoints. Transferring ﬁne details to the LR image can overcome the limitation of
SISR and has demonstrated promising performance in re-cent work of [35, 23, 34, 33, 30].
Previous methods aimed at designing various ways to handle two critical issues in this task: a) Correspond use-ful content in Ref images with LR images. b) Transfer fea-tures from Ref images to facilitate HR image reconstruc-tion. To address the ﬁrst issue, methods perform spatial alignment between the Ref and LR images [35, 23] us-ing optical ﬂow or deformable convolutions [2, 36]. These alignment-based methods face challenges in, e.g., ﬁnding long-distance correspondence. Other methods follow patch matching [34, 33, 30] in the feature space. State-of-the-art methods generally perform dense patch matching, leading to very high computational cost and large memory usage.
For the second issue, our ﬁnding is that even if LR and
Ref images share similar content, color and luminance may 6368
differ. Previous methods directly concatenate the LR fea-tures with the Ref ones and fuse them in convolution layers, which is not optimal.
To address the above problems, we propose a RefSR method called MASA-SR, which improves patch match-ing and transfer. The design of MASA has several ad-vantages. First, the proposed Match & Extraction Module (MEM) performs correspondence matching in a coarse-to-ﬁne manner, which largely reduces the computational cost while maintaining the matching quality. By leveraging the local coherence property of natural images, for each patch in the LR feature maps, we shrink its search space from the whole Ref feature map to a speciﬁc Ref block.
Second, the Spatial Adaptation Module is effective in handling the situations where there exists large disparity in color or luminance distribution between the LR and Ref im-ages. It learns to remap the distribution of the Ref features to LR ones in a spatially adaptive way. Useful information in the Ref features thus can be transferred and utilized more effectively.
To the best of our knowledge, our model achieves state-of-the-art performance for the RefSR task. Our contribu-tions are as follows.
• The proposed Match & Extraction Module signiﬁ-cantly reduces the computational cost of correspon-dence matching in the deep feature space. Our results show that a two-orders-of-magnitude reduction mea-sured in FLOPS is achieved.
• The proposed Spatial Adaptation Module is robust to
Ref images with different color and luminance distri-butions. It enables the network to better utilize useful information extracted from Ref images. 2.