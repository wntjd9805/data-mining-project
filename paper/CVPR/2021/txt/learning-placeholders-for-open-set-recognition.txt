Abstract
Traditional classiﬁers are deployed under closed-set set-ting, with both training and test classes belong to the same set. However, real-world applications probably face the in-put of unknown categories, and the model will recognize them as known ones. Under such circumstances, open-set recognition is proposed to maintain classiﬁcation perfor-mance on known classes and reject unknowns. The closed-set models make overconﬁdent predictions over familiar known class instances, so that calibration and thresholding across categories become essential issues when extending to an open-set environment. To this end, we proposed to learn
PlaceholdeRs for Open-SEt Recognition (PROSER), which prepares for the unknown classes by allocating placehold-ers for both data and classiﬁer. In detail, learning data placeholders tries to anticipate open-set class data, thus transforms closed-set training into open-set training. Be-sides, to learn the invariant information between target and non-target classes, we reserve classiﬁer placeholders as the class-speciﬁc boundary between known and unknown.
The proposed PROSER efﬁciently generates novel class by manifold mixup, and adaptively sets the value of reserved open-set classiﬁer during training. Experiments on various datasets validate the effectiveness of our proposed method. 1.

Introduction
Recent years have witnessed the rapid development of supervised learning, aiming to obtain the knowledge of ﬁnite known classes. During the testing process, the well-trained model matches an incoming instance to the class with the highest posterior probability. However, this closed-world as-sumption comes to an end when the test set includes unseen categories [16, 2, 42, 35]. Since it is impossible to cover all classes in the world as training set [19, 33], the model would treat all novel category instances as known ones. As a result, the performance decays, which is unbearable in real-world
†Correspondence to: Han-Jia Ye (yehj@lamda.nju.edu.cn)
Figure 1. The drawback of threshold-based open-set recognition.
It deﬁnes novel class space if conﬁdence below a ﬁxed threshold.
Bird and cat are known classes, while vehicle and tiger are open-set classes in the left and right ﬁgures, respectively. Since the distributions of the vehicle and tiger categories differ, it is hard to rely on a single threshold to recognize unknown classes with diverse characteristics. The same threshold which well separates vehicles apart from known classes is not suitable for tigers. applications. Open-set recognition [27, 3, 38, 26] is thus proposed to conduct classiﬁcation on known instances while at the same time detect those from unknown classes.
Facing the unknown input of novel categories, an intu-itive way to separate known and unknown instances is to exert a threshold over the output probability [10]. It assumes the model produces a higher probability for known classes than unknowns. However, deep learning methods tend to overﬁt the training instances and produce overconﬁdent pre-dictions [27, 8, 9]. As a result, the model would output a high probability even for an unknown class instance, making the threshold hard to tune. Besides, the class-compositions are diverse, as shown in Figure 1. Since the semantic in-formation of known classes differs in different tasks, it is hard to acquire an optimal threshold that suits all open-set tasks. Consequently, it is urgent to calibrate closed-set classi-ﬁers. Other methods try to foresee the distributions of novel classes and calibrate the output with open-set probability.
[6] proposed G-OpenMax, which utilizes GAN to generate unknown samples for training novel classiﬁer. [22] tried to generate images lying between decision boundaries as counterfactual instances. [26] combined self-supervision and augment input with generated open-set samples, which yields high disparity. These methods try to anticipate novel class distributions with generative models, and transform the closed-set training into open-set training. 4401
The aim to boost open-set recognition can be summarized as a calibration problem [8, 34]. Firstly, to make the closed-set model prepare for unknown classes, data placeholders of the novel class should be augmented and transform open-set into closed-set. Secondly, to better separate known and unknown instances, overconﬁdent predictions should be cali-brated by reserving classiﬁer placeholders for novel classes.
Motivated by the problems above, we proposed to learn
PlaceholdeRs for Open-SEt Recognition (PROSER), aiming to calibrate open-set classiﬁers from two aspects. In detail, we augment the closed-set classiﬁer with an extra classiﬁer placeholder, which stands for the class-speciﬁc threshold between known and unknown. We reserve the placeholder for open-set classes to acquire the invariant information be-tween target and non-target classes. Besides, to efﬁciently anticipate the distribution of novel classes, we consider gen-erating data placeholders, which mimic open-set categories with a limited complexity cost. Consequently, we can trans-form closed-set classiﬁers into open-set ones, and adaptively predicts the class-speciﬁc threshold during testing. Experi-ments on various datasets validate the effectiveness of our proposed method on unknown detection and open-set recog-nition problems. Additionally, the visualization on decision boundaries indicates PROSER learns adaptive threshold for different class combinations.
In the following sections, we start with a brief review of related work, and then give the proposed PROSER and experiment results. After that, we conclude the paper. 2.