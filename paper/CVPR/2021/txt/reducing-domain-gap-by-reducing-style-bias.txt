Abstract
Convolutional Neural Networks (CNNs) often fail to main-tain their performance when they confront new test domains, which is known as the problem of domain shift. Recent stud-ies suggest that one of the main causes of this problem is
CNNs‚Äô strong inductive bias towards image styles (i.e. tex-tures) which are sensitive to domain changes, rather than contents (i.e. shapes). Inspired by this, we propose to reduce the intrinsic style bias of CNNs to close the gap between do-mains. Our Style-Agnostic Networks (SagNets) disentangle style encodings from class categories to prevent style biased predictions and focus more on the contents. Extensive ex-periments show that our method effectively reduces the style bias and makes the model more robust under domain shift.
It achieves remarkable performance improvements in a wide range of cross-domain tasks including domain generaliza-tion, unsupervised domain adaptation, and semi-supervised domain adaptation on multiple datasets.1 1.

Introduction
Despite the huge success of Convolutional Neural Net-works (CNNs) fueled by large-scale training data, their per-formance often degrades signiÔ¨Åcantly when they encounter test data from unseen environments. This phenomenon, known as the problem of domain shift [41], comes from the representation gap between training and testing domains.
For a more reliable deployment of CNNs to ever-changing real-world scenarios, the community has long sought to make
CNNs robust to domain shift under various problem settings such as Domain Generalization (DG) [29, 9, 4, 8], Unsuper-vised Domain Adaptation (UDA) [33, 11, 46, 34, 42], and
Semi-Supervised Domain Adaptation (SSDA) [7, 50, 44].
In stark contrast to the vulnerability of CNNs against domain shift, the human visual recognition system general-izes incredibly well across domains. For example, young children learn many object concepts from pictures, but they naturally transfer their knowledge to the real world [10].
*Equal contribution 1Code: https://github.com/hyeonseobnam/sagnet
Similarly, people can easily recognize objects in cartoons or paintings even if they have not seen the same style of an image before. Where does such a difference come from?
A recent line of studies has revealed that standard CNNs have an inductive bias far different from human vision: while humans tend to recognize objects based on their contents (i.e. shapes) [27], CNNs exhibit a strong bias towards styles (i.e. textures) [1, 14, 21]. This may explain why CNNs are intrin-sically more sensitive to domain shift because image styles are more likely to change across domains than the contents.
Geirhos et al. [14] supported this hypothesis by showing that
CNNs trained with heavy augmentation on styles become more robust against various image distortions. Research on CNN architectures [40, 28] has also demonstrated that adjusting the style information in CNNs helps to address multi-domain tasks.
In this paper, we experimentally analyze the relation be-tween CNNs‚Äô inductive bias and representation gap across domains, and exploit this relation to address domain shift problems. We propose Style-Agnostic Networks (SagNets) which effectively improve CNNs‚Äô domain transferability by controlling their inductive bias, without directly reducing domain discrepancy. Our framework consists of separate content-biased and style-biased networks on top of a feature extractor. The content-biased network is encouraged to fo-cus on contents by randomizing styles in a latent space. The style-biased network is led to focus on styles in the opposite way, against which the feature extractor adversarially makes the styles incapable of discriminating class categories. At test time, the prediction is made by the combination of the feature extractor and the content-biased network, where the style bias is substantially reduced.
We show that there exists an apparent correlation between
CNNs‚Äô inductive bias and their ability to handle domain shift: reducing style bias reduces domain discrepancy. Based on this property, SagNets make signiÔ¨Åcant improvements in a wide range of domain shift scenarios including DG, UDA, and SSDA, across several cross-domain benchmarks such as
PACS [29], OfÔ¨Åce-Home [47], and DomainNet [42].
Our method is orthogonal to the majority of existing do-main adaptation and generalization techniques that utilize 8690
Input ùê±
Content-biased prediction (target)
ùê≥
Feature
Extractor
ùêÜ"
Style Randomization content
AdaIN
Content-Biased
Network ùêÜ#
Shared style
Adversarial
Learning style
Random
Interpolation
Feature
Extractor
ùêÜ"
ùê≥‚Ä≤ content
AdaIN
Content Randomization
Style-Biased
Network ùêÜ$ cat
ùê≤
Style-biased
Prediction (auxiliary)
Randomly chosen image ùê±‚Ä≤
Figure 1: Our Style-Agnostic Network (SagNet) reduces style bias to reduce domain gap. It consists of three sub-networks‚Äîa feature extractor, a content-biased network, and a style-biased network‚Äîthat are jointly trained end-to-end. The content-biased network is led to focus on the content of the input via style randomization (SR), where the style of the input is replaced by an arbitrary style through AdaIN [24]. Conversely, the style-biased network is led to focus on the style by content randomization (CR), while an adversarial learning makes the feature extractor generate less style-biased representation. domain information for training (e.g. aligning the source and target domains [11, 34, 44]). In other words, SagNets only control the intrinsic bias of CNNs without even requir-ing domain labels nor multiple domains. This approach is not only scalable to more practical scenarios where domain boundaries are unknown or ambiguous, but also able to com-plement existing methods and bring additional performance boosts as demonstrated in our extensive experiments. 2.