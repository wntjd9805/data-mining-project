Abstract
Semi-supervised learning is a useful tool for image seg-mentation, mainly due to its ability in extracting knowledge from unlabeled data to assist learning from labeled data.
This paper focuses on a popular pipeline known as self-learning, where we point out a weakness named lazy mim-icking that refers to the inertia that a model retains the pre-diction from itself and thus resists updates. To alleviate this issue, we propose the Asynchronous Teacher-Student Op-timization (ATSO) algorithm that (i) breaks up continual learning from teacher to student and (ii) partitions the un-labeled training data into two subsets and alternately uses one subset to ﬁne-tune the model which updates the labels on the other. We show the ability of ATSO on medical and natural image segmentation. In both scenarios, our method reports competitive performance, on par with the state-of-the-arts, in either using partial labeled data in the same dataset or transferring the trained model to an unlabeled dataset. 1.

Introduction
Semantic segmentation plays an important role in im-age understanding. Recently, the fast development of deep learning [17] provides a powerful tool for dense image prediction [6, 20], but for many scenarios such as medi-cal image analysis, data annotation is often expensive but there may exist abundant unlabeled data.
In addition, it is a common requirement of transferring a segmentation model from one domain to another without extra annota-tions. Both scenarios fall into the area of semi-supervised learning which focuses on learning from both labeled data and unlabeled data while the labeled part is often smaller.
An effective pipeline is known as self-learning, in which an initial model is trained on the labeled part (training set) and ﬁne-tuned on the unlabeled part (reference set) with the pseudo labels generated by itself. We refer to this pipeline as teacher-student optimization, a variant of knowledge dis-tillation [14] that has straightforward applications on medi-cal image analysis [47].
However, we notice a factor that harms the efﬁciency of utilizing unlabeled data. In the self-learning procedure, the similarity between the teacher and student, two variants of the target model, tend to increase. Consequently, the super-vision that the student model obtains from the pseudo labels becomes weak and the learning process quickly arrives at a plateau. We call this phenomenon lazy mimicking: the teacher model stores knowledge in the pseudo labels for the student model to learn; once a prediction error appears, it is likely to persist throughout the self-learning procedure; therefore, inaccuracy accumulates and ﬁnally downgrades the quality of the generated pseudo labels. We ﬁnd that lazy mimicking quantitatively reﬂects in that the pseudo la-bels are not improved during the learning process – in other words, the accuracy on the reference set stops growing but the model itself does not know. From the viewpoint of op-timization, lazy mimicking is caused by the self-learning process gradually pushing the teacher and student models, as a whole, towards a local optimum.
To break up the optimization trap and alleviate lazy mim-icking, we present the asynchronous teacher-student op-timization (ATSO) algorithm. ATSO puts forward two sim-ple modiﬁcations beyond the self-learning pipeline to break up the chain of ‘error inheritance’. First, we switch off continual learning and start each generation from the same initialized model. Second, we prevent using the pseudo la-bels generated by a teacher model to supervise its direct stu-dent, which involves partitioning the reference set into two subsets – in each round of teacher-student optimization, we generate the pseudo labels on any subset based on a teacher model that was not trained on the same set of data. As we 11235
shall see in experiments, both strategies are helpful to im-prove the quality of the pseudo labels and, consequently, boost the ﬁnal segmentation accuracy.
We evaluate ATSO on two kinds of segmentation data, medical images and autonomous driving images. For medi-cal analysis, we use the NIH and MSD datasets for pancreas segmentation from CT scans. ATSO shows promising seg-mentation results using 10% or 20% of labeled data of NIH, surpassing the previous state-of-the-arts and approaching the fully-supervised upper-bound. ATSO also works well in transferring a model trained on NIH to MSD that is com-pletely unlabeled. For autonomous driving, two popular datasets named Cityscapes and Mapillary are investigated.
Compared to the state-of-the-arts that used strong data aug-mentations on Cityscapes, ATSO produces competitive seg-mentation accuracy with just basic-level augmentations. In transferring a model from Cityscapes and Mapillary, ATSO makes use of super-class pseudo labels to avoid the instabil-ity of training, and achieves satisfying results.
In summary, the contribution of this paper is two fold.
First, this is the ﬁrst work to reveal the lazy mimicking phenomenon in the self-learning pipeline. Second, the
ATSO pipeline is presented that alleviates the above burden and improves semi-supervised image segmentation. Third, the idea of super-class pseudo labels is helpful to stabilize knowledge distillation in multi-class segmentation tasks. 2.