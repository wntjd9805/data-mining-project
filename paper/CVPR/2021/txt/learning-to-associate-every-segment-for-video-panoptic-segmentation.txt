Abstract
Temporal correspondence - linking pixels or objects across frames - is a fundamental supervisory signal for the video models. For the panoptic understanding of dynamic scenes, we further extend this concept to every segment.
Speciﬁcally, we aim to learn coarse segment-level matching and ﬁne pixel-level matching together. We implement this idea by designing two novel learning objectives. To validate our proposals, we adopt a deep siamese model and train the model to learn the temporal correspondence on two different levels (i.e., segment and pixel) along with the target task.
At inference time, the model processes each frame indepen-dently without any extra computation and post-processing.
We show that our per-frame inference model can achieve new state-of-the-art results on Cityscapes-VPS and VIPER datasets. Moreover, due to its high efﬁciency, the model runs in a fraction of time (3×) compared to the previous state-of-the-art approach. 1.

Introduction
A holistic understanding of a video requires pixel-level information of different semantics, object instances, back-ground stuffs as well as their temporal changes. Despite being very challenging, having such video understanding is crucial for various vision applications such as autonomous driving, robot control, video editing, and augmented real-ity. Video semantic segmentation has been regarded as one of the representative proxies for this ambitious goal. The community has proposed a large number of learning-based approaches under two main research directions of improving accuracy [9,11,12,21,31,50] and efﬁciency [20,27,30,38,51].
While there has been a ﬂurry of advances in model designs, signiﬁcantly less effort has been made to the training ob-jectives. The main bottleneck of developing a useful super-vision signal is the scarcity of video annotations available.
Most video segmentation benchmarks [7, 49] provide anno-tations for only a single frame per video clip, which limits these annotations still to an image level.
In line with the growing importance of the ﬁeld and to
Figure 1: Overview of our approach. Temporal correspondence learning is fundamental for video understanding. In this work, we extend this concept to every segment in a video. Speciﬁcally, given a pair of frames, It and It+δ, a siamese model, f , learns to associate every segment at two different levels jointly: segment-level via contrastive loss and pixel-level via tube-matching loss. meet the high demand for video labels, Kim et al. [24] re-cently presented a new challenging dense video understand-ing problem called video panoptic segmentation (VPS) and presented a pair of video annotations: Cityscapes-VPS and
VIPER datasets. The problem uniﬁes the existing video semantic segmentation [11, 38, 50] and video instance seg-mentation [2, 48]. It aims at a simultaneous prediction of object classes, masks, instance id associations, and seman-tic segmentation for all pixels in a video. The two VPS benchmarks provide real-world and simulation-based video datasets, opening up the possibilities to probe new video-speciﬁc training signals.
Identifying temporal correspondence - “what went where” - in a video is a crucial requirement of robust visual reasoning in space and time. Numerous methods for learn-ing temporal correspondences, from pixel-level to object-2705
level, have been developed so far, e.g., optical-ﬂow esti-mation [18, 29, 39] and object tracking [3, 40, 41, 43, 44].
However, most of the previous methods aim at addressing a single-level of temporal correspondence at a time, and less attempt has been made to solve different levels of temporal correspondences jointly. For the dense panoptic segmen-tation of a video, we argue that the model representations should support reasoning at various levels of temporal cor-respondences. At the same time, this should be considered for every individual segment 1 in a video. To this end, we train a video segmenter that simultaneously learns correspon-dences across frames at both segment-level and pixel-level (see Fig. 1). In the following, we describe these two impor-tant views in turn: 1) Segment-level correspondence learning: Let’s imagine we assign an id tag for each segment in a video. Naturally, we provide the same id tag for the same segment over time.
We formulate this concept as a graph matching problem. In practice, we construct the graph from video frames, where nodes encode segments in each frame and edges are afﬁni-ties between them. We then aim to learn features such that strong edges represent temporal correspondences. This is achieved through the contrastive learning [14], which can encourage pairs of segment embedding to have strong edges if they are temporally associated or weak edges otherwise.
Also, as time goes, the segments in a video undergo dynamic appearance changes such as deformation, occlusion, and per-spective distortion. Therefore, without composing multiple handcrafted data augmentations [1, 4–6, 13, 15, 32], our for-mulation of matching the same segment at temporally distant frames naturally leads to learning segment representations invariant in such visual distortions. 2) Pixel-level correspondence learning: The optical ﬂow provides dense pixel-level correspondences where each pixel in the current frame goes in the next frame. The photometric loss is often used for learning this. Recent works [9, 30] gen-eralize the photometric loss to the logit domain, i.e., segmen-tation output, with the motivation of predicting temporally consistent labels. However, as warping loss often assumes independence between each pixel, higher-level correlations or structures over pixels are hardly modeled. To alleviate this issue, we introduce “tubes” formed by linking the seg-ment masks along the time axis. The model then learns to minimize the mismatch between the prediction tubes and the ground-truth tubes, globally optimizing the entire chain of intermediate mask predictions. This constraint allows the model to capture ﬁne-grained changes in segments, e.g., shape, boundary, and motion tendencies over time. Empiri-cally, we show that the proposed mask tube matching loss performs better than the warping loss in learning an accurate segmentation model. 1Throughout the paper, we use the term segment to denote the region of both foreground things and background stuffs in the video.
We aim to learn these two different levels of temporal correspondences jointly. To do so, we propose an efﬁcient framework. Speciﬁcally, we use a deep siamese model and train with a pair of frames, where a neighbor reference frame in a pair is randomly selected with a time gap relative to the current target frame. Our model can encode strong temporal consistency into the features during training without using any heavy feature aggregation [50] or fusion [45] operations, resulting in an efﬁcient yet strong video model. Without bells and whistles, our model achieves new state-of-the-art results on Cityscapes-VPS and VIPER datasets while running in a fraction of time compared to the previous state-of-the-art approach [24]. We summarize the contributions of this paper as follows. 1. We generalize the temporal correspondence learning to every segment in a video. We present to learn coarse segment-level matching and ﬁne pixel-level matching together. We achieve this by designing two novel objec-tive functions with an efﬁcient learning framework. 2. We propose a new supervised2 contrastive learning method to learn the temporal correspondences in a video. In particular, we aim to maximize the mutual information between representations of temporally dis-tant frames of the same segment. 3. We achieve new state-of-the-art on benchmarks, clearly demonstrating the effectiveness of our approach. We additionally provide extensive experimental analysis with ablation studies. 2.