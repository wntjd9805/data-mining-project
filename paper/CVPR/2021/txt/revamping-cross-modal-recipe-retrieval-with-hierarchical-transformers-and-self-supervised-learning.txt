Abstract
Cross-modal recipe retrieval has recently gained sub-stantial attention due to the importance of food in people’s lives, as well as the availability of vast amounts of digital cooking recipes and food images to train machine learn-ing models.
In this work, we revisit existing approaches for cross-modal recipe retrieval and propose a simpliﬁed end-to-end model based on well established and high per-forming encoders for text and images. We introduce a hier-archical recipe Transformer which attentively encodes in-dividual recipe components (titles, ingredients and instruc-tions). Further, we propose a self-supervised loss function computed on top of pairs of individual recipe components, which is able to leverage semantic relationships within recipes, and enables training using both image-recipe and recipe-only samples. We conduct a thorough analysis and ablation studies to validate our design choices. As a result, our proposed method achieves state-of-the-art performance in the cross-modal recipe retrieval task on the Recipe1M dataset. We make code and models publicly available1. 1.

Introduction
Food is one of the most fundamental and important el-ements for humans, given its connection to health, culture, personal experience, and sense of community. With the de-velopment of the Internet and the rise of social networks, we witnessed a substantial surge in digital recipes that are shared online by users. Designing powerful tools to nav-igate such large amounts of data can support individuals in their cooking activities to enhance their experience with food, and has thus become an attractive research ﬁeld [30].
Often times, digital recipes come along with companion content such as photos, videos, nutritional information, user reviews, and comments. The availability of such rich large scale food datasets has opened the doors for new applica-tions in the context of food computing [37, 34, 24], one of 1https://github.com/amzn/image-to-recipe-transformers
Figure 1: Model overview. Our method is composed of three distinct parts: the image encoder φimg, the recipe en-coder φrec, and the training objectives Lpair and Lrec. the most prevalent ones being cross-modal recipe retrieval, where the goal is to design systems that are capable of ﬁnd-ing relevant cooking recipes given a user submitted food im-age. Approaching this challenge requires developing mod-els in the intersection of natural language processing and computer vision, as well as being able to deal with unstruc-tured, noisy, and incomplete data.
In this work, we focus on learning joint representations for textual and visual modalities in the context of food images and cooking recipes. Recent works on the task of cross-modal recipe retrieval [37, 3, 7, 42, 50] have in-troduced approaches for learning embeddings for recipes and images, which are projected into a joint embedding space that is optimised using contrastive or triplet loss func-tions. Advances were made by proposing complex models and loss functions, such as cross-modal attention [14], ad-versarial networks [50, 42], the use of auxiliary semantic losses [37, 3, 42, 50, 42], multi-stage training [37, 13], and reconstruction losses [14]. These works are either comple-mentary or orthogonal to each other while bringing certain disadvantages such as glueing independent models [37, 13], which needs extra care, relying on a pre-trained text rep-resentations [37, 3, 7, 42, 50, 13] and complex training pipelines involving adversarial losses [50, 42]. In contrast to previous works, we revisit ideas in the context of cross-modal recipe retrieval and propose a simpliﬁed end-to-end joint embedding learning framework that is plain, effective, 15475
and straightforward to train. Figure 1 shows an overview of our proposed approach.
Unlike previous works using LSTMs to encode recipe text [44, 5, 6, 37, 3], we introduce a recipe encoder based on Transformers [40], with the goal of obtaining strong rep-resentation for recipe inputs (i.e. titles, ingredients, and instructions) in a bottom-up fashion (see Figure. 1, left).
Following recent works in the context of text summariza-tion [48, 26], we leverage the structured nature of cooking recipes with hierarchical Transformers, which encode lists of ingredients and instructions by extracting sentence-level embeddings as intermediate representations, while learning relationships within each textual modality. Our experiments show superior performance of Transformer-based recipe en-coders with respect to their LSTM-based counterparts that are commonly used for cross-modal recipe retrieval.
Training joint embedding models requires cross-modal paired data, i.e., each image must be associated to its corre-sponding text. In the context of cross-modal recipe retrieval, this involves quadruplet samples of pictures, title, ingredi-ents, and instructions. Such a strong requirement is often not fulﬁlled when dealing with large scale datasets curated from the Web, such as Recipe1M [37]. Due to the unstruc-tured nature of recipes that are available online, Recipe1M largely consists of text-only samples, which are often ei-ther ignored or only used for pretraining text embeddings.
In this work, we propose a new self-supervised triplet loss computed between embeddings of different recipe compo-nents, which is optimised jointly and end-to-end with the main triplet loss computed on paired image-recipe embed-dings (see Lrec in Figure 1). The addition of this new loss allows us to use both paired and text-only data during train-ing, which in turn improves retrieval results. Further, thanks to this loss, embeddings from different recipe components are aligned with one another, which allows us to recover (or hallucinate) them when they are missing at test time.
Our method encodes recipes and images using simple yet powerful model components, and is optimised using both paired and unpaired data thanks to the new self-supervised recipe loss. Our approach achieves state-of-the-art results on Recipe1M, one of the most prevalent dataset in the com-munity. We perform an ablation study to quantify the con-tribution of each of the design choices, which range from how we represent recipes, the impact of our proposed self-supervised loss, and a state-of-the-art comparison.
The contributions of this work are the following. (1) We propose a recipe encoder based on hierarchical Transform-ers that signiﬁcantly outperforms its LSTM-based counter-parts on the cross-modal recipe retrieval task. (2) We in-troduce a self-supervised loss term that allows our model to learn from text-only samples by exploiting relationships between recipe components. (3) We perform extensive ex-perimentation and ablation studies to validate our design recipe encoders, image encoders, impact of choices (i.e. each recipe component). (4) As a product of our analysis, we propose a simple, yet effective model for cross-modal recipe retrieval which achieves state-of-the-art performance on Recipe1M, with a medR of 3.0, and Recall@1 of 33.5, improving the performance of the best performing model
[13] by 1.0 and 3.5 points, respectively. 2.