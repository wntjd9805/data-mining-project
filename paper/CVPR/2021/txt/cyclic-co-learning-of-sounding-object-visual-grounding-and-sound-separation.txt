Abstract
There are rich synchronized audio and visual events in our daily life. Inside the events, audio scenes are associated with the corresponding visual objects; meanwhile, sounding objects can indicate and help to separate their individual sounds in the audio track. Based on this observation, in this paper, we propose a cyclic co-learning (CCoL) paradigm that can jointly learn sounding object visual grounding and audio-visual sound separation in a uniﬁed framework. Con-cretely, we can leverage grounded object-sound relations to improve the results of sound separation. Meanwhile, beneﬁting from discriminative information from separated sounds, we improve training example sampling for sound-ing object grounding, which builds a co-learning cycle for the two tasks and makes them mutually beneﬁcial. Exten-sive experiments show that the proposed framework outper-forms the compared recent approaches on both tasks, and they can beneﬁt from each other with our cyclic co-learning.
The source code and pre-trained models are released in https://github.com/YapengTian/CCOL-CVPR21. 1.

Introduction
Seeing and hearing are two of the most important senses for human perception. Even though the auditory and visual information may be discrepant, the percept is uniﬁed with multisensory integration [5]. Such phenomenon is consid-ered to be derived from the characteristics of speciﬁc neu-ral cell, as the researchers in cognitive neuroscience found the superior temporal sulcus in the temporal cortex of the brain can simultaneously response to visual, auditory, and tactile signal [18, 40]. Accordingly, we tend to perform as unconsciously correlating different sounds and their visual producers, even in a noisy environment. For example, for a cocktail-party scenario contains multiple sounding and silent instruments as shown in Fig. 1, we can effortlessly
ﬁlter out the silent ones and identify different sounding ob-∗Corresponding authors.
Figure 1. Our model can perform audio-visual joint perception to simultaneously identify silent and sounding objects and separate sounds for individual audible objects. jects, and simultaneously separate the sound for each play-ing instrument, even faced with a static visual image.
For computational models, the multi-modal sound sep-aration and sounding object alignment capacities reﬂect in audio-visual sound separation (AVSS) and sound source vi-sual grounding (SSVG), respectively. AVSS aims to sep-arate sounds for individual sound sources with help from visual information, and SSVG tries to identify objects that make sounds in visual scenes. These two tasks are primar-ily explored isolatedly in the literature. Such a disparity to human perception motivates us to address them in a co-learning manner, where we leverage the joint modeling of the two tasks to discover objects that make sounds and sep-arate their corresponding sounds without using annotations.
Although existing works on AVSS [8, 32, 11, 49, 13, 46, 9] and SSVG [27, 38, 43, 3, 33] are abundant, it is non-trivial to jointly learn the two tasks. Previous AVSS meth-ods implicitly assume that all objects in video frames make sounds. They learn to directly separate sounds guided by encoded features from either entire video frames [8, 32, 49] or detected objects [13] without parsing which are sounding or not in unlabelled videos. At the same time, the SSVG methods mostly focus on the simple scenario with single sound source, barely exploring the realistic cocktail-party environment [38, 3]. Therefore, these methods blindly use information from silent objects to guide separation learn-2745
ing, also blindly use information from sound separation to identify sounding objects.
Toward addressing the drawbacks and enabling the co-learning of both tasks, we introduce a new sounding object-It targets to separate aware sound separation strategy. sounds guided by only sounding objects, where the audio-visual scenario usually consists of multiple sounding and silent objects. To address this challenging task, the SSVG can jump in to help identify each sounding object from a mixture of visual objects, whose objective is unlike previ-ous approaches that make great efforts on improving the localization precision of sound source in simple audiovi-sual scenario [38, 43, 3, 32]. Accordingly, it is challenging to discriminatively discover isolated sounding objects in-side scenarios via the predicted audible regions visualized by heatmaps [38, 3, 20]. For example, two nearby sound-ing objects might be grouped together in a heatmap and we have no good principle to extract individual objects from a single located region.
To enable the co-learning, we propose to directly dis-cover individual sounding objects in visual scenes from visual object candidates. With the help of grounded ob-jects, we learn sounding object-aware sound separation.
Clearly, a good grounding model can help to mitigate learn-ing noise from silent objects and improve separation. How-ever, causal relationship between the two tasks cannot en-sure separation can further enhance grounding, because they only loosely interacted during sounding object selec-tion. To alleviate the problem, we use separation results to help sample more reliable training examples for ground-ing. It makes the co-learning in a cycle and both ground-ing and separation performance will be improved, as illus-trated in Fig. 2. Experimental results show that the two tasks can be mutually beneﬁcial with the proposed cyclic co-learning, which leads to noticeable performance and out-performs recent methods on sounding object visual ground-ing and audio-visual sound separation tasks.
The main contributions of this paper are as follows: (1)
We propose to perform sounding object-aware sound sepa-ration with the help of visual grounding task, which essen-tially analyzes the natural but previously ignored cocktail-party audiovisual scenario. (2) We propose a cyclic co-learning framework between AVSS and SSVG to make these two tasks mutually beneﬁcial. (3) Extensive experi-ments and ablation study validate that our models can out-perform recent approaches, and the tasks can beneﬁt from each other with our cyclic co-learning. 2.