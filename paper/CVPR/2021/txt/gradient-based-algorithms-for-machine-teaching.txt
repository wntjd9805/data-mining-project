Abstract
The problem of machine teaching is considered. A new formulation is proposed under the assumption of an optimal student, where optimality is deﬁned in the usual machine learning sense of empirical risk minimization. This is a sensi-ble assumption for machine learning students and for human students in crowdsourcing platforms, who tend to perform at least as well as machine learning systems. It is shown that, if allowed unbounded effort, the optimal student always learns the optimal predictor for a classiﬁcation task. Hence, the role of the optimal teacher is to select the teaching set that minimizes student effort. This is formulated as a problem of functional optimization where, at each teaching iteration, the teacher seeks to align the steepest descent directions of the risk of (1) the teaching set and (2) entire example population.
The optimal teacher, denoted MaxGrad, is then shown to maximize the gradient of the risk on the set of new examples selected per iteration. MaxGrad teaching algorithms are
ﬁnally provided for both binary and multiclass tasks, and shown to have some similarities with boosting algorithms.
Experimental evaluations demonstrate the effectiveness of
MaxGrad, which outperforms previous algorithms on the classiﬁcation task, for both machine learning and human students from MTurk, by a substantial margin. 1.

Introduction
The success of deep learning has been driven, in large part, by the availability of large and carefully curated datasets for tasks such as image recognition [8, 22], action recog-nition [19, 11], object detection [24], etc. These datasets usually contain everyday objects, actions, or scenes and can be scalably annotated on crowdsourcing platforms such as
Amazon Mechanical Turk (MTurk). This is, however, usu-ally not true for expert domains, such as biology or medical imaging. While data collection can still be easy in these domains, annotations require highly specialized and domain speciﬁc knowledge. This is beyond the reach of crowdsourc-ing annotators. On the other hand, annotation by specialists is usually too expensive and rarely feasible at a large scale.
Figure 1: MaxGrad is an iterative machine teaching algorithm. At each iteration, the teacher selects new examples from a large dataset, to complement a small teaching set used by a student to learn a task.
Examples are selected by comparing the current student model to the optimal model for the large dataset. The optimal student uses the teaching set to update his/her model and feeds back its predictions to the teacher.
This has motivated extensive research in alternative and less label-intensive forms of learning, including few-shot learn-ing [42, 1], transfer learning [21, 46], semi-supervised learn-ing [47, 23], and self-supervised learning [35, 20]. However, these approaches usually underperform supervised learning from large and fully labeled datasets. In result, there has recently been interest in machine teaching algorithms capa-ble of training crowdsource annotators to label data from specialized domains.
The goal of machine teaching is to design systems that can teach students efﬁciently and automatically. Machine teaching is a broad research problem [52], where humans can utilize domain knowledge to teach machines or vice-versa. In this work, we restrict the discussion to the narrow task of image classiﬁcation, where a machine teaches hu-man learners to discriminate between different image classes.
Although the proposed ideas are general, we target the ap-plication of teaching image annotators in crowd-sourcing platforms. This exploits the fact that a relatively small anno-tated dataset can be leveraged to train crowd workers, which can then annotate large numbers of images, enabling scalable supervised learning of image classiﬁers. While classiﬁcation has been the task of choice for much machine teaching work, it should be noted that several other tasks and applications have also been investigated [6, 5, 44].
The machine teaching set-up considered in this work is 1387
the iterative interaction set-up of Figure 1. At each iteration, the teacher selects new examples from a large dataset, to complement a small set of examples, known as the teaching set, which is used by the student to learn the target task. By comparing the current student model and the optimal model for the large dataset, the teacher seeks to select the examples that most help the student learn. The central question in this set-up is how to select the teaching set. Ideally, this set should pack as much information for class discrimination as possible into the smallest number of examples.
In the literature, there have been many attempts to design optimal teaching algorithms [39, 26, 27, 30]. This usually requires the assumption of certain student properties. Al-though past works have proposed different student models, these frequently rely on assumptions that are questionable for the crowdsource annotation context. For example, a popular assumption [39, 29, 7] is that the student only has access to a countable set of hyperplane hypotheses. While justiﬁed by the fact that human students have limited ability and memory, this assumption overly underestimates their learning ability. In fact, several machine teaching works explicitly assume that students have limited capacity or are otherwise sub-optimal learners [30, 18, 39, 50]. This is not supported by studies with real students, which found that humans have strong learning ability [13, 9, 15, 38].
In this work, we assume that the student is an optimal learner. Optimality is deﬁned in the standard machine learn-ing sense, i.e. that the student learns a predictor of minimum empirical risk in the teaching set. This always holds for ma-chine learning students, which are deﬁned in this way, and is sensible for human students, who usually do not under-perform machine learning students, especially on few-shot learning scenery in practice. It does assume that students are engaged in the learning task, i.e. giving their best effort. This is sensible in the crowdsourcing scenario, where students are free-willing participants rated by their task performance. We show that, if allowed unbounded effort, the optimal student will always learn the optimal predictor for the task. This implies that the only role of the teacher is to optimize learn-ing speed, i.e. select the teaching examples that enable the student to learn with least effort.
We then formulate the search for the optimal teacher as a problem of functional optimization where, at each teaching iteration, the teacher aims to align the steepest descent direc-tion of the teaching set risk with that of the empirical risk over the entire example population. This is shown to have as optimal solution the MaxGrad teacher, which maximizes the gradient of the risk on the set of new examples selected per iteration. MaxGrad teaching algorithms are ﬁnally pro-vided for both binary and multiclass tasks, and shown to have some similarities with boosting algorithms [33, 12, 34].
Experimental evaluations demonstrate the effectiveness of
MaxGrad, which outperforms previous algorithms on the classiﬁcation task, for both machine learning and human students from MTurk. 2.