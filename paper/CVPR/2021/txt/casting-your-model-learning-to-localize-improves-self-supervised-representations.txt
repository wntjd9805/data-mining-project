Abstract
Recent advances in self-supervised learning (SSL) have largely closed the gap with supervised ImageNet pretrain-ing. Despite their success these methods have been pri-marily applied to unlabeled ImageNet images, and show marginal gains when trained on larger sets of uncurated images. We hypothesize that current SSL methods perform best on iconic images, and struggle on complex scene im-ages with many objects. Analyzing contrastive SSL meth-ods shows that they have poor visual grounding and re-ceive poor supervisory signal when trained on scene im-ages. We propose Contrastive Attention-Supervised Tuning (CAST) to overcome these limitations. CAST uses unsuper-vised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention loss. Experiments on COCO show that CAST signiﬁcantly improves the features learned by SSL methods on scene im-ages, and further experiments show that CAST-trained mod-els are more robust to changes in backgrounds. Our code is available at https://github.com/salesforce/CAST/. 1.

Introduction
Self-supervised learning (SSL) of visual feature repre-sentations has seen great interest in recent years. SSL in computer vision aims to learn feature representations with-out using any human annotations, which can be utilized by downstream tasks such as supervised image classiﬁca-tion [1, 2], object detection [3, 4], and semantic segmenta-tion [5, 6]. Recent SSL methods based on contrastive learn-ing [7, 8] have begun to match or even outperform super-vised pretraining on several downstream tasks [9–14].
The promise of self-supervised methods is that they ought to allow us to learn better features by scaling to ever-larger training sets, without the need for expensive human-provided labels. Unfortunately, the success of re-cent SSL methods has been largely conﬁned to unlabeled images from the ImageNet [2] training set. Na¨ıvely ap-plying them to larger uncurated sets of internet images
*Equal Contribution (a) Poor visual grounding ability (b) Sampling issues with complex images
Figure 1: We identify two issues with recent contrastive approaches to self-supervised learning: (a) Poor ground-ing: On iconic images, contrastive methods can match key and query but use the wrong image regions to do so. Grad-CAM [22] reveals that the model puts high weight (red) on background regions, and low weight (blue) on the object of interest. (b) Inconsistent Samples: On complex images, randomly sampled crops may portray different objects, giv-ing an inconsistent learning signal. We show that correcting these issues improves self-supervised learning. has shown marginal gains [11, 12, 14] despite using image sets that are orders of magnitude larger than ImageNet (eg.
Instagram-1B [15], YFCC100M [16], JFT-300M [17]).
We hypothesize that current SSL methods perform best when trained on iconic images of single objects (like those in ImageNet) but struggle when trained on more complex scene images with many objects. Indeed, current SSL meth-ods struggle even when trained on curated datasets of scene images [18, 19] such as COCO [20] or Places205 [21].
In this paper, we analyze contrastive self-supervised models to understand the cause of these limitations and pro-pose a solution to overcome them. Speciﬁcally, we ﬁnd that existing contrastive self-supervised models have poor visual grounding ability and they receive imperfect supervi-sory signal when augmented views contain different visual concepts, which is common in images of complex scenes.
These issues may arise from the practice of training the 111058
instance discrimination task with random views from im-ages. This practice does not encourage semantic under-standing, and models often cheat by exploiting low-level visual cues or spurious background correlations. For ex-ample, in Figure 1a, the model relies on the grass to match the two augmented views of the dog. Augmented views for training these models commonly start with taking ran-dom crops from an image. This strategy may be acceptable for iconic images. However, for scene images, like those in COCO, two views may contain semantically distinct ob-jects (such as the crops in Figure 1b). This fact may ex-plain diminishing improvements of contrastive SSL models trained on varied web images, and the reduction in their per-formance when trained with scene images alone.
To mitigate these limitations, we propose Contrastive
Attention-Supervised Tuning (CAST), a training method to improve the visual grounding ability of contrastive SSL methods. CAST consists of two algorithmic components: (a) an intelligent geometric transform for cropping dif-ferent views from an input image, based on constraints derived from an unsupervised saliency map, and (b) a
Grad-CAM [22]-based attention loss that provides explicit grounding supervision by forcing the model to attend to ob-jects that are common across the crops. train the Momentum Contrastive Encoder
We
[12], a leading contrastive learning method, (MoCo) using CAST on the COCO dataset. We evaluate its performance using image classiﬁcation, object detection, and instance segmentation tasks, obtaining robust gains in all cases. Additional experiments on the