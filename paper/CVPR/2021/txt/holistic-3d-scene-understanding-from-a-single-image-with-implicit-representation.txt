Abstract
We present a new pipeline for holistic 3D scene under-standing from a single image, which could predict object shapes, object poses, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from in-accurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit rep-resentation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also reﬁne the 3D object pose and scene layout via a novel implicit scene graph neu-ral network that exploits the implicit local object features.
A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estima-tion, and 3D object detection. 1.

Introduction 3D indoor scene understanding is a long-lasting com-puter vision problem and has tremendous impact on sev-eral applications, e.g., robotics, virtual reality. Given a sin-gle color image, the goal is to reconstruct the room layout as well as each individual object and estimate its seman-tic type in the 3D space. Over decades, there are plenty of works consistently improving the performance of such a task over two focal points of the competition. One is the 3D shape representation preserving ﬁne-grained geome-try details, evolving from the 3D bounding box, 3D vol-ume, point cloud, to the recent triangulation mesh. The other is the joint inference of multiple objects and layout in the scene leveraging contextual information, such as co-occurring or relative locations among objects of multiple
*Equal contribution
†Corresponding author
Project webpage: https://chengzhag.github.io/publication/im3d/
Figure 1: Our proposed pipeline takes a single image as in-put, estimates layout and object poses, then reconstructs the scene with Signed Distance Function (SDF) representation. categories. However, the cluttered scene is a double-blade sword, which unfortunately increases the complexity of 3D scene understanding by introducing large variations in ob-ject poses and scales, and heavy occlusion. Therefore, the overall performance is still far from satisfactory.
In this work, we propose a deep learning system for holistic 3D scene understanding, which predicts and reﬁnes object shapes, object poses, and scene layout jointly with deep implicit representation. At ﬁrst, similar to previous methods, we exploit standard Convolutional Neural Net-works (CNN) to learn an initial estimation of 3D object poses, scene layout as well as 3D shapes. Different from previous methods using explicit 3D representation like vol-ume or mesh, we utilize the local structured implicit rep-resentation of shapes motivated by [12]. Instead of taking depth images as input like [12], we design a new local im-plicit shape embedding network to learn the latent shape code directly from images, which can be further decoded to generate the implicit function for 3D shapes. Due to the power of implicit representation, the 3D shape of each ob-ject can be reconstructed with higher accuracy and ﬁner sur-face details compared to other representations.
Then, we propose a novel graph-based scene context net-work to gather information from local objects, i.e., bottom-up features extracted from the initial predictions, and learn to reﬁne the initial 3D pose and scene layout via scene con-text information with the implicit representation. Being one of the core topics studied in scene understanding, the con-text has been achieved in the era of deep learning mainly 8833
from two aspects - the model architecture and the loss func-tion. From the perspective of model design, we exploit the graph-based convolutional neural network (GCN) to learn context since it has shown competitive performance to learn context [58]. With the deep implicit representation, the learned local shape latent vectors are naturally a compact and informative feature measuring of the object geometries, which results in more effective context models compared to features extracted from other representations such as mesh.
Not only the architecture, but the deep implicit represen-tation also beneﬁts the context learning on the loss func-tion. One of the most basic contextual information yet still missing in many previous works - objects should not inter-sect with each other, could be easily applied as supervision by penalizing the existence of 3D locations with negative predicted SDF in more than one objects1. We deﬁne this constraint as a novel physical violation loss and ﬁnd it par-ticularly helpful in preventing intersecting objects and pro-ducing reasonable object layouts.
Overall, our contributions are mainly in four aspects.
First, we design a two-stage single image-based holistic 3D scene understanding system that could predict object shapes, object poses, and scene layout with deep implicit representation, then optimize the later two. Second, a new image-based local implicit shape embedding network is proposed to extract latent shape information which leads to superior geometry accuracy. Third, we propose a novel
GCN-based scene context network to reﬁne the object ar-rangement which well exploits the latent and implicit fea-tures from the initial estimation. Last but not least, we de-sign a physical violation loss, thanks to the implicit repre-sentation, to effectively prevent the object intersection. Ex-tensive experiments show that our model achieves the state-of-the-art performance on the standard benchmark. 2.