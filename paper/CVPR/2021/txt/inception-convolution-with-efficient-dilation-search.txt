Abstract
Inception convolution
Inception convolution
As a variant of standard convolution, a dilated convolu-tion can control effective receptive ﬁelds and handle large scale variance of objects without introducing additional computational costs. To fully explore the potential of di-lated convolution, we proposed a new type of dilated con-volution (referred to as inception convolution), where the convolution operations have independent dilation patterns among different axes, channels and layers. To develop a practical method for learning complex inception convolu-tion based on the data, a simple but effective search algo-rithm, referred to as efﬁcient dilation optimization (EDO), is developed. Based on statistical optimization, the EDO method operates in a low-cost manner and is extremely fast when it is applied on large scale datasets. Empirical results validate that our method achieves consistent performance gains for image recognition, object detection, instance seg-mentation, human detection, and human pose estimation.
For instance, by simply replacing the 3 × 3 standard con-volution in the ResNet-50 backbone with inception convolu-tion, we signiﬁcantly improve the AP of Faster R-CNN from 36.4% to 39.2% on MS COCO. 1.

Introduction
As an important concept of convolution neural network,
In [29], the receptive ﬁeld has been extensively studied.
Luo et al. showed that the intensity in each receptive ﬁeld roughly obeys a Gaussian distribution and only few pixels around the central part of the receptive ﬁeld can effectively contribute to the response of the output neuron. Further-more, in the previous works [23, 32], a more carefully de-ﬁned optimal receptive ﬁeld (ORF) has been evaluated for different tasks.
The requirement of the optimal receptive ﬁeld is due to the variance of the input image sizes or the scales of objects
∗Equal contribution
……
……
……
Figure 1. Inception convolution contains rich dilation patterns along both spatial axes and channels in each convolution layer. of interest. For instance, for image classiﬁcation, the input sizes tend to be small (e.g, 224 × 224), while for object de-tection, the input sizes are much larger and the objects can be from a large range of scales. Correspondingly, different tasks would require different ERFs. The different require-ments of ERFs from different tasks makes it necessary to develop a general and practical optimization algorithm to learn the optimal ERF for a speciﬁc task.
As discussed in [29], the dilation value of dilated con-volution kernels is a highly effective hyper-parameter to control the receptive ﬁeld for different tasks. The work in
[23] proposed to assign different dilation values at different stages of a CNN, which achieves consistent performance improvements. Subsequently, NATS [32] divided a con-volution operation into different groups with each having independent dilation values. However, they apply the stan-dard network architecture search methods in the relatively coarse search spaces, which neglects the ﬁne-grained inner structure of dilated convolution. Therefore, in this work, we focus on exploring the search problem in the dilation domain to efﬁciently learn the optimal receptive ﬁeld.
First of all, we would like to have a more ﬂexible search space when compared with [23]. Flexibility can make us learn the optimal receptive ﬁeld in order to better ﬁt to dif-ferent datasets. As shown in Figure 1, we propose a new type of dilated convolution, called Inception Convolution, which contains as much as possible dilation patterns. In the 11486
space of inception convolution, the dilation pattern along each axis, each channel, and each convolution layer is in-dependently deﬁned. As a result, a dense range of possible receptive ﬁelds are considered in our inception convolution.
For optimization, a direct solution is to use the exist-ing works in neural architecture search (NAS), which en-ables automatic search for the optimal combination of vari-ous network operations. DARTS [27] and single path one-shot [15] (SPOS) are two main families of efﬁcient NAS methods. DARTS trained a supernet, where discrete oper-ation selection was relaxed to a continuous weighted sum of the output from all candidate operations. After training, in each block, the operation with the largest architecture weight was chosen. SPOS randomly selected an operation sequence (subnet) from a pre-trained supernet and the same operation in different sequences share the same weights.
After training, SPOS selected the best operation sequence via sampling and evaluation of multiple sequences with the shared-weights.
However, both DARTS and SPOS are not suitable for our huge search space. In DARTS, during training all opera-tions in a block are applied to the input to make the architec-ture weights aware of each operation’s importance, but the number of dilation patterns for a convolution layer (block) is large, i.e., 16 if each of the two axes has 4 choices. It means DARTS requires 16 sequential calculations, thus it has low GPU utility and huge computational costs. SPOS samples the operation sequences during training. However, in our search space, the number of dilation patterns even in a single convolution layer is huge, i.e., (dmax)2C, where C is the number of channels and dmax is the maximum dilation value. Due to the huge number of dilation patterns (paths), it is an extremely difﬁcult task for SPOS as well.
In this work, we propose a simple and efﬁcient dilation optimization algorithm (EDO). In EDO, each layer of the supernet is a standard convolution operation whose kernel covers all possible dilation patterns. After pre-training of the supernet, we select the dilation pattern for each channel in each convolution layer by solving a statistical optimiza-tion problem. Speciﬁcally, for each layer, according to the pre-trained weights, we minimize the L1 error between the output of the original convolution layer and the output of the learned dilated convolution layer with the selected dilation pattern, based on which we can learn the optimal dilation pattern for this layer.
EDO supports efﬁcient channel-wise dilation pattern se-lection over our complete dilation pattern search space.
When compared with the search based method in [15], the search cost of our methods is very low. When compared with the differentiable NAS methods [27, 3], EDO con-verts sequential calculation related to different dilation pat-terns into a parallel way, thus it has lower computation cost and higher GPU utilization. Further, when compared with
SPOS, we do not need to design further mechanism to han-dle the extremely large number of dilation patterns (paths).
Our contributions are three folds: 1) We propose a new type of dilated convolution, referred to as inception convo-lution, which can be readily combined with a large number of backbones for various visual recognition tasks; 2) We also propose a low-cost statistical optimization based net-work architecture search algorithm EDO, which can efﬁ-ciently learn the optimal receptive ﬁeld based on the data; 3)
Comprehensive experiments demonstrate that our learnt in-ception convolution in combination with various backbones generally achieves performance improvement for various visual tasks without introducing any additional computa-tional costs. 2.