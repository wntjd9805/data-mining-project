Abstract
Images synthesized by powerful generative adversar-ial network (GAN) based methods have drawn moral and privacy concerns. Although image forensic models have reached great performance in detecting fake images from real ones, these models can be easily fooled with a sim-ple adversarial attack. But, the noise adding adversarial samples are also arousing suspicion.
In this paper, in-stead of adding adversarial noise, we optimally search ad-versarial points on face manifold to generate anti-forensic fake face images. We iteratively do a gradient-descent with each small step in the latent space of a generative model, e.g. Style-GAN, to ﬁnd an adversarial latent vector, which is similar to norm-based adversarial attack but in latent space. Then, the generated fake images driven by the ad-versarial latent vectors with the help of GANs can defeat main-stream forensic models. For examples, they make the accuracy of deepfake detection models based on Xception or EfﬁcientNet drop from over 90% to nearly 0%, mean-while maintaining high visual quality. In addition, we ﬁnd manipulating noise vectors n at different levels have differ-ent impacts on attack success rate, and the generated ad-versarial images mainly have changes on facial texture or face attributes. 1.

Introduction
Nowadays, it is increasingly hard for human eyes to tell a real image from a fake one with the rapid improvement of image generation techniques. Manipulated or generated fake images may draw social and privacy concern if being abused by malicious attackers. An attacker may register an account with photos belonging to a non-existent person or swap one person’s face to another, thus causing privacy and security issues. Image forensic models are designed to clarify those images from real ones, and have gained con-*Corresponding author.
Figure 1. Adversarial images generated by different methods. Up-per left is the original Style-GAN-generated image. Upper right is the image generated by our method. Lower left and Lower right are adversarial images generated by FGSM[8] and PGD[21] Linf norm-based attack respectively under the same perturbation level.
Although all these images can bypass the target forensic model, images generated by our method are more invisible to human eyes. siderable performance on several benchmarks and datasets
[17, 30]. However, a smarter attacker may attempt to gener-ate images which can bypass those detectors while keeping high visual quality. These images may escape the detection procedure and spread in social media. In order to combat the generation and spread of undetectable “deep fakes”, it is necessary for image forensics researchers themselves to de-velop and study anti-forensic operations[32] to improve the robustness and generalization ability of the existed forensic models.
In this paper, we propose to efﬁciently generate adver-sarial high visual quality fake images to fool forensic de-tectors. By adversarially exploring on the manifold of the recent powerful generative model Style-GAN[13], we can 5789
therefore generate the adversarial fake face images that fool forensic models. Though StyleGAN is capable of generat-ing high-resolution images with various styles and stochas-tic details, the generated fake images are easily detected by models based on Xception or EfﬁcientNet with accuracy of over 90%. But with intentionally iteratively searching these adversarial vectors in its latent space with a gradient de-scent manner, we successfully screen out fakes images that will be detected by forensic models as real ones.
Although one can exploit existing adversarial attack methods [8, 21] to deceive a forensic model, it may hold visible perturbations brought by the optimization process in image space, which make it detectable by human eyes or specially designed detectors [24]. Recently proposed unre-stricted adversarial attack methods [31, 36, 11] could gen-erate adversarial images with less suspicious visual artifacts by training GAN models, but they mainly focus on defeat-ing classiﬁcation or recognition tasks.
Our method has superiority in the following aspects:
First, because we do modiﬁcations on the manifold, we don’t have to care much about the image pixel constraint, which makes a higher updating strength possible. Second, unlike norm-based attack which leave visible artifacts onto the image, our method can generate the same image without obvious artifacts, see in Figure 1.
Our contribution are as follows: 1. We propose a novel method of generating adversar-ial anti-forensic images via exploring Style-GAN’s mani-fold. Images generated by our method can successfully by-pass two image forensic models, Xception [2] and Efﬁcient-Net [33]. indicating the demand for more robust forensic models. 2. We compare our method with nowadays widely-applied norm-based adversarial attacks and show that the proposed method can achieve the same attack success rate while introducing less visible perturbation, making it harder for our adversarial image to be detected by human eyes. 3. We conduct our attack in different ensemble ways and have shown our adversarial images can transfer between different forensic models, causing a threat even in the situ-ation where the architecture of forensic models is unknown to the attacker. 4. We show some interesting effects between the adver-sarial strength, the level of input noise vector and the at-tributes of our generated images, which are worthy of in-vestigation in future. 2.