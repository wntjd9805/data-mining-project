Abstract
This paper presents a novel method for embedding trans-fer, a task of transferring knowledge of a learned embed-ding model to another. Our method exploits pairwise sim-ilarities between samples in the source embedding space as the knowledge, and transfers them through a loss used for learning target embedding models. To this end, we de-sign a new loss called relaxed contrastive loss, which em-ploys the pairwise similarities as relaxed labels for inter-sample relations. Our loss provides a rich supervisory sig-nal beyond class equivalence, enables more important pairs to contribute more to training, and imposes no restriction on manifolds of target embedding spaces. Experiments on metric learning benchmarks demonstrate that our method largely improves performance, or reduces sizes and output dimensions of target models effectively. We further show that it can be also used to enhance quality of self-supervised representation and performance of classiﬁcation models. In all the experiments, our method clearly outperforms exist-ing embedding transfer techniques. 1.

Introduction
Deep metric learning aims to learn an embedding space where samples of the same class are grouped tightly to-gether. Such an embedding space has played important roles in many tasks including image retrieval [19, 20, 29, 40, 41], few-shot learning [35, 39, 42], zero-shot learn-ing [3, 58], and self-supervised representation learning [4, 14, 44]. In these tasks, the performance and efﬁciency of models rely heavily on the quality and dimension of their learned embedding spaces. To obtain high-quality and com-pact embedding spaces, previous methods have proposed new metric learning losses [19, 29, 40, 41, 46, 52], ad-vanced sampling strategies [13, 22, 47, 49], regularization techniques [18, 28], or ensemble models [21, 30, 31].
For the same purpose, we study transferring knowledge of a learned embedding model (source) to another (target), which we call embedding transfer. This task can be consid-ered as a variant of knowledge distillation [16] that focuses on metric learning instead of classiﬁcation. The knowl-edge captured by the source embedding model can provide rich information beyond class labels such as intra-class vari-ations and degrees of semantic afﬁnity between samples.
Given a proper way to transfer the knowledge, embedding transfer enables us to improve the performance of target em-bedding models or compress them, as knowledge distilla-tion does for classiﬁcation models [11, 16, 36, 50, 55].
Existing methods for embedding transfer extract knowl-edge from a source embedding space in forms of proba-bility distributions of samples [33], their geometric rela-tions [32, 53], or the rank of their similarities [6]. The knowledge is then transferred by forcing target models to approximate those extracted patterns directly in their em-bedding spaces. Although these methods shed light on the effective yet less explored approach to enhancing the perfor-mance of metric learning, there is still large room for further improvement. In particular, they fail to utilize detailed inter-sample relations in the source embedding space [6, 33] or blindly accept the transferred knowledge without consider-ing the importance of samples [32, 53].
This paper presents a new embedding transfer method that overcomes the above limitations. Our method deﬁnes the knowledge as pairwise similarities between samples in a source embedding space. Pairwise similarities are useful to characterize an embedding space in detail, thus have been widely used for learning embedding spaces [12, 37, 40, 46] and identifying underlying manifolds of data [9, 43]. Also, they capture detailed inter-sample relations, which are miss-ing in probability distributions [33] and the rank of similar-ities [6] used as knowledge in previous work.
To transfer the knowledge effectively, we propose a new loss, called relaxed contrastive loss, that is used for learning target embedding models with the knowledge in the form of pairwise similarities. Our loss utilizes the pairwise sim-ilarities as relaxed labels of inter-sample relations, unlike conventional metric learning losses that rely on binary la-bels indicating class equivalence between samples (i.e., 1 if two samples are of the same class and 0 otherwise) as su-pervision. By replacing the binary labels with the pairwise similarities, our loss can provide rich supervision beyond 3967
1
@
R
Embedding Transfer
Ours
RKD
PKT
DarkRank
Deep Metric Learning
PA
DiVA
MS
CUB-200-2011
Cars-196
SOP
Figure 1. Accuracy in Recall@1 on the three standard benchmarks for deep metric learning. All embedding transfer methods adopt
Proxy-Anchor (PA) [19] with 512 dimension as the source model. Our method achieves the state of the art when embedding dimension is 512, and is as competitive as recent metric learning models even with a substantially smaller embedding dimension. In all experiments, it is superior to other embedding transfer techniques. More results can be found in Table 1 and 2. what the binary labels offer, such as the degree of similarity and hardness of a pair of training samples.
Speciﬁcally, the proposed loss pushes apart or pulls to-gether a pair of samples in a target embedding space fol-lowing the principle of the original contrastive loss [12], but the semantic similarity of the pair given by the knowledge controls the strength of pushing and pulling. Also, we re-veal that the loss lets more important pairs contribute more to learning the target embedding model, thus resolves the limitation of previous methods that treat samples equally during transfer [32, 53]. In addition to the use of relaxed relation labels, we further modify the loss so that it does not impose any restriction on the manifold of target embedding space, unlike conventional losses that enforce target embed-ding spaces ℓ2 normalized. This modiﬁcation enables to utilize given embedding dimensions more effectively and provides extra performance improvement.
The efﬁcacy of the proposed method is ﬁrst demon-strated on public benchmark datasets for deep metric learn-ing. Our method substantially improves image retrieval per-formance when the target model has the same architecture as its source counterpart, and greatly reduces the size and embedding dimension of the target model with a negligible performance drop when the target model is smaller than the source model, as shown in Fig. 1. We also show that our method enhances the quality of self-supervised representa-tion through self embedding transfer and the performance of classiﬁcation models in the knowledge distillation set-ting. In all the experiments, our method outperforms exist-ing embedding transfer techniques [6, 32, 33]. 2.