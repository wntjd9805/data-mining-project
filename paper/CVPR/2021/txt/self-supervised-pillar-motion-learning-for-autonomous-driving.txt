Abstract
Autonomous driving can beneﬁt from motion behavior comprehension when interacting with diverse trafﬁc partic-ipants in highly dynamic environments. Recently, there has been a growing interest in estimating class-agnostic mo-tion directly from point clouds. Current motion estimation methods usually require vast amount of annotated train-ing data from self-driving scenes. However, manually la-beling point clouds is notoriously difﬁcult, error-prone and time-consuming. In this paper, we seek to answer the re-search question of whether the abundant unlabeled data collections can be utilized for accurate and efﬁcient motion learning. To this end, we propose a learning framework that leverages free supervisory signals from point clouds and paired camera images to estimate motion purely via self-supervision. Our model involves a point cloud based structural consistency augmented with probabilistic motion masking as well as a cross-sensor motion regularization to realize the desired self-supervision. Experiments reveal that our approach performs competitively to supervised meth-ods, and achieves the state-of-the-art result when combin-ing our self-supervised model with supervised ﬁne-tuning. 1.

Introduction
Understanding the motion of various trafﬁc agents is cru-cial for self-driving vehicles to be able to safely operate in dynamic environments. Motion provides pivotal informa-tion to facilitate a variety of onboard modules ranging from detection, tracking, prediction to planning. A self-driving vehicle is typically equipped with multiple sensors, and the most commonly used one is LiDAR. How to represent and extract temporal motion from point clouds is therefore one of the fundamental research problems in autonomous driv-ing [9, 20, 39]. This is however challenging in the sense that (1) there exist numerous agent categories and each cat-egory exhibits speciﬁc motion behavior; (2) point cloud is sparse and lacks of exact correspondence between sweeps; and (3) estimating process is required to meet tight runtime constraint and limited onboard computation.
This work was done while C. Luo was interning at QCraft.
Figure 1. An overview of the proposed self-supervised pillar mo-tion learning by our designed free supervisory signals from point clouds and paired camera images. (a) illustrates a point cloud in
BEV with the dotted gray lines showing the ﬁeld of view of the back right camera. (b) shows the projected points with color en-coding optical ﬂow (ego-motion factorized out) on the back right camera image. Note that the white points are static. We attach the original optical ﬂow of this image on bottom right for reference. (c) is the predicted pillar motion ﬁeld, where hue and saturation correspond to motion direction and magnitude, and the gray are static pillars. (d) demonstrates a zoomed-in area of (c).
A traditional autonomy stack usually performs motion estimation by ﬁrst recognizing other trafﬁc participants in the scene and then predicting how the scene might progress given their current states [9, 18]. However, most recogni-tion models are only trained to classify and localize objects from a handful of known categories. This closed-set sce-nario is apparently insufﬁcient for a practical autonomy sys-tem to perceive motion of a large diversity of instances that are not seen during training. As the lower-level informa-tion compared to object semantics, motion should be ide-ally estimated in an open-set setting irrespective of whether objects belong to a known or unknown category. One ap-pealing way to predict class-agnostic motion is to estimate scene ﬂow from point clouds by estimating the 3D veloc-ity of each point [20, 24]. Unfortunately, this dense motion
ﬁled prediction is currently computationally prohibitive to 3183
process one complete LiDAR sweep, ruling out the practi-cal use for self-driving vehicles that require real-time and large-scale point cloud processing.
Another possibility to represent and estimate motion is based on bird’s eye view (BEV). In this way, a point cloud is discretized into grid cells, and motion information is de-scribed by encoding each cell with a 2D displacement vec-tor indicating the position into the future of the cell on the ground plane [8, 17, 39]. This compact representation suc-cessfully simpliﬁes scene motion as the motion taking place on the ground plane is the primary concern for autonomous driving, while the motion in the vertical direction is not as much important or useful. Additionally, point clouds rep-resented in this form are efﬁcient since all key operations can be conducted via 2D convolutions that are extremely fast to compute on GPUs. Recent works also show that this representation can be readily generalized to class-agnostic motion estimation [8, 17]. However, they have to rely on large amounts of annotated point cloud data with object de-tection and tracking as proxy motion supervision, which is expensive and difﬁcult to obtain in practice.
Statistics ﬁnds that a self-driving vehicle generates over 1 terabyte of data per day but only less than 5% of the data is used [1]. Thus, learning without requiring manual la-beling is of critical importance in order to fully harness the abundant data. While the recent years have seen growing in-terests in self-supervised learning for language [5, 15] and vision [14, 34], self-supervision for point clouds still falls behind, yet has great potential to open up the possibility to utilize practically inﬁnite training data that is continuously collected by the world-wide self-driving ﬂeets.
In light of the above observations, we propose a self-supervised learning framework that exploits free supervi-sory signals from multiple sensors for open-set motion es-timation, as shown in Figure 1. To take advantage of the merits of motion representation in BEV, we organize a point cloud into pillars (i.e., vertical columns) [16], and refer to the velocity associated with each pillar as pillar motion.
We introduce a point cloud based self-supervision by as-suming pillar or object structure constancy between two consecutive sweeps. However, this does not hold in most cases due to the lack of exact point correspondence caused by the sparse scans of LiDAR. Our solution towards miti-gating this difﬁculty is to make use of optical ﬂow extracted from camera images to provide self-supervised and cross-sensory regularization. As illustrated in Figure 2, this de-sign leads to a uniﬁed learning framework that subsumes the interactions between LiDAR and the paired cameras: (1) point clouds facilitate factorizing ego-motion out from opti-cal ﬂow; (2) image motion provides auxiliary regularization for learning pillar motion in point clouds; (3) probabilis-tic motion masking formed by back-projected optical ﬂow promotes structural consistency in point clouds. Note that the camera-related components are only used in training and discarded for inference, thus no additional computations are introduced for the camera modality at runtime.
To our knowledge, this work provides the ﬁrst learning paradigm that is able to perform pillar motion prediction in a fully self-supervised framework. We propose novel self-supervisory and cross-sensory signals by tightly inte-grating point clouds and paired camera images to achieve the desired self-supervision. Experiments show that our ap-proach compares favorably to the existing supervised meth-ods. Our code and model will be made available at https:
//github.com/qcraftai/pillar-motion. 2.