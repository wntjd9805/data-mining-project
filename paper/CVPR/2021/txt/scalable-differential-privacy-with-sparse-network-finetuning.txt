Abstract
We propose a novel method for privacy-preserving train-ing of deep neural networks leveraging public, out-domain data. While differential privacy (DP) has emerged as a mechanism to protect sensitive data in training datasets, its application to complex visual recognition tasks re-mains challenging.
Traditional DP methods, such as
Differentially-Private Stochastic Gradient Descent (DP-SGD), perform well only on simple datasets and shallow networks, while recent transfer learning-based DP meth-ods often make unrealistic assumptions about the availabil-ity and distribution of public data. In this work, we argue that minimizing the number of trainable parameters is the key to improving the privacy-performance tradeoff of DP on complex visual recognition tasks. Inspired by this argu-ment, we also propose a novel transfer learning paradigm that ﬁnetunes a very sparse subnetwork with DP. We con-duct extensive experiments and ablation studies on two vi-sual recognition tasks: CIFAR-100 ! CIFAR-10 (standard
DP setting) and the CD-FSL challenge (few-shot, multiple levels of domain shifts) and demonstrate competitive exper-imental performance. 1.

Introduction
As computer vision becomes increasingly ubiquitous, the robustness and privacy of vision models are a growing concern. In fact, there are ample examples of privacy at-tacks on standard deep learning models successfully reveal-ing the contents of training data [40, 16, 35] – one attack was able to reconstruct credit card and social security num-bers [7]. This is particularly concerning in the ﬁeld of com-puter vision, where many applications, e.g., medical imag-ing, work with sensitive and legally-protected data. The onus of protecting private data falls upon machine learn-ing practitioners, and, indeed, this responsibility may be encoded into law by regulations such as the EU’s General
Data Privacy Regulation (GDPR) and the California Con-sumer Privacy Act [11].
Although many notions of privacy have been proposed, notably including k-anonymity [43], and its extension l-Figure 1. We propose a training framework that improves the privacy-utility trade-off for deep neural networks on complex vi-sual recognition tasks. By introducing a novel transfer learning paradigm, our model trained with differential privacy is able to achieve a performance comparable to its non-private counterpart. diversity [31], differential privacy [13] has emerged as the gold-standard for the ﬁeld. Differential privacy is a formal-ization of the notion of data privacy, providing strict upper bounds on the information about a data record, which may be obtained from resulting models [13, 9]. This is an at-tractive guarantee – many applications of computer vision involve sensitive datasets, which carry a strong obligation to protect user data in which the exposure of even singular data records is problematic [42, 3]. Differentially private models also beneﬁt from several auxiliary guarantees, such as robustness to adversarial examples [27], and compliance under data privacy laws [11]. Although differential privacy is not a concept native to machine learning research, arising instead from research into database privacy [13], differen-tially private machine learning has been a burgeoning ﬁeld of research [1].
In practice, there are many obstacles to building power-ful differentially private machine learning systems. There is an inherent tradeoff between model utility and privacy
[18] – larger networks, in particular, suffer from far greater disruption during training compared to their non-private 5059
counterparts, which results in signiﬁcant penalties to utility.
This is due to the implementation of differentially private machine learning – namely, differential privacy requires bounding the inﬂuence of each example on the mini-batch gradient. Given a data sample xi, the DP-SGD [1] algo-rithm clips the per-sample gradient g(xi) in `2 norm, i.e., the gradient vector g is replaced by g/ max(1, kgk2
C ) for a clipping threshold C. It is evident that the norm is propor-tional to the number of training parameters, and thus will be large in deep neural networks, which leads to dramati-cally greater gradient clipping in larger networks. On the other hand, deep convolutional neural networks have en-joyed great success in large-scale image and video recog-nition tasks, and it has been shown that the depth of neural networks is crucial for the expressive power of deep learn-ing [23].
In this work, we propose a novel solution to improve the privacy-utility tradeoff in deep neural networks with differ-ential privacy (Figure 1). Our key idea is to leverage ad-ditional, public, datasets to instill strong representations in large models, which are then adapted to private datasets at a minimal privacy cost. To further minimize the negative effect of differential privacy, we minimize the number of trainable parameters to only those necessary for effective transfer learning. Not all neurons are created equal – in particular, we identify normalization parameters [8, 51, 5] as carrying domain-speciﬁc information, and ﬁnd that the domain gap between public and private datasets can be sig-niﬁcantly minimized by only ﬁnetuning these parameters.
Besides, we draw insights from model pruning [15] and propose a novel approach to selecting and ﬁnetuning a very small subset of parameters in convolutional layers.
In summary, our key contributions are as follows: 1. We develop a method for effectively scaling differen-tial privacy to large neural networks, by leveraging out-of-domain transfer learning and sparse network ﬁne-tuning. 2. We enable differential private training of models on ex-tremely small (few-shot) private datasets at a reason-able privacy cost. 3. We achieve state-of-the-art performance with a smaller privacy budget on CIFAR-10, a prototypical bench-mark for differentially private machine learning. 2.