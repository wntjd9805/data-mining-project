Abstract
Video instance segmentation (VIS) is a new and critical task in computer vision. To date, top-performing VIS meth-ods extend the two-stage Mask R-CNN by adding a track-In ing branch, leaving plenty of room for improvement. contrast, we approach the VIS task from a new perspective and propose a one-stage spatial granularity network (SG-Net). Compared to the conventional two-stage methods,
SG-Net demonstrates four advantages: 1) Our method has a one-stage compact architecture and each task head (de-tection, segmentation, and tracking) is crafted interdepen-dently so they can effectively share features and enjoy the joint optimization; 2) Our mask prediction is dynamically performed on the sub-regions of each detected instance, leading to high-quality masks of ﬁne granularity; 3) Each of our task predictions avoids using expensive proposal-based
RoI features, resulting in much reduced runtime complex-ity per instance; 4) Our tracking head models objects’ cen-terness movements for tracking, which effectively enhances the tracking robustness to different object appearances. In evaluation, we present state-of-the-art comparisons on the
YouTube-VIS dataset. Extensive experiments demonstrate that our compact one-stage method can achieve improved performance in both accuracy and inference speed. We hope our SG-Net could serve as a strong and ﬂexible base-line for the VIS task. Our code will be available here1. 1.

Introduction
Video instance segmentation (VIS) is a challenging vi-sion task introduced by [43]. Given a video frame, an al-gorithm aims to perform the task of detection, segmenta-tion, and tracking of instances simultaneously. The VIS task holds valuable possibilities for applications which requires
*indicates equal contributions. 1https://github.com/goodproj13/SG-Net
Figure 1. A detailed comparison with MaskTrack R-CNN. The large images in the ﬁrst row are the results of our method. We fur-ther zoom in our results and compare against results from Mask-Track R-CNN. We dynamically divide a target instance into sub-regions based on its bounding box and perform instance segmen-tation on each sub-region to achieve spatial granularity. Both ex-amples are cropped from the original images for better illustration. video-level object masks and temporal descriptions such as augmented reality, video editing, and autonomous driving.
The seminal work, MaskTrack R-CNN [43] follows a two-stage paradigm as it is extended from Mask R-CNN
[13]. MaskTrack R-CNN ﬁrst uses region proposal net-work (RPN) from Faster R-CNN [31] to produce a set of candidate proposals. Then, regions-of-interests (RoI) features based on the proposals are cropped out and fed into each task head to predict bounding boxes, instance masks, and object trackings respectively. Despite a few works being proposed recently, the dominant VIS frame-works [11, 16, 19, 25, 26] follow the two-stage paradigm.
However, the two-stage paradigm may encounter a num-ber of issues. First, it is difﬁcult for each sub-task head (detection, segmentation, and tracking) in the two-stage ap-9816
×
× proach to share features, which causes a trouble for network architecture optimization. Second, the cropped RoI features are resized into patches of a uniformed size (e.g., 14 14 28 in Mask R-CNN [13]), which restricts the output or 28 resolution of the instance mask. This practice could par-ticularly compromise the prediction of large instances as they need higher resolutions to retain details of the object boundary. Third, the candidate proposals are redundant rep-resentations because their numbers are much larger than the
ﬁnal predictions. The mask head and tracking head have to repeatedly encode the proposal-based RoI features for the ﬁnal predictions. Speciﬁcally, the mask head requires a stack of convolutions (e.g., four 3 3 convolutional lay-ers in Mask R-CNN [13]) to obtain a receptive ﬁeld large enough to understand sufﬁcient image context. Therefore, the inference runtime is largely depended on the number of detected objects that appeared on the video frame and would degrade considerably with the increase of predictions.
×
Considering the above limitations, we attempt to solve the VIS task from a new perspective. We treat all three sub-tasks, detection, segmentation and tracking in VIS as interconnected problems that should be considered interde-pendently. Recent advances in object detection illuminate that one-stage methods such as FCOS [39] and CenterNet
[47] can outperform their two-stage counterparts in accu-racy. Both methods use ﬂexible convolutional operations, which are convenient for multi-task implementations and multi-network optimization (i.e., instance segmentation and tracking). Their successes enable the possibility to imple-ment a one-stage approach for the VIS task.
For the instance segmentation task, we recognize two important predecessors, BlendMask [5] and CondInst [38], which are both built on the FCOS framework. Compared to the Mask R-CNN-based approaches, they only use fully convolutional network (FCN) architectures, which help them to avoid the RoI operations (i.e., RoIpool, RoIAlign, cropping, and resizing). This improvement signiﬁcantly preserves the feature map resolution and retains details of the mask boundary. Moreover, both methods implement a light-weight mask head, which makes them very robust in real-time video tasks. However, both methods segment ob-jects on the instance level and ignore the potential to achieve more granularity on objects.
In this work, our primary focus is to investigate ways to circumvent the heavy proposal-based two-stage approach and ﬁnd a ﬂexible solution for the VIS task.
Inspired by [5, 38], we generalize the mask prediction by model-ing spatial scales of objects with semantic information for lower-level granularity. We also propose an easy track strat-egy, which directly uses the object centerness from detec-tion to delineate the video temporal coherence. Concretely, our work delivers the following contributions:
• We attempt to solve the VIS task from a new per-spective. To this end, we devise a compact one-stage method called SG-Net. Our method dynamically di-vides instance into sub-regions and performs segmen-tation on each region for Spatial Granularity, thus the name of our network, SG-Net. Compared to Mask-Track R-CNN [43], our method achieves more appeal-ing segmentation behavior as it can enrich object de-tails and produce masks with more accurate edges (As shown in Figure 1).
• Our method is proposal-free and efﬁcient. Remov-ing proposals allows us to assign heavier duties to the mask prediction module with an affordable computa-tion overhead. Particularly, our inference time does not increase with the growing number of predictions as the two-stage methods do.
• Our entire architecture consists of only convolutional operations, which is tied to the state of the art one-stage object detector, FCOS [39]. We organically craft each task head interdependently so they can effectively share features and enjoy joint optimization.
• Our tracking head models the movement of object cen-terness for tracking, which is simple and effective.
Compared to MaskTrack R-CNN [43], our tracking head is more robust to different object shapes and sizes as well as appearance changes. 2.