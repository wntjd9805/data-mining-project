Abstract
We present Modular interactive VOS (MiVOS) frame-work which decouples interaction-to-mask and mask propa-gation, allowing for higher generalizability and better per-formance. Trained separately, the interaction module con-verts user interactions to an object mask, which is then temporally propagated by our propagation module using a novel top-k ﬁltering strategy in reading the space-time memory. To effectively take the user’s intent into account, a novel difference-aware module is proposed to learn how to properly fuse the masks before and after each interac-tion, which are aligned with the target frames by employ-ing the space-time memory. We evaluate our method both qualitatively and quantitatively with different forms of user interactions (e.g., scribbles, clicks) on DAVIS to show that our method outperforms current state-of-the-art algorithms while requiring fewer frame interactions, with the addi-tional advantage in generalizing to different types of user interactions. We contribute a large-scale synthetic VOS dataset with pixel-accurate segmentation of 4.8M frames to accompany our source codes to facilitate future research. 1.

Introduction
Video object segmentation (VOS) aims to produce high-quality segmentation of a target object instance across an input video sequence, which has wide applications in video understanding and editing. Existing VOS methods can be categorized by the types of user input: semi-supervised methods require pixel-wise annotation of the ﬁrst frame, while interactive VOS approaches take user interactions (e.g., scribbles or clicks) as input where users can iteratively reﬁne the results until satisfaction.
This paper focuses on interactive VOS (iVOS) which
ﬁnds more applications in video editing, because typical user interactions such as scribbles or clicks (a few seconds per frame) are much easier than specifying full annotation
Source code, pretrained models and dataset are available at: https:
//hkchengrex.github.io/MiVOS. This research is supported in part by Kuaishou Technology and the Research Grant Council of the Hong
Kong SAR under grant no. 16201420.
Figure 1. User annotates one of the frames (e.g., with clicks at the top-left frame) and MiVOS bidirectionally propagates the masks to the entire video sequence. Our difference-aware fusion mod-ule guides the segmentation network to correct the masks across frames based on user’s intended correction on another frame (e.g., with scribbles on the bottom-right frame). (∼79 seconds per instance), with the iterative or successive reﬁnement scheme allowing the user more control over re-sult accuracy versus interaction budget trade-off [1].
Conceptually, iVOS can be considered as the combina-tion of two tasks: interaction understanding (e.g., mask gen-eration from interactions [2, 3, 4, 5]) and temporal propaga-tion (e.g., semi-supervised VOS methods [6, 7, 8]). Current methods usually perform the two tasks jointly, using inter-connected encoders [9, 10, 11] or memory-augmented in-teraction features [12, 13, 14]. The strong coupling limits the form of user interaction (e.g., scribbles only) and makes training difﬁcult. Attempts to decouple the two tasks fail to reach state-of-the-art accuracy [15, 16] as user’s intent cannot be adequately taken into account in the propagation process.
One advantage of uniﬁed methods over decoupled meth-ods is that the former can efﬁciently pick up small correc-tive interactions across many frames, which is suited to the
DAVIS evaluation robot [1]. However, we believe that hu-man users tend to interactively correct a single frame to high accuracy before checking other frames, as the visual examination itself takes time and human labor while free for an evaluation robot. Our method requires less interacted frames by letting the user focus on a single frame multiple times while attaining the same or even better accuracy. Our method is efﬁcient as single-frame interaction can be done almost instantly [4], with the more time-consuming propa-gation performed only sparsely. 5559
In this paper we present a decoupled modular framework to address the iVOS problem. Note that na¨ıve decoupling may lead to loss of user’s intent as the original interaction is no longer available in the propagation stage. This problem is circumvented by our new difference-aware fusion mod-ule which models the difference in the mask before and af-ter each interaction to inject the user’s intent in propagation.
Thus the user’s intent is preserved and propagated to the rest of the video sequence. We argue that mask difference is a better representation than raw interactions which is unam-biguous and does not depend on interaction types. With our decoupling approach, our method can accept different types of user interactions and achieve better performance on vari-ous qualitative and quantitative evaluations. Our main con-tributions can be summarized as follows:
• We innovate on the decoupled interaction-propagation framework and show that this approach is simple, ef-fective, and generalizable.
• We propose a novel lightweight top-k ﬁltering scheme for the attention-based memory read operation in mask generation during propagation.
• We propose a novel difference-aware fusion module to faithfully capture the user’s intent which improves iVOS accuracy and reduces the amount of user inter-action. We will show how to efﬁciently align the masks before and after an interaction at the target frames by using the space-time memory in propagation.
• We contribute a large-scale synthetic VOS dataset with 4.8M frames to accompany our source codes to facili-tate future research. 2.