Abstract
We present Neural Splines, a technique for 3D surface re-construction that is based on random feature kernels arising from inﬁnitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neu-ral network-based techniques and widely used Poisson Sur-face Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical ex-pressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants. 1.

Introduction
Estimating a 3D surface from a scattered point cloud is a classical and important problem in computer vision and computer graphics. In this task, the input is a set of 3D points sampled from an unknown surface and, possibly, normals of the surface at those points. The goal is to estimate a representation of the complete surface from which the input samples were obtained, for example, a polygonal mesh or an implicit function. This problem is challenging in practice: It is inherently ill-posed, since an inﬁnite number of surfaces can interpolate the data. Furthermore, the input 3D points are often incomplete and noisy, as they are acquired from range sensors such as LIDAR, structured light, and laser scans. Ideally, the recovered surface should not interpolate noise but preserve key features and surface details.
Many early surface reconstruction techniques consider a kernel formulation of the surface reconstruction prob-lem, using translation-invariant kernels such as biharmonic
RBFs [9], Gaussian kernels [22], or compactly supported
RBFs [34]. Currently, the most widely used method for
∗Work done prior to joining Amazon
Figure 1. Neural Splines use points (the white dots) and normals (the white arrows) as input and estimate a scalar function whose zero level set (the black lines) corresponds to the reconstructed surface and whose gradient agrees with the normals. surface reconstruction is Screened Poisson Surface Recon-struction [31], which solves a variant of the Poisson equation to ﬁnd an implicit function whose zero-level set produces an approximating surface. We show in this paper that this method can also be viewed as a kernel method for a particular choice of kernel.
More recently, many papers have used neural networks to represent an implicit function or a local chart in a manifold atlas as a means of reconstructing a surface
[44, 19, 26, 3, 24, 40, 42]. These methods can be integrated into a data-driven learning pipeline or directly applied in the so called “overﬁtting” regime, where a massively overpa-rameterized (i.e., more parameters than input points) neural network is ﬁtted to a single input point cloud as a functional 9949
representation for a surface. Empirical evidence has shown that these methods enjoy some form of “implicit regulariza-tion” that biases the recovered surface towards smoothness.
Moreover, employing early stopping in gradient descent can prevent these approaches from interpolating noise.
Under certain parameter initializations, inﬁnitely-wide neural networks de facto behave as kernel machines [30, 14], deﬁning Reproducing Kernel Hilbert Spaces (RKHS), whose kernel is obtained by linearizing the neural network mapping around its initialization. While the kernel regime simpli-ﬁes non-linear neural network learning into a convex pro-gram and provides a simple explanation for the success-ful optimization of overparameterized models, it cannot ex-plain the good generalization properties observed for high-dimensional problems due to the inability of the RKHS to ap-proximate non-smooth functions [4]. However, the situation for low-dimensional problems such as surface reconstruc-tion is entirely different, and RKHS can provide powerful characterizations of regularity. In this context, [45] shows that in the univariate case the RKHS norm associated with a wide ReLU network is a weighted curvature, and leads to cubic spline interpolants. In higher dimensions, similar (albeit more complex) characterizations of the RKHS norm exist [35] (see also Proposition 2). In order to assess the beneﬁts of neural networks on such low-dimensional prob-lems, it is thus important to ﬁrst understand their linearized counterparts, given by their associated kernel machines.
In this work, we demonstrate that in fact kernels arising from shallow ReLU networks are extremely competitive for 3D surface reconstruction, achieving state-of-the-art results: outperforming classical methods as well as non-linear meth-ods based on far more complex neural network optimization.
Kernels provide many advantages over neural networks in our context: (i) They are well understood theoretically. (ii) Kernel regression boils down to solving a linear system, and avoids gradient descent that suffers from slow conver-gence. (iii) Kernel-based interpolants are represented using a number of parameters that is linear in the size of the input, whereas overparameterized neural networks require many more parameters than points. (iv) The inductive bias of ker-nel methods can be characterized explicitly via the RKHS norm (Section 3.3). (v) Kernel methods lend to scalable and efﬁcient implementations (Section 3.5). We provide explicit expressions for two kinds of inﬁnite-width ReLU kernels, their derivatives, and their corresponding RKHS norms. We further argue that these kernels can be viewed as a multi-dimensional generalization of cubic spline interpolation in 1D. Moreover, we show that Poisson Surface Reconstruction can itself be viewed as a kernel method and give an expres-sion for its RKHS norm, suggesting that kernels are a broad framework which enable the rigorous understanding both traditional and modern surface reconstruction techniques. 1.1. Additional