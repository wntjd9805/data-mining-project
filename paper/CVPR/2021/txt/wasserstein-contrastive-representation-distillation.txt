Abstract
The primary goal of knowledge distillation (KD) is to en-capsulate the information of a model learned from a teacher network into a student network, with the latter being more compact than the former. Existing work, e.g., using Kullback-Leibler divergence for distillation, may fail to capture impor-tant structural knowledge in the teacher network and often lacks the ability for feature generalization, particularly in sit-uations when teacher and student are built to address differ-ent classiﬁcation tasks. We propose Wasserstein Contrastive
Representation Distillation (WCoRD), which leverages both primal and dual forms of Wasserstein distance for KD. The dual form is used for global knowledge transfer, yielding a contrastive learning objective that maximizes the lower bound of mutual information between the teacher and the stu-dent networks. The primal form is used for local contrastive knowledge transfer within a mini-batch, effectively matching the distributions of features between the teacher and the stu-dent networks. Experiments demonstrate that the proposed
WCoRD method outperforms state-of-the-art approaches on privileged information distillation, model compression and cross-modal transfer. 1.

Introduction
The recent success of deep learning methods has brought about myriad efforts to apply them beyond benchmark datasets, but a number of challenges can emerge in real-world scenarios. For one, as the scale of deep learning models continues to grow (e.g., [21, 15]), it has become in-creasingly difﬁcult to deploy such trained networks on more computationally-restrictive platforms, such as smart phones, remote sensors, and edge devices. Additionally, deep net-works require abundant data for training, but large datasets are often private [36], classiﬁed [29], or institutional [39], which the custodians may be hesitant to release publicly.
Labeled datasets in specialized domains may also be rare or expensive to produce. Finally, despite ample datasets from neighboring modalities, conventional frameworks lack clear ways to leverage cross-modal data.
*Equal contribution
Knowledge distillation (KD), which has become an in-creasingly important topic in the deep learning community, offers a potential solution to these challenges. In KD, the goal is to improve a student model’s performance by supple-menting it with additional feedback from a teacher model.
Often the teacher has larger capacity than the student, or has access to additional data that the student does not. As such, KD can transfer this additional and valuable knowledge from the teacher to the student. In early KD methods [23], this supplemental supervision was imposed by asking the student to minimize the Kullback-Leibler (KL) divergence between its output prediction distribution and the teacher’s.
Given that the prediction probability distribution contains richer and more informative signals than the one-hot labels, student models have been shown to beneﬁt from this extra supervision. However, the low dimensionality of prediction distribution means that the amount of information encoded (therefore transferred) can be limited. For cross-modal trans-fer, these predictions may even be irrelevant, making KL divergence unable to transfer meaningful information.
In contrast, intermediate representations present an oppor-tunity for more informative learning signals, as a number of recent works have explored [37, 50, 40, 42, 41]. However, as observed by [42], these methods often compare poorly with the basic KD, potentially due to the challenge of deﬁning a proper distance metric between features of the teacher and student networks. Furthermore, they heavily rely on strate-gies to copy teacher’s behavior, i.e., aligning the student’s outputs to those from the teacher. We argue that such prac-tice overlooks a key factor: the teacher’s experience may not necessarily generalize well to the student.
Motivated by this, we present Wasserstein Constrastive
Representation Distillation (WCoRD), a new KD frame-work that reduces the generalization gap between teacher and student to approach better knowledge transfer. Speciﬁ-cally, our approach constitutes distillation and generalization blocks, realized by solving the dual and primal form of the
Wasserstein distance (WD), respectively. For better distil-lation, we leverage the dual form of WD to maximize the mutual information (MI) between student and teacher repre-sentation distributions, using an objective inspired by Noise 16296
Contrastive Estimation (NCE) [18]. Unlike previous meth-ods [42], we propose to impose a 1-Lipschitz constraint to the critic via spectral normalization [31]. By shifting the critic to one based on optimal transport, we improve stability and sidestep some of the pitfalls of KL divergence minimiza-tion [8, 30]. We term this as global contrastive knowledge transfer.
For better generalization, we also use the primal form of
WD to indirectly bound generalization error via regulariz-ing the Wasserstein distance between the feature distribu-tions of the student and teacher. This results in a relaxation that allows for coupling student and teacher features across multiple examples within each mini-batch, as opposed to the one-to-one matching in previous methods (i.e., strictly copying the teacher’s behavior). In principle, this serves to directly match the feature distributions of the student and teacher networks. We term this local contrastive knowledge transfer. With the use of both primal and dual forms, we are able to maximize MI and simultaneously minimize the feature distribution discrepancy.
The main contributions are summarized as follows. (i)
We present a novel Wasserstein learning framework for rep-resentation distillation, utilizing the dual and primal forms of the Wasserstein distance for global constrastive learning and local feature distribution matching, respectively. (ii)
To demonstrate the superiority of the proposed approach, we ﬁrst conduct comprehensive experiments on benchmark datasets for model compression and cross-modal transfer. To demonstrate versatility, we further apply our method to a real-world dataset for privileged information distillation. 2.