Abstract
Image-to-image (I2I) translation methods based on gen-erative adversarial networks (GANs) typically suffer from overﬁtting when limited training data is available. In this work, we propose a data augmentation method (ReMix) to tackle this issue. We interpolate training samples at the fea-ture level and propose a novel content loss based on the perceptual relations among samples. The generator learns to translate the in-between samples rather than memoriz-ing the training set, and thereby forces the discriminator to generalize. The proposed approach effectively reduces the ambiguity of generation and renders content-preserving results. The ReMix method can be easily incorporated into existing GAN models with minor modiﬁcations. Experimen-tal results on numerous tasks demonstrate that GAN mod-els equipped with the ReMix method achieve signiﬁcant im-provements. 1.

Introduction
In recent years, Generative Adversarial Networks (GANs) [11] have shown much progress in numerous tasks including image-to-image translation. Well-designed adver-sarial losses [11, 27, 25, 1, 12, 26] provide effective domain-level supervision, making the translated results indistin-guishable from the real samples. The GAN-based methods heavily rely on vast quantities of training examples. For instance, Karras et al. [19, 20] use 70K high-quality face images to train their models. However, collecting a large amount of image data can be prohibitively expensive or im-plausible (e.g., for masterpieces by artists). This issue high-lights the importance of training GANs with limited data.
Unfortunately, reducing the amount of training data often leads to severe model overﬁtting. Recent ﬁndings [18, 42] reveal that GANs easily memorize a small training set and then render drastically degraded results in the testing set.
∗corresponding author
Figure 1. Overview of the proposed data augmentation method.
We use the image reconstruction task as an example. The input x is ﬁrst encoded into representation e and then decoded into the output y, and superscript indicates the index of samples. The in-terpolated data e′ is the convex combination of e1 and e2. In this case, we have d(e′, e1
), where d denotes the disc-tance function. We propose to maintain s(y′, x1
), where s is the similarity measure. Here we omit the outputs from x1 and x2 for clarity.
) > s(y′, x2
) < d(e′, e2
Some efforts have recently been made to tackle this prob-lem. The adaption-based approaches [24, 30] use exter-nal datasets as an alternative. They ﬁrst learn a semanti-cally related translation and then adapt it to the translation of interest. Despite the effectiveness, these approaches re-quire additional image collection. Several data augmenta-tion schemes [40, 18, 35, 42, 43] tailored for GANs have been developed to alleviate the need for additional datasets.
They use groups of image transformations (e.g., cropping, resizing, and cutout [10]) to augment the inputs of the dis-criminator. Even with limited data, these methods can pre-vent the discriminator from overﬁtting, allowing effective adversarial supervision. However, augmenting data for the generator is infeasible due to the problem of leaking [18].
For the image-to-image translation tasks, these methods cannot prevent the generator from memorizing how to trans-15018
late the given source images.
To facilitate training GANs with limited data in image-to-image translation, we propose a data augmentation strat-egy named ReMix. We mix source images in the feature space using convex combinations. The generator learns to map the mixed samples to the target space against over-ﬁtting.
In addition, the discriminator is improved in the process of distinguishing the augmented fake samples. We present a novel content loss that maintains the perceptual relations among the samples. The proposed loss avoids the model from producing ambiguous results from the aug-mented data. In Figure 1, the image reconstruction task is il-lustrated as an example. We aim to reconstruct two samples x1 and x2, and synthesize a virtual input e′ by interpolating the intermediate features e1 and e2. However, the recon-struction target for the input e′ is unknown, so the corre-sponding output y′ requires additional constraints on image content. To this end, we propose to constrain the perceptual relationships among {x1, x2, y′} based on the relationships among {e1, e2, e′}. Concretely, if e′ is closer to e1 (or e2), we then enforce the output y′ to be more similar to x1 (or x2) than the other one. In this manner, we provide effec-tive supervision and neatly sidestep estimating the targets for the interpolated inputs.
The ReMix method can be incorporated into existing methods easily. Only a few lines of codes are required to modify the original loss function. In the experiments, we evaluate the proposed method on several tasks, including cross-spectrum face translation on the CASIA dataset [23], animal face translation on the AFHQ dataset [7], and im-age synthesis from semantic label maps on the Cityscapes dataset [8]. We use the state-of-the-art models [37, 28, 7, 20] on these tasks as the baselines. Experimental re-sults demonstrate that the models equipped with the ReMix method achieve signiﬁcant improvements. We also train these models with 10% available data and still get compa-rable performances.
The main contributions are summarized as follows:
• We propose a data augmentation strategy based on feature-level interpolation. Our method reduces the overﬁtting problem of GANs, particularly for the image-to-image translation tasks.
• We propose to maintain the perceptual relations among samples to optimize the interpolated translations. Our scheme reduces the ambiguity of generation and forces the model to learn content-preserving translations.
• We achieve signiﬁcant improvements in multiple im-age synthesis tasks. In addition, we produce plausible results with only 10% training data. 2.