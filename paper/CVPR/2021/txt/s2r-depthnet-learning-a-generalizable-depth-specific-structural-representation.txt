Abstract
Human can infer the 3D geometry of a scene from a sketch instead of a realistic image, which indicates that the spatial structure plays a fundamental role in understand-ing the depth of scenes. We are the ﬁrst to explore the learning of a depth-speciﬁc structural representation, which captures the essential feature for depth estimation and ig-nores irrelevant style information. Our S2R-DepthNet (Syn-thetic to Real DepthNet) can be well generalized to un-seen real-world data directly even though it is only trained on synthetic data. S2R-DepthNet consists of: a) a Struc-ture Extraction (STE) module which extracts a domain-invariant structural representation from an image by dis-entangling the image into domain-invariant structure and domain-speciﬁc style components, b) a Depth-speciﬁc At-tention (DSA) module, which learns task-speciﬁc knowledge to suppress depth-irrelevant structures for better depth esti-mation and generalization, and c) a depth prediction mod-ule (DP) to predict depth from the depth-speciﬁc representa-tion. Without access of any real-world images, our method even outperforms the state-of-the-art unsupervised domain adaptation methods which use real-world images of the tar-get domain for training. In addition, when using a small amount of labeled real-world data, we achieve the state-of-the-art performance under the semi-supervised setting. 1.

Introduction
Monocular depth estimation is a long-standing challeng-ing task, which aims to predict the continuous depth value of each pixel from a single color image. This task has a wide range of application in various ﬁelds, such as au-tonomous driving [11, 14], 3D scene reconstruction [49, 21] and robot navigation [44], etc. Recently, a wide vari-ety of algorithms based on deep convolutional neural net-*This work was done when Xiaotian Chen was an intern at Microsoft
Research Asia.
†Corresponding author.
Figure 1. Visualization of our learnt structural representations. It can be seen that even though the input color images from synthetic dataset and real-world dataset are very different in appearance, our structural representations share many similarities, such as layout and object shapes, etc. Furthermore, our depth-speciﬁc structure map suppresses the depth-irrelevant structures on the smooth sur-face, e.g., lanes on the road and photos on the wall. works (DCNNs) have achieved good performance with suf-ﬁcient amounts of annotated data [17, 3, 46, 8, 27, 7, 6].
However, obtaining depth annotations is costly and time-consuming [11, 14, 1]. Some recent methods have investi-gated self-supervised depth estimation from stereo images pairs [11, 14] or video sequence [58, 15, 56] by view recon-struction. But stereo pairs or video sequences may not al-ways be available in existing datasets. Besides, these mod-els are often limited to the training dataset domain, having difﬁculty in scaling to various application scenes.
Some researchers switched to use synthetic images [9, 40] for training where depth annotations can be acquired directly. However, there is usually a domain gap between synthetic data and real-world data, which is caused by style discrepancies across different domains. To address this issue, some domain adaptation methods [55, 1, 25, 53] try to align the feature space of synthetic and real-world images [55, 25] or translate synthetic images to realistic-looking ones [55, 1, 53]. However, these methods all re-quire access to the real-world images of the target domain during the training process, but it is impractical to collect real-world images of various scenes. 3034
Given the above limitations, we consider a more practi-cal domain generalization scenario. In our setting, we only use a large amount of labeled synthetic data without access of any real-world images of the target domain in the train-ing stage. Compared to domain adaptation methods, this is a more difﬁcult task, because we do not even know the style of the real-world images during the training process.
Aiming for better generalizable depth estimation, we need to seek for the essential representation for this task.
Structure information is found to be very important for this task [18, 53, 60, 3, 35], and some previous works explore introducing heuristic structure information to network ar-chitecture [3, 35] or loss design [53, 18, 60, 35]. We are the
ﬁrst one to explore learning a depth-speciﬁc structural rep-resentation for generalizable depth estimation. The image representation can be decomposed into a domain-invariant structure component and a domain-speciﬁc style compo-nent [24, 19, 20]. The structure component can be fur-ther divided into a depth-speciﬁc structure component and a depth-irrelevant structure component. The depth-speciﬁc structure component is the most essential for depth estima-tion and can be effectively transferred from synthetic do-main to real-world domain.
In order to obtain the depth-speciﬁc structural represen-tation, we ﬁrst extract a general domain-invariant structure map from the image using a proposed Structure Extraction (STE) module by decomposing the image into structure and style components inspired by [20]. However, the structural representation we thus obtain is a general and low-level image structure, which contains a large amount of depth-irrelevant structures, such as structures on a smooth surface (e.g. lanes on the road or photos on the wall). Furthermore, we propose a Depth-speciﬁc Attention (DSA) module to ex-tract high-level semantic information from the input image and help to suppress the depth-irrelevant structures. Since only depth-speciﬁc structural information can pass the STE and DSA modules to the depth prediction (DP) module, our
S2R-DepthNet trained on synthetic data can be well gener-alized to unseen real-world images.
We visualise our learnt structural representation and depth-speciﬁc structural representation in Figure 1. Even though there is a distinct style difference between the im-ages from synthetic and real-world image dataset, our learnt structure maps and depth-speciﬁc structure maps share many similarities. Furthermore, the depth-speciﬁc structure map discards depth-irrelevant structures, e.g. lanes. The highlighted sky is an important cue for vanishing point that is helpful for depth estimation, which is similar to [18].
Main contributions: (i) We are the ﬁrst to learn a structural representation for generalizable depth estimation, which captures essential structural information and discards style information. S2R-DepthNet can be well generalized to unseen real-world data when only trained on synthetic data. (ii) We propose a two-stage structural representation learn-ing pipeline: a general low-level Structure Extraction mod-ule to discard style information and a Depth-speciﬁc At-tention module to suppress depth-irrelevant structure with depth-speciﬁc knowledge. (iii) We enable a more practical scenario for the depth estimation task, where there is only a large amount of synthetic data but it is hard to acquire real-world data images or depth annotations.
We carry out extensive experiments to demonstrate the effectiveness of our proposed domain-invariant structural representations. Even though we do not use real-world im-ages for training, our method still outperforms the state-of-the-art domain adaptation methods that use real-world im-ages of the target domain for training. Surprisingly, when our method uses a small amount of labeled real-world data for training, it also achieves the state-of-the-art performance under the semi-supervised setting. 2.