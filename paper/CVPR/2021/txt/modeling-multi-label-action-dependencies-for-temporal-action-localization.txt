Abstract
Real-world videos contain many complex actions with inherent relationships between action classes. In this work, we propose an attention-based architecture that models these action relationships for the task of temporal action localization in untrimmed videos. As opposed to previous works that leverage video-level co-occurrence of actions, we distinguish the relationships between actions that oc-cur at the same time-step and actions that occur at dif-ferent time-steps (i.e. those which precede or follow each other). We deﬁne these distinct relationships as action de-pendencies. We propose to improve action localization per-formance by modeling these action dependencies in a novel attention-based Multi-Label Action Dependency (MLAD) layer. The MLAD layer consists of two branches: a Co-occurrence Dependency Branch and a Temporal Depen-dency Branch to model co-occurrence action dependencies and temporal action dependencies, respectively. We ob-serve that existing metrics used for multi-label classiﬁca-tion do not explicitly measure how well action dependencies are modeled, therefore, we propose novel metrics that con-sider both co-occurrence and temporal dependencies be-tween action classes. Through empirical evaluation and extensive analysis, we show improved performance over state-of-the-art methods on multi-label action localization benchmarks (MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric. Code is publicly available at https://github.com/ptirupat/MLAD. 1.

Introduction
Understanding and localizing actions in complex video sequences is a heavily researched problem in computer vi-sion. The task of action localization in the untrimmed video involves predicting the action, or actions, present at each time-step of the video sequence. Several works present top-down methods, that propose temporal regions of a video which are then classiﬁed and reﬁned [5, 12, 38, 4, 14, 54].
Figure 1. Two action sequences from the MultiTHUMOS dataset.
The ﬁrst sequence (top) shows action dependencies within a given time-step: “Basketball Dribble” and “Run” co-occur. The bottom sequence shows action dependencies across time-steps: “Fall” fol-lows “Jump”. The table above each frame shows the comparison of probability scores predicted by our model with the I3D base-line for each action class present at that time-step. Modeling both types of dependencies is beneﬁcial for correctly detecting actions.
Other approaches produce bottom-up predictions for each time-step directly from the frame-level or clip-level features
[24, 22, 26, 37, 27]. Recent bottom-up methods tend to per-form best on the multi-label case, where multiple actions can be present within the same time-step.
Although these works achieve strong multi-label action localization performance, they do not explicitly model the relationships between the different action labels, which can be extremely useful for determining the presence or absence of classes within a video. Previous works have used label co-occurrence to improve performance on image classiﬁca-tion [44, 49, 9], and video action recognition [2, 30]. How-ever, the later works measure the video-level co-occurrence of actions, which does not differentiate between actions that occur within the same time-step and across different time-steps. This may be acceptable when the problem is video-level single-label action recognition, but when the task is 1460
to temporally localize multiple actions (as is the case with multi-label temporal action localization) the distinction be-tween these co-occurrences allows for more ﬁne-grained modeling of action relationships. We deﬁne these distinct action class relationships as action dependencies.
Videos contain two types of action dependencies: i) co-occurrence dependencies, involving actions that occur at the same time (this is most analogous to object class co-occurrence within images), and ii) temporal dependencies, involving actions that precede or follow each other. To illus-trate, consider Figure 1 showing sample frames from pole vault and basketball videos. An example of a co-occurrence dependency is present in the ﬁrst video snippet: the action
“run” often occurs with the action “basketball dribble” in a basketball game, so the presence of one action gives addi-tional prior information about the other. The second video snippet is an example of a temporal dependency. Using the available label information from the previous clips, one could infer the label following “jump” to be “fall” in the ﬁ-nal clip even without visual or motion features correspond-ing to the person performing the action.
In this work, we present a method that leverages both action dependency types to improve learned feature repre-sentations for the task of multi-label temporal action detec-tion. We propose an attention-based layer to reﬁne class-level features based on these dependencies. Co-occurrence dependencies are modeled by reﬁning action features based on the presence, or absence, of other actions within a time-step; temporal dependencies are modeled by reﬁning fea-tures based on all the time-steps of an input video sequence.
In both cases, attention maps are generated which allows for improved interpretability of our model. Differing from ac-tion recognition methods that employ class co-occurrence
[30], our approach does not require a ground-truth action co-occurrence matrix, but rather learns action dependencies from the training data.
To better understand how our approach models ac-tion dependencies, we present novel metrics for evaluat-ing temporal action localization methods. Whereas pre-vious multi-label evaluation methods, like mean average precision (mAP) and F1-score, tend to evaluate per-frame class performance independently, our proposed action-conditional precision and recall metrics explicitly measure how well pair-wise class/action dependencies are modeled both within a time-step and through different time-steps.
Our proposed metrics are general - they can be applied to both images and videos by measuring performance on both co-occurrence and temporal action dependencies.
Our main contributions include the following:
•
We present a novel network architecture that models both co-occurrence action dependencies and temporal action dependencies.
•
•
We propose multi-label performance metrics to mea-sure a method’s ability to model class co-occurrence across time-steps as well as within a time-step.
We evaluate the proposed approach on two large scale publicly available multi-label action datasets, outper-forming existing state-of-the-art methods. 2.