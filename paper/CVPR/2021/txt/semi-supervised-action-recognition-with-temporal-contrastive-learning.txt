Abstract
Learning to recognize actions from only a handful of la-beled videos is a challenging problem due to the scarcity of tediously collected activity labels. We approach this problem by learning a two-pathway temporal contrastive model using unlabeled videos at two different speeds lever-aging the fact that changing video speed does not change an action. Speciﬁcally, we propose to maximize the simi-larity between encoded representations of the same video at two different speeds as well as minimize the similarity between different videos played at different speeds. This way we use the rich supervisory information in terms of
‘time’ that is present in otherwise unsupervised pool of videos. With this simple yet effective strategy of manipulat-ing video playback rates, we considerably outperform video extensions of sophisticated state-of-the-art semi-supervised image recognition methods across multiple diverse bench-mark datasets and network architectures. Interestingly, our proposed approach beneﬁts from out-of-domain unlabeled videos showing generalization and robustness. We also per-form rigorous ablations and analysis to validate our ap-proach. Project page: https://cvir.github.io/TCL/. 1.

Introduction
Supervised deep learning approaches have shown re-markable progress in video action recognition [7, 15, 16, 17, 36, 49]. However, being supervised, these models are critically dependent on large datasets requiring tedious hu-man annotation effort. This motivates us to look beyond the supervised setting as supervised methods alone may not be enough to deal with the volume of information contained in videos. Semi-supervised learning approaches use structural invariance between different views of the same data as a source of supervision for learning useful representations. In recent times, semi-supervised representation learning mod-els [10, 29, 38, 50] have performed very well even surpass-ing its supervised counterparts in case of images [22, 47].
∗The ﬁrst two authors contributed equally. y c a r u c c a 1
-p o
T 60 50 40 30 20 10 0
Mini-Something-V2
Pseudo-label
Fully Supervised (100%)
TCL (Ours)
FixMatch y c a r u c c a 1
-p o
T 100 90 80 70 60 50
Jester
Pseudo-label
Fully Supervised (100%)
TCL (Ours)
FixMatch 1 5 10 15 20 25 30 33 0 5 10 15
Percentage  of labeled data
Percentage  of labeled data
Figure 1: Comparison of top-1 accuracy for TCL (Ours) with
Pseudo-Label [35] and FixMatch [47] baselines trained with different percentages of labeled training data. We evaluate the efﬁcacy of the approaches in terms of the least proportion of la-beled data required to surpass the fully supervised [36] perfor-mance (shown with the red dotted line). With only 33% and 15% of labeled data, our proposed TCL framework surpasses the su-pervised approaches in Mini-Something-V2 [23] and Jester [37] datasets respectively. The two other compared methods fail to reach the accuracy of the fully supervised approach with such small amount of labeled data. (Best viewed in color.)
Notwithstanding their potential, semi-supervised video action recognition has received very little attention. Triv-ially extending the image domain approaches to videos without considering the rich temporal information may not quite bridge the performance gap between the semi and the fully supervised learning. But, in videos, we have another source of supervision: time. We all know that an action rec-ognizer is good if it can recognize actions irrespective of whether the actions are performed slowly or quickly. Re-cently, supervised action recognition has beneﬁted a lot by using differently paced versions of the same video during training [17, 54]. Motivated by the success of using slow and fast versions of videos for supervised action recognition as well as by the success of the contrastive learning frame-works [26, 41], we propose Temporal Contrastive Learn-ing (TCL) for semi-supervised action recognition in videos where consistent features representing both slow and fast versions of the same videos are learned.
Starting with a model trained with limited labeled data, we present a two-pathway model that processes unlabeled videos at two different speeds and ﬁnds their representa-tions. Though played at two different speeds, the videos share the same semantics. Thus, similarity between these 10389    
representations are maximized. Likewise, the similarity be-tween the representations of different videos are minimized.
We achieve this by minimizing a modiﬁed NT-Xent con-trastive loss [10, 50] between these videos with different playback rates. While minimizing a contrastive loss helps to produce better visual representations by learning to be in-variant to different views of the data, it ignores information shared among samples of same action class as the loss treats each video individually. To this end, we propose a new per-spective of contrastive loss between neighborhoods. Neigh-borhoods are compact groups of unlabeled videos with high class consistency. In absence of ground-truth labels, groups are formed by clustering videos with same pseudo-labels and are represented by averaging the representations of the constituent videos. Contrastive objective between groups formed off the two paths explores the underlying class con-cept that traditional NT-Xent loss among individual video instances does not take into account. We term the con-trastive loss considering only individual instances as the instance-contrastive loss and the same between the groups as the group-contrastive loss respectively.
We perform extensive experiments on four standard datasets and demonstrate that TCL achieves superior per-formance over extended baselines of state-of-the-art image domain semi-supervised approaches. Figure 1 shows com-parison of TCL with Pseudo-Label [35] and FixMatch [47] trained using different percentages of labeled training data.
Using the same backbone network (ResNet-18), TCL needs only 33% and 15% of labeled data in Mini-Something-V2 [9] and Jester [37] respectively to reach the performance of the fully supervised approach [36] that uses 100% la-beled data. On the other hand, the two compared methods fail to reach the accuracy of the fully supervised approach with such small amount of labeled data. Likewise, we ob-serve as good as 8.14% and 4.63% absolute improvement in recognition performance over the next best approach, Fix-Match [47] using only 5% labeled data in Mini-Something-V2 [9] and Kinetics-400 [32] datasets respectively. In a new realistic setting, we argue that unlabeled videos may come from a related but different domain than that of the labeled data. For instance, given a small set of labeled videos from a third person view, our approach is shown to beneﬁt from us-ing only ﬁrst person unlabeled videos on Charades-Ego [44] dataset, demonstrating the robustness to domain shift in the unlabeled set. To summarize, our key contributions include:
• First of all, we treat the time axis in unlabeled videos specially, by processing them at two different speeds and propose a two-pathway temporal contrastive semi-supervised action recognition framework.
• Next, we identify that directly employing con-trastive objective instance-wise on video representa-tions learned with different frame-rates may miss cru-cial information shared across samples of same in-herent class. A novel group-contrastive loss is pio-neered to couple discriminative motion representation with pace-invariance that signiﬁcantly improves semi-supervised action recognition performance.
• We demonstrate through experimental results on four datasets, TCL’s superiority over extended baselines of successful image-domain semi-supervised approaches.
The versatility and robustness of our approach in case of training with unlabeled videos from a different do-main is shown along with in-depth ablation analysis pinpointing the role of the different components. 2.