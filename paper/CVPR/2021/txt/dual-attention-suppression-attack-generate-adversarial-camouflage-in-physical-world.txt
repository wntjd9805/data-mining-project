Abstract
Deep learning models are vulnerable to adversarial ex-amples. As a more threatening type for practical deep learning systems, physical adversarial examples have re-ceived extensive research attention in recent years. How-ever, without exploiting the intrinsic characteristics such as model-agnostic and human-speciﬁc patterns, existing works generate weak adversarial perturbations in the physical world, which fall short of attacking across different models and show visually suspicious appearance. Motivated by the viewpoint that attention reﬂects the intrinsic characteristics of the recognition process, this paper proposes the Dual
Attention Suppression (DAS) attack to generate visually-natural physical adversarial camouﬂages with strong trans-ferability by suppressing both model and human attention.
As for attacking, we generate transferable adversarial cam-ouﬂages by distracting the model-shared similar attention patterns from the target to non-target regions. Meanwhile, based on the fact that human visual attention always focuses on salient items (e.g., suspicious distortions), we evade the human-speciﬁc bottom-up attention to generate visually-natural camouﬂages which are correlated to the scenario context. We conduct extensive experiments in both the digi-tal and physical world for classiﬁcation and detection tasks on up-to-date models (e.g., Yolo-V5) and demonstrate that our method outperforms state-of-the-art methods.1 1.

Introduction
Deep neural networks (DNNs) have achieved remarkable performance across a wide areas of applications, e.g., com-puter vision [24, 35], natural language [42], and acoustics
[34], etc, but they are vulnerable to adversarial examples
*Corresponding author 1Our code can be found in https://github.com/nlsde-safety-team/DualAttentionAttack. (a) (c) (b) (d)
Figure 1. (a) shows the suspicious appearance of camouﬂages gen-erated by previous work (i.e., UPC [19]). (b) is the painted car that commonly exists in the physical world. (c) shows the adversar-ial example (classiﬁed as pop bottle) generated by existing work (i.e., CAMOU [52]) and its corresponding attention map. (d) shows the adversarial example (classiﬁed as Shih-Tzu) gen-erated by our DAS and its distracted attention map.
[44, 36]. These elaborately designed perturbations are im-perceptible to humans but can easily lead DNNs to wrong predictions, which pose a strong security challenge to deep learning applications in both the digital and physical world
[22, 13, 31, 37, 51].
In the past years, a long line of work has been pro-posed to perform adversarial attacks in different scenarios under different settings [26, 7, 2]. Though challenging deep learning, adversarial examples are also valuable for under-standing the behaviors of DNNs, which could provide in-sights into the blind-spots and help to build robust mod-els [20, 45, 28, 50]. Generally, adversarial attacks can be divided into two categories: digital attacks, which attack
DNNs by perturbing the input data in the digital space; and physical attacks, which attack DNNs by modifying the vi-8565
sual characteristics of the real object in the physical world.
In contrast to the attacks in the digital world [23, 48, 21, 52], adversarial attacks in the physical world are more challeng-ing due to the complex physical constraints and conditions (e.g., lighting, distance, camera, etc.), which will impair the attacking ability of generated adversarial perturbations
[12]. In this paper, we mainly focus on the more challenging physical world attack task, which is also more meaningful to the deployed deep learning applications in practice.
Though several attempts have been adopted to perform physical attacks [31, 19, 30], existing works always ig-nore the intrinsic characteristics such as model-agnostic and human-speciﬁc patterns so that their attacking abilities are still far from satisfactory. In particular, the limitations can be summarized as (1) the existing methods ignore the com-mon patterns among models and generate adversarial per-turbations using model-speciﬁc clues (e.g., gradients and weights of a speciﬁc model), which fails to attack across different target models. In other words, the transferability of adversarial perturbations is weak, which impairs their at-tacking abilities in the physical world; (2) current methods generate adversarial perturbations with a visual suspicious appearance which is poorly aligned with human perception and even attracts the human attention. For example, painted on the adversarial camouﬂage [19], the classiﬁer misclassi-ﬁes the car into a bird. However, as shown in Figure 1(a), the camouﬂage apparently contains un-natural and suspi-cious bird-related features (e.g., bird head), which attracts human attention.
To address the mentioned problems, this paper proposes the Dual Attention Suppression (DAS) attack by suppress-ing both the model and human attention. Regarding the transferability for attacks, inspired by the biological ob-servation that cerebral activities between different individ-uals share similar patterns when stimulus features are en-countered [49] (i.e., selected attention [27]), we perform adversarial attacks by suppressing the attention patterns shared among different models. Speciﬁcally, we distract the model-shared similar attention from target to non-target re-gions via connected graphs. Thus, target models will be misclassiﬁed by not paying attention to the objects in the target region. Since our generated adversarial camouﬂage captures model-agnostic structures, it can transfer among different models, which improves the transferability.
As for the visual naturalness, psychologists have found that the bottom-up attention of human vision will alert peo-ple to salient objects (e.g., distortion) [6]. Existing methods generate physical adversarial examples with visually suspi-cious appearance, which shows salient features to human perception. Thus, we try to evade this human-speciﬁc vi-sual attention by generating adversarial camouﬂage which contains high semantic correlation to scenario context. As a result, the generated camouﬂage is more unsuspicious and natural in terms of human perception. Figure 1(c) is the adversarial camouﬂage generated by CAMOU [52] which is suspicious to human vision. By contrast, our generated adversarial camouﬂage yields a more natural appearance as shown in Figure 1(d).
To the best of our knowledge, we are the ﬁrst to exploit the shared attention characteristics among models and gen-erate adversarial camouﬂages world by suppressing both the model and human attention. Extensive experiments in both the digital and physical world on both classiﬁcation and detection tasks are conducted which demonstrate that our method outperforms other state-of-the-art methods. 2.