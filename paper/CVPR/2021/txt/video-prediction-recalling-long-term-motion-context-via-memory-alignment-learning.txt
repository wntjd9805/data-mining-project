Abstract
Our work addresses long-term motion context issues for predicting future frames. To predict the future precisely, it is required to capture which long-term motion context (e.g., walking or running) the input motion (e.g., leg move-ment) belongs to. The bottlenecks arising when dealing with the long-term motion context are: (i) how to predict the long-term motion context naturally matching input se-quences with limited dynamics, (ii) how to predict the long-term motion context with high-dimensionality (e.g., com-plex motion). To address the issues, we propose novel mo-tion context-aware video prediction. To solve the bottle-neck (i), we introduce a long-term motion context memory (LMC-Memory) with memory alignment learning. The pro-posed memory alignment learning enables to store long-term motion contexts into the memory and to match them with sequences including limited dynamics. As a result, the long-term context can be recalled from the limited in-put sequence. In addition, to resolve the bottleneck (ii), we propose memory query decomposition to store local motion context (i.e., low-dimensional dynamics) and recall the suit-able local context for each local part of the input individu-ally. It enables to boost the alignment effects of the mem-ory. Experimental results show that the proposed method outperforms other sophisticated RNN-based methods, espe-cially in long-term condition. Further, we validate the ef-fectiveness of the proposed network designs by conducting ablation studies and memory feature analysis. The source code of this work is available†. 1.

Introduction
Video prediction in computer vision is to estimate up-coming future frames at pixel-level from given previous frames. Since predicting the future is an important base-∗Corresponding author
†https://github.com/sangmin-git/LMC-Memory ment for intelligent decision-making systems, the video pre-diction has attracted increasing attention in industry and
It has the potential to be applied to var-research ﬁelds. ious tasks such as weather forecasting [40], trafﬁc situa-tion prediction [5], and autonomous driving [4]. However, the pixel-level video prediction is still challenging mainly due to the difﬁculties of capturing high-dimensionality and long-term motion dynamics [11, 33, 34, 36].
Recently, several studies with deep neural networks (DNNs) have been proposed to capture the high-dimensionality and the long-term dynamics of video data in the video prediction ﬁeld [7, 11, 29, 33–36]. The models considering the high-dimensionality of videos tried to sim-plify the problem by constraining motion and disentangling components [7, 11, 33]. However, these methods did not consider the long-term frame dynamics, which leads to pre-dicting blurry frames or wrong motion trajectories. Recur-rent neural networks (RNNs) have been developed to cap-ture the long-term dynamics with consideration for long-term dependencies in the video prediction [34–36]. The long-term dependencies in the RNNs is about remember-ing past step inputs. The RNN-based methods exploited the memory cell states in the RNN unit. The cell states are re-currently changed according to the current input sequence to remember the previous steps of the sequence. However, it is difﬁcult to capture the long-term motion dynamics for the input sequence with limited dynamics (i.e., short-term mo-tion) because such cell states mainly depend on revealing relations within the current input sequence. For example, given short-length input frames for a walking motion, the leg movement from the input is limited itself. Therefore, it is difﬁcult to grasp what will happen to the leg in the fu-ture through the cell states of the RNNs. In this case, the long-term motion context of the partial action may not be properly captured by the RNN-based methods.
Our work addresses long-term motion context issues for predicting future frames, which have not been properly dealt with in previous video prediction works. To predict the future precisely, it is required to capture which long-3054
term motion context the input motion belongs to. For exam-ple, in order to predict the future of leg movement, we need to know such partial leg movement belongs to either walk-ing or running (i.e., long-term motion context). The bot-tlenecks arising when dealing with long-term motion con-text are as follows: (i) how to predict the long-term motion context naturally matching input sequences with limited dy-namics, (ii) how to predict the long-term motion context with high-dimensionality.
In this paper, we propose novel motion context-aware video prediction to address the aforementioned issues. To solve the bottleneck (i), we introduce a long-term motion context memory (LMC-Memory) with memory alignment learning. Contrary to the internal memory cells of the
RNNs, the LMC-Memory externally exists with its own parameters to preserve various long-term motion contexts of training data, which are not limited to the current input.
Memory alignment learning is proposed to effectively store long-term motion contexts into the LMC-Memory and re-call them even with inputs having limited dynamics. Mem-ory alignment learning contains two training phases to align long-term and short-term motions: hPhase 1i storing long-term motion context from long-term sequences into the memory, hPhase 2i matching input short-term sequences with the stored long-term motion contexts in the memory.
As a result, the long-term motion context (e.g., long-term walking dynamics) can be recalled from the input short-term sequence alone (e.g., short-term walking clip).
Furthermore, to resolve the bottleneck (ii), we propose decomposition of a memory query that is used to store and recall the motion context. Even if various motion contexts of training data are stored in the LMC-Memory, it is difﬁcult to capture the motion context that is exactly matched with the input. This is because motions of video sequences have high-dimensionality (e.g., complex motion with local mo-tion components). The dimensionality indicates the number of pixels in a video sequence. Since each motion is slightly different from one another in a global manner even for the same category, the proposed memory query decomposition is useful in that it enables to store local context (i.e., low-dimensional dynamics) and recall the suitable local context for each local part of the input individually. It can boost the alignment effects between the input and the stored long-term motion context in the LMC-Memory.
The major contributions of the paper are as follows.
• We introduce novel motion context-aware video pre-diction to solve the inherent problem of the RNN-based methods in capturing long-term motion context.
We address the arising long-term motion context issues in the video prediction.
• We propose the LMC-Memory with memory align-ment learning to address storing and recalling long-term motion contexts. Through the learning, it is pos-sible to recall long-term motion context corresponding to an input sequence even with limited dynamics.
• To address the high-dimensionality of motions, we de-compose memory query to separate an overall motion into local motions with low-dimensional dynamics. It makes it possible to recall suitable local motion con-text for each local part of the input individually. 2.