Abstract
Tasks that involve high-resolution dense prediction re-quire a modeling of both local and global patterns in a large input ﬁeld. Although the local and global structures of-ten depend on each other and their simultaneous modeling is important, many convolutional neural network (CNN)-based approaches interchange representations in different resolutions only a few times. In this paper, we claim the im-portance of a dense simultaneous modeling of multiresolu-tion representation and propose a novel CNN architecture called densely connected multidilated DenseNet (D3Net).
D3Net involves a novel multidilated convolution that has different dilation factors in a single layer to model differ-ent resolutions simultaneously. By combining the multidi-lated convolution with the DenseNet architecture, D3Net incorporates multiresolution learning with an exponentially growing receptive ﬁeld in almost all layers, while avoiding the aliasing problem that occurs when we naively incorpo-rate the dilated convolution in DenseNet. Experiments on the image semantic segmentation task using Cityscapes and the audio source separation task using MUSDB18 show that the proposed method has superior performance over state-of-the-art methods. 1.

Introduction
Dense prediction tasks such as semantic segmenta-tion and audio source separation typically accept high-dimensional input data and produce predictions with the same (or similar) dimensions. To efﬁciently handle high-dimensional data and model the context that lies in a large
ﬁeld, various neural network architectures have been pro-posed [24, 38, 47, 44].
In particular, convolutional neu-ral networks (CNNs) have become an essential component, and a variety of advanced CNN architectures have been pro-posed to improve performance on the basis of motivations such as making the networks deeper while improving a gra-dient ﬂow [13, 16, 18], multibranch convolution [40, 39] and explicitly modeling interchannel dependences of con-volutional features [14]. One key component of these archi-tectures is a skip connection that creates short paths from
In [16], a simple yet power-early layers to later layers. ful skip connectivity pattern that connects all preceding lay-ers, called DenseNet, is proposed. Such dense connectivity allows maximum information ﬂow, making CNNs deeper while keeping the model size small by efﬁciently reusing intermediate representations of preceding layers.
One of the beneﬁts of a deeper CNN is its larger recep-tive ﬁeld that allows a large context to be modeled, which is important for tasks that require the utilization of a wide-area or long-term dependence in a high-resolution input.
For example, sufﬁciently large parts of objects have to be modeled for semantic segmentation [2, 50, 1, 55, 56, 33, 47, 8, 52, 57], whereas modeling a long-term dependence is shown to be important for various audio tasks such as audio event recognition and source separation [43, 42, 41]. Al-though the receptive ﬁeld grows linearly with respect to the number of layers stacked, the simple stacking of convolu-tion layers is not the optimal way to increase it, as too many layers are required to cover a sufﬁciently large input, which makes the network training difﬁcult. A popular approach to incorporate a large context with a reasonable model size is to repeatedly downsample intermediate network outputs and apply operations in lower resolution representations. In dense prediction tasks, the low-resolution representations are again upsampled to recover the resolution lost while car-rying over the global perspective from downsampled layers
[28, 29, 24, 47, 41]. Another approach is dilated convo-lution, where dilation factors are set to grow exponentially as layers are stacked; and therefore, the networks cover a large receptive ﬁeld with a small number of layers. Dilated convolution is shown to be effective for many tasks that re-quire high-resolution dense predictions [50, 3, 46]. Most previously proposed CNN architectures interchange infor-mation in different resolutions only a few times, e.g., once or a few times at the end of the network [24, 55, 56], or once at the beginning or end of each module [29, 47]. However, since the local and global patterns can depend on each other, i.e., a local structure can be more accurately estimated by knowing a global structure and vice versa, a more frequent (dense) interchange of information among representations in multiple resolutions could be beneﬁcial.
In this work, we propose a novel CNN architecture for 993
Figure 1. Illustration of D2 block. (a) The connectivity pattern is the same as that in DenseNet except that the D2 block involves the multidilated convolution. (b) Illustration of the multidilated convolution at the third layer. The production of a single feature map involves multiple dilation factors depending on the input channel. For clarity, we omit the normalization and nonlinearity from the illustration. densely incorporating representations in multiple resolu-tions. We combine advantages of the dense skip connec-tions and dilated convolution, and propose a novel net-work architecture called the multidilated dense block (D2 block). To appropriately combine them, we propose a mul-tidilated convolution layer that has multiple dilation factors within a single layer. A dilation factor depends on which skip connection the feature maps come from, as shown in
Fig.1. Multidilated convolution can prevent the occurrence of aliasing that occurs when a standard dilated convolution is applied to feature maps whose receptive ﬁeld is smaller than the dilation factor. Furthermore, we propose a nested architecture of multidilated dense blocks to effectively re-peat dilation factors multiple times with dense connections that ensure sufﬁcient depth, which is required for model-ing each resolution. We call the nested architecture densely connected multidilated DenseNet (D3Net) 1.
Although neural network architecture search (NAS) has been actively investigated to automatically ﬁnd a suitable network architecture [21, 26], it is often difﬁcult to identify the key element for achieving good performance from the learnt architecture. We believe that this work provides an-other insight into the design of CNN architectures for dense prediction tasks, namely, the frequent interchange of infor-mation in multiple resolutions.
The contributions of this work are as follows: (i) We claim the importance of the dense multiresolution repre-sentation learning and propose the D2 block that combines dense skip connections with dilated convolution. The D2 block incorporates a novel multidilated convolution that en-ables multiresolution information interchange in most of the layers while avoiding the aliasing problem that occurs in a naive way of incorporating dilation in DenseNet. (ii) We further introduce a nested architecture of multidilated dense blocks called the D3 block to effectively apply different di-lation factors multiple times to provide a sufﬁcient model-ing capacity in each resolution. (iii) We conduct intensive 1Code is available at https : / / github . com / sony / ai -research-code/tree/master/d3net experiments on two dense prediction tasks in different do-mains (image semantic segmentation and audio source sep-aration) and show the effectiveness of the proposed meth-ods. The proposed architecture exhibits superior perfor-mance over state-of-the-art baselines in both tasks, demon-strating its generality against the task type and data domain. 2.