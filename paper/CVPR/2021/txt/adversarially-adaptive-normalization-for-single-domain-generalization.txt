Abstract
Single domain generalization aims to learn a model that performs well on many unseen domains with only one do-main data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model’s generalization capability. The impact on do-main generalization of the statistics of normalization lay-In this paper, we propose ers is still underinvestigated. a generic normalization approach, adaptive standardiza-tion and rescaling normalization (ASR-Norm), to comple-ment the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental re-sults show that ASR-Norm can bring consistent improve-ment to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improve-ment introduced by ASR-Norm is agnostic to the choice of
ADA methods. 1.

Introduction
Deep learning has achieved remarkable success in vari-ous areas [25, 28] where the training and test data are sam-pled from the same domain. In real applications, however, there is a great chance of applying a deep learning model to the data from a new domain unseen in the training dataset.
The model that performs well on the training domain often cannot maintain a consistent performance on a new domain
[5, 53], due to the cross-domain distributional shift [37].
To address the potential discrepancies between the train-ing and test domains, a number of works [6, 20, 61] have been proposed to learn domain-invariant features using the
∗The main work was done during an internship at Google Research.
Figure 1: Illustration of single domain generalization with the PACS [30] benchmark. The dataset contains 4 domains: art paint, cartoon, sketch, and photo domains, which share the same categories that include dog, elephant, giraffe, gui-tar, house, horse, and person. Single domain generalization aims at training a model on one source domain data (art paint domain in the shown case), while generalizing well to other domains with very different visual presentations. training data from multiple source domains [30, 45] to im-prove the model’s generalization capability across domains.
However, acquiring multi-domain training data is both chal-lenging and expensive. Alternatively, a more practical but less investigated solution is to train the model on a single source domain data and enhance its capability of general-izing to other unseen domains (see the example in Fig. 1).
This emerging learning paradigm is referred to as single do-main generalization [42].
Existing works on single domain generalization [20, 42, 52, 53, 60] try to improve the generalization capability through adversarial domain augmentation (ADA), which synthesizes new training images in an adversarial way to mimic virtual challenging domains. The model therefore learns the domain-invariant features to improve its gener-alization performance. In this work, we propose to tackle the single domain generalization challenge from a differ-ent perspective, building an adaptive normalization in the
ADA framework to improve the model’s domain general-18208
ization named as adaptive standardization and rescaling normalization (ASR-Norm), in which the standardization and rescaling statistics are both learned to be adaptive to each individual input sample. When being used with ADA
[20, 52, 53], ASR-Norm can learn the normalization statis-tics by approximately optimizing a robust objective, making the statistics be adaptive to the data coming from different domains, and hence helping the model to generalize better across domains than traditional normalization approaches.
We also show that ASR-Norm can be viewed as a generic form of the traditional normalization approaches including
BN, IN, layer normalization (LN) [1], group normalization (GN) [55], and switchable normalization (SN) [32].
Our main contributions are as follows: (1) We propose a novel adaptive normalization, the missing ingredient for current works on ADA for single domain generalization.
To the best of our knowledge, the proposed ASR-Norm is the ﬁrst to learn both standardization and rescaling statistics in normalization with neural networks. (2) We show that
ASR-Norm can bring consistent improvements to the state-of-the-art ADA approaches on three commonly used sin-gle domain generalization benchmarks. The performance gain is agnostic to the choice of ADA methods and becomes more signiﬁcant as the domain discrepancy increases. 2.