Abstract
Human pose transfer has received great attention due to its wide applications, yet is still a challenging task that is not well solved. Recent works have achieved great suc-cess to transfer the person image from the source to the target pose. However, most of them cannot well capture the semantic appearance, resulting in inconsistent and less realistic textures on the reconstructed results. To address this issue, we propose a new two-stage framework to han-dle the pose and appearance translation. In the ﬁrst stage, we predict the target semantic parsing maps to eliminate the difﬁculties of pose transfer and further beneﬁt the lat-ter translation of per-region appearance style. In the sec-ond one, with the predicted target semantic maps, we sug-gest a new person image generation method by incorpo-rating the region-adaptive normalization, in which it takes the per-region styles to guide the target appearance gen-eration. Extensive experiments show that our proposed
SPGNet can generate more semantic, consistent, and photo-realistic results and perform favorably against the state of the art methods in terms of quantitative and qualitative evaluation. The source code and model are available at https://github.com/cszy98/SPGNet.git. 1.

Introduction
Given an image of a speciﬁc person under a certain per-spective or pose, the human pose transfer task aims to gen-erate images of the person with the same appearance and meanwhile under the given target pose, which more impor-tantly, are expected to be as photo-realistic as possible. The task has a wide range of applications, such as generating videos with pose sequences [2, 8, 12, 19, 40], data augmenta-tion for person re-identiﬁcation task [28, 47] and multi-view display for virtual try-on [14, 41, 42].
Basically, human pose transfer is a non-rigid deformation of the 3D human. Only using one 2D source image and target pose to generate another view of the human body is still a challenging task due to the following difﬁculties: (i) spatial rearrangement of the appearance features, (ii) the inference of the self-occlusion region, (iii) the photo-realistic results.
All these make the task valuable and remain an active topic in the community of computer vision.
On the one hand, along with the rapid development of deep learning, cGAN [24] based human pose transfer meth-ods [22, 27, 36] have achieved signiﬁcant progress. However, this task is an unaligned image to image translation due to the inconsistent poses between the source and target images.
Directly taking the concatenation of the source appearance and target pose into a general encoder-decoder framework cannot fully exploit the correspondence between the source and target appearance, thus resulting in less realistic results.
On the other hand, to facilitate the spatial rearrangement of the source appearance, deformation [18, 19, 29, 31] and disentanglement [5, 23] based methods have been suggested.
Albeit the appearance alignment problem can be addressed, the generated distortion and blurry appearance caused by the warped and disentangled features are inevitable, giving rise to visually unpleasing results. The recent progressive gener-ation [35, 47] also achieved plausible performance, but the
ﬁnal results usually lose semantic details. Though improve-ments have been obtained, the semantic generation remains uninvestigated reasonably in the human pose transfer task.
In this paper, we present a two-stage framework to ad-dress the above issues. In the ﬁrst stage, we predict the target semantic parsing maps. Compared with these directly generating the target image, the prediction of target parsing maps is much easier, because we do not need to consider the texture translation and this will make the network focus on only one single task, i.e., pose transfer without considering the effect of appearance that may bring the burden to the learning of the network. With the given pose and source parsing map, we suggest a SPATN model to generate the target semantic maps in a progressive manner. We observe that the target semantic maps can bring the following ben-eﬁts: (i) it can not only provide the pose information, but 10806
also the speciﬁcation of each body region, making our model more robust when dealing with complex poses, (ii) with the per-region styles, it can help to transfer them to the target region separately, which is more effective in generating the semantic and photo-realistic results. In the second stage, we propose a SPGNet by incorporating the semantic region adaptive normalization to generate the target image. To be speciﬁc, it takes the per-region styles that are extracted from the source appearance and then broadcasted to the target regions to assist the semantic generation on each body part, resulting in the ﬁnal photo-realistic results. This manner can well address the inconsistent poses and self-occlusion prob-lems. We note that there are also some works that attempt to utilize the predicted target parsing maps to improve the generation of the ﬁnal results [4, 23, 33]. However, Dong et al. [4] and Song et al. [33] both directly take the pre-dicted target parsing map and source image as input. This is more like a cGAN, which can bring limited improvements to the ﬁnal semantic reconstruction. Men et al. [23] also suggest the extraction of per-region style codes, but these codes are utilized in the reconstruction process by the AdaIN layer [11], in which the learned afﬁne parameters from the source appearance are uniform across spatial coordinates.
We argue that compared with the global normalization, the region adaptive normalization is more ﬂexible and suitable for this task by specifying the style to each target region.
Experiments are conducted to evaluate the effectiveness of our proposed SPGNet on two challenging datasets, i.e.,
DeepFashion [21] and Market-1501 [45]. Results show that our SPGNet performs favorably against the state of the arts, and yields more consistent and photo-realistic results. The main contributions of this work include:
• We predict the target semantic maps as a supplement to the pose representation, and then further utilize it to the per-region style translation by region adaptive normalization, thereby eliminating the difﬁculties of the pose transfer and improving the reconstruction quality.
• In comparison with these state of the arts methods, ex-periments demonstrate the superiority of our SPGNet in generating favorable and photo-realistic person image. 2.