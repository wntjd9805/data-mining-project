Abstract
Camera pose estimation in known scenes is a 3D geom-etry task recently tackled by multiple learning algorithms.
Many regress precise geometric quantities, like poses or 3D points, from an input image. This either fails to generalize to new viewpoints or ties the model parameters to a speciﬁc scene. In this paper, we go Back to the Feature: we argue that deep networks should focus on learning robust and invariant visual features, while the geometric estimation should be left to principled algorithms. We introduce PixLoc, a scene-agnostic neural network that estimates an accurate 6-DoF pose from an image and a 3D model. Our approach is based on the direct alignment of multiscale deep features, casting camera localization as metric learning. PixLoc learns strong data priors by end-to-end training from pixels to pose and ex-hibits exceptional generalization to new scenes by separating model parameters and scene geometry. The system can local-ize in large environments given coarse pose priors but also improve the accuracy of sparse feature matching by jointly reﬁning keypoints and poses with little overhead. The code will be publicly available at github.com/cvg/pixloc. 1.

Introduction
Visual localization is the problem of estimating the cam-era position and orientation for a given image in a known scene. Solving this problem is a key step towards truly autonomous robots such as self-driving cars and is a pre-requisite for Augmented and Virtual Reality systems.
State-of-the-art approaches to visual localization com-monly rely on correspondences between 2D pixel positions and 3D points in the scene [13,16,29,52,70,71,73,81,82,84].
Such a formulation estimates the camera pose using a
Perspective-n-Point (PnP) solver [1, 14, 31, 39, 40] inside a RANSAC loop [6, 19, 28, 43]. These 2D-3D correspon-dences are traditionally computed by matching local image features. Recent localization systems can handle large scenes with complex geometry and appearance changes over time.
*denotes equal contribution
R, t ? fe atures
CNN 3D  model reference query
PixLoc
Localization without retraining
Feature 
Alignment
Pose
R, t
Loss
Figure 1. Learning scene-agnostic localization. Deep neural net-works should not have to rediscover well-understood geometric principles. We only need to learn good features: PixLoc is trained end-to-end to estimate the pose of an image by aligning deep fea-tures with a reference 3D model via a differentiable optimization.
They leverage deep neural networks that learn to extract such features [8, 23, 25, 63, 67, 78, 94], to match them [63, 71], and to ﬁlter outlier correspondences [12, 41, 58, 71, 85].
Training a feature matching pipeline in an end-to-end manner is challenging and unstable as its complexity hin-ders gradients propagation [8]. An alternative is to train a convolutional neural network (CNN) to regress geometric quantities such as camera poses [5, 24, 35, 37, 42, 90, 97] or the 3D scene coordinate corresponding to each pixel [9–11, 13, 16, 17, 45, 80, 93]. While these approaches can be trained end-to-end, they come with their own drawbacks. Absolute pose and coordinate regression are scene-speciﬁc and re-quire to be trained for or adapted to new scenes [16, 17].
Generalization to new viewing conditions, e.g., localizing night-time images when training only on daytime photos, and handling larger, more complex scenes [78, 82] are open challenges for such approaches. Additionally, absolute or relative pose regression has limited accuracy and often fails to generalize to new viewpoints [76, 97]. While regressing poses relative to a set of reference images [5, 24, 42, 97] is in theory scene-agnostic, generalization to strongly differing scenes without a signiﬁcant drop in pose accuracy [76, 97] has, to the best of our knowledge, not been shown so far. 3247
Reference image
Aligned query pose change 24.2°/ 4.6m edge, PixLoc is the ﬁrst end-to-end visual localization ap-proach to exhibit such exceptional generalization. pose change 20.5°/ 0.5m
Figure 2. Alignment for localization. Although only based on local gradients, direct alignment works well thanks to deep fea-tures, despite the coarse initial pose estimate and strong appearance changes. Here points travel from crosses to colored dots.
What hinders the generalization of existing end-to-end regression methods is that they predict camera poses or 3D geometry solely from image information. In practice, such quantities are often readily available. Pose priors can be obtained via image retrieval or sensors such as GPS. At the same time, the 3D scene geometry is often provided as a by-product of the 3D reconstruction systems that generate the training poses, e.g. with Structure-from-Motion or SLAM.
Inspired by direct image alignment [22, 26, 27, 61, 88, 89] and learned image representations for outlier rejection [41], we argue that end-to-end visual localization algorithms should focus on representation learning. Rather than de-voting model capacity and data to learn basic geometric relations or encode 3D maps, they should rely on well-understood geometric principles and instead learn robustness to appearance and structural changes.
In this paper, we introduce a trainable algorithm, PixLoc, that localizes an image by aligning it to an explicit 3D model of the scene based on dense features extracted by a CNN (Figure 1). By relying on classical geometric optimization, the network does not need to learn pose regression itself, but only to extract suitable features, making the algorithm accurate and scene-agnostic. We train PixLoc end-to-end, from pixels to pose, by unrolling the direct alignment and supervising only the pose. Given an initial pose obtained by image retrieval, our formulation results in a simple local-ization pipeline competitive with complex state-of-the-art approaches, even when the latter are trained speciﬁcally per scene. PixLoc can also reﬁne poses estimated by any exist-ing approach as a lightweight post-processing step. Through detailed experiments, we show that our method generalizes well to new scenes, e.g., from outdoor to indoor scenes, and challenging viewing conditions. To the best of our knowl-2.