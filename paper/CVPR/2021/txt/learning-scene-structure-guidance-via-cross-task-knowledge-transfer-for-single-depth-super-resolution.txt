Abstract
Existing color-guided depth super-resolution (DSR) ap-proaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to re-cover the degraded depth map due to their geometrical simi-larity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the ﬁrst time to learn the cross-modality knowl-edge at training stage, where both RGB and depth modali-ties are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network archi-tecture. Speciﬁcally, we construct an auxiliary depth esti-mation (DE) task that takes an RGB image as input to es-timate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bi-lateral cross-task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure predic-tion (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive ex-periments demonstrate that our scheme achieves superior performance in comparison with other DSR methods. 1.

Introduction
To better understand a scene image, depth information is supplemented to the RGB images, providing the key clue
*Corresponding author: yexch@dlut.edu.cn. This work was support-ed by National Natural Science Foundation of China (NSFC) under Grant 61702078, 61772108, 61976038, 61772106. about the scene and enabling wide applications in 3D recon-struction [9], autonomous navigation [20], monitoring [2], and so on. However, acquiring depth information for indoor and outdoor scenes needs expensive cost and great efforts, especially for high-quality and high-resolution (HR) depth maps. As such, one of the effective post processing tech-niques, Depth Super-Resolution (DSR), is greatly desired to yield HR depth maps to alleviate this problem. Many ef-forts have been taken along the direction of DSR. Usually,
ﬁne scene structures are easily lost or severely destroyed in low-resolution (LR) depth map because of the limited spa-tial resolution. An RGB image and its associated depth map are the photometric and geometrical representations of the same scene, and have a strong structural similarity. Most ex-isting DSR methods learn structural complementarity from
RGB images to recover the degraded depth maps.
Previous color-guided DSR methods take advantage of
RGB-D image pairs via a two-way fusion architecture, in which an extra branch is required to extract structural guid-ance from RGB image. As illustrated in Figure 1 (a), RGB image and LR depth map are often processed by separate branches and ﬁltered together through a joint branch to out-put the HR result [26, 31, 4, 21]. However, due to the simple feature aggregation at a speciﬁc layer in the middle of the network, high-frequency structure information from RGB image is more likely to be lost in the process of feature ex-traction. Therefore, as shown in Figure 1 (b), some novel methods [18, 12, 49] incorporate a new paradigm of feature aggregation, i.e, multi-scale fusion, to allow the network to learn rich hierarchical features at different levels. This in turn makes the network to retain more spatial details for re-covering both ﬁne-scale and large-scale structures.
Although existing color-guided DSR methods have demonstrated remarkable progress, several limitations stil-l remain. First, these methods require paired RGB-D data as training examples to jointly recover the degraded depth 7792
Figure 1. Color-guided DSR paradigms. (a) Joint ﬁltering, (b) Multi-scale feature aggregation, (c) Our cross-task interaction mechanism to distill knowledge from RGB image to DSR task without changing its network architecture. map. However, the paired data may be limited or expensive to be collected in actual testing environment. For example,
RGB image and depth map are captured by separate depth and RGB sensors with different resolutions and views, thus needing accurate calibration and rectiﬁcation between them to obtain the registered pairs. Actually, most of real-world applications still come with only a single LR depth map, which raises the above question. Second, considering the memory consumption and computing burden, the process-ing on the HR RGB data also hinders the practical appli-cation. Moreover, although RGB features can be used as structural guidance to resolve the degradation in DSR, RGB discontinuities do not always coincide with those of depth map (structure inconsistency), which results in noticeable artifacts such as texture copying and depth bleeding. There-fore, how to leverage RGB information to help recover the depth map and simultaneously satisfy the actual testing en-vironment, still needs to be studied.
Motivated by the above analysis, this paper breaks away from the shackles of general paradigms and introduces a novel scene structure guidance learning method for the task of DSR, as shown in Figure 1 (c). We explore for the
ﬁrst time to learn the cross-modality knowledge at train-ing stage, where both RGB and depth modalities are avail-able, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single
DSR task without changing its network architecture.
Speciﬁcally, as illustrated in Figure 2, inspired by the success of multi-task learning [52, 22, 46], we construct an auxiliary depth estimation (DE) task that takes RGB image as input to estimate a depth map. Upon this, we propose a cross-task interaction module to realize bilateral knowledge transfer between DSR task and DE task. Different from the commonly used distillation techniques [37, 16, 22], we
ﬁrst design a cross-task distillation that encourages DSR network (DSRNet) and DE network (DENet) to learn from each other, i.e., the roles of teacher and student will dynam-ically switch between both tasks based on their current per-formances on depth recovery in the iterative collaborative training. A multi-space distillation scheme is introduced to distill knowledge from the perspective of output and afﬁni-ty spaces, which can better describe the essential structural characteristics of depth map. Moreover, to address the prob-lem of RGB-D structure inconsistency, we construct a struc-ture prediction (SP) task that provides extra structure regu-larization to help both DSRNet and DENet learn more in-formative structure representations for depth recovery. We come up with an uncertainty-induced attention fusion mod-ule to provide a reasonable input for the SP network (SP-Net), in which the uncertainty maps acquired from both D-SRNet and DENet are used to re-weight their features for strengthening effective structural knowledge. Extensive ex-periments demonstrate that our single DSR method even outperforms the color-guided DSR methods on benchmark datasets in terms of both accuracy and runtime. The main contributions are summarized as follows,
• So far as we know, our proposed paradigm of DSR is the ﬁrst work that learns with multiple modalities as inputs at training stage, but tests on only single LR depth modality.
• A cross-task distillation scheme is proposed to encour-age DSRNet and DENet to learn from each other in a col-laborative training mode.
• A structure prediction network is advanced to pro-vide structure regularization for helping DSRNet resolve the problem of structural inconsistency. 2.