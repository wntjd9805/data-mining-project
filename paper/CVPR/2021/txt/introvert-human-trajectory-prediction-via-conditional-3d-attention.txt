Abstract
Predicting human trajectories is an important compo-nent of autonomous moving platforms, such as social robots and self-driving cars. Human trajectories are affected by both the physical features of the environment and social in-teractions with other humans. Despite recent surge of stud-ies on human path prediction, most works focus on static scene information, therefore, cannot leverage the rich dy-namic visual information of the scene. In this work, we pro-pose Introvert, a model which predicts human path based on his/her observed trajectory and the dynamic scene context, captured via a conditional 3D visual attention mechanism working on the input video. Introvert infers both environ-ment constraints and social interactions through observing the dynamic scene instead of communicating with other hu-mans, hence, its computational cost is independent of how crowded the surrounding of a target human is. In addition, to focus on relevant interactions and constraints for each human, Introvert conditions its 3D attention model on the observed trajectory of the target human to extract and focus on relevant spatio-temporal primitives. Our experiments on
ﬁve publicly available datasets show that the Introvert im-proves the prediction errors of the state of the art. 1.

Introduction
Predicting future trajectories of humans in dynamic en-vironments, such as streets, airports, shopping malls and sports ﬁelds, is an important task in computer vision with applications in autonomous driving, human-robot interac-tion, urban safety and advertising, among others [50, 11, 48, 21, 15]. Forecasting human motions, however, is an ex-tremely difﬁcult problem, due to physical, social and mental factors that collectively inﬂuence people’s trajectories. In particular, as we move in an environment, we avoid physical constraints and obstacles, follow landmarks, yield right-of-way to nearby people, follow social norms and change our trajectory based on changes in the environment. This has motivated a large body of works in recent years that aim to model and incorporate various inﬂuencing factors for hu-man trajectory prediction [1, 39, 23, 3, 13].
Prior Works and Challenges. Earlier works [14, 9, 10, 24, 25, 30, 36, 4, 49, 46, 51, 42, 53] have designed energy functions to model human-human interactions, also referred to as “social forces”. Despite their relative success, such methods require careful feature and energy function design, which often could capture only simple interactions but not complex interactions in crowded environments. To mitigate these limitations, more recent methods have proposed data-driven approaches by leveraging advances in deep neural networks. In particular, sequence prediction methods based on recurrent neural networks (RNNs) model each person’s trajectory by an RNN, whose latent states capture human motion, followed by social pooling that allows recurrent models of nearby trajectories to share their states [1, 13].
However, they cannot capture the inﬂuence of farther peo-ple to a target trajectory, while giving the nearby trajecto-ries the same importance weights. To overcome these lim-itations, attention-based models have been integrated with
RNNs [39, 3] and spatio-temporal graphs [41, 40, 33, 20] to weigh different trajectories by adjusting the importance of neighbors to each target human. However, most approaches discussed above rely on only kinematics data, which con-tains information about only moving agents in the scene.
Given that videos contains rich information about phys-ical conﬁguration of the scene and navigation constraints, several works have tried to use the visual context of the scene in conjunction with kinematics data for more effective predictions. This has been achieved by concatenating the states of all RNNs with visual features of a current frame ex-tracted via CNNs [40, 26, 41], which could be followed by an attention model to select relevant features [39, 40]. How-ever, existing works face multiple challenges. First, current methods extract visual information that is often shared and identical for all people moving in the environment. How-ever, in practice, each person’s trajectory depends on the region of the terrain where he/she is moving, physical con-straints between the current position and the intended des-tination, as well as other humans relevant to the path. In other words, different parts of the scene and visual features have different importance that depends on the target human.
Second, visual features obtained by encoding one frame at 116815
a time cannot capture the complex interactions and social norms, which is why existing methods require to incorpo-rate social interactions through pooling states of RNNs op-erating on kinematics data.
More importantly, from a computational stand point, during inference time, one needs to ﬁrst run a human detec-tion and tracking algorithm for all people in the scene and then connect RNNs using nearest neighbors graphs or at-tention to be able to predict the trajectory of a target human.
This prohibits existing methods from being run in real-time at the inference time, especially in crowded environments with many humans, but one or a few targets of interests.
Paper Contributions. In this paper, we develop an efﬁcient framework for human trajectory prediction using a condi-tional 3D visual attention mechanism, which addresses the aforementioned challenges. We argue that the video itself (not an individual frame) contains all necessary informa-tion about the motions and interactions of humans as well as dynamic constraints, e.g., moving vehicles, and static con-straints, e.g., buildings and sidewalks, of the environment.
This can be seen from the fact that kinematic trajectories are extracted from videos, hence, cannot contain more in-formation than the video itself. Thus, instead of modeling human-human interactions by connecting nearby or all re-current models of human trajectories in the scene, we lever-age the video to extract 3D visual interaction information (2 spatial and 1 temporal dimensions). This removes the need for running a detection and tracking algorithm for ev-ery human in the scene, hence, increases the efﬁciency at test time, where only the video and tracking of the target human would be needed.
We develop a sequence to sequence method that consists of two parallel encoding streams, which gather 3D visual and kinematics information relevant to a target human, and one decoding stream that predicts the future trajectory of the target human. To focus on relevant social interactions and physical constraints for each human, our visual encoder uses a conditional 3D attention mechanism that receives the input video and conditioning on the observed trajectory of the target human, extracts spatio-temporal primitives and learns to attend to most informative primitives. These ex-tracted primitives could be e.g., parts of a sidewalk, few vehicles, distant landmarks as well as nearby or distant hu-mans in the scene. By experiment on UCY [27] and ETH
[35] datasets, we show that out method signiﬁcantly im-proves the state of the art performance, reducing the average prediction error on 5 datasets from 0.41 to 0.34. 2.