Abstract
Physical adversarial attacks against object detectors have seen increasing success in recent years. However, these attacks require direct access to the object of interest in order to apply a physical patch. Furthermore, to hide mul-tiple objects, an adversarial patch must be applied to each object. In this paper, we propose a contactless translucent physical patch containing a carefully constructed pattern, which is placed on the camera’s lens, to fool state-of-the-art object detectors. The primary goal of our patch is to hide all instances of a selected target class.
In addition, the optimization method used to construct the patch aims to ensure that the detection of other (untargeted) classes re-mains unharmed. Therefore, in our experiments, which are conducted on state-of-the-art object detection models used in autonomous driving, we study the effect of the patch on the detection of both the selected target class and the other classes. We show that our patch was able to prevent the detection of 42.27% of all stop sign instances while main-taining high (nearly 80%) detection of the other classes. 1.

Introduction
In recent years, deep neural networks (DNNs) and partic-ularly convolutional neural networks (CNNs) have become a state-of-the-art solution for computer vision tasks, such as image classiﬁcation [13, 8], object detection [23, 24], and image segmentation [2, 1]. This is mainly due to
DNNs’ ability to accurately model complex multivariate data. However, such models’ effectiveness depends heav-ily on their robustness to attacks that target the model itself; i.e., adversarial learning attacks.
Adversarial attacks have become a major focus of the machine learning research community, primarily in the computer vision domain [30, 25]. Recent studies have demonstrated the ability to implement evasion attacks (i.e., misclassiﬁcation of an object) by applying a physical patch on the targeted object. Some examples include hiding a person using a small printed cardboard plate [33] or wear-able clothes [34, 35, 10], concealing a stop sign by attach-ing small black and white stickers to it [6], or crafting a
Figure 1: Illustrating the physical translucent patch, placed on the camera lens, which results in the failure to detect the stop sign while correctly identifying the other objects. new stop sign with speciﬁc patterns on its background [3].
However, the main limitation of those physical attacks is that they require access to the object itself (to apply the ad-versarial patch). Furthermore, to hide multiple objects, an adversarial patch must be applied to each object.
We propose a novel physical attack aimed at fooling common object detection models by mounting a carefully crafted patch on the camera lens. Our attack (patch) is cal-culated by a gradient-based optimization process which re-sults in a universal adversarial patch that takes the following goals into consideration: 1) successfully hide all instances of a target class from the object detector, 2) minimize the at-tack’s impact on the detection of untargeted classes, and 3) produce a printable patch that is as unnoticeable as possible.
To the best of our knowledge, the only study that at-tempted to implement a camera-based physical attack is the work performed by Li et al. [15], in which an image classi-ﬁcation model was targeted. Unlike an image classiﬁcation model, which predicts the class of a single (dominant) ob-ject in the image, an object detection model is capable of i) detecting and classifying multiple objects regardless of their location and dominance within the image, and ii) pro-cessing thousands of candidate bounding boxes centered on each output pixel. Thus, in our attack all of the candidate 15232
objects must be manipulated, signiﬁcantly complicating the attack. Furthermore, in this research we study the effect of the physical patch on the untargeted classes, which was not addressed by Li et al.
To explore the feasibility of our approach, we ﬁrst demonstrated the ability to deceive Tesla’s advanced driv-ing assistance system (ADAS) by applying two simple fully colored translucent patches on the camera’s lens. We show that the ADAS misclassiﬁes a stop sign (when a red-colored patch is used) and interprets a red trafﬁc light as a green one (when using a cyan-colored patch). Then, we evalu-ated our proposed attack on CNN-based object detection models using datasets related to the autonomous car use case. Since autonomous cars operate in a real-world envi-ronment, we use the latest real-time object detection model,
YOLOv5 [11], a recent improvement to YOLOv3 [23]. In our evaluation, we select the stop sign class as the targeted class, with the aim of preventing any of the instances from being detected by the object detector, both in the digital and physical domain. The results show that we are able to de-crease the average precision of the detector for the stop sign class by 42.47% (digitally) and 42.27% (physically), while the detection of other object classes remains high.
We summarize the contributions of our work as follows:
• We present the ﬁrst camera-based physical adversarial attack on object detection models.
• We craft a universal perturbation to fool the model for all instances of a speciﬁc object class, while maintain-ing the detection of untargeted objects.
• We demonstrate the transferability of the attack when the patch is generated using a surrogate model and then applied to a different model.
• The design and optimization process we propose take real-world constraints into account, including printing limitations and accurate patch placement, resulting in a practical attack. 2.