Abstract
Disentangled representations support a range of down-stream tasks including causal reasoning, generative model-ing, and fair machine learning. Unfortunately, disentangle-ment has been shown to be impossible without the incorpo-ration of supervision or inductive bias. Given that supervi-sion is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an un-supervised, deep State-Space-Model for Video Disentangle-ment (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts de-coder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including iden-tity and dynamics transfer, sequence generation, Fr´echet In-ception Distance, and factor classiﬁcation. VDSM achieves state-of-the-art performance and exceeds adversarial meth-ods, even when the methods use additional supervision. 1.

Introduction
In general, humans are able to reason about the identity of an object and the object’s motion independently, thereby implying that identity and motion are considered as dis-entangled generative attributes [38, 9].
In other words, a change to an object’s motion does not affect the object’s identity. For example, in sign language translation, the canonical form of a gesture exists independently of the iden-tity or appearance of the signer. In order to reason inde-pendently about the latent factors underlying identity and motion, it is therefore desirable to seek disentanglement.
Achieving disentangled representations is a long stand-ing goal for machine learning, and supports causal reason-ing [72, 10, 90, 97], fair machine learning [63, 66, 20, 99],
Target Action
Target Action
Source ID
Source ID
Source ID
Source ID
Figure 1. VDSM action transfer for the Sprites dataset [60]. The target action embedding is transferred swapped with one from a different identity and the sequence is generated. generalizability [41, 13], structured/controllable inference and prediction [106], attribute transfer [14, 73], and im-proved performance on downstream tasks [9, 57, 79, 74, 98]. Unfortunately, being able to consistently learn a disen-tangled representation such that the factors correspond with meaningful attributes has been shown to be both theoreti-cally and empirically impossible without the use of some form of supervision or inductive bias [21, 64]. However, acquiring high-quality supervision is expensive and time-consuming. Whilst many methods rely on such supervision, we consider how the implicit structure embedded in video data can be leveraged and reﬂected in the structure of the model, in order to achieve unsupervised disentanglement.
We propose Video Disentanglement via State-Space-Modeling (VDSM). VDSM is motivated by a careful con-sideration of the generative structure of a video sequence, which is assumed to be composed of identity (i.e. the canonical appearance of an object or person), an action 8176
(i.e. dictating the dynamics governing change over time), and pose (i.e. the time varying aspects of appearance).
In summary, VDSM: 1) Is a completely unsupervised ap-proach that avoids the need for adversarial training. 2) In-corporates a novel structure designed to factorise appear-ance and motion, using a strong mixture of decoders to separate identity. 3) Produces embeddings that achieve state-of-the-art classiﬁcation performance. 4) Far exceeds
GAN based approaches in sequence generation (evidenced by FID scores). 5) Produces superior disentanglement com-pared to approaches which use forms of supervision. 6) Ex-ceeds accuracy consistency by over 30% compared to the nearest competing approach. 7) Produces qualitative results that reﬂect the quantitative performance in terms of disen-tanglement and image quality.
The rest of this paper is structured as follows. First we discuss related work in Section 2 before describing the structure and training of VDSM in Section 3. We provide qualitative and quantitative experiments in Section 4 and conclude in Section 5. 2.