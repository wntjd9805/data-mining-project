Abstract
Recently, the ever-increasing capacity of large-scale an-notated datasets has led to profound progress in stereo matching. However, most of these successes are limited to a speciﬁc dataset and cannot generalize well to other datasets. The main difﬁculties lie in the large domain dif-ferences and unbalanced disparity distribution across a va-riety of datasets, which greatly limit the real-world applica-bility of current deep stereo matching models. In this paper, we propose CFNet, a Cascade and Fused cost volume based network to improve the robustness of the stereo matching network. First, we propose a fused cost volume represen-tation to deal with the large domain difference. By fusing multiple low-resolution dense cost volumes to enlarge the receptive ﬁeld, we can extract robust structural representa-tions for initial disparity estimation. Second, we propose a cascade cost volume representation to alleviate the un-balanced disparity distribution. Speciﬁcally, we employ a variance-based uncertainty estimation to adaptively adjust the next stage disparity search space, in this way driving the network progressively prune out the space of unlikely cor-respondences. By iteratively narrowing down the dispar-ity search space and improving the cost volume resolution, the disparity estimation is gradually reﬁned in a coarse-to-ﬁne manner. When trained on the same training images and evaluated on KITTI, ETH3D, and Middlebury datasets with the ﬁxed model parameters and hyperparameters, our pro-posed method achieves the state-of-the-art overall perfor-mance and obtains the 1st place on the stereo task of Ro-bust Vision Challenge 2020. The code will be available at https://github.com/gallenszl/CFNet. 1.

Introduction
Stereo matching, i.e. estimating a disparity/depth map from a pair of stereo images, is fundamental to various ap-plications such as autonomous driving [3], robot navigation
[1], SLAM [8, 10], etc. Recently, many deep learning-based
*Yuchao Dai is the corresponding author.
Figure 1. Performance comparison in terms of generalization abil-ity on the Middlebury, ETH3D, and KITTI 2015 datasets. Bad 2.0, bad1.0, and D1 all (the lower the better) are used for evaluation.
All methods are trained on the same training images and tested on three datasets with single model parameters and hyper-parameters.
Our CFNet achieves state-of-the-art generalization and performs well on all three real-world datasets. stereo methods have been developed and achieved impres-sive performance on most of the standard benchmarks.
However, current state-of-the-art methods are generally limited to a speciﬁc dataset due to the signiﬁcant domain shifts across different datasets. For example, the KITTI dataset [9, 19] focuses on real-world urban driving sce-narios while Middlebury [22] concentrates on indoor high-resolution scenes. Consequently, methods that are state-of-the-art on one dataset often cannot achieve comparable per-formance on a different one without substantial adaptation (visualization comparison can be seen in Fig. 2). However, real-world applications require the approaches to general-ize well to different scenarios without adaptation. Thus, we need to push methods to be robust and perform well across different datasets with the ﬁxed model parameters and hy-perparameters.
The difﬁculties in designing a robust stereo matching system come from the large domain differences and unbal-anced disparity distribution between a variety of datasets.
As illustrated in Fig. 2 (a), there are signiﬁcant domain dif-ferences across various datasets, e.g., indoors vs outdoors, color vs gray, and real vs synthetic, which leads to the learned features distorted and noisy [37].
In addition, as illustrated in Fig. 3, the disparity range of half-resolution images in Middlebury [22] is even more than 6 times larger than full-resolution images in ETH3D [24] (400 vs 64). 13906
(a) Left image (b) CFNet (c) GANet (d) HSMNet
Figure 2. Visualization of some state-of-the-art methods’ performance on three real-world dataset testsets (from top to bottom: KITTI,
Middlebury, and ETH3D). All methods are trained with a combination of KITTI, Middlebury, and ETH3D train images. GANet [36] and
HSMNet [33] can achieve good performance on one speciﬁc dataset but perform poorly on the other two even if they have included targeted domain images in the training process. Our CFNet achieves SOTA or near SOTA performance on all three datasets without any adaptation.
Such unbalanced disparity distribution makes the current approaches trained with a ﬁxed disparity range cannot cover the whole disparity range of another dataset without sub-stantial adaption.
In this paper, we propose a cascade and fused cost vol-(1) ume representation to alleviate the above problems.
Towards the large domain differences, we propose to fuse multiple low-resolution dense cost volumes to enlarge the receptive ﬁeld for capturing global and structural represen-tations. Previous work [37] observes that the limited effec-tive receptive ﬁeld of convolutional neural networks [17] is the major reason the network is domain-sensitive to dif-ferent datasets and proposes a learnable non-local layer to enlarge the receptive ﬁeld. Inspired by it, we ﬁnd that dif-ferent scale low-resolution cost volumes can cover multi-scale receptive ﬁelds and are complementary to each other in promoting the network to look at different scale image regions. Thus, we can fuse multiple low-resolution dense cost volumes to guide the network to learn geometric scene information which is invariant across different datasets. In addition, this operation only adds a slight computation com-plexity. (2) Towards the unbalanced disparity distribution, we propose a cascade cost volume representation and em-ploy a variance-based uncertainty estimation to adaptively adjust the next stage disparity search range. That is, our method only needs to cover the union of all datasets’ dis-parity distribution (disparity range) at the ﬁrst stage. Then we can employ our uncertainty estimation to evaluate pixel-level conﬁdence of disparity estimation and prune out un-likely correspondences, guiding our network to look at more possible disparity search space at the next stage. In addition, we can save a lot of computational complexity by pruning out unlikely correspondences.
Experimentally, all methods are trained on the same training images and tested on three real-world datasets (KITTI2015, Middlebury, and ETH3D) with ﬁxed model parameters and hyperparameters. As shown in Fig. 1, our method performs well on all three datasets and achieves state-of-the-art overall performance without adaptation.
In summary, our main contributions are:
• We propose a fused cost volume representation to re-duce the domain differences across datasets.
• We propose a cascade cost volume representation and develop a variance-based uncertainty estimation to bal-ance different disparity distributions across datasets.
• Our method shows great generalization ability and ob-tains the 1st place on the stereo task of Robust Vision
Challenge 2020.
• Our method has great ﬁnetuning performance with low latency and ranks 1st on the popular KITTI 2015 and
KITTI 2012 benchmarks among the published meth-ods less than 200ms inference time. 2.