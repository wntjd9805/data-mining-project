Abstract
Learning to detect novel objects from few annotated ex-amples is of great practical importance. A particularly challenging yet common regime occurs when there are ex-tremely limited examples (less than three). One critical fac-tor in improving few-shot detection is to address the lack of variation in training data. We propose to build a bet-ter model of variation for novel classes by transferring the shared within-class variation from base classes. To this end, we introduce a hallucinator network that learns to gener-ate additional, useful training examples in the region of in-terest (RoI) feature space, and incorporate it into a mod-ern object detection model. Our approach yields signiﬁcant performance improvements on two state-of-the-art few-shot detectors with different proposal generation procedures. In particular, we achieve new state of the art in the extremely-few-shot regime on the challenging COCO benchmark. 1.

Introduction
Modern deep convolutional neural networks (CNNs) rely heavily on large amounts of annotated images [29]. This data-hungry nature limits their applicability to some practi-cal scenarios such as autonomous driving, where the cost of annotating examples is prohibitive, or which involve never-before-seen concepts [9,51]. By contrast, humans can rapidly grasp a new concept and make meaningful general-izations, even from a single example [31]. To bridge this gap, there has been a recent resurgence of interest in few-shot or low-shot learning that aims to learn novel concepts from very few labeled examples [8, 10, 34, 37, 42].
Despite notable successes, most of the existing work has focused on simple classiﬁcation tasks with artiﬁcial settings and small-scale datasets [34, 37]. However, few-shot object detection, a task of great practical importance that learns an object detector from only a few annotated bounding box ex-amples [18, 38, 39], is far less explored. Few-shot detection requires determining where an object is as well as what it is (and handling distracting background regions [13], etc.), and is much harder than few-shot classiﬁcation. The most
Hallucinator
Test
Region
Proposal
Network
Figure 1. Learning to detect a novel class, fennec fox, from a sin-gle training example (i.e., 1-shot detection) using a serial detec-tor. The region proposal network (RPN) generates a few high intersection-over-union (IoU) boxes for the detector’s classiﬁer.
The pink circle represents the classiﬁer decision boundary learned from these boxes. Due to a lack of sample variation, the decision boundary is not accurately estimated. With hallucinated examples (image in backslash) produced by our hallucinator, the classiﬁer learns a better decision boundary (the dotted circle), thus being able to potentially correct previously misclassiﬁed instances. difﬁcult regime occurs when there are very limited exam-ples (less than 3) for novel classes (Figure 1), which is a common yet extremely challenging case in the real world.
While few-shot classiﬁcation approaches are helpful (e.g., [2, 4, 18, 33, 41]), few-shot detection is much more than a straightforward application of few-shot classiﬁcation approaches. The state-of-the-art two-stage ﬁne-tuning ap-proach (TFA) [38] learns a better representation for few-shot detection, through (1) pre-training on base classes with abundant data and then only ﬁne-tuning the box classiﬁer and regressor on novel classes, and (2) introducing instance-level feature normalization to the box classiﬁer during ﬁne-tuning. Despite the improvement, its performance in the extremely low-data regime is still far from satisfying.
We argue that, to fully improve extremely-few-shot de-tection performance, a key factor is to effectively deal with the lack of variation in training data. This is because for an object detector to be accurate, its classiﬁer must build a use-ful model of variation in appearance with very few exam-ples. More concretely, a modern object detector ﬁrst ﬁnds promising image locations, typically boxes, using a region proposal network (RPN) [28], then passes promising boxes 13008
through a classiﬁer to determine what object is present, and
ﬁnally performs various cleanup operations such as non-maximum suppression (NMS), aimed at avoiding duplicate predictions and improving localization. Now assume that the detector must learn to detect a novel category from a single example (Figure 1). The only way the classiﬁer can build a model of the category’s variation in appearance is by learning from the high intersection-over-union (IoU) boxes reported by the RPN. Although there is variation of boxes produced by the RPN, the variation from a single example is too weak to train the classiﬁer for the novel class.
To overcome this issue, one strategy is to adjust the learning procedure for RPN, so that it reports highly infor-mative boxes. Contemporary work [48] achieves this by training multiple RPN’s be somewhat redundant and coop-erating. Hence, if one RPN misses a highly informative box, another will get it. This cooperating RPN’s (CoRPNs) ap-proach, while helpful, is still insufﬁcient. In the extremely-few-shot regime, all positive novel class proposals produced by the multiple RPN’s are only slightly modiﬁed from and thus similar to the few available positive instances (with light-weighted cropping and scaling operations); their vari-ation is signiﬁcantly limited for building a strong classiﬁer.
In this paper, we propose a different perspective on build-ing a model of variation for novel classes, by transferring the shared within-class variation from base classes. In fact, many modes of variation in the visual world (e.g., cam-era pose, lighting changes, and even articulation) are shared across categories and can generalize to unseen classes [30].
While such within-class variation is difﬁcult to be encoded through the proposal generation procedure, it can be effec-tively captured by learning to hallucinate examples [40].
To this end, we introduce a hallucinator network into a modern object detection model. The hallucinator net-work performs data hallucination for the box classiﬁer in the learned region of interest (RoI) feature space. We train the hallucinator on data-abundant base classes, encoding the rich structure of their shared modes of variation. We then use the learned hallucinator to generate additional novel class examples and thus produce an augmented training set for building better classiﬁers, as shown in Figure 1.
Note that the existing strategy for training the hallucina-tor in few-shot classiﬁcation [40] is coupled with a com-plicated meta-learning process, making it difﬁcult to ap-ply to state-of-the-art few-shot detectors like TFA [38] or
CoRPNs [48]. We overcome this challenge by introduc-ing a much simpler yet effective training procedure: we train our hallucinator and the detector’s classiﬁer in an EM-like (expectation-maximization) manner, where a “strongest possible” classiﬁer is trained ﬁrst with all the available base class data; the hallucinator is then trained under the guid-ance of this already-trained classiﬁer; and ﬁnally, the clas-siﬁer is re-trained and reﬁned based on the set of augmented examples (with hallucinated examples) on novel classes.
Our contributions are three-fold. (1) We investigate a critical yet under-explored issue in extremely-few-shot de-tection (e.g., as few as one) – the lack of variation in train-ing data. (2) We propose a novel data hallucination based approach to address this issue, which effectively transfers shared modes of within-class variation from base classes to novel classes. Our approach is simple, general, and can work with different region proposal procedures. (3)
Our approach signiﬁcantly outperforms the state-of-the-art
TFA [38] and most recent cooperating RPN’s [48] detectors in the extremely-few-shot regime. Our code is available at https://github.com/pppplin/HallucFsDet. 2.