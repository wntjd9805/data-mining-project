Abstract 1.

Introduction
We present the ﬁrst deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face mod-els it not only captures identity-speciﬁc geometry, texture, and expressions of the frontal face, but also models the en-tire head, including hair. We collect a new dataset consist-ing of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the ﬁrst full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requir-ing difﬁcult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape.
With that, dense correspondences between shapes can be (iv) This architecture allows us to se-learned implicitly. mantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle com-ponents. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and appli-cations such as semantic head editing and texture transfer.
We will make our model publicly available1. 1http://gvv.mpi-inf.mpg.de/projects/i3DMM/ 3D morphable models (3DMMs) are parametric mod-els of geometry and appearance of human faces, with widespread use in applications such as image and video editing, face recognition and cognitive science [20]. These models are trained using 3D scans of humans, e.g., laser scans [4], depth sensor-based scans [9], or photometric multi-view scans [28]. As 3DMMs usually learn a defor-mation space of a ﬁxed template mesh, the training shapes commonly need to be brought into dense surface correspon-dence with each other. Computing such correspondence for the face region alone is already hard and often requires a challenging non-convex optimizaton problem [20]; comput-ing dense correspondence for the rest of the head and the hair is close to impossible. Therefore, as well as due to the lack of large 3D scan datasets with hair, most 3DMMs only capture the face region. While some recent approaches aim to model the complete head [15, 28, 41, 40], they do not capture the hair region, and the appearance in the head re-gion (if captured) is very limited.
In this work, we present i3DMM, the ﬁrst implicit 3D morphable model which captures the full head region, in-cluding hair deformations and appearance. We capture a new dataset of full photogrammetric head scans of 64 peo-ple for training. Each subject performs several expres-sions and natural hair is captured. Since such scans are 12803
noisy, especially in the hair region, our training algorithm uses an adaptive sampling strategy based on the quality of reconstructions in different regions of the head, allowing us to effectively handle noise without smoothing out de-tails. In contrast to existing 3DMMs, which use a mesh-representation with a ﬁxed template, we implicitly repre-sent surfaces using signed distance functions. This allows us to capture large deformations easily, which is particularly convenient for the hair region. The implicit representation also allows us to avoid computing dense correspondence between training scans. Our method is inspired by recent works on deep implicit surface modeling [36, 31, 32], which demonstrate the advantages of using an implicit representa-tion compared to voxel grids, point clouds or meshes. Our main technical innovation compared to these works is that we use a novel neural network architecture which decou-ples the learning process, separating it into learning a refer-ence shape, learning geometry deformations with respect to this reference, and learning a color network. This allows us to automatically compute dense correspondences between any two shapes with only sparse supervision on salient face landmarks. This decoupling is also essential for disentan-gling the learned space into semantically meaningful para-metric components, an important feature of 3DMMs.
Most classical models disentangle identity geometry, ex-pression geometry, and appearance of the face. This is suf-ﬁcient for several applications in face editing [52, 51, 48], but does not allow the explicit control of head parts other than the facial region. Our model allows for unprecedented control over the different semantic modes of full heads. To this end, we learn to disentangle the geometry and color, where geometry is further separated into identity, expres-sion, and hairstyle components, and color is further sepa-rated into identity and hairstyle components.
In summary, our main contributions are: 1. A method for learning full head 3D morphable models directly from rigidly aligned real-world noisy 3D scans without dense ground truth correspondences. 2. A novel network architecture which can compute dense correspondences between 3D shapes repre-sented using implicit functions.
This network is trained only with sparse supervision. 3. A training method for disentangling (a) the color and geometry components, and (b) the identity, expression, and hairstyle compo-nents of the geometry; and the identity and hairstyle components of the color.
We compare our model to several state-of-the-art 3DMMs by ﬁtting to 3D scans. We also demonstrate the quality of our learned dense correspondences by showing texture transfer between scans, and show that i3DMM beneﬁts ap-plications such as semantic head editing, and one-shot com-putation of segmentation masks and landmarks on 3D scans. 2.