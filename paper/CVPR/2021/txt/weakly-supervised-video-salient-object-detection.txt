Abstract
Signiﬁcant performance improvement has been achieved for fully-supervised video salient object detection with the pixel-wise labeled training datasets, which are time-consuming and expensive to obtain. To relieve the bur-den of data annotation, we present the ﬁrst weakly super-vised video salient object detection model based on rela-beled “ﬁxation guided scribble annotations”. Speciﬁcally, an “Appearance-motion fusion module” and bidirectional
ConvLSTM based framework are proposed to achieve ef-fective multi-modal learning and long-term temporal con-text modeling based on our new weak annotations. Fur-ther, we design a novel foreground-background similarity loss to further explore the labeling similarity across frames.
A weak annotation boosting strategy is also introduced to boost our model performance with a new pseudo-label gen-eration technique. Extensive experimental results on six benchmark video saliency detection datasets illustrate the effectiveness of our solution1. 1.

Introduction
Video salient object detection (VSOD) models are de-signed to segment salient objects in both the spatial domain and the temporal domain. Existing VSOD methods focus on two different solutions: 1) encoding temporal informa-tion using a recurrent network [30, 10, 44], e.g. LSTM; and 2) encoding geometric information using the optical ﬂow constraint [18, 29]. Although considerable performance im-provements have been achieved, we argue that the huge bur-den of pixel-wise labeling makes VSOD a much more ex-pensive task than the RGB image-based saliency detection task [14, 23, 22, 48, 43].
The standard pipeline to train a deep video saliency
∗Corresponding author: Junwei Han (junweihan2010@gmail.com) 1Our code and data is publicly available at: https://github. com/wangbo-zhao/WSVSOD. (a) Image (b) Full Anno. (c) Weak Anno. (d) Image (e) TENet[29] (f) Ours
Figure 1. Training with our weak annotation (c), we achieve com-petitive performance (f) compared with TENet[29] (e).
Firstly, the detection model involves two main steps. network is pre-trained on an existing static RGB image-based saliency detection training dataset, e.g. DUTS [35] or
MSRA10K [6]. Then, it is ﬁne-tuned on video saliency de-tection datasets, e.g. DAVSOD [10] and DAVIS [28]. The main reason for using this strategy is that video saliency datasets usually have limited scene diversity. Although the largest DAVSOD dataset [10] has more than 10K frames for training, the large redundancy across the frames of each clip makes it still insufﬁcient to effectively train deep video saliency models. Speciﬁcally, DAVSOD has a total of 107 clips for training and validation, which only indicates around 107 diverse scenes. Hence, directly training with a
VSOD dataset may lead to poor model generalization abil-ity, as the model may overﬁt on the highly redundant data.
To obtain an effective video saliency detection model, existing fully supervised VSOD methods [18, 29, 10] rely on both RGB image saliency datasets and VSOD training datasets. The problem behind the above pipeline is the huge requirement for pixel-wise labeling, which is time-consuming and expensive to obtain. For example, RGB image saliency training datasets have more than 10K la-beled samples [35, 6]. Further, as shown in Tab. 1, widely used VSOD training datasets (DAVSOD and DAVIS) con-16826
Table 1. Details of existing video sod datasets. Dataset: name of the dataset, Size: number of frames, Annotated size: labeled frames(per pixel), Training: Frames used for training, /: this dataset is not split.
Dataset
DAVSOD[10]
VOS[19]
DAVIS[28]
ViSal[39]
FBMS[26]
SegV2[16]
Released Year 2019 2018 2016 2015 2014 2013
Size 23,938 116,103 3,455 963 13,860 1,065
Annotated size Training 12,670 5,927 2,079
/ 353
/ 23,938 7,467 3,455 193 720 1,065 tain more than 14K pixel-wise labeled frames. Both of them required large burden to perform data annotations.
To relieve the burden of pixel-wise labeling, one can re-sort the weakly supervised learning technique [47, 35] to learn saliency from image scribble or image-level labels.
In this paper, considering the efﬁciency of scribble anno-tation, we aim to learn a weakly supervised video saliency detection network via scribble. However, the main prob-lem is that the per-image labeled scribble has no tempo-ral information. To incorporate temporal information into our weak annotation, we adopt the ﬁxation annotation in existing VSOD training datasets as guidance, and propose
ﬁxation guided scribble annotation as shown in Fig. 1 (c).
Speciﬁcally, we ﬁrst deﬁne the regions that have the peak re-sponse of ﬁxation as foreground and those without ﬁxation as background. Then we label both foreground scribble and background scribble following [47].
Based on the ﬁxation guided scribble annotation, we de-sign an appearance-motion fusion module to fuse both ap-pearance information from the RGB image and motion in-formation from optical ﬂow as shown in Fig. 2. Further-more, a bidirectional LSTM [30] based temporal informa-tion enhanced module is presented to further obtain long-term temporal information. Note that, we use scribble an-notation from S-DUTS [35] to pre-train our video saliency detection network as the conventional way. Build upon both scribble annotation from the RGB image saliency dataset and video saliency dataset, our weakly supervised video saliency detection network leads to a very cheap conﬁgura-tion compared with existing deep video saliency detection models. Moreover, considering the cross-frame redundancy of the video saliency dataset, we introduce the foreground-background similarity loss to fully explore our weak anno-tation. We also introduce a weak annotation boosting strat-egy by leveraging our scribble annotation and the saliency map generated from the off-the-shelf fully-supervised SOD model. Beneﬁting from these, our model can achieve com-parable results with state-of-the-art fully-supervised meth-ods. e.g. Fig. 1 (f) and (e).
Our main contributions are: 1) We introduce the ﬁrst weakly supervised video salient object detection network based on our ﬁxation guided scribble annotation; 2) We propose an appearance-motion fusion module and a tempo-ral information enhance module to effectively fuse appear-ance and motion features; 3) We present the foreground-background similarity loss to explore our weak annotation in adjacent frames; 4) We combine saliency maps generated from an off-the-shelf saliency model and our scribble anno-tations to further boost model performance. 2.