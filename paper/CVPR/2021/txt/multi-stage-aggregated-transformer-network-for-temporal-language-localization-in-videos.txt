Abstract
We address the problem of localizing a speciﬁc momen-t from an untrimmed video by a language sentence query.
Generally, previous methods mainly exist two problems that are not fully solved: 1) How to effectively model the ﬁne-grained visual-language alignment between video and lan-guage query? 2) How to accurately localize the moment in the original video length? In this paper, we streamline the temporal language localization as a novel multi-stage ag-gregated transformer network. Speciﬁcally, we ﬁrst intro-duce a new visual-language transformer backbone, which enables iterations and alignments among all elements in visual and language sequences. Different from previous multi-modal transformers, our backbone keeps both struc-ture uniﬁed and modality speciﬁc. Moreover, we also pro-pose a multi-stage aggregation module topped on the trans-former backbone. In this module, we compute three stage-speciﬁc representations corresponding to different moment stages respectively, i.e. starting, middle and ending stages, for each video element. Then for a moment candidate, we concatenate the starting/middle/ending representations of its starting/middle/ending elements respectively to form the
ﬁnal moment representation. Because the obtained momen-t representation captures the stage speciﬁc information, it is very discriminative for accurate localization. Extensive experiments on ActivityNet Captions and TACoS datasets demonstrate our proposed method achieves signiﬁcant im-provements compared with all other methods. 1.

Introduction
Temporal localization is a prominent and fundamental problem for video analysis in the computer vision commu-∗Corresponding author.
In the past years, there are lots of works that have nity. been conducted for temporal action localization [28, 38, 48, 3, 42, 18, 47]. Recently, the task of temporally localiz-ing natural language in videos has been attracting the in-terest of researchers. The task aims to localize the temporal moment corresponding to a language sentence query in an untrimmed video. Compared with temporal action local-ization, temporal language localization is more challenging and has vast potential applications, such as video retrieval, video captioning, and human-computer interaction, etc.
There are many approaches that have been proposed for temporal language localization [1, 7, 20, 41, 40, 45, 35, 19, 25, 26, 43]. Although those approaches have achieved many promising results, there are still several critical prob-lems that have not been fully solved: 1) How to effectively model the ﬁne-grained visual-language alignment between video and language query? 2) How to accurately localize the moment in the original video length? For the ﬁrst prob-lem, most existing approaches often process video and lan-guage sequences separately and then fuse them. However, processing the two modalities separately, e.g., ﬁrst encoding the query sentence into a single vector, will inevitably lose some detailed semantics and thus cannot achieve detailed interaction and alignment. Besides, the temporal relations in the video sequence are often modeled by local operations, which is not sufﬁcient to obtain enough contextual informa-tion. For the second problem, previous approaches usually use the full convolution, mean pooling or RoI (Region of
Interest) pooling [38, 10] operations to obtain the feature representation for moment candidates. We argue that these kinds of representations are not discriminative enough. For instance, the moment or event often contains some differ-ent stages, e.g. the starting, middle and ending stages. The information of those stages is very important for accurate moment localization. However, the mean pooling opera-tion discards the stage information, thus cannot match the 12669
different stages precisely. Although the convolution or RoI pooling operations can model the different stages to some extent, they do not rely on explicit stage-speciﬁc represen-tations. Besides, convolution operation densely using all the elements in the moment candidate is hard to catch the key elements for localization, and it also cannot adapt to vari-ous dynamics in the moment because of the ﬁxed structure of convolution kernel.
To address these problems, in this paper we propose a novel multi-stage aggregated transformer network for tem-poral language localization in videos. Our proposed net-work mainly contains two components: the visual-language transformer backbone and the multi-stage aggregation mod-ule topped on the transformer backbone. Speciﬁcally, we
ﬁrst introduce a new visual-language transformer back-bone, which simultaneously processes both the video and language sequences to effectively model the ﬁne-grained visual-language interactions and alignments. Our trans-former backbone is inspired by recently proposed visual-language BERT models [31, 30, 17, 16, 5], which encode the visual and language sequences into a uniﬁed sequence and process it utilizing a single BERT. This kind of archi-tecture processes the two sequences from different modal-ities in a compact and efﬁcient way. However, we argue that different modalities have modality speciﬁc contents and
It is not optimal to encode sequences relation patterns. from different modalities into a uniﬁed sequence and mod-el them without a difference. In our transformer backbone, we also keep a single BERT architecture but decouple the
BERT parameters into different groups to process the visual and language contents respectively. Thus, our transformer backbone keeps the compactness and efﬁciency of the sin-gle BERT structure while models the two modalities more effectively. Moreover, in order to achieve more accurate moment localization, we propose a multi-stage aggregation module topped on the transformer backbone. In this mod-ule, we compute three stage-speciﬁc representations corre-sponding to three different temporal stages respectively, i.e. starting, middle and ending stages, for each element in the video sequence. Then for a moment candidate, we con-catenate the starting representation of its starting element, middle representation of its middle element and ending rep-resentation of its ending element to form the ﬁnal moment representation. Because the three representations capture the speciﬁc information about different stages respective-ly, the obtained moment representation is stage sensitive, which is very discriminative for accurate localization. The whole architecture of our proposed network is conceptually simple and efﬁcient. Not only does it achieve superior lo-calization performance, but also achieves a very fast speed.
To summarize, our main contributions are three-fold:
• We propose a novel streamline network based on a new visual-language transformer backbone for tempo-In our transformer back-ral language localization. bone, we keep a single BERT architecture but de-couple the parameters into different groups to process the modality speciﬁc contents respectively.
It is the
ﬁrst attempt of utilizing the uniﬁed cross-modal trans-former network to solve the ﬁne-grained visual lan-guage alignment problem for temporal language local-ization.
• We propose a multi-stage aggregation module topped on the transformer backbone for more accurate lan-guage localization. The obtained representation con-sists of several sparsely selected and stage speciﬁc rep-resentations, which is very discriminative for accurate moment localization.
• We conduct extensive experiments on ActivityNet
Captions and TACoS datasets and the experimental results demonstrate the effectiveness of our proposed network. We believe our work will promote the future research of this new kind of architecture for temporal language localization. 2.