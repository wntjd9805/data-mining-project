Abstract
This paper is about action segmentation under weak su-pervision in training, where the ground truth provides only a set of actions present, but neither their temporal ordering nor when they occur in a training video. We use a Hid-den Markov Model (HMM) grounded on a multilayer per-ceptron (MLP) to label video frames, and thus generate a pseudo-ground truth for the subsequent pseudo-supervised training. In testing, a Monte Carlo sampling of action sets seen in training is used to generate candidate temporal se-quences of actions, and select the maximum posterior se-quence. Our key contribution is a new anchor-constrained
Viterbi algorithm (ACV) for generating the pseudo-ground truth, where anchors are salient action parts estimated for each action from a given ground-truth set. Our evalua-tion on the tasks of action segmentation and alignment on the benchmark Breakfast, MPII Cooking2, Hollywood Ex-tended datasets demonstrates our superior performance rel-ative to that of prior work. 1.

Introduction
This paper is about action segmentation by labeling video frames with action classes under weak, set-level su-In the set-supervised training, the pervision in training. ground truth is a set of actions present in a training video, but their temporal ordering, the number of action instances, and their start and end frames are unknown. This is an im-portant problem arising in many recent applications, such as, those dealing with big video datasets with automatically generated set-level annotations (e.g., via word-based video retrievals from Youtube), for which human annotations are not available (e.g. because scaling human annotations over numerous video retrieval results is difﬁcult). Our main chal-lenge is that the provided ground truth – being a set with arbitrarily ordered distinct labels of action classes – does not provide sufﬁcient constraints for a reliable learning of action segmentation.
Prior work typically adopts the following framework.
In training, an action model is used to label every video frame, and thus generate a pseudo-ground truth for the sub-sequent pseudo-supervised training of the model. In testing, a Monte Carlo sampling is ﬁrst used to generate candidate temporal sequences of actions, and then the learned model is applied to identify the best scoring candidate sequence as the solution of action segmentation. Differences among ex-isting approaches mostly lie in how the pseudo-ground truth is generated. For example, in [27], binary classiﬁers are in-dependently trained for each action to label frames using multi-instance learning. Such a training, however, cannot learn temporal spans of and transitions between actions, so these parameters are heuristically set in [27] for an HMM inference on test videos. The approach in [20] casts the set-supervised training as the NP-hard “all-color shortest path” problem [1], where frame labeling of a training video is constrained such that every action label from the ground-truth set appears at least once. For generating the pseudo-ground truth, they infer an HMM using a greedy two-step algorithm called set-constrained Viterbi (SCV). The SCV
ﬁrst produces an initial action segmentation by running the vanilla Viterbi, and then ﬂips low-scoring frame labels to actions that have been missed in the initial segmentation but are present in the ground-truth set. Thus, the SCV produces an approximate solution to the NP-hard problem by projec-tion to the legal domain. There are many shortcomings of
[20] related to the heuristic, greedy ﬂipping of frame labels.
In this paper, we improve the most critical step of the above framework – generation of the pseudo-ground truth.
As in [20], we also pose the set-supervised training as the
“all-color shortest path” problem [1]. Our key difference is that we formulate a more effective differentiable approx-imation to this NP-hard problem that allows for an end-to-end training, reduces training complexity, and ultimately leads to signiﬁcant performance gains over [20].
As illustrated in Fig. 1, all legal action segmentations of a training video can be represented by distinct paths in a directed video segmentation graph. A legal (or valid) path includes every action from the ground-truth set at least once.
Our goal is to efﬁciently identify the highest-scoring valid path. For this scoring, we use an HMM grounded via a two-layer MLP onto video frames, as shown in Fig. 2. In-9806
Figure 1. We cast the problem of generating pseudo-ground truth as ﬁnding an optimal “all-color shortest path” [1] in which every action from the ground-truth set occurs at least once. A video has numerous “all-color shortest paths”. Our key contribution is an efﬁcient approximation to this NP-hard problem. stead of using the solution-by-projection of [20], we spec-ify an Anchor-Constrained Viterbi (ACV) algorithm that ef-ﬁciently approximates the MAP “all-color shortest path”.
Efﬁciency comes from signiﬁcantly reducing the number of valid action segmentations by considering only those that include salient action parts, called anchor segments or an-chors. Importantly, the ACV enables an end-to-end training, as we use the generated pseudo-ground truth for computing the cross-entropy loss and regularization in the subsequent fully-supervised training of our HMM and MLP.
Given a training video, our approach begins by estimat-ing saliency scores of all actions from the ground-truth set at every frame using the MLP. Then, for each action, we select the most salient frame in the video to represent a cen-ter of that action’s anchor segment. Finally, we approx-imate a globally optimal action segmentation by running the standard Viterbi algorithm over the densely connected video graph whose nodes are frames and colored edges rep-resent action segments (see Fig. 1). Efﬁciency is achieved by considering only those paths in the video graph that pass through the anchor edges (i.e., segments).
The resulting pseudo-ground truth is used in the second stage of our training, where we jointly learn parameters of the HMM and MLP using the cross entropy loss on the pre-viously generated frame-wise pseudo-ground truth. We reg-ularize this learning with a diversity loss aimed at maximiz-ing a distance between saliency scores of distinct actions detected along the video. This is motivated by the follow-ing reasoning. For distinct actions, we expect that temporal patterns of their respective saliency scores across the frames
Figure 2. We use an HMM to ﬁnd the MAP “all-color shortest path” frame labeling of a training video, which is then taken as the pseudo-ground truth. For the MAP inference in training, we specify the new Anchor-Constrained Viterbi (ACV) algorithm, and diversity regularization which minimizes correlations of detected actions along the video. The pseudo-ground truth is used for com-puting the cross-entropy loss and regularization in the subsequent fully-supervised training of our HMM and MLP, enabling end-to-end training. are different. Thus, our diversity loss ensures that the salien-cies of every action are sufﬁciently discriminative in time so as to facilitate action segmentation.
In evaluation, we address action segmentation and action alignment on the Breakfast, MPII Cooking2, Hollywood
Extended datasets. Our experiments demonstrate that we outperform the state of the art on both tasks.
In the following, Sec. 2 reviews related literature, Sec. 3 deﬁnes our problem and models, Sec. 4 speciﬁes our ACV and regularization, Sec. 5 describes our inference on test videos, and Sec. 6 presents our results. 2.