Abstract
We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions.
Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D lo-cation and whose outputs are the following scene properties at that input location: volume density, surface normal, ma-terial parameters, distance to the ﬁrst surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indi-rect illumination effects. The predicted visibility and surface intersection ﬁelds are critical to our model’s ability to simu-late direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a sin-gle light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a signiﬁcant challenge to prior work. 1.

Introduction
A central problem in computer vision is that of inferring the physical geometry and material properties that together explain observed images. In addition to its importance for recognition and robotics, a solution to this open problem would have signiﬁcant value for computer graphics — the ability to create realistic 3D models from standard photos could democratize 3D content creation and allow anyone to use real-world objects in photography, ﬁlmmaking, and game development. In this paper, we work towards this goal and present an approach for estimating a volumetric 3D representation from images of a scene under arbitrary known lighting conditions, such that high-quality novel images can be rendered from arbitrary unseen viewpoints and under
Figure 1: We optimize a Neural Reﬂectance and Visibility Field (NeRV) 3D representation from a set of input images of a scene illuminated by known but unconstrained lighting. Our NeRV repre-sentation can be rendered from novel views under arbitrary lighting conditions not seen during training. Here, we visualize example input data and renderings for two scenes. The ﬁrst two output rendered images for each scene are from the same viewpoint, each illuminated by a point light at a different location, and the last image is from a different viewpoint under a random colored illumination. novel unobserved lighting conditions, as shown in Figure 1.
The vision and graphics research communities have re-cently made substantial progress towards the novel view synthesis portion of this goal. The Neural Radiance Fields (NeRF) [32] approach has shown that it is possible to syn-thesize photorealistic images of scenes by training a simple neural network to map 3D locations in the scene to a contin-uous ﬁeld of volume density and color. Volume rendering is trivially differentiable, so the parameters of a NeRF can be optimized for a single scene by using gradient descent to min-imize the difference between renderings of the NeRF and a set of observed images. Though NeRF produces compelling results for view synthesis, it does not provide a solution for relighting. This is because NeRF models just the amount of 7495
outgoing light from a location — the fact that this outgoing light is the result of interactions between incoming light and the material properties of an underlying surface is ignored.
At ﬁrst glance, extending NeRF to enable relighting ap-pears to require only changing the image formation model: instead of modeling scenes as ﬁelds of density and view-dependent color, we can model surface normals and material properties (e.g. the parameters of a bi-directional reﬂectance distribution function (BRDF)) and simulate the transport of the scene’s light sources (which we assume are known) according to the rules of physically based rendering [38].
However, simulating the attenuation and reﬂection of light by particles is fundamentally challenging in NeRF’s neural volumetric representation because content can exist any-where within the scene, and determining the density at any location requires querying a neural network. Consider the na¨ıve procedure for computing the radiance along a single camera ray due to direct illumination, as illustrated in Fig-ure 2: First, we query NeRF’s multi-layer perceptron (MLP) for the volume density at samples along the camera ray to determine the amount of light reﬂected by particles at each location that reaches the camera. For each location along the camera ray, we then query the MLP for the volume density at densely-sampled points between the location and every light source to estimate the attenuation of light before it reaches that location. This procedure quickly becomes prohibitively expensive if we want to model environment light sources or global illumination, in which case scene points may be illuminated from all directions. Prior methods for estimat-ing relightable volumetric representations from images have not overcome this challenge, and can only simulate direct illumination from a single point light source when training.
The problem of efﬁciently computing visibility is well explored in the graphics literature. In standard raytracing graphics pipelines, where the scene geometry is ﬁxed and known ahead of time, a common solution is to precompute a data structure that can be efﬁciently queried to obtain the visibility between pairs of scene points, or between scene points and light sources. This can be accomplished with approaches including octrees [44], distance transforms [8], or bounding volume hierarchies [38]. But these existing approaches do not provide a solution to our task — our ge-ometry is unknown, and our model’s estimate of geometry changes constantly as it is optimized. Though conventional data structures could perhaps be used to accelerate rendering after optimization is complete, we need to efﬁciently query the visibility between points during optimization, and ex-isting solutions are prohibitively expensive to rebuild after each training iteration (of which there may be millions).
In this work, we present a method to train a NeRF-like model that can simulate realistic environment lighting and global illumination. Our key insight is to train an MLP to act as a lookup table into a visibility ﬁeld during rendering.
Figure 2: We visualize how “Neural Visibility Fields” reduce the computational burden of volume rendering a camera ray with direct (top) and one-bounce indirect (bottom) illumination compared to na¨ıve raymarching, alongside each solution’s computational com-plexity (n is the number of samples along each ray, ` is the number of light sources, and d is the number of sampled indirect illumina-tion directions). Black dots represent evaluating a shape MLP for volume density at a position, red arrows represent evaluating the visibility MLP at a position along a direction, and the blue arrow represents evaluating the visibility MLP for the expected termina-tion depth of a ray at a position along a direction (output visibility multipliers and termination depths from the visibility MLP are displayed as text). Brute-force light transport simulation through
NeRF’s volumetric representation with na¨ıve raymarching (left) is intractable. By approximating visibility with a neural visibility
ﬁeld (right) that is optimized alongside the shape MLP, we are able to make optimization with complex illumination tractable.
Instead of estimating light or surface visibility at a given 3D position along a given direction by densely evaluating an MLP for the volume density along the corresponding ray (which would be prohibitively expensive), we simply query this visibility MLP to estimate visibility and expected termi-nation depth in any direction (see Figure 2). This visibility
MLP is optimized alongside the MLP that represents volume density, and is supervised to be consistent with the volume density samples observed during optimization. Using this neural approximation of the true visibility ﬁeld signiﬁcantly eases the computational burden of estimating volume ren-dering integrals while training. Our resulting system, which we call “NeRV” (“Neural Reﬂectance and Visibility Fields”) enables the recovery of a NeRF-like model that supports relighting in addition to view synthesis. While previous so-lutions for relightable NeRFs [3] were limited to controlled settings which required input images to be illuminated by a single point light, NeRV supports training with arbitrary en-vironment lighting and “one-bounce” indirect illumination. 7496
2.