Abstract
We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually repre-sent parts of an image, it is challenging for existing vision-language models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to “Seeing Out of tHe bOx” that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box anno-tations which enables inference 10 times faster than region-based approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding.
VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-ﬂy and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings.
In particular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accu-racy on NLVR2 test-P split, 6.7% accuracy on SNLI-VE test split, respectively. 1.

Introduction
With the success of Transformer and self-supervised learning, we have recently witnessed a boosting number of research works on cross-modal learning, especially on vision-language pre-training (VLPT) [7, 22, 23, 27, 34, 37, 48]. VLPT models learn better cross-modal representa-tion with large-scale easy-accessible image-text pairs. They have established state-of-the-art results in many vision-*Equal Contribution. This work was performed when Zhicheng Huang,
Zhaoyang Zeng and Yupan Huang were visiting Microsoft Research Asia as research interns. boat man woman
Task I: TR
Baseline: A couple sit in a   
× boat on the sea.
Ours: A couple sit on the 
√ shore next to a boat on the sea.
Task II: VQA 
Q: What are the people doing?
×
Baseline: Boating.
Ours: Chatting.
√
Figure 1: Comparisons of SOHO and region-based meth-ods by top-1 image-to-text retrieval (TR) and visual ques-tion answering (VQA) results. Baselines lack global con-text and fail to understand the image. SOHO discovers vi-sual clues out of region boxes and infers correct human ac-tivities. [Best viewed in color.] language tasks, such as visual question answering (VQA)
[3], image-text retrieval [25], natural language for visual reasoning (NLVR) [35], etc.
Visual representation plays an important role in VLPT models. The recent success of VLPT models has been ac-companied by the usage of region-based image features, which are extracted by object detectors pre-trained on the
Visual Genome dataset [2]. However, there are three chal-lenges to directly utilize region-based image features for vision-language understanding. Firstly, regions focus on objects inside bounding boxes while neglecting the contex-tual information out of the boxes, which is important for relation understanding and reasoning. For example in Fig-ure 1, we can easily detect “man”, “woman” and “boat” in the image. However, without the contextual information out of these boxes, a model will misunderstand the relation as
“people boating” and result in an incorrect answer for ei-ther text retrieval or VQA. Secondly, visual understanding of images will be limited to the pre-deﬁned categories for regions. Thirdly, most region-based image features are ex-tracted by a detection model, which will suffer from low quality, noise, and over-sampling [2] and rely on large-scale boxes annotation data. Although some works try to train detection model[38, 46] with weakly-supervised, the per-formance is far below the requirements. Recently, some 12976
​
works challenge that grid-based convolutional features are also effective to learn visual representations [9, 16, 17, 32].
Among them, Jiang et al. show that grid features can be equally effective as region features for VQA [17]. Sariy-ildiz et al. and Desai et al. use image-text data to train visual backbone for recognition tasks (e.g., image classiﬁ-cation) [9, 32]. These models are designed either for spe-ciﬁc vision-language task [17] or vision task [9, 32]. In this paper, we focus on VLPT and propose one of the ﬁrst end-to-end VLPT model without relying on region features.
To overcome the limitation of region-based image fea-tures and better utilize image-text pairs for cross-modal understanding, we propose SOHO, an end-to-end vision-language pre-training framework to directly learn image embedding, language embedding, and their semantic align-ment from image-text pairs. Compared with existing VLPT works, SOHO adopts a simple pipeline that does not require a complicated visual backbone for pre-training and releases the design effort for VLPT tasks. Without the requirement of laborious annotated categories or boxes, SOHO can en-rich visual semantics by directly optimizing visual repre-sentations by a wider range of image-text data.
End-to-end learning for vision and language raises chal-lenges by different representations of the two modalities.
Visual representation at pixel-level is much more diverse and dense than language embedding. And the lack of ex-plicit supervision for pixel-level language adds the difﬁculty to alignment learning. To tackle the above problems, we introduce a visual dictionary (VD) which represents more comprehensive and compact semantics in visual domain.
To learn the visual dictionary, we design a moving-averaged encoder to group visual pixels with similar visual semantics.
VD can be dynamically updated through our trainable CNN backbone directly from visual-language data during pre-training. For pre-training tasks, we propose a novel Masked
Vision Modeling (MVM) based on the learned visual dictio-nary besides two commonly used tasks, Masked Language
Modeling (MLM) and Image-Text Matching (ITM).
Our contributions are summarized as follows: (i) We propose SOHO, one of the ﬁrst end-to-end VLPT models to learn cross-modal representation directly with image-text pairs. Without the need of extracting bounding boxes, our model can achieve at least 10 times speedup for inference. (ii) To better align visual features and language tokens, we propose a novel dynamic-updated visual dictionary that represents a visual abstraction of similar semantics in im-ages. (iii) We conduct extensive experiments on four well-established downstream tasks. Experimental results show that SOHO can improve the SOTA performance with abso-lute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR2 test-P split, 6.7% accuracy on SNLI-VE test split, and 0.56% VQA score on
VQA2.0 test-std split. We will release both model and code to facilitate the research community1. 2.