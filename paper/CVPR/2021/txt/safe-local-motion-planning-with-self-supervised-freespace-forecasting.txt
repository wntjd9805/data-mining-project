Abstract
Safe local motion planning for autonomous driving in dynamic environments requires forecasting how the scene evolves.
Practical autonomy stacks adopt a semantic object-centric representation of a dynamic scene and build object detection, tracking, and prediction modules to solve forecasting. However, training these modules comes at an enormous human cost of manually annotated objects across frames.
In this work, we explore future freespace as an alternative representation to support motion plan-ning. Our key intuition is that it is important to avoid stray-ing into occupied space regardless of what is occupying it. Importantly, computing ground-truth future freespace is annotation-free. First, we explore freespace forecasting as a self-supervised learning task. We then demonstrate how to use forecasted freespace to identify collision-prone plans from off-the-shelf motion planners. Finally, we propose fu-ture freespace as an additional source of annotation-free supervision. We demonstrate how to integrate such supervi-sion into the learning-based planners. Experimental results on nuScenes and CARLA suggest both approaches lead to a signiﬁcant reduction in collision rates.1 1.

Introduction
Motion planning in dynamic environments requires fore-casting how the scene imminently evolves. What repre-sentation should we forecast to support planning? In prac-tice, standard autonomy stacks forecast a semantic object-centric representation by building perceptual modules such as object detection, tracking, and prediction [42]. However, in the context of machine learning, training these modules comes at an enormous annotation cost, requiring massive 1Code will be available at https://github.com/peiyunh/ff 12732
amounts of data manually annotated with object labels, in-cluding both 3D trajectories and semantic categories (e.g., cars, pedestrians, bicyclists, etc). With autonomous ﬂeets gathering petabytes of data, it’s impossible to label data at a rate that keeps up with the rate of data collection.
To avoid the need for such costly annotations, and to en-able learning at scale, we explore an alternative freespace-centric representation to support motion planning (Fig. 1).
We believe this is effective for two primary reasons. First, freespace is a natural cue for safe planning - it is generally important to avoid straying into occupied space, regardless of what is occupying it. Second, gathering training data for freespace forecasting is annotation-free given LiDAR scans recorded from an autonomous vehicle.
In this work, we propose two approaches for using a freespace-centric representation to assist with planning.
First, we explore freespace forecasting as a self-supervised learning task. We point out essential modeling choices for building an effective predictor that forecasts freespace.
Then, given an off-the-shelf black-box motion planner, we demonstrate that self-supervised future freespace predic-tions can be used to identify candidate plans that are likely to collide with objects in the near future.
Lastly, we propose using future freespace as an addi-tional source of supervision when learning to plan. Many planners learn from expert demonstrations, and for exam-ple, learn to imitate good habits like maintaining a wide safety margin when approaching pedestrians in the street.
However, it is difﬁcult for the learner to know which other actions are bad, since there may have been multiple rea-sonable actions that could have been taken. We use future freespace to identify a subset of other actions that are clearly poor because they collide with an obstacle. We empirically show that imitative learning-based planners with such ad-ditional supervision produce motion plans that are far safer and less likely to induce collisions.
Contributions: We explore a self-supervised freespace-centric representation as an alternative to the predominantly supervised object-centric representation. We are the ﬁrst to integrate self-supervised freespace predictions with an existing planner and demonstrate promising results. We also propose simple modiﬁcations to existing learning ap-proaches to planning that allow future freespace to be used as an additional source of self-supervision. Finally, we demonstrate promising results on planning benchmarks. 2.