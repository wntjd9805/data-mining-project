Abstract
Time-to-contact (TTC), the time for an object to collide with the observer’s plane, is a powerful tool for path plan-ning: it is potentially more informative than the depth, ve-locity, and acceleration of objects in the scene—even for humans. TTC presents several advantages, including re-quiring only a monocular, uncalibrated camera. However, regressing TTC for each pixel is not straightforward, and most existing methods make over-simplifying assumptions about the scene. We address this challenge by estimating
TTC via a series of simpler, binary classiﬁcations. We pre-dict with low latency whether the observer will collide with an obstacle within a certain time, which is often more criti-cal than knowing exact, per-pixel TTC. For such scenarios, our method offers a temporal geofence in 6.4 ms—over 25× faster than existing methods. Our approach can also esti-mate per-pixel TTC with arbitrarily ﬁne quantization (in-cluding continuous values), when the computational budget allows for it. To the best of our knowledge, our method is the ﬁrst to offer TTC information (binary or coarsely quan-tized) at sufﬁciently high frame-rates for practical use. 1.

Introduction
Path planning, whether for robotics or automotive appli-cations, requires accurate perception, which in turn, bene-ﬁts from depth information. Many modalities exist to infer depth. Strategies such as lidar estimates depth accurately but only at sparse locations, in addition to being expen-sive. Depth can also be estimated with strategies such as stereo [34], but these introduce issues such as calibration drift over time.
An alternative is to use a monocular camera—an at-tractive, low-cost solution, with light maintenance require-ments. The motion of the camera induces optical ﬂow be-tween consecutive frames, which carries information on the scene’s depth. Depth, however, can only be estimated in the constrained case of static scenes. For dynamic scenes, the 2D ﬂow of a pixel is a function of its depth, its velocity, and the velocity of the camera. Disentangling these three com-ponents is an under-constrained and challenging problem.
This work was done while A. Badki was interning at NVIDIA.
Project page: https://github.com/NVlabs/BiTTC
I0
I1
I0
I1
Q
C
Q
C
τ ≤ 2 s y a w a g n i v o m
B
τ ≤ 0.8 s m o v i n g t o w a r d s
B
Figure 1: Given I0 and I1, our binary time-to-contact (TTC) es-timation acts as a temporal geofence detecting objects that will collide with the camera plane within a given time, B . It only takes 6.4 ms to compute. Our method can also output quantized
TTC, Q , and continuous TTC, C .
Previous approaches either ignore dynamic regions [35], or use strong scene priors [26, 44, 46, 43, 25] to hallucinate their depth. Do we really need to disentangle them? The role of perception is to inform decisions. An object mov-ing towards the camera is more critical than another that is potentially closer, but moving away from the camera. Dif-ferently put, predicting the time at which an object would make contact with the camera may be more valuable than knowing its actual depth, velocity, or acceleration [22].
In fact, time-to-contact (TTC), the time for an object to collide with the camera plane under the current velocity conditions, is a traditional concept in psychophysics [38, 14] as well as computer vision [37, 5]. TTC can be esti-mated from the ratio of an object’s depth and its velocity relative to the camera, even when the problem of regressing either one independently is ill-posed. However, TTC esti-mation has its own challenges, forcing most of the existing approaches to severely constrain their scope. For instance, they assume that the scene is static, or that a mask for dy-namic objects is provided [16, 32]. The recent approach by Yang and Ramanan tackles some of these challenges by learning a mapping between optical ﬂow and TTC directly, thus producing a per-pixel TTC estimate [42]. However, it relies on accurate optical ﬂow estimation and inherits its limitations, including its heavy computational load.
Unlike most existing approaches, we side-step the need to explicitly compute the optical ﬂow. Our learning-based approach estimates per-pixel TTC directly from images.
We leverage the relationship between an object’s TTC and the ratio of the size of its image in different frames [5, 15]. 12946
However, because regressing this scale factor exactly is challenging, we focus on whether the size of the object’s image is increasing, indicating a collision at some time in the future, or decreasing, indicating that the object is mov-ing away.
More concretely, inspired by Badki et al. [1], we per-form a series of binary classiﬁcations with respect to dif-ferent scale factors, each corresponding to a speciﬁc TTC.
Each classiﬁcation yields a binary TTC map with respect to the desired time threshold, Figure 1, insets B . Our binary map, efﬁcient to compute, acts as a temporal geofence in front of the camera: it identiﬁes objects within a given TTC, in 6.4 ms.1 This is useful when a quick reaction time is im-portant. We can also estimate per-pixel TTC with arbitrary quantization, as shown in Figure 1, insets Q , or contin-uous, Figure 1, insets C . These different levels of quan-tization, from binary to continuous, can be predicted with the same core network. In fact, quantization levels can be added, removed, or moved dynamically at inference time, based on the current needs of the autonomous agent. Given the scarcity of TTC ground truth data, to impose additional inductive bias to our network we also introduce binary op-tical ﬂow estimation as an auxiliary task. We achieve com-petitive performance for TTC estimation against existing methods, even stereo-based methods, but we are from 25× to several orders of magnitude faster. 2.