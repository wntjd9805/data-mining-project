Abstract
A deep facial attribute editing model strives to meet two requirements: (1) attribute correctness – the target attribute should correctly appear on the edited face image; (2) ir-relevance preservation – any irrelevant information (e.g., identity) should not be changed after editing. Meeting both requirements challenges the state-of-the-art works which resort to either spatial attention or latent space factoriza-tion. Speciﬁcally, the former assume that each attribute has well-deﬁned local support regions; they are often more ef-fective for editing a local attribute than a global one. The latter factorize the latent space of a ﬁxed pretrained GAN into different attribute-relevant parts, but they cannot be trained end-to-end with the GAN, leading to sub-optimal so-lutions. To overcome these limitations, we propose a novel latent space factorization model, called L2M-GAN, which is learned end-to-end and effective for editing both local and global attributes. The key novel components are: (1) A la-tent space vector of the GAN is factorized into an attribute-relevant and irrelevant codes with an orthogonality con-straint imposed to ensure disentanglement. (2) An attribute-relevant code transformer is learned to manipulate the at-tribute value; crucially, the transformed code are subject to the same orthogonality constraint. By forcing both the original attribute-relevant latent code and the edited code to be disentangled from any attribute-irrelevant code, our model strikes the perfect balance between attribute correct-ness and irrelevance preservation. Extensive experiments on CelebA-HQ show that our L2M-GAN achieves signiﬁ-cant improvements over the state-of-the-arts. 1.

Introduction
Facial attribute editing [45, 48, 54, 3, 4, 29, 47], i.e., manipulating the semantic attributes of a real face im-age, has a wide range of real-world application scenarios 2951
such as entertainment, auxiliary psychiatric treatment, and data augmentation for other facial tasks. With the tremen-dous success of deep generative models [12, 37, 26], fa-cial attribute editing has become topical in recent works
[33, 17, 52, 16, 27, 46, 58], most of which are based on generative adversarial networks (GANs) [12].
One of the main challenges for facial attribute editing is to meet two requirements simultaneously: (1) attribute cor-rectness – the target attribute should correctly appear on the edited image; (2) irrelevance preservation – the irrelevant information (e.g., identity, or other attributes) should not be changed during attribute editing. However, meeting both requirements is hard because there often exist strong cor-relations between different attributes (e.g., moustache and gender) as well as between attributes and identity [16, 58].
As a result, editing one attribute may result in unintended altering of other characteristics of the face image.
To achieve attribute correctness whilst avoiding unin-tended altering, many recent methods [16, 27] resort to spa-tial attention. The assumption is that each attribute has lo-cal support regions which can be modeled using an attention module on feature maps of an encoder-decoder GAN frame-work. Once these support regions are identiﬁed, image ma-nipulation can be restricted to those regions thus stopping unwanted changes in other regions. This assumption is valid for some local attributes such as bangs or glasses. It is how-ever problematic when it comes to global attributes such as smiling/gender/age, for which support regions are global and overlapping between attributes is inevitable.
Another recent line of approach is to focus on the factor-ization of the latent space learned by a face synthesis GAN into attribute-relevant latent codes [46, 58]. Given a ﬁxed pretrained GAN, the latent space vector is mapped to each attribute via subspace projection. However, there are two issues with this approach: (a) It relies on a ﬁxed pretrained
GAN to provide the latent space. Without end-to-end train-ing with the factorization model, this latent space could be (b) Guided by semantic labels, it can only sub-optimal. disentangle different semantic attributes from each other.
However, there are also other characteristics of a face im-age that are not described by a set of pre-deﬁned attributes, e.g., identity and lighting condition.
To overcome the limitations of the current state-of-the-arts [16, 27, 46, 58], we propose a novel latent space factor-ization model, called learning-to-manipulate GAN or L2M-GAN, which is learned end-to-end and effective for editing both local and global attributes (see Figure 1). Similar to
[46, 58], our L2M-GAN model is designed to factorize a
GAN latent space into semantic codes guided by attribute annotations, without imposing any spatial constraints on feature maps as in [16, 27]. Differently, for each attribute, both attribute-relevant and -irrelevant codes are factorized explicitly. Moreover, the disentanglement between the two codes are enforced both before and after the editing. Con-cretely, inspired by the latest StarGAN v2 [6], we apply a style encoder on the input image to obtain the source style/latent space code. Further, we devise a new style trans-former which is the key component for facial attribute edit-ing. It is composed of two main modules: (1) a decomposer for disentangling the source style code into two orthogo-nal parts – an attribute-relevant code and everything else in an attribute-irrelevant code; (2) a domain transformer for transforming the attribute-relevant code from its origi-nal value/domain to a target one (e.g., unsmiling to smil-ing). Crucially, the transformed code is also subject to the orthogonality constraint w.r.t. the factorized attribute-irrelevant code. In this way, the attribute correctness and ir-relevance preservation requirements are fulﬁlled explicitly in our L2M-GAN. Further, unlike the latest works [46, 58], our L2M-GAN can now be trained end-to-end.
Our main contributions are three-fold: (1) For the ﬁrst time, we propose an end-to-end GAN model namely L2M-GAN for facial attribute editing by explicitly factorizing the latent space vector of a GAN into attribute-relevant and (2) To facilitate the latent space fac--irrelevant codes. torization, we devise a novel style transformer by impos-ing the orthogonality constraint on the factorized attribute-relevant and -irrelevant codes both before and after the edit-ing/transformation. (3) Extensive experiments on CelebA-HQ [23] show that our L2M-GAN achieves signiﬁcant im-provements over the state-of-the-arts.
Importantly, once learned, our L2M-GAN has a wide use in other attribute ma-nipulation tasks (e.g., attribute strength manipulation and manipulation with reference images) without re-training, and also generalizes well from photo to anime faces. 2.