Abstract
This paper considers the problem of unsupervised person re-identiﬁcation (re-ID), which aims to learn discriminative models with unlabeled data. One popular method is to ob-tain pseudo-label by clustering and use them to optimize the model. Although this kind of approach has shown promis-ing accuracy, it is hampered by 1) noisy labels produced by clustering and 2) feature variations caused by camera shift.
The former will lead to incorrect optimization and thus hin-ders the model accuracy. The latter will result in assign-ing the intra-class samples of different cameras to different pseudo-label, making the model sensitive to camera vari-In this paper, we propose a uniﬁed framework to ations. solve both problems. Concretely, we propose a Dynamic and Symmetric Cross-Entropy loss (DSCE) to deal with noisy samples and a camera-aware meta-learning algo-rithm (MetaCam) to adapt camera shift. DSCE can allevi-ate the negative effects of noisy samples and accommodate the change of clusters after each clustering step. MetaCam simulates cross-camera constraint by splitting the training data into meta-train and meta-test based on camera IDs.
With the interacted gradient from meta-train and meta-test, the model is enforced to learn camera-invariant features.
Extensive experiments on three re-ID benchmarks show the effectiveness and the complementary of the proposed DSCE and MetaCam. Our method outperforms the state-of-the-art methods on both fully unsupervised re-ID and unsupervised domain adaptive re-ID. 1.

Introduction
Person re-identiﬁcation (re-ID) attempts to ﬁnd matched pedestrians of a query in a non-overlapping camera sys-*Equal contribution: yangfx@stu.xmu.edu.cn
†Corresponding author: {zhiming.luo, szlig}@xmu.edu.cn (a) Initial State (b) w/o MetaCam (c) w/ MetaCam (1) (2) (3) (4)
Figure 1. Illustration of camera variations in person re-ID (a) and the comparison between methods trained without or with the pro-posed MetaCam ((b) and (c), respectively). Different colors rep-resent different identities and different shapes indicate different camera IDs. At the initial state, samples under different cameras may suffer from appearance changes of viewpoints ((1) & (2)), il-lumination ((3) & (4)), and other factors. Without considering this factor, the trained model may be sensitive to camera variations and may wrongly split intra-class features to different centers. Our proposed MetaCam enables the model to learn camera-invariant features by explicitly considering cross-camera constraint. tem. Recent CNN-based works [31, 35] have achieved impressive accuracies, but their success is largely depen-dent on sufﬁcient annotated data that require a lot of label-ing cost. In contrast, it is relatively easy to obtain a large collection of unlabeled person images, fostering the study of unsupervised re-ID. Commonly, unsupervised re-ID can be divided into two categories depending on whether us-ing an extra labeled data, i.e., unsupervised domain adap-tation (UDA) [37, 49, 7] and fully unsupervised re-ID (FU) [19, 20, 42]. In UDA, we are given a labeled source domain and an unlabeled target domain. The data of two domains have different distributions and are used to train a model that generalizes well on the target domain. The fully unsupervised re-ID is more challenging since only un-labeled images are provided to train a deep model. In this study, we will mainly focus on this setting, and call it as unsupervised re-ID for simplicity. 4855
Recent popular unsupervised re-ID methods [19, 34, 20, 42] mainly adopt clustering to generate pseudo-label for un-labeled samples, enabling the training of the model in a su-pervised manner. Pseudo-label generation and model train-ing are applied iteratively to train an accurate deep model.
Despite their effectiveness, existing methods often ignore two important factors during this process. (1) Noisy labels brought by clustering. The clustering algorithm cannot en-sure intra-samples to be assigned with the same identity, which inevitably will introduce noisy labels in the labeling step. The errors of noisy labels will be accumulated during training, thereby hindering the model accuracy. (2) Feature variations caused by camera shifts. As shown in Fig. 1, intra-class samples under different cameras may suffer from the changes of viewpoint (e.g., (1) and (2) in Fig. 1), illu-mination (e.g., (3) and (4) in Fig. 1), and other environmen-tal factors. At the start of unsupervised learning (“initial state” in Fig. 1), these signiﬁcant variations will cause large gaps between the intra-class features of different cameras.
In such a situation, it is difﬁcult for the clustering algorithm to cluster samples with the same identity from all cameras into the same cluster. Consequently, training with the sam-ples mined by the clustering will lead to unexpected sep-aration for intra-class samples (“w/o MetaCam” in Fig. 1) and the model might be sensitive to camera variations dur-ing testing. In this paper, we attempt to solve the above two crucial problems for robust unsupervised re-ID.
For the ﬁrst issue, we try to adopt the technique of learn-ing with noisy labels (LNL) for robust training. LNL is well-studied in image classiﬁcation, however, most of the existing methods cannot be directly applied to our sce-nario. This is because the centers and pseudo-label will change after each clustering step. To overcome this difﬁ-culty, this paper proposes a dynamic and symmetric cross-entropy loss (DSCE) for unsupervised re-ID. We maintain a feature memory to store all image features, which enables us to dynamically build new class centers and thus to be adaptable to the change of clusters. With the dynamic cen-ters, a robust loss function is proposed for mitigating the negative effects caused by noisy samples.
For the second issue, we attempt to explicitly consider camera-invariant constraint during training. Indeed, person re-ID is a cross-camera retrieval process, aiming to learn a model that can well discriminate samples under different cameras. If a model trained with samples from some of the cameras can also generalize to distinguish samples from the rest of the cameras, then, we could obtain a model that can extract the intrinsic feature without camera-speciﬁc bias and is robust to camera changes. Inspired by this, this paper in-troduces a camera-aware meta-learning (MetaCam), which aims to learn camera-invariant representations by simulat-ing the cross-camera re-identiﬁcation process during train-ing. Speciﬁcally, MetaCam separates the training data into meta-train and meta-test, ensuring that they belong to en-tirely different cameras. We then enforce the model to learn camera-invariant features under both camera settings by up-dating the model with meta-train and validating the updated model with meta-test. Along with learning from different meta divisions, the model is gradually optimized to gener-alize well under all cameras. As shown in Fig. 1, Meta-Cam gathers intra-class features of different cameras into the same cluster, which is beneﬁcial for mining pseudo-label and learning camera-invariant features. In summary, our main contributions can be summarized in three aspects:
• We propose a dynamic and symmetric loss (DSCE), which enables the model to be robust to noisy labels during training in the context of changes of clusters and thus promotes the model performance.
• We propose a camera-aware meta-learning algorithm (MetaCam) for adapting the shifts caused by cameras.
By simulating the cross-camera searching process dur-ing training, MetaCam can effectively improve the ro-bustness of the model to camera variations.
• We introduce a uniﬁed framework that can jointly take advantage of the proposed DSCE and MetaCam, en-abling us to learn a more robust re-ID model.
Extensive experiments on three large-scale datasets demonstrate the advantages of our DSCE and MetaCam for the fully unsupervised re-ID. Besides, further experiments on the UDA setting show that our method can also achieve state of the art. 2.