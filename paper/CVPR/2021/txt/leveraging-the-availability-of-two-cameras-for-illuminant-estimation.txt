Abstract
Most modern smartphones are now equipped with two rear-facing cameras – a main camera for standard imaging and an additional camera to provide wide-angle or tele-In this paper, we leverage the photo zoom capabilities. availability of these two cameras for the task of illumination estimation using a small neural network to perform the il-lumination prediction. Speciﬁcally, if the two cameras’ sen-sors have different spectral sensitivities, the two images pro-vide different spectral measurements of the physical scene.
A linear 3×3 color transform that maps between these two observations – and that is unique to a given scene illumi-nant – can be used to train a lightweight neural network comprising no more than 1460 parameters to predict the scene illumination. We demonstrate that this two-camera approach with a lightweight network provides results on par or better than much more complicated illuminant estima-tion methods operating on a single image. We validate our method’s effectiveness through extensive experiments on ra-diometric data, a quasi-real two-camera dataset we gen-erated from an existing single camera dataset, as well as a new real image dataset that we captured using a smart-phone with two rear-facing cameras. 1.

Introduction
An overwhelming percentage of consumer photographs are currently captured using smartphone cameras. A recent trend in smartphone imaging system design is to employ two (or more) rear-facing cameras to ameliorate the lim-itations imposed by the smartphone compact form factor.
In most cases, the two rear-facing cameras have different focal lengths and lens conﬁgurations to allow the smart-phone to deliver DSLR-like optical capabilities (i.e., wide-angle and telephoto). In addition, the two-camera setup has been leveraged for applications such as synthetic bokeh ef-fect [48] and reﬂection removal [40]. Given the utility of the two-camera conﬁguration, this design trend is likely to con-tinue for the foreseeable future. In this work, we show that the two-camera setup has another beneﬁt, that of improving
Figure 1: (A) Most modern smartphones use two rear-facing cameras. Typically, the spectral characteristics of these two cameras’ sensors are slightly different. (B) Thus, a two-camera system furnishes two different measurements of the scene being imaged. Our proposed two-camera al-gorithm harnesses this extra information for more accurate and efﬁcient illuminant estimation. the accuracy of illuminant estimation.
Illuminant estimation is the most critical step for com-putational color constancy. Color constancy refers to the ability of the human visual system to perceive scene col-ors as being the same even when observed under different illuminations [39]. Cameras do not innately possess this il-lumination adaptation ability; the raw-RGB image recorded by the camera sensor has signiﬁcant color cast due to the scene’s illumination. As a result, computational color con-stancy is applied to the camera’s raw-RGB sensor image as one of the ﬁrst steps in the in-camera imaging pipeline to remove this undesirable color cast. The main goal of the camera’s auto-white-balance (AWB) module, which is mo-tivated by the concept of computational color constancy, is illuminant estimation. AWB involves estimating the scene illumination in the sensor’s raw-RGB color space and then applying a simple 3×3 diagonal matrix computed directly 6637
from the estimated illumination parameters to perform the white-balance correction. Thus, accurate estimation of the scene illumination is crucial to ensuring correct scene col-ors in the camera image.
We demonstrate that two-camera systems have the po-tential to provide more accurate illuminant estimation com-pared to existing single-camera methods. A key insight is that the spectral characteristics of the main camera’s sen-sor are typically different from that of the second camera’s.
This is due to a variety of reasons. For example, the pitch of the photodiodes and overall resolution of the two sensors are often different to accommodate the different optics as-sociated with each sensor. These differences impact which color ﬁlter arrays (CFA) manufacturers can use in the sen-sor’s production process. This results in the two CFAs hav-ing different spectral sensitivities to incoming light. While on the surface this may appear to be a disadvantage, differ-ences in the CFA between the two cameras can be corrected for by the later stages of the camera imaging pipeline to en-sure the ﬁnal output colors appear the same (e.g., see [36]).
However, for our purpose, the sensors’ unprocessed raw im-ages effectively provide different spectral measurements of the underlying scene. It is this complementary information that allows us to design a two-camera illumination estima-tion algorithm as shown in Fig. 1.
Contribution We propose to train a neural network for il-luminant estimation that receives as input a 3 × 3 matrix computed between the two cameras’ raw sensor images si-multaneously capturing the same scene. Prior work [21] has shown that the color transformation between different spec-tral samples of the same scene has a unique signature that is related to the scene illumination. This allows the color transformation itself to be used as the feature for illumina-tion estimation. Thus, in contrast to existing single-camera illumination estimation methods that train their deep net-works directly on image data, or on image histograms, our network needs to examine only nine parameters in the color transformation matrix. As a result, we can train a very lightweight neural network comprising just 1460 parame-ters that can be efﬁciently run on-device in real time. We test our proposed approach extensively with experiments on radiometric data, a quasi-real two-camera dataset we generated from an existing single-camera color constancy dataset [16], and ﬁnally on a real two-camera dataset that we captured using a Samsung S20 Ultra smartphone. We com-pare our technique against several state-of-the-art single-image illuminant estimation methods and demonstrate on par or even improved performance. 2.