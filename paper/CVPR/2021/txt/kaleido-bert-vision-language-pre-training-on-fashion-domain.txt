Abstract
We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT , which introduces a novel kaleido strategy for fashion cross-modality representations from transformers.
In contrast to random masking strat-egy of recent VL models, we design alignment guided mask-ing to jointly focus more on image-text semantic relations.
To this end, we carry out ﬁve novel tasks, i.e., rotation, jigsaw, camouﬂage, grey-to-color, and blank-to-color for self-supervised VL pre-training at patches of different scale.
Kaleido-BERT is conceptually simple and easy to extend to the existing BERT framework, it attains state-of-the-art re-sults by large margins on four downstream tasks, includ-ing text retrieval (R@1: 4.03% absolute improvement), im-age retrieval (R@1: 7.13% abs imv.), category recognition
† Equal; * Corresponding author: Deng-Ping Fan (dengpfan@gmail.com). (ACC: 3.28% abs imv.), and fashion captioning (Bleu4: 1.2 abs imv.). We validate the efﬁciency of Kaleido-BERT on a wide range of e-commerical websites, demonstrating its broader potential in real-world applications. 1.

Introduction
Transformers [14, 68], ﬁrst designed for Natural Lan-guage Processing (NLP), have achieved great success in a number of other areas as well [5, 11], including the vi-sion (e.g., Selﬁe [66], DETR [6], ViT [34], and PVT [69]) and vision-language (ViLBERT [45], VL-BERT [60], OS-CAR [42]) communities. However, for VL Pre-Training
Model (PTM), current approaches, such as VL-BERT [60] and UNITER [9] focus on learning text and image represen-tation of a general domain (i.e., coarse matching). As such, these techniques will beneﬁt for general cross-modality rep-resentation learning. 12647
No.
Pre-Training Model Year Pub.
Architecture
Training Set
Core Idea 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28
VisualBERT [40]
CBT [63]
VideoBERT [64]
B2T2 [2]
LXMERT [38]
ViLBERT [45]
ImageBERT [55]
Unicoder-VL [38]
VLP [82]
VL-BERT [61]
VD-BERT [70]
VLN-BERT [48]
HERO [39]
XGPT [73]
InterBERT [43]
VILLA [20]
ActBERT [83]
PREVALENT [24] 12-IN-1 [46]
Pixel-BERT [27]
FashionBERT [21]
UNITER [9]
VisDial-BERT [50]
OSCAR [42]
ERNIEL-VIL [78]
RVL-BERT [10]
UniVL [47]
MMFT-BERT [32] 2019 arXiv 2019 arXiv 2019 ICCV 2019 EMNLP 2019 EMNLP 2019 NeurlIPS 2020 arXiv 2020 AAAI 2020 AAAI 2020 ICLR 2020 EMNLP 2020 ECCV 2020 EMNLP 2020 arXiv 2020 arXiv 2020 NeurlIPS 2020 CVPR 2020 CVPR 2020 CVPR 2020 arXiv 2020 SIGIR 2020 ECCV 2020 ECCV 2020 ECCV 2020 arXiv 2020 arXiv 2020 arXiv 2020 EMNLP
Coco
Kinetics [30]
SC
CC
VG+Coco
CC
CC+VG+SC
CC+SBU
CC
CC
VisDial [12]
Matterport3D [7]
TV+HT100M
CC+SC
Coco+CC+SBU
Coco+CC+SBU
HT100M
Matterport3D [7]
ES
Coco+VG
Fashion-Gen [58] 1-Stream 2-Stream 1-Stream 1-Stream 2-Stream 2-Stream 1-Stream 1-Stream 1-Stream 1-Stream 1-Stream 2-Stream 3-Stream 1-Stream 1-Stream 2-Stream 1-Stream 2-Stream 2-Stream 1-Stream 1-Stream 1-Stream Coco+VG+CC+SBU 2-Stream 1-Stream 2-Stream 1-Stream 2-Stream 3-Stream
CC+VQA [4]
ES
CC+SBU
VDR [44]
HT100M
TV
First Image-Text PTM AFAK
Noise contrastive estimation loss
First Vedio-Text PTM AFAK
Explicitly reference RoI to text before inputting PTM
Three encoders for ROIs, language, and cross-modality features
Cross-modality co-attention layers
Pre-training with a large-scale Image-Text dataset
Masked object classiﬁcation
Uniﬁed en/decoder PTM for VL generation and understanding
Visual feature embedding
Video-Dialog pre-training
Path selection in VL navigation
Video-subtitle matching & Frame order modeling
Improve VL generative ability
Masked group modeling
Adversarial pre-training and ﬁnetune
Global frame and local object regions & Tangled transformer
Pre-train with image-text-action triplets
Multi-task training
Pixel-level VL semantics alignment
Patches & Adaptive loss
Conditional masking & Word region alignment
Adapt ViLBERT for visual dialog
Object tags as anchor points
VL PTM with knowledge-enhanced ERNIE [80]
Visual relationship detection with VL-BERT
Five pre-training objectives & Two pre-training strategies
Multi-modal fusion PTM
Image
Video
Video
Image
Image
Image
Image
Image
Image
Image
Video
Image
Video
Image
Image
Image
Video
Image
Image
Image
Image
Image
Image
Image
Image
Image
Vedio
Image
Domain Pre-train Finetune
X
X
X
X
X
X
X
X
X
X
X
U
U/G/O
G/O
U
U
U
U
U
U/G
U
O
O
U
U/G
U
U/O
U/O
O
U
U
U
U
O
U/G
U
U
U/G
U
Vision Feature
Code
RoI
Frame
Frame
RoI
RoI
RoI
RoI
RoI
RoI
RoI
RoI
RoI
Frame
RoI
RoI
RoI
Frame & RoI
Image
RoI
Pixel
Patch
RoI
RoI
RoI
RoI
RoI
Frame
RoI
Torch
N/A
N/A
Tensorﬂow
Torch
Torch
N/A
N/A
Torch
Torch
Torch
N/A
N/A
N/A
Torch
Torch
N/A
Caffe & C++
Torch
N/A
Tensorﬂow
Torch
Torch
Torch
Paddle
Torch
N/A
Torch
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X 29 Kaleido-BERT (OUR) 2021 CVPR 1-Stream
Fashion-Gen [58]
Kaleido patches & Pre-alignment with masking
Image
U/G
Patch & Coordinate
Tensorﬂow
Table 1: Summary of 28 previous representative cross-modality methods and our Kaleido-BERT model. Training Set: Coco =
MSCOCO Caption [8]. VG = Visual Genome [35]. CC = Conceptual Caption [59]. SBU = SBU Captions [53]. TV = TVQA [37].
HT100M = HowTo100M [49]. SC: Self Collection. ES: 12-in-1 and OSCAR ensemble 12, 5+ datasets, respectively. Finetune: U =
Understanding tasks (e.g. classiﬁcation). G = Generation tasks (e.g. image caption). O = Others (e.g. action task).
However, in the various e-commercial situations (e.g., accessories, clothing, toys), the main goal is to learn the
ﬁne-grained representation (e.g. short sleeve, cotton and jersey) rather than only the coarse representation (what, where) in the general domain. In this case, the current gen-eral VL models [9, 60] are sub-optimal for fashion-based tasks [1, 26, 67], and could be unfavorable when deploying global features based models to attribute-aware tasks, such as searching for a speciﬁc fashion captioning [75] and fash-ion catalog/object [15], where it is essential to extract ﬁne-grained features or similarities [65] from image and text.
In this paper, we propose a novel framework (see Fig. 1) for the fashion-based tasks. The core idea is to focus on
ﬁne-grained representation learning and to bridge the se-mantic gaps between text and image. To achieve this goal, we ﬁrst introduce an efﬁcient “kaleido” strategy, which ex-tracts a series of multi-grained image patches for the image-modality. As a result, our model is named as Kaleido-BERT. This strategy is scalable and largely alleviates the aforementioned coarse presentation issue by introducing the patch-variant pre-training scheme. Furthermore, to bridge the semantic gap between different modalities, attention mechanism is employed to build pre-alignments between kaleido patches and text tokens. This pre-alignment infor-mation further guides the masking strategy for pre-training.
Kaleido-BERT1 is forced to explicitly learn semantic infor-mation across modalities.
In summary, our contributions are as follows:
• Kaleido Patch Generator: We propose the kaleido strategy to generate a kaleido of multi-grained fea-tures. With the related pre-training tasks, i.e. rotation, 1 https://github.com/mczhuge/Kaleido-BERT/. jigsaw, camouﬂage, grey-to-color, and blank-to-color,
Kaleido-BERT learns ﬁne-grained cross-modality in-formation and outperforms the ﬁxed-patch or RoI-based VL models in the fashion domain.
• Attention-based Alignment Generator: Kaleido-BERT introduces the pre-alignment strategy to infer a cross-modality mapping between kaleido patches and text tokens. These pre-alignment pairs largely ﬁll the semantic gaps between modalities.
• Alignment Guided Masking: We present an alignment-guided masking strategy to explicitly force
Kaleido-BERT to learn the semantic connections be-tween vision and language. Experiments show the im-portance of the attention-based pre-alignment and the alignment masking strategy. 2.