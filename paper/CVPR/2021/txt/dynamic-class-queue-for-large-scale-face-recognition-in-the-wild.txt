Abstract
Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world ap-plications, yet it remains challenging. The difﬁculties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classiﬁcation-based representation learning with deep neu-ral networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of iden-tities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems.
Speciﬁcally, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-ﬂy which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model paral-lel, we empirically verify in large-scale datasets that 10% of classes are sufﬁcient to achieve similar performance as using all classes. Moreover, the class weights are dynam-ically generated in a few-shot manner and therefore suit-able for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest pub-lic dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 in-stances. Code is available at https://github.com/ bilylee/DCQ 1.

Introduction
Recently, face recognition has witnessed great progress along with the development of deep neural networks and large-scale datasets. The size of the public training set has
*Corresponding to Bi Li and Teng Xi. Equal contribution.
FC Layer
Class Queue enter
"
SGD update
! weight generation
"′
" leave (cid:1) all classes vs. subset (cid:2) SGD-update vs. feedforward generation (a) FC-based (b) DCQ-based
Figure 1. High-level comparison between the fully connected layer (FC layer) used in classiﬁcation and the proposed DCQ module.
Generally, there are two differences. 1) For the FC layer, all classes in the training dataset are included in the FC layer (each row in the FC layer represents a class weight). While for DCQ, only a subset is used. 2) The class weights in the FC layer are ran-domly initialized and then updated via SGD. In contrast, DCQ gets the class weights in a few-shot manner based on another instance x′ with the same identity as input x. been steadily increasing from CASIA-Webface[27] (10K identity, 0.5M instance) → MS-Celeb-1M[7] (100K iden-tity, 5M instance) → MF2[19] (672K identity, 4.7M in-stance). For commercial applications, the training dataset easily scales up to millions of identities and it is a matter of time to reach billions of identities.
More data brings better performance. However, the training difﬁculty accumulates along with the growth of the training data. First of all, it simply needs more com-puting resources. For classiﬁcation-based methods, where each identity is taken as a class and the feature extractor is learned through the classiﬁcation task, the memory con-sumption of the fully connected layer W ∈ RD×C linearly scales up to the number of identities C in the training set.
So is the cost to compute the matrix multiplication between the FC layer and the input feature. Secondly, for data gath-ered in the real world, the class distribution is typically long-tailed, that is, some classes have abundant instances 3763
(head classes) while most classes have few instances (tail classes). For example, the MF2 dataset contains images gathered from Flickr, and over 88% of identities have less than 10 images. As witnessed by various studies[2, 29, 33], long-tailed classiﬁcation is itself a challenging problem.
To tackle the computing resource constraint, one option is to adopt pairwise-based methods which have the bene-ﬁts of being class-number agnostic and therefore can be po-tentially extended to datasets with an arbitrary number of identities. However, the pair sampling mechanism is crit-ical for this method to achieve good performance[21] and it takes a much longer time to converge. Another option is to dynamically reduce the number of classes used during training. Zhang et al.[30] propose to use a hashing forest to partition the space of class weights into small cells. Given a training sample x, they walk through the forest to ﬁnd the closest cell and use classes within as the FC Layer. Con-current with our work, An et al.[1] demonstrate that ran-domly sampling classes from all classes can also achieve matching performance as using all classes. These methods can largely reduce the computational cost by using a sub-set of classes for computing the matrix multiplication and the softmax function. However, they still require all class weights to be stored in the memory. [30] uses a parame-ter server to overcome this problem while [1] uses model parallel1 to distribute the class weights into several GPUs.
Moreover, since the class weights W are updated by SGD, they also need to store all optimization-related stats of the class weights such as the momentum in memory.
As for long-tailed class distribution, a simple solution would be removing tail classes such that the class distribu-tion is balanced. However, the full potential of the training data is undermined in this way. Another option is to use class-based sampling during training, i.e. sampling classes with equal probability[3]. However, as demonstrated in our experiment, this is not necessary and even harmful for our method. Zhong et al.[33] propose a two-stage train-ing mechanism where the model is ﬁrstly trained with only the head classes and then ﬁnetuned with both head and tail classes. As noted by the authors, the second stage needs careful manual tuning to achieve good performance. It is of interest to design methods that require only single-stage training.
In this work, we propose a single-stage method that is computationally efﬁcient and has low memory consump-tion. The key innovation is to design a dynamic class queue.
The meaning of “dynamic” is two-fold. First, the class sub-set used for classiﬁcation is dynamically selected. The com-putational cost is reduced since only a part of the classes are involved in the computation. Second, the class weights are dynamically generated on-the-ﬂy instead of learning via 1It splits the model into parts and places them in different GPUs to meet memory constraints.
SGD. Since the class weight is dynamically generated at each iteration, it does not require storing all class weights or optimization-related stats in the memory. These class weights are stored in a queue where the queue size can be 10x smaller than all classes in our experiments. Importantly, the class weights are dynamically generated in a few-shot way which is friendly to tail classes and therefore helpful for training long-tailed datasets. For a visual illustration of the proposed method, please refer to Figure 1.
We empirically verify that the proposed method is ef-fective and efﬁcient. By using a single server and less than 9GB memory per GPU card without model parallel, the pro-posed method uses 10% of classes while still achieving sim-ilar performance as the baseline which uses all classes in the large-scale dataset (MS1MV2[5]). The proposed method is most useful in the real-world long-tailed dataset. We demonstrate this on the MF2[19] dataset which outperforms a strong baseline by 0.75% (absolute change) in identiﬁca-tion and 1.72% in veriﬁcation tasks with only 10% classes. 2.