Abstract
Learning latent variable models with deep top-down ar-chitectures typically requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference step typically relies on either time-consuming long-run Markov chain Monte
Carlo (MCMC) sampling or a separate inference model for variational learning. In this paper, we propose to use a short-run MCMC, such as a short-run Langevin dynamics, as an approximate ﬂow-based inference engine. The bias existing in the output distribution of the non-convergent short-run
Langevin dynamics is corrected by the optimal transport (OT), which aims at transforming the biased distribution produced by the ﬁnite-step MCMC to the prior distribution with a minimum transport cost. Our experiments not only verify the effectiveness of the OT correction for the short-run
MCMC, but also demonstrate that the latent variable model trained by the proposed strategy performs better than the variational auto-encoder (VAE) in terms of image recon-struction/generation and anomaly detection. 1.

Introduction
Recent years have seen a great success of deep generative models in numerous computer vision applications, such as image generation [10, 16, 13], image recovery [21, 12, 23], image representation [33, 29], image disentanglement [35, 4, 25], anomaly detection [34, 31], etc. Such models typically include simple and expressive generator networks, which are latent variable models assuming that each observed ex-ample is generated by a low-dimensional vector of latent variables, and the latent vector follows a non-informative prior distribution, such as Gaussian distribution. Since high dimensional visual data (e.g., images) usually lie on low-dimensional manifolds embedded in the high-dimensional space, learning latent variable models of visual data is of fun-damental importance in the ﬁeld of computer vision for the sake of unsupervised representation learning. The challenge mainly comes from the inference of the latent variables for each observation, which typically relies on Markov chain
Monte Carlo (MCMC) [24, 6] methods to draw fair samples from the analytically intractable posterior distribution (i.e., the conditional distribution of the latent variables given the observed example). Since the posterior distribution of the latent variables is parameterized by a highly non-linear deep neural network, the MCMC-based inference can suffer from non-convergence and inefﬁciency problems, thus affecting the accuracy of the model parameter estimation.
To avoid inefﬁcient MCMC sampling from the posterior, variational inference [16] becomes an attractive alternative by approximating the intractable posterior via a tractable network. Despite the growing prevalence and popularity of the variational auto-encoder (VAE) [16], its drawbacks are increasingly obvious. (i) It parameterizes the intrinsic itera-tive inference process by an extrinsic feedforward inference model. These extra parameters due to the reparameterization have to be estimated together with those of the generator net-work. (ii) Such a joint training is to be accomplished by max-imizing the variational lower bound. Thus, the accuracy of
VAE heavily depends on the accuracy of the inference model as an approximation of the true posterior distribution. Only when the Kullback-Leibler (KL)-divergence between the in-ference and the posterior distribution is equal to zero, the variational inference is equivalent to the desired maximum likelihood estimation. This goal is usually infeasible in prac-tice. (iii) An extra effort is required to made in designing the inference model of VAE, especially for the generators that have complicated dependency structures with the latent variables, e.g., [30] proposed a top-down generator with mul-tiple layers of latent variables, [39, 40] proposed dynamic generators with time sequences of latent variables. It is not a simple task to design inference models that infer latent variables for models mentioned above. An arbitrary design of the inference model cannot guarantee the performance. 115415
In this paper, we will totally abandon the idea of reparam-eterizing the inference process, and reuse the MCMC-based inference for training deep latent variable models. To be speciﬁc, we use a short-run MCMC, such as a short-run
Langevin dynamics [19, 26], to perform the inference of the latent vectors during training. However, the convergence of ﬁnite-step Langevin dynamics in each iteration might be questionable, so we accept the bias existing in such a short-run MCMC and propose to use the optimal transport (OT) method [38] to correct the bias. The OT can be adopted to transform an arbitrary probability distribution to a de-sired distribution with a minimum transport cost. Thus, we can use the OT cost to measure the difference between two probability distributions. We treat the short-run MCMC as a learned ﬂow model whose parameters are from the la-tent variable model. We correct the bias of the short-run
MCMC by performing an optimal transport from the result distribution produced by the short-run MCMC to the prior distribution. This operation is to minimize the OT cost be-tween the inference distribution and the prior distribution, in which we don’t optimize any parameters in the ﬂow model but update its output. With the corrected inference output, we can update the parameters of the latent variable model more accurately.
Speciﬁcally, our algorithm iterates the following three steps: (i) inference step: inferring the latent variables for each observed example by a short-run Langevin dynamics that samples from the posterior distribution; (ii) correction step: moving the population of all the inferred latent vectors to the prior distribution through optimal transport; (iii) learn-ing step: update the model parameters by gradient descent based on the corrected latent vectors and the corresponding observed examples.
There are several advantages in the proposed algorithm: (i) efﬁciency: The learning and inference of the model are efﬁcient with a short-run MCMC. (ii) convenience: The approximate inference model represented by the short-run
MCMC is automatic in the sense that there is nothing to worry about the design and training of a separate inference model. Both bottom-up inference and top-down genera-tion are governed by the same set of parameters. (iii) ac-curacy: the optimal transport corrects the errors of the non-convergent short-run MCMC inference, thus improves the accuracy of the model parameter estimation.
The contributions of the paper are three-fold: (i) We propose to train a deep latent variable model by a non-convergent short-run MCMC inference with OT correction. (ii) We extend the semi-discrete OT algorithm to approxi-mate the one-to-one map between the inferred latent vectors and the samples drawn from the prior distribution in our settings. (iii) We provide strong empirical results in our ex-periments to verify the effectiveness of the proposed strategy to train deep latent variable models. 2.