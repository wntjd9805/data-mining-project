Abstract
This paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point from a rgb-camera) and a cloud. Given an image (e.g. from a 3D Lidar scanner) cap-general point cloud (e.g. tured at different locations in the same scene, our method estimates the relative rigid transformation between the co-ordinate frames of the camera and Lidar. Learning com-mon feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difﬁculty by converting the registration problem into a classiﬁcation and inverse cam-era projection optimization problem. A classiﬁcation neu-ral network is designed to label whether the projection of each point in the point cloud is within or beyond the cam-era frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results on Ox-ford Robotcar and KITTI datasets demonstrate the feasi-bility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P. 1.

Introduction
Image-to-point cloud registration refers to the process of
ﬁnding the rigid transformation, i.e., rotation and translation that aligns the projections of the 3D point cloud to the im-age. This process is equivalent to ﬁnding the pose, i.e., ex-trinsic parameters of the imaging device with respect to the reference frame of the 3D point cloud; and it has wide ap-plications in many tasks in computer vision, robotics, aug-mented/virtual reality, etc.
Although the direct and easy approach to solve the regis-tration problem is to work with data from the same modal-ity, i.e., image-to-image and point cloud-to-point cloud, several limitations exist in these same-modality registration approaches. For point cloud-to-point cloud registration, it is impractical and costly to mount expensive and hard-to-maintain Lidars on large ﬂeet of robots and mobile devices during operations. Furthermore, feature-based point cloud-to-point cloud registration [6, 43, 21, 40] usually requires
Figure 1. Illustration of feature based registration on the left, e.g., 2D3D-MatchNet, and our feature-free DeepI2P on the right. In-stead of detecting and matching features across modalities, we convert the registration problem into a classiﬁcation problem. storage of D-dimensional features (D ≫ 3) in addition to the (x, y, z) point coordinates, which increases the mem-ory complexity. For image-to-image registration, meticu-lous effort is required to perform SfM [37, 36, 12] and store the image feature descriptors [29, 22] corresponding to the reconstructed 3D points for feature matching. Additionally, image features are subjected to illumination conditions, sea-sonal changes, etc. Consequently, the image features stored in the map acquired in one season/time are hopeless for reg-istration after a change in the season/time.
Cross-modality image-to-point cloud registration can be used to alleviate the aforementioned problems from the same modality registration methods. Speciﬁcally, a 3D point cloud-based map can be acquired once with Lidars, and then pose estimation can be deployed with images taken from cameras that are relatively low-maintenance and less costly on a large ﬂeet of robots and mobile de-vices. Moreover, maps acquired directly with Lidars cir-cumvents the hassle of SfM, and are largely invariant to seasonal/illumination changes. Despite the advantages of cross-modality image-to-point cloud registration, few re-search has been done due to its inherent difﬁculty. To the best of our knowledge, 2D3D-MatchNet [11] is the only prior work on general image-to-point cloud registra-tion. This work does cross-modal registration by learning to match image-based SIFT [22] to point cloud-based ISS [45] keypoints using deep metric-learning. However, the method suffers low inlier rate due to the drastic dissimilarity in the
SIFT and ISS features across two modalities.
In this paper, we propose the DeepI2P: a novel approach for cross-modal registration of an image and a point cloud 15960
without explicit feature descriptors as illustrated in Fig. 1.
Our method requires lesser storage memory, i.e., O(3N ) for the reference point cloud since we do not rely on feature descriptors to establish correspondences. Furthermore, the images captured by cameras can be directly utilized without
SfM. We solve the cross-modal image-to-point cloud regis-tration problem in two stages. In the ﬁrst stage, we design a two-branch neural network that takes the image and point cloud as inputs, and outputs a label for every point that in-dicates whether the projection of this point is within or be-yond the image frustum. The second stage is formulated as an unconstrained continuous optimization problem. The objective is to ﬁnd the optimal camera pose, i.e., the rigid transformation with respect to the reference frame of the point cloud, such that 3D points labeled as within the cam-era frustum is correctly projected into the image. Standard solvers such as the Gauss-Newton algorithm can be used to solve our camera pose optimization problem. Extensive ex-perimental results on the open-source Oxford Robotcar and
KITTI datasets show the feasibility of our approach.
The main contributions of this paper are listed as follow:
• We circumvent the challenging need to learn cross-modal feature descriptor for registration by casting the problem into a two-stage classiﬁcation and optimiza-tion framework.
• A two-branch neural network with attention modules to enhance cross-modality fusion is designed to learn labels of whether a 3D point is within or beyond the camera frustum.
• The inverse camera projection optimization is pro-posed to solve for the camera pose with the classiﬁ-cation labels of the 3D points.
• Our method and the experimental results show a proof-of-concept that cross-modal registration can be achieved with deep classiﬁcation. 2.