Abstract
For learned image compression, the autoregressive con-text model is proved effective in improving the rate-distortion (RD) performance. Because it helps remove spa-tial redundancies among latent representations. However, the decoding process must be done in a strict scan order, which breaks the parallelization. We propose a paralleliz-able checkerboard context model (CCM) to solve the prob-lem. Our two-pass checkerboard context calculation elimi-nates such limitations on spatial locations by re-organizing the decoding order. Speeding up the decoding process more than 40 times in our experiments, it achieves signiﬁcantly improved computational efﬁciency with almost the same rate-distortion performance. To the best of our knowledge, this is the ﬁrst exploration on parallelization-friendly spa-tial context model for learned image compression. 1.

Introduction
Image compression is a vital and long-standing research topic in multimedia signal processing. Various algorithms are designed to reduce spatial, visual, and statistical redun-dancies to produce more compact image representations.
Common image compression algorithms like JPEG [15],
JPEG2000 [30] and BPG [8] all follow a general pipeline, where lossless entropy coders [23] are used after image transformations and quantization. In those non-learned im-age compression methods, content loss only occurs in the quantization process. The transformations involved mainly include Discrete Cosine Transformation and Wavelet Trans-formation, which are lossless.
In recent years, many state-of-the-art (SOTA) deep learn-ing and computer vision techniques have been introduced to build powerful learned image compression methods. Many studies aim to establish novel image compression pipelines based on recurrent neural networks [35], convolutional au-toencoders [5, 34, 31, 3, 6], or generative adversarial net-works [1]. Some of them [26, 18, 11] have attained a better
*Corresponding author. This work is done when Dailan He, Yaoyan
Zheng and Baocheng Sun are interns at SenseTime Research. (a) serial 3 × 3 (b) serial 5 × 5 (c) ours 3 × 3 (d) ours 5 × 5
Figure 1. Masked convolutions modeling spatial causal context.
The red blocks denote elements to en/de-code. Latents in yellow and blue locations are currently visible (all of them are visible dur-ing encoding, and those who have been decoded are visible during decoding). A context modeling can be conducted using a masked convolution which is centered at the red location and only con-volves with the yellow latents. (a)(b) 3 × 3 and 5 × 5 instances of the widely used serial context model. This context model requires strict Z-ordered serial decoding, which limits the computational efﬁciency. (c)(d) Our proposed checkerboard context model with a kernel size of 3 × 3 and 5 × 5. After decoding all anchors, which are latents in blue and yellow locations, the context calculating for all non-anchors can be run in parallel. performance than those currently SOTA conventional com-pression techniques such as JPEG2000 [30] and BPG [8] on both the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) [36] distortion metrics. Of particular note is, even the intra coding of Versatile Video
Coding (VVC) [9], an upcoming video coding standard, has been approached by a recent learning-based method [10].
The common key of those currently most successful ap-proaches is the entropy modeling and optimizing method, with autoencoder based structure to perform a nonlinear transform coding [14, 5]. By estimating the probability dis-tribution of latent representations, such models can mini-mize the entropy of these representations to be compressed, which directly correlates to the ﬁnal code length using arithmetic encoding [32] or range encoding [24], and en-able a differentiable form of rate-distortion (RD) optimiza-tion. Another important aspect is the introduction of hyper-prior [6]. Hyper latent is the further extracted representa-tion, which provides side-information implicitly describing the spatial correlations in latent. Adopting hyperprior mod-eling allows entropy models to approximate latent distribu-tions more precisely and beneﬁts the overall coding perfor-14771
mance. This method [6] is referred to as a scale hyperprior framework in their later work [26], where hyper latent is used to predict the entropy model’s scale parameter.
The context model [26, 18], inspired by the concept of context from traditional codecs, is used to predict the prob-ability of unknown codes based on latents that have already been decoded (as shown in Figure 1(a-b)). This method is referred to as a mean-scale hyperprior framework [26, 16], where hyper latent and context are used jointly to predict both the location (i.e. mean value) and scale parameter of the entropy model. Evaluated by previous works [26, 18], combining all the above-mentioned components (differen-tiable entropy modeling, hyper latent, and context model) can beat BPG in terms of PSNR and MS-SSIM. These context models are extended to more powerful and, of course, more computation-consuming ones by a series of later works [38, 19, 11, 10].
Though it seems promising, there are still many prob-lems to solve before those models can be used in practice.
The above-mentioned context model, which plays a crucial role in achieving SOTA performance and is adopted by most recent works, has a horribly low computational efﬁciency because of the lack of parallelization [26, 16]. Recent works focusing on the real-world deployment of practical neural image compression choose to omit the context model due to its inefﬁciency. They choose to use only the scale hy-perprior framework [4] or the mean-scale hyperprior frame-work but without the context model [16]. Li et al. introduce
CCN [21] for a faster context calculation with moderate par-allelizability but its efﬁciency is still limited by image size.
In order to develop a practical and more effective learning-based image codec, it is essential to investigate more efﬁ-cient context models.
In this paper, we propose a novel parallelizable checker-board context model along with a two-pass decoding method to achieve a better balance between RD perfor-mance and running efﬁciency. 2.