Abstract
Garment transfer shows great potential in realistic ap-plications with the goal of transfering outﬁts across dif-ferent people images. However, garment transfer between images with heavy misalignments or severe occlusions still remains as a challenge. In this work, we propose Comple-mentary Transfering Network (CT-Net) to adaptively model different levels of geometric changes and transfer outﬁts between different people.
In speciﬁc, CT-Net consists of three modules: i) A complementary warping module ﬁrst estimates two complementary warpings to transfer the de-sired clothes in different granularities. ii) A layout predic-tion module is proposed to predict the target layout, which guides the preservation or generation of the body parts in the synthesized images. iii) A dynamic fusion module adap-tively combines the advantages of the complementary warp-ings to render the garment transfer results. Extensive ex-periments conducted on DeepFashion dataset demonstrate that our network synthesizes high-quality garment transfer images and signiﬁcantly outperforms the state-of-art meth-ods both qualitatively and quantitatively. Our source code will be available online. 1.

Introduction
Most existing virtual try-on methods are based on sim-plifying assumptions: (i) Pure clothing images or 3D infor-mation are available. (ii) Pose changes are simple without heavy misalignments or severe occlusions. We argue that these simplifying assumptions greatly limit the application scope of these methods in the realistic virtual try-on sce-narios. To address this issue, we propose Complementary
Transfering Network (CT-Net), a novel image-based gar-ment transfer network that does not rely on pure clothing images or 3D information while capable to adaptively deal with different levels of geometric changes. As shown in
Figure 1, given a target person image I T and a model image
I M , without any restriction to the poses or shapes of I T and
I M , our CT-Net synthesizes photo-realistic garment trans-fer results, in which the person in I T wearing the clothes depicted in I M with well-preserved details.
Figure 1. Garment transfer results generated by CT-Net. First row: model images. First column: target person images. As shown above, CT-Net naturally transfers clothes across differ-ent people with arbitrary poses or shapes and synthesizes photo-realistic images with well-preserved characteristics of the desired clothes and distinct identities of humans. Please refer to supple-mentary materials for more results.
*Guosheng Lin is the corresponding author
Despite various methods have been proposed to realize 9899
virtual try-on in different settings [13, 34, 39, 38, 27, 6, 11, 37], there is still a gap between these methods and the unlimited realistic scenarios. Some methods [10, 4, 26] involve 3D information to deal with occlusions, but they are greatly limited by expensive devices and high computa-tional costs. Others [13, 34, 39, 38] may rely on stand-alone clothing images, which are not easy to get timely online.
Moreover, most of them attempt to model the geometric changes of the clothes utilizing a Thin Plate Spline (TPS) warping. Because TPS warping is limited by a small num-ber of parameters and only capable to shape simple defor-mations, their methods fail to deal with complex cases with heavy misalignments or severe occlusions. Garment trans-fer methods aim to transfer outﬁts across different people.
Although prior arts [27, 11, 37] have achieved considerable progress, none of them address the issue of large geometric changes.
We aim to fulﬁll this gap by proposing a novel gar-ment transfer network, Complementary Transfering Net-work (CT-Net), which precisely transfers outﬁts across dif-ferent people while tolerating different levels of geometric changes. As shown in Figure 2, CT-Net has three modules:
First, a Complementary Warping Module (CWM) is in-troduced to warp the desired clothes into the target region.
Specially, we simultaneously estimate two complementary (a) Distance warpings with different levels of freedom:
ﬁelds guided (DF-guided) dense warping. (b) Thin Plate
Spline (TPS) warping. DF-guided dense warping has a high degree of freedom and is utilized to warp the desired clothes to be well-aligned with the target pose; while limited by a small number of parameters, TPS warping roughly trans-fers the desired clothes into the target region with well-preserved textures.
Second, a Layout Prediction Module (LPM) is intro-duced to predict the target layout, in which the target person wearing the desired clothes. Compared to prior works, which may suffer from the misalignments between inputs [27, 11, 38], our Layout Prediction Module makes more accurate predictions based on the aligned warping re-sults from Complementary Warping Module. Leveraging the predicted target layouts, our network dynamically de-termines the non-target body areas and the occluded body areas, which guides the adaptive preservation and genera-tion. Beneﬁted from joint training, Layout Prediction Mod-ule also adds spatial constraints to the training of comple-mentary warpings, encouraging the warping results to be more coherent with the target person.
Third, a Dynamic Fusion Module (DFM) integrates all the information provided by previous modules to render the garment transfer results. Speciﬁcally, our Dynamic Fusion
Module adopts an attention mechanism to adaptively com-bine the advantages of the two complementary warpings and synthesizes photo-realistic garment transfer results with well-preserved characteristics of the clothes.
Extensive experiments conducted on DeepFashion dataset demonstrate the superiority of our method compared to the state-of-art methods. Our main contributions can be summarized as follows:
• We propose a novel image-based garment transfer net-work, which adaptively combines two complementary warpings to model different levels of geometric changes and synthesizes photo-realistic garment transfer results with well-preserved characteristics of the clothes and dis-tinct human identities.
• A novel Layout Prediction Module makes precise predic-tion on the target layout, which clearly shapes the synthe-sized results, guides the adaptive preservation and gener-ation of the body parts and adds spatial constraints to the training of the complementary warpings.
• Evaluated on DeepFashion [21] dataset, CT-Net synthe-sizes high-quality garment transfer results and outper-forms all the state-of-art methods both qualitatively and quantitatively. 2.