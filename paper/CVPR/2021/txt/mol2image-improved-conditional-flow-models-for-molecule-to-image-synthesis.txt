Abstract
In this paper, we aim to synthesize cell microscopy images under different molecular interventions, motivated by practi-cal applications to drug development. Building on the recent success of graph neural networks for learning molecular embeddings and ﬂow-based models for image generation, we propose Mol2Image: a ﬂow-based generative model for molecule to cell image synthesis. To generate cell features at different resolutions and scale to high-resolution images, we develop a novel multi-scale ﬂow architecture based on a Haar wavelet image pyramid. To maximize the mutual information between the generated images and the molec-ular interventions, we devise a training strategy based on contrastive learning. To evaluate our model, we propose a new set of metrics for biological image generation that are robust, interpretable, and relevant to practitioners. We show quantitatively that our method learns a meaningful embedding of the molecular intervention, which is translated into an image representation reﬂecting the biological effects of the intervention. 1.

Introduction
High-content cell microscopy assays are gaining traction in recent years as the rich morphological data from the im-ages proves to be more informative for drug discovery than conventional targeted screens [6, 12, 55]. Motivated by these developments, we aim to build, to our knowledge, the ﬁrst generative model to synthesize cell microscopy images un-der different molecular interventions, translating molecular information into a high-content and interpretable image rep-resentation of the intervention. Such a system has numerous practical applications in drug development – for example, it could enable practitioners to virtually screen compounds based on their predicted morphological effects on cells, al-lowing more efﬁcient exploration of the vast chemical space and reducing the resources required to perform extensive experiments [46, 53, 57]. Small molecules are known to
*To whom correspondence should be addressed: KY, karren@mit.edu;
CU, cuhler@mit.edu
Figure 1: Molecule to Image Synthesis. High-content cell morphology images captured under different molecular inter-ventions enable practitioners to assess a broad spectrum of drug effects. We improve on state-of-the-art ﬂow-based gen-erative models to build a molecule to image synthesis model with potential applications to virtual chemical screening. enter cells and alter their biological functions and pathways, leading to changes in cell shape, structure, organization, etc., that are visible in microscopy images [14, 13]. In contrast to conventional models that predict speciﬁc chemical proper-ties, a molecule-to-image synthesis model has the potential to produce a panoptic view of the morphological effects of a drug that captures a broad spectrum of properties such as mechanisms of action [33, 34, 45] and gene targets [4].
To build our molecule-to-image synthesis model (Mol2Image), we integrate state-of-the-art graph neural net-works for learning molecular representations with ﬂow-based generative models. Flow-based models are a relatively recent class of generative models that learn the data distribution by directly inferring the latent distribution and maximizing the log-likelihood of the data [9, 10, 26]. Compared to other 6688
classes of deep generative models such as variational au-toencoders (VAEs) [27] and generative adversarial networks (GANs) [17], ﬂow-based models do not rely on approximate posterior inference or adversarial training to learn the data distribution and are less prone to training instability and mode collapse, making them reliable and advantageous for biological practitioners [54].
However, molecule-to-image synthesis is a challenging task that highlights key, unsolved problems in ﬂow-based image generation. First, state-of-the-art ﬂow models such as NVP [10] and Glow [26] cannot be trained on large im-ages such as full-resolution cell images (e.g., 512 × 512) due to memory constraints, as ﬂow models contain signiﬁ-cantly more parameters than other generative models. Sec-ond, conditional variants of ﬂow models are not nearly as well-developed as their counterparts in generative adversar-ial networks. While many variants of conditional GANs have been developed to synthesize images from complex information such as text [47], conditional ﬂow models have so far synthesized images from binary vectors such as im-age classes or attributes [26, 32] or other images [1], where the correspondence between the image and the conditioning information is more straightforward. These open problems in ﬂow-based models limit their application to real-world problems such as molecule-to-image synthesis.
Contributions. The contributions of this work are two-fold. (1) We improve on state-of-the-art ﬂow-based generative models to develop a model that can generate high-resolution cell images conditioned on molecular interventions. Our methodological contributions to ﬂow models include:
• A new multi-scale ﬂow model based on the framework of a Haar wavelet image pyramid that is trained to generate images in a coarse-to-ﬁne fashion and can scale to large, high-resolution cell images. The existing state-of-the-art model, Glow [26], cannot be trained on images larger than 256 × 256 due to memory constraints. Our principled choice of the Haar wavelet image pyramid enables us to scale training to large images, while preserving the original objective of maximizing the log-likelihood of the data.
• A training algorithm for conditional ﬂow models that lever-ages contrastive learning to maximize the mutual infor-mation between the generated images and the condition-ing molecules. Although we focus on molecule-to-image synthesis, this approach can potentially extend to other challenging applications of conditional ﬂow models, e.g., text-to-image synthesis [47]. (2) We establish a new benchmark for molecule to image synthesis on the Cell Painting dataset [2], a high-content cell microscopy assay, motivated by practical applications to drug development and virtual chemical screening. To evaluate models on this task, we propose a new set of evalua-tion metrics speciﬁc to cell image generation that are robust, interpretable, and relevant to practitioners. We show that our approach outperforms the baselines on this task, indicating potential for virtual screening. 2.