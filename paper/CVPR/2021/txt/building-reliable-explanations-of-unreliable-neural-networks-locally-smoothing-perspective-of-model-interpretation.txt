Abstract
We present a novel method for reliably explaining the predictions of neural networks. We consider an explanation reliable if it identiﬁes input features relevant to the model output by considering the input and the neighboring data points. Our method is built on top of the assumption of smooth landscape in a loss function of the model predic-tion: locally consistent loss and gradient proﬁle. A theoreti-cal analysis established in this study suggests that those lo-cally smooth model explanations are learned using a batch of noisy copies of the input with the L1 regularization for a saliency map. Extensive experiments support the analy-sis results, revealing that the proposed saliency maps re-trieve the original classes of adversarial examples crafted against both naturally and adversarially trained models, signiﬁcantly outperforming previous methods. We further demonstrated that such good performance results from the learning capability of this method to identify input features that are truly relevant to the model output of the input and the neighboring data points, fulﬁlling the requirements of a reliable explanation. 1.

Introduction test
The recent progress of deep neural networks has led to their adoption in various decision-critical applications, in-cluding medical, ﬁnance, and legal ﬁelds and autonomous vehicles. However, the high modeling capacity of deep models renders the inner operations of the models mostly uninterpretable and demands human-understandable expla-nations for model predictions. For this purpose, the model output attribution for input features is a popular idea. The attribution aims to identify the importance of input features using end-to-end relationships between inputs and model predictions. We use a saliency map as the visual form of an explanation. A saliency map is a common approach to vi-sual tasks to implement pixel-level or regional attributions for a given image [32, 25, 9, 37, 16, 22].
*Correspondence to: Sungchan Kim (s.kim@jbnu.ac.kr).
The susceptibility of neural networks can cause false predictions for a given imperceptibly modiﬁed image [12], which has emerged as a new challenge in explaining model predictions [9, 3, 37, 10, 5, 14, 29, 33]. For instance, re-cent work has demonstrated that input can be manipulated, resulting in different saliency maps without damaging the classiﬁcation accuracy [10, 5, 14, 33]. Such a false explana-tion is primarily due to the fragility of learned models that have highly nonsmooth decision boundaries rather than due to the explanation methods. Although adversarial training has addressed these concerns [17, 39, 24, 7, 11, 36, 19], it is not always applicable and still incomplete.
This discussion indicates the need to build a reliable model explanation with two requirements if the goal is to recover input features that are important in a local neigh-borhood: ﬁrst, a reliable explanation method should be ro-bust so that it generates consistent explanations along with neighboring (and thus similar) data points; second, expla-nations generated by the method must have high ﬁdelity for model predictions.
We propose RelEx, a novel method to reliably explain predictions of neural network-based classiﬁers. RelEx aims to generate robust and yet accurate saliency maps of pixel-level importance for a given image. Inspired by recent work on adversarial training [18, 17, 19], we built RelEx on top of an assumption on a locally smooth explanation for the vicinity of the input.
Although substantial work has proposed creating saliency maps based on gradient [28, 34, 30, 1, 27, 25] or perturbation [20, 9, 37, 16, 22], researchers have hardly ad-dressed both robustness and accuracy in a single method.
Existing methods often fail to ﬁnd out essential features of the input and neighborhood for the model predictions de-spite being visually plausible, as shown in this study. More-over, using either a random or adversarial perturbation of data points can manipulate their explanations [10, 5]. Re-cent work has addressed such an issue partially [3, 9, 37, 5].
In contrast to the existing methods, we construct an analy-sis to characterize the behavior of the proposed explanation method based on the assumption of the local explanation.
Speciﬁcally, the contributions of this paper are as follows. 6468
` 1   norm of    norm of  perturbation  perturbation  0  0.07  0.1  0.3  1  f  tion  0  0.07  0.1  0.3  1  white stork 0.986  2.98e-10  7.78e-12  5.21e-11  1.84e-11  chimpanzee 0.963  0.047  0.003  3.83e-08  6.61e-14  0.999  0.999  0.999  0.999  0.999  0.967  0.967  0.961  0.941  0.835  x  ed)  0.846  0.516  0.212  0.179  0.007  d  d  0.252  0.103  0.063  0.009  4.47e-04 
RelEx  (Proposed) 
SmGrad 
IntGrad  0.029  0.015  0.028  (a) Naturally trained ResNet-50 0.035  0.002  0.121  0.051  (b) Adversarilly trained ResNet-50 0.025  0.05  0.002 
Figure 1. Comparison of saliency maps. Images on the top row depict adversarial examples created by a PGD attack [17] for given perturbation distance in the ℓ1-norm on (a) naturally trained and (b) adversarially trained ResNet-50. Numbers below images represent its softmax scores while ones below saliency maps indicate the scores of corresponding explanations. Our method generates saliency maps that leads to the scores close to 1 consistently in the presence of perturbations. The saliency maps of other methods, SmGrad [30] and
IntGrad [34], are visually plausible but irrelevant to the score. (See the supp. S3.7 for more results)
• We establish a quadratic approximation on a locally smooth landscape from the model explanation per-spective to identify a trade-off between the accuracy and robustness of saliency maps. Our analysis reveals that the robustness of an explanation method is bet-ter achieved at the cost of the reduced accuracy of the model explanation and that the curvature of a loss function for learning a saliency map is inverse-proportional to the ℓ1-norm of saliency maps to which, however, the explanation accuracy is proportional. A similar trace-off was investigated in adversarial train-ing [36, 39]; but it was hardly addressed in the context of an explanation method. Although the use of the ℓ1-norm is not new this work is the ﬁrst attempt to address its effects on building a reliable explanation method to the best of our knowledge.
• Our analysis leads to an easy-to-implement objective function to learn a saliency map by using backpropa-gation. We need only noisy copies of an input image as a batch for the optimization that is regularized by the
ℓ1-norm of a saliency map.
• RelEx identiﬁes input features relevant to a decision for all points in a neighborhood over the existing meth-ods when applying it to naturally and adversarially trained models. We demonstrate that explanations by the proposed method achieved a remarkably robust re-trieval of the target classes from adversarial examples created via strong white-box attacks (Figure 1). Exten-sive evaluations indicate that such an advantage our method is due to learning appropriate saliency maps even with severe perturbations. 2.