Abstract
Physical adversarial examples for camera-based com-puter vision have so far been achieved through visible ar-tifacts — a sticker on a Stop sign, colorful borders around eyeglasses or a 3D printed object with a colorful texture.
An implicit assumption here is that the perturbations must be visible so that a camera can sense them. By contrast, we contribute a procedure to generate, for the ﬁrst time, physical adversarial examples that are invisible to human eyes. Rather than modifying the victim object with visible artifacts, we modify light that illuminates the object. We demonstrate how an attacker can craft a modulated light signal that adversarially illuminates a scene and causes targeted misclassiﬁcations on a state-of-the-art ImageNet deep learning model. Concretely, we exploit the radiomet-ric rolling shutter effect in commodity cameras to create precise striping patterns that appear on images. To human eyes, it appears like the object is illuminated, but the camera creates an image with stripes that will cause ML models to output the attacker-desired classiﬁcation. We conduct a range of simulation and physical experiments with LEDs, demonstrating targeted attack rates up to 84%. 1.

Introduction
Recent work has established that deep learning models are susceptible to adversarial examples — manipulations to model inputs that are inconspicuous to humans but induce the models to produce attacker-desired outputs [36, 17, 11].
Early work in this space investigated digital adversarial ex-amples where the attacker can manipulate the input vector, such as modifying pixel values directly in an image classiﬁ-cation task. As deep learning has found increasing applica-tion in real-world systems like self-driving cars [26, 15, 31],
UAVs [8, 30], and robots [38], the computer vision com-munity has made great progress in understanding physical adversarial examples [14, 5, 34, 24, 10] because this attack
∗Both authors contributed equally to this work.
With Attack Signal
Without Attack Signal
Figure 1: Images as seen by human (without border) and as captured by camera (in black border) with the attack signal (left two images) and without (right two images). The image without the attack signal is classiﬁed as coffee mug (conﬁdence 55%), while the image with the attack signal is classiﬁed as perfume (conﬁdence 70%). The attack is robust to camera orientation, distance, and ambient lighting. modality is the most realistic in physical systems.
Existing physical attacks include adding stickers on Stop signs that make models output Speed limit instead [14], colorful patterns on eyeglass frames to trick face recogni-tion [34], and 3D-printed objects with speciﬁc textures [6].
However, all existing works add artifacts to the object (such as sticker or color patterns) that are visible to a human. In this work, we generate adversarial perturbations on real-world objects that are invisible to human eyes, yet produce misclas-siﬁcations. Our approach exploits the differences between human and machine vision to hide adversarial patterns.
We show an invisible physical adversarial example in Fig. 1, generated by manipulating the light that shines on the object. The light creates adversarial patterns in the image that only a camera perceives. In particular, we show how an attacker can exploit the radiometric rolling shutter (RS) effect, a phenomenon that exists in rolling shutter cam-eras that perceive a scene whose illumination changes at a high frequency. Digital cameras use the rolling shutter technique to obtain high resolution images at higher rate and at a cheaper price [3, 27]. Rolling shutter technology is used in a majority of consumer-grade cameras, such as cellphones [19], AR glasses [32] and machine vision [1, 2]. the adversarially-illuminated object results in an image that contains multi-colored stripes. We contribute an algorithm for creating a
Due to the rolling shutter effect, 14666
time-varying high-frequency light pattern that can create such stripes. To the best of our knowledge, this is the ﬁrst demonstration of physical adversarial examples that exploit the radiometric rolling shutter effect, and thus, contributes to our evolving understanding of physical attacks on deep learning camera-based computer vision.
Similar to prior work on physical attacks, the main chal-lenge is obtaining robustness to dynamic environmental con-ditions such as viewpoint and lighting. However, in our setting, there are additional environmental conditions that pose challenges in creating these attacks. Speciﬁcally: (1)
Camera exposure settings inﬂuence how much of the rolling shutter effect is present, which affects the attacker’s ability to craft adversarial examples. — long exposures lead to less pronounced rolling shutter, providing less control. (2) The attacker’s light signal can be de-synchronized with respect to the camera shutter, thus causing the camera to capture the adversarial signal at different offsets causing the striping pattern to appear at different locations on the image, that can destroy its adversarial property. (3) The space of possible perturbations is limited compared to existing attacks. Unlike sticker attacks or 3D objects that can change the victim ob-ject’s texture, our attack only permits striped patterns that contain a limited set of translucent colors. (4) Difference in the light produced by RGB LEDs and the color perceived by camera sensor makes it harder to realize a physical signal.
To tackle the above challenges, we create a simulation framework that captures these environmental and camera imaging conditions. The simulation is based on a differen-tiable analytical model of image formation and light signal transmission and reception when the radiometric rolling shutter effect is present. Using the analytical model, we then formulate an optimization objective that we can solve using standard gradient-based methods to compute an adversarial light signal that is robust to these unique environmental and camera imaging conditions. We fabricate this light signal using programmable LEDs.
Although light-based adversarial examples are limited in the types of perturbation patterns compared to sticker-based ones, they have several advantages: (1) The attack is stealthier than sticker-based ones, as the attacker can simply turn the light source to a constant value to turn OFF the attack. (2) Unlike prior work using sticker or 3D printed object, the perturbation is not visible to human eyes. (3) The attack is dynamic and can change on-the-ﬂy — in a sticker-based attack, once the sticker has been placed, the attack effect cannot be changed unless the sticker is physically replaced. In our setting, the attacker can simply change the light signal and thus, change the adversarial effect.
We characterize this new style of invisible physical adver-sarial example using a state-of-the-art ResNet-101 classiﬁer trained using ImageNet [13]. We conduct physical testing of our attack algorithm under various viewpoints, ambient lighting conditions, and camera exposure settings. For exam-ple, for the coffee mug shown in Fig. 1 we obtain a targeted fooling rate of 84%under a variety of conditions. We ﬁnd that the attack success rate is dependent on the camera expo-sure setting: exposure rates shorter than 1/750s produce the most successful and robust attacks.
The main contributions of our work are the following:
• We develop techniques to modulate visible light that can illuminate an object to cause misclassiﬁcation on deep learning camera-based vision classiﬁers, while being com-pletely invisible to humans. Our work contributes to a new class of physical adversarial examples that exploit the differences between human and machine vision.
• We develop a differentiable analytical model of image formation under the radiometric rolling shutter effect and formulate an adversarial objective function that can be solved using standard gradient descent methods.
• We instantiate the attack in a physical setting and charac-terize this new class of attack by studying the effects of camera optics and environmental conditions, such as cam-era orientation, lighting condition, and exposure. Code is available at https://github.com/EarlMadSec/ invis-perturbations. 2.