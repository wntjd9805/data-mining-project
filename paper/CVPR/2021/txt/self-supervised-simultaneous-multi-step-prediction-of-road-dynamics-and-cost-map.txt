Abstract
While supervised learning is widely used for percep-tion modules in conventional autonomous driving solutions, scalability is hindered by the huge amount of data label-ing needed. In contrast, while end-to-end architectures do not require labeled data and are potentially more scalable, interpretability is sacriﬁced. We introduce a novel archi-tecture that is trained in a fully self-supervised fashion for simultaneous multi-step prediction of space-time cost map and road dynamics. Our solution replaces the manually designed cost function for motion planning with a learned high dimensional cost map that is naturally interpretable and allows diverse contextual information to be integrated without manual data labeling. Experiments on real world driving data show that our solution leads to lower number of collisions and road violations in long planning horizons in comparison to baselines, demonstrating the feasibility of fully self-supervised prediction without sacriﬁcing scalabil-ity. 1.

Introduction
Conventional autonomous driving (AD) stacks consist of various modules [30]. A perception component is responsi-ble for detecting objects in the scene and a prediction mod-ule for projecting their positions in the future. Based on their outputs a motion planner generates a desired trajectory according to a manually speciﬁed cost function [6], which is in turn executed by a controller. A key advantage of this approach is the interpretability of the ﬁnal decision. For example, in case of accident, each component can be inves-tigated individually. However, with different parts designed and tuned separately, each module is not aware of the errors made by the other parts. In many cases, there is no clear way for estimating the model uncertainty and propagating
*The authors contributed equally.
Figure 1. High Dimensional Cost Map Estimation. it to the system. In addition to massive amount of human-labelled data required to train the perception components, the manual design and tuning of the cost function for mo-tion planning tends to limit the system’s ability for dealing with complex driving scenarios .
As an alternative, several works [2, 7, 34, 5] proposed driving systems that use raw sensory input to directly pro-duce control commands (i.e., acceleration and steering).
This approach allows full backpropagation and eliminates the need for a cost function. Since a large quantity of data can be collected from cars equipped with appropriate sen-sors and directly used for training without human labelling, this approach is potentially highly scalable with data and compute. However, such a monolithic approach lacks in-ternally interpretable components, offers little insight as to how system faults may arise, and is thus ill-suited for safety-critical real-world deployment.
In this paper, we propose a new approach that allows meaningful interpretation and avoids manual data label-ing and design of cost function. Our approach is cen-tered around a novel architecture for learning-based space-time Cost Map Estimation (CME). The proposed method is highly scalable as it can be trained in a fully self-supervised fashion. Moreover, the space-time cost map has both a natural interface with motion planner and an interpretable domain-speciﬁc semantics.
Speciﬁcally, our architecture encodes high dimensional
Occupancy Grid Maps (OGM)s as well as other contextual information (e.g., drivable area and intersections) and si-8494
multaneously predicts the OGMs and estimate the cost map for multiple steps into the future. This leads to interpretable intermediate representations in the form of OGMs. A cost map (CM) is a grid map where the value of each cell repre-sents the cost of driving into that cell location. By extending the predicted CM multiple steps into the future, we arrive at a sequence of space-time CMs. These CMs can then be used by a motion planner to rank possible future trajectories through integrating the cost over the cells these trajectories occupy.
Importantly, while it is obvious that OGM prediction training can be made self-supervised using sequences of driving data (e.g., [16]) so long as occupancy estimation is accurate, labels for self-supervised training of CM esti-mation are much harder to synthesize. To solve this prob-lem, we decompose the CME objective into two parts. The
ﬁrst one injects the prior knowledge about the environment where it is available (e.g., occupied cells are high cost).
However, there is no explicit information about the cost of most of the cells. Hence, we propose using an auxiliary task to guide the training. Using auxiliary objectives for improv-ing the performance of a model in a primary task has been proven to be effective in different ﬁelds [13, 29, 3]. Simi-larly, in this work we deﬁne an auxiliary imitation task that forces the model to predict the expert’s intention and tra-jectory based on the estimated CMs. For this task a data-driven set of intentions capturing different modes of driving is used. This objective term pushes the model to ﬁll in the blanks and arrive at complete and systematically accurate predictions of the CM.
The main contributions of this paper are as follows:
• An architecture for estimating CMs simultaneously with OGM prediction from human driving data that is fully self-supervised and requires no extra data label-ing,
• A set of speciﬁc training objectives that combines en-vironment constraints, expert’s behavior, map infor-mation as well as solving auxiliary imitation tasks leading to estimating space-time cost maps,
• An empirical demonstration of the effectiveness of this design in the overall performance of an AD system through multiple experiments and generalization tests. 2.