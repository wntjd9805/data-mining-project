Abstract
Audio-visual speech separation
We introduce a new approach for audio-visual speech separation. Given a video, the goal is to extract the speech associated with a face in spite of simultaneous back-ground sounds and/or other human speakers. Whereas ex-isting methods focus on learning the alignment between the speaker’s lip movements and the sounds they gener-ate, we propose to leverage the speaker’s face appearance as an additional prior to isolate the corresponding vocal qualities they are likely to produce. Our approach jointly learns audio-visual speech separation and cross-modal speaker embeddings from unlabeled video. It yields state-of-the-art results on ﬁve benchmark datasets for audio-visual speech separation and enhancement, and general-izes well to challenging real-world videos of diverse sce-narios. Our video results and code: http://vision. cs.utexas.edu/projects/VisualVoice/. 1.

Introduction
Human speech is rarely observed in a vacuum. Amidst the noisy din of a restaurant, we concentrate to parse the words of our dining partner; watching a heated presiden-tial debate, we disentangle the words of the candidates as they talk over one another; on a Zoom call we listen to a colleague while our children chatter and play a few yards away. Presented with such corrupted and entangled sounds, the human perceptual system draws heavily on visual infor-mation to reduce ambiguities in the audio [62] and modulate attention on an active speaker in a busy environment [29].
Automating this process of speech separation has many valuable applications, including assistive technology for the hearing impaired, superhuman hearing in a wearable aug-mented reality device, or better transcription of spoken con-tent in noisy in-the-wild Internet videos.
While early work in automatic speech separation relied solely on the audio stream [53, 78, 18], recent work explores ways to leverage its close connections to the visual stream as well [21, 2, 19, 55, 15]. By analyzing the facial motion in concert with the emitted speech, these methods steer the
Distinctive voice tracks aid embedding learning
Vocal and facial prior aids separation
Pull
Push
Cross-modal embedding space
Cross-modal face-to-voice matching
Figure 1: We propose a multi-task learning framework to jointly learn audio-visual speech separation and cross-modal face-voice embeddings. Our approach leverages the complementary cues between lip movements and cross-modal speaker embeddings for speech separation. The em-beddings serve as a prior for the voice characteristics that enhances speech separation; the cleaner separated speech in turn produces more distinctive audio embeddings. audio separation module towards the relevant portions of the sound that ought to be separated out from the full audio track. For example, the mouth articulates in different shapes consistent with the phonemes produced in the audio, mak-ing it possible to mask a spectrogram for the target human speaker based on audio-visual (AV) consistency. However, solely relying on lip movements can fail when lip motion becomes unreliable, e.g., the mouth region is occluded by the microphone or the speaker turns their head away.
While AV synchronization cues are powerful, we ob-serve that the consistency between the speaker’s facial ap-pearance and their voice is also revealing for speech sep-aration. Intuitively, attributes like gender, age, nationality, and body weight are often visible in the face and give a prior for what sound qualities (tone, pitch, timbre, basis of articulation) to listen for when trying to separate that per-son’s speech from interfering sounds. For example, female speakers often register in higher frequencies, a heavier per-son may exhibit a wider range of sound intensities [9], and an American speaker may sound more nasal. The face-voice 15495
association, supported by cognitive science studies [11], is today often leveraged for speaker identiﬁcation given the recording of a single speaker [50, 49, 39, 74].
In con-trast, the speech separation problem demands discovering a cross-modal association in the presence of multiple over-lapping sounds.
Our key insight is that these two tasks—cross-modal matching and speech separation—are mutually beneﬁcial.
The cleaner the sound separation, the more accurately an embedding can link the voice to a face; the better that em-bedding, the more distinctive is the prior for the voice char-acteristics which will in turn aid separation. We thus aim to
“visualize” the voice of a person based on how they look to better separate that voice’s sound. See Figure 1.
To this end, we propose VISUALVOICE, a multi-task learning framework to jointly learn audio-visual speech sep-aration together with cross-modal speaker embeddings. We introduce a speech separation network that takes video of a human speaker talking in the presence of other sounds (speech or otherwise) and returns the isolated sound track for just their speech. Our network relies on facial ap-pearance, lip motion, and vocal audio to solve the separa-tion task, augmenting the conventional “mix-and-separate” paradigm for audio-visual separation to account for a cross-modal contrastive loss requiring the separated voice to agree with the face. Notably, our approach requires no identity la-bels and no enrollment of speakers, meaning we can train and test with fully unlabeled video.
Our main contributions are as follows. Firstly, we in-troduce an audio-visual speech separation framework that leverages complementary cues from facial motion and cross-modal face-voice attributes. Secondly, we devise a novel multi-task framework that successfully learns both separation and cross-modal embeddings in concert. Finally, through experiments on 5 benchmark datasets, we demon-strate state-of-the-art results for audio-visual speech separa-tion and enhancement in challenging scenarios. The embed-ding learned by our model additionally improves the state of the art for unsupervised cross-modal speaker veriﬁcation, emphasizing the yet-unexplored synergy of the two tasks. 2.