Abstract
We introduce a novel deep neural network to estimate a depth map from a single monocular indoor panorama. The network directly works on the equirectangular projection, exploiting the properties of indoor 360◦ images. Starting from the fact that gravity plays an important role in the design and construction of man-made indoor scenes, we propose a compact representation of the scene into vertical slices of the sphere, and we exploit long- and short-term relationships among slices to recover the equirectangular depth map. Our design makes it possible to maintain high-resolution information in the extracted features even with a deep network. The experimental results demonstrate that our method outperforms current state-of-the-art solutions in prediction accuracy, particularly for real-world data. 1.

Introduction
Understanding the 3D layout of an indoor scene from images is a crucial task in many domains [45, 23, 24]. Fast depth estimation from single images is a fundamental sub-problem, as associating metric information to visual data is paramount for a variety of applications, including mobile
Augmented Reality platforms, indoor mapping, autonomous navigation, 3D reconstruction, and scene understanding.
Since estimation of depth from single images is inherently ambiguous, all solutions must rely on prior information to guide reconstruction towards plausible architectural shapes that ﬁt the input. In this context, we have recently seen an extraordinary development of data-driven methods that learn these priors from example data.
Early approaches were designed for a camera with a con-ventional limited ﬁeld-of-view (FoV) (e.g., FCRN[14]). In recent years, however, 360◦ capture has emerged as a very appealing solution, since it provides the quickest and most complete single-image coverage and is supported by a wide variety of professional and consumer capture devices that make acquisition fast and cost-effective [37]. Since adapt-ing monocular depth estimation models designed for tradi-tional images to 360◦ depth estimation has been shown to produce sub-optimal results [44], speciﬁc 360◦ solutions have been recently introduced. In this context, many recent works [31, 44, 17] have adapted perspective depth estima-tion methods to omnidirectional imagery by proposing vari-ous types of distortion-aware convolution ﬁlters. However, few of them have explored the large-FoV nature provided by 360◦ images, which can provide, in one shot, the full-geometric context of an indoor scene [41].
In this work, we introduce a novel deep neural network solution, called SliceNet, which predicts the depth map of an indoor 360◦ image leveraging the characteristics of a gravity-aligned equirectangular projection of an interior scene. Since gravity plays an important role in the design and construction of interior environments, world-space vertical and horizon-tal features have different characteristics in most, if not all, man-made environments. Our network design starts from the assumption that capture of the scene through an equirect-angular image is aligned to the gravity vector (i.e., camera is placed on an horizontal-ground plane), too, and, thus, it is rational to assume that gravity-aligned processing of images can directly exploit gravity-aligned world-space features [3].
In our network, an input equirectangular image is partitioned into vertical slices by performing a contractive encoding to reduce the input tensor only along the vertical direction, re-sulting in a compact and ﬂattened sequence of slices made of a set of features. To preserve global information, we perform slicing over four different resolution levels, concatenating the result at the end (Sec. 3). This sequential representation enables the use of a convolutional long short-term memory (LSTM) network [26] to recover, with low computational 11536
overhead, long- and short-term spatial relationships among slices. Decoding proceeds symmetrically with respect to en-coding, thereby increasing only the vertical resolution of the feature map, until the target resolution is reached (Fig. 1(a)).
Our contributions are summarized as follows:
• We introduce a slice-based representation of an omni-directional image that directly exploits the character-istics of the equirectangular projection of an indoor scene, without the need for distortion-aware convolu-tion and transformation [44, 33], multi-branch architec-tures [33, 11] or additional information and priors [11].
Our representation based on vertical slices is very ro-bust, as demonstrated by the important advantage in performance achieved in real-world cases (e.g., Stan-ford2D3D [27] and Matterport3D [19]), where a large area around the poles of the panorama is not acquired by the instrument (see Sec. 5.2 for details).
• We specialize and reﬁne feature ﬂattening, which has proven to be effective to regress one-dimensional ten-sors [30], for bi-dimensional depth encoding. In par-ticular, we introduce an asymmetric contraction of the input tensor based on vertical slicing at different res-olutions, so that the resulting feature map is ﬂattened along a single direction (in our case, the sphere horizon), and we merge slices at different resolutions, so as to ex-ploit deeper levels with larger receptive ﬁelds to capture global information, while at the same time exploiting higher resolution layers to preserve high-frequency de-tails (Sec. 3). Our ablation study (Sec. 5.3) demonstrates the advantages of our approach.
• We introduce, for depth estimation from a single image, a LSTM multi-layer module to effectively recover long and short term spatial relationships between slices in the presence of a large number of features per slice due to the concatenation of multiscale representations. With this architectural choice, the decoder is simple and fol-lows the same multi-layer scheme of the encoder with a vertical upsampling rather than a vertical reduction. We do not need, in particular, the chaining of up-projection blocks [10], making it easier to scale the method to dif-ferent input resolutions. The ablation study (Sec. 5.3) conﬁrms the beneﬁts of the method by comparing differ-ent decoder conﬁgurations with or without LSTM and chaining up-projection blocks.
We tested our network on both synthetic and real datasets [27, 19, 44, 43, 42]. Our experimental results (Sec. 5) demon-strate that our method outperforms current state-of-the-art methods [14, 44, 33] in prediction accuracy, especially when working on real-world scenes. Exploiting gravity alignment leads to an efﬁcient network structure, without signiﬁcant limitations on the applicability of the approach. As men-tioned, gravity-aligned capture is a very common setup, and, as determined by our tests, Sec. 5.3, all the public 3D in-door datasets commonly used for training and testing recon-struction solutions, both synthetic [43, 42] and real [27, 19], appear to have very small orientation deviations. Even in cases where this assumption is not veriﬁed at capture time, several orthogonal solutions exist to gravity-rectify images in a pre-processing step (e.g., [34, 12, 3]), simplifying the practical application of gravity-oriented methods. Moreover, as demonstrated by our ablation study (Sec. 5.3), our method is robust to small variations of the inclination. 2.