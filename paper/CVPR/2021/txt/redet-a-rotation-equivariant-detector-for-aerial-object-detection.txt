Abstract
Recently, object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. Therefore, the detector requires more parameters to encode the orientation information, which are often highly redundant and inefﬁcient. Moreover, as or-dinary CNNs do not explicitly model the orientation varia-tion, large amounts of rotation augmented data is needed to train an accurate object detector.
In this paper, we propose a Rotation-equivariant Detector (ReDet) to ad-dress these issues, which explicitly encodes rotation equiv-ariance and rotation invariance. More precisely, we in-corporate rotation-equivariant networks into the detector to extract rotation-equivariant features, which can accu-rately predict the orientation and lead to a huge reduc-tion of model size. Based on the rotation-equivariant fea-tures, we also present Rotation-invariant RoI Align (RiRoI
Align), which adaptively extracts rotation-invariant features from equivariant features according to the orientation of
RoI. Extensive experiments on several challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that our method can achieve state-of-the-art perfor-mance on the task of aerial object detection. Compared with previous best results, our ReDet gains 1.2, 3.5 and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 re-spectively while reducing the number of parameters by 60% (313 Mb vs. 121 Mb). The code is available at: https:
//github.com/csuhan/ReDet. 1.

Introduction
This paper studies the problem of object detection in aerial images, a recently-emerged challenging problem in
The study of this paper is funded by the National Natural Sci-ence Foundation of China (NSFC) under grant contracts No.61922065,
No.61771350 and No.41820104006 and 61871299. It is also supported by Supercomputing Center of Wuhan University.
∗Equal contribution.
Corresponding author: Gui-Song Xia (guisong.xia@whu.edu.cn).
Figure 1. Illustration of our method (top) and comparisons of
RRoI warping (bottom). CNN features are not equivariant to the rotation Tr, i.e., feeding a rotated image to CNNs is not the same as rotating feature maps of the original image. Therefore,
In the corresponding RoI features are not invariant to rotation. contrast, our method adopts rotation-equivariant CNNs (ReCNN) to extract rotation-equivariant features. Let I and Φ be the input and ReCNN respectively, the equivariance of our method can be expressed as: Φ(TrI) = TrΦ(I), i.e., applying a rotation Tr to the image I is the same as the rotation of features. Since we have obtained rotation-equivariant features, rotation-invariant features can be extracted by RRoI warping. While RRoI Align can only achieve rotation invariance in the spatial dimension, we present a novel Rotation-invariant RoI (RiRoI) Align to extract rotation-invariant features in both spatial and orientation dimensions. computer vision [35]. Different from objects in nature im-ages, objects in aerial images are often distributed with ar-bitrary orientation. To cope with these challenges, aerial object detection are usually formulated as an oriented ob-ject detection task by relying on Oriented Bounding Boxes (OBBs) representation instead of using Horizontal Bound-ing Boxes (HBBs) [7, 35, 38, 40].
Recently, many well-designed oriented object detectors have been proposed and reported promising results on chal-lenging aerial image datasets [21, 35]. In order to achieve accurate object detection in unconstrained aerial images, most of them are devoted to extract rotation-invariant fea-tures [7, 10, 22, 37]. In practice, Rotated RoI (RRoI) warp-2786
ing (e.g., RRoI Pooling [22] and RRoI Align [7]) is the most commonly used method to extract rotation-invariant fea-tures, which can warp region features precisely according to the bounding boxes of RRoI in the 2D planar. However,
RRoI warping with regular CNN features can not produce exactly rotation-invariant features. The rotation invariance is approximated by employing larger capacity networks and more training samples to model the rotation variation. As shown in Fig. 1, the regular CNNs are not equivariant to the rotation, i.e., feeding a rotated image to CNNs is not the same as rotating feature maps of the original image.
Therefore, region features warped from regular CNN fea-ture maps are usually unstable and delicate as the orienta-tion changes.
Some recently proposed methods [5, 13, 33] extend
CNNs to larger groups and achieve rotation equivariance1 with group convolutions [5]. Feature maps of these meth-ods have additional orientation channels recording features from different orientations. However, directly applying the ordinary RRoI warping to rotation-equivariant features is unable to produce rotation-invariant features, as it can only warp region features in the 2D planar, i.e., the spatial dimen-sion, while the orientation channels are still misaligned. To extract completely rotation-invariant features, we also need to adjust the orientation dimension of feature maps accord-ing to the orientation of RRoI.
In this paper, we propose a Rotation-equivariant Detec-tor (ReDet) to extract completely rotation-invariant features from rotation-equivariant features. As shown in Fig. 1, our method consists of two parts: rotation-equivariant feature extraction and rotation-invariant feature extraction. Firstly, we incorporate rotation-equivariant networks into the back-bone to produce rotation-equivariant features, which can ac-curately predict the orientation and reduce the complexity of modeling orientation variations. Since directly apply the
RRoI warping still cannot extract rotation-invariant features from the rotation-equivariant features, we propose a novel
Rotation-invariant RoI Align (RiRoI Align). It can warp re-gion features according to the bounding boxes of RRoI in the spatial dimension and align features in the orientation dimension by circularly switching orientation channels and feature interpolation. Finally, the combination of rotation-equivariant backbone and RiRoI Align forms our ReDet to extract completely rotation-invariant features for accurate aerial object detection.
Extensive experiments performed on the challenging aerial image datasets DOTA [35] and HRSC2016 [21] demonstrate the effectiveness of our method. We summary our contributions as: (1) We propose a Rotation-equivariant
Detector for high-quality aerial object detection, which en-codes both rotation equivariance and rotation invariance. To 1Equivariance is a property that applying transformations to the input produces transformations of the feature in a predictable way.
Figure 2. Model size vs. accuracy (mAP) on DOTA-v1.5. We evaluate RetinaNet OBB [18], Faster R-CNN OBB (FR) [27],
Mask R-CNN (Mask) [11] and Hybrid Task Cascade (HTC) [2] with ResNet18 (R18) and ResNet50 (R50) backbones. Note all al-gorithms are our re-implemented version for DOTA, which is con-sistent with Tab. 7. Our ReDet is tested with ReResNet18 (ReR18) and ReResNet50 (ReR50) backbones. Compared with other meth-ods with R18/R50 backbones, our ReDet with a ReR18 back-bone achieves competitive performance. Using a deeper backbone (ReR50), our ReDet outperforms all methods by a large margin and achieves better model size vs. accuracy trade-off. our best knowledge, it is the ﬁrst time that rotation equiv-ariance has been systematically introduced into oriented (2) We design a novel RiRoI Align to object detection. extract rotation-invariant features from rotation-equivariant features. Different from other RRoI warping methods,
RiRoI Align produces completely rotation-invariant fea-(3) Our tures in both spatial and orientation dimensions. method achieves the state-of-the-art 80.10, 76.80 and 90.46 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016, respec-tively. Compared with previous best results, our method gains 1.2, 3.5 and 2.6 mAP improvements. Compared with the baseline, our method shows consistent and substantial improvements and reduces the number of parameters by 60% (313 Mb vs. 121 Mb). Moreover, our method achieves better model size vs. accuracy trade-off (shown in Fig. 2). 2.