Abstract
This paper proposes an image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles. Different from previous image-to-image translation methods that formulate the translation as pixel-wise prediction, we deal with such an artistic cre-ation process in a vectorized environment and produce a se-quence of physically meaningful stroke parameters that can be further used for rendering. Since a typical vector ren-der is not differentiable, we design a novel neural renderer which imitates the behavior of the vector renderer and then frame the stroke prediction as a parameter searching pro-cess that maximizes the similarity between the input and the rendering output. We explored the zero-gradient problem on parameter searching and propose to solve this problem from an optimal transportation perspective. We also show that previous neural renderers have a parameter coupling problem and we re-design the rendering network with a ras-terization network and a shading network that better han-dles the disentanglement of shape and color. Experiments show that the paintings generated by our method have a high degree of ﬁdelity in both global appearance and lo-cal textures. Our method can be also jointly optimized with neural style transfer that further transfers visual style from other images. Our code and animated results are available at https:// jiupinjia.github.io/ neuralpainter/ . 1.

Introduction
Creating artistic paintings is one of the deﬁning charac-teristics of humans and other intelligent species. In recent years, we saw great advancements in generative modeling of image translation or style transfer which utilizes neu-ral network as a generative tool [6, 13, 23, 38]. Previous image-to-image translation and style transfer methods typ-15689
ically formulate the translation either as a pixel-wise map-ping [13, 38] or a continuous optimization process in their pixel space [6]. However, as an artistic creation process, the paintings usually proceed as a sequentially instantiated process that creates using brushes, from abstract to con-crete, and from macro to detail. This process is fundamen-tally different from how neural networks create artwork that produces pixel-by-pixel results. To fully master the profes-sional painting skills, people usually need a lot of practice and learn domain expertise. Even for a skilled painter with years of practice, it could still take hours or days to create a realistic painting artwork.
In this paper, we explore the secret nature of human painting and propose an automatic image-to-painting trans-lation method that generates vivid and realistic paintings with controllable styles. We refer to our method as “Styl-ized Neural Painter”. Instead of manipulating each of the pixels in the output image, we simulate human painting be-havior and generate vectorized strokes sequentially with a clear physical signiﬁcance. Those generated stroke vectors can be further used for rendering with arbitrary output res-olution. Our method can “draw” in a variety of painting styles, e.g. oil-painting brush, watercolor ink, marker-pen, and tape art. Besides, our method can also be naturally embedded in a neural style transfer framework and can be jointly optimized to transfer its visual style based on differ-ent style reference images.
In our method, different from the previous stroke-based rendering methods that utilize step-wise greed search [8, 19], recurrent neural network [7], or reinforcement learn-ing [5, 11, 33, 36], we reformulate the stroke prediction as a “parameter searching” process that aims to maximize the similarity between the input and the rendering output in a self-supervised manner. Considering that a typical graphic renderer is not differentiable, we take advantage of the neural rendering that imitates the behavior of the graphic rendering and make all components in our method differentiable. We show that previous neural stroke render-ers [11, 29, 30] may suffer from the parameter coupling problem when facing complex rendering scenarios, e.g., brushes with real-world textures and color-transition. We, therefore, re-design the neural renderer and decomposed the rendering architecture into a rasterization network and a shading network, which can be jointly trained and ren-dered with much better shape and color ﬁdelity. We also found interestingly that the pixel-wise similarity like ℓ1 or
ℓ2 pixel loss, may have an intrinsic ﬂaw of zero-gradient on optimizing over the vectorized parameters, although these losses have been widely used in a variety of image trans-lation tasks [13, 17, 38]. We show that this problem lies in the different nature of stroke parameterization and ras-terization, and propose to solve this problem from the per-spective of optimal transportation. Speciﬁcally, we consider the movement of a stroke from one location to another as a transportation process, where we aim to minimize the ef-forts of that movement.
We test our method on various real-world images and photos, including human portraits, animals, scenery, daily objects, art photography, and cartoon images. We show that our method can generate vivid paintings with a high degree of realism and artistic sense in terms of both global visual appearance and local texture ﬁdelity.
The contribution of our paper is summarized as follows:
• We propose a new method for stroke based image-to-painting translation. We re-frame the stroke prediction as a parameter searching processing. Our method can be jointly optimized with neural style transfer in the same framework.
• We explore the zero-gradient problem on parameter searching and view the stroke optimization from an optimal transport perspective. We introduce a differ-entiable transportation loss and improves stroke con-vergence as well as the painting results.
• We design a new neural renderer architecture with a dual-pathway rendering pipeline (rasterization + shad-ing). The proposed renderer better deals with the dis-entanglement of the shape and color and outperforms previous neural renderers with a large margin. 2.