Abstract 1.

Introduction
Neural networks have shown great abilities in estimat-ing depth from a single image. However, the inferred depth maps are well below one-megapixel resolution and often lack ﬁne-grained details, which limits their practical-ity. Our method builds on our analysis on how the input resolution and the scene structure affects depth estimation performance. We demonstrate that there is a trade-off be-tween a consistent scene structure and the high-frequency details, and merge low- and high-resolution estimations to take advantage of this duality using a simple depth merg-ing network. We present a double estimation method that improves the whole-image depth estimation and a patch se-lection method that adds local details to the ﬁnal result.
We demonstrate that by merging estimations at different resolutions with changing context, we can generate multi-megapixel depth maps with a high level of detail using a pre-trained model. (∗) denotes equal contribution.
Monocular or single-image depth estimation aims to ex-tract the structure of the scene from a single image. Un-like in settings where raw depth information is available from depth sensors or multi-view data with geometric con-straints, monocular depth estimation has to rely on high-level monocular depth cues such as occlusion boundaries and perspective. Data-driven techniques based on deep neu-ral networks have thus become the standard solutions in modern monocular depth estimation methods [11, 13, 14, 15, 29]. Despite recent developments in the ﬁeld including in network design [12, 18, 21, 33], incorporation of high-level constraints [43, 50, 56, 58], and supervision strate-gies [14, 15, 16, 20, 23], achieving high-resolution depth es-timates with good boundary accuracy and a consistent scene structure remains a challenge. State-of-the-art methods are based on fully-convolutional architectures which in princi-ple can handle inputs of arbitrary sizes. However, practi-cal constraints such as available GPU memory, lack of di-verse high-resolution datasets, and the receptive ﬁeld size of CNN’s limit the potential of current methods. 9685
Figure 2: The pipeline of our method: (b) We ﬁrst start with feeding the image in low- and high-resolution to the network, here shown results with MiDaS [35], and merge them to get a base estimate with a consistent structure with good boundary localization. (c) We then determine different patches in the image. We show a subset of selected patches with their depth estimates. (d) We merge the patch estimates onto our base estimate from (b) to get our ﬁnal high-resolution result.
We present a method that utilizes a pre-trained monocu-lar depth estimation model to achieve high-resolution re-sults with high boundary accuracy. Our main insight comes from the observation that the output characteristics of monocular depth estimation networks change with the resolution of the input image. In low resolutions close to the training resolution, the estimations have a consistent structure while lacking high-frequency details. When the same image is fed to the network in higher resolutions, the high-frequency details are captured much better while the structural consistency of the estimate gradually degrades.
We claim following our analysis in Section 3 that this du-ality stems from the limits in the capacity and the receptive
ﬁeld size of a given model. We propose a double-estimation framework that merges two depth estimations for the same image at different resolutions adaptive to the image content to generate a result with high-frequency details while main-taining the structural consistency.
Our second observation is on the relationship between the output characteristics and the amount and distribution of high-level depth cues in the input. We demonstrate that the models start generating structurally inconsistent results when the depth cues are further apart than the receptive ﬁeld size. This means that the right resolution to input the image to the network changes locally from region to region. We make use of this observation by selecting patches from the input image and feeding them to the model in resolutions adaptive to the local depth cue density. We merge these esti-mates onto a structurally consistent base estimate to achieve a highly detailed high-resolution depth estimation.
By exploiting the characteristics of monocular depth es-timation models, we achieve results that exceed the state-of-the-art in terms of resolution and boundary accuracy with-out retraining the original networks. We present our results and analysis using two state-of-the-art monocular depth es-timation methods [35, 48]. Our double-estimation frame-work alone improves the performance considerably without too much computational overhead while our full pipeline shown in Figure 2 can generate highly detailed results even for very complex scenes as Figure 1 demonstrates. 2.