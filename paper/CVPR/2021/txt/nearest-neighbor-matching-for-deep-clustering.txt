Abstract
Deep clustering gradually becomes an important branch in unsupervised learning methods. However, current ap-proaches hardly take into consideration the semantic sam-ple relationships that existed in both local and global fea-tures. In addition, since the deep features are updated on-the-ﬂy, relying on these sample relationships may construct more semantically conﬁdent sample pairs, leading to infe-rior performance. To tackle this issue, we propose a method called Nearest Neighbor Matching (NNM) to match samples with their nearest neighbors from both local (batch) and global (overall) levels. Speciﬁcally, for the local level, we match the nearest neighbors based on batch embedded fea-tures, as for the global one, we match neighbors from over-all embedded features. To keep the clustering assignment consistent in both neighbors and classes, we frame consis-tent loss and class contrastive loss for both local and global levels. Experimental results on three benchmark datasets demonstrate the superiority of our new model against state-of-the-art methods. Particularly on the STL-10 dataset, our method can achieve supervised performance. As for the CIFAR-100 dataset, our NNM leads 3.7% against the latest comparison method. Our code will be available at https://github.com/ZhiyuanDang/NNM . 1.

Introduction
Unsupervised learning approach becomes emerging re-cently since the expensive label acquisition. As an impor-tant branch of unsupervised learning, clustering methods have attracted more attention, which goal is that grouping the samples into clusters, such that similar samples into the same cluster while dissimilar ones into different clusters.
Traditional clustering methods, such as K-Means [28],
Spectral Clustering [43], Nonnegative Matrix Factorization
[2], have been widely applied in various tasks. However,
∗Corresponding author.
Figure 1. The illustration of our idea. We propose to match more semantically nearest neighbors from between local (batch) and global (overall) level. Beneﬁt from the dynamic updated deep fea-tures with iteration and epoch increases, we can construct more and more semantically conﬁdent sample pairs from samples and its neighbors. these methods only focus on low-level information, leading to their suboptimal performance. With the aid of the im-pressive deep learning methods [26], deep clustering meth-ods are proposed to non-linearly transform the original sam-ples into a latent embedded space for successfully pro-viding more effective features and promising performance.
Although conducting cluster analysis with learnable deep learning representations shows the potential to beneﬁt clus-tering on such unlabelled data, how to improve the semantic conﬁdence of these clusters remains an open question.
There are some methods proposed to solve it. Accord-ing to the data training strategy, current deep clustering ap-proaches could be roughly divided into two categories: The
ﬁrst one (such as DEC [38], JULE [39], DAC [5], Deep-Cluster [3], DDC [4], DCCM [36]) usually iteratively eval-uate the clustering assignment from the up-to-date model and supervise the network training processes by the esti-mated information; The second one (such as ADC [14], IIC
[21], PICA [20], DCDC[10]) simultaneously learn both the 13693
feature representation and clustering assignment at the same time without explicit phases of clustering. Different from these previous studies, SCAN [34] is proposed to do se-mantic clustering with mining nearest neighbors. However, these approaches hardly take into consideration the seman-tic sample relationships existed in both local and global fea-tures. In addition, since the deep features are updated on-the-ﬂy, relying on these sample relationships may construct more semantically conﬁdent sample pairs, leading to infe-rior performance.
As shown in Fig. 1, with the intuition that fully adopting the rich sample relationships existed in both local and global features, we plan to search the nearest neighbors from both these features. Note that we can easily construct semantic sample pairs from samples and its nearest neighbors. There-fore, in this paper, we propose a method named Nearest
Neighbors Matching (NNM) for deep clustering. Specif-ically, for the local level, we match the nearest neighbors based on batch embedded features, as for the global, we match the neighbors from overall embedded features. To keep the clustering assignment consistent in both neighbors and classes, we adopt consistent loss and class contrastive loss for both local and global levels. Beneﬁt from this novel
NNM loss, our method can obtain more semantic clustering assignments. Signiﬁcantly, our NNM is a plug-in module that can be applied in any works to obtain more semantic feature representations.
Our major contributions can be summarized as follows:
• We propose a novel deep clustering framework called
NNM that is based on two-level nearest neighbors matching. Different from previous methods, NNM matches the nearest neighbors from local and global levels, and thus further improve clustering perfor-mance. In addition, our NNM is a plug-in module that can be adopted in any network to learn a more seman-tic feature representation.
• We provide the confusion matrices of three widely used benchmark datasets, after optimizing NNM loss.
As shown in Fig. 4, desired confusion matrices should be block-diagonal. The most conﬁdent samples are shown together with the name of the matched ground-truth classes in Fig. 5. Besides that, we also con-duct more ablation studies about numbers of cluster-ing heads and nearest neighbors to ﬁgure out the useful techniques for current deep clustering.
• Extensive experimental results on three benchmark datasets demonstrate the superiority of our NNM against other state-of-the-art methods. Particularly on the STL-10 dataset, our method can achieve super-vised performance. As for the CIFAR-100 dataset, our NNM leads 3.7% against the latest comparison method. 2.