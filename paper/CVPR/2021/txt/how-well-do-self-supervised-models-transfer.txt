Abstract 1.

Introduction
Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recog-nition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform su-pervision, conﬁrming the recently observed trend in the lit-erature. We ﬁnd ImageNet Top-1 accuracy to be highly cor-related with transfer to many-shot recognition, but increas-ingly less so for few-shot, object detection and dense predic-tion. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learn-ers fail to preserve colour information as well as supervised alternatives, but tend to induce better classiﬁer calibration, and less attentive overﬁtting than supervised learners.
Computer vision in the last decade has been driven by increasingly sophisticated convolutional neural networks (CNNs) and the increasingly large datasets used to train them. Nevertheless, progress in this paradigm is ultimately bottlenecked by the data annotation process. This has moti-vated a growing wave of research in self-supervised repre-sentation learning, where CNN representations are trained on pretext tasks with freely available labels. Once trained, these CNN representations can be used to learn new tasks more data efﬁciently through feature re-use or ﬁnetuning.
Self-supervised learning (SSL) has been around for some time [47], but historically has lagged behind state of the art supervised representation learning. However, the recent pace of progress has increased dramatically and led to self-supervised deep representations that appear to approach and possibly even surpass that of fully-supervised representa-tions [17, 5]. This has raised hopes that self-supervised methods could indeed replace the ubiquitous annotation-intensive paradigm of supervised deep learning in state of the art computer vision going forward. 5414
importance of
Given the growing practical self-supervised learning as it approaches state of the art in com-puter vision tasks, there is increasing interest in understand-ing and benchmarking its empirical performance. Major re-cent evaluation studies have looked at aspects such as the ﬁt between CNN architectures and choice of pretext task [27] and the impact of the pre-training set size and CNN capacity on downstream task performance [16].
Despite this initial progress, there are a number of impor-tant open questions that remain to to be understood. Firstly, given the plethora of self-supervised representations on the market using diverse pre-text tasks and data-augmentations: which methods are the most empirically effective? This is currently hard to assess given the limited commonality in the evaluation conditions reported by each method. Sec-ondly: While the most widely adopted benchmark metric is image classiﬁcation performance, there are hopes that pre-trained representations will generalise to other downstream tasks such as detection and dense prediction [16]. How-ever, the published self-supervision literature is particularly inconsistent with regard to benchmarking these alternative tasks, making it impossible to determine the most effective
In particular, while we hope that the methods methods. with best performance on the most popular benchmark of
ImageNet recognition will also perform well on alternative tasks, this conjecture has never been systematically tested empirically. Thirdly: While core academic vision research is happy to focus on ImageNet as a benchmark, the wider community of computer vision practitioners work with di-verse data types from medical [54] to agricultural [40], to earth-observation [24] data and beyond. From this per-spective a crucial question is to what extent self-supervised features pre-trained on ImageNet can generalise directly to these diverse downstream tasks? This is important to know practically, because it dictates whether users in dif-ferent vision domains can use pre-trained features directly, or whether they would need to collect their own datasets and perform domain-speciﬁc self-supervised learning – a major data, compute and environmental [48] hurdle given that state of the art methods can take around 20 GPU days to train [8]. Academically, this is also important to know, as an indicator of whether pursuing higher ImageNet accuracy in self-supervised learning research leads to higher accu-racy on diverse real-world vision tasks, or is our research overﬁtting to ImageNet recognition?
To answer these questions and more, we conduct a large empirical benchmarking study on the efﬁcacy of different pre-trained representations for diverse downstream tasks. In particular, we evaluate 13 pre-trained self-supervised mod-els on 40 transfer tasks covering many-shot and few-shot image classiﬁcation, object detection, surface normal pre-diction and semantic segmentation, as summarised in Fig. 1.
Our downstream tasks cover diverse datasets with a wide range of similarity to the source ImageNet data, which all our models were pre-trained on.
Among other questions, we aim to answer the following:
Q1. How do state of the art self-supervised methods com-pare to supervised feature learning for diverse downstream datasets and tasks? A: The best self-supervised methods can match and outperform supervised representation learn-ing across most tasks considered. Only in few-shot recog-nition with small domain shift to ImageNet does supervised representation learning win.
Q2. Do self-supervised representations that perform well on ImageNet classiﬁcation systematically perform well on diverse downstream datasets and tasks? A: For recognition on datasets similar to ImageNet, performance is highly cor-related. However, for some of the least similar recognition datasets such as ISIC2018, there is little to no correlation with ImageNet performance. For different tasks such as de-tection and dense prediction, correlation exists but is lower than for recognition.
Q3.
Is there a best self-supervised representation over-all? A: No. For example, the recent methods SwAV and
DeepCluster-v2 work well for recognition on ImageNet-like data, but under-perform on non-recognition tasks and on different data such as medical skin images. This suggests that the vision of a universal pre-trained model suited for all downstream tasks is yet to be realised.
Q4. Do self-supervised and supervised features represent the same information? A: Contemporary self-supervised features seem to discard colour information, presumably due to the data augmentation they use. They also tend to be more attentively diffuse in contrast to the high spatial focus of attention in supervised features, which may contribute to their improved uncertainty calibration. 2.