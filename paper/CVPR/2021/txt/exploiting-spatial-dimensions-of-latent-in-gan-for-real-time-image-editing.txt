Abstract
Generative adversarial networks (GANs) synthesize re-alistic images from random latent vectors. Although ma-nipulating the latent vectors controls the synthesized out-puts, editing real images with GANs suffers from i) time-consuming optimization for projecting real images to the latent vectors, ii) or inaccurate embedding through an en-coder. We propose StyleMapGAN: the intermediate latent space has spatial dimensions, and a spatially variant mod-ulation replaces AdaIN. It makes the embedding through an encoder more accurate than existing optimization-based methods while maintaining the properties of GANs. Exper-imental results demonstrate that our method signiﬁcantly outperforms state-of-the-art models in various image ma-nipulation tasks such as local editing and image interpo-lation. Last but not least, conventional editing methods on
GANs are still valid on our StyleMapGAN. Source code is available at https://github.com/naver-ai/
StyleMapGAN . 1.

Introduction
Generative adversarial networks (GANs)
[16] have evolved dramatically in recent years, enabling high-ﬁdelity image synthesis with models that are learned directly from data [6, 24, 25]. Recent studies have shown that GANs naturally learn to encode rich semantics within the la-tent space, thus changing the latent code leads to ma-nipulating the corresponding attributes of the output im-ages [22, 42, 17, 15, 43, 3, 50, 5]. However, it is still chal-lenging to apply these manipulations to real images since the GAN lacks an inverse mapping from an image back to its corresponding latent code.
One promising approach for manipulating real images is image-to-image translation [21, 57, 9, 26, 28], where the model learns to synthesize an output image given a user’s input directly. However, these methods require pre-deﬁned tasks and heavy supervision (e.g., input-output pairs, class labels) for training and limit the user controllability at infer-ence time. Another approach is to utilize pretrained GAN models by directly optimizing the latent code for an indi-vidual image [1, 2, 56, 34, 36]. However, even on high-end
GPUs, it requires minutes of computation for each target image, and it does not guarantee that the optimized code would be placed in the original latent space of GAN.
A more practical approach is to train an extra encoder, which learns to project an image into its corresponding la-tent code [31, 55, 39, 33, 40]. Although this approach en-ables real-time projection in a single feed-forward manner, it suffers from the low ﬁdelity of the projected image (i.e., losing details of the target image). We attribute this limita-tion to the absence of spatial dimensions in the latent space.
Without the spatial dimensions, an encoder compresses an image’s local semantics into a vector in an entangled man-ner, making it difﬁcult to reconstruct the image (e.g., vector-based or low-resolution bottleneck layer is not capable of producing high-frequency details [30, 8]).
As a solution to such problems, we propose StyleMap-GAN which exploits stylemap, a novel representation of the latent space. Our key idea is simple. Instead of learn-ing a vector-based latent representation, we utilize a tensor with explicit spatial dimensions. Our proposed representa-tion beneﬁts from its spatial dimensions, enabling GANs to easily encode the local semantics of images into the latent space. This property allows an encoder to effectively project an image into the latent space, thus providing high-ﬁdelity and real-time projection. Our method also offers a new ca-pability to edit speciﬁc regions of an image by manipulating the matching positions of the stylemap.
On multiple datasets, our stylemap indeed substantially enhances the projection quality compared to the traditional vector-based latent representation (§4.3). Furthermore, we show the advantage of our method over state-of-the-art methods on image projection, interpolation, and local edit-ing (§4.4 & §4.5). Finally, we show that our method can transplant regions even when the regions are not aligned be-tween one image and another (§4.6). 2.