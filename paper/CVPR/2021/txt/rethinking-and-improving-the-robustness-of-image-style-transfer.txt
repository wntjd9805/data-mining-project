Abstract
On pre-trained models
On random models
Extensive research in neural style transfer methods has shown that the correlation between features extracted by a pre-trained VGG network has a remarkable ability to cap-ture the visual style of an image. Surprisingly, however, this stylization quality is not robust and often degrades signif-icantly when applied to features from more advanced and lightweight networks, such as those in the ResNet family.
By performing extensive experiments with different network architectures, we ﬁnd that residual connections, which rep-resent the main architectural difference between VGG and
ResNet, produce feature maps of small entropy, which are not suitable for style transfer. To improve the robustness of the
ResNet architecture, we then propose a simple yet effective solution based on a softmax transformation of the feature activations that enhances their entropy. Experimental results demonstrate that this small magic can greatly improve the quality of stylization results, even for networks with random weights. This suggests that the architecture used for feature extraction is more important than the use of learned weights for the task of style transfer. 1.

Introduction
Image style transfer aims to map a content image into the style of a different reference image.
It has received substantial attention, particularly with the introduction of neural style transfer algorithms based on deep networks. A consistent observation from this work [43, 24, 30, 23, 25, 19, 9, 2, 8] is that the correlation between the activations of a pre-trained VGG [33] network has remarkable ability to capture the visual style of an image. It is, however, puz-zling that when the VGG is replaced by architectures of better performance in other tasks, e.g. classiﬁcation, such as the ResNet [14, 44, 42], InceptionNet [35, 34, 36] or
DenseNet [16], stylization performance degrades signiﬁ-cantly. This is even more puzzling because, when imple-mented with the VGG, style transfer is very robust. For example, a VGG model with random weights performs com-parably to a pre-trained model [14, 3].
Content p-VGG r-VGG
Style p-ResNet r-ResNet
Figure 1: Neural style transfer by different architectures, using the methods of [7, 27] (‘p-’, ‘r-’ denotes pre-trained and randomly initialization. Please zoom in the picture for a detailed comparison).
Figure 1 shows an example of style transfer using differ-ent models. The VGG transfers style (color, texture, strokes) more faithfully than the ResNet, for both pre-trained and random weights. While these observations have spurred dis-cussion in the literature, about why style transfer is much more effective for the VGG [27, 12, 4], there are still no clear answers. In particular, there has not been a compre-hensive study of (i) what architectural differences between the VGG and other networks cause this striking performance difference, and (ii) what remedies could make non-VGG networks perform as well as the VGG. One explanation is that VGG features are more robust than others. To validate this conjecture, [27, 4] trained a ResNet with adversarial examples [10] to improve feature robustness. They found this can signiﬁcantly improve stylization quality. However, the fact that the VGG with random weights can generate comparable results [13, 3] suggests that robustness is not a property of the training data, but inherent to the architecture.
In this work, we investigate this hypothesis by comparing stylizations produced with activations from different archi-tectures. We seek the architectural properties that explain the differences between these activations, and how these could 124
explain the discrepancy between stylization results. Taking the ResNet as a non-VGG architecture representative, we study the statistics of both activations and the derived Gram matrices, usually used to encode image style. A striking observation is that, when normalized into a probability dis-tribution, the ResNet activations of deeper layers have large peaks and small entropy. This shows that they are dominated by a few feature channels and have nearly deterministic cor-relation patterns. It suggests that the optimization used to synthesize the stylized images is biased into replicating a few dominant patterns of the style image and ignoring the majority. This explains why the ResNet is unable to transfer high-level style patterns, such as strokes, that are usually captured in deeper layers of the network. In contrast, VGG activations are approximately uniform for all layers, cap-turing a much larger diversity of style patterns. We then analyze the architectural properties that could lead to very peaky activations, and conclude that they can be, in signiﬁ-cant part, explained by the existence of residual or shortcut connections between layers. The fact that these connections are prevalent in most modern architectures explains why the robustness problem is so widespread. In summary, residual connections are not good for style transfer.
We then investigate whether it is possible to solve the ro-bustness problem without changing the network architecture, and in a manner that is compatible with the large diversity of stylization losses in the literature. Taking inspiration from knowledge distillation, we propose to smooth the activations used in the computation of these loss functions. This can be implemented by adding a simple softmax transformation to existing losses. We denote the novel version of stylization as Stylization With Activation smoothinG (SWAG). Experi-ments show that SWAG is an important contribution at three levels. First, it improves the performance of several popu-lar stylization algorithms for several popular architectures, including ResNet, Inception, and WideResNet. Second, for these architectures, it improves the performance of random networks to the level of pre-trained ones. Third, for pre-trained networks, non-VGG models with SWAG can even outperform the VGG with standard stylization. 2.