Abstract
We introduce a new dataset for the emotional artiﬁcial intelligence research: identity-free video dataset for Micro-Gesture Understanding and Emotion analysis (iMiGUE).
Different from existing public datasets, iMiGUE focuses on nonverbal body gestures without using any identity infor-mation, while the predominant researches of emotion anal-ysis concern sensitive biometric data, like face and speech.
Most importantly, iMiGUE focuses on micro-gestures, i.e., unintentional behaviors driven by inner feelings, which are different from ordinary scope of gestures from other gesture datasets which are mostly intentionally performed for illus-trative purposes. Furthermore, iMiGUE is designed to eval-uate the ability of models to analyze the emotional states by integrating information of recognized micro-gesture, rather than just recognizing prototypes in the sequences separate-ly (or isolatedly). This is because the real need for emo-tion AI is to understand the emotional states behind ges-tures in a holistic way. Moreover, to counter for the chal-lenge of imbalanced sample distribution of this dataset, an unsupervised learning method is proposed to capture laten-t representations from the micro-gesture sequences them-selves. We systematically investigate representative meth-ods on this dataset, and comprehensive experimental result-s reveal several interesting insights from the iMiGUE, e.g., micro-gesture-based analysis can promote emotion under-standing. We conﬁrm that the new iMiGUE dataset could advance studies of micro-gesture and emotion AI. 1.

Introduction
Emotional artiﬁcial intelligence (emotion AI) is using machine learning methods to enable computers to under-∗Corresponding author. Xin Liu’s work mostly done at the University of Oulu. This paper is supported by Academy of Finland, KAUTE foun-dation, and National Natural Science Foundation of China.
Figure 1. Three frames (cropped) from a post-match press confer-ence video to illustrate the identity-free micro-gestures, such as
“cover face”, “fold arms”, and “cross ﬁngers”. Could machine recognize these micro-gestures, and understand emotional states of the player in a holistic way, and further identify if the player has won or lost the match (positive or negative emotional states)? stand human emotions.
It plays a vital role in human-computer interaction since emotions are on all the time, p-resented in all kinds of human activities, thinking, and de-cision makings. According to psychological studies, body language is an essential part for understanding human emo-tions. Every day, we respond to thousands of such nonver-bal behaviors including facial expressions, eye movements or gaze, tone of voices, gestures, touches, and the use of space. Body language-based emotion understanding has at-tracted extensive attention in the communities of computer vision and affective computing, and a considerable number of datasets have been proposed, e.g., posed facial expres-sions [30, 56, 92, 82, 25, 96], spontaneous facial behav-iors [1, 47, 4, 51, 12, 35], micro-expressions [91, 41, 11], voice/speech [65, 66, 51, 61], social signals [28, 29], and multi-modal datasets with facial expressions and physio-logical signals [73, 34, 51, 61]. Although computational methodologies were proposed correspondingly and consec-utively to improve the performance on these datasets, there are still signiﬁcant gaps between current studies and the needs of real applications. Major limitations include: 1) Intentional behavioral-based gestures. Previous gesture studies mostly focused on illustrative (or iconic) gestures [84], e.g., waving hands as “hello” or “goodbye”, which are intentionally performed for conveying certain 10631
meanings or feelings during interactions. However, in many occasions people would suppress or hide their feelings (e-specially negative ones) rather than expressing them. Pre-vious studies [2, 3, 6] showed that there is a special group of gestures, the micro-gestures (MGs), which are helpful to understand such suppressed or hidden emotions. The ma-jor difference between MGs and illustrative gestures is that
MGs are unintentional behaviors elicited by people’s inner feelings, e.g., rubbing hands due to stress, and the function of MGs is for relieving or protecting oneself from negative feelings rather than presenting for others. Thus, being able to automatically recognize MGs would allow emotion un-derstanding at a better level. To the best of our knowledge, there is no publicly available dataset for this emotional MGs research in the ﬁeld of computer vision. 2) Gap between behavior recognition and emotion understanding. Most existing datasets only aim to eval-uate approaches that can detect and recognize prototypes of behaviors (including gestures). In fact, the actual need of e-motion AI is not merely to recognize certain behaviors, but to uncover the emotion underneath. Consider a post-match interview scenario, a player is interviewed by reporters over several question & answer rounds (see Fig. 1). Some MGs could be observed, e.g., cross arms (defensive) and cover face (upset or ashamed), but it is hoped that the machine can understand (identify) if the player has a positive or negative feeling (e.g., caused by winning or losing of the match). 3) Sensitive biometric data. Most of the existing datasets involve sensitive biometric data. Actually, biomet-ric data based identity recognition plays a critical role in a variety of applications and has gained great success in the past decade. While every coin has two sides, biometric in-formation is so sensitive that is particularly prone to be (i-dentity) stolen, misused, and unauthorized tracked. With the concerns of privacy grows, more attention should be paid to protect biometric data of individuals.
Psychological studies [16] showed that there are over 215 behaviors associated with psychological discomfort and most of them are not in the face. MGs are subtle and (some of them) short, mostly out of our awareness or no-tice during live interactions [21]. It would be of great value if we can develop computer vision methods to capture and recognize these neglected clues for better emotion under-standing. In this paper, we introduce a novel MGs dataset to address afore mentioned limitations. The key contribu-tions are summed up as follows: 1) Instead of using facial or vocal-expressions for emo-tion understanding, the proposed dataset offers an approach where the identity-free MGs are explored for hidden emo-tion understanding, and privacy of the individuals could be preserved. As far as we know, iMiGUE is the ﬁrst public benchmark focuses on emotional MGs. This is the ﬁrst in-vestigation of such gestures from the computer vision per-spective. Moreover, to deal with the issue of imbalanced classes distribution, an unsupervised model is proposed. 2) iMiGUE is not only for MG recognition, but also pro-vides a hierarchy that allows exploration of the relationship between MGs and emotion, i.e, associates the MGs holis-tically for emotion understanding. As such, the data in iMiGUE are annotated on two levels: the MG categories were annotated on video clip-level, and the emotion cate-gories were labeled on video-level. 3) Comprehensive experiments are conducted on the iMiGUE to provide baseline results.
In video clip-level, the experimental results show that even fully supervised learning SOTA models cannot yield satisfactory accuracy on iMiGUE, which could verify that the challenges of rec-ognizing such hardly noticeable MGs. The proposed un-supervised method can achieve competitive performance compared with many supervised models.
In video-level, we ﬁnd micro-gesture is a vital factor for emotion under-standing. We only employed a simple recurrent neural net-work (RNN) network to achieve MGs analysis in a holistic way, but its emotion understanding result can beat existing action/gesture recognition-based models. The dataset and
ﬁndings will serve as a launch-pad for exploring identity-free MG-based emotion AI. 2.