Abstract
We present a simple, eﬀective, and general activa-tion function we term ACON which learns to activate the neurons or not. Interestingly, we ﬁnd Swish, the recent popular NAS-searched activation, can be interpreted as a smooth approximation to ReLU. Intuitively, in the same way, we approximate the more general Maxout family to our novel ACON family, which remarkably improves the performance and makes Swish a special case of ACON.
Next, we present meta-ACON, which explicitly learns to optimize the parameter switching between non-linear (activate) and linear (inactivate) and provides a new de-sign space. By simply changing the activation function, we show its eﬀectiveness on both small models and highly optimized large models (e.g. it improves the ImageNet top-1 accuracy rate by 6.7% and 1.8% on MobileNet-0.25 and ResNet-152, respectively). Moreover, our novel
ACON can be naturally transferred to object detection and semantic segmentation, showing that ACON is an eﬀective alternative in a variety of tasks. Code is available at https: // github. com/ nmaac/ acon . 1.

Introduction
The Rectiﬁed Linear Unit (ReLU) [13, 24, 39] has become an eﬀective component in neural networks and a foundation of many state-of-the-art computer vision al-gorithms. Through a sequence of advances, the Swish ac-tivation [41] searched by the Neural Architecture Search (NAS) technique achieves top accuracy on the challeng-ing ImageNet benchmark [9, 42]. It has been shown by many practices to ease optimization and achieve better performance [18, 49]. Our goal is to interpret the mechanism behind this searched result and investigate more eﬀective activation functions.
Despite the success of NAS on modern activations, a 1This work is supported by The National Key Research and
Development Program of China (No. 2017YFA0700800) and
Beijing Academy of Artiﬁcial Intelligence (BAAI).
Swish
SENet meta-acon
)
% ( t n e m e v o r p m
I y c a r u c c
A 2.5 2 1.5 1 0.5 0
ShuffleNetV2 ResNet50 ResNet101 ResNet152
Figure 1. ImageNet top-1 accuracy relative improvements compared with the ReLU baselines. As the models go larger,
Swish and SENet gain smaller, but Meta-ACON still improves very stably and remarkably even on the substantially deep and highly optimized ResNet152. The relative improvements of Meta-ACON are about twice as much as SENet. natural question to ask is: how does the NAS-searched
Swish actually work? Despite its widespread use, this activation function is still poorly understood. We show that Swish can be surprisingly represented as a smooth approximation to ReLU, by a simple and general ap-proximation formula (Equ. 2).
This paper pushes the envelop further: our method, called ACON, follows the spirit of the ReLU-Swish conver-sion and approximates the general Maxout [12] family to our novel ACON family by the general approximation for-mula. We show the converted functions (ACON-A, ACON-B,
ACON-C) are smooth and diﬀerentiable, where Swish is merely a case of them (ACON-A). ACON is conceptually simple and does not add any computational overhead, however, it improves accuracy remarkably. To achieve this result, we identify the ﬁxed upper/lower bounds in the gradient as the main obstacle impeding from improving accuracy and present the ACON with learnable upper/lower bounds (see Fig. 3).
In principle, ACON is an extension of Swish and has a dynamic non-linear degree, where a switching factor decays to zero as the non-linear function becomes linear. 8032    
Input layer
Convolutional layer
BatchNorm layer
ReLU activation
Input layer
Convolutional layer
BatchNorm layer
ACON
Figure 2. We propose a novel activation function we term the ACON that explicitly learns to activate the neurons or not.
Left: A ReLU network; Right: An ACON network that learns to activate (orange) or not (white).
Intuitively, this switching factor enables ACON to switch between activating or not. However, evidence [41] has shown that optimizing the factor simply by using the gradient descent cannot learn to switch between lin-ear and non-linear well. Therefore, we optimize the switching factor explicitly for fast learning and present meta-ACON that learns to learn whether to activate or not (see Fig. 2). Despite it seems a minor change, meta-ACON has a large impact: it has signiﬁcant improvements on various tasks very stably (even the highly-optimized and extremely deep SENet-154) and provides a new architecture design space in the meta learner, which could be layer-wise, channel-wise, or pixel-wise. The design in the provided space is beyond the focus of this paper, but it is suggestive for future research.
ACON transfers well on a wide range of tasks. No matter for small models or large models, our approach surpasses the ReLU counterpart signiﬁcantly: it im-proves the ImageNet top-1 accuracy rate by 6.7% and 1.8% on MobileNet-0.25 and ResNet-152, respectively.
We show its generality on object detection and semantic segmentation tasks.
We summarize our contributions as follows: (1) We present a novel perspective to understand Swish as a smoothed ReLU; (2) from this valuable perspective, we connect the two seemingly unrelated forms (ReLU and
Swish), and smooth ReLU’s general Maxout family to
Swish’s general ACON family; (3) we present meta-ACON that explicitly learns to activate the neurons or not, improves the performance remarkably. 2.