Abstract
Unsupervised feature learning has made great strides with contrastive learning based on instance discrimina-tion and invariant mapping, as benchmarked on curated class-balanced datasets. However, natural data could be highly correlated and long-tail distributed. Natural between-instance similarity conﬂicts with the presumed instance dis-tinction, causing unstable training and poor performance.
Our idea is to discover and integrate between-instance similarity into contrastive learning, not directly by instance grouping, but by cross-level discrimination (CLD) between instances and local instance groups. While invariant map-ping of each instance is imposed by attraction within its augmented views, between-instance similarity could emerge from common repulsion against instance groups.
Our batch-wise and cross-view comparisons also greatly improve the positive/negative sample ratio of contrastive learning and achieve better invariant mapping. To effect both grouping and discrimination objectives, we impose them on features separately derived from a shared representation.
In addition, we propose normalized projection heads and unsupervised hyper-parameter tuning for the ﬁrst time.
Our extensive experimentation demonstrates that CLD is a lean and powerful add-on to existing methods such as NPID, MoCo, InfoMin, and BYOL on highly correlated, long-tail, or balanced datasets. It not only achieves new state-of-the-art on self-supervision, semi-supervision, and transfer learning benchmarks, but also beats MoCo v2 and
SimCLR on every reported performance attained with a much larger compute. CLD effectively brings unsupervised learning closer to natural data and real-world applications.
Our code is publicly available at: https://github.com/frank-xwang/CLD-UnsupervisedLearning. 1.

Introduction
Representation learning aims to extract latent or semantic information from raw data. Typically, a model is ﬁrst trained on a large-scale annotated dataset [34] and then tuned on a small-scale dataset for a downstream task [25]. As the model gets bigger and deeper [26, 29], more annotated data are needed; supervised pre-training is no longer viable.
Self-supervised learning [13, 44, 63, 41, 14, 39] gets around labeling with a pre-text task which does not require annotations and yet would be better accomplished with se-mantics. For example, to predict the color of an object from its grayscale image does not require labeling; however, do-ing it well would require a sense of what the object is. The biggest drawback is that pre-text tasks are domain-speciﬁc and hand-designed, and they are not directly related to down-stream semantic classiﬁcation.
Unsupervised contrastive learning has emerged as a direct winning alternative [53, 64, 58, 6, 24]. The training objec-tive and the downstream classiﬁcation are aligned on discrim-ination, albeit at different levels of granularities: training is to discriminate known individual instances, whereas testing is to discriminate unknown groups of instances.
Contrastive learning approaches have made great strides with two ideas: invariant mapping [23] and instance discrim-ination [53]. That is, the learned representation should be 1) stable for certain transformed versions of an instance, and 2) distinctive for different instances. Both aspects can be formulated without labels, and the feature learned appears to automatically capture semantic similarity, as benchmarked by downstream classiﬁcation on standard datasets such as CI-FAR100 and ImageNet [6]. However, these datasets are cu-rated with distinctive and class-balanced instances, whereas natural data could be highly correlated within the class (e.g., repeats) and long-tail distributed across classes.
Natural between-instance similarity demands instance grouping not instance discrimination, where all the instances are presumed different. Consequently, feature learning by instance discrimination is unstable and under-performing without instance grouping, whereas instance grouping based on the feature learned without instance discrimination is eas-ily trapped into degeneracy. Ad-hoc tricks [3, 4] and mutual information maximization with a uniform class distribution prior [32] have been used to prevent feature degeneracy.
We propose to discover and integrate between-instance similarity into contrastive learning, not directly by instance grouping, e.g., by imposing group-level discrimination as 12586
x′ i xi x′ i xi a) instance discrimination b) instance-group discrimination c) Our cross-level discrimination boosts existing SOTA’s
Figure 1: Our unsupervised feature learning discovers simi-lar instances and integrates grouping into instance-level dis-crimination, outperforming the state-of-the-art (SOTA) clas-siﬁers on highly correlated, long-tail, or balanced datasets. a) Instance discrimination presumes all instances distinctive:
Instance xi attracts (↔) its augmented version x′ i and repels (>−<) all other instances including those highly similar ones. b) We propose cross-level discrimination (CLD) between instance xi and local groups of alternative views {x′ j}. xi attracts (↔) the group centroid that x′ i belongs to and re-pels (>−<) other group centroids. Visually similar instances tend to attract/repel the same group centroids and are thus mapped closer. c) Our CLD can be added to existing meth-ods such as NPID [53], MoCo [24], MoCo v2 [7], InfoMin
[49] and BYOL [21]. It consistently provides a signiﬁcant performance boost on highly correlated (HC), long-tail (LT), and standard balanced ImageNet datasets.
DeepCluster [3, 4] or by regulating instance-level discrimi-nation based on the grouping outcome as Local Aggregation (LA) [64], but by imposing cross-level discrimination (CLD) between instances and local instance groups.
Contrastive learning is built upon dual forces of attraction and repulsion [23]. Existing methods generally assume repul-sion between different instances and attraction within known groupings of instances, e.g., between augmented views of the same data instances [53, 64, 24], or between data cap-tured from different times, views, or modalities of the same physical instances [42, 1, 50, 48].
Feature learning with between-instance similarity calls for attraction within unknown groupings, not the universal between-instance repulsion (Fig. 1a). An chicken-and-egg challenge is to discover such groupings for feature learning while the feature for the groupings is still to be developed.
Our key insight is that grouping could result from not just attraction, but also common repulsion. While invariant map-ping is achieved by within-instance similarity from attraction across augmented views, between-instance similarity can emerge from repulsion against common instance groups, the centroids of which are more stable in the developing feature space. That is, to discover the most discriminative feature that also respects natural instance grouping, we desire each instance to attract the closest group related by augmentation and repel groups of other instances that are far from it.
In our approach (Fig. 1b), between-instance similarity, un-known a priori, is not captured directly as attraction between instances, but by more likely common attraction and repul-sion between each instance and instance group centroids.
By pulling an instance towards and pushing it against more stable instance groups, similar instances get mapped closer in the feature space. To effect both grouping and discrimina-tion objectives on feature learning, we also impose them on features separately derived from a shared representation.
Such an interplay between attraction and repulsion has been utilized to model perceptual popout [60, 2], as well as simultaneous image segmentation and depth segregation
[59, 38]. However, those works are prior to deep learning and aim at grouping pixels based on certain ﬁxed pixel-level feature such as edges, whereas our work aims at learning the image-level feature discriminatively.
We add CLD to popular state-of-the-art (SOTA) unsu-pervised feature learning approaches (Fig. 1c), e.g., NPID
[53], MoCo [24], InfoMin [49] (all three based on instance discrimination), and BYOL [21] (focusing only on invariant mapping without instance discrimination). CLD delivers a signiﬁcant performance boost not only on highly correlated, long-tail, and balanced datasets, but also on all the self-supervision, semi-supervision, and transfer learning bench-marks under fair comparison settings [53, 24, 62].
Our work makes three major contributions. 1) We ex-tend unsupervised feature learning to natural data with high correlation and long-tail distributions. 2) We propose cross-level discrimination between instances and local groups, to discover and integrate between-instance similarity into con-trastive learning. We also propose normalized projection heads and unsupervised hyper-parameter tuning. 3) Our experimentation demonstrates that adding CLD to existing methods has an negligible overhead and yet delivers a sig-niﬁcant boost. It achieves new SOTA on all the benchmarks, and beats MoCo v2 [7] and SimCLR [6] on every reported performance attained with a much larger compute. 2.