Abstract
We present GATSBI, a generative model that can trans-form a sequence of raw observations into a structured latent representation that fully captures the spatio-temporal con-text of the agent’s actions. In vision-based decision making scenarios, an agent faces complex high-dimensional obser-vations where multiple entities interact with each other. The agent requires a good scene representation of the visual observation that discerns essential components and con-sistently propagates along the time horizon. Our method,
GATSBI, utilizes unsupervised object-centric scene repre-sentation learning to separate an active agent, static back-ground, and passive objects. GATSBI then models the in-teractions reﬂecting the causal relationships among decom-posed entities and predicts physically plausible future states.
Our model generalizes to a variety of environments where dif-ferent types of robots and objects dynamically interact with each other. We show GATSBI achieves superior performance on scene decomposition and video prediction compared to its state-of-the-art counterparts. 1.

Introduction
An ideal intelligent agent should be able to learn various tasks in diverse environments without relying on speciﬁc sen-sor conﬁgurations or control parameters. Recent approaches employ visual observation as the sole input to infer the phys-ical context of the agent and its surroundings, thus aim to adapt to a general setup. One may interpret the visual in-put via conventional computer vision techniques employing deep neural networks [20, 33]. While they exhibit perfor-mance comparable to human perception, such approaches require a large volume of annotated database. Not only are the groundtruth labels costly to obtain, but also such super-vised approaches are limited to speciﬁc tasks that they are trained on.
In contrast, unsupervised generative models extract the
Figure 1. Our method, GATSBI, can explicitly identify the agent by utilizing the keypoint-based heatmap. Thus, the observation is decomposed into the agent, background, and objects. In addition,
GATSBI infers the dynamic properties of the agent and temporally models the agent-centric interaction with the objects. latent variables that encode the compositional relationship between different entities without prior knowledge [25]. The quality of representation can be veriﬁed by the ability of reconstructing the input video sequence from the disentan-gled latent variables [5]. In an ideal case, the latent variables contain the time-varying composition between the agent and the set of objects, and the structural knowledge must prop-agate temporally with a consistent inference of the latent dynamics. The latent dynamics of the learned representation reﬂect the underlying physics between the extracted entities, thus the agent can leverage the latent dynamics in predicting the various physical contexts conditioned on its own action.
We propose a fully-unsupervised action-conditioned video prediction model, named Generative Agent-cenTric
Spatio-temporal oBject Interaction (GATSBI). Our method is explicitly designed for vision-based learning of robot agents and is able to distinguish the active, passive and static com-3074
ponents from the robot-object interaction sequence, Fig. 1.
Conditioning only on actions and a few frames, the learned latent dynamics can predict the long-term future observa-tions without any prior labels of individual components or physics model.
Our generative model sequentially factorizes each video frame into individual components and extracts the latent dy-namics. Speciﬁcally, our unsupervised network ﬁrst models relatively large scene components as 2D-Gaussian mixture model (GMM). In addition, a group of 2D-Gaussian key-points captures actively moving pixels in response to the given action. One of the GMM modes that matches best with the keypoint-based representation is selected and re-ﬁned to learn the latent dynamics of the active agent. In the meantime, small passive objects are extracted by attention-based object discovery models [30]. Finally, graph neural networks (GNN) encodes the interactions between the active agent, passive objects, and static background that are disen-tangled in the extracted latent variable. The three different categories of the scene entities are reﬂected as inherent phys-ical properties within the graphical model, which correctly updates the state of each object in response to the diverse interactions.
In summary, GATSBI is an unsupervised representation learning framework that infers a decomposed latent represen-tation of the observation sequence and predicts associated latent dynamics in an agent-centric manner. GATSBI can distinguish various components and correctly understand the causal relationship between them from a sequence of visual observations without speciﬁc labels or prior. Being able to locate the active agent, the acquired latent representation is aware of the dynamics in response to the control action, and can readily be applied to an agent in making physically-plausible decisions. We provide extensive investigation on both qualitative and quantitative performance of GATSBI for video prediction on various robot-object interaction sce-narios. We also compare our model with previous methods on spatio-temporal representation learning and show their promise and limitation. 2.