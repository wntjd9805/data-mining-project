Abstract
Recent progress on visual question answering has ex-plored the merits of grid features for vision language tasks.
Meanwhile, transformer-based models have shown remark-able performance in various sequence prediction prob-lems. However, the spatial information loss of grid fea-tures caused by ﬂattening operation, as well as the defect of the transformer model in distinguishing visual words and
In this paper, non visual words, are still left unexplored. we ﬁrst propose Grid-Augmented (GA) module, in which relative geometry features between grids are incorporated to enhance visual representations. Then, we build a BERT-based language model to extract language context and pro-pose Adaptive-Attention (AA) module on top of a trans-former decoder to adaptively measure the contribution of visual and language cues before making decisions for word prediction. To prove the generality of our proposals, we apply the two modules to the vanilla transformer model to build our Relationship-Sensitive Transformer (RSTNet) for image captioning task. The proposed model is tested on the MSCOCO benchmark, where it achieves new state-of-art results on both the Karpathy test split and the online test server. Source code is available at GitHub 1. 1.

Introduction
Image captioning task aims to automatically generate a natural language sentence to describe the visual content of a given image. The encoder-decoder framework inspired by neural machine translation [34] has been widely adopted by captioning models[39, 40, 41, 5, 25], in which the CNN based encoder extracts visual features and the RNN based decoder generates the output sentence. Besides, the atten-*corresponding author 1https://github.com/zhangxuying1004/RSTNet h 0 i j
⋯ ⋯ ⋯ j i h×w w (a)
Caption: A man hitting a tennis  ball with a racquet. (b)
Figure 1. This paper aims at reducing the spatial information loss of features and characterizing the visualizability of words in Cap-tioning task. (a) shows the loss of spatial information when the grid features are ﬂattened and fed to the transformer encoder. (b) illustrates the examples of visual (red) and non-visual (blue) word. tion mechanism was introduced in order to help the model focus on the relevant positions when generating each word
[41, 16]. Based on the encoder-decoder framework, most efforts to improve image captioning model focus on two main aspects: a) optimizing the visual features extracted from the input image [2, 42, 12], and b) improving the model structure for feature processing [41, 2, 23, 6, 28].
In terms of visual representation, region-based visual features [2] have become the dominant approach in major vision and language tasks like image captioning and visual question answering. However, the region extraction pro-cess is so time-consuming that currently most of the models with region features are directly trained and evaluated on cached visual features. Recently, Jiang et al. [15] revisited the grid features for VQA and demonstrated that grid fea-tures extracted from exactly the same layer of region feature detector [30] work quite well, both in speed and accuracy.
In this paper, we also utilize grid features as the main vi-sual representation for our captioning model. Nevertheless, grid features are ﬂattened when fed to a transformer model, which inevitably leads to the loss of spatial information, as shown in Figure 1(a). Thus, we propose Grid-Augmented (GA) module which incorporates the spatial geometric re-lationships between relative locations into grids in order to facilitate a more comprehensive use of the grid features. 15465
In terms of model structure for feature processing, trans-former [37] based captioning models [13, 6, 28] have been leading state-of-the-art performance on public benchmarks.
The transformer architecture is able to better capture the re-lationship between visual features and process sequences in parallel during training. However, not all words in a cap-tion are visual words and have corresponding visual signals due to the semantic gap between vision and language [23], as shown in Figure 1(b). For the attention module in trans-former decoder layer, the intermediate representations used to predict each word are stacked together. As a result, all word predictions are treated equally, based on Scaled Dot-Product [37] operation, whether the word is a visual word or non-visual word. In other words, no effective measures have taken to process visual words and non-visual words differently for transformer based image captioning models.
Thus, We build Adaptive Attention (AA) module based on language context and visual signals for transformer ar-chitecture to measure the contribution of visual signals and language context for a ﬁne-grained caption generation.
We apply the GA module and AA module to our transformer based image captioning model, Relationship-Sensitive Transformer (RSTNet). For each attention mod-ule of transformer encoder, the relative geometry informa-tion of grid features is incorporated to calculate a more ac-curate attention distribution. For the decoder, there will be a trade-off between the contributions of visual and language cues rather than predicting words directly.
We extensively evaluate our RSTNet on the MSCOCO benchmark dataset [22], where quantitative and qualitative experiments prove the effectiveness of our model. In partic-ular, our proposed RSTNet achieves state-of-the-art perfor-mance both ofﬂine and online. To gain more insights, we used the intermediate output of our RSTNet to measure the visualness of each word appeared in the Karpathy [17] test split of MSCOCO, which not only demonstrates the effec-tiveness of the proposed model but also reveals the impact of the semantic gap in a more intuitive way.
Our contributions can be summarized as follows:
•
•
•
We propose a Grid-Augmented (GA) module, an ex-tension to the ﬂattened grid features, to boost the cap-tioning performance by integrating the spatial informa-tion of raw visual features extracted from an image.
We propose an Adaptive-Attention (AA) module, dy-namically measuring the contribution of visual signals and language signal for the prediction of each word, to facilitate a more ﬁne-grained captioning generation.
We apply GA module and AA module into our RST-Net to achieve new state-of-art performance on COCO benchmark dataset. To grain more insights, We de-ﬁne a cross-domain attribute termed visualness, which quantitatively measures the visualizability of each word in vocabulary. 2.