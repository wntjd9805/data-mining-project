Abstract
Pixel-wise segmentation is one of the most data and an-notation hungry tasks in our ﬁeld. Providing representa-tive and accurate annotations is often mission-critical es-pecially for challenging medical applications. In this pa-per, we propose a semi-weakly supervised segmentation al-gorithm to overcome this barrier. Our approach is based on a new formulation of deep supervision and student-teacher model and allows for easy integration of different supervi-sion signals.
In contrast to previous work, we show that care has to be taken how deep supervision is integrated in lower layers and we present multi-label deep supervision as the most important secret ingredient for success. With our novel training regime for segmentation that ﬂexibly makes use of images that are either fully labeled, marked with bounding boxes, just global labels, or not at all, we are able to cut the requirement for expensive labels by 94.22% – narrowing the gap to the best fully supervised baseline to only 5% mean IoU. Our approach is validated by extensive experiments on retinal ﬂuid segmentation and we provide an in-depth analysis of the anticipated effect each annotation type can have in boosting segmentation performance. 1.

Introduction
Medical imaging tools have become a central part in mod-ern health care. In direct consequence, hundreds of millions of 2D- and 3D-images are recorded per year [41]. With this ever-growing number, medical personnel has become increasingly entangled in their evaluation. Deep learning with the use of artiﬁcial neural networks has found use to al-leviate the necessary effort needed for interpretation of such images. However, training these models requires enormous amounts of annotated data, especially in case of semantic segmentation. The annotation process for this task is al-ready extremely difﬁcult in real-world scenarios such as in urban street-scenes, where the pixel-wise annotation pro-cess of a single image could span up to 90 minutes [15].
Figure 1. Annotations for segmentation are costly, especially when experts need to provide them. We show how our semi-weakly semantic segmentation method can use different annotation types and how the recognition performance beneﬁts from them.
This problem is ampliﬁed in the medical domain as we further require domain experts to annotate data who are severely time-restricted due to their clinical work. Thus, we are faced with the problem of minimizing the needed annotation effort while maximizing model accuracy.
The majority of existing approaches follow two orthog-onal paths: incorporating non-annotated data [24] which is fast to obtain or using cheap weak labels with different lev-els of quality ranging from image-level [10, 23, 32, 47] over single point [7] to bounding box annotations [29, 55]. Both these so called semi- and weakly-supervised approaches led to great insights and convincing results. However, they ig-nore that practical applications are often faced with several types of supervision simultaneously (Fig. 1).
In very common scenarios, we are provided with a small pixel-wise annotated data set, with automatically parsed image-level labels from medical reports, and with large amounts of additional unlabeled data from the same distri-bution. Currently, it is largely unanswered how such diverse supervision types can be uniﬁed to train a semantic segmen-tation system. To this end, we go beyond standard weakly-or semi-supervised learning and investigate also the semi-weakly supervised setting in the low-resource scenario of 9532
the medical domain.
For dealing with only few annotated examples, we pro-pose a novel pathway to integrate training signals deep into segmentation network layers via a new take on deep super-vision. We then amplify these signals by enriching weakly-or entirely un-labeled images via our novel approach to in-fer robust pseudo-labels using a mean-teacher segmentation model.
Furthermore, as iterative training processes in low-data scenarios are often unstable, we present experiments fol-lowing a rigorous evaluation protocol and report test accu-racies with standard deviations along numerous data-splits.
Our contributions amount to: (1) We present the ﬁrst thorough investigation of varying numbers of training samples and a large diversity of supervision types for semantic segmentation. (2) We introduce a novel perspective on the deep supervi-sion paradigm adapting it to segmentation in our Multi-label Deep Supervision technique. With this, we intro-duce a ﬂexible semi-weakly supervised pathway to in-tegrate either un- or weakly labeled images: our novel
Self-Taught Deep Supervision approach. (3) Finally, our best performing method Mean-Taught
Deep Supervision adds invariance towards perturba-tions and a robust pseudo-label generation, achieving results close to fully supervised baselines while using only a fraction of 5.78% strong labels. 2.