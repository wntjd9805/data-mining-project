Abstract
Despite the recent progress of generative adversarial networks (GANs) at synthesizing photo-realistic images, producing complex urban scenes remains a challenging problem. Previous works break down scene generation into two consecutive phases: unconditional semantic layout syn-thesis and image synthesis conditioned on layouts. In this work, we propose to condition layout generation as well for higher semantic control: given a vector of class propor-tions, we generate layouts with matching composition. To this end, we introduce a conditional framework with novel architecture designs and learning objectives, which effec-tively accommodates class proportions to guide the scene generation process. The proposed architecture also allows partial layout editing with interesting applications. Thanks to the semantic control, we can produce layouts close to the real distribution, helping enhance the whole scene genera-tion process. On different metrics and urban scene bench-marks, our models outperform existing baselines. More-over, we demonstrate the merit of our approach for data augmentation: semantic segmenters trained on real layout-image pairs along with additional ones generated by our approach outperform models only trained on real pairs. 1.

Introduction
Generative Adversarial Networks (GANs) [1, 8, 13, 21] have become powerful tools to generate photo-realistic im-ages based on a collection of examples. When trained on real photo portraits in particular, they can produce stunning results [14, 15]. However, for complex structured images like urban scenes, they still struggle to produce satisfactory results: not only do generated scenes exhibit various types of artifacts, but they are also difﬁcult to use for downstream tasks. In automotive applications for instance, generating a wide range of synthetic driving scenes to replace or comple-ment limited amounts of real annotated data is expected to help training better models in the future, for critical-safety tasks such as object detection and segmentation. This is not
∗Main part of this work was done during an internship at Valeo.ai
†Inria, ´Ecole normale sup´erieure, CNRS, PSL Research University 35% 29% 5% 7% 7% 5% 5% 3% 27% 9% 27% 12% 1% 1% 1% 1% 4% 3% 3% 4% 3% 2% 1% 5%
Layout Synthesis conditioned on
Class Proportions
Image Synthesis conditioned on
Semantic Layout
Figure 1: Scene generation guided by semantic proportions.
The proposed approach, “Semantic Palette”, allows a tight control of class proportions when generating semantic layouts and, condi-tioned on the latter, photo-realistic scenes such as urban scenes. yet the case, and our work is motivated by this goal.
Our target application here is image semantic segmen-tation, the task of predicting semantic layouts, that is, a class label (or a set of class probabilities) for each pixel of a picture. State-of-the-art models being fully supervised, their training requires scene examples with corresponding semantic layouts. Hence, generating this type of data with a GAN amounts to producing matching image-layout pairs.
To this end, recent works advocate decoupling the synthe-sis process into two consecutive phases: ﬁrst generating se-mantic layouts with plausible object arrangements [2, 11], then translating these layouts into realistic images [24, 33].
To improve the usability of such a pipeline, we mostly focus here on the ﬁrst layout generation step. Existing works [2, 11] cast it as a standard generative process that turns a random input code into a semantic map. While simple to use, this approach offers no real control on the modes of the output distribution [22], which is a limitation for complex scenes. In contrast, we propose to control the generation of layouts with a target distribution of semantic classes in the scene. Depending on applications, this class histogram can be manually deﬁned, automatically derived from a true one, or sampled from a suitable distribution.
To this end, we introduce a conditional layout GAN that takes a class histogram (the semantic code or palette) as in-put beside the standard random noise. As a result, our full image-layout generation pipeline (Figure 1) offers a sim-ple yet powerful control over the scene composition. This ability brings beneﬁts in various applications, ranging from 9342
real image editing to data augmentation for improved model training of a downstream task.
Using a progressive GAN [13] as base architecture to generate semantic layouts, we propose novel architecture designs and learning objectives to achieve our goal. First, we inject the semantic code throughout the progressive pipeline, i.e., at multiple intermediate scales. To explicitly enforce the targeted class distribution while avoiding degen-erate soft class assignments, we propose: a semantically-assisted activation (SAA) module along with two new learn-ing objectives, as well as a novel residual conditional fusion module to ease the progressive propagation of the semantic target through the scales. Lastly, we introduce a variant of the proposed framework that allows partial editing of sub-regions in existing semantic layouts.
Our main experiments are conducted on different urban scene datasets. Using suitable direct metrics, we ﬁrst as-sess the quality of the generated layouts and of the images derived from them. We also assess thoroughly the merit of our approach in the light of semantic segmentation down-stream task. To this end, we train segmentation models on synthesized (resp. real) data and measure their performance on real (resp. synthesized) data, as a way to compare our method with the baselines. We ﬁnally assess the ability of several approaches to improve model training through aug-mentation of a real-data training set. In all experiments, Se-mantic Palette outperforms baselines and produces scenes that follow better the distribution of real ones. More im-portantly, for real-world applications, using it to extend real datasets boosts performance in semantic segmentation.
In summary, our main contributions are:
• A novel layout generative model that allows control of the distribution of semantic classes. This beneﬁts both the quality of the images that are subsequently gen-erated and the practical use of these images. To fur-ther enhance the quality of the generated scene-layout pairs, our method allows end-to-end training of both layout and image generators.
• A variant of our framework for partial editing of se-mantic layouts. This further beneﬁts downstream task training and opens up interesting applications like se-mantic editing of real images.
• Extensive evaluations on three different driving bench-marks. The proposed framework signiﬁcantly outper-forms several baseline approaches. 2.