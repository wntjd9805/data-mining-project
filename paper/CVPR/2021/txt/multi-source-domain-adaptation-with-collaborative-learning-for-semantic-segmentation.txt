Abstract
Multi-source unsupervised domain adaptation (MSDA) aims at adapting models trained on multiple labeled source domains to an unlabeled target domain. In this paper, we propose a novel multi-source domain adaptation framework based on collaborative learning for semantic segmentation.
Firstly, a simple image translation method is introduced to align the pixel value distribution to reduce the gap between source domains and target domain to some extent.
Then, to fully exploit the essential semantic information across source domains, we propose a collaborative learning method for domain adaptation without seeing any data from target domain. In addition, similar to the setting of unsupervised domain adaptation, unlabeled target domain data is leveraged to further improve the performance of domain adaptation. This is achieved by additionally con-straining the outputs of multiple adaptation models with pseudo labels online generated by an ensembled model.
Extensive experiments and ablation studies are conducted on the widely-used domain adaptation benchmark datasets in semantic segmentation. Our proposed method achieves 59.0% mIoU on the validation set of Cityscapes by training on the labeled Synscapes and GTA5 datasets and unlabeled training set of Cityscapes. It signiﬁcantly outperforms all previous state-of-the-arts single-source and multi-source unsupervised domain adaptation methods. 1.

Introduction
Semantic segmentation as one of the core tasks in com-puter vision community, aims to assign semantic label to each pixel of images, e.g., person, car, road and etc.. With the development of convolutional neural networks (CNNs), semantic segmentation has made great progress recent-ly. For example, recent deep methods [2, 5, 7, 11, 41], have achieved superior performance on almost all public
∗Corresponding author
GTA5
Synscapes
Synthia
Cityscapes
Transfer
Labeled Sources
Unlabeled Target
Figure 1. Multi-source domain adaptation for semantic segmenta-tion. The left shows synthetic images and corresponding labels generated from different simulators, which suffer domain shift between each other but share similar semantic contexts. The right part shows unlabeled target images sampled from real scenes. benchmarks. However, their success is based on the large numbers of densely annotated images which used to train the networks. Dense pixel-level annotation for semantic segmentation is very laborious and expensive, e.g., anno-tating one image in the Cityscapes dataset [4] takes about 90 minutes, which makes it difﬁcult and sometimes even impossible to collect large amounts of densely annotated images for semantic segmentation. Thanks to the recent progress in graphics and simulation infrastructure, simula-tors can generate lots of images with dense annotation for semantic segmentation, such as recent proposed large-scale dense labeled datasets SYNTHIA [28], GTA5 [27] and
Synscapes [36]. Although the huge amounts of annotated synthesized images are very close to the real scene, there is still great domain gap between synthetic datasets and real scene datasets. The domain gap causes another problem that networks trained on synthetic datasets often perform poorly on real target scenes. To handle this issue, many un-/semi-supervised domain adaptation (UDA) approaches are proposed, like [13, 21, 29, 31, 32, 35, 37] and etc., with the purpose of mitigating the gap between synthetic source and 11008
real target domain. Over the past years, UDA has made a great progress.
Although existing works have greatly boosted the per-formance of UDA for semantic segmentation, most of them focus on single source. Seldom works consider a more prac-tical setting where labeled datasets from multiple sources with different distributions are available, e.g., SYNTHIA and GTA5. Training with multiple sources can further alleviate the problem on lack of annotated data. Moreover, multiple sources sampled from different distribution can also encourage networks to learn more essential knowledge for semantic segmentation. A straightforward approach is to simply combine all source domains into a single one, and then trains a UDA model on the combined sources and target domain dataset. This simple method can indeed boost the performance, but it does not fully exploit the abundant information across multiple source domains. Domain shift across multiple sources restricts the power within them in learning a more powerful domain adaptation model.
There are several multi-source deep UDA methods are proposed recently to exploit multiple source domains for better adaptation. They align different domains by trans-lating images from source domains to the target style via generative adversarial networks (GAN). However, most of them [23, 30, 15] work on image classiﬁcation task except for MADAN [43] which works on semantic segmentation, a pixel-wise prediction task.
In this paper, we propose an approach based on collaborative learning and image translation to address multi-source domain adaptation for semantic segmentation.
Our observation shows that appearance discrepancy e-specially color discrepancy between source domains and target domain has a great impact on the performance of adaptation. Existing works [34, 12, 38, 10] demonstrate that style transfer could reduce this discrepancy in some extent. However, most of them are complicated to plug in networks during training process. Therefore, we propose a simple image translation method to ﬁrst mitigate domain gap between sources and target. Unlike MADAN [43],
FDA [37] and GAN-based translation methods, we propose to translate source domain images to the target style by aligning different distributions to the target domain in LAB color space.
In addition, we observe that apart from discrepancy in appearance, images from different domains do still share much similarity in semantic contexts as shown in Fig 1. The shape of instances (person, car, bike and etc.) and spatial layout of different instances (cars always on the road, sidewalk adjacent to the road, sky on the top and etc.) are almost the same in all domains. Two collaborative learning strategies are proposed to explore essential and domain-invariant semantic contexts across different domains. First we propose a collaborative learning between source domains to investigate the case of domain adaptation without seeing any data from target domain, which is also called domain generalization in previous works [6, 22, 40]. For each source domain, we have a semantic segmentation network supervised by annotation maps, and an additional soft supervision coming from other models trained on a different source domain. In addition, similar to previous UDA methods [43, 37], we also consider making full use of the unlabeled data of the target domain to further boost the performance. A collaborative learning based on target domain is proposed, in which an ensemble of models trained on source domains is used to produce pseudo labels for data from target domain in an online fashion. In turn, each model can be additionally supervised by the generated pseudo labels. Such two collaborations help constantly improve each model’s adaptation capability to target domain during the training process.
The performance of our method signiﬁcantly outper-forms other state-of-the-art single-source and multi-source
UDA methods. This success of proposed method is mainly attributed to the effective image translation and domain-invariant feature learning. Note that, our method can be trained in both end-to-end and stage-wise. 2.