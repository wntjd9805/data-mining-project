Abstract
In this paper, we present a new dataset with the target of advancing the scene parsing task from images to videos.
Our dataset aims to perform Video Scene Parsing in the
Wild (VSPW), which covers a wide range of real-world sce-narios and categories. To be speciﬁc, our VSPW is fea-tured from the following aspects: 1) Well-trimmed long-temporal clips. Each video contains a complete shot, last-ing around 5 seconds on average. 2) Dense annotation.
The pixel-level annotations are provided at a high frame rate of 15 f/s. 3) High resolution. Over 96% of the cap-tured videos are with high spatial resolutions from 720P to 4K. We totally annotate 3,536 videos, including 251,633 frames from 124 categories. To the best of our knowl-edge, our VSPW is the ﬁrst attempt to tackle the challenging video scene parsing task in the wild by considering diverse scenarios. Based on VSPW, we design a generic Tempo-ral Context Blending (TCB) network, which can effectively harness long-range contextual information from the past frames to help segment the current one. Extensive experi-ments show that our TCB network improves both the seg-mentation performance and temporal stability comparing with image-/video-based state-of-the-art methods. We hope that the scale, diversity, long-temporal, and high frame rate of our VSPW can signiﬁcantly advance the research of video scene parsing and beyond. The dataset is available at https://www.vspwdataset.com/. 1.

Introduction
Scene parsing aims to assign a unique semantic label to every pixel in a given image, which is a fundamen-tal research topic in the computer vision community and has many potential applications such as image editing, au-tonomous driving and robotics. With the development of the Convolutional Neural Networks (CNNs), many kinds of fully convolutional neural networks [45, 72, 10, 26] have
†Corresponding author.
‡Part of this work was done when Jiaxu Miao was an intern at Baidu
Research.
Figure 1. (a) The category cloud of our VSPW. (b) One example for the video scene annotation. been proposed to advance this research area. In addition, several image-based datasets, e.g., Pascal-Context [18],
ADE20K [75], COCO-Stuff [5], Cityscapes [16], have also been collected to evaluate the effectiveness of these scene parsing approaches.
However, the real-world is actually video-based rather than a static state, and learning to perform video scene parsing is more reasonable and practical for realistic appli-cations. Although remarkable progress has been made in image-based scene parsing, few works have been proposed to consider the video scene parsing, which is mainly limited by the lack of suitable benchmarks. Although CamVID [2] has been proposed to tackle the video scene parsing, this dataset is heavily limited by its small scale (701 frames from 6 videos), low frame rate (1 f/s), and single scenario (only the street view is considered). Cityscapes [16] and
NYUv2 [55] are often used for the video scene parsing task.
However, they are actually image-based datasets because only one frame or several nonadjacent frames in a video clip are annotated.
To advance the scene parsing task from images to videos, 4133
we present a new dataset in this work, aiming at perform-ing the challenging yet practical Video Scene Parsing in the
Wild (VSPW). The dataset covers a wide range of real-world scenarios (e.g., art galleries, lecture rooms, beach, and street views) and categories from both things (e.g., per-son, car, desk) and stuff (e.g., road, wall, sky). To the best of our knowledge, our VSPW is the ﬁrst attempt to tackle the challenging video scene parsing task by considering di-verse scenarios. Concretely, our VSPW has the following characteristics:
• Well-trimmed long-temporal clips. Based on the pre-deﬁned real-world scenarios, we collected the related videos from the Internet. Each video is carefully exam-ined and trimmed into a complete shot, lasting around 5 seconds on average.
• Dense annotation. Different from previous similar works [2], we provide the pixel-level annotations at a competitive frame rate of 15 f/s, making the temporal information be well considered to learn better video scene parsing models.
• High resolution. We abandon those poor videos with low resolution or heavy shake, and only keep high-quality ones. Within our VSPW, over 96% of videos are with high spatial resolutions from 720P to 4K.
Overall, our VSPW totally provides 3,536 annotated videos, including 251,633 frames from 124 categories. La-beling such a large-scale dataset for video scene parsing is very challenging, e.g., time-consuming, expensive, and hard to keep category consistency across the whole video, which may be the main reason that prevents the video scene parsing from being well studied till now. To efﬁciently and accurately facilitate the annotation process, we develop a human-computer collaboration scheme, which can not only signiﬁcantly reduce the human effort but also guarantee high-quality annotation masks.
Based on our VSPW, we further propose a simple end-to-end Temporal Context Blending (TCB) network. Our
TCB enables the network to harness long-range contex-tual information from previous frames to help segment the current one, which effectively alleviates those false predictions caused by motion blur, view and scale varia-tions, etc. Extensive experiments on cutting-edge image-based [72, 9, 69, 62] and video-based [20, 44] segmentation methods are evaluated as strong baselines. Compared with these baselines, our TCB network shows its advantages in terms of both segmentation performance and temporal sta-bility. We hope our VSPW can signiﬁcantly motivate more researchers to develop efﬁcient & accurate algorithms and help ease the future research of video scene parsing. 2.