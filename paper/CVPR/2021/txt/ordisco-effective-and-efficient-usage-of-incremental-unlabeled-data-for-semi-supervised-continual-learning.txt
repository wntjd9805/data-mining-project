Abstract
Continual learning usually assumes the incoming data are fully labeled, which might not be applicable in real ap-plications. In this work, we consider semi-supervised con-tinual learning (SSCL) that incrementally learns from par-tially labeled data. Observing that existing continual learn-ing methods lack the ability to continually exploit the unla-beled data, we propose deep Online Replay with Discrim-inator Consistency (ORDisCo) to interdependently learn a classiﬁer with a conditional generative adversarial network (GAN), which continually passes the learned data distribu-tion to the classiﬁer. In particular, ORDisCo replays data sampled from the conditional generator to the classiﬁer in an online manner, exploiting unlabeled data in a time- and storage-efﬁcient way. Further, to explicitly overcome the catastrophic forgetting of unlabeled data, we selectively stabilize parameters of the discriminator that are impor-tant for discriminating the pairs of old unlabeled data and their pseudo-labels predicted by the classiﬁer. We exten-sively evaluate ORDisCo on various semi-supervised learn-ing benchmark datasets for SSCL, and show that ORDisCo achieves signiﬁcant performance improvement on SVHN,
CIFAR10 and Tiny-ImageNet, compared to strong base-lines. 1.

Introduction
Current achievements of deep neural networks (DNNs) heavily rely on large amounts of supervised data, which are expensive and difﬁcult to acquire simultaneously. There-fore, the ability of continual learning (CL) on incremental training samples becomes extremely important. Numerous efforts have been devoted to CL, which aim to continually learn new training samples without catastrophic forgetting of the learned data distribution [29]. Existing CL methods mainly fall in two categories: weight regularization meth-*Corresponding author: C. Li and J. Zhu. ods [29, 11, 1] and replay-based methods [29, 32, 35], and have achieved promising results in purely supervised set-tings.
In many real-world applications, nevertheless, the incre-mental data are often partially labeled. For example, in face recognition [33], a device continually obtains user data for unlocking. These increasing data could be used to update the model for better user experience. However, true labels of the incoming data are usually unavailable unless the user provides the password for veriﬁcation. Since frequently asking for labelling would affect user experience, most of the input data are unlabeled. Similar scenarios occur in
ﬁngerprint identiﬁcation [46] and video recognition [20].
Though such scenarios are common in our daily life, they are seldom studied in the CL literature. Therefore, in this paper, we focus on the challenging and realistic task that continually learns incremental partially labeled data. For simplicity, we refer to it as semi-supervised continual learn-ing (SSCL).
Different from supervised CL, SSCL provides insufﬁ-cient supervision and a large amount of unlabeled data. As is well known, unlabeled data are crucial in semi-supervised scenarios [48] but are massive to exploit. In fact, we con-duct preliminary experiments in SSCL and empirically ver-ify that the representative CL methods including the weight regularization methods [29, 11, 1] and the replay-based methods [29, 32, 35] may not effectively exploit the unla-beled data. Speciﬁcally, the joint training of a strong semi-supervised classiﬁer signiﬁcantly outperforms the best of existing CL strategies on the same classiﬁer (see results in
Fig. 1 and Appendix A). We identify this as the catastrophic forgetting of unlabeled data problem in SSCL.
To this end, we present deep online replay with discrim-inator consistency (ORDisCo), a framework to continually learn a semi-supervised classiﬁer and a conditional gener-ative adversarial networks (GAN) [23] together in SSCL.
Speciﬁcally, ORDisCo is formulated as a minimax adver-sarial game [7], where a generator tries to capture the un-derlying joint distribution of partially labeled data, and help 5383
the classiﬁer make accurate predictions. At the same time, the classiﬁer also predicts pseudo-labels for unlabeled data to improve the training of the conditional generator. In con-trast to previous work [35, 43, 28], ORDisCo replays data sampled from the conditional generator to the classiﬁer in an online manner, which is time- and storage-efﬁcient to exploit the large amount of unlabeled data in SSCL. Fur-ther, to explicitly overcome the catastrophic forgetting of unlabeled data, we selectively stabilize parameters of the discriminator that are important for discriminating the pairs of old unlabeled data and their pseudo-labels predicted by the classiﬁer.
We follow the New Instance and New Class scenarios of
CL [29] to split commonly used SSL benchmark datasets
[16, 27], including SVHN, CIFAR10 and Tiny-ImageNet, as the evaluation benchmarks for SSCL. To simulate the practical scenarios, all methods continually receive a batch of data with a few labels during training. Extensive evalua-tions on such benchmarks show that ORDisCo can signiﬁ-cantly outperform strong baselines in SSCL.
In summary, our contributions include: (i) We consider a realistic yet challenging task called semi-supervised con-tinue learning (SSCL). We provide a systematical study of existing CL strategies and show that the catastrophic for-getting of unlabeled data is the key challenge in SSCL. (ii) We present ORDisCo to continually learn a classiﬁer and a conditional GAN in SSCL. The generator replays data in an online manner with a consistency regularization on the discriminator to address the catastrophic forgetting of unlabeled data; (iii) We evaluate ORDisCO on various benchmarks and demonstrate that ORDisCO signiﬁcantly improves both classiﬁcation and conditional image genera-tion over strong baselines in SSCL. 2.