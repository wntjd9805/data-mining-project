Abstract
Point clouds produced by 3D scanning are often sparse, non-uniform, and noisy. Recent upsampling approaches aim to generate a dense point set, while achieving both distribution uniformity and proximity-to-surface, and pos-sibly amending small holes, all in a single network. Af-ter revisiting the task, we propose to disentangle the task based on its multi-objective nature and formulate two cas-caded sub-networks, a dense generator and a spatial re-ﬁner. The dense generator infers a coarse but dense out-put that roughly describes the underlying surface, while the spatial reﬁner further ﬁne-tunes the coarse output by ad-justing the location of each point. Speciﬁcally, we design a pair of local and global reﬁnement units in the spatial reﬁner to evolve a coarse feature map. Also, in the spa-tial reﬁner, we regress a per-point offset vector to further adjust the coarse outputs in ﬁne scale. Extensive quali-tative and quantitative results on both synthetic and real-scanned datasets demonstrate the superiority of our method over the state-of-the-arts. The code is publicly available at https://github.com/liruihui/Dis-PU . 1.

Introduction
Point clouds, as a compact representation of 3D data, are widely explored by both traditional and deep-learning-based methods for many applications [6, 4, 18], e.g., self-∗corresponding author driving cars, robotics, rendering, and medical analysis, etc.
However, raw point clouds produced by 3D scanning are often locally sparse and non-uniform, possibly with small holes on the object surface; see a real-scanned example shown on the top of Figure 1(a). Obviously, we need to amend such raw data, before we can effectively use it for rendering, analysis, or general processing.
The goal of point cloud upsampling is not limited to gen-erating a dense point set from the sparse input. Very impor-tantly, the generated points should also faithfully locate on the underlying surface and cover the surface with a uniform distribution. As an inference-based problem, these goals are very demanding, due to the limited information available in the sparse input. Besides being sparse, the input points can be non-uniform and noisy, and they may not well represent
ﬁne structures (if any) on the underlying surface.
Beneﬁted from data-driven machine learning and deep neural network models, several deep-learning-based meth-ods [28, 27, 26, 12, 21] have been proposed for point cloud upsampling and they demonstrated superior performance, compared with traditional methods [1, 17, 10]. The general approach taken in existing learning-based methods is that they ﬁrst design an upsampling module to expand the num-ber of points in the feature space, then formulate losses to constrain the output points for the distribution uniformity and proximity-to-surface. In other words, the designed up-sampling module is expected not only to infer and gener-ate dense points from the sparse input, but also to produce points that are uniform, clean, and faithfully located on the 344
underlying surface. However, it is very hard for a network to meet all the requirements at the same time. Therefore, the dense points produced by existing works still tend to be non-uniform or retain excessive noise (see the top results in Figures 1 (b) & (c)), thus resulting in low-quality recon-structed meshes (see results in the bottom row).
After revisiting the point cloud upsampling task, we pro-pose to disentangle the task into two sub-goals: (i) to ﬁrst generate a coarse but dense point set with less details to roughly describe the underlying surface, and then (ii) to re-ﬁne the coarse points to better cover the underlying surface for distribution uniformity and proximity-to-surface. To do so, we formulate an end-to-end disentangled reﬁnement framework, which consists of two cascaded sub-networks, a dense generator and a spatial reﬁner, which are designed to aim for sub-goals (i) and (ii), respectively. Particularly, we design the spatial reﬁner with a pair of local and global reﬁnement units to evolve the coarse feature map inside the reﬁner to take into account both the local and global geo-metric structures. Further, inspired by the residual-learning strategy [8], we formulate the spatial reﬁner to regress a per-point offset vector for ﬁne-tuning the coarse outputs by ad-justing the location of each point. Compared with directly predicting the ﬁnal reﬁned 3D point coordinates, regressing a small residual is much easier for the network.
Compared with current upsampling methods [28, 26, 12], our disentangled reﬁnement pipeline assigns lower re-quirements to each sub-network, so that both the dense generator and the spatial reﬁner could be more focused on their own sub-goals. In addition, the cascading scheme al-lows the two sub-networks to complement each other dur-ing network learning, thus leading to a substantial perfor-mance boost; see Figure 1(d). Extensive experimental re-sults demonstrate that our method outperforms others on both real-scanned and synthetic inputs. 2.