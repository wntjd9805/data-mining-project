Abstract
Baseline
Baseline+AugQ
Baseline+AugS
Few-shot object detection (FSOD) aims to learn detec-tors that can be generalized to novel classes with only a few instances. Unlike previous attempts that exploit meta-learning techniques to facilitate FSOD, this work tackles the problem from the perspective of sample expansion. To this end, we propose a simple yet effective Transforma-tion Invariant Principle (TIP) that can be ﬂexibly applied to various meta-learning models for boosting the detection performance on novel class objects. Speciﬁcally, by intro-ducing consistency regularization on predictions from var-ious transformed images, we augment vanilla FSOD mod-els with the generalization ability to objects perturbed by various transformation, such as occlusion and noise. Im-portantly, our approach can extend supervised FSOD mod-els to naturally cope with unlabeled data, thus addressing a more practical and challenging semi-supervised FSOD problem. Extensive experiments on PASCAL VOC and
MSCOCO datasets demonstrate the effectiveness of our TIP under both of the two FSOD settings. 1.

Introduction
While the availability of large number of labeled data has enabled deep neural networks to dominate the computer vision community. they struggle in addressing problems with scarce labeled data [6, 15]. In contrast, humans can rapidly learn new concepts with only a few examples avail-able. This big gap between humans and deep neural net-works provides fertile ground for developing deep learning techniques. Due to this fact, few-shot learning, which learns algorithms that allow for better generalization on tasks with a few labeled training samples, has become topical. Differ-ent from most previous works designed for few-shot classi-ﬁcation, we focus on a more challenging and more practical case – few-shot object detection (FSOD) [5, 9, 2]. Speciﬁ-cally, given a set of base classes with rich labeled data per class and a set of novel class with a few labeled data per class, FSOD aims to learn a model to detect objects from both base and novel classes. t o h
S f o r e b m u
N 10 5 3 2 1 20 25 30 35 40 45 50 55 60
Mean Average Precision (%)
Figure 1. Results with naive data augmentation. The evaluation metrics are the 1, 2, 3, 5, 10-shot detection performance (i.e., mean
Average Precision, mAP) on the ﬁrst novel class set of PASCAL
VOC dataset. Notations: ‘Baseline’ – A meta-learning-based ap-proach [23] that has achieved the state-of-the-art results; ‘Baseline
+ AugS’ – Augmenting support images when training the base-line model. ‘Baseline + AugQ’ – Augmenting query images when training the baseline model.
Recent progress of FSOD has featured meta-learning strategy [10, 24, 23].
It uses a pool of auxiliary detec-tion tasks generated from base class training set to perform transfer learning to novel class tasks with only a few exam-ples available. Here, each auxiliary task is constructed to simulate the few-shot scenario: given a small training set (called support set) with a labeled instance per class, and a small test set (called query set), a meta learner trains the tar-get detector in a guided manner: For each class, its support sample is used to extract class-wise representative features and embedded into a guidance vector. Then the guidance vector is incorporated into query feature learning to facili-tate the query sample features suitable for detecting objects of the target class.
Orthogonal to the design of meta-learning strategy, we tackle this challenging FSOD problem from the perspec-tive of sample expansion and further improve its perfor-mance. A naive solution is to involve transformed images 13094    
TSNE Visualization of 
Guidance Vectors
Bicycle
Train
Bus
Figure 2. The TSNE Visualization of guidance vectors obtained by naive data augmentation solution. In this ﬁgure, guidance vec-tors from different classes are illustrated in different colors. The guidance vectors generated by using transformed images are de-noted by hollow circles, while the guidance vectors of original images are denoted by solid circles. These original images and its transformed variants are also provided. The guidance vectors of transformed images generated by using the same original im-age are shown to be separable in the guidance embedding space.
This means that the learned guidance vectors didn’t effectively en-code the representative features which should be invariant to image transformation. into the training process. However, it is impressive that sim-ply adding the data augmentation techniques leads to very limited improvement or even performance drop, as shown in Figure 1. To analysis this phenomenon, we provide a
TSNE visualization of the guidance vectors extracted by us-ing transformed images, as shown in Figure 2. Here, guid-ance vectors from different classes are illustrated in differ-ent colors. The guidance vectors of original images are de-noted by solid circles, while the guidance vectors of their transformed images are denoted by hollow circles. These original images and their transformed variants are also pro-vided. We can observe that guidance vectors of transformed images generated by using the same original image are sep-arable in the guidance embedding space. This means that the learned guidance vectors didn’t effectively encode the class-wise representative features which should be invariant to image transformation.
To overcome this issue, a novel approach named by
Transformation Invariant Principle (TIP) is proposed. The
TIP applies consistency regularization on guidance vectors from various transformed images to provide additional su-pervision for guidance learning. As illustrated in Figure 3, the TIP for guidance extraction branch is implemented by adding a Transformed Guidance Consistency (TGC) Loss on the top of the guidance vectors of original images and their transformed variants. The TGC Loss computes the dif-ference between guidance vectors generated from an origi-nal image and its transformed variants. Moreover, the TIP introduces the proposal consistent regularization into query image prediction to generate transformation invariant query features. This is implemented as a proposal detection net-work that takes transformed images as inputs and outputs suitable Region of Interest (RoI) proposals for their orig-inal images. The prediction of bounding boxes is condi-tioned on these RoI proposals and the transformation in-variant guidance vectors learned by TGC Loss. The pro-posed TIP can be used to cope with unlabeled images and thus extend our approach to a more realistic yet more chal-lenging scenario, i.e. semi-supervised FSOD. In this way, both fully-supervised and semi-supervised FSOD problems can be handled in a uniﬁed detection framework. Exper-imental results on PASCAL VOC and MSCOCO datasets demonstrate that our approach is effective for both of the two FSOD settings.
In summary, our contributions are three folds:
• To the best of our knowledge, this is the ﬁrst work to address the challenging FSOD problem from the per-spective of sample expansion.
• We propose a simple yet effective approach, named by
TIP, to improve the generalization ability over trans-formed images, and experimental results demonstrate that our method achieves state-of-the-art results on two benchmark datasets.
• Our approach can be easily extended to a more realistic yet more challenging semi-supervised FSOD scenario, with superior performance obtained. This further vali-dates the effectiveness of the proposed approach. 2.