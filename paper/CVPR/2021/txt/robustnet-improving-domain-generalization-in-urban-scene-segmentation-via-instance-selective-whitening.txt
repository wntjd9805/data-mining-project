Abstract 1.

Introduction
Enhancing the generalization capability of deep neural networks to unseen domains is crucial for safety-critical applications in the real world such as autonomous driv-ing. To address this issue, this paper proposes a novel in-stance selective whitening loss to improve the robustness of the segmentation networks for unseen domains. Our ap-proach disentangles the domain-speciﬁc style and domain-invariant content encoded in higher-order statistics (i.e., feature covariance) of the feature representations and se-lectively removes only the style information causing domain shift. As shown in Fig. 1, our method provides reasonable predictions for (a) low-illuminated, (b) rainy, and (c) un-seen structures. These types of images are not included in the training dataset, where the baseline shows a signiﬁcant performance drop, contrary to ours. Being simple yet effec-tive, our approach improves the robustness of various back-bone networks without additional computational cost. We conduct extensive experiments in urban-scene segmentation and show the superiority of our approach to existing work.
Our code is available at this link1.
* indicates equal contribution 1 https://github.com/shachoi/RobustNet.
When deploying deep neural networks (DNNs) trained on a given dataset (i.e., source domain) in real-world unseen data (i.e., target domain), DNNs often fail to perform prop-erly due to the domain shift. Overcoming this issue is cru-cial, especially for safety-critical applications such as au-tonomous driving. In particular, real-world data consist of unexpected and unseen samples, for example, those images taken under diverse illumination, adverse weather condi-tions, or from different locations. It is generally impossible to model such a full data distribution with limited training data, so reducing the domain gap between source and tar-get domains has been a long-standing problem in computer vision.
Domain adaptation (DA) is an approach to mitigate the performance degradation caused by such a domain gap [3, 12, 18, 11, 65, 39, 58, 41, 50]. Generally, DA focuses on adapting the source domain distribution to that of the target domain, but it requires access to the samples in the target domain, which limits their applicability. When we set the entire real world as a target domain, it is difﬁcult in pactice to obtain data samples that fully cover the target domain.
Domain generalization (DG) overcomes this limitation 11580
loss that alleviates the limitations of the existing whitening transformation for domain generalization, by selectively re-moving information that causes a domain shift while main-taining a discriminative power of feature within DNNs. Our method does not rely on an explicit closed-form whiten-ing transformation, but implicitly encourage the networks to learn such a whitening transformation through the proposed loss function, thus requiring negligible computational cost.
As illustrated in Fig. 2, our method selectively removes only those feature covariances that respond sensitively to pho-tometric augmentation such as color transformation. Our experiments on urban-scene segmentation in DG settings, performed using several backbone networks, show evidence that our approach consistently boosts the DG performance.
The main contributions include the following:
• We propose an instance selective whitening loss for domain generalization, which disentangles domain-speciﬁc and domain-invariant properties from higher-order statistics of the feature representation and selec-tively suppresses domain-speciﬁc ones.
• Our proposed loss can easily be used in existing mod-els and signiﬁcantly improves the generalization abil-ity with negligible computational cost.
• We apply the proposed loss to urban-scene segmen-tation in a DG setting and show the superiority of our approach over existing approaches in both a qualitative and quantitative manner. 2.