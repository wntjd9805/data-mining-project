Abstract 1.

Introduction
The combination of traditional rendering with neural networks in Deferred Neural Rendering (DNR) [38] pro-vides a compelling balance between computational com-plexity and realism of the resulting images. Using skinned meshes for rendering articulating objects is a natural ex-tension for the DNR framework and would open it up to a plethora of applications. However, in this case the neural shading step must account for deformations that are pos-sibly not captured in the mesh, as well as alignment in-accuracies and dynamics—which can confound the DNR pipeline. We present Articulated Neural Rendering (ANR), a novel framework based on DNR which explicitly addresses its limitations for virtual human avatars. We show the su-periority of ANR not only with respect to DNR but also with methods specialized for avatar creation and anima-In two user studies, we observe a clear preference tion. for our avatar model and we demonstrate state-of-the-art performance on quantitative evaluation metrics. Perceptu-ally, we observe better temporal stability, level of detail and plausibility. More results are available at our project page: https://anr-avatars.github.io.
Capturing realistic appearance is one of the important goals of computer vision. Progress in 3D rendering and neural networks has led to approaches with remarkable ﬁ-delity [22, 23, 29, 30]. These methods often use expensive and intricate capture setups which prevent easy digitization and transfer of the resulting models [7, 8, 11]. The recent deferred neural rendering paradigm offers an exciting op-portunity to work with inaccurate geometry and relatively simple neural shaders while capturing complex scenes with view-dependent effects realistically [1, 27, 38].
In a ﬁrst step, the geometry is rasterized using a neural latent texture which is then translated to an RGB image using a convolu-tional network. Both, the rendering network as well as the neural texture, are optimized to produce realistic results.
Deferred neural rendering works particularly well for rigid objects. Its pipeline could be extended to deformable objects in a natural way: a skinned mesh could be used for capturing the geometry. The rasterized neural texture from the posed mesh could then be translated to an RGB image.
While this idea is conceptually simple, the neural network has to learn more complex deformation-dependent effects.
Furthermore, the mesh used for rendering is usually not a 3722
Figure 2: Schematic overview of the proposed framework. Given a coarse, animated 3D body mesh, ANR produces a detailed avatar. Using rasterized IUV images of the mesh using a weak perspective projection, we render an 8 channel neural texture into image space. A ﬁrst stage, R1, transforms the texture into another, reﬁned latent representation, which we combine with the normal information. The second stage R2 uses this information to create an RGB rendering and a foreground mask. The rendering can extend beyond the coarse mesh, in this case we visualize only slight reﬁnement for painting the shirt. perfect representation of the real geometry, leading to align-ment problems. These problems are currently not taken into account [1, 27, 38], which limits the application of DNR in scenarios with deformable objects.
We present Articulated Neural Rendering (ANR) to ac-count for these problems. ANR systematically rebuilds
DNR from the neural shading model architecture to the op-timization scheme. We use ANR to tackle one of the most challenging problems for animation: virtual human avatars.
Fig. 1 shows an example of an avatar rendered using ANR.
Concretely, ANR employs a simple statistical human body model ﬁtted to a training video to capture the body shape statistics and 3D pose information for each frame [39]. This body model only represents the coarse body geometry without clothing and hair. Consequently, di-rect use of the DNR pipeline leads to unrealistic and blurry results. We use keyframes from the video to learn the static appearance encoded in the neural texture, and use the other frames to learn the dynamic pose-conditioned rendering of the appearance. Our keyframes-based training scheme en-ables the model to converge 5X faster and produces quan-titatively better avatars than DNR. We simultaneously train
ANR on multiple identities in a single model, leading to de-coupling of the neural texture and the shading model. Ow-ing to the consistent surface parameterization of the statisti-cal body model, our model can leverage such semantic cor-respondences to modify and mix components from multi-ple neural textures, enabling virtual try-on by changing re-gions in the neural texture. While our model works solely in 2D, we experimentally validate that it can render near photorealistic and persistent 3D appearance of people with a very small network (161M parameters). In two user stud-ies, we demonstrate that we not only outperform the DNR pipeline, but also several methods dedicated to creating vir-tual avatars [36, 41]. Perceptually, the presented method is temporally stable and captures ﬁne appearance details.
Our contributions are threefold. First, we present ANR, a novel neural rendering framework, to generate high-quality virtual avatars from coarse 3D shape and arbitrary skeletal motions. Our key is to account for geometric misalignment of the coarse body mesh and pose-dependent deformation.
Second, we showcase ANR as the ﬁrst neural avatar model that can capture and render multiple identities with only one set of network parameters in addition to an identity speciﬁc neural texture map. Third, we demonstrate that ANR al-lows easy appearance editing or mixing of identities. This is novel in the context of neural rendering for avatars. 2.