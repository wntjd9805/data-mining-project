Abstract
Differentiable Architecture Search (DARTS) has at-tracted extensive attention due to its efﬁciency in searching for cell structures. DARTS mainly focuses on the opera-tion search and derives the cell topology from the operation weights. However, the operation weights can not indicate the importance of cell topology and result in poor topology rating correctness. To tackle this, we propose to Decouple the Operation and Topology Search (DOTS), which decou-ples the topology representation from operation weights and makes an explicit topology search. DOTS is achieved by introducing a topology search space that contains combi-nations of candidate edges. The proposed search space directly reﬂects the search objective and can be easily ex-tended to support a ﬂexible number of edges in the searched cell. Existing gradient-based NAS methods can be incor-porated into DOTS for further improvement by the topol-ogy search. Considering that some operations (e.g., Skip-Connection) can affect the topology, we propose a group operation search scheme to preserve topology-related oper-ations for a better topology search. The experiments on CI-FAR10/100 and ImageNet demonstrate that DOTS is an ef-fective solution for differentiable NAS. The code is released at https://github.com/guyuchao/DOTS. 1.

Introduction
Neural Architecture Search (NAS) has attracted exten-sive attention for its potential to ﬁnd the optimal architec-ture in a large search space automatically. Previous re-inforcement learning and evolutionary learning based ap-proaches [34, 41, 58] require a full training process to val-idate the architecture performance, consuming hundreds of
GPU-days to search. To reduce the search cost, one-shot methods [8, 10, 18, 40] adopt the weight sharing strategy, which trains the supernet once and derives child architec-∗Both authors contributed equally to this work.
†M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author. ture performance from the supernet directly. Recent meth-ods [4, 6, 50, 51] based on differentiable architecture search (DARTS) [35] also adopt the weight sharing strategy and further reduce the search cost by unifying the supernet train-ing and child architecture searching.
In DARTS, the operation selection is parameterized with learnable operation weights, which is updated with the su-pernet training. After training, the operation weights are used to rank the importance of operations and topology. The edge importance in DARTS is represented as the largest op-eration weight on this edge. DARTS retains the two most important edges for each intermediate node to derive the topology of the searched cell. A question is raised: whether the edge importance indicated by the operation weights ac-curately ranks the stand-alone model’s performance. As illustrated in Fig. 1, we ﬁnd no obvious rank correlation, which implies that DARTS has no superiority over choos-ing edges randomly (see more details in Sec. 3.2). Further-more, DARTS’ handcraft policy of edge numbers restricts their potential to ﬁnd more ﬂexible cell structures.
This paper addresses the above problems via Decoupling
Operation and Topology Search (DOTS). The meaning of decoupling is two-fold. On the one hand, we decouple the topology representation from the operation weights. In de-tail, we introduce a topology search space containing the pairwise combinations of edges. The topology search space is continuously relaxed, and the relaxed topology weights model the combinatorial distribution of candidate edges.
The proposed topology search space directly reﬂects the search objective and can be easily extended to support a
ﬂexible number of edges. On the other hand, we decou-ple the operation and topology search processes. The over-all search process is divided into the operation search stage and the topology search stage, in which we update operation weights and edge combination weights, respectively. With decoupling the two searching processes, existing gradient-based NAS methods can be directly incorporated into the
DOTS’ operation search and get further improvement by the topology search. Furthermore, the topology search is performed in a shrunk supernet, making it more efﬁcient 12311
(a) Operation Weight, CIFAR10 (b) Edge Combination Weight, CIFAR10 (c) Operation Weight, CIFAR100 (d) Edge Combination Weight, CIFAR100
Figure 1: Rank correlation analysis between different edge importance representations and the stand-alone model per-formance. The edge combination importance is indicated by operation weights (DARTS) and edge combination weights (DOTS). We calculate the Kendall Tau metric [25] to measure the rank correlation. and accurate. Considering that some operations (e.g., Skip-Connection) can affect the topology, we adopt a group strat-egy in the operation search to preserve these topology-related operations for a better topology search.
We summarize our contributions as follows:
• We propose to decouple the operation and topology search, which decouples both the topology represen-tation and search processes. Such decoupling leads to the correct rating of stand-alone models with different topologies.
• The proposed topology search space can be extended to support a ﬂexible number of edges in the searched cell, fulﬁlling its potential to search for more complex structures.
• Existing gradient-based methods can be incorporated into DOTS and get further improvement by the topol-ogy search. DOTS only costs 0.26 and 1.3 GPU-days to search from scratch and achieves 97.51% and 76.0% accuracy on CIFAR10 and ImageNet. Better perfor-mance can be achieved if the constraint of edge num-bers is removed. 2.