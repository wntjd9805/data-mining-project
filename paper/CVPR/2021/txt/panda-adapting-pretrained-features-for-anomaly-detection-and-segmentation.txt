Abstract
Anomaly detection methods require high-quality fea-tures.
In recent years, the anomaly detection community has attempted to obtain better features using advances in deep self-supervised feature learning. Surprisingly, a very promising direction, using pre-trained deep features, has been mostly overlooked. In this paper, we ﬁrst empirically establish the perhaps expected, but unreported result, that combining pre-trained features with simple anomaly detec-tion and segmentation methods convincingly outperforms, much more complex, state-of-the-art methods.
In order to obtain further performance gains in anomaly detection, we adapt pre-trained features to the target distri-bution. Although transfer learning methods are well estab-lished in multi-class classiﬁcation problems, the one-class classiﬁcation (OCC) setting is not as well explored. It turns out that naive adaptation methods, which typically work well in supervised learning, often result in catastrophic col-lapse (feature deterioration) and reduce performance in
OCC settings. A popular OCC method, DeepSVDD, ad-vocates using specialized architectures, but this limits the adaptation performance gain. We propose two methods for combating collapse: i) a variant of early stopping that dynamically learns the stopping iteration ii) elastic reg-ularization inspired by continual learning. Our method,
PANDA, outperforms the state-of-the-art in the OCC, out-lier exposure and anomaly segmentation settings by large margins1. 1.

Introduction
Detecting anomalous patterns in data is of key impor-tance in science and industry. In the computational anomaly detection task, the learner observes a set of training exam-ples. The learner is then tasked to classify novel test sam-ples as normal or anomalous. There are multiple anomaly detection settings investigated in the literature, correspond-ing to different training conditions. In this work, we deal
*Equal contribution 1The code is available at github.com/talreiss/PANDA with three settings: i) anomaly detection - when only nor-mal images are used for training ii) anomaly segmentation
- detecting all the pixels that contain anomalies, given nor-mal images as input. iii) Outlier Exposure (OE) - where an external dataset simulating the anomalies is available.
In recent years, deep learning methods have been intro-duced for anomaly detection, typically extending classical methods with deep neural networks. Different auxiliary tasks (e.g. autoencoders or rotation classiﬁcation) are used to learn representations of the data, while a great variety of anomaly criteria are then used to determine if a given sam-ple is normal or anomalous. An important issue for current methods is the reliance on limited normal training data for representation learning, which limits the quality of learned representations. Nearly all state-of-the-art anomaly detec-tion methods rely on self-supervised feature learning - i.e. using the limited normal training data for learning strong features. The motivation for this is twofold: i) the fear that features trained on auxiliary domains will not generalize well to the target domain. ii) the curiosity to investigate the top performance achievable without ever looking at any external dataset (we do not address this question here).
In other parts of computer vision, features pre-trained on external datasets are often used to improve performance on tasks trained on new domains - and our reasonable hypoth-esis is that this should also be the case for image anomaly detection and segmentation. We present very simple base-lines that use pretrained features trained on a large external data and K-nearest neighbor (kNN) retrieval to signiﬁcantly outperform all previous methods on anomaly detection and segmentation, even on images of distant target domains.
We then tackle the technical challenge of obtaining stronger performance by further adaptation to the normal training data. Although feature adaptation has been ex-tensively researched in the multi-class classiﬁcation setting, limited work was done in the OCC setting. Unfortunately, it turns out that feature adaptation for anomaly detection often suffers from catastrophic collapse - a form of deterioration of the pre-trained features, where all (including anomalous) samples, are mapped to the same point. DeepSVDD [23] proposed to overcome collapse by removing biases from the 2806
model architecture, but this restricts network expressively and limits the pre-trained models that can be borrowed off-the-shelf. Perera and Patel [21] proposed to jointly train
OCC with the original task which has several limitations and achieves only limited adaptation success.
Our ﬁrst ﬁnding is that simple training with constant-duration early stopping (with no bells-and-whistles) already achieves top performance. To remove the dependence on the number of epochs for early stopping, we propose two techniques to overcome catastrophic collapse: i) an adap-tive early stopping method that selects the stopping iter-ation per-sample, using a novel generalization criterion -this technique is designed to overcome a special problem of
OCC, namely that there are no anomalies in the validation set ii) elastic regularization, motivated by continual learn-ing, that postpones the collapse. Thorough experiments demonstrate that we outperform the state-of-the-art by a wide margin (ROCAUC): e.g. CIFAR10 results: 96.2% vs. 90.1% without outlier exposure and 98.9% vs. 95.6% with outlier exposure. We also achieve 96.0% vs. 89.0% on anomaly segmentation on MVTec.
We present insightful critical analyses: i) We show that pre-trained features strictly dominate current self-supervised RotNet-based feature learning methods. We discuss the relative merits of each paradigm and conclude that for most practical purposes, using pre-trained features is preferable. ii) We analyse the results of the popular
DeepSVDD method and discover that its feature adapta-tion, which is designed to prevent collapse, does not im-prove over simple data whitening.
Contributions: To summarize our main ocntributions in this paper:
• Demonstrating that a simple baseline outperforms all current methods in image anomaly detection and seg-mentation - extensive analysis shows the generality of the result.
• Identifying that popular SOTA methods do not outper-form linear whitening in OCC feature adaptation.
• Proposing several effective solutions for feature adap-tation for OCC.
• Extensive evaluation, obtaining results that signiﬁ-cantly improve over the current state-of-the-art. 1.1.