Abstract
We address the problem of localizing a speciﬁc moment described by a natural language query. Existing works in-teract the query with either video frame or moment pro-posal, and neglect the inherent structure of moment con-struction for both cross-modal understanding and video content comprehension, which are the two crucial chal-lenges for this task. In this paper, we disentangle the ac-tivity moment into boundary and content. Based on the explored moment structure, we propose a novel Structured
Multi-level Interaction Network (SMIN) to tackle this prob-lem through multi-levels of cross-modal interaction coupled with content-boundary-moment interaction. In particular, for cross-modal interaction, we interact the sentence-level query with the whole moment while interacting the word-level query with content and boundary, as in a coarse-to-ﬁne manner. For content-boundary-moment interaction, we capture the insightful relations between boundary, con-tent, and the whole moment proposal. Through multi-level interactions, the model obtains robust cross-modal repre-sentation for accurate moment localization. Extensive ex-periments conducted on three benchmarks (i.e., Charades-STA, ActivityNet-Captions, and TACoS) demonstrate the proposed approach outperforms the state-of-the-art meth-ods. 1.

Introduction
With the wide popularity of online videos, automati-cally understanding and analyzing the video content has drawn increasing attention. Recently, due to the limita-tion of the pre-deﬁned action categories and the ﬂexibility of using a natural language sentence for describing the ac-tivity in videos, video moment localization is proposed in the works [1, 7]. Its aim is to localize a temporary segment from an untrimmed video, containing the activity described
*Corresponding author. (a): An example of video moment localization queried
Figure 1. by a natural language sentence. (b): Existing approaches simply conduct vision-language interaction at frame-level or proposal-level. (c): Our method ﬁrst fuses the sentence with the video frame as a coarse-grained cross-modal interaction. Then we con-duct ﬁne-grained vision-language interaction between words and boundary/content of a moment proposal. Meanwhile, we conduct structured moment interaction to explore the relations between boundary, content, and the moment. (Best viewed in color) by the given language query.
For this task, there are two key challenges: (1) cross-modal understanding between the language query and com-plicated video content, and (2) elaborate video content com-prehension for localizing the target moment in videos with complex backgrounds. Existing works locate the moment by interacting the query with either video frame represen-tation [4, 5, 6, 10, 12, 16, 33, 42, 45], or moment proposal representation [1, 7, 9, 19, 20, 43, 44]. For vision-language interaction, these works neglect the inherent structure of 7026
the moment that one is constructed by content and bound-ary. For video content comprehension, they directly uti-lize the features of video frame or proposal as the moment representation. These methods miss the discriminative in-formation contained in moment boundary and content and the ﬁne-grained correlation between them to query; there-fore, they predict coarse boundary and misaligned moment.
In fact, moment content, boundary, and the whole moment have different representation ability to represent a moment, and thus it is non-trivial to (1) perform cross-modal inter-action between them and language query respectively for comprehensive cross-modal understanding; (2) conduct the content-boundary-moment interaction (termed as structured moment interaction in this work) for elaborate video content comprehension.
Motivated by the above observations, this paper proposes a novel Structured Multi-level Interaction Network (SMIN) for video moment localization by incorporating multiple levels of vision-language interaction and moment structured interaction into a joint procedure. First, we design the multi-ple levels of vision-language interaction for detailed vision-language understanding. As illustrated in Figure 1(b)(c), in contrast to previous works that simply use frame-level or proposal-level interaction to fuse video and query, we lever-age the inherent moment structure and introduce a coarse-to-ﬁne cross-modal interaction. In detail, the coarse-grained sentence representation is interacted with the video frame in the backbone before proposal generation, while the ﬁne-grained word representation is interacted with boundary and content separately. Second, based on the inherent struc-ture of a moment, we introduce the structured moment in-teraction by exploiting the structural relationships between content, boundary, and the whole moment. This interac-tion helps perform the elaborate video comprehension. Fi-nally, we build the content unit, boundary unit, and mo-ment unit for incorporating the multi-level cross-modal in-teraction and structured moment interaction as a structured multi-level interaction procedure to extract robust moment representation for accurate moment localization.
To summarize, our contributions are as follows:
• We disentangle the inherent structure of moment that one is constructed with boundary and content, and leverage this structure for comprehensive vision-language understanding and elaborate video compre-hension.
• We propose a novel Structured Multi-level Interaction
Network (SMIN) to incorporate ﬁne-grained cross-modal interaction and detailed structured moment in-teraction into a joint procedure with the disentangled moment structure.
• We conduct experiments on three popular benchmarks to verify the effectiveness of our approach, which per-forms superior to the state-of-the-art methods. 2.