Abstract
Knowledge distillation aims to transfer representation ability from a teacher model to a student model. Previous approaches focus on either individual representation dis-tillation or inter-sample similarity preservation. While we argue that the inter-sample relation conveys abundant in-formation and needs to be distilled in a more effective way.
In this paper, we propose a novel knowledge distillation method, namely Complementary Relation Contrastive Dis-tillation (CRCD), to transfer the structural knowledge from the teacher to the student. Speciﬁcally, we estimate the mu-tual relation in an anchor-based way and distill the anchor-student relation under the supervision of its corresponding anchor-teacher relation. To make it more robust, mutual relations are modeled by two complementary elements: the feature and its gradient. Furthermore, the low bound of mu-tual information between the anchor-teacher relation distri-bution and the anchor-student relation distribution is max-imized via relation contrastive loss, which can distill both the sample representation and the inter-sample relations.
Experiments on different benchmarks demonstrate the ef-fectiveness of our proposed CRCD. 1.

Introduction
Knowledge distillation aims to transfer the knowledge from one deep learning model (the teacher) to another (the student), such as distilling a large network into a smaller one [19, 49, 2, 48, 12] or ensembling a collection of models into a single model [29, 37, 27, 45]. It has a wide range of applications in the industry especially when a neural net-work needs to be efﬁciently deployed on devices with lim-ited computational resources [9, 54, 38]. Although great progress has been achieved in the knowledge distillation
†This work was done when Jinguo Zhu was an intern at SenseTime.
‡Corresponding authors.
Figure 1: Sample contrastive distillation vs. Relation pre-serving distillation. Four neighboring samples and their cor-responding features are displayed, and capital letters are
A closer to f T used to identify them. While pulling f S
A , sample contrastive distillation will simultaneously push f S
A away from f T
D without distinction, whereas re-lation preserving distillation preserves the feature relations across the feature space, thus f S
A can be optimized along the optimal direction.
C and f T
B , f T regime, there is still no consensus on what kind of knowl-edge really needs to be preserved in the distillation [14].
As one of the most effective distillation methods, CRD
[41] holds the view that the representational knowledge is structured. So It tries to capture the correlations and higher-order output dependencies for each sample, which is dif-ferent from the original KD objective introduced in [19] that treats all dimensions as independent information. CRD 9260
leverages the family of contrastive objectives [20, 40, 46, 4] to maximize a lower-bound of the mutual information be-tween the teacher and student representations.
It essen-tially performs knowledge distillation based on the individ-ual samples, enforcing the representation consistency be-tween the teacher model and the student model.
However, neither CRD nor other sample-based distil-lation methods can effectively preserve inter-sample rela-tions, which are more valuable than the sample represen-tations themselves in many practical tasks, e.g., retrieval and classiﬁcation. As shown in Fig. 1, when using sam-ple contrastive distillation methods, e.g., CRD, the opti-mized forces from other neighbors just push the student rep-resentation of sample A away when contrasted negatively, which may not be optimal and can break the latent struc-tural geometry of neighboring samples. Some recent works have shown that transferring the mutual similarity instead of actual representation is beneﬁcial to student representa-tion learning [43, 32, 34, 33]. These methods directly esti-mate the relations in teacher space by computing the inter-sample similarities, then mimic these similarities in the stu-dent space via L2 loss or KL divergence, ignoring the high-order dependency within the representation in both relation estimation and knowledge distillation.
To robustly distill the structural knowledge of the teacher space, we deﬁne a new cross-space relation between two samples and supervise this new relation by its correspond-ing relation in the teacher representation space. More speciﬁcally, given the teacher and student representation of one sample, we select a neighboring sample’s representa-tion from the teacher representation space as an anchor. The anchor-student relation is encouraged to be consistent with the anchor-teacher relation. Our method brings at least three merits for distillation. (1) It simultaneously optimizes the representation and relation. When the anchor-student re-lation is pushed to be consistent with the anchor-teacher relation, the student representation is actually optimized along the optimal direction of representation learning. (2)
The anchor-student relation is more effective for distillation compared with the student-student relation (where two rep-resentations are both from the student space) in the conven-tional KD family [43, 32, 34]. The student-student relation is unstable because the two representations in the student space are not well optimized and they will drift signiﬁcantly during distillation, while the anchor representation within the anchor-student relation is ﬁxed, which can effectively (3) As optimize the representation in the student space. the anchor can be randomly selected from the neighborhood of the considered sample, the student representation of one sample is supervised by multiple relations from different anchors, which guarantees the robustness of the distillation.
The representation relation is modeled by two comple-the feature and its gradient. The fea-mentary elements: ture relation reﬂects the structural information in represen-tational space, and the gradient relation is computed by the feature gradients after backward propagation. As gradients measure the fastest rate and direction for loss minimiza-tion, gradient relation can explore the structural information of optimization kinetics in representational space [18, 39].
During the distillation, we maximize the mutual informa-tion between the anchor-teacher relation and the anchor-student relation for both two elements. The maximization problem can further surrogate to maximize the lower bound of mutual information which has been well solved by con-trastive learning [46]. Our method is therefore denoted by
Complementary Relation Contrastive Distillation(CRCD).
In summary, the main contributions of CRCD are three-fold. First, we deﬁne a new anchor-based cross space re-lation and adopt it to effectively and robustly distill both sample representations and inter-sample relations. Second, the new relation is modeled by two complementary ele-ments, i.e., the feature and its gradients, which capture the structure information of the feature and the optimization ki-netics, respectively. Last, we maximize the low bound of mutual information between the anchor-teacher relation and the anchor-student relation and derive an efﬁcient solution in the form of contrastive learning. Extensive experiments empirically validate the effectiveness of CRCD and further improve the current state-of-the-art in various benchmarks. 2.