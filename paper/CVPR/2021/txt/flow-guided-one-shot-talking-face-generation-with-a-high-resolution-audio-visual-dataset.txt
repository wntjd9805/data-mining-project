Abstract
One-shot talking face generation should synthesize high visual quality facial videos with reasonable animations of expression and head pose, and just utilize arbitrary driv-ing audio and arbitrary single face image as the source.
Current works fail to generate over 256×256 resolution realistic-looking videos due to the lack of an appropriate high-resolution audio-visual dataset, and the limitation of the sparse facial landmarks in providing poor expression details. To synthesize high-deﬁnition videos, we build a large in-the-wild high-resolution audio-visual dataset and propose a novel ﬂow-guided talking face generation frame-work. The new dataset is collected from youtube and con-sists of about 16 hours 720P or 1080P videos. We leverage the facial 3D morphable model (3DMM) to split the frame-work into two cascaded modules instead of learning a di-rect mapping from audio to video. In the ﬁrst module, we propose a novel animation generator to produce the move-ments of mouth, eyebrow and head pose simultaneously. In the second module, we transform animation into dense ﬂow to provide more expression details and carefully design a novel ﬂow-guided video generator to synthesize videos. Our method is able to produce high-deﬁnition videos and out-performs state-of-the-art works in objective and subjective comparisons*. 1.

Introduction
Given one reference facial image and one driving audio, one-shot talking face generation aims at synthesizing a talk-ing avatar video with reasonable facial animations corre-sponding to the driving audio. Talking face generation is of importance for many applications, including virtual assis-tants, mixed realities, animation movies, and so forth. Due to its wide applications, talking face generation draws con-*Yu Ding is the corresponding author.
*The HDTF dataset etc. github.com/MRzzm/HDTF for research purpose are at https://
Figure 1. Our method synthesizes high-resolution talking face videos with one driving audio and one reference facial image. siderable attention for a long time.
While many works[7, 6, 18, 13, 51, 5, 48, 29, 42, 49, 4] make great efforts to synthesize realistic-looking videos, the generation of high-resolution videos is still a challenge.
Current best work[49] just generate videos with 256×256 resolution(see Figure10(e) for example), however, directly employing their model on 512×512 image will get blurry results (see Figure10(f) for example). Several factors result in this challenge.
The ﬁrst reason is that there are no appropriate datasets for high-resolution talking face generation. Table 1 illus-trates some common audio-visual datasets (all available datasets are listed in [3]). As shown in Table 1, cur-rent audio-visual datasets consist of in-the-wild datasets and in-the-lab datasets. In-the-wild datasets contain larger scale and more subjects, but they all lack video resolu-tion. There are two main reasons: On one hand, their videos are collected from the internet published in the past 2∼5 years, and at that time the internet videos generally have low resolution. On the other hand, most in-the-wild datasets do not focus on the task of talking face generation, e.g., Voxceleb[26, 8] is built for speaker identiﬁcation and
LRW[9] is built for word recognition, so they do not pay attention to the video resolution. For in-the-lab datasets, while they record high resolution face videos, the number of subjects and sentences is limited because of the expen-sive labor costs. The largest MEAD[43] only records 159 sentences with 60 actors.
The second reason is that previous works are not de-3661
Table 1. Statistics of current common audio-visual datasets.
Dataset name
LRW [9]
Voxceleb1[26]
Voxceleb2[8]
GRID[11]
RAVDESS[24]
MEAD[43]
Our HDTF
Environment Year 2016 2017 2018 2006 2018 2020 2020
Wild
Wild
Wild
Lab
Lab
Lab
Wild
Resolution 360P∼480P 360P∼720P 360P∼720P 720×576 1280×1024 1920×1080 720P∼1080P
Subject Hours 173 352 2442 27.5 7 40 15.8 1k+ 1251 6112 34 24 60 300+ sentence 1k 100k 1128k 51 8 159 10k+ signed reasonably to handle high-resolution videos and are limited by the input of sparse facial landmarks.
Initial works[7, 5, 42] directly utilize an end-to-end framework to synthesize the video from audio. Their synthetic results even have a low deﬁnition on 128×128 videos. Other re-cent advances[6, 49, 13, 4] leverage facial landmarks to split the pipeline into two cascaded modules. They pro-duce sparse facial landmarks in the ﬁrst module, and fur-ther generate videos from synthetic landmarks in the second module. Two modules are trained separately to alleviate the pressure of the network, thus lead to high visual qual-ity results. However, in the second module, their methods are still hard to generate high resolution videos. We care-fully discuss the reasons in Section 7. On one hand, some works directly utilize the network to learn the sophisticated mapping from landmark to image. This mapping become too complex to handle on high-resolution videos, e.g., [49] synthesize blurry results on 512×512 resolution(see Fig-ure10(g) and Figure11(a) for example). On the other hand, although some works carefully design their network to ex-plicitly model the process of image synthesis, the sparse landmark is too coarse and lose many facial expression details, e.g., [34] synthesize facial image with inaccuracy mouth shape and poor wrinkles(see Figure11(c) for exam-ple).
In order to achieve above challenge and promote the development of high-resolution talking face generation, we ﬁrst build a large in-the-wild high-resolution audio-visual dataset, named High-deﬁnition Talking Face Dataset (HDTF). The HDTF dataset is collected from youtube web-site published in recent two years and consists of about 16 hours 720P∼1080P videos. There are over 300 subjects and 10k different sentences in HDTF dataset. Our HDTF dataset has higher video resolution than previous in-the-wild datasets and more subjects/sentences than in-the-lab datasets.
Next, we propose a novel ﬂow-guided framework to syn-thesize high visual quality videos. Figure 2 illustrates the pipeline of our method. Our work ﬁrst leverages 3DMM[1] to split the framework into two cascaded modules, named audio-to-animation module and animation-to-video mod-ule. Compared with the facial landmarks, 3DMM is in-sensitive to noise due to the prior knowledge of the face.
In audio-to-animation module, 3DMM is used to decouple the face into facial animation parameters (mouth, eyebrow and head pose) and we propose a novel style-speciﬁc anima-tion generator to produce the full animation parameters with multi-task learning strategy. Our generator considers the difference of speaking style between different identity[50], and has capacity to synthesize subject-dependent anima-tions.
In animation-to-video module, we propose a ﬂow-guided framework to synthesize high visual quality videos.
Our method utilizes 3DMM to transform animation param-eters to dense ﬂow. Dense ﬂow has beneﬁts of provid-ing richer facial details than sparse landmarks. Then, a novel video generator is proposed to synthesize talking face videos from dense ﬂow. Our generator is carefully designed to explicitly control the process of frame generation, so it is easy to generate more realistic results.
Our contributions are summarized as follows:
• We build a large in-the-wild audio-visual dataset, with higher video resolution than previous in-the-wild datasets and more subjects/sentences than in-the-lab datasets.
• We propose a novel style-speciﬁc animation generator to produce speciﬁc style animation parameters depend-ing on the reference identity.
• To the best of our knowledge, we are the ﬁrst to uti-lize one animation generator with multi-task learning to produce the animation parameters of mouth, eye-brow and head pose simultaneously in one-shot talking face generation.
• We propose a novel carefully-designed ﬂow-guided framework to synthesize higher visual quality videos than previous landmark-based approaches. 2.