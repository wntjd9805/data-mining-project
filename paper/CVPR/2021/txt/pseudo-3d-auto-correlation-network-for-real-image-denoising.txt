Abstract
The extraction of auto-correlation in images has shown great potential in deep learning networks, such as the self-attention mechanism in the channel domain and the self-similarity mechanism in the spatial domain. However, the realization of the above mechanisms mostly requires com-plicated module stacking and a large number of convolu-tion calculations, which inevitably increases model com-plexity and memory cost. Therefore, we propose a pseudo 3D auto-correlation network (P3AN) to explore a more efﬁ-cient way of capturing contextual information in image de-noising. On the one hand, P3AN uses fast 1D convolution instead of dense connections to realize criss-cross interac-tion, which requires less computational resources. On the other hand, the operation does not change the feature size and makes it easy to expand. It means that only a simple adaptive fusion is needed to obtain contextual information that includes both the channel domain and the spatial do-main. Our method built a pseudo 3D auto-correlation at-tention block through 1D convolutions and a lightweight 2D structure for more discriminative features. Extensive exper-iments have been conducted on three synthetic and four real noisy datasets. According to quantitative metrics and visual quality evaluation, the P3AN shows great superiority and surpasses state-of-the-art image denoising methods. 1.

Introduction
As a low-level vision task, image denoising aims to re-cover the underlying clean image from an observed noisy one, which is a fundamental step for various high-level vision and image analysis applications [42].
In recent years, many advanced methods have achieved remarkable progress in removing synthesized additive white Gaussian noise. However, the noise in real images often has a compli-cated generation process in CCD or CMOS camera systems.
Affected by different devices and image signal processing (ISP) pipelines within the camera, the denoising algorithms based on synthetic data are inherently difﬁcult to simu-*Haoqian Wang is the corresponding author. late and remove irregular real noise accurately [1, 36]. For blind image denoising, the low-quality noise images with-out speciﬁc noise statistical priors become the only source of guidance. So it is particularly important to capture auto-correlation prior information from the input.
The image auto-correlation prior has been widely ex-plored and played an essential role in many traditional noise reduction algorithms. In order to obtain more robust learn-ing and presentation abilities, recent methods try to use deep learning methods to accumulate more useful and compre-hensive prior knowledge. In recent years, auto-correlation features extracted by convolutional neural networks (CNN) can be divided into channel-wise and spatial-wise features, which are calculated by the channel-based model and space-based model, respectively. Similar to other high-level vision tasks, channel-based model can capture non-linear cross-channel information about channels of interest, which is de-ﬁned as a self-attention mechanism [3, 12, 22, 29, 34, 55].
Through features are aggregated and recalibrated in differ-ent ways, self-attention can capture cross-channel interac-tion. Lightweight attention aggregation is conducive to as-sign sophisticated channel-wise dependencies efﬁciently.
The space-based model revolves around the spatial self-similarity that has been proven as a powerful feature of natural images [30, 35, 44]. A common way to obtain the global auto-correlation in the spatial domain is using a non-local block [56]. As shown in Fig. 1(a), it calculates the response as the weighted sum of all pixels. The spatial auto-correlation is hidden in the pixel-wise long-range contextual information. The dense connection generates huge attention maps with high complexity. Huang et al. proposed criss-cross attention with sparsely-connected graphs, as shown in Fig. 1(b) [24]. They only extract contextual information in its horizontal and vertical direction to narrow the search.
But they still need to traverse each pixel to obtain full-image auto-correlation for each position, which does not fully play the role of sparse connections. Although the idea of utiliz-ing auto-correlation prior of images has achieved great suc-cesses in various restoration tasks, most CNNs built with this still suffer from a heavier computational burden. 16175
Therefore, the exploration of image auto-correlation en-counters a bottleneck in balancing model performance and computational complexity. On the one hand, most exist-ing methods are still dedicated to building more sophis-ticated auto-correlation modules to achieve better perfor-mance, which further increases application difﬁculty in the real world. On the other hand, the feature maps are usu-ally 3-dimensional, so a complete auto-correlation feature should have contextual information from both channel and spatial directions. It is not feasible to introduce 3D convo-lution accompanying the explosive growth of parameters.
There are currently some methods that combine the channel self-attention and the spatial self-similarity through a series or parallel structure [15, 46]. This step-by-step operation increases the complexity of the model and destroys the con-tinuous correlation in the local area.
To address the above problems efﬁciently, we proposed a pseudo 3D auto-correlation network (P3AN) to simulate 3D convolution with a 2D structure and integrate channel and spatial auto-correlation into a uniﬁed module. As shown in Fig. 1(c), to avoid the high computational complexity caused by huge attention maps, we use fast 1D convolution instead of a non-local densely-connected layer. The fea-ture correlations in a speciﬁc horizontal or vertical direc-tion are captured and merged to minimize the correlation map. At the same time, parameter sharing is introduced into convolution operations to save GPU memory. Our op-eration does not change the size of the feature map, which makes cross-directional fusion possible. It can be seen from
Fig. 1(d) that only simple channel integration and adaptive fusion are needed to integrate the three directions of hori-zontal, vertical, and channel. Through consecutive pseudo 3D auto-correlation blocks (P3AB) stack and skip connec-tion, each location can collect contextual information by considering all local pixels in 3D space. Our method re-alizes a lightweight pseudo 3D interaction network that can be learned end-to-end with low time and space complexity.
The main innovative contributions are as follows:
• We propose a novel spatial auto-correlation module with fast 1D convolution. The strategy of direction in-dependence and parameter sharing can effectively re-duce time and space complexity while capturing con-textual information from full image dependencies.
• The operation in one-dimensional space avoids dimen-sionality reduction, which is easy to expand. We de-sign a lightweight 2D structure to adaptively fuse the correlation features in the three directions of horizon-tal, vertical, and channel, and get more discriminative features for real image denoising.
• We evaluate quantitative indicators and visual quality on synthetic and real noise datasets. Our proposed net-work has lower model complexity and higher perfor-mance than state-of-the-art image denoising methods.
W
W
W
W
W
W
W
W
H
H
H
H
H
H
H
H
W
W
W
W
H
H
H
H
H W(cid:117)
H W(cid:117)
H W(cid:117)
H W(cid:117)
W (cid:117)
H
W
W (cid:117) (cid:117)
H
H
W (cid:117)
H
Spatial Auto-Correlation with Fully-Connected 
Spatial Auto-Correlation with Fully-Connected 
Spatial Auto-Correlation with Fully-Connected  (a) Spatial Auto-Correlation with Fully-Connected.
H+W-1
Spatial Auto-Correlation with Fully-Connected 
H+W-1
H+W-1
H+W-1
W (cid:117)
H
W
W (cid:117) (cid:117)
H
H
W (cid:117)
H (b) Spatial Auto-Correlation with Sparsely-Connected.
W
W
W
W
H
H
H
H c c c c
Spatial Auto-Correlation with Sparsely-Connected 
Spatial Auto-Correlation with Sparsely-Connected 
Spatial Auto-Correlation with Sparsely-Connected 
Spatial Auto-Correlation with Sparsely-Connected 
Spatial Auto-Correlation with 1D Convolution
Spatial Auto-Correlation with 1D Convolution
Spatial Auto-Correlation with 1D Convolution
Spatial Auto-Correlation with 1D Convolution
W
W
W
W
W
W
W
W (c) Spatial Auto-Correlation with 1D Convolution.
W
W
W
W
H
H
H
H
H
H
H
H
C
C
C
C
H
H
H
H c c c c
Pseudo 3D Auto-Correlation with 1D Convolution
Pseudo 3D Auto-Correlation with 1D Convolution
Pseudo 3D Auto-Correlation with 1D Convolution
Pseudo 3D Auto-Correlation with 1D Convolution w w h c 1 1 1 h h h c h c w c c w w 1D Convolution 1 1 1 w h 1 1 c 1 1 1 1 w w h h c c
Sigmoid 1D Convolution 1D Convolution 1D Convolution
Sigmoid
Sigmoid
Sigmoid 1 h c 1 1
Adaptive Fusion
Adaptive Fusion
Adaptive Fusion
Adaptive Fusion
Horizontal 1 1 1 h 1 1 c 1 1 1 1 h h c c
Horizontal
Vertical
Horizontal
Horizontal h h h 3c 3c 3c
Vertical
Channel
Channel
Vertical
Vertical h
Channel
Channel 3c w w w w
C
C
C
C (d) Pseudo 3D Auto-Correlation with 1D Convolution.
Figure 1. Diagram of four methods to aggregate auto-correlation.
Each position (e.g., red) can collect information from other pixels 2.