Abstract
We introduce a novel representation learning method to disentangle pose-dependent as well as view-dependent fac-tors from 2D human poses. The method trains a network us-ing cross-view mutual information maximization (CV-MIM) which maximizes mutual information of the same pose per-formed from different viewpoints in a contrastive learning manner. We further propose two regularization terms to en-sure disentanglement and smoothness of the learned repre-sentations. The resulting pose representations can be used for cross-view action recognition.
To evaluate the power of the learned representations, in addition to the conventional fully-supervised action recog-nition settings, we introduce a novel task called single-shot cross-view action recognition. This task trains mod-els with actions from only one single viewpoint while models are evaluated on poses captured from all pos-sible viewpoints. We evaluate the learned representa-tions on standard benchmarks for action recognition, and show that (i) CV-MIM performs competitively compared with the state-of-the-art models in the fully-supervised sce-narios; (ii) CV-MIM outperforms other competing meth-ods by a large margin in the single-shot cross-view set-ting; (iii) and the learned representations can signiﬁcantly boost the performance when reducing the amount of super-vised training data. Our code is made publicly available at https : / / github . com / google - research / google-research/tree/master/poem. 1.

Introduction
Understanding human poses and actions is a fundamen-tal problem in computer vision due to its broad applications in the real world, such as video content analysis, intelligent photography, AR/VR techniques, and human-computer in-terface. Recently, remarkable improvements have been
∗This work was done while the author was a research intern at Google.
Training Dataset
Training View
Multi-View Images
View 1
Encoding 
Model
Testing Views
Encoding 
Model
Pose
View
Maximize Cross-View MI
View 1
View 2
View 3
View 4
View-Disentangled 
Representation Learning
Single-Shot Cross-View 
Action Recognition
Figure 1. Left: We propose to learn view-disentangled representa-tion for human poses by maximizing cross-view mutual informa-tion. Right: The learned representation can be applied to down-stream tasks such as single-shot cross-view action recognition. achieved with deep learning approaches [9, 20, 28, 54, 55].
However, these data-driven approaches are usually vulner-able to changes of viewpoints.
In particular, testing-time unseen viewpoints often lead to signiﬁcant degradation in recognition performance [47].
To mitigate this issue, methods for cross-view action recognition [47] have been proposed, where models are trained with a set of actions captured from different view-points simultaneously so that they can be applied to novel views unseen from training at testing time. Previous studies usually require extensive supervision from multiple views to learn view-invariant features [46, 47] or transferable rep-resentations [21, 23] for action recognition. Collecting la-beled action data at scale from multiple views can be expen-sive and challenging in the wild due to potential limitation of camera placement, scene and actor setup, etc.
We address this challenge by proposing a novel view-12793
disentangled representation learning approach. To train the representation model, we only require pairs of 2D poses captured from different viewpoints without additional task-relevant supervision, which are widely available in standard multi-view human action datasets [17, 38]. Our target is to disentangle pose-dependent (view-invariant) and view-dependent representations from 2D poses, which has not been well-explored in existing works.
To achieve this, we train a representation-learning func-tion, i.e., an encoder, following the Mutual Information (MI) maximization principle [4]. Speciﬁcally, in order to fulﬁll the view-disentanglement constraint, we propose to maximize the cross-view MI, i.e., the dependency between learned representations of the same pose from different views. In addition, we theoretically motivate two regular-ization terms that encourage disentanglement and smooth-ness of the learned representation to further improve its power. Our objective is optimized in a contrastive manner based on recent advances made in MI estimation [3, 7, 15, 30, 31]. Compared to approaches based on cross recon-struction [29], the proposed approach yields stronger rep-resentative powers by using negative training pairs which provide an additional source of supervision [6, 15].
We show that the resulting pose representation can be used for action recognition in a fully-supervised setting. To further demonstrate its view-disentangled property, we in-troduce a novel and more challenging task, namely, single-shot cross-view action recognition. In this setting, recogni-tion models are trained with 2D poses from one single view but expected to generalize to unseen views at testing time.
This setting is highly practical for real-world applications: it only requires collecting training data from a ﬁxed camera view, and the resulting recognition model can be applied to various difference views. Note that success in this task re-quires not only discriminative but also view-invariant rep-resentations for 2D poses. Fig. 1 summarizes our represen-tation learning framework and its application to single-shot cross-view action recognition downstream task.
To sum up, our main contributions of this work include: (i) a novel objective to learn view-disentangled represen-tation for 2D human poses by maximizing cross-view MI; (ii) two regularization techniques to guarantee disentangle-ment and smoothness of the learned representation; (iii) a newly proposed task called single-shot cross-view action recognition that can be used for evaluating view-invariant representation for human poses. We evaluate the proposed method on standard benchmarks for action recognition un-der scenarios of full-supervision, single-shot cross-view set-ting, and supervision with limited data. Experimental re-sults show that our approach is comparable to the state of the art in the fully-supervised setting, while it can consis-tently and statistically signiﬁcantly outperform competing methods under the other two scenarios. 2.