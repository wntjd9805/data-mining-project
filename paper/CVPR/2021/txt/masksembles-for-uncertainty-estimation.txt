Abstract
Deep neural networks have amply demonstrated their prowess but estimating the reliability of their predictions re-mains challenging. Deep Ensembles are widely considered as being one of the best methods for generating uncertainty estimates but are very expensive to train and evaluate.
MC-Dropout is another popular alternative, which is less expensive, but also less reliable. Our central intuition is that there is a continuous spectrum of ensemble-like mod-els of which MC-Dropout and Deep Ensembles are extreme examples. The ﬁrst one uses effectively inﬁnite number of highly correlated models while the second one relies on a
ﬁnite number of independent models.
To combine the beneﬁts of both, we introduce Masksem-bles. Instead of randomly dropping parts of the network as in MC-dropout, Masksemble relies on a ﬁxed number of bi-nary masks, which are parameterized in a way that allows to change correlations between individual models. Namely, by controlling the overlap between the masks and their size one can choose the optimal conﬁguration for the task at hand. This leads to a simple and easy to implement method with performance on par with Ensembles at a fraction of the cost. We experimentally validate Masksembles on two widely used datasets, CIFAR10 and ImageNet. 1.

Introduction
The ability of deep neural networks to produce useful predictions is now abundantly clear but assessing the relia-bility of these predictions remains a challenge. Among all the methods that can be used to this end, MC-Dropout [8] and Deep Ensembles [21] have emerged as two of the most popular ones. Both of those methods exploit the concept of ensembles to produce uncertainty estimates. MC-Dropout does so implicitly - by training a single stochastic network, where randomness is achieved by dropping different subsets of weights for each observed sample. At test time, one can
*This work was supported in part by the Swiss National Science Foun-dation obtain multiple predictions by running this network multi-ple times, each time with a different weight conﬁguration.
Deep Ensembles, on the other hand, build an explicit en-semble of models, where each model is randomly initialized and trained independently using stochastic gradient descent.
As importantly, both methods rely on the fact that the out-puts of individual models are diverse, which is achieved by introducing stochasticity into the training or testing process.
However, simply adding randomness does not always lead to diverse predictions. In practice, MC-Dropout often performs signiﬁcantly worse than Deep Ensembles on un-certainty estimation tasks [21, 28, 12]. We argue that this can be attributed to the fact that each weight is dropped randomly and independently from the others. For many conﬁgurations of hyperparameters, in particular the dropout rate, this results in similar weight conﬁgurations and, conse-quently, less diverse predictions [6]. Deep Ensembles seem to not share this weakness for reasons that are not yet fully understood and usually produce signiﬁcantly more reliable uncertainty estimates. Unfortunately, this comes at a price, both at training and inference time: Building an ensemble requires training multiple models and using it means load-ing all of them simultaneously in memory, typically a very scarce resource.
In this work, we introduce Masksembles, an approach to uncertainty estimation that tackles these challenges and produces reliable uncertainty estimates on par with Deep
Ensembles at a signiﬁcantly lower computational cost. The main idea behind the method is simple - introduce a more structured way to drop model parameters than that of MC-Dropout.
Masksembles produces a ﬁxed number of binary masks which specify the network parameters to be dropped. The properties of the masks effectively deﬁne the properties of the ﬁnal ensemble in terms of capacity and correlations.
During training, for each sample we randomly choose one of the masks and drop the corresponding parts of the model just like standard dropout. During inference, we run the model multiple times, once per mask, to obtain a set of predictions and, ultimately, an uncertainty estimate. Our method has three key hyperparameters: the total number of 13539
masks, the average overlap between masks and the number of ones and zeros in each mask. Intuitively, using a large number of masks approximates MC-Dropout, while using a set of non-overlapping, completely disjoint masks, yields an Ensemble-like behavior. In other words, as illustrated in
Fig. 5 Masksembles deﬁnes a spectrum of model conﬁgu-rations of which MC-Dropout and Deep Ensembles can be seen as extreme cases. We evaluate our method on several synthetic and real datasets and demonstrate that it outper-forms MC-Dropout in terms of accuracy, calibration, and out-of-distribution (OOD) detection performance. When compared to Deep Ensembles, our method has similar per-formance, but is more favorable in terms of training time and memory requirements. Furthermore, Masksembles is simple to implement and can serve as a drop-in replacement for MC-Dropout in virtually any model. To summarize, the main contributions of this work are as follows:
• We propose an easy to implement, drop-in replacement method that performs better than MC-Dropout, at a similar computational cost, and that matches Ensem-bles at a fraction of the cost.
• We provide theoretical insight into two popular uncer-tainty estimation methods - MC-Dropout and Ensem-bles, by creating a continuum between the two.
• We provide a comprehensive evaluation of our method on several public datasets. This validates our claims and provides guidance for the choice of Masksemble parameters in practical applications. 2.