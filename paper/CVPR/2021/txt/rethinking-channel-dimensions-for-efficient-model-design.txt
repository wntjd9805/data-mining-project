Abstract
Designing an efﬁcient model within the limited compu-tational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise conﬁguration of the channel di-mensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension conﬁguration towards better performance than the convention. To this end, we empirically study how to de-sign a single layer properly by analyzing the rank of the out-put feature. We then investigate the channel conﬁguration of a model by searching network architectures concerning the channel conﬁguration under the computational cost re-striction. Based on the investigation, we propose a simple yet effective channel conﬁguration that can be parameter-ized by the layer index. As a result, our proposed model following the channel parameterization achieves remark-able performance on ImageNet classiﬁcation and transfer learning tasks including COCO object detection, COCO in-stance segmentation, and ﬁne-grained classiﬁcations. Code and ImageNet pretrained models are available at https:
//github.com/clovaai/rexnet. 1.

Introduction
Designing a lightweight network architecture is cru-cial for both researchers and practitioners. Popular net-works [47, 15, 20, 46] designed for ImageNet classiﬁcation share a similar design convention where a low dimensional input channel is expanded by a few channel expansion lay-ers towards surpassing the number of classes. Lightweight models [20, 46, 19, 59, 35, 49, 54, 3, 50] also follow this conﬁguration but further shrinks some channels for com-putational efﬁciency, which leads to the promising trade-offs between the computational cost and accuracy. In other words, the degree of channel expansion at layers is quite different, where earlier layers have a smaller channel di-mension; the penultimate layer that largely expands dimen-sion above the number of classes. This is to realize ﬂop-efﬁciency by narrow channel dimensions at earlier layers; to get model expressiveness with sufﬁcient channel dimen-sion at the ﬁnal feature (see Table 1).
This channel conﬁguration was ﬁrstly introduced by Mo-bileNetV2 [46] and became the design convention of con-ﬁguring channel dimensions in lightweight networks, but how to adjust the channel dimensions towards the optimal under the restricted computational cost has not been pro-foundly studied. As shown in Table 1, even network ar-chitecture search (NAS)-based models [54, 3, 19, 49, 51, 50, 38, 6, 7, 30] were designed upon the convention or lit-tle more exploration within few options near the conﬁgura-tion [11, 53] and focused on searching building blocks. Take one step further from the design convention, we hypothesize that the compact models designed by the conventional chan-nel conﬁguration may be limited in the expressive power due to mainly focusing on ﬂop-efﬁciency; there would exist a more effective conﬁguration over the traditional one.
In this paper, we investigate an effective channel conﬁg-uration of a lightweight network with additional accuracy gain. Inspired by the works [56, 60], we conjecture the ex-pressiveness of a layer can be estimated by the matrix rank of the output feature. Technically, we study with the aver-aged rank computed from the output feature of a bunch of networks that are randomly generated with random sizes to reveal the proper range of expansion ratio at an expansion layer and make a link with a rough design principle. Based on the principle, we move forward to ﬁnd out an overall channel conﬁguration in a network. Speciﬁcally, we search network architectures to identify the channel conﬁguration yielding a better accuracy over the aforementioned conven-tion. It turns out that the best channel conﬁguration is pa-rameterized as a linear function by the block index in a net-work. This parameterization is similar to the conﬁguration used in the works [22, 13], and we reveal the parameteriza-tion is also effective in designing a lightweight model.
Based on the investigation, we propose a new model upon the searched channel parameterization. It turns out that a simple modiﬁcation upon MobileNetV2 could show re-markable improvement in performance on ImageNet classi-ﬁcation. Only with the new channel conﬁguration, our mod-els outperform the state-of-the-art networks such as Efﬁ-1732
Network
Stem / Building blocks’ output channel dimensions
Top-1 Params Flops 32 / 16(×1)-24(×2)-32(×3)-64(×4)-96(×3)-160(×3)-320(×1) 72.0% 3.4M 0.30B
MobileNetV2 [46] 16 / 16(×1)-24(×4)-32(×4)-64(×4)-112(×4)-184(×4)-352(×1) 74.9% 5.5M 0.38B
FBNet-C [54]
ProxylessNas-R [3] 32 / 16(×1)-32(×2)-40(×4)-80(×4)-96(×4)-192(×4)-320(×1) 74.6% 4.1M 0.32B 32 / 16(×1)-24(×2)-40(×3)-80(×4)-112(×2)-160(×3)-320(×1) 75.2% 3.9M 0.31B
MNasNet-A1 [49]
MixNet-M [51] 77.0% 5.0M 0.36B 24 / 24(×1)-32(×2)-40(×4)-80(×4)-120(×4)-200(×4)
EfﬁcinetNet-B0 [50] 32 / 16(×1)-24(×2)-40(×2)-80(×3)-112(×3)-192(×4)-320(×1) 77.3% 4.8M 0.39B 77.6% 5.9M 0.36B
AtomNas-C [38]
FairNas-A [6] 77.5% 5.9M 0.39B 77.2% 5.3M 0.39B
FairDARTS-C [7] 77.5% 6.1M 0.59B
SE-DARTS+ [30] 32 / 16(×1)-24(×4)-40(×4)-80(×4)-96(×4)-192(×4)-320(×1) 32 / 16(×1)-32(×2)-40(×4)-80(×4)-96(×4)-192(×4)-320(×1) 32 / 16(×1)-32(×2)-40(×4)-80(×5)-96(×3)-192(×4)-320(×1) 32 / 16(×1)-24(×4)-40(×4)-80(×4)-96(×4)-192(×4)-320(×1)
Table 1. Channel conﬁgurations in lightweight models. We present the output channel dimensions of the stem’s 3×3 convolution (e.g., 16, 24, 32) and the building blocks with the number of repeated layers (e.g., ×1, ×2, ×3, ×4, ×5) in block index order. All the models after
MobileNetV2 have similar channel conﬁgurations to that of MobileNetV2, which have repeated channel dimensions for each stage. cientNets [50] whose architectures were found by the com-pound scaling with TPUs. This stresses the effectiveness of our channel conﬁguration over the convention and may en-courage the researchers in the NAS ﬁeld to adopt our chan-nel conﬁguration into the network search space for further performance boosts. The performance improvement of Im-ageNet classiﬁcation is well transferred to the object detec-tion and instance segmentation on the COCO dataset [32] and the various ﬁne-grained classiﬁcation tasks. This indi-cates our backbones work as strong feature extractors.
Our contributions are 1) a study on designing a single layer (§3); 2) a network architecture exploration concern-ing the channel conﬁguration towards a simple yet effec-tive parameterization (§4); 3) using our models to achieve remarkable results on ImageNet [45] outperformed recent lightweight models including NAS-based models (§5); 4) revealing the high applicability of our ImageNet-pretrained backbones transferring to several tasks including object de-tection, instance segmentation and ﬁne-grained classiﬁca-tion, which indicate the high expressiveness of our model and the effectiveness of our channel conﬁguration (§5). 2.