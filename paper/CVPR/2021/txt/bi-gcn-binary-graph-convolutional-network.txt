Abstract
Graph Neural Networks (GNNs) have achieved tremen-dous success in graph representation learning. Unfortu-nately, current GNNs usually rely on loading the entire at-tributed graph into network for processing. This implicit assumption may not be satisﬁed with limited memory re-sources, especially when the attributed graph is large. In this paper, we pioneer to propose a Binary Graph Convo-lutional Network (Bi-GCN), which binarizes both the net-work parameters and input node features. Besides, the orig-inal matrix multiplications are revised to binary operations for accelerations. According to the theoretical analysis, our
Bi-GCN can reduce the memory consumption by an aver-age of ∼30x for both the network parameters and input data, and accelerate the inference speed by an average of
∼47x, on the citation networks. Meanwhile, we also de-sign a new gradient approximation based back-propagation method to train our Bi-GCN well. Extensive experiments have demonstrated that our Bi-GCN can give a compara-ble performance compared to the full-precision baselines.
Besides, our binarization approach can be easily applied to other GNNs, which has been veriﬁed in the experiments. 1.

Introduction
In the past few years, Graph Neural Networks (GNNs), which can learn effective representations from irregular data, have given excellent performances in various graph-based tasks [18, 28, 31, 30]. Considering the superior rep-resentation abilities of these newly developed GNNs, re-searchers have also applied them to many tasks, including natural language processing [33], computer vision [24], etc.
Unfortunately, the current success of GNNs is attributed to an implicit assumption that the input of GNNs contains the entire attributed graph. If the entire graph is too large to be fed into GNNs due to limited memory resources, in both
*Corresponding author. (a) (b)
Figure 1. Performances on the Cora dataset. Note that the model size is measured in bits and the number of cycle operations, which will be introduced in Sec. 5, is employed to reﬂect the inference speed. Bi-GCN gives the fastest inference speed and the lowest memory consumption with comparable accuracy. the training and inference process, which is highly likely when the scale of the graph increases, the performances of
GNNs may degrade drastically.
To tackle this problem, an intuitive solution is sampling, e.g., sampling a subgraph with a suitable size to be entirely loaded into GNNs. The sampling based methods can be classiﬁed into two categories, neighbor sampling [12, 5] and graph sampling [4, 6, 34]. Neighbor sampling selects a
ﬁxed number of neighbors for each node in the next layer to ensure that every node can be sampled. Thus, it can be utilized in both the training and inference process. Unfor-tunately, when the number of layers increases, the problem of neighbor explosion [34] arises, such that both the train-ing and inference time will increase exponentially. Differ-ent from neighbor sampling, graph sampling samples a set of subgraphs in the training process, which can avoid the problem of neighbor explosion. However, it cannot guar-antee that every node can be at least sampled once in the whole training/inference process. Thus it is only feasible for the training process, because the testing process usually requires GNNs to process each node in the graph.
Another feasible solution is compressing the size of the input graph data and the GNN model to better utilize the limited memory and computational resources. Several ap-proaches have been proposed to compress the convolutional 1561
neural networks (CNNs), such as designing shallow net-works [2], pruning [9], designing compact layers [27] and quantizing the parameters [15]. In quantization-based meth-ods, binarization [15, 22, 21] has achieved a great success in many CNN-based practical vision tasks when a faster speed and a lower memory consumption is desired.
However, compared to the CNN compression methods, the compression of GNNs possesses unique challenges.
Firstly, since the input graph data is usually much larger than the GNN models, the compression of the loaded data demands more attention. Secondly, the GNNs are usually shallow, e.g., the standard GCN [18] only has 2 layers, which contain less redundancies, thus the compression will be more difﬁcult to be achieved. At last, the nodes tend to be similar to its neighbors in the high-level semantic space, while they tend to be different in the low-level feature space, which is different from the grid-like data, such as images, videos, etc. This characteristic requires the compressed
GNNs to possess sufﬁcient parameters for representations.
In general, the tradeoff between the compression ratio and accuracy in the compressed GNNs requires careful designs.
To tackle the memory and complexity issues, SGC [29], which is a 1-layered GNN, compresses GCN [18] by re-moving nonlinearities and collapsing weight matrices be-tween consecutive layers. This shallow GNN can accelerate both the training and inference processes with comparable performance. Although SGC compresses the network pa-rameters, it does not compress the loaded data, which is the major memory consumption when processing the graphs with GNNs.
In this paper, to alleviate the memory and complexity is-sue, we pioneer to propose a binarized GCN, named Binary
Graph Convolutional Network (Bi-GCN), which is a sim-ple yet efﬁcient approximation of GCN [18], by binarizing the parameters and node attribute representations. Speciﬁ-cally, the binarization of the weights is performed by split-ting them into multiple feature selectors and maintaining a scalar per selector to further reduce the quantization er-rors. Similarly, the binarization of the node features can be carried out by splitting the node features and assigning an attention weight to each node. By employing those ad-ditional scalars, more efﬁcient information can be learned and retained efﬁciently. After binarizing the weights and node features, the computational complexity and the mem-ory consumptions induced by the network parameters and input data can be largely reduced. Since the existing binary back propagation method [22] has not considered the rela-tionships among the binary weights, we also design a new back propagation method by tackling this issue. An intuitive comparison between our Bi-GCN and the baseline methods is shown in Figure 1, which demonstrates that our Bi-GCN can achieve the fastest inference speed and lowest memory consumption with a comparable accuracy compared to the standard full-precision GNNs.
Our proposed Bi-GCN can reduce the redundancies in the node representations while maintain the principle infor-mation. When the number of layers increases, Bi-GCN also gives a more obvious reductions of the memory consump-tions of the parameters and effectively alleviates the over-ﬁtting problem. Besides, our binarization approach can be easily applied to other GNNs.
The contributions are summarized as follows:
• We pioneer to propose a binarized GCN, named Bi-nary Graph Convolutional Network (Bi-GCN), which can signiﬁcantly reduce the memory consumptions by
∼30x for both the network parameters and input node attributes, and accelerate the inference by an average of ∼47x, on the citation networks, theoretically.
• We design a new back propagation method to effec-tively train our Bi-GCN, by considering the relation-ships among the binary weights in the back propaga-tion process.
• With respect to the signiﬁcant memory reductions and accelerations, our Bi-GCN can also give a comparable performance compared to the standard GCN on four benchmark datasets. 2.