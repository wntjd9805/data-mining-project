Abstract
Video-based person re-identiﬁcation aims to match pedestrians from video sequences across non-overlapping
The key factor for video person re-camera views. identiﬁcation is to effectively exploit both spatial and tem-poral clues from video sequences. In this work, we propose a novel Spatial-Temporal Correlation and Topology Learn-ing framework (CTL) to pursue discriminative and robust representation by modeling cross-scale spatial-temporal correlation. Speciﬁcally, CTL utilizes a CNN backbone and a key-points estimator to extract semantic local features from human body at multiple granularities as graph nodes.
It explores a context-reinforced topology to construct multi-scale graphs by considering both global contextual infor-mation and physical connections of human body. Moreover, a 3D graph convolution and a cross-scale graph convolu-tion are designed, which facilitate direct cross-spacetime and cross-scale information propagation for capturing hi-erarchical spatial-temporal dependencies and structural in-formation. By jointly performing the two convolutions, CTL effectively mines comprehensive clues that are complemen-tary with appearance information to enhance representa-tional capacity. Extensive experiments on two video bench-marks have demonstrated the effectiveness of the proposed method and the state-of-the-art performance. 1.

Introduction
Person re-identiﬁcation (Re-ID) is an important technol-ogy to retrieve a person-of-interest across non-overlapping cameras. It has drawn increasing attention during the past few years, owing to its broad application in many realistic scenarios, such as video surveillance [11, 47] and behavior analysis [43] etc. However, this task remains challenging due to the variations in illumination, viewpoint and pose, as well as the inﬂuence of background clutter and occlusion.
Existing person Re-ID approaches are mainly divided
* Corresponding author.
Figure 1. Three example video sequences on MARS and iLIDS-VID datasets with partial occlusions, inaccurate detection and viewpoint variation. into two categories: image-based methods [39, 33, 41, 25] and video-based methods [36, 26, 22]. The former ex-ploits static images without temporal information to retrieve pedestrians. It has achieved impressive advances with the surge of deep learning technique in recent years [18]. How-ever, image-based person Re-ID heavily relies on the qual-ity of static images, which are sensitive to noise, occlusion and viewpoint variation, etc. Different from static images with limited content, video sequences contain rich spatial-temporal information across a long span of time, which can provide clean and informative clues against these problem
[22, 8]. Thus, video-based person Re-ID has the potential to solve the restrictions in image-based person Re-ID.
A typical video-based person Re-ID pipeline extracts and aggregates spatial and temporal clues from video se-quences to generate discriminative representations. Some preliminary methods [29, 10, 13, 52] extract appearance features from each frame independently, and aggregate them into video-level representation by temporal pooling layer or recurrent neural network (RNN). In presence of partial occlusions, inaccurate detection and viewpoint vari-4370
ation, the learned features are often corrupted, result in sig-niﬁcant performance degradation. Figure 1 illustrates some video sequences of pedestrians on MARS [52] and iLIDS-VID [42] datasets with these issues. Recent works attempt to address them by dividing video frames into horizon-tal rigid stripes [8, 6, 43] or utilizing attention mechanism
[22, 21, 36, 31, 14] to discover distinctive partial regions for extracting local appearance features. However, much back-ground noise is blended in their located partial regions, thus they can not learn precise aligned part features from videos
[53]. Considering that, a few works [3, 19, 9, 51] employ pose estimation model [30] to adaptively locate key-points of pedestrians for extracting aligned part features. How-ever, drastic viewpoint and pose variations as well as occlu-sion within videos affect the reliability of pose estimation model. Meanwhile, these methods only extract local fea-tures with ﬁxed semantics from one-granularity partition, which can not cover all discriminative clues. Further, all the aforementioned methods only model the temporal rela-tion across different frames, while neglecting complicated spatial-temporal dependencies and structural information of different body parts within a frame or across frames, re-stricting the capability of pedestrian representation.
In this work, we propose a novel Spatial-Temporal
Correlation and Topology Learning framework (CTL) for video-based person re-identiﬁcation, which pursues dis-criminative and robust representations. CTL extracts lo-cal features at multi-granularity levels to capture diverse discriminative semantics and alleviate unstable pose esti-mation results, and learns the potential cross-scale spatial-temporal dependencies and structure information among body parts for enhancing feature representation. Speciﬁ-cally, CTL employs a CNN backbone and a key-points esti-mator to extract semantic part features from human body at three granularities as graph nodes.
It then explores a context-skeleton enriched topology to construct multi-scale graphs by considering both global contextual infor-mation and physical connections of human body, which ef-fectively models the intrinsic spatial-temporal linkages be-tween nodes. Moreover, a 3D graph convolution and a cross-scale graph convolution are designed for these multi-scale graphs, which facilitate direct cross-spacetime and cross-scale information propagation for capturing hierar-chical spatial-temporal dependencies and structural infor-mation. By jointly performing the two convolutions, CTL effectively mines comprehensive and discriminative clues that are complementary with appearance information to en-rich representation. Extensive experiments on two video datasets, i.e., MARS and iLIDS-VID, have demonstrated the effectiveness of the proposed approach.
Although graph modeling has been explored in person
Re-ID, most of them only construct a graph on image-level
[33, 1, 45, 23] without considering temporal relation. A few of preliminary works [43, 44, 46] extend graph modeling to video person Re-ID. However, they neglect the spatial structural information within each frame [43, 44], or sim-ply utilize factorized spatial and temporal graph modeling
[46], failing to capture complex spatial-temporal relation.
Further, all of them essentially belong to a local method.
They utilize pair-wise feature afﬁnity to measure the link-age between two nodes, while ignoring the impact of global contextual information from all other nodes, which is sig-niﬁcance for learning reliable and useful graph topology.
The main contributions of this paper are as follow-ing: (1) We propose a novel Spatial-Temporal Correlation and Topology Learning framework (CTL) for person re-identiﬁcation in videos. (2) We learn a context-reinforced topology to construct multi-scale graphs by considering both global contextual information and physical connec-tions of human body. (3) We develop a 3D graph convolu-tion and a cross-scale graph convolution to model high-oder spatial-temporal dependencies and structural information. 2.