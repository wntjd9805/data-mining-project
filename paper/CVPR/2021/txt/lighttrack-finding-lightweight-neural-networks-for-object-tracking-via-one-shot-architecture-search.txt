Abstract
Object tracking has achieved signiﬁcant progress over the past few years. However, state-of-the-art trackers be-come increasingly heavy and expensive, which limits their deployments in resource-constrained applications. In this work, we present LightTrack, which uses neural architec-ture search (NAS) to design more lightweight and efﬁcient object trackers. Comprehensive experiments show that our
LightTrack is effective. It can ﬁnd trackers that achieve su-perior performance compared to handcrafted SOTA track-ers, such as SiamRPN++ [30] and Ocean [56], while us-ing much fewer model Flops and parameters. Moreover, when deployed on resource-constrained mobile chipsets, the discovered trackers run much faster. For example, on
Snapdragon 845 Adreno GPU, LightTrack runs 12× faster than Ocean, while using 13× fewer parameters and 38× fewer Flops. Such improvements might narrow the gap be-tween academic models and industrial deployments in ob-ject tracking task. LightTrack is released at here. 1.

Introduction
Object tracking is one of the most fundamental yet chal-lenging tasks in computer vision. Over the past few years, due to the rise of deep neural networks, object tracking has achieved remarkable progress. But meanwhile, track-ing models are becoming increasingly heavy and expensive.
For instance, the latest SiamRPN++ [30] and Ocean [56] trackers respectively utilize 7.1G and 20.3G model Flops as well as 11.2M and 25.9M parameters to achieve state-of-the-art performance, being much more complex than the early SiamFC [5] method (using 2.7G Flops and 2.3M pa-rameters), as visualized in Fig. 1. Such large model sizes and expensive computation costs hinder the deployment of tracking models in real-world applications, such as cam-era drones, industrial robotics, and driving assistant system, where model size and efﬁciency are highly constrained.
∗Equal contributions. Work performed when Bin and Kan are interns of MSRA. † Corresponding author: houwen.peng@microsoft.com. 790M FLOPs 35.7 EAO 36
LightTrack (Ours) 34 530M FLOPs 33.3 EAO 32
SiamRPN++ (MobileNet-V2)
Ocean-offline (ResNet-50)
SiamRPN++ (ResNet-50)
)
% (
O
A
E 9 1 0 2
T
O
V 30 28 26 24 22 20
SiamRPN (AlexNet)
SiamFC++ (GoogleNet) 2M 4M 8M
SiamDW-RPN (CIResNet-22) 16M 32M
Mobile Setting (< 600M FLOPs)
SiamFC (AlexNet)
AutoML
Handcrafted 64M
T h e h i g h e r t h e b e t t e r 18 400 600 7000 2000 1000 40000 20000 10000 100000
The lower the better 3000 4000
Model FLOPs (Million)
Figure 1: Comparisons with state-of-the-art trackers in terms of EAO performance, model Flops and parameters on
VOT-19 benchmark. The circle diameter is in proportion to the size of model parameter. The proposed LightTrack is su-perior than SiamFC [5], SiamRPN [31], SiamRPN++ [30],
SiamFC++ [52] and Ocean [56], while using much fewer
Flops and parameters. Best viewed in color.
There are two straightforward ways to tackle the com-plexity and efﬁciency issues. One is model compression, while the other is compact model designing. Existing off-the-shelf compression techniques such as pruning and quan-tization can reduce model complexity, while they inevitably bring non-negligible performance degradation due to infor-mation loss [21, 38]. On the other hand, handcrafting new compact and efﬁcient models is engineering expensive and heavily relies on human expertise and experience [55, 15].
This paper introduces a new solution – automating the design of lightweight models with neural architecture search (NAS), such that the searched trackers can be car-ried out in an efﬁcient fashion on resource-limited hard-ware platforms. It is non-trivial because that object track-ers typically need ImageNet pre-training, while NAS algo-rithms require the performance feedback on the target track-ing task as supervision signals. Based upon recent one-15180              
shot NAS [41, 4, 20], we propose a new search algorithm dedicated to object tracking task, called LightTrack. It en-codes all possible architectures into a backbone supernet and a head supernet. The backbone supernet is pre-trained on ImageNet then ﬁne-tuned with tracking data, while the head supernet is directly trained on tracking data. The su-pernets are trained only once, then each candidate architec-ture inherits its weights from the supernets directly. Archi-tecture search is performed on the trained supernets, using tracking accuracy and model complexity as the supervision guidance. On the other hand, to reduce model complexity, we design a search space consisting of lightweight build-ing blocks, such as depthwise separable convolutions [11] and inverted residual structure [45, 23]. Such search space allows the one-shot NAS algorithm to search for more com-pact neural architectures, striking a balance between track-ing performance and computational costs.
Comprehensive experiments verify that LightTrack is ef-fective.
It is able to search out efﬁcient and lightweight object trackers. For instance, LightTrack ﬁnds a 530M
Flops tracker, which achieves an EAO of 0.33 on VOT-19 benchmark, surpassing the SOTA SiamRPN++ [30] by 4.6% while reducing its model complexity (48.9G Flops) by 98.9%. More importantly, when deployed on resource-limited chipsets, such as edge GPU and DSP, the discov-ered tracker performs very competitive and runs much faster than existing methods. On Snapdragon 845 Adreno 630
GPU [3], our LightTrack runs 12× faster than Ocean [56] (38.4 v.s. 3.2 fps), while using 13× fewer parameters (1.97 v.s. 25.9 M) and 38× fewer Flops (530 v.s. 20,300 M). Such improvements enable deep tracking models to be easily de-ployed and run at real-time speed on resource-constrained hardware platforms.
This work makes the following contributions.
• We present the ﬁrst effort on automating the design of neural architectures for object tracking. We develop a new formulation of one-shot NAS and use it to ﬁnd promising architectures for tracking.
• We design a lightweight search space and a dedicated search pipeline for object tracking. Experiments verify the proposed method is effective. Besides, the searched trackers achieve state-of-the-art performance and can be deployed on diverse resource-limited platforms. 2.