Abstract
Learning to model and reconstruct humans in clothing is challenging due to articulation, non-rigid deformation, and varying clothing types and topologies. To enable learn-ing, the choice of representation is the key. Recent work uses neural networks to parameterize local surface ele-ments. This approach captures locally coherent geome-try and non-planar details, can deal with varying topol-ogy, and does not require registered training data. How-ever, naively using such methods to model 3D clothed hu-mans fails to capture ﬁne-grained local deformations and generalizes poorly. To address this, we present three key innovations: First, we deform surface elements based on a human body model such that large-scale deformations caused by articulation are explicitly separated from topo-logical changes and local clothing deformations. Second, we address the limitations of existing neural surface ele-ments by regressing local geometry from local features, sig-niﬁcantly improving the expressiveness. Third, we learn a pose embedding on a 2D parameterization space that en-codes posed body geometry, improving generalization to unseen poses by reducing non-local spurious correlations.
We demonstrate the efﬁcacy of our surface representation by learning models of complex clothing from point clouds. The clothing can change topology and deviate from the topol-ogy of the body. Once learned, we can animate previously unseen motions, producing high-quality point clouds, from which we generate realistic images with neural rendering.
We assess the importance of each technical contribution and show that our approach outperforms the state-of-the-art methods in terms of reconstruction accuracy and infer-ence time. The code is available for research purposes at https://qianlim.github.io/SCALE. 1.

Introduction
While models of humans in clothing would be valuable for many tasks in computer vision such as body pose and shape estimation from images and videos [9, 15, 31, 32, 35, 36] and synthetic data generation [60, 61, 71, 83], most ex-isting approaches are based on “minimally-clothed” human body models [2, 30, 42, 49, 54, 75], which do not repre-sent clothing. To date, statistical models for clothed hu-mans remain lacking despite the broad range of potential
† Now at Facebook Reality Labs. 16082
applications. This is likely due to the fact that modeling 3D clothing shapes is much more difﬁcult than modeling body shapes. Fundamentally, several characteristics of clothed bodies present technical challenges for representing cloth-ing shapes.
The ﬁrst challenge is that clothing shape varies at dif-ferent spatial scales driven by global body articulation and local clothing geometry. The former requires the repre-sentation to properly handle human pose variation, while the latter requires local expressiveness to model folds and wrinkles. Second, a representation must be able to model smooth cloth surfaces and also sharp discontinuities and thin structures. Third, clothing is diverse and varies in terms of its topology. The topology can even change with the mo-tion of the body. Fourth, the relationship between the cloth-ing and the body changes as the clothing moves relative to the body surface. Finally, the representation should be com-patible with existing body models and should support fast inference and rendering, enabling real-world applications.
Unfortunately, none of the existing 3D shape representa-tions satisfy all these requirements. The standard approach uses 3D meshes that are draped with clothing using physics simulation [3, 38, 41]. These require manual clothing de-sign and the physics simulation makes them inappropriate for inference. Recent work starts with classical rigged 3D meshes and blend skinning but uses machine learning to model clothing shape and local non-rigid shape deforma-tion. However, these methods often rely on pre-deﬁned gar-ment templates [8, 37, 45, 53], and the ﬁxed correspondence between the body and garment template restricts them from generalizing to arbitrary clothing topology. Additionally, learning a mesh-based model requires registering a com-mon 3D mesh template to scan data. This is time consum-ing, error prone, and limits topology change [56]. New neu-ral implicit representations [12, 46, 51], on the other hand, are able to reconstruct topologically varying clothing types
[13, 16, 65], but are not consistent with existing graphics tools, are expensive to render, and are not yet suitable for fast inference. Point clouds are a simple representation that also supports arbitrary topology [21, 39, 77] and does not require data registration, but highly detailed geometry re-quires many points.
A middle ground solution is to utilize a collection of parametric surface elements that smoothly conform to the global shape of the target geometry [20, 25, 80, 82, 84].
As each element can be freely connected or disconnected, topologically varying surfaces can be effectively modeled while retaining the efﬁciency of explicit shape inference.
Like point clouds, these methods can be learned without data registration.
However, despite modeling coherent global shape, exist-ing surface-element-based representations often fail to gen-erate local structures with high-ﬁdelity. The key limiting factor is that shapes are typically decoded from global la-tent codes [25, 80, 82], i.e. the network needs to learn both the global shape statistics (caused by articulation) and a prior for local geometry (caused by clothing deformation) at once. While the recent work of [24] shows the ability to handle articulated objects, these methods often fail to capture local structures such as sharp edges and wrinkles, hence the ability to model clothed human bodies has not been demonstrated.
In this work, we extend the surface element represen-tation to create a clothed human model that meets all the aforementioned desired properties. We support articulation by deﬁning the surface elements on top of a minimal clothed body model. To densely cover the surface, and effectively model local geometric details, we ﬁrst introduce a global patch descriptor that differentiates surface elements at dif-ferent locations, enabling the modeling of hundreds of local surface elements with a single network, and then regress local non-rigid shapes from local pose information, produc-ing folding and wrinkles. Our new shape representation,
Surface Codec of Articulated Local Elements, or SCALE, demonstrates state-of-the-art performance on the challeng-ing task of modeling the per-subject pose-dependent shape of clothed humans, setting a new baseline for modeling topologically varying high-ﬁdelity surface geometry with explicit shape inference. See Fig. 1.
In summary, our contributions are: (1) an extension of surface element representations to non-rigid articulated ob-ject modeling; (2) a revised local elements model that gen-erates local geometry from local shape signals instead of a global shape vector; (3) an explicit shape representation for clothed human shape modeling that is robust to vary-ing topology, produces high-visual-ﬁdelity shapes, is eas-ily controllable by pose parameters, and achieves fast in-ference; and (4) a novel approach for modeling humans in clothing that does not require registered training data and generalizes to various garment types of different topology, addressing the missing pieces from existing clothed human models. We also show how neural rendering is used to-gether with our point-based representation to produce high-quality rendered results. The code is available for research purposes at https://qianlim.github.io/SCALE. 2.