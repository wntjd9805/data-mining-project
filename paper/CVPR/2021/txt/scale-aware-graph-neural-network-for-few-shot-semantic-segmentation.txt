Abstract
Few-shot semantic segmentation (FSS) aims to segment unseen class objects given very few densely-annotated sup-port images from the same class. Existing FSS methods ﬁnd the query object by using support prototypes or by directly relying on heuristic multi-scale feature fusion. However, they fail to fully leverage the high-order appearance rela-tionships between multi-scale features among the support-query image pairs, thus leading to an inaccurate localiza-tion of the query objects. To tackle the above challenge, we propose an end-to-end scale-aware graph neural network (SAGNN) by reasoning the cross-scale relations among the support-query images for FSS. Speciﬁcally, a scale-aware graph is ﬁrst built by taking support-induced multi-scale query features as nodes and, meanwhile, each edge is mod-eled as the pairwise interaction of its connected nodes. By progressive message passing over this graph, SAGNN is capable of capturing cross-scale relations and overcom-ing object variations (e.g., appearance, scale and location), and can thus learn more precise node embeddings. This in turn enables it to predict more accurate foreground ob-jects. Moreover, to make full use of the location relations across scales for the query image, a novel self-node col-laboration mechanism is proposed to enrich the current node, which endows SAGNN the ability of perceiving differ-ent resolutions of the same objects. Extensive experiments on PASCAL-5i and COCO-20i show that SAGNN achieves state-of-the-art results. 1.

Introduction
Deep convolutional neural networks [14,29,40] have ad-vanced the development of many downstream vision tasks, such as semantic segmentation [1, 3, 21, 25, 49], which is a dense prediction task. To achieve an effective segmentation model, e.g. Deeplab [3], large amounts of pixel-wise im-age annotations are required for model training, which are costly and time consuming to acquire. Weakly supervised
*Corresponding author
Support Image (a)
Query Image (b)
Support Image
Shared
Backbone
Shared
Backbone
Shared
Backbone
Shared
Backbone
⊙Multi-scale 
Feature Modeling
Global or Multiple 
Local Prototypes
Prediction
Prediction
⊙Scale-Aware 
Node Embedding
Scale-Aware 
Graph (cid:2273)
Query Image
Embedded Nodes
Cross-Scale Relation Reasoning on  (cid:2321)
Figure 1. Comparisons of SAGNN and existing FSS models. (a)
Existing methods usually ﬁrst generate a global or multiple local prototypes by masked averaging/clustering over the foreground of the support image. Then these support prototypes are applied to query features (and/or their multi-scale counterparts) for the later prediction of the query mask. However, this paradigm segments the human region to horse. This happens because it is difﬁcult to fully capture the object variations (e.g., the scale, appearance, and spatial location of horse in the support and query images are differ-ent from each other) by such limited prototypes. (b) Our SAGNN can well estimate the foreground horse in the query image. It bene-ﬁts from our scale-aware node embedding and cross-scale relation reasoning on the scale-aware graph. Zoom in for details. learning [23, 43] can alleviate the annotation costs to some extent; however, the model performance drops signiﬁcantly under this scenario. Moreover, both fully and weakly super-vised models suffer poor generalization to unseen domains with only a few densely-annotated training images avail-able. As such, few-shot semantic segmentation (FSS) [27] is proposed for dealing with the unseen object segmentation and data annotation issue.
FSS aims to segment the foreground object of an unseen class in a query image by merely utilizing a few (one) anno-tated support images (image), where both the support/query images share a common unseen class. Typically, a base training set with annotations, which has different object categories from the unseen class set, can assist the learn-ing of the FSS model. Inspired by few-shot classiﬁcation and meta-learning [30], the segmentor can be trained by 5475
episode-based meta-training – i.e., each episode (sub-task) is sampled from the base training set and consists of support and query images – to mimic the testing scheme on the un-seen classes. During each sub-task, support images are used as guidance to segment the query image. The difference be-tween the ground-truth mask and the predicted query mask can thus supervise the model training.
Speciﬁcally, most of the existing FSS models [5,7,22,28, 34] adopt a two-stream (support and query) metric-based network. The global [48] or multiple local [10, 20, 41] prototypes (Fig. 1(a)) for a class are ﬁrst calculated from the support branch, based on the ground-truth mask of the support image. Then, these prototypes are applied to the query feature map (e.g., by cosine similarity) from the query branch, thus leading to a support-induced matching score map, which serves as an important clue for encoding the query mask. At ﬁrst, since the above process is a few-to-many matching, it is difﬁcult to fully capture the object scale/appearance/location variations by such limited pro-totypes (the human region is mis-segmented as horse in
Fig. 1(a)).
In addition, in the query branch, multi-scale features [19, 31], e.g., ASPP [4], have been widely-used for accurate foreground estimates. However, the high-order appearance relationships among them have still not been completely leveraged for better inferring query ob-jects. Moreover, some works [33, 42, 46] propose a pixel-to-pixel matching paradigm among support/query images based on non-local attention variants [36], which still suf-fers from the object scale variations. For instance, if the horse region in the support image is very small, most loca-tions of the segmented query horse will correspond to the same location from the support horse region.
To address the above challenges, we propose an end-to-end scale-aware graph neural network (SAGNN) (§3.3) to explicitly and comprehensively reason the high-order ap-pearance relationships across the multi-scale support-query features for FSS. As in Fig. 1(b), since FSS is a struc-tural prediction task, SAGNN models the object variations among support-query images in a structure-to-structure manner. Here, structure means 1) each node in SAGNN is a whole feature map (the embedded nodes in Fig. 1(b)) cor-responding to a speciﬁc scale; and 2) the node features are updated in a holistic manner by aggregating neighborhood node features (cross-scale relation reasoning in Fig. 1(b)).
In this way, SAGNN can preserve and transfer more struc-tural context from the seen to the unseen domains. Specif-ically, we ﬁrst build a scale-aware graph with each node representing a support-induced multi-scale query feature.
Meanwhile, the edge in the graph is the pairwise interaction of its connected nodes. By message passing to encourage favorable information exchange among support-query pairs over this graph, SAGNN can capture higher-order cross-scale relationships and overcome harmful object variations (e.g., scale, appearance, and spatial location variations for the horse object in Fig. 1), and thus lead to a precise lo-calization of query objects in a structural way. Further-more, we propose a novel self-node collaboration mecha-nism (Fig. 3) for enriching the current node features during feature aggregation, which can make full use of the location relations across scales for the query image. To sum up, our main contributions are:
• We propose a scale-aware graph neural network (SAGNN) which explicitly reasons the cross-scale rela-tionships among support-query images in a structure-to-structure manner, for tackling the FSS task. To the best of our knowledge, this is the ﬁrst work to do this in the FSS domain.
• We propose a novel self-node collaboration mechanism for enriching the current node features during feature ag-gregation, which can bring to SAGNN a signiﬁcant perfor-mance gain.
• We set new state-of-the-art results on two FSS bench-marks. Our SAGNN solves FSS from the perspective of structural modeling, which sheds light for future research in this ﬁeld. 2.