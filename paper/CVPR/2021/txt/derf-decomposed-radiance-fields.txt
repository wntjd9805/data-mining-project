Abstract
With the advent of Neural Radiance Fields (NeRF), neu-ral networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their ap-plicability in practical scenarios.
In this paper, we pro-pose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter’s
Algorithm for efﬁcient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3× more efﬁcient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0 dB in PSNR (for the same inference cost). 1.

Introduction
While high-quality rendering of virtual scenes has long been associated with traditional computer graphics [16, 14], there have been promising developments in using neural net-works for photo-realistic rendering [21, 9, 11, 13]. These neural rendering methods have the potential to reduce the amount of human interaction that is needed to digitize the real world. We believe the further development of neural scene representations will increase the viability of 3D con-tent creation in-the-wild and continue pushing neural render-ing to higher levels of life-like detail.
Among existing neural rendering methods, those that operate in 3D have lately drawn much interest [19, 9, 13].
Unlike those based on convolutional neural networks [6], these methods do not operate in image-space, and rather train volumetric representations of various types: they de-Figure 1. We render a scene (a) from a decomposed neural repre-sentation (b), consisting of a collection of spatially localized neural networks. Each of these networks render a convex portion of the im-age, and these are then composited into the output via the Painter’s
Algorithm. Depending on the level of decomposition, this can lead to faster rendering, or to renderings that have the same runtime, but contain sharper details. In (c), we decompose the scene into 16 parts, which leads to sharper details than (d), with similar runtime.
ﬁne functions that can be queried in space during a volume rendering operation. This is essential, as volume rendering introduces an inductive bias towards rendering phenomena, so that effects like occlusion and parallax are modeled by construction, rather than being emulated by image-space operations.
However, neural volume rendering is far from being a fully developed technology. The two development axes are 14153
Figure 2. Diminishing returns – We sweep through network architectures varying by depth and width to show how the gains in quality diminish with increased capacity. The total number of network parameters varies linearly with network depth (left) and quadratically with the number of units in each layer (right). All networks trained for 300k iterations on the NeRF “room” scene. generality (removing assumptions about the input, hence al-lowing their application to more general visual phenomena) and performance (increasing the efﬁciency of training and/or inference). In this paper, we focus on performance, and speciﬁcally the inference performance. Hence, the natural question then becomes “why are neural volume rendering models so incredibly slow?” Let us consider Neural Radi-ance Fields (NeRF) [13] as the cardinal example. These method requires hundreds of MLP invocations per pixel to compute the samples needed by volume rendering. This results in an extremely compute-intensive inference process needing ≈108 network evaluations and minutes of computa-tion to render a one megapixel image on a modern NVIDIA
RTX 2080 Ti accelerator.
Naturally, to accelerate inference one could trade away model capacity, but doing so na¨ıvely results in lower render-ing quality. However, as illustrated in Figure 2, there are diminishing returns regarding how the capacity of neural networks (i.e. number of layers or number of neurons per layer) affects ﬁnal rendering quality. We take advantage of this phenomena and accelerate neural rendering by dividing the scene into multiple areas (i.e. spatial decomposition), and employing a small(er) networks in each of these areas.
Due to hardware limitations in the memory architecture of accelerators, not all decompositions are appropriate. For ex-ample, a random decomposition of the volume would result in random sub-network invocations. Coalescing invocations so that memory access are contiguous is possible, but in our experiments we found the re-ordering operations are not sufﬁciently fast, and any efﬁciency gain from using smaller networks was lost. Hence, our question becomes “Can we design a spatial decomposition that minimizes the chance of random memory access?” We address this question by noting that we can elegantly overcome these limitations if we decompose space with Voronoi Diagrams [3, Ch. 7]. More speciﬁcally, the convex cells of the Voronoi diagram can be rendered independently, after which the Painter’s Algo-rithm [3, Ch. 12] can be used to composite the ﬁnal image.
We formulate our Voronoi decomposition to be differ-entiable, and train end-to-end to ﬁnd an optimal cell ar-rangement. By doing so we increase the efﬁciency of the rendering process by up to a factor of three without any loss in rendering quality. Alternatively, with the same rendering cost, we enhance the rendering quality by a PSNR of up to 1.0dB (recall that Peak Signal to Noise Ratio is expressed in log-scale).
Contributions. To summarize, our main contributions are:
• We highlight the presence of diminishing returns for net-work capacity in NeRF, and propose spatial decomposi-tions to address this issue.
• We demonstrate how a decomposition based on Voronoi
Diagrams may be learned to optimally represent a scene.
• We show how this decomposition allows the whole scene to be rendered by rendering each part independently, and compositing the ﬁnal image via Painter’s Algorithm.
• In comparison to the NeRF baseline, these modiﬁcations result in improvement of rendering quality for the same computational budget, or faster rendering of images given the same visual quality. 2.