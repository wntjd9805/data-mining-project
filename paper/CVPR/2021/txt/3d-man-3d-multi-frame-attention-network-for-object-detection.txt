Abstract 3D object detection is an important module in au-tonomous driving and robotics. However, many existing methods focus on using single frames to perform 3D de-tection, and do not fully utilize information from multi-ple frames.
In this paper, we present 3D-MAN: a 3D multi-frame attention network that effectively aggregates features from multiple perspectives and achieves state-of-the-art performance on Waymo Open Dataset. 3D-MAN
ﬁrst uses a novel fast single-frame detector to produce box proposals. The box proposals and their corresponding fea-ture maps are then stored in a memory bank. We design a multi-view alignment and aggregation module, using atten-tion networks, to extract and aggregate the temporal fea-tures stored in the memory bank. This effectively combines the features coming from different perspectives of the scene.
We demonstrate the effectiveness of our approach on the large-scale complex Waymo Open Dataset, achieving state-of-the-art results compared to published single-frame and multi-frame methods. 1.

Introduction 3D object detection is an important problem in com-puter vision as it is widely used in applications, such as au-tonomous driving and robotics. Autonomous driving plat-forms require precise 3D detection to build an accurate rep-resentation of the world, which is in turn used in down-stream models that make critical driving decisions.
LiDAR provides a high-resolution accurate 3D view of the world. However, at any point of time, the LiDAR sensor collects only a single perspective of the scene. It is often the case that the LiDAR points detected on an observed object correspond to only a partial view of it. Detecting these par-tially visible instances is an ill-posed problem because there exist multiple reasonable predictions (shown as red and blue boxes in the upper row of Figure 1). These potential am-biguous scenarios can be a bottleneck for single-frame 3D detectors (Table 1).
∗Work done during an internship at Google Brain. t-3 t-2 t-1 t
Figure 1. Upper row: Potential detections given LiDAR from a single frame demonstrating ambiguity between many reasonable predictions. Lower row: After merging the points aligned across 4 frames, there is more certainty for the correct box prediction.
IoU threshold
AP (%) 0.3 94.72 0.5 88.97 0.7 63.27
Table 1. We vary the intersection-over-union (IoU) threshold for considering a predicted box correctly matched to a ground-truth box, and measure the performance of the PointPillars model on the
Waymo Open Dataset’s validation set. A lower IoU threshold cor-responds to allowing less accurate boxes to match. This shows that improving the box localization could signiﬁcantly improve model performance.
In the autonomous driving scenario, as the vehicle pro-gresses, the sensors pick up multiple views of the world, making it possible to resolve the aforementioned localiza-tion ambiguity. Multiple frames across time can provide different perspectives of an observed object instance. An effective multi-frame detection method should be able to ex-tract relevant features from each frame and aggregate them, so as to obtain a representation that combines multiple per-spectives (Figure 1). Research in 3D multi-frame detec-tion has been limited due to a lack of available datasets with well-calibrated multi-frame data. Fortunately, recently released large-scale 3D sequence datasets (NuScenes [2],
Waymo Open Dataset [23]) have made such data available.
A straight-forward approach to fusing multi-frame point 11863
Model 1-frame 4-frames 8-frames
Stationary (%) 60.01 62.4 63.7
Slow (%) Medium (%) 66.64 67.39 67.98 65.02 66.68 66.29
Fast (%) 71.90 77.99 72.30
Table 2. Velocity breakdowns of vehicle AP metrics for PointPil-lars models using point concatenation. For the 8-frame model, we ﬁnd that its beneﬁts come from slow-moving vehicles. Fast-moving objects no longer beneﬁt from a large number of frames since the LiDAR points are no longer aligned across the frames. clouds is to use point concatenation, which simply com-bines points across different frames together [2]. The com-bined point cloud is then used as input to a single-frame de-tector. This approach works well for static and slow-moving objects since the limited movement implies that the LiDAR points will be mostly aligned across the frames. However, when objects are fast-moving or when longer time horizons are considered, this approach may not be as effective since the LiDAR points are no longer aligned (Table 2).
As an alternative to point concatenation, Fast-and-furious [14] attempts to fuse information across frames by concatenating at a feature map level. However, this still runs into the same challenge with misaligned feature maps for fast-moving objects and longer time horizons. Recent approaches [10, 34] propose using recurrent layers such as
Conv-LSTM or Conv-GRU to aggregate the information across frames. It turns out that these recurrent approaches are often computationally expensive.
Our Approach. We propose 3D-MAN: a 3D multi-frame attention network that is able to extract relevant features from past frames and aggregate them effectively. 3D-MAN has three components: (i) a fast single-frame detector, (ii) a memory bank, and (iii) a multi-view alignment and aggre-gation module.
The fast single-frame detector (FSD) is an anchor-free one-stage detector with a novel learning strategy. We show that a max-pooling based non-maximum suppres-sion (NMS) algorithm together with a novel Hungarian-matching based loss is an effective method to generate high-quality proposals at real-time speeds. These proposals and the last feature map from FSD are then fed into a memory bank. The memory bank stores both predicted proposals and feature maps in previous frames so as to maintain dif-ferent perspectives for each instance across frames.
The stored proposals and features in the memory bank are ﬁnally fused together through the multi-view alignment and aggregation module (MVAA), which produces fused multi-view features for target proposals that are used to regress bounding boxes for ﬁnal predictions. MVAA has two stages: a multi-view alignment stage followed by a multi-view aggregation stage. The alignment stage works on each stored frame independently; it uses target propos-als as queries into a stored frame to extract relevant fea-tures. The aggregation stage then merges across frames for each target proposal independently. This can be viewed as a form of factorization over the attention across proposals and frames.
We evaluate our model on large-scale Waymo Open
Dataset [23]. Experimental results demonstrate that our method outperforms published state-of-the-art single-frame methods and multi-frame methods. Our primary contribu-tions are listed below.
Key Contributions.
• We propose 3D-MAN: a 3D multi-frame attention net-work for object detection. We demonstrate that our method achieves state-of-the-art performance on the
Waymo Open Dataset [23] and provide thorough ab-lation studies.
• We introduce a novel training strategy for a fast single-frame detector method that uses max-pooling to per-form non-maximum suppression and a variant of Hun-garian matching to compute a detection loss.
• We design an efﬁcient multi-view alignment and ag-gregation module to extract and aggregate relevant fea-tures from multiple frames in a memory bank. This module produces features containing information from multiple perspectives that perform well for classiﬁca-tion and bounding box regression. 2.