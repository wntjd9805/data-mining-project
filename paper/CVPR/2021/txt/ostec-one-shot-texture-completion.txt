Abstract
The last few years have witnessed the great success of non-linear generative models in synthesizing high-quality photorealistic face images. Many recent 3D facial texture reconstruction and pose manipulation from a single im-age approaches still rely on large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Yet the collection of such a large scale high-resolution 3D texture dataset is still very costly and difﬁcult to maintain age/ethnicity balance. Moreover, regression-based approaches suffer from generalization to the in-the-wild conditions and are unable to ﬁne-tune to a target-image. In this work, we propose an unsupervised approach for one-shot 3D facial texture completion that does not re-quire large-scale texture datasets, but rather harnesses the knowledge stored in 2D face generators. The proposed ap-proach rotates an input image in 3D and ﬁll-in the unseen regions by reconstructing the rotated image in a 2D face generator, based on the visible parts. Finally, we stitch the most visible textures at different angles in the UV image-plane. Further, we frontalize the target image by project-ing the completed texture into the generator. The qual-itative and quantitative experiments demonstrate that the completed UV textures and frontalized images are of high quality, resembles the original identity, can be used to train a texture GAN model for 3DMM ﬁtting and improve pose-invariant face recognition.1 1.

Introduction
The problem of 3D face texture completion (as shown in
Fig. 2) refers generally to the problem of recovering near ear-to-ear visible and non-visible colour from a single im-age [11] in a “canonical”, deformation-free parameteriza-tion of the face surface (usually referred as UV-space). A very similar problem is that of producing arbitrary face ro-tations from a single image [51, 5]. Both of the above problems have important applications in many different do-mains of face analysis such as pose-invariant face recogni-tion [11, 5], as well developing of 3D Morphable Model (3DMM) algorithms [6, 19] and creating complete head 1Project Page: https://github.com/barisgecer/OSTeC 7628
avatars from single images [32]. That is why 3D face tex-ture completion, as well as, producing face rotations has been very popular in the intersection of machine learning and computer vision, offering an important application do-main to the advancements of machine learning in each era (from robust component analysis [38] to modern deep learn-ing [11, 51]).
The problem of predicting the missing colour in the texture coordinated of the UV space or predicting a new view from a single image has been the application domain of many machine learning algorithms starting from sim-ple nearest-neighbour interpolation, (i.e. Fig. 2c), regres-sion techniques using linear-statistical priors (e.g., Robust
Principal Component Analysis [7]) to modern deep learn-ing regression techniques such as image-to-image transla-tion models using conditional Generative Adversarial Net-works (GANs) [27]. The problem has been modeled as fully supervised, i.e. the regression model was trained with pairs of missing and complete 3D facial texture [11], or re-cently using self-supervised methods and image rendering
[51]. Nevertheless, fully-supervised or self-supervised, to the best of our knowledge, all current methods belong in the family of regression techniques.
Contrary to the above, we take a radically different line of work in this paper: We propose to re-think the 3D fa-cial texture prediction and rotation generation as an op-timisation problem and design our method as a one-shot texture completion approach. One of the key problems of regression-based approaches such as [51] is that they may lose the identity because the function they learn is quite generic. Contrary, our approach optimises, along-side many other functions, identity-related features. Our method pro-duces visually stunning results in both 3D texture comple-tion as well as frontalization (for some results please inspect
Fig. 1). Another by-product of our method is a 3D texture model learned from in-the-wild images that, as we show, can be used for training state-of-the-art 3D face reconstruc-tion algorithms such as GANFit [19] (which was trained with around 10K 3D faces captured in well-controlled con-ditions which are not released to the public).
In short, the contributions of our paper are as follows:
• We re-design the problem of 3D facial texture com-pletion as a one-shot optimisation-based approach.
We propose a well-engineered novel methodology and cost function suitable for the task.
• We capitalize on the power of 2D face generators to recover unseen part of 2D face by rotating it in 3D. So that, there would no need for 3D data collection.
• We show the effectiveness of the proposed approach in qualitative and quantitative experiments. Additionally, we apply the method to many in-the-wild images in or-der to train a large-scale prior of the 3D facial texture which we use to train state-of-the-art 3D face recon-struction algorithms. 2.