Abstract
Feature alignment is an approach to improving robust-ness to distribution shift that matches the distribution of feature activations between the training distribution and test distribution. A particularly simple but effective approach to feature alignment involves aligning the batch normalization statistics between the two distributions in a trained neural network. This technique has received renewed interest lately because of its impressive performance on robustness bench-marks. However, when and why this method works is not well understood. We investigate the approach in more de-tail and identify several limitations. We show that it only signiﬁcantly helps with a narrow set of distribution shifts and we identify several settings in which it even degrades performance. We also explain why these limitations arise by pinpointing why this approach can be so effective in the ﬁrst place. Our ﬁndings call into question the utility of this ap-proach and Unsupervised Domain Adaptation more broadly for improving robustness in practice. 1.

Introduction
A foundational assumption made in most of machine learning is that the training distribution is identical to the test distribution. However, this assumption is commonly violated in practice, which can substantially decrease the performance of models [10, 24]. This can be especially problematic in high-stakes applications such as autonomous vehicles. One way of improving robustness is to exploit unlabeled test data to adapt the model to the new distribution. This process is called Unsupervised Domain Adaptation (UDA) [32].
A common approach in UDA, known as feature align-ment or domain alignment, is to align the feature acti-vations between the source and target distributions [5– 8, 13, 17, 20, 26, 29, 30, 32]. Feature alignment has also been applied beyond UDA in domains such as causal infer-ence [15, 28]. Simple forms of feature alignment normalize the features of a trained model so that the training set and test set have the same ﬁrst and second order statistics in some feature space [19, 30], while other approaches match distributions in more complicated ways, such as by being indistinguishable to an adversarial discriminator [8, 20].
We focus on one simple feature alignment method: Adap-tive Batch Normalization (AdaBN) [19]. Like many other popular and effective feature alignment methods (e.g. Cari-ucci et al. [5], Roy et al. [26], Sun and Saenko [31], Sunet al. [30], Wang et al. [33]), AdaBN is normalization-based, meaning it matches ﬁrst and second order statistics between the two feature distributions. It is also a post-hoc method, meaning it aligns features for a model that has already been trained, making it particularly simple and applicable even for unforseen distribution shifts. Given a neural network trained on source data with Batch Normalization (BN) [14], AdaBN re-estimates the BN statistics of that model using the target data. In other words, AdaBN aligns the mean and variance of each channel in the network across the two distributions.
Despite its simplicity, in recent work Nado et al. [21],
Schneider et al. [27] showed that aligning batch norm statis-tics between the train and test distributions can be used to achieve state-of-the-art accuracy on the robustness bench-mark ImageNet-C [10]. Schneider et al. [27] argue that we should therefore start using normalization-based feature alignment methods whenever we evaluate robustness. Nado et al. [21] additionally ﬁnd that aligning BN statistics does not help as much for some other types of distribution shift.
However, neither paper describes why this method works well on ImageNet-C or why it does not help as much with other types of distribution shift.
We build on this work by investigating when and why methods like AdaBN help. Our ﬁndings include:
•
•
•
Showing that aligning BN statistics can actually de-grade accuracy on several types of distribution shift, both conceptually and in practice.
Identifying implicit symmetry assumptions made by these methods and showing how violations of these assumptions can cause performance degradation.
Demonstrating and explaining how aligning BN statis-tics primarily helps with distribution shifts that involve changes in local image statistics.
Our ﬁndings have several implications. While aligning
BN statistics is an effective method for improving robustness 12525
in some settings, it only signiﬁcantly helps on a narrow set of distribution shifts and can even degrade performance. These limitations may prevent it from being useful in practical ap-plications. Furthermore, we ﬁnd that existing justiﬁcations of feature alignment are inadequate for explaining when and why these methods work. Future work on UDA should explicitly identify the properties of data distributions and neural networks that these methods rely on in practice. Fi-nally, some of our ﬁndings apply to UDA more broadly, calling into question whether UDA is a strong approach to improving the robustness of machine learning systems in the
ﬁrst place. More work is therefore needed to make UDA practical for improving robustness. 2.