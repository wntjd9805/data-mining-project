Abstract
Automated diagnosis using deep neural networks in chest life-radiography can help radiologists detect threatening diseases. However, existing methods only pro-vide predictions without accurate explanations, undermin-ing the trustworthiness of the diagnostic methods. Here, we present XProtoNet, a globally and locally interpretable di-agnosis framework for chest radiography. XProtoNet learns representative patterns of each disease from X-ray images, which are prototypes, and makes a diagnosis on a given X-ray image based on the patterns. It predicts the area where a sign of the disease is likely to appear and compares the features in the predicted area with the prototypes. It can provide a global explanation, the prototype, and a local explanation, how the prototype contributes to the prediction of a single image. Despite the constraint for interpretability,
XProtoNet achieves state-of-the-art classiﬁcation perfor-mance on the public NIH chest X-ray dataset. 1.

Introduction
Chest radiography is the most widely used imaging examination for diagnosing heart and other chest dis-eases [13]. Detecting a disease through chest radiography is a challenging task that requires professional knowledge and careful observation. Various automated diagnostic methods have been proposed to reduce the burden placed on radiol-ogists and the likelihood of mistakes; methods using deep neural networks (DNNs) have achieved especially high lev-els of performance in recent decades [10, 12, 17, 23, 32].
However, the black-box characteristics of DNNs discourage users from trusting DNN predictions [4, 22]. Since medical decisions may have life-or-death consequences, medical-diagnosis applications require not only high performance but also a strong rationale for judgment. Although many automated diagnostic methods have presented localization as an explanation for prediction [12, 20, 23, 31, 32], this
∗Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>.
Figure 1. Our proposed framework, XProtoNet, learns prototypes that are used to identify each disease. Given an input image,
XProtoNet compares the feature in the occurrence area of the input image with the prototypes and thereby diagnoses diseases. Yellow contours denote the learned prototypes. provides only the region on which the network is focusing within a given image, not the manner by which the network makes a decision [26].
Interpretable models, unlike conventional neural net-works, are designed to operate in a human-understandable manner [26]. Case-based models learn discriminative fea-tures of each class, which are referred to as prototypes, and classify an input image by comparing its features with the prototypes [3, 8, 16]. Such models provide two types of interpretation: global and local explanations. A global explanation is a class-representative feature that is shared by multiple data points belonging to the same class [14,25].
A local explanation, by contrast, shows how the prediction of a single input image is made. In other words, the global 15719
explanation ﬁnds the common characteristic by which the model deﬁnes each class, while the local explanation ﬁnds the reason that the model sorts a given input image into a certain class. The global explanation can be likened to the manner in which radiologists explain common signs of diseases in X-ray images, whereas the local explanation can be likened to the manner in which they diagnose individual cases by examining the part of a given X-ray image that pro-vides information about a certain disease. This suggests that case-based models are suitable for building an interpretable automated diagnosis system.
ProtoPNet [3], which motivates our work, deﬁnes a pro-totype as a feature within a patch of a predeﬁned size obtained from training images, and compares a local area in a given input image with the prototypes for classiﬁca-tion. Despite such constraint for interpretability, it achieves performance comparable to that of conventional uninter-pretable neural networks in ﬁne-grained classiﬁcation tasks.
However, with a patch of a predeﬁned size, it is difﬁcult to reﬂect features that appear in a dynamic area, such as a sign of disease in medical images. For example, to identify cardiomegaly (enlargement of the heart), it is necessary to look at the whole heart [24]; to identify nodule, it is nec-essary to ﬁnd an abnormal spot whose diameter is smaller than a threshold [7]. Depending on the ﬁxed size of the patch, the prototypes may not sufﬁciently present the class-representative feature or may even present a class-irrelevant feature, leading to diagnostic failure. To address this prob-lem, we introduce a method of training the prototypes to present class-representative features within a dynamic area (see the prototypes of each disease in Figure 1).
In this paper, we propose an interpretable automated diagnosis framework, XProtoNet, that predicts an occur-rence area where a sign of a given disease is likely to appear and learns the disease-representative features of the occurrence area as prototypes. The occurrence area is adap-tively predicted for each disease, enabling the prototypes to present discriminative features for diagnosis within the adaptive area for the disease. Given a chest X-ray image,
XProtoNet diagnoses disease by comparing the features of the image with the learned prototypes. As shown in Fig-ure 1, it can provide both global explanations—the discrim-inative features allowing the network to screen for a certain disease—and local ones—e.g., a rationale for classifying a single chest X-ray image. We evaluate our method on the public NIH chest X-ray dataset [32], which provides 14 chest-disease labels and a limited number of bounding box annotations. We also conduct further analysis of XProtoNet with a prior condition to have speciﬁc features as proto-types using the bounding box annotations. Despite strong constraints to make the network interpretable, XProtoNet achieves state-of-the-art diagnostic performance.
The main contributions of this paper can be summarized as follows:
• We present, to the best of our knowledge, the ﬁrst interpretable model for diagnosis in chest radiography that can provide both global and local explanations.
• We propose a novel method of learning disease-representative features within a dynamic area, improv-ing both interpretability and diagnostic performance.
• We demonstrate that our proposed framework outper-forms other state-of-the-art methods on the public NIH chest X-ray dataset. 2.