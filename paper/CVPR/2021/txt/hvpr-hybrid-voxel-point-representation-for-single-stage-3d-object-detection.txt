Abstract
We address the problem of 3D object detection, that is, estimating 3D object bounding boxes from point clouds. 3D object detection methods exploit either voxel-based or point-based features to represent 3D objects in a scene.
Voxel-based features are efﬁcient to extract, while they fail to preserve ﬁne-grained 3D structures of objects. Point-based features, on the other hand, represent the 3D struc-tures more accurately, but extracting these features is com-putationally expensive. We introduce in this paper a novel single-stage 3D detection method having the merit of both voxel-based and point-based features. To this end, we pro-pose a new convolutional neural network (CNN) architec-ture, dubbed HVPR, that integrates both features into a single 3D representation effectively and efﬁciently. Specif-ically, we augment the point-based features with a mem-ory module to reduce the computational cost. We then ag-gregate the features in the memory, semantically similar to each voxel-based one, to obtain a hybrid 3D representa-tion in a form of a pseudo image, allowing to localize 3D objects in a single stage efﬁciently. We also propose an At-tentive Multi-scale Feature Module (AMFM) that extracts scale-aware features considering the sparse and irregular patterns of point clouds. Experimental results on the KITTI dataset demonstrate the effectiveness and efﬁciency of our approach, achieving a better compromise in terms of speed and accuracy. 1.

Introduction 3D object detection is an essential technique for scene understanding, which aims at predicting 3D bounding boxes
It can also be exploited as a ba-of objects in a scene. including au-sic building block for many applications, tonomous driving and robotics. Recent approaches to 3D object detection focus on learning discriminative 3D repre-sentations using point clouds acquired from a LiDAR sen-sor. Point clouds provide accurate depth information of ob-jects, but they are sparse with the densities largely varying
*Corresponding author.
Figure 1: Runtime and accuracy comparison of detection results on the KITTI [10] test set. We compare our model with voxel-based methods on the car class for three difﬁculty levels. Voxel-based methods using pseudo image representations (Voxel-PI) are shown as circles, and 3D voxel-based methods (Voxel-3D) are plotted as triangles. Our method gives a better compromise in terms of accuracy and runtime for all cases. SE: SECOND [42];
PP: PointPillars [18]; TA: TANet [23]; AS: Associate-3D [7]; 3D: 3DIoULoss [49]; SA: SA-SSD [12]; HS: HotSpotNet [3]. Best viewed in color. w.r.t distances from the sensor.
There are many attempts to learn 3D feature representa-tions using point clouds with deep neural networks, which can be categorized into two groups: Voxel-based and point-based methods. The ﬁrst approaches [7, 12, 18, 35, 43, 46] transform raw point clouds to structured grid representa-tions, e.g., birds-eye-view (BEV) [35, 43] or voxels [12, 18, 33, 46, 50], to extract 3D representations using con-volutional neural networks (CNNs). The voxelization and downsampling operations in these methods allow to ex-tract compact 3D features efﬁciently, but they fail to pre-serve ﬁne-grained 3D structures of objects. The second ap-proaches [29, 34, 38, 44, 45], on the other hand, exploit raw point clouds directly to extract point-wise features us-ing e.g., PointNet++ [30, 31]. They provide more discrim-14605
inative 3D representations than the voxel-based one, and thus give better detection results. Processing a large-scale point cloud data, however, needs lots of computational cost.
We present in this paper a novel single-stage 3D object detection framework that integrates voxel-based and point-based features effectively and efﬁciently to obtain discrimi-native 3D representations. To this end, we introduce a new
CNN architecture, dubbed HVPR, that consists of a two-stream encoder for voxel-based and point-based features and a memory module. It augments the encoder for point-based features with the memory module to reduce the com-putational cost. Namely, we update and store point-based features from the encoder to memory items during training, and do not use the encoder at test time, avoiding heavy com-putation. Speciﬁcally, we aggregate the point-wise features in memory items, semantically similar to each voxel-based one, to obtain a pseudo image representation, enabling ex-ploiting hybrid 3D representations efﬁciently for localiz-ing objects in a single stage. This also encourages voxel-based features to incorporate ﬁne-grained representations of point-based features, which are particularly effective for localizing small or largely occluded objects acquired from sparse point clouds. We also introduce a detection network with an Attentive Multi-scale Feature Module (AMFM).
Given the hybrid pseudo image, it extracts multi-scale fea-tures, and AMFM reﬁnes them using 3D scale representa-tions to obtain scale-aware features, which are crucial espe-cially for 3D object detection, due to the sparse and irregu-lar patterns of point clouds. Extensive experimental results on standard benchmarks demonstrate the effectiveness and efﬁciency of our approach to exploiting hybrid feature rep-resentations. The main contributions of this paper can be summarized as follows:
• We introduce a novel single-stage framework for 3D object detection using hybrid 3D representations. We propose to use a memory module to augment point-based features, maintaining the efﬁciency of a single-stage method.
• We introduce AMFM that provides scale-aware features considering the sparse and irregular patterns of point clouds explicitly, allowing to consider complex scale vari-ations across objects for 3D object detection.
• We demonstrate that our approach gives a better compro-mise in terms of speed and accuracy, compared to the state of the art. Our model runs at 36.1fps, while achieving competitive performance on the KITTI dataset [10].
Our code and models are available online: https:// cvlab.yonsei.ac.kr/projects/HVPR. 2.