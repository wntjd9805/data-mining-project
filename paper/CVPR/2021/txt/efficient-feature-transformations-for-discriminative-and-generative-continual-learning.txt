Abstract
As neural networks are increasingly being applied to real-world applications, mechanisms to address dis-tributional shift and sequential task learning without forgetting are critical. Methods incorporating network expansion have shown promise by naturally adding model capacity for learning new tasks while simultaneously avoiding catastrophic forgetting. However, the growth in the number of additional parameters of many of these types of methods can be computationally expensive at larger scales, at times prohibitively so. Instead, we pro-pose a simple task-speciﬁc feature map transformation strategy for continual learning, which we call Eﬃcient
Feature Transformations (EFTs). These EFTs provide powerful ﬂexibility for learning new tasks, achieved with minimal parameters added to the base architecture. We further propose a feature distance maximization strat-egy, which signiﬁcantly improves task prediction in class incremental settings, without needing expensive genera-tive models. We demonstrate the eﬃcacy and eﬃciency of our method with an extensive set of experiments in discriminative (CIFAR-100 and ImageNet-1K) and generative (LSUN, CUB-200, Cats) sequences of tasks.
Even with low single-digit parameter growth rates, EFTs can outperform many other continual learning methods in a wide range of settings. 1.

Introduction
While deep learning has led to impressive advances in many ﬁelds, neural networks still tend to struggle in sequential learning settings, largely due to catastrophic forgetting [31, 39]: when the training distribution of a model shifts over time, neural networks overwrite previously learned knowledge if not repeatedly revisited during training. Pragmatically, this typically means that data collection must be completed before training a neural network, which can be problematic in settings like reinforcement learning [34] or the real world [44], which is constantly evolving. Otherwise, the model must constantly be re-trained as new data arrives. This limitation signiﬁcantly hampers building and deploying intelligent systems in changing environments. learning methods
A variety of continual
[37] have been proposed to address this shortcoming.
Regularization-based methods [18, 62, 36, 1, 41] pre-vent forgetting by constraining model parameters from drifting too far away from previous solutions, but they can also restrict the model’s ability to adapt to new tasks, often resulting in sub-optimal solutions. Addi-tionally, regularization methods commonly make the assumption that each weight’s importance for a task is independent, which may explain why they have dif-ﬁculty scaling to more complex networks and tasks.
Replay methods [27, 46, 36, 42] retain knowledge by rehearsing on data saved from previous tasks. While eﬀective at preventing forgetting, the performance of replay-based approaches is highly dependent on the size and contents of the memory buﬀer, and in certain strict settings, saving any data at all may not be an option.
The nature of this replay buﬀer also tends to highly bias the model toward recently learned tasks. As the number of tasks grows, performance degrades quickly, especially in large-scale settings [38, 40].
As an alternative to regularization or replay, ex-pansion methods [43, 59, 58, 32] combat forgetting by growing the model with each task. Expansion alleviates catastrophic forgetting by design, and additional nec-essary capacity can be easily added to accommodate new knowledge needed for new tasks. This ability to scale arbitrarily without needing to save any data gives expansion methods the best chance to succeed in large-scale settings. However, added capacity to model future tasks needs to be carefully balanced against the number of parameters added, especially since the number of tasks the model must learn is often unknown ahead of time. Too ineﬃcient of an approach can easily exceed computational resources even after a moderate number of tasks. Moreover, many previous expansion methods 113865
Figure 1. Left: EFT transforms global feature map F to a task-speciﬁc feature map H with parameters τt. Right: Parameter growth for the 10 task on the LeNet architecture, EFT shows signiﬁcantly lower growth than other expansion models. are either computationally ineﬃcient or inﬂate model size by a signiﬁcant amount.
To overcome these limitations, we propose a com-pact, task-speciﬁc feature map transformation for large-scale continual learning, which we call Eﬃcient Feature
Transformation (EFT). In particular, we partition the model into global parameters (θ) and task-speciﬁc local parameters (τt), with the pair (θ, τt) as the optimal parameters for a particular task t (see Figure 1). In constructing these local transforms, we leverage eﬃ-cient convolution operations [14, 49, 56], maintaining expressivity, while keeping model growth small. We also minimize the impact on the global base architecture, al-lowing us to use pre-existing architectures, which can be critical for achieving strong performance in large-scale settings. This compact nature of the added transforma-tions also makes EFTs faster to train than comparable methods because we have to update only task-speciﬁc parameters. Finally, we propose a strategy for max-imizing feature distance to improve task prediction, a critical component for continual learning methods operating in class incremental settings.
To show the eﬃcacy and eﬃciency of the proposed approach, we extensively evaluate our model on a vari-ety of datasets and architectures. In class incremental and task incremental sequential classiﬁcation settings,
EFTs achieve signiﬁcant performance gains on CIFAR-100 [19] and ImageNet [6] with only a minor growth in parameter count and computation. We also evaluate our approach for continual generative modeling, demon-strating a 22.7% relative improvement in FID [12] on the LSUN [60], CUB-200 [52], and ImageNet [6] cat datasets compared to recent state-of-the-art models. 2. Methods 2.1. Efﬁcient Feature Transforms
Commonly used modern vision and language archi-tectures can be quite large, sometimes having tens to hundreds of millions of parameters [11, 7]. This can make them incompatible with many previous contin-ual learning methods: if one must add entire parallel columns [43] or regularize the entirety of a model’s weights [18] per task t, it can very easily exceed system capacity. Additionally, many of large-scale networks have been carefully engineered through years of manual architecture search, resulting in structures with speciﬁc hyperparameter settings or inductive biases that make them especially eﬀective [3]. Modiﬁcations to these design may lead to unintentional degradation of the model’s overall eﬀectiveness. Many previous continual learning methods signiﬁcantly alter the base network in a way that may be detrimental to overall performance.
With these considerations in mind, we propose par-titioning the network into a global base network pa-rameterized by θ and task-speciﬁc transformations τt.
During a task t, only the task-speciﬁc parameters τt are trained; previous local parameters τ<t and global parameters θ remain unchanged. Under this set-up, the global network can be any architecture, preferably an eﬀective one for the problem at hand. Of particu-lar note, this means that pre-trained weights can also be employed, if available and appropriate. Because attempting to transform parameters θ in its entirety can be expensive, we instead propose task-speciﬁc lo-cal transformations of the features at various layers within the network. We aim to keep the task-speciﬁc transformations minimal. In particular, using eﬃcient operations, our approach can keep the number of param-eters in τt very small without degrading performance.
To ensure network compatibility, we also ensure that the dimensionality of each transformed feature tensor remains the same as the original, meaning that this operation can be inserted into any architecture without any changes. We outline here how this can be done for 2D convolutional and fully connected layers. We focus on these two operations as they comprise the backbone of many deep architectures, but these concepts can be generalized to other types of layers as well. 2.1.1 2D Convolutional Layer
Convolutional neural networks (CNNs) [22] play a major role in many modern computer vision algorithms, with 213866
the 2D convolutional layer being one of the primary building blocks. Let I be the input to a convolutional layer. In the typical formulation, each convolution layer is composed of K convolutional ﬁlters W; each ﬁlter
Wk in W has size (m × n × c), with m and n being the
ﬁlter’s spatial dimensions and c the number of channels in I. Each ﬁlter Wk convolved with I produces an output feature map Fk ∈ RM ×N . The whole operation can be summarized as
F = W ∗ I (1) with ∗ being the 2D convolution operator and F ∈
RM ×N ×K being all feature maps Fk stacked into a tensor. Generally, this operation includes an additive bias, which we omit here for notational convenience.
Rather than adjusting W directly, we instead pro-pose appending a small convolutional transformation to change the features F . As this operation needs to be done for each task t, we seek to do so eﬃciently, more so than existing expansion-based methods. We do this by leveraging groupwise [20, 56] and depthwise [14] convolutions, producing new features for a speciﬁc task.
We use two types of convolutional kernels: ωs ∈ R3×3×a for capturing spatial features within groups of channels and ωd ∈ R1×1×b for capturing features across channels at every pixel in F , where a and b are hyperparameters deﬁning the group size. For the former case, we per-form a groupwise convolution with cardinality a using depth-a convolutions. In other words, we split convo-lutional feature maps F into K/a groups, and for each spatially convolve a unique set of a ﬁlters ωs i over the group of feature maps. The resulting K/a groups of a feature maps H s i , are all concatenated into a single tensor H s ∈ RM ×N ×K. Importantly, this is the same dimensions as the original feature map F , keeping with our goal of keeping the architecture unchanged after inserting this transformation. A similar operation is done for ﬁlters ωd, but with cardinality b and depth b, to constitute feature maps H d ∈ RM ×N ×K. The construction of both feature maps can be expressed as:
H s ai:(ai+a−1) = [ωs i,1 ∗ Fai:(ai+a−1) | . . . |
ωs i,a ∗ Fai:(ai+a−1)], i ∈ {0, . . . ,
K a
− 1} (2)
H d bi:(bi+b−1) = [ωd i,1 ∗ Fbi:(bi+b−1) | . . . |
ωd i,b ∗ Fbi:(bi+b−1)], i ∈ {0, . . . ,
K b
− 1} (3) where | is the concatenation operation, H s
RM ×N ×a, and H d ai:(ai+a−1) ∈ bi:(bi+b−1) ∈ RM ×N ×b. Generally, a ≪
Figure 2. Illustration of the proposed parameter-eﬃcient transformation of convolutional features.
K and b ≪ K, so this operation is far more parameter eﬃcient than simply using another convolutional layer with K ﬁlters. The latter requires at least as many parameters as the original network itself for each task, the same as learning separate models per task.
We combine the features from the spatial and depth convolutions H s and H d additively:
H = H s + γH d (4) where γ ∈ {0, 1} is an indicator indicating if the point-wise convolutions ωd are employed. Note, if γ = 0, we do not perform convolutions with ωd; this sacriﬁces some expressivity, but further reduces the number of added parameters per task. For example, the extreme case of a = 1 and γ = 0 results in a small 0.17% increase in parameters per task in ResNet-18 while still achieving state-of-the-art performance on the ImageNet-1K/10 continual learning task. 2.1.2 Fully Connected Layer
Fully connected layers are common in many architec-tures, typically to project from one dimensionality to another. In convolutional neural networks, they are fre-quently used to project to the number of output classes to produce the ﬁnal prediction. A fully connected layer is implemented as a matrix multiply between a weight matrix W and an input vector v, producing an output feature vector f = W v; the bias vector is again omitted for convenience.
As with the convolutional layer, where we transform the convolutional features F to a task-speciﬁc feature
H with additional convolutional operations, we also transform output vector f with another fully connected layer (parameterized by E) to a task-speciﬁc feature vector h. Like in the convolutional case, we must put restrictions on the form of E to prevent this operation from being overly costly. In particular, we constrain E to be diagonal, which signiﬁcantly reduces the number of parameters. This operation can be expressed as h = Ef . In practice, since E is diagonal, this operation can be implemented as a Hadamard product. 313867
2.1.3 Parallel Adaption 2.3. Summary
We have thus far introduced our approach as an eﬃcient, sequential feature transformation, adapting the features to each task after its computation in the base network.
An alternative parameterization is to compute these fea-ture calibrations in parallel (Figure 1, center), making the transformation additive rather than compositional:
H ′ = (W ∗ I) ⊕ (τ l t ∗ I)
= F ⊕ H (5) (6) where ⊕ is element-wise addition and τ l t is the task speciﬁc parameter at layer l. Empirically we ﬁnd that both sequential and parallel models achieve similar per-formance. Unless otherwise mentioned, we report our results with sequential transformations. A comparison of the two types is shown in the supplementary material. 2.2. Task Prediction
Knowing which task t an inference-time input corre-sponds to is a common implicit assumption in continual learning, but in some settings this information is un-available, necessitating predicting the task t alongside the class; such a setting has been referred to as class incremental learning (CIL) [15]. Predicting the task ID can be challenging, especially when data from previous tasks is not saved. We choose to predict t by selecting the task-speciﬁc set of parameters τt that produces the maximum conﬁdence prediction for the input. However, a naive, direct measurement of the post-softmax pre-diction entropy often performs poorly, as the typical cross-entropy training objective tends to produce high conﬁdence in deterministic neural networks, even for out-of-distribution (OoD) samples [10]. To remedy this, we propose feature distance maximization, a simple regularization to increase task prediction ability.
Without intervention, the pre-output layer feature distributions learned by the parameters τt for each task t may show overlap, which may hinder attempts to discriminate task features. To mitigate this, we add a regularizer to create a margin between the features produced by each task’s τt for a given task’s data:
LM = t−1
X i=1 max(∆ − KL(Pt||Qi), 0) (7) where Pt = N (µt, Σt) and Qi = N (µi, Σi) are the distributions of the current task t and earlier task i < t.
This regularizer helps the model learn representations for τt such that the current task data (Dt) has at least
∆ separability in the feature space from the features encoded by previous tasks’ parameters τ<t.
The joint loss for Eﬃcient Feature Transforms (EFTs) is deﬁned as:
LEFT = L(ˆy, y) + λLM (8) where L(ˆy, y) is the standard cross-entropy loss between the predicted class ˆy and ground truth y, and λ is a weighting hyperparameter. At test time, for a given input x, we measure the output entropy with each τt and predict the task ID as arg maxt(x|θ, τt). Once we have the task ID, we can choose the task-speciﬁc parameter
τt to predict a class.
In summary, we recognize that if one adjusts the original model parameters in a given layer (what we call “global” model parameters), this leads to an adjust-ment in the features output from the layer. However, the number of layer-dependent global parameters may be massive, and adjustment of them may cause loss of information accrued from prior datasets and tasks.
Since layer-wise adjustment of global parameters sim-ply adjusts the output feature map, we leave the global layer-dependent features unchanged, and introduce a new set of lightweight task-speciﬁc parameters, that di-rectly reﬁne the feature map itself. These task-speciﬁc parameters may adjust to new tasks, while maintain-ing the knowledge represented in well-training global model parameters. We have developed methods to ap-ply this concept to convolutional and fully-connected layers. Additionally, we make the model more amenable to task ID prediction by maximizing a margin between task-speciﬁc feature distributions. 3.