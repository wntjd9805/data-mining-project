Abstract
Adverse weather conditions, including snow, rain, and fog, pose a major challenge for both human and computer vision. Handling these environmental conditions is essen-tial for safe decision making, especially in autonomous ve-hicles, robotics, and drones. Most of today’s supervised imaging and vision approaches, however, rely on train-ing data collected in the real world that is biased towards good weather conditions, with dense fog, snow, and heavy rain as outliers in these datasets. Without training data, let alone paired data, existing autonomous vehicles often limit themselves to good conditions and stop when dense fog or snow is detected. In this work, we tackle the lack of supervised training data by combining synthetic and indi-rect supervision. We present ZeroScatter, a domain trans-fer method for converting RGB-only captures taken in ad-verse weather into clear daytime scenes. ZeroScatter ex-ploits model-based, temporal, multi-view, multi-modal, and adversarial cues in a joint fashion, allowing us to train on unpaired, biased data. We assess the proposed method on in-the-wild captures, and the proposed method outper-forms existing monocular descattering approaches by 2.8 dB PSNR on controlled fog chamber measurements. 1.

Introduction
In the presence of a scattering medium, such as fog or snow, photons no longer propagate along a straight path but instead are redirected by particles, potentially many times, until arriving at the camera. This includes forward scattered light emitted from sources in the scene, e.g., an oncoming vehicle headlight, captured as a passive component by an
RGB camera or human eye, and backward scattering ob-served when actively illuminating the scene, e.g., in auto-motive lidar or with the ego-vehicle headlights. While ad-verse weather conditions that include severe scattering are heavily underrepresented in existing training and evaluation datasets [45, 13, 9], these rare scenarios are a signiﬁcant contributing factor for fatal automotive accidents [4], as a direct result of vision impairment for human drivers.
Supervised imaging and vision approaches are also fun-damentally limited in adverse weather conditions. Adverse
*indicates equal contribution. weather conditions follow a long-tail distribution where such environments are rarely encountered during day-to-day driving, making data collection, training, and evalua-tion challenging [37]. As a result, critical computer vision tasks such as object detection and tracking are often trained on clear day inputs and fail to generalize when the input scene is perturbed by adverse effects from scattering me-dia. Even if adverse weather data is available, the scatter-ing media would still affect the quality of human annota-tions used for supervision. Furthermore, supervised dehaz-ing and defogging methods are restricted by the difﬁculty of acquiring paired perturbed and clear data, which is in-feasible due to the dynamic nature of real-world automo-tive scenes. As such, supervised training on real-world data has been a fundamental challenge for imaging and vision in harsh weather conditions. To tackle this problem, existing approaches attempt to solve a domain transfer problem us-ing simulated scattering media [35, 36, 42, 18]. However, these simulation models do not adequately simulate the ef-fects that are observed in the wild. Unsupervised learning approaches have demonstrated impressive ability for image domain transfer but remain restricted to a single domain, e.g. faces, and small image resolutions [57, 24].
Researchers have also adopted alternative sensing modalities beyond conventional intensity imaging, e.g. lidar and radar, in robotic and automotive applications. However, they do not offer a solution in backscatter-limited weather scenarios. Speciﬁcally, pulsed lidar sensors that record the round-trip time of the ﬁrst response fail to extract meaning-ful scene surfaces in severe snow and fog, fundamentally limited by backscatter [5], and indeed trail the performance of RGB stereo depth methods [16] in dense fog. While the mm-wavelengths of radar systems penetrate dense fog, ex-isting radar systems are limited to low angular resolution, and hence do not allow for scene understanding tasks be-yond the detection and tracking of objects with a large radar cross-section [27]. At the same time, RGB intensity cam-eras have become a ubiquitous sensor technology because of their low-cost and high spatial resolutions up to 250 MPix in modern commodity sensors [38], deployed across appli-cation domains from miniature smartphone cameras to au-tomotive imaging systems. As such, in this work, we ad-dress the task of imaging through scattering media using conventional RGB cameras. 3476
Automotive RGB Camera System
Input RGB Capture
PFF-Net [2018]
EPDN [2019]
ZeroScatter
Figure 1: Scattering stemming from snow, rain, or fog signiﬁcantly reduces the perceptible quality of RGB captures and impact downstream computer vision tasks such as object detection. The proposed method, which we dub “ZeroScatter”, reliably removes these scattering effects for unseen automotive scenes.
We tackle this challenge by proposing ZeroScatter, a novel domain transfer method that converts RGB images corrupted by adverse weather effects into clear day scenes.
To do this, we exploit a variety of training signals in order to achieve robust descattering performance on real-world ex-amples. First, we employ a synthetic weather model us-ing cycle consistency training. Second, we employ tem-poral and multi-view consistency to ensure stable model performance and to eliminate spurious adverse weather ef-fects such as snowﬂakes, leveraging an adverse weather dataset [5]. Third, we employ multi-modal supervision us-ing auxiliary data acquired by gated imagers [17]. Gated imaging is an emerging time-of-ﬂight imaging technology that records photons with speciﬁc return times which allows it to image objects at select distances. This imaging modal-ity is less susceptible to path lengths and provides higher contrast training signal for ZeroScatter. All of these train-ing cues enable ZeroScatter to reliably reconstruct RGB captures that have been corrupted by adverse weather. For quantitative evaluation, we evaluated ZeroScatter on scenes with synthetically generated and laboratory generated ad-verse weather where we demonstrate 2.8 dB PSNR im-provement over state-of-the-art methods.
Speciﬁcally, we make the following contributions:
• We propose a novel domain adaptation method which we call ZeroScatter for eliminating scattering media from conventional RGB captures, operating at real-time frame rates of 20 FPS.
• We employ a novel combination of synthetic and real-world data to train ZeroScatter with unpaired, biased datasets. To this end, we incorporate model-based cues jointly with multi-modal, multi-view, temporal and ad-versarial cues.
• In addition to qualitative improvements on real-world captures, we outperform state-of-the-art methods in controlled fog-chamber evaluation. Our method also outperforms state-of-the-art object detection in harsh weather at long distances. 2.