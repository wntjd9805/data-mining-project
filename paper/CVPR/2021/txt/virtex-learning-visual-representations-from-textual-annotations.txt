Abstract
The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quan-tities of unlabeled images.
In contrast, we aim to learn high-quality visual representations from fewer images. To this end we revisit supervised pretraining, and seek data-efﬁcient alternatives to classiﬁcation-based pretraining. We propose VirTex – a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks includ-ing image classiﬁcation, object detection, and instance seg-mentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet – supervised or unsu-pervised – despite using up to ten times fewer images. 1.

Introduction
The prevailing paradigm for learning visual representa-tions is ﬁrst to pretrain a convolutional network [1, 2] to perform image classiﬁcation on ImageNet [3, 4], then trans-fer the learned features to downstream tasks [5, 6]. This ap-proach has been wildly successful, and has led to signiﬁcant advances on a wide variety of computer vision problems such as object detection [7], semantic [8] and instance [9] segmentation, image captioning [10–12], and visual ques-tion answering [13, 14]. Despite its practical success, this approach is expensive to scale since the pretraining step re-lies on images annotated by human workers.
For this reason, there has been increasing interest in un-supervised pretraining methods that use unlabeled images to learn visual representations which are then transferred to downstream tasks [15–21]. Some recent approaches have begun to match or exceed supervised pretraining on Ima-geNet [22–26], and have been scaled to hundreds of mil-lions [22, 25, 27, 28] or billions [24] of images.
Continuing to scale unsupervised pretraining to ever-larger sets of unlabeled images is an important scientiﬁc
Figure 1: Learning visual features from language: First, we jointly train a ConvNet and Transformers using image-caption pairs, for the task of image captioning (top). Then, we transfer the learned ConvNet to several downstream vi-sion tasks, for example object detection (bottom). goal. But we may also ask whether there are alternate ways of pretraining that learn high-quality visual representations with fewer images. To do so, we revisit supervised pre-training and seek an alternative to traditional classiﬁcation pretraining that uses each image more efﬁciently.
In this paper we present an approach for learning Visual representations from Textual annotations (VirTex). Our ap-proach is straightforward: ﬁrst, we jointly train a ConvNet and Transformer [29] from scratch to generate natural lan-guage captions for images. Then, we transfer the learned features to downstream visual recognition tasks (Figure 1).
We believe that using language supervision is appealing due to its semantic density. Figure 2 compares different pre-training tasks for learning visual representations. Captions provide a semantically denser learning signal than unsu-pervised contrastive methods and supervised classiﬁcation.
Hence, we expect that using textual features to learn visual features may require fewer images than other approaches.
Another beneﬁt of textual annotations is simpliﬁed data collection. To collect classiﬁcation labels, typically human experts ﬁrst build an ontology of categories [3, 4, 30, 31], then complex crowdsourcing pipelines are used to elicit la-bels from non-expert users [32, 33]. In contrast, natural lan-guage descriptions do not require an explicit ontology and can easily be written by non-expert workers, leading to a 11162
Figure 2: Comparison of pretraining tasks for learning visual representations: Contrastive self-supervised learning methods provide a semantically sparse learning signal, encouraging different transforms of an image to have similar features.
Image classiﬁcation pairs an image with a single semantic concept, providing moderate semantic density. Multi-label clas-siﬁcation, object detection, and instance segmentation increase semantic density by labeling and localizing multiple objects.
Captions describe multiple objects, their attributes, relationships, and actions, giving a semantically dense learning signal. In this work, we aim to leverage this semantic density of captions to learn visual representations in a data-efﬁcient manner. simpliﬁed data collection pipeline [34–36]. Large quanti-ties of weakly aligned images and text can also be obtained from internet images [37–39].
Our main contribution is to show that natural language can provide supervision for learning transferable visual representations with better data-efﬁciency than other ap-proaches. We train models from scratch on the COCO
Captions dataset [36], and evaluate the learned features on downstream tasks including image classiﬁcation, object de-tection, instance segmentation, and low-shot recognition.
On all tasks, VirTex matches or exceeds the performance of existing methods for supervised or unsupervised pre-training on ImageNet, despite using up to 10× fewer im-ages. Our code and pretrained models are available at https://github.com/kdexd/virtex 2.