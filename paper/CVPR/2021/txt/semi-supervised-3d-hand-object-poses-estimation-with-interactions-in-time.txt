Abstract
Estimating 3D hand and object pose from a single im-age is an extremely challenging problem: hands and objects are often self-occluded during interactions, and the 3D an-notations are scarce as even humans cannot directly label the ground-truths from a single image perfectly. To tackle these challenges, we propose a uniﬁed framework for esti-mating the 3D hand and object poses with semi-supervised learning. We build a joint learning framework where we per-form explicit contextual reasoning between hand and object representations. Going beyond limited 3D annotations in a single image, we leverage the spatial-temporal consistency in large-scale hand-object videos as a constraint for generat-ing pseudo labels in semi-supervised learning. Our method not only improves hand pose estimation in challenging real-world dataset, but also substantially improve the object pose which has fewer ground-truths per instance. By training with large-scale diverse videos, our model also generalizes better across multiple out-of-domain datasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object 1.

Introduction
Hands are humans’ primary means of interacting with the physical world. Capturing the 3D pose of the hands and the objects interacted by hands is a crucial step in un-*Equal contribution. derstanding human actions. It is also the central part for a variety of applications including augmented reality [44, 25], third-person imitation learning [17, 10], and human-machine interaction [58]. While 3D pose estimation on hands and objects have been studied for a long time in computer vision captured with depth cameras [72, 36, 34, 69, 32] or RGB-D sensors [70, 49, 38, 54, 11] in controlled environments, re-cent research has also achieved encouraging results on pose estimation from a single monocular RGB image [75, 37, 66].
Despite the efforts, current approaches still highly rely on human annotations for 3D poses, which are extremely difﬁ-cult to obtain: Researchers have been collecting data with motion capture [50, 13, 74], or aligning mesh models to the real images [19, 30, 16, 4]. Given insufﬁcient annotations for supervised learning, it limits the trained model from gen-eralizing to novel scenes and out-of-domain environments.
To enable better estimation performance and generalization ability, we look into video data of hands and objects in the wild, without using the 3D annotations.
Speciﬁcally, we propose to exploit hand-object interac-tions over time. The poses of the hands and objects are usually highly correlated: The 3D pose of the hand when it is grasping the object often indicates the orientation of the object; the object pose also provides constraints on how the hand can approach and interact with the object. When observing from the videos, the 3D poses for both hands and objects should change smoothly and continuously. This con-tinuity provides a cue for selecting coherent and accurate 3D hand and object pose estimation results when human 14687
annotations are not available.
In this paper, we introduce a semi-supervised learning approach for 3D hand and object pose estimation with videos.
We ﬁrst train a joint model for both 3D hand pose and 6-Dof object pose estimation with supervised learning using fully annotated data. Then we deploy the model for hand pose estimation in large-scale videos without 3D annotations. We collect the estimation results as novel pseudo-labels for self-training. Speciﬁcally, to utilize the interaction information between hand and object, we design a uniﬁed framework that extracts the representation from the whole input image, and uses RoIAlign [20] to further obtain the object and hand region representations. Building on these representations, we apply two different branches of sub-networks to estimate the 3D poses for hand and object, respectively. We use a relational module [61] which bridges the two branches for encoding the mutual context between hand and object.
To perform semi-supervised learning with hand-object videos, we deploy our uniﬁed model on each frame for pseudo-label generation, as illustrated in Figure 1. Given the 3D hand pose results from our model, we design spatial-temporal consistency constraints to ﬁlter unstable and inac-curate estimations. Intuitively, we only keep the results as pseudo-labels if they change continuously over time, which indicates the robustness of the estimation. We then perform self-training with the newly collected data and labels.
We experiment by training the initial model on the HO-3D dataset [16], and perform semi-supervised learning with the Something-Something video dataset [14]. By learning from the pseudo-labels from large-scale videos using our approach, we achieve a large gain over state-of-the-art ap-proaches in the HO-3D benchmark. We also show signiﬁcant improvements in 3D hand pose estimation which generalizes to the out-of-domain datasets including FPHA [11] and Frei-Hand [76] datasets. More surprisingly, even though we only use pseudo-labels for hands, our joint self-training improves the object pose estimation by a large margin (more than 10% in some objects).
Our contributions include: (i) An uniﬁed framework for joint 3D hand and object pose estimation; (ii) A semi-supervised learning pipeline which exploits large-scale un-labeled hand-object interaction videos; (iii) Substantial per-formance improvement on hand and object pose estimation, and generalization to out-of-domain data. 2.