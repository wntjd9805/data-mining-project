Abstract
Dynamic Time Warping (DTW) is widely used for tem-poral data processing. However, existing methods can nei-ther learn the discriminative prototypes of different classes nor exploit such prototypes for further analysis. We propose
Discriminative Prototype DTW (DP-DTW), a novel method to learn class-speciﬁc discriminative prototypes for tem-poral recognition tasks. DP-DTW shows superior perfor-mance compared to conventional DTWs on time series clas-siﬁcation benchmarks1. Combined with end-to-end deep learning, DP-DTW can handle challenging weakly super-vised action segmentation problems and achieves state of the art results on standard benchmarks. Moreover, detailed reasoning on the input video is enabled by the learned ac-tion prototypes. Speciﬁcally, an action-based video sum-marization can be obtained by aligning the input sequence with action prototypes. 1.

Introduction
Temporal data is a common data form and widely exists in different domains [15], e.g., ﬁnance, industrial processes and video sequences. Analyzing temporal sequences is thus an important task. However, a signiﬁcant challenge arises when comparing two sequences as they are not guaranteed to be aligned. They can be varied in temporal length and/or observation speed. Therefore, alignment is essential be-fore comparisons, e.g., computing their discrepancy value.
Naive pre-processing such as interpolation, cyclic repeat ex-tension, place-holder insertion and down-sampling are used to align sequences with different lengths. These methods either modify the original data distribution or suffer from data loss and fail to handle the issue of varied speeds.
Dynamic Time Warping (DTW) [30, 2] was proposed to handle misalignment issues in temporal data. The opti-mal monotonic alignment between two input sequences is provided by a dynamic programming procedure. DTW is 1Code available at https://github.com/BorealisAI/TSC-Disc-Proto thus robust to inputs with varied temporal lengths and ob-servation speeds. The discrepancy value between the two sequences can then be computed based on the alignment.
With DTW and its discrepancy, a prototype over a set of sequences can be obtained by averaging. This technique is known as DTW barycenter averaging (DBA) [27] and en-ables several tasks, e.g., clustering and classiﬁcation. How-ever, DBA considers the intra-class samples only and ne-glects the inter-class ones in learning the class-speciﬁc pro-totypes for time series classiﬁcation (TSC). Discriminative prototypes thus fail to be obtained and classiﬁcation perfor-mance is negatively affected.
Besides the conventional multi-class single label TSC setting [9], we focus on solving the weakly supervised ac-tion segmentation problem in video data [21, 3]. Three ma-jor challenges are highlighted. First, the video data is cap-tured from realistic scenarios, such as daily activities and movies. It thus contains sophisticated spatial-temporal dy-namics. Secondly, multiple actions are performed sequen-tially in each video. Last but not least, instead of labelling the action at each frame, only the action order is provided as weak supervision. To better handle the complex temporal structure of video inputs, deep models are widely adopted to extract frame-wise deep feature representations. How-ever, training deep models with such weak supervision is not straightforward. Existing methods [29, 12, 6, 22] fol-low very similar paradigms. Speciﬁcally, deep models ﬁrst provide the action predictions of each frame. Different al-gorithms are then proposed to encode the frame-wise pre-dictions with the given action transcript and result in differ-ent learning objectives. For example, the dynamic program-ming procedure of DTW is exploited by D3TW [6] as its en-coding algorithm. To obtain a differentiable DTW loss for deep learning, a relaxation technique, as in Soft-DTW [8] can be adopted. No existing work attempts to learn dis-criminative prototype sequences of different actions nor use them for action segmentation.
In this paper, we propose a novel DTW method, Discrim-inative Prototype DTW (DP-DTW), for temporal recogni-tion problems.
In the TSC setting, each sequence corre-sponds to a single class. Instead of averaging the sequences 8395
Figure 1. Bold lines represent sequences with varied temporal lengths. (a) DTW computes the discrepancy ˆd and alignment A between a pair of sequences. (b) DBA computes a prototype by averaging (denoted as ¯Σ) the samples within a class. Different classes are indicated by colors. (c) DP-DTW focuses on the inter-class variance. Each input should be closest (shown as bold dashed line) to the prototype sequence of the same class (color). within a class as in DBA [27], DP-DTW computes the dis-crepancies between an input and the prototypes of different classes and then is supervised by discriminative loss. Class-speciﬁc distinctive temporal dynamics are thus represented by such learned prototypes. Illustrations of DTW, DBA and the proposed DP-DTW are shown in Figure 1.
More importantly, DP-DTW can handle temporal recog-nition of a sequence of multiple classes, as in the weakly supervised action segmentation problem. Speciﬁcally, each action is a class and has its prototype sequence in DP-DTW.
An input video can contain multiple actions performed one after another. Only the action ordering is recorded in the transcript as weak supervision. By concatenating the ac-tion prototypes in order, each transcript has its ordering sequence representation in DP-DTW. During training, dis-criminative losses are applied on the DTW discrepancies between the deep representation of the input video and the ordering sequences. As a result, the discriminative action prototypes are learned. With the retrieved or given tran-script of a testing video, the action segmentation is obtained based on the DTW alignment between the input and order-ing sequences. Each frame is assigned to the action (pro-totype) it aligns with. As a by-product, action-based key frames can be selected by the learned prototypes and used as a summarization of the input video. The process of DP-DTW mentioned above is illustrated in Figure 2.
The contributions of the proposed method are three-fold. (1) DP-DTW learns discriminative class-speciﬁc prototypes for TSC. (2) By modeling each action with a temporal se-quence as a prototype, the training and inference of DP-Figure 2. DP-DTW for weakly supervised action segmentation.
Each action, indicated by a color, is represented by a prototype sequence with temporal length 4. The frame-wise deep represen-tation s of input video X is extracted by Φ, e.g., GRU. An action transcript O has its ordering sequence P. Evaluated by the DTW discrepancies, a training sample should be closer to its ground truth O+ than a negative transcript O−. Hinge loss is used as the discriminative objective. The testing transcript O∗ is retrieved or given with its sequence P ∗. Based on the DTW alignment A∗ between s and P ∗, an action segmentation, indicated by the col-ored box on the frame, is obtained. Moreover, the action-based key frames are selected as a video summary.
DTW for weakly supervised action segmentation are uniﬁed under DTW. With the distinctive action dynamics captured by the learned prototypes, action segmentation can then beneﬁt from an optimal temporal alignment. (3) Action-based video summarization is obtained as a detailed analy-sis and by-product of the discriminative prototypes learned by DP-DTW. DP-DTW is evaluated on different temporal recognition tasks. On the TSC benchmarks [9], DP-DTW outperforms the competitive DTW baselines. The effective-ness of DP-DTW on weakly supervised action segmentation is demonstrated by state of the art results on two challeng-ing datasets [21, 3]. Detailed analysis, i.e., action-based summarization, on such videos is enabled by DP-DTW. 2.