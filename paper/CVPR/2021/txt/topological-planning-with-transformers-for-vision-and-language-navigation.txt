Abstract
Conventional approaches to vision-and-language navi-gation (VLN) are trained end-to-end but struggle to per-form well in freely traversable environments. Inspired by the robotics community, we propose a modular approach to VLN using topological maps. Given a natural language instruction and topological map, our approach leverages attention mechanisms to predict a navigation plan in the map. The plan is then executed with low-level actions (e.g.
FORWARD, ROTATE) using a robust controller. Experiments show that our method outperforms previous end-to-end ap-proaches, generates interpretable navigation plans, and ex-hibits intelligent behaviors such as backtracking. 1.

Introduction
Enabling robots to understand natural language and carry out communicated tasks has long been desired. A critical step towards this goal in the context of mobile robotics is vision-and-language navigation (VLN) [4].
In VLN, the agent is provided with a navigation instruction such as:
“Exit the bedroom, walk to the end of the hall, and enter the kitchen on your right.” The agent is then expected to follow the instruction and move to the speciﬁed destination.
The majority of VLN systems [16, 20, 24, 29, 30, 43, 48, 55] are end-to-end deep learning models and utilize unstruc-tured memory such as LSTM [22]. These methods work well when the movement is constrained to pre-deﬁned lo-cations, but performance drops signiﬁcantly when the agent is allowed to move freely [26]. Moreover, learning to per-form navigation, including mapping, planning, and control, in a fully end-to-end manner can be difﬁcult and expensive.
Such approaches often require millions of frames of experi-ence [9, 47, 50], and yet performance substantially degrades without ground truth odometry [9, 50].
To address the aforementioned issues, recent visual robot navigation literature has explored using structured memory (e.g. metric maps, topological memory) and using a modu-lar approach, where the algorithm is explicitly divided into relevant subcomponents such as mapping, planning, and control [6, 7, 8, 15, 17, 23, 33, 39]. These approaches have been demonstrated to work well for tasks such as target im-age navigation and environment exploration. However, they have not been well studied in the context of VLN.
In this work, we employ a modular approach and lever-age topological maps for VLN. Topological maps, in-spired in part by cognitive science, typically represent en-vironments as graphs where nodes correspond to places and edges denote environment connectivity or reachability.
Compared with metric maps, topological maps eliminate the need for meticulous map construction. They promote efﬁcient planning, interpretable navigation plans, and nav-igation robustness using cheaper sensors [33, 44]. In par-ticular, the symbolic nature of topological maps lends them suitable for navigation with language [32, 44], as the space discretization provided by the maps can facilitate learning a relationship between instructions and spatial locations.
Importantly, using topological maps synergizes well with sequence prediction models. Predicting a naviga-tion plan in a topological map bears many similarities with predicting sequences for language tasks such as lan-guage modeling and neural machine translation (NMT). By drawing the parallel between navigation planning and lan-guage sequence prediction, we can leverage powerful atten-tion mechanisms [46] that have enabled signiﬁcant break-throughs in language tasks for the navigation problem. Re-cently, these attention-based models have even been demon-strated to achieve comparable performance to convolutional neural networks on image recognition tasks [14].
We propose using a cross-modal attention-based trans-former to compute navigation plans in topological maps based on language instructions. Whereas a language model predicts a word at a time, our transformer predicts one topo-logical map node in the navigation plan at a time. Struc-turing our model in this manner allows the agent to attend to relevant portions of the navigation instruction and rele-vant spatial regions of the navigation environment during the navigation planning process. For example, this enables our model to relate the word “bedroom” in a navigation in-struction to the physical room in the environment. 11276
Topological Environment Map
Navigation Plan 1. Planning 6 5 4 3 1 2
Cross-modal 
Transformer 
Planner
Natural Language Instruction
“Exit the bedroom and turn left. Go around the corner and down the  hallway. Make a right turn and stop in the doorway on the right.”
GT start / goal
Planned path 2. Control: Plan Execution exit the bedroom...
...turn left... 3 2 3 2 3 2 1 1 1
Traversal
Localization
Traversal
Figure 1: The agent uses the natural language instruction to generate a navigation plan in the topological map (left). A controller then executes the predicted plan by sequentially traversing to each subgoal node in the plan (right). As the agent approaches the subgoal node, it consumes that active node, and the subsequent node in the plan becomes the new active node.
Altogether, we propose a full navigation system for
VLN. Unlike much of the prior work in VLN, we use a more challenging setup, allowing the agent to freely tra-verse the environment using low-level discrete actions. To this end, we deﬁne a topological map representation that can be constructed by the agent after it has freely explored the environment. The maps are used as part of a modular navigation framework which decomposes the problem into planning and control (Fig. 1). For each navigation episode, our agent ﬁrst uses the cross-modal transformer to compute a global navigation plan from the navigation instruction and topological map. This navigation plan is executed by a ro-bust local controller that outputs low-level discrete actions.
We evaluate our approach using the VLN-CE dataset
[26]. Our experiments show that cross-modal attention-based planning is effective, and that our modular approach enables learning a robust controller capable of correcting for navigation mistakes like moving in the wrong direction. 2.