Abstract
We present a new generative autoencoder model with dual contradistinctive losses to improve generative autoencoder that performs simultaneous inference (reconstruction) and synthesis (sampling). Our model, named dual contradis-tinctive generative autoencoder (DC-VAE), integrates an instance-level discriminative loss (maintaining the instance-level ﬁdelity for the reconstruction/synthesis) with a set-level adversarial loss (encouraging the set-level ﬁdelity for the reconstruction/synthesis), both being contradistinctive. Ex-tensive experimental results by DC-VAE across different res-olutions including 32×32, 64×64, 128×128, and 512×512 are reported. The two contradistinctive losses in VAE work harmoniously in DC-VAE leading to a signiﬁcant qualitative and quantitative performance enhancement over the base-line VAEs without architectural changes. State-of-the-art or competitive results among generative autoencoders for image reconstruction, image synthesis, image interpolation, and representation learning are observed. DC-VAE is a general-purpose VAE model, applicable to a wide variety of downstream tasks in computer vision and machine learning. 1.

Introduction
Tremendous progress has been made in deep learning for the development of various learning frameworks [40, 24, 17, 64]. Autoencoder (AE) [44, 27] aims to compactly represent and faithfully reproduce the original input signal by concatenating an encoder and a decoder in an end-to-end learning framework. The goal of AE is to make the encoded representation semantically efﬁcient and sufﬁcient to reproduce the input signal by its decoder. Autoencoder’s generative companion, variational autoencoder (VAE) [38], additionally learns a variational model for the latent variables to capture the underlying sample distribution.
The key objective for a generative autoencoder is to main-tain two types of ﬁdelities: (1) an instance-level ﬁdelity to make the reconstruction/synthesis faithful to the individual input data sample, and (2) a set-level ﬁdelity to make the reconstruction/synthesis of the decoder faithful to the entire input data set. The VAE/GAN algorithm [42] combines a
* indicates equal contribution.
Code: https://github.com/mlpc-ucsd/DC-VAE. 823
reconstruction loss (for instance-level ﬁdelity) with an ad-versarial loss (for set-level ﬁdelity). However, the result of
VAE/GAN is sub-optimal, as shown in Table 1.
The pixel-wise reconstruction loss in the standard VAE
[38] typically results in blurry images with degenerated se-mantics. A possible solution to resolving the above conﬂict lies in two aspects: (1) turning the measure in the pixel space into induced feature space that is more semantically meaning-ful; (2) changing the L2 distance (per-pixel) into a learned instance-level distance function for the entire image (akin to generative adversarial networks which learn set-level dis-tance functions). Taking these two steps allows us to design an instance-level classiﬁcation loss that is aligned with the adversarial loss in the GAN model enforcing set-level ﬁdelity.
Motivated by the above observations, we develop a new gen-erative autoencoder model with dual contradistinctive losses by adopting a discriminative loss performing instance-level classiﬁcation (enforcing the instance-level ﬁdelity), which is rooted in metric learning [41] and contrastive learning
[21, 66, 63]. Combined with the adversarial losses for the set-level ﬁdelity, both terms are formulated in the induced feature space performing contradistinction: (1) the instance-level contrastive loss considers each input instance (image) itself as a class, and (2) the set-level adversarial loss treats the entire input set as a positive class. We name our method dual contradistinctive generative autoencoder (DC-VAE) and make the following contributions:
• We develop a new algorithm, dual contradistinctive gen-erative autoencoder (DC-VAE), by combining instance-level and set-level classiﬁcation losses in the VAE frame-work, and systematically show the signiﬁcance of these two loss terms in DC-VAE;
• The effectiveness of DC-VAE is illustrated in a number of tasks, including image reconstruction, image synthesis, image interpolation, and representation learning by recon-structing and sampling images across different resolutions including 32 × 32, 64 × 64, 128 × 128, and 512 × 512;
• Under the new loss term, DC-VAE attains a signiﬁ-cant performance boost over the competing methods without architectural change, thus potentially provid-ing a handy solution for many AE/VAE based model-ing/representation applications. DC-VAE helps greatly reducing the performance gap for image synthesis be-tween the baseline VAE to the competitive GAN models. 2.