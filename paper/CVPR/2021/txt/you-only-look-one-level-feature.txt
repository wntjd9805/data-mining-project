Abstract
This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimiza-tion problem in object detection rather than multi-scale fea-ture fusion. From the perspective of optimization, we in-troduce an alternative way to address the problem instead of adopting the complex feature pyramids - utilizing only one-level feature for detection. Based on the simple and ef-ﬁcient solution, we present You Only Look One-level Fea-ture (YOLOF). In our method, two key components, Di-lated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the pro-posed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while be-ing 2.5× faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7× less training epochs. Code is available at https://github.com/megvii-model/YOLOF. 1.

Introduction
In state-of-the-art two-stage detectors [19, 10, 1] and one-stage detectors [20, 35], feature pyramids become an essential component. The most popular way to build fea-ture pyramids is the feature pyramid networks (FPN) [19], which mainly brings two beneﬁts: (1) multi-scale feature fusion: fusing multiple low-resolution and high-resolution feature inputs to obtain better representations; (2) divide-and-conquer: detecting objects on different levels regarding objects’ scales. A common belief for FPN is that its success relies on the fusion of multiple level features, inducing a
∗This work is done during Qiang Chen’s internship at MEGVII Tech-nology.
†Corresponding author.
C5
C4
C3
C5
C4
C3
MiMo 35.9 mAP (a) Multiple-in-Multiple-out
MiSo 23.9 mAP
P7
P6
P5
P4
P3
P7
P6
P5
P4
P3
C5
C4
C3
C5
C4
C3
SiMo 35.0 mAP (b) Single-in-Multiple-out
SiSo 23.7 mAP
P7
P6
P5
P4
P3
P7
P6
P5
P4
P3 (c) Multiple-in-Single-out (d) Single-in-Single-out
Figure 1. Comparison of box AP among the Multiple-in-Multiple-out (MiMo), Single-in-Multiple-out (SiMo), Multiple-in-Single-out (MiSo), and Single-in-Single-out (SiSo) encoders on COCO validation set. Here, we adopt the original RetinaNet [20] as our baseline model, where C3, C4, and C5 denote output features of the backbone with a downsample rate of {8, 16, 32} and P3 to
P7 represent the feature levels used for ﬁnal detection. All results reported in the ﬁgure use the same backbone, ResNet-50 [11]. The structure of MiMo is same as the FPN in RetinaNet [20]. line of studies of designing complex fusion methods manu-ally [22, 14, 25], or via Neural Architecture Search (NAS) algorithms [7, 34]. However, the belief ignores the function of the divide-and-conquer in FPN. It leads to fewer studies on how these two beneﬁts contribute to FPN’s success and may hinder new advances.
This paper studies the inﬂuence of FPN’s two bene-ﬁts in one-stage detectors. We design experiments by de-coupling the multi-scale feature fusion and the divide-and-conquer functionalities with RetinaNet [20]. In detail, we consider FPN as a Multiple-in-Multiple-out (MiMo) en-coder, which encodes multi-scale features from the back-bone and provides feature representations for the decoder (the detection heads). We conduct controlled compar-isons among Multiple-in-Multiple-out (MiMo), Single-in-Multiple-out (SiMo), Multiple-in-Single-out (MiSo), and
Single-in-Single-out (SiSo) encoders in Figure 1. Surpris-ingly, the SiMo encoder, which only has one input feature
C5 and does not perform feature fusion, can achieve com-113039
parable performance with the MiMo encoder (i.e., FPN).
The performance gap is less than 1 mAP. In contrast, the performance drops dramatically (≥ 12 mAP) in MiSo and
SiSo encoders. These phenomenons suggest two facts: (1) the C5 feature carries sufﬁcient context for detecting ob-jects on various scales, which enables the SiMo encoder to achieve comparable results; (2) the multi-scale feature fusion beneﬁt is far away less critical than the divide-and-conquer beneﬁt, thus multi-scale feature fusion might not be the most signiﬁcant beneﬁt of FPN, which is also demon-strated by ExFuse [44] in semantic segmentation. Thinking one step deeper, divide-and-conquer is related to the opti-mization problem in object detection. It divides the com-plex detection problem into several sub-problems by object scales, facilitating the optimization process.
The above analysis suggests that the essential factor for the success of FPN is its solution to the optimization prob-lem in object detection. The divide-and-conquer solution is a good way. But it brings memory burdens, slows down the detectors, and make detectors’ structure complex in one-stage detectors like RetinaNet [20]. Given that the C5 fea-ture carries sufﬁcient context for detection, we show a sim-ple way to address the optimization problem.
We propose You Only Look One-level Feature (YOLOF), which only uses one single C5 feature (with a downsample rate of 32) for detection. To bridge the performance gap between the SiSo encoder and the MiMo encoder, we ﬁrst design the structure of the encoder properly to extract the multi-scale contexts for objects on various scales, compen-sating for the lack of multiple-level features; then, we ap-ply a uniform matching mechanism to solve the imbalance problem of positive anchors raised by the sparse anchors in the single feature.
Without bells and whistles, YOLOF achieves compa-rable results with its feature pyramids counterpart Reti-naNet [20] but 2.5× faster.
In a single feature manner,
YOLOF matches the performance of the recent proposed
DETR [2] while converging much faster (7×). In a nutshell, the contributions of this paper are:
• We show that the most signiﬁcant beneﬁts of FPN is its divide-and-conquer solution to the optimization prob-lem in dense object detection rather than the multi-scale feature fusion.
• We present YOLOF, which is a simple and efﬁcient baseline without using FPN. In YOLOF, we propose two key components, Dilated Encoder and Uniform
Matching, bridging the performance gap between the
SiSo encoder and the MiMo encoder.
• Extensive experiments on COCO benchmark indicates the importance of each component. Moreover, we con-duct comparisons with RetinaNet [20] and DETR [2].
We can achieve comparable results with a faster speed on GPUs. 2.