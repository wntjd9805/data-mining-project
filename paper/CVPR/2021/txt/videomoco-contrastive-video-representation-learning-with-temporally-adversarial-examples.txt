Abstract
MoCo [11] is effective for unsupervised image repre-sentation learning. In this paper, we propose VideoMoCo for unsupervised video representation learning. Given a video sequence as an input sample, we improve the tem-poral feature representations of MoCo from two perspec-tives. First, we introduce a generator to drop out several frames from this sample temporally. The discriminator is then learned to encode similar feature representations re-gardless of frame removals. By adaptively dropping out different frames during training iterations of adversarial learning, we augment this input sample to train a tempo-rally robust encoder. Second, we use temporal decay to model key attenuation in the memory queue when comput-ing the contrastive loss. As the momentum encoder updates after keys enqueue, the representation ability of these keys degrades when we use the current input sample for con-trastive learning. This degradation is reﬂected via temporal decay to attend the input sample to recent keys in the queue.
As a result, we adapt MoCo to learn video representations without empirically designing pretext tasks. By empower-ing the temporal robustness of the encoder and modeling the temporal decay of the keys, our VideoMoCo improves MoCo temporally based on contrastive learning. Experiments on benchmark datasets including UCF101 and HMDB51 show that VideoMoCo stands as a state-of-the-art video represen-tation learning method. 1.

Introduction
Unsupervised (i.e., self-supervised) feature representa-tion learning receives tremendous investigations along with the development of convolutional neural networks (CNNs).
This learning scheme does not require cumbersome man-∗T. Pan and Y. Song contribute equally. Y. Song is the corresponding author. The code is available at https://github.com/tinapan-pt/VideoMoCo.
Figure 1. VideoMoCo improves MoCo [11] temporally from two perspectives. First, by taking a video sequence as a training sam-ple, we introduce adversarial learning to augment this sample tem-porally. Second, we use a temporal decay (i.e., ti) to attenuate the contributions from older keys in the queue. To this end, the en-coder is learned via temporal augmentation within each sample and temporally contrastive learning across different samples. ual label collections and produces deep features represent-ing general visual contents. These features can be fur-ther adapted to suit downstream visual recognition scenar-ios including image classiﬁcation [21, 53], object detec-tion [48, 20], visual tracking [44, 45], and semantic seg-mentation [33]. Among various unsupervised represen-tation learning studies, contrastive learning [9] is devel-oped extensively. By treating one sample as positive and the remaining ones as negative (i.e., instance discrimina-tion), contrastive learning improves feature discrimination by considering the massive sample storage [48, 11, 40, 28] and data augmentation [2]. To effectively handle large-scale samples, MoCo [11] builds an on-the-ﬂy dictionary with a queue and a moving-averaged encoder. The learned fea-ture representations have signiﬁcantly improved a series of downstream recognition performances to approach those by using supervised feature representations.
The evolution of contrastive learning heavily focuses 11205
on feature representations from static images while leav-ing the temporal video representations less touched. One of the reasons is that large-scale video data is difﬁcult to store in memory. Using a limited number of video sam-ples leads to inadequate contrastive learning performance.
On the other hand, attempts on unsupervised video repre-sentation learning focus on proposing pretext tasks related to a sub-property of video content. Examples include se-quence sorting [23], optical ﬂow estimation [5], video play-back rate perception [52], pace prediction [43], and tem-poral transformation recognition [14]. Different from these pretext designs, we aim to learn a task-agnostic feature rep-resentation for videos. With effective data storage and CNN update [11] at hand, we rethink unsupervised video repre-sentation learning from the perspective of contrastive learn-ing, where the features are learned naturally to discriminate different video sequences without introducing empirically designed pretext tasks.
In this work, we propose VideoMoCo that improves
MoCo for unsupervised video representation learning.
VideoMoCo follows the usage of queue structure and a moving-averaged encoder of MoCo, which computes a con-trastive loss (i.e., InfoNCE [31]) efﬁciently among large-scale video samples. Given a training sample with ﬁxed-length video frames, we introduce adversarial learning to improve the temporal robustness of the encoder. As shown in Fig. 2, we use a generator (G) to adaptively drop out sev-eral frames. The sample with remaining frames, together with the original sample with full frames, are sent to the discriminator / encoder (D) for differentiating. Their dif-ference is then utilized reversely to train G. As a result, G removes temporally important frames based on the current state of D. And D is learned to produce similar feature rep-resentations regardless of frame removal. During different training iterations, the frames removed by G are different.
This sample is then augmented adversarially to train a tem-porally robust D. After adversarial learning only D is kept to extract temporally robust features.
The adversarial learning drops out several frames of an input video sample. We treat its remaining frames as a query sample and perform contrastive learning with keys in the memory queue. However, we notice that the mo-mentum encoder updates after keys enqueue. The feature representation of the keys is not up-to-date when we com-pute the contrastive loss. To mitigate this effect, we model the degradation of these keys by proposing a temporal de-cay. If a key stays longer in the queue, its contribution is less via this decay. To this end, we attend the query sam-ple to recent keys during the contrastive loss computation.
The encoder is thus learned more effectively without being heavily interfered by the ’ancient’ keys. The temporally ad-versarial learning and the temporal decay improve the tem-poral feature representation of MoCo. The experiments on several action recognition benchmarks verify that our pro-posed VideoMoCo performs favorably against state-of-the-art video representation approaches.
We summarize our main contributions as follows:
• We propose temporally adversarial learning to improve the feature representation of the encoder.
• We propose a temporal decay to reduce the effect from historical keys in the memory queues during con-trastive learning.
• Experiments on the standard benchmarks show that our VideoMoCo extends MoCo to a state-of-the-art video representation learning method. 2.