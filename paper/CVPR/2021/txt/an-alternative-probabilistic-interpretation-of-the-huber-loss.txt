Abstract
The Huber loss is a robust loss function used for a wide range of regression tasks. To utilize the Huber loss, a pa-rameter that controls the transitions from a quadratic func-tion to an absolute value function needs to be selected. We believe the standard probabilistic interpretation that relates the Huber loss to the Huber density fails to provide ade-quate intuition for identifying the transition point. As a re-sult, a hyper-parameter search is often necessary to deter-mine an appropriate value.
In this work, we propose an alternative probabilistic interpretation of the Huber loss, which relates minimizing the loss to minimizing an upper-bound on the Kullback-Leibler divergence between Laplace distributions, where one distribution represents the noise in the ground-truth and the other represents the noise in the prediction.
In addition, we show that the parameters of the Laplace distributions are directly related to the tran-sition point of the Huber loss. We demonstrate, through a toy problem, that the optimal transition point of the Huber loss is closely related to the distribution of the noise in the ground-truth data. As a result, our interpretation provides an intuitive way to identify well-suited hyper-parameters by approximating the amount of noise in the data, which we demonstrate through a case study and experimentation on the Faster R-CNN and RetinaNet object detectors. 1.

Introduction
A typical problem in machine learning is estimating a
R given a set
Rn to y function Fθ that maps from x
N of training examples i=0. The parameters of the function θ are often determined by minimizing a loss function
∈ xi, yi}
{
=
D
∈
,
L
ˆθ = arg min
θ (yi −
L
Fθ(xi)) (1)
N i=0
X and the choice of loss function can be crucial to the perfor-mance of the model. The Huber loss is a robust loss func-tion that behaves quadratically for small residuals and lin-early for large residuals [9]. The loss function was proposed over a half-century ago, and it is still widely used today for a variety of regression tasks, including 2D object detection
[4, 14, 16, 18], 3D object detection [2, 3, 10, 22], shape and pose estimation [6, 11, 20], and stereo estimation [1].
A challenge with utilizing the Huber loss in practice is selecting an appropriate value to transition from a quadratic error to a linear error. Under certain assumptions, mini-mizing a loss function can be interpreted as maximizing the likelihood of yi given xi,
N
ˆθ = arg max
θ p(yi| xi, θ) (2)
∝ exp [ xi, θ) i=0
Y when p(yi|
Fθ(xi))]. Therefore, the (yi −
−L estimate ˆθ that minimizes the Huber loss can be interpreted as the maximum likelihood estimate of θ when p(yi| xi, θ) is the Huber density [9]. The Huber density can be difﬁ-cult to interpret; as a result, hyper-parameter search is often employed to identify a satisfactory transition point for the
Huber loss.
In this work, we propose an alternative probabilistic in-terpretation of the Huber loss. Our interpretation assumes yi is a noisy estimate of the true value y∗ i , and we show that minimizing the Huber loss is equivalent to minimizing an upper-bound on the Kullback-Leibler (KL) divergence,
N
D (p(y∗ i | yi) q(y∗ i | k xi, θ)) (3) i=0
X yi) and q(y∗ i | when p(y∗ xi, θ) are Laplace distributions and i | the scale of the distributions are directly related to the tran-sition point of the Huber loss. For real-world problems, the value of yi corresponding to xi is often provided by a hu-man annotator; therefore, it is likely to contain some amount of noise. We believe that approximating the amount of noise in the ground-truth is a more intuitive way to determine the transition point for the Huber loss than reasoning about the
Huber density.
In the following sections, we survey the related work (Section 2), review the Huber loss and maximum likeli-hood estimation in detail (Section 3), propose our alter-5261
native probabilistic interpretation of the Huber loss (Sec-tion 4), utilize a toy problem to illustrate the relationship between the optimal transition point of the Huber loss and the noise distribution of the ground-truth (Section 5), lever-age our interpretation to analyze the loss functions utilized by modern object detectors (Section 6), and show that our proposed interpretation can lead to better hyper-parameters (Section 7). 2.