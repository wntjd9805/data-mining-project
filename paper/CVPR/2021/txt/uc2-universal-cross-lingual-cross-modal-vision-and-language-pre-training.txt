Abstract
Vision-and-language pre-training has achieved impres-sive success in learning multimodal representations be-tween vision and language. To generalize this success to non-English languages, we introduce UC2, the ﬁrst ma-chine translation-augmented framework for cross-lingual cross-modal representation learning. To tackle the scarcity problem of multilingual captions for image datasets, we
ﬁrst augment existing English-only datasets with other lan-guages via machine translation (MT). Then we extend the standard Masked Language Modeling and Image-Text
Matching training objectives to multilingual setting, where alignment between different languages is captured through shared visual context (i.e., using image as pivot). To fa-cilitate the learning of a joint embedding space of images and all languages of interest, we further propose two novel pre-training tasks, namely Masked Region-to-Token Mod-eling (MRTM) and Visual Translation Language Modeling (VTLM), leveraging MT-enhanced translated data. Evalua-tion on multilingual image-text retrieval and multilingual visual question answering benchmarks demonstrates that our proposed framework achieves new state of the art on diverse non-English benchmarks while maintaining compa-rable performance to monolingual pre-trained models on
English tasks. 1.

Introduction
The world we navigate through is a multimodal and mul-tilingual kaleidoscope. While tremendous success has been realized in multimodal research with the advent of vision-and-language (V+L) pre-training [10, 35, 36, 45, 26], the majority of current literature is biased towards English. Al-though English-trained V+L models can be ﬁnetuned on each target language (given that there is sufﬁcient language-speciﬁc data in downstream task), maintaining language-speciﬁc models for every language in the world (6,900+) is impossible given insurmountable development and main-Figure 1. A topology comparison between existing work (M3P) and our proposed UC2. M3P combines two types of pre-training tasks and the cross-modal Transformer works only on images and
English captions. Our UC2 builds a cross-lingual cross-modal
Transformer over images and all the other languages. tenance cost [23]. Naturally, a “Tower of Babel” strategy starts to gain interest in the community, aiming at build-ing one giant model that can handle all languages, notable examples including massively multilingual neural machine translation [1], cross-lingual language model [32], and mul-tilingual multimodal representation learning [18, 24].
Early works on cross-lingual multimodal tasks mainly focus on machine translation [22, 55, 6, 50, 2] and image-text retrieval [18, 28, 5, 19, 49]. The goal is to construct a common embedding space for vision and cross-lingual in-puts, and draw visual concepts from images and similar se-mantics from languages close together in the feature space.
However, due to the scarcity of large-scale training cor-pora, these models are validated only on small task-speciﬁc datasets, thus scaling and generalizing these models to more languages is non-trivial.
Recent release of large-scale multimodal datasets [41] and multilingual corpora (e.g. Wikipedia in 100 languages) has served as a key impetus to accelerate fast advances in
V+L pre-training [10, 36, 45, 54] and multilingual language modeling [12, 11, 23], which makes pre-training large-scale multilingual V+L models possible. A pioneering work is
M3P [24], which formulates the training process as alter-nating V+L pre-training between cross-modal monolingual 4155
Figure 2. An overview of UC2 model. Figure (a) shows the construction of multilingual multimodal pre-training corpus via machine translation. (b) depicts the overall UC2 framework, which is pre-trained with a massive corpus of multilingual caption-image pairs. Figure (c) and (d) illustrate details of four pre-training tasks. corpus and mono-modal cross-lingual corpus. It relies on
English as the focal point to build a bridge between im-age and different languages, which inevitably introduces linguistic discrepancy into downstream tasks that rely on direct alignment between image and Non-English language (e.g., Image-to-German retrieval), as shown in Figure 1 (a).
In this paper, we propose a new pre-training framework,
UC2 (Universal Cross-lingual Cross-modal pre-training), which pivots primarily on images and complementarily on
English for multilingual multimodal representation learn-ing (Figure 1 (b)). The major challenge is that pivoting on images requires paired image and aligned multilingual data (e.g., image-English, image-German), while existing
V+L datasets only contain image-English pairs. To ﬁll this blank, we propose to augment English-only datasets with other languages via machine translation (MT), and leverage the augmented datasets for pre-training. To the best of our knowledge, this is the ﬁrst known effort in creating large-scale training datasets with multilingual image captions.
In addition to extending two widely-adopted pre-training tasks (Masked Language Modeling and Image-Text Match-ing) to a multilingual setting, we further propose two novel pre-training objectives, namely Masked Region-to-Token
Language Modeling (MRTM) and Visual Translation Lan-guage Modeling (VTLM). MRTM encourages ﬁne-grained alignment between words and image regions, by sharing the embedding space of word tokens and region labels (i.e., object class predictions from an object detector). VTLM is designed to jointly learn cross-lingual cross-modal map-ping from parallel textual corpora and paired images. Ex-tensive experiments demonstrate that our proposed UC2 framework achieves new state of the art over multiple mainstream benchmarks such as Multi30k [16, 15, 4] and
COCO [9, 51, 34] across multilingual image-text retrieval and visual question answering (VQA) tasks.
Our contributions are summarized as follows. (i) We construct a multilingual V+L corpus, and propose the
ﬁrst MT-augmented cross-lingual cross-modal pre-training framework UC2, which pivots on both images and English language for joint representation learning. (ii) We pro-pose new pre-training tasks, Masked Region-to-Token Lan-guage Modeling and Visual Translation Language Model-ing, two effective learning objectives for multilingual mul-timodal tasks. (iv) We achieve new state of the art on multi-ple multilingual image-text retrieval and VQA benchmarks, outperforming existing methods. 2.