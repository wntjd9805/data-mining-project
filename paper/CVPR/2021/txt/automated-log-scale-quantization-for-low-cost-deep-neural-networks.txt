Abstract
Quantization plays an important role in deep neural net-work (DNN) hardware. In particular, logarithmic quanti-zation has multiple advantages for DNN hardware imple-mentations, and its weakness in terms of lower performance at high precision compared with linear quantization has been recently remedied by what we call selective two-word logarithmic quantization (STLQ). However, there is a lack of training methods designed for STLQ or even logarith-mic quantization in general. In this paper we propose a novel STLQ-aware training method, which signiﬁcantly out-performs the previous state-of-the-art training method for
STLQ. Moreover, our training results demonstrate that with our new training method, STLQ applied to weight parame-ters of ResNet-18 can achieve the same level of performance as state-of-the-art quantization method, APoT, at 3-bit pre-cision. We also apply our method to various DNNs in image enhancement and semantic segmentation, showing compet-itive results. 1.

Introduction
Quantization plays an important role in implementing energy-efﬁcient hardware for deep neural networks. Previ-ous work on quantization mostly considers uniform quan-tization, but non-uniform quantization schemes such as logarithmic-scale quantization (log-scale quantization, for short) [19, 15] can have advantages over linear quantization in terms of hardware implementation, since it can replace multiplier hardware with a shifter or an adder, depending on whether log-scale quantization is applied to one or both operands.
Log-scale quantization places more quantization bound-aries for low-magnitude values at the expense of less bound-†Corresponding author.
This work was supported by Samsung Advanced Institute of Tech-nology, Samsung Electronics Co., Ltd., IITP grant funded by MSIT of
Korea (No.2020-0-01336, Artiﬁcial Intelligence Graduate School Program (UNIST)), and Free Innovative Research Fund of UNIST (1.170067.01). aries for high-magnitude values. This can sometimes lead to a lower expected quantization error overall, but often dis-proportionately high error that happens at high-magnitude values can undermine the accuracy of a log-scale quan-tization scheme signiﬁcantly, resulting in worse perfor-mance than linear quantization at the same bit-width [15].
This problem of log-scale quantization has recently been addressed by selective two-word logarithmic quantization schemes (e.g. SLQ [16], FLightNN [9]). Selective two-word logarithmic quantization (STLQ) employs the vanilla log-scale quantization most of the time, but when the quan-tization error is higher than a certain threshold, the error is quantized again, emitting two log-quantized words. The de-quantization process is straightforward. Importantly, this scheme allows the same low-complexity hardware of log-scale quantization to be used while showing similar accu-racy as linear quantization.
However, the previous work [16, 9] has important short-comings. First, it uses hyperparameter(s) to control the ratio of two-word quantization, which is suboptimal if the same value is used for all layers/channels, or cumbersome to opti-mize if different values are used. Second, though some pre-vious work [9] uses trainable latent variables to balance be-tween accuracy and model sparsity through differentiable training, their optimization goal does not explicitly target model size or the number of two-word ratio, but instead some measure of quantization error, which is only loosely related to the two-word ratio. Consequently the training re-sult is not optimal in terms of accuracy vs. two-word quan-tization ratio. Third, ﬁnding the best weight parameters for a given two-word quantization ratio, which is a very use-ful use case scenario, is not directly supported by previous work.
In this paper we propose a novel differentiable training framework for STLQ. Our method can directly optimize weights for a given two-word quantization ratio. Starting with a two-word ratio constraint has a surprising side effect that we use to our advantage in our training. That is, we can pre-select and separate the weight parameters that are likely to require two-word quantization from the rest of the 742
weight parameters, and treat the two groups differently with different loss functions. We also propose per-tile quantiza-tion, which is to determine the number of quantized words per what is known as weight tile, based on the observation that the only requirement for efﬁcient hardware implemen-tation of STLQ is that all weight parameters within a weight tile be quantized with the same number of words.
These optimizations help us achieve signiﬁcantly better performance than the state-of-the-art training method for
STLQ [9], which is also based on differentiable training, and achieve close to ﬂoating-point baseline performance for
ResNet models using 3-bit STLQ weight and 3-bit linear ac-tivation, with a low two-word ratio (5∼15%).
We also apply STLQ and our training method to more challenging networks and tasks such as MobileNetV2,
ShufﬂeNetV2, image enhancement [2], and semantic seg-mentation [4], which typically require higher precision ac-tivation and weight. Our training result shows very com-petitive results at 3-bit. It is important to note that in all the experiments we use 3-bit STLQ quantization but some-times higher two-word ratios to achieve good performance (for MobileNetV2 and ShufﬂeNetV2), which attests to the
ﬂexibility of the STLQ scheme.
We also compare against state-of-the-art quantization,
APoT [17], which is more a novel quantizer than a train-ing method. Our 3-bit ResNet results compare favorably against APoT in terms of accuracy vs. FixOPS (a mea-sure of hardware complexity). Considering the advantages of STLQ such as the reusability of logarithmic hardware and ﬂexibility to increase performance by adjusting two-word ratio, we believe that our training method for STLQ is a very valuable contribution.
We make the following contributions. First, we propose a novel training framework for STLQ. It takes a two-word quantization ratio as a constraint, and ﬁnds the best trained weights. Second, our training method signiﬁcantly outper-forms the previous state-of-the-art STLQ training method.
Third, we have applied this method to various applications such as image classiﬁcation, image enhancement, and se-mantic segmentation, which has shown competitive results. 2.