Abstract
This paper presents a view-guided solution for the task of point cloud completion. Unlike most existing methods di-rectly inferring the missing points using shape priors, we address this task by introducing ViPC (view-guided point cloud completion) that takes the missing crucial global structure information from an extra single-view image. By leveraging a framework that sequentially performs effective cross-modality and cross-level fusions, our method achieves signiﬁcantly superior results over typical existing solutions on a new large-scale dataset we collect for the view-guided point cloud completion task. 1.

Introduction
Point cloud has attracted increasing research interest due to its wide range of applications in various ﬁelds such as auto-driving [16], robotics [27], geography [29], phenomics
In practice, the point cloud’s
[19], and archaeology [4]. data quality directly acquired by the depth scanning devices can be affected by many factors such as occlusions between objects and low scanning precision, which may lead to the poor-quality point cloud, e.g., incomplete, sparse, and noisy point cloud.
Existing methods, mainly including point cloud comple-tion [42], denoising [5], and super-resolution (up-sampling)
[38], have been proposed for the task of point cloud en-hancement. Early methods generated enhanced point cloud by mainly using shape prior information [17] or hand-crafted geometric regularities [35]. In recent years, data-driven methods, especially deep learning techniques like
PointNet [28] and DGCNN [37], have made signiﬁcant progress on this problem. Compared to traditional meth-ods, these deep learning based methods have demonstrated signiﬁcant advantages in processing objects with irregular structure and geometry.
*Corresponding authors
+
ViPC
Partial Point Cloud
Single View Image
Complete Point Cloud
Figure 1. ViPC is a new approach completing a partial point cloud by leveraging the complementary information from an ex-tra single-view image.
In this paper, we focus on the following point completion task: the input point cloud is incomplete but with limited noise, while our method outputs a complete point cloud.
Studying this problem addresses a common problem in real-world 3D data acquisition where a 3D scanner with a RGB camera is occluded by other objects in the environment. The most recent solutions to this problem are data-driven, lever-aging an encoder-decoder architecture [9, 42, 24, 39]. In those methods, an encoder transfers the incomplete input point cloud into the feature space, and then a decoder re-constructs a complete point cloud by transferring the fea-tures back to Euclidean space. The whole network works as a parameterized model by learning a mapping between the two latent spaces of incomplete and complete point cloud.
In the cases where there is a large degree of incompleteness in the input point cloud, learning this mapping with only the single-modality point cloud data is challenging because of the following factors: 1) there is a great uncertainty in infer-ring the missing points due to the limited amount of infor-mation available, 2) point cloud is of an unstructured data, together with inherent sparseness, it is difﬁcult to determine whether a blank 3D space is caused by inherent spareness or incompleteness.
In this paper, we seek a more applicable solution to the point cloud completion task. Speciﬁcally, we address the task with the help of the image modality and propose a view-guided point completion framework (ViPC) as illus-trated in Figure. 1. This setting of sensor fusion is increas-ingly common as the hardware cost decreases (e.g., the In-15890
tel Real Sense D455 and Microsoft Kinect devices). The key challenge of solving this problem is how to effectively fuse the information of pose and local details provided by the partial point cloud and the global structure informa-tion provided by the single-view image. This is not trivial because it involves a two-dimensional challenge: “cross-modality” (the information is from both image and point cloud modalities) and “cross-level” (local details and global structure are information from different levels). We ad-dress the problem by a three-stage framework which ﬁrst address the cross-modality challenge and then the cross-level challenge. Speciﬁcally, the cross-modality challenge is addressed by reconstructing a coarse point cloud from the single-view image and transferring all the information required by the completion to the identical point cloud do-main. The cross-level challenge is addressed by a differ-ential reﬁnement strategy empowered by a network called
“Dynamic Offset Predictor” which can deferentially reﬁne the points in a coarse point cloud: performing a light reﬁne-ment for low-quality points while a heavy reﬁnement for high-quality points.
To better investigate the problem, we have built a large-scale dataset called ShapeNet-ViPC on existing ShapeNet dataset [6]. Our dataset includes 38,328 objects from 13 categories. Each object has 24 sets of ground-truth data con-sisting of two incomplete point clouds produced under two typical data acquisition scenarios, a view-aligned image and a complete ground-truth point cloud. Extensive evaluations on ShapeNet-ViPC demonstrate that the proposed approach can achieve signiﬁcantly superior results than existing state-of-the-art approaches.
In summary, the main contributions of our methods are threefold: 1. We propose a new solution for point cloud comple-tion, in which an extra single-view image explicitly provides the crucial global structural prior information for completion. 2. We design a new general deep network for point cloud reﬁnement which can deferentially reﬁne the points in a point cloud. 3. We build a large-scale dataset for the point cloud com-pletion task on the ShapeNet dataset. This dataset sim-ulates point cloud defects caused by various kinds of occlusions. It could be used as a benchmark for future research of point cloud completion. 2.