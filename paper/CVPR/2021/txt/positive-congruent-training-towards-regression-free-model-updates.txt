Abstract
Both Wrong
Positive Flips
Reducing inconsistencies in the behavior of different ver-sions of an AI system can be as important in practice as re-ducing its overall error. In image classiﬁcation, sample-wise inconsistencies appear as “negative ﬂips”: A new model incorrectly predicts the output for a test sample that was correctly classiﬁed by the old (reference) model. Positive-congruent (PC) training aims at reducing error rate while at the same time reducing negative ﬂips, thus maximizing congruency with the reference model only on positive pre-dictions, unlike model distillation. We propose a simple approach for PC training, Focal Distillation, which enforces congruence with the reference model by giving more weights to samples that were correctly classiﬁed. We also found that, if the reference model itself can be chosen as an ensemble of multiple deep neural networks, negative ﬂips can be further reduced without affecting the new model’s accuracy. 1.

Introduction
Imagine a “new and improved” version of the software that manages your photo collection exhibiting mistakes ab-sent in the old one. Even if the average number of errors has decreased, every new mistake on your old photos feels like a step backward (Fig. 1), leading to perceived regression.1
We tackle regression in the update of image classiﬁcation models, where an old (reference) model is replaced by a new (updated) model. We call test samples that are correctly la-beled by both the new and the old models positive-congruent.
On the other hand, negative ﬂips are samples correctly clas-siﬁed by the old model but incorrectly by the new one. Their
*Currently at The Chinese University of Hong Kong. Work conducted while at AWS. 1In the Software Industry, regression refers to the deterioration of per-formance after an update. Even if the updated model has on average better performance, any regression risks breaking post-processing pipelines, which is why trained models are updated sporadically despite the steady stream of improvements reported in the literature.
Dog
Dog
Cat
Cat
Cat
Dog
Negative Flips
Dog
Cat
Cat
Dog
Cat
Cat
Dog
Dog
Old Model (Err. 57%) 
New Model (Err. 42%) 
Both Correct
Figure 1: Regression in model update: When updating an old classiﬁer (red) to a new one (dashed blue line), we correct mistakes (top-right, white), but we also introduce errors that the old classiﬁer did not make (negative ﬂips, bottom-left, red). While on average the errors decrease (from 57% to 42% in this toy example), regression can wreak havoc with downstream processing, nullifying the beneﬁt of the update. fraction of the total number is called negative ﬂip rate (NFR), which measures regression. There are also positive ﬂips, where the old model makes mistakes that the new one cor-rects, which are instead beneﬁcial.
Reducing the NFR could be accomplished by reducing the overall error rate (ER). However, reducing the ER is neither necessary nor sufﬁcient to reduce the NFR. In fact, models trained on the same data with different initial conditions, data augmentations, and hyperparameters tend to yield similar error rates, but with errors occurring on different samples.
For instance, two ResNet152 models trained on ImageNet with just different initializations achieve the same accuracy of 78.3% but differ on 10.4% of the samples in the validation set (Fig. 2). Assuming an equal portion of positive and negative ﬂips, updating from one model to another incurs a 5.2%N F R. This can be reduced simply by trading errors 14299
Error rate
Accuracy Gain
Negative Flip Rate alexnet squeezenet shufflenet inception model googlenet mobilenet vgg19 mnasnet densenet resnet152 40 20 0(%) alexnet squeeze. shuffle. incep. google. mobile. vgg19 mnasnet dense. res.152 alexnet squeeze. shuffle. incep. google. mobile. vgg19 mnasnet dense. res.152
Figure 2: Differences in overall accuracy vs. negative ﬂip rates. We measure the accuracy gains and negative ﬂip rates on the ILSVRC12 [26] validation set when we update from various old models (y-axis) to new models (x-axis). Multiple CNN architectures [16, 11, 36] are used for comparison. First we observe training the same architecture twice can cause regression.
If the new model has higher/lower capacity than the old one, the error rate decreases/increases (above/below the diagonal), but the NFR is always positive. Note that in some cases the NFR is of the same order of the error rate change. while maintaining an equal error rate. Thus, reducing the
ER is not necessary to reduce the NFR. Reducing the ER to a value other than zero is not sufﬁcient either: From Fig. 2 we see that, when updating an old AlexNet [16] to a new
Resnet-152 [11] ER reduces signiﬁcantly, but we still suffer a 3% NFR.
On the other hand, NFR can be made zero trivially by copying the old model, albeit with no ER reduction. Model distillation tries to bias the update towards the old model while reducing the ER. However, we wish to mimic the old model only when it is right. Thus, regression-free updates are not achieved by ordinary distillation.
Reducing the error rate and the NFR are two separate and independent goals. We call any training procedure that aims to minimize both the error rate and the negative ﬂip rate
Positive-Congruent (PC) training, or PCT for short.
We ﬁrst propose a simple method for PC training when the old model is given. In this case, PCT is achieved by minimizing an additional loss term along with the standard classiﬁcation loss (empirical cross-entropy). We consider several variants for the additional loss, and ﬁnd Focal Distil-lation (FD), a variant of the distillation loss that we introduce to bias the model towards positive congruence (Sect. 4.2), to be most effective (Sect. 5).
We also explore the problem of “future-prooﬁng” the reference model to facilitate subsequent PC training. This forward setting pertains to selecting the reference model, rather than PC training new ones. Motivated by the obser-vation above that different training instantiations can yield the same error rate with different erroneous samples, we propose a simple approach using ensembles. We show that this results in lower regression. In practice, ensembles are not viable at inference time in large-scale applications due to the high cost. Nonetheless, this approach can be used as a paragon for PC training of a single model in future works.
Our contributions can be summarized as follows: (i) We formalize the problem of quality regression in pairs of clas-siﬁers, and introduce the ﬁrst method for positive congruent training (Sect. 3); (ii) we propose a variant of model distil-lation (Sect. 4.2) to perform PC training of a deep neural network (DNN); (iii) we show that reference models can be adapted for future PC-Training by replacing a single model with an ensemble of DNNs (Sect. 4.3). We conduct ex-periments on large scale image classiﬁcation benchmarks (Sect. 5), providing both a baseline (Focal Distillation) and a paragon (Ensemble) for future evaluation. 2.