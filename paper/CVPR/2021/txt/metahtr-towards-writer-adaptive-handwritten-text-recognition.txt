Abstract
Handwritten Text Recognition (HTR) remains a chal-lenging problem to date, largely due to the varying writing styles that exist amongst us. Prior works however generally operate with the assumption that there is a limited number of styles, most of which have already been captured by ex-isting datasets. In this paper, we take a completely different perspective – we work on the assumption that there is al-ways a new style that is drastically different, and that we will only have very limited data during testing to perform adaptation. This creates a commercially viable solution – being exposed to the new style, the model has the best shot at adaptation, and the few-sample nature makes it practi-cal to implement. We achieve this via a novel meta-learning framework which exploits additional new-writer data via a support set, and outputs a writer-adapted model via sin-gle gradient step update, all during inference (see Figure 1). We discover and leverage on the important insight that there exists few key characters per writer that exhibit rel-atively larger style discrepancies. For that, we addition-ally propose to meta-learn instance speciﬁc weights for a character-wise cross-entropy loss, which is speciﬁcally de-signed to work with the sequential nature of text data. Our writer-adaptive MetaHTR framework can be easily imple-mented on the top of most state-of-the-art HTR models. Ex-periments show an average performance gain of 5-7% can be obtained by observing very few new style data (≤ 16). 1.

Introduction
Handwritten Text Recognition (HTR) has been a long-standing research problem in computer vision [6, 35, 47, 29]. As a fundamental means of communication, handwrit-ten text can appear in a variety of forms such as memos, whiteboards, handwritten notes, stylus-input, postal au-tomation, reading aid for visually handicapped, etc [49]. In general, the target of automatic HTR is to transcribe hand-∗Interned with SketchX
Attention
Module t r o p p u
S t e
S
C
N
N
B a c k b o n e
B i
-L
S
T
M 2 l a y e r
D e c o d e r
G
R
U
C o m p u e t
Inner Loop Gradient Update
Attention
Module n o i t a u a v
E l t e
S
C
N
N
B a c k b o n e
B i
-L
S
T
M 2 l a y e r
D e c o d e r
G
R
U
Writer Adapted Model (a)
Writer-A
Writer-B
Writer-C
Writer-D
Writer-E
Writer-F (b)
Figure 1. (a) During inference, our MetaHTR framework exploits additional handwritten images of a speciﬁc writer through support set, and gives rise to a writer adapted model via single gradient step update. (b) Varying styles across different writers (IAM). written text to its digital content [40] so that the textual con-tent can be made freely accessible.
Handwriting recognition is inherently difﬁcult due to its free ﬂowing nature and complex shapes assumed by char-acters and their combinations [6]. Torn pages, and warped or touching lines also make HTR more challenging. Most importantly however, handwritten texts are diverse across individual handwritten styles where each style can be very unique [13, 23] – while some might prefer an idiosyncratic style of writing certain characters like ‘G’ and ‘Z’, others may choose a cursive style with uneven line-spacing.
Modern deep learning based HTR models [28, 40, 26] mostly tackle these challenges by resourcing to a large amount of training data. The hope is that most style varia-tions would have already been captured because of the data volume. Albeit with limited success, it had become ap-parent that these models tend to over-ﬁt to styles already captured, but generalising poorly to those unseen. This is largely owning to the uniquely different styles amongst writers – there is always a new style that is unobserved, and is drastically different to the already captured (see Figure 1). The practical implication of this is, e.g., my iPad does 15830        
not recognise my handwriting as well as it does for my 4-year-old. Our ultimate vision is therefore to offer an “adapt to my writing” button, where one is asked to write a spe-ciﬁc sentence, so to make recognition performance of my own writing on par with that of my child.
Prior work on resolving the style gap remains very lim-ited. A very recent attempt turns to training using syn-thetic data, so to help the model to become more accom-modating towards new styles [24]. However, synthetic data can hardly mimic all writer-speciﬁc styles found in the real-world, especially when the style is very unique. Al-though domain adaptation and generalisation approaches might sound viable, they generally do not offer satisfac-tory performance (as shown later in experiments), and re-quire additional training via multiple gradient update steps.
The sub-optimal performance can be mostly attributed to the large and often very unique domain gaps new writing styles bring, as opposed to the common dataset biases stud-ied by domain adaption/generalisation.
In this paper, we turn to a meta-learning formulation, which not only yields performances that are of potential commercial value (from 81.3% to 89.2% Word Recognition
Accuracy), but also offers quick adaption (with just a single gradient update) using very few samples (≤16). The general motivation behind meta-learning [14, 31, 43] matches ours very well – absorbing information from related tasks and generalise onto unseen ones, by performing quick adapta-tion using a small set of examples during testing. However, getting it to work with HTR has its own challenges, and to our knowledge has not been tackled before in the litera-ture. The main challenges come from the inherent character sequence recognition nature of HTR, which is different to conventional meta-learning whose objective is mostly few-shot classiﬁcation [14, 42]. Furthermore, we importantly discovers that there also exists character-level style discrep-ancies, which when unaccounted for would trigger signiﬁ-cant performance drop (see Section 3.4).
To address these speciﬁc challenges, we ﬁrst introduce a character-wise cross-entropy loss to our meta-learning framework. This albeit being a simple change, is crucial in light of the sequence recognition nature of our problem.
We further guide the adaptation by introducing instance-speciﬁc weights on top of the character-wise loss, instead of treating all characters equally by simply averaging [40, 28].
Modelling such character-speciﬁc weight is however non-trivial, as no ﬁxed weight labels exist to supervise the learn-ing process. Consequently, we let the model learning-to-learn instance-speciﬁc weights for character-wise cross-entropy loss during the adaptation step. Our ﬁnal model,
MetaHTR, is therefore a meta-learning pipeline for writer-adaptive HTR where the model itself adaptively weights different characters to prioritise learning from more dis-crepant characters. That is, during inference, our MetaHTR framework exploits few additional handwritten images of a speciﬁc writer through a support set, and gives rise to a writer-adapted model via a single gradient update step (see
Figure 1). Our meta-learning design can be coupled with any state-of-the-art HTR model, and empirical investigation shows that model agnostic meta-learning (MAML) pipeline
[14] provides a legitimate choice to design our MetaHTR framework upon.
Contributions of this paper can be summarised as fol-lows: (1) We introduce for the ﬁrst time, the problem of writer-adaptive HTR, where the model adapts to new writ-ing styles with only very few samples during inference, (2)
We introduce a meta-learning framework to tackle this new problem, by introducing learnable instance-wise weights for a character-speciﬁc loss speciﬁcally designed for HTR. (3) We conﬁrm that our framework consistently improves upon even the most recent state-of-the-art methods. 2.