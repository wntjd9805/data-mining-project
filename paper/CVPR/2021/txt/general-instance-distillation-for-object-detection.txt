Abstract
Teacher Backbone
Student Backbone
In recent years, knowledge distillation has been proved to be an effective solution for model compression. This approach can make lightweight student models acquire the knowledge extracted from cumbersome teacher mod-els. However, previous distillation methods of detection have weak generalization for different detection frameworks and rely heavily on ground truth (GT), ignoring the valu-able relation information between instances. Thus, we pro-pose a novel distillation method for detection tasks based on discriminative instances without considering the posi-tive or negative distinguished by GT, which is called gen-eral instance distillation (GID). Our approach contains a general instance selection module (GISM) to make full use of feature-based, relation-based and response-based knowl-edge for distillation. Extensive results demonstrate that the student model achieves signiﬁcant AP improvement and even outperforms the teacher in various detection frame-works. Speciﬁcally, RetinaNet with ResNet-50 achieves 39.1% in mAP with GID on COCO dataset, which sur-passes the baseline 36.2% by 2.9%, and even better than the ResNet-101 based teacher model with 38.1% AP. 1.

Introduction
In recent years, the accuracy of object detection has made a great progress due to the blossom of deep con-volutional neural network (CNN). The deep learning net-work structure, including a variety of one-stage detection models [19, 23, 24, 25, 17] and two-stage detection mod-els [26, 16, 8, 2], has replaced the traditional object detec-tion and has become the mainstream method in this ﬁeld.
Furthermore, the anchor-free frameworks [13, 5, 32] have also achieved better performance with more simpliﬁed ap-∗The ﬁrst two authors contribute equally and the order is alphabetical.
†This work was done when Zeren was an intern at MEGVII Tech.
Det Head cls reg
Det Head cls reg
Distillation Loss
GIs Feature
Loss
GIs Relation
Loss
GIs Response 
Loss
GISM
Figure 1. Overall pipeline of general instance distillation (GID).
General instances (GIs) are adaptively selected by the output both from teacher and student model. Then the feature-based, relation-based and response-based knowledge are extracted for distillation based on the selected GIs. proaches. However, these high-precision deep learning based models are usually cumbersome, while a lightweight with high performance model is demanded in practical ap-plications. Therefore, how to ﬁnd a better trade-off between the accuracy and efﬁciency has become a crucial problem.
Knowledge Distillation (KD), proposed by Hinton et al.
[10], is a promising solution for the above problem. Knowl-edge distillation is to transfer the knowledge of large model to small model, thereby improving the performance of the small model and achieving the purpose of model compres-sion. At present, the typical forms of knowledge can be di-vided into three categories [7], response-based knowledge
[10, 22], feature-based knowledge [27, 35, 9] and relation-based knowledge [22, 20, 31, 33, 15]. However, most of the distillation methods are mainly designed for multi-class classiﬁcation problems. Directly migrating the classiﬁca-tion speciﬁc distillation method to the detection model is 17842
less effective, because of the extremely unbalanced ratio of positive and negative instances in the detection task. Some distillation frameworks designed for detection tasks cope with this problem and achieve impressive results, e.g. Li
[14] address the problem by distilling the positive et al. and negative instances in a certain proportion sampled by
[34] further propose to only dis-RPN, and Wang et al. till the near ground truth area. Nevertheless, the ratio be-tween positive and negative instances for distillation needs to be meticulously designed, and distilling only GT-related area may ignore the potential informative area in the back-ground. Moreover, current detection distillation methods cannot work well in multi detection frameworks simultane-ously, e.g. two-stage, anchor-free methods. Therefore, we hope to design a general distillation method for various de-tection frameworks to use as much knowledge as possible effectively without concerning the positive or negative.
Towards this goal, we propose a distillation method based on discriminative instances, utilizing response-based knowledge, feature-based knowledge as well as relation-based knowledge, as shown in Fig 1. There are several ad-vantages: (i) We can model the relational knowledge be-tween instances in one image for distillation. Hu et al.
[11] demonstrates the effectiveness of relational informa-tion on detection tasks. However, the relation-based knowl-edge distillation in object detection has not been explored (ii) We avoid manually setting the proportion of the yet. positive and negative areas or selecting only the GT-related areas for distillation. Though GT-related areas are almost informative, the extremely hard and simple instances may be useless, and even some informative patches from the background can be useful for students to learn the gener-alization of teachers. Besides, we ﬁnd that the automatic selection of some discriminative instances between the stu-dent and teacher for distillation can make knowledge trans-ferring more effective. Those discriminative instances are called general instances (GIs), since our method does not care about the proportion between positive and negative (iii) Our meth-instances, nor does it rely on GT labels. ods have robust generalization for various detection frame-works. GIs are calculated upon the output from student and teacher model without relying on certain modules from a speciﬁc detector or some key characteristic, such as anchor, from a particular detection framework.
•
•
To sum up, this paper makes the following contributions:
Deﬁne general instance (GI) as the distillation target, which can effectively improve the distillation effect of the detection model.
Based on GI, we ﬁrst introduce the relation-based knowledge for distillation on detection tasks and inte-grate it with response-based and feature-based knowl-edge, which makes student surpass the teacher.
We verify the effectiveness of our method on the
•
MSCOCO [18] and PASCAL VOC [6] datasets, in-cluding one-stage, two-stage and anchor-free methods, achieving state-of-the-art performance. 2.