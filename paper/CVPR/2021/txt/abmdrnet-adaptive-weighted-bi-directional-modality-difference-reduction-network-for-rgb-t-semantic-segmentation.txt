Abstract
RGB Image to Thermal Image Translation
Multi-level RGB features
Semantic segmentation models gain robustness against poor lighting conditions by virtue of complementary infor-mation from visible (RGB) and thermal images. Despite its importance, most existing RGB-T semantic segmentation models perform primitive fusion strategies, such as con-catenation, element-wise summation and weighted summa-tion, to fuse features from different modalities. These strate-gies, unfortunately, overlook the modality differences due to different imaging mechanisms, so that they suffer from the reduced discriminability of the fused features. To address such an issue, we propose, for the ﬁrst time, the strategy of bridging-then-fusing, where the innovation lies in a novel
Adaptive-weighted Bi-directional Modality Difference Re-duction Network (ABMDRNet). Concretely, a Modality Dif-ference Reduction and Fusion (MDRF) subnetwork is de-signed, which ﬁrst employs a bi-directional image-to-image translation based method to reduce the modality differ-ences between RGB features and thermal features, and then adaptively selects those discriminative multi-modality fea-tures for RGB-T semantic segmentation in a channel-wise weighted fusion way. Furthermore, considering the impor-tance of contextual information in semantic segmentation, a Multi-Scale Spatial Context (MSC) module and a Multi-Scale Channel Context (MCC) module are proposed to ex-ploit the interactions among multi-scale contextual infor-mation of cross-modality features together with their long-range dependencies along spatial and channel dimensions, respectively. Comprehensive experiments on MFNet dataset demonstrate that our method achieves new state-of-the-art results.
*Equally corresponding authors.
Translation network
Supervision
Supervision
Translation network
Multi-level thermal features
Thermal Image to RGB Image Translation (a)
RGB
Thermal (b) (e) (c) (f) (d) (g)
Figure 1. Illustration of modality difference reduction. (a) Bi-directional modality difference reduction. (b)-(d) Original RGB features, thermal features and their fused features, respectively. (e)-(g) RGB features, thermal features and their fused features af-ter reducing modality differences, respectively. 1.

Introduction
Semantic segmentation aims to assign category labels to each pixel in a natural image, which plays an important role in many computer vision task, such as autonomous driving
[6, 31], pedestrian detection [1], pathological analysis [20, 26] and so on.
So far, CNN-based RGB semantic segmentation meth-ods [14,15,20,27] have achieved prominent results in many large-scale datasets [5, 16]. However, their performance may signiﬁcantly degrade under poor lighting conditions.
To boost semantic segmentation performance, recent re-searches pay more attention to RGB-T semantic segmen-tation [9, 22, 25], where thermal images may complement rich contour information and semantic information to RGB images under poor lighting conditions.
Existing models for multi-modality pixel-level predic-tion tasks, including RGB-T semantic segmentation and
RGB-T salient object detection, usually adopt simple strate-gies, such as element-wise summation [25], concatenation
[9] and weighted summation [8, 32], to capture the comple-mentary information from paired RGB and thermal images.
However, they usually ignore the modality differences be-tween RGB images and thermal images, which are caused 2633
by different imaging mechanisms. Such negligence may lead to inadequate cross-modality complementary informa-tion exploitation. As shown in Fig. 1, the people region, marked by red dotted box in Fig. 1(b), has low intensity values, while the same region in Fig. 1(c) has higher inten-sity values. If simple fusion operations are employed, the discriminative target information in the thermal image will be noticeably suppressed in the fused features, as shown in
Fig. 1(d).
To solve this problem, we propose a novel multi-modality feature fusion subnetwork, i.e., Modality Differ-ence Reduction and Fusion (MDRF), to better exploit the multi-modality complementary information from RGB im-ages and thermal images via a novel strategy of bridging-then-fusing. In the bridging stage, as shown in Fig. 1(a), a bi-directional image-to-image translation [13, 33] based method is employed to reduce the differences between RGB and thermal features. The basic idea is that when trans-ferring images from one modality to another, some non-discriminative single-modality information, caused by dif-ferent imaging mechanisms (e.g., Fig. 1(b) and Fig. 1(c)), will be translated into discriminative ones (e.g., Fig. 1(e) and Fig. 1(f)) by virtue of the complementary supervision information from the images of another modality. As a re-sult, the modality differences between the extracted single-modality RGB and thermal features will be reduced for bet-ter fusion (e.g., Fig. 1(d) and Fig. 1(g)). Then, in the fusing stage, a novel fusion module, i.e., Channel Weighted Fusion (CWF) module, is presented to capture the cross-modality information between the corresponding channels of single-modality RGB and thermal features, whose modality differ-ences have been reduced in the ﬁrst step. As shown in Fig. 1(d) and Fig. 1(g), higher discriminative fused features may be obtained by using the single-modality features that have reduced modality differences than those original ones.
Furthermore, the diversity of objects, e.g., categories, sizes and shapes, in a given image is also problematic for semantic segmentation. Multi-scale contextual information and their long-range dependencies have been proved to be effective to address such an issue in RGB semantic segmen-tation. However, in multi-modality semantic segmentation, especially for RGB-T semantic segmentation [9, 22, 25], multi-scale contextual information of cross-modality fea-tures and their long-range dependencies are not in place yet.
In RGB-T semantic segmentation, only MFNet [9] added several mini-inception blocks in the encoder to obtain some contextual information. But this is far limited for semantic segmentation.
Inspired by [3, 6, 30], we propose two novel modules, i.e., a Multi-Scale Spatial Context (MSC) module and a
Multi-Scale Channel Context (MCC) module, to exploit the multi-scale contextual information of cross-modality fea-tures and their long-range dependencies along spatial and channel dimensions, respectively. First, multi-scale fea-tures are obtained by performing the Atrous Spatial Pyra-mid Pooling (ASPP) module [3] on the original fused cross-modality features. Then the long-range dependencies for these multi-scale features along the spatial and channel di-mensions are established by jointly using the original fused cross-modality features and their corresponding multi-scale features in MSC and MCC, respectively. With MSC and
MCC cooperative, the multi-scale contextual information of cross-modality features and their long-range dependencies will be fully exploited for RGB-T semantic segmentation.
The main contributions of this paper are summarized as follows: (1) An end-to-end ABMDRNet is presented to facilitate
RGB-T semantic segmentation by simultaneously consid-ering multi-modality difference reduction and multi-scale contextual information of cross-modality data. Comprehen-sive experimental results show that our model achieves new state-of-the-art performance on MFNet dataset. (2) An MDRF subnetwork is presented to effectively capture the cross-modality information from the RGB and images via a strategy of bridging-then-fusing, thermal which ﬁrst employs a bi-directional image-to-image trans-lation based method to bridge the modality gaps between multi-modality data and then adaptively selects those dis-criminative multi-modality features for RGB-T semantic segmentation. (3) An MSC module and an MCC module are presented to fully exploit the multi-scale contextual information of cross-modality features and their long-range dependencies along the spatial and channel dimensions, respectively. 2.