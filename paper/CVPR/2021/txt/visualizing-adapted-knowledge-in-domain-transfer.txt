Abstract
A source model trained on source data and a target model learned through unsupervised domain adaptation (UDA) usually encode different knowledge. To understand the adaptation process, we portray their knowledge dif-ference with image translation.
Speciﬁcally, we feed a translated image and its original version to the two mod-els respectively, formulating two branches. Through up-dating the translated image, we force similar outputs from the two branches. When such requirements are met, dif-ferences between the two images can compensate for and hence represent the knowledge difference between models.
To enforce similar outputs from the two branches and de-pict the adapted knowledge, we propose a source-free im-age translation method that generates source-style images using only target images and the two models. We visual-ize the adapted knowledge on several datasets with differ-ent UDA methods and ﬁnd that generated images success-fully capture the style difference between the two domains.
For application, we show that generated images enable fur-ther tuning of the target model without accessing source data. Code available at https://github.com/hou-yz/DA_visualization. 1.

Introduction
Domain transfer or domain adaptation aims to bridge the distribution gap between source and target domains.
Many existing works study the unsupervised domain adap-tation (UDA) problem, where the target domain is unla-beled [27, 6, 46, 1, 11]. In this process, we are interested in what knowledge neural networks learn and adapt.
Essentially, we should visualize the knowledge differ-ence between models: a source model trained on the source domain, and a target model learned through UDA for the target domain. We aim to portray the knowledge difference with image generation. Given a translated image and its original version, we feed the two images to the source and the target model, respectively. It is desired that differences between image pairs can compensate for the knowledge dif-(a) Target images (real-world) (b) Generated source-style images (c) Unseen source images (synthetic)
Figure 1: Visualization of adapted knowledge in unsuper-vised domain adaptation (UDA) on the VisDA dataset [38].
To depict the knowledge difference, in our source-free im-age translation (SFIT) approach, we generate source-style images (b) from target images (a).
Instead of accessing source images (c), the training process is guided entirely by the source and target models, so as to faithfully portray the knowledge difference between them. ference between models, leading to similar outputs from the two branches (two images fed to two different models).
Achieving this, we could also say that the image pair repre-sent the knowledge difference.
This visualization problem is very challenging and heretofore yet to be studied in the literature. It focuses on a relatively understudied ﬁeld in transfer learning, where we distill knowledge differences from models and embed it in generated images. A related line of works, traditional image translation, generates images in the desired style utilizing content images and style images [7, 13, 48], and is applied 13824
in pixel-level alignment methods for UDA [26, 2, 44, 11].
However, relying on images from both domains to indicate the style difference, such works cannot faithfully portray the knowledge difference between source and target models, and are unable to help us understand the adaptation process.
In this paper, we propose a source-free image translation (SFIT) approach, where we translate target images to the source style without using source images. The exclusion of source images prevents the system from relying on image pairs for style difference indication, and ensures that the system only learns from the two models. Speciﬁcally, we feed translated source-style images to the source model and original target images to the target model, and force similar outputs from these two branches by updating the generator network. To this end, we use the traditional knowledge dis-tillation loss and a novel relationship preserving loss, which maintains relative channel-wise relationships between fea-ture maps. We show that the proposed relationship preserv-ing loss also helps to bridge the domain gap while chang-ing the image style, further explaining the proposed method from a domain adaptation point of view. Some results of our method are shown in Fig. 1. We observe that even un-der the source-free setting, knowledge from the two models can still power the style transfer from the target style to the source style (SFIT decreases color saturation and whitens background to mimic the unseen source style).
On several benchmarks [19, 36, 39, 38], we show that generated images from the proposed SFIT approach signiﬁ-cantly decrease the performance gap between the two mod-els, suggesting a successful distillation of adapted knowl-edge. Moreover, we ﬁnd SFIT transfers the image style at varying degrees, when we use different UDA methods on the same dataset. This further veriﬁes that the SFIT visualizations are faithful to the models and that different
UDA methods can address varying degrees of style differ-ences. For applications, we show that generated images can serve as an additional cue and enable further tuning of target models. This also falls into a demanding setting of UDA, source-free domain adaptation (SFDA) [17, 20, 24], where the system has no access to source images. 2.