Abstract
Current deep learning architectures suffer from catas-trophic forgetting, a failure to retain knowledge of previously learned classes when incrementally trained on new classes. The fundamental roadblock faced by deep learning methods is that the models are optimized as “black boxes,” making it difﬁcult to properly adjust the model parameters to preserve knowledge about previously seen data. To overcome the problem of catastrophic forgetting, we propose utilizing an alternative “white box” architec-ture derived from the principle of rate reduction, where each layer of the network is explicitly computed without back propagation. Under this paradigm, we demonstrate that, given a pretrained network and new data classes, our approach can provably construct a new network that emulates joint training with all past and new classes.
Finally, our experiments show that our proposed learning algorithm observes signiﬁcantly less decay in classiﬁcation performance, outperforming state of the art methods on
MNIST and CIFAR-10 by a large margin and justifying the use of “white box” algorithms for incremental learning even for sufﬁciently complex image data. 1.

Introduction
Humans are capable of acquiring new information con-tinuously while retaining previously obtained knowledge.
This seemingly natural capability, however, is extremely difﬁcult for deep neural networks (DNNs) to achieve. In-cremental learning (IL), also known as continual learning or life-long learning, thus studies the design of machine learning systems that can assimilate new information with-out forgetting past knowledge.
In incremental learning, models go through rounds of training sessions to accumulate knowledge for a particular objective (e.g. classiﬁcation). Speciﬁcally, under class in-cremental learning (class-IL), an agent has access to train-ing data from a subset of the classes, known as a task, at each training session and is evaluated on all seen classes at inference time. The overarching goal is to precisely ﬁne-∗ The ﬁrst two authors contributed equally to this work. tune a model trained on previously seen tasks to additionally classify new classes of data. However, due to the absence of old data, such models often suffer from catastrophic for-getting [14], which refers to a drastic drop in performance after training incrementally on different tasks.
In the last few years, a ﬂurry of continual learning al-gorithms have been proposed for DNNs, aiming to alle-viate the effect of catastrophic forgetting. These meth-ods can be roughly partitioned into three categories: 1) regularization-based methods that often involve knowledge distillation [12, 6, 19, 25], 2) exemplar-based methods that keep partial copies of data from previously learned tasks
[16, 1, 22], and 3) modiﬁed architectures that attempt to utilize network components specialized for different tasks
[17, 19, 11].
In practice, these algorithms exhibit vary-ing performance across different datasets and their ability to mitigate catastrophic forgetting is inadequate. Factors including domain shift [18] across tasks and imbalance of new and past classes [22] are part of the reason.
The fundamental roadblock in deep continual learning is that DNNs are trained and optimized in a “black box” fashion. Each model contains millions of mathematical op-erations and its complexity prevents humans from follow-ing the mapping from data input to prediction. Given our current limited understanding of network parameters, it is difﬁcult, if not impossible, to precisely control the parame-ters of a pre-trained model such that the decision boundary learned ﬁts to new data without losing its understanding of old data.
In this work, we take a drastically different approach to incremental learning. We avoid “black box” architectures entirely, and instead utilize a recently proposed “white box”
DNN architecture derived from the principle of rate reduc-tion [2]. Termed ReduNet, each layer of this DNN can be explicitly computed in a forward-propagation fashion and each parameter has precise statistical interpretations. The so-constructed network is intrinsically suitable for incre-mental learning because the second-order statistics of any previously-seen training data is preserved in the network parameters to be leveraged for future tasks.
We propose a new incremental learning algorithm utiliz-1125
ing ReduNet to demonstrate the power and scalability of de-signing more interpretable networks for continual learning.
Speciﬁcally, we prove that a ReduNet trained incrementally can be constructed to be equivalent to one obtained by joint training, where all data, both new and old, is assumed to be available at training time. Finally, we observe that ReduNet performs signiﬁcantly better on MNIST [9] and CIFAR-10
[7] in comparison to current continual DNN approaches. 2.