Abstract
Facial attribute editing aims to manipulate the image with the desired attribute while preserving the other de-tails. Recently, generative adversarial networks along with the encoder-decoder architecture have been utilized for this task owing to their ability to create realistic images. How-ever, the existing methods for the unpaired dataset cannot still preserve the attribute-irrelevant regions properly due to the absence of the ground truth image. This work proposes a novel, intuitive loss function called the CAM-consistency loss, which improves the consistency of an input image in image translation. While the existing cycle-consistency loss ensures that the image can be translated back, our approach makes the model further preserve the attribute-irrelevant regions even in a single translation to another domain by using the Grad-CAM output computed from the discrimi-nator. Our CAM-consistency loss directly optimizes such a Grad-CAM output from the discriminator during train-ing, in order to properly capture which local regions the generator should change while keeping the other regions unchanged. In this manner, our approach allows the gener-ator and the discriminator to collaborate with each other to improve the image translation quality. In our experiments, we validate the effectiveness and versatility of our proposed
CAM-consistency loss by applying it to several represen-tative models for facial image editing, such as StarGAN,
AttGAN, and STGAN. 6509
1.

Introduction
Image-to-image translation is a key task in computer vi-sion, the aim of which is to learn the mapping of an in-put image in a source domain to the one in a target do-main. Since generative adversarial networks (GANs) [6] have been proposed, showing their ability to create real-istic images, numerous studies on image translation, such as facial attribute editing, have been conducted due to its practicality. However, it is unrealistic to ﬁnd all the paired datasets for various attributes (e.g., the same person with a different gender), so unpaired image translation approaches have also been studied. For example, Zhu et al. [31] intro-duced CycleGAN, which can translate the images between different domains using the unpaired dataset via the cycle-consistency loss. StarGAN [3] and AttGAN [8] were also proposed to edit the facial attributes while achieving multi-domain translations using a single generator.
Facial attribute editing, which aims to manipulate the particular attribute with the given face image, is still a chal-lenging task. One such challenge lies in difﬁculty in pre-serve attribute-irrelevant regions while changing the desired attribute of a given image. For example, the existing models often change the overall color of an image to a golden color when imposing a blond hair attribute. Recently, additional modules were proposed to preserve attribute-irrelevant re-gions. Zhang et al. [29] proposed SaGAN that changes only the partial region of an image based on the estimated seg-mentation masks. However, such local manipulation may not be applicable in the case of changing the global attribute (e.g., gender and age) translation.
In response, CAFE-GAN [12] attempted to preserve the attribute-irrelevant re-gions by predicting the attribute-relevant information in fea-ture maps, not at a pixel level, using the attention branch network (ABN) [5]. RelGAN [16] uses the relative at-tributes and utilizes a conditional adversarial loss by taking triplets consisting of two images and a vector of modiﬁed attributes for image translation. However, these approaches are not easily applicable to the general architectures since they require speciﬁc modules, and they do not employ an explicit loss for pixel-level preservation.
To further address this issue, we propose a novel, in-tuitive loss function called the CAM-consistency loss for image-to-image translation by utilizing the Grad-CAM [20] in adversarial training. Our proposed loss is widely ap-plicable to the existing image translation approaches such as StarGAN [3], AttGAN [8], and STGAN [18] without modifying their architectures. Furthermore, our CAM-consistency loss can overcome various limitations of the ex-isting methods since it enforces the generator to preserve the irrelevant regions of an image at a pixel level while making the discriminator attend to the attribute-relevant informa-tion at a feature level. This allows the model to generate the image at once while preserving the attribute-irrelevant re-gions as shown in Figure 1. In summary, our contributions include:
• We propose a novel loss function called the CAM-consistency loss, which can directly enforce the gen-erator to preserve the attribute-irrelevant regions while the discriminator handles the attribute-relevant re-gions. It works even without any additional informa-tion such as segmentation maps of each attribute or any modiﬁcation of the network architectures, and also it allows the generator and the discriminator collaborate with each other for the better performance.
• Our proposed CAM-consistency loss overcomes the limitations of the existing image-to-image translation approaches by directly preserving attribute-irrelevant regions computed by the discriminator.
• We demonstrate the possibility of using the Grad-CAM directly as training objectives, rather than just a visualization tool. 2.