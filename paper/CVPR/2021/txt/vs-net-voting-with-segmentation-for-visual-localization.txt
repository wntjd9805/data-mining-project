Abstract 1.

Introduction
Visual localization is of great importance in robotics and computer vision. Recently, scene coordinate regression based methods have shown good performance in visual lo-calization in small static scenes. However, it still estimates camera poses from many inferior scene coordinates. To ad-dress this problem, we propose a novel visual localization framework that establishes 2D-to-3D correspondences be-tween the query image and the 3D map with a series of learnable scene-speciﬁc landmarks. In the landmark gen-eration stage, the 3D surfaces of the target scene are over-segmented into mosaic patches whose centers are regarded as the scene-speciﬁc landmarks. To robustly and accurately recover the scene-speciﬁc landmarks, we propose the Voting with Segmentation Network (VS-Net) to segment the pixels into different landmark patches with a segmentation branch and estimate the landmark locations within each patch with a landmark location voting branch. Since the number of landmarks in a scene may reach up to 5000, training a seg-mentation network with such a large number of classes is both computation and memory costly for the commonly used cross-entropy loss. We propose a novel prototype-based triplet loss with hard negative mining, which is able to train semantic segmentation networks with a large number of la-bels efﬁciently. Our proposed VS-Net is extensively tested on multiple public benchmarks and can outperform state-of-the-art visual localization methods. Code and models are available at https://github.com/zju3dv/VS-Net.
*Zhaoyang Huang and Han Zhou assert equal contributions.
†Corresponding author: Guofeng Zhang.
‡The authors from Zhejiang University are also afﬁliated with ZJU-SenseTime Joint Lab of 3D Vision. This work was partially supported by
NSF of China(Nos. 61822310 and 61932003), Centre for Perceptual and
Interactive Intelligence Limited, the General Research Fund through the
Research Grants Council of Hong Kong under Grants (Nos. 14208417 and 14207319), and CUHK Strategic Fund.
Localization [58, 44, 40] is a pivotal technique in many real-world applications, such as Augmented Reality (AR),
Virtual Reality (VR), robotics, etc. With the popularity and low cost of visual cameras, visual localization has attracted widespread attention from the research community.
Recently, scene coordinate regression based methods [8, 7, 25], which learn neural networks to predict dense scene coordinates of a query image and recover the camera pose through RANSAC-PnP [18], dominate visual localization and achieve state-of-the-art localization accuracy in small static scenes. Compared with classical feature-based visual localization frameworks [16, 26, 61, 44] relying on iden-tiﬁed map points from Structure-from-Motion (SfM) tech-niques, it only requires to estimate 2D-to-3D correspon-dences and can be beneﬁted from high-precision sensors.
Although scene coordinates construct dense 2D-3D corre-spondences, most of them are unable to recover reliable camera poses.
In dynamic environments, there could ex-ist moving objects and varying lighting conditions which raise the outlier ratio and increase the probability of choos-ing an erroneous pose with RANSAC algorithms. In addi-tion, even after outlier rejection with RANSAC, there might exist inferior scene coordinates that lead to inaccurate local-ization.
In the hope of estimating camera poses more ro-bustly and accurately, we propose Voting with Segmenta-tion Network (VS-Net) to identify and localize a series of scene-speciﬁc landmarks through a Voting-by-Segmentation framework.
In contrast with scene coordinate regres-sion methods that predict pixel-wise dense 3D scene co-ordinates, the proposed framework only estimates a small quantity of scene-speciﬁc landmarks (or 2D-3D correspon-dences) that are of much higher accuracy.
Unlike feature-based visual localization methods, where landmarks are directly extracted from the images accord-ing to certain rules, we manually specify a series of scene-6101
(a) Query Image (b) Errors of Scene Coordinates (c) Errors of Scene-Speciﬁc Landmarks
Figure 1: Reprojection errors of 2D-to-3D correspondences of scene coordinates and scene-speciﬁc landmarks. (a) The query image. (b) The reprojection errors of dense scene coordinates predicted by the regression-only network [25]. (c) The reprojection errors of scene-speciﬁc landmarks and their surrounding patches by the proposed method. Pixels belonging to the same landmark are painted with the same color representing the landmark’ reprojection error. The white pixels in (c) are
ﬁletered by our voting-by-segmentation algorithm speciﬁc landmarks from each scene’s reconstructed 3D sur-faces. The 3D surface of a scene is ﬁrst uniformly divided into a series of 3D patches, and we deﬁne the centers of the 3D patches as the 3D scene-speciﬁc landmarks. Given a new image obtained from a new viewpoint, we aim to iden-tify the 3D scene-speciﬁc landmarks’ projections on the 2D image. The Voting-by-Segmentation framework with the
VS-Net casts the landmark localization problem as a combi-nation of patch-based landmark segmentation coupled with pixel-wise direction voting problem. Each pixel in the im-age is ﬁrst segmented into one of the pre-deﬁned patches (landmarks) and the pixels classiﬁed into the kth landmark are responsible for estimating the corresponding landmark’s 2D location. To achieve the goal, the proposed VS-Net also estimates a 2D directional vector at each pixel loca-tion, which is trained to point towards the pixel’s corre-sponding landmark. For a given patch, such predicted di-rectional vectors can be treated as directional votes. With a
RANSAC algorithm, for each predicted patch, the accurate 2D landmark location can be accurately estimated. In con-trast to existing scene coordinate regression methods, in our proposed framework, pixels or regions that are poorly seg-mented with erroneous patch labels and directional votes can be robustly ﬁltered out as those pixels have low vot-ing consistency. Therefore, this strategy ensures that the survived landmarks are of high accuracy and the inferior pixels would not jeopardize the accuracy of camera pose estimation. It results in fewer landmarks with lower outlier ratios and reprojection errors than scene coordinate regres-sion methods (Fig. 1).
The patch-based landmark segmentation in our VS-Net requires assigning pre-deﬁned patch labels, i.e., landmark
IDs, to pixels. However, the number of patches or land-marks in a scene can reach tens of thousands. Directly adopting the conventional cross-entropy loss for multi-class segmentation requires huge memory and computational costs as the number of parameters in the classiﬁcation layer increases proportionally to the number of patches. We pro-pose prototype-based triplet loss to address this problem, which avoids computing complete label scores by devel-oping pixel-wise triplet loss with prototypes. Moreover, prototype-based triplet loss improves the training efﬁciency by online mining informative negative prototypes.
In summary, our proposed approach has the following (1) We propose the novel VS-Net major contributions: framework that casts the problem of visual localization from scene-speciﬁc landmarks as a voting-by-segmentation prob-lem. Camera poses estimated from the proposed scene-speciﬁc landmarks are shown to be more robust and accu-rate. (2) We propose the prototype-based triplet loss for patch-based landmark segmentation with a large number of classes, which shows competitive segmentation accuracy while saving much computation and memory. To our best knowledge, we are the ﬁrst to address the problem of a large number of classes in image segmentation. (3) The VS-Net signiﬁcantly outperforms previous scene coordinate regres-sion methods and representative SfM-based visual localiza-tion methods on both the popular 7Scenes dataset and the
Cambridge Landmarks dataset. 2.