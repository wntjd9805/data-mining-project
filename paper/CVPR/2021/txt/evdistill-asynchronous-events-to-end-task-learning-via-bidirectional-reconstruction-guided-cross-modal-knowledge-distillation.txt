Abstract
Event cameras sense per-pixel intensity changes and produce asynchronous event streams with high dynamic range and less motion blur, showing advantages over the conventional cameras. A hurdle of training event-based models is the lack of large qualitative labeled data. Prior works learning end-tasks mostly rely on labeled or pseudo-labeled datasets obtained from the active pixel sensor (APS) frames; however, such datasets’ quality is far from rival-ing those based on the canonical images.
In this paper, we propose a novel approach, called EvDistill, to learn a student network on the unlabeled and unpaired event data (target modality) via knowledge distillation (KD) from a teacher network trained with large-scale, labeled im-age data (source modality). To enable KD across the un-paired modalities, we ﬁrst propose a bidirectional modal-ity reconstruction (BMR) module to bridge both modalities and simultaneously exploit them to distill knowledge via the crafted pairs, causing no extra computation in the inference.
The BMR is improved by the end-tasks and KD losses in an end-to-end manner. Second, we leverage the structural similarities of both modalities and adapt the knowledge by matching their distributions. Moreover, as most prior fea-ture KD methods are uni-modality and less applicable to our problem, we propose an afﬁnity graph KD loss to boost the distillation. Our extensive experiments on semantic seg-mentation and object recognition demonstrate that EvDis-till achieves signiﬁcantly better results than the prior works and KD with only events and APS frames. 1.

Introduction
Event cameras have recently received much attention in the computer vision and robotics community for their dis-tinctive advantages, such as high dynamic range (HDR) and much less motion blur. Event cameras sense the in-tensity changes at each pixel asynchronously and produce event streams encoding time, pixel location, and polarity
∗These two authors contributed equally.
Figure 1: EvDistill distills knowledge from a teacher network trained with large labeled images to a student network learn-ing unpaired and unlabeled events for the end-tasks. To distill knowledge, a bidirectional modality reconstruction and distribu-tion adaptation schemes, with the novel KD losses, are proposed. (sign) of intensity changes. Recently, deep neural network (DNN)-based methods with large-scale, labeled image data have shown signiﬁcant performance gains on many tasks.
However, learning effective event-based DNNs has been impeded by the lack of large pixel-level labeled event data.
Prior works learning event-based high-level tasks have re-sorted to the manually annotated task-speciﬁc datasets in a supervised manner [1, 4, 7, 18, 19, 37, 40, 54, 62, 69]. Al-though some labeled event datasets [30, 45, 83] have been collected, the quantity and quality are far less favorable compared to those based on the canonical images. Some works [1, 18] have made pseudo labels using the active pixel sensor (APS) images; however, these labels are less accu-rate due to the low quality of APS images and considerable domain gap with the source data. While [83, 85] have ex-plored unsupervised learning, they only focus on the pixel-level prediction tasks, e.g., depth estimation. Another line of research has reconstructed intensity images from events
[41, 54, 59, 63, 65, 66], and these images have been used to learn DNNs on end-tasks, e.g., object recognition [54]; however, annotated labels are still needed, and extra latency is introduced in the inference time.
We explore to leverage large labeled image data (a.k.a. source modality) and the learned models, and aim to learn a model on the unpaired and unlabeled events (a.k.a. target 1608
modality) via cross-modal learning [22, 81] and knowledge distillation (KD) [22, 61, 68]. Most existing cross-modal learning methods have relied on paired data (e.g., image and depth) with the same labels [22, 23, 35, 46, 67, 71, 76, 81] or extra information (e.g., data or labels) [2, 17, 50, 77] or grafting networks between modalities (e.g., image to ther-mal) [29] for learning the end-tasks. Some works have ex-plored the unpaired multi-modality data [13, 35]; however, it is assumed that labels for both modalities are available, which is difﬁcult to achieve for the event data.
To overcome these limitations, we propose a novel method, called EvDistill, to efﬁciently learn a student net-work on the unpaired and unlabeled event data by distill-ing the knowledge from a robust teacher network trained with large labeled image data, as shown in Fig. 1. Firstly, we propose a novel bidirectional modality reconstruction (BMR) module to bridge both modalities, and then simul-taneously exploit them to distill knowledge via the crafted pairs, adding no extra computation cost during inference (Sec. 3.2). Importantly, the BMR is improved by the end-task and the KD losses in an end-to-end manner. That is,
BMR produces the crafted pairs of both modalities to dis-till knowledge to the student network in the forward pass, and KD facilitates the learning of the BMR in the backward pass. Secondly, as the feature representations of two modal-ities extracted from the task networks could suffer from dis-tribution mismatch, we leverage the structural similarities and adapt knowledge by matching the class distributions based on the BMR module (Sec. 3.3). Moreover, as most existing feature KD methods are limited to uni-modality
[33, 55, 78], we propose a novel graph afﬁnity KD loss and other losses to learn a better model on the event data (Sec. 3.4). We evaluate the performance of the proposed framework on three datasets in semantic segmentation (Sec. 4.1) and one dataset in object recognition (Sec. 4.2).
The experiments show that our approach achieves signif-icantly better performance than the prior works for both end-tasks and the naive setting, KD with only events and the APS frames (when APS frames are available). The val-idation code and trained models are available at https:
//github.com/addisonwang2013/evdistill. 2.