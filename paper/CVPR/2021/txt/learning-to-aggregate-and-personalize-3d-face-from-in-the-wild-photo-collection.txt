Abstract
Non-parametric face modeling aims to reconstruct 3D face only from images without shape assumptions. While plausible facial details are predicted, the models tend to over-depend on local color appearance and suffer from ambiguous noise. To address such problem, this paper presents a novel Learning to Aggregate and Personalize (LAP) framework for unsupervised robust 3D face model-ing. Instead of using controlled environment, the proposed method implicitly disentangles ID-consistent and scene-speciﬁc face from unconstrained photo set. Speciﬁcally, to learn ID-consistent face, LAP adaptively aggregates intrin-sic face factors of an identity based on a novel curricu-lum learning approach with relaxed consistency loss. To adapt the face for a personalized scene, we propose a novel attribute-reﬁning network to modify ID-consistent face with target attribute and details. Based on the proposed method, we make unsupervised 3D face modeling beneﬁt from mean-ingful image facial structure and possibly higher resolu-tions. Extensive experiments on benchmarks show LAP re-covers superior or competitive face shape and texture, com-pared with state-of-the-art (SOTA) methods with or without prior and supervision. 1.

Introduction
Monocular 3D reconstruction of human face is a long-standing problem with potential applications including ani-mation, biometrics and human digitalization. It is an essen-tially ill-posed problem requiring strong assumption, e.g., shape-from-shading approaches [67]. With 3D Morphable
Model (3DMM) [4] proposed, the reconstruction can be achieved through optimization on low-dimensional param-eters [38, 37, 73]. Recently, deep neural networks are intro-duced to regress 3DMM parameters from 2D images with
Figure 1. (a) Qualitative comparison between our method and Un-sup3D [59] and DF2Net [65]. Our results show better shape of organs with ﬁner details and less noise. (b) Distribution of pa-rameter difference in expression basis [5] between all single-ID image pairs on face dataset [30, 61]. With the manually computed threshold of 0.04, it reveals that quite amounts (about 70%) of im-age pairs have similar expressions and non-rigid difference. Such conclusion inspires us to approximate expressions by mean condi-tions with reasonable relaxations, and learn a multi-image consis-tent face without 3DMM prior. Better showed by zooming in. supervision [71, 35, 9, 12, 28] or improve 3DMM with non-linearity [49, 36, 51, 14, 48, 70]. Meanwhile, as these single-view guided methods may suffer from 2D ambiguity, other 3DMM-based works are proposed to leverage multi-view consistency [53, 47, 62, 57, 2]. While 3DMM provides reliable priors for 3D face modeling, it also brings potential drawbacks: as built from a small amount of subjects (e.g.,
BFM [33] with 200 subjects) and rigidly controlled con-ditions, models may be fragile to large variations of iden-tity [72], and have limitations on building teeth, skin details or anatomic grounded muscles [10].
Due to the aforementioned limitations, an alternative ap-proach is learning to model 3D face without 3DMM as-sumption, e.g., regressing face normal or depth directly from an input image [18, 52, 42, 65, 1] with ground truth scans and pseudo labels. Despite the efﬁciency of these approaches, they cannot model facial texture or a canon-114214
ical view without occlusion. Recent work Unsup3D [59] uses a weakly symmetric constraint to disentangle a face into intrinsic factors and accomplishes the canonical recon-struction in unsupervised manner. In summary, these non-parametric methods predict plausible facial structure via re-construction or rendering loss [22]. However, without re-liable 3DMM prior, they tend to suffer from ambiguity of image appearance. As illustrated in Fig. 1(a), results of Un-sup3D [59] and DF2Net [65] have coarse or inconsistent shape of organs. Further, Unsup3D suffers from noise and discontinuity when reproduced for higher-scale reconstruc-tion (Unsup3D-re), which makes the resolution less valu-able. Such phenomenon is due to improper disentangling of albedo, illumination and geometry due to ambiguity of image details and noises as discussed in [50, 24, 10]. On top of these, we argue that predicting meaningful and con-sistent facial structure is a key point of unsupervised non-parametric 3D face modeling.
To achieve such goal, a better disentangling approach could be: ﬁrst modeling basic facial geometry and texture of an identity, then adding speciﬁc attributes and details for a target scene. Actually, basic facial structure is mainly based on bones and anatomic grounded muscles of an iden-tity, which can be enhanced by using 3DMM across un-constrained image set against ambiguous noise [40, 8, 13].
However, multi-image clues are difﬁcult to introduce for non-parametric methods due to lack of shape topology. To tackle this problem, we make two assumptions for image collections: i) Besides the shape, the appearance of an iden-tity due to basic facial structure, like winkles and occlusion of illumination, are similar enough; ii) Non-rigid shape de-formation (mainly about expression) among faces are with limited extent. The ﬁrst assumption has been demonstrated by works of [40, 8, 13]. For the second one, we compare the expression difference of all image pairs of photo sets in datasets. By using released SOTA 3DMM based model [8], we analyse the distribution of mean parameter difference of image pairs on expression basis [5] in Fig. 1(b). With the computed similarity threshold 0.04 from manually se-lected 1k separate image pairs with similar/dissimilar ex-pression, we observe that about 70% pairs are below the threshold with mild non-rigid difference. Such conclusion makes it possible to approximate expressions by mean con-ditions with reasonable relaxations, and learn a multi-image consistent face without 3DMM prior.
In this paper, we propose a novel Learning to Aggregate and Personalize (LAP) framework for unsupervised non-parametric 3D face modeling. LAP ﬁrst aggregates consis-tent face factors of an identity from in-the-wild photo col-lection, and then personalizes such factors to reconstruct a scene-speciﬁc face for a target image of the same ID. Con-cretely, LAP decodes a pair of ID-consistent albedo and depth by adaptively aggregating a global ID code from an
[71, 35, 9, 12, 72]
[49, 36, 14, 51, 50, 48, 27]
[47, 57, 2, 43]
[40, 8, 13]
[18, 52, 1, 65]
[42, 39, 59]
Ours
MI-Consistency
×
×
Constrained
In-the-Wild
×
×
In-the-Wild
Shape Assumption 3DMM 3DMM 3DMM 3DMM
No
No
No
Supervision 3DMM paramter
I
I
I 3DMM parameter, 3D scan, I
I
I
Table 1. Comparison with selected existing method on different settings. Constrained/In-the-wild means the condition of image set, and I means image. image set, and reconstructs a 3D face aligned to each input image based on an estimated speciﬁc light and pose. Such aggregation model is optimized by a curriculum learning method with relaxed consistency loss, which helps to over-come large facial variations and lack of pre-deﬁned topol-ogy. Moreover, to personalize a speciﬁc face, LAP modiﬁes
ID-consistent face through an attribute-reﬁning network for modeling speciﬁc attributes and details. In this way, LAP achieves disentangling of ID-consistent facial structure and scene-speciﬁc local details in an unsupervised manner with-out 3DMM shape assumption. With LAP framework, we manage to model 3D face from arbitrary number of images, or even single image in superior quality and higher resolu-tion than State-of-the-Art (SOTA) methods.
In summary, this paper has contributions in followings: i) We propose a novel Learning to Aggregate and Per-sonalize (LAP) framework to disentangle ID-consistent and scene-speciﬁc 3D face from multi or single image, without 3DMM assumptions in fully unsupervised manner. ii) With a novel relaxed curriculum aggregation method,
LAP is able to predict ID-consistent face factors against large facial variations of in-the-wild photo set. iii) Based on the ID-consistent factors, LAP uses an attribute-reﬁning network to model scene-speciﬁc 3D face with less noise and ﬁner details of higher resolutions. 2.