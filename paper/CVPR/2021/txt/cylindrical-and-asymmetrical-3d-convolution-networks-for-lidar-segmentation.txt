Abstract
State-of-the-art methods for large-scale driving-scene
LiDAR segmentation often project the point clouds to 2D space and then process them via 2D convolution. Although this corporation shows the competitiveness in the point cloud, it inevitably alters and abandons the 3D topology and geometric relations. A natural remedy is to utilize the 3D voxelization and 3D convolution network. However, we found that in the outdoor point cloud, the improvement ob-tained in this way is quite limited. An important reason is the property of the outdoor point cloud, namely sparsity and varying density. Motivated by this investigation, we pro-pose a new framework for the outdoor LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolu-tion networks are designed to explore the 3D geometric pat-tern while maintaining these inherent properties. Moreover, a point-wise reﬁnement module is introduced to alleviate the interference of lossy voxel-based label encoding. We evaluate the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method achieves the 1st place in the leaderboard of SemanticKITTI and out-performs existing methods on nuScenes with a noticeable margin, about 4%. Furthermore, the proposed 3D frame-work also generalizes well to LiDAR panoptic segmentation and LiDAR 3D detection. 1.

Introduction 3D LiDAR sensor has become an indispensable device in modern autonomous driving vehicles. It captures more precise and farther-away distance measurements of the sur-* denotes the equal contribution and code at https : / /
Corresponding email: github . com / xinge008 / Cylinder3D. zhuxinge123@gmail.com rounding environments than conventional visual cameras.
The measurements of the sensor naturally form 3D point clouds that can be used to realize a thorough scene under-standing for autonomous driving planning and execution, in which LiDAR segmentation is crucial for driving-scene un-derstanding. It aims to identify the pre-deﬁned categories of each 3D point, such as car, truck etc., which provides point-wise perception information of the overall 3D scene.
Recently, the advances in deep learning have signiﬁ-cantly pushed forward the state of the art in image seg-mentation. Some existing LiDAR segmentation approaches follow this route to project the 3D point clouds onto a 2D space and process them via 2D convolution networks, in-cluding range image based [23, 37] and bird’s-eye-view im-age based [46]. However, this group of methods lose and alter the accurate 3D geometric information during the 3D-to-2D projection (as shown in the top row of Fig. 1a).
A natural alternative is to utilize the 3D partition and 3D convolution networks to process the point cloud and main-tain their 3D geometric relations. However, in our initial at-tempts, we directly apply the 3D voxelization [12, 7] and 3D convolution networks to outdoor LiDAR point cloud, only to ﬁnd very limited performance gain (as shown in Fig. 1b).
Our investigation into this issue reveals a key difﬁculty of outdoor LiDAR point cloud, namely sparsity and varying density, which is also the key difference to indoor scenes with dense and uniform-density points. However, previ-ous 3D voxelization methods consider the point cloud as a uniform one and split them via the uniform cube, while neglecting the varying-density property of outdoor point cloud. Consequently, this effect to apply the 3D partition to outdoor point cloud is met with fundamental difﬁculty.
Motivated by these ﬁndings, we propose a new frame-work to outdoor LiDAR segmentation that consists of two key components, i.e., 3D cylindrical partition and asymmet-rical 3D convolution networks, which maintain the 3D ge-9939
Range Image 2D Convolution
Involved Points
LiDAR Point Cloud
Cubic Partition
Point Distribution mIoU 67.8 58.5 55.9 54.3 53.9 52.2
SAD
PolarNet
RandLA
RangeNet
Ours 3DVoxel
LiDAR Point Cloud
Cylindrical Partition
Point Distribution (a) 2D (b) 3D
Figure 1: (a) Range Image (2D projection) v.s. Cubic Partition v.s. Cylindrical Partition. From top row, it can be found that range image abandons the 3D topology, where 2d convolution processes points in different locations (far away from each other in green circles). From bottom part, cylindrical partition generates the more balanced point distribution than cubic partition (89% v.s. 61% cells containing points). (b) Applying the regular 3D voxel partition and 3D convolution directly (i.e., 3DVoxel) gets limited performance gain compared to projection-based (2D) methods [46, 14, 23, 40], while our method achieves a remarkable performance gain by further tackling the inherent difﬁculty of outdoor LiDAR point clouds (showing results on SemanticKITTI dataset). ometric information and handle these issues from partition and networks, respectively. Here, cylindrical partition re-sorts to the cylinder coordinates to divide the point cloud dynamically according to the distance (Regions that are far away from the origin have much sparse points, thus requir-ing a larger cell), which produces a more balanced point distribution (as shown in Fig. 1a); while asymmetrical 3D convolution networks strengthen the horizontal and verti-cal kernels to match the point distribution of objects in the driving scene and enhance the robustness to the sparsity.
Moreover, voxel based methods might divide the points with different categories into the same cell and cell label encoding would inevitably cause the information loss. To alleviate the interference of lossy label encoding, a point-wise module is introduced to further reﬁne the features ob-tained from voxel-based network. Overall, the corporation of these components well maintains the geometric relation and tackle the difﬁculty of outdoor point cloud, thus im-proving the effectiveness of 3D frameworks.
We evaluate the proposed method on two large-scale outdoor datasets, including SemanticKITTI [2] and nuScenes [5]. Our method achieves the 1st place in the leaderboard of SemanticKITTI and also outperforms the ex-isting methods on nuScenes with a large margin. We also extend the proposed cylindrical partition and asymmetrical 3D convolution networks to LiDAR panoptic segmentation and LiDAR 3D detection. Experimental results show its strong generalization capability and scalability.
The contributions of this work mainly lie in three as-pects: (1) We reposition the focus of outdoor LiDAR seg-mentation from 2D projection to 3D structure, and further investigate the inherent properties (difﬁculties) of outdoor (2) We introduce a new framework to ex-point cloud. plore the 3D geometric pattern and tackle these difﬁculties caused by sparsity and varying density, through cylindri-cal partition and asymmetrical 3D convolution networks. (3) The proposed method achieves the state of art on Se-manticKITTI and nuScenes, and also shows strong gener-alization capability on LiDAR panoptic segmentation and
LiDAR 3D detection. 2.