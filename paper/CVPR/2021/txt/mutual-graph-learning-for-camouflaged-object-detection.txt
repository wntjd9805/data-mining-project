Abstract
Automatically detecting/segmenting object(s) that blend in with their surroundings is difÔ¨Åcult for current mod-els. A major challenge is that the intrinsic similarities be-tween such foreground objects and background surround-ings make the features extracted by deep model indistin-guishable. To overcome this challenge, an ideal model should be able to seek valuable, extra clues from the given scene and incorporate them into a joint learning frame-work for representation co-enhancement. With this inspi-ration, we design a novel Mutual Graph Learning (MGL) model, which generalizes the idea of conventional mutual learning from regular grids to the graph domain. SpeciÔ¨Å-cally, MGL decouples an image into two task-speciÔ¨Åc fea-ture maps ‚Äî one for roughly locating the target and the other for accurately capturing its boundary details ‚Äî and fully exploits the mutual beneÔ¨Åts by recurrently reasoning
Importantly, their high-order relations through graphs. in contrast to most mutual learning approaches that use a shared function to model all between-task interactions,
MGL is equipped with typed functions for handling differ-ent complementary relations to maximize information in-teractions. Experiments on challenging datasets, including
CHAMELEON, CAMO and COD10K, demonstrate the ef-fectiveness of our MGL with superior performance to exist-ing state-of-the-art methods. Code is available at https:
//github.com/fanyang587/MGL. 1.

Introduction
CamouÔ¨Çage is an important skill in nature, because it helps certain animals hide from their predators by blend-ing in with their surroundings. The ability of camouÔ¨Çag-ing, which is closely related to how human perception works, has attracted increasing research attention over past decades. Biological and psychological studies show that it is hard for human beings to quickly spot camouÔ¨Çaged ani-mals or objects [4, 48]. A possible reason is that the primi-*Corresponding author: Fan Yang (fanyang uestc@hotmail.com)
Recurrent
"ùêπ"
"ùêπ!
ùêπ"
ùêπ!
Figure 1: Illustration of MGL. Given an image (a), we use a ResNet-FCN as the backbone (b) to extract task-speciÔ¨Åc features for the camouÔ¨Çaged object detection (COD) and camouÔ¨Çaged object-aware edge extraction (COEE), respec-tively (c). Then, we exploit the mutual beneÔ¨Åts from both tasks by reasoning about their mutual relations with the co-operation of the Region-Induced Graph Reasoning (RIGR) module and Edge-Constricted Graph Reasoning (ECGR) module in a recurrent manner (d). Finally, the evolved fea-tures (e) are mapped into the results (f). tive function of our visual system may be designed to detect topological properties [2], thus making it difÔ¨Åcult to iden-tity camouÔ¨Çaged animals/objects that break up visual edge information of their ‚Äòtrue‚Äô bodies.
In spite of these biol-ogy discoveries, how to make up for this ‚ÄòÔ¨Çaw‚Äô in human perception by Machine is, unfortunately, still an under-explored topic in computer vision.
Identifying a camouÔ¨Çaged object from its background, also known as camouÔ¨Çaged object detection (COD) [7], is a valuable, yet challenging task [9]. ‚ÄòSeeing through camou-Ô¨Çage‚Äô has promising prospects for facilitating various real-life tasks, including image retrieval [29], species discov-ery [42], trafÔ¨Åc risk management, medical image analy-sis [10, 12, 58], etc. However, the existing deep models are still incapable of fully resolving the intrinsic visual similar-ities between foreground objects and background surround-ings. To overcome this difÔ¨Åculty, current approaches distill additional knowledge by extracting auxiliary features from the shared context, e.g., features for identiÔ¨Åcation [9] or classiÔ¨Åcation [20], to signiÔ¨Åcantly augment the underlying 12997
representations for camouÔ¨Çaged object detection. Although their notable successes truly demonstrate the beneÔ¨Åt of ex-ploiting extra knowledge in camouÔ¨Çaged object detection, there are still three major open issues. First, the mutual in-Ô¨Çuence between COD and its auxiliary task is overlooked or poorly investigated. More speciÔ¨Åcally, because the ex-isting efforts [9, 20, 63] only exploit extra information from the auxiliary task to guide/assist the main task (i.e. COD), while ignoring the important collaborative relationship be-tween them, these models may fail to a local minimum [49].
Second, as the cross-task dependencies are modeled only in the original coordinate space, more global, higher-order guidance information may be lost. As we demonstrate em-pirically, current COD models become ineffective under heavy occlusions and indeÔ¨Ånable boundaries, because they fail to incorporate higher-order information into the rep-resentation learning process. Third, according to recent biological discoveries [17, 53, 54], a key factor for con-cealment/camouÔ¨Çage is the edge disruption. Unfortunately, how to enhance true edge visibility for facilitating the rep-resentation learning for COD is not investigated by existing arts [9, 20], which deÔ¨Ånitely would weaken, or at least not fully utilize, the COD model‚Äôs learning power.
Targeting at these drawbacks, we present a novel Mutual
Graph Learning model (MGL) to sufÔ¨Åciently and compre-hensively exploit mutual beneÔ¨Åts between camouÔ¨Çaged ob-ject detection (COD) and its auxiliary task. Considering that the edge disruption should be one of the key factors for cam-ouÔ¨Çage [17, 53, 54], we treat the camouÔ¨Çaged object-aware edge extraction (COEE) as an auxiliary task and incorporate it into our MGL for mutual learning. As shown in Figure 1, our MGL has a well-designed interweaving architecture that strengthens the interaction and cooperation between tasks.
Importantly, instead of ‚Äòna¬®ƒ±vely‚Äô fusing the learned features from two tasks as in the existing works, MGL precisely exploits useful information from the counterparts for rep-resentation co-enhancement by explicitly reasoning about the complementary relations between COD and COEE with two typed functions. To mine the semantic guidance infor-mation from COD and assist COEE, we develop a novel
Region-Induced Graph Reasoning (RIGR) module to rea-son about the high-level dependencies, and transfer seman-tic information from COD to augment underlying represen-tations for COEE; To improve the true edge visibility, a new
Edge-Constricted Graph Reasoning (ECGR) module is used to explicitly incorporate the edge information from COEE to, in turn, better guide the representation learning for COD.
Importantly, our RIGR and ECGR can be formulated in a re-current manner to recursively mine the mutual beneÔ¨Åts and incorporate valuable information from their counterparts.
We demonstrate the effectiveness of our MGL by com-paring it against strong baselines and current state-of-the-art methods through extensive experiments on a variety of benchmarks. The experiment results clearly demonstrate its superiority over existing methods in mining mutual guid-ance information for camouÔ¨Çaged object detection. The contributions of this work are summarized as follows:
‚Ä¢ A novel graph-based, mutual learning approach for camouÔ¨Çaged object detection. To our knowledge, this is the Ô¨Årst attempt to exploit mutual guidance knowledge between two closely related tasks, i.e., COD and COEE, using the graph-based techniques for camouÔ¨Çaged object detection. This approach is able to capture semantic guid-ance knowledge and spatial supportive information for mutually boosting the performance of both tasks.
‚Ä¢ Carefully designed graph-based interaction functions for fully mining typed guidance information. Unlike conventional mutual learning approaches, our MGL en-sembles two distinct graph-based interaction modules to reason about typed relations: RIGR for mining seman-tic guidance information from COE to assist COEE and,
ECGR for incorporating true edge priors to enhance the underlying representations of COD.
‚Ä¢ State-of-the-art results on widely-used benchmarks.
Our MGL sets new records on a variety of benchmarks, i.e., CHAMELEON [47], CAMO [20] and COD10K [9], and outperforms existing COD models by a large margin. 2.