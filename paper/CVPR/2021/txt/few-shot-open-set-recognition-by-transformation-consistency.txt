Abstract
In this paper, we attack a few-shot open-set recogni-tion (FSOSR) problem, which is a combination of few-shot learning (FSL) and open-set recognition (OSR). It aims to quickly adapt a model to a given small set of labeled sam-ples while rejecting unseen class samples. Since OSR re-quires rich data and FSL considers closed-set classiﬁcation, existing OSR and FSL methods show poor performances in solving FSOSR problems. The previous FSOSR method fol-lows the pseudo-unseen class sample-based methods, which collect pseudo-unseen samples from the other dataset or synthesize samples to model unseen class representations.
However, this approach is heavily dependent on the com-position of the pseudo samples. In this paper, we propose a novel unknown class sample detector, named SnaTCHer, that does not require pseudo-unseen samples. Based on the transformation consistency, our method measures the dif-ference between the transformed prototypes and a modiﬁed prototype set. The modiﬁed set is composed by replacing a query feature and its predicted class prototype. SnaTCHer rejects samples with large differences to the transformed prototypes. Our method alters the unseen class distribu-tion estimation problem to a relative feature transformation problem, independent of pseudo-unseen class samples. We investigate our SnaTCHer with various prototype transfor-mation methods and observe that our method consistently improves unseen class sample detection performance with-out closed-set classiﬁcation reduction. 1.

Introduction
Recently, deep neural networks show outstanding per-formance in various computer vision problems. There are many reasons for this achievement, but there is no doubt that a large volume of high-quality datasets has been a great aid. However, in real-world applications, a large amount of high-quality data is not always available for various rea-sons, such as a high labeling cost of experts or the need to collect rare data.
Few-shot learning (FSL) methods [28, 6, 27, 35] are pro-Figure 1. A visualization of the few-shot open-set recognition task.
Few-shot learning methods fail to recognize unseen class sam-ples, and open-set recognition methods require a large amount of datasets. Few-shot open-set recognition is a generalized few-shot learning task, where the model has to identify unseen class queries while classifying seen class queries correctly. posed to reduce the data dependency. It assumes a severe condition where a few labeled data are available for train-ing, such as one to ﬁve for each class. Various methods show remarkable improvements in FSL problems, however, these methods are limited to closed-set problems where training samples and testing samples share the same class pool. On the contrary, in real-world scenarios, there could be irregular inputs to network models that could damage the reliability of models. For instance, let us consider an auto-matic face tag system in images for social network services.
For a new picture, the system should tag the face region of a friend or ask the user for manual tagging if the person in the image is not on the friend list. The system requires two crucial features for better user experience: correct tag-ging (i.e., correct closed-set classiﬁcation) and correct ask-ing (i.e., correct unseen class sample detection). However, the nature of FSL methods forces to tag the person as one in the friend list.
Open-set recognition (OSR) manages the unseen class detection problem in the large-scaled data situations.
It aims to detect unseen class samples (i.e., unseens) from 12566
seen class sample a set of prototypes
{ unseen class sample
}
{
}
{
}
{
}
{
<
{
}
} k and P
Figure 3. A visualization of our method. Each colored box rep-resents a prototype or a query feature. SnaTCHer replaces a pre-dicted class prototype to the query sample in the set of prototypes u), then it measures differences between the proto-(P type set and the replaced set after the feature transformation T (·).
SnaTCHer rejects samples by the distance from the transformed prototype set T (P). Our method alters estimating feature distribu-tion of unseen class samples for the detection to the relative feature transformation problem. lected class prototype is replaced with the query feature vector. Since unseen samples tend to form a distinctive feature space from known samples, the transformed fea-tures after the replacement are more likely to be far away from the transformed prototypes than that of seen class sam-ples by the transformation consistency. The difference after the transformation is used to identify unseen class samples.
Note that SnaTCHer does not require additional pseudo-unseen samples to train the unseen sample detector. Our method shifts the training paradigm of the unseen sam-ple detector from estimating unseen sample distributions to training a feature transformer that uses relationships be-tween features. This approach is more straightforward than estimating the unseen sample distribution directly.
We evaluate our method with various transformation methods [36, 35, 30, 1, 3] including our normalization-based method. Our SnaTCHer signiﬁcantly improves the unseen detection capability, without classiﬁcation perfor-mance degradation of various few-shot learning models.
Furthermore, we propose a cross-domain FSOSR bench-mark that compares inter-domain robustness of FSOSR methods. Our method achieves the best performance in the cross-domain evaluation either.
To summarize, our contribution is three-fold:
•
We propose a novel unseen sample detector for
FSOSR, named SnaTCHer, based on the transforma-tion consistency. Our SnaTCHer improves the un-seen class sample detection capability without pseudo-unseen samples. 12567
Figure 2. Inﬂuence of pseudo-unseen data conﬁgurations on
FSOSR tasks. Error bars represent 95% conﬁdence intervals.
Closest samples pseudo-unseen data from the nearest classes to current prototypes, and Farthest collects from the farthest classes.
The gathered pseudo-unseen samples are trained with a prediction entropy maximization loss widely used in pseudo-unseen based methods [5, 20, 16]. seen class samples (i.e., seens), while maintaining the clas-siﬁcation capability [2, 7, 21, 29, 23]. These OSR meth-ods utilize the characteristics of seens from an informative dataset to organize an unseen class sample detector. How-ever, the rich data information of seen classes is not guar-anteed in FSL problems. These OSR methods suffer from performance degrading under the FSL condition for many reasons, such as overﬁtting.
In this paper, we attack the few-shot open-set recogni-tion (FSOSR) problem. FSOSR aims to distinguish unseen class samples from seen class samples while maintaining the classiﬁcation capability, using a few labeled supports.
We illustrate the concept of FSOSR in Fig. 1. The previ-ous FSOSR study [16] deals with this problem by training the feature extractor with pseudo-unseen class samples ag-gregated from additional non-overlapped classes. However, we observe that the pseudo-unseen based approach is heav-ily dependent on the quality of pseudo-unseen samples as il-lustrated in Fig. 2. Furthermore, these methods assume that unseen class instances are visually similar to the pseudo-unseen samples, which is not guaranteed in real-world sit-uations. These problems are critical for FSL cases since the target task distribution is unknown during training the model with the base data.
To this end, we propose a novel unknown detector, named SnaTCHer, that does not require pseudo-unseen
Figure 3 shows the concept of samples for training.
SnaTCHer. Our method utilizes the transformation con-sistency [4], where similar samples remain close after the transformation. The transformer is trained to form a task-adaptive feature space using class representation vectors (i.e., prototypes). For the unseen detection, a query ﬁrstly selects its closest transformed prototype. Then, the se-•
•
We show the limitations of pseudo-unseen class sample-based methods on FSOSR tasks. These meth-ods heavily depend on the pseudo-unseen data conﬁg-uration.
We conduct extensive experiments of our method on various benchmarks, and show our method achieves the best performance in the unseen sample detection without performance loss of the classiﬁcation. 2.