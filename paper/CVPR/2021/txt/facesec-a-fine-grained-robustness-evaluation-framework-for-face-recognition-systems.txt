Abstract
We present FACESEC, a framework for ﬁne-grained ro-bustness evaluation of face recognition systems. FACESEC evaluation is performed along four dimensions of adversar-ial modeling: the nature of perturbation (e.g., pixel-level or face accessories), the attacker’s system knowledge (about training data and learning architecture), goals (dodging or impersonation), and capability (tailored to individual in-puts or across sets of these). We use FACESEC to study
ﬁve face recognition systems in both closed-set and open-set settings, and to evaluate the state-of-the-art approach for defending against physically realizable attacks on these. We
ﬁnd that accurate knowledge of neural architecture is signiﬁ-cantly more important than knowledge of the training data in black-box attacks. Moreover, we observe that open-set face recognition systems are more vulnerable than closed-set sys-tems under different types of attacks. The efﬁcacy of attacks for other threat model variations, however, appears highly dependent on both the nature of perturbation and the neural network architecture. For example, attacks that involve ad-versarial face masks are usually more potent, even against adversarially trained models, and the ArcFace architecture tends to be more robust than the others. 1.

Introduction
Face recognition has received much attention [12, 23, 25, 34, 15, 33] in recent years. Empowered by deep convolu-tional neural networks (CNNs), it has become widely used in various areas, including security-sensitive applications, such as airport check-in, online ﬁnancial transactions, and mobile device login. The success of such deep face recognition is particularly striking, with >99% prediction accuracy on benchmark datasets [23, 16, 15, 6].
*Work done during an internship at NEC Laboratories America.
†Corresponding author.
Despite its widespread success in computer vision appli-cations, recent studies have found that deep face recognition models are vulnerable to adversarial examples in both dig-ital space [18, 8, 36] and physical space [26]. The former directly modiﬁes an input face image by adding impercep-tible perturbations to mislead face recognition (henceforth, digital attacks). The latter is characterized by adding adver-sarial perturbations that can be realized on physical objects (e.g., wearing an adversarial eyeglass frame [26]), which are subsequently captured by a camera and then fed into a face recognition model to fool prediction (henceforth, physically realizable attacks). As such, the aforementioned domains, especially critical domains such as security and ﬁnance, are subjected to risks of opening the backdoor for the attackers.
For example, in face recognition supported ﬁnancial/banking services, an illegal user may bypass biometric veriﬁcation and steal money from victims’ accounts. Therefore, there exists a vital need for methods that can comprehensively and systematically evaluate the robustness of face recognition systems in adversarial settings, which in turn can shed light on the design of robust models for downstream tasks.
The main challenges of comprehensive evaluation of the robustness of face recognition lie in dealing with the diversity of face recognition systems and adversarial environments.
First, different face recognition systems consist of various key components (e.g., training data and neural architecture); such diversity results in different performance and robust-ness. To enable comprehensive and systematic evaluations, it is crucial to assess the robustness of every individual or a combination of face recognition components in adversarial settings. Second, adversarial example attacks can vary by the nature of perturbations (e.g., pixel-level or physical space), an attacker’s goal, knowledge, and capability. For a given face recognition system, its robustness against a speciﬁc type of attack may not generalize to other kinds [35].
In spite of recent advances in adversarial attacks [26, 8, 36] that demonstrate the vulnerability of face recognition 13254
systems, most existing methods fail to address the aforemen-tioned challenges due to the following reasons. First, current efforts appeal to either white-box attacks or black-box at-tacks to obtain a lower bound or upper bound of robustness.
These bounds indicate the vulnerability of face recognition systems in adversarial settings but lack the understanding of how each component of face recognition contributes to such vulnerability. Second, while most existing approaches focus on a speciﬁc type of attack (e.g., digital attacks that incur imperceptible noise [8, 36]), they fail to explore the different levels of robustness in response to various attacks (e.g., physically realizable attacks).
To bridge this gap, we propose FACESEC, a ﬁne-grained robustness evaluation framework for face recognition sys-tems. FACESEC incorporates four dimensions in evaluation: the nature of adversarial perturbations (pixel-level or face accessories), the attacker’s accurate knowledge about the target face recognition system (training data and neural ar-chitecture), goals (dodging or impersonation), and capability (individual or universal attacks). Speciﬁcally, we implement both digital and physically realizable attacks in FACESEC.
We leverage the PGD attack [18], the state-of-the-art digi-tal attack paradigm, and the eyeglass frame attack [26] as the representative of physically realizable attacks. Addi-tionally, we propose two novel physically realizable attacks: one involves pixel-level adversarial stickers on human faces, and the other adds color grids on face masks. Moreover, to facilitate universal attacks that produce image-agnostic perturbations, we propose a systematic approach that works on top of the attack paradigms described above.
In summary, this paper makes the following contributions: (1) We propose FACESEC, the ﬁrst robustness evaluation framework that enables researchers to (i) identify the vulnerability of each face recognition component to ad-versarial examples, and (ii) assess different levels of robustness under various adversarial circumstances. (2) We propose two novel physically realizable attacks: the pixel-level sticker attack and the grid-level face mask attack. These allow us to explore adversarial robustness against different types of physically realizable perturba-tions. Particularly, the latter responds to the pressing needs for security analysis of face recognition systems, as face masks have become common face accessories during the COVID-19 pandemic. (3) We propose a general approach to produce universal adversarial examples for a batch of face images. Com-pared to previous works, our paradigm has a signiﬁcant speedup and is more efﬁcient in evaluation. (4) We perform a comprehensive evaluation on ﬁve publicly available face recognition systems in various settings to demonstrate the efﬁcacy of FACESEC. n o i t a c i i f i t n e d
I e c a
F i n o t a c i f i r e
V e c a
F
Test
I mage
Classifier
I dentity
Neur al 
Networ ks
Tr aining
Set
Test
I mage
Galler y
I mage
Tr aining
Set
Classifier
I dentity: i
Same L abel?
I dentity: j
Neur al 
Networ ks
Feature 
Extr actor
Neur al 
Networ ks
Feature
Extr actor
Feature 
Vector
M ost  similar ?
Feature 
Vector s
Feature  Vector i
Similar ity?
Neur al 
Networ ks
Feature 
Vector j
Test
I mage
Galler y
Tr aining
Set
Test
I mage
Galler y
I mage
Tr aining
Set
Closed-set Face Recognition
Open-set Face Recognition
Figure 1. Closed-set and open-set face recognition systems. 2.