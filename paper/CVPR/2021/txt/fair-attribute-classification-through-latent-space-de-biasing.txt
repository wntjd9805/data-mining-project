Abstract
Fairness in visual recognition is becoming a prominent and critical topic of discussion as recognition systems are deployed at scale in the real world. Models trained from data in which target labels are correlated with protected attributes (e.g., gender, race) are known to learn and exploit those correlations. In this work, we introduce a method for training accurate target classiﬁers while mitigating biases that stem from these correlations. We use GANs to generate realistic-looking images, and perturb these images in the underlying latent space to generate training data that is balanced for each protected attribute. We augment the original dataset with this generated data, and empirically demonstrate that target classiﬁers trained on the augmented dataset exhibit a number of both quantitative and qualitative beneﬁts. We conduct a thorough evaluation across multiple target labels and protected attributes in the CelebA dataset, and provide an in-depth analysis and comparison to existing literature in the space. Code can be found at https://github. com/princetonvisualai/gan-debiasing. 1.

Introduction
Large-scale supervised learning has been the driving force behind advances in visual recognition. Recently, however, there has been a growing number of concerns about the disparate impact of these visual recognition systems. Face recognition systems trained from datasets with an under-representation of certain racial groups have exhibited lower accuracy for those groups [9]. Activity recognition mod-els trained on datasets with high correlations between the activity and the gender expression of the depicted person have over-ampliﬁed those correlations [46]. Computer vision systems are statistical models that are trained to maximize accuracy on the majority of examples, and they do so by ex-ploiting the most discriminative cues in a dataset, potentially learning spurious correlations. In this work, we introduce a new framework for training computer vision models that aims to mitigate such concerns, illustrated in Figure 1.
One proposed path for building ‘fairer’ computer vision
Figure 1: Training a visual classiﬁer for an attribute (e.g., hat) can be complicated by correlations in the training data. For example, the presence of hats can be correlated with the presence of glasses. We propose a dataset augmentation strategy using Generative Adversarial Networks (GANs) that successfully removes this correlation by adding or removing glasses from existing images, creating a balanced dataset. systems is through a ‘fairer’ data collection process. Works such as [9, 43] propose techniques for better sampling data to more accurately represent all people. Creating a perfectly bal-anced dataset, however, is infeasible in many cases. With the advances in Generative Adversarial Networks (GANs) [17], several works propose using generated data to augment real-world datasets [12, 35, 42]. These methods have been growing in computational and algorithmic complexity (e.g.,
[35, 42] adding multiple loss functions to GAN training), necessitating access to a sufﬁcient number of inter-sectional real-world samples. In contrast, we demonstrate a simple and novel data augmentation technique that uses a single
GAN trained on a biased real-world dataset.
Illustrative example: Consider our example from Figure 1.
Our goal is to train a visual recognition model that recog-nizes the presence of an attribute, such as wearing a hat.
Suppose in the real world wearing a hat is correlated with wearing glasses—for example, because people often wear both hats and sunglasses outside and take them off inside.
This correlation may be reﬂected in the training data, and a classiﬁer trained to recognize a hat may rely on the presence of glasses. Consequently, the classiﬁer may fail to recognize a hat in the absence of glasses, and vice versa.
We propose using a GAN to generate more images with hats but not glasses and images with glasses but not hats, such that WearingHat is de-correlated from Glasses in the training data, by making perturbations in the latent space.
Building on work by Denton et al. [14], which demonstrates 19301
Figure 2: Consider a GAN trained on a biased real-world dataset of faces where the presence of hats is correlated with the presence of glasses. Naively moving in a direction that adds glasses also adds a hat (Top). We learn a direction in the latent space that allows us to add glasses, while not adding a hat (Bottom). Note that attributes apart from the target attribute can change. a method for learning interpretable image manipulation di-rections, we propose an improved latent vector perturba-tion method that allows us to preserve the WearingHat attribute while changing the Glasses attribute (Figure 2).
Protected attributes: Our goal is to examine and mitigate biases of sensitive attributes such as gender expression, race, or age in visual classiﬁers. However, visual manipulations or explicit classiﬁcations along these dimensions have the po-tential to perpetuate harmful stereotypes (see [23]). Hence in our illustrations, we use Glasses as the protected at-tribute, as it has a clear visual signal. In the quantitative experimental results, we report our ﬁndings on the more sensitive protected attributes of gender expression and age.
Contributions: We propose a method for perturbing vectors in the GAN latent space that successfully de-correlates target and protected attributes and allows for generating a de-biased dataset, which we use to augment the real-world dataset. At-tribute classiﬁers trained with the augmented dataset achieve quantitative improvements in several fairness metrics over both baselines and prior work [35, 36, 41], while maintain-ing comparable average precision. Furthermore, we analyze the CelebA [28] attributes with respect to label character-istics1, discriminability, and skew, and discuss how these factors inﬂuence our method’s performance. We also evalu-ate our design choices with ablation studies and the results demonstrate the effectiveness of our augmentation method.2 2.