Abstract 1.

Introduction
Image-to-Image (I2I) multi-domain translation models are usually evaluated also using the quality of their seman-tic interpolation results. However, state-of-the-art models frequently show abrupt changes in the image appearance during interpolation, and usually perform poorly in inter-polations across domains. In this paper, we propose a new training protocol based on three speciﬁc losses which help a translation network to learn a smooth and disentangled latent style space in which: 1) Both intra- and inter-domain interpolations correspond to gradual changes in the gener-ated images and 2) The content of the source image is better preserved during the translation. Moreover, we propose a novel evaluation metric to properly measure the smoothness of latent style space of I2I translation models. The proposed method can be plugged in existing translation approaches, and our extensive experiments on different datasets show that it can signiﬁcantly boost the quality of the generated images and the graduality of the interpolations.
Translating images from one domain to another is a challenging image manipulation task that has recently drawn increasing attention in the computer vision commu-nity [9, 10, 16, 17, 26, 29, 37, 43]. A “domain” refers to a set of images sharing some distinctive visual pattern, usu-ally called “style” (e.g., the gender or the hair color in face datasets) [10, 16, 43]. The Image-to-Image (I2I) translation task aims to change the domain-speciﬁc aspects of an image while preserving its “content” (e.g., the identity of a person or the image background) [16]. Since paired data (e.g., im-ages of the same person with different gender) are usually not available, an important aspect of I2I translation models is the unsupervised training [43]. Moreover, it is usually de-sirable to synthesize the multiple appearances modes within the same style domain, in such a way to be able to generate diverse images for the same input image.
Recent work addresses the I2I translation using multi-* These two authors contributed equally to this work. Correspondence to: wei.wang@unitn.it and denadai@fbk.eu. 10785        
Gap (a)
Female
Male (b) (c) (d)
Figure 2: An illustration of the relation between smoothness and disentanglement of the style space. (a) Two well-separated distributions with a large margin in between. The intermediate area can lead to the generation of artifacts because it has not been sufﬁciently explored during training. (b) When the margin is reduced, the corresponding image appearance changes are smoother. (c) A t-SNE visualization of randomly sampled style codes using StarGAN v2 [10], which shows a disentangled style space but also that the inter-domain area generates images with artifacts. (d) The same visualization shows that, using our method, despite the disentanglement is preserved, the inter-domain area generates realistic images. ple domains [9, 26, 10] and generating multi-modal out-puts [26, 10]. These Multi-domain and Multi-modal Unsu-pervised Image-to-Image Translation (MMUIT) models are commonly evaluated based on the quality and the diversity of the generated images, including the results obtained by interpolating between two endpoints in their latent represen-tations (e.g., see Fig. 1). However, interpolations are usually computed using only points belonging to the same domain, and most of the state-of-the-art MMUIT methods are in-clined to produce artifacts or unrealistic images when tested using across-domain interpolations. This is shown in Fig. 2 (c), where, using the state-of-the-art StarGAN v2 [10], the inter-domain area in the style space frequently generates ar-tifacts. Another common and related problem is the lack of graduality in both intra and inter domain interpolations, i.e., the generation of abrupt appearance changes corresponding to two close points in the latent space.
In this paper, we address the problem of learning a smoothed and disentangled style space for MMUIT models, which can be used for gradual and realistic image interpo-lations within and across domains. With “disentangled” we mean that the representations of different domains are well separated and clustered (Fig. 2), so that intra-domain in-terpolations correspond to only intra-domain images. With
“smoothed” we mean that the semantics of the style space changes gradually and these changes correspond to small changes in the human perceptual similarity.
The main idea of our proposal is based on the hypothesis that the interpolation problems are related to the exploration of latent space areas which correspond to sparse training data. We again refer to Fig. 2 to illustrate the intuition be-hind this observation. Many MMUIT methods use adver-sarial discriminators to separate the distributions of differ-ent domains [10]. However, a side-effect of this disentan-glement process is that some areas of the latent space do not correspond to real data observed during training. Con-sequently, when interpolating in those areas, the decoding process may lead to generating unrealistic images. We pro-pose to solve this problem jointly using a triplet loss [35, 4] and a simpliﬁed version of the Kullback-Leibler (KL) di-vergence regularization [24]. The former separates the do-mains using a small margin on their relative distance, while the latter encourages the style codes to lie in a compact space. The proposed simpliﬁed KL regularization does not involve the estimation of parametric distributions [24] and it can be easily plugged in Generative Adversarial Networks (GANs) [10, 3]. On the other hand, differently from ad-versarial discrimination, the triplet-loss margin can control the inter-domain distances and help to preserve the domain disentanglement in the compact space, Finally, we also en-courage the content preservation during the translation us-ing a perceptual-distance based loss. Fig. 1 shows some interpolation results obtained using our method. In Sec. 6 we qualitatively and quantitatively evaluate our approach and we show that it can be plugged in different existing
MMUIT methods improving their results. The last contri-bution of this paper concerns the proposal of the Perceptual
Smoothness (PS) metric based on the perceptual similar-ity of the interpolated images, to quantitatively evaluate the style smoothness in MMUIT models.
The contributions of this paper can be summarized
First, we propose a new training strategy as follows. based on three speciﬁc losses which improve the interpo-lation smoothness and the content preservation of different
MMUIT models. Second, we propose a novel metric to
ﬁll-in the gap of previous MMUIT evaluation protocols and quantitatively measure the smoothness of the style space. 2.