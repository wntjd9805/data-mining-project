Abstract
Font generation is a challenging problem especially for some writing systems that consist of a large number of characters and has attracted a lot of attention in recent years. However, existing methods for font generation are often in supervised learning. They require a large num-ber of paired data, which is labor-intensive and expensive to collect. Besides, common image-to-image translation models often deﬁne style as the set of textures and col-ors, which cannot be directly applied to font generation.
To address these problems, we propose novel deformable generative networks for unsupervised font generation (DG-Font). We introduce a feature deformation skip connection (FDSC) which predicts pairs of displacement maps and em-ploys the predicted maps to apply deformable convolution to the low-level feature maps from the content encoder. The outputs of FDSC are fed into a mixer to generate the ﬁ-nal results. Taking advantage of FDSC, the mixer outputs a high-quality character with a complete structure. To fur-ther improve the quality of generated images, we use three deformable convolution layers in the content encoder to learn style-invariant feature representations. Experiments demonstrate that our model generates characters in higher quality than state-of-art methods. The source code is avail-able at https://github.com/ecnuycxie/DG-Font. 1.

Introduction
Every day, people consume a massive amount of texts for information transfer and storage. As the representation of texts, the font is closely related to our daily life. Font gen-eration is critical in many applications, e.g., font library cre-ation, personalized handwriting, historical handwriting imi-tation, and data augmentation for optical character recogni-tion and handwriting identiﬁcation. Traditional font library creating methods heavily rely on expert designers by draw-*Corresponding author: xychen@cee.ecnu.edu.cn
Figure 1. Unsupervised font generation results. The reference calligraphy is a Tang poem written by a calligrapher, and imita-tion result is another famous Tang poem generated from our model which are with rich details, such as stroke tips, joined-up writing, and thickness of strokes. ing each glyph individually, which is especially expensive and labor-intensive for logographic languages such as Chi-nese (more than 60,000 characters), Japanese (more than 50,000 characters), and Korean (11,172 characters).
Recently, the development of convolutional neural net-works enables automatic font generation without human ex-perts. There have been some attempts to explore font gener-ation and achieve promising results. [49, 1, 18] utilize deep neural networks to generate entire sets of letters for certain alphabet languages. Two notable projects, “Rewrite" [40] and “zi2zi" [61], generate logographic language characters by learning a mapping from one style to another with thou-sands of paired characters. After that, EMD [58] and SA-VAE [44] design neural networks to separate the content and style representation, which can extend to generate char-acter of new styles or contents. However, these methods are 5130
in supervised learning and required a large amount of paired training samples.
Some other methods exploit auxiliary annotations (e.g., strokes, radicals) to facilitate high-quality font generation.
For example, [30] utilizes labels for each stroke to generate glyphs by writing trajectories synthesis.
[26] employ the radical decomposition (e.g., radicals or sub-glyphs) of char-acters to achieve font generation for certain logographic lan-guage. DM-Font [7] and its improved version LF-Font [39] propose disentanglement strategies to disentangle complex glyph structures, which help capture local details in rich text design. However, these methods rely on prior knowledge and can only apply to speciﬁc writing systems. Some labels such as the stroke skeleton can be estimated by algorithms, but the estimation error would decrease the generated qual-ity. Also, these methods still require thousands of paired data and annotated labels for training. Recently, there are some attempts [19, 9] for unsupervised font generation. [9] introduces a novel module that transfers the features across sequential DenseNet blocks [23]. [19] proposes a fast skele-ton extraction method to obtain the skeleton of characters, and then utilize the extracted skeleton to facilitate font gen-eration.
For the problem of image-to-image translation, a series of works in unsupervised learning have been proposed by combining adversarial training [32, 54] with consistent con-straints [59, 47, 3]. FUNIT [33] maps an image of a source class to an analogous image of a target class by leverag-ing a few target class images. They extract the style fea-ture of the target class images and employ adaptive instance normalization (AdaIN) [25] to combine the content and the style features. However, these image-to-image translation methods cannot be directly applied to font generation tasks.
Although consistent constraints preserve the structure of a content image, they still encounter some problems for font generation (e.g., blurry, missing some strokes). Also, they usually deﬁne the style as the set of textures and colors.
The AdaIN-based methods transfer style by aligning feature statics, which tends to transform texture and color, which is not suitable to transform local style patterns (e.g., geomet-ric deformation) for the font. Moreover, [9, 19] achieve un-supervised font generation by learning a mapping between two fonts directly, they also ignore the geometric deforma-tion for the font. To learn the mapping across geometry variations, [20] introduces a discriminator with dilated con-volutions as well as a multi-scale perceptual loss that is able to represent error in the underlying shape of objects. [52] disentangles image space into a Cartesian product of the ap-pearance and the geometry latent spaces.
Compelled by the above observations, we propose a novel deformable generative model for unsupervised font generation (DG-Font). The proposed method is designed to deform and transform the character of one font to another by leveraging the provided images of the target font. The proposed DG-Font separates style and content respectively and then mix two domain representations to generate target characters. We introduce a feature deformation skip con-nection (FDSC) which predicts pairs of displacement maps and employs the predicted maps to apply deformable con-volution to the low-level feature maps from the content en-coder. The outputs of FDSC are fed into a mixer to generate the ﬁnal results. To distinguish different styles, we train our model with a multi-task discriminator, which ensures that each style can be discriminated independently. In addition, another two reconstruction losses are adopted to constrain the domain-invariant characteristics between generated im-ages and content images.
The feature deformation skip connection (FDSC) mod-ule is used to transform the low-level feature of content im-ages, which preserves the pattern of character (e.g., strokes and radicals). Different from the image-to-image transla-tion problem that deﬁnes style as a set of texture and color, the style of font is basically deﬁned as geometric transfor-mation, stroke thickness, tips, and joined-up writing pat-tern. For two fonts with the same content, they usually have correspondence for each stroke. Taking advantage of the spatial relationship of fonts, the feature deformation skip connection (FDSC) is used to conduct spatial deformation, which effectively ensures the generated image to have com-plete structures.
Extensive experiments demonstrate that our model achieves comparable results to the state-of-the-art font gen-eration methods. Besides, results show that our model is able to extend to generate unseen style character. 2.