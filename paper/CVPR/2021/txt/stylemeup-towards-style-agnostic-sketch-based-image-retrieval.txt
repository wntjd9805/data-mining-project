Abstract
Existing Model
Our Model
Sketch-based image retrieval (SBIR) is a cross-modal matching problem which is typically solved by learning a joint embedding space where the semantic content shared between photo and sketch modalities are preserved. How-ever, a fundamental challenge in SBIR has been largely ig-nored so far, that is, sketches are drawn by humans and considerable style variations exist amongst different users.
An effective SBIR model needs to explicitly account for this style diversity, crucially, to generalise to unseen user styles. To this end, a novel style-agnostic SBIR model is proposed. Different from existing models, a cross-modal variational autoencoder (VAE) is employed to explicitly dis-entangle each sketch into a semantic content part shared with the corresponding photo, and a style part unique to the sketcher. Importantly, to make our model dynamically adaptable to any unseen user styles, we propose to meta-train our cross-modal VAE by adding two style-adaptive components: a set of feature transformation layers to its en-coder and a regulariser to the disentangled semantic con-tent latent code. With this meta-learning framework, our model can not only disentangle the cross-modal shared se-mantic content for SBIR, but can adapt the disentanglement to any unseen user style as well, making the SBIR model truly style-agnostic. Extensive experiments show that our style-agnostic model yields state-of-the-art performance for both category-level and instance-level SBIR. 1.

Introduction
Sketch as an input modality has been proven to be a wor-thy complement to text for photo image retrieval [8, 10, 4].
Its precision in visual description is particularly useful for
ﬁne-grained retrieval, where the goal is to ﬁnd a speciﬁc ob-ject instance rather than category [44, 49, 3]. Research has
ﬂourished in recent years, where the main focus has been on addressing the sketch-photo domain gap [33, 16, 44] and data scarcity [5, 3, 10, 35, 12]. Thanks to these combined efforts, reported performances have already shown promise for practical adaptation.
User 1
User 2
User 3
Style 1
Style 2
Style 3
Figure 1. Owing to subjective interpretation, different users sketch the same object instance (a shoe here) very differently. Without considering this style diversity, an existing SBIR model yields completely different results for different sketches. With our style-agnostic model, the same intended object is retrieved.
However, there is an important issue that has largely been ignored so far and has impeded the effectiveness of existing
SBIR models – sketches are drawn by humans and there exists considerable style variations amongst users (Fig. 1).
This is a result of subjective interpretation and different drawing skills of different users. Consequently, even with the same object instance as reference, sketches of different users can look drastically different as shown in the example in Fig. 1. Existing SBIR models [30, 8, 47, 35, 44] focus primarily on bridging the gap between the photo and sketch modalities. This is typically achieved by learning a joint embedding space where only the common semantic con-tent part of a matching photo-sketch pair are preserved for matching. However, the large style variations of different users mean that the shared common semantic content can also vary (e.g., Fig. 1 shows that different users may choose to depict different characteristics of the same shoe). Cru-cially, it can vary in an unpredictable way – a commercial
SBIR model will be used mostly by users whose sketches have never been used for model training. These models are thus poorly equipped to cope with this style diversity and unable to generalise to new user styles.
In this paper, a novel style-agnostic SBIR framework is 8504
proposed which explicitly accounts for the style diversity and importantly can adapt dynamically to any unseen user styles without any model retraining. Different from exist-ing SBIR models which focus solely on the shared seman-tic content between the photo and sketch modality and dis-card the modal-speciﬁc parts, we argue that in order to ef-fectively deal with the style variations unique to the sketch modality, a disentanglement model is needed. With such a model, the user style can be modelled explicitly, making way for better generalisation.
The core of our style-agnostic SBIR framework is thus a disentanglement model, that takes a sketch or photo im-age as input and decomposes its content into a cross-modal shared semantic part to be used for retrieval, and a modal-speciﬁc part – in case of sketch, it corresponds to the user’s drawing style. Disentangling sketching styles is however a challenging task. Existing style disentanglement methods usually cater to problems where the style information carry less variance (e.g., schools of art, building styles, etc) and hence is comparatively easier to separate [24]. For sketches however, we are faced with much larger variability where each user has a unique style that can manifest itself in dif-ferent ways for different object instances. The disentangle-ment should thus be able to dynamically adapt to new user styles and new object categories for better generalisation.
To this end, we propose a novel disentanglement model with meta-learning, that generalises to unseen user styles.
Concretely, we employ a cross-modal translation vari-ational autoencoder (VAE) framework [23] to project a sketch/photo into its modal-invariant semantic part, and modal-speciﬁc part. The VAE is used for both sketch recon-struction and translation as well as sketch-to-photo trans-lation to exploit the shared semantic content across both modalities and styles. To make the disentanglement dy-namically adaptable and generalisable, a popular gradient-based meta-learning model namely model-agnostic meta-learning (MAML) [14] is adopted. Designed for few-shot learning, the original MAML cannot be directly ap-plied. We thus introduce two new components as shown in Fig. 2: (i) A set of feature transformation layers sit-ting between the VAE encoder layers for the encoder adap-tation, and (ii) a regulariser designed to adapt the disen-tangled modal-invariant semantic content part of the latent code produced by the encoder. Both component’s parame-ters are meta-learned using MAML for fast adaptation to new style/categories sampled during episodic training of
MAML. Once trained, these two components are responsi-ble for adaptation to new/unseen user styles and object cat-egories/instances, therefore achieving style-agnostic SBIR.
Our contributions are as follows: (a) For the ﬁrst time, we propose the concept of style-agnostic SBIR to deal with a largely neglected user style diversity issue in SBIR. (b) We introduce a novel style-agnostic SBIR framework based on disentangling a photo/sketch image into a modal-invariant semantic content part suitable for SBIR and a model-speciﬁc part that needs to be explicitly modelled in order to minimise its detrimental effects on retrieval. (c)
To make the disentanglement generalisable to unseen user styles and object categories/instances, feature transforma-tion layers and latent modal-invariant code regulariser are introduced to a VAE, both of which are meta-learned us-ing a MAML framework for style/category/instance adap-tation. (d) Extensive experiments show that state-of-the-art performances can be achieved as a direct result of the style-agnostic design. 2.