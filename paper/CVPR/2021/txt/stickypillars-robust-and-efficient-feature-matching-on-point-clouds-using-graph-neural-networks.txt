Abstract
Robust point cloud registration in real-time is an impor-tant prerequisite for many mapping and localization algo-rithms. Traditional methods like ICP tend to fail without good initialization, insufﬁcient overlap or in the presence of dynamic objects. Modern deep learning based registration approaches present much better results, but suffer from a heavy runtime. We overcome these drawbacks by introduc-ing StickyPillars, a fast, accurate and extremely robust deep middle-end 3D feature matching method on point clouds.
It uses graph neural networks and performs context aggre-gation on sparse 3D key-points with the aid of transformer based multi-head self and cross-attention. The network out-put is used as the cost for an optimal transport problem whose solution yields the ﬁnal matching probabilities. The system does not rely on hand crafted feature descriptors or heuristic matching strategies. We present state-of-art art ac-curacy results on the registration problem demonstrated on the KITTI dataset while being four times faster then leading deep methods. Furthermore, we integrate our matching sys-tem into a LiDAR odometry pipeline yielding most accurate results on the KITTI odometry dataset. Finally, we demon-strate robustness on KITTI odometry. Our method remains stable in accuracy where state-of-the-art procedures fail on frame drops and higher speeds. 1.

Introduction
Point cloud registration, the process of ﬁnding a spatial transformation aligning two point clouds, is an essential com-puter vision problem and a precondition for a wide range of tasks in the domain of real-time scene understanding or applied robotics, such as odometry, mapping, re-localization or SLAM. New generations of 3D sensors, like depth cam-eras or LiDARs (light detection and ranging), as well as multi-sensor setups provide substantially more ﬁne-grained and reliable data enabling dense range perception at a large
ﬁeld of view. These sensors substantially increase the expec-tations on point cloud registration and an exact matching of feature correspondences.
State-of-the-art 3D point cloud registration employs lo-cally describable features in a global optimization step
[43, 57, 21]. Most methods do not rely on modern machine learning algorithms, although they are part of the best per-forming approaches on odometry challenges like KITTI [14].
In contrast, recent research for point cloud processing, e.g. classiﬁcation and segmentation [34, 35, 18, 60], relies on neural networks and promises substantial improvements for registration, mapping and odometry [12, 20]. The limitation of all none neural network-based odometry and mapping methods is that they perform odometry estimation using a global rigid body operation. Those approaches assume many static objects within the environment and proper viewpoints.
However, real world measurements are generally unstable under challenging situations, e.g. many dynamic objects or widely varying viewpoints and small overlapping areas.
Hence, the mapping quality itself is suffering from artifacts (blurring) and is often not evaluated qualitatively. To over-come these limitations, we propose StickyPillars a novel registration approach for point clouds utilizing graph neural networks based on pillar shaped point descriptors. Inspired by [9, 52], our approach computes feature correspondence rather than end-to-end odometry estimations. By reducing the registration problem to an inter-cloud correspondence search on a sparse subset of selected key-points and utiliz-ing a ground truth match matrix in the training process, we are able to predict poses with a very low computation time while being robust against the inﬂuence of dynamic objects.
We demonstrate StickyPillars’s robust real-time registration capabilities (see Fig. 1) and its conﬁdence under challeng-313
ing conditions, such as dynamic environments, challenging viewpoints and small overlapping areas. We evaluate our technique on the KITTI odometry benchmark [14] and signif-icantly outperform state-of-the-art frame to frame matching approaches e.g. the one used in LOAM[57]. Those improve-ments enable more precise odometry estimation for applied robotics. 2.