Abstract
Video understanding calls for a model to learn the char-acteristic interplay between static scene content and its dy-namics: Given an image, the model must be able to pre-dict a future progression of the portrayed scene and, con-versely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bi-jective mapping between the video domain and the static content as well as residual information. In contrast to com-mon stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is nat-urally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus lay-ing the basis for controlled video synthesis. Experiments on diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results. Our project page is available at https://bit.ly/3dg90fV . 1.

Introduction
Anticipating and predicting what happens next are key features of human intelligence that allow us to understand and deal with the ever-changing environment that governs our everyday life [8]. Consequently, the ability to foresee and hallucinate the future progression of a scene is a cor-nerstone of artiﬁcial visual understanding with applications including autonomous driving [48, 49, 28], medical treat-ment [5, 18, 6], and robotic planning [20, 24, 10].
Predicting and synthesizing plausible future progres-sions from a given image requires a deep understanding of how scenes and objects within video are depicted, interplay with each other, and evolve over time. While an image pro-*Indicates equal supervision.
Figure 1. Our approach establishes a bijective mapping between the image and the video domain by introducing a residual repre-sentation ν describing the latent scene dynamics. This allows us not only to synthesize diverse videos but also to extend our ap-proach to gain control over the video synthesis task. vides information about the observed scene content, such as object appearance and shape, the challenge is to under-stand the missing information constituting potential futures, such as the scene dynamics setting the scene in motion. Due to the ambiguity and complexity of capturing this informa-tion, many works [42, 25, 11, 74] directly focus on predict-ing likely video continuations, often resorting to simplify-ing assumptions (e.g., dynamics modelled by optical ﬂow
[21, 59, 76]) and side information (e.g., semantic keypoints
[56, 72, 46, 22]). However, truly understanding the synthe-sis problem not only requires to infer such image continua-tions but, conversely, also demands when observing a video sequence to describe and represent the instantiated scene dynamics animating its initial frame. 3742
Consequently, the image-to-video synthesis task should be modelled as a translation between the image and video domains, ideally by an invertible mapping between them.
Since the content information describing an image only ac-counts for a small fraction of the video information, in par-ticular missing the temporal dimension, learning an invert-ible mapping requires a dedicated residual representation that captures all missing information. Once learned, given an initial image and an instantiation of the latent residual, we can combine them to synthesize the corresponding fu-ture video sequence.
In this paper, we frame image-to-video synthesis as an invertible domain transfer problem and implement it using a conditional invertible neural network (cINN). To account for the domain gap between images and videos, we intro-duce a dedicated probabilistic residual representation. The bijective nature of our mapping ensures that only informa-tion complementary to that in the initial image is captured.
Using a probabilistic formulation, the residual representa-tion allows to sample and thus synthesize novel future pro-gressions in video with the same start frame. To reduce the complexity of the learning task, we train a separate con-ditional variational encoder-decoder architecture to com-pute a compact, information preserving representation for the video domain. Moreover, our speciﬁc framing of learn-ing the residual representation allows to easily incorporat-ing extra conditioning information to exercise control over the image-to-video synthesis process. Our contributions can be summarized as follows:
• We frame image-to-video synthesis as an invertible do-main transfer problem and learn a dedicated residual representation to capture the domain gap.
• Our framework naturally extends to incorporate ex-plicit conditioning factors for exercising control over the synthesis process.
• Extensive evaluations on four video datasets, ranging from structured human motion synthesis to subtle dy-namic textures, show strong results demonstrating the effectiveness of our approach. 2.