Abstract
Self-supervised learning has gained prominence due to its efﬁcacy at learning powerful representations from un-labelled data that achieve excellent performance on many challenging downstream tasks. However, supervision-free pre-text tasks are challenging to design and usually modal-ity speciﬁc. Although there is a rich literature of self-supervised methods for either spatial (such as images) or temporal data (sound or text) modalities, a common pre-text task that beneﬁts both modalities is largely missing. In this paper, we are interested in deﬁning a self-supervised pre-text task for sketches and handwriting data. This data is uniquely characterised by its existence in dual modalities of rasterized images and vector coordinate sequences. We address and exploit this dual representation by proposing two novel cross-modal translation pre-text tasks for self-supervised feature learning: Vectorization and Rasteriza-tion. Vectorization learns to map image space to vector coordinates and rasterization maps vector coordinates to image space. We show that our learned encoder modules beneﬁt both raster-based and vector-based downstream ap-proaches to analysing hand-drawn data. Empirical evi-dence shows that our novel pre-text tasks surpass existing single and multi-modal self-supervision methods. 1.

Introduction
Deep learning architectures [65, 24] have become the de-facto choice for most computer vision applications. How-ever, their success heavily depends on access to large scale labelled datasets [51] that are both costly and time-consuming to collect. In order to alleviate the data anno-tation bottleneck, many unsupervised methods [43, 31, 15, 10, 23] propose to pre-train a good feature representation from large scale unlabelled data. A common approach is to deﬁne a pre-text task whose labels can be obtained free-of-cost, e.g. colorization [66], jigsaw solving [43], image rotation prediction [18], etc. The motivation is that a net-Vectorization
Sketch Image
Sketch Vector
Rasterization
Sketch Vector
Sketch Image
Figure 1. Schematic of our proposed self-supervised method for sketches. Vectorization drives representation learning for sketch images; rasterization is the pre-text task for sketch vectors. work trained to solve such a pre-text task should encode high-level semantic understanding of the data that can be used to solve other downstream tasks like classiﬁcation, re-trieval, etc. Apart from traditional object classiﬁcation, de-tection or semantic segmentation, self-supervision has been extended to sub-domains like human pose-estimation [32], co-part segmentation [27], and depth estimation [19].
In this paper, we propose a self-supervised method for a class of visual data that is distinctively different than photos: sketches [65, 55] and handwriting [50] images. Although sketch and handwriting have been studied as two separate topics by different communities, there exists an underlying similarity in how they are captured and represented. More speciﬁcally, they are both recorded as the user’s pen tip follows a trajectory on the canvas, and rendered as sparse black and white lines in image space. Both are abstract, in the sense that the same object or grapheme can be drawn in many possible ways [57, 22]; while sketch in particular poses the challenge of variable levels of detail [52] depicted.
Both sketch [62] and handwriting [20] can be represented in rasterized pixel space, or as a temporal point sequence
[22]. While each modality has its own beneﬁts, we pro-pose a novel self-supervised task that takes advantage of this dual image/vector space representation. In particular, 5672
we use cross modal translation between image and vector space as a self-supervised task to improve downstream per-formance using either representation (Figure 1).
Most existing self-supervised methods are deﬁned for single data modalities. Existing methods for images [11, 23, 18] or videos [34] are designed for pixel perfect ren-derings of scenes or objects, and as such are not suited for sparse black and white handwritten images. For ex-ample colorization [66] and super-resolution [35] pre-texts, and augmentation strategies such as color distortion, bright-ness, and hue adjustment used by state of the art contrastive methods [23, 11, 21] – are not directly applicable to line drawings. For vector sequences, self-supervised methods typically addressed at speech such as Contrastive Predic-tive Coding (CPC) [25] could be used off-the-shelf but do not explicitly handle the stroke-by-stroke nature of hand-writing. Conversely, BERT-like pre-training strategies have had some success with vector-modality sketches [37] but cannot be applied to image-modality sketches. In contrast, our framework can be used to learn a powerful representa-tion for both image and vector domain sketch analysis tasks.
Although a multi-view extension of contrastive learning for self-supervision [59] has been attempted, we show empiri-cally that our cross-view rasterization/ vectorization synthe-sis approach provides a superior self-supervision strategy.
In summary, we design a novel self-supervised frame-work that exploits the dual raster/vector sequence nature of sketches and handwritten data through cross-modal transla-tion (Figure 1). Our cross-modal framework is simple and easy to implement from off-the-shelf components. Nev-ertheless it learns powerful representations for both raster and vector represented downstream sketch and handwrit-ing analysis tasks. Empirically, our framework surpasses state of the art self-supervised methods and approaches and sometimes surpasses the fully supervised alternatives. 2.