Abstract
Is critical input information encoded in speciﬁc sparse pathways within the neural network? In this work, we dis-cuss the problem of identifying these critical pathways and subsequently leverage them for interpreting the network’s response to an input. The pruning objective — selecting the smallest group of neurons for which the response remains equivalent to the original network — has been previously proposed for identifying critical pathways. We demonstrate that sparse pathways derived from pruning do not neces-sarily encode critical input information. To ensure sparse pathways include critical fragments of the encoded input in-formation, we propose pathway selection via neurons’ con-tribution to the response. We proceed to explain how critical pathways can reveal critical input features. We prove that pathways selected via neuron contribution are locally linear (in an ℓ2-ball), a property that we use for proposing a fea-ture attribution method: “pathway gradient”. We validate our interpretation method using mainstream evaluation ex-periments. The validation of pathway gradient interpreta-tion method further conﬁrms that selected pathways using neuron contributions correspond to critical input features.
The code1 2 is publicly available. 1.

Introduction
Understanding the rationale behind the response of a neural network is of considerable signiﬁcance. Such trans-parency is required for adoption and safe deployment in
Interpreting the response also mission-critical domains. helps in debugging and designing neural networks, and quenches the intellectual curiosity over how neural net-works function [15, 47, 24, 63, 10, 50].
What insights can we acquire about the underpinnings 1https://github.com/CAMP-eXplain-AI/PathwayGrad 2https://github.com/CAMP-eXplain-AI/RoarTorch of a neural network’s response by putting the networks un-der the microscope and analyzing neurons and pathways?
By ”pathway”, we refer to a union of paths (equivalently a sub-network) that connect the input to the output. Discov-ering to what patterns neurons correspond — also known as neural decoding in computational neuroscience [6] — has revealed human interpretable concepts encoded in neurons of artiﬁcial neural networks, e.g. curve and circle detectors
[41, 65]. Analyzing the neural pathways has recently re-vealed human interpretable connections between concepts encoded within each neuron on the pathway, e.g. circles being assembled from curves [40].
In this work we dis-cuss pathways responsible for the network’s response given a speciﬁc input, but how can we identify these pathways?
Deep rectiﬁed neural networks encode the input informa-tion using a sparse set of active neurons [12], and their infer-ence can be deemed as a pursuit algorithm for sparse coding
[43, 59]. Such sparse coding of information is akin to how biological neurons encode information in the brain [42, 16].
Yu et al. [64] reported that the pathways of active neurons in artiﬁcial neural networks overlap signiﬁcantly for inputs of a given class. Recently, [63] proposed using the pruning objective and knowledge distillation [19] to show that sig-niﬁcantly higher levels of sparsity (∼87% for VGG-16 [54] on ImageNet [7]) can be achieved while keeping the predic-tion intact. These highly sparse pathways are reported as the critical paths and are shown to be different for inputs of different classes and adversarial inputs [45, 63, 64].
We ﬁrst investigate, whether these highly sparse path-ways derived from the pruning objective indeed encode crit-ical input features. We show that the pruning objective has solutions that are not critical pathways, even though they have the same response as the original network. To illus-trate how the pruning objective can result in such pathways, we construct a pathological greedy pruning algorithm that by design searches for irrelevant pathways while satisfying the pruning objective. Furthermore, we analyze the path-ways selected by distillation guided routing [63] and ob-13528
serve a similar phenomenon. We also use feature visual-ization [41, 30] to decode and semantically analyze the pathways.
If these pathways do not encode critical input features, how can we ﬁnd such pathways? Numerous works have studied the importance of individual neurons for the neural response, and how each neuron encodes information speciﬁc to one or a subset of classes [69, 4, 36, 41].
It is therefore intuitive that selected sparse pathways should encompass important/critical neurons for the correspond-ing response. We thus investigate selecting pathways based on neuron contributions as opposed to the pruning objec-tive.
In order to compute the importance of neurons, we use notions of marginal contribution and the Shapley value
[51, 29, 2, 60, 70]. The ﬁrst section of the work is devoted to the discussion of critical pathways.
We proceed to answer how critical pathways can help us interpret the response of the network. We prove that in rectiﬁed neural networks, pathways selected by neuron contributions are locally linear. We leverage this property and propose an input feature attribution methodology which we refer to as ”pathway gradient”. We evaluate our attribution methodology with input degradation [48], sanity checks [1], and Remove-and-Retrain (ROAR) [20] on Cifar10 [25], Bridsnap [5], and ImageNet [7] datasets.
By validating our attribution methodology, we also validate that selected pathways using neuron contributions indeed correspond to critical input features. In summary, the main contributions of the paper are:
• We show that the pruning objective does not neces-sarily extract critical pathways. We illustrate how the pruning can fail by proposing a pathological greedy algorithm that by design searches for irrelevant pathways.
Subsequently, we propose selecting pathways based on neuron contributions instead.
• We prove that critical pathways selected by neuron con-tributions are locally linear (ℓ2-ball) in rectiﬁed networks.
Using local linearity, we propose a feature attribution approach, ”pathway gradient”, that reveals input features associated with features encoded in the critical pathways.
• We empirically show that computing contribution (ap-proximated Shapley value) of neurons rather than input pixels, improves input feature attribution. 2.