Abstract (a)
Loss-based trajectory (b)
Loss & metric-based trajectory
We study the problem of directly optimizing arbitrary non-differentiable task evaluation metrics such as misclas-siﬁcation rate and recall. Our method, named MetricOpt, operates in a black-box setting where the computational de-tails of the target metric are unknown. We achieve this by learning a differentiable value function, which maps com-pact task-speciﬁc model parameters to metric observations.
The learned value function is easily pluggable into existing optimizers like SGD and Adam, and is effective for rapidly
ﬁnetuning a pre-trained model. This leads to consistent im-provements since the value function provides effective met-ric supervision during ﬁnetuning, and helps to correct the potential bias of loss-only supervision. MetricOpt achieves state-of-the-art performance on a variety of metrics for (im-age) classiﬁcation, image retrieval and object detection.
Solid beneﬁts are found over competing methods, which of-ten involve complex loss design or adaptation. MetricOpt also generalizes well to new tasks and model architectures. 1.

Introduction
In real-world vision applications, machine learning mod-els are usually evaluated on a variety of complex evalua-tion metrics. For example, one may evaluate a classiﬁcation model using Mis-Classiﬁcation Rate (MCR), and evaluate a ranking model using recall. Many of these metrics are non-continuous, non-differentiable, or non-decomposable, which poses challenges for direct metric optimization due to the difﬁculty of obtaining an informative gradient (e.g., it is zero almost everywhere for MCR). In other scenarios, the computational details may be unknown for a black box met-ric function. Hence its true gradient is simply inaccessible, which further increases the challenge of metric optimiza-tion.
As a common practice, people usually rely on a sur-rogate differentiable loss, which can be easily optimized with Stochastic Gradient Descent (SGD). One example is the widely used cross-entropy loss [15] for classiﬁcation problems. While cross-entropy loss can be regarded as a
Metric space
Metric space
High
Low error  metric error  metric time (iteration) time (iteration)
Metric observation
Interpolated metric contour
Figure 1. Motivation (a) Optimization with loss-only supervision may travel through several “bumps” in the metric space, and tends to converge to a suboptimal solution in terms of evaluation metric.
We propose to collect a sparse set of (black-box) metrics along the optimization trajectory. Then we use the temporally interpo-lated metrics to meta-learn a differentiable value function, which can provide effective metric supervision to augment and improve any loss optimization process. (b) This way, we ﬁnd continuous decrease in the error metric while keeping the loss from growing. smooth relaxation of MCR, this loss is not a good proxy for other metrics like recall. When the loss does not match the target metric, inferior performance can be obtained [23].
State-of-the-art approaches follow two main paradigms to address this loss-metric mismatch issue. One is to intro-duce better metric-aligned surrogate losses, e.g., AUCPR loss [9]. These hand-designed losses not only require te-dious manual effort and white-box metric formulation, but also tend to be speciﬁc to a given metric. Another paradigm is to learn adaptive losses in a relaxed or interpolated surro-gate space [16, 23, 24, 28], which is inherently sub-optimal when compared to optimization in the original space.
In this paper, we propose to directly adapt the gradient-based optimization process to optimize black-box metrics, without knowing any details about the metric function. To do so, we ﬁrst meta-learn a differentiable value function to model the metric observations along optimization trajecto-ries. We focus on the model ﬁnetuning setup which can pro-vide meaningful metrics to learn our value function. Once learned, the value function can provide useful metric super-174
vision, including approximate metric gradients, to augment surrogate loss gradients. As a result, we can use the value function to ﬁnetune a new model that has been pre-trained using any given surrogate loss. Fig. 1 illustrates the high level idea. Intuitively, the value function is trained to pro-duce meaningful adjustments to the optimization trajectory driven by loss only, leading to corrective directions on the metric landscape.
In practice, we parameterize the value function by a lightweight network for fast training and inference speeds.
The input are a small set of adapter parameters that modu-late a pre-trained model. Such a compact parameterization has been shown to sufﬁce for task specialization [49], and removes our need to ﬁnetune the entire network. We meta-learn our value function using an enhanced ordinal regres-sion objective, which is uncertainty-aware to avoid overcon-ﬁdent metric estimates. We then show that it is straightfor-ward to apply our value function to off-the-shelf optimizers like SGD and Adam [25], and also to a learned optimizer.
The resulting method MetricOpt is shown to consis-tently improve different evaluation metrics across the tasks of (image) classiﬁcation, image retrieval and object detec-tion. MetricOpt not only outperforms prior methods based on strong surrogate losses (either hand-designed or adap-tively learned), but also shares speed advantages as a fast
ﬁnetuning method, often with no more than thousands of tuning steps. Furthermore, MetricOpt generalizes well to new tasks, e.g., from CIFAR-10 [27] to ImageNet [7] clas-siﬁcation. We summarize our contributions as follows:
• We introduce a differentiable value function to model a black-box evaluation metric.
• We show the value function is easily pluggable to existing optimizers, resulting in a fast ﬁnetuning approach.
• We show MetricOpt consistently improves over differ-ent surrogate losses without tedious loss engineering, achieves state-of-the-art performance for various tasks and metrics, and generalizes to out-of-distribution tasks and model architectures. 2.