Abstract
Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be con-trollable. While several recent works investigate how to dis-entangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional genera-tive neural feature ﬁelds allows us to disentangle one or multiple objects from the background as well as individual objects’ shapes and appearances while learning from un-structured and unposed image collections without any ad-ditional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and al-lows for translating and rotating them in the scene as well as changing the camera pose. 1.

Introduction
The ability to generate and manipulate photorealistic im-age content is a long-standing goal of computer vision and graphics. Modern computer graphics techniques achieve impressive results and are industry standard in gaming and movie productions. However, they are very hardware ex-pensive and require substantial human labor for 3D content creation and arrangement.
In recent years, the computer vision community has made great strides towards highly-realistic image gener-ation.
In particular, Generative Adversarial Networks (GANs) [24] emerged as a powerful class of generative models. They are able to synthesize photorealistic images at resolutions of 10242 pixels and beyond [6, 14, 15, 39, 40].
Figure 1: Overview. We represent scenes as compositional generative neural feature ﬁelds. For a randomly sampled camera, we volume render a feature image of the scene based on individual feature ﬁelds. A 2D neural rendering network converts the feature image into an RGB image.
While training only on raw image collections, at test time we are able to control the image formation process wrt. camera pose, object poses, as well as the objects’ shapes and appearances. Further, our model generalizes beyond the training data, e.g. we can synthesize scenes with more objects than were present in the training images. Note that for clarity we visualize volumes in color instead of features.
Despite these successes, synthesizing realistic 2D im-ages is not the only aspect required in applications of gen-erative models. The generation process should also be con-trollable in a simple and consistent manner. To this end, many works [9, 25, 39, 43, 44, 48, 54, 71, 74, 97, 98] investi-gate how disentangled representations can be learned from data without explicit supervision. Deﬁnitions of disentan-glement vary [5, 53], but commonly refer to being able to control an attribute of interest, e.g. object shape, size, or pose, without changing other attributes. Most approaches, however, do not consider the compositional nature of scenes and operate in the 2D domain, ignoring that our world is three-dimensional. This often leads to entangled represen-tations (Fig. 2) and control mechanisms are not built-in, but need to be discovered in the latent space a posteriori. These properties, however, are crucial for successful applications, e.g. a movie production where complex object trajectories 11453
(a) Translation of Left Object (2D-based Method [71]) (b) Translation of Left Object (Ours) (d) Add Objects (Ours) (c) Circular Translation (Ours)
Figure 2: Controllable Image Generation. While most generative models operate in 2D, we incorporate a compo-sitional 3D scene representation into the generative model.
This leads to more consistent image synthesis results, e.g. note how, in contrast to our method, translating one object might change the other when operating in 2D (Fig. 2a and 2b). It further allows us to perform complex operations like circular translations (Fig. 2c) or adding more objects at test time (Fig. 2d). Both methods are trained unsupervised on raw unposed image collections of two-object scenes. need to be generated in a consistent manner.
Several recent works therefore investigate how to incor-porate 3D representations, such as voxels [32,63,64], prim-itives [46], or radiance ﬁelds [77], directly into generative models. While these methods allow for impressive results with built-in control, they are mostly restricted to single-object scenes and results are less consistent for higher reso-lutions and more complex and realistic imagery (e.g. scenes with objects not in the center or cluttered backgrounds).
Contribution:
In this work, we introduce GIRAFFE, a novel method for generating scenes in a controllable and photorealistic manner while training from raw unstructured image collections. Our key insight is twofold: First, incor-porating a compositional 3D scene representation directly into the generative model leads to more controllable im-age synthesis. Second, combining this explicit 3D repre-sentation with a neural rendering pipeline results in faster inference and more realistic images. To this end, we repre-sent scenes as compositional generative neural feature ﬁelds (Fig. 1). We volume render the scene to a feature image of relatively low resolution to save time and computation.
A neural renderer processes these feature images and out-puts the ﬁnal renderings. This way, our approach achieves high-quality images and scales to real-world scenes. We
ﬁnd that our method allows for controllable image synthesis of single-object as well as multi-object scenes when trained on raw unstructured image collections. Code and data is available at https://github.com/autonomousvision/giraffe. 2.