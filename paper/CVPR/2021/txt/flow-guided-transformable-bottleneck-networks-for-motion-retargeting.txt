Abstract
Human motion retargeting aims to transfer the motion of one person in a “driving” video or set of images to another person. Existing efforts leverage a long training video from each target person to train a subject-speciﬁc motion transfer model. However, the scalability of such methods is limited, as each model can only generate videos for the given target subject, and such training videos are labor-intensive to acquire and process. Few-shot motion transfer techniques, which only require one or a few im-ages from a target, have recently drawn considerable at-tention. Methods addressing this task generally use either 2D or explicit 3D representations to transfer motion, and in doing so, sacriﬁce either accurate geometric modeling or the ﬂexibility of an end-to-end learned representation.
Inspired by the Transformable Bottleneck Network, which renders novel views and manipulations of rigid objects, we propose an approach based on an implicit volumetric rep-resentation of the image content, which can then be spa-tially manipulated using volumetric ﬂow ﬁelds. We address the challenging question of how to aggregate information across different body poses, learning ﬂow ﬁelds that allow for combining content from the appropriate regions of in-put images of highly non-rigid human subjects performing complex motions into a single implicit volumetric represen-tation. This allows us to learn our 3D representation solely from videos of moving people. Armed with both 3D object understanding and end-to-end learned rendering, this cat-egorically novel representation delivers state-of-the-art im-age generation quality, as shown by our quantitative and qualitative evaluations. 1.

Introduction
Retargeting human body motion — transferring motion from a “driving” image or video of one subject (the source) to another subject (the target), using one or more refer-ence images of the target subject in an arbitrary pose —
*Work done while at Snap Inc. has received a great deal of attention in recent years, due to numerous practical and entertaining applications in content generation [62, 8]. Such applications include transferring sophisticated athletic techniques or dancing performances to untrained celebrities for special effects for cinema and television; creating amusing performances for one’s friends or acquaintances for sheer entertainment; and creating plau-sible motion sequences from photos or videos depicting fa-mous and important political ﬁgures (including historical
ﬁgures who may no longer be alive to perform such actions) for the creation of plausible full-body “deepfake” videos.
However, retaining the target subject’s identity while ren-dering them in novel, unseen poses is highly challenging, and the state-of-the-art is still far from plausible.
Many approaches to this task learn to render a speciﬁc person [1, 7, 8, 13, 20, 26, 49, 64, 63, 66, 67, 73] con-ditioned on the desired pose. This requires a large num-ber of training frames of that person, and incurs substan-tial training time that must be repeated per each new sub-ject. By contrast, in the few-shot setting, addressed in this work, only a few reference images of the target are avail-able, and video generation from those images should be fast (i.e., requiring no subject-speciﬁc training). To over-come the lack of data for a given subject, many other tech-niques [4, 29, 32, 34, 40, 43, 44, 46] leverage existing hu-man body models [2, 22, 36] to construct an approximate representation of the subject that can then be manipulated and rendered. While the 3D nature of these representa-tions often leads to improved performance over their purely 2D counterparts [3, 38, 39, 51, 55], their explicit nature, which faces the limitations of capturing salient details with standard human body models, also leads to reduced mod-eling power and therefore ﬁdelity. Large variations in the clothing (e.g., dresses or jackets that do not conform to the body shape), body type, or hair of the source and target sub-jects, for example, cannot easily be represented with stan-dard models that only represent the body itself.
In this work we attain more ﬂexible and expressive mod-eling power by exploiting a representation that allows for 3D modeling and manipulation, and yet is fully implicit, i.e. it can be fully learned, even though we use no explicit 10795
ground-truth 3D information such as meshes or voxel grids as supervision. Recently, just such a representation, the
Transformable Bottleneck Network (TBN) [45], has been shown to produce excellent results on novel view synthesis of rigid objects. In that work, image content is encoded into an implicit volumetric representation (the “bottleneck”), in which each of the encoded features in this volume corre-spond to the local structure and appearance of the corre-sponding spatial region in the volume depicted in the im-age. However, while it requires no 3D supervision, it is trained using multi-view datasets of rigid objects depicted from multiple viewpoints to produce implicit volumes that can then be rigidly transformed to produce novel views of the depicted content corresponding to changes in viewpoint.
We build upon this approach to address the challenge of performing motion retargeting for non-rigid humans (for which multiple images of a given subject may be available,
In doing so, we ad-but in dramatically different poses). dress several challenges: how to aggregate volumetric fea-tures from images with changes in camera and body pose, and how to learn this aggregation from videos without ex-plicit 3D or camera pose supervision. With such an im-plicit representation, to synthesize a novel pose, we achieve non-rigid implicit volume aggregation and manipulation by learning a 3D ﬂow to resample the 3D body model from input images captured with the subject performing various poses or under different viewpoints. To allow for expressing large-scale motion while retaining ﬁne-grained details in the synthesized images, we propose a multi-resolution scheme in which image content is encoded, transformed and aggre-gated into bottlenecks of different resolution scales.
As we focus on transferring motion between human sub-jects, our network pipeline is designed and trained specif-ically to extract and manipulate the foreground of the en-coded images, with a separate network for extracting and compositing the background with the synthesized result.
Our training scheme employs techniques and loss functions precisely designed for the challenging task of producing plausible motion retargeting without 3D supervision or the use of explicit 3D models, e.g. making use of specialized training techniques to teach the network to synthesize plau-sible results when no ground-truth images corresponding to the applied spatial manipulation is available. We thus avoid the limitations of explicit body representations [32, 34], which may lead to unrealistic results due to the limited re-construction accuracy and mesh precision. Furthermore, it allows for learning directly from real 2D images and videos without requiring the tedious and cumbersome collection of copious high-ﬁdelity 3D data [29, 44, 14].
In our experiments, we demonstrate that our approach qualitatively and quantitatively outperforms state-of-the-art approaches to human motion transfer, despite the few im-ages used for inference, and even allows for plausible mo-tion transfer when using only a single image of the target.
In summary, our key contributions are:
• A novel set of neural network architectures for per-forming implicit volumetric human motion retargeting, which exploits the power of 3D human motion model-ing while avoiding the limitations of standard 3D human body modeling techniques.
• A framework to train these networks to attain high-ﬁdelity human motion transfer using only a few example images of the target subject performing various poses, without requiring target-speciﬁc training.
• Evaluations demonstrating our few-shot approach out-performs state-of-the-art alternatives both quantitatively and qualitatively, even those requiring training models for each new subject with substantial training data. 2.