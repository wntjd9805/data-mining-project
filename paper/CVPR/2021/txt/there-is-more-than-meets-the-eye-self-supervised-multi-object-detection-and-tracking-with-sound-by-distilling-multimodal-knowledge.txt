Abstract
Attributes of sound inherent to objects can provide valu-able cues to learn rich representations for object detection and tracking. Furthermore, the co-occurrence of audiovisual events in videos can be exploited to localize objects over the image ﬁeld by solely monitoring the sound in the environ-ment. Thus far, this has only been feasible in scenarios where the camera is static and for single object detection. Moreover, the robustness of these methods has been limited as they pri-marily rely on RGB images which are highly susceptible to illumination and weather changes. In this work, we present the novel self-supervised MM-DistillNet framework consist-ing of multiple teachers that leverage diverse modalities in-cluding RGB, depth and thermal images, to simultaneously exploit complementary cues and distill knowledge into a sin-gle audio student network. We propose the new MTA loss function that facilitates the distillation of information from multimodal teachers in a self-supervised manner. Addition-ally, we propose a novel self-supervised pretext task for the audio student that enables us to not rely on labor-intensive manual annotations. We introduce a large-scale multimodal dataset with over 113,000 time-synchronized frames of RGB, depth, thermal, and audio modalities. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods while being able to detect multiple objects using only sound during inference and even while moving. 1.

Introduction
Human perception is deceptively effortless, we can sense people behind our backs and in the dark, although we cannot remotely see them. This elucidates that perception is inher-ently a complex cognitive phenomenon that is facilitated by the integration of various sensory modalities [11]. “There is
More than Meets the Eye” aptly summarizes this complexity of our visual perception system. Modeling this ability us-ing learning algorithms is, however, far from being solved.
∗Equal contribution.
Figure 1. Our proposed cross-modal MM-DistillNet distills knowl-edge exploiting complementary cues from multimodal visual teach-ers into an audio student. During inference, the model detects and tracks multiple objects in the visual frame using only audio as input.
The natural co-occurrence of modalities such as images and audio in videos provides strong cues for supervision that can be exploited to learn more robust perception models in a self-supervised manner. Attributes of sound inherent to objects in the scene also contain rich time and frequency domain information that is valuable for grounding sounds within a visual scene. In this sense, the characteristics of sound are complementary and correlated to the visual infor-mation [40]. Cross-modal learning from images and sound exploits this natural correspondence between audio-visual streams that represent the same event. As a result, the inte-gration of sound with vision enables us to use one modality to supervise the other as well as to use both modalities to supervise each other jointly [8, 6, 58].
Generally, training models to detect objects requires large amounts of groundtruth annotations for supervision. How-ever, we can train models to recognize objects that produce sound without relying on labeled data by jointly leveraging audio-visual learning using the teacher-student strategy [22].
With this approach, numerous works [1, 49, 8] have used the audio-visual correlation to localize sounds sources. More-over, recent work [46] has shown that we can exploit this 111612
audio-visual synchronicity to detect and track an object over the visual frame. Thus far, this promising capability has only been shown to be feasible in scenarios where the camera is static and for detecting a single object at a time using stereo sound and metadata containing camera pose information as input. Moreover, it distills knowledge only from models trained with RGB images, which are highly susceptible to perceptual changes such as varying types, scales, and vis-ibility of objects, domain differences in terms of weather, illumination, and seasonality, among many others. Address-ing these challenges will enable us to employ the system for detection and tracking in a wide variety of applications.
In this work, we present the novel self-supervised Multi-Modal Distillation Network (MM-DistillNet) that provides effective solutions to the aforementioned problems. Our framework illustrated in Fig. 1 consists of multiple teacher networks, each of which takes a speciﬁc modality as input, for which we use RGB, depth, and thermal to maximize the complementary cues that we can exploit (appearance, geom-etry, reﬂectance). The teachers are ﬁrst individually trained on diverse pre-existing datasets to predict bounding boxes in their respective modalities. We then train the audio student network to learn the mapping of sounds from a microphone array to bounding box coordinates of the combined teachers’ prediction, only on unlabeled videos. To do this, we present the novel Multi-Teacher Alignment (MTA) loss to simultane-ously exploit complementary cues and distill object detection knowledge from multimodal teachers into the audio student network in a self-supervised manner. During inference, the audio student network detects and tracks objects in the visual frame using only sound as an input. Additionally, we present a self-supervised pretext task for initializing the audio stu-dent network in order to not rely on labor-intensive manual annotations and to accelerate training.
To facilitate this work, we collected a large-scale driv-ing dataset with over 113,000 time-synchronized frames of RGB, depth, thermal, and multi-channel audio modali-ties. We present extensive experimental results comparing the performance of our proposed MM-DistillNet with exist-ing methods as well as baseline approaches, which shows that it substantially outperforms the state-of-the-art. More importantly, for the ﬁrst time, we demonstrate the capabil-ity to detect and track objects in the visual frame, from only using sound as an input, without any meta-data and even while moving in the environment. We also present detailed ablation studies that highlight the novelty of the contributions that we make. Finally, we make our dataset, code and models publicly available at http://rl.uni-freiburg.de/research/multimodal-distill. 2.