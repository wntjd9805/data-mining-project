Abstract
The standard way of training video models entails sam-pling at each iteration a single clip from a video and op-timizing the clip prediction with respect to the video-level label. We argue that a single clip may not have enough temporal coverage to exhibit the label to recognize, since video datasets are often weakly labeled with categorical information but without dense temporal annotations. Fur-thermore, optimizing the model over brief clips impedes its ability to learn long-term temporal dependencies. To over-come these limitations, we introduce a collaborative mem-ory mechanism that encodes information across multiple sampled clips of a video at each training iteration. This enables the learning of long-range dependencies beyond a single clip. We explore different design choices for the collaborative memory to ease the optimization difﬁculties.
Our proposed framework is end-to-end trainable and sig-niﬁcantly improves the accuracy of video classiﬁcation at a negligible computational overhead. Through extensive ex-periments, we demonstrate that our framework generalizes to different video architectures and tasks, outperforming the state of the art on both action recognition (e.g., Kinetics-400 & 700, Charades, Something-Something-V1) and ac-tion detection (e.g., AVA v2.1 & v2.2). 1.

Introduction
In recent years, end-to-end learning of 3D convolu-tional networks (3D CNNs) has emerged as the prominent paradigm for video classiﬁcation [2,5,7,9,10,22,33,39,41– 43,45,47,53]. Steady improvements in accuracy have come with the introduction of increasingly deeper and larger net-works. However, due to their high computational cost and large memory requirements, most video models are opti-mized at each iteration over short, ﬁxed-length clips rather than the entire video.
Although widely used in modern video models, the clip-level learning framework is sub-optimal for video-level
*Work done during an internship at Facebook AI.
Clip-level Learning
Backbone sampled clip
Backbone sampled clip iteration j
… iteration i (a) Standard: clip-level learning on individual clips at each iteration.
Video-level Learning
Collaborative Memory
Backbone
Backbone
Backbone sampled clip sampled clip sampled clip (b) Our framework: video-level learning with collaborative memory.
Figure 1: Clip-level learning vs. our proposed end-to-end video-level learning framework. (Action label: something being deﬂected from something.) classiﬁcation. First, capturing long-range temporal struc-ture beyond short clips is not possible as the models are only exposed to individual clips during training. Second, the video-level label may not be well represented in a brief clip, which may be an uninformative segment of the video or include only a portion of the action, as shown in Fig-ure 1(a). Thus, optimizing a model over individual clips using video-level labels is akin to training with noisy la-bels. Recent attempts to overcome these limitations include methods that build a separate network on top of the clip-based backbone [20, 50, 56]. However, these approaches either cannot be trained end-to-end with the backbone (i.e., the video model is optimized over pre-extracted clip-level features) or require ad-hoc backbones which hinder their application in the current landscape of evolving architec-tures.
In this paper, we propose an end-to-end learning frame-work that optimizes the classiﬁcation model using video-level information collected from multiple temporal loca-tions of the video, shown in Figure 1(b). Our approach 7567
hinges on a collaborative memory mechanism that accumu-lates video-level contextual information from multiple clips sampled from the video. Within the same training iteration, this contextual information is shared back with all the clips to enhance the individual clip representations. The collabo-rative memory allows the model to capture long-range tem-poral dependencies beyond individual short clips by gen-erating clip-speciﬁc memories that encode the relation be-tween each local clip and the global video-level context.
Our experiments demonstrate that the proposed training framework is effective and generic. Speciﬁcally, our ap-proach does not make any assumption about the backbone architecture. We empirically show that it consistently yields signiﬁcant gains in accuracy when applied to different state-of-the-art architectures (e.g. SlowFast [10], R(2+1)D [43],
I3D-NL [47]). We also introduce and compare several de-sign variants of the collaborative memory. Furthermore, we demonstrate that the accuracy improvements come at a neg-ligible computational overhead and without an increase in memory requirements. Finally, we show that our frame-work can be extended to action detection where it yields signiﬁcant improvements without requiring extra informa-tion, such as optical ﬂow and object detection predictions, which are commonly used in previous work [38, 40]. We summarize our major contributions as follows:
• A new framework that enables end-to-end learning of video-level dependencies for clip-based models.
• A new collaborative memory mechanism that facili-tates information exchange across multiple clips. We explore different design choices and provide insights about the optimization difﬁculties.
• Experiments demonstrating that our collaborative memory framework generalizes to different backbones and tasks, producing state of the art results for action recognition and detection. 2.