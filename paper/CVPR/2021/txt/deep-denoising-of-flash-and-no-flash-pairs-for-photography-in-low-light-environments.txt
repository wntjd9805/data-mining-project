Abstract
We introduce a neural network-based method to denoise pairs of images taken in quick succession, with and with-out a ﬂash, in low-light environments. Our goal is to pro-duce a high-quality rendering of the scene that preserves the color and mood from the ambient illumination of the noisy no-ﬂash image, while recovering surface texture and detail revealed by the ﬂash. Our network outputs a gain map and a ﬁeld of kernels, the latter obtained by linearly mixing elements of a per-image low-rank kernel basis. We
ﬁrst apply the kernel ﬁeld to the no-ﬂash image, and then multiply the result with the gain map to create the ﬁnal output. We show our network effectively learns to produce high-quality images by combining a smoothed out estimate of the scene’s ambient appearance from the no-ﬂash image, with high-frequency albedo details extracted from the ﬂash input. Our experiments show signiﬁcant improvements over alternative captures without a ﬂash, and baseline denoisers that use ﬂash no-ﬂash pairs. In particular, our method pro-duces images that are both noise-free and contain accurate ambient colors without the sharp shadows or strong specu-lar highlights visible in the ﬂash image. 1.

Introduction
Flash photography has long been a convenient way to capture high-quality images in low-light conditions. A ﬂash illuminates the scene with a bright burst of light at the time of exposure, allowing the camera to acquire a photograph with a much higher signal-to-noise ratio than would be pos-sible under the dim ambient lighting alone and without in-troducing any motion or defocus blur. The ﬂash addresses the problem of limited illumination at its root—by adding light to the scene. However, ﬂash illumination is not with-out drawbacks. An on-camera ﬂash often creates unappeal-ing ﬂat shading and harsh shadows, resulting in images that fail to capture the true mood and ambience of the scene.
Researchers have considered combining pairs of ﬂash
Figure 1: Given a pair of images of low-light scenes cap-tured with and without a ﬂash (left), our method produces a high-quality image of the scene under ambient lighting (right). This output is generated by ﬁltering the no-ﬂash im-age with a predicted ﬁeld of kernels—to capture a smoothed stimate of scene appearance under ambient lighting, fol-lowed by multiplication with a scale map that introduces high-frequency detail illuminated by the ﬂash. and no-ﬂash images—captured in quick succession with and without the ﬂash—to create a single enhanced photo-graph that is both noise-free and accurately represents the scene under ambient lighting. This is achieved by merging information about the ambient scene appearance from the noisy no-ﬂash image, with high-frequency surface image details revealed by the ﬂash [9, 29]. However, these meth-ods assume moderate levels of noise in the no-ﬂash image, and that the ﬂash and no-ﬂash pair are, or can be, aligned.
In extremely low light, the no-ﬂash image can be very noisy, especially when using mobile phone cameras with small apertures. This precludes the use of traditional 2063
ﬂash/no-ﬂash methods, since the noise obscures even the low-frequency shading information in the no-ﬂash image and makes automatic alignment of the pair unreliable. In comparison, modern neural network-based denoising meth-ods [5, 34, 40, 41] can produce reasonable estimates from a noisy no-ﬂash image alone—although at high noise-levels, they still struggle to reconstruct high-frequency detail.
In this work, we leverage both the ability of modern neu-ral networks to encode strong natural image priors, and the unique combination of appearance information available in a ﬂash and no-ﬂash image pair. Speciﬁcally, we consider the task of producing a high-quality image of the scene under ambient lighting given a ﬂash and no-ﬂash pair as input. We focus on extremely low-light scenes such that the no-ﬂash image shows signiﬁcant noise, and the appearance of the no-ﬂash image is entirely dominated by the ﬂash illumina-tion. We further assume unknown geometric misalignment between the image pair, due to camera movement typically observed in hand-held burst photography [33].
Under these conditions, we train a deep neural network to take noisy, misaligned no-ﬂash/ﬂash image pairs as input, and output a denoised image of the scene under the scene’s ambient illumination. Rather than directly predicting the denoised image, our network outputs a kernel ﬁeld used to
ﬁlter the no-ﬂash image, and a scale map that is multiplied with this ﬁltered output to incorporate high-frequency im-age details from the ﬂash image. To use the regularizing ef-fect of kernels to effectively ﬁlter out the high levels of noise in the no-ﬂash input while overcoming its signiﬁcant mem-ory and computational costs [26], our network combines a recent kernel basis prediction approach [35] with efﬁcient kernel up-sampling. The use of a scale map is inspired by classical ﬂash/no-ﬂash approaches [9, 29] that adopt mul-tiplicative combination based on a view of factorizing im-ages into albedo and shading, where the former is common across the input pair while the latter is not.
We evaluate our approach extensively under different ambient light levels and spatial misalignment, and demon-strate state-of-the-art results for low-light denoising (see ex-ample result in Fig. 1). Our method outperforms denoising without a ﬂash—when using a single or burst of two no-ﬂash images. This demonstrates that a ﬂash input, despite often representing drastically different shading, is still infor-mative towards ambient appearance. Our method also out-performs other standard denoising approaches trained di-rectly on ﬂash/no-ﬂash pairs, highlighting the importance of the formulation and design of our network architecture.
Code and pre-trained models for our method are avail-able at https://www.cse.wustl.edu/~zhihao.xia/deepfnf/. 2.