Abstract
Deep neural networks are vulnerable to adversarial ex-amples that mislead the models with imperceptible pertur-bations. Though adversarial attacks have achieved incredi-ble success rates in the white-box setting, most existing ad-versaries often exhibit weak transferability in the black-box setting, especially under the scenario of attacking models with defense mechanisms. In this work, we propose a new method called variance tuning to enhance the class of iter-ative gradient based attack methods and improve their at-tack transferability. Speciﬁcally, at each iteration for the gradient calculation, instead of directly using the current gradient for the momentum accumulation, we further con-sider the gradient variance of the previous iteration to tune the current gradient so as to stabilize the update direction and escape from poor local optima. Empirical results on the standard ImageNet dataset demonstrate that our method could signiﬁcantly improve the transferability of gradient-based adversarial attacks. Besides, our method could be used to attack ensemble models or be integrated with var-ious input transformations.
Incorporating variance tun-ing with input transformations on iterative gradient-based attacks in the multi-model setting, the integrated method could achieve an average success rate of 90.1% against nine advanced defense methods, improving the current best attack performance signiﬁcantly by 85.1% . Code is avail-able at https://github.com/JHL-HUST/VT. 1.

Introduction
Deep Neural Networks (DNNs) are known to be vulnera-ble to adversarial examples [31, 8], which are indistinguish-able from legitimate ones by adding small perturbations, but lead to incorrect model prediction. In recent years, it has garnered an increasing interest to craft adversarial examples
[22, 3, 14, 1, 16], because it not only can identify the model vulnerability [3, 1], but also can help improve the model robustness [8, 21, 32, 37]. Moreover, adversarial examples
*Corresponding author.
MI-FGSM
VMI-FGSM
Raw Image
NI-FGSM
VNI-FGSM
Figure 1: Adversarial examples crafted by MI-FGSM [6],
NI-FGSM [18], the proposed VMI-FGSM and VNI-FGSM on the Inc-v3 model [30] with the maximum perturbation of ǫ = 16. VMI-FGSM and VNI-FGSM generate visually similar adversaries as other attacks but lead to much higher transferability. also exhibit good transferability across the models [25, 19], i.e. the adversaries crafted for one model can still fool other models, which enables black-box attacks in the real-world applications without any knowledge of the target model.
In the white-box setting that the attacker can access the architecture and parameters of the target model, existing ad-versarial attacks have exhibited great effectiveness [14, 3, 1] but with low transferability, especially for models equipped with defense mechanisms [32, 34, 17, 23]. To address this issue, recent works focus on improving the transferability of adversarial examples by advanced gradient calculation (e.g.
Momentum, Nesterov’s accelerated gradient, etc.) [6, 18], attacking multiple models [19], or adopting various input transformations (e.g. random resizing and padding, trans-lation, scale, admix, etc.) [35, 7, 18, 33]. However, there still exists a big gap between white-box attacks and transfer-based black-box attacks with regard to attack performance.
In this work, we propose a novel variance tuning itera-tive gradient-based method to enhance the transferability of the generated adversarial examples. Different from existing 1924
gradient-based methods that perturb the input in the gradi-ent direction of the loss function, or momentum iterative gradient-based methods that accumulate a velocity vector in the gradient direction, at each iteration our method ad-ditionally tunes the current gradient with the gradient vari-ance in the neighborhood of the previous data point. The key idea is to reduce the variance of the gradient at each iteration so as to stabilize the update direction and escape from poor local optima during the search process. Empirical results on the standard ImageNet dataset demonstrate that, compared with state-of-the-art momentum-based adversar-ial attacks [6, 18], the proposed method could achieve sig-niﬁcantly higher success rates for black-box models, mean-while maintain similar success rates for white-box models.
For instance, the proposed method improves the success rates of the momentum based attack [6] for more than 20% using adversarial examples generated on the Inc-v3 model
[30]. The adversarial examples crafted by various attacks are illustrated in Figure 1.
To further demonstrate the effectiveness of our method, we combine variance tuning with several gradient-based at-tacks for ensemble models [19] and integrate these attacks with various input transformations [35, 7, 18]. Extensive experiments show that our integrated method could remark-ably improve the attack transferability. In addition, we com-pare our attack method with the state-of-the-art attack meth-ods [6, 35, 7, 18] against nine advanced defense methods
[17, 34, 36, 9, 20, 12, 5, 23]. Our integrated method yields an average success rate of 67.0%, which outperforms the baselines by a large margin of 17.5% in the single model setting, and an average success rate of 90.1%, which out-performs the baselines by a clear margin of 6.6% in the multi-model setting. 2.