Abstract
We present Mobile Video Networks (MoViNets), a fam-ily of computation and memory efﬁcient video networks that can operate on streaming video for online inference. 3D convolutional neural networks (CNNs) are accurate at video recognition but require large computation and mem-ory budgets and do not support online inference, making them difﬁcult to work on mobile devices. We propose a three-step approach to improve computational efﬁciency while substantially reducing the peak memory usage of 3D
CNNs. First, we design a video network search space and employ neural architecture search to generate efﬁcient and diverse 3D CNN architectures. Second, we introduce the
Stream Buffer technique that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, we pro-pose a simple ensembling technique to improve accuracy further without sacriﬁcing efﬁciency. These three progres-sive techniques allow MoViNets to achieve state-of-the-art accuracy and efﬁciency on the Kinetics, Moments in Time, and Charades video action recognition datasets. For in-stance, MoViNet-A5-Stream achieves the same accuracy as
X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less memory. Code is available at https:
//github.com/google-research/movinet. 1.

Introduction
Efﬁcient video recognition models are opening up new opportunities for mobile camera, IoT, and self-driving ap-plications where efﬁcient and accurate on-device process-ing is paramount. Despite recent advances in deep video modeling, it remains difﬁcult to ﬁnd models that run on mobile devices and achieve high video recognition accu-racy. On the one hand, 3D convolutional neural networks (CNNs) [65, 69, 19, 18, 52] offer state-of-the-art accuracy, but consume copious amounts of memory and computation.
On the other hand, 2D CNNs [40, 76] require far fewer re-∗Work done as a part of the Google AI Residency.
Figure 1. Accuracy vs. FLOPs and Memory on Kinetics 600.
MoViNets are more accurate than 2D networks and more efﬁcient than 3D networks. Top (log scale): MoViNet-A2 achieves 6% higher accuracy than MobileNetV3 [26] at the same FLOPs while
MoViNet-A6 achieves state-of-the-art 83.5% accuracy being 5.1x faster than X3D-XL [18]. Bottom: Streaming MoViNets require 10x less memory at the cost of 1% accuracy. Note that we only train on the 93% of Kinetics 600 examples that are available at the time of writing. Best viewed in color. sources suitable for mobile and can run online using frame-by-frame prediction, but fall short in accuracy.
Many operations that make 3D video networks accurate (e.g., temporal convolution, non-local blocks [69], etc.) re-quire all input frames to be processed at once, limiting the opportunity for accurate models to be deployed on mobile devices. The recently proposed X3D networks [18] pro-vide a signiﬁcant effort to increase the efﬁciency of 3D
CNNs. However, they require large memory resources on large temporal windows which incur high costs, or small temporal windows which reduce accuracy. Other works aim 16020
to improve 2D CNNs’ accuracy using temporal aggrega-tion [40, 17, 70, 43, 16], however their limited inter-frame interactions reduce these models’ abilities to adequately model long-range temporal dependencies like 3D CNNs.
This paper introduces three progressive steps to design efﬁcient video models which we use to produce Mobile
Video Networks (MoViNets), a family of memory and computation efﬁcient 3D CNNs. 1. We ﬁrst deﬁne a MoViNet search space to allow Neu-ral Architecture Search (NAS) to efﬁciently trade-off spatiotemporal feature representations. 2. We then introduce Stream Buffers for MoViNets, which process videos in small consecutive subclips, re-quiring constant memory without sacriﬁcing long tem-poral dependencies, and which enable online infer-ence. 3. Finally, we create Temporal Ensembles of streaming
MoViNets, regaining the slightly lost accuracy from the stream buffers.
First, we design the MoViNet search space to explore how to mix spatial, temporal, and spatiotemporal operations such that NAS can ﬁnd optimal feature combinations to trade-off efﬁciency and accuracy. Figure 1 visualizes the ef-ﬁciency of the generated MoViNets. MoViNet-A0 achieves similar accuracy to MobileNetV3-large+TSM [26, 40] on
Kinetics 600 [32] with 75% fewer FLOPs. MoViNet-A6 achieves state-of-the-art 83.5% accuracy, 1.6% higher than
X3D-XL [18], requiring 60% fewer FLOPs.
Second, we create streaming MoViNets by introducing the stream buffer to reduce memory usage from linear to constant in the number of input frames for both training and inference, allowing MoViNets to run with substantially fewer memory bottlenecks. E.g., the stream buffer reduces
MoViNet-A5’s memory usage by 90%. In contrast to tradi-tional multi-clip evaluation approaches [54, 67] which also reduce memory, a stream buffer carries over temporal de-pendencies between consecutive non-overlapping subclips by caching feature maps at subclip boundaries. The stream buffer allows for a larger class of operations to enhance on-line temporal modeling than the recently proposed temporal shift [40]. We equip the stream buffer with temporally uni-directional causal operations like causal convolution [46], cumulative pooling, and causal squeeze-and-excitation [27] with positional encoding to force temporal receptive ﬁelds to look only into past frames, enabling MoViNets to oper-ate incrementally on streaming video for online inference.
However, the causal operations come at a small cost, reduc-ing accuracy on Kinetics 600 by 1% on average.
Third, we temporally ensemble MoViNets, showing that they are more accurate than single large networks while achieving the same efﬁciency. We train two streaming
MoViNets independently with the same total FLOPs as a single model and average their logits. This simple technique gains back the loss in accuracy when using stream buffers.
Taken together, these three techniques create MoViNets that are high in accuracy, low in memory usage, efﬁcient in computation, and support online inference. We search for MoViNets using the Kinetics 600 dataset [6] and test them extensively on Kinetics 400 [32], Kinetics 700 [7],
Moments in Time [45], Charades [53], and Something-Something V2 [22]. 2.