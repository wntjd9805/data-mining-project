Abstract
Constructing and maintaining a consistent scene model on-the-ﬂy is the core task for online spatial perception, in-terpretation, and action.
In this paper, we represent the scene with a Bayesian nonparametric mixture model, seam-lessly describing per-point occupancy status with a contin-uous probability density function. Instead of following the conventional data fusion paradigm, we address the problem of online learning the process how sequential point cloud data are generated from the scene geometry. An incremen-tal and parallel inference is performed to update the pa-rameter space in real-time. We experimentally show that the proposed representation achieves state-of-the-art accu-racy with promising efﬁciency. The consistent probabilistic formulation assures a generative model that is adaptive to different sensor characteristics, and the model complexity can be dynamically adjusted on-the-ﬂy according to differ-ent data scales. 1.

Introduction
Simultaneous Localization and Mapping (SLAM) has been recently viewed as a potential perceptual tool towards
Spatial AI [9] as it allows a mobile device to perceive the world and estimate the sensor state. Along with the evolu-tion of SLAM systems towards spatial perception arises an increasing demand for a more expressive map that can in-crementally distill knowledge from different kinds of data into a compact parameter space. Finding an appropriate representation has been a central task of establishing such a comprehensive map.
In this paper, we aim to maintain a continuous proba-bility ﬁeld that allows for storing data into a uniﬁed prob-abilistic form. The probability ﬁeld offers a generative ex-tension to different spatial representations, e.g., point cloud, occupancy grid, mesh at arbitrary resolution. Practically, we propose a continuous probability density function as the map representation using a Bayesian nonparametric mix-ture model. When obtaining 3D point cloud data, the scene geometry is depicted as a continuous probability ﬁeld of spatial occupancy status. This representation owns the fol-lowing properties: 1) Probabilistic. The Bayesian fashion not only quantiﬁes uncertainties explicitly, but also allows to incorporate all sorts of information from different sensor inputs in a uniﬁed probabilistic manner; 2) Adaptive and dy-namic. The nonparametric property offers an inherently in-ﬁnite capacity [68] that guarantees an adaptive model com-113111
plexity with respect to acquired data scale; 3) Compact and expressive. The mixture model maintains a continuous and dense probability ﬁeld in a discrete and sparse parameter space, where both geometry and uncertainty are kept within
Gaussian components.
Speciﬁcally, we formulate the mapping as an online
Bayesian learning problem: the map provides a generative process of the observations, and we use acquired streaming data to learn it incrementally. The incremental inference can be viewed as a transition from geometry prior to posterior given streaming data. As the posterior is intractable to com-pute and represent, we resort to a parallel and incremental approach. The global distribution is parallelly distributed to local processing in an incremental fashion, guaranteeing efﬁcient inference for accurate scene geometry.
In summary, our major contributions include a novel scene representation using the Bayesian nonparametric mixture model and a principled way of online Bayesian learning for efﬁcient map updating. Our method obtains a continuous high-quality scene representation incrementally, and achieves state-of-the-art results as demonstrated in the qualitative and quantitative experiments. 2.