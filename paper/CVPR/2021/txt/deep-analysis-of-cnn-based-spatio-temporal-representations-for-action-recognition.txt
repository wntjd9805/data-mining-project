Abstract
In recent years, a number of approaches based on 2D or 3D convolutional neural networks (CNN) have emerged for video action recognition, achieving state-of-the-art results on several large-scale benchmark datasets. In this paper, we carry out in-depth comparative analysis to better un-derstand the differences between these approaches and the progress made by them. To this end, we develop an uniﬁed framework for both 2D-CNN and 3D-CNN action models, which enables us to remove bells and whistles and provides a common ground for fair comparison. We then conduct an effort towards a large-scale analysis involving over 300 action recognition models. Our comprehensive analysis re-veals that a) a signiﬁcant leap is made in efﬁciency for ac-tion recognition, but not in accuracy; b) 2D-CNN and 3D-CNN models behave similarly in terms of spatio-temporal representation abilities and transferability. Our codes are available at https://github.com/IBM/action-recognition-pytorch. 1.

Introduction
With the recent advances in convolutional neural net-works (CNNs) [59, 24] and the availability of large-scale video benchmark datasets [31, 44], deep learning ap-proaches have dominated the ﬁeld of video action recogni-tion by using 2D-CNNs [68, 38, 8] or 3D-CNNs [2, 22, 10] or both [40, 57]. The 2D CNNs perform temporal model-ing independent of 2D spatial convolutions while their 3D counterparts learn space and time information jointly by 3D convolution. These methods have achieved state-of-the-art performance on multiple large-scale benchmarks such as
Kinetics [31] and Something-Something [20].
Although CNN-based approaches have made impressive progress in action recognition, there are several fundamen-tal questions that still largely remain unanswered in the
ﬁeld. For example, what contributes to improved spatio-Figure 1: Recent progress of action recognition on Kinetics-400 (only models based on InceptionV1 and ResNet50 are in-cluded). Models marked with ∗ are re-trained and evaluated (see
Section 6.2) while others are from the existing literature. The size of a circle indicates the 1-clip FLOPs of a model. With temporal pooling turned off, I3D performs on par with the state-of-the-art approaches. Best viewed in color. temporal representations of these recent approaches? Do these approaches enable more effective temporal modeling, the crux of the matter for action recognition? Furthermore, there seems no clear winner between 2D-CNN and 3D-CNN approaches in terms of accuracy. 3D models report better performance than 2D models on Kinetics while the latter are superior on Something-Something. How differ-ently do these two types of models behave with regard to spatial-temporal modeling of video data?
We argue that the difﬁculty of understanding the recent progress on action recognition is mainly due to the lack of fairness in performance evaluation related to datasets, backbones and experimental practices. In contrast to im-age recognition where ImageNet [4] has served as a gold-standard benchmark for evaluation, there are at least 4∼5 popular action datasets widely used for evaluation (see Fig-ure 2). While Kinetics-400 [31] has recently emerged as a primary benchmark for action recognition, it is known to be strongly biased towards spatial modeling, thus being 6165
inappropriate for validating a model’s capability of spatio-temporal modeling. In addition, there seems to be a ten-dency in current research to overly focus on pursuing state-of-the-art (SOTA) performance, but overlooking other im-portant factors such as the backbone networks and the num-ber of input frames. For instance, I3D [2] based on 3D-InceptionV1 has become a “gatekeeper” baseline to com-pare with for any recently proposed approaches of action recognition. However such comparisons are often unfair against stronger backbones such as ResNet50 [24]. As shown in Figure 1, I3D, with ResNet50 as backbone, per-forms comparably with or outperforms many recent meth-ods that are claimed to be better. As a result, such evaluation are barely informative w.r.t whether the improved results of an approach come from a better backbone or the algorithm itself. As discussed in Section 3, performance evaluation in action recognition may be further confounded by many other issues such as variations in training and evaluation protocols, model inputs and pretrained models.
In light of the great need for better understanding of
CNN-based action recognition models, in this paper we pro-vide a common ground for comparative analysis of 2D-CNN and 3D-CNN models without any bells and whis-tles. We conduct comprehensive experiments and analysis to compare several representative 2D-CNN and 3D-CNN methods on three large-scale benchmark datasets. Our main goal is to deliver deep understanding of the important ques-tions brought up above, especially, a) the current progress of action recognition and b) the differences between 2D-CNN and 3D-CNN methods w.r.t spatial-temporal represen-tations of video data. Our systematic analysis provides in-sights to researchers to understand spatio-temporal effects of different action models across backbone and architecture and will broadly simulate discussions in the community re-garding a very important but largely neglected issue of fair comparison in video action recognition.
The main contributions of our work as follows:
• A uniﬁed framework for Action Recognition. We present a uniﬁed framework for 2D-CNN and 3D-CNN approaches and implement several representative methods for comparative analysis on three standard ac-tion recognition benchmark datasets.
• Spatio-Temporal Analysis. We systematically com-pare 2D-CNN and 3D-CNN models to better under-stand the differences and spatio-temporal behavior of these models. Our analysis leads to some interest-ing ﬁndings as follows: a) Temporal pooling tends to suppress the efﬁcacy of temporal modeling in an ac-tion model, but surprisingly provides a signiﬁcant per-formance boost to TSN [68]; b) By removing non-structural differences between 2D-CNN and 3D-CNN models, they behave similarly in terms of spatio-temporal representation abilities and transferability.
• Benchmarking of SOTA Approaches. We thor-oughly benchmarked several SOTA approaches and compared them with I3D. Our analysis reveals that
I3D still stays on par with SOTA approaches in terms of accuracy (Figure 1) and the recent advance in ac-tion recognition is mostly on the efﬁciency side, not on accuracy. Our analysis also suggests that the in-put sampling strategy taken by a model (i.e. uniform or dense sampling) should be considered for fairness when comparing two models (Section 6.2). 2.