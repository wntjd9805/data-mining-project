Abstract
We address the problem of estimating depth with multi modal audio visual data. Inspired by the ability of animals, such as bats and dolphins, to infer distance of objects with echolocation, some recent methods have utilized echoes for depth estimation. We propose an end-to-end deep learn-ing based pipeline utilizing RGB images, binaural echoes and estimated material properties of various objects within a scene. We argue that the relation between image, echoes and depth, for different scene elements, is greatly inﬂuenced by the properties of those elements, and a method designed to leverage this information can lead to signiﬁcantly im-proved depth estimation from audio visual inputs. We pro-pose a novel multi modal fusion technique, which incor-porates the material properties explicitly while combining audio (echoes) and visual modalities to predict the scene depth. We show empirically, with experiments on Replica dataset, that the proposed method obtains 28% improve-ment in RMSE compared to the state-of-the-art audio-visual depth prediction method. To demonstrate the effectiveness of our method on larger dataset, we report competitive per-formance on Matterport3D, proposing to use it as a multi-modal depth prediction benchmark with echoes for the ﬁrst time. We also analyse the proposed method with exhaus-tive ablation experiments and qualitative results. The code and models are available at https://krantiparida. github.io/projects/bimgdepth.html 1.

Introduction
Humans perceive the surroundings using multiple sen-sory inputs such as sound, sight, smell and touch, with dif-ferent tasks involving different combinations of such inputs.
In computer vision, multimodal learning has also gained interest. As one popular stream, researchers have leveraged audio and visual inputs for addressing challenging prob-lems. These problems can be broadly divided into three cat-egories: (i) using audio modality only as the input, to learn a seemingly visual task, e.g. using echo for depth predic-tion [9], (ii) using visual modality as auxilliary information for an audio task, e.g. using videos to convert mono audio to (cid:54)(cid:68)(cid:80)(cid:72)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)(cid:3)(cid:82)(cid:81)(cid:3)(cid:72)(cid:70)(cid:75)(cid:82)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:89)(cid:76)(cid:86)(cid:88)(cid:68)(cid:79)(cid:3)(cid:80)(cid:82)(cid:71)(cid:68)(cid:79)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86) (cid:40)(cid:70)(cid:75)(cid:82)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:48)(cid:68)(cid:87)(cid:72)(cid:85)(cid:76)(cid:68)(cid:79)(cid:3)(cid:68)(cid:90)(cid:68)(cid:85)(cid:72)(cid:3) (cid:68)(cid:87)(cid:87)(cid:72)(cid:81)(cid:87)(cid:76)(cid:82)(cid:81)(cid:15)(cid:3)(cid:72)(cid:17)(cid:74)(cid:17)(cid:3) (cid:75)(cid:76)(cid:74)(cid:75)(cid:72)(cid:85)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:82)(cid:81)(cid:3) (cid:72)(cid:70)(cid:75)(cid:82)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:69)(cid:72)(cid:87)(cid:87)(cid:72)(cid:85)(cid:3) (cid:86)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)(cid:85)(cid:72)(cid:73)(cid:79)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3) (cid:86)(cid:88)(cid:85)(cid:73)(cid:68)(cid:70)(cid:72)(cid:86)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:3)(cid:90)(cid:68)(cid:79)(cid:79)(cid:86) (cid:14) (cid:14) (cid:40)(cid:91)(cid:76)(cid:86)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:51)(cid:85)(cid:82)(cid:83)(cid:82)(cid:86)(cid:72)(cid:71)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)
Figure 1. We address the problem of depth prediction using mul-timodal audio (binaural echo) and visual (monocular RGB) inputs.
We propose an attention based fusion mechanisms, where the at-tention maps are inﬂuenced by automatically estimated material properties of the scene objects. We argue that capturing the mate-rial properties while fusing echo with images is beneﬁcial as the light and sound reﬂection characteristics depend not only on the depth, but also on the material of the scene elements. binaural audio [14], and (iii) using both audio visual modal-ities together, e.g. for depth prediction [12]. Here, we fol-low the third line of work, and address the problem of depth map prediction using both audio and visual inputs. Stud-ies in psychology and perception indicate that both sound and vision complement each other, i.e. visual information helps calibrate the auditory information [22] while auditory grouping helps solve visual ambiguity [42]. Many animals, like bats and dolphins, use echolocation to estimate the dis-tances of objects from them. Visually impaired humans have also been reported to use echolocation [1]. Motivated by such cases, Christensen et. al [9, 10] recently showed that depth maps can be predicted directly from stereo sound.
Gao et. al [12] showed that by fusing features from binaural echoes with the monocular image features, depth estimation can be improved. Inspired by these ﬁndings, we work with similar reasoning, i.e. sound contains useful information to predict depth, and that echoes, used along with monocular images, improve depth estimation.
Going beyond the current methods which do simple combinations of features from echoes and images [12], we 18268
Figure 2. Comparison of our method with the existing approaches argue that the material properties of the objects in the scene signiﬁcantly inform the spatial ﬁdelity of the two streams.
Some objects may lend better depth estimates with echoes, while some may prefer the visual modality more. Deriv-ing from this motivation, we propose a novel end-to-end learnable network with a multimodal fusion module. This novel module incorporates material properties of the scene and fuses the two modalities with spatial attention maps in-dicating the ﬁdelity of the respective modality for different spatial locations. The material properties are automatically estimated using a sub-network initialized with training on auxiliary data on materials. As the ﬁnal depth prediction, the method fuses the depth maps produced by the audio and visual inputs, modulated by the predicted attention maps.
Fig. 1 illustrates the difference with a real output of an exist-ing method and the proposed approach, showing qualitative improvements.
We demonstrate the advantages of the proposed method with experiments on Replica [37] and Matterport3D [7] datasets. We outperform the previous state-of-the-art on
Replica dataset by ∼ 28% RMSE. On Matterport3D, which is more complex and larger (5x) than Replica, we provide results on the multimodal depth prediction task for the ﬁrst time, and compare the proposed method with existing ap-proaches and challenging baselines. We also show that the proposed network can estimate better depth with low reso-lution images. This is important in practical systems work-ing on depth estimation from monocular images, as sensors capturing echoes can be used along with cameras, to not only enhance the performance of existing setup but also suf-fer lesser degradation in depth prediction with the reduction in the quality of images. Further, we give ablation experi-ments to systematically evaluate the different aspects of the proposed method.
In summary, we make the following contributions:
• We propose a novel end-to-end learnable deep neu-ral network to estimate depth from binaural audio and monocular images.
• We provide exhaustive quantitative and qualitative re-sults on Replica and Matterport3D datasets. On Replica, we outperform the previous state-of-the-art by ∼ 28%.
On Matterport3D, we provide results benchmarking ex-isting methods. The proposed method achieves state-of-the-art performance, outperforming the existing best method on Matterport3D by ∼ 4%.
• We provide exhaustive ablation experiments on the de-sign choices in the network, and validate our intuitions with representative qualitative results. 2.