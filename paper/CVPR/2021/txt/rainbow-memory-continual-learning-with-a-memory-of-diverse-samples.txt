Abstract
Continual learning is a realistic learning scenario for
AI models. Prevalent scenario of continual learning, how-ever, assumes disjoint sets of classes as tasks and is less realistic rather artiﬁcial. Instead, we focus on ‘blurry’ task boundary; where tasks shares classes and is more realis-tic and practical. To address such task, we argue the im-portance of diversity of samples in an episodic memory.
To enhance the sample diversity in the memory, we pro-pose a novel memory management strategy based on per-sample classiﬁcation uncertainty and data augmentation, named Rainbow Memory (RM). With extensive empirical validations on MNIST, CIFAR10, CIFAR100, and ImageNet datasets, we show that the proposed method signiﬁcantly improves the accuracy in blurry continual learning setups, outperforming state of the arts by large margins despite its simplicity. Code and data splits will be available in https://github.com/clovaai/rainbow-memory. 1.

Introduction
Continual learning (CL) or class incremental learning (CIL) is known to particularly suffer from the catastrophic forgetting with respect to model generalization, due to in-accessibility to the data of previous tasks. It is because the class distributions of each task continuously change given a task stream. This makes continual learning particularly challenging; most AI models suffer from under real-world application scenarios across various domains [38, 20, 30].
To address the issue of changing data distribution for con-tinual learning, there are many approaches proposed such as momentum matching [29], sample generation [42, 46, 24, 43], regularization on parameters [27, 5], and sampling-based memory management [38, 39].
However, they are mostly evaluated in a rather artiﬁ-cial task setup of disjoint, where tasks do not share the classes [37]. For real-world applications, we consider a
∗ indicates equal contribution. † indicates corresponding author.
.
Class Distribution class 1 class 2 class 3 q e r f class
Previous Task
…
+
Sampling
. q e r f class
Current Task
+
Sampling
<Episodic Memory>
<Episodic Memory>
Augment
Augment t=T-1 t=T
Figure 1: Blurry-CIL (class incremental learning) setup (top) and overview of our proposed approach (bottom). In the blurry-CIL, the tasks share classes, contrary to conventional disjoint-CIL. Pro-posed memory management strategy updates an episodic memory with samples of the current task to keep diverse exemplars in the memory. Data augmentation (DA) further enhances the diversity of the kept exemplars. more realistic and practical setting of blurry-CIL where the classes shared across the tasks [38] (illustrated at the top of
Figure 1). The blurry-CIL setup requires that (1) each task is given sequentially as a stream, (2) the majority (assigned) classes of tasks differ from each other, and (3) a model can leverage only a very small portion of data of previous tasks.
For instance, suppose an e-commerce service that catego-rizes new items with their images taken by a seller. For each category, the number of newly registered items conspicu-ously depend on various factors such as season and tran-sient event. The popularity period of items varies according to their characteristics as shown in Figure 2; e.g., swimming suits are prevalent in summer and padding jumper are much more registered in winter.
In recent literature, the methods storing a small portion of old data have shown promising results in preserving the information of old classes when training new classes for the blurry-CIL setup [38], thus alleviating catastrophic forget-ting [16]. This strategy naturally raises the question: what is the optimal strategies to manage the memory? Since the number of stored samples is much smaller than that of the 18218
Swimming suite Mask
Paepaero (Snack)
Padding jumper
)
% ( y c n e u q e r
F d e z i l a m r o
N 100 80 60 40 20 0
Nov.2019
Jan.2020
Mar.2020
May.2020
Jul.2020
Sep.2020
Figure 2: Popularity changes of four items including swimming suite, snack, mask, and padding jumper during one year in a real-world e-commerce service. Each item has its own popular-ity period and this phenomena is more similar to blurry-CIL than disjoint-CIL because most item categories do not disjointly appear in real-world applications. incoming new-class, the samples in the memory would in-cur either overﬁtting or be ignored during training due to its small size compared to that of samples of incoming tasks.
As a straightforward solution, if we gradually increase the memory size when the samples are incoming, the problem-setting fails to hold an important resource constraint of the
CIL; a limited ﬁxed memory requirement. Therefore, we need a strategy to maintain sufﬁcient information of the old class with a small number of samples.
To address this problem, we investigate two factors for better continual learning on the newly deﬁned blurry-CIL setup; sampling for the memory and augmenting the data in the memory. First, we propose a perturbation-induced un-certainty to select samples for the memory by measuring the per-sample robustness against the perturbations. To measure the uncertainty, we deﬁne a prior distribution that draws the perturbed samples and approximates the robustness (i.e., in-verse of uncertainty) described as a likelihood function in a Bayesian formulation. We ﬁll the memory slots with the samples drawn from the distribution corresponding to the robustness. We show that the diversity-induced memory by sampling both perturbation-robust and fragile data helps the models to preserve discriminative boundary for each class.
Second, we investigate the effect of the diversity ac-quired by data augmentation in the blurry-CIL. In partic-ular, label mixing-based data augmentation, such as Cut-Mix [49], projects the input samples into a more complex dimension by mixing the image-label of multiple data sam-ples randomly and has reported notable successes in various recognition tasks [47, 26]. It provides additionally rich di-versity of stored samples in the episodic memory. Along with the label mixing augmentation, we exploit the ef-fects of composition of multiple data augmentations for en-hancing the diversity, beneﬁting from conventional methods such as ﬂipping, shearing, or color jittering and recent au-tomated data augmentation researches [9, 10, 32]. Incorpo-rating the two proposals, we name our method as Rainbow
Memory or RM for short.
Our RM is mainly evaluated in blurry-CIL setup on
MNIST, CIFAR10, CIFAR100 and ImageNet datasets, compared with various standard CIL methods. The exten-sive experimental validations show that our approach effec-tively addresses blurry-CIL, outperforming state-of-the art baselines with signiﬁcant margins (over 14%p and 9% of accuracy on all datasets evaluated). In addition, our method comparably performs to the other methods in disjoint-CIL set-up even if it is designed for blurry-CIL setup.
We summarize the contributions as follows:
• We propose a new diversity-aware sampling method for effectively managing the memory with limited ca-pacity by leveraging classiﬁcation uncertainty.
• We propose to augment the samples in the memory to further enhance the diversity of the samples.
• Our RM outperforms previous methods in blurry-CIL setup by large margins.
• We release the source code of RM and the evaluation protocol including the task splits of blurry-CIL for fu-ture research in this avenue. 2.