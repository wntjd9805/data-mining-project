Abstract
A fundamental challenge faced by existing Fine-Grained
Sketch-Based Image Retrieval (FG-SBIR) models is the data scarcity – model performances are largely bottlenecked by the lack of sketch-photo pairs. Whilst the number of pho-tos can be easily scaled, each corresponding sketch still needs to be individually produced. In this paper, we aim to mitigate such an upper-bound on sketch data, and study whether unlabelled photos alone (of which they are many) can be cultivated for performance gain. In particular, we in-troduce a novel semi-supervised framework for cross-modal retrieval that can additionally leverage large-scale unla-belled photos to account for data scarcity. At the center of our semi-supervision design is a sequential photo-to-sketch generation model that aims to generate paired sketches for unlabelled photos.
Importantly, we further introduce a discriminator-guided mechanism to guide against unfaith-ful generation, together with a distillation loss-based regu-larizer to provide tolerance against noisy training samples.
Last but not least, we treat generation and retrieval as two conjugate problems, where a joint learning procedure is de-vised for each module to mutually beneﬁt from each other.
Extensive experiments show that our semi-supervised model yields a signiﬁcant performance boost over the state-of-the-art supervised alternatives, as well as existing methods that can exploit unlabelled photos for FG-SBIR. 1.

Introduction
With the ever rising popularity of touch screen devices, sketch-based image retrieval (SBIR) has witnessed signiﬁ-cant interest within the vision community [5, 34, 35, 50, 11, 45]. Despite starting as a category-level retrieval problem
[9, 10, 3, 12], the ﬁne-grained nature of sketches stirred cur-rent research focus more towards ﬁne-grained SBIR (FG-SBIR) [5, 28] – which aims to retrieve a particular photo based on a query sketch at an intra-category basis.
Recent FG-SBIR works [48, 26, 5, 28] predominately
Generated
Sketches
Synthetic  Sketch-Photo Pairs
Labelled  Sketch-Photo Pairs
Photo to Sketch
Generation Model
Joint 
Training
Fine-Grained
SBIR Model
Unlabelled
Photos
Labelled  Sketch-Photo Pairs
Training with labelled data
Training with un-labelled data
Figure 1. Our proposed method additionally leverages large scale photos without any manually labelled paired sketches to improve
FG-SBIR performance. Moreover, we show that the two conjugate process, photo-to-sketch generation and ﬁne-grained SBIR, could improve each other by joint training. rely on fully-supervised triplet loss-based deep networks to yield retrieval performances of practical value. The under-lying assumption is largely inline with the progression of supervised photo-only models – that one can always (rela-tively easily) obtain additional labelled training data to sus-tain desired performance gains. This assumption however does not hold for FG-SBIR – sketch-photo pairs can not be easily scaled as per their photo-only counterparts. That is, instead of crawling and then labelling photos, the corre-sponding sketch for any given photo will need to be sepa-rately drawn by hand. As a result, current FG-SBIR datasets still remain in their thousands (6.7K for QMUL-ShoeV2
[48, 39], and 2K for QMUL-ChairV2 [39]), while photo-datasets [32] are available in millions. This data scarcity problem has consequently resulted in very recent attempts that aim at designing generalisable and zero-shot models
[26], yet performances of these models remain far away from fully-supervised alternatives.
In this paper, we face the music and make the bold as-sumption that there will hardly be sufﬁciently large sketch-photo pairs to train a good model. Instead, we test the hy-pothesis that – freely-available unlabelled photo data would 4247
help to mitigate the performance gap imposed by the lack of speciﬁcally collected photo-sketch paired data. Our ut-most contribution is therefore a semi-supervised FG-SBIR framework where unlabelled photo data (i.e., photos with-out matching sketches) are used alongside photo-sketch pairs for model training. We differ signiﬁcantly to con-ventional semi-supervised classiﬁcation methods [36, 29] – other than learning pseudo photo labels via a learnable clas-siﬁer, our “label” for a photo is in the form of a visual sketch which needs to be generated rather than classiﬁed. Thus, at the core of our design is a sequential photo-to-sketch gen-eration model that outputs pseudo sketches for unlabelled photos. The hope is therefore that such pseudo sketch-photo pairs could augment the training of a retrieval model.
Naively cascading a generator with a retrieval model however would not work. This is mainly because off-the-shelf photo-to-sketch generation models [37, 7] could sometimes generate unfaithful sketches that may not resem-ble their corresponding photos, especially when it comes to ﬁne-grained visual features. The downstream retrieval model would then have no way of knowing which pseudo sketch and photo pairs are worth training with, ultimately resulting in performance degradation. This leads to an im-portant design consideration of ours – we advocate that there is positive complementarity between generation and retrieval that can be explored via joint learning (Figure 1). The intuition is simple – pseudo sketches automati-cally generated from unlabelled photos can help to semi-supervise a better retrieval model, and vice versa that better retrieval model can feed back to the generator in producing more faithful sketch-photo pairs.
The key therefore lies with how such positive exchange cycle can be facilitated between the generator and retrieval model. To this end, novelty lies in the components intro-duced in both generator and retrieval model designs, and in how they are jointly trained. First, we formulate a novel sequential photo-to-sketch generator with spatial resolution preservation and a cross-modal 2D-attention mechanism.
Second, a discriminator is formulated in the retrieval model, to quantify the reliability of generated pseudo photo-sketch pairs. Reliability scores are then used for instance-wise weighting of triplet-loss values upon updating the retrieval model. A consistency loss (via distillation) is further intro-duced to simultaneously suppress the noisy training signal coming from pseudo photo-sketch pairs. Third, to enable exchange from retrieval to generation, we rely on the fol-lowing intuition – good synthetic pairs would trigger a low value on the resulting triplet loss and a higher output of the discriminator. Feeding these training signals back to the generator would however involve passing through a non-differentiable rasterization operation (Figure 2). We thus employ a policy-gradient [40] based reinforcement learning scheme that feeds back these signals as rewards.
In summary, our contributions are: (a) For the ﬁrst time, we propose to solve the data scarcity problem in FG-SBIR by adopting semi-supervised approach that addition-ally leverages large scale unlabelled photos to improve re-(b) To this end, we couple sequential trieval accuracy. sketch generation process with ﬁne-grained SBIR model in a joint learning framework based on reinforcement learn-ing. (c) We further propose a novel photo-to-sketch genera-tor and introduce a discriminator guided instance weighting along with consistency loss to retrieval model training with noisy synthetic photo-sketch pairs. (d) Extensive experi-ments validate the efﬁcacy of our approach for overcom-ing data scarcity in FG-SBIR (Figure 4) – we can already reach performances at par with prior arts with just a frac-tion (≈60%) of the training pairs, and obtain state-of-the-art performances on both QMUL-Shoe and QMUL-Chair with the same training data (by ≈6% and ≈7% respectively). 2.