Abstract
We address the problem of performing backpropagation for computation graphs involving 3D transformation groups
SO(3), SE(3), and Sim(3). 3D transformation groups are widely used in 3D vision and robotics, but they do not form vector spaces and instead lie on smooth manifolds.
The standard backpropagation approach, which embeds 3D transformations in Euclidean spaces, suffers from numeri-cal difﬁculties. We introduce a new library, which exploits the group structure of 3D transformations and performs backpropagation in the tangent spaces of manifolds. We show that our approach is numerically more stable, eas-ier to implement, and beneﬁcial to a diverse set of tasks.
Our plug-and-play PyTorch library is available at https:
//github.com/princeton-vl/lietorch. 1.

Introduction 3D transformation groups—SO(3), SE(3), Sim(3)— have been extensively used in a wide range of computer vi-sion and robotics problems. Important applications include
SLAM, 6-dof pose estimation, multiview reconstruction, in-verse kinematics, pose graph optimization, geometric reg-istration, and scene ﬂow.
In these domains, the state of the system—conﬁguration of robotic joints, camera poses, non-rigid deformations—can be naturally represented as 3D transformations.
Recently, many of these problems have been approached using deep learning, either in an end-to-end manner[31, 10, 40, 8, 38, 22, 18] or composed as hybrid systems[21, 11, 29, 19]. A key ingredient of deep learning is auto-differentiation, in particular, backpropagation through a computation graph. A variety of general-purpose deep learning libraries such as PyTorch [27] and TensorFlow [1] have been developed such that backpropagation is automat-ically performed by the library—users only need to specify the computation graph and supply any custom operations.
A basic assumption of existing deep learning libraries is that a computation graph represents a composition of map-pings between Euclidean spaces. That is, each node of the graph represents a differentiable mappings between Eu-clidean spaces, e.g. from Rm to Rn. This assumption allows us to use the standard deﬁnition of the gradient of a function f (x) : Rn → R as ∂f
∂x2 , . . . , ∂f
∂x1 , ∂f
], and use the
∂xn chain rule to backpropagate the gradient: ∂f
∂y = ∂f
∂x · ∂x
∂y ,
∂y is the Jacobian matrix ( ∂xi where ∂x
∂yj
∂x = [ ∂f
).
However, this assumption breaks down when the com-putation graph involves 3D transformations, such as when a network iteratively updates its estimate of the camera pose [31, 40, 33, 10, 20]. 3D transformations do not form vector spaces and instead lie on smooth manifolds; the no-tion of addition is undeﬁned and the standard notion of gra-dient deﬁned in Euclidean spaces does not apply.
A typical approach in prior work is to embed the 3D transforms in a Euclidean space. For example, a rigid body transform from SE(3) is represented as a 4 × 4 matrix and treated as a vector in R16. That is, SE(3) corresponds to a subset of R16, or more speciﬁcally, an embedded sub-manifold of R16. Functions involving SE(3) such as the exponential map and the inverse are replaced with their ex-tensions in the embedding space, i.e. the matrix exponential and the matrix inverse: exp : se(3) 7→ SE(3) −→ exp : R6 7→ R4×4 inv : SE(3) 7→ SE(3) −→ inv : R4×4 7→ R4×4. (1)
Now, the computation graph involves only Euclidean ob-jects, and backpropagation can be performed as usual as long as the Jacobian of exp and inv can be calculated.
There are several problems with this approach. First, while the extended functions such as the matrix exponential are smooth as a whole, the individual substeps needed for computing the backward passes often contain singularities.
As a result, small deviations off the manifold can result in numerical instabilities causing gradients to explode. It can often be quite difﬁcult to implement these functions in li-braries such as PyTorch[27] and Tensorﬂow[1] in a way that numerically stable gradients are achieved through automatic differentiation. As a case in point, the commonly used Py-Torch3D library[28] returns nan-gradients when the identity matrix is given as input the matrix log. Furthermore, some 10338
Figure 1. Tangent space Backpropagation. The forward pass is a composition of mappings between Lie groups. The backward pass propogates gradients in the tangent space of each element. extended functions have very complicated backward pass, which leads to extremely large computation graphs, espe-cially for groups like Sim(3) which involve complex ex-pressions for the matrix exponential and logarithm. Partly for this reason, to our knowledge, no prior work has per-formed backpropagation on Sim(3).
Due to the aforementioned problems, existing deep learning libraries are unable to handle 3D transformations reliably and transparently. And incorporating 3D transfor-mations into a computation graph typically requires a sub-stantial amount of painstaking, ad-hoc manual effort.
In this work, we seek to make backpropagation on 3D transformations robust and “painless”. We introduce a new approach for performing backpropagation though mixed computation graphs consisting of both real vectors and 3D transformations. Instead of embedding transformation groups in Euclidean spaces, we retain the group structure and perform backpropagation directly in the tangent space of each group element (Fig. 1). For example, while a rigid body transformation T ∈ SE(3) may be represented as a
R4×4 matrix, we backpropagate the gradient in the tangent space se(3), in particular, as a 6-dimensional vector in a lo-cal coordinate system centered at T .
We show that performing differentiation in the tangent space has several advantages – Numerical Stability: By performing backpropagation in the tangent space, we avoid needing to differenti-ate through singularity-ridden substeps required for the embedding approach, guaranteeing numerically stable gradients. This allows us to provide groups such as
Sim(3) which do not give stable gradients when auto-matic differentiation is directly applied. – Representation Agnostic: The computation of the gra-dients does not depend on how the 3D transformations are represented. Both quaternions and 3 × 3 matri-ces can be used to represent rotations without changing how the backward pass is computed. – Reduced Computation Graphs: When functions such as exp and log are implemented directly in PyTorch, the output of each individual steps within exp and log need to be stored for the backward pass, leading to unnecessarily large computation graphs. We avoid the need to store intermediate values by differentiating through these functions using the group structure. – Manifold Optimization: Our library can be directly used for problems where the variables we want to op-timize are 3D transforms. Since we compute gradients directly in the tangent space, we avoid the need to re-project gradients.
We demonstrate use cases of our approach on a wide range of vision and robotics tasks. We show our approach can be used for pose graph optimization, inverse kinemat-ics, RGB-D scan registration, and RGB-D SLAM.
Our approach is implemented as an easy-to-use, plug-and-play PyTorch library. It allows users to insert 3D trans-formations, either as parameters or activations, into a com-putation graph, just as regular tensors; backpropagation is taken care of transparently. Our 3D transformation objects expose an interface similar to the Tensor object, support-ing arbitrary batch shapes, indexing, and reshaping opera-tions.
Our contributions are two-fold. First, we introduce a new method of auto-differentiation involving 3D transformation groups by performing backpropagation in the tangent space.
To our knowledge, this is the ﬁrst time backpropagation is performed in the tangent space of Lie groups for training neural networks. Second, we introduce LieTorch, an open-source, easy-to-use PyTorch library that implements tangent space backpropagation. We expect our library to be a useful tool for researchers in 3D vision and robotics. 2.