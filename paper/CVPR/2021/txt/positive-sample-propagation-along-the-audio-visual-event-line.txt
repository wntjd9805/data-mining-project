Abstract
Visual and audio signals often coexist in natural environ-ments, forming audio-visual events (AVEs). Given a video, we aim to localize video segments containing an AVE and identify its category. In order to learn discriminative fea-tures for a classiﬁer, it is pivotal to identify the helpful (or positive) audio-visual segment pairs while ﬁltering out the irrelevant ones, regardless whether they are synchronized or not. To this end, we propose a new positive sample prop-agation (PSP) module to discover and exploit the closely related audio-visual pairs by evaluating the relationship within every possible pair.
It can be done by construct-ing an all-pair similarity map between each audio and vi-sual segment, and only aggregating the features from the pairs with high similarity scores. To encourage the net-work to extract high correlated features for positive sam-ples, a new audio-visual pair similarity loss is proposed.
We also propose a new weighting branch to better exploit the temporal correlations in weakly supervised setting. We perform extensive experiments on the public AVE dataset and achieve new state-of-the-art accuracy in both fully and weakly supervised settings, thus verifying the effectiveness of our method. 1.

Introduction
Recent literature has shown that by fusing multi-modality information can lead to better deep feature pre-sentation, i.e., audio-visual fusion [2] and text-visual fu-sion [20]. However, building a large scale multi-modality pre-training datasets would require heavy manual labours to clean and annotate the raw video sets. To relief the manual labour, recent work either focuses on learning from noise supervision [5, 12] or tries to automatically ﬁlter out un-paired samples [28].
The task of Audio-Visual Event (AVE) localization [28]
*Corresponding author.
Figure 1. An illustration of the AVE localization task. Each video segment is composed of an audio and a visual component. In this example, the “hum” of the bus exists in all the segments (audio modality), but the visual images of the “bus” only appear in the third and fourth segments (visual modality). So only these two segments (red boxes) are localized as (bus) event, the remaining are recognized as background. is served for the latter purpose. An AVE often refers as an event that is both audible and visible in a video segment, i.e., a sound source appears in an image (visible) while the source of the sound also exists in audio portion (audible).
As shown in Fig. 1, a bus humming is an AVE in the third and fourth segments as we can see a bus and hear it hum-ming simultaneously in these video segments. The AVE localization task is to ﬁnd these video segments that contain an audio-visual event and classify it into a certain category1.
There are two relations that need to be considered in the AVE task: intra-modal relations and cross-modal re-lations. The former often addresses temporal relations in one single modality while the later also takes audio and vi-sual relations into account. The pioneer work [14, 28] of-ten tries to regress the class by concatenating features from synchronized audio-visual pairs. Since these methods do not explicitly consider the intra-modal or cross-modal re-lations, their accuracy is often unsatisfying. The follow-1Note that there is a fundamental difference between the Multimedia
Event Detection (MED) task and the AVE localization task: MED is a retrieval task that aims to ﬁnd video clips that are associated with a partic-ular event from a video archive while AVE localization is a classiﬁcation problem. 8436
ing works [27, 30, 31, 32] utilize a self-attention mech-anism to explicitly encode the temporal relations within intra-modality and some of them [21, 22, 31, 32] also aggre-gate better audio-visual feature representations by encoding cross-modal relations. However, these methods often ig-nore the interference caused by irrelevant audio-visual seg-ment pairs during the fusion process. In this paper, we argue that by only aggregating features from positive samples, i.e., high-relevant audio-visual pairs, we can have better AVE lo-calization accuracy.
Speciﬁcally, we propose a new Positive Sample Propaga-tion (PSP) module. In a nutshell, PSP ﬁrst constructs an all-pair similarity map between each audio and visual segment and cuts off the entries that are below a pre-set similarity threshold, and then aggregates the audio and visual features without considering the negative and weak entries in an on-line fashion. Through various visualizations we show that
PSP allows more relevant features that are not necessarily synchronized to be aggregated in an online fashion.
Apart from PSP that can be used in both fully and weakly supervised settings (refer Sec. 3 for the setting details), we further propose two improvements that work under each set-ting, respectively. On the one hand, an audio-visual pair similarity loss is introduced under the fully supervised set-ting that encourages the network to learn high correlated features of audio and visual segments if they belong to the same event. On the other hand, we propose a weighting branch in the weakly supervised setting, which gives tem-poral weights to the segment features.
We evaluate our method on the standard AVE dataset [28]. We show that the proposed techniques con-sistently beneﬁt our system and when combined allow us to achieve state-of-the-art performance under both fully and weakly supervised settings. 2.