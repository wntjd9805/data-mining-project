Abstract 1.

Introduction
State-of-the-art video super-resolution (VSR) methods focus on exploiting inter- and intra-frame correlations to estimate high-resolution (HR) video frames from low-resolution (LR) ones.
In this paper, we study VSR from an exotic perspective, by explicitly looking into the role of temporal frequency of video frames. Through experiments, we observe that a higher frequency, and hence a smaller pixel displacement between consecutive frames, tends to de-liver favorable super-resolved results. This discovery moti-vates us to introduce Event Cameras, a novel sensing de-vice that responds instantly to pixel intensity changes and produces up to millions of asynchronous events per second, to facilitate VSR. To this end, we propose an Event-based
VSR framework (E-VSR), of which the key component is an asynchronous interpolation (EAI) module that reconstructs a high-frequency (HF) video stream with uniform and tiny pixel displacements between neighboring frames from an event stream. The derived HF video stream is then encoded into a VSR module to recover the desired HR videos. Fur-thermore, an LR bi-directional interpolation loss and an HR self-supervision loss are also introduced to respectively reg-ulate the EAI and VSR modules. Experiments on both real-world and synthetic datasets demonstrate that the proposed approach yields results superior to the state of the art.
The goal of video super-resolution (VSR) is to recover a high-resolution (HR) video frame from a sequence of low-resolution (LR) frames. With the prevalence of recent HR display technology, VSR techniques have been attracting in-creasing attention from both the academic and the industrial community. Various applications of VSR include entertain-ment [21], surveillance [26], as well as medical and satellite imaging [8]. Recently, VSR has also been applied to facili-tate high-level vision tasks like action recognition [63].
State-of-the-art VSR techniques have relied on deep neu-ral networks to model intra-frame correlations and inter-frame coherence, so as to recover HR frames. They can be broadly categorized into two streams, the ones based on convolutional neural network (CNN) [53, 50, 24, 61, 18, 13] and those based on recurrent neural network (RNN) [11, 48, 37, 4, 12, 9]. The former category retains temporal informa-tion by concatenating multiple consecutive frames as inputs to produce a single HR estimate. The latter category, on the other hand, relies on recurrent connections to capture the temporal dependencies across a sequence of video frames.
Both categories have demonstrated visually plausible and quantitatively encouraging results.
Unlike existing approaches that focus on spatial and tem-poral dependencies, we study the VSR task from an exotic 7772
!"#$%
!"#
!"#&%
LR Frames
HF Events 
⋯ time time
'"#
E-VSR 
System
VSR 
Module
EAI 
Module
Figure 2. The proposed event-based VSR (E-VSR) system re-ceives low-resolution (LR) videos and high-frequency (HF) event streams as input, and produces high-resolution (HR) video frames.
It comprises a general VSR module as well as a EAI module, which exploits event data to generate asynchronous frames with tiny pixel displacements to facilitate VSR. aspect by exploring the impact of temporal frequency on super-resolved results. We found that a higher frequency or frame rate, and consequently a smaller pixel displace-ment between successive frames, leads to superior super-resolved results. A couple of examples are demonstrated in Fig. 1, where we simulate the varying frame rates of the same videos by sampling frames with different inter-vals from a high-speed camera [46]. We observe that as the frame rate increases, the super-resolved results derived from a state-of-the-art VSR approach [9] also improve, both visu-ally and quantitatively. This is not totally unexpected, since the larger pixel displacements would, intuitively, make the
VSR system harder to capture longer-range temporal depen-dencies and to utilize the contextual information between frames, resulting in inferior results.
Inspired by this discovery, we introduce to the VSR task a novel sensing modality, Event Camera, in the aim to boost the VSR performance by injecting high-frequency (HF) event data into the super-resolving process. In contrast to conventional cameras that capture images at a ﬁxed frame rate, Event Cameras asynchronously respond to intensity changes of each pixel in the microsecond level [5, 32], and produce up to millions of asynchronous events each second.
The generated asynchronous event data, therefore, precisely measure the pixel variants within an extremely short tempo-ral interval. Apart from the very high speed, Event Cameras also offer numerous other beneﬁts, such as high dynamic range and ultra-low power.
We further propose a novel event-based VSR (E-VSR) system that explicitly accounts for event data, as shown in
Fig. 2. It takes as input both a high-frequency (HF) event stream and a regular LR video stream, and then feeds the data of two modalities to a general VSR module along-side with an event-based asynchronous interpolation (EAI) module. The goal of the EAI module is to leverage the
HF event stream to synthesize asynchronous neighboring frames with tiny and uniform RGB pixel displacements in a given video context. Speciﬁcally, within EAI we intro-duce an event-based dynamic conventional layer to handle the spatially- and temporally-varying thresholds in event data. The outputs of EAI are encoded into the VSR module to establish correspondences between consecutive frames.
Moreover, we pose a novel LR bi-directional interpolation loss and an HR self-supervised loss, so as to enforce the consistency between the predicted HR frames and the event stream.
We evaluate the proposed E-VSR on the CED color event dataset [42], where the RGB frames and the corre-sponding color events are collected in a wide range of nat-ural scenes. Due to the novelty of color event data, the
CED dataset is currently the only public dataset that con-tains real color events captured in practical scenarios. To address this issue of limited event data, we build a sim-ulated color event dataset, which is publicly available at https://osf.io/6c3d9/, using an event simula-tor [34, 6]. Experiments on both datasets demonstrate that
E-VSR yields super-resolved results superior to the state of the art, both qualitatively and quantitatively.
In sum, our contribution is a novel scheme that ex-ploits the asynchronous HF event data, delivered by event cameras, to boost the VSR performance.
Its rationale is grounded by the observation that HF streams, and hence smaller pixel displacements, tend to yield favorable VSR re-sults. The proposed E-VSR system generates neighboring frames with tiny and uniform pixel displacements derived from the event streams, which facilitate the establishment of temporal correspondence and further strengthen the VSR process. Both quantitative and qualitative results showcase that the proposed E-VSR consistently outperforms the state of the art. 2.