Abstract
Multi-object tracking from RGB-D video sequences is a challenging problem due to the combination of changing viewpoints, motion, and occlusions over time. We observe that having the complete geometry of objects aids in their tracking, and thus propose to jointly infer the complete ge-ometry of objects as well as track them, for rigidly moving objects over time. Our key insight is that inferring the com-plete geometry of the objects signiﬁcantly helps in track-ing. By hallucinating unseen regions of objects, we can ob-tain additional correspondences between the same instance, thus providing robust tracking even under strong change of appearance. From a sequence of RGB-D frames, we detect objects in each frame and learn to predict their complete object geometry as well as a dense correspondence map-ping into a canonical space. This allows us to derive 6DoF poses for the objects in each frame, along with their corre-spondence between frames, providing robust object tracking across the RGB-D sequence. Experiments on both synthetic and real-world RGB-D data demonstrate that we achieve state-of-the-art performance on dynamic object tracking.
Furthermore, we show that our object completion signiﬁ-cantly helps tracking, providing an improvement of 6.5% in mean MOTA. 1.

Introduction
Understanding how objects move over time is funda-mental towards higher-level perception of real-world envi-ronments, with applications ranging from mixed reality to robotic perception. In the context of static scenes, signiﬁ-cant progress has been made in RGB-D tracking and recon-struction [22, 17, 23, 32, 5, 9]; however, the assumption of a static environment signiﬁcantly limits applicability to real-world environments which are often dynamic, with objects 6071
moving over time. In the case of scenes where a number of objects might be rigidly moving, robust tracking remains a signiﬁcant challenge, as views and occlusion patterns of the objects can change appreciably over time.
Several approaches have been developed to address the problem of dynamic object tracking in RGB-D sequences by detecting objects and then ﬁnding correspondences be-tween frames [24, 25, 34]. While results have shown no-table promise, these methods only consider the observed geometry of the objects, and so tracking objects under faster object or camera motion can result in insufﬁcient overlap of observed geometry to ﬁnd reliable correspondences, result-ing in tracking failure.
To address these challenges, we observe that humans can effectively track objects by leveraging prior knowledge of the underlying object geometry, which helps to constrain the problem even under notable view changes or signiﬁcant occlusions. Thus, our key idea is to learn to ‘see behind objects’ by hallucinating the complete object geometry in order to aid object tracking. We learn to jointly infer for each object its complete geometry as well dense tracking correspondences, providing 6DoF poses for the objects for each frame.
From an RGB-D sequence, we formulate an end-to-end approach to detect objects, characterized by their 3D bound-ing boxes, then predict for each object its complete geome-try as well as a dense correspondence mapping to its canon-ical space. We then leverage a differentiable pose optimiza-tion based on the predicted correspondences of the complete object geometry to provide the object poses per frame as well as their correspondence within the frames.
Our experiments show that our joint object completion and tracking provides notably improved performance over state of the art by 6.5% in MOTA. Additionally, our ap-proach provides encouraging results for scenarios with chal-lenging occlusions. We believe this opens up signiﬁcant potential for object-based understanding of real-world envi-ronments. 2.