Abstract
Monocular 3D reconstruction of articulated object cat-egories is challenging due to the lack of training data and the inherent ill-posedness of the problem. In this work we use video self-supervision, forcing the consistency of con-secutive 3D reconstructions by a motion-based cycle loss.
This largely improves both optimization-based and learning-based 3D mesh reconstruction. We further introduce an in-terpretable model of 3D template deformations that controls a 3D surface through the displacement of a small number of local, learnable handles. We formulate this operation as a structured layer relying on mesh-laplacian regularization and show that it can be trained in an end-to-end manner.
We ﬁnally introduce a per-sample numerical optimisation approach that jointly optimises over mesh displacements and cameras within a video, boosting accuracy both for training and also as test time post-processing.
While relying exclusively on a small set of videos col-lected per category for supervision, we obtain state-of-the-art reconstructions with diverse shapes, viewpoints and textures for multiple articulated object categories. Sup-plementary materials, code, and videos are provided on the project page: https://fkokkinos.github.io/ video_3d_reconstruction/. 1.

Introduction
Monocular 3D reconstruction of general articulated cate-gories is a task that humans perform routinely, but remains challenging for current computer vision systems. The break-throughs achieved for humans [3, 17, 10, 47, 29, 21, 30, 16, 4] have relied on expressive articulated shape priors [26] and mocap recordings to provide strong supervision in the form of 3D joint locations. Still, for general articulated categories, such as horses or cows, the problem remains in its infancy due to both the lack of strong supervision [55] and the in-herent challenge of representing and learning articulated deformations for general categories.
Recent works have started tackling this problem by re-Figure 1: We tackle the problem of monocular 3D recon-struction for articulated object categories by guiding the deformation of a mesh template (top) through a sparse set of 3D control points regressed by a network (middle). Despite using only weak supervision in the form of keypoints, masks and video-based correspondence our approach is able to cap-ture broad articulations, such as opening wings, as well as motion of the lower limbs and neck (bottom). lying on minimal, 2D-based supervision such as manual keypoint annotations or masks [43] and learning morphable model priors [43, 19, 18, 9] or hand-crafted mesh segmen-tations [23]. In this work we leverage the rich information available in videos, and use networks trained for the 2D tasks of object detection, semantic segmentation, and optical ﬂow to complement (optional) 2D keypoint-level supervision.
We make three contributions towards pushing the en-velope of monocular 3D object category reconstruction, by injecting ideas from structure-from-motion (SFM), geometry processing and bundle adjustment in the task of monocular 3D articulated reconstruction.
Firstly, we draw inspiration from 3D vision which has traditionally relied on motion information for SFM [38, 11],
SLAM [20, 28] or Non-Rigid SFM [39, 8, 7]. These category-agnostic techniques interpret 2D point trajectories in terms of an underlying 3D scene and a moving camera. In this work we use the same principle to supervise monocular 11737
Figure 2: Training overview: Two consecutive frames are separately processed by a network that estimates the camera pose, deformation and UV texture parameters. The network regresses per frame a mesh V∗ by estimating offsets to the handles H of the template shape and consequently solving the respective Laplacian optimization problem. The predictions are supervised by per-frame losses on masks, appearance, and optionally keypoints as well as a novel, intra-frame, motion-based loss that compares the predictions of an optical ﬂow network to the mesh-based prediction of pixel displacements (‘mesh ﬂow’). 3D category reconstruction, effectively allowing us to lever-age video as a source of self-supervision. In particular we establish dense correspondences between consecutive video frames through optical ﬂow and force the back projections of the respective 3D reconstructions to be consistent with the optical ﬂow results. This loss can be back-propagated through the 3D lifting pipeline, allowing us to supervise both the camera pose estimation and mesh reconstruction modules through video. Beyond coming for free, this su-pervision also ensures that the resulting models will exhibit a smaller amount of jitter and be more ﬂexible when pro-cessing videos, since the motion-based loss can penalize inconsistencies across consecutive frames and failure to co-vary with moving object parts.
Secondly, we introduce a model for regularised mesh deformations that allows for learnable, part-level mesh con-trol and is back-propagateable, providing us with a drop-in replacement to the common morphable model paradigm adopted in [18]. For this we rely on the Laplacian surface deformation algorithm [34], commonly used in geometry processing to deform a template mesh through a set of con-trol points (‘handles’) while preserving the surface structure and details. We observe that the result of this optimization-based algorithm is differentiable in its inputs, i.e. can be used as a structured layer, while incurring no additional cost at test time since the expression for the optimum can be folded within a linear layer. We incorporate this operation as the top layer of a deep network tasked with regressing the position of the control points given an RGB image. Our results show that we can learn meaningful control points that allow us to capture limb articulations while also providing a human-interpretable interface that enables manual post-processing and reﬁnement using any available 3D software.
Thirdly, we adopt an optimization-based approach to 3D reconstruction that is inspired from bundle adjustment [40]: given a video, we use the ‘bottom-up’ reconstructions of con-secutive frames delivered by our CNN in terms of cameras and handle positions as the initialisation for a numerical op-timisation algorithm. We then jointly optimise the per-frame mask and/or keypoint reprojection losses, and video-level motion consistency losses with respect to the cameras and handle variables, giving a ‘top-down’ reﬁnement of our so-lution that better matches the image evidence. We show that this improves the results at test-time based on whatever image evidence can be obtained without manual annotation.
We evaluate our approach on 3D shape, pose and texture reconstruction on a range of object categories that exhibit challenging articulations. Our ablation highlights the im-portance of the employed self-supervised losses and the tolerance of our method to the number of learnable handles, while both our qualitative and quantitative results indicate that our method largely outperforms recent approaches. 2.