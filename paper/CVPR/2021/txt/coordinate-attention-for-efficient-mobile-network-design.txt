Abstract
Recent studies on mobile network design have demon-strated the remarkable effectiveness of channel atten-tion (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the posi-tional information, which is important for generating spa-tially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embed-ding positional information into channel attention, which we call “coordinate attention”. Unlike channel attention that transforms a feature tensor to a single feature vec-tor via 2D global pooling, the coordinate attention factor-izes channel attention into two 1D feature encoding pro-cesses that aggregate features along the two spatial di-rections, respectively.
In this way, long-range dependen-cies can be captured along one spatial direction and mean-while precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complemen-tarily applied to the input feature map to augment the rep-resentations of the objects of interest. Our coordinate at-tention is simple and can be ﬂexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and
EfﬁcientNet with nearly no computational overhead. Exten-sive experiments demonstrate that our coordinate attention is not only beneﬁcial to ImageNet classiﬁcation but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is avail-able at https://github.com/Andrew- Qibin/
CoordAttention. 1.

Introduction
Attention mechanisms, used to tell a model “what” and
“where” to attend, have been extensively studied [47, 29] and widely deployed for boosting the performance of mod-ern deep neural networks [18, 44, 3, 25, 10, 14]. How-ever, their application for mobile networks (with limited model size) signiﬁcantly lags behind that for large networks
Figure 1. Performance of different attention methods on three clas-sic vision tasks. The y-axis labels from left to right are top-1 ac-curacy, mean IoU, and AP, respectively. Clearly, our approach not only achieves the best result in ImageNet classiﬁcation [33] against the SE block [18] and CBAM [44] but performs even better in down-stream tasks, like semantic segmentation [9] and COCO object detection [21]. Results are based on MobileNetV2 [34].
[36, 13, 46]. This is mainly because the computational over-head brought by most attention mechanisms is not afford-able for mobile networks.
Considering the restricted computation capacity of mo-bile networks, to date, the most popular attention mech-anism for mobile networks is still the Squeeze-and-Excitation (SE) attention [18]. It computes channel atten-tion with the help of 2D global pooling and provides no-table performance gains at considerably low computational cost. However, the SE attention only considers encoding inter-channel information but neglects the importance of positional information, which is critical to capturing object structures in vision tasks [42]. Later works, such as BAM
[30] and CBAM [44], attempt to exploit positional informa-tion by reducing the channel dimension of the input tensor and then computing spatial attention using convolutions as shown in Figure 2(b). However, convolutions can only cap-ture local relations but fail in modeling long-range depen-dencies that are essential for vision tasks [48, 14].
In this paper, beyond the ﬁrst works, we propose a novel and efﬁcient attention mechanism by embedding positional information into channel attention to enable mobile net-works to attend over large regions while avoiding incur-ring signiﬁcant computation overhead. To alleviate the po-sitional information loss caused by the 2D global pooling, we factorize channel attention into two parallel 1D feature encoding processes to effectively integrate spatial coordi-13713
nate information into the generated attention maps. Speciﬁ-cally, our method exploits two 1D global pooling operations to respectively aggregate the input features along the ver-tical and horizontal directions into two separate direction-aware feature maps. These two feature maps with embed-ded direction-speciﬁc information are then separately en-coded into two attention maps, each of which captures long-range dependencies of the input feature map along one spa-tial direction. The positional information can thus be pre-served in the generated attention maps. Both attention maps are then applied to the input feature map via multiplication to emphasize the representations of interest. We name the proposed attention method as coordinate attention as its op-eration distinguishes spatial direction (i.e., coordinate) and generates coordinate-aware attention maps.
Our coordinate attention offers the following advantages.
First of all, it captures not only cross-channel but also direction-aware and position-sensitive information, which helps models to more accurately locate and recognize the objects of interest. Secondly, our method is ﬂexible and light-weight, and can be easily plugged into classic build-ing blocks of mobile networks, such as the inverted resid-ual block proposed in MobileNetV2 [34] and the sandglass block proposed in MobileNeXt [49], to augment the fea-tures by emphasizing informative representations. Thirdly, as a pretrained model, our coordinate attention can bring signiﬁcant performance gains to down-stream tasks with mobile networks, especially for those with dense predic-tions (e.g., semantic segmentation), which we will show in our experiment section.
To demonstrate the advantages of the proposed approach over previous attention methods for mobile networks, we conduct extensive experiments in both ImageNet classiﬁ-cation [33] and popular down-stream tasks, including ob-ject detection and semantic segmentation. With a compa-rable amount of learnable parameters and computation, our network achieves 0.8% performance gain in top-1 classiﬁ-cation accuracy on ImageNet. In object detection and se-mantic segmentation, we also observe signiﬁcant improve-ments compared to models with other attention mechanisms as shown in Figure 1. We hope our simple and efﬁcient design could facilitate the development of attention mecha-nisms for mobile networks in the future. 2.