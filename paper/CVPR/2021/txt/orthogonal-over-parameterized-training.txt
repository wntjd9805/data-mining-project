Abstract
The inductive bias of a neural network is largely deter-mined by the architecture and the training algorithm. To achieve good generalization, how to effectively train a neural network is of great importance. We propose a novel orthogo-nal over-parameterized training (OPT) framework that can provably minimize the hyperspherical energy which charac-terizes the diversity of neurons on a hypersphere. By main-taining the minimum hyperspherical energy during train-ing, OPT can greatly improve the empirical generalization.
Speciﬁcally, OPT ﬁxes the randomly initialized weights of the neurons and learns an orthogonal transformation that applies to these neurons. We consider multiple ways to learn such an orthogonal transformation, including unrolling or-thogonalization algorithms, applying orthogonal parame-terization, and designing orthogonality-preserving gradient descent. For better scalability, we propose the stochastic
OPT which performs orthogonal transformation stochasti-cally for partial dimensions of neurons. Interestingly, OPT reveals that learning a proper coordinate system for neurons is crucial to generalization. We provide some insights on why OPT yields better generalization. Extensive experiments validate the superiority of OPT over the standard training. 1.

Introduction
The inductive bias encoded in a neural network is gen-erally determined by two major aspects: how the neural network is structured (i.e., network architecture) and how the neural network is optimized (i.e., training algorithm). For the same network architecture, using different training algo-rithms could lead to a dramatic difference in generalization performance [36, 60] even if the training loss is close to zero, implying that different training procedures lead to different inductive biases. Therefore, how to effectively train a neural network that generalize well remains an open challenge.
Recent theories [16, 15, 34, 45] suggest the importance of over-parameterization in linear neural networks. For example, [16] shows that optimizing an underdetermined quadratic objective over a matrix M with gradient descent
Figure 1: Overview of the orthogonal over-parameterized training frame-work. OPT learns an orthogonal transformation for each layer in the neural network, while keeping the randomly initialized neuron weights ﬁxed. on a factorization of M leads to an implicit regularization that may improve generalization. There is also strong em-pirical evidence [11, 51] that over-parameterzing the con-volutional ﬁlters under some regularity is beneﬁcial to gen-eralization. Our paper aims to leverage the power of over-parameterization and explore more intrinsic structural priors in order to train a well-performing neural network.
∈
Motivated by this goal, we propose a generic orthogo-nal over-parameterized training (OPT) framework for neu-ral networks. Different from conventional neural training,
Rd with the mul-OPT over-parameterizes a neuron w tiplication of a learnable layer-shared orthogonal matrix
Rd×d and a ﬁxed randomly-initialized weight vector
R
∈
Rd, and it follows that the equivalent weight for the neu-v
∈ ron is w = Rv. Once each element of the neuron weight v has been randomly initialized by a zero-mean Gaussian distribution [20, 14], we ﬁx them throughout the entire train-ing process. Then OPT learns a layer-shared orthogonal transformation R that is applied to all the neurons (in the same layer). An illustration of OPT is given in Fig. 1. In contrast to standard neural training, OPT decomposes the neuron into an orthogonal transformation R that learns a proper coordinate system, and a weight vector v that con-trols the speciﬁc position of the neuron. Essentially, the of different neurons determine weights the relative positions, while the layer-shared orthogonal ma-trix R speciﬁes the coordinate system. Such a decoupled parameterization enables strong modeling ﬂexibility.
, vn ∈ v1,
· · ·
Rd
{
}
Another motivation of OPT comes from an empirical ob-servation that neural networks with lower hyperspherical 7251
energy generalize better [49]. Hyperspherical energy quanti-ﬁes the diversity of neurons on a hypersphere, and essentially characterizes the relative positions among neurons via this form of diversity. [49] introduces hyperspherical energy as a regularization in the network but do not guarantee that the hyperspherical energy can be effectively minimized (due to the existence of data ﬁtting loss). To address this issue, we leverage the property of hyperspherical energy that it is inde-pendent of the coordinate system in which the neurons live and only depends on their relative positions. Speciﬁcally, we prove that, if we randomly initialize the neuron weight v with certain distributions, these neurons are guaranteed to attain minimum hyperspherical energy in expectation. It follows that OPT maintains the minimum energy during training by learning a coordinate system (i.e., layer-shared orthogonal matrix) for the neurons. Therefore, OPT is able to provably minimize the hyperspherical energy.
We consider several ways to learn the orthogonal trans-formation. First, we unroll different orthogonalization algo-rithms such as Gram-Schmidt process, Householder reﬂec-tion and L¨owdin’s symmetric orthogonalization. Different unrolled algorithms yield different implicit regularizations to construct the neuron weights. For example, symmetric orthogonalization guarantees that the new orthogonal basis has the least distance in the Hilbert space from the original non-orthogonal basis. Second, we consider to use a special parameterization (e.g., Cayley parameterization) to construct the orthogonal matrix, which is more efﬁcient in training.
Third, we consider an orthogonality-preserving gradient de-scent to ensure that the matrix R stays orthogonal after each gradient update. Last, we relax the original optimization problem by making the orthogonality constraint a regular-ization for the matrix R. Different ways of learning the orthogonal transformation may encode different inductive biases. We note that OPT aims to utilize orthogonalization as a tool to learn neurons that maintain small hyperspheri-cal energy, rather than to study a speciﬁc orthogonalization method. Furthermore, we propose a reﬁnement strategy to reduce the hyperspherical energy for the randomly initial-ized neuron weights
. In speciﬁc, we directly minimize the hyperspherical energy of these random weights as a preprocessing step before training them on actual data.
To improve scalability, we further propose the stochastic
OPT that randomly samples neuron dimensions to perform orthogonal transformation. The random sampling process is repeated many times such that each dimension of the neuron is sufﬁciently learned. Finally, we provide some theoretical insights and discussions to justify the effectiveness of OPT.
The advantages of OPT are summarized as follows:
, vn} v1,
· · ·
{
•
OPT is a generic neural network training framework with strong ﬂexibility. There are many different ways to learn the orthogonal transformations and each one imposes a unique inductive bias. Our paper compares how different
•
•
• orthogonalizations may affect generalization in OPT.
OPT is the ﬁrst training framework where the hyperspher-ical energy is provably minimized (in contrast to [49]), leading to better empirical generalization. OPT reveals that learning a proper coordinate system is crucial to gen-eralization, and the hyperspherical energy is sufﬁciently expressive to characterize relative neuron positions.
There is no extra computational cost for the OPT-trained neural network in inference. In the testing stage, it has the same inference speed and model size as the normally trained network. Our experiments also show that OPT performs well on a diverse class of neural networks and therefore is agnostic to different neural architectures.
Stochastic OPT can greatly improve the scalability of
OPT while enjoying the same guarantee to minimize hy-perspherical energy and having comparable performance. 2.