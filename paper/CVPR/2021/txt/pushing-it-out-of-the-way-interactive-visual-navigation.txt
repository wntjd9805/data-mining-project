Abstract
Interactive_Visual_Navigation.
We have observed signiﬁcant progress in visual naviga-tion for embodied agents. A common assumption in study-ing visual navigation is that the environments are static; this is a limiting assumption. Intelligent navigation may involve interacting with the environment beyond just moving for-ward/backward and turning left/right. Sometimes, the best way to navigate is to push something out of the way. In this paper, we study the problem of interactive navigation where agents learn to change the environment to navigate more ef-ﬁciently to their goals. To this end, we introduce the Neural
Interaction Engine (NIE) to explicitly predict the change in the environment caused by the agent’s actions. By model-ing the changes while planning, we ﬁnd that agents exhibit signiﬁcant improvements in their navigational capabilities.
More speciﬁcally, we consider two downstream tasks in the physics-enabled, visually rich, AI2-THOR environment: (1) reaching a target while the path to the target is blocked (2) moving an object to a target location by pushing it. For both tasks, agents equipped with an NIE signiﬁcantly outperform agents without the understanding of the effect of the ac-tions indicating the beneﬁts of our approach. The code and dataset are available at github.com/KuoHaoZeng/ 1.

Introduction
Embodied AI has witnessed remarkable progress over the past few years owing to advances in learning algorithms, benchmarks, and standardized tasks. A popular task that has received a considerable amount of attention is visual navi-gation [3, 5, 8, 29, 39, 48], where the goal is to navigate towards a speciﬁc coordinate or object within an unseen en-vironment. One of the common implicit assumptions for these navigation methods is that the scene is static, and the agent cannot interact with the objects to change their pose.
Consider the scenario that the path of the agent towards the target location is blocked by an obstacle (e.g., a chair) as shown in Fig. 1 (top). To reach the target, the agent has to move the obstacle out of the way. Therefore, planning for reaching the target requires not only understanding the outcome of agent actions but also the dynamics of agent-object interactions. There are many factors such as object size, spatial relationship with other objects in the scene, and reaction of the object to the applied forces, that inﬂuence the outcome of the interaction with the object. Hence, long-9868
horizon planning for navigation conditioned on the object dynamics offers unique challenges that are often overlooked in the recent navigation literature.
The ﬁrst challenge is to learn whether an action affects the pose of an object or not. Navigation actions (e.g., rotate right or move ahead) typically do not affect the position of objects in the world coordinate frame while interaction ac-tions (e.g., pushing an object) can change the object pose.
The objects move in the ego-centric view of the agent due to agent movements or interaction with objects. Learning how objects move as a result of camera motion or interaction im-poses the second challenge. Learning how to interact with objects is another challenge. For example, the agent should learn that pushing an object against a wall does not change its pose.
In this paper, we propose a novel model for navigation while interacting with objects within a scene that jointly plans a sequence of actions and predicts the changes in the scene conditioned on those actions. More speciﬁcally, the model includes a Neural Interaction Engine (NIE) module that predicts the afﬁne transformation of objects from the perspective of the agent conditioned on the actions. The goal is to learn if/how the actions affect the pose of the ob-jects. The NIE module receives gradients for not only the prediction of the pose in the next frame but also the naviga-tion policy.
We evaluate our model on two downstream tasks Ob-sNav and ObjPlace. The goal of ObsNav is to reach a spe-ciﬁc coordinates in a scene while the paths from the initial location of the agent to the target are blocked by objects.
The goal of ObjPlace is to push an object on the ﬂoor while navigating so it reaches a target point. These are challeng-ing tasks since the agent requires an accurate understand-ing of the dynamics of the objects and their interaction with other objects in the scene. We perform our experiments in 120 scenes of the physics-enabled AI2-THOR [19] environ-ment. Our experiments show signiﬁcant improvement over baselines that are not capable of explicitly predicting the ef-fect of interactions showing the merit of our NIE model.
In summary, we highlight three primary contributions. (1) We propose Neural Interaction Engine, as a model for predicting the state of the observed objects conditioned on (2) We propose new datasets for two the agent actions. navigation-based tasks using a physics-enabled framework, which enables changing the pose of objects and models rich object-object and agent-object interactions. (3) We show that predicting the outcome of actions is a crucial capability for embodied agents by showing signiﬁcant improvements over baselines that do not possess this capability. 2.