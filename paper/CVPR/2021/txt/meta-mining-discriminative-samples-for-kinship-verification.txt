Abstract
Kinship veriﬁcation aims to ﬁnd out whether there is a kin relation for a given pair of facial images. Kinship ver-iﬁcation databases are born with unbalanced data. For a database with N positive kinship pairs, we naturally obtain
N (N − 1) negative pairs. How to fully utilize the limit-ed positive pairs and mine discriminative information from sufﬁcient negative samples for kinship veriﬁcation remain-s an open issue. To address this problem, we propose a
Discriminative Sample Meta-Mining (DSMM) approach in this paper. Unlike existing methods that usually construct a balanced dataset with ﬁxed negative pairs, we propose to utilize all possible pairs and automatically learn discrim-inative information from data. Speciﬁcally, we sample an unbalanced train batch and a balanced meta-train batch for each iteration. Then we learn a meta-miner with the meta-gradient on the balanced meta-train batch. In the end, the samples in the unbalanced train batch are re-weighted by the learned meta-miner to optimize the kinship mod-els. Experimental results on the widely used KinFaceW-I, KinFaceW-II, TSKinFace, and Cornell Kinship datasets demonstrate the effectiveness of the proposed approach. 1.

Introduction
Facial appearance conveys valuable information, such as identity [7, 23], age [19, 46], gender [17, 33], emotion-s [16, 44], social relation [18, 54], and so on. Recently, a variety of efforts [15, 21, 25, 32, 45] have been devoted to kinship veriﬁcation, which predicts the existence of kinship for a given pair of facial images. As an emerging task, it has many potential applications including missing children search [25], intelligent family album organization [15, 29], and social media analysis [49, 58].
Existing kinship databases organize data in terms of pos-∗ Corresponding author (a) Existing Method (b) Our Method
Figure 1. The key idea of our method. (a) The existing method constructs a ﬁxed database by sampling the equal size of nega-tive samples. Then a kinship model is learned with the balanced database. (b) Our method utilizes all possible pairs and does not discard any negative samples. Speciﬁcally, we sample an unbal-anced train batch and a balanced meta-train batch for each itera-tion. A meta-miner is introduced to mine the samples in the train-ing batch. Our method alternately optimizes the kinship model and the proposed meta-miner via a meta-learning framework . itive samples. For example, KinFaceW-I and KinFaceW-II [25] datasets collect parent-child image pairs from the
Internet search. Assuming that a kinship database contains
N positive kinship pairs, the negative samples are generated by combining all unrelated parent-child image pairs. There-fore, we obtain N (N − 1) negative samples, which is far more than the number of positive pairs. Existing methods usually randomly sample ﬁxed N negative samples to con-struct a balanced database. However, this strategy simply 16135
ignores the remaining N (N − 2) negative samples leading to overﬁtting. Besides, the real decision boundary cannot be well learned with randomly selected N negative samples.
One simple strategy to address this issue is to sample a bal-anced batch from positive samples and all possible negative pairs separately. Nevertheless, most negative pairs are easy samples and they contribute little to the network training.
In this work, we investigate how to mine discriminative information from limited positive pairs and sufﬁcient neg-ative samples and propose a Discriminative Sample Meta-Mining (DSMM) strategy via a meta-learning framework.
Speciﬁcally We ﬁrst randomly sample an unbalanced train batch from all possible pairs with a positive to negative ratio of 1 : C, where C > 1. Then we aim to mine the discrim-inative samples by re-weighting these samples in the train-ing batch.
Instead of manually selecting ﬁxed weighting functions, we consider automatically learning the weight-ing functions from data. Encouraged by the recent success of meta-learning [9, 41, 43], we introduce a meta-miner net-work, which predicts the weight for each sample and is op-timized with the meta-gradient. Concretely, another bal-anced meta-train batch is sampled from all possible pairs.
Then we optimize the kinship model and meta-miner net-work alternately with these two batches. After one-step-forward optimization for the kinship model on the unbal-anced train batch, we train the meta-miner network on the balanced meta-train batch with one-step-forward parame-ters of the kinship model. In the end, we perform the real optimization for the kinship model with the sample weight-s generated by the updated meta-miner network. Figure 1 shows the key idea of our method. The contributions of this paper are summarized as follows:
• To the best of our knowledge, the proposed discrimina-tive sample meta-mining approach is the ﬁrst attempt to fully utilize the unbalanced data of kinship databas-es via a meta-learning framework.
• Our method proposes to simultaneously sample an un-balanced train batch and a balanced meta-train batch, and then perform sample mining on the training batch with the guidance of a balanced meta-train batch.
• Extensive experiments on four widely used kinship databases illustrate that the proposed method achieves state-of-the-art results. 2.