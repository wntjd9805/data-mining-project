Abstract
We introduce a meta-regularization framework for learning-based image registration. Current learning-based image registration methods use high-resolution architec-tures such as U-Nets to produce spatial transformations, and impose simple and explicit regularization on the output of the network to ensure that the estimated displacements are smooth. While this approach works well on small de-formations, it has been known to struggle when the defor-mations are large. Our method uses a more advanced form of meta-regularization to increase the generalization ability of learned registration models. We motivate our approach based on Reproducing Kernel Hilbert Space (RKHS) theory, and approximate that framework via a meta-regularization convolutional layer with radially symmetric, positive semi-deﬁnite ﬁlters that inherent its regularization properties. We then provide a method to learn such regularization ﬁlters while also learning to register. Our experiments on syn-thetic and real datasets as well as ablation analysis show that our method can improve anatomical correspondence compared to competing methods, and reduce the percentage of folding and tear in the large deformation setting, reﬂect-ing better regularization and model generalization. 1.

Introduction
Deformable image registration is the process of trans-forming images to be in anatomical correspondence with
It is a critical step in many applications in one another. medical imaging including segmentation, diagnosis, and computational anatomy. In the traditional approach to im-age registration, the transformation is obtained by solving an optimization problem over a space of transformations based on image similarity [1]. This optimization problem is solved for a speciﬁc pair, and no generalization is ob-tained throughout this process. This demands repeating the optimization process for any new pair of images which is inefﬁcient and time-consuming.
Recent advances in deep learning have enabled a paradigm shift in the ﬁeld. With the introduction of differ-entiable Spatial Transformer layers [2] and the development of advanced architectures such as U-Nets [3] and Fully Con-volutional Networks (FCNs) [4], a new level of abstraction has become achievable. Instead of optimizing for every in-put image pair, a deep model can be trained to learn how to register in an end-to-end fashion [5]. Once a model is ob-tained, it can be used to infer dense spatial transformations between new pairs in a single forward pass which is much faster than running an optimizer.
As variations in anatomical structures are usually local and non-rigid, the registration we seek is deformable, and the transformation model must have many degrees of free-dom to capture the local changes with high resolution [6].
Therefore, one of the most challenging parts of the regis-tration process is ensuring that the estimated spatial trans-formation between images is smooth and realistic, and that the high degrees of freedom don’t result in overﬁtting. Fur-thermore, the choice of the regularization function has fun-damental consequences on the class of transformations that the model can produce, and is therefore key to building a successful model. Most learning-based registration meth-ods impose an explicit and generic regularization loss on the output of the registration function, which is usually a penalty on the ﬁrst order derivative of the displacement ﬁeld
[7, 8, 9, 10, 11, 12]. We ﬁnd this approach to be too lim-ited, as it presumes the same simple smoothness properties regardless of image content and permissible deformations, which is not realistic. For example, some transformation models must preserve discontinuities in the case of sliding organs, but would not accurately apply elsewhere [13].
Furthermore, many state of the art learning-based image registration networks leverage the high-resolution proper-ties of U-Nets for good performance [7, 14, 11, 12]. How-ever, this type of architecture, while good in retaining spa-tial resolution, struggles with datasets that have large de-formations [12][15][16]. We have also observed this in practice through controlled experiments, as we will show in 5.4.1. Motivated to solve these two important problems, we propose a meta-regularization framework for learning-based image registration models. Speciﬁcally, we ﬁrst build on the rich framework of Reproducing Kernel Hilbert Space 110928
(RKHS) theory. As this framework is computationally ex-pensive, we then show how in some special cases it can be simpliﬁed to a convolution operation that can approximately retain its regularization properties. Once we establish this, we propose a simple method to train a meta-regularization convolutional layer in registration networks to learn how to regularize, thereby increasing the model’s generalizability.
Our main contributions are: 1. We introduce a meta-regularization framework to in-crease the generalization ability of learning-based de-formable image registration methods. 2. We draw connections between regularizing displace-ment ﬁelds through (a) applying differential operators and (b) convolving with radially symmetric, rank-1, positive semi-deﬁnite (PSD) ﬁlters. 3. We propose a method to train convolution ﬁlters in a registration network to have spatial regularization properties. 4. We demonstrate that this method can both improve anatomical correspondence and reduce the folding and tear in the predicted displacement ﬁelds, especially in the large deformation setting. 2.