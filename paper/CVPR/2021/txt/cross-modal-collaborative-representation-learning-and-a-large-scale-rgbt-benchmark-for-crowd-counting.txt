Abstract
Crowd counting is a fundamental yet challenging task, which desires rich information to generate pixel-wise crowd density maps. However, most previous methods only used the limited information of RGB images and cannot well discover potential pedestrians in unconstrained scenarios.
In this work, we ﬁnd that incorporating optical and ther-mal information can greatly help to recognize pedestrians.
To promote future researches in this ﬁeld, we introduce a large-scale RGBT Crowd Counting (RGBT-CC) bench-mark, which contains 2,030 pairs of RGB-thermal images with 138,389 annotated people. Furthermore, to facili-tate the multimodal crowd counting, we propose a cross-modal collaborative representation learning framework, which consists of multiple modality-speciﬁc branches, a modality-shared branch, and an Information Aggregation-Distribution Module (IADM) to capture the complementary information of different modalities fully. Speciﬁcally, our
IADM incorporates two collaborative information transfers to dynamically enhance the modality-shared and modality-speciﬁc representations with a dual information propaga-tion mechanism. Extensive experiments conducted on the
RGBT-CC benchmark demonstrate the effectiveness of our framework for RGBT crowd counting. Moreover, the pro-posed approach is universal for multimodal crowd count-ing and is also capable to achieve superior performance on the ShanghaiTechRGBD [22] dataset. Finally, our source code and benchmark have been released at http:// lingboliu.com/RGBT_Crowd_Counting.html. 1.

Introduction
Crowd counting [18, 10] is a fundamental computer vi-sion task that aims to automatically estimate the number of people in unconstrained scenes. Over the past decade, this task has attracted a lot of research interests due to its huge
*The corresponding author is Liang Lin. Lingbo Liu and Jiaqi Chen share ﬁrst-authorship. application potentials (e.g., trafﬁc management [62, 28] and video surveillance [52]). During the recent COVID-19 pan-demic [47], crowd counting has also been employed widely for social distancing monitoring [11].
In the literature, numerous models [64, 43, 27, 56, 1, 21, 26, 34, 30, 32] have been proposed for crowd counting.
Despite substantial progress, it remains a very challenging problem that desires rich information to generate pixel-wise crowd density maps. However, most previous methods only utilized the optical information extracted from RGB images and may fail to accurately recognize the semantic objects in unconstraint scenarios. For instance, as shown in Fig. 1-(a,b), pedestrians are almost invisible in poor illumination conditions (such as backlight and night) and they are hard to be directly detected from RGB images. Moreover, some human-shaped objects (e.g., tiny pillars and blurry trafﬁc lights) have similar appearances to pedestrians [59] and they are easily mistaken for people when relying solely on opti-cal features. In general, RGB images cannot guarantee the high-quality density maps, and more comprehensive infor-mation should be explored for crowd counting.
Fortunately, we observe that thermal images can greatly facilitate distinguishing the potential pedestrians from clut-tered backgrounds. Recently, thermal cameras have been extensively popularized due to the COVID-19 pandemic, which increases the feasibility of thermal-based crowd counting. However, thermal images are not perfect. As shown in Fig. 1-(c,d), some hard negative objects (e.g., heat-ing walls and lamps) are also highlighted in thermal images, but they can be eliminated effectively with the aid of optical information. Overall, RGB images and thermal images are highly complementary. To the best of our knowledge, no attempts have been made to simultaneously explore RGB and thermal images for estimating the crowd counts. In this work, to promote further researches of this ﬁeld, we propose a large-scale benchmark “RGBT Crowd Counting (RGBT-CC)”, which contains 2,030 pairs of RGB-thermal images and 138,389 annotated pedestrians. Moreover, our bench-mark makes signiﬁcant advances in terms of diversity and difﬁculty, as these RGBT images were captured from un-4823
Figure 1. Visualization of RGB-thermal images in our RGBT-CC benchmark. When only using optical information of RGB images, we cannot effectively recognize pedestrians in poor illumination conditions, as shown in (a) and (b). When only utilizing thermal images, some heating negative objects are hard to be distinguished, as shown in (c) and (d). constrained scenes (e.g., malls, streets, train stations, etc.) with various illumination (e.g., day and night).
Nevertheless, capturing the complementarities of multi-modal data (i.e., RGB and thermal images) is non-trivial.
Conventional methods [22, 67, 37, 15, 54, 46] either feed the combination of multimodal data into deep neural net-works or directly fuse their features, which could not well exploit the complementary information.
In this work, to facilitate the multimodal crowd counting, we intro-duce a cross-modal collaborative representation learning framework, which incorporates multiple modality-speciﬁc branches, a modality-shared branch, and an Information
Aggregation-Distribution Module (IADM) to fully capture the complementarities among different modalities. Speciﬁ-cally, our IADM is integrated with two collaborative com-ponents, including i) an Information Aggregation Trans-fer that dynamically aggregates the contextual information of all modality-speciﬁc features to enhance the modality-shared feature and ii) an Information Distribution Trans-fer that propagates the modality-shared information to sym-metrically reﬁne every modality-speciﬁc feature for further representation learning. Furthermore, the tailor-designed
IADM is embedded in different layers to learn the cross-modal representation hierarchically. Consequently, the pro-posed framework can generate knowledgeable features with comprehensive information, thereby yielding high-quality crowd density maps.
It is worth noting that our method has three appealing properties. First, thanks to the dual information propaga-tion mechanism, IADM can effectively capture the multi-modal complementarities to facilitate the crowd counting task. Second, as a plug-and-play module, IADM can be easily incorporated into various backbone networks for end-to-end optimization. Third, our framework is universal for multimodal crowd counting. Except for RGBT counting, the proposed method can also be directly applied for RGB-Depth counting. In summary, the major contributions of this work are three-fold:
• We introduce a large-scale RGBT benchmark to pro-mote the research of crowd counting, in which 138,389 pedestrians are annotated in 2,030 pairs of RGB-thermal images captured in unconstrained scenarios.
• We develop a cross-modal collaborative representation learning framework, which is capable of fully learning the complementarities among different modalities with a Information Aggregation-Distribution Module.
• Extensive experiments conducted on two multimodal benchmarks (i.e., RGBT-CC and ShanghaiTechRGBD
[22]) greatly demonstrate that the proposed method is effective and universal for multimodal crowd counting. 2.