Abstract
Estimating 3D scene ﬂow from a sequence of monocu-lar images has been gaining increased attention due to the simple, economical capture setup. Owing to the severe ill-posedness of the problem, the accuracy of current methods has been limited, especially that of efﬁcient, real-time ap-proaches. In this paper, we introduce a multi-frame monoc-ular scene ﬂow network based on self-supervised learning, improving the accuracy over previous networks while re-taining real-time efﬁciency. Based on an advanced two-frame baseline with a split-decoder design, we propose (i) a multi-frame model using a triple frame input and convo-lutional LSTM connections, (ii) an occlusion-aware census loss for better accuracy, and (iii) a gradient detaching strat-egy to improve training stability. On the KITTI dataset, we observe state-of-the-art accuracy among monocular scene
ﬂow methods based on self-supervised learning. 1.

Introduction
Scene ﬂow estimation, that is the task of estimating 3D structure and 3D motion of a dynamic scene, has been re-ceiving increased attention together with a growing interest and demand for autonomous navigation systems. Many ap-proaches have been proposed, based on various input data such as stereo images [3, 27, 38, 57, 68], RGB-D sequences
[21, 37, 50], or 3D point clouds [4, 17, 35, 72].
Recently, monocular scene ﬂow approaches [7, 23, 36, 73] have shown the possibility of estimating 3D scene ﬂow from a pair of temporally consecutive monocular frames only, obviating complicated, expensive sensor setups such as a stereo rig, RGB-D sensors, or a LiDAR scanner. Only a simple, affordable monocular camera is needed. The avail-ability of ground-truth data has been another key challenge for scene ﬂow estimation in general. To address this, meth-ods based on self-supervised learning [23, 36] have shown it possible to train CNNs for jointly estimating depth and scene ﬂow without expensive 3D annotations. Yet, their accuracy is bounded by the limitation of only using two frames as input, their underlying proxy loss, and training in-stabilities due to the difﬁculty of optimizing CNNs for mul-}
}
It−2
It−1
It
It+1
Depth
Conv.
ConvLSTM 3D Scene Flow
Figure 1. Our multi-frame monocular scene ﬂow approach in-puts three frames and estimates depth and scene ﬂow (visualized in 3D [78] with scene ﬂow color coding). tiple tasks, particularly in a self-supervised manner [36].
Semi-supervised methods have demonstrated promising accuracy by combining CNNs with energy minimization [7] or sequentially estimating optical ﬂow and depth to infer 3D scene ﬂow [73]. Those methods, however, do not reach real-time efﬁciency due to iterative energy minimization [7] or the additional processing time from pre-computing depth and optical ﬂow [73] beforehand. Yet, computational efﬁ-ciency is important for autonomous navigation applications.
In this paper, we introduce a self-supervised monocular scene ﬂow approach that substantially advances the previ-ously most accurate real-time method of Hur et al. [23], while keeping its advantages (e.g., computational efﬁciency and training on unlabeled data). We ﬁrst analyze the tech-nical design, revealing some limitations, and propose an improved two-frame backbone network to overcome them.
Next, we introduce a multi-frame formulation that tempo-rally propagates the estimate from the previous time step for more accurate and reliable results in consecutive frames.
Previous monocular methods [7, 23, 36, 73] utilize only two frames as input, which is a minimal setup for demonstrating the underlying ideas. In contrast, our approach is the ﬁrst to demonstrate how to exploit multiple consecutive frames, which are naturally available in most real-world scenarios.
We make the following main contributions: (i) We un-cover some limitations of the baseline architecture of [23] and introduce an advanced two-frame basis with a split-de-coder design. Contradicting the ﬁnding of [23] on using a single joint decoder, our split decoder is not only faster and more stable to train, but delivers competitive accuracy. 2684
(ii) Next, we introduce our multi-frame network based on overlapping triplets of frames as input and temporally prop-agating the previous estimate via a convolutional LSTM
[20, 59] (cf . Fig. 1). (iii) Importantly, we propagate the hid-den states using forward warping, which is especially ben-eﬁcial for handling occlusion; it is also more stable to train than using backward warping in a self-supervised setup. (iv) We propose an occlusion-aware census transform to take occlusion cues into account, providing a more robust measure for brightness difference as a self-supervised proxy loss. (v) Lastly, we introduce a gradient detaching strat-egy that improves not only the accuracy but also the train-ing stability, which self-supervised methods for multi-task learning can beneﬁt from. We successfully validate all our design choices through an ablation study.
Training on KITTI raw [11] in a self-supervised manner, our model improves the accuracy of the direct baseline of
[23] by 15.4%. Owing to its self-supervised design, our ap-proach also generalizes to different datasets. We can option-ally perform (semi-)supervised ﬁne-tuning on KITTI Scene
Flow Training [43, 44], where we also outperform [23], re-ducing the accuracy gap to semi-supervised methods [7, 73] while remaining many times more efﬁcient. 2.