Abstract
Pretrained on Cityscapes
Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them
In this paper, we difﬁcult to generalize to new scenes. propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical ﬂow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncer-tainties. Meanwhile, an online learned photometric uncer-tainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environ-ments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor
TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods. 1.

Introduction
Estimating camera motion from monocular videos plays an essential role in many real-world applications, such as autonomous driving and robotics. This problem is usually solved by visual odometry (VO) or simultaneous localiza-tion and mapping (SLAM). Classic SLAM/VO methods [7, 8, 10, 26] perform well in favorable conditions but often fail in challenging situations (e.g. textureless region, dynamic object) due to the reliance on low-level features and hand-crafted pipeline. Since deep neural networks are able to extract high-level features and infer end-to-end by learning from data, many learning-based VO methods [21, 22, 39, 47] have been proposed to break through the limitations of classic SLAM/VO. Among them, self-supervsied VO methods are able to jointly learn camera pose, depth and optical ﬂow by minimizing photometric error [39], which have shown promising results in recent years.
Test on KITTI
Pretrained on KITTI
Test on TUM
Figure 1. In this paper, we focus on the generalization ability to unseen environments of deep VO. When the test data are different from the training data, previous methods fail to generalize while our method still performs well with very small trajectory error.
However, learning-based VO often fails during inference when the scenes are different from the training data. The inability of pretrained VO to generalize to unseen environ-ments limits its wide applications [21, 44]. To this end, the pretrained networks are required to achieve real-time online adaptation in a self-supervised manner.
As a result, several previous works [3, 21, 44] have been proposed to mitigate the domain generalization problem of stereo matching and VO. However, the performance is still much inferior to classic methods in terms of accuracy and the pretrained networks suffer from slow convergence.
These methods treat VO as a black-box by learning all components (pose, depth, optical ﬂow, etc.) but ignore well-deﬁned geometric computations and optimization methods, which leads to slow convergence during online adaptation. 432113184
Existing deep VO methods predict depth by single-view estimation, which is an ill-posed problem [22]. The learned depth has a strong reliance on the training dataset. During inference, the camera intrinsics, scene layouts and distances are usually different. Meanwhile, the camera pose is learned rather than calculated analytically, which requires favor-able camera motion with sufﬁcient disparity (e.g. KITTI dataset). Therefore, these methods tend to fail when faced with unseen or more complicated motion patterns.
In addition, existing learning-based methods do not explicitly ensure multi-view geometric consistency during inference, which leads to large scale drift in trajectories.
In order to improve the online adaptation of VO to unseen environments, we propose a self-supervised frame-work that combines the advantage of deep learning and geometric computations. The proposed framework utilizes scene-agnostic 3D geometry constraints and Bayesian in-ference formulations to speed up online adaptation. During inference, the single-view depth estimation is used as a prior of the current scene geometry and is continuously improved with incoming observations by a probabilistic
Bayesian updating framework. The reﬁned depth is used as Maximum A Posteriori (MAP) to train DepthNet for better estimation at the next timestep. Instead of predicting pose by PoseNet, our framework solves pose analytically from optical ﬂow and reﬁned depth. Meanwhile, in order to deal with observation noise, the proposed method online learns depth and photometric uncertainties which are used in the depth reﬁnement process and differentiable Gauss-Newton optimization, respectively. Finally, the optimized pose, depth and ﬂow are used for online self-supervision.
Our framework ensures scale consistency by exploiting multi-view geometric constraints. The well-deﬁned scene-agnostic computation helps our VO framework achieve good generalization ability across different scene condi-tions. Our contributions can be summarized as follows:
• We propose a generalizable deep VO that uses scene-agnostic geometric formulation and Bayesian infer-ence to speed up self-supervised online adaptation.
• The predicted depth is continuously reﬁned by a
Bayesian fusion framework, which is further used to train depth and optical ﬂow during online learning.
• We introduce online learned depth and photometric uncertainties for better depth reﬁnement and differen-tiable Gauss-Newton optimization.
Our method achieves much better generalization than state-of-the-art baselines when tested cross different do-mains, including Cityscapes [4] to KITTI [13] and outdoor
KITTI to indoor TUM [31] datasets. Meanwhile, we also achieve state-of-the-art depth estimation results on KITTI and NYUv2 [30] datasets. 2.