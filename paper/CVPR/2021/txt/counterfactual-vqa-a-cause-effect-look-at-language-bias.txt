Abstract
VQA models may tend to rely on language bias as a shortcut and thus fail to sufﬁciently learn the multi-modal knowledge from both vision and language. Recent debias-ing methods proposed to exclude the language prior dur-ing inference. However, they fail to disentangle the “good” language context and “bad” language bias from the whole.
In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of ques-tions on answers and reduce the language bias by subtract-ing the direct language effect from the total causal effect.
Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA back-bones and fusion strategies, 2) achieves competitive per-formance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa. 1.

Introduction
Visual Question Answering (VQA) [8, 4] has become the fundamental building block that underpins many frontier interactive AI systems, such as visual dialog [15], vision-and-language navigation [6], and visual commonsense rea-soning [51]. VQA systems are required to perform visual analysis, language understanding, and multi-modal reason-ing. Recent studies [19, 3, 8, 19, 25] found that VQA mod-els may rely on spurious linguistic correlations rather than multi-modal reasoning. For instance, simply answering
“tennis” to the sport-related questions and “yes” to the ques-tions “Do you see a ...” can achieve approximately 40% and 90% accuracy on the VQA v1.0 dataset. As a result, VQA models will fail to generalize well if they simply memorize the strong language priors in the training data [2, 19], espe-cially on the recently proposed VQA-CP [3] dataset where the priors are quite different in the training and test sets.
One straightforward solution to mitigate language bias is to enhance the training data by using extra annota-tions or data augmentation. In particular, visual [14] and textual [23] explanations are used to improve the visual grounding ability [37, 44]. Besides, counterfactual train-ing samples generation [11, 1, 55, 18, 28] helps to balance the training data, and outperform other debiasing methods by large margins on VQA-CP. These methods demonstrate the effect of debiased training to improve the generaliz-ability of VQA models. However, it is worth noting that
VQA-CP was proposed to validate whether VQA models can disentangle the learned visual knowledge and memo-rized language priors [3]. Therefore, how to make unbiased inference under biased training still remains a major chal-lenge in VQA. Another popular solution [10, 13] is using a separate question-only branch to learn the language prior in the training set. During the test stage, the prior is mitigated by excluding the extra branch. However, we argue that the language prior consists of both “bad” language bias (e.g., binding the color of bananas with the major color “yellow”) and “good” language context (e.g., narrowing the answer space based on the question type “what color”). Simply ex-cluding the extra branch cannot make use of the good con-text. Indeed, it is still challenging for recent debiasing VQA methods to disentangle the good and bad from the whole.
Motivated by counterfactual reasoning and causal ef-fects [29, 30, 31], we propose a novel counterfactual infer-ence framework called CF-VQA to reduce language bias in VQA. Overall, we formulate language bias as the di-rect causal effect of questions on answers, and mitigate the bias by subtracting the direct language effect from the total causal effect. As illustrated in Figure 1, we introduce two scenarios, conventional VQA and counterfactual VQA, to estimate the total causal effect and direct language effect, respectively. These two scenarios are deﬁned as follows:
Conventional VQA: What will answer A be, if machine hears question Q, sees image V , and extracts the multi-modal knowledge K?
Counterfactual VQA: What would A be, if machine hears
Q, but had not extracted K or seen V ? 112700
Biased 
Training
What color are  the bananas?
Debiased 
Inference
What color are  the bananas?
A: Yellow.
A: Yellow.
A: Yellow.
A: Yellow.
A: Green. mostly yellow,  seldom green (pure language effect) (total effect) yellow green white (Imagination) green banana yellow green white
Conventional VQA
Counterfactual VQA
Answer: Yellow.
Answer: Green. yellow green white yellow green white yellow green white yellow green white (traditional strategy) (CF-VQA) mostly yellow,  seldom green green banana language  prior multi-modal knowledge
Figure 1: Our cause-effect look at language bias in VQA. Conventional VQA depicts the fact where machine hears the ques-tion and extracts the multi-modal knowledge. Counterfactual VQA depicts the scenario where machine hears the question but the knowledge is blocked. We subtract the pure language effect from the total effect for debiased inference.
Intuitively, conventional VQA depicts the scenario where both Q and V are available. In this case, we can estimate the total causal effect of V and Q on A. However, conventional
VQA cannot disentangle the single-modal linguistic corre-lation and multi-modal reasoning, i.e., direct and indirect effects. Therefore, we consider the following counterfac-tual question: “What would have happened if the machine had not performed multi-modal reasoning?” The answer to this question can be obtained by imagining a scenario where the machine hears Q, but the multi-modal knowledge
K is intervened under the no-treatment condition, i.e., V and Q had not been accessible. Since the response of K to Q is blocked, VQA models can only rely on the single-modal impact. Therefore, language bias can be identiﬁed by estimating the direct causal effect of Q on A, i.e., pure language effect. The training stages follows language-prior based methods [10, 13] that train an ensemble model with a prevailing VQA model and single-modal branches. Dur-ing the test stage, CF-VQA uses the debiased causal effect for inference, which is obtained by subtracting the pure lan-guage effect from the total effect. Perhaps surprisingly, re-cent language-prior based methods [10, 13] can be further uniﬁed into our proposed counterfactual inference frame-work as special cases.
In particular, CV-VQA can easily improve RUBi [10] by 7.5% with only one more learnable parameter. Experimental results show that CF-VQA out-performs the methods without data argumentation by large margins on the VQA-CP dataset [3] while remaining stable on the balanced VQA v2 dataset [19].
The main contribution of this paper is threefold. First, our counterfactual inference framework is the ﬁrst to for-mulate language bias in VQA as causal effects. Second, we provide a novel causality-based interpretation for recent de-biasing VQA works [10, 13]. Third, our cause-effect look is general and suitable for different baseline VQA architec-tures and fusion strategies. 2.