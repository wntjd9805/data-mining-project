Abstract
Scene ﬂow depicts the dynamics of a 3D scene, which is critical for various applications such as autonomous driving, robot navigation, AR/VR, etc. Conventionally, scene ﬂow is estimated from dense/regular RGB video frames. With the development of depth-sensing technologies, precise 3D measurements are available via point clouds which have sparked new research in 3D scene ﬂow. Nevertheless, it remains challenging to extract scene ﬂow from point clouds due to the sparsity and irregularity in typical point cloud sampling patterns. One major issue related to irregular sampling is identiﬁed as the randomness during point set ab-straction/feature extraction—an elementary process in many
ﬂow estimation scenarios. A novel Spatial Abstraction with
Attention (SA2) layer is accordingly proposed to alleviate the unstable abstraction problem. Moreover, a Temporal
Abstraction with Attention (TA2) layer is proposed to rec-tify attention in temporal domain, leading to beneﬁts with motions scaled in a larger range. Extensive analysis and experiments veriﬁed the motivation and signiﬁcant perfor-mance gains of our method, dubbed as Flow Estimation via Spatial-Temporal Attention (FESTA), when compared to several state-of-the-art benchmarks of scene ﬂow estimation. 1.

Introduction
Our world is dynamic. To promptly predict and respond to the ever changing surroundings, humans are able to per-ceive a moving scene and decipher the 3D motion of indi-vidual objects. This capability to capture and infer from scene dynamics is also desirable for computer vision appli-cations. For instance, a self-driving car can maneuver its actions upon perceiving the motions in its surroundings [19]; whereas a robot can exploit the scene dynamics to facilitate its localization and mapping process [2]. Moreover, with ad-vances in depth-sensing technologies, especially the LiDAR technologies [7], point cloud data have become a common conﬁguration in such applications.
*Authors contributed equally. Work done while Haiyan Wang was an intern at InterDigital. (a) FPS (b) Grouping (c) AP (d) Regrouping
Figure 1: Given two consecutive point clouds, down-sampled points produced by (a) Farthest Point Sampling (FPS, in dark red) are different which make them intractable in scene ﬂow estima-tion. However, by appending our (c) Aggregate Pooling (AP), stable corresponding points (in blue) are synthesized for scene ﬂow estimation.
To describe the motion of individual points in the 3D world, scene ﬂow extends 2D optical ﬂow to the 3D vector
ﬁeld representing the 3D scene dynamics [28]. Hence, just like 2D optical ﬂow needs to be estimated from video frames comprising images [31, 10], 3D scene ﬂow needs to be in-ferred from point cloud data [9]. However, it is non-trivial to accurately estimate scene ﬂow from point clouds.
Unstable abstraction: Pioneered by PointNet [25] and its extension, PointNet++ [26], deep neural networks (DNNs) have recently been enabled to directly consume 3D point clouds for various vision tasks. As shown in Figure 1a and Figure 1b, the grouping based on the Farthest Point
Sampling (FPS) is widely utilized during the feature extrac-tion process. It is treated as a basic point set abstraction unit for segmentation as well as scene ﬂow estimation, e.g.,
FlowNet3D [16] and MeteorNet [17]. The naive FPS is sim-ple and computationally affordable, but problematic. Given two object point clouds, both representing the same mani-fold, FPS would likely down-sample them differently [21] (see Figure 1a). This inconsistency due to randomness in naive FPS is undesired for vision and machine tasks. With 14173
two differently down-sampled point clouds, the subsequent grouping and abstraction would lead to two dissimilar sets of local features. Thus, it becomes intractable to estimate the scene ﬂow when comparing the features extracted via FPS.
To resolve this problem, we propose a Spatial Abstraction with Attention (SA2) layer which adaptively down-samples and abstracts the input point clouds. Compared to FPS, our
SA2 layer utilizes a trainable Aggregate Pooling (AP) mod-ule to generate much more stable down-sampled points, e.g., blue points in Figure 1c. They deﬁne attended regions [6] (e.g., green circles in Figure 1d) for subsequent processing.
Motion coverage: Similar to many deep matching algo-rithms for stereo matching and optical ﬂow estimation, it is difﬁcult to have a single DNN that can accurately estimate both large-scale motion and small-scale motion [11, 23]. To tackle this problem, we iterate the network for ﬂow reﬁne-ment with a proposed Temporal Abstraction with Attention (TA2) layer. It shifts the temporal attended regions to the more correspondent areas according to the initial scene ﬂow obtained at the ﬁrst iteration.
In summary, we adaptively shift the attended regions when seeking abstraction from one point cloud spatially, and when fusing information across two point clouds temporally.
We name our proposal Flow Estimation via Spatial-Temporal
Attention, or FESTA for short. The main contributions of our work are listed as follows: (i) We propose the SA2 layer for stable point cloud ab-straction. It shifts the FPS down-sampled points to invariant positions for deﬁning the attended regions, regardless of how the point clouds were sampled from the scene manifold. Effectiveness of the SA2 layer is veriﬁed both theoretically and empirically. (ii) We propose the TA2 layer to estimate both small- and large- scale motions. It emphasizes the regions that are more likely to ﬁnd good matches between the point clouds, regardless of the scale of the motion. (iii) Our proposed FESTA architecture achieves the state-of-the-art performance for 3D point cloud scene ﬂow esti-mation on both synthetic and real world benchmarks.
Our method signiﬁcantly outperforms the state-of-the-art methods of scene ﬂow estimation. 2.