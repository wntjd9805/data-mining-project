Abstract 1.

Introduction
We present an algorithm for estimating consistent dense depth maps and camera poses from a monocular video.
We integrate a learning-based depth prior, in the form of a convolutional neural network trained for single-image depth estimation, with geometric optimization, to estimate a smooth camera trajectory as well as detailed and sta-ble depth reconstruction. Our algorithm combines two complementary techniques: (1) ﬂexible deformation-splines for low-frequency large-scale alignment and (2) geometry-aware depth ﬁltering for high-frequency alignment of ﬁne depth details. In contrast to prior approaches, our method does not require camera poses as input and achieves ro-bust reconstruction for challenging hand-held cell phone captures containing a signiﬁcant amount of noise, shake, motion blur, and rolling shutter deformations. Our method quantitatively outperforms state-of-the-arts on the Sintel benchmark for both depth and pose estimations and attains favorable qualitative results across diverse wild datasets.
Dense per-frame depth is an important intermediate rep-resentation that is useful for many video-based applications, such as 3D video stabilization [37], augmented reality (AR) and special video effects [59, 39], and for converting videos for virtual reality (VR) viewing [22]. However, estimating accurate and consistent depth maps for casually captured videos still remains very challenging. Cell phones contain small image sensors that may produce noisy images, espe-cially in low lighting situations. They use a rolling shut-ter that may result in wobbly image deformations. Hand-held captured casual videos often contain camera shake and motion blur, and dynamic objects, such as people, animals, and vehicles. In addition to all these degradations, there ex-ist well-known problems for 3D reconstruction that are not speciﬁc to video, including poorly textured image regions, repetitive patterns, and occlusions.
Traditional algorithms for dense reconstruction that combine Structure from Motion (SFM) and Multi-view
Stereo (MVS) have difﬁculties dealing with these chal-lenges. The SFM step suffers from the limitations of ac-curacy and availability of correspondence and often fails 1611
entirely, as explained below, preventing further processing.
Even when SFM succeeds, the MVS reconstructions typi-cally contain a signiﬁcant amount of holes and noises.
Learning-based algorithms [35, 34, 48] are better equipped to handle with this situation. Instead of match-ing points across frames and geometric triangulation they employ priors learned from diverse training datasets. This enables them to handle many of the challenging situations aforementioned. However, the estimated depth is only de-ﬁned up to scale, and, while plausible, is not necessarily accurate, i.e., it lacks geometric consistency.
Hybrid algorithms [39, 36, 70, 56] achieve desirable characteristics of both approaches by combining learnt pri-ors with geometric reasoning. These methods often as-sume precise per-frame camera poses as auxiliary inputs, which are typically estimated with SFM. However, SFM al-gorithms are not robust to the issues described above. In such situations, SFM might fail to register all frames or produce outlier poses with large errors. As a consequence, hybrid algorithms work well when the pose estimation suc-ceeds and fail catastrophically when it does not. This prob-lem of robustness makes these algorithms unsuitable for many real-world applications, as they might fail in unpre-dictable ways. Recently, a hybrid approach proposed in the
DeepV2D work [56] attempts to interleave pose and depth estimations in inference for an ideal convergence, which performs reasonably well on static scenes but still does not prove the capability of tackling dynamic scenes.
We present a new algorithm that is more robust and does not require poses as input. Similar to Luo et al. [39], we leverage a convolutional neural network trained for single-image depth estimation as a depth prior and optimize the alignment of the depth maps. However, their test-time ﬁne-tuning formulation requires a pre-established geometric re-lationship between matched pixels across frames, which, in turn, requires precisely calibrated camera poses and per-frame depth scale factors. In contrast, we jointly optimize extrinsic and intrinsic camera pose parameters as well as 3D alignment of the estimated depth maps using continu-ous optimization. Na¨ıve alignment using rigid-scale trans-formations does not result in accurate poses because the in-dependently estimated per-frame depth maps usually con-tain random inaccuracies. These further lead to misalign-ment, which inevitably imposes noisy errors onto the esti-mated camera trajectory. We resolve it by turning to a more
ﬂexible deformation model, using spatially-varying splines.
They provide a more exact alignment, which, in succession, results in smoother and more accurate trajectories.
The spline-based deformation achieves accurate low-frequency alignment. To further improve high-frequency details and remove residual jitter, we use a geometry-aware depth ﬁlter. This ﬁlter is capable of bringing out ﬁne depth details, rather than blurring them because of the precise alignment from the previous stage.
As shown in previous work, the learning-based prior is resilient to moderate amounts of dynamic motion. We make our method even more robust to large dynamic motion by incorporating automatic segmentation-based masks to relax the geometric alignment requirements in regions containing people, vehicles, and animals.
We evaluate our method qualitatively (visually) by pro-cessing all 90 sequences from the DAVIS dataset (originally designed for dynamic video object segmentation) [46] and comparing against previous methods (of which many fail).
We further, evaluate quantitatively on the 23 sequences from the Sintel dataset [4], for which ground truth pose and depth are available. 2.