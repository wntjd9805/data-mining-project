Abstract
Object detection is an important computer vision task with plenty of real-world applications; therefore, how to enhance its robustness against adversarial attacks has emerged as a crucial issue. However, most of the pre-vious defense methods focused on the classiﬁcation task and had few analysis in the context of the object detection task. In this work, to address the issue, we present a novel class-aware robust adversarial training paradigm for the object detection task. For a given image, the proposed ap-proach generates an universal adversarial perturbation to simultaneously attack all the occurred objects in the image through jointly maximizing the respective loss for each ob-ject. Meanwhile, instead of normalizing the total loss with the number of objects, the proposed approach decomposes the total loss into class-wise losses and normalizes each class loss using the number of objects for the class. The adversarial training based on the class weighted loss can not only balances the inﬂuence of each class but also ef-fectively and evenly improves the adversarial robustness of trained models for all the object classes as compared with the previous defense methods. Furthermore, with the re-cent development of fast adversarial training, we provide a fast version of the proposed algorithm which can be trained faster than the traditional adversarial training while keep-ing comparable performance. With extensive experiments on the challenging PASCAL-VOC and MS-COCO datasets, the evaluation results demonstrate that the proposed de-fense methods can effectively enhance the robustness of the object detection models. 1.

Introduction
Due to the recent breakthroughs of deep learning, deep learning-based approaches have achieved promising per-formance for many computer vision tasks, such as object recognition [8], [23], [7] and object detection [6]. How-ever, researchers found there exists potential security issues
Figure 1. Detection results after attacked by different adversar-ial examples to the vanilla SSD model. (a) ground true, (b) we craft the adversarial example through the 10-step PGD optimiza-tion with the budget ǫ = 8 on the multi-task loss as described in equation (1), (c) the detection result of the proposed class-wise attack. These detection examples show the adversarial examples generated by the proposed method can more evenly attack all the objects occurred in the image than (b). for deep learning-based approaches. Szegedy et al.
[25]
ﬁrst crafted adversarial examples by adding imperceptible perturbations to the input images, which can easily fool the deep learning-based classiﬁcation models to generate unex-pected outputs. From then on, many new attack methods, including Fast Signed Gradient Method (FGSM) [5], Deep-Fool [16], Projected Gradient Descent (PGD) [14], Carlini and Wagner Attack (C&W) [1], have been proposed to pro-duce various adversarial examples that further expose the vulnerability of the deep learning classiﬁcation models. On the other hand, object detection is one of the most important and active research ﬁelds for computer vision with plenty of real-world applications. Unfortunately, as the classiﬁca-tion problem, it also suffers from the threat of these adver-sarial attacks, such as the physical adversarial patch attack to affect the steering behavior of self-driving cars [24] or the detection results of a face detector [21]. However, as compared with the development of attack methods, the de-fense algorithms to improve the robustness of object detec-tion models are relatively few.
∗Work done during a research assistantship at Academia Sinica.
In order to defend against these attacks, various meth-10420
ods have been proposed to enhance the robustness of the deep learning models, and one of the most effective defense approaches is adversarial training [26]. In addition, for the object detection task, the approaches can be roughly catego-rized into two types: one-stage detector [13], [18] and two-stage detector [4], [3], [19], and we focus on the one-stage detector (i.e., single-shot object detector (SSD) [13]) due to its faster detection speed and more complex nature than the two-stage detector where the nature of the two-stage de-tector is more similar with that of image classiﬁcation task (i.e., it also performs the classiﬁcation and regression tasks on the object proposals generated by the region proposal network.). Although there exists algorithms [30] to enhance the robustness of the one-stage detector, there are still some unsolved problems: vanilla adversarial training using the overall loss of one-stage object detector does not properly take all the objects occurred in an image into consideration.
As shown in equation (1), the object detection loss of a spe-ciﬁc object consists of a classiﬁcation loss to identify the object class and a regression loss for bounding box regres-sion of the object. The total loss for all the occurred objects in a given image can be written as follows:
L = 1
No  
No i=1
X lcls (Oi, {yi} , θ) + lreg (Oi, {bi} , θ) (1)
! where Oi presents i-th matched default box in the image,
No is the number of matched default boxes, lcls and lreg are the losses of the classiﬁcation branch and regression branch respectively.
As shown in Figure 1 , not all of the detected objects in an image by an object detector can be attacked success-fully if we generate the adversarial examples directly using the total loss described in the equation (1) since the sub-loss for a speciﬁc object (i.e., the loss of a speciﬁc object might go to inﬁnity.) and a speciﬁc object class (i.e., in a given image, there are more objects of a speciﬁc class than other classes.) might dominate the overall loss value during the generation process of adversarial examples. To address these issues, we present a novel class-aware robust adver-sarial training for the object detection task. For a given image, the proposed approach generates an universal ad-versarial perturbation to simultaneously attack all the oc-curred objects in the image through jointly maximizing the respective loss for each object. For the classiﬁcation and regression losses of each object, we clip each of them re-spectively to avoid the situation that the speciﬁc object loss dominates the overall loss. Meanwhile, instead of normal-izing the total loss with the number of objects, the proposed approach decomposes the total loss into class-wise losses and normalizes each class loss using the number of objects for the corresponding class to mitigate the situation that the loss of a speciﬁc class dominates others. The adversarial training based on the proposed class weighted loss can not only balances the inﬂuence of each class but also effectively and evenly improves adversarial robustness of trained mod-losscls + lossreg losscls + lossreg losscls + lossreg
… losscls + lossreg clip clip clip clip
Object-wise Losses
Object Detector
Clean Image
Weighted 
Class-wise
Loss
+
Backpropagation
Adversarial Image
Figure 2. The framework of generating class-wise adversarial ex-amples. In the process of class-wise adversarial generation, we
ﬁrst separate task-oriented losses into object-wise losses and clip each classiﬁcation and regression loss of an object to force the values of them in the same scale. Then, we generate perturbations from the weighted class-wise loss. Finally, we add the class-wise adversarial perturbations into clean images to generate the class-wise adversarial images. els for all the object classes as compared with the previous defense methods. In addition, due to the high computational cost of vanilla adversarial training, we also adopt the recent developed fast adversarial training methods [20] into the proposed approach to accelerate the training speed to sufﬁce the practical needs of real-world applications. With exten-sive experiments on the challenging PASCAL VOC [2] and
MS-COCO [12] datasets, the evaluation results demonstrate that the proposed defense methods can effectively enhance the robustness of the object detection models. We summa-rize the main contributions of our work as follows:
• We provide a systematic analysis and design several efﬁcient and effective adversarial training algorithms for object detection, especially for the situations when there are multiple objects from different classes ap-pearing in a given image. The proposed approaches can craft adversarial examples which can more evenly attack all the objects occurred in an image than previ-ous methods and help improve the adversarial robust-ness of the trained model with adversarial training.
• We build the connection between the universal adver-sarial perturbation in the context of image classiﬁca-tion and the object detection. 2.