Abstract
Photorealistic rendering of dynamic humans is an im-portant capability for telepresence systems, virtual shop-ping, special effects in movies, and interactive experiences such as games. Recently, neural rendering methods have been developed to create high-ﬁdelity models of humans and objects. Some of these methods do not produce re-sults with high-enough ﬁdelity for driveable human models (Neural Volumes) whereas others have extremely long ren-dering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and con-tinuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a contin-uous learned scene function that maps every position and its corresponding local animation code to a view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dy-namic radiance ﬁeld can be used to synthesize novel un-seen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.
See our project page1 for more results. 1.

Introduction
Modeling, rendering, and animating dynamic human heads at high ﬁdelity, for example for virtual reality re-mote communication applications, is a highly challenging research problem because of the tremendous complexity of the geometry of the human head and the appearance vari-ations of human skin, hair, teeth, and eyes. Skin exhibits subsurface scattering and shows ﬁne-scale geometric pore-level detail, while the human eyes and teeth are both translu-1https://ziyanw1.github.io/hybrid nerf/ cent and reﬂective at the same time. High ﬁdelity modeling and rendering of human hair is challenging due to its thin geometric structure and light scattering properties. Impor-tantly, the face is not static, but changes dynamically with expression and posture.
Recent work on neural rendering learns either discrete or continuous neural scene representations to achieve view-point and animation controllable rendering. Discrete neural scene representations are based on meshes [32, 15, 19, 8, 31], point clouds [34, 1, 21], voxel grids [17, 29], or multi-plane images [36, 22]. However, each of these represen-tations has drawbacks: Meshes, even if dynamically tex-tured [16], struggle to model thin and detailed structures, such as hair. Point clouds, by design, do not provide con-nectivity information and thus lead to undeﬁned signals in areas of sparse sampling, while making explicit occlusion reasoning challenging. Multi-plane images yield photo-realistic rendering results under constrained camera motion, but produce ’stack of cards’-like artifacts [36] when the camera moves freely. Volumetric representations [17] based on discrete uniform voxel grids are capable of modeling thin structures, e.g., hair, using semi-transparency. While these approaches achieve impressive results, they are hard to scale up due to their innate cubic memory complexity.
To circumvent the cubic memory complexity of these ap-proaches, researchers have proposed continuous volumet-ric scene representations based on fully-connected networks that map world coordinates to a local feature representa-tion. Scene Representation Networks (SRNs) [30] employ sphere marching to extract the local feature vector for every point on the surface, before mapping to pixel colors. This approach is limited to modeling diffuse objects, making it unsuitable for representing human heads at high ﬁdelity.
Neural radiance ﬁelds [23] have shown impressive re-sults for synthesizing novel views of static scenes at im-pressive accuracy by mapping world coordinates to view-dependent emitted radiance and local volume density. A very recent extension [14] speeds up rendering by applying a static Octree to cull free space. While they have shown
ﬁrst results on a simple synthetic dynamic sequence, it is unclear how to extend the approach to learn and render 5704
photo-realistic dynamic sequences of real humans. In ad-dition, it is unclear how to handle expression interpolation and the synthesis of novel unseen motions given the static nature of the Octree acceleration structure.
As discussed, existing work on continuous neural 3D scene representations mainly focuses on static scenes and dynamic scene modeling and editing are not directly achiev-able under the current frameworks. In this work, we pro-pose a novel compositional 3D scene representation for learning high-quality dynamic neural radiance ﬁelds that addresses these challenges. To this end, we bridge the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of anima-tion codes with a continuous learned scene function. We start by extracting a global animation code from a set of input images using a convolutional encoder network. The global code is then mapped to a 3D-structure-aware grid of local animation codes as well as a coarse opacity ﬁeld. A novel importance sampling approach employs the regressed coarse opacity to speed up rendering. To facilitate gener-alization across motion and shape/appearance variation, in addition to conditioning the dynamic radiance ﬁeld on the global animation code, we additionally condition it on a lo-cal code which is sampled from the 3D-structure-aware grid of animation codes. The ﬁnal pixel color is computed by volume rendering. In summary, the main contributions of our work are
• A novel compositional 3D representation for learning high-quality dynamic neural radiance ﬁelds of human heads in motion based on a 3D-structure-aware grid of local animation codes.
• An importance sampling strategy tailored to human heads that reduces unnecessary computation in free space and enables faster volumetric rendering.
• State-of-the-art results for synthesizing novel views of dynamic human heads that outperform competing methods in terms of quality. 2.