Abstract
Most explanation methods in deep learning map impor-tance estimates for a model’s prediction back to the original input space. These “visual” explanations are often insufﬁ-cient, as the model’s actual concept remains elusive. More-over, without insights into the model’s semantic concept, it is difﬁcult —if not impossible— to intervene on the model’s behavior via its explanations, called Explanatory Interac-tive Learning. Consequently, we propose to intervene on a
Neuro-Symbolic scene representation, which allows one to revise the model on the semantic level, e.g. “never focus on the color to make your decision”. We compiled a novel con-founded visual scene data set, the CLEVR-Hans data set, capturing complex compositions of different objects. The results of our experiments on CLEVR-Hans demonstrate that our semantic explanations, i.e. compositional expla-nations at a per-object level, can identify confounders that are not identiﬁable using “visual” explanations only. More importantly, feedback on this semantic level makes it possi-ble to revise the model from focusing on these factors. 1.

Introduction
Machine learning models may show Clever-Hans like moments when solving a task by learning the “wrong” thing, e.g. making use of confounding factors within a data set. Unfortunately, it is not easy to ﬁnd out whether, say, a deep neural network is making Clever-Hans-type mistakes because they are not reﬂected in the standard performance measures such as precision and recall. Instead, one looks at their explanations to see what features the network is actu-ally using [23]. By interacting with the explanations, one may even ﬁx Clever-Hans like moments [43, 50, 47, 44].
This Explanatory Interactive Learning (XIL), however, very much depends on the provided explanations. Most explanation methods in deep learning map importance es-*Equal contribution
Figure 1: Neuro-Symbolic explanations are needed to re-vise deep learning models from focusing on irrelevant features via global feedback rules. timates for a model’s prediction back to the original input space [46, 49, 48, 45, 7]. This is somewhat reminiscent of a child who points towards something but cannot articu-late why something is relevant. In other words, “visual” ex-planations are insufﬁcient if a task requires a concept-level understanding of a model’s decision. Without knowledge about and symbolic access to the concept level, it remains difﬁcult—if not impossible—to ﬁx Clever-Hans behavior.
To illustrate this, consider the classiﬁcation task depicted in Fig. 1. It shows a complex scene consisting of objects, which vary in position, shape, size, material, and color.
Now, assume that scenes belonging to the true class show a large cube and a large cylinder. Unfortunately, during training, our deep network only sees scenes with large, gray cubes. Checking the deep model’s decision process us-ing visual explanations conﬁrms this: the deep model has learned to largely focus on the gray cube to classify scenes to be positive. An easy ﬁx would be to provide feedback in the form of “never focus on the color to make your deci-sion” as it would eliminate the confounding factor. Unfor-tunately, visual explanations do not allow us direct access to the semantic level— they do not tell us that “the color gray 3619
is an important feature for the task at hand” and we cannot provide feedback at the symbolic level.
Triggered by this, we present the ﬁrst Neuro-Symbolic
XIL (NeSy XIL) approach that is based on decomposing a visual scene into an object-based, symbolic representation and, in turn, allows one to compute and interact with neuro-symbolic explanations. We demonstrate the advantages of
NeSy XIL on a newly compiled, confounded data set, called
CLEVR-Hans. It consists of scenes that can be classiﬁed based on speciﬁc combinations of object attributes and re-lations. Importantly, CLEVR-Hans encodes confounders in a way so that the confounding factors are not separable in the original input space, in contrast to many previous con-founded computer vision data sets.
To sum up, this work makes the following contributions: (i) We conﬁrm empirically on our newly compiled con-founded benchmark data set, CLEVR-Hans, that Neuro-Symbolic concept learners [31] may show Clever-Hans mo-ments, too. (ii) To this end, we devise a novel Neuro-Symbolic concept learner, combining Slot Attention [28] and Set Transformer [24] in an end-to-end differentiable fashion. (iii) We provide a novel loss to revise this Clever-Hans behaviour. (iv) Given symbolic annotations about in-correct explanations, even across a set of several instances, we efﬁciently optimize the Neuro-Symbolic concept learner (v) Thus to be right for better Neuro-Symbolic reasons. we introduce the ﬁrst XIL approach that works on both the visual and the conceptual level. These contributions are important to make progress towards creating conver-sational explanations between machines and human users
[53, 33]. This is necessary for improved trust development and truly Explanatory Interactive Learning: symbolic ab-stractions help us, humans, to engage in conversations with one another and to convey our thoughts efﬁciently, without the need to specify much detail.1 2.