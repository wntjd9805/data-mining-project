Abstract
Invariant approaches have been remarkably successful in tackling the problem of domain generalization, where the objective is to perform inference on data distributions different from those used in training. In our work, we in-vestigate whether it is possible to leverage domain infor-mation from the unseen test samples themselves. We pro-pose a domain-adaptive approach consisting of two steps: a) we ﬁrst learn a discriminative domain embedding from unsupervised training examples, and b) use this domain em-bedding as supplementary information to build a domain-adaptive model, that takes both the input as well as its domain into account while making predictions. For un-seen domains, our method simply uses few unlabelled test examples to construct the domain embedding. This en-ables adaptive classiﬁcation on any unseen domain. Our approach achieves state-of-the-art performance on various domain generalization benchmarks. In addition, we intro-duce the ﬁrst real-world, large-scale domain generalization benchmark, Geo-YFCC, containing 1.1M samples over 40 training, 7 validation and 15 test domains, orders of mag-nitude larger than prior work. We show that the existing approaches either do not scale to this dataset or underper-form compared to the simple baseline of training a model on the union of data from all training domains. In contrast, our approach achieves a signiﬁcant 1% improvement. 1.

Introduction
Domain generalization refers to the problem of learn-ing a classiﬁer from a heterogeneous collection of distinct training domains that can generalize to new unseen test do-mains [7]. Among the various formalizations proposed for the problem [19, 6], the most effective is domain-invariant learning [4, 15]. It learns feature representations invariant to the underlying domain, providing a universal classiﬁer that can generalize to new domains [26, 26, 33, 25, 16].
It can be demonstrated that domain-invariant classiﬁers
∗Work done while visiting Facebook AI.
Figure 1: A visual for adaptive domain generalization. Here we denote by ˆfi the optimal classiﬁer in a class for do-main i (out of 8). For test domain DT , the optimal universal classiﬁer ˆf∗ may be far from the optimal classiﬁer for DT .
F minimize the average risk across domains [33]. However, this may not guarantee good performance for any speciﬁc test domain, particularly if the distribution of domains has high variance, as visualized heuristically in Figure 1. In this case, the optimal classiﬁer for a target domain can lie far from the optimal universal classiﬁer. In our work, we pro-pose an adaptive classiﬁer that can be adapted to any new domain using very few unlabelled samples without any fur-ther training. Unlike invariant approaches, this requires a few unsupervised samples from any domain while testing1.
However, this requirement is trivial, since this set is avail-able by deﬁnition in all practical settings.
Our approach consists of two steps. We ﬁrst embed each domain into a vector space using very few unlabelled sam-ples from the domain. Next, we leverage these domain embeddings as supplementary signals to learn a domain-adaptive classiﬁer. During testing, the classiﬁer is supplied the corresponding embedding obtained from test samples.
Our contributions can be summarized as follows. 1. We adapt low-shot prototypical learning [39] to con-struct domain embeddings from unlabelled samples of each domain. We also provide an algorithm to use these embed-dings as supplementary signals to train adaptive classiﬁers. 2. We justify our design choices with novel theoreti-1This does not correspond to unsupervised domain adaptation, where unsupervised samples are assumed to be present during training. 114340
cal results based on the framework of kernel mean embed-dings [18]. Furthermore, we leverage the theory of risk min-imization under product kernel spaces [7] to derive general-ization bounds on the average risk for adaptive classiﬁers. 3. We introduce the ﬁrst large-scale, real-world domain generalization benchmark, dubbed Geo-YFCC, which con-tains 40 training, 7 validation and 15 test domains, and an overall 1.1M samples. Geo-YFCC is constructed from the popular YFCC100M [42] dataset partitioned by geo-graphical tags, and exhibits several characteristics of do-main shift [23], label shift [3], and long-tailed class distri-butions [49, 45, 9] common in real-world classiﬁcation. Be-cause of its scale and diversity, this benchmark is substan-tially more challenging than the incumbent benchmarks. 4. On existing benchmarks and notably, two large-scale benchmarks, we demonstrate the strength of ﬁne-tuning (ERM) with adaptive classiﬁcation over existing ap-proaches. In addition to the effectiveness of adaptive clas-siﬁcation, this suggests, along with the claims of Gulrajani and Lopez-Paz [19] that rigorous model selection and large benchmarks are essential in understanding domain general-ization, where naive ERM can also be a powerful baseline. 2.