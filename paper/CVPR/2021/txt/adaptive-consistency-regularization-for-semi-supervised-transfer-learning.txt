Abstract
While recent studies on semi-supervised learning have shown remarkable progress in leveraging both labeled and unlabeled data, most of them presume a basic setting of the model is randomly initialized.
In this work, we consider semi-supervised learning and transfer learning jointly, leading to a more practical and competitive paradigm that can utilize both powerful pre-trained models from source domain as well as labeled/unlabeled data in the target do-main. To better exploit the value of both pre-trained weights and unlabeled target examples, we introduce adaptive con-sistency regularization that consists of two complementary components: Adaptive Knowledge Consistency (AKC) on the examples between the source and target model, and
Adaptive Representation Consistency (ARC) on the target model between labeled and unlabeled examples. Examples involved in the consistency regularization are adaptively selected according to their potential contributions to the target task. We conduct extensive experiments on popular benchmarks including CIFAR-10, CUB-200, and MURA, by
ﬁne-tuning the ImageNet pre-trained ResNet-50 model. Re-sults show that our proposed adaptive consistency regular-ization outperforms state-of-the-art semi-supervised learn-ing techniques such as Pseudo Label, Mean Teacher, and
FixMatch. Moreover, our algorithm is orthogonal to existing methods and thus able to gain additional im-provements on top of MixMatch and FixMatch. Our code is available at https://github.com/Walleclipse/Semi-Supervised-Transfer-Learning-Paddle. 1.

Introduction
Deep neural networks have achieved great success in su-pervised learning tasks especially in computer vision [21, 15]. Yet, this heavily relies on a large amount of labeled data. As data annotation is usually expensive and time-*Equal contributions and by alphabetical order. † Correspondence. consuming, Semi-Supervised Learning (SSL), which pur-sues the goal of effectively leveraging both labeled and unlabeled data, is widely studied. Recent state-of-the-art methods can be roughly summarized in three categories, which are consistency based regularization [22, 42], entropy minimization [12] and pseudo label [23].
While most works focus on the general setting that train-ing a randomly initialized model from scratch, we consider a more realistic setting utilizing the powerful pre-trained model which is adequately ﬁt on large-scale datasets for general purposes such as ImageNet [6] and Places365 [54].
These pre-trained models are empirically proven to have ex-cellent transferability on various down-streaming tasks [49] and can signiﬁcantly improve the generalization capacity of target tasks especially when the sample size is relatively small. Moreover, they are free to fetch and can be efﬁciently
ﬁne-tuned to adapt to new tasks. A recent study [55] points out that the beneﬁt of semi-supervised learning sometimes may be marginal when ﬁne-tuning a pre-trained model on the target dataset. However, the investigation of a system-atic solution on DNN-based semi-supervised transfer learn-ing has rarely been delved into.
In this work, we propose a semi-supervised transfer learning framework beyond a simple combination of these two kinds of algorithms. We extend the effective idea of consistency regularization in semi-supervised learning to adapt to inductive transfer learning, where the pre-trained weight learned by the source task is available. Speciﬁ-cally, our method is composed of two essential components: (1) Adaptive Knowledge Consistency (AKC) on the exam-ples between the source and target model. We utilize tar-get examples to transfer knowledge from the pre-trained model and help generalize the target model inspired by re-cent studies about knowledge distillation [51] and transfer learning [24]. To cope with the risk of negative transfer [43] caused by the discrepancy between the source and target task, we use the knowledge adaptive sample importance for proper cross-task knowledge consistency regularization. In-tuitively, we are inclined to select examples lying in the 6923
trusted region of the source model. (2) Adaptive Represen-tation Consistency (ARC) on the target model between la-beled and unlabeled examples. In transfer learning applica-tions, labeled examples are often insufﬁcient and thus they are prone to be projected onto an inappropriate representa-tion with only the supervision of their labels. To tackle this problem we utilize ample unlabeled examples to adjust the representation produced by supervised learning to the real target domain. This is achieved by minimizing their Max-imum Mean Discrepancy (MMD) distance. Furthermore, we adaptively decide the sample set used for restricting the representation distance. An intuitive explanation about the motivation of ARC is showed in supplementary A.
We evaluate our method on several semi-supervised transfer learning settings considering various typical scenar-ios. We use popular datasets CIFAR-10, CUB-200-2011,
MIT Indoor 67, and MURA,covering domains including objects, animals, scenes and, radiographs.
Our main contributions can be summarized in the fol-lowing points.
• To the best of our knowledge, we are the ﬁrst to pro-pose an advanced end-to-end semi-supervised transfer learning framework for deep neural networks. Consid-ering incorporating inductive transfer learning, our re-search is closer to the actual problems in practice. Pre-vious empirical study [55] provided observations and understandings by directly combining SSL with ﬁne-tuning, but did not develop effective algorithms.
• We introduce adaptive consistency regularization to improve semi-supervised transfer learning by exploit-ing the characteristics of both semi-supervised learn-ing and transfer learning, including cross-task knowl-edge distillation with adaptive sample importance named Adaptive Knowledge Consistency and repre-sentation adaptation for supervised learning using se-lected unlabeled data as the reference named Adaptive
Representation Consistency.
• We conduct extensive experiments and show that the proposed adaptive consistency regularization is su-perior to classic semi-supervised learning algorithms such as Pseudo Label, Mean Teacher, and MixMatch on various semi-supervised transfer learning tasks.
Furthermore, our method is shown orthogonal to exist-ing methods and can obtain additional improvements even on top of MixMatch and FixMatch, which com-bine several state-of-the-art SSL techniques. 2.