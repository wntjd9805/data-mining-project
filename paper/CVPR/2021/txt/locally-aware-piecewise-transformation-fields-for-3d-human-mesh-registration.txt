Abstract
Registering point clouds of dressed humans to paramet-ric human models is a challenging task in computer vi-sion. Traditional approaches often rely on heavily engi-neered pipelines that require accurate manual initializa-tion of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major ob-stacle is that, despite recent effort on rotation representa-tion learning in neural networks, regressing joint rotations from point clouds or images of humans is still very chal-lenging. To this end, we propose novel piecewise trans-formation ﬁelds (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We com-bine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultane-ously predict shape and per-point correspondences between the posed space and the canonical space for clothed hu-man. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone trans-formations and joint rotations can be obtained efﬁciently via a least-square ﬁtting given the estimated point corre-spondences, circumventing the challenging task of directly 7639
regressing joint rotations from neural networks. Further-more, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capabil-ity and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that ﬁt-ting parametric models with poses initialized by our net-work results in much better registration quality, especially for extreme poses. 1.

Introduction
Human pose and shape registration from sensor inputs is a long-standing problem in computer vision. While unstruc-tured 3D point clouds are becoming increasingly available, accurately registering these point clouds to parametric hu-man shape models [4, 18, 21, 28, 37, 57] still remains chal-lenging, especially when considering clothed human with arbitrary poses. Traditional approaches often assume access to either temporal sequences of dense scans [10, 40, 59] or manually enforced constraints [3, 4, 9, 10, 18, 39, 41].
These requirements limit the applicability of traditional ap-proaches to static, arbitrarily obtained point clouds.
With the advent of neural implicit functions [12, 31, 32, 36], learning-based methods that reconstruct human shapes from point clouds are becoming increasingly accurate [13].
However, most existing neural implicit models treat recon-structed human shapes as static objects and do not provide a way to register such reconstructions to parametric body models. Note that in the context of this paper, we refer to reconstruction as implicit surface reconstruction from point clouds, while registration refers to ﬁnding the shape and poses of a parametric model which best explains the in-put point cloud or the reconstructed surface. Some works distinguish registration (registering a point cloud to a tem-plate) [3, 4, 9, 17, 18, 55] and model-ﬁtting (estimating pa-rameters of a parametric model) [6, 7, 62]. In this context, our approach falls into the model-ﬁtting category. However the deﬁnitions of registration and model-ﬁtting are not con-sistent across papers, and for the ease of understanding we follow the convention of our major baseline [6] and use reg-istration and model-ﬁtting interchangeably.
More recently, IPNet [6] has been proposed for auto-matic point cloud registration of clothed humans. It predicts two sets of implicit surfaces, one for the clothed human and one for the human body under-cloth. IPNet then exploits optimization-based registration to ﬁt a parametric model to the implicit surfaces from a ﬁxed initial pose for all the sub-jects, with additional information about body part labels for each point in space. However, such semantic information is still very coarse and registration may fail when the target pose deviates too much from the initial pose (Fig. 1).
We observe that, although the local point cloud fea-tures [13] lead to reliable surface reconstruction, which facilitates the registration of parametric human models (e.g. SMPL [28]), an accurate pose initialization is still key to reliable registration, as the underlying optimization prob-lem is non-convex, thus reliable estimates are only obtained when initialized close to the solution. However, it is difﬁ-cult to estimate parametric poses from point clouds directly, because regressing pose parameters from neural networks is hard and unintuitive (as investigated in [6], as well as demonstrated in our experiments in Sec. 5.3).
In this paper, we introduce a novel approach to estimat-ing the SMPL [28] pose parameters, based on implicit rep-resentations and local point cloud features. Instead of re-gressing pose parameters directly from global features, we introduce a set of transformation functions, which take a query point and a local point cloud feature as input, and transform the query point to the rest-pose space (Fig. 2).
We assign one such transformation function per body-part, and name these functions Piecewise Transformation Fields (PTF). The general idea of PTF is inspired by the obser-vation that the rigid bone-transformations in SMPL can be calculated if we know point correspondences between the posed space and the unposed space. Another motivation for
PTF is that by transforming query points into a canonical space before the occupancy query, we make the occupancy learning/inference task easier [14, 19]. With PTF, our novel occupancy networks can estimate for each point: 1) its double-layer occupancy value (i.e. inside body, in-between body and cloth, outside cloth) like IPNet does, 2) which body-part it belongs to, and 3) its corresponding position in rest-pose space. With this information, we can then extract a mesh-surface of human body, along with the semantic part label and the corresponding position in the rest-pose for each surface vertex. As a side-beneﬁt, rigid transformations for each body-part can be estimated directly via least-square
ﬁtting. In terms of registering implicit surfaces to paramet-ric models, our approach also employs optimization-based registration as IPNet. But unlike IPNet, our approach ex-ploits point correspondences which allow us to more accu-rately initialize the pose parameters. This makes the regis-tration process much more stable and accurate, especially for extreme poses; this will be evidenced in Section 5.
In summary, the contribution of this work is three-fold: (1) We propose Piecewise Transformation Fields (PTF) that learn to transform arbitrary points from posed space to rest-pose space. (2) We combine our PTF modules with occupancy networks, and achieve state-of-the-art re-sults on clothed human reconstruction from point clouds on the CAPE dataset [30] while reducing the number of pa-rameters by half. (3) We propose an alternative learning-based method for estimating joint rotations of the para-metric SMPL model from point clouds. Our learning-based method takes advantage of local point-aligned fea-7640
tures and produces more accurate and robust estimations than direct regression from global features. When ﬁtting parametric models to implicit reconstructions using our estimated poses as initialization, we achieve 18% reduc-tion in registration error on average. Code is available at https://taconite.github.io/PTF/website/PTF.html.
The remainder of this paper is structured as follows: in
Section 2 we give an overview of existing works that are related to our approach. In Section 3 we review the funda-mentals, SMPL [28] and NASA [14], upon which we build our proposed approach. In Section 4 we introduce our pro-posed PTF and its application to fast joint rotation estima-tion. In Section 5 we benchmark our PTF model, showing its advantages for registration and reconstruction. In Sec-tion 6 we conclude and discuss possible future works. 2.