Abstract
Geometry Generator
We present a new method for few-shot human motion transfer that achieves realistic human image generation with only a small number of appearance inputs. Despite re-cent advances in single person motion transfer, prior meth-ods often require a large number of training images and take long training time. One promising direction is to per-form few-shot human motion transfer, which only needs a few of source images for appearance transfer. However, it is particularly challenging to obtain satisfactory transfer re-sults. In this paper, we address this issue by rendering a human texture map to a surface geometry (represented as a
UV map), which is personalized to the source person. Our geometry generator combines the shape information from source images, and the pose information from 2D keypoints to synthesize the personalized UV map. A texture genera-tor then generates the texture map conditioned on the tex-ture of source images to ﬁll out invisible parts. Further-more, we may ﬁne-tune the texture map on the manifold of the texture generator from a few source images at the test time, which improves the quality of the texture map with-out over-ﬁtting or artifacts. Extensive experiments show the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively. Our code is avail-able at https://github.com/HuangZhiChao95/
FewShotMotionTransfer. 1.

Introduction
Human motion transfer [7, 9, 12, 17, 22, 27, 34, 44, 45] generates videos of one person that takes the same motion as the person in a target video, which has huge potential applications in virtual characters, movie making, etc. The rapid growth of generative networks [11] and image transla-tion frameworks [15, 40] enables generating photo-realistic images for human motion transfer. Basically, one would ex-tract the pose sequence of the target video and take the pose as the input to generate the video for a new person.
…
Source Person
Target Pose
…
Mask
Render
UV Coordinate
Texture Generator
Complete
Texture Map
Source Textures
Figure 1. Method overview. Our method contains three key com-ponents: a geometry generator for generation of a personalized
UV map, a texture generator to ﬁll out incomplete texture, and a neural renderer rendering the human image. Detailed network structures are illustrated in Figure 2.
The appearance of the new person can be provided in two ways. One type of models aim to train an individual model for a speciﬁc person. To obtain such a model, one needs to collect a large number of images for the new person and trains a network to translate the pose to the image of the person [7, 39]. Then the appearance information is stored in the weights of the network and the image of new pose can be directly generated by taking the new pose as input. How-ever, such methods need a large amount of training data and training time to obtain a model for the new person, which hinders the applications of these approaches.
For the other type, the information of appearance is pro-2297
vided by taking a few images of the new person as input.
Few-shot human motion transfer requires the network to learn the complicated relationship between human appear-ance and pose by only looking at few human images. The relationship is very hard to learn and generalize to unseen people. Therefore, directly conditioning the output image on the pose and the appearance leads to poor quality. Some approaches warp the input appearance to the output with op-tical ﬂows [38] or afﬁne transformations [3, 44] to generate a coarse pose of the new person. However, the mapping is usually inaccurate and fails to recover realistic human from the intermediate warping result. Even if the architec-ture gets more and more complicated, there are still many artifacts in the images of few-shot human motion transfer.
The appearance of one person at different pose is the same. Therefore, we can directly transfer the pixels from the source pose to the target pose without generating the pixels. DensePose [2] provides the UV map of one per-son so that the texture can be transferred between different poses to synthesize the human at the new pose. However, directly using the original model to transfer the texture fails to generate realistic human image [2, 28].
The DensePose can be trained to better ﬁt the generation of human and achieves high quality avatars [31]. However, their method is only suitable for single person and cannot be directly used for few-shot synthesis. On the one hand, their generator is not able to synthesize accurate geometry (i.e. IUV representation) of different people whose shapes are different. On the other hand, their method cannot pro-duce complete texture map from only few the source human images. We propose a new method that generalizes the al-gorithm for the few-shot scenario and get better results than previous few-shot approaches. As shown in Figure 1, we use a geometry generator to generate personalized UV map given a target pose and a few source images. Meanwhile, a texture generator merges each incomplete texture map and hallucinates the invisible. Then the texture map is rendered to the UV map to generate an image with target pose and source appearance. The decoupling generation of personal-ized geometry and texture leads to better quality of motion transfer.
We summarize our contributions as follows: 1. We propose a geometry generator to predict an accu-rate personalized UV map and a texture generator to gen-erate a complete texture map. These two generators work collaboratively for rendering high quality human images. 2. By training on multiple videos of multiple persons and ﬁne-tuning on a few examples of an unseen person, our method successfully transfers geometry and texture knowl-edge to the new person. 3. Experiments demonstrate that our method generates better human motion transfer results than state-of-the-art methods both qualitatively and quantitatively. 2.