Abstract
Capturing interpretable variations has long been one of the goals in disentanglement learning. However, unlike the independence assumption, interpretability has rarely been exploited to encourage disentanglement in the unsupervised setting. In this paper, we examine the interpretability of dis-entangled representations by investigating two questions: where to be interpreted and what to be interpreted? A latent code is easily to be interpreted if it would consistently im-pact a certain subarea of the resulting generated image. We thus propose to learn a spatial mask to localize the effect of each individual latent dimension. On the other hand, inter-pretability usually comes from latent dimensions that cap-ture simple and basic variations in data. We thus impose a perturbation on a certain dimension of the latent code, and expect to identify the perturbation along this dimension from the generated images so that the encoding of simple variations can be enforced. Additionally, we develop an unsupervised model selection method, which accumulates perceptual distance scores along axes in the latent space.
On various datasets, our models can learn high-quality dis-entangled representations without supervision, showing the proposed modeling of interpretability is an effective proxy for achieving unsupervised disentanglement. 1.

Introduction
Learning disentangled representations in generative models has gained increasing interest in recent years [18, 41, 1, 25]. Disentangled representations are supposed to capture independent factors of variations in data [2], which should ideally coincide with natural concepts summarized by humans. These representations can usually be applied to various downstream tasks such as controllable image gener-ation and manipulation [34, 56, 51, 32, 35], domain adapta-tion [46, 5], abstract reasoning [54], and machine learning fairness [9, 40].
Adopting the deﬁnition from [10], we can characterize disentanglement from three perspectives: informativeness, independence, and interpretability. In the context of unsu-pervised disentangled representation learning, the ﬁrst two properties have been commonly adopted as proxies to en-courage the disentanglement in representations. Methods built based on the framework of the Generative Adversarial
Networks (GANs) [16] maximize the mutual information between a subset of latent variables and the generated sam-ples [8, 21, 38]. On the other hand, the methods based on the Variational Autoencoders (VAEs) [31, 18, 29, 7, 33, 22] usually enforce the statistical independence in latent codes.
Unlike the informativeness and independence properties, the interpretability property in disentanglement has rarely been explored in the unsupervised setting. Partially due to the meaning of the term interpretability which indicates the correspondence between the learned representations and human-deﬁned concepts, it sounds impossible to approach this goal without revealing the ground-truth labels. Un-fortunately, omitting the modeling of interpretability leaves the repre-the existing unsupervised models a huge ﬂaw: sentations satisfying the informativeness and independence goals are far from unique, in which case the target repre-sentation is indeed included in the solution pool but not distinguishable from other entangled ones. A most intu-itive example could be the rotation of coordinates in the latent space, where the existing approaches built for mod-eling informativeness and independence are blind to this transformation. This nonuniqueness problem is explained by the impossibility conclusion of unsupervised disentan-glement drawn in [41], and also agrees with the results about the rotation invariance in [44]. On the contrary, modeling interpretability by providing models with ground-truth labels solves such nonuniqueness problem, which co-incides with the supervised and semi-supervised settings
[47, 30, 11, 32, 57, 34, 42, 43].
A rising problem is, can we enforce interpretability in representations without supervision? A precise matching between the target concepts and the learned representations is unrealistic because it depends on how the target solution is deﬁned. For example, digital color can be represented in RGB or HSV, but an unsupervised model does not know which one is more preferable without being told which one is wanted. However in more general cases, there is no doubt 5861
that interpretable variations are identiﬁable out of noninter-pretable ones by humans without effort, i.e. the complex world is decomposed into basic concepts that is compre-hensible to most people. The insight is that there exist some general biases in humans’ deﬁnition of concepts, and they can be borrowed to heuristically guide a model to prefer a more interpretable representation than a noninterpretable one. These biases are not precise knowledge about individ-ual concepts, but some general information that is assumed to be shared by the interpretable concepts, so that noninter-pretable ones are ﬁltered out.
In this paper, we exploit two hypotheses about inter-pretability to learn disentangled representations. The ﬁrst one is Spatial Constriction: a representation is usually in-terpretable if we can consistently tell where the controlled variations are in an image. The second hypothesis is Percep-tual Simplicity: an interpretable code usually corresponds to a concept consisting of perceptually simple variations. For the ﬁrst one, we design a module to restrict the impact of each latent code in speciﬁc areas on feature maps during generation. For the second one, we design a loss to en-courage the model to embed simple data variations along each latent dimension. These two contributions are orthog-onal and can be used jointly. In addition, we show that for a disentangled model, its accumulated perceptual distance along latent axes are generally smaller than on other latent directions. This observation corresponds to the Perceptual
Simplicity assumption, and inspires us to propose an unsu-pervised model selection method. We conduct experiments on various datasets including CelebA, Shoes, Clevr, FFHQ,
DSprites and 3DShapes to evaluate our proposed modules.
We also conduct experiments to show that the proposed TPL score is an effective method for unsupervised model selec-tion. These experiments justify modeling of interpretability in learning disentangled representations. 2.