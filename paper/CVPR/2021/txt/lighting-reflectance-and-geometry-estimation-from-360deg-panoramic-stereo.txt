Abstract
We propose a method for estimating high-deﬁnition spatially-varying lighting, reﬂectance, and geometry of a scene from 360◦ stereo images. Our model takes advantage of the 360◦ input to observe the entire scene with geomet-ric detail, then jointly estimates the scene’s properties with physical constraints. We ﬁrst reconstruct a near-ﬁeld envi-ronment light for predicting the lighting at any 3D location within the scene. Then we present a deep learning model that leverages the stereo information to infer the reﬂectance and surface normal. Lastly, we incorporate the physical constraints between lighting and geometry to reﬁne the re-ﬂectance of the scene. Both quantitative and qualitative ex-periments show that our method, beneﬁting from the 360◦ observation of the scene, outperforms prior state-of-the-art methods and enables more augmented reality applications such as mirror-objects insertion. 1.

Introduction
Intrinsic decomposition of scene properties is a long-standing and essential task in computer vision. It includes the estimation of lighting, geometry, and reﬂectance of an arbitrary scene. Inferring the above properties of a scene en-ables us to develop various novel applications, especially in augmented reality, such as object insertion and scene modi-ﬁcation. It is a challenging and extremely under-constrained problem because of the complexity of light transportation on complicated geometry and various material reﬂectances in real-world. The majority of previous methods used per-spective cameras for solving this problem. However, the limited ﬁeld of view of a perspective camera results in the lack of observation of the entire scene, making this inverse problem even more intractable.
To overcome the problem, we propose a method that uses a pair of 360◦ images under equirectangular projection as input. Our method utilizes this input to bring up many ad-vantages that the perspective approach does not. Firstly, the 360◦ image captures the entire scene at once, offering us an adequate observation for lighting estimation. Secondly,
Figure 1. Our system consists of two 360◦ cameras in a top-bottom setting. We present the predicted reﬂectance and surface normal of the scene at the bottom-left of this ﬁgure. The bottom-right are vir-tual mirror-objects relighted by our illumination map at two differ-ent locations. Our method can recognize the geometric difference among 3D locations and preserve high-frequency information of the lighting, enabling us to insert mirror-objects around the entire scene with appealing reﬂection effects. the stereo input naturally encodes the depth information, making the geometry estimation possible. Furthermore, by jointly leveraging the physical constraints between lighting and geometry, the reﬂectance can be revealed. Figure. 1 il-lustrates the camera setting for capturing 360◦ stereo input and the estimated results of our method.
Leveraging 360◦ stereo input, we achieve two strengths in lighting estimation: (i) our lighting is spatially-varying and 3D coherent, which means the lighting will be differ-ent and changing smoothly for different 3D locations con-dition on the scene geometry; (ii) our lighting is in high-deﬁnition, which means it is generated in high-resolution and contains high-frequency details of the scene to enable mirror-like objects insertion. The lighting estimated by 110591
perspective methods rarely has these properties. They ei-ther estimate the lighting globally [20, 16] or per-pixel-individually [24, 11, 18], having no consistency between different locations. In addition, because of the limited ﬁeld of view in perspective images, prior works have difﬁcul-ties in ‘inferring’ the unseen regions of the scene in high-deﬁnition. But, our method naturally avoids this problem.
The reﬂectance estimation is also an ill-posed problem under the perspective cameras [4]. However, with the 360◦ stereo images, the input contains more information about the scene’s lighting and geometry, giving us substantial leads and more constraints to infer both reﬂectance and nor-mal.
In this paper, we present a method that utilizes the strengths of the 360◦ input to jointly estimate the high-deﬁnition and spatially-varying lighting, reﬂectance, and geometry of the entire scene. Our contributions are: 1. A near-ﬁeld environment that can generate spatially-varying and 3D coherent high-deﬁnition illu-mination maps when given any 3D location within the scene. light 2. A deep learning model that can estimate the reﬂectance and surface normal of the entire scene. 3. A rendering and reﬁnement model that leverages the physical constraints between lighting and geometry to jointly estimate a ﬁner reﬂectance. 2.