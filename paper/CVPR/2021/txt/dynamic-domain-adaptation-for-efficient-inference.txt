Abstract
Source
Target
Easy to Transfer
Hard to Transfer
Domain adaptation (DA) enables knowledge transfer from a labeled source domain to an unlabeled target do-main by reducing the cross-domain distribution discrep-ancy. Most prior DA approaches leverage complicated and powerful deep neural networks to improve the adaptation capacity and have shown remarkable success. However, they may have a lack of applicability to real-world situa-tions such as real-time interaction, where low target infer-ence latency is an essential requirement under limited com-putational budget. In this paper, we tackle the problem by proposing a dynamic domain adaptation (DDA) framework, which can simultaneously achieve efﬁcient target inference in low-resource scenarios and inherit the favorable cross-domain generalization brought by DA. In contrast to static models, as a simple yet generic method, DDA can integrate various domain confusion constraints into any typical adap-tive network, where multiple intermediate classiﬁers can be equipped to infer “easier” and “harder” target data dy-namically. Moreover, we present two novel strategies to fur-ther boost the adaptation performance of multiple predic-tion exits: 1) a conﬁdence score learning strategy to derive accurate target pseudo labels by fully exploring the predic-tion consistency of different classiﬁers; 2) a class-balanced self-training strategy to explicitly adapt multi-stage clas-siﬁers from source to target without losing prediction di-versity. Extensive experiments on multiple benchmarks are conducted to verify that DDA can consistently improve the adaptation performance and accelerate target inference un-der domain shift and limited resources scenarios. 1.

Introduction
Many intelligent technologies are boosted by the rapid development of computational capacity [32, 48, 44] and deep neural networks [43, 21, 9, 15]. To ensure high relia-bility, their loaded deep models have to be trained with mas-sive amount of data, so as to enumerate all possible practical scenarios. Unfortunately, there is always a future situation
†C. Liu is the corresponding author.
Aeroplane
Bicycle 
Car
Figure 1. Motivation of the proposed dynamic domain adapta-tion, which seeks to balance the cross-domain classiﬁcation per-formance and the computational cost for target inference. The target images in left column are easy to transfer with a small model, whereas the “harder” images require computational expen-sive models to be correctly recognized. that is unpredictable, and even an object in the same envi-ronment may display visual diversity at different times. For instance, the photos captured by cameras of self-driving car may exhibit large variations under different lighting con-ditions of day and night. This would inevitably lead to degraded recognition, since test data (target domain) and training data (source domain) follow different distributions.
Such a phenomenon is known as domain shift [47], which can be tackled by domain adaptation (DA) tech-niques [33]. To date there have been considerable research efforts in DA, ﬂourishing with impressive results, especially when applying deep neural networks [5, 29, 40, 23, 25].
They delve into searching for a feature space in which la-beled and rich information in source domain can be trans-ferred to unlabeled but related target domain. Generally, these prevailing deep DA methods leverage static and high-complexity base learners owing to their good transferable capacity brought by deep and wide architectures. However, they do not consider the transferability of different target samples as shown in Fig. 1. In consequence, they may not be applicable in some real-world situations that require real-time responses or are delay-sensitive to stringent computa-tional resource constraints at inference time.
To allow deep networks to get a grip back on fast in-ference, there are several techniques that can effectively re-7832
duce redundant computational burdens, including network pruning [22, 27, 54], architecture design [12, 17, 41], and knowledge distillation [10, 1]. Although computation accel-eration can be achieved, they are vulnerable to lightweight networks [12, 14] that are highly optimized. In contrast, an-other line of work is to explore adaptive inference [50, 31], which focuses on dynamically determining the inference structures conditioned on the complexity of input samples and has gained increasing attention recently. Nevertheless, all these methods suffer from poor generalization perfor-mance to a new domain, especially when the domain dis-crepancy is large. Even when state-of-the-art DA methods are applied to these models, as shown in the experiments, they still cannot achieve satisfying adaptation performance with efﬁcient inference guaranteed.
Therefore, there is a strong motivation to apply model on resource-constrained device to handle domain discrep-ancy without losing accuracy. To tackle this problem, in this paper, we propose a novel framework named Dynamic
Domain Adaptation (DDA), which can effectively equip vanilla domain adaptation with efﬁcient target inference to balance transferable performance and computational cost in the test phrase. Here, we take the representative adaptive network MSDNet [13] as our backbone network that has multiple intermediate classiﬁers at different depth of the network. It could save a large amount of computational cost on “easy” samples. Further, we expect that a qualiﬁed so-lution should be feasible in situations of anytime prediction and budget prediction ‡ under vanilla DA scenarios.
To be speciﬁc, on top of the multi-exist adaptive archi-tecture, we ﬁrst seek to apply domain confusion constraints to each of the classiﬁer to reduce cross-domain distribution discrepancy of multi-scale features. Based on the direct fea-ture alignment, the multiple classiﬁers should be able to achieve consistent predictions on samples that are “easy” to transfer. Thereby, these target data could be leveraged as
“labeled” data to further retrain the network with pseudo target supervision, which could signiﬁcantly improve the target prediction performance. Notably, different from aug-menting labeled target set relying on a single classiﬁer, here we exploit probability predictions from multiple classiﬁers and propose a novel and effective conﬁdence score strategy to discover highly conﬁdent pseudo-labeled target samples.
By leveraging the calculated conﬁdence score, a trustwor-thy target set with pseudo labels can be generated, and as the training proceeds, this target set will be more and more precise.
Based on the trustworthy target set, we then utilize the proposed class-balanced self-training strategy to retrain all the classiﬁers progressively while preserving the prediction
‡Anytime and budget predictions are two classical settings to evaluate the effectiveness of adaptive inference models, which has been described in detail in [13]. diversity among exits. As a result, the classiﬁers at dif-ferent stages will be gradually adapted from source to tar-get by the class-balanced self-training. In such a way, our method does not only maintain the efﬁciency of adaptive network, but also signiﬁcantly improve the transferability of each classiﬁer.
In general, we highlight the three-fold contributions.
• We propose a dynamic domain adaptation framework to simultaneously achieve satisfying DA performance and fast target inference with low computational cost, which successfully sheds new light on future direction for efﬁcient inference of DA towards resource-limited devices.
• Two simple yet effective strategies, conﬁdence score learning and class-balanced self-training, are intro-duced. By utilizing them, highly conﬁdent pseudo-labeled target samples can be selected to retrain all the classiﬁers, which could signiﬁcantly improve their adaptation performance.
• Comprehensive experimental results verify that the proposed method could greatly save time and compu-tational resources at both anytime and budget predic-tion settings with promising cross-domain recognition accuracy. 2.