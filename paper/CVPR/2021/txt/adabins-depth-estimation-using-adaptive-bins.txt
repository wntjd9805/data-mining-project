Abstract
We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neu-ral network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformer-based architecture block that divides the depth range into bins whose center value is estimated adaptively per image.
The ﬁnal depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins.
Our results show a decisive improvement over the state-of-the-art on several popular depth datasets across all metrics.
We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model. 1.

Introduction
This paper tackles the problem of estimating a high qual-ity dense depth map from a single RGB input image. This is a classical problem in computer vision that is essential for many applications [26, 30, 16, 6]. In this work, we pro-pose a new architecture building block, called AdaBins that leads to a new state-of-the-art architecture for depth estima-tion on the two most popular indoor and outdoor datasets,
NYU [36] and KITTI [13].
The motivation for our work is the conjecture that current architectures do not perform enough global analysis of the output values. A drawback of convolutional layers is that they only process global information once the tensors reach a very low spatial resolution at or near the bottleneck. How-ever, we believe that global processing is much more pow-erful when done at high resolution. Our general idea is to perform a global statistical analysis of the output of a tradi-tional encoder-decoder architecture and to reﬁne the output with a learned post-processing building block that operates at the highest resolution. As a particular realization of this idea, we propose to analyze and modify the distribution of the depth values.
Figure 1: Illustration of AdaBins: Top: input RGB images.
Middle: depth predicted by our model. Bottom: histogram of depth values of the ground truth (blue) and predicted bin density (red) with depth values increasing from left to right.
Note that the predicted bin-centers are focused near smaller depth values for closeup images but are widely distributed for images with a wider range of depth values.
Depth distribution corresponding to different RGB in-puts can vary to a large extent (see Fig. 1). Some images have most of the objects located over a very small range of depth values. Closeup images of furniture will, for example, contain pixels most of which are close to the camera while other images may have depth values distributed over a much broader range, e.g. a corridor, where depth values range from a small value to the maximum depth supported by the network. Along with the ill-posed nature of the problem, such a variation in depth distribution makes depth regres-sion in an end-to-end manner an even more difﬁcult task.
Recent works have proposed to exploit assumptions about indoor environments such as planarity constraints [25, 21] to guide the network, which may or may not hold for a real-world environment, especially for outdoors scenes.
Instead of imposing such assumptions, we investigate an approach where the network learns to adaptively focus on regions of the depth range which are more probable to occur 4009
Standard Encoder-Decoder
AdaBins Module
Hybrid Regression
Input RGB
H×W×3
Encoder
Decoder mViT
R
Conv
Softmax
N classes
Eq.3
Depth image h× w×1
Bin widths: b
Eq.2
Bin centers: c(b)
Figure 2: Overview of our proposed network architecture. Our architecture consists of two major components: an encoder-decoder block and our proposed adaptive bin-width estimator block called AdaBins. The input to our network is an RGB image of spatial dimensions H and W , and the output is a single channel h × w depth image (e.g., half the spatial resolution). in the scene of the input image.
Our main contributions are the following:
• We propose an architecture building block that per-forms global processing of the scene’s information.
We propose to divide the predicted depth range into bins where the bin widths change per image. The ﬁ-nal depth estimation is a linear combination of the bin center values.
• We show a decisive improvement for supervised single image depth estimation across all metrics for the two most popular datasets, NYU [36] and KITTI [13].
• We analyze our ﬁndings and investigate different mod-iﬁcations on the proposed AdaBins block and study their effect on the accuracy of the depth estimation. 2.