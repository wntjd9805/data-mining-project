Abstract
Building instance segmentation models that are data-efﬁcient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (e.g., [13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. How-ever, we ﬁnd that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training).
On COCO instance segmentation, we achieve 49.1 mask
AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to signiﬁcant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by
+3.6 mask AP on rare categories. 1 1.

Introduction
Instance segmentation [22, 10] is an important task in computer vision with many real world applications.
In-stance segmentation models based on state-of-the-art con-volutional networks [11, 57, 67] are often data-hungry.
At the same time, annotating large datasets for instance segmentation [40, 21] is usually expensive and time-consuming. For example, 22 worker hours were spent per
∗Equal contribution. Correspondence to: golnazg@google.com.
†Work done during an internship at Google Research. 1Code and checkpoints for our models are available at https :
//github.com/tensorflow/tpu/tree/master/models/ official/detection/projects/copy_paste
Figure 1. Data-efﬁciency on the COCO benchmark: Combining the Copy-Paste augmentation along with Strong Aug. (large scale jittering) allows us to train models that are up to 2× more data-efﬁcient than Standard Aug. (standard scale jittering). The aug-mentations are highly effective and provide gains of +10 AP in the low data regime (10% of data) while still being effective in the high data regime with a gain of +5 AP. Results are for Mask R-CNN EfﬁcientNet-B7 FPN trained on an image size of 640×640. 1000 instance masks for COCO [40]. It is therefore impera-tive to develop new methods to improve the data-efﬁciency of state-of-the-art instance segmentation models.
Here, we focus on data augmentation [50] as a simple way to signiﬁcantly improve the data-efﬁciency of instance segmentation models. Although many augmentation meth-ods such as scale jittering and random resizing have been widely used [26, 25, 20], they are more general-purpose in nature and have not been designed speciﬁcally for in-stance segmentation. An augmentation procedure that is more object-aware, both in terms of category and shape, is likely to be useful for instance segmentation. The Copy-Paste augmentation [13, 12, 15] is well suited for this need.
By pasting diverse objects of various scales to new back-ground images, Copy-Paste has the potential to create chal-lenging and novel training data for free. 2918
Figure 2. We use a simple copy and paste method to create new images for training instance segmentation models. We apply random scale jittering on two random training images and then randomly select a subset of instances from one image to paste onto the other image.
The key idea behind the Copy-Paste augmentation is to paste objects from one image to another image. This can lead to a combinatorial number of new training data, with multiple possibilities for: (1) choices of the pair of source image from which instances are copied, and the target im-age on which they are pasted; (2) choices of object instances to copy from the source image; (3) choices of where to paste the copied instances on the target image. The large variety of options when utilizing this data augmentation method al-lows for lots of exploration on how to use the technique most effectively. Prior work [12, 15] adopts methods for de-ciding where to paste the additional objects by modeling the surrounding visual context. In contrast, we ﬁnd that a sim-ple strategy of randomly picking objects and pasting them at random locations on the target image provides a signiﬁcant boost on top of baselines across multiple settings. Specif-ically, it gives solid improvements across a wide range of settings with variability in backbone architecture, extent of scale jittering, training schedule and image size.
In combination with large scale jittering, we show that the Copy-Paste augmentation results in signiﬁcant gains in the data-efﬁciency on COCO (Figure 1). In particular, we see a data-efﬁciency improvement of 2× over the com-monly used standard scale jittering data augmentation. We also observe a gain of +10 Box AP on the low-data regime when using only 10% of the COCO training data.
We then show that the Copy-Paste augmentation strategy provides additional gains with self-training [44, 73] wherein we extract instances from ground-truth data and paste them onto unlabeled data annotated with pseudo-labels. Using an EfﬁcientNet-B7 [56] backbone and NAS-FPN [17] ar-chitecture, we achieve 57.3 Box AP and 49.1 Mask AP on
COCO test-dev without test-time augmentations. This result surpasses the previous state-of-the-art instance seg-mentation models such as SpineNet [11] (46.3 mask AP) and DetectoRS ResNeXt-101-64x4d with test time aug-mentation [43] (48.5 mask AP). The performance also sur-passes state-of-the-art bounding box detection results of
EfﬁcientDet-D7x-1536 [57] (55.1 box AP) and YOLOv4-P7-1536 [61] (55.8 box AP) despite using a smaller image size of 1280 instead of 1536.
Finally, we show that the Copy-Paste augmentation re-sults in better features for the two-stage training procedure typically used in the LVIS benchmark [21]. Using Copy-Paste we get improvements of 6.1 and 3.7 mask AP on the rare and common categories, respectively.
The Copy-Paste augmentation strategy is easy to plug into any instance segmentation codebase, can utilize un-labeled images effectively and does not create training or inference overheads. For example, our experiments with
Mask-RCNN show that we can drop Copy-Paste into its training, and without any changes, the results can be eas-ily improved, e.g., by +1.0 AP for 48 epochs. 2.