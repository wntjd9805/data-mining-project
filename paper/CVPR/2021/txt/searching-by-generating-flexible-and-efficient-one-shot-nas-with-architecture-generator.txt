Abstract
In one-shot NAS, sub-networks need to be searched from the supernet to meet different hardware constraints. How-ever, the search cost is high and N times of searches are needed for N different constraints. In this work, we pro-pose a novel search strategy called architecture genera-tor to search sub-networks by generating them, so that the search process can be much more efﬁcient and ﬂexi-ble. With the trained architecture generator, given target hardware constraints as the input, N good architectures can be generated for N constraints by just one forward pass without re-searching and supernet retraining. More-over, we propose a novel single-path supernet, called uni-ﬁed supernet, to further improve search efﬁciency and re-duce GPU memory consumption of the architecture gener-ator. With the architecture generator and the uniﬁed super-net, we propose a ﬂexible and efﬁcient one-shot NAS frame-work, called Searching by Generating NAS (SGNAS). With the pre-trained supernt, the search time of SGNAS for N dif-ferent hardware constraints is only 5 GPU hours, which is 4N times faster than previous SOTA single-path methods.
After training from scratch, the top1-accuracy of SGNAS on ImageNet is 77.1%, which is comparable with the SO-TAs. The code is available at: https://github.com/ eric8607242/SGNAS. 1.

Introduction
It is time-consuming and difﬁcult to manually design neural architectures under speciﬁc hardware constraints.
Neural architecture search (NAS) [30][2][1] aiming at au-tomatically searching the best neural architecture is thus highly demanded. However, how to efﬁciently and ﬂexi-bly determine the architectures conforming to various con-straints is still very challenging [4].
Figure 1. Overview of SGNAS. Given the target hardware con-straint as the input, the architecture generator can generate archi-tecture parameters instantly within the inference time of one for-ward pass. With the generated parameters, the speciﬁc architec-tures can be sampled from the uniﬁed supernet.
For example, 2,000 GPU days are needed by an RL method
[1], and 3,150 GPU days are needed by the evolution algo-rithm [30].
To improve searching efﬁciency, one-shot NAS methods
[2][26][5][37] were proposed to encode the entire search space into an over-parameterized neural network, called a supernet. Once the supernet is trained, all sub-networks in the supernet can be evaluated by inheriting the weights of the supernet without additional training. One-shot NAS methods can be divided into two categories: differentiable
NAS (DNAS) and single-path NAS. utilizes additional
In addition to optimizing the supernet, DNAS differentiable
[16][26][37][38][28] parameters, called architecture parameters, to indicate the architecture distribution in the search space. Because
DNAS couples architecture parameters optimization for N different hardware with supernet optimization, constraints, the supernet and the architecture parameters should be trained jointly for N times to ﬁnd N different best architectures. This makes DNAS methods inﬂexible.
The earliest NAS methods were developed based on rein-forcement learning (RL) [33][1] or the evolution algorithm
[30]. However, extremely expensive computation is needed.
In contrast, single-path methods [18][10][9][40] decou-ple supernet training from architecture searching. For su-pernet training, only a single path consisting of one block in 983
each layer is activated and is optimized in one iteration. The main idea is to simulate discrete neural architectures in the search space and save GPU memory consumption. Once the supernet is trained, different search strategies, like the evolution algorithm [40][18], can be used to search the ar-chitecture under different constraints without retraining the supernet. Single-path methods are thus more ﬂexible than
DNAS. However, re-executing the search strategy N times for N different constraints is costly and not ﬂexible enough.
On top of one-shot NAS, we especially investigate ef-ﬁciency and ﬂexibility. For efﬁciency, we mean that, when the supernet is available, the time required to search the best architecture for a speciﬁc hardware constraint. For ﬂexibil-ity, we mean that, when N different hardware constraints are to be met, how much total time required to search for N best architectures. As a comparison instance, GreedyNAS
[40] takes almost 24 GPU hours to search for the best neural architecture under a speciﬁc constraint. Totally 24N GPU hours are required for N different constraints.
In this work, we focus on improving efﬁciency and ﬂex-ibility of the search strategy of the single-path method. The main idea is searching the best architecture by generating it. First, we decouple supernet training from architecture searching and train the supernet as a single-path method.
After obtaining the supernet, we propose to build an archi-tecture generator to generate the best architecture directly.
Given a hardware constraint as input, the architecture gen-erator can generate the architecture parameter within the inference time of one forward pass. This method is ex-tremely efﬁcient and ﬂexible. The total search time for var-ious hardware constraints of the architecture generator is only 5 GPU hours. Moreover, we do not need to re-execute search strategies or re-train the supernet once the architec-ture generator is trained. When N different constraints are to be met, the search strategy only needs to be conducted once, which is more ﬂexible than N searches required in previous single-path methods [10][40][9].
The aforementioned idea is on top of a trained supernet.
However, we notice that searching on a single-path supernet still requires a lot of GPU memory and time because of the huge number of supernet parameters and complex supernet structure. Previous single-path NAS methods [18][40][9] determine a block for each layer, and there may be different candidate blocks with various conﬁgurations. For example,
GreedyNAS [40] has 13 types of candidate blocks for each layer, and thus size of the search space is 13L, where L de-notes the total number of layers in the supernet. Inspired by the ﬁne-grained supernet in AtomNAS [28], we propose a novel single-path supernet called uniﬁed supernet to re-duce GPU memory consumption. In the uniﬁed supernet, we only construct a block called uniﬁed block in each layer.
There are multiple sub-blocks in the uniﬁed block, and each sub-block can be implemented by different operations. By combining sub-blocks, all conﬁgurations can be described in a block. In this way, the number of parameters of the uni-ﬁed supernet is much fewer than previous single-path meth-ods.
The contributions of this paper are summarized as fol-lows. With the architecture generator and the uniﬁed super-net, we propose Searching by Generating NAS (SGNAS), which is a ﬂexible and efﬁcient one-shot NAS framework.
We illustrate the process of SGNAS in Fig. 1. Given various hardware constraints as the input, the architecture genera-tor can generate the best architecture for different hardware constraints instantly in one forward pass. After training the best architecture from scratch, the evaluation results show that SGNAS achieves 77.1% top-1 accuracy on the Ima-geNet dataset [14] at around 370M FLOPs, which is compa-rable with the state-of-the-arts of the single-path methods.
Meanwhile, SGNAS outperforms SOTA single-path NAS in terms of efﬁciency and ﬂexibility. 2.