Abstract
Scene text retrieval aims to localize and search all text instances from an image gallery, which are the same or similar with a given query text. Such a task is usually re-alized by matching a query text to the recognized words, outputted by an end-to-end scene text spotter. In this pa-per, we address this problem by directly learning a cross-modal similarity between a query text and each text in-stance from natural images. Speciﬁcally, we establish an end-to-end trainable network, jointly optimizing the pro-cedures of scene text detection and cross-modal similar-ity learning. In this way, scene text retrieval can be sim-ply performed by ranking the detected text instances with the learned similarity. Experiments on three benchmark datasets demonstrate our method consistently outperforms the state-of-the-art scene text spotting/retrieval approaches.
In particular, the proposed framework of joint detection and similarity learning achieves signiﬁcantly better per-formance than separated methods. Code is available at: https://github.com/lanfeng4659/STR-TDSL. 1.

Introduction
In the past few years, scene text understanding has re-ceived rapidly growing interest from this community due to a large amount of everyday scene images that contain texts. Most previous approaches for scene text understand-ing focus on the topics of text detection, text recognition and word spotting, succeeding in many practical applica-tions such as information security, intelligent transportation system [38, 29], geo-location, and visual search [3]. Differ-ent from the above topics, we study the task of scene text retrieval introduced by Mishra et al. [26], aiming to search all the text instances from a collection of natural images, which are the same or similar with a given text query. Dif-ferent from a scene text reading system that must localize and recognize all words contained in scene images, scene text retrieval only looks for the text of interest, given by a
*Corresponding author.
Figure 1. Given a query text “google”, the end-to-end scene text retrieval method aims to retrieve the images containing “google” from gallery, as well as their locations in the images. user. Such a task is quite useful in many applications in-cluding product image retrieval, book search in library [37] and key frame extraction of videos [30].
As depicted in Fig. 1, the goal of scene text retrieval is to return all images that are likely to contain the query text, as well as the bounding boxes of such text. In this sense, scene text retrieval is a cross-modal retrieval/matching task that aims to close the semantic gap between a query text and each text proposal. Traditional text retrieval meth-ods [2, 1, 5, 7] are often designed to handle cropped doc-ument text images. Since the well-cropped bounding boxes are not easy to obtain by a detector for natural images, these methods cannot be directly applied in scene text retrieval.
Mishra et al. [26] ﬁrst study text retrieval in scene images, which is cast as two separated sub-tasks: text detection and text image retrieval. However, the performance is limited, as their framework is designed based on handcraft features.
Another feasible solution to scene text retrieval is based on an end-to-end text recognition system, such as [14, 12].
Under this setting, the retrieval results are determined ac-cording to the occurrences of the given query word within the spotted words. As veriﬁed by [26, 6], such methods of-4558
ten achieve unsatisfactory retrieval performance, as the end-to-end text recognition system is optimized with a different evaluation metric that requires high accuracy in terms of both detection and recognition. However, for a retrieval en-gine, more text proposals can be leveraged to reduce the per-formance loss brought by the missed detections. Gomez et al. [6] directly predict the corresponding Pyramidal His-togram Of Character [2] (PHOC) of each text instance for ranking. However, these methods don’t explicitly learn the cross-modal similarity.
In this paper, we propose a new deep learning frame-work for scene text retrieval that combines the stages of text detection and cross-modal similarity learning. Our ba-sic idea is motivated by the recent end-to-end text spotting methods [22, 24, 17, 12, 34, 18], which unify the two sub-tasks of text detection and text recognition with an end-to-end trainable network. Such methods usually achieve better spotting performance than those optimized separately, ben-eﬁting from feature sharing and joint optimization. Specif-ically, our network for scene text retrieval is optimized by two tasks: a text detection task that aims to predict bound-ing boxes of candidate text instances, and a similarity learn-ing task for measuring the cross-modal similarity between a query text and each bounding box. With joint optimiza-tion, the text detection task and similarity learning task are complementary to each other. On the one hand, the detec-tion task can pay more attention to the recall than the pre-cision in order to avoid missing detections, as the feature matching process by the similarity learning task can effec-tively eliminate false alarms. On the other hand, the faithful cross-modal similarity provided by the similarity learning task can be used for looking for an accurate bounding box that contains a given text. In addition, both tasks share CNN features, signiﬁcantly improving the inference speed.
Unlike general objects such as person and car, scene text is a kind of sequence-like object. In order to learn a proper similarity between a query text and a text proposal, we adopt the normalized edit distances as the supervision of the pairwise loss function, which has been widely used in string matching. However, learning such similarity is not easy, as the similarity values of all word pairs for training are not evenly distributed in the similarity space. For exam-ple, the number of word pairs with low similarity is much larger than those with high similarity. As a result, the net-work is prone to distinguish dissimilar word pairs but difﬁ-cult to similar word pairs. To ease this problem, we propose a word augmentation method: a series of pseudowords that are very similar to a given query text, are randomly gener-ated, which are fed into the network together with the query text during the training process.
The contribution in this work is three-fold. First, we present a new end-to-end trainable network for joint op-timizing scene text detection and cross-modal similarity learning, which is able to efﬁciently search text instances from natural images that contain a query text. Second, we propose a word augmentation method by generating similar pseudowords as input queries, which enhances the discrim-inatory power of the learned similarity. Third, we collect and annotate a new dataset for Chinese scene text retrieval, consisting of 23 pre-deﬁned query words and 1667 Chinese scene text images. This dataset is adopted to verify the ef-fectiveness of text retrieval methods over non-latin scripts. 2.