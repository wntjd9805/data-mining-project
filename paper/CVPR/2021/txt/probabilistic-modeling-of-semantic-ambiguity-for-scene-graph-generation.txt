Abstract
To generate “accurate” scene graphs, almost all exist-ing methods predict pairwise relationships in a determin-istic manner. However, we argue that visual relationships are often semantically ambiguous. Speciﬁcally, inspired by linguistic knowledge, we classify the ambiguity into three types: Synonymy Ambiguity, Hyponymy Ambiguity, and
Multi-view Ambiguity. The ambiguity naturally leads to the issue of implicit multi-label, motivating the need for diverse
In this work, we propose a novel plug-and-predictions. play Probabilistic Uncertainty Modeling (PUM) module. It models each union region as a Gaussian distribution, whose variance measures the uncertainty of the corresponding vi-sual content. Compared to the conventional determinis-tic methods, such uncertainty modeling brings stochasticity of feature representation, which naturally enables diverse predictions. As a byproduct, PUM also manages to cover more ﬁne-grained relationships and thus alleviates the is-sue of bias towards frequent relationships. Extensive exper-iments on the large-scale Visual Genome benchmark show that combining PUM with newly proposed ResCAGCN can achieve state-of-the-art performances, especially under the mean recall metric. Furthermore, we show the universal ef-fectiveness of PUM by plugging it into some existing models and provide insightful analysis of its ability to generate di-verse yet plausible visual relationships.
Figure 1. Examples of semantic ambiguity in Visual Genome dataset. The ﬁrst two columns show the comparisons between two plausible predicates for similar visual scenes and the right-most column illustrates the corresponding phenomenons in seman-tic space. (a) carrying and holding share overlapping deﬁni-tions and are interchangeable to describe the relationship between a man and an umbrella. (b) Both on and laying on are reason-able to describe the scene where a cat is on top o a bench, despite their semantic speciﬁcity difference. (c) Different human annota-tors focus on different points of view, i.e. using (actional) vs. in front of (spatial), to describe a working man and a laptop. 1.

Introduction
∗Equal contribution.
†Work done in part during an internship at Tencent AI Lab.
‡Yong Zhang, Baoyuan Wu and Yujiu Yang are the corresponding au-thors. This research was partially supported by the Key Program of Na-tional Natural Science Foundation of China under Grant No. U1903213 and the Guangdong Basic and Applied Basic Research Foundation (No. 2019A1515011387). Baoyuan Wu is supported by the Natural Science
Foundation of China under grant No. 62076213, the university develop-ment fund of the Chinese University of Hong Kong, Shenzhen under grant
No. 01001810, and the special project fund of Shenzhen Research Institute of Big Data under grant No. T00120210003.
Scene graph generation (SGG) has been an important task in computer vision, serving as an intermediate task to bridge the gap between upstream object detection [21] and downstream high-level visual understanding tasks, such as image captioning [33, 42] and visual question answer-ing [36].
Intuitively, the latter would get greater beneﬁt from more human-like scene graphs.
Almost all existing works [30, 35, 4, 24] view SGG as an objective task and predict pairwise relationships in a de-12527
terministic manner. Namely, given a pair of objects, they always generate an identical predicate. However, compared with humans, such methods pursue “accurate” scene graphs but overlook the intrinsic semantic ambiguity of visual re-lationships. Speciﬁcally, the collaborative annotations from human annotators tend to be diverse, covering different de-scriptions of relationships for similar visual scenes.
We observe that there exist multiple types of semantic
In-ambiguity in the large-scale Visual Genome dataset. spired by linguistic knowledge, we classify the ambigu-ity into three types. The ﬁrst type is Synonymy Ambi-guity, where multiple synonymous predicates that share overlapping deﬁnitions are suitable to describe similar vi-sual scenes. For example, in Figure 1 (a), carrying and holding are interchangeable to describe the relationship between a man and an umbrella. If we visualize these two words in the semantic space, they should point to the same position where the visual relationship lies. The second one is Hyponymy Ambiguity, indicating that different humans tend to use predicates across adjacent abstract levels. One would simply use on to describe the visual scene where a cat is on top of a bench, while others may choose to use more ﬁne-grained laying on, as shown in Figure 1 (b).
In this case, laying on is a hyponym of on. Namely, the semantic range of the former is included by that of the latter. As for the third type, we notice that different human annotators often focus on different types of visual relation-ships, which originate from different points of view. There-fore, we refer to this phenomenon as Multi-view Ambigu-ity. An example is illustrated in Figure 1 (c), where both using (actional) and in front of (spatial) are plausi-ble to describe the relationship between a working man and a laptop. If we consider the visual scene in three-dimension space, it can be a multicolor sphere that reﬂects different colors from different views. Although most predicates have single labels in the dataset, due to the ubiquitous semantic ambiguity mentioned above, we argue that many of them should have multiple labels, since similar visual scenes are annotated as different predicates. We refer to the issue as an implicit multi-label problem, which motivates the need to generate diverse predictions for visual relationships.
In this work, we focus on modeling the semantic am-biguity of visual relationships and propose a novel plug-and-play Probabilistic Uncertainty Modeling (PUM) mod-ule which can be easily deployed in any existing SGG model. Speciﬁcally, we utilize a probability distribution to represent each union region, rather than a deterministic fea-ture vector as in previous methods. From a geometric per-spective, the probabilistic representation allows us to map each visual relationship to a soft region in space, instead of merely a single point [26]. For ease of modeling, we adopt
Gaussian distributions to represent them. Namely, each union region is now parametrized by a mean and variance.
The former acts like the normal feature vector as in the con-ventional model, whereas the latter measures the feature un-certainty. To some extent, in this way, the feature instance of each union region can be viewed as a random variable drawn from a Gaussian distribution. Thanks to this uncer-tainty modeling, ambiguous union regions will be assigned to Gaussian distributions with large variances, which gen-erate diverse samples and result in diverse predictions. As a byproduct, we ﬁnd that PUM also manages to cover more
ﬁne-grained relationships and thus well alleviates the infa-mous issue of bias towards frequent relationships [4, 24].
We ﬁrstly demonstrate the effectiveness of PUM on the
Visual Genome benchmark. Combining with the newly pro-posed Residual Cross-attention Graph Convolutional Net-work (ResCAGCN) in concurrent work [39], we achieve state-of-the-art performances under the existing evaluation metrics, especially the mean recall. Note that our PUM can serve as a plug-and-play component. Therefore, we plug
PUM into state-of-the-art models and observe obvious uni-versal improvement over these baselines, which mainly lies in the mean recall again. We owe the performance gain in the mean recall to the ability to generate diverse relation-ships, which improves the chances to hit the ground-truth with infrequent predicate labels. We further propose ora-cle recall as an indirect evaluation metric to measure the diversity of multiple inferences, which takes results of mul-tiple consecutive predictions as an ensemble and computes recall. The oracle recall of the proposed model increases with the number of predictions, indicating that the model generates plausible diverse relationships and thus gradually covers the ground-truth more and more.
Overall, our contributions can be summarized as follows:
• We identify the semantic ambiguity of visual relation-ships and propose a novel plug-and-play Probabilistic
Uncertainty Modeling (PUM) module, which utilizes a probability distribution to represent each union region instead of a deterministic feature vector.
• Combining PUM with ResCAGCN, we achieve state-of-the-art performances on the large-scale Visual
Genome benchmark under the existing evaluation met-rics, especially the mean recall.
• Extensive evaluations demonstrate the superiority of
PUM to alleviate the bias towards frequent categories when plugged into the existing SGG models, reﬂected in the improvement on the mean recall.
• To the best of our knowledge, we are the ﬁrst to ex-plore diverse predictions for SGG. We conduct exper-iments both qualitatively and quantitatively to demon-strate that the proposed PUM module can generate di-verse yet plausible relationships. 12528
Figure 2. Existing SGG framework usually includes the following steps: (a) utilizing Faster R-CNN to obtain object proposals; (b) fusing features globally to obtain object labels, object features, and union region features; (c) conditioned on the results of the previous step, modeling each union region as a deterministic vector to predict the relationship. In this work, we replace (c) with Probabilistic Uncertainty
Modeling (PUM) in (d), where each union region is represented by a probability distribution instead. In such way, diversity in scene graph generation is naturally achieved. 2.