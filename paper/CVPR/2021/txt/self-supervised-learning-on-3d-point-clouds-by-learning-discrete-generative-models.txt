Abstract
While recent pre-training tasks on 2D images have proven very successful for transfer learning, pre-training for 3D data remains challenging. In this work, we intro-duce a general method for 3D self-supervised representa-tion learning that 1) remains agnostic to the underlying neural network architecture, and 2) speciﬁcally leverages the geometric nature of 3D point cloud data. The pro-posed task softly segments 3D points into a discrete number of geometric partitions. A self-supervised loss is formed under the interpretation that these soft partitions implic-itly parameterize a latent Gaussian Mixture Model (GMM), and that this generative model establishes a data likelihood function. Our pretext task can therefore be viewed in terms of an encoder-decoder paradigm that squeezes learned rep-resentations through an implicitly deﬁned parametric dis-crete generative model bottleneck. We show that any ex-isting neural network architecture designed for supervised point cloud segmentation can be repurposed for the pro-posed unsupervised pretext task. By maximizing data like-lihood with respect to the soft partitions formed by the un-supervised point-wise segmentation network, learned repre-sentations are encouraged to contain compositionally rich geometric information. In tests, we show that our method naturally induces semantic separation in feature space, re-sulting in state-of-the-art performance on downstream ap-plications like model classiﬁcation and semantic segmenta-tion. 1.

Introduction
There has been a growing emergence of increasingly ef-fective self-supervised learning methods developed in the
Instead of human-form of unsupervised pretext tasks. annotated supervision, the pretext task itself is designed to create its own supervisory signal. For example, learn-ing to predict or discriminate data augmentations that pre-serve the semantics of the input have recently been shown to yield rich latent representations for downstream tasks [18, 6, 34, 33, 14]. One of the longstanding goals of unsu-pervised learning has been to improve transfer learning to the point where unsupervised pre-training combined with downstream supervision outperforms the traditional fully supervised training pipeline.
In recent years, we’ve seen this come to fruition in several domains, with methods like BERT [11] for NLP, and BYOL [18], SimCLR [6, 7],
CPC [34, 22], and others claiming top performance on im-age classiﬁcation benchmarks.
Compared to deep learning for NLP and 2D computer vision, deep learning for 3D perception remains a relatively nascent ﬁeld and self-supervised 3D learning even more so.
One could argue there are several reasons for this, but the most salient is perhaps the lack of a common representa-tion: while NLP has word embeddings and 2D computer vision has 2D images, 3D data enjoys no such universal and obvious data structure. The basic representation for 3D data is extremely fractured: for 3D content creation, trian-gular meshes are the de facto standard, yet most 3D sen-sors produce raw data in the form of 3D point clouds. Tra-ditional 3D vision algorithms leverage structures like Oc-trees [23] or Hashed Voxel Lists [32], but deep learning approaches favor structures more amenable to differentia-bility and/or efﬁcient neural processing, like sparse voxel networks [8, 44], implicit functions [30], graphs [27], or point-based networks [36, 37, 46]. Thus, it is still an active area of research to ﬁnd the proper universal “3D backbone” that can become as ubiquitous as ResNet [21] is for learning representations of 2D images.
In terms of self-supervised learning on 3D data, the lack of standardization of basic 3D data processing further mag-niﬁes these issues since any technique designed for a partic-ular architecture or data representation might have limited long-term utility. Furthermore, though it is relatively easy to obtain large amounts of unlabeled 3D data given the re-cent proliferation of self-driving cars and cheap commodity 3D sensors [53, 25], it can be difﬁcult and time-consuming to produce accurate ground truth 3D annotations. Thus, it could be argued that the relative need for strong performing self-supervised methods for 3D data is much higher than in the 2D regime.
In this work, our goal is to try to devise a self-supervised 8248
(a) Traditional Autoencoder Bottleneck (b) Proposed “Parametric” Bottleneck
Figure 1. In a traditional autoencoder paradigm, the input data P is passed through a latent bottleneck z from which the original input is explicitly reconstructed. That is, ˆP = dφ(z) and z = eψ(P). The autoencoding task is then deﬁned by a loss according to a predeﬁned distance measure between the input P and output point cloud ˆP (e.g. Chamfer Loss). In our proposed setup, the bottleneck layer has an interpretable form as the constituent parameters Θ of a discrete generative model (GMM). Given any point-wise classiﬁcation network fψ, we design a ﬁxed parameter-less function ξ such that Θ = ξ(P, fψ(P)). The PDF over 3D space is deﬁned by g(·; Θ). Instead of explicitly generating ˆP by iid sampling from the generative model g, we can use the negative log likelihood of the input P to directly impose a loss. 3D representation learning method that 1) remains agnostic to the speciﬁc choice of 3D representation or the underlying neural network architecture, and 2) speciﬁcally leverages the geometric nature of 3D point cloud data. To this end, we propose a pre-training task that can be applied to any off-the-shelf network architecture that outputs point-wise classiﬁcation scores (e.g. logits), which we connect to the geometric nature of 3D point clouds by re-interpreting these classiﬁcation scores in the context of probabilistic geomet-ric spatial partition assignments. Any network designed for common point-wise classiﬁcation tasks like semantic seg-mentation [3, 2] or part segmentation [4] can be leveraged without modiﬁcation for our proposed 3D representation learning task, regardless of whether the architecture’s un-derlying representation uses voxels, SDFs, graphs, unstruc-tured points, etc.
Unlike a traditional supervised semantic segmentation paradigm, however, we have no supervision in the form of per-point class labels for ground truth partition assignment.
Thus, if we wish to utilize segmentation networks to learn representations in an unsupervised fashion, we need to cre-ate something like “pseudo-labels” automatically from the data itself that we can compare against during training in order to develop a self-supervised loss function.
One way to solve this problem is by the so-called jigsaw type approaches (e.g. [33, 41]) that coarsely discretize the input space and create this pseudo-label directly from the voxel-id that each particular point falls into. Then, the vox-els are randomly permuted and the neural network’s task is to classify the voxel-id of each point. Though these have seen success in the 3D domain [41], the voxel-wise permu-tation operation leads to a destruction of the point cloud’s original overall global geometry, even when its global ge-ometry is arguably the strongest semantic cue. The jigsaw method therefore must rely on learning local (intra-voxel) features in order to position points globally, such that the global layout is only implicitly learned as a byproduct of local feature learning.
Compared to jigsaw tasks that learn to reassemble pic-tures or point clouds from a set of permuted disjoint parti-tions/voxels, our proposed pretext task learns the partition-ing function itself to softly assign point clouds into geomet-rically coherent overlapping clusters. In doing so, we avoid any augmentation of the data that might degrade its geo-metric coherency and thus its semantic information. How-ever, this leads to the question: what differentiates a “good” partitioning from a “bad” partitioning? Here, we take in-spiration from recent work on deep learning for point cloud registration [52], where given two point clouds offset by an unknown rotation and translation, a point cloud segmen-tation network is used to implicitly infer a pair of latent and transformation-equivariant Gaussian Mixture Models (GMMs).
In contrast to this work, however, we remove the registration objective completely and instead adapt this method to work only on a single point cloud without any spatial transformation or data augmentation necessary. To do this, we directly utilize the data likelihood of the implic-itly deﬁned GMM.
Given N points and J “pseudo-classes” and the N × J matrix of logits S from an underlying point cloud segmen-tation network, we propose dual interpretations of S: 1) S partitions 3D space: S deﬁnes a soft spatial partition-ing in 3D space of the input data into J discrete partitions. 2) S predicts latent posteriors: S calculates the posterior log probabilities of a set of latent binary variables that cor-respond each point to one of J components of a latent gen-8249
erative mixture model.
If we allow the latent model in the second interpreta-tion to be implicitly deﬁned through the ﬁrst interpretation, we can assert “goodness-of-ﬁt” evaluations by calculating the total data likelihood of the input point cloud with re-spect to the spatial density deﬁned by the mixture model.
In doing so, our self-supervision becomes the likelihood of this discrete generative model with respect to the input.
One can view this proposed design as learning to proba-bilistically autoencode through a parametric discrete gen-erative model bottleneck (termed “parametric bottleneck” elsewhere in this paper), where instead of a learned decoder, as is normally the case when autoencoding, we directly uti-lize the 3D PDF induced by the speciﬁc values of the bot-tleneck layer. Refer to Figure 1 for a graphical depiction of these differences. In summary, for a given point cloud, our pretext task is to learn the parameters of a point-wise clas-siﬁcation network such that its output S produces the most likely spatial partitioning of these 3D points with respect to the discrete generative model implicitly deﬁned by S. 2.