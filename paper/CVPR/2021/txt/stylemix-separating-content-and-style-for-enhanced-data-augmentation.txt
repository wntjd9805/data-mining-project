Abstract
In spite of the great success of deep neural networks for many challenging classiﬁcation tasks, the learned networks are vulnerable to overﬁtting and adversarial attacks. Re-cently, mixup based augmentation methods have been ac-tively studied as one practical remedy for these drawbacks.
However, these approaches do not distinguish between the content and style features of the image, but mix or cut-and-paste the images. We propose StyleMix and StyleCutMix as the ﬁrst mixup method that separately manipulates the con-tent and style information of input image pairs. By care-fully mixing up the content and style of images, we can cre-ate more abundant and robust samples, which eventually enhance the generalization of model training. We also de-velop an automatic scheme to decide the degree of style mix-ing according to the pair’s class distance, to prevent messy mixed images from too differently styled pairs. Our exper-iments on CIFAR-10, CIFAR-100 and ImageNet datasets show that StyleMix achieves better or comparable perfor-mance to state of the art mixup methods and learns more robust classiﬁers to adversarial attacks. 1.

Introduction
Although deep neural networks have achieved remark-able achievements in many classiﬁcation tasks [18, 15, 31, 10], the learned networks are vulnerable to overﬁt-ting and adversarial attacks. As one of the most promis-ing approaches to alleviate these issues, the data augmen-tation and regularization methods have been actively stud-ied [30, 4, 29, 27, 1, 16]. As the earliest work, Mixup [30] augments a new sample by mixing two samples via interpo-lation of both images and labels. Instead of using the entire object regions, CutMix [29] cuts and pastes patches from one image onto the other image along with the ground truth labels being mixed proportionally to the area of patches.
Manifold Mixup [27] regularizes to prevent the network
*Equal contribution from overﬁtting on the intermediate representations that in-terpolate hidden states. These approaches certainly improve classiﬁcation performance and robustness over input noise and attacks, but they do not distinguish between the content and style of the images for generating new mixup samples.
Content generally refers to the shape and form of an image, while the style primarily includes texture and color [5]. If convolutional neural networks are trained with no distinc-tion between content and style, they tend to be biased to fo-cus on a small faction of information to learn categories [6] (e.g. an elephant is identiﬁed using only gray skin texture no matter how the foreground looks like).
In this work, we argue that carefully mixing up the con-tent and style of two input images can beneﬁt creating more abundant and robust samples, which eventually enhance the generalization of model training. The style transfer meth-ods [5, 14, 26, 19] have shown great progress recently and proved that deep networks encode not only the content but also the style information of images and offer the possibil-ity to alter the distribution of low-level visual features in the image while preserving the semantic content. Thus, by separately considering the content and style of each input image, we have two levels of options to augment training data such as generating content-preserved images with di-verse styles, varying foreground objects in the same styled images, or both. Thus, classiﬁers are fully learned to recog-nize both content and style features to enhance the classiﬁ-cation performance and robustness over noise and attacks.
We propose StyleMix as a new mixup method for data augmentation that can generate various training samples through convex combinations of content and style charac-teristics (Figure 1). We then extend to StyleCutMix that al-lows sub-image level manipulation based on the cut-and-paste idea of CutMix [29]. Finally, we develop a scheme to automatically decide the degree of style mixing accord-ing to the class distance between a given pair of images.
With classiﬁcation experiments on CIFAR-10 [17], CIFAR-100 [17] and ImageNet datasets [3], each of our propos-als nontrivially improves the classiﬁcation performance and eventually attains comparative performance to state-of-the-14862
Input 1
Input 2
Mixup
StyleMix
CutMix StyleCutMix
Method
Content  label
Parrot 0.5
Panda 0.5
Parrot 0.4
Panda 0.6
Parrot 0.2
Panda 0.8
Parrot 0.2
Panda 0.8
Style  label
✘
Parrot 0.8
Panda 0.2
✘
Parrot 0.6
Panda 0.4
Figure 1: Visual comparison of our StyleMix and Style-CutMix with previous approaches using two inputs from
Rainbow lorikeet and Red panda. While Mixup [30] and
CutMix [29] are done at the image level, our methods sep-arately consider the content and style of images to create more abundant and robust samples. art mixup methods. The contributions of this work can be outlined as follows: 1. We propose StyleMix as the ﬁrst mixup method that separately manipulates the content and style informa-tion of input image pairs, to the best of our knowledge. 2. Contrary to style transfer tasks where the content and style inputs are clearly identiﬁed, in the mixup context, two input images are any pairs from the training set.
To prevent messy mixed images from too differently styled pairs that signiﬁcantly harm the performance, we propose an automatic scheme to decide the degree of style mixing according to the pair’s class distance. 3. In the experiments, StyleCutMix outperforms state-of-the-art mixup methods on CIFAR-10/100 and is comparable on ImageNet, including Cutout [4], Grid-Mix [1], Manifold Mixup [27], CutMix [29], Aug-Mix [11] and Puzzle Mix [16]. We also show that our methods improve the robustness of classiﬁers to adver-sarial attacks better than other recent mixup methods. 2.