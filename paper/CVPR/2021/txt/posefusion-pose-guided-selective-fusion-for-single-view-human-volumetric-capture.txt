Abstract
We propose POse-guided SElective Fusion (POSEFu-sion), a single-view human volumetric capture method that leverages tracking-based methods and tracking-free infer-ence to achieve high-ﬁdelity and dynamic 3D reconstruc-tion. By contributing a novel reconstruction framework which contains pose-guided keyframe selection and robust implicit surface fusion, our method fully utilizes the advan-tages of both tracking-based methods and tracking-free in-ference methods, and ﬁnally enables the high-ﬁdelity recon-struction of dynamic surface details even in the invisible re-gions. We formulate the keyframe selection as a dynamic programming problem to guarantee the temporal continuity of the reconstructed sequence. Moreover, the novel robust implicit surface fusion involves an adaptive blending weight to preserve high-ﬁdelity surface details and an automatic collision handling method to deal with the potential self-collisions. Overall, our method enables high-ﬁdelity and dynamic capture in both visible and invisible regions from a single RGBD camera, and the results and experiments show that our method outperforms state-of-the-art methods. 1.

Introduction
Human volumetric capture, due to their potential value in holographic communication, online education, games and the movie industry has been a popular topic in computer vision and graphics for decades. Multi-view camera array methods [4, 11, 28, 5, 29, 49, 32, 9, 37, 21] can achieve high-ﬁdelity human volumetric capture using multiple RGB or depth sensors but suffer from sophisticated equipment or run-time inefﬁciency, which limits their application deploy-ment.
In contrast, single-view human volumetric capture
[23, 59, 12, 34, 52, 14, 38, 39, 15, 25, 44] has attracted more and more attention for its convenient setup.
Current methods for single-view human volumetric cap-ture can be roughly classiﬁed into two categories: tracking-based methods and tracking-free ones. Tracking-based methods utilize a pre-scanned template [23, 12, 14, 15] or continuously fused mesh [34, 52, 44] as the reference model, and solve or infer the deformation of the reference
Figure 1. High-ﬁdelity and dynamic results reconstructed using our method. model parameterized by embedded skeletons [50, 3, 47], node graph [23, 34], parametric body models (e.g., SMPL
[30]) [57], or a combination of them [51, 52, 14, 15]. In these tracking-based methods, previous observations are in-tegrated into the current frame after calculating the defor-mations across frames, thus plausible geometric details are preserved in the invisible regions. In addition, the recon-structed models are temporally continuous thanks to the frame-by-frame tracking. However, none of their defor-mation representations, neither skeletons nor node graph, is able to describe topological changes or track extremely large non-rigid deformations (Fig. 7(c)), which is an inher-ent drawback of the tracking-based methods.
On the other end of the spectrum, tracking-free meth-ods [46, 33, 10, 42, 58, 1, 56, 38] mainly focus on geomet-ric and/or texture recovery from a single RGB(D) image.
By learning from a large amount of 3D human data, these methods demonstrate promising human reconstruction with high-ﬁdelity details in visible regions and plausible shape in invisible areas [38, 39]. As the reconstruction for the current frame is independent from the previous frames, these meth-ods can easily handle topological changes. However, their results may deteriorate in the cases of challenging human poses and/or severe self-occlusions. Besides, the recon-structions in the invisible regions are usually oversmoothed for the lack of observations (Fig. 7(d)). What’s worse, tracking-free methods are incapable of generating tempo-rally continuous results when applied on video inputs.
By reviewing the advantages and the drawbacks of these two types of methods, it is easy to notice that tracking-based methods and tracking-free inference are naturally comple-mentary as shown in Tab. 1. A straightforward way is to 14162
combine both branches by integrating the inferred models of all the other frames in the monocular RGBD sequence into the current frame to recover the invisible regions. The beneﬁts of such a pipeline are: a) topological changes and large deformations can be accurately reconstructed using tracking-free inference directly, b) the invisible surfaces can be faithfully recovered by integrating the other frames into current frame, and ﬁnally c) temporal continuity is guar-anteed by tracking the whole sequence frame-by-frame.
However, the afore-mentioned pipeline still has limitations.
Speciﬁcally, if we fuse all the other frames indiscriminately, we can only generate static surfaces with all the dynamic changing details averaged together. Moreover, it remains difﬁcult for such a pipeline to handle the artifacts caused by self-collisions. To this end, we further propose POse-guided SElective Fusion (POSEFusion), a novel pipeline that contains pose-guided keyframe selection and adaptive implicit surface fusion. In this pipeline, we only integrate the keyframes selected by our proposed pose-guided met-ric, which takes into account both visibility complementar-ity and pose similarity.
Our key observations are: a) keyframes with similar poses to the current frame enable the recovery of physically plausible dynamic details, b) keyframes with complemen-tary viewpoints to the current frame avoid to oversmooth the visible regions, and c) the adaptive fusion considers depth observations and visibility promotes to preserve the surface details and resolves collision artifacts. Based on these ob-servations, the limitations of the simple pipeline are suc-cessfully overcome.
Speciﬁcally, we start with SMPL [30] tracking for all the frames given a monocular RGBD sequence as input.
We then utilize the SMPL model as a robust and effective proxy and propose a novel criterion to quantify pose similar-ity and visibility complementarity. Based on this criterion, we select appropriate keyframes for each frame. Note that per-frame keyframe selection cannot guarantee the tempo-ral continuity of invisible details; therefore, we further for-mulate the selection as a dynamic programming of min-cost path to reconstruct dynamic and temporally continuous in-visible details.
In implicit surface fusion, we propose an adaptive blending weight which considers depth and visi-bility information to avoid oversmooth fusion and preserve the observed details. Finally, we propose an automatic col-lision handling scheme to deal with possible self-collisions while maintaining adjacent details.
In summary, this paper proposes the following technical contributions:
• A new human volumetric capture pipeline that leverages tracking-based methods and tracking-free inference, and achieves high-ﬁdelity and dynamic reconstruction in both visible and invisible regions from a single RGBD camera (Sec. 3.2).
• A new pose-guided keyframe selection scheme that con-siders both pose similarity and visibility complementar-ity and enables detailed and pose-guided reconstruction in the invisible regions (Sec. 4.2).
• A robust implicit surface fusion scheme that involves an adaptive blending weight conditioned by depth observa-tions and visibility, and an automatic collision handling method which considers an adjacent no-collision model into the fusion procedure to maintain the adjacent details while eliminating collision artifacts (Sec. 4.3).
Building on these novel techniques, POSEFusion is the ﬁrst single-view approach that is able to capture high-ﬁdelity and dynamic details in both visible and invisible regions.
Given a monocular RGBD sequence as input, our method is able to produce compelling human reconstruction results with complete, dynamic, temporally continuous, and high-ﬁdelity details. The experimental results prove that our method outperforms state-of-the-art methods. 2.