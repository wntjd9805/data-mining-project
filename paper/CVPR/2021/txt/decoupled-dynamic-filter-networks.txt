Abstract
Convolution is one of the basic building blocks of CNN architectures. Despite its common use, standard convo-lution has two main shortcomings: Content-agnostic and
Computation-heavy. Dynamic ﬁlters are content-adaptive, while further increasing the computational overhead.
Depth-wise convolution is a lightweight variant, but it usu-ally leads to a drop in CNN performance or requires a larger number of channels. In this work, we propose the
Decoupled Dynamic Filter (DDF) that can simultaneously tackle both of these shortcomings. Inspired by recent ad-vances in attention, DDF decouples a depth-wise dynamic
ﬁlter into spatial and channel dynamic ﬁlters. This decom-position considerably reduces the number of parameters and limits computational costs to the same level as depth-wise convolution. Meanwhile, we observe a signiﬁcant boost in performance when replacing standard convolution with DDF in classiﬁcation networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the top-1 accuracy, while their computational costs are reduced by nearly half. Ex-periments on the detection and joint upsampling networks also demonstrate the superior performance of the DDF up-sampling variant (DDF-Up) in comparison with standard convolution and specialized content-adaptive layers. The project page with code is available1. 1.

Introduction
Convolution is a fundamental building block of convolu-tional neural networks (CNNs) that have seen tremendous success in several computer vision tasks, such as image classiﬁcation, semantic segmentation, pose estimation, to name a few. Thanks to its simple formulation and opti-mized implementations, convolution has become a de facto standard to propagate and integrate features across image
In this work, we aim to alleviate two of its main pixels. shortcomings: Content-agnostic and Computation-heavy.
Content-agnostic. Spatial-invariance is one of the promi-nent properties of a standard convolution. That is, convolu-∗Work carried out during the visit of J. Zhou and Z. Pi at UC Merced
†Corresponding author 1https://thefoxofsky.github.io/project_pages/ddf
Figure 1. Comparison between convolution, the dynamic ﬁlter, and DDF. Top: Convolution shares a static ﬁlter among pixels and samples. Medium: The dynamic ﬁlter generates one complete
ﬁlter for each pixel via a separate branch. Bottom: DDF decouples the dynamic ﬁlter into spatial and channel ones. tion ﬁlters are shared across all the pixels in an image. Con-sider the sample road scene shown in Figure 1 (top). The convolution ﬁlters are shared across different regions such as buildings, cars, roads, etc. Given the varied nature of contents in a scene, a spatially shared ﬁlter may not be op-timal to capture features across different image regions [52, 42]. In addition, once a CNN is trained, the same convolu-tion ﬁlters are used across different images (for instance im-ages taken in daylight and at night). In short, standard con-volution ﬁlters are content-agnostic and are shared across images and pixels, leading to sub-optimal feature learning.
Several existing works [23, 48, 42, 57, 49, 45, 22, 11] pro-pose different types of content-adaptive (dynamic) ﬁlters for
CNNs. However, these dynamic ﬁlters are either compute-intensive [57, 23], memory-intensive [42, 22], or special-ized processing units [11, 48, 49, 45]. As a result, most of the existing dynamic ﬁlters can not completely replace 6647
standard convolution in CNNs and are usually used as a few layers of a CNN [49, 45, 42, 22], or in tiny architec-ture [57, 23], or in speciﬁc scenarios, like upsampling [48].
• Fast runtime. DDF has similar computational costs as depth-wise convolution, so its inference speed is faster than both standard convolution and dynamic ﬁlters.
Computation-heavy. Despite the existence of highly-optimized implementations, the computation complexity of standard convolution still increases considerably with the enlarge in the ﬁlter size or channel number. This poses a signiﬁcant problem as convolution layers in modern CNNs have a large number of channels in the orders of hundreds or even thousands. Grouped or depth-wise convolutions are commonly used to reduce the computation complexity.
However, these alternatives usually result in CNN perfor-mance drops when directly used as a drop-in replacement to standard convolution. To retain similar performance with depth-wise or grouped convolutions, we need to consider-ably increase the number of feature channels, leading to more memory consumption and access times.
In this work, we propose the Decoupled Dynamic Fil-ter (DDF) that simultaneously addresses both the above-mentioned shortcomings of the standard convolution layer.
The full dynamic ﬁlter [57, 23, 49, 45] uses a separate net-work branch to predict a complete convolution ﬁlter at each pixel. See Figure 1 (middle) for an illustration. We observe that this dynamic ﬁltering is equivalent to applying atten-tion on unfolded input features, as illustrated in Figure 3.
Inspired by the recent advances in attention mechanisms that apply spatial and channel-wise attention [36, 50], we propose a new variant of the dynamic ﬁlter where we de-couple spatial and channel ﬁlters. In particular, we adopt separate attention-style branches that individually predict spatial and channel dynamic ﬁlters, which are then com-bined to form a ﬁlter at each pixel. See Figure 1 (bottom) for an illustration of DDF. We observe that this decoupling of the dynamic ﬁlter is efﬁcient yet effective, making DDF to have similar computational costs as depth-wise convo-lution while achieving better performance against existing dynamic ﬁlters. This lightweight nature enables DDF to be directly inserted as a replacement of the standard convolu-tion layer. Unlike several existing dynamic ﬁltering layers, we can replace all k × k (k > 1) convolutions in a CNN with DDF. We also propose a variant of DDF, called DDF-Up, that can be used as a specialized upsampling or joint-upsampling layer.
We empirically validate the performance of DDF by drop-in replacing convolution layers in several classiﬁca-tion networks with DDF. Experiments indicate that apply-ing DDF consistently boosts the performance while reduc-ing computational costs. In addition, we also demonstrate the superior upsampling performance of DDF-Up in object detection and joint upsampling networks. In summary, DDF and DDF-Up have the following favorable properties:
• Content-adaptive. DDF provides spatially-varying ﬁl-tering that makes ﬁlters adaptive to image contents.
• Smaller memory footprint. DDF signiﬁcantly reduces memory consumption of dynamic ﬁlters, making it pos-sible to replace all standard convolution layers with DDF.
• Consistent performance improvements. Replacing a standard convolution with DDF / DDF-Up results in con-sistent improvements and achieves the state-of-the-art performance across various networks and tasks. 2.