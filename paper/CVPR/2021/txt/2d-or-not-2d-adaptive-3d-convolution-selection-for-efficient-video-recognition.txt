Abstract 3D convolutional networks are prevalent for video recognition. While achieving excellent recognition perfor-mance on standard benchmarks, they operate on a sequence of frames with 3D convolutions and thus are computation-ally demanding. Exploiting large variations among differ-ent videos, we introduce Ada3D, a conditional computa-tion framework that learns instance-speciﬁc 3D usage poli-cies to determine frames and convolution layers to be used in a 3D network. These policies are derived with a two-head lightweight selection network conditioned on each in-put video clip. Then, only frames and convolutions that are selected by the selection network are used in the 3D model to generate predictions. The selection network is optimized with policy gradient methods to maximize a re-ward that encourages making correct predictions with lim-ited computation. We conduct experiments on three video recognition benchmarks and demonstrate that our method achieves similar accuracies to state-of-the-art 3D models while requiring 20% − 50% less computation across differ-ent datasets. We also show that learned policies are trans-ferable and Ada3D is compatible to different backbones and modern clip selection approaches. Our qualitative analysis indicates that our method allocates fewer 3D convolutions and frames for “static” inputs, yet uses more for motion-intensive clips. 1.

Introduction
Videos are expected to make up a staggering 82% of In-ternet trafﬁc by 2022 [1], which demands approaches that can understand video content like actions and events accu-rately and efﬁciently. Key to video recognition is temporal modeling to capture relationships among different frames.
Towards this goal, extensive studies have been conducted with 3D convolutional networks by extending 2D convo-lutions over time [33, 4, 35, 34, 9, 8, 45]. While of-*Corresponding author.
Figure 1: A conceptual overview of our approach.
Ada3D learns to adaptively keep/discard 3D convolutional layers and frames conditioned on input clips for efﬁcient video recognition. Fewer 3D convolutions and frames are kept for clips that contain discriminative static cues and contextual information, while more are used for motion-intensive clips, in pursuit of a reduced overall computational cost without sacriﬁcing recognition accuracy. Black mask indicates the frame is discarded. fering excellent recognition accuracy on standard bench-marks [4, 13, 16], 3D models are often computationally ex-pensive due to the costly convolution operations along the temporal axis on a large number of stacked frames. For ex-ample, at the clip-level 1, a standard ResNet50 [14] model only requires 4.1 GFLOPs (giga ﬂoating-point operations) to compute predictions for a single image, while a SlowFast network [9] with the same ResNet50 backbone needs 16 times more computation (65.7 GFLOPs). Furthermore, the computational cost linearly grows with the number of clips uniformly sampled through the entire sequence for video-level prediction aggregation.
But are 3D convolutions really important for recognizing different types of videos? Do we really need them through-1Here, we use “clip” in a broad sense; for 2D models, a clip is a single
RGB frame while for 3D models it is a stack of frames. 6155
out the network? Is it necessary to perform 3D convolution on a ﬁxed number of stacked frames for all different sam-ples? Intuitively, 3D convolutions are critical for captur-ing changing patterns among inputs. However, due to large intra-class and inter-class variations, some videos are rela-tively more “static” than others, for which using a computa-tionally expensive 3D model on redundant inputs might be unnecessary. This paper seeks to develop a computationally efﬁcient framework for video recognition by learning how many frames to use and whether to use 3D convolutions in 3D networks. This is an orthogonal yet complementary di-rection to existing work on fast video recognition, which either designs lightweight 3D architectures [35, 44, 8, 34] or develops clip selection schemes to use fewer clips for classiﬁcation [43, 20, 12, 41, 48].
With this in mind, we introduce Ada3D, an end-to-end framework that learns adaptive 3D convolution usage con-ditioned on each input clip sample for efﬁcient video recog-nition. For each clip, deriving a dynamic inference strat-egy entails (1) learning how many frames are used as in-puts to the 3D network; (2) conditioned on these selected frames, determining how many 3D convolutional layers are activated; (3) and most importantly, making correct pre-dictions while only using a small number of input frames and 3D convolutions. By doing so, Ada3D allocates more computational resources to videos with complicated motion patterns while performing economical inference for “easy static” videos, enabling efﬁcient video classiﬁcation while maintaining reliable classiﬁcation accuracy. While appeal-ing, learning whether to keep/discard input frames and 3D convolutions is a non-trivial task, as it requires making bi-nary decisions that are non-differentiable.
To this end, Ada3D is built upon a reinforcement learn-ing framework [32].
In particular, given a video clip,
Ada3D trains a two-head selection network to produce a frame usage policy and a convolution usage policy, indicat-ing which frames in the input stack and which 3D convo-lutions in the network should be kept or discarded, respec-tively. Then, conditioned on the derived policies, dynamic inference is performed on a pretrained 3D network with se-lected frames and 3D convolutions for fast recognition. The selection network is optimized with policy gradient meth-ods [32] to maximize a reward function that is carefully de-signed to incentivize using as few computational resources as possible while making correct predictions. We further jointly ﬁnetune the selection network with the 3D network such that the 3D model is able to adapt to the adaptive infer-ence paradigm. It worth nothing that the selection network is designed to be lightweight so that its computational over-head is negligible.
We conduct extensive experiments to evaluate Ada3D on
ActivityNet [16], FCVID [19], Mini-Kinetics-200 [44, 4], and demonstrate that Ada3D is able to save 20% to 50% computation on different datasets while maintaining simi-lar recognition performance compared with baselines. We show policies learned on Mini-Kinetics-200 can be further transferred to the full Kinetics dataset [4]. In addition, we show the approach is compatible with different 3D models and it is also complementary to other clip-level selection methods [20, 43, 41, 12, 48]. We also demonstrate quali-tatively that our method learns to allocate fewer 3D convo-lutions and frames for clips that are relatively more static, while applying more computation to motion-intensive clips. 2.