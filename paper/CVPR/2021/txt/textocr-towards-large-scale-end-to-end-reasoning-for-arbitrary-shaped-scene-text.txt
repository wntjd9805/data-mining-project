Abstract
A crucial component for the scene text based reasoning re-quired for TextVQA and TextCaps datasets involve detecting and recognizing text present in the images using an optical character recognition (OCR) system. The current systems are crippled by the unavailability of ground truth text an-notations for these datasets as well as lack of scene text detection and recognition datasets on real images disallow-ing the progress in the ﬁeld of OCR and evaluation of scene text based reasoning in isolation from OCR systems. In this work, we propose TextOCR, an arbitrary-shaped scene text detection and recognition with 900k annotated words col-lected on real images from TextVQA dataset. We show that current state-of-the-art text-recognition (OCR) models fail to perform well on TextOCR and that training on TextOCR helps achieve state-of-the-art performance on multiple other
OCR datasets as well. We use a TextOCR trained OCR model to create PixelM4C model which can do scene text based rea-soning on an image in an end-to-end fashion, allowing us to revisit several design choices to achieve new state-of-the-art performance on TextVQA dataset. 1.

Introduction
The computer vision community has recently seen a surge in interest to understand and reason on the text present in the images (scene text) beyond the OCR extraction. In particular, multiple datasets have been introduced that focus on visual question answering (VQA) [53, 4, 39] and image caption-ing [51] but in the context of scene text. These tasks involve understanding the objects and text in the image and then reasoning over the spatial and semantic relations between these along with a textual input (e.g. question). Though the
∗Equal Contribution. Correspondence to textvqa@fb.com
OCR systems have matured, they still don’t work well on pictures involving real-life scenarios given the lack of large annotated real scene text OCR datasets. The text extracted by the OCR systems doesn’t mean anything in itself until it is used to solve a task which involves using the scene text.
Other than VQA and image captioning, the potential use cases include several impactful and interesting tasks such the hate speech and misinformation detection [25].
Although, the ﬁeld has witnessed success and progress in datasets on downstream OCR applications, the performance of state-of-the-art models on these datasets are nowhere close to human accuracy due to multiple factors which includes the quality of the OCR extracted from existing OCR systems, unavailability of ground-truth text annotations for the real-world images, and no feedback to OCR system to improve detection or extraction based on the errors in the downstream application i.e. no end-to-end training.
In this paper, we introduce a new dataset, TextOCR which aims to bridge these gaps by providing (i) high quality and large quantity text annotations on TextVQA images (ii) al-lowing end-to-end training of downstream application mod-els with OCR systems and thus allowing ﬁne-tuning of OCR pipeline based on the task involved. Prior to TextOCR, many
OCR datasets exist [38, 59, 35, 24, 23, 45, 48, 8, 61, 34, 57, 41, 50, 40, 9, 54] that propelled the ﬁeld’s development, but many of these are either relatively small, or focus mostly on outdoor or store-front scenes. As a result, OCR mod-els trained on these datasets usually don’t perform well on downstream tasks from other scene types. Moreover, exist-ing datasets usually have a low number of words per image, making them less dense, diverse and ideal to train OCR models for tasks commonly having a high text density. As a solution, we present the TextOCR dataset that contains more than 28k images and 903k words in total, averaging 32 words per image. Jointly with existing TextVQA [53] and TextCaps [51] datasets, it can also serve as an OCR 8802
Figure 1: PixelM4C - An end-to-end TextVQA model. In this work, we bridge the gap between arbitrary scene-text detection/recognition and scene-text based reasoning in TextVQA [53] and TextCaps [51]. We introduce TextOCR, largest real scene-text detection and recognition dataset with 900k annotated arbitrary-shaped words collected on TextVQA images.
Further, we build PixelM4C, an end-to-end TextVQA model which uses TextOCR trained Mask TextSpotter v3 [30] and
M4C [17] models to do text-based reasoning directly on the images unlike previous works which rely on pre-extracted OCR text and features. The solid lines in the ﬁgure show backpropagable paths. upper bound for researchers working on them to evaluate their methods’ reasoning capabilities on a fair ground.
In addition to TextOCR, we present a novel architecture,
PixelM4C, that connects an OCR model, Mask TextSpotter (MTS) v3 [30] with downstream TextVQA model, M4C [17], in an end-to-end trainable fashion, as illustrated in Figure 1.
Through extensive analysis and ablations possible with end-to-end PixelM4C, we revisit and improve design choices from prior work to achieve new state-of-the-art performance on TextVQA dataset [53] under comparable settings and show TextOCR’s impact on performance on new TextCaps dataset [51]. In summary, our main contributions include:
• A large and diverse OCR dataset with ∼1M arbitrary-shaped word annotations (3x larger than existing datasets), with high density of ∼32 words per image.
• Extensive experiments to evaluate TextOCR showing that it is effective both as (i) a training data to push OCR state-of-the-art on multiple datasets and (ii) testing data to offer a new challenge to the community.
• A new end-to-end novel architecture, PixelM4C for
TextVQA and TextCaps, which connects Mask TextSpot-ter (MTS) v3 [30] to M4C [17] allowing extensive anal-ysis and revisiting prior work’s design decisions.
• State-of-the-art on TextVQA [53] using OCR tokens gen-erated from TextOCR trained OCR models and insights from PixelM4C ablations under comparable settings. 2.