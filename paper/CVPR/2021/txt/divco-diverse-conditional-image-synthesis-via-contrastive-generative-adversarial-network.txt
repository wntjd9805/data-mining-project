Abstract
Class A
Class B
Class A
Class B
Conditional generative adversarial networks (cGANs) target at synthesizing diverse images given the input condi-tions and latent codes, but unfortunately, they usually suffer from the issue of mode collapse. To solve this issue, previ-ous works [47, 22] mainly focused on encouraging the cor-relation between the latent codes and their generated im-ages, while ignoring the relations between images generat-ed from various latent codes. The recent MSGAN [27] tried to encourage the diversity of the generated image but only considers “negative” relations between the image pairs.
In this paper, we propose a novel DivCo framework to properly constrain both “positive” and “negative” rela-tions between the generated images speciﬁed in the latent space. To the best of our knowledge, this is the ﬁrst attemp-t to use contrastive learning for diverse conditional image synthesis. A novel latent-augmented contrastive loss is in-troduced, which encourages images generated from adja-cent latent codes to be similar and those generated from distinct latent codes to be dissimilar. The proposed latent-augmented contrastive loss is well compatible with var-ious cGAN architectures. Extensive experiments demon-strate that the proposed DivCo can produce more diverse images than state-of-the-art methods without sacriﬁcing vi-sual quality in multiple unpaired and paired image genera-tion tasks. Training code and pretrained models are avail-able at https://github.com/ruiliu-ai/DivCo. 1.

Introduction
Generative adversarial network (GAN) [9] has shown great potential to capture complex distributions and gen-erate high-dimensional samples since its ﬁrst introduction in 2014. The follow-up years witnessed its great progress-es and successes in synthesizing realistic high-resolution images [35, 28, 1, 18, 19, 3]. Based on GANs, condi-tional generative adversarial networks (cGANs) [29] were proposed, which focus not only on producing realistic im-ages but more on preserving the input conditional informa-(a) Target distribution (b) Latent regression loss
Class A
Class B
Class A
Class B (c) Mode seeking loss (d) Our contrastive loss
Figure 1. 2D toy experiment for demonstrating the generation distribution learned with different regularization terms. (a) The ground-truth distributions for the two different classes, each of which is a Gaussian mixture model. (b) The generated samples from the learned distribution by the latent regression loss [47] shows its weakness in covering all modes of the ground truths. (c)
The generated samples from the learned distribution by the mod-e seeking loss [27] shows similar patterns regardless of different classes. (d) The generated samples from the learned distribution by our proposed latent-augmented contrastive loss demonstrates an unbiased distribution which is properly dependent on both con-ditional input and latent codes. tion. For example, ACGANs [32] generated diverse im-ages conditioned on class labels. Pix2pix [16] and Cycle-GAN [46] translated images across two characteristic do-mains to change their visual styles under paired and un-paired settings, respectively. There exist other cGANs that condition on input images, e.g. super-resolution [21], style transfer [23, 22], inpainting [41], denoising [4], etc., as well as cGANs conditioned on text descriptions [44, 43, 40].
Real-world scenarios expect the synthetic samples to be diverse and able to manipulate ﬂexibly. However, all the applications mentioned above suffer from the problem of mode collapse to some extent, even though randomly sam-pled latent codes are added as additional inputs. To deal with this shortcoming, many works attempted to enhance the correlation between input latent codes and output im-ages to ensure that the latent codes have control over the generated images. BicycleGAN [47] and DRIT [22] adopt-ed a latent regression loss term, which encourages the mod-16377
el to recover the input latent code from the generated im-ages. However, the effect of this term on boosting the gener-ation diversity is far from satisfactory (see Fig. 1(b)), which is due to the fact that this term only considers the relation between individual latent codes and their generated images.
The relations between images generated from various latent codes are more valuable but were neglected. MSGAN [27] tried to tackle the problem and proposed a mode seeking loss, which aims to improve generation diversity by max-imizing the dissimilarity of two arbitrary images. Howev-er, the distance between two sampled latent codes may be close to each other and their synthesized images should not be pushed away. By imposing such strong yet sub-optimal constraints between pairs of generated images, the learned distribution easily turns out to be biased, i.e. the generation results only depend on latent codes while ignoring the con-ditional input (see Fig. 1(c)). Such a phenomenon can also be observed in our later experiments (refer to Fig. 4(b) and
Fig. 5).
We argue that the unsatisfaction of MSGAN [27] de-rives from its strategy on always treating any image pairs as
“negative” pairs while ignoring “positive” pairs. However, they should be equally crucial for the generator to correctly capture the semantics of various latent codes. Towards this end, we attempt to learn unbiased distributions by consid-ering the “positive” and “negative” relations simultaneous-ly in the form of contrastive learning. Contrastive learning was widely applied in self-supervised representation learn-ing tasks [33, 39, 5, 11] and recently showed its great poten-tial in conditional image synthesis [34, 17] by maintaining the correspondences between each generated image and it-s conditional input. In this work, we further demonstrate how to adapt it to diverse conditional image synthesis by our newly proposed latent-augmented contrastive loss.
Speciﬁcally, conditioned on the same class code, the in-troduced latent-augmented contrastive loss forces the visual relations of the generated images to be correlated to the dis-tances between their input latent codes, i.e. “positive” im-ages with close latent codes should be similar while “neg-ative” images with distinct latent codes should be far away from each other in the feature space. Note that the crit-ical view (“positive” or “negative”) selection in our pro-posed framework is achieved via a novel latent augmen-tation scheme, i.e. a positive code is sampled within a s-mall hyper-sphere around the query code in the latent space while negative ones are sampled outside that hyper-sphere.
As a result, the problem of mode collapse can be alleviated to large extent by regularizing the generator on better under-standing the structure of latent space. Better performance on both learned distributions (Fig. 1(d)) and generation re-sults (Section 4) indicate the effectiveness of our method.
Our contributions are summarized as three-fold:
• We for the ﬁrst time adapt contrastive learning to en-courage diverse image synthesis. The proposed Div-Co learning scheme can be readily integrated into ex-isting conditional generative adversarial networks with marginal modiﬁcations.
• A novel latent-augmented contrastive loss is proposed to discriminate the latent representations of generated samples in a contrastive manner. The issue of mode collapse in cGANs has been much alleviated.
• Extensive experiments in different conditional genera-tion tasks demonstrate that our proposed method helps existing frameworks improve the performance of di-verse image synthesis without sacriﬁcing the visual quality of the generated images. 2.