Abstract accuracy. We also release the codes.
Despite the recent success of single image-based 3D hu-man pose and shape estimation methods, recovering tem-porally consistent and smooth 3D human motion from a video is still challenging. Several video-based methods have been proposed; however, they fail to resolve the sin-gle image-based methods’ temporal inconsistency issue due to a strong dependency on a static feature of the current frame. In this regard, we present a temporally consistent mesh recovery system (TCMR). It effectively focuses on the past and future frames’ temporal information without being dominated by the current static feature. Our TCMR signif-icantly outperforms previous video-based methods in tem-poral consistency with better per-frame 3D pose and shape 1.

Introduction
Various methods have been proposed to analyze hu-mans from images, ranging from estimating a simplistic 2D skeleton to recovering 3D human pose and shape. Despite the recent improvements, estimating 3D human pose and shape from images is still a challenging task, especially in the monocular case due to depth ambiguity, limited training data, and complexity of human articulations.
Most of the previous methods [6, 11, 15, 16, 22, 26] at-tempt to recover 3D human pose and shape from a single image. They are generally based on parametric 3D hu-man mesh models, such as SMPL [18], and directly regress 11964
the model parameters from the input image. Although sin-gle image-based methods predict a reasonable output from a static image, they tend to produce temporally inconsis-tent and unsmooth 3D motion when applied to a video per frame. The temporal instability is from inconsistent 3D pose errors for consecutive frames. For example, the er-rors could occur in different 3D directions, or the following frames’ pose outputs could remain relatively the same, not reﬂecting the motion.
Several methods [12, 14, 19] have been proposed to ex-tend the single image-based methods to the video case ef-fectively. They feed a sequence of images to the pretrained single image-based 3D human pose and shape estimation networks [11, 15] to obtain a sequence of static features.
All input frames’ static features are passed to a temporal encoder, which encodes a temporal feature for each input frame. Then, a body parameter regressor outputs SMPL pa-rameters for each frame from the temporal feature of the corresponding time step.
Although the above works quantitatively improved the per-frame 3D pose accuracy and motion smoothness, their qualitative results still suffer from the temporal inconsis-tency aforementioned, as shown in Figure 1. We argue that the failure comes from a strong dependency on the static feature of the current frame. For terminological conve-nience, we use a word current to indicate the time step of a target frame where SMPL parameters to be estimated. The
ﬁrst reason for the strong dependency is a residual connec-tion between the current frame’s static and temporal fea-tures. While the residual connection has been widely ver-iﬁed to facilitate a learning process, naively applying it to the temporal encoding can hinder the system from learn-ing useful temporal information. Given that the static fea-ture is extracted by the pretrained network [11, 15], it con-tains a strong cue for the SMPL parameters of the current frame. Thus, the residual connection’s identity mapping of the static feature can make the SMPL parameter regres-sor heavily depend on it and leverage the temporal feature marginally. This procedure can constrain the temporal en-coder from encoding more meaningful temporal features.
The second reason is the temporal encoding that takes static features from all frames, which include a current static fea-ture. The current static feature has the largest potential to affect the current temporal feature, from which SMPL pa-rameters are predicted. This phenomenon is caused by the current static feature having the most crucial information for 3D human pose and shape of a current frame. Although the dominance will increase the per-frame accuracy of 3D pose and shape estimation, it can prevent the temporal en-coder from fully exploiting the past and future frames’ tem-poral information. Taken together, the existing video-based methods have a strong preference for the current static fea-ture, and suffer from the temporal inconsistency issue as single image-based methods do.
In this work, we propose a temporally consistent mesh recovery system (TCMR). It is designed to resolve the strong dependency on the current static feature for tempo-rally consistent and smooth 3D human motion output from a video. First, although we follow the previous video-based works [12, 14, 19] to encode a temporal feature of the cur-rent frame, we remove the residual connection between the static and temporal features. Moreover, we introduce Pose-Forecast, which consists of two temporal encoders, to fore-cast a current pose from the past and future frames without the current frame. The temporal features from PoseForecast are free from the current static feature; however, they con-tain essential temporal information of the past and future frames to forecast a current pose. The temporal features from PoseForecast are integrated with the current temporal feature, which is extracted from all input frames, to predict current SMPL parameters. The parameters estimated from the integrated temporal feature are the ﬁnal output in infer-ence time. By removing the strong dependency on the cur-rent static feature, our SMPL parameter regressor can have more chance to focus on the past and future frames without being dominated by the current frame.
Despite its simplicity, we observed that our newly de-signed temporal architecture is highly effective on obtaining the temporally consistent and smooth 3D human motion. It also improves the accuracy of the 3D pose and shape per frame by utilizing better temporal information. We show that the proposed TCMR outperforms the previous video-based methods [12,14,19] on various 3D video benchmarks, especially in temporal consistency.
Our contributions can be summarized as follows.
• We present a temporally consistent mesh recovery sys-tem (TCMR), which produces temporally consistent and smooth 3D human motion from a video.
It ef-fectively leverages temporal information from the past and future frames without being dominated by the static feature of the current frame.
• Despite its simplicity, TCMR not only improves the temporal consistency of 3D human motion but also in-creases per-frame 3D pose and shape accuracy com-pared to a baseline method.
• TCMR outperforms previous video-based methods in temporal consistency by a large margin while achiev-ing better per-frame 3D pose and shape accuracy. 2.