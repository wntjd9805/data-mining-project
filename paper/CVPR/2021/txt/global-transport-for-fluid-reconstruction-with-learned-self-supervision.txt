Abstract
We propose a novel method to reconstruct volumetric
ﬂows from sparse views via a global transport formulation.
Instead of obtaining the space-time function of the obser-vations, we reconstruct its motion based on a single initial state. In addition we introduce a learned self-supervision that constrains observations from unseen angles. These vi-sual constraints are coupled via the transport constraints and a differentiable rendering step to arrive at a robust end-to-end reconstruction algorithm. This makes the recon-struction of highly realistic ﬂow motions possible, even from only a single input view. We show with a variety of synthetic and real ﬂows that the proposed global reconstruction of the transport process yields an improved reconstruction of the
ﬂuid motion. 1.

Introduction
The ambient space for many human activities and man-ufacturing processes is ﬁlled with ﬂuids whose motion we can only observe indirectly [53, 61, 72, 79]. Once passive markers, e.g., in the form of ink or dye, are injected into the
ﬂuid, it is possible to draw conclusions about the transport induced by the motion of the ﬂuid. This form of motion re-construction is highly important for a large variety of appli-cations, from medical settings [59], over engineering [67] to visual effects [15], but at the same time poses huge chal-lenges.
Fluids on human scales are typically turbulent, and ex-hibit a highly complex mixture of translating, shearing, and rotating motions [60]. In addition, the markers by construc-tion need to be sparse or transparent in order not to fully occupy and occlude the observed volume. Several works have alleviated these challenges with specialized hardware
[18, 77] or by incorporating the established physical model for ﬂuids, the Navier-Stokes equations, into the reconstruc-tion [11, 21]. However, despite improvements in terms of quality of the reconstruction, the high non-linearity of the reconstruction problem coupled with ambiguous observa-tions can cause the optimizations to ﬁnd undesirable mini-mizers that deviate from ground truth motions.
In order to obtain a solution, existing approaches com-pute volumetric observations over time [21, 9, 79]. Hence, despite including a physical model, the observed quantities represent unknowns per time step, and are allowed to devi-ate from the constraints of the model. We make a central observation: when enforcing the physical model over the course of the complete trajectory of the observed ﬂuid, the corresponding reconstructions yield a motion that better ad-heres to the ground truth. Thus, instead of a sequence, our reconstruction results in a single initial state of the density.
This state is evolved via a global transport over time purely by the physical model and the temporal reconstruction of the motion ﬁeld. In addition to an improved reconstruction of the motion, this strict differentiable physics prior [7, 28] allows us to work with very sparse observations, with only a single viewpoint in the extreme. 1632
In order to better constrain the degrees of freedom in this single-viewpoint scenario, we propose a method inspired by generative adversarial networks (GANs) [19, 76]. Due to the complete lack of observations from other viewpoints, we make use of a small dataset of example motions, and train a convolutional neural network alongside the motion reconstruction that serves as a discriminator. This discrim-inator is evaluated for randomly sampled viewpoints in or-der to provide image space constraints that are coupled over time via our global transport formulation.
While existing works also typically focus on linear im-age formation models [30, 22], we combine the visual and transport constraints with a fully differentiable volumetric rendering pipeline. We account for complex lighting ef-fects, such as absorption and self-shadowing, which are key elements to capture the visual appearance of many real-world marker observations.
To summarize, the main contributions of our work are:
• A global multi-scale transport optimization via a dif-ferentiable physical model that yields purely transport-based reconstructions.
• A learned visual prior to obtain motions from very sparse and single input views.
• The inclusion of differentiable rendering with explicit lighting and volumetric self-shadowing.
To the best of our knowledge, these contributions make it possible for the ﬁrst time to construct a ﬂuid motion from sparse views in an end-to-end optimization, even from a sin-gle viewpoint. An overview is given in Figure 1. 2.