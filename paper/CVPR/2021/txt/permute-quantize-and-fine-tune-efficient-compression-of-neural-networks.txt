Abstract
Compressing large neural networks is an important step for their deployment in resource-constrained computational platforms. In this context, vector quantization is an appeal-ing framework that expresses multiple parameters using a single code, and has recently achieved state-of-the-art net-work compression on a range of core vision and natural language processing tasks. Key to the success of vector quantization is deciding which parameter groups should be compressed together. Previous work has relied on heuristics that group the spatial dimension of individual convolutional
ﬁlters, but a general solution remains unaddressed. This is desirable for pointwise convolutions (which dominate mod-ern architectures), linear layers (which have no notion of spatial dimension), and convolutions (when more than one
ﬁlter is compressed to the same codeword). In this paper we make the observation that the weights of two adjacent layers can be permuted while expressing the same function.
We then establish a connection to rate-distortion theory and search for permutations that result in networks that are eas-ier to compress. Finally, we rely on an annealed quantization algorithm to better compress the network and achieve higher
ﬁnal accuracy. We show results on image classiﬁcation, ob-ject detection, and segmentation, reducing the gap with the uncompressed model by 40 to 70% w.r.t. the current state of the art. All our experiments can be reproduced using the code at https://github.com/uber-research/ permute-quantize-finetune. 1.

Introduction
State-of-the-art approaches to many computer vision tasks are currently based on deep neural networks. These networks often have large memory and computational re-quirements, limiting the range of hardware platforms on which they can operate. This poses a challenge for appli-cations such as virtual reality and robotics, which naturally rely on mobile and low-power computational platforms for large-scale deployment. At the same time, these networks are often overparameterized [5], which implies that it is pos-sible to compress them – thereby reducing their memory and computation demands – without much loss in accuracy.
Scalar quantization is a popular approach to network compression where each network parameter is compressed individually, thereby limiting the achievable compression rates. To address this limitation, a recent line of work has focused on vector quantization (VQ) [13, 47, 54], which compresses multiple parameters into a single code. Conspic-uously, these approaches have recently achieved state-of-the-art compression-to-accuracy ratios on core computer vision and natural language processing tasks [10, 48].
A key advantage of VQ is that it can naturally exploit redundancies among groups of network parameters, for ex-ample, by grouping the spatial dimensions of convolutional
ﬁlters in a single vector to achieve high compression rates.
However, ﬁnding which network parameters should be com-pressed jointly can be challenging; for instance, there is no notion of spatial dimension in fully connected layers, and it is not clear how vectors should be formed when the vector size is larger than a single convolutional ﬁlter – which is always true for pointwise convolutions. Current approaches either employ clustering (e.g., k-means) using the order of the weights as obtained by the network [13, 47, 54], which is suboptimal, or search for groups of parameters that, when compressed jointly, minimize the reconstruction error of the network activations [10, 48, 54], which is hard to optimize.
In this paper, we formalize the notion of redundancy among parameter groups using concepts from rate-distortion theory, and leverage this analysis to search for permutations of the network weights that yield functionally equivalent, yet easier-to-quantize networks. The result is Permute, Quan-tize, and Fine-tune (PQF), an efﬁcient algorithm that ﬁrst searches for permutations, codes and codebooks that mini-mize the reconstruction error of the network weights, and then uses gradient-based optimization to recover the accu-racy of the uncompressed network. Our main contributions can be summarized as follows: 1. We study the invariance of neural networks under per-mutation of their weights, focusing on constraints in-15699
duced by the network topology. We then formulate a permutation optimization problem to ﬁnd functionally equivalent networks that are easier to quantize. Our result focuses on improving a quantization lower bound of the weights; therefore 2. We use an efﬁcient annealed quantization algorithm that reduces quantization error and leads to higher ac-curacy of the compressed networks. Finally, 3. We show that the reconstruction error of the network parameters is inversely correlated with the ﬁnal network accuracy after gradient-based ﬁne-tuning.
Put together, the above contributions deﬁne a novel method that produces state-of-the-art results in terms of model size vs. accuracy. We benchmark our method by compressing popular architectures for image classiﬁcation, and object detection & segmentation, showcasing the wide applicability of our approach. Our results show a 40-60% relative error reduction on Imagenet object classiﬁcation over the current state-of-the-art when compressing a ResNet-50 [21] down to about 3 MB (∼31× compression). We also demonstrate a relative 60% (resp. 70%) error reduction in object detection (resp. mask segmentation) on COCO over previous work, by compressing a Mask-RCNN architecture down to about 6.6
MB (∼26× compression). 2.