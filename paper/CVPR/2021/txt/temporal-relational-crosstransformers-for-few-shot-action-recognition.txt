Abstract
We propose a novel approach to few-shot action recog-nition, ﬁnding temporally-corresponding frame tuples be-tween the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes us-ing the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video repre-sentations are formed from ordered tuples of varying num-bers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared.1
Our proposed Temporal-Relational CrossTransform-ers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and
UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers. 1.

Introduction
Few-shot methods aim to learn new classes with only a handful of labelled examples. Success in few-shot ap-proaches for image classiﬁcation [11, 19, 8] and object recognition [26, 15] has triggered recent progress in few-shot video action recognition [31, 32, 3, 27, 4]. This is of particular interest for ﬁne-grained actions where collecting enough labelled examples proves challenging [5, 12, 6].
Recent approaches that achieve state-of-the-art perfor-mance [3, 27, 4] acknowledge the additional challenges in few-shot video recognition, due to varying action lengths and temporal dependencies. However, these match the query video (i.e. the video to be recognised) to the single best video in the support set (i.e. the few labelled exam-ples per class), e.g. [27], or to the average across all support set videos belonging to the same class [3, 4]. Inspired by part-based few-shot image classiﬁcation [8], we consider that, within a few-shot regime, it is advantageous to com-pare sub-sequences of the query video to sub-sequences of 1 Code is available at https://github.com/tobyperrett/TRX
Figure 1: For a 3-way 5-shot example, pairs of temporally-ordered frames in the query (red, green, blue) are compared against all pairs in the support set (max attention with cor-responding colour). Aggregated evidence is used to con-struct query-speciﬁc class prototypes. We show a correctly-recognised query using our method from SSv2 class “Fail-ing to put something into something because it does not ﬁt”. all support videos when constructing class prototypes. This better accumulates evidence, by matching sub-sequences at various temporal positions and shifts.
We propose a novel approach to few-shot action recog-nition, which we term Temporal-Relational CrossTrans-formers (TRX). A query-speciﬁc class prototype is con-structed by using an attention mechanism to match each query sub-sequence against all sub-sequences in the sup-port set, and aggregating this evidence. By performing the attention operation over temporally-ordered sub-sequences 475
rather than individual frames (a concept similar to that in many-shot action-recognition works, e.g. [29, 10]), we are better able to match actions performed at different speeds and in different parts of videos, allowing distinction be-tween ﬁne-grained classes. Fig. 1 shows an example of how a query video attends to multiple support set videos using temporally-ordered tuples.
Our key contributions can be summarised as follows:
•
•
•
•
We introduce a novel method, called the Temporal-Relational CrossTransformer (TRX), for few-shot action recognition.
We combine multiple TRXs, each operating over a differ-ent number of frames, to exploit higher-ordered temporal relations (pairs, triples and quadruples).
We achieve state-of-the-art results on the few-shot benchmarks for Kinetics [5], Something-Something V2 (SSv2) [12], HMDB51 [16] and UCF101 [21].
We perform a detailed ablation, demonstrating how TRX utilises multiple videos from the support set, of different lengths and temporal shifts. Results show that using tuple representations improves over single-frames by 5.8% on
SSv2 where temporal ordering proves critical. 2.