Abstract
We investigate the weakly-supervised audio-visual video parsing task, which aims to parse a video into temporal event segments and predict the audible or visible event categories. The task is challenging since there only exist video-level event labels for training, without indicating the temporal boundaries and modalities. Previous works take the overall event labels to supervise both audio and visual model predictions. However, we argue that such overall la-bels harm the model training due to the audio-visual asyn-chrony. For example, commentators speak in a basketball video, but we cannot visually ﬁnd the speakers. In this pa-per, we tackle this issue by leveraging the cross-modal cor-respondence of audio and visual signals. We generate re-liable event labels individually for each modality by swap-ping audio and visual tracks with other unrelated videos. If the original visual/audio data contain event clues, the event prediction from the newly assembled data would still be highly conﬁdent. In this way, we could protect our models from being misled by ambiguous event labels. In addition, we propose the cross-modal audio-visual contrastive learn-ing to induce temporal difference on attention models within videos, i.e., urging the model to pick the current temporal segment from all context candidates. Experiments show we outperform state-of-the-art methods by a large margin. 1.

Introduction
We humans explore and perceive the sounding environ-ments with sensory streams, including visual, auditory, tac-tile, etc. Among these simultaneous sensory streams, vision and audio are two fundamental streams that widely convey massive information in our daily life.
Audio-visual comprehension [23, 40, 9, 39, 50] is more robust in identifying the ongoing events compared to those vision models [45, 34]. For example, occlusions and blind
*This work was done when Yu Wu interned at Baidu Research. Yi Yang is the corresponding author.
Basketball
Speech 0s 2s 4s 6s 8s 10s
Basketball
Cheering
Speech (from the commentator)
Speech (from the player)
Figure 1. Examples of the audio-visual video parsing task. Col-ored rectangles indicate the ground truth events. Taking the visual and audio data as input, we aim at identifying the audible and vis-ible events and their temporal location. Note that the visual and audio events might be asynchronous. spots are common in egocentric videos and web videos, where the object of interest is outside of the ﬁeld-of-view (FoV). In such situations, auditory signals could provide re-liable clues for video understanding.
Existing audio-visual research works [1, 5, 7, 10, 12, 19, 6, 21, 27, 31, 53, 54, 57] usually assume audio and visual data are always correlated and temporally aligned. How-ever, this alignment might not always hold in practice. We may ﬁnd lots of videos whose sound originates outside of the scene view. Despite the nonalignment, audio signals are still important in understanding the events, such as out-of-screen motorcycle racing.
In this paper, we focus on the audio-visual video parsing (AVVP) task [39], which aims at providing a detailed analysis of auditory, visual, and audio-visual events in videos without such alignment assumptions.
As shown in Fig. 1, the target of AVVP is to recognize event categories in each sensory modality and localize them tem-porally in videos.
Due to exhausting labeling cost, Tian et al. [39] proposed the weakly-supervised learning for the AVVP task, which only requires sparse labeling on the presence or absence of 1326
event categories for training. The weakly supervised labels only indicate which event occurs in the video, without de-tailed modalities and temporal boundaries. The weak la-bels are more comfortable to annotate and can be boosted with automatic annotation (tags) for web videos. To solve the challenging issue, Tian et al. [39] proposed introducing cross-modal and self-modal attention to obtain aggregated features. The model is optimized in the Multimodal Multi-ple Instance Learning (MMIL) way, which regards overall event labels as the optimization targets for both audio and visual predictions.
However, audio and visual content are naturally different sensory streams. Visual data are captured by speciﬁc cam-era views, while audio signals collected by microphones could perceive all audible events of the scenes. Unlike other weakly supervised learning tasks, some event information may only exist in a single modality (either audio signals or visual signals). It would be irrational to optimize both modality predictions to be close to the overall event labels.
For example, in a basketball match video, there might be commentators speaking, but we cannot ﬁnd them visually (see Fig. 1). It harms the visual model optimization if we follow the universal weakly supervised learning way.
In this paper, we propose to tackle the challenging task by exploring heterogeneous clues. We alleviate the modal-ity uncertainty issue and generate reliable event labels indi-vidually for each modality without additional annotations.
To achieve the goal, we exchange the audio and visual track of a training video with other unrelated videos. Our moti-vation is that the newly assembled video’s prediction would still be highly conﬁdent if the visual/audio signals do con-tain clues of the target event. Otherwise, the event infor-mation is not visible/audible in the corresponding modality.
In this way, we could obtain precise modality-aware event labels and protect models from being misled by the ambigu-ous overall labels. To the best knowledge of ours, we are the
ﬁrst that swap audio and visual tracks with other videos to assess the modality uncertainty.
In addition, we also propose to induce temporal differ-ence within videos in a contrastive learning manner. Previ-ous methods obtain enhanced modality features by leverag-ing all temporal contexts of the whole video. We argue that these might harm the model performance since it obscures the temporal difference within an event video. Since we do not have temporal annotations in training, inspired by self-supervised learning [16, 51], we propose to introduce contrastive learning to introduce temporal difference into aggregated features. We urge the attention model to pick the correct temporal cross-modal segment features from all candidate distractors. Thus the aggregated feature would be more likely the information that happens at this segment instead of all context features, leading to better temporal lo-calization performances. To summarize, our contributions are as follows:
• We propose to address the modality uncertainty is-sue by exchanging audio and visual tracks with other videos. Thus we can obtain accurate modality-aware event supervision instead of ambiguous overall labels.
• We further introduce temporal heterogeneous con-strain into the attention model via contrastive learning, which alleviates the ambiguous temporal boundaries issues in the weakly-supervised AVVP task.
• Experiments show our method signiﬁcantly outper-forms the state-of-the-art methods by a large margin on all evaluation metrics. Speciﬁcally, we improve the segment-level audio-visual parsing accuracy from 48.9% to 55.1% on the LLP dataset. 2.