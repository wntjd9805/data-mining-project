Abstract
What would be the effect of locally poking a static scene? We present an approach that learns naturally-looking global articulations caused by a local manipulation at a pixel level. Training requires only videos of moving ob-jects but no information of the underlying manipulation of the physical scene. Our generative model learns to infer natural object dynamics as a response to user interaction and learns about the interrelations between different object body regions. Given a static image of an object and a local poking of a pixel, the approach then predicts how the ob-ject would deform over time. In contrast to existing work on video prediction, we do not synthesize arbitrary realistic videos but enable local interactive control of the deforma-tion. Our model is not restricted to particular object cate-gories and can transfer dynamics onto novel unseen object instances. Extensive experiments on diverse objects demon-strate the effectiveness of our approach compared to com-mon video prediction frameworks. Project page is available at https://bit.ly/3cxfA2L. 1.

Introduction
From infancy on we learn about the world by manipulat-ing our immediate environment and subsequently observing the resulting diverse reactions to our interaction. Particu-larly in early years, poking, pulling, and pushing the objects around us is our main source for learning about their inte-gral parts, their interplay, articulation and dynamics. Con-sider children playing with a plant. They eventually com-prehend how subtle touches only affect individual leaves, while increasingly forceful interactions may affect larger and larger constituents, thus ﬁnally learning about the en-tirety of the dynamics related to various kinds of interac-tions. Moreover, they learn to generalize these dynamics across similar objects, thus becoming able to predict the re-action of a novel object to their manipulation.
Training artiﬁcial visual systems to gain a similar under-standing of the distinct characteristics of object articulation and its distinct dynamics is a major line of computer vision
Figure 1. Our approach for interactive image-to-video synthesis learns to understand the relations between the distinct body parts of articulated objects from unlabeled video data, thus enabling synthesis of videos showing natural object dynamics as responses to local interactions. research. In the realm of still images the interplay between object shape and appearance has been extensively studied, even allowing for controlled, global [43, 41, 21, 16, 15] and local [39, 73] manipulation. Existing work on object dy-namics, however, so far is addressed by either extrapola-tions of observed object motion [67, 27, 11, 33, 67, 45, 5, 61] or only coarse control of predeﬁned attributes such as explicit action labels [71] and imitation of previously ob-served holistic motion [1, 14]. Directly controlling and, even further, interacting with objects on a local level how-ever, so far, is a novel enterprise. Teaching visual systems to understand the complex dynamics of objects both aris-ing by explicit manipulations of individual parts and to pre-dict and analyze the behavior [12, 3, 4] of the remainder of the object is an exceptionally challenging task. Similarly to a child in the example above, such systems need to know about the natural interrelations of different parts of an ob-ject [72]. Moreover, they have to learn how these parts are related by their dynamics to plausibly synthesize temporal 5171
object articulation as a response to our interactions.
In this paper, we present a generative model for inter-active image-to-video synthesis which learns such a ﬁne-grained understanding of object dynamics and, thus, is able to synthesize video sequences that exhibit natural responses to local user interactions with images on pixel-level. Using intuitions from physics [56], we derive a hierarchical recur-rent model dedicated to model complex, ﬁne-grained object dynamics. Without making assumptions on objects we learn to interact with and no ground-truth interactions provided, we learn our model from video sequences only.
We evaluate our model on four video datasets compris-ing the highly-articulated object categories of humans and plants. Our experiments demonstrate the capabilities of our proposed approach to allow for ﬁne-grained user interac-tion. Further, we prove the plausibility of our generated ob-ject dynamics by comparison to state-of-the-art video pre-diction methods in terms of visual and temporal quality.
Figure 1 provides an overview over the capabilities of our model. 2.