Abstract
We present the ﬁrst single-view 3D reconstruction net-work aimed at recovering geometric details from an input image which encompass both topological shape structures and surface features. Our key idea is to train the network to learn a detail disentangled reconstruction consisting of two functions, one implicit ﬁeld representing the coarse 3D shape and the other capturing the details. Given an in-put image, our network, coined D2IM-Net, encodes it into global and local features which are respectively fed into two decoders. The base decoder uses the global features to re-construct a coarse implicit ﬁeld, while the detail decoder re-constructs, from the local features, two displacement maps, deﬁned over the front and back sides of the captured ob-ject. The ﬁnal 3D reconstruction is a fusion between the base shape and the displacement maps, with three losses enforcing the recovery of coarse shape, overall structure, and surface details via a novel Laplacian term. 1.

Introduction
Reconstructing 3D shapes from single-view RGB im-ages is the prototypical ill-posed problem in computer vi-sion. Recently, rapid advances in deep learning have pro-pelled the development of data-driven single-view 3D re-construction methods. In particular, the emergence of neu-ral implicit models [5, 27, 22] for 3D shape representation learning has led to much improved reconstruction quality compared to methods designed for voxel grids, meshes, and point clouds. However, while technically the implicit ﬁelds could be sampled to an arbitrarily high spatial resolution, state-of-the-art reconstruction methods still are unable to adequately recover ﬁne-level geometric details.
Implicit reconstruction networks such as IM-Net [5] and
Occupancy Network [22] learn to predict an implicit func-tion, given a feature encoding of the input image, by min-imizing a reconstruction loss. These networks generalize well to new images, but only in terms of the coarse shapes; they are not designed to recover geometric details which are often of small scale and do not incur a sufﬁcient penalty on the loss terms. In a more recent work, DISN, Xu et al. [44]
Figure 1. Our network learns to reconstruct a detail disentangled 3D representation from single-view images. The disentangled de-tails enable detail transfer and 3D reconstruction (shown in two views) with the transferred details from image to another. account for both global and local image features to predict a combined signed distance ﬁeld (SDF) so as to minimize a single reconstruction loss like prior works. Their network can better resolve structural details, such as the slats in the back of a chair, that are well captured by local image fea-tures. However, the rest of the details, in particular surface details, which are just as important for visual perception (e.g., of depth and material), are still not well recovered.
In this paper, we wish to develop an implicit single-view 3D reconstruction network which can recover both topolog-ical structures and surface details from an input image. Our key idea is that to best reconstruct the details, we ought to train the network to learn a detail disentangled reconstruc-tion consisting of two functions, one representing the coarse 3D shape and one capturing the details. However, the main ensuing challenge is that geometric details are so varied that there is no general and reliable way to deﬁne what the de-tails are or what a coarse shape should be. The network must learn the disentangled representations without direct supervision using ground-truth training data.
Figure 2 illustrates the pipeline of our detail disentangled implicit reconstruction network, coined D2IM-Net. Given a single RGB input image, the network encodes it into global and local features which are respectively fed into two de-coders. The base decoder uses the global features to re-construct a coarse (i.e., base) implicit ﬁeld, while the detail decoder reconstructs, from the local features, a pair of 2D 10246
Figure 2. The pipeline of our single-view 3D reconstruction network D2IM-Net consists of three stages. An encoder extracts global and local features from the input image. This is followed by two decoder branches which respectively predict a base or coarse shape from global features and two displacement maps (back and front) from local features. The ﬁnal 3D reconstruction is a fusion between the base shape and the displacement maps, with three losses enforcing recovery of coarse shape, overall structure, and surface details (Laplacian).
Figure 3. Illustration of a ground-truth (GT) shape+SDF (a) and a disentanglement into a base shape+SDF (b) and a displacement
ﬁeld (c). Bottom row plots SDF, displacement ﬁeld, and Laplacian values along the front surface (purple lines) of the GT shape. We see close resemblance between the Laplacian of the displacement
ﬁeld values and that of the GT SDF: blue vs. red curves in (e).
Note that at training, only the GT SDF is known (indicated by orange borders in the ﬁgure); all other ﬁelds are to be learned. displacement maps, deﬁned over the front and back sides of the captured object that are visible to the camera.
In the absence of any ground-truth displacement maps for training, or coarse shapes for that matter, we must rely on the original 3D shapes (e.g., from ShapeNet) or their as-sociated SDFs to deﬁne the network losses. We ﬁrst observe that the Laplacian of the SDF of a shape near the shape surface is sensitive to local geometry variations1, i.e., the 1The Laplacian of a signed distance function at a point x is proportional to the mean curvature of the isosurface passing through x [8].
Figure 4. A visualization of 3D shapes reconstructed by the two decoders of D2IM-Net demonstrates detail disentanglement: our network learns to recover surface details via the front displacement map and other details from the back map. The network was trained on ShapeNet across 13 shape categories. surface details. Furthermore, this Laplacian function resem-bles the Laplacian of the front displacement map if the front side of the coarse shape is mostly ﬂat; see Figure 3. Based on these observations, we deﬁne a corresponding Laplacian loss to optimize the front displacement map.
In addition, we deﬁne a base loss and an SDF loss, both with respect to the ground-truth SDF, where the SDF loss is computed against a fusion between the predicted coarse
SDF and the predicted displacement maps, both the front and the back. As the back displacement map is not fac-tored into the Laplacian loss, it does not capture surface details. However, with local image features as input, the
SDF loss does enforce the back map to help reconstruct the overall shape structure, including topological details. Fig-ure 4 visualizes the disentangled functions our network re-constructs on two examples, where the predicted displace-ment maps evidently represent shape details, encompassing 10247
both topological structures and surface features, while the base decoder reconstructs the coarse shape.
We train our network on ShapeNet Core [3] across all 13 shape categories and test the network on single-view reconstruction for a variety of 3D objects, including those captured in “images in the wild”. We conduct various ab-lation studies and present both qualitative and quantitative comparisons between D2IM-Net and representative single-view 3D reconstruction methods including IM-Net [5] and
DISN [44]. While the focus of our work is on reconstructing shape details, evaluations are conducted on images contain-ing objects with varying degrees of geometric details and using different error metrics applicable to overall shapes and edge revelation. Finally, we develop and demonstrate a novel application of D2IM-Net, where the ability to learn detail functions from images enables detail transfer from an image onto a reconstructed 3D shape; see Figure 1. 2.