Abstract
We propose SelfDoc, a task-agnostic pre-training frame-work for document image understanding. Because docu-ments are multimodal and are intended for sequential read-ing, our framework exploits the positional, textual, and vi-sual information of every semantically meaningful compo-nent in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly ﬁne-grained with excessive contextualization. Be-yond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal informa-tion from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mecha-nism for multimodal feature fusion by adaptively empha-sizing language and vision signals. Our framework beneﬁts from self-supervised pre-training on documents without re-quiring annotations by a feature masking training strategy.
It achieves superior performance on multiple downstream tasks with signiﬁcantly fewer document images used in the pre-training stage compared to previous works. 1.

Introduction
Documents, such as business forms, scholarly and news articles, invoices, letters, and text-based emails, encode and convey information through language, visual content, and layout structure. Automated document understanding is a crucial research area for business and academic values.
It can signiﬁcantly reduce labor-intensive document work-ﬂows through automated entity recognition, document clas-siﬁcation, semantic extraction, document completion, etc.
Many works have been proposed applying machine learning for document analysis [13, 15, 37, 36, 15, 18].
However, parsing a document remains non-trivial and poses
*This work was done during the author’s internship at Adobe Research. multiple challenges. One challenge is modeling and under-standing contextual information when interpreting content.
For example, since information in documents is organized for sequential reading, the interpretation of a piece of con-tent relies heavily on its surrounding context. Similarly, a heading can indicate and summarize the meaning of sub-sequent blocks of text, and a caption could be useful for understanding a related ﬁgure. Another challenge is effec-tively incorporating the cues from multiple data modalities.
In contrast to other data formats like images or plain text, documents combine textual and visual information, and both of the two modalities are complemented by the doc-ument layout. Additionally, from a practical perspective, many tasks related to document understanding are label-scarce. A framework that can learn from unlabeled docu-ments (i.e., pre-training) and perform model ﬁne-tuning for speciﬁc downstream applications is more preferred than the one that requires fully-annotated training data.
In this work, we develop a task-agnostic representation learning framework for document images. Our model fully exploits the textual, visual, and positional information of every semantically meaningful component in a document, e.g., text block, heading, and ﬁgure. To model the internal relationships among components in documents, we adopt the contextualized attention mechanism from natural lan-guage processing (NLP) [32] and employ it at the com-ponent level. We design two branches separately for tex-tual and visual representation learning, and later encourage cross-modal learning with the proposed cross-modality en-coder. In order to seek a better modality fusion for down-stream usage, we propose a modality-adaptive attention mechanism to fuse the language and vision features adap-tively. Moreover, our framework learns a generic represen-tation from a collection of unlabeled documents via self-supervised learning, and afterward, it will be ﬁne-tuned on various document-related downstream applications.
There are two major differences between our SelfDoc and LayoutLM [36], which also introduces a task-agnostic document pre-training framework by applying 2D posi-5652
tional encoding to BERT model [9]. 1) Instead of using word as the basic unit for model input, we adopt semanti-cally meaningful components (e.g., text block, heading, ﬁg-ure) as the model input. In a document, a single word can be understood within the local context where it is found, and does not always require analyzing the entire page for every word. For instance, an answer in a questionnaire tends to be a complete sentence and already delivers semantics. Intro-ducing the contextualization between every single word in documents may be redundant and also ignore localized con-text; 2) We advance the interaction between language and vision modality in the model’s pre-training stage, therefore our model can efﬁciently leverage the multimodal informa-tion from unlabeled data. Comparatively, LayoutLM only considers a single modality in the pre-training stage and in-corporates the visual clues during the ﬁne-tuning phase.
We evaluate our model on three downstream tasks: doc-ument entity recognition, document classiﬁcation, and doc-ument clustering. With the help of our pre-training method, we achieve leading performance on these applications over other pre-training and task-speciﬁc models.
In short, our work contributes to the advancement of document analysis and intelligence by 1) introducing SelfDoc, a novel task-agnostic self-supervised learning framework for document data. Our model establishes the contextualization over a block of content and involves multimodal information; 2) modeling information from multiple modalities via cross-modal learning in the pre-training stage, and proposing a modality-adaptive attention mechanism to fuse language and vision features for downstream usages; 3) demonstrat-ing superior performance by using fewer samples for pre-training. SelfDoc achieves surpassing performance on mul-tiple downstream tasks comparing to other methods. 2.