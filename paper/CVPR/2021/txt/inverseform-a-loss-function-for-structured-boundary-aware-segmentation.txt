Abstract
We present a novel boundary-aware loss term for seman-tic segmentation using an inverse-transformation network, which efﬁciently learns the degree of parametric transfor-mations between estimated and target boundaries. This plug-in loss term complements the cross-entropy loss in capturing boundary transformations and allows consistent and signiﬁcant performance improvement on segmentation backbone models without increasing their size and com-putational complexity. We analyze the quantitative and qualitative effects of our loss function on three indoor and outdoor segmentation benchmarks, including Cityscapes,
NYU-Depth-v2, and PASCAL, integrating it into the train-ing phase of several backbone networks in both single-task and multi-task settings. Our extensive experiments show that the proposed method consistently outperforms base-lines, and even sets the new state-of-the-art on two datasets. 1.

Introduction
Semantic segmentation is a fundamental computer vi-sion task with numerous real-world applications such as au-tonomous driving, medical image analysis, 3D reconstruc-It aims to perform tion, AR/VR and visual surveillance. pixel-level labeling with a given set of target categories.
There have been notable advancements in semantic seg-mentation thanks to recent solutions based on deep learn-ing models, such as the end-to-end fully convolutional net-works (FCN) [30] that lead to signiﬁcant performance gains in popular benchmarks. Extensive studies have been con-ducted to improve semantic segmentation. One prevail-ing direction is to integrate multi-resolution and hierarchi-cal feature maps [40][38]. Another ambitious objective is to exploit boundary information to enhance segmenta-tion [50][39] further, driven by the observations that seg-mentation prediction errors are more likely to occur near the boundaries [39] [22]. In parallel, multi-task learning (MTL)
*Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc.
Image
Baseline
Baseline w/ ours
Figure 1: Left: Images from Cityscapes val benchmark [9].
Middle: Segmented prediction for an HRNet-48-OCR [45]
[49] baseline. Right: Same backbone trained using our In-verseForm boundary loss. Our model achieves clear im-provements, e.g. the curbside boundary in the top ﬁgure is aligned better with the structure of boundary, and the curb-side in the bottom is correctly detected. frameworks [14][31] explored joint optimization of seman-tic segmentation and supplemental tasks such as boundary detection [14] [44][31] or depth estimation [48].
Our approach is aligned with making the best use of boundary exploration. One of the main differences is that most of the previous works use a weighted cross-entropy loss as their loss function for boundary detection, which we show in Figure 2, is sub-optimal for measuring the bound-ary shifts. This is also partially observed in [23]. The cross-entropy loss mainly builds on estimated and ground-truth pixel label changes but ignores the spatial distance of the pixels from the target boundaries. It cannot effectively mea-sure localized spatial variations such as translation, rotation, or scaling between predicted and target boundaries. 5901
Motivated to address this limitation, we introduce a boundary distance-based measure called InverseForm, into the popular segmentation loss functions. We design an in-verse transformation network to model the distance between boundary maps, which can efﬁciently learn the degree of parametric transformations between local spatial regions.
This measure allows us to achieve a signiﬁcant and consis-tent improvement in semantic segmentation using any back-bone model without increasing the inference size and com-putational complexity of the network.
Speciﬁcally, we propose a boundary-aware segmenta-tion scheme during the training phase, by integrating our spatial-distance loss, InverseForm, into the existing pixel-based loss. Our distance-based loss complements the pixel-based loss in capturing boundary transformations. We uti-lize our inverse transformation network for distance mea-surement from boundaries and jointly optimize pixel-label accuracy and boundary distance. We can integrate our pro-posed scheme into any segmentation model; for instance, we adopt the latest HRNet [45] architecture as one of the backbones since it maintains high resolution feature maps.
We also adopt various MTL frameworks [31] [44] to leverage on their boundary detection task towards further segmentation performance improvement, at no added com-putational and memory cost. In this variant, we show a con-sistent performance improvement across all tasks as a plus.
We conduct comprehensive experiments on large
[37], benchmark
Cityscapes [9] and PASCAL-Context [15].
For NYU-Depth-v2, we show that InverseForm based segmentation outperforms the state-of-art in terms of mean intersection-over-union (mIoU). We also show that we outperform state-of-the-art Multi-task learning methods on PAS-CAL in terms of their multi-task performance.
This includes superior performance in predicted edge quality on odsF-score [32] and improvement in other tasks, such as human-parts estimation and saliency estimation, on mIoU.
Then, we perform rigorous experiments on Cityscapes to compare our method with contemporary works such as
SegFix [50] and Gated-SCNN [39]. including NYU-Depth-v2 datasets
The contributions of our work include the following:
• We propose a boundary distance-based measure, In-verseForm, to improve semantic segmentation. We show that our speciﬁcally tailored measure is signiﬁ-cantly more capable of capturing the spatial boundary transforms than cross-entropy based measures, thus re-sulting in more accurate segmentation results.
• Our scheme is agnostic to the backbone architecture choice and is very ﬂexible to be plugged into any exist-ing segmentation model, with no additional inference cost. It does not impact the main structure of the net-work due to its plug-and-play property. It is ﬂexible and can ﬁt into multi-task learning frameworks.
• We show through extensive experiments that our boundary-aware-segmentation method consistently outperforms its baselines, and also outperforms the state-of-the-art methods in both single-task (on NYU-Depth-v2) and multi-task settings (on PASCAL). 2.