Abstract
Indoor scene semantic parsing from RGB images is very challenging due to occlusions, object distortion, and view-point variations. Going beyond prior works that leverage ge-ometry information, typically paired depth maps, we present a new approach, a 3D-to-2D distillation framework, that enables us to leverage 3D features extracted from large-scale 3D data repositories (e.g., ScanNet-v2) to enhance 2D features extracted from RGB images. Our work has three novel contributions. First, we distill 3D knowledge from a pretrained 3D network to supervise a 2D network to learn simulated 3D features from 2D features during the training, so the 2D network can infer without requiring 3D data. Second, we design a two-stage dimension normaliza-tion scheme to calibrate the 2D and 3D features for better integration. Third, we design a semantic-aware adversarial training model to extend our framework for training with un-paired 3D data. Extensive experiments on various datasets,
ScanNet-V2, S3DIS, and NYU-v2, demonstrate the superior-ity of our approach. Also, experimental results show that our 3D-to-2D distillation improves the model generalization. 1.

Introduction
Indoor scene parsing from images plays an important role in many applications such as robot navigation and augmented reality. Though a considerable amount of advancements have been obtained with convolutional neural networks, this task is still very challenging, since the task inherently suffers from various issues, including distorted object shapes, severe occlusions, viewpoint variations, and scale ambiguities.
One approach to address the issues is to leverage auxiliary geometric information to obtain structured information that complements the RGB input. For the auxiliary input, existing methods typically employ the depth map that associates with the input RGB image. However, earlier methods [15, 11, 6, 37, 41, 47, 10] require the availability of the depth map
*: Corresponding authors
Figure 1. Compared with extracting features solely from the input image (a) for semantic parsing, our new approach (b) efﬁciently distills 3D features learned from a large-scale 3D data repository to train the 2D CNN to learn to enhance its features for better semantic parsing. Our framework needs point cloud inputs only in training but not in testing, and the point cloud can be paired or unpaired. inputs not only in the training but also in the testing. As a result, they have limited applicability to general situations, in which depth is not available. This is in contrast to the ubiquity of 2D images, which can be readily obtained by the many photo-taking devices around us.
To get rid of the constraint, several methods [50, 58, 54, 59, 22] propose to predict a depth map from the RGB input, then leverage the predicted depth to boost the scene parsing performance. However, depth prediction from a single image is already a very challenging task on its own. Hence, the performance of these methods largely depends on the quality of the predicted depth. Also, the additional depth prediction raises the overall complexity of the network.
Besides the above issues, a common limitation of the prior works is that they only explore the depth map as the auxiliary geometry cue. Yet, a depth map can only give a partial view of a 3D scene, so issues like occlusions and viewpoint varia-tions are severe. Further, they all require paired RGB-depth data in training. So, they are limited for use on datasets with 4464
depth maps, which require tedious manual preparation, e.g., hardware setups, complicated calibrations, etc.
In this work, we present the ﬁrst ﬂexible and lightweight framework (see Figure 1), namely 3D-to-2D distillation, to distill occlusion-free, viewpoint-invariant 3D representations derived from 3D point clouds for embedding into 2D CNN features by training the network to learn to simulate 3D fea-tures from the input image. Our approach leverages existing large-scale 3D data repositories such as ScanNet-v2 [8] and
S3DIS [1] and recent advancements in 3D scene understand-ing [14, 7, 17, 21] for 3D feature extraction, and allows the use of unpaired 3D data to train the network.
For the 2D CNN to effectively learn to simulate 3D fea-tures, our 3D-to-2D distillation framework incorporates a two-stage dimension normalization (DN) module to explic-itly align the statistical distributions of the 2D and 3D fea-tures. So, we can effectively reduce the numerical distribu-tion gap between the 2D and 3D features, as they are from different data modalities and neural network models. Also, a Semantic Aware Adversarial Loss (SAAL) is designed to serve as the objective of model optimization without paired 2D-3D data to make the framework ﬂexible to leverage ex-isting 3D data repository and boost its applicability.
We conduct extensive experiments on indoor scene pars-ing datasets ScanNet-v2 [8], S3DIS [1], and NYU-v2 [44].
With only a negligible amount of extra computation cost, our approach consistently outperforms the baselines includ-ing the state-of-the-art depth-assisted semantic parsing ap-proach [22] and our two baselines that leverage depth maps, manifesting the superiority of our approach. Besides, our further in-depth experiments on a depth reconstruction task implies that our framework can effectively embed 3D rep-resentations into 2D features and produce much better re-construction results. More importantly, our model obtains a signiﬁcant performance gain (19.08% vs. 27.22% mIoU), even when evaluated on data from an unseen domain, sug-gesting that the 3D information embedded by our 3D-to-2D distillation helps promote the generalizability of CNNs. 2.