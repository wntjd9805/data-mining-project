Abstract
Image of the scene
Ground truth
Feature learning for 3D object detection from point clouds is very challenging due to the irregularity of 3D point cloud data. In this paper, we propose Pointformer, a Trans-former backbone designed for 3D point clouds to learn fea-tures effectively. Speciﬁcally, a Local Transformer module is employed to model interactions among points in a local region, which learns context-dependent region features at an object level. A Global Transformer is designed to learn context-aware representations at the scene level. To fur-ther capture the dependencies among multi-scale represen-tations, we propose Local-Global Transformer to integrate local features with global features from higher resolution.
In addition, we introduce an efﬁcient coordinate reﬁnement module to shift down-sampled points closer to object cen-troids, which improves object proposal generation. We use
Pointformer as the backbone for state-of-the-art object de-tection models and demonstrate signiﬁcant improvements over original models on both indoor and outdoor datasets. 1.

Introduction 3D object detection in point clouds is essential for many real-world applications such as autonomous driving [10] and augmented reality [18]. Compared to images, 3D point clouds can provide detailed geometry and capture 3D struc-ture of the scene. On the other hand, point clouds are irreg-ular, which can not be processed by powerful deep learn-ing models, such as convolutional neural networks directly.
This poses a big challenge for effective feature learning.
The common feature processing methods in 3D detec-tion can be roughly categorized into three types, based on the form of point cloud representations. Voxel-based ap-*Equal contribution.
†Work done prior to Amazon.
‡Corresponding author.
Top-50 attention Top-100 attention Top-200 attention
Figure 1. Attention maps directly from Pointformer block, darker blue indicates stronger attention. For the key point (star), Pointformer ﬁrst focuses on the local region of the same object (the back of the chair), then spreads the attention to other regions (the legs), ﬁnally attends to points from other objects glob-ally (other chairs), leveraging both local and global dependencies. proaches [28, 12, 42] gridify the irregular point clouds into regular voxels and are followed by sparse 3D convolutions to learn high dimensional features. Though effective, voxel-based approaches face the dilemma between efﬁciency and accuracy. Speciﬁcally, using smaller voxels gains more pre-cision, but suffers from higher computational cost. Con-versely, using larger voxels misses potential local details in the crowded voxels.
Alternatively, point-based approaches [25], inspired by the success of PointNet [21] and its variants, consume raw points directly to learn 3D representations, which mitigates the drawback of converting point clouds to some regular structures. Leveraging learning techniques for point sets, point-based approaches avoid voxelization-induced infor-mation loss and take advantage of the sparsity in point clouds by only computing on valid data points. Neverthe-less, due to the irregularity of point cloud data, point-based learning operations have to be permutation-invariant and adaptive to the input size. To achieve this, it learns sim-ple symmetric functions (e.g. using point-wise feedforward networks with pooling functions) which highly restricts its representation power. 7463
Feature Learning Module n o i t c e e
D t s d a e
H
Scene Point Cloud
Pointformers
Upsampling
Keypoints Abstraction
Context Aggregation
Output from Previous Block 3D Boxes  Pointformer Block  Upsample Block
Learned Representations
Local
Transformer
Local-Global
Transformer
Multiscale Cross-Attention
Global
Transformer
A Pointformer Block
Figure 2. The Pointformer backbone for 3D object detection in point clouds. A basic feature learning block consists of three parts: a Local
Transformer to model interactions in the local region; a Local-Global Transformer to integrate local features with global information; a
Global Transformer to capture context-aware representations at the scene level.
Hybrid approaches [41, 15, 39, 24] attempt to combine both voxel-based and point-based representations. [41, 15] leverages PointNet features at the voxel level and a column of voxels (pillar) level respectively. [39, 24] deeply inte-grate voxel features and PointNet features at the scene level.
However, the fundamental difference between the two rep-resentations could pose a limit on the effectiveness of these approaches for 3D point-cloud feature learning.
To address the above limitations, we resort to the Trans-former [30] models, which have achieved great success in the ﬁeld of natural language processing. Transformer models [8] are very effective at learning context-dependent representations and capturing long range dependencies in the input sequence. Transformer and the associate self-attention mechanism not only meet the demand of permu-tation invariance, but also are proved to be highly expres-sive. Speciﬁcally, [6] proves that self-attention is at least as expressive as convolution. Currently, self-attention has been successfully applied to classiﬁcation [23] and 2D ob-ject detection [2] in computer vision. However, the straight-forward application of Transformer to 3D point clouds is prohibitively expensive because computation cost grows quadratically with the input size.
To this end, we propose Pointformer, a backbone for 3D point clouds to learn features more effectively by leveraging the superiority of the Transformer models on set-structured data. As shown in Figure 2, Pointformer is a U-Net structure with multi-scale Pointformer blocks. A Pointformer block consists of Transformer-based modules that are both expres-sive and friendly to the 3D object detection task. First, a
Local Transformer (LT) module is employed to model in-teractions among points in the local region, which learns context-dependent region features at an object level. Sec-ond, a coordinate reﬁnement module is proposed to ad-just centroids sampled from Furthest Point Sampling (FPS) which improves the quality of generated object proposals.
Third, we propose Local-Global Transformer (LGT) to inte-grate local features with global features from higher resolu-tion. Finally, Global Transformer (GT) module is designed to learn context-aware representations at the scene level. As illustrated in Figure 1, Pointformer can capture both local and global dependencies, thus boosting the performance of feature learning for scenes with multiple cluttered objects.
Extensive experiments have been conducted on several detection benchmarks to verify the effectiveness of our ap-proach. We use the proposed Pointformer as the backbone for three object detection models, CBGS [42], VoteNet [19], and PointRCNN [25], and conduct experiments on three in-door and outdoor datasets, SUN-RGBD [27], KITTI [10], and nuScenes [1] respectively. We observe signiﬁcant im-provements over the original models on all experiment set-tings, which demonstrates the effectiveness of our method.
In summary, we make the following contributions:
• We propose a pure transformer model, Pointformer, which serves as a highly effective feature learning backbone for 3D point clouds. Pointformer is permu-tation invariant, local and global context-aware.
• We show that Pointformer can be easily applied as the drop-in replacement backbone for state-of-the-art 3D object detectors for the point cloud.
• We perform extensive experiments using Pointformer as the backbone for three state-of-the-art 3D object detectors, and show signiﬁcant performance gains on several benchmarks including both indoor and out-door datasets. This demonstrates that the versatility of Pointformer as 3D object detectors are typically de-signed and optimized for either indoor or outdoor only. 7464
Positional Encoding
Coordinate Refinement
FFN
Attention maps
Furthest Point 
Sampling
Grouping
Features
Self-attention
Transformer Block  x2
Max-pooling
FFN
Figure 3. Illustration of the Local Transformer. Input points are ﬁrst down-sampled by FPS and generate local regions by ball query.
Transformer block takes point features and coordinates as input and generate aggregated features for the local region. To further adjust the centroid points, attention maps from the last Transformer layer are adopted for coordinate reﬁnement. As a result, points are pushed closer to the object centers instead of surfaces. 2.