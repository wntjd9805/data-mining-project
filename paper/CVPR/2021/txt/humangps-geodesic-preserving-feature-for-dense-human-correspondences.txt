Abstract 1.

Introduction
In this paper, we address the problem of building dense correspondences between human images under arbitrary camera viewpoints and body poses. Prior art either as-sumes small motion between frames or relies on local de-scriptors, which cannot handle large motion or visually am-biguous body parts, e.g., left vs. right hand. In contrast, we propose a deep learning framework that maps each pixel to a feature space, where the feature distances reﬂect the geodesic distances among pixels as if they were projected onto the surface of a 3D human scan. To this end, we intro-duce novel loss functions to push features apart according to their geodesic distances on the surface. Without any se-mantic annotation, the proposed embeddings automatically learn to differentiate visually similar parts and align differ-ent subjects into an uniﬁed feature space. Extensive experi-ments show that the learned embeddings can produce accu-rate correspondences between images with remarkable gen-eralization capabilities on both intra and inter subjects. 1
∗Work done while the author was an intern at Google. 1Project webpage: https://feitongt.github.io/HumanGPS/
Finding correspondences across images is one of the fundamental problems in computer vision and it has been studied for decades. With the rapid development of digi-tal human technology, building dense correspondences be-tween human images has been found to be particularly use-ful for many applications, such as non-rigid tracking and reconstruction [12, 11, 36, 13], neural rendering [48], and appearance transfer [62, 57]. Traditional approaches in computer vision extract image features on local keypoints and generate correspondences between points with simi-lar descriptors after performing a nearest neighbor search, e.g., SIFT [29]. More recently, deep learning methods
[26, 59, 42, 14], replaced hand-crafted components with full end-to-end pipelines. Despite their effectiveness on many tasks, these methods often deliver sub-optimal results when performing dense correspondences search on humans, due to the high variation in human poses and camera viewpoints and visual similarity between body parts. As a result, the existing methods either produce sparse matches, e.g., skele-ton joints [7], or dense but imprecise correspondences [15]. 11820
In this paper, we propose a deep learning method to learn a Geodesic PreServing (GPS) feature taking RGB images as input, which can lead to accurate dense corre-spondences between human images through nearest neigh-bor search (see Figure 1). Differently from previous meth-ods using triplet loss [42, 18], i.e. hard binary decisions, we advocate that the feature distance between pixels should be inversely correlated to their likelihood of being cor-respondences, which can be intuitively measured by the geodesic distance on the 3D surface of the human scan (Fig-ure 2). For example, two pixels having zero geodesic dis-tance means they project to the same point on the 3D surface and thus a match, and the probability of being correspon-dences becomes lower when they are apart from each other, leading to a larger geodesic distance. While the geodesic preserving property has been studied in 3D shape analy-sis [43, 23, 33], e.g., shape matching, we are the ﬁrst to extend it for dense matching in image space, which encour-ages the feature space to be strongly correlated with an un-derlying 3D human model, and empirically leads to accu-rate, smooth, and robust results.
To generate supervised geodesic distances on the 3D sur-face, we leverage 3D assets such as RenderPeople [1] and the data acquired with The Relightables [16]. These high quality 3D models can be rigged and allow us to gener-ate pairs of rendered images from the same subject under different camera viewpoints and body poses, together with geodesic distances between any locations on the surface.
In order to enforce soft, efﬁcient, and differentiable con-straints, we propose novel single-view and cross-view dense geodesic losses, where features are pushed apart from each other with a weight proportional to their geodesic distance.
We observe that the GPS features not only encode local image content, but they also have a strong semantic mean-ing. Indeed, even without any explicit semantic annotation or supervision, we ﬁnd that our features automatically dif-ferentiate semantically different locations on the human sur-face and it is robust even in ambiguous regions of the human body (e.g., left hand vs. right hand, torso vs. back). More-over, we show that the learned features are consistent across different subjects, i.e., the same semantic points from other persons still map to a similar feature, without any inter-subject correspondence data provided during the training.
In summary, we propose to learn an embedding that signiﬁcantly improves the quality of the dense correspon-dences between human images. The core idea is to use the geodesic distance on the 3D surface as an effective supervi-sion and combine it with novel loss functions to learn a dis-criminative feature. The learned embeddings are effective for dense correspondence search, and they show remark-able intra- and inter-subjects robustness without the need of any cross-subject annotation or supervision. We show that our approach achieves state-of-the-art performance on both
Figure 2. Core idea: we learn a mapping from RGB pixels to a feature space that preserves geodesic properties of the underlying 3D surface. The 3D geometry is only used in the training phase. intra- and inter-subject correspondences and that the pro-posed framework can be used to boost many crucial com-puter vision tasks that rely on robust and accurate dense correspondences, such as optical ﬂow, human dense pose regression [15], dynamic fusion [36] and image-based mor-phing [24, 25]. 2.