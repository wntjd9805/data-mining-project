Abstract
MoCo
BYOL
SimCLR
SwAV
We present a large-scale study on unsupervised spatiotem-poral representation learning from videos. With a uniﬁed per-spective on four recent image-based frameworks, we study a simple objective that can easily generalize all these meth-ods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across: (i) different unsupervised frameworks, (ii) pre-training datasets, (iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, e.g., we discover that encouraging long-spanned persis-tency can be effective even if the timespan is 60 seconds.
In addition to state-of-the-art results in multiple bench-marks, we report a few promising cases in which unsuper-vised pre-training can outperform its supervised counterpart.
Code will be made available at https://github.com/ facebookresearch/SlowFast. 1.

Introduction
A series of recent methods on unsupervised representation learning from images [36, 12, 32, 9] are based on maximiz-ing a similarity objective for different views of the same image under data augmentations [18, 90]. In addition to the artiﬁcial augmentations on images, videos can provide natural augmentations of visual content under various chang-ing factors, such as motion, deformation, occlusion, and illu-mination. This work aims to generalize these image-based methods [36, 12, 32, 9] into space-time.
We study a simple objective that can be easily incorpo-rated into these image-based methods. Our hypothesis is that the visual content is often temporally-persistent along a timespan in the video. This persistency may involve an action (e.g., a person dancing), an object (e.g., an individ-ual person, who transitions from running to walking), and a scene (e.g., a room with people moving), covering short to long spans, with different levels of visual invariance (ac-tion, object, scene). Our objective simply encourages the visual representations in different clips of the same video y c a r u c c a r a e n i l s c i t e n i
K 75 65 55 45 35 k 1
θ
… 1
Number of temporal clips  2
ρ 3 q
θ
θ k 2
θ t 1 t 2
Time
…
Figure 1. Learning to maximize the similarity between differ-ent temporal clips of the same video encourages feature persis-tency over time. A query clip (q) is matched to multiple key clips (k1, k2, . . .) that are temporally shifted. This method can be incor-porated into several unsupervised learning frameworks (MoCo [36],
SimCLR [12], BYOL [32], SwAV [9]). The ﬁgure on the top shows that increasing the number (ρ) of temporal clips improves represen-tation quality for all these frameworks. to be similar. We empirically ﬁnd that this objective works well across different unsupervised frameworks (MoCo [36],
SimCLR [12], BYOL [32], SwAV [9]), either with or without using dissimilar (negative) samples.
Our objective is a natural generalization of crops in im-ages [18, 90] to clips in videos. This allows us to make use of the recent unsupervised learning frameworks with minimal modiﬁcations. We aim to learn a high-level repre-sentation of the categorical semantics present in a video by enforcing persistency of the representation over space-time.
We investigate factors such as the effective timespan, t, be-tween positives, and number of temporal clips, ρ, to ﬁnd that longer timespans (up to a minute) and multiple samples are beneﬁcial for downstream performance (Fig. 1).
Our unsupervised training is performed on large-scale data, including Kinetics [47] (240k videos) and three ver-sions of million-scale Instagram sets. In addition to standard linear probing, we evaluate representation quality on mul-tiple classiﬁcation and detection downstream datasets, e.g.,
Charades [76], Something-Something [31], and AVA [33]. 3299    
Our results suggest that unsupervised pre-training can achieve competitive performance in videos, and it can sur-pass the supervised pre-training counterparts in a few cases.
Finally, our study also reveals room for improvement along multiple directions.
In summary, our large-scale study involves the following
ﬁve aspects: (i) Four unsupervised learning frameworks (MoCo [36],
SimCLR [12], BYOL [32], SwAV [9]) viewed from a uni-ﬁed perspective, and incorporated with a simple temporal persistency objective; (ii) Three pre-training datasets, including the relatively well-controlled Kinetics [47] and the relatively “in-the-wild”
Instagram sets at million-scale; (iii) Six downstream datasets/tasks for evaluating repre-sentation quality; (iv) Ablation experiments on different factors, such as temporal samples, contrastive objective, momentum en-coders, training duration, backbones, data augmentation, curated vs. uncurated, trimmed vs. untrimmed, etc.; and (v) State-of-the-art results of unsupervised video represen-tation learning on established benchmarks, UCF-101 [78],
HMDB51 [50] and Kinetics-400 [47] . 2.