Abstract 1.

Introduction
We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Speciﬁcally, we implicitly model a defor-mation graph via a deep neural network. This neural de-formation graph does not rely on any object-speciﬁc struc-ture and, thus, can be applied to general non-rigid defor-mation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface con-sistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not as-sume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings.
Our experiments demonstrate that our Neural Deformation
Graphs outperform state-of-the-art non-rigid reconstruc-tion approaches both qualitatively and quantitatively, with 64% improved reconstruction and 54% improved deforma-tion tracking performance. Code is publicly available.1 1aljazbozic.github.io/neural deformation graphs
Capturing non-rigidly deforming surfaces is essential to-wards reconstructing and understanding real-world environ-ments, which are often highly dynamic. While impres-sive advances have been made in reconstructing static 3D scenes [8, 21], dynamic tracking and reconstruction re-mains very challenging. A plethora of domain-speciﬁc dy-namic tracking methods has been developed (e.g., human bodies, faces, hands), leveraging strong domain shape and motion priors for robust tracking [4, 28, 31, 41]. How-ever, real-world environments encompass a vast diversity of deformable objects – including people with clothing or animals – making domain speciﬁc shape priors often intractable for general deformable reconstruction; in this work, we thus focus on general non-rigid 3D reconstruction without shape or motion priors for general object tracking and reconstruction.
A seminal work in non-rigid 3D reconstruction is Dy-namicFusion [32], which was the ﬁrst approach to demon-strate real-time dense reconstruction of dynamic scenes us-ing just a single RGB-D sensor. DynamicFusion showed promising results towards dynamic reconstruction, but still struggles in many real-world scenarios, which typically in-clude strong deformations and fast frame-to-frame motion, due to its low-level, local correspondence association step. 1450
In particular, the incremental construction of a deformation graph is prone to error aggregation and can lead to track-ing failures. Recently, data-driven methods based on deep learning have been introduced [3, 25, 2] that learn priors of non-rigidly deforming objects from dense ﬂow annota-tions. These approaches leverage a similar incremental de-formation graph construction as DynamicFusion, but learn to establish more robust tracking via more sophisticated correspondence optimization based on data-driven priors.
However, despite more robust correspondences, these meth-ods still operate on a frame-by-frame basis, thus, aggregate tracking errors and are unable to recover if tracking fails. In order to address these shortcomings without assuming data-driven priors, we propose a globally-consistent neural de-formation graph which allows for non-rigid reconstruction from commodity sensor observations, represented as signed distance ﬁelds (see Fig. 1). The neural deformation graph gives access to the per frame deformation graph nodes and stores the global graph connectivity. To robustly optimize for consistent deformations over fast motions, we introduce viewpoint consistency (independently for every frame) as well as graph and surface consistency constraints (between pairs of frames). Our viewpoint consistency loss measures the consistency of graph node position predictions w.r.t. ro-tation augmentation. The graph and surface consistency losses encourage deformations to be modeled in our Neural
Deformation Graph such that local graph edge distances are preserved between frames and the deformed surface geom-etry of a source frame aligns well with the geometry of the target frame. Additionally, our approach does not assume temporally close frames, thus making it easily applicable to low FPS settings or the combination of independently cap-tured depth recordings.
Since there exists no general canonical pose (like a T-pose of a human [28]) that ﬁts all deformable objects, we avoid modeling it explicitly.
Instead, we propose to em-ploy a set of implicit functions that are centered around the deformation graph nodes. Speciﬁcally, we model local signed distance functions (SDFs) using multi-layer percep-trons (MLPs) that can be deformed to ﬁt any frame, without requiring an explicit canonical pose. The global shape is evaluated by the integration of these local MLP predictions.
To summarize, our technical contributions are:
• a globally-optimized deformation graph that is able to handle deformations present in all frames of an un-structured dataset or a sequence of an object;
• a combination of per-frame viewpoint consistency and frame-to-frame graph and surface consistency for ro-bust tracking of fast deformations;
• an implicit deformable multi-MLP shape representa-tion anchored on the scene-speciﬁc deformation graph. 2.