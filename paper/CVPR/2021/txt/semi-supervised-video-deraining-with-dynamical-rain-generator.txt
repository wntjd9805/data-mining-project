Abstract
While deep learning (DL)-based video deraining meth-ods have achieved signiﬁcant successes in recent years, they still have two major drawbacks. Firstly, most of them are insufﬁcient to model the characteristics of rain layers con-tained in rainy videos. In fact, the rain layers exhibit strong visual properties (e.g., direction, scale, and thickness) in spatial dimension and causal properties (e.g., velocity and acceleration) in temporal dimension, and thus can be mod-eled by the spatial-temporal process in statistics. Secondly, current DL-based methods rely heavily on the labeled train-ing data, whose rain layers are synthetic, thus leading to a deviation from real data. Such a gap between synthetic and real data sets results in poor performance when applying them to real scenarios. To address these issues, this paper proposes a new semi-supervised video deraining method, in which a dynamical rain generator is employed to ﬁt the rain layer for the sake of better depicting its intrinsic character-istics. Speciﬁcally, the dynamical generator consists of one emission model and one transition model to simultaneously encode the spatial appearance and temporal dynamics of rain streaks, respectively, both of which are parameterized by deep neural networks (DNNs). Furthermore, different prior formats are designed for the labeled synthetic and unlabeled real data so as to fully exploit their underlying common knowledge. Last but not least, we design a Monte
Carlo-based EM algorithm to learn the model. Extensive experiments are conducted to verify the superiority of the proposed semi-supervised deraining model. 1.

Introduction
Rain is a very common bad weather that exists in many videos. The appearance of rain not only negatively affects the visual quality of the video, but also seriously deterio-*Corresponding author. rates the performance of subsequent video processing algo-rithms, e.g., semantic segmentation [38], object detection
[9], and autonomous driving [7]. Thus, as a necessary video pre-processing step, video deraining has attracted much at-tentions from the computer vision community.
As an ill-posed inverse problem raised by Garg and Na-yar [15], various methods have been proposed to handle the video deraining task [47]. Most of the traditional methods focus on exploiting rational prior knowledge for the back-ground or rain layers so as to obtain a proper separation between them. For example, low-rankness [23, 24, 53] is widely used to encode the temporal correlations of back-ground video. As for rain streaks, many visual character-istics, such as photometric appearance [16], geometrical features [41], chromatic consistency [36], local structure correlations [8] and multi-scale convolutional sparse cod-ing [31], have been explored in the past few years. Differ-ent from these deterministic assumptions for rain streaks,
Wei et al. [53] ﬁrstly regard them as random variables, and use Gaussian mixture model (GMM) to ﬁt them. Al-beit substantiated to be effective in some ideal scenarios, these traditional methods are mainly limited by the subjec-tive manually-designed prior knowledge and huge compu-tation burden.
Recently, owning to the powerful nonlinear ﬁtting ca-pability of DNNs, DL-based methods facilitate signiﬁcant improvements for the video deraining task. The core idea of this methodology is to directly train a derainer parame-terized by DNNs based on synthetic rainy/clean video pairs in an end-to-end manner. Most of these methods leverage different technologies, e.g., superpixel alignment [6], dual-level ﬂow [56] and self-learning [58], to extract clean back-grounds from rainy videos. In addition, Liu et al. [34, 35] design a recurrent network to jointly perform both the rain degradation classiﬁcation and rain removal tasks.
Even though these DL-based methods have achieved im-pressive deraining results on some synthetic benchmarks, there still exists large room to further increase the perfor-642
(a1) (b1) (c1) (a2) (b2)
Figure 1. The comparison of typical synthetic and real rainy im-ages in NTURain data set. (a1)-(c1): synthetic rainy images, (a2)-(c2): real rainy images. (c2) mance and the generalization capability in real applications.
On one hand, most of these methods make efforts to depict the background, but neglect to model the intrinsic charac-teristics of the rain layers. In fact, the rain layer in video, which is an image sequence of rain steaks, can be repre-sented by a spatial-temporal process. Speciﬁcally, the ran-domly scattered rain streaks in each time frame are charac-terized with evident visual properties (e.g., direction, scale, and thickness) in the spatial dimension, and the rain layers in different time frames correspond to a continuous time series along the temporal dimension, showing the causal properties (e.g., velocity and acceleration) of the rain dy-namics. Therefore, elaborately representing and exploiting these intrinsic physical properties underlying the rain layers in video data is expected to facilitate the rain removal task.
On the other hand, it is well known that the performance of DL-based methods heavily relies on a large amount of pre-collected training data, i.e., rainy/clean video pairs. In fact, due to the high labor cost to obtain such video pairs in real scenes, most of current methods have to use synthetic ones, which are manually simulated based on the photo-realistic rendering technique [17] or professional photog-raphy and human supervision [50]. Fig.1 presents several typical frames of synthetic and real rainy images in NTU-Rain [6] data set, which is widely used as a benchmark for current video deraining methods. It can be easily seen that the rain patterns in synthetic and real rainy images are obvi-ously different, and the real ones contain more complex and diverse rain types. Because of such a deviation between synthetic and real data sets, the performances of these DL-based methods deteriorate seriously in the real cases. To deal with the generic video deraining task, it is thus critical to build a reasonable semi-supervised learning framework that sufﬁciently exploits the common knowledge in the la-beled synthetic and unlabeled real data.
To address these issues, in this paper we propose a semi-supervised video deraining method, in which a dynamical rain generator is adopted to mimic the generation process of the rain layers in video, hopefully better capturing the in-trinsic knowledge simultaneously from the spatial and tem-poral dimensions. Besides, the real rainy videos are taken into consideration in our model as unlabeled data, in order to achieve more robust deraining results. In summary, the contributions of this work are as follows:
Firstly, we propose a new probabilistic video deraining method, in which a dynamical rain generator, consisting of a transition model and an emission model, is employed to ﬁt the rain layers in videos. Speciﬁcally, the transition model is used to represent the dynamics of rains in a low-dimensional state space, while the emission model seeks to generate the observed rain streaks in the image space from the state space. To increase the capacities of such a dynam-ical rain generator, both the transition and emission models are parameterized by DNNs. Secondly, a semi-supervised learning mechanism is designed by constructing different prior formats for labeled synthetic data and unlabeled real data. Speciﬁcally, for the labeled synthetic data, the corre-sponding ground truth rain-free videos are included into an elaborate prior distribution as a strong constraint. As for the unlabeled real data, we introduce the 3-D Markov Random
Field (MRF) to model the temporal consistencies and cor-relations of the underlying backgrounds. Thirdly, a Monte
Carlo-based EM algorithm is designed to learn the model.
In the expectation step, the posterior of the latent variables is intractable due to the usages of DNNs to parameterize the generator and derainer, thus the Langevin dynamics is adopted to approximate the expectation. 2.