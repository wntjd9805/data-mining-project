Abstract 1.

Introduction
Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particu-lar importance for virtual telepresence. Currently, the high-est quality is achieved by volumetric approaches trained in a person-speciﬁc manner on multi-view data. These mod-els better represent ﬁne structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation param-eters. While such architectures achieve impressive render-ing quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance ﬁelds with local, pixel-aligned features ex-tracted directly from the inputs, thus side-stepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.
We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.
The acquisition and rendering of photo-realistic human heads is a highly challenging research problem with high signiﬁcance for virtual telepresence applications. Human heads are challenging to model and render due to their com-plex geometry and appearance properties: sub-surface scat-tering of skin, ﬁne-scale surface detail, thin-structured hair, and the human eyes as well as the teeth are both specular and translucent. Most existing approaches require complex and expensive multi-view capture rigs (with up to hundreds of cameras) to reconstruct even a person-speciﬁc model of a human head.
Currently, the highest-quality approaches are those that employ volumetric models rather than a textured mesh, since they can better learn to represent ﬁne structures on the face like hair, which is critical to achieving a photo-realistic appearance. These volumetric models [13] typi-cally employ a global code to represent facial expressions or only work for static scenes [16, 9]. While such archi-tectures achieve impressive rendering quality, they can not easily be adapted to a multi-identity setting. A global code, as is used to control expression, is not sufﬁcient for model-ing identity variation across subjects. There has been sig-niﬁcant progress of late in using implicit models to repre-sent scenes and objects. These models have the advantage 11733
that the scene is represented as a parametric function in a continuous space, which allows for ﬁne-grained inference of geometry and texture [22]. But these approaches can not model view-dependent effects and it is challenging to repre-sent for example hair with a textured surface. The approach of Sitzmann et al. [27] can generalize across objects, but only at low resolutions and can only handle purely Lamber-tian surfaces, which is not sufﬁcient for human heads. De-spite the recent success and advantages of such scene repre-sentation approaches, there are several limitations. In par-ticular, most of the above methods train a network to model only a single scene or object. Methods which can generate multiple objects are typically limited in terms of quality and resolution of the predicted texture and geometry.
We present pixel-aligned volumetric avatars (PVA), a novel framework for the estimation of a volumetric 3D avatar from only a few input images of a human head.
Our approach is able to generalize to unseen identities at test time. Methods such as Scene Representation Networks (SRNs) [25], which generate a set of weights from a global image encoding (i.e., a single latent code vector per image), have difﬁculty generalizing to local changes (e.g., facial ex-pressions) and fail to recover high-frequency details even when these are visible in the input images. This is because the global latent code summarizes information in the im-age and must discard some information to generate a com-pact encoding of the data. To improve generalization across identities, we instead parameterize the volumetric model via local, pixel-aligned features extracted from the input im-ages.
We show that our model can synthesize novel views for unseen identities and expressions while preserving high fre-quency details in the rendered avatar. To summarize, our contribution are:
• We introduce a novel pixel-aligned radiance ﬁeld that predicts implicit shape and appearance from a sparse set of posed images.
• Our model generalizes to unseen identities and expres-sions at test time.
• We demonstrate state of the art performance on novel view synthesis compared to recent approaches. 2.