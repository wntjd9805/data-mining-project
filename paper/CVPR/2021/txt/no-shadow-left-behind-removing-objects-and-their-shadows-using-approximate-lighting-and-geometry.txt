Abstract
Removing objects from images is a challenging technical problem that is important for many applications, including mixed reality. For believable results, the shadows that the object casts should also be removed. Current inpainting-based methods only remove the object itself, leaving shad-ows behind, or at best require specifying shadow regions to inpaint. We introduce a deep learning pipeline for remov-ing a shadow along with its caster. We leverage rough scene models in order to remove a wide variety of shadows (hard or soft, dark or subtle, large or thin) from surfaces with a wide variety of textures. We train our pipeline on syntheti-cally rendered data, and show qualitative and quantitative results on both synthetic and real scenes. 1.

Introduction
Mixed reality aims to seamlessly combine the virtual and the real. As one example, imagine an interior design app that lets you try out new furniture. Most previous work in augmented reality focuses on inserting virtual objects – for instance, putting a virtual sofa into your living room. The scope of these applications can be greatly expanded by also enabling manipulation of real-world objects – imagine re-moving the futon that you intend to replace with the sofa, and moving a coffee table over to make more room for it.
Previous work on object removal has focused solely on the inpainting problem – that is, replacing the pixels previ-ously occupied by the removed object. However, for realis-tic results, we need to remove the sofa and the shadows it casts on the wall and the ﬂoor, as well as the reﬂection on the hardwood ﬂoor. For the purposes of this paper, we focus only on the shadow removal problem.
Existing inpainting-based approaches for object removal either ignore the shadows of the object, or mark them to be inpainted as well. However, very large shadows may leave little image content to copy pixels from. Furthermore, this approach requires segmenting out the object’s shadow in addition to the object itself – a difﬁcult task, as varying lighting conditions can cause multiple shadows, very soft shadows, or overlapping shadows, and a surface texture may have dark regions that could be mistaken for shadows.
Inspired by Debevec’s [9] work in virtual insertion of ob-jects in scenes, we use a scene proxy to help determine the visual effects of a scene manipulation. Debevec performs the scene edit on the proxy model, and renders the proxy pre- and post-edit. The pixelwise difference between the 116397
two renderings, which for object insertion contains shad-ows and reﬂections of the virtual objects, is then applied to the input image to produce the ﬁnal output. This method is known as differential rendering. However, it is not practical to solve the shadow removal problem by applying the pix-elwise difference directly, since the shadows in the proxy model are only a rough estimate of the real shadows. To account for this, we propose a neural network based system for more general differential rendering for object removal.
An obvious question is, how do we obtain an editable scene proxy? One could use a depth camera, monocular depth estimation [13, 14], or a global model obtained as a side effect of localization [8] for the geometry. For light-ing, the possibilities include a mirror sphere, panorama, or learning based methods [22, 29, 30]. In this paper we use depth maps captured by an affordable depth sensor and a 360◦ panorama, but the method is not fundamentally lim-ited to proxies obtained by these devices, nor do the proxy models need to be very accurate. Our proxy mesh is gen-erated from a single depth map, thus modeling only front facing surfaces, and our lighting is captured as an uncali-brated HDR environment map with only very rough align-ment. We show that even this constrained and incomplete proxy provides enough information to generate plausible re-moval results across a wide range of conditions.
In this paper we present a method for removing an ob-ject and its shadows from an input image, given a rough model of the scene and the mask of the object. Our system is more accurate and produces fewer visual artifacts than a general image-to-image translation system or an inpaint-ing method, even when the inpainting method is given the shadow regions it should replace. 2.