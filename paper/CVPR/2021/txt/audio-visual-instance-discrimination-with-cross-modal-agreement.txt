Abstract
We present a self-supervised learning approach to learn audio-visual representations from video and audio. Our method uses contrastive learning for cross-modal discrimi-nation of video from audio and vice-versa. We show that op-timizing for cross-modal discrimination, rather than within-modal discrimination, is important to learn good represen-tations from video and audio. With this simple but powerful insight, our method achieves highly competitive performance when ﬁnetuned on action recognition tasks. Furthermore, while recent work in contrastive learning deﬁnes positive and negative samples as individual instances, we general-ize this deﬁnition by exploring cross-modal agreement. We group together multiple instances as positives by measuring their similarity in both the video and audio feature spaces.
Cross-modal agreement creates better positive and negative sets, which allows us to calibrate visual similarities by seek-ing within-modal discrimination of positive instances, and achieve signiﬁcant gains on downstream tasks. 1.

Introduction
Imagine the sound of waves. This sound can evoke the memory of many scenes - a beach, a pond, a river, etc. A single sound serves as a bridge to connect multiple sceneries.
It can group visual scenes that ‘go together’, and set apart the ones that do not. We leverage this property of freely occur-ring audio to learn video representations in a self-supervised manner.
A common technique [2, 41, 62, 63] is to setup a veriﬁca-tion task that requires predicting if an input pair of video and audio is ‘correct’ or not. A correct pair is an ‘in-sync’ video and audio and an incorrect pair can be constructed by using
‘out-of-sync’ audio [41] or audio from a different video [2].
However, a task that uses a single pair at a time misses a key opportunity to reason about the data distribution at large.
In our work, we propose a contrastive learning framework to learn cross-modal representations in a self-supervised manner by contrasting video representations against mul-∗Work done while interning at Facebook AI Research. tiple audios at once (and vice versa). We leverage recent advances [28, 61, 80, 86] in contrastive learning to setup a Audio-Visual Instance Discrimination (AVID) task that learns a cross-modal similarity metric by grouping video and audio instances that co-occur. We show that the cross-modal discrimination task, i.e., predicting which audio matches a video, is more powerful than the within-modal discrimina-tion task, predicting which video clips are from the same video. With this insight, our technique learns powerful visual representations that improve upon prior self-supervised meth-ods on action recognition benchmarks like UCF-101 [76] and HMDB-51 [42].
We further identify important limitations of the AVID task and propose improvements that allow us to 1) reason about multiple instances and 2) optimize for visual similarity rather than just cross-modal similarity. We use Cross-Modal
Agreement (CMA) to group together videos with high simi-larity in video and audio spaces. This grouping allows us to directly relate multiple videos as being semantically similar, and thus directly optimize for visual similarity in addition to cross-modal similarity. We show that CMA can iden-tify semantically related videos, and that optimizing visual similarity among related videos signiﬁcantly improves the learned visual representations. Speciﬁcally, CMA is shown to improve upon AVID on action recognition tasks such Ki-netics [83], UCF-101 [76] and HMDB-51 [42] under both linear probing and full ﬁne-tuning evaluation protocols. 2.