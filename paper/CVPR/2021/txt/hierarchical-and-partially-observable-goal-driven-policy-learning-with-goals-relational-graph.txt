Abstract
We present a novel two-layer hierarchical reinforcement learning approach equipped with a Goals Relational Graph (GRG) for tackling the partially observable goal-driven task, such as goal-driven visual navigation. Our GRG captures the underlying relations of all goals in the goal space through a Dirichlet-categorical process that facilitates: 1) the high-level network raising a sub-goal towards achieving a desig-nated ﬁnal goal; 2) the low-level network towards an opti-mal policy; and 3) the overall system generalizing unseen environments and goals. We evaluate our approach with two settings of partially observable goal-driven tasks — a grid-world domain and a robotic object search task. Our ex-perimental results show that our approach exhibits superior generalization performance on both unseen environments and new goals 1. 1.

Introduction
Goal-driven visual navigation deﬁnes a task where an intelligent agent (with an on-board camera) is expected to take reasonable steps to navigate to a user-speciﬁed goal in an unknown environment (see Figure 1 left). It is a funda-mental yet essential capability for an agent and could serve as an enabling step for other tasks, such as Embodied Ques-tion Answering [8] and Vision-and-Language Navigation [3].
Goal-driven visual navigation could be formulated as a par-tially observable goal-driven task. In this paper, we present a novel Hierarchical Reinforcement Learning approach with a Goals Relational Graph formulation (HRL-GRG) tackling it. Formally, a partially observable goal-driven task yields a 10-tuple < S, A, T, G, R, Ω, O, Gd, Φ, γ >, in which S is a set of states, A is a set of actions, T : S × A × S → [0, 1] is a state transition probability function, G ⊆ S is a set of goal states, R : S × A × S × G → R is a reward function, 1Codes and models are available at https://github.com/Xin-Ye-1/HRL-GRG.
Ω is a set of observations that are determined by conditional observation probability O : S × Ω → [0, 1]. Similarly,
Gd is a set of goal descriptions that describe observations with goal recognition probability Φ : Ω × Gd → [0, 1].
In particular, g is the goal state of the corresponding goal description gd iff Φ(argmaxω O(g, ω), gd) is larger than a pre-deﬁned threshold. γ ∈ (0, 1] is a discount factor.
The objective of a partially observable goal-driven task is to maximize the expected discounted cumulative rewards
E[P∞ t γtrt+1(st, at, st+1, g)|st, g] by learning an optimal action policy to select an action at at the state st given the observation ot and the goal description gd.
Classic RL methodology optimizes an agent’s decision-making action policy in a given environment [30]. To make
RL towards real-world applicable, equipped with deep neural networks, Deep Reinforcement Learning (DRL) [22] algo-rithms are able to directly take the high dimensional sensory inputs as states S and learn the optimal action policy that generalizes across various states. However, the applicability of most advanced RL algorithms is still limited to domains with fully observed state space S and/or ﬁxed goal states G, which is not the case in reality [22, 21, 20, 29, 11].
For real-world applications like visual navigation, an agent’s sensory inputs capture the local information of its sur-rounding environments (a partially observable state space).
Additionally, the real-world applications could be subject to goal changes, requiring a system to be goal-adaptive. There-fore, a real-world application can be formulated as a par-tially observable goal-driven task, that is different from a fully observable goal-driven task [22, 21, 11, 25] or a par-tially observable task [13, 18, 12]. It requires the agent to be capable of inferring its state in the augmented state space
S × G. Namely, the agent should take actions based on its current relative states with respect to the goal states, which can only be estimated from its sensory observations Ω and the goal descriptions Gd. This is challenging due to 1) the large augmented state space, 2) the different modalities that the observations Ω and the goal descriptions Gd could have.
For example, while RGB images are usually taken as the 14101
observations, semantic labels are more efﬁcient in describing task goals [5]. mance in both experiments over other baseline approaches, with extensive ablation analysis.
To address the challenges, our HRL-GRG incorporates a novel Goals Relational Graph (GRG), which is designed to learn goal relations from the training data through a Dirichlet-categorical process [31] dynamically. In such a way, our model estimates the agent’s states in terms of the learned relations between sub-goals that are visible in the agent’s cur-rent observations and the designated ﬁnal goal. Furthermore, our HRL-GRG decomposes the partially observable goal-driven task into two sub-tasks: 1) a high-level sub-goal se-lection task, and 2) a low-level fully observable goal-driven task. Speciﬁcally, the high-level layer selects a sub-goal sg ∈ Gd that is observable in the current sensory input o, i.e. Φ(o, sg) > 0, and could also contribute to achieving the designated ﬁnal goal g ∈ Gd. The objective of the low-level layer is to achieve the observable sub-goal, yielding a well-studied fully observable task [22, 21, 11].
Many prior DRL methods tackling partially observ-able tasks [13, 18, 12] are not designed for goal-driven tasks. Therefore, their learned policies are not goal-adaptive.
Adapting to new goals is critical for real-world tasks, such as goal-driven visual navigation [40], robotic object search
[38, 36, 5] and room navigation [38]. Current goal-driven vi-sual navigation methods generally neglect the essential role of estimating the agent’s state under the partially observable goal-driven settings effectively, thus their performance still leaves much to be desired especially in terms of general-ization ability (in-depth discussion in Section 2). Here, we argue and show our novel GRG modeling ﬁlls the gap.
Formally, we deﬁne GRG as a complete weighted di-rected graph < V, E, W > in which V = Gd is a set of nodes representing the goals Gd, and E is the directed edges connecting two nodes with the weights W . We incorporate
GRG into HRL via two aspects: 1) weighing each candi-date sub-goal in the high-level layer by C(τ ∗), the cost of the optimal plan τ ∗ from the sub-goal to the goal over the
GRG; 2) terminating the low-level layer referring to the op-timal plan τ ∗ from the proposed sub-goal to the goal over the GRG. To empirically validate the presented system, we start with demonstrating the effectiveness of our method in a grid-world domain where the environments are partially observable and a set of goals following a pre-deﬁned rela-tion are speciﬁed as the task goals. The design follows the intuition in real-world applications that certain relations hold in the goal space. For example, in the robotic object search task, users arrange the household objects in accordance with their functionalities. Another example is the indoor naviga-tion task where room layouts are not random. Furthermore, in addition to the grid-world experiment, we also apply our method to tackle the robotic object search task in both the
AI2-THOR [15] and the House3D [34] benchmark environ-ments. We show HRL-GRG model exhibits superior perfor-2.