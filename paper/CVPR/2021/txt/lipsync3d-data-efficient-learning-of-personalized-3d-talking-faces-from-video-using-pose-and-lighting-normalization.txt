Abstract
In this paper, we present a video-based learning frame-work for animating personalized 3D talking faces from au-dio. We introduce two training-time data normalizations that signiﬁcantly improve data sample efﬁciency. First, we isolate and represent faces in a normalized space that de-couples 3D geometry, head pose, and texture. This decom-poses the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo con-stancy of skin to isolate and remove spatio-temporal light-ing variations. Together, these normalizations allow sim-ple networks to generate high ﬁdelity lip-sync videos under novel ambient illumination while training with just a single speaker-speciﬁc video. Further, to stabilize temporal dy-namics, we introduce an auto-regressive approach that con-ditions the model on its previous visual state. Human rat-ings and objective metrics demonstrate that our method out-performs contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications en-abled by our framework. 1.

Introduction
“Talking head” videos, consisting of closeups of a talk-ing person, are widely used in newscasting, video blogs, online courses, etc. Other applications that feature talking faces prominently are face-to-face live chat, 3D avatars and animated characters in games and movies. We present a deep learning approach to synthesize 3D talking faces (both photorealistic and animated) driven by an audio speech sig-nal. We use speaker-speciﬁc videos to train our model in a data-efﬁcient manner by employing 3D facial tracking. The resulting system has multiple applications, including video editing, lip-sync for dubbing of videos in a new language, personalized 3D talking avatars in gaming, VR and CGI, as well as compression in multimedia communication.
The importance of talking head synthesis has led to a variety of methods in the research literature. Many recent
∗Equal contribution
†Work done while an intern at Google (a)
Synthesized
Video
Mesh
Generation from Audio 3D Alignment 3D
Rendering
Blending
Target
Face Detection
Target Face
Warping (b)
Input Audio
Source
Target
Video
Figure 1: Flow diagram of our approach to (a) generate a dynamically textured 3D face mesh from audio, and (b) insert the generated face mesh into a target video to create a synthesized talking head video from new audio input. techniques [6, 7, 40, 43, 28, 30] use the approach of regress-ing facial motion from audio, employing it to deform one or more reference images of the subject. These approaches can inherit the realism of the reference photos, however, the results do not accurately reproduce 3D facial articulation and appearance under general viewpoint and lighting varia-tions. Another body of research predicts 3D facial meshes from audio [38, 13, 19, 11]. These approaches are directly suitable for VR and gaming applications. However, visual realism is often restricted by the quality of texturing. Some recent approaches [32, 33, 14] attempt to bridge the gap by combining 3D prediction with high-quality rendering, but are only able to edit ﬁxed target videos that they train on.
Our work encompasses several of the scenarios men-tioned above. We can use 3D information to edit 2D video, including novel videos of the same speaker not seen dur-ing training. We can also drive a 3D mesh from audio or text-to-speech (TTS), and synthesize animated characters by predicting face blendshapes. Next, we highlight some of our key design choices.
Personalized models: We train personalized speaker-speciﬁc models, instead of building a single universal model to be applied across different people. While universal mod-2755
els like Wav2Lip [30] are easier to reuse for novel speak-ers, they need large datasets for training and do not ad-equately capture person-speciﬁc idiosyncrasies [5]. Per-sonalized models like ours and NVP [33] produce results with higher visual ﬁdelity, more suitable for editing long speaker-speciﬁc videos. Additionally, our model can be trained entirely using a single video of the speaker. 3D pose normalization: We use a 3D face detector [20] to obtain the pose and 3D landmarks of the speaker’s face in the video. This information allows us to decompose the face into a normalized 3D mesh and texture atlas, thus decoupling head pose from speech-induced face deforma-tions, e.g. lip motion and teeth/tongue appearance.
Lighting normalization: We design a novel algorithm for removing spatial and temporal lighting variations from the 3D decomposition of the face by exploiting traits such as facial symmetry and albedo constancy of the skin. This lighting normalization removes another confounding factor that can otherwise affect the speech-to-lips mapping.
Data-efﬁcient learning: Our model employs an encoder-decoder architecture that computes embeddings from audio spectrograms, and decodes them to predict the decomposed 3D geometry and texture. Pose and light-ing normalization allows us to train this model in a data-efﬁcient manner. The model complexity is greatly reduced, since the network is not forced to disentangle unrelated head pose and lighting changes from speech, allowing it to syn-thesize high quality lip-sync results even from short training videos (2-5 minutes long). Lighting normalization allows training and inference illumination to be different, which obviates the need to train under multiple lighting scenarios.
The model predicts 3D talking faces instead of just a 2D image, even though it learns just from video, broadening its applicability. Finally, pose and lighting normalization can be applied in a backward fashion to align and match the ap-pearance of the synthesized face with novel target videos.
See Figure 1 for an overview of our approach.
Our key technical contributions are:
• A method to convert arbitrary talking head video footage into a normalized space that decouples 3D pose, geome-try, texture, and lighting, thereby enabling data-efﬁcient learning and versatile high-quality lip-sync synthesis for video and 3D applications.
• A novel algorithm for normalizing facial lighting in video that exploits 3D decomposition and face-speciﬁc traits such as symmetry and skin albedo constancy.
• To our best knowledge, this is the ﬁrst attempt at dis-entangling pose and lighting from speech via data pre-normalization for personalized models.
• An easy-to-train auto-regressive texture prediction model for temporally smooth video synthesis.
• Human ratings and objective metrics suggest that our method outperforms contemporary audio-driven video reenactment baselines in terms of realism, lip-sync and visual quality scores. 2.