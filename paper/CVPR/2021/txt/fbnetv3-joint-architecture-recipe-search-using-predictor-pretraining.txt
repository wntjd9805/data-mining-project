Abstract
Neural Architecture Search (NAS) yields state-of-the-art neural networks that outperform their best manually-designed counterparts. However, previous NAS methods search for architectures under one set of training hyper-parameters (i.e., a training recipe), overlooking superior architecture-recipe combinations. To address this, we present Neural Architecture-Recipe Search (NARS) to search both (a) architectures and (b) their corresponding training recipes, simultaneously. NARS utilizes an accuracy pre-dictor that scores architecture and training recipes jointly, guiding both sample selection and ranking. Furthermore, to compensate for the enlarged search space, we leverage
“free” architecture statistics (e.g., FLOP count) to pretrain the predictor, signiﬁcantly improving its sample efﬁciency and prediction reliability. After training the predictor via constrained iterative optimization, we run fast evolution-ary searches in just CPU minutes to generate architecture-recipe pairs for a variety of resource constraints, called
FBNetV3. FBNetV3 makes up a family of state-of-the-art compact neural networks that outperform both automati-cally and manually-designed competitors. For example, FB-NetV3 matches both EfﬁcientNet and ResNeSt accuracy on
ImageNet with up to 2.0× and 7.1× fewer FLOPs, respec-tively. Furthermore, FBNetV3 yields signiﬁcant performance gains for downstream object detection tasks, improving mAP despite 18% fewer FLOPs and 34% fewer parameters than
EfﬁcientNet-based equivalents. 1.

Introduction
Designing efﬁcient computer vision models is a challeng-ing but important problem: A myriad of applications from autonomous vehicles to augmented reality require compact models that must be highly accurate – even under constraints
*Equal contribution
Figure 1: ImageNet accuracy vs. model FLOPs comparison of
FBNetV3 with other efﬁcient convolutional neural networks. FB-NetV3 achieves 80.8% (82.8%) top-1 accuracy with 557M (2.1G)
FLOPs, setting a new SOTA for accuracy-efﬁciency trade-offs. on power, computation, memory, and latency. The number of possible constraint and architecture combinations is combi-natorially large, making manual design a near impossibility.
In response, recent work employs neural architecture search (NAS) to design state-of-the-art efﬁcient deep neu-ral networks. One category of NAS is differentiable neural architecture search (DNAS). These path-ﬁnding algorithms are efﬁcient, often completing a search in the time it takes to train one network. However, DNAS cannot search for non-architecture hyperparameters, which are crucial to the model’s performance. Furthermore, supernet-based NAS methods suffer from a limited search space, as the entire supergraph must ﬁt into memory to avoid slow convergence
[5] or paging. Other methods include reinforcement learn-ing (RL) [45], and evolutionary algorithms (ENAS) [41].
However, these methods share several drawbacks: 1. Ignore training hyperparameters: NAS, true to its name, searches only for architectures but not the asso-ciated training hyperparameters (i.e., “training recipe”).
This ignores the fact that different training recipes may 116276
Training
Model
Recipe-1
Recipe-2
ResNet18 (1.4x width)
ResNet18 (2x depth) 70.8% 70.7% 73.3% 73.8%
Table 1: Different training recipe could switch the ranking of architectures. ResNet18 1.4x width and 2x depth refer to ResNet18 with 1.4 width and 2.0 depth scaling factor, respectively. Training recipe details can be found in Appendix A.1. drastically change the success or failure of an architec-ture, or even switch architecture rankings (Table 1). 2. Support only one-time use: Many conventional NAS approaches produce one model for a speciﬁc set of re-source constraints. This means that deploying to a line of products, each with different resource constraints, requires rerunning NAS once for each resource setting.
Alternatively, model designers may search for one model and scale it suboptimally, using manual heuristics, to ﬁt new resource constraints. 3. Prohibitively large search space to search: Naïvely including training recipes in the search space is either im-possible (DNAS, supernet-based NAS) or prohibitively expensive, as architecture-only accuracy predictors are already computationally expensive to train (RL, ENAS).
To overcome these challenges, we propose Neural
Architecture-Recipe Search (NARS) to address the above limitations. Our insight is three-fold: (1) To support re-use of NAS results for multiple resource constraints, we train an accuracy predictor, then use the predictor to ﬁnd architecture-recipe pairs for new resource constraints in just CPU minutes. (2) To avoid the pitfalls of architecture-only or recipe-only searches, this predictor scores both training recipes and ar-chitectures simultaneously. (3) To avoid prohibitive growth in predictor training time, we pretrain the predictor on proxy datasets to predict architecture statistics (e.g., FLOPs, #Pa-rameters) from architecture representations. After sequen-tially performing predictor pretraining, constrained itera-tive optimization, and predictor-based evolutionary search,
NARS produces generalizable training recipes and compact models that attain state-of-the-art performance on ImageNet, outperforming all the existing manually designed or auto-matically searched neural networks. We summarize our contributions below: 1. Neural Architecture-Recipe Search: We propose a predictor that jointly scores both training recipes and architectures, the ﬁrst joint search, over both training recipes and architectures, at scale to our knowledge. 2. Predictor pretraining: To enable efﬁcient search over this larger space, we furthermore present a pretraining
Figure 2: Accuracy improvement on existing architectures with the searched training recipe. WSL refers to the weakly supervised learning model using 1B additional images [33]. technique, signiﬁcantly improving the accuracy predic-tor’s sample efﬁciency. 3. Multi-use predictor: Our predictor can be used in fast evolutionary searches to quickly generate models for a wide variety of resource budgets in just CPU minutes. 4. State-of-the-art ImageNet accuracy per FLOP for the searched FBNetV3 models. For example, our FB-NetV3 matches EfﬁcientNet accuracy with as low as 49.3% fewer FLOPs, as shown in Fig. 1. 5. Generalizable training recipe: NARS’s recipe-only search achieves signiﬁcant accuracy gains across var-ious neural networks, as illustrated in Fig. 2. Our
ResNeXt101-32x8d achieves 82.6% top-1 accuracy; this even outperforms its weakly-supervised counterpart trained on 1B extra images [33]. 2.