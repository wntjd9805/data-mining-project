Abstract
Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassiﬁcations on model inputs containing a speciﬁc
“trigger.” Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that apply digitally generated patterns as triggers. A critical question remains unanswered: “can backdoor attacks succeed using physi-cal objects as triggers, thus making them a credible threat against deep learning systems in the real world?”
We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task.
Using 7 physical objects as triggers, we collect a custom dataset of 3205 images of 10 volunteers and use it to study the feasibility of “physical” backdoor attacks under a va-riety of real-world conditions. Our study reveals two key
ﬁndings. First, physical backdoor attacks can be highly successful if they are carefully conﬁgured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model’s dependence on key facial features. Sec-ond, four of today’s state-of-the-art defenses against (dig-ital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses.
Our study conﬁrms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classiﬁcation tasks. We need new and more robust defenses against backdoors in the physical world. 1.

Introduction
Despite their known impact on numerous applications from facial recognition to self-driving cars, deep neural net-works (DNNs) are vulnerable to a range of adversarial at-tacks [4, 29, 16, 28, 21, 2, 6]. One such attack is the back-door attack [10, 23], in which an attacker corrupts (i.e. poi-sons) a dataset to embed hidden malicious behaviors into models trained on this dataset. These behaviors only acti-vate on inputs containing a speciﬁc “trigger” pattern.
Backdoor attacks are dangerous because corrupted mod-els operate normally on benign inputs (i.e. achieve high classiﬁcation accuracy), but consistently misclassify any inputs containing the backdoor trigger. This dangerous property has galvanized efforts to investigate backdoor at-tacks and their defenses, from government funding initia-tives (e.g. [39]) to numerous defenses that either identify corrupted models or detect inputs containing triggers [5, 9, 11, 33, 42].
Current literature on backdoor attacks and defenses mainly focuses on digital attacks, where the backdoor trig-ger is a digital pattern (e.g. a random pixel block in Fig-ure 1a) that is digitally inserted into an input. These digital attacks assume attackers have run-time access to the image processing pipeline to digitally modify inputs [15]. This rather strong assumption signiﬁcantly limits the applicabil-ity of backdoor attacks to real-world settings.
In this work, we consider a more realistic form of the backdoor attack. We use everyday, physical objects as back-door triggers, included naturally in training images, thus eliminating the need to compromise the image processing pipeline to add the trigger to inputs. An attacker can acti-vate the attack simply by wearing/holding the physical trig-ger object, e.g. a scarf or earrings. We call these “physical” backdoor attacks. The natural question arises: “can back-door attacks succeed using physical objects as triggers, thus making them a credible threat against deep learning sys-tems in the real world?”
To answer this question, we perform a detailed empirical study on the training and execution of physical backdoor attacks under a variety of real-world settings. We focus pri-marily on the task of facial recognition since it is one of the most security-sensitive and complex classiﬁcation tasks in practice. Using 7 physical objects as triggers, we collect a custom dataset of 3205 face images of 10 volunteers∗. To our knowledge, this is the ﬁrst large dataset for backdoor attacks using physical object triggers without digital ma-nipulation.
∗We followed IRB-approved steps to protect the privacy of our study participants. For more details, see §3.1. 6206
Digital Trigger
Square
Dots
Sunglasses
Physical Triggers
Tattoo Outline Tattoo Filled-in White Tape
Bandana
Earrings
VGG16
DenseNet
ResNet50 91  ±  7%   98  ±  1%  100  ±  0%  100  ±  0%   96  ±  3%  98  ±  4%  100  ±  0%   94  ±  4%  100  ±  0%  99  ± 1%   95 ±  2%  99 ±  1%  99  ±  1%   95  ±  2%  99  ±  1%  98  ±  3%   81  ±  8%  95  ±  5%  98  ±  1%   98  ±  0%  99  ±  0%   69 ±  4%   85 ±  2%  58 ±  4% 
Figure 1: Attack success rates of physical triggers in facial recognition models trained on various architectures.
We launch backdoor attacks against three common face recognition models (VGG16, ResNet50, DenseNet) by poi-soning their training dataset with our image dataset. We adopt the common (and realistic) threat model [10, 19, 18, 40], where the attacker can corrupt training data but cannot control the training process.
Our key contributions and ﬁndings are as follows:
Physical backdoor attacks are viable and effective. We use the BadNets method [10] to generate backdoored mod-els and ﬁnd that when a small fraction of the dataset is poi-soned, all but one of the 7 triggers we consider (“earrings”) lead to an attack success rate of over 90%. Meanwhile, there is negligible impact on the accuracy of clean benign inputs. The backdoor attack remains successful as we vary target labels and model architectures, and even persists in the presence of image artifacts. We also conﬁrm some of these ﬁndings using a secondary object recognition dataset.
Empirical analysis of contributing factors. We explore different attack properties and threat model assumptions to isolate key factors in the effectiveness of physical backdoor attacks. We ﬁnd that the location of the trigger is a critical factor in attack success, stemming from models’ increased sensitivity to features centered on the face and reduced sen-sitivity to the edge of the face. We identify this as the cause of why earrings fail as triggers.
We relax our threat model and ﬁnd that attackers can still succeed when constrained to poisoning a small fraction of classes in the dataset. Additionally, we ﬁnd that models poisoned by backdoors based on digitally injected physi-cal triggers can be activated by a subject wearing the actual physical triggers at run-time.
Existing defenses are ineffective.
Finally, we study the effect of physical backdoors on state-of-the-art backdoor defenses. We ﬁnd that four strong defenses, Spectral
Signatures [38], Neural Cleanse [42], STRIP [9], and
Activation Clustering [5], all fail to perform as expected on physical backdoor attacks, primarily because they assume that poisoned and clean inputs induce different internal model behaviors. We ﬁnd that these assumptions do not hold for physical triggers.
Key Takeaways. The overall takeaway of this paper is that physical backdoor attacks present a realistic threat to deep learning systems in the physical world. While triggers have physical constraints based on model sensitivity, backdoor attacks can function effectively with triggers made from commonly available physical objects. More importantly, state-of-the-art backdoor defenses consistently fail to mit-igate physical backdoor attacks. Together, these ﬁndings highlight a critical need to develop more robust defenses against backdoor attacks that use physical triggers. 2.