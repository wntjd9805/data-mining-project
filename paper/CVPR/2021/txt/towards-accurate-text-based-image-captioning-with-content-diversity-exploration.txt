Abstract
Text-based image captioning (TextCap) which aims to read and reason images with texts is crucial for a machine to understand a detailed and complex scene environment, considering that texts are omnipresent in daily life. This task, however, is very challenging because an image often contains complex texts and visual information that is hard to be described comprehensively. Existing methods attempt to extend the traditional image captioning methods to solve this task, which focus on describing the overall scene of im-ages by one global caption. This is infeasible because the complex text and visual information cannot be described well within one caption. To resolve this difﬁculty, we seek to generate multiple captions that accurately describe dif-ferent parts of an image in detail. To achieve this purpose, there are three key challenges: 1) it is hard to decide which parts of the texts of images to copy or paraphrase; 2) it is non-trivial to capture the complex relationship between diverse texts in an image; 3) how to generate multiple cap-tions with diverse content is still an open problem. To con-quer these, we propose a novel Anchor-Captioner method.
Speciﬁcally, we ﬁrst ﬁnd the important tokens which are supposed to be paid more attention to and consider them as anchors. Then, for each chosen anchor, we group its rel-evant texts to construct the corresponding anchor-centred graph (ACG). Last, based on different ACGs, we conduct the multi-view caption generation to improve the content diversity of generated captions. Experimental results show that our method not only achieves SOTA performance but also generates diverse captions to describe images. 1.

Introduction
The texts are omnipresent in our daily life and play an important role in helping humans or intelligent robots to
*Authors contributed equally.
†Corresponding author
Figure 1. Comparison with existing methods. For a given image, existing methods tend to generate only one global caption. Un-like them, we ﬁrst select and group texts to anchor-centred graphs (ACGs), and then decide which parts of the texts to copy or para-phrase. Our method is able to achieve higher accuracy and gener-ate diverse captions to describe the image from different views.
In the image cap-understand the physical world [13]. tioning area, the texts contained in images are also of critical importance and often provide valuable informa-tion [5, 19, 20, 34, 41] for caption generation. In this sense,
Sidorov et al. [40] propose a ﬁne-grained image caption-ing task, i.e., text-based image captioning (TextCap), aim-ing to generate image captions that not only ‘describe’ vi-sual contents but also ‘read’ the texts in images, such as billboards, road signs, commodity prices and etc. This task is very practical since the ﬁne-grained image captions with rich text information can aid visually impaired people to comprehensively understand their surroundings [13]
Some preliminary tries for the TextCap task seek to di-rectly extend existing image captioning methods [2, 19, 21] to this new setting. However, such methods usually tend to describe prominent visual objects or overall scenes with-out considering the texts in images. Recently, M4C-Captioner [40] tries to use additional OCR tools [4, 6, 31] to recognise texts in images. It is still hard to well describe the complex text and visual information within one caption.
To resolve this difﬁculty, we propose to generate multiple diverse captions focusing on describing different parts of an image. However, there are still some challenges.
First, it is hard to decide which parts of the texts are most 12637
crucial for describing the images. As shown in Figure 1, an image often contains a lot of texts, but only a small part of the texts play a key role in caption generation. For example, a PC keyboard contains many letters, but we do not need a caption that covers all the recognised letters.
Second, it is non-trivial to capture the complex relation-ship between diverse texts in an image. The correct under-standing of such a relationship is essential for generating accurate captions. For example, to accurately describe a cup, we might use its brand and capacity. But these texts have no relevance to the contents on the computer screen.
More critically, how to generate multiple captions de-scribing different contents remains unknown. Current im-age captioning methods [2, 19, 21] often only generate a content-monotone caption. They tend to focus on a small part of the contents in the image, such as the time in the monitor in Figure 1. To comprehensively describe an im-age, one better solution is to generate diverse captions, where each caption focuses on describing one relevant part.
To address the above issues, we design a new Anchor-Captioner architecture that mainly consists of two key mod-ules, i.e., an anchor proposal module (AnPM) and an an-chor captioning module (AnCM). Speciﬁcally, AnPM is proposed to understand the texts in an image and to cap-ture the complex relationships between different texts. To be speciﬁc, we ﬁrst employ an anchor predictor to rank the importance of each token. Then, we choose several im-portant tokens to decide which parts of texts are most in-formative and need to be carefully considered. After that, considering each chosen token as an anchor, we use a re-current neural network to model its complex relationships with other tokens and to construct an anchor-centred graph (ACG) for each anchor. Each ACG denotes a group of the relevant tokens which are supposed to be included in the same caption. Based on the different ACGs for an image, we apply AnCM to generate diverse captions that cover var-ious OCR tokens. To be speciﬁc, we ﬁrst generate a visual-speciﬁc caption to model global visual information. Then, we take each ACG as guidance to reﬁne the visual cap-tion and generate multiple text-speciﬁc captions that con-tain ﬁne-grained text information. Extensive experimental results on TextCaps dataset demonstrate the effectiveness of our proposed method.
In summary, our main contributions are as follows: 1. We propose to exploit ﬁne-grained texts information to generate multiple captions that describe different parts of images, instead of generating a single caption to handle them as a whole. 2. We propose an anchor proposal module (AnPM) and an anchor captioning module (AnCM) to select and group texts to anchor-centred graphs (ACGs) and then generate diverse captions based on ACGs. 3. We achieve the state-of-the-art results on TextCaps dataset, in terms of both accuracy and diversity. 2.