Abstract
The inability of state-of-the-art semantic segmentation methods to detect anomaly instances hinders them from be-ing deployed in safety-critical and complex applications, such as autonomous driving. Recent approaches have focused on either leveraging segmentation uncertainty to identify anomalous areas or re-synthesizing the image from the semantic label map to ﬁnd dissimilarities with the in-put image.
In this work, we demonstrate that these two methodologies contain complementary information and can be combined to produce robust predictions for anomaly segmentation. We present a pixel-wise anomaly detection framework that uses uncertainty maps to improve over exist-ing re-synthesis methods in ﬁnding dissimilarities between the input and generated images. Our approach works as a general framework around already trained segmentation networks, which ensures anomaly detection without com-promising segmentation accuracy, while signiﬁcantly out-performing all similar methods. Top-2 performance across a range of different anomaly datasets shows the robustness of our approach to handling different anomaly instances. 1.

Introduction
Recent advances in deep learning have shown signiﬁ-cant improvements in the ﬁeld of computer vision. Neural networks have become the de-facto methodology for clas-siﬁcation, object detection, and semantic segmentation due to their high accuracy in comparison to previous methods
[35, 34, 41]. However, while the predictions of these net-works are highly accurate, they usually fail when encoun-tering anomalous inputs (i.e. instances outside the training distribution of the network).
* Equal Contribution
This work was partially supported by the Hilti Group and the National
Center of Competence in Research (NCCR) Robotics through the Swiss-National Science Foundation. 16918
With this work, we focus on the inability of existing se-mantic segmentation models to localize anomaly instances and how this limitation hinders them from being deployed in safety-critical, in-the-wild scenarios. Consider the case of a self-driving vehicle that uses a semantic segmentation model. If the agent encounters an anomalous object (i.e. a wooden box in the middle of the street), the model could wrongly classify this object as part of the road and lead the vehicle to crash.
To detect such anomalies in the input, we build our approach upon two established groups of methods. The
ﬁrst group uses uncertainty estimation to detect anoma-lies. Their intuition follows that a low-conﬁdence predic-tion is likely an anomaly. However, uncertainty estimation methods themselves are still noisy and inaccurate. Previ-ous works [24, 4] have shown that these models fail to de-tect many unexpected objects. Example failure cases are shown in Figure 1 (top and bottom) where the anomaly ob-ject is either detected but miss-classiﬁed or non-detected and blended with the background. In both cases, the seg-mentation network is overconﬁdent about its prediction and, thus, the estimated uncertainty (softmax entropy) is low.
The second group focuses on re-synthesizing the input image from the predicted semantic map and then comparing the two images (input and generated) to ﬁnd the anomaly.
These models have shown promising results when dealing with segmentation overconﬁdence but fail when the seg-mentation outputs a noisy prediction for the unknown ob-ject, as shown in Figure 1 (middle). This failure is explained by the inability of the synthesis model to reconstruct noisy patches of the semantic map, which complicates ﬁnding the differences between input and synthesized images.
In this paper, we propose a novel pixel-level anomaly framework that combines uncertainty and re-synthesis ap-proaches in order to produce robust predictions for the dif-ferent anomaly scenarios. Our experiments show that un-certainty and re-synthesis approaches are complementary to each other, and together they cover the different outcomes when a segmentation network encounters an anomaly.
Our framework builds upon previous re-synthesis meth-ods [24, 12, 38] of reformulating the problem of segment-ing unknown classes as one of identifying differences be-tween the input image and the re-synthesised image from a predicted semantic map. We improve over those frame-works by integrating different uncertainty measures, such as softmax entropy [10, 21], softmax difference [31], and perceptual differences [16, 8] to assist the dissimilarity net-work in differentiating the input and generated images. The proposed framework successfully generalizes to all anoma-lies scenarios, as shown in Figure 1, with minimal addi-tional computation effort and without the need to jeopardize the segmentation network accuracy (no re-training neces-sary), which is one common ﬂaw of other anomaly detec-tors [3, 26, 27]. Besides maintaining state-of-the-art perfor-mance in segmentation, eliminating the need for re-training also reduces the complexity of adding an anomaly detector to future segmentation networks, as training these networks is non-trivial.
We evaluate our framework in public benchmarks for anomaly detection, where we compare to methods similar to ours that not compromise segmentation accuracy, as well as those requiring full retraining. We also demonstrate that our framework is able to generalize to different segmenta-tion and synthesis networks, even when these models have lower performance. We replace the segmentation and syn-thesis models with lighter architectures to prioritize speed in time-critical scenarios like autonomous driving.
In summary, our contributions are the following: – We present a novel pixel-wise anomaly detection framework that leverages the best features of existing uncertainty and re-synthesis methodologies. – Our approach is robust to the different anomaly sce-narios, achieving state-of-the-art performance on the
Fishyscapes benchmark while maintaining state-of-the-art segmentation accuracy. – Our proposed framework is able to generalize to dif-ferent segmentation and synthesis networks, serving as a wrapper methodology to existing segmentation pipelines. 2.