Abstract with state-of-the-art methods concerning both attribute and identity ﬁdelity.
In this work, we propose a novel two-stage framework named FaceInpainter to implement controllable Identity-Guided Face Inpainting (IGFI) under heterogeneous do-mains. Concretely, by explicitly disentangling foreground and background of the target face, the ﬁrst stage focuses on adaptive face ﬁtting to the ﬁxed background via a Styled
Face Inpainting Network (SFI-Net), with 3D priors and tex-ture code of the target, as well as identity factor of the source face.
It is challenging to deal with the inconsis-tency between the new identity of the source and the origi-nal background of the target, concerning the face shape and appearance on the fused boundary. The second stage con-sists of a Joint Reﬁnement Network (JR-Net) to reﬁne the swapped face. It leverages AdaIN considering identity and multi-scale texture codes, for feature transformation of the decoded face from SFI-Net with facial occlusions. We adopt the contextual loss to implicitly preserve the attributes, en-couraging face deformation and fewer texture distortions.
Experimental results demonstrate that our approach han-dles high-quality identity adaptation to heterogeneous do-mains, exhibiting the competitive performance compared
†Corresponding author 1.

Introduction
Identity swapping is a technique that enables manipulat-ing the face appearance according to the source face while preserving the attributes of the target face. DeepFakes [1],
FaceSwap [2], FaceShifter [18], and SimSwap [5] are the prominent methods. These methods enable the effortless creation of fake faces, while the controllable performance and visual realism still need to be improved. Furthermore, face manipulation provides more challenging and diverse samples to facilitate face forensics [31].
The face swapping task is evaluated mainly from two as-pects, i.e. identity and attribute ﬁdelity for the source and target face, respectively. FaceShifter [18] adopts SPADE
[28] and AdaIN [14] mechanisms to integrate spatial at-tributes and identity styles in the decoder, while some re-dundant identity features of the source, e.g., hair, make it difﬁcult to preserve attributes in some challenging cases, as shown in Figure 2. SimSwap [5] uses a weak feature match-ing loss based on the last few layers of the discriminator, to avoid the attribute distortions during identity modiﬁca-5089
tion. Despite this implicit constraint works well, it is similar to the commonly used perceptual loss for texture matching used in [38, 49, 24], which is more appropriate for paired data learning. Thus, the generated faces of SimSwap pre-serve lots of attributes in some challenging cases, so that they look more like the target face. In the non-photorealistic domain, we take FaceSwap [2] as an example, the main is-sues of this 3D-based method include low resolution and obvious modiﬁcation artifacts around the facial area.
In this work, we ﬁrst study the heterogeneous iden-tity swapping problem. The heterogeneous face has been studied in some works [42, 10, 11, 41].
It is challeng-ing to perfectly ﬁt target faces with a new identity under heterogeneous domains, especially for an extreme pose, expression, and lighting conditions. We propose an ef-ﬁcient Identity-Guided Face Inpainting (IGFI) framework to improve the controllability and generalization of the face-swapping model. Facial attributes of the target face are supposed to be preserved during the IGFI process, in-cluding head pose, expression, lighting, facial conclusions, hairstyle, and other background contents. Additionally, we attempt to adaptively improve the visual quality for the tar-get scenes with low resolution.
In our work, the identity feature is extracted from the source face via a pretrained state-of-the-art face recogni-tion model [6]. Generally, the identity feature contains the high-level semantic representation of the source, which is used to control the identity modiﬁcation. As for attribute preservation of the target, a 3D ﬁtting model [12] is used to extract 3DMM parameters of the target, such as expression and posture, which are vital to performing IGFI based on the ﬁxed background. For the purpose of preserving tex-ture, a pretrained face parsing model [19] is used to extract the foreground content (i.e. face and neck), which is fed into a pretrained VGG network [33] to obtain the texture style.
After that, the multiple factors with regard to shape and tex-ture from the source and target faces are recombined into one style code, to achieve efﬁcient and controllable IGFI in speciﬁc scenes, based on the Styled Face Inpainting Net-work (SFI-Net). Particularly, to preserve the high-ﬁdelity non-face areas (e.g. background, hair, clothes, etc.), we di-rectly add them to the last layer of SFI-Net.
To weaken the inconsistency of the target face shape and the face inpainting result for a new identity, we mask both the face and neck regions which are visually crucial for the facial shape representation in the ﬁrst stage. However, for some complex scenes, the segmented background and the generated foreground can not be well integrated. Therefore, in the second stage, we propose a Joint Reﬁnement Net-work (JR-Net) to jointly reﬁne the identity, attributes, and boundary fusion, by adopting Adaptive Instance Normaliza-tion (AdaIN) [14] with respect to identity and multi-scale texture codes. With super-resolution and occlusion-aware
Source
Target
FaceShifter
Ours
Source
Target
SimSwap
Ours
Source1
Target1
FaceSwap1
Ours1
Source2
Target2
FaceSwap2
Ours2
Figure 2: There are still some challenging situations, e.g., preserving excessive source identity (FaceShifter [18]), pre-serving more attributes (SimSwap [5]), and suffering from low resolution and boundary artifacts (FaceSwap [2]). Our model realizes more controllable face swapping in photore-alistic (row 1) and non-photorealistic (row 2) domains. modules, JR-Net further optimizes the visual and occlusion perception of the result from SFI-Net. For non-aligned fea-ture matching between the target and deformed face, we adopt the contextual constraints [23] which can implicitly keep the attribute consistency. As shown in Figure 1, our method can generate high-quality identity-guided swapped faces adapted to various heterogeneous domains, even in cartoon and exaggerated styles.
Our contributions are three folds.
• We ﬁrst study the heterogeneous identity swapping task, and propose a novel solution to deal with the
Identity-Guided Face Inpainting (IGFI) problem. By considering high-ﬁdelity identity and attributes, our approach FaceInpainter achieves more controllable and higher quality results than previous face inpaint-ing methods.
• We propose an effective IGFI framework. In the ﬁrst stage, we introduce a Styled Face Inpainting Network (SFI-Net) to map the identity and attribute codes to the swapped face. The second stage contains a Joint Re-ﬁnement Network (JR-Net) that reﬁnes the attributes and identity details, generating occlusion-aware and high-resolution swapped faces with visually natural fused boundary.
• With achieving high ﬁdelity of contextual attribute and identity, we achieve good generalization under hetero-geneous domains both visually and quantitatively. 2.