Abstract
Reconstructing a 3D hand from a single-view RGB im-age is challenging due to various hand conﬁgurations and depth ambiguity. To reliably reconstruct a 3D hand from a monocular image, most state-of-the-art methods heav-ily rely on 3D annotations at the training stage, but ob-taining 3D annotations is expensive. To alleviate reliance on labeled training data, we propose S2HAND, a self-supervised 3D hand reconstruction network that can jointly estimate pose, shape, texture, and the camera viewpoint.
Speciﬁcally, we obtain geometric cues from the input image through easily accessible 2D detected keypoints. To learn an accurate hand reconstruction model from these noisy geometric cues, we utilize the consistency between 2D and 3D representations and propose a set of novel losses to ra-tionalize outputs of the neural network. For the ﬁrst time, we demonstrate the feasibility of training an accurate 3D hand reconstruction network without relying on manual an-notations. Our experiments show that the proposed self-supervised method achieves comparable performance with recent fully-supervised methods. The code is available at https://github.com/TerenceCYJ/S2HAND. 1.

Introduction
Reconstructing 3D human hands from a single image is important for computer vision tasks such as hand-related action recognition, augmented reality, sign language trans-lation, and human-computer interaction [21, 33, 43]. How-ever, due to the diversity of hands and the depth ambiguity in monocular 3D reconstruction, image-based 3D hand re-construction remains a challenging problem.
In recent years, we have witnessed fast progress in re-covering 3D representations of human hands from images.
In this ﬁeld, most methods were proposed to predict 3D hand pose from the depth image [1, 10, 15, 22, 49] or the
∗Work done during an internship at Tencent AI Lab.
†Corresponding author: tuzhigang@whu.edu.cn
Figure 1: Given a collection of unlabeled hand images, we learn a 3D hand reconstruction network in a self-supervised manner. Top: the training uses a collection of unlabeled hand images and their corresponding noisy detected 2D keypoints. Bottom: our model outputs accurate hand joints and shapes, as well as vivid textures.
RGB image [2, 8, 24, 37, 52]. However, the surface infor-mation is needed in some applications such as grasping an object by a virtual hand [21], where the 3D hand pose rep-resented by sparse joints is not sufﬁcient. To better display the surface information of the hand, previous studies pre-dict the triangle mesh either via regressing per-vertex coor-dinate [16, 29] or by deforming a parametric hand model
[19, 20]. Outputting such high-dimensional representation from 2D input is challenging for neural networks to learn, thus resulting in the training process relying heavily on 3D annotations such as dense hand scans, model-ﬁtted paramet-ric hand mesh, or human-annotated 3D joints. Besides, the hand texture is important in some applications, such as vivid hands reconstruction in immersive virtual reality. But only recently has a study exploring parametric texture estimation in a learning-based hand recovery system [35], while most 10451
Approach
[35]
[20, 30]
[48]
[16]
[29]
[51]
[3, 7, 50]
[52]
[24]
[4]
[37]
[8]
Ours
Supervision 3DM, 3DJ, 2DKP, I, TI 3DM, 3DJ 3DM*, 3DJ, 2DKP, 2DS, Syn 3DM*, 3DJ, 2DKP, D* 3DM*, D2DKP 3DJ, 2DKP, Mo 3DJ, 2DKP, 2DS 3DJ, 2DKP, 2DS 3DJ, 2DKP 3DJ*, 2DKP, 2DS 3DJ*, 2DKP 2DKP, D
D2DKP, I
Outputs 3DM, 3DJ, Tex 3DM, 3DJ 3DM, 3DJ 3DM, 3DJ 3DM, 3DJ 3DM, 3DJ 3DM, 3DJ 3DJ 3DJ 3DM, 3DJ, Tex 3DJ 3DJ 3DM, 3DJ, Tex
Table 1: A comparison of some representative 3D hand recovery approaches with highlighting the differences between the supervi-sion and the outputs. We use the weakest degree of supervision and output the most representations. 3DM: 3D mesh, 3DJ: 3D joints,
I: input image, TI: an additional set of images with clear hand tex-ture, Tex: texture, 2DKP: 2D keypoints, 2DS: 2D silhouette, D: depth, D2DKP: detected 2D keypoints, Syn: extra synthetic se-quence data, Mo: extra motion capture data. * indicates that the study uses multiple datasets for training, and at least one dataset used the supervision item. previous works do not consider texture modeling.
Our key observation is that the 2D cues in the image space are closely related to the 3D hand model in the real world. The 2D hand keypoints contain rich structural infor-mation, and the image contains texture information. Both are important for reducing the use of expensive 3D annota-tions but have not been investigated much. In this way, we could directly use 2D annotations and the input image to learn the structural and texture representations without us-ing 3D annotations. However, it is still labor-consuming to annotate 2D hand keypoints. To completely save the cost of manual annotation, we propose to extract some geomet-ric representations from the unlabeled image to help shape reconstruction and use the texture information contained in the input image to help texture modeling.
Motivated by the above observations, this work seeks to train an accurate and robust 3D hand reconstruction net-work only using supervision signals obtained from the in-put images and eliminate all manual annotations, which is the ﬁrst attempt in this task. To this end, we use an off-the-shelf 2D keypoint detector [9] to produce some noisy 2D keypoints and supervise the hand reconstruction by these noisy detected 2D keypoints and the input image. To bet-ter achieve this goal, there are several issues that need to be addressed. First, how to efﬁciently use joint-wise 2D keypoints to supervise the ill-posed monocular 3D hand re-construction? Second, since our setting does not use any ground truth annotation, how do we handle the noise in the 2D detection output?
To address the ﬁrst issue, a model-based autoencoder is presented to estimate 3D joints and shape, where the output 3D joints are projected into image space and forced to align with the detected keypoints during training. However, if we only align keypoints in image space, invalid hand poses of-ten occur. This may be an invalid 3D hand conﬁgure that could be projected to be the correct 2D keypoints. Also, 2D keypoints cannot reduce the scale ambiguity of the pre-dicted 3D hand. Thus, we design a series of priors em-bedded in the model-based hand representations to help the neural network output hand with a reasonable pose and size.
To address the second issue, a trainable 2D keypoint es-timator and a novel 2D-3D consistency loss are proposed.
The 2D keypoint estimator outputs joint-wise 2D keypoints and the 2D-3D consistency loss links the 2D keypoint esti-mator and the 3D reconstruction network to make the two mutually beneﬁcial to each other during the training. In ad-dition, we ﬁnd that the detection accuracy of different sam-ples varies greatly, thus we propose to distinguish each de-tection item to weigh its supervision strength accordingly.
In summary, we present a S2HAND (self-supervised 3D hand reconstruction) model which enables us to train a neural network that can predict 3D pose, shape, texture, and camera viewpoint from a hand image without any ground truth annotation, except that we use the outputs from a 2D keypoint detector (Fig. 1).
Our main contributions are summarized as follows:
• We present the ﬁrst self-supervised 3D hand recon-struction network, which accurately outputs 3D joints, mesh, and texture from a single image, without using any annotated training data.
• We exploit an additional trainable 2D keypoint estima-tor to boost the 3D reconstruction through a mutual improvement manner, in which a novel 2D-3D consis-tency loss is proposed.
• We introduce a hand texture estimation module to learn vivid hand texture through self-supervision.
• We benchmark self-supervised 3D hand reconstruction on some currently challenging datasets, where our self-supervised method achieves comparable performance to previous fully-supervised methods. 2.