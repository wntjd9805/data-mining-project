Abstract
We explore and analyze the latent style space of Style-GAN2, a state-of-the-art architecture for image genera-tion, using models pretrained on several different datasets.
We ﬁrst show that StyleSpace, the space of channel-wise style parameters, is signiﬁcantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to con-trol a distinct visual attribute in a highly localized and dis-entangled manner. Third, we propose a simple method for identifying style channels that control a speciﬁc attribute, using a pretrained classiﬁer or a small number of exam-ple images. Manipulation of visual attributes via these
StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric.
Finally, we demonstrate the applicability of StyleSpace con-trols to the manipulation of real images. Our ﬁndings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces. 1.

Introduction
Modern Generative Adversarial Networks (GANs) are able to produce a wide variety of highly realistic synthetic images. The phenomenal success of these generative mod-els underscores the need for a better understanding of “what makes them tick” and what kinds of control these models offer over the generated data. Of particular practical impor-tance are controls that are interpretable and disentangled, as they suggest intuitive image manipulation interfaces.
In traditional GAN architectures, such as DCGAN [25] and Progressive GAN [16], the generator starts with a ran-dom latent vector, drawn from a simple distribution, and transforms it into a realistic image via a sequence of convo-lutional layers. Recently, style-based designs have become increasingly popular, where the random latent vector is ﬁrst transformed into an intermediate latent code via a mapping function. This code is then used to modify the channel-wise activation statistics at each of the generator’s convolu-tion layers. BigGAN [6] uses class-conditional BatchNorm
[14], while StyleGAN [17] uses AdaIN [13] to modulate channel-wise means and variances. StyleGAN2 [18] con-trols channel-wise variances by modulating the weights of the convolution kernels. It has been shown that the inter-mediate latent space is more disentangled than the initial one [17]. Additionally, Shen et al. [28] show that the latent space of StyleGAN [17, 18] is more disentangled than that of Progressive GAN [16].
Some control over the generated results may be obtained via conditioning [20], which requires training the model with annotated data. In contrast, style-based design enables discovering a variety of interpretable generator controls af-ter training the generator. However, current methods require either a pretrained classiﬁer [10, 28, 29, 34], a large set of paired examples [15], or manual examination of many can-didate control directions [12], which limits the versatility of these approaches. Furthermore, the individual controls dis-covered by these methods are typically entangled, affecting multiple attributes, and are often non-local.
In this work, our goal is to understand to what degree disentanglement is inherent in style-based generator archi-tectures. Perhaps an even more important question is to how to ﬁnd these disentangled controls? In particular, can this be done in an unsupervised manner, or with only a small amount of supervision? In this paper we report several ﬁnd-ings with respect to these questions.
Recent studies of disentangled representations [8, 27] consider a latent representation to be perfectly disentan-gled if each latent dimension controls a single visual at-tribute (disentanglement), and each attribute is controlled by a single dimension (completeness). Following this termi-nology, we explore the latent space of StyleGAN2 [18]. Un-like other works that analyze the (intermediate) latent space
W or W+ [1], we examine StyleSpace, the space spanned by the channel-wise style parameters, denoted S. In Sec-tion 3 we measure and compare the disentanglement and completeness of these spaces using the metrics proposed for this purpose [8]. To our knowledge we are the ﬁrst to apply this quantitative framework to models trained on real 12863
Amount of hair (6 364)
Pillow presence (8 119)
Hubcap style (12 113)
) 6 8 2 1 1 ( s s e n y e r g r i a
H
) 0 2 4 6 ( e l y t s r e v o
C
) 2 4 1 2 1 ( r o l o c r a
C
Figure 1. Disentanglement in style space, demonstrated using three different datasets. Each of the three groups above shows two manip-ulations that occur independently inside the same semantic region (hair, bed, and car, from left to right). The indices of the manipulated layer and channel are indicated in parentheses. data. Our experiments reveal that S is signiﬁcantly better disentangled than W or W+.
In Section 4 we propose a simple method for detect-ing StyleSpace channels that control the appearance of lo-cal semantic regions in the image. By computing the gra-dient maps of generated images with respect to different style parameters, we identify those channels that are con-sistently active in speciﬁc semantic regions, such as hair or mouth, in the case of portraits. We demonstrate the ef-fectiveness of this approach across three different datasets (FFHQ [17], LSUN Bedroom, and LSUN Car [36]). The
StyleSpace channels that we detect are highly localized, af-fecting only a speciﬁc area without any visible impact of other regions. They are also surprisingly well disentangled from each other, as demonstrated in Figure 1.
Our next goal is to identify style channels that control a speciﬁc target attribute. To achieve this goal we require a set of exemplar images that exhibit the attribute of interest.
The basic idea is to compare the average style vector across the exemplar set to the population average, thereby detect-ing dimensions that deviate the most. Our experiments indi-cate that such dimensions usually indeed control the target attribute, and reveal that a single attribute is typically con-trolled by only a few different StyleSpace channels.
To our knowledge, there is no metric to compare the disentanglement of different image manipulation controls.
In Section 6 we propose Attribute Dependency (AD) as a measure for how manipulating a target attribute affects other attributes. Comparing manipulations performed in
StyleSpace to those in W and W+ spaces [12, 29], shows that our controls exhibit signiﬁcantly lower AD.
Finally, we share our insights about the pros and cons of two major image inversion methods, latent optimization
[18, 1, 2] and encoders [38]. We show that a combination of the two may be used in order to apply our StyleSpace controls to disentangled manipulation of real images. 2.