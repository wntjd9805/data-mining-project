Abstract
Reusing features in deep networks through dense con-nectivity is an effective way to achieve high computational efﬁciency. The recent proposed CondenseNet [14] has shown that this mechanism can be further improved if redundant fea-tures are removed. In this paper, we propose an alternative approach named sparse feature reactivation (SFR), aiming at actively increasing the utility of features for reusing. In the proposed network, named CondenseNetV2, each layer can simultaneously learn to 1) selectively reuse a set of most important features from preceding layers; and 2) actively update a set of preceding features to increase their utility for later layers. Our experiments show that the proposed mod-els achieve promising performance on image classiﬁcation (ImageNet and CIFAR) and object detection (MS COCO) in terms of both theoretical efﬁciency and practical speed. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved remarkable success in the past few years [30, 7, 15].
However, their state-of-the-art performance is usually fueled with sufﬁcient computational resources, which hinders de-ploying deep models on low-compute platforms, e.g., mobile phones and Internet of Things (IoT) products. This issue has motivated a number of researchers on designing efﬁcient
CNN architectures [15, 3, 11, 40, 24, 31]. Among these efforts, DenseNet [15] is a promising architecture that im-proves the computational efﬁciency by reusing early features with dense connections.
Recently, it has been shown that dense connectivity may introduce a large number of redundancies when the network becomes deeper [14]. In a dense network, the output of a layer will never be modiﬁed once it is produced. Given that
∗Equal contribution.
†Corresponding author. (a) DenseNet (b) CondenseNet
Features from   -th layer
Input features of           -th layer
Convolutional layer
Copy
Removed connection
Reactivation connection
Figure 1. Different feature reuse patterns in (a) DenseNet [15], (b)
CondenseNet [14] and (c) Ours. (c) Ours shallow features will be repeatedly processed by their follow-ing layers, directly exploiting them in deep layers might be inefﬁcient or even redundant. CondenseNet [14] alleviates this problem via strategically pruning less important connec-tions in DenseNet. ShufﬂeNetV2 [24] shares a similar spirit, where early features are dropped according to layer-distance, leading to an exponentially decaying of long-distance feature re-usage. Although both models show their effectiveness, we hypothesize that straightforwardly abandoning long con-nections is overly aggressive. These early features which are considered to be “obsolete ” at deeper layers may contain useful information, which can beneﬁt network generalization ability, and potentially contribute to a more efﬁcient model if properly utilized.
In this paper, instead of directly discarding obsolete fea-tures, we are interested in whether we can revive them to make obsolete features useful again. To this end, we devel-op a novel module to conduct feature reactivation, which learns to update shallow features and enables them to be more efﬁciently reused by deep layers. Our main idea is illustrated in Figure 1. Compared to DenseNet [15] and
CondenseNet [14], where earlier features keep unchanged 3569
throughout the whole feed-forward process, we propose to allow the outputs of a layer to be reactivated by later layers.
Such a way keeps features maps always “fresh” at each dense layer, and therefore the redundancy in dense connections can be largely reduced.
Although the feature reactivation procedure effectively reduces the redundancy in dense connections, naively reac-tivating all features will introduce excessive extra compu-tation, which still hurts the overall efﬁciency. In fact, it is unnecessary to reactivate all features since a large number of them can be already effectively reused without any change in dense connections, resulting in that only sparse feature reactivation (SFR) is required. For this purpose, we develop a cost-efﬁcient SFR module which actively and selectively reactivates early features at each layer, using the increments learned from the newly produced feature maps. Importantly, both the features to be updated and the updating formulas are determined automatically via learning. During the training process, we ﬁrst assume all previous features require reacti-vating, and then gradually remove the reactivation that have less effect on feature re-usage. Moreover, the resulting SFR modules can be converted to efﬁcient group convolutions at test time. As a consequence, the proposed method involves minimal extra computational cost or latency and keeps early features “fresh” even through very deep layers, which leads to a signiﬁcant efﬁciency gain.
We implement SFR on the basis of the efﬁcient Con-denseNet [14], where the SFR along with the learned group convolutions (LGCs) [14] can be learned compatibly to im-prove the efﬁciency of dense networks. The resulting net-work, CondenseNetV2, are empirically evaluated on image classiﬁcation benchmarks (ImageNet and CIFAR) and the
COCO object detection task. The results demonstrate that
SFR signiﬁcantly boosts the performance by encouraging long-distance feature reusing, and that CondenseNetV2 com-pares favorably with even state-of-the-art light-weighted deep models. We also show that SFR can be plugged into any CNNs that adopt the concatenation based feature reusing mechanism to further improve their efﬁciency, such as Shuf-ﬂeNetV2 [24]. 2.