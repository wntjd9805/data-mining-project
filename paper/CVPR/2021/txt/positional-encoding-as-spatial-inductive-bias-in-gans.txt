Abstract
SinGAN shows impressive capability in learning inter-nal patch distribution despite its limited effective receptive
ﬁeld. We are interested in knowing how such a translation-invariant convolutional generator could capture the global structure with just a spatially i.i.d. input. In this work, taking
SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit posi-tional encoding when using zero padding in the generators.
Such positional encoding is indispensable for generating im-ages with high ﬁdelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN.
We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alterna-tive positional encodings and analyze their effects. Based on a more ﬂexible positional encoding explicitly, we pro-pose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional genera-tor StyleGAN2. Besides, the explicit spatial inductive bias substantially improves SinGAN for more versatile image manipulation.1 1.

Introduction
SinGAN [42] and StyleGAN [21, 22] are among the few representative Generative Adversarial Networks (GANs) that show impressive image generative capability. Both of their generators are based on a fully translation-invariant convolu-tional network. One would expect that in an unconditional setting with a spatially i.i.d. input, the translation invari-ance property should result in position-agnostic outputs like
Fig. 1(b). Nonetheless, the results of SinGAN shows surpris-ingly structured results like Fig. 1(a).
We carefully study this phenomenon and ﬁnd that it is the zero padding that causes a location-aware bias in the distribution of feature maps. Such a spatial bias gradually spreads from the border to the center of feature maps through 1Project page: https://nbei.github.io/gan-pos-encoding.html
Original Image (a) SinGAN (b) SinGAN w/o padding
Original Image (c) SinGAN w/ padding (d) SinGAN w/o padding (e) SinGAN w/ Cartesian Grid (f) SinGAN w/ SPE
Figure 1: Images sampled from the internal patch distribu-tion learned by SinGAN. Above the dotted line, we present sampled balloons with standard SinGAN and padding-free
SinGAN. A more challenging case of generating a school of
ﬁsh is shown below the dotted line. (c)-(f) show the effects of different positional encodings that we explore on SinGAN. the stacked convolutional layers in the generator. One can regard this spatial bias as an implicit positional encoding, which contributes to the high ﬁdelity of images generated by
SinGAN and StyleGAN. Interestingly, we also observe this phenomenon in other unconditional generative architectures such as DCGAN [36] and PGGAN [20].
Our observation reveals the importance of introducing positional encoding in generative models. The original in-tention of zero padding is to maintain the spatial size of fea-ture maps. It is not specially designed to offer the required spatial inductive bias. In particular, we ﬁnd that the bias caused by zero padding is unbalanced over the image space.
Since paddings are introduced at image borders and corners, the positional encoding at those locations is structured. In contrast, the spatial encoding in the center region is highly unstructured due to the gradually diminishing effects of zero padding from borders to the center. The shortcoming of such bias can be observed from Fig. 1 where we use SinGAN 13569
to synthesize an image of a school of ﬁsh. In this example,
SinGAN generates relatively more structured output at the borders but inferior results at the center of the image.
The aforementioned example suggests the shortcoming of padding in serving the role of positional encoding. The desired positional encoding should keep a consistent spatial structure and be invariant to scale transformation. In this study, we investigate two alternatives for explicit positional encoding, i.e., the normalized Cartesian spatial grid [19] and 2D sinusoidal positional encoding [9, 32, 34, 47], which both guarantee a balanced spatial inductive bias over the whole image space. We show that these explicit positional encodings allow a convolutional generator to generate im-ages that exhibit a more stable structure and more reasonable patch reoccurrence given an arbitrary scale in the generation, as shown in Fig. 1(e) and Fig. 1(f).
With the more ﬂexible explicit positional encodings, we can redesign convolutional generative models to synthesize images at multiple scales even just using a single model.
Achieving this functionality is challenging with existing models. One will typically need to train different generators with different upsampling blocks. We show that multi-scale generation with a single fully convolutional generator is pos-sible using our newly proposed multi-scale training strategy based on explicit positional encodings. We call it Multi-Scale training with PositIon Encodings (MS-PIE). We demonstrate its effectiveness in the state-of-the-art unconditional genera-tor StyleGAN2. With MS-PIE, a single StyleGAN2 that is designed for 256 × 256 image generation yields compelling generation quality at multiple scales up to 512 × 512 or even 1024 × 1024 pixels, despite that it only contains limited upsampling blocks in its architecture.
We summarize the contributions of this study as follows: (1) we reveal the phenomenon where zero padding uninten-tionally introduces implicit (but useful) positional encoding in existing convolutional generators. We study this phe-nomenon through detailed theoretical and empirical analy-ses. While the inﬂuence of padding to translation-invariant convolution has been discussed in recent works [2, 18, 23], these studies focus on image classiﬁcation and detection.
Our research is the ﬁrst study that investigates the impacts of such spatial bias on image generation. (2) We further investigate and present two explicit positional encodings as two new spatial inductive bias in generators, which can sub-stantially improve the versatility and robustness of SinGAN. (3) We propose a new multi-scale training strategy to achieve high-quality multi-scale synthesis with a single StyleGAN2 that is originally designed for 256 × 256 generation. 2.