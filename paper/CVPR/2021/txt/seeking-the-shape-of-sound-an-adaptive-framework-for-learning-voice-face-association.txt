Abstract
Nowadays, we have witnessed the early progress on learning the association between voice and face automati-cally, which brings a new wave of studies to the computer vi-sion community. However, most of the prior arts along this line (a) merely adopt local information to perform modality alignment and (b) ignore the diversity of learning difﬁculty across different subjects. In this paper, we propose a novel framework to jointly address the above-mentioned issues.
Targeting at (a), we propose a two-level modality alignment loss where both global and local information are consid-ered. Compared with the existing methods, we introduce a global loss into the modality alignment process. The global component of the loss is driven by the identity classiﬁca-tion. Theoretically, we show that minimizing the loss could maximize the distance between embeddings across differ-ent identities while minimizing the distance between em-beddings belonging to the same identity, in a global sense (instead of a mini-batch). Targeting at (b), we propose a dynamic reweighting scheme to better explore the hard but valuable identities while ﬁltering out the unlearnable iden-tities. Experiments show that the proposed method outper-forms the previous methods in multiple settings, including voice-face matching, veriﬁcation and retrieval. 1.

Introduction
Voice and face share various potential characteristics, e.g., gender, ethnicity, age, which are helpful for identiﬁ-∗Corresponding authors cation and matching. Literatures [23, 10, 15] show that hu-mans can hear the voice of an unknown person and match the corresponding face with higher accuracy than chance, and vice versa. From the perspective of brain science, mul-timodal brain regions exist in the human brain, which pro-cess both voices and faces to form person identity represen-tations [26]. Can machines learn such ability to recognize the face with the same identity only by hearing the voice, or recognize the voice from the face? In recent years, re-searchers have started to seek an answer to this interesting question [17, 30]. The research of this technology is beneﬁ-cial to many application scenarios, including criminal inves-tigation, synthesis or retrieval of human faces from voices
[19, 31, 2, 3], etc. This task can be specialized as cross-modal matching, veriﬁcation and retrieval problems. Dif-ferent from the audio-visual speech recognition task [34], the voice-face association problem is aim to ﬁnd the iden-tity relationship between face and voice, rather than the re-lationship between voice and facial action.
In recent years, we have witnessed some progress of early studies along this line. As a representative exam-ple, SVHF [17] regards the matching problem as a binary classiﬁcation problem, and has achieved comparable perfor-mance with human baseline in both voice-to-face matching and face-to-voice matching. Beneﬁt from the development of deep learning and the cross-modal retrieval technology, some recent work [11, 27, 8, 33, 16] has further veriﬁed the feasibility of this problem through deep metric learning.
Wen et al. [30] boost the performance with multiple super-vision.
Despite previous methods have been able to reach the 16347
easy hard
Figure 1. Accuracy of different identities in the validation set un-der the 1:2 voice-to-face matching setting. There is a signiﬁcant gap between identities, performances of some identities are even lower than chance (50%). same level as untrained humans, there are still two prob-lems in learning voice-face association. (a) The ﬁrst prob-lem is that contrastive loss functions used in previous work only use local information in a mini-batch, which leads to slow convergence. (b) The second one is that the diversity of difﬁculty across identities is ignored. Here the diversity of difﬁculty means that there are obvious differences in the difﬁculty of learning voice-face association among differ-ent subjects. To illustrate this problem, we train a model and test its accuracy on 1:2 voice-to-face matching for dif-ferent subjects. The result is shown in Fig. 1, from which it can be noticed that the accuracy of identities is signiﬁcantly different. This phenomenon coincides with what we ﬁnd in reality. For example, not all male voices are low and rough, and there is an unignorable fraction of male voices that have their own characteristics.
In Fig. 1, we show some easy and hard identities in the obtained results. We observe that the accuracy distribution is relatively uniform, which vali-dates our assumption that the learning difﬁculty is diverse.
Moreover, there exists an unignorable fraction of identities suffering from a accuracy worse than random guess. This validates the existence of extremely hard identities. Identi-ties of this kind are hardly learnable. What is worse, they might even confuse the model and shift the correct decision boundary.
In this paper, we name the hard but learnable identity as hard identity, and the extremely hard identity as personalized identity. In this sense, a reasonable learning method should explore deeper into the hard identities while
ﬁltering out the personalized ones.
Based on the above consideration, in this paper, we pro-pose an adaptive framework for the voice-face association learning. To overcome (a), we introduce a two-level modal-ity alignment, which consists of implicit and explicit modal-ity alignment. The implicit alignment is implemented with an identity-classiﬁcation-driven loss. With the theoreti-cal analysis, we show that minimizing the implicit align-ment loss could maximize the distance of embedding across modalities and identities and minimize the distance of em-beddings across modalities but belong to the same iden-tity. Moreover, the distance is measured from a global perspective instead of a local mini-batch. In this way, the implicit alignment introduces global information and iden-tity semantics in the embeddings. Moreover, the explicit alignment, as a complementary component, aligns the two modalities in a mini-batch directly. For (b), we propose an adaptive framework to handle the hard identities and per-sonalized identities with dynamic identity weights. The hard identities obviously contribute to the bottleneck of the performance of the learning methods. We propose an adap-tive weighting strategy to gradually increase the weights of the hard identities. This encourages the network to dive deeper into the associations between voice and face. Since personalized identities are extremely hard to learn, and their gradients are larger throughout the training phase, forcing the model to learn these samples will reduce the generaliza-tion of the model. Therefore, our proposed strategy adap-tively assigns zero weights to personalized identities.
In a nutshell, the main contributions of this work can be summarized as follows:
• We propose explicit modality alignment and implicit modality alignment to effectively learn the voice-face association in a comprehensive manner.
• We propose an adaptive identity re-weighting frame-work to better explore cross-modal associations from hard identities, and excluding personalized identities for generalization.
• Experiments under various settings are conducted to illustrate the effectiveness of the proposed framework. 2.