Abstract
Recent implicit neural rendering methods have demon-strated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efﬁcient repre-sentations of static scenes that encode all scene objects into a single neural network, and they lack the ability to repre-sent dynamic scenes and decompose scenes into individual objects.
In this work, we present the ﬁrst neural render-ing method that represents multi-object dynamic scenes as scene graphs. We propose a learned scene graph repre-sentation, which encodes object transformations and radi-ance, allowing us to efﬁciently render novel arrangements and views of the scene. To this end, we learn implicitly en-coded scenes, combined with a jointly learned latent repre-sentation to describe similar objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes – only by observing a video of this scene – and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at un-seen poses. 1.

Introduction
View synthesis and scene reconstruction from a set of captured images are fundamental problems in computer graphics and computer vision. Classical methods rely on sequential reconstructions and rendering pipelines that ﬁrst recover a compact scene representation, such as a point-cloud or textured meshes using multi-view stereo [1, 12, 28, 29], which is then used to render novel views using efﬁcient direct or global illumination rendering methods. These sequential pipelines also can recover hierarchical scene representations [13], representing dynamic scenes [8, 25], and efﬁciently rendering novel views [4]. However, tradi-tional pipelines struggle to capture highly view-dependent features at discontinuities, or illumination-dependent re-ﬂectance of scene objects.
Recently, these challenges have been tackled by neural rendering methods. The most successful methods [22, 16] depart from explicit scene representations such as meshes and estimated BRDF models, and instead learn fully im-plicit representations that embed three dimensional scenes in functions, supervised by a sparse set of images during the training. Speciﬁcally, implicit scene representation like
Neural Radiance Fields (NeRF) by Mildenhall et al. [22] encode scene representations within the weights of a neural network that map 3D locations and viewing directions to a neural radiance ﬁeld. The novel renderings from this repre-sentation improve on previous methods of discretized voxel grids [31].
Besides their advantages, recent learned methods encode the entire scene into a single, static network that does not al-low for hierarchical representations or dynamic scenes that are supported by traditional pipelines. Thus, existing neural rendering approaches assume that the training images stem from an underlying scene that does not change between view samples. More recent approaches, such as NeRF-W
[19], attempt to improve on this shortcoming by learning to ignore dynamic objects that cause occlusions in the static scene. However, this approach still relies on a underlying static scene to learn the representation.
In this work, we present the ﬁrst method that is able to learn a representation for complex, dynamic, multi-object scenes. Our method decomposes a scene into its static and dynamic parts and learns their representations structured as a scene graph, with transformations deﬁned by tracking in-formation and supervised by the RGB frames of a video.
The proposed approach allows us to synthesize novel views of a scene, or render views for completely unseen arrange-ments of dynamic objects. Furthermore, we show that the method can also be used for 3D object detection via inverse rendering.
Using automotive data sets, our experiments conﬁrm that our method is capable of representing scenes with highly dynamic objects. We assess the method by generating un-seen views of novel scene compositions with unseen sets of objects at unseen poses. 2856
Speciﬁcally, we make the following contributions:
• We propose a novel neural rendering method that de-composes dynamic, multi-object scenes into a learned scene graph with decoupled object transformations and scene representations.
• We learn object representations for each scene graph node directly from a set of video frames and corre-sponding tracking data. We encode instances of a class of objects using a shared sparse volumetric representa-tion.
• We validate the method on simulated and experimen-tal data by rendering novel unseen views and unseen dynamic scene arrangements of the represented scene.
• We demonstrate that the proposed method facilitates 3D object detection via inverse rendering. 2.