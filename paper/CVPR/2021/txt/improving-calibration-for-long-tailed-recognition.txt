Abstract
Org. CIFAR-100
CIFAR-100-LT, IF100
Deep neural networks may perform poorly when train-ing datasets are heavily class-imbalanced. Recently, two-stage methods decouple representation learning and classi-ﬁer learning to improve performance. But there is still the vital issue of miscalibration. To address it, we design two methods to improve calibration and performance in such scenarios. Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-conﬁdence for classes and im-prove classiﬁer learning. For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework.
Our proposed methods set new records on multiple popu-lar long-tailed recognition benchmark datasets, including
CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. 1.

Introduction such as ImageNet
With numerous available large-scale and high-quality datasets,
[27], COCO [19], and
Places [40], deep convolutional neural networks (CNNs) have made notable breakthrough in various computer vi-sion tasks, such as image recognition [16, 10], object detec-tion [26], and semantic segmentation [6]. These datasets are usually artiﬁcially balanced with respect to the number of instances for each object/class. However, in many real-world applications, data may follow unexpected long-tailed distributions, where the numbers of instances for different classes are seriously imbalanced. When training CNNs on these long-tailed datasets, the performance notably degrades.
To address this terrible issue, a number of methods were proposed for long-tailed recognition.
Recently, many two-stage approaches have achieved signiﬁcant improvement comparing with one-stage meth-ods. Deferred re-sampling (DRS, [4]) and deferred re-weighting (DRW, [4]) ﬁrst train CNNs in a normal way in
Stage-1. DRS tunes CNNs on datasets with class-balanced
CIFAR-100-LT, IF100, cRT CIFAR-100-LT, IF100, LWS
Figure 1: Reliability diagrams of ResNet-32. From top left to bottom right: the plain model trained on the original balanced CIFAR-100 dataset, the plain model, cRT, and LWS trained on CIFAR-100-LT with IF 100. resampling while DRW tunes CNNs by assigning different weights to classes in Stage-2. Zhou et al. [39] proposed bilateral branch network (BBN) in one stage to simulate the process of DRS by dynamically combining instance-balanced sampler and the reverse-balanced sampler. Kang et al. [15] proposed two-stage decoupling models, classiﬁer re-training (cRT) and learnable weight scaling (LWS), to further boost performance, where decoupling models freeze the backbone and just train the classiﬁer with class-balanced resampling in Stage-2.
Conﬁdence calibration [24, 9] is to predict probability by estimating representative of true correctness likelihood. It is important for recognition models in many applications [1, 16489
14]. Expected calibration error (ECE) is widely used in measuring calibration of the network. To compute ECE, all
N predictions are ﬁrst grouped into B interval bins of equal size. ECE is deﬁned as:
• We extensively validate our MiSLAS on multiple long-tailed recognition benchmark datasets – experimental re-sults manifest the effectiveness. Our method yields new state-of-the-art. acc(Sb) − conf(Sb)
× 100%, 2.