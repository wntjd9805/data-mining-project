Abstract
Lip reading aims to predict the spoken sentences from silent lip videos. Due to the fact that such a vision task usually performs worse than its counterpart speech recog-nition, one potential scheme is to distill knowledge from a teacher pretrained by audio signals. However, the latent domain gap between the cross-modal data could lead to a learning ambiguity and thus limits the performance of lip reading. In this paper, we propose a novel collabora-tive framework for lip reading, and two aspects of issues are considered: 1) the teacher should understand bi-modal knowledge to possibly bridge the inherent cross-modal gap; 2) the teacher should adjust teaching contents adaptively with the evolution of the student. To these ends, we in-troduce a trainable “master” network which ingests both audio signals and silent lip videos instead of a pretrained teacher. The master produces logits from three modalities of features: audio modality, video modality, and their com-bination. To further provide an interactive strategy to fuse these knowledge organically, we regularize the master with the task-speciﬁc feedback from the student, in which the requirement of the student is implicitly embedded. Mean-while, we involve a couple of “tutor” networks into our system as guidance for emphasizing the fruitful knowledge
ﬂexibly. In addition, we incorporate a curriculum learning design to ensure a better convergence. Extensive experi-ments demonstrate that the proposed network outperforms the state-of-the-art methods on several benchmarks, includ-ing in both word-level and sentence-level scenarios. 1.

Introduction
Lip reading, which is also referred to as visual speech recognition, aims at predicting words or sentences being spoken from muted lip videos. This vision task enables to switch speech to text without relying on hearing, and there-fore, it could apply to many practical scenarios, such as dub-bling for silent ﬁlms, creating a voice for aphonia patients, and serving for security systems. To tackle this problem, early researches usually adopt HMM with designed hand-∗The ﬁrst two authors contribute equally.
†Corresponding author (hesfe@scut.edu.cn). (a) Traditional KD (b) Ours
Figure 1: (a) Traditional knowledge distillation from pre-trained audio teacher to a video student. (b) Our method distills advanced knowledge from the master that trained not only with both audio and video data, but also the feedback from the student, leading to a more compatible knowledge transfer. Furthermore, the introduced audio and video tutors provide additional cues to the student for further bringing the cross-modal gap. crafted features [6, 10, 19], whereas recent works exploit deep neural networks [18, 29, 23] for lip reading.
Notwithstanding the tremendous success of deep learn-ing and large benchmark construction [8, 1, 2], the admitted most critical challenge is the inherent limitation of visual in-formation, which severely impedes a good performance of lip reading models. For example, different characters “p” and “b” share a similar lip shape so that they are hard to be distinguished in video clips. In contrast, such an uncertainty can be uniquely identiﬁed by audio information, and audio based speech recognition would not be affected by ambigu-ities caused by the intrinsic limitation of visual information.
Speciﬁcally, the counterpart of lip reading task, i.e., speech recognition, could achieve a much more accurate transla-tion of speech to text. The performance gap can even reach 40% [1] on the metric of word error rate (WER).
Based on this fact, one potential solution is to trans-fer knowledge from audio data to video data via knowl-edge distillation (KD) [15] (Fig. 1a). Several previous ap-proaches [29, 31] attempt to build KD-based models, which consist of a teacher network pretrained by audio signals and a student network utilized for lip reading. The audio information is introduced in this way and supposed to be 13325
Table 1: The WER (%) of applying knowledge distillation between audio and video model on LRS2-BBC dataset.
Task
Distilled?
Teacher WER
Lip reading
Speech recognition
Lip reading (KD)
Lip reading (KD)
%
%
!
!
--Video
Audio 57.5 15.7 53.4 54.2 a complementary clue for facilitating the performance of the student. Due to the existed heterogeneity between two modalities, however, such a general audio teacher may only provide limited hidden knowledge to the student for pro-motion. This observation is examined in Tab. 1, in which we conduct an experiment on LRS2-BBC [1] dataset to test the impact of the teachers which share a same structure but pretrained on different modalities. Note that except for the teachers, the structures of both student networks are also the same. Several interesting facts can be observed: (i) The performance gap (about 40% in WER) between lip reading and speech recognition tasks is similar as reported in pre-vious research [1]. (ii) When distilling knowledge from an audio teacher, the performance of the student is even worse than that of the student with distilled knowledge from the video teacher. Combining (i) and (ii), we can conclude that: a “master” that merely has an advanced accuracy (au-dio teacher in this case) does not act as a good teacher to the video student; while the video teacher which shares the same data domain with the student, can supervise the stu-dent for learning more distilled knowledge representations.
Obviously, cross-modal gap is the reason of why this phe-nomenon occurs. Then we spontaneously raise a question:
In lip reading, how can the visual student learn more com-prehensible knowledge from the audio “master”?
We consider the above problem from two aspects. First, the teacher should understand bi-modal knowledge to pos-sibly bridge the inherent cross-modal gap. Second, the teacher should dynamically adjust teaching contents in con-sistent with the evolution of the -student. In this way, the changing requirement of the student would help the teacher to regulate and emphasize the important knowledge from different modalities. Therefore, the cross-modal gap could be fused with a clear aim. In this paper, we propose an inno-vative deep lip reading model (see in Fig. 1b). Instead of us-ing a pretrained teacher network, we design a trainable and much more powerful network named “master”. To produce a more compatible knowledge with regard to visual student, the master takes not only speech audio data but also lip video data as inputs, and produces three types of probabil-ities respectively based on audio modality, video modality, and their combination. Then to fuse these knowledge adap-tively, we incorporate a couple of “tutor” networks into our framework as knowledge fusion guidance, which are pre-trained from audio and video data respectively. Based on the interactions among the “master”, the “tutors”, and the requirement (feedback) of the student, we design a dynamic fusion loss to balance yet fuse different types of knowl-edge. We also propose a curriculum learning strategy to mitigate the learning ambiguity during training, ensuring a better convergence.
In summary, our key contributions are as follows:
• We propose a collaborative learning based framework for lip-reading. Unlike most other existing methods directly using a pretrained teacher to distill knowledge, we embed an advanced trainable master network into our system. The master could be adjusted according to the feedback of the student, and thus provides bi-modal knowledge dynamically for the student to learn in a better way.
• We incorporate a couple of tutor networks into our sys-tem, which are respectively pretrained by audio and video data. To get the master, the tutors, and the stu-dent to cooperate, we particularly tailor a dynamic fu-sion loss to guide the student to learn audio-visual probabilities, which alleviates ambiguities caused by the cross-domain gap.
• We present a curriculum learning strategy for lip-reading. By measuring and sorting the difﬁculty of samples, it could enhance the effectiveness of model training as well as ensuring a better convergence.
• We outperform state-of-the-art lip-reading methods on three benchmarks, indicating the effectiveness of the proposed method. 2.