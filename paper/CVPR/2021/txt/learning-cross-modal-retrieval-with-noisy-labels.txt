Abstract
Recently, cross-modal retrieval is emerging with the help of deep multimodal learning. However, even for unimodal data, collecting large-scale well-annotated data is expen-sive and time-consuming, and not to mention the additional challenges from multiple modalities.
Although crowd-sourcing annotation, e.g., Amazon’s Mechanical Turk, can be utilized to mitigate the labeling cost, but leading to the unavoidable noise in labels for the non-expert annotating.
To tackle the challenge, this paper presents a general Multi-modal Robust Learning framework (MRL) for learning with multimodal noisy labels to mitigate noisy samples and cor-relate distinct modalities simultaneously. To be speciﬁc, we propose a Robust Clustering loss (RC) to make the deep networks focus on clean samples instead of noisy ones. Be-sides, a simple yet effective multimodal loss function, called
Multimodal Contrastive loss (MC), is proposed to maxi-mize the mutual information between different modalities, thus alleviating the interference of noisy samples and cross-modal discrepancy. Extensive experiments are conducted on four widely-used multimodal datasets to demonstrate the effectiveness of the proposed approach by comparing to 14 state-of-the-art methods. 1.

Introduction
With rapid growth of multimedia data, cross-modal re-trieval becomes a compelling topic in the multimodal learn-ing community due to its ﬂexibility in retrieving semanti-cally relevant samples across distinct modalities, e.g., im-age query text [6, 16]. However, most existing methods require clean-annotated training data, which are expensive and time-consuming. Although some unsupervised multi-modal learning methods can mitigate such labeling pres-sure, their performance is usually much worse than the su-pervised counterparts’ [60]. To balance performance and labeling cost, semi-supervised multimodal learning meth-*Corresponding author: Xi Peng (pengx.gm@gmail.com). ods are proposed to simultaneously utilize labeled and un-labeled data to learn common discriminative representa-tions [61, 17]. However, semi-supervised approaches still require a certain number of clean-annotated data to reach reasonable performance.
To alleviate the high labeling cost, some non-expert sources, e.g., Amazon’s Mechanical Turk and the surround-ing tags of collected data, can be used to annotate large-scale data, but resulting in unavoidable noise in labels [48].
Some recent unimodal studies reveal that DNNs easily over-ﬁt to noisy labels leading to poor generalization perfor-mance [59, 28].
It is challenging to learn with noisy la-bels. To tackle the challenge, numerous studies are con-ducted to explore how to robustly learn with noisy labels, such as correction methods [49, 9], MentorNet [19, 58], and
Co-teaching [10]. Although they achieve promising per-formance in the unimodal scenario, they cannot simultane-ously tackle multiple modalities, such as real-world multi-media data. Hence, it is signiﬁcant and valuable to explore how to learn satisfactory representations from multimodal data with noisy labels, but which is rarely touched in previ-ous works.
We perform an empirical study of recent cross-modal learning methods under noisy labels with results shown in
Figure 2. From the ﬁgure, one can see that the networks will fast overﬁt to the noisy training set with a widely-used loss function cross-entropy [50, 53] in multimodal learn-ing. Moreover, different modalities exist a large diversity in validation set since they may lay in completely different spaces with heterogeneity, making learning from noisy sam-ples more difﬁcult. Lastly, noisy labels can confuse the dis-criminative connections across distinct modalities, result-ing in difﬁculty bridging the heterogeneous gap. Thus, it is more challenging and complex to consider both noisy labels and cross-modal discrepancy simultaneously.
To address the aforementioned problems, we propose a
Multimodal Robust Learning framework (MRL) to simul-taneously mitigate the inﬂuence of noisy samples and nar-row the heterogeneous gap in this paper. The pipeline of the proposed method is shown in Figure 1 wherein our 15403
Figure 1: The pipeline of the proposed method for m modalities, e.g., images X1 with noisy labels Y1, and texts Xm with noisy labels Ym. The modality-speciﬁc networks learn common representations for m different modalities. The Robust
Clustering loss Lr is adopted to mitigate the noise in labels for learning discrimination and narrow the heterogeneous gap.
The outputs of networks interact with each other to learn common representations by using instance- and pair-level contrast, i.e., multimodal contrastive learning (Lc), thus further mitigating noisy labels and cross-modal discrepancy. Lc tries to maximally scatter inter-modal samples while compacting intra-modal points over the common unit sphere/space. 1 0.8 y c a r u c c
A 0.6 0.4 0.2 0 0 50
Epoch (a) 0.6 0.5 0.4 0.3 0.2 0.1 y c a r u c c
A 100 0 0 0.4 0.3 0.2 0.1 0 100 50
Epoch (b)
)
P
A
M ( i i n o s c e r
P e g a r e v
A n a e
M
Figure 2: Training with Cross-Entropy loss (CE) [53] on
INRIA-Websearch [23] under 0.6 symmetric noise. (a) Ac-curacy vs. epoch on the training set of INRIA-Websearch for the image modality and the text modality, respec-tively. (b) Accuracy/MAP vs. epoch on the validation set of INRIA-Websearch. Accuracy is utilized to evaluate the classiﬁcation performance on the individual modality.
Mean Average Precision (MAP) is adopted to evaluate the retrieval performance across different modalities, i.e., im-age query text (Image → Text) and text query image (Text
→ Image). From the ﬁgure, we can see that noisy labels will make the multimodal learning overﬁt on the noisy training set while corrupting performance on the validation set. method consists of multiple modality-speciﬁc networks and two novel losses: Robust Clustering (RC) and Multimodal
Contrastive (MC) losses. To be speciﬁc, we present a novel common clustering loss to alleviate traditional classiﬁca-tion loss functions (e.g., cross-entropy) overﬁtting to noisy labels with a common classiﬁer. From the previous stud-ies [2, 10, 3], clean samples are easier for learning than noisy/incorrect samples, and leading to faster learning and lower loss for clean samples. This similar phenomenon can be observed in Figure 2, wherein the networks can faster learn clean samples and achieve a certain accuracy but decreasing the performance with the interference of noisy samples after further training. To distract the atten-tion of deep networks from noisy samples to clean ones, our
RC automatically weakens the inﬂuence of minor losses, which are more likely to be produced by noisy samples, for alleviating the interference of noisy labels, thus em-bracing more robustness.
In addition to mitigating noisy samples, RC also can narrow the heterogeneous gap by pro-jecting different modalities into a common clustering space.
Besides, inspired by recent unimodal contrastive learning works [54, 4], we propose a simple yet effective multimodal loss function, termed Multimodal Contrastive loss (MC), to simultaneously maximize the inter-instance and inter-pair variances in the intra- and inter-modalities. Different from previous contrastive learning methods, our MC maximizes the mutual information between intrinsically co-occurred modalities, which can further narrow the heterogeneous gap across distinct modalities while excavating the discrimina-tion from instance-level contrasting. Hence, our MC can further mitigate the interference of noisy samples by con-trasting their inter- and intra-modal counterparts with an un-supervised manner.
The main novelties and contributions of this work are summarized as follows:
• We propose a novel framework for cross-modal re-trieval with noisy labels. It can robustly learn the com-mon discriminative representations from noisy labels by using both supervised and unsupervised manners.
• We present a Robust Clustering loss (RC) that im-proves robustness and narrows the cross-modal gap on noisy samples simultaneously. 25404      
• A novel multimodal contrastive loss is proposed to maximize the inter-instance variances while minimiz-ing the intra-instance ones by considering the inter-and intra-modality similarities.
• Extensive experiments are conducted on four widely-used multimodal datasets to demonstrate the robust performance of the proposed methods for noisy labels. 2.