Abstract
Demographic bias is a signiﬁcant challenge in practical face recognition systems. Existing methods heavily rely on accurate demographic annotations. However, such annota-tions are usually unavailable in real scenarios. Moreover, these methods are typically designed for a speciﬁc demo-graphic group and are not general enough. In this paper, we propose a false positive rate penalty loss, which mit-igates face recognition bias by increasing the consistency of instance False Positive Rate (FPR). Speciﬁcally, we ﬁrst deﬁne the instance FPR as the ratio between the number of the non-target similarities above a uniﬁed threshold and the total number of the non-target similarities. The uniﬁed threshold is estimated for a given total FPR. Then, an ad-ditional penalty term, which is in proportion to the ratio of instance FPR overall FPR, is introduced into the denom-inator of the softmax-based loss. The larger the instance
FPR, the larger the penalty. By such unequal penalties, the instance FPRs are supposed to be consistent. Com-pared with the previous debiasing methods, our method re-quires no demographic annotations. Thus, it can mitigate the bias among demographic groups divided by various at-tributes, and these attributes are not needed to be previ-ously predeﬁned during training. Extensive experimental results on popular benchmarks demonstrate the superiority of our method over state-of-the-art competitors. Code and pre-trained models are available at https://github. com/xkx0430/FairnessFR. 1.

Introduction
With the increasing deployment of face recognition sys-tems, fairness in face recognition has received broad inter-est from research communities [19, 3, 5, 15, 14, 22]. This is partially due to the enormous impact brought in our daily life by face recognition systems. For example, when auto-*denotes the corresponding author.
Figure 1. FNR and FPR curves of the four races in RFW [19]. FNR and FPR are calculated with a ResNet34 [2], which is trained on the public balanced dataset BUPT-Balanced [20]. Lower is better.
Given a speciﬁc threshold, PPR varies signiﬁcantly among differ-ent races than FNR. (e.g., the standard deviation (std) of FPR at
Tu=0.31 is 7.6, while the std of FNR at Tu=0.31 is 0.97).) matic face recognition is applied to crime prevention, unfair prediction may lead to unfair treatment of individuals across different demographic groups.
Previous studies [14, 19, 3, 4, 16] mainly improve the fairness of face recognition in two aspects, i.e., datasets and algorithms. Since the widely-used public large-scale face datasets, such as CASIA-WebFace [21], VGGFace2 [1], and MS-Celeb-1M [6] are collected from the Internet, they inevitably encode gender, ethnic, and culture biases. Thus, the works in [14, 19, 18, 9] propose some new face recog-nition datasets that contain relatively balanced samples in ethnicity, age, and other facial attributes. However, it is quite challenging to construct a balanced dataset in vari-ous attributes. What is more, the racial bias of the models trained with such balanced datasets cannot be eliminated completely [18]. Therefore, a novel algorithm that can mit-igate the bias regardless of whether training datasets are balanced or not is imperative. Recently, several algorithms supervised by demographic attribute information are intro-duced to alleviate demographic bias. For example, Wang et al. [19] propose a deep information maximization adapta-tion network by transferring recognition knowledge from 1578
Caucasians to other races. With similar ideas, they pro-pose another method based on a widely-used margin-based loss function in face recognition, in which Q-learning learns the optimal margins of non-Caucasians with a manually-selected margin of Caucasians [18]. Different from the above methods take Caucasians as a reference, Gong et al. [4] present a debiasing adversarial network with four speciﬁc classiﬁers, in which one classiﬁer is designed for identity and the other three are designed for demographic attributes. They further introduce a group adaptive classiﬁer by using adaptive convolution kernels and attention mech-anisms based on their demographic attributes [5]. How-ever, all the above methods are explicitly designed to miti-gate the bias in demographic groups divided by race. Thus, these methods have poor transferability and generalization.
Moreover, they rely on accurate demographic attribute an-notations, which are usually not available.
To address the above problem, we ﬁrst evaluate the bias in face recognition from another perspective. Previous methods [4, 18, 19] mainly adopt the standard deviation of accuracy in each demographic group as the bias of a speciﬁc face recognition algorithm. In contrast, we analyze the bias in face recognition by two commonly-used evaluation met-rics, i.e., false positive rate (FPR), and false negative rate (FNR). As shown in Fig. 1, FPR varies signiﬁcantly among different races than FNR, which shares a similar observa-tion with [12]. Thus, it is essential to promote the consis-tency of FPR across each race group to mitigate the bias in face recognition. Based on this observation, we propose a false positive rate penalty loss, which mitigates face recog-nition bias by increasing the consistency of instance FPR.
By generalizing the consistency of FPRs across each de-mographic group to the consistency of FPRs across each instance, our method is generic to improve the fairness of face recognition across the demographic groups divided by various attributes, such as race, gender, and age. Speciﬁ-cally, we ﬁrst deﬁne the instance FPR as the ratio between the number of the non-target similarities above a uniﬁed threshold and the total number of the non-target similari-ties. Then, an additional penalty term in proportion to the ratio of instance FPR overall FPR is introduced into the de-nominator of the softmax-based loss. A larger ratio between each instance and the overall FPR yields a larger loss value.
By such unequal penalties, the instance FPRs are supposed to be much consistent. Compared with the previous debi-asing methods, our method ﬁrstly requires no demographic annotations of images; secondly can be easily embedded into the commonly used softmax-based loss function in face recognition; and ﬁnally can mitigate the bias across all de-mographic group divided by various kinds attributes, such as race, gender, and age.
To sum up, the contributions of this work are three-fold:
• To our best knowledge, it is the ﬁrst work that alle-viates the bias in face recognition by promoting the consistency of instance FPRs, which provides a new perspective to improve face recognition fairness.
• Our false positive rate penalty loss can improve the fairness across demographic groups divided by vari-ous kinds of attributes. Moreover, our method requires no demographic group annotation.
• We conduct extensive experiments on popular facial benchmarks, which demonstrate the superiority of our method over the SOTA competitors. 2.