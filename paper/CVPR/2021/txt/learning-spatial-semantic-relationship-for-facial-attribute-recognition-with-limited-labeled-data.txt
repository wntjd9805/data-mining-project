Abstract
Recent advances in deep learning have demonstrated ex-cellent results for Facial Attribute Recognition (FAR), typ-ically trained with large-scale labeled data. However, in many real-world FAR applications, only limited labeled da-ta are available, leading to remarkable deterioration in per-formance for most existing deep learning-based FAR meth-ods. To address this problem, here we propose a method termed Spatial-Semantic Patch Learning (SSPL). The train-ing of SSPL involves two stages. First, three auxiliary tasks, consisting of a Patch Rotation Task (PRT), a Patch Seg-mentation Task (PST), and a Patch Classiﬁcation Task (PC-T), are jointly developed to learn the spatial-semantic re-lationship from large-scale unlabeled facial data. We thus obtain a powerful pre-trained model.
In particular, PRT exploits the spatial information of facial images in a self-supervised learning manner. PST and PCT respectively capture the pixel-level and image-level semantic informa-tion of facial images based on a facial parsing model. Sec-ond, the spatial-semantic knowledge learned from auxiliary tasks is transferred to the FAR task. By doing so, it enables that only a limited number of labeled data are required to
ﬁne-tune the pre-trained model. We achieve superior per-formance compared with state-of-the-art methods, as sub-stantiated by extensive experiments and studies. 1.

Introduction
Facial attribute recognition (FAR), which aims to predict multiple attributes (such as gender, age, and race) of a given facial image, can greatly facilitate a variety of application-s, including face veriﬁcation and identiﬁcation [19, 29, 32], image generation [9, 36], and image retrieval [4, 33]. How-ever, FAR is challenging due to signiﬁcant facial appear-ance variations caused by pose, illumination, occlusion, etc.
State-of-the-art deep learning-based FAR methods usu-∗Corresponding author (email: yanyan@xmu.edu.cn).
Figure 1 – Illustration of our proposed SSPL method. First, three auxiliary tasks (namely, PRT, PST, and PCT) are jointly learned to model the spatial-semantic relationship of facial im-ages from large-scale unlabeled data, and a pre-trained model is obtained. Then, the pre-trained model is transferred to perform
FAR with limited labeled data. ally rely heavily on a large number of labeled training da-ta for achieving good classiﬁcation accuracy. Unfortunate-ly, in many real-world FAR applications, often only a s-mall number of training data are labeled since labeling a massive amount of multi-attribute images can be very time-consuming and costly. As a result, the performance of these deep learning-based FAR methods signiﬁcantly decreases in real-world applications. Here, we focus on the challeng-ing problem of FAR with limited labeled data.
To alleviate the challenge of learning with limited la-beled data, considerable efforts [6, 7, 8, 25, 34] have been spent on extracting high-level feature representations from unlabeled data in an unsupervised manner. Among these ef-forts, self-supervised learning has emerged as a prominent learning paradigm. The training of self-supervised learn-ing involves two tasks: a pretext task and a downstream task [16]. Apart from self-supervised learning methods, some semi-supervised learning methods [1, 13, 18, 23, 26] have also been proposed, where labeled and unlabeled data are simultaneously used for training. 11916
The tasks targeted by self-supervised learning and semi-supervised learning methods are usually image classiﬁ-cation [15, 30], object detection [11, 12], and semantic segmentation [5, 38]. Different from these tasks, FAR is a multi-attribute classiﬁcation task, where the spatial-semantic relationship of facial images is critical to classi-fy attributes. For example, to identify the “BigNose” and
“PointyNose” attributes, it is natural to locate the nose re-gion and determine whether the nose is big and pointy at a semantic level. Similarly, the “Smiling” and “MouthOpen” attributes are predicted by exploiting the semantic informa-tion in the mouth region. Therefore, for FAR, it is pivotal to learn ﬁne-grained feature representations, in particular capturing the spatial-semantic relationship, from unlabeled facial data.
In this work, we propose a novel Spatial-Semantic Patch
Learning method (SSPL) to address the problem of effec-tively learning the spatial-semantic relationship for achiev-ing state-of-the-art FAR with limited labeled data. To this end, as shown in Figure 1, the training of SSPL involves t-wo stages. First, three auxiliary tasks consisting of a Patch
Rotation Task (PRT), a Patch Segmentation Task (PST), and a Patch Classiﬁcation Task (PCT) are jointly proposed and trained to obtain a powerful pre-trained model. Second, the pre-trained model is transferred to perform FAR by ﬁne-tuning on limited labeled data.
Speciﬁcally, given several facial patches (one of which is rotated), PRT predicts the index of the rotated patch to ex-ploit the spatial information of facial images. Meanwhile,
PST performs semantic segmentation to assign a seman-tic label to each pixel in a randomly selected facial patch and PCT predicts facial component labels of this patch, such that PST and PCT can respectively encode the pixel-level and image-level semantic information of facial im-ages. These three tasks and their joint training effectively capture the spatial-semantic relationship between facial re-gions, which in turn leads to a signiﬁcant improvement of
FAR when only limited labeled data are available.
Our main contributions are summarized as follows.
• We propose the SSPL method to address the problem of FAR with limited labeled data. SSPL effectively ex-ploits both the spatial and semantic information from unlabeled facial data to obtain a powerful pre-trained model, ensuring that an attribute recognition model can be easily ﬁne-tuned to accurately predict facial at-tributes by using only limited labeled data.
• We elaborately design three auxiliary tasks (i.e., PRT,
PST, and PCT) targeted for FAR. These auxiliary tasks are jointly trained to make use of the intrinsic relation-ship between patch rotation prediction and patch seg-mentation/classiﬁcation. This enables the network to effectively extract semantic-aware ﬁne-grained feature representations.
• Our experiments convincingly show that the pro-posed method performs favorably against state-of-the-art methods in the case of limited labeled data, demon-strating the potentials of learning the spatial-semantic relationship of facial images for FAR. 2.