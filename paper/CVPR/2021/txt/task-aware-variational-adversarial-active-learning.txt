Abstract
Often, labeling large amount of data is challenging due to high labeling cost limiting the application domain of deep learning techniques. Active learning (AL) tackles this by querying the most informative samples to be annotated among unlabeled pool. Two promising directions for AL that have been recently explored are task-agnostic approach to select data points that are far from the current labeled pool and task-aware approach that relies on the perspective of task model. Unfortunately, the former does not exploit struc-tures from tasks and the latter does not seem to well-utilize overall data distribution. Here, we propose task-aware varia-tional adversarial AL (TA-VAAL) that modiﬁes task-agnostic
VAAL, that considered data distribution of both label and unlabeled pools, by relaxing task learning loss prediction to ranking loss prediction and by using ranking conditional generative adversarial network to embed normalized ranking loss information on VAAL. Our proposed TA-VAAL outper-forms state-of-the-arts on various benchmark datasets for classiﬁcations with balanced / imbalanced labels as well as semantic segmentation and its task-aware and task-agnostic
AL properties were conﬁrmed with our in-depth analyses. 1.

Introduction
Deep learning has achieved remarkable performance in various computer vision tasks such as classiﬁcation [18, 13], object detection [27, 26], and semantic segmentation [19, 4] due to massive datasets with annotations such as ImageNet for image classiﬁcation [7] and PASCAL VOC for classi-ﬁcation, detection, segmentation [9]. Obtaining good an-notations is challenging and has often been a large-scale project. Moreover, there are often cases where labeling mas-sive amount of data is even more challenging or infeasible due to high labeling cost such as labeling by experts [8] or long labeling time per large-scale sample such as videos [1] or pathology images [3]. Labeling cost seems to be a factor to limit the scope of applicability of deep learning to more
†Corresponding author. research areas and more institutes with less labeling budget.
Active learning (AL) is one of the approaches to over-coming limited labeling budget by selecting data to label for the best possible performance [30, 11]. AL has been widely investigated in relatively traditional machine learning settings [5, 34, 2, 21, 23, 30, 14, 20, 36, 32, 25] and recently in deep learning settings [11, 29, 38, 40, 35, 31, 16].
Existing AL approaches can be categorized into two groups: Task-agnostic (or distribution-based) and task-aware methods. Suppose that our goal is to learn a functional model f that maps from the input domain X to the corre-sponding output domain Y, each equipped with the corre-sponding probability distributions P (x) and P (y). Task-agnostic approaches select data instances to label by ex-ploiting the input distribution P (x). These are especially effective in identifying inﬂuential points, e.g. these lying in high-density regions such that once labeled, large numbers of neighboring samples can beneﬁt from propagating these la-bels [20, 39, 29, 31]. A major drawback of these approaches is that they do not take account how outputs y depend on inputs x: For example, for classiﬁcations, it would be more effective to label data instances that lie in the vicinity of decision boundaries than these lying in high-density regions where most data points belong to the same class.
Task-aware approaches explicitly address this limitation by modeling such dependence, e.g. via estimating the con-ditional distribution P (y|x). These are effective in identify-ing difﬁcult data points (e.g. these close to decision bound-aries) [36, 14, 11, 35, 40]. However, they do not directly consider how the labeled samples make inﬂuence on the en-tire dataset. Further, as P (y|x) is unknown a priori, the label selection process has to rely on the learner f as a surrogate to P (y|x) but such a learner might be inaccurate at the early stage of AL, thereby providing a poor estimate of P (y|x).
Recently, there was an attempt (SRAAL) to combine the task-aware and task-agnostic approaches with a uncertainty indicator and with a uniﬁed representation for both labeled and unlabeled data [42]. Even though SRAAL achieved state-of-the-art performance, it did not use the information about the loss that is directly related to the given task [40] 8166
Figure 1: Visual results of active learning methods (Learning loss [40], VAAL [31], our TA-VAAL) on imbalanced toy example at the 5th stage. Red and blue dots indicate samples assigned to class 0 and 1, respectively. Ten samples at that stage (denoted by black cross) were selected using each method. The oracle decision boundary of the model is shown as a black dash line.
Learning loss identiﬁed difﬁcult samples near the decision boundary and VAAL found inﬂuential samples over the entire set.
Our TA-VAAL selected samples that are both difﬁcult (near decision boundary) and inﬂuential (over the entire set). and its task learner seems to be limited only to VAE-type networks with a latent space for its uniﬁed representation.
Moreover, its implementation is not yet available online.
In this paper, we propose a novel alternative AL scheme that combines the beneﬁts of these two groups of approaches.
Speciﬁcally, our algorithm builds upon two recent state-of-the-art approaches: Variational adversarial active learning (VAAL) [31] models how adding labels to selected data points make inﬂuence on the entire set. As a model-agnostic approach, this method does not exploit the structure P (y|x) of the problem at hand. We address this by combining it with the recent learning loss approach [40]. This algorithm learns to estimate the errors of the predictions (loss) made by the learner and therefore helps identify difﬁcult data points.
Here is the summary of our contributions:
• Proposing to relax the goal of loss prediction module [40] from accurate loss prediction to loss ranking prediction, which is still directly connected to the task. This relaxation leads to altering the loss for learning prediction module to remove margins for ranking and to add ranking loss in [28].
• Proposing Task-Aware Variational Adversarial Active
Learning (TA-VAAL) to embed the normalized ranking loss information from any given task learner (with or without la-tent space) on the latent space of VAAL [31] via ranking con-ditional generative adversarial network (RankCGAN) [28] to reshape the latent space of it. This approach is signiﬁ-cantly more robust than the original learning loss approach, especially at the early stage. By combining these two algo-rithms with our embedding strategy, our method offers the capability of identifying difﬁcult and inﬂuential data points (see Figure 1; see Section 4 for details).
• Demonstrating the superior performance of our proposed
TA-VAAL over state-of-the-art works (Learning loss [40],
VAAL [31], Coreset [29], Monte-Carlo dropout [11]) by evaluating on various classiﬁcation benchmark datasets: CI-FAR10, CIFAR100 that have the same number of images per class (balanced) as well as Caltech101, modiﬁed CI-FAR10 that has different numbers of images for classes (imbalanced), and on Cityscapes semantic segmentation benchmark dataset and by in-depth empirical analyses to conﬁrm our proposed approach. Our codes are available at https://github.com/cubeyoung/TA-VAAL. 2.