Abstract 1.

Introduction
While single-image super-resolution (SISR) has at-tracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame super-resolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increas-ing popularity of burst photography, have made MFSR an important problem for real-world applications.
We propose a novel architecture for the burst super-resolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved
RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise opti-cal ﬂow. The information from all frames are then adap-tively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth.
We perform comprehensive experimental analysis, demon-strating the effectiveness of the proposed architecture.
Super-resolution (SR) is the task of generating a high-resolution (HR) image, given one or several low-resolution (LR) observations. It is a widely studied problem [6, 8, 20, 21, 23, 24, 26, 29, 39, 42, 45, 48, 51] with numerous prac-tical applications. In recent years, the SR community has mainly focused on the single image super-resolution (SISR) task, where an HR image is estimated from a single LR input. Due to the ill-posed nature of the SISR problem, these methods are limited to adding high frequency details through learned image priors.
The multi-frame super-resolution (MFSR), on the other hand, aims to reconstruct the original HR image using mul-tiple LR images. If the input images have sub-pixel shifts with respect to each other, due to e.g. camera motion, they provide different LR samplings of the underlying scene.
MFSR approaches can thus exploit this additional signal in-formation to generate a higher quality image, compared to the SISR approaches (see Fig. 1). The MFSR problem nat-urally arises in the increasingly popular mobile burst pho-tography, where the images have different sub-pixel shifts due to natural hand tremors [45]. This opens up the possi-bility of using MFSR to overcome the resolution constraint 19209
in mobile cameras imposed by the cost and size restrictions.
Despite the aforementioned advantages, MFSR has re-ceived little attention in recent years. This is in stark con-trast to SISR, where deep learning has led to signiﬁcant advancements in SR performance. Compared to the SISR case, the MFSR problem imposes signiﬁcant challenges when developing deep learning based solutions. Firstly, a
MFSR architecture must be able to align the noisy input frames with sub-pixel accuracy in order to enable fusion.
Secondly, it should be able to effectively fuse the informa-tion from the aligned frames, while being robust to align-ment errors. Furthermore, the lack of benchmark datasets for the general MFSR task has led to a limited interest in the MFSR problem. We address these issues by proposing a novel deep learning based approach for the MFSR prob-lem, along with a real-world dataset.
Our network directly operates on noisy RAW bursts cap-tured from a hand-held camera and generates a denoised, demosaicked, and super-resolved image as output. This is achieved by developing a novel attention-based fusion mod-ule which can adaptively merge an arbitrary number of in-put frames in order to produce a high quality output. Our approach is not limited to simple motions between the im-ages, such as translation or homography. Instead, we esti-mate dense pixel-wise optical ﬂow to align the deep feature encoding of each input frame. The aligned representations of each frame are then merged by computing element-wise fusion weights. This allows the network to adaptively select the reliable and informative content from each image, while discarding, e.g., misaligned regions.
The conventional approach in SISR is to train and eval-uate models on synthetically generated data. However, this has been shown to not generalize to real-world images due to inaccuracies in data generation model [3, 30, 31, 32]. Ac-curately modelling the image formation process for MFSR is further challenging due to the additional complexity in-troduced by camera motion. We therefore introduce the
BurstSR dataset: the ﬁrst real-world burst super-resolution dataset. Our dataset consists of 200 RAW bursts captured using a hand held mobile camera. Furthermore, we provide a high quality HR ground truth for each burst using a DSLR with zoom lens. We believe that our BurstSR dataset can serve as a valuable benchmark and source of training data to stimulate future research in MFSR.
Contributions: Our main contributions are summarized (i) We introduce the ﬁrst real world burst as follows. super-resolution dataset consisting of RAW bursts and cor-(ii) We propose a novel responding HR ground truths.
MFSR architecture which can perform joint denoising, de-mosaicking, and SR using bursts captured from a handheld camera. (iii) Our architecture employs an attention-based fusion method to adaptively merge the input images to gen-erate high quality HR output (iv) We further address mis-alignment issues encountered when training on real world data by introducing a loss function which can internally cor-rect these mis-alignments.
We perform comprehensive experiments on a synthetic dataset, as well as the BurstSR test set, in order to validate our contributions. Our approach demonstrates promising
SR performance on real world bursts, signiﬁcantly outper-forming alternative methods in a user study. We also pro-vide a detailed ablative study, analysing the impact of key components in the proposed MFSR architecture. 2.