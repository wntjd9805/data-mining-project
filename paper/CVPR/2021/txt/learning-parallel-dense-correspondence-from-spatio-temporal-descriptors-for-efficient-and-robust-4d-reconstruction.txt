Abstract
Time
This paper focuses on the task of 4D shape reconstruc-tion from a sequence of point clouds. Despite the recent success achieved by extending deep implicit representations into 4D space [29], it is still a great challenge in two re-spects, i.e. how to design a ﬂexible framework for learn-ing robust spatio-temporal shape representations from 4D point clouds, and develop an efﬁcient mechanism for cap-turing shape dynamics.
In this work, we present a novel pipeline to learn a temporal evolution of the 3D human shape through spatially continuous transformation func-tions among cross-frame occupancy ﬁelds. The key idea is to parallelly establish the dense correspondence between predicted occupancy ﬁelds at different time steps via explic-itly learning continuous displacement vector ﬁelds from ro-bust spatio-temporal shape representations. Extensive com-parisons against previous state-of-the-arts show the supe-rior accuracy of our approach for 4D human reconstruc-tion in the problems of 4D shape auto-encoding and com-pletion, and a much faster network inference with about 8 times speedup demonstrates the signiﬁcant efﬁciency of our approach. The trained models and implementation code are available at https://github.com/tangjiapeng/
LPDC-Net. 1.

Introduction
We are surrounded by spatio-temporally changing envi-ronments that consist of various dynamics, such as observer
∗Corresponding author (a) Input (b) OFlow (b) OFlow  IoU: 76.0%
IoU: 76.0%
Corr: 0.090 
Corr: 0.090     Time: 1.850s    Time: 1.850ss (c) Ours  IoU: 79.5%
Corr: 0.077    Time: 0.247s
Figure 1: Given a sequence of 3D point clouds sampled in space and time, our goal is to reconstruct time-varying sur-faces with dense correspondences. Compared to the state-of-the-art, i.e. OFlow [29], our approach can obtain more accurate geometries (higher IoU), better coherence (lower correspondence error) while supporting 8x faster inference. movements, object motions, and human articulations. Re-constructing the human bodies evolving over time is vital 6022
for various application scenarios such as robot perception, autonomous driving, and virtual/augmented reality.
Traditional works have achieved varying degrees of suc-cess in learning 4D reconstruction (i.e. 3D reconstruction along time) from a temporal sequence of point clouds, they are faced with various restrictions including the require-ment of an expensive template mesh [1, 15, 41, 16, 18] or the dependence on smooth and clean inputs in space and time [43]. To overcome these issues, OccFlow [29] proposes a learning-based 4D reconstruction framework that establishes dense correspondences between occupancy
ﬁelds by calculating the integral of a motion vector ﬁeld deﬁned in space and time to implicitly describe the trajec-tory of a 3D point. Although impressive results have been achieved, there are still several inherent limitations in this framework. Firstly, its spatial encoder does not take into account the aggregation of shape properties from multiple frames, which degrades the capability to recover accurate
In addition, its temporal encoder ig-surface geometries. nores the time information which is of great importance to capture the temporal dynamics. Secondly, the integral of estimated immediate results leads to accumulated predic-tion errors in the temporal continuity and the reconstructed geometries. Lastly, it demonstrates low computational ef-ﬁciency during training and inference because of the de-manding computations of solving complex neural ordinary differential equations [6] to sequentially calculate the tra-jectories of points over time.
To tackle the above-mentioned problems, we aim to de-sign a novel framework for 4D shape reconstruction from spacetime-sampled point clouds, to advance the 4D recon-struction from computational efﬁciency, accurate geome-try, and temporal continuity. Our key idea is a mecha-nism which parallelly establishes the dense correspondence among different time-step occupancy ﬁelds predicted from the learned robust spatio-temporal shape representations.
A high-level design of our proposed approach is a combi-nation of static implicit ﬁeld learning and dynamic cross-frame correspondence predicting. The former one focuses on occupancy ﬁeld predictions from a novel spatio-temporal encoder that can effectively aggregate the shape properties with the temporal evolution to improve the robustness of geometry reconstructions. The latter one is utilized to iden-tify the accurate correspondences within cross-frame occu-pancy ﬁelds, which are produced from representative em-beddings describing the spatio-temporal changes in an efﬁ-cient manner. The key to achieving this goal is a strategy of simultaneously learning occupancy ﬁeld transformation from a ﬁrst time step to others. It can help to remarkably re-duce the convergence time in the network training, because of the bypassing of the expensive computation caused by solving ordinary differential equations. Moreover, beneﬁt-ing from the advantages of parallel isosurface deformations for the different time steps, our method provides a signiﬁ-cant speed-up of the inference time. As shown in Fig. 1, we can achieve more robust surface reconstructions and more accurate correspondence prediction while allowing for con-siderably faster inference.
The main contribution can be summarized as follow:
• We propose a learning framework of modeling the temporal evolution of the occupancy ﬁeld for 4D shape reconstruction, which is capable of capturing accurate geometry recoveries and coherent shape dynamics.
• We develop a novel strategy of establishing cross-frame shape correspondences by paralleling modeling occupancy ﬁeld transformations from the ﬁrst frame to others, which signiﬁcantly improves the network com-putation efﬁciency, especially in the inference stage.
• We propose a novel 4D point cloud encoder design that performs efﬁcient spatio-temporal shape properties ag-gregation from 4D point cloud sequences, which im-proves the robustness of reconstructed geometries.
Extensive ablation studies are conducted to validate the effectiveness of our proposed module designs. Compar-isons against previous state-of-the-arts on the challenging
D-FAUST dataset demonstrate the superior accuracy and ef-ﬁciency of our approach in the problems of 4D shape auto-encoding and completion. 2.