Abstract
As the computing power of modern hardware is in-creasing strongly, pre-trained deep learning models (e.g.,
BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation abil-ity of transformer and its variant architectures.
In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing trans-former (IPT). To maximally excavate the capability of trans-former, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails.
In addition, the con-trastive learning is introduced for well adapting to differ-ent image processing tasks. The pre-trained model can therefore efﬁciently employed on desired task after ﬁne-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github. com/huawei-noah/Pretrained-IPT and https:
/ / gitee . com / mindspore / mindspore / tree / master/model_zoo/research/cv/IPT 1.

Introduction
Image processing is one component of the low-level part of a more global image analysis or computer vision system.
Results from the image processing can largely inﬂuence the subsequent high-level part to perform recognition and un-derstanding of the image data. Recently, deep learning has been widely applied to solve low-level vision tasks, such as image super-resolution, inpainting, deraining and coloriza-tion. As many image processing tasks are related, it is nat-∗Corresponding author
SISR x2
SISR x3
SISR x4 0.4dB↑ 0.4dB↑ 29.6 29.5 29.4 29.3 29.2 29.1 29 28.9 0.4dB↑ 27.3 27.2 27.1 27 26.9 26.8 26.7 26.6
HAN (ECCV 2020)
IPT
HAN (ECCV 2020)
IPT
HAN (ECCV 2020)
IPT
Denoising (30)
Denoising (50)
Deraining 2.0dB↑ 1.8dB↑ 31.5 31 30.5 30 29.5 29 28.5 28 1.6dB↑ 42 41.5 41 40.5 40 39.5 39 33.8 33.7 33.6 33.5 33.4 33.3 33.2 33.1 34 33.5 33 32.5 32 31.5 31 30.5
RDN (CVPR 2018)
IPT
RDN (CVPR 2018)
IPT
RCDNet (CVPR 2020)
IPT
Figure 1. Comparison on the performance of the proposed IPT and the state-of-the-art image processing models on different tasks. ural to expect a model pre-trained on one dataset can be helpful for another. But few studies have generalized pre-training across image processing tasks.
Pre-training has the potential to provide an attractive so-lution to image processing tasks by addressing the follow-ing two challenges: First, task-speciﬁc data can be limited.
This problem is exacerbated in image processing task that involves the paid-for data or data privacy, such as medical images [8] and satellite images [73]. Various inconsistent factors (e.g. camera parameter, illumination and weather) can further perturb the distribution of the captured data for training. Second, it is unknown which type of image pro-cessing job will be requested until the test image is pre-sented. We therefore have to prepare a series of image pro-cessing modules at hand. They have distinct aims, but some underlying operations could be shared.
It is now common to have pre-training in natural lan-guage processing and computer vision [12]. For example, the backbones of object detection models [86, 85] are of-ten pre-trained on ImageNet classiﬁcation [18]. A num-12299
ber of well-trained networks can now be easily obtained from the Internet, including AlexNet [41], VGGNet [56] and ResNet [33]. The seminal work Transformers [61] have been widely used in many natural language process-ing (NLP) tasks, such as translation [64] and question-answering [58]. The secret of its success is to pre-train transformer-based models on a large text corpus and ﬁne-tune them on the task-speciﬁc dataset. Variants of Trans-formers, like BERT [19] and GPT-3 [5], further enriched the training data and improved the pre-training skills. There have been interesting attempts on extending the success of
Transformers to the computer vision ﬁeld. For example,
Wang et al. [62] and Fu et al. [25] applied the self-attention based models to capture global information on images. Car-ion et al. [7] proposed DERT to use transformer architec-tures for an end-to-end object detection. Most recently,
Dosovitskiy et al. [22] introduced Vision Transformer (ViT) to treat input images as 16×16 words and attained excellent results on image recognition.
The aforementioned pre-training in computer vision and natural language mostly investigate a pretest classiﬁcation task, but both the input and the output in an image pro-cessing task are images. A straightforward application of these existing pre-training strategies might not be feasible.
Further, how to effectively address different target image processing tasks in the pre-training stage remains a hard challenge. It is also instructive to note that the pre-training of image processing models enjoys a convenience of self-generating training instances based on the original real im-ages. The synthetically manipulated images are taken for training, while the original image itself is the ground-truth to be reconstructed.
In this paper, we develop a pre-trained model for im-age processing using the transformer architecture, namely,
Image Processing Transformer (IPT). As the pre-trained model needs to be compatible with different image process-ing tasks, including super-resolution, denoising, and derain-ing, the entire network is composed of multiple pairs of head and tail corresponding to different tasks and a sin-gle shared body. Since the potential of transformer needs to be excavated using large-scale dataset, we should pre-pair a great number of images with considerable diversity for training the IPT model. To this end, we select the Im-ageNet benchmark which contains various high-resolution with 1,000 categories. For each image in the ImageNet, we generate multiple corrupted counterparts using several carefully designed operations to serve different tasks. For example, training samples for the super-resolution task are generated by downsampling original images. The entired dataset we used for training IPT contains about over 10 mil-lions of images. speciﬁc head, and the generated features are cropped into patches (i.e., “words”) and ﬂattened to sequences subse-quently. The transformer body is employed to process the
ﬂattened features in which position and task embedding are utilized for encoder and decoder, respectively. In addition, tails are forced to predict the original images with differ-ent output sizes according to the speciﬁc task. Moreover, a contrastive loss on the relationship between patches of different inputs is introduced for well adopting to differ-ent image processing tasks. The proposed image processing transformer is learned in an end-to-end manner. Experimen-tal results conducted on several benchmarks show that the pre-trained IPT model can surpass most of existing meth-ods on their own tasks by a signiﬁcant enhancement after
ﬁne-tuning. 2.