Abstract
High Dynamic Range (HDR) deghosting is an indispens-able tool in capturing wide dynamic range scenes without ghosting artifacts. Recently, convolutional neural networks (CNNs) have shown tremendous success in HDR deghost-ing. However, CNN-based HDR deghosting methods re-quire collecting large datasets with ground truth, which is a tedious and time-consuming process. This paper proposes a pioneering work by introducing zero and few-shot learn-ing strategies for data-efﬁcient HDR deghosting. Our ap-proach consists of two stages of training. In stage one, we train the model with few labeled (5 or less) dynamic sam-ples and a pool of unlabeled samples with a self-supervised loss. We use the trained model to predict HDRs for the un-labeled samples. To derive data for the next stage of train-ing, we propose a novel method for generating correspond-ing dynamic inputs from the predicted HDRs of unlabeled data. The generated artiﬁcial dynamic inputs and predicted
HDRs are used as paired labeled data. In stage two, we
ﬁnetune the model with the original few labeled data and artiﬁcially generated labeled data. Our few-shot approach outperforms many fully-supervised methods in two publicly available datasets, using as little as ﬁve labeled dynamic samples. 1.

Introduction
Unlike the human eye, a standard digital camera has lim-ited dynamic range that it can recognize in a scene. All objects beyond the recognizable dynamic range are thresh-olded to the minimum or maximum pixel intensity value, thus losing their details in the process. High Dynamic
Range imaging is an algorithmic solution to this problem.
It creates an image with a wider dynamic range than a stan-dard camera image, closer to what human eyes perceive.
The generated HDR image contains details in both bright
* equal contribution
Figure 1. Qualitative results by different methods for an example from Kalantari et al. dataset [19]. As shown in the zoomed in boxes, our proposed few-shot approach using only 5 labeled dy-namic sequences and pool of unlabeled sequences, generate better results without any artifacts, than existing methods trained with full dataset of 74 labeled dynamic sequences. and dark regions.
To generate an HDR image, multiple images with differ-ent exposure values (also known as exposure stack or LDR images) are captured and merged. The merging process is simple when the exposure stack’s input images are static without any camera or object motion. However, such an assumption is too good to be true in real-world scenarios.
Most often, the exposure stack is dynamic, consisting of camera and object motion. Fusing such dynamic exposure stack naively results in undesirable ghosting artifacts. The process of fusing dynamic exposure stack without ghosting artifacts is known as HDR deghosting.
A widely followed approach for HDR deghosting is to register the LDRs ﬁrst and identify moving regions. Once identiﬁed, either the motion affected regions are excluded 4875
in those images, or a chosen reference image is used [10, 13, 17, 22, 33, 39, 59]. Such methods result in only LDR content for moving regions. Another popular approach is to align images using estimated dense correspondence be-tween a reference image and input LDRs, and merging the aligned images. Dense registration techniques like optical
ﬂow methods introduce warping artifacts in heavily satu-rated and occluded regions [3, 11, 21, 48, 51, 60]. Patch-based optimization methods [16, 44] synthesize static se-quences from dynamic input sequences and merge them to generate the ﬁnal HDR image. These methods are compu-tationally expensive and hallucinate false details in heavily saturated regions (see Fig. 1).
Recently proposed deep learning-based methods offer a signiﬁcant advantage over traditional methods in terms of deghosting quality and computation time [19, 34, 35, 53, 56]. CNNs can learn complex fusion rules using abundant training data with ground truth HDRs. However, collecting a large amount of labeled training data for HDR deghosting is challenging, due to the reasons listed below.
Difﬁculty in capturing labeled data: Capturing a sin-gle labeled sample requires considerable effort. First, a dy-namic exposure stack with controlled object motion is cap-tured. Then, a static exposure stack of the same scene, with-out object motion, is captured with a tripod to generate the ground truth [19]. In this process, it must be ensured that there are no other unexpected or unwanted motions in the scene, such as tree, cloud, or vehicle motions. These con-straints are applicable only for very few scene types and human-controllable motion, thus limiting dataset diversity.
Post-capture manual examination: Another major dif-ﬁculty in collecting large-scale supervised HDR deghosting datasets is the post-capture manual examination. All sam-ples must be carefully examined for any unwanted motion in the static exposure stack. If any sample has such discrep-ancies present, then that sample cannot be used to gener-ate artifact-free ground truth and hence has to be discarded.
This often leads to almost a quarter of the captured samples getting discarded. Kalantari et al. [19] discarded 25 sam-ples from the captured 114, and Prabhakar et al. [35] had to discard 118 from the captured 700 samples, after manually scrutinizing every single one.
Furthermore, existing datasets are limited in the diver-sity of camera parameters used to capture them, such as
ISO and exposure levels. However, collecting new large datasets with ground truth for different settings is cumber-some and highly inconvenient. Due to the above reasons,
HDR deghosting CNNs are limited by the diversity, scale, and training dataset settings. The above limitations offer all the more reason to explore data-efﬁcient Deep HDR
Deghosting methods.
We address these limitations by proposing zero and few-shot learning strategies for HDR deghosting while using a pool of unlabeled dynamic exposure stacks. Many diverse unlabeled samples can be effortlessly captured without wor-rying about collecting ground truth for the same. It does not require a tripod since we do not have to capture a corre-sponding static exposure stack. Also, it does not require post-capture scrutiny, as it is not constrained and can thus include any diverse motion or scene.
Our approach consists of two stages of training. In the
ﬁrst stage, we train a model with a supervised loss for few labeled dynamic samples and use a self-supervised loss for the unlabeled samples. Then, we use the trained model to predict HDRs for the unlabeled samples and call them as predicted HDRs. Since the HDR images predicted by the model will inherently contain errors, they cannot be treated as proper ground truth for the unlabeled samples. There-fore, we generate artiﬁcial dynamic input images that cor-respond to the predicted HDR images and use them along with few labeled dynamic images to improve the model in second stage. In summary, the main contributions of our paper are as follows:
• To the best of our knowledge, this is the ﬁrst work to explore zero-shot and few-shot learning with unlabeled images for Deep HDR Deghosting.
• We propose a novel method to generate labeled dy-namic training data from unlabeled dynamic data.
• Our method trained with only 5 labeled dynamic sam-ples and unlabeled samples achieves comparable, if not better, results than existing methods trained on complete datasets in a supervised fashion, on two pub-licly available datasets.
• We perform comprehensive experiments and ablation studies to demonstrate the signiﬁcance of various com-ponents of our proposed approach. 2.