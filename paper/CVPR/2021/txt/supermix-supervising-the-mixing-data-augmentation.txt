Abstract
This paper presents a supervised mixing augmentation method termed SuperMix, which exploits the salient regions within input images to construct mixed training samples.
SuperMix is designed to obtain mixed images rich in visual features and complying with realistic image priors. To en-hance the efﬁciency of the algorithm, we develop a variant of the Newton iterative method, 65× faster than gradient descent on this problem. We validate the effectiveness of Su-perMix through extensive evaluations and ablation studies on two tasks of object classiﬁcation and knowledge distil-lation. On the classiﬁcation task, SuperMix provides com-parable performance to the advanced augmentation meth-ods, such as AutoAugment and RandAugment.
In par-ticular, combining SuperMix with RandAugment achieves 78.2% top-1 accuracy on ImageNet with ResNet50. On the distillation task, solely classifying images mixed using the teacher’s knowledge achieves comparable performance to the state-of-the-art distillation methods. Furthermore, on average, incorporating mixed images into the distillation objective improves the performance by 3.4% and 3.1% on
CIFAR-100 and ImageNet, respectively. The code is avail-able at https://github.com/alldbi/SuperMix. 1.

Introduction
Despite the revolutionary performance of deep neural networks (DNNs), they easily overﬁt when the training set is qualitatively or quantitatively deﬁcient [27, 33]. Quality of the data can be interpreted as how well the data is ex-pressive of the true distribution of inputs in the underlying task. This helps the model to learn discriminative patterns likely to occur at inference time. Quantity of the data, on the other hand, allows the model to observe discriminative pat-terns from different views and generalize the task-speciﬁc notions according to the major factors of variation in the in-put domain. Although analytical analysis of such important properties of the data has remained arduous [16], empirical evaluations on training deep models often highlight a com-mon observation: incorporating more data leads to a better
Figure 1: SuperMix combines salient regions in input im-ages to construct unseen data for training. generalization [25, 13]. Hence, data augmentation has be-come a fundamental component of the training paradigms, aiming to enlarge the training set by transforming images in the given dataset.
Conventional image data augmentation involves combi-nations of context-preserving transformations, such as hori-zontal ﬂip, crop, scale, color manipulation, and cut out [17, 12, 9]. Recently, notable efforts have been devoted to im-proving the augmentation, e.g., by automating the search for the optimal augmentation policies [4, 20, 5]. The majority of these methods have focused on transforming single im-ages, while ignoring the potentially very useful combination of multiple images for augmentation. To address this short-coming, several studies have considered combining multi-ple images to construct novel images [18, 22, 34, 32, 29].
However, these methods either mix images blindly and dis-regard the salient regions [34, 11, 32, 29] or do not scale to large-scale problems [18]. Furthermore, the current mixing functions are not expressive enough and often suppresses visual patterns by averaging or covering features in one im-age with the trivial features in another image. The corre-sponding pseudo labels are also not accurate and constrain the training performance [11].
This paper presents a mixing augmentation approach termed SuperMix, which exploits the salient regions of in-put images to construct more advantageous mixed data. The supervision for this purpose can be obtained from the target model itself, i.e., self-training [26, 30, 23, 19, 2, 31], or 13794
a more sophisticated model aiming to guide a student net-work via knowledge transfer [1, 14]. Figure 1 provides a visual comparison of mixed images produced by different methods. In a nutshell, the contributions of the paper are as follows:
• We formalize the problem of supervised mixing aug-mentation using a set of mixing masks associating the pixel value at each spatial location in the mixed image to the spatial locations in the input images.
• The optimization problem is carefully constrained to assure that the solutions are rich in salient features and comply with the realistic image priors.
• We develop a modiﬁed Newton iterative algorithm for
SuperMix, suitable for large-scale applications. This approach provides 65× speed-up as compared to SGD on ImageNet.
• We demonstrate that mixed images intrinsically induce smooth predictions, and thus, help reveal knowledge of the teacher model in knowledge distillation. 2.