Abstract
We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects—in real time.
Unlike traditional MPI that uses a set of simple RGBα planes, our technique models view-dependent effects by in-stead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon ﬁne detail and produces state-of-the-art re-sults. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset de-signed to test the limit of view-dependent modeling with signiﬁcantly more challenging effects such as the rainbow reﬂections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000× faster rendering time than the state of the art.
For real-time demos, visit https://nex-mpi.github.io/ 1.

Introduction
Novel view synthesis is an exciting and long-standing problem that draws much attention from both the computer vision and graphics communities. The problem comprises
*Authors contributed equally to this work. two intriguing challenges of how to construct a visual scene representation from only a sparse set of images and how to render such a representation from unseen perspectives. A wide range of applications are possible from this area of research ranging from virtually visiting tourist attractions to viewing any online product all around in 3D; however, such experiences would only become most compelling and practical when the representation allows photo-realistic and real-time synthesis.
One candidate that can serve this purpose is multiplane image (MPI) [53] which approximates the scene’s light ﬁeld with a set of parallel semi-transparent planes placed along a reference viewing frustum. This representation is shown to be more effective than traditional 3D mesh reconstruc-tion in reproducing complex scenes with challenging occlu-sions, thin structures, or planar reﬂections. However, the standard RGBα representation of MPI is limited to diffuse surfaces whose appearance stays constant regardless of the viewing angle. This greatly limits the types of objects and scenes that MPI can capture. Recent research on implicit scene representation has made signiﬁcant progress in the past months [21, 32, 18, 50, 42] and can be applied to view synthesis problem. Unfortunately, its expensive network in-ference still prohibits real-time rendering, and reproducing complex surface reﬂectance with high ﬁdelity still remains a challenge. Our method breaks these limits on both fronts. 8534
We introduce NeX, a new scene representation based on
MPI that models view-dependent effects by performing ba-sis expansion on the pixel representation in our MPI. In particular, rather than storing static color values as in tra-ditional MPI, we represent each color as a function of the viewing angle and approximate this function using a lin-ear combination of spherical basis functions learned from a neural network. Furthermore, we propose a hybrid pa-rameter modeling strategy that models high-frequency de-tail in an explicit structure within an implicit MPI modeling framework. This strategy helps improve ﬁne detail that is difﬁcult to model by a neural network and produces sharper results in fewer training iterations.
We evaluate our algorithm on benchmark forward-facing datasets and compare against state-of-the-art approaches in-cluding NeRF [22] and DeepView [6]. These datasets, how-ever, contain mostly diffuse scenes and fairly simple view-dependent effects and cannot be used to judge the new limit of our algorithm. Thus, we collect a new dataset, Shiny, with signiﬁcantly more challenging view-dependent effects such as the rainbow reﬂections on a CD, refraction through non-planar glassware or a magnifying glass. Our method achieves the best overall scores across all major metrics on these datasets. We provide quantitative and qualitative re-sults and ablation studies to justify our main technical con-tributions. Compared to the recent state of the art, NeRF
[22], our method captures more accurate view-dependent effects and produces sharper results—all in real time. 2.