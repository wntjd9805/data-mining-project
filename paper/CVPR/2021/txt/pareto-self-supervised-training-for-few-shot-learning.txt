Abstract networks fail to address such a problem.
While few-shot learning (FSL) aims for rapid generaliza-tion to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly com-puted from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has re-cently drawn much attention to deal with few labeled data.
Previous works beneﬁt from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challeng-ing to select a proper weight to balance tasks and reduce task conﬂict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identiﬁed. Then, an effective preferred
Pareto exploration is proposed to ﬁnd a set of optimal solu-tions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance. 1.

Introduction
Although deep learning has achieved great success in a variety of ﬁelds, limitations still exist in many practical applications where labeled samples are intrinsically rare or expensive. Different from humans who can easily learn to accomplish new tasks with a few examples, it is difﬁcult for machines to rapidly generalize to new concepts with very little supervision, which draws considerable attention to the challenging few-shot learning (FSL) setting. As train-ing large models with few labeled samples leads to over-ﬁtting or even non-convergence, conventional deep neural
∗Corresponding author.
Recently, self-supervised learning (SSL) attracts many researchers for its soaring performance without involving manual labels. By deﬁning pretext tasks to exploit the struc-tural information of data itself, supervisory signals can be easily developed to learn useful general-purpose represen-tations [2, 20, 31]. As self-supervised learning can improve the generalization of the network under the limitation of la-beled data, some recent few-shot auxiliary learning (FSAL) works [16, 39] take few-shot learning as learning main task with self-supervised auxiliary tasks. To encourage that the few-shot task beneﬁts from auxiliary tasks, some parame-ters are shared across tasks to inductive knowledge trans-fer. However, as the objectives of distinct tasks are differ-ent and the relationship between objectives is complicated and unknown, optimizing each task not only promotes each other but also naturally conﬂicts. A typical solution to sup-press such conﬂict is to optimize the shared parameters by minimizing a weighted sum of the empirical risk for each task, where each weight of the empirical risk can be viewed
In previous works [16, 39], these trade-as the trade-off. offs are usually set by experience in practical situations.
It is difﬁcult to ﬁnd optimal trade-offs. Moreover, these works [16, 39] attempt to ﬁnd one single solution for all objectives, which is likely to sacriﬁce the performance of the main task and be inconsistent with the goal of few-shot auxiliary learning.
According to the above discussion, few-shot auxiliary learning with conﬂicting objectives requires better model-ing of the trade-off between tasks, which is beyond what a linear combination achieves. To overcome the issue, we propose a novel approach named Pareto self-supervised training (PSST) for few-shot learning. PSST explicitly casts few-shot auxiliary learning as a multi-objective op-timization problem, with the overall objective of ﬁnding a Pareto optimal solution of network parameters [26, 27].
However, different from previous works that explore in the global space [27], PSST uses an effective preferred Pareto exploration for FSL. Speciﬁcally, PSST decomposes the 13663
r o r r
E 2 k s a
T
Preferred 
Pareto 
Exploration (a)
Task 1 Error (b) learn a meta-learner to adjust the optimization algorithm, usually by providing better initialization or search steps for parameters. Model-based [1, 17, 19, 29] approaches de-pend on well-designed models, whose parameters are ob-tained with its internal architecture or a meta-learner for fast learning. Metric-based approaches [30, 38, 40] learn a gen-eralizable embedding model to transform all samples into a common metric space, where speciﬁc distance measures can be employed with the nearest neighbor classiﬁers.
Figure 1: Illustrative examples of Pareto exploration in (a) previous works and (b) our PSST. 2.2. Self Supervised Learning few-shot auxiliary problem into several constrained multi-objective subproblems with different trade-off preferences, and then identiﬁes the preference region where the main task achieves the best performance. As illustrated in Fig. 1, the desired space of exploration is thus restricted by only exploring in the preference region where the given points achieve better performance in task 1 rather than task 2. Ex-periments demonstrate that this improvement can suppress the accumulation of residual error, which contributes to ef-ﬁciently ﬁnding a more accurate Pareto solution. To sum-marize, our main contributions are as follows:
• We point out that existing few-shot auxiliary learning methods face with a linear combination of conﬂicting objectives, and propose a multi-objective optimization solution to address the issue.
• We propose a novel Pareto self-supervised training (PSST) approach for few-shot auxiliary learning. To achieve better performance for the main task and meanwhile improve the efﬁciency and accuracy of the exploration, PSST pioneers a preferred Pareto explo-ration that explores in the identiﬁed preference region.
• We conduct extensive experiments to demonstrate that our PSST can better model the trade-off between tasks, which leads to state-of-the-art performance on several benchmark datasets. 2.