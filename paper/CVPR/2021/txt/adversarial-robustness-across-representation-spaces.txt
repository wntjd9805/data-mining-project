Abstract
Adversarial robustness corresponds to the susceptibil-ity of deep neural networks to imperceptible perturbations made at test time. In the context of image tasks, many algo-rithms have been proposed to make neural networks robust to adversarial perturbations made to the input pixels. These perturbations are typically measured in an ℓp norm. How-ever, robustness often holds only for the speciﬁc attack used for training. In this work we extend the above setting to con-sider the problem of training of deep neural networks that can be made simultaneously robust to perturbations applied in multiple natural representations spaces. For the case of image data, examples include the standard pixel represen-tation as well as the representation in the discrete cosine transform (DCT) basis. We design a theoretically sound algorithm with formal guarantees for the above problem.
Furthermore, our guarantees also hold when the goal is to require robustness with respect to multiple ℓp norm based attacks. We then derive an efﬁcient practical implementa-tion and demonstrate the effectiveness of our approach on standard datasets for image classiﬁcation.1 1.

Introduction
In recent years deep learning has enjoyed tremendous success in solving a variety of machine learning tasks, even achieving or surpassing human level performance in certain cases [14, 15]. At the same time important vulnerabilities in these systems have also been discovered. One such example is their susceptibility to imperceptible perturbations made to the input at test time [24]. This has led to the new paradigm of adversarial machine learning, i.e., making deep neural networks robust to test time perturbations. There has been a ﬂurry of recent works in this area with several proposed defenses [18, 29, 6, 17] and methods to attack and evalu-∗Equal contribution and corresponding authors. 1Code available at https : / / github . com / tensorflow / neural- structured- learning/tree/master/research/ multi_representation_adversary. ate these defenses [5, 3, 26]. When studying the design of networks robust to adversarial attacks several aspects need to be considered such as a) what perturbations can the ad-versary apply to the input, and b) what information does the adversary have about the neural network? One widely-studied setting in the current literature is white box attacks under ℓp norm perturbations [10, 18]. Here the adversary has complete knowledge of the neural network and its pa-rameters, and given an input x it can perturb it to x′ such that kx − x′kp ≤ ǫ for some p ≥ 1 speciﬁed apriori. In the context of image data this corresponds to applying perturba-tions to the input pixels. Current approaches for defending against such attacks are based on studying variants of the following robust objective:
E min
θ (x,y)∼D(cid:2) max x′:kx−x′kp≤ǫ
L(fθ(x′), y)(cid:3). (1)
Here (x, y) is an example and label pair drawn from the data distribution, f is a neural network parameterized by weights θ and L is a standard loss function such as the cross entropy loss. As an example the popular projected gradient descent (PGD) method [18] proposes to optimize the above objective by alternately maximizing the inner objective via gradient ascent and then performing the outer minimization via gradient descent. The recent work of [22] combines the above objective with Gaussian smoothing to achieve cer-tiﬁed robustness guarantees, and another popular method namely the TRADES algorithm [29] adds a regularization term requiring the predictions of the network at x and x′ to be close to each other.
In this work we aim to address two main limitations of current approaches to adversarial machine learning. The
ﬁrst concerns the choice of the representation in which the adversary applies the perturbations. Using images as an ex-ample, current approaches model the adversary as making small magnitude changes in the pixel representation of the image. However, given that the adversary has full access to the input x, apriori there is no reason to restrict the per-turbations to only the pixel representations. Real data such as images have many other natural representations, such as the discrete cosine transform (DCT) basis for images. One 7608
could envision an adversary making changes to the input image in the DCT basis that are still imperceptible but don’t satisfy the small ℓp norm property in the pixel basis. Empir-ical attacks based on this have been shown to be successful in recent works [4]. Hence it is important to consider adver-sarial robustness in other representations for a model to be truly robust. Secondly, current approaches ﬁx a represen-tation and the perturbation model, and design an algorithm to achieve robustness for that speciﬁc setting.
In general such networks do not turn out to be robust to other types of attacks. For example a network trained to be robust to ℓ∞ norm perturbations in the pixel representation may not be robust to ℓ1 norm perturbations.
Ideally, one would like to train networks that can be si-multaneously robust to multiple attack models in multiple representation spaces. At the same time it is desirable to have a scalable solution with training cost not that much more than standard adversarial training in a ﬁxed attack model. This is precisely the problem that we solve in this work. Our main contributions are listed below.
• We propose and motivate the problem of studying ro-bustness to adversarial perturbations in multiple repre-sentation spaces and under multiple attack models.
• We propose a min-max formulation of the above sce-nario and use ideas from the theory of online learn-ing, in particular the multiplicative weights update method [13] to design an algorithm for our formula-tion and provide theoretical guarantees to justify our approach.
• We extend our theoretically principled algorithm to de-sign a practical implementation that can scale to mul-tiple representation spaces and multiple attack models with training cost not signiﬁcantly more than that of standard adversarial training for a ﬁxed attack model and representation space. We demonstrate the effec-tiveness of our algorithm for image classiﬁcation tasks on the MNIST [16] and the CIFAR-10 [14] datasets. 2.