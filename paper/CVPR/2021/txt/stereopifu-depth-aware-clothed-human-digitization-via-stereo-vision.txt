Abstract
In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit func-tion representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectiﬁed images.
First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware recon-struction. Moreover, the novel relative z-offset is employed to associate predicted high-ﬁdelity human depth and occu-pancy inference, which helps restore ﬁne-level surface de-tails. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Con-sequently, our StereoPIFu can naturally infer the human body’s spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human perfor-mance. Compared with previous works, our StereoPIFu sig-niﬁcantly improves the robustness, completeness, and accu-racy of the clothed human reconstruction, which is demon-strated by extensive experimental results. 1.

Introduction
Human digitization is the key to many applications like AR/VR, virtual try-on, holographic communication,
*Corresponding author
ﬁlm/game production etc. While high-ﬁdelity and geo-metric detail preserved 3D human digitalization can be achieved with high-end acquisition equipment and well-designed capture environment [17, 22], it is not suitable for general consumers. Recently, with the popularization of consumer-level acquisition devices, human digitization with simple inputs becomes a hot research topic in the ﬁeld of 3D computer vision and computer graphics.
Many methods have been proposed for clothed hu-man reconstruction from simple inputs (e.g., single im-age). Among them, some methods reconstruct the 3D human body with the help of parametric models [13, 29, 33, 34, 36, 45, 27]. However, parametric models are mainly used for reconstructing naked body and can not deal with topology changes. To solve these problems, im-plicit function based representations have been recently in-troduced [25, 43, 38, 46, 47, 16, 10, 21, 23]. Represen-tatively, Saito et al. [46] proposed Pixel-Aligned Implicit
Function (PIFu), which performs implicit function predic-tion based on the z-value of a 3D query point and its pro-jected 2D image feature. PIFu is memory-efﬁcient and can generate a plausible surface with a single image. Later, PI-FuHD [47] further improves the results of PIFu in the aspect of ﬁne-level geometric details recovery with the aid of pre-dicted normal maps and higher resolution. However, like other single image-based approaches, PIFu can not predict the precise spatial location and suffers from depth ambi-guity, which results in inconsistent results noticeable from different perspectives. Besides, PIFu related methods are 535
prone to generate wrong structures like broken limbs.
A natural way to resolve the depth ambiguity is to take multi-view images as input. However, existing approaches for 3D human digitization rarely fully utilize the rich ge-ometric relationships endowed by stereo vision. For ex-ample, Gilbert et al. [20] only exploits visual hull ex-tracted from multi-view silhouettes, and PIFu-like meth-ods [25, 46] aggregate the projected pixel features from multi-view images with a pooling layer for each query point. As shown in Fig. 1, this simple operation fails to perceive the depth information. On the other hand, some deep learning-based depth estimation methods explore ge-ometric correlations between multi-view images and obtain satisfying results [30, 15, 31, 14, 58, 53]. The key to su-perior performance is the constructed cost volume feature, which encodes the correlation between a pixel and its can-didate pixels in another view. Generally, these methods pre-dict disparity, namely the horizontal displacement between a pair of corresponding pixels from rectiﬁed stereo pair im-ages. The pixel’s predicted disparity can be obtained by calculating the weighted average of these candidate dispar-ities, and the predicted depth can be recovered via triangu-lation [31, 15, 53].
To overcome the limitations of classic PIFu-like meth-ods mentioned in the above, we propose StereoPIFu (Stereo
Vision-Based Pixel-Aligned Implicit Function) for depth-aware clothed humans digitization. Compared with PIFu,
StereoPIFu has two novel designs to improve its represen-tation ability. First, we introduce additional novel voxel-aligned features as input to the implicit function. Dif-ferent from the voxel-aligned features of previous meth-ods [23, 16], our voxel-aligned features are extracted from volume data widely used in stereo vision-based depth es-timation networks. Speciﬁcally, we construct two volume data from the feature maps of the image pair. The volumes deﬁne a spatial grid and for a 3D query point, we can do trilinear interpolation on its neighboring voxels in the vol-umes to generate voxel-aligned features. The features con-tain rich geometric information, indicating the correlation between the query point and the underlying surface. With voxel-aligned features as input, StereoPIFu can predict hu-mans’ spatial location in camera space without the need to normalize human shape into a canonical space like PIFu.
Second, we introduce human shape priors to guide the rep-resentation. Speciﬁcally, a high-quality human depth map is obtained based on the above-mentioned volume data. More-over, the relative z-offset between the query point and its projected pixel’s predicted depth is added as another input of the implicit function. Compared with using the absolute z-value, relative z-offset can generate more realistic details.
More importantly, the predicted depth map provides com-plete surface constraints for the human body and effectively eliminates broken limbs in reconstructed results. With the above novel designs, StereoPIFu can recover high-ﬁdelity geometric details and accurate geometric shape of human body. In summary, the paper includes the following contri-butions:
• We propose StereoPIFu, a novel implicit representa-tion integrating stereo vision to PIFu representation, which makes full use of binocular images and enables high-quality depth-aware reconstruction of the clothed human body.
• We utilize the novel well-designed voxel-aligned fea-tures and predicted depth map to help occupancy in-ference. The geometric correlation contained in the voxel-aligned features and depth priors signiﬁcantly improve the robustness, completeness, and accuracy of clothed human reconstruction. 2.