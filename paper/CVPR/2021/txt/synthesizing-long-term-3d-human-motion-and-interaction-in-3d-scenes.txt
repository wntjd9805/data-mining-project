Abstract
Synthesizing 3D human motion plays an important role in many graphics applications as well as understanding hu-man activity. While many efforts have been made on gener-ating realistic and natural human motion, most approaches neglect the importance of modeling human-scene interac-tions and affordance. On the other hand, affordance rea-soning (e.g., standing on the ﬂoor or sitting on the chair) has mainly been studied with static human pose and ges-tures, and it has rarely been addressed with human motion.
In this paper, we propose to bridge human motion synthesis and scene affordance reasoning. We present a hierarchi-cal generative framework to synthesize long-term 3D hu-man motion conditioning on the 3D scene structure. Build-ing on this framework, we further enforce multiple geom-etry constraints between the human mesh and scene point clouds via optimization to improve realistic synthesis. Our experiments show signiﬁcant improvements over previous approaches on generating natural and physically plausible human motion in a scene.1 1.

Introduction
Capturing and synthesizing realistic human motion in 3D scenes has played an essential role in various applications in 1Project page: https://jiashunwang.github.io/Long-term-Motion-in-3D-Scenes virtual reality, video game animations and human-robot in-teractions. As shown in Fig. 1, given the 3D scenes, our goal is to generate long-term human motion and interaction in the scene, such as walking around the room avoiding col-lision with the furniture (left), as well as walking through the hallway, turning around and then sitting down (right).
To achieve this, there are two main challenges on: (i) gen-erating realistic motion in long-term; (ii) modeling human-scene interaction and affordance.
Recent works have made substantial efforts on human motion synthesis, which generates visually appealing and natural pose sequences using optimization-based statistical models [68, 5], or deep neural networks [25, 24, 69]. How-ever, while focusing on realistic motion, these works rarely address the interactions between the human and the scene.
On the other hand, a line of researches on 3D scene affor-dance [65, 36, 71] has studied the “opportunities for inter-actions” [14] in the scene. For example, Wang et al. [65] propose to learn to predict human skeletons from an empty scene by training with a large-scale sitcom dataset. While focusing on the scene context, these approaches are only able to generate a single static human pose.
In this paper, we intend to bridge human motion syn-thesis and affordance learning. We consider a novel prob-lem setting: Given the start and the end positions far away in a 3D scene, synthesize the human motion moving in be-tween. To generate long-term motion in the scene, instead 9401
of synthesizing a long route of poses at one time, we in-troduce a 2-level hierarchical framework: (i) we ﬁrst set several sub-goal positions between the start and the end lo-cations. We predict the human pose for each sub-goal, start and end positions, conditioning on the 3D scene context. (ii) we synthesize the short-term human motion between every two sub-goals, using the predicted poses on the sub-goals as well as the 3D scene as inputs. The short-term motion will be then connected together for the ﬁnal long-term motion synthesis.
We model the interaction between human and the scene in both stages of our framework. Instead of using human skeletons, we emphasize that we adopt the differentiable
SMPL-X [47] model for representing both the shape and the pose of the human, which allows more ﬂexible geom-etry constraints and more realistic modeling of contacts.
Speciﬁcally, in the ﬁrst stage, given a single sub-goal in a 3D scene, we utilize a Conditional Variational Autoen-coder (CVAE) [56] to generate the SMPL-X parameters. In the second stage for short-term motion generation, we use a bi-directional LSTM [22] which takes the start-end hu-man SMPL-X representations and 3D scene representation as inputs and generates a sequence of human bodies repre-sented by SMPL-X. Besides training the deep models in a data-driven manner for motion synthesis, we also perform explicit geometry reasoning between the human mesh and 3D scene point clouds by optimization. Our optimization approach considers both the naturalness of motion and the physical collisions with the environment. By unifying both learning-based and optimization-based techniques, we are able to synthesize realistic human motion in long-term.
We perform our experiments on both the PROX [18] and the MP3D [7] 3D environments. By considering 3D scene affordance and structural constraints in motion synthesis, we qualitatively show realistic and physically plausible hu-man motion generation results. We also show large ad-vantages quantitatively against state-of-the-art motion and human pose generation approaches, using multiple metrics and human evaluation.
Our contributions in this paper include: (i) A hierarchical learning framework for motion synthesis considering both the realism of the motion and the affordance of the scene; (ii) An optimization process to explicitly improve the syn-thesized human poses; (iii) state-of-the-art motion synthesis results on various 3D environments. 2.