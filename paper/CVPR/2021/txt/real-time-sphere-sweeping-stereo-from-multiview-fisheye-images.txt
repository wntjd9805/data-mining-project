Abstract
A set of cameras with ﬁsheye lenses have been used to capture a wide ﬁeld of view. The traditional scan-line stereo algorithms based on epipolar geometry are directly inap-plicable to this non-pinhole camera setup due to optical characteristics of ﬁsheye lenses; hence, existing complete 360◦ RGB-D imaging systems have rarely achieved real-time performance yet. In this paper, we introduce an efﬁ-cient sphere-sweeping stereo that can run directly on multi-view ﬁsheye images without requiring additional spherical rectiﬁcation. Our main contributions are: First, we intro-duce an adaptive spherical matching method that accounts for each input ﬁsheye camera’s resolving power concerning spherical distortion. Second, we propose a fast inter-scale bilateral cost volume ﬁltering method that reﬁnes distance in noisy and textureless regions with optimal complexity of
O(n). It enables real-time dense distance estimation while preserving edges. Lastly, the ﬁsheye color and distance im-ages are seamlessly combined into a complete 360◦ RGB-D image via fast inpainting of the dense distance map. We demonstrate an embedded 360◦ RGB-D imaging prototype composed of a mobile GPU and four ﬁsheye cameras. Our prototype is capable of capturing complete 360◦ RGB-D videos with a resolution of two megapixels at 29 fps. Re-sults demonstrate that our real-time method outperforms traditional omnidirectional stereo and learning-based om-nidirectional stereo in terms of accuracy and performance. 1.

Introduction
Efﬁcient and accurate understanding of the appearance and structure of 3D scenes is a vital capability of computer vision used in many applications, such as autonomous ve-hicle [38], robotics [60], augmented/mixed reality [51, 5], etc. Conventional stereo cameras with ordinary lenses pro-vide a narrow ﬁeld of view, insufﬁcient to capture scenes in all directions. In order to capture scenes in all directions, we can build a multi-camera setup like a light-ﬁeld camera array [6, 44], but it signiﬁcantly increases manufacturing cost, in addition to computational cost for processing multi-ple input, to obtain omnidirectional panorama and distance. (b) Input ﬁsheye images (c) Our panorama result 29 fps (a) Our prototype (d) Our distance result
Figure 1: (a) Our prototype built on an embedded system. (b) Four input ﬁsheye images. (c) & (d) Our results of om-nidirectional panorama and dense distance map (shown as the inverse of distance). It took just 34 ms per frame on this device. Refer to the supplemental video for real-time demo.
It is a natural choice to use a smaller number of ﬁsheye lenses to reduce the number of cameras while covering all directions. The omnidirectional camera conﬁguration with multiple ﬁsheye lenses suffers from an inevitable tradeoff between performance and accuracy when computing full 360◦ panorama and distance due to the optical character-istics of ﬁsheye lenses presented subsequently.
First, the conventional pinhole camera model is invalid for ﬁeld of views of 180◦ or more even when the calibra-tion model can accommodate a wider FoV [25, 12, 53].
Accordingly, unlike ordinary stereo, we cannot ﬁnd stereo correspondence rapidly by sweeping plane candidates [24] in wide angle ﬁsheye images. Second, epipolar lines on
ﬁsheye images are curved [39, 50], requiring warp-aware correspondence search with spatial variation, signiﬁcantly increasing computational costs. An equirectangular or a latitude-longitude projection can be employed to obtain straight epipolar lines [52, 34]. However, it introduces se-11423
vere image distortion, and a given disparity does not corre-spond to the same distance depending on its position in the image. Simply estimating the disparity in the equirectangu-lar domain before converting to distance [32, 35] breaks the local disparity consistency assumption of cost aggregation.
Lastly, we cannot merge multiview ﬁsheye images as a 360◦ panorama image accurately without having a 360◦ dense distance map, and a clear 360◦ dense distance map cannot be ﬁltered and obtained without a 360◦ panorama image. It is a chicken-and-egg problem when combining multiview
ﬁsheye images to a 360◦ RGB-D image with high accuracy.
In this work, we propose real-time sphere-sweeping stereo that can run directly on multiview ﬁsheye images, without requiring additional spherical rectiﬁcation using equirectangular or latitude-longitude projection by tackling three key points. First, we propose an adaptive spherical matching method that allows us to evaluate stereo matching directly on the ﬁsheye image domain with consideration of the regional discrimination power of distance in each ﬁsh-eye image. Second, we introduce fast inter-scale cost vol-ume ﬁltering of optimal complexity O(n) that allows for a stable sphere sweeping volume in noisy and textureless
It enables 360◦ dense distance estimation in all regions. directions in real time while preserving edges. Lastly, col-ors at different distance maps are combined into a complete 360◦ panorama and distance map seamlessly through fast inpainting using the dense distance map.
We implemented our algorithm on a prototype made of an embedded computer with a mobile GPU and four ﬁsheye cameras (Figure 1). Our prototype captures complete 360◦
RGB-D video that includes color and distance at every pixel with a resolution of two megapixels at 29 fps. Results vali-date that our real-time algorithm outperforms the traditional omnidirectional stereo and the learning-based 360◦ stereo algorithms in terms of accuracy and performance. 2.