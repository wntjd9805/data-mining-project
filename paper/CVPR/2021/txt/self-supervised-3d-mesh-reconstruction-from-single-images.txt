Abstract
Rendered Shape
Input
𝒌𝒕𝒉
Recent single-view 3D reconstruction methods recon-struct object’s shape and texture from a single image with only 2D image-level annotation. However, without ex-plicit 3D attribute-level supervision, it is still difﬁcult to achieve satisfying reconstruction accuracy. In this paper, we propose a Self-supervised Mesh Reconstruction (SMR) approach to enhance 3D mesh attribute learning process.
Our approach is motivated by observations that (1) 3D at-tributes from interpolation and prediction should be consis-tent, and (2) feature representation of landmarks from all images should be consistent. By only requiring silhouette mask annotation, our SMR can be trained in an end-to-end manner and generalizes to reconstruct natural objects of birds, cows, motorbikes, etc. Experiments demonstrate that our approach improves both 2D supervised and unsu-pervised 3D mesh reconstruction on multiple datasets. We also show that our model can be adapted to other image synthesis tasks, e.g., novel view generation, shape trans-fer, and texture transfer, with promising results. Our code is publicly available at https://github.com/Jia-Research-Lab. 1.

Introduction
Single-view 3D Object Reconstruction is to recover 3D information, such as shape and texture, of the object from a single image [7, 15, 20, 41]. It is a long-standing prob-lem in computer vision with various applications, including 3D scene analysis, robot navigation, and virtual/augmented reality. Traditional methods usually ﬁt the parameters of a 3D prior morphable model, such as 3DMM [1] for faces and SMPL [23] for human. Building these prior models is expensive and time-consuming, and thus is not quickly ap-plicable to many different natural objects.
In the deep learning era, deep models can learn to re-construct 3D objects in a supervised manner
[7]. 3D su-pervised reconstruction methods [4, 39, 5, 6, 25, 31, 11] di-rectly minimize the discrepancy between the ground-truth 3D attributes and the predicted ones. They usually achieve
$
𝒌𝒕𝒉
𝒌𝒕𝒉
Landmark feature
𝑶
𝒇𝒌
U-Net
𝒌𝒕𝒉 landmark
Landmark Classification
Encode
Interpolation
𝒌𝒕𝒉
Rendering
Target Shape
Interpolation
𝒌𝒕𝒉
Encode (a) Interpolated Consistency (b) Landmark Consistency
Figure 1: Our proposed self-supervised methods for 3D mesh reconstruction.
Interpolated consistency provides
ﬁne-grained 3D annotations to train the reconstruction model by self-supervised regression. Landmark consistency further improves the reconstructed quality in local regions by self-supervised classiﬁcation for landmarks. supreme performances but have to be trained on synthe-sized or 3D scanned datasets with ground-truth 3D an-notations. Meanwhile, since 2D attributes (e.g., silhou-ette mask or landmark) are usually easier to be obtained than 3D attributes, 2D supervised reconstruction methods
[16, 22, 3, 15, 28, 8] do not require 3D annotations. The key module of 2D supervised approaches is a differentiable render [24, 16, 22], which builds a differentiable stream to link 3D model space to 2D images and makes it possible to reconstruct 3D objects through 2D image-level supervision.
Though 2D supervised reconstruction alleviates the de-pendency on 3D annotation, it is mainly to minimize the image-level reconstruction error and does not ensure 3D at-tribute prediction accuracy. The 3D reconstructed results provided in the work of ARCH [11] show that combining 2D with 3D supervision can further improve the accuracy of 3D reconstruction. Therefore, we raise the question if it is possible to achieve 3D attribute-level reconstruction only with 2D annotation.
In this work, we propose Self-Supervised Mesh Recon-struction (SMR) to reconstruct category-speciﬁc 3D mesh objects from single images. 3D attributes, including cam-era, shape, texture, and light, are ﬁrst predicted by attribute encoder and then are supervised at both 2D image and 3D 6002
attribute levels. At the 2D image level, similar to other 2D supervision approaches [3, 15], reconstructed models are rendered to the same images as original input. At the 3D attribute level, as illustrated in Fig. 1, our two novel self-supervised methods, i.e. Interpolated Consistency (IC) and
Landmark Consistency (LC), further improve the learning process of 3D mesh attributes.
For Interpolated Consistency (IC), our motivation is that the interpolated 3D attributes should be consistent with their rendered images’ encoded attributes. In other words, the in-terpolated attributes can be treated as the pseudo 3D anno-tation to train the reconstruction model by self-supervised learning, as illustrated in Fig. 1(a). Compared with the original [10, 18] or randomly augmented attribute in [30], our interpolated attributes can render images with more viewpoints, geometrical structures, and appearances, thus is more efﬁcient to promote the learning process of the target attribute encoder.
Moreover, we propose Landmark Consistency (LC) to further improve landmark-level reconstruction, as illus-trated in Fig. 1(b). If the local parts of a 3D object are well reconstructed, visible landmark feature should be consistent across all images. We treat the mesh vertices as the land-marks of objects. Then the feature of each visible landmark is classiﬁed to the mesh index. This ensures specialty of each landmark and improves the local quality of 3D mesh reconstruction.
Our ﬁnal contributions are: 1. We propose interpolated consistency and landmark consistency as two self-supervised methods to learn the 3D mesh attributes. 2. We propose SMR to reconstruct category-speciﬁc 3D mesh objects from a collection of single images.
It is an end-to-end training approach and is general to model 3D objects. 3. Experiments on the ShapeNet [2] and the BFM [41] datasets demonstrate that our method steadily im-proves both 2D supervised and unsupervised recon-struction. On the CUB-200-2011 [38] dataset, our
SMR outperforms current state-of-the-art mesh recon-struction methods [15, 18, 20]. 2.