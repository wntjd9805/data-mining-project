Abstract
Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent
In this objects due to refraction and absorption of light. paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to gener-alize to unseen objects and achieve fast inference speed.
Based on this representation, we present a novel frame-work that can complete missing depth given noisy RGB-D input. We further improve the depth estimation itera-tively using a self-correcting reﬁnement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs signiﬁcantly better than the current state-of-the-art methods on both synthetic and real world data.
In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method,
ClearGrasp [43]. Code will be released at https :
//research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit. 1.

Introduction
Depth data captured from RGB-D cameras has been widely used in many applications such as augmented re-ality and robot manipulation. Despite their popularity, commodity-level depth sensors, such as structured-light cameras and time-of-ﬂight cameras, fail to produce correct depth for transparent objects due to the lack of light reﬂec-tion. As a result, many algorithms utilizing RGB-D data cannot be directly applied to recognize transparent objects which are very common in household scenarios.
Previous works on estimating geometry of transparent objects are often studied under controlled settings [47, 51].
Recently, Li et al. [26] proposed a physically-based neu-1Work done while author was an intern at NVIDIA.
Figure 1. Our method can predict the depth of unseen transparent objects from a noisy RGB-D image. We back-project the depth map into the point cloud and render it in a novel viewpoint to better visualize the 3D shape. Zoom in to see details. ral network to reconstruct 3D shape of transparent objects from multi-view images. Although their method is less re-stricted compared to previous ones, it still requires the envi-ronment map and the refractive index of transparent objects.
ClearGrasp [43] achieves impressive results on depth com-pletion of transparent objects. It ﬁrst predicts masks, oc-clusion boundaries and surface normals from RGB images using deep networks, and then optimizes initial depth based on the network predictions. However, the optimization re-quires transparent objects to have contact edges with other non-transparent objects. Otherwise, the depth in transpar-ent area becomes undetermined and can be assigned ran-dom value. In addition, it can not be deployed in real time applications due to the expensive optimization process.
In this paper, to overcome the limitations of existing works, we present a fast end-to-end framework for depth completion of transparent objects from a single RGB-D im-age. The core to our approach is a Local Implicit Depth
Function (LIDF) deﬁned on ray-voxel pairs consisting of 4649
camera rays and their intersecting voxels. The motivations for LIDF are: 1) The depth of a transparent object can be inferred from its color and the depth of its non-transparent neighborhood.
In particular, color can provide useful vi-sual cues for the 3D shape and curvature while local depth helps to reason about the spatial arrangement and location of transparent objects. 2) Directly regressing the complete depth map using a deep network can easily overﬁt to the ob-jects and scenes in the training data. By learning at the local scale (a voxel in our case) instead of the whole scene, LIDF can generalize to unseen objects because different objects may share similar local structures. 3) Voxel grids provide a natural partition of the 3D space. By deﬁning implicit function on ray-voxel pairs, we can signiﬁcantly reduce the inference time as the model only needs to consider occu-pied voxels intersected by the camera ray. Based on these motivations, we present a model to estimate the depth of a pixel by learning the relationship between the camera ray and its intersecting voxels given the color and local depth information. To further utilize the geometry of transparent object itself, we propose a depth reﬁnement model to up-date the prediction iteratively by combining the input RGB, input depth points and the predicted depth from LIDF. To train the whole pipeline, we create a large scale synthetic dataset, Omiverse Object dataset, using the NVIDIA Omni-verse platform [1]. Our dataset provides over 60,000 images including both transparent and opaque objects in different scenes. The dataset is generated with diverse object mod-els and poses, lighting conditions, camera viewpoints and background textures to close the sim-to-real gap. Experi-ments show that training on the Omniverse Object dataset can boost the performance for both our approach and com-peting methods in real-world testing cases.
Our approach is inspired by recent advances in neural radience ﬁeld [34, 44, 27]. NeRF [34] can learn a con-tinuous function of the 3D geometry and appearance of a scene, thus achieving accurate reconstruction and render-ing results. However, NeRF has slow inference speed due to inefﬁcient 3D points sampling. Our approach tackles this problem by querying intersecting voxels instead of 3D points along the ray. NSVF [27] also utilizes a sparse voxel grid to reduce the inference time, but it still needs to sam-ple 3D points inside the voxel while our method directly learns the offset of a ray-voxel pair to obtain the possible terminating 3D location of the ray. Experiments demon-strate that learning offsets can produce better depth than
In ad-sampling 3D points based on heuristic strategies. dition, NeRF-based methods needs to train a network for every new scene to model the complex geometry and ap-pearance whereas our method generalizes to unseen objects and scenes in depth completion of transparent objects.
Our contributions are summarized as follows: 1) We pro-pose LIDF, a novel implicit representation deﬁned on ray-voxel pairs, leading to fast inference speed and good gener-ality. 2) We present a two-stage system, including networks to learn LIDF and a self-correcting reﬁnement model, for depth completion of transparent objects. 3) We build a large scale synthetic dataset proved to be useful to transparent ob-jects learning. 4) Our full pipeline is evaluated qualitatively and quantitatively, and outperform the current state-of-the-art in terms of accuracy and speed. 2.