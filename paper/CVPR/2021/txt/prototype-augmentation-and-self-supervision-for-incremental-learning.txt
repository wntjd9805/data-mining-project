Abstract
Despite the impressive performance in many individual tasks, deep neural networks suffer from catastrophic for-getting when learning new tasks incrementally. Recently, various incremental learning methods have been proposed, and some approaches achieved acceptable performance rely-ing on stored data or complex generative models. However, storing data from previous tasks is limited by memory or pri-vacy issues, and generative models are usually unstable and inefﬁcient in training. In this paper, we propose a simple non-exemplar based method named PASS, to address the catas-trophic forgetting problem in incremental learning. On the one hand, we propose to memorize one class-representative prototype for each old class and adopt prototype augmen-tation (protoAug) in the deep feature space to maintain the decision boundary of previous tasks. On the other hand, we employ self-supervised learning (SSL) to learn more gen-eralizable and transferable features for other tasks, which demonstrates the effectiveness of SSL in incremental learn-ing. Experimental results on benchmark datasets show that our approach signiﬁcantly outperforms non-exemplar based methods, and achieves comparable performance compared to exemplar based approaches. 1.

Introduction
Incremental learning (IL) enables humans to acquire novel experience continually while maintaining existing knowledge. In dynamic and open environment, it is crit-ical for modern artiﬁcial intelligence to have the ability of IL because training examples in real-world applications usually appear sequentially. For instance, a face recognition sys-tem may encounter new faces which need to be added and learned throughout its life without forgetting or re-learning the people already learned. However, deep neural networks (DNNs) tend to adjust the learned parameters to new task
∗Corresponding author. and almost fully forget previously acquired knowledge. Mo-tivated by this, a multitude of works [28, 34, 43, 51, 42] have recently emerged that try to alleviate the catastrophic forgetting [16, 37, 13] problem. In this paper, we consider a challenging scenario of class-incremental learning (CIL), in which each task in the sequence contains a set of classes disjoint from the old tasks, and the model need to learn a uniﬁed classiﬁer that can classify all classes seen at different stages without the task-identiﬁer at inference time.
Intuitively, catastrophic forgetting is caused by overlap-ping or confusion between the representations of new and old classes in the feature space. When learning new classes, the decision boundary for previous classes can be dramati-cally changed, and the uniﬁed classiﬁer is severely biased.
To address this issue and maintain previous knowledge, one can store a fraction of old data to jointly train the model with current data [50, 43, 50, 6, 12]. However, storing data is undesirable due to memory limits or privacy issues, in which the data are not allowed to be stored. An alternative way is to learn deep generative models to generate pseudo-samples of previous classes [46, 49, 51, 25]. Nevertheless, it is inef-ﬁcient to train big generative models such as GAN [17, 3] and autoencoder [27, 25] for complex datasets (e.g., natural images). Moreover, the generative models also suffer from catastrophic forgetting. Another direction is to identify and penalize future changes to some important parameters of the original model [28, 54]. These regularization strategies are effective in scenarios where multi-head classiﬁers are used and the task-identiﬁer is available at inference. However, as noticed in some works [23, 48], those methods show poor performance in CIL scenario.
Besides the catastrophic forgetting, another obstacle for
IL is the task-level overﬁtting phenomenon, which has been ignored by previous works. Speciﬁcally, DNNs can easily overﬁt to the training task when learning task continually. In-tuitively, the model may focus on capturing features that are useful for current task, while discarding those less discrim-inative directions which could capture data characteristics for future tasks. This may not be a problem for common 5871
Figure 1: Motivation of PASS. (a) When learning new task, the decision boundary of previous tasks could be dramatically changed, resulting in catastrophic forgetting. ProtoAug is proposed to restrain the decision boundary, thus maintaining the discrimination and balance between old and new classes. (b) If the learned features are task-speciﬁc in each stage, the model trained on previous task might be a bad initialization for current task. We propose to leverage the beneﬁt of SSL to learn richer and more transferable features. Intuitively, different tasks would be closer in the parameter space, and it would be easier to
ﬁnd a model to perform well on all tasks, thus improving both the stability and plasticity of the model. single task learning scenario, but leads particles inﬂuence for IL since the model for current task is initialized with previous model. A recent study [42] found that a model trained from scratch using samples stored can surprisingly outperforms many recently proposed algorithms. This study indicates that the previous model, which mainly carries task-speciﬁc features, might be a bad initialization for current task, as shown in Fig. 1(b). Consequently, the model would need more updates to perform well on current task, which increases the forgetting problem on the other hand.
Motivated by the above analysis, we propose to improve
CIL performance by maintaining the decision boundary and reducing task-level overﬁtting phenomenon, as shown in
Fig. 1. The proposed PASS mainly consists of Prototype
Augmentation and Self-Supervision. On the one hand, prototype augmentation (protoAug) memorizes one class-representative prototype (typically the class mean in the deep feature space) for each old class, and augments the mem-orized prototypes via Gaussian noise when learning new classes. Then, the augmented prototypes and deep features of new data are jointly classiﬁed to maintain the discrimi-nation and balance between old and new classes. This is inspired by a recent work [35] in long-tailed recognition which expands the distribution of the tail classes by aug-menting the tail classes with certain disturbances. While
[35] focuses on class-imbalance learning and learns the em-bedding augmentation strategy from the head classes, in our work, we focus on CIL and investigate the value of simple
Gaussian noise based augmentation.
On the other hand, we take inspiration from self-supervised learning (SSL) to alleviate task-level overﬁtting phenomenon in IL. In particular, SSL aims to learn trans-ferable representations that would be useful for other tasks.
Inspired by the natural connection between IL and SSL, we propose to leverage the beneﬁt of SSL to learn task-agnostic and transferable representations. Intuitively, with SSL, dif-ferent tasks would be closer in the parameter space, and the model trained on current task would be a better initial-ization for learning the next task. In conclusion, our main contributions are summarized as follows:
• We propose a simple and effective non-exemplar based method to overcome catastrophic forgetting problem in
CIL by memorizing and augmenting prototypes of old classes in the deep feature space.
• We emphasize the task-level overﬁtting phenomenon in IL, and adopt self-supervised learning to learn more generalizable and transferable features.
• Our method signiﬁcantly outperforms non-exemplar based methods and obtains comparable results com-pared to exemplar based methods in CIL scenario. 2.