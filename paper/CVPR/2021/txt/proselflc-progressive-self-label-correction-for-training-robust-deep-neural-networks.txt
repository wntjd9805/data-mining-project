Abstract
To train robust deep neural networks (DNNs), we system-atically study several target modiﬁcation approaches, which include output regularisation, self and non-self label cor-rection (LC). Two key issues are discovered: (1) Self LC is the most appealing as it exploits its own knowledge and requires no extra models. However, how to automatically decide the trust degree of a learner as training goes is not well answered in the literature? (2) Some methods penalise while the others reward low-entropy predictions, prompting us to ask which one is better? issue,
To resolve the ﬁrst taking two well-accepted propositions–deep neural networks learn meaningful pat-terns before ﬁtting noise [3] and minimum entropy regu-larisation principle [10]–we propose a novel end-to-end method named ProSelfLC, which is designed according to learning time and entropy. Speciﬁcally, given a data point, we progressively increase trust in its predicted label distri-bution versus its annotated one if a model has been trained for enough time and the prediction is of low entropy (high conﬁdence). For the second issue, according to ProSelfLC, we empirically prove that it is better to redeﬁne a meaning-ful low-entropy status and optimise the learner toward it.
This serves as a defence of entropy minimisation.
We demonstrate the effectiveness of ProSelfLC through extensive experiments in both clean and noisy settings. The source code is available at https://github.com/
XinshaoAmosWang/ProSelfLC-CVPR2021. 1.

Introduction
There exist many target (label) modiﬁcation approaches.
They can be roughly divided into two groups: (1) Output regularisation (OR), which is proposed to penalise over-conﬁdent predictions for regularising deep neural networks.
It includes label smoothing (LS) [42, 29] and conﬁdence penalty (CP) [33]; (2) Label correction (LC). On the one
*Prof. David A. Clifton was supported by the National Institute for
Health Research (NIHR) Oxford Biomedical Research Centre (BRC). hand, LC regularises neural networks by adding the simi-larity structure information over training classes into one-hot label distributions so that the learning targets become structured and soft. On the other hand, it can correct the semantic classes of noisy label distributions. LC can be further divided into two subgroups: Non-self LC and Self
LC. The former requires extra learners, while the latter re-lies on the model itself. A typical approach of Non-self
LC is knowledge distillation (KD), which exploits the pre-dictions of other model(s), usually termed teacher(s) [17].
Self LC methods include Pseudo-Label [23], bootstrapping (Boot-soft and Boot-hard) [35], Joint Optimisation (Joint-soft and Joint-hard) [43], and Tf-KDself [55]. According to an overview in Figure 1 (detailed derivation is in Section 3 and Table 1), in label modiﬁcation, the output target of a data point is deﬁned by combining a one-hot label distribu-tion and its corresponding prediction or a predeﬁned label distribution.
Firstly, we present the drawbacks of existing approaches: (1) OR methods naively penalise conﬁdent outputs without leveraging easily accessible knowledge from other learners or itself (Figure 1a); (2) Non-self LC relies on accurate aux-iliary models to generate predictions (Figure 1b). (3) Self
LC is the most appealing because it exploits its own knowl-edge and requires no extra learners. However, there is a core question that is not well answered:
In Self LC, how much should we trust a learner to leverage its knowledge?
As shown in Figure 1b, in Self LC, for a data point, we have two labels: a predeﬁned one-hot q and a predicted structured p.
Its learning target is (1 − ǫ)q + ǫp, i.e., a trade-off between q and p, where ǫ deﬁnes the trust score of a learner. In existing methods, ǫ is ﬁxed without consid-ering that a model’s knowledge grows as the training pro-gresses. For example, in bootstrapping, ǫ is ﬁxed through-out the training process. Joint Optimisation stage-wisely trains a model. It fully trusts predicted labels and uses them to replace old ones when a stage ends, i.e., ǫ = 1. Tf-KDself trains a model by two stages: ǫ = 0 in the ﬁrst one while ǫ is tuned for the second stage. Note that p is gen-752
1 0 0 1  1/3 1/3 1/3 1/2 1/3 1/6 1 0 0 1  0 0
LS
CP (a) OR includes LS [42] and CP [33]. LS softens a target by adding a uniform label distribution. CP changes the probability 1 to a smaller value 1 − ǫ in the one-hot target. The double-ended arrow means factual equivalence, because an output is deﬁnitely non-negative after a softmax layer. (b) LC contains Self LC [23, 35, 43, 55] and Non-self LC [17]. The parameter ǫ deﬁnes how much a predicted label distribution is trusted.
Figure 1: Target modiﬁcation includes OR (LS and CP), and LC (Self LC and Non-self LC). Assume there are three training classes. q is the one-hot target. u is a uniform label distribution. p denotes a predicted label distribution. The target combination parameter is ǫ ∈ [0, 1]. erated by a preceding-stage model in stage-wise training, which requires signiﬁcant human intervention and is time-consuming in practice.
To improve Self LC, we propose a novel method named
Progressive Self Label Correction (ProSelfLC), which is end-to-end trainable and needs negligible extra cost. Most importantly, ProSelfLC modiﬁes the target progressively and adaptively as training goes. Two design principles of
ProSelfLC are: (1) When a model learns from scratch, hu-man annotations are more reliable than its own predictions in the early phase, during which the model is learning sim-ple meaningful patterns before ﬁtting noise, even when se-vere label noise exists in human annotations [3]. (2) As a learner attains conﬁdent knowledge as time progresses, we leverage it to revise annotated labels. This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios [9, 10].
Secondly, note that OR methods penalise low entropy while LC rewards it, intuitively leading to a second vital question:
Should we penalise a low-entropy status or reward it?
Entropy minimisation is the most widely used principle in machine learning [14, 38, 9, 10, 22]. In standard classiﬁca-tion, minimising categorical cross entropy (CCE) optimises a model towards a low-entropy status deﬁned by human an-notations, which contain noise in very large-scale machine learning. As a result, conﬁdence penalty becomes popular for reducing noisy ﬁtting. In contrast, we prove that it is better to reward a meaningful low-entropy status redeﬁned by our ProSelfLC. Therefore, our work offers a defence of entropy minimisation against the recent conﬁdence penalty practice [42, 29, 33, 6].
Finally, we summarise our main contributions:
• We provide a theoretical study on popular target mod-iﬁcation methods through entropy and KL divergence
[21]. Accordingly, we reveal their drawbacks and pro-pose ProSelfLC as a solution. ProSelfLC can: (1) en-hance the similarity structure information over training classes; (2) correct the semantic classes of noisy label distributions. ProSelfLC is the ﬁrst method to trust self knowledge progressively and adaptively.
• Our extensive experiments: (1) defend the entropy minimisation principle; (2) demonstrate the effective-ness of ProSelfLC in both clean and noisy settings. 2.