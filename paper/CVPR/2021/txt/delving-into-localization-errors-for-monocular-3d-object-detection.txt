Abstract
Estimating 3D bounding boxes from monocular images is an essential component in autonomous driving, while ac-curate 3D object detection from this kind of data is very challenging.
In this work, by intensive diagnosis experi-ments, we quantify the impact introduced by each sub-task and found the ‘localization error’ is the vital factor in re-stricting monocular 3D detection. Besides, we also investi-gate the underlying reasons behind localization errors, an-alyze the issues they might bring, and propose three strate-gies. First, we revisit the misalignment between the center of the 2D bounding box and the projected center of the 3D object, which is a vital factor leading to low localization ac-curacy. Second, we observe that accurately localizing dis-tant objects with existing technologies is almost impossible, while those samples will mislead the learned network. To this end, we propose to remove such samples from the train-ing set for improving the overall performance of the detec-tor. Lastly, we also propose a novel 3D IoU oriented loss for the size estimation of the object, which is not affected by ‘localization error’. We conduct extensive experiments on the KITTI dataset, where the proposed method achieves real-time detection and outperforms previous methods by a large margin. The code will be made available at: https:
//github.com/xinzhuma/monodle. 1.

Introduction
Remarkable progress has been achieved in 3D detection, especially for LiDAR/stereo-based approaches [44, 19, 33, 9, 37], along with the advances in deep neural networks. In contrast, the accuracy of 3D detection from only monocular images [35, 2, 10, 25, 24, 12] is obviously lower than that from LiDAR or stereo. In this work, we aim to quantita-tively identify the problem and propose our solutions.
To investigate and quantify the underlying factors that restrict the performance of monocular 3D object detection,
Figure 1: Range-wise evaluation on the KITTI validation set. Metric is AP40 of the Car category under moderate set-ting. The sampling interval is 10 m. For example, the corre-sponding value at horizontal axis 20 represents the overall performance of all samples between 15 m and 25 m. we conduct intensive diagnostic experiments for this task, inspired by the error identifying methods [20, 43, 17, 1] commonly used in the 2D detection scope. Speciﬁcally, we build our baseline model (see Section 3.2 for details) based on CenterNet [43] and progressively replace predicted items with their ground-truth values. To better analyze the error patterns, we evaluate the results in a range-wise manner and show the summary of those experiments in Figure 1. Based on our investigation, we have the following three observa-tions and corresponding designs.
Observation 1: The most striking feature in Figure 1 is the leap in performance when using ground-truth location, reaching a level similar to the state-of-the-art LiDAR-based methods, suggesting the localization error is the key factor in restricting monocular 3D detection. Furthermore, except for depth estimation, detecting the projected center of the 3D object also plays an important role in restoring the 3D position of the object. To this end, we revisit the misalign-ment between the center of the 2D bounding box and the projected center of the 3D object. Besides, we also conﬁrm the necessity of keeping 2D detection related branches in 4721
monocular 3D detector. In this way, 2D detection is used as the correlated auxiliary task to help learning the features shared with 3D detection, which is different from the exist-ing work in [23] that discards 2D detection.
Observation 2: An apparent trend reﬂected in Figure 1 is that the detection accuracy signiﬁcantly decreases with respect to the distance (the low performance of very close range objects will be discussed in supplementary materials).
More importantly, all the models cannot output any true positive samples beyond a certain distance. We found that it is almost impossible to detect distant objects accurately with existing technologies due to the inevitable localization errors (see Section 4.4 for details). In this case, whether it is beneﬁcial to add these samples into the training set be-comes a question. In fact, there is a clear domain gap be-tween ‘bad’ samples and ‘easy-to-detect’ samples and forc-ing the network to learn from those samples will reduce its representative ability for the others, which will thus impair the overall performance. Based on the observation above, we propose two schemes. The ﬁrst scheme removes dis-tant samples from the training set and the second scheme reduces the training loss weights of these samples.
Observation 3: We found that, except for localization error, there are also some other vital factors, such as dimen-sion estimation, restricting monocular 3D detection (there is still 27.4% room for improvements even we use the ground-truth location). Existing methods in this scope tend to op-timize each component of the 3D bounding box indepen-dently, and the studies in [35, 36] conﬁrm the effectiveness of this strategy. However, the failure to consider the contri-bution of each loss item to the ﬁnal metric (i.e. 3D IoU) may lead to sub-optimal optimization. To alleviate this problem, we propose an IoU oriented loss for 3D size estimation. The new IoU oriented loss dynamically adjust the loss weight for each side in sample level according its contribution rate to the 3D IoU.
In summary, the key contributions of this paper are as follows: First, we conduct intensive diagnostic experiments for monocular 3D detection. In addition to ﬁnding that the
‘localization error’ is the main problem restricting monoc-ular 3D detection, we also quantify the overall impact of each sub-task. Second, we investigate the underlying rea-sons behind localization error, analyze the issues it might bring. Accordingly, we propose three novel strategies op-erating on annotations, training samples, and optimization losses to alleviate problems caused by localization error for boosting the detection.
Experimental results show the effectiveness of the pro-posed strategies. In particular, compared with existing best-performing monocular 3D object detection approaches, the proposed method achieves at least 1.6 points AP40 im-provements on the bird’s view detection and 3D object de-tection in the KITTI dataset. 2.