Abstract
Human multimodal emotion recognition involves time-series data of different modalities, such as natural lan-guage, visual motions, and acoustic behaviors. Due to the variable sampling rates for sequences from different modalities, the collected multimodal streams are usually unaligned. The asynchrony across modalities increases the difï¬culty on conducting efï¬cient multimodal fusion. Hence, this work mainly focuses on multimodal fusion from un-aligned multimodal sequences. To this end, we propose the Progressive Modality Reinforcement (PMR) approach based on the recent advances of crossmodal transformer.
Our approach introduces a message hub to exchange infor-mation with each modality. The message hub sends common messages to each modality and reinforces their features via crossmodal attention. In turn, it also collects the reinforced features from each modality and uses them to generate a re-inforced common message. By repeating the cycle process, the common message and the modalitiesâ€™ features can pro-gressively complement each other. Finally, the reinforced features are used to make predictions for human emotion.
Comprehensive experiments on different human multimodal emotion recognition benchmarks clearly demonstrate the superiority of our approach. 1.

Introduction
Human multimodal emotion recognition focuses on rec-ognizing the sentiment attitude of humans from video clips [26, 16, 25, 17, 5]. This task involves time-series
âˆ— Corresponding authors. data of different modalities, e.g., natural language, facial gestures, and acoustic behaviors. The multimodal setting can provide rich information for thorough sentiment under-standing.
In practice, however, the collected multimodal streams are usually asynchronous, due to the variable sam-pling rates for sequences from different modalities. For ex-ample, the video frame with a depressed facial expression may relate to a negative word spoken in the past. The asyn-chrony across different modalities can increase the difï¬culty on conducting efï¬cient multimodal fusion.
The previous works address the above issues by pre-deï¬ned word-level alignment [22, 13, 19, 24]. To this end, the visual and acoustic sequences are ï¬rst manually aligned in the resolution of the textual words. Multimodal fusion is then conducted on the aligned time steps. How-ever, the manual word-alignment process is usually labor-intensive and requires domain knowledge. Recently, Tsai et al. propose the Multimodal Transformer (MulT) ap-proach to fuse crossmodal information from unaligned data sequences [18]. Their approach introduces the modality reinforcement unit to reinforce a target modality with in-formation from a source modality by learning the direc-tional pairwise attention between elements across modali-ties (see Fig. 1(a)), based on the recent advances of trans-former [20]. By exploring the crossmodal interaction be-tween elements via the crossmodal attention operations,
MulT can implement multimodal fusion from asynchronous sequences without explicitly aligning the data.
In their approach, however, the modality reinforcement of each direction is performed independently and does not exchange information with each other. Hence, the multi-modal fusion only appears between each directional modal-ity pair, but not across all the modalities involved in human 2554
ğ‘ [ğ·+1]
ğ¿
ğ‘ [ğ·+1]
ğ´
ğ‘ [ğ·+1]
ğ‘‰
ğ‘ [ğ·+1]
ğ¿
MRU[ğ·]
ğ‘‰ â†’ğ¿
MRU[ğ·]
ğ´â†’ğ¿
MRU[ğ·]
ğ‘‰ â†’ğ´
MRU[ğ·]
ğ¿â†’ğ´
MRU[ğ·]
ğ¿â†’ğ‘‰
MRU[ğ·]
ğ´â†’ğ‘‰
FFL[ğ·]
MRU[ğ·]
ğ‘‰ â†’ğ¿
MRU[ğ·]
ğ´â†’ğ¿
FFL[ğ·]
ğ‘ğ‘‰
ğ‘ğ´
ğ‘ğ‘‰
ğ‘ğ¿
ğ‘ğ¿
ğ‘ğ´
MRU[2]
ğ‘‰ â†’ğ¿
MRU[2]
ğ´â†’ğ¿
MRU[2]
ğ‘‰ â†’ğ´
MRU[2]
ğ¿â†’ğ´
MRU[2]
ğ¿â†’ğ‘‰
MRU[2]
ğ´â†’ğ‘‰
FFL[2]
MRU[2]
ğ‘‰ â†’ğ¿
MRU[2]
ğ´â†’ğ¿
FFL[2]
ğ‘ğ‘‰
MRU[1]
ğ‘‰ â†’ğ¿
ğ‘ğ´
MRU[1]
ğ´â†’ğ¿
ğ‘ğ‘‰
MRU[1]
ğ‘‰ â†’ğ´
ğ‘ğ¿
MRU[1]
ğ¿â†’ğ´
ğ‘ğ¿
MRU[1]
ğ¿â†’ğ‘‰
ğ‘ğ´
MRU[1]
ğ´â†’ğ‘‰
FFL[1]
MRU[1]
ğ‘‰ â†’ğ¿
MRU[1]
ğ´â†’ğ¿
FFL[1]
ğ‘ğ‘‰
MRU[0]
ğ‘‰ â†’ğ¿
ğ‘ğ´
MRU[0]
ğ´â†’ğ¿
ğ‘ğ‘‰
MRU[0]
ğ‘‰ â†’ğ´
ğ‘ğ¿
MRU[0]
ğ¿â†’ğ´
ğ‘ğ¿
MRU[0]
ğ¿â†’ğ‘‰
ğ‘ğ´
MRU[0]
ğ´â†’ğ‘‰
FFL[0]
MRU[0]
ğ‘‰ â†’ğ¿
MRU[0]
ğ´â†’ğ¿
FFL[0]
ğ‘ğ‘‰
ğ‘ğ¿
ğ‘ğ´
ğ‘ğ¿
ğ‘ğ‘‰
ğ‘ğ´
ğ‘ğ¿
ğ‘ğ´
ğ‘ğ¿
ğ‘ğ‘‰
ğ‘ğ´
ğ‘ğ‘‰
ğ‘ğ‘‰
ğ‘ğ¿
ğ‘ğ¿
ğ‘ğ´ (a) (b)
Figure 1: The architecture of the MulT model. MRU[i] sâ†’t represents a modality reinforcement unit, in which a source modality s reinforces a target modality t by attending to the crossmodal interaction between elements. FFL[i] represents a feed-forward layer. (a) the low-level version in which the target modality is reinforced by repeatedly attending to the low-level features of the source modality; (b) the high-level version in which the target modality is reinforced by repeatedly attending to higher-level features of the source modality. emotion recognition. It is inefï¬cient to fuse the sequences of multiple modalities by using the pairwise manner. For example, the redundant information can be introduced by directly concatenating the visual sequence reinforced by the language modality and that reinforced by the acoustic modality. It is crucial to conduct multimodal fusion by con-sidering the three-way interactions across all the involved modalities.
Moreover, the independent pairwise fusion approach fails to exploit the high-level features of the source modal-ity. For each directional modality pair, as shown in Fig. 1(a), the target modality is reinforced by repeatedly attending to the low-level features of the source modality.
Intuitively, the deep interactions cross modalities cannot be explored via the semi-shallow structure. Their approach also notices this problem and attempts to implement crossmodal atten-tion via the high-level features of the source modality by stacking feed-forward layers over the source modality (see
Fig. 1(b)). However, reduced performance is observed. This is because that the source branch does not receive clear su-pervision to update its feed-forward layers, since the modal-ity reinforcement operations mainly focus on generating a reinforced target modality. As a result, it is unclear whether the high-level features of the source modality are better than the low-level features. Instead, the increased modal com-plexity can reduce the performance.
Motivated by the above observations, this work pro-poses the Progressive Modality Reinforcement (PMR) ap-proach for multimodal fusion from unaligned multimodal sequences. Our approach introduces a message hub to exchange information with each modality. As shown in
Fig. 2, the message hub can send common messages to each modality in order to reinforce their features via crossmodal
In turn, it also collects the reinforced features attention. from each modality and uses them to generate an improved common message.
In our approach, hence, the common message and the modalitiesâ€™ features progressively comple-ment each other. Moreover, we introduce a dynamic ï¬lter mechanism in the modality reinforcement unit to dynam-ically determine the passed proportions of the reinforced features. Compared with the prior MulT model [18], the ad-vantage of our approach lies in two aspects. First, the com-mon message promotes effective information ï¬‚ow across modalities and encourages the crossmodal attention oper-ations to explore the element-level dependencies across all the three modalities instead of the directional pairwise de-pendencies. Second, the progressive reinforcement strat-egy provides an effective way to leverage the high-level features of the source modality for modality reinforcement.
Unlike in Fig. 1(b), the feature of the source modality can receive clear supervision in the reinforcement unit where it is considered as the target modality. The superiority of our approach is veriï¬ed via extensive empirical experiments on different human multimodal emotion recognition bench-marks with both the word-aligned setting and the unaligned setting.
To sum up, the contributions of this work are mainly three-fold:
â€¢
â€¢
We introduce the message hub to explore the three-way interactions across all the involved modalities under the background of multimodal fusion from unaligned multimodal sequences.
We propose the progressive strategy to leverage the high-level features of the source modality for multi-modal fusion. 2555
â€¢
Our approach can obtain better results than the exist-ing state-of-the-art works over different human multi-modal emotion recognition benchmarks. 2.