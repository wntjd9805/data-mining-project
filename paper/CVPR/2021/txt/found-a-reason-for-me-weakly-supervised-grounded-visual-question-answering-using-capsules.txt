Abstract
The problem of grounding VQA tasks has seen an in-creased attention in the research community recently, with most attempts usually focusing on solving this task by using pretrained object detectors. However, pre-trained object detectors require bounding box annotations for detecting relevant objects in the vocabulary, which may not always be feasible for real-life large-scale applications. In this pa-per, we focus on a more relaxed setting: the grounding of relevant visual entities in a weakly supervised manner by training on the VQA task alone. To address this problem, we propose a visual capsule module with a query-based selec-tion mechanism of capsule features, that allows the model to focus on relevant regions based on the textual cues about visual information in the question. We show that integrat-ing the proposed capsule module in existing VQA systems signiﬁcantly improves their performance on the weakly su-pervised grounding task. Overall, we demonstrate the ef-fectiveness of our approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the CLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with groundtruth bounding boxes for objects that are rele-vant for the correct answer, as well as on GQA, a real world
VQA dataset with compositional questions. We show that the systems with the proposed capsule module consistently outperform the respective baseline systems in terms of an-swer grounding, while achieving comparable performance on VQA task.1 1.

Introduction
VQA systems have now matured to the point where their us-age is increasing in real life applications such as answering questions based on radiology images [1], helping visually impaired people [10], and human-robot interactions [38].
However, with the increasing maturity of such systems, it also becomes important to know how the answer is actually generated in order to assess if it is based on the right cues 1Code will be available at https://github.com/aurooj/
WeakGroundedVQA_Capsules.git
Figure 1. Problem deﬁnition: Given an input image and a ques-tion, we want to answer the question as well as localize the evi-dence (shown in green boxes) with VQA supervision alone. Best viewed in color. or not. If the question is “Are there black horses to the right of the vehicle?” (see ﬁgure 1), it may be important to know if the answer is generated because the network found black horses at the right place in the image or not. This allows to judge the overall correctness beyond simply evaluating the textual answer. Recent works [17, 36, 4, 20] try to address this problem by starting to evaluate not only the VQA accu-racy, but also the accuracy of grounding that the answer is based on. The grounding of an answer is usually assessed by considering the respective attention map of the image for the given answer, and by evaluating if the objects that are relevant for the right answer are attended to or not.
To achieve good grounding accuracy, most approaches in this ﬁeld rely on input feature maps from object detection models that are pretrained with the relevant object classes.
This restricts the scope to known object classes such as MS
COCO [31], or require to annotate the regions of relevant objects, and to pretrain an object detector for them[17].
Only few attempts have been made so far to address this problem to train both, the VQA as well as the grounding, without pretrained object detection based on the informa-tion of the VQA task alone as e.g. in context of the GQA dataset by only using spatial (appearance) features [17].
This paper focuses on exactly this scenario: weakly super-vised visual grounding based on VQA supervision. The idea here is that both tasks, the visual question-answering as well as the correct visual grounding, should be learned 8465
from the VQA task alone. Hence, we do not use any object-level information as an input or in supervision.
The correct grounding in this case is usually based on two major tasks, ﬁnding the relevant visual instances and, usually, modeling the relation between those instances as seen in ﬁgure 1. To address this problem, we propose ex-tending current VQA frameworks with capsules. Capsule networks were introduced by Sabour et al. [39], and have shown promising results for image interpretability [25] and segmentation in various ﬁelds such as 3D point clouds [48], videos [7] and medical images [28]. This is the result of capsule layers’ ability to learn part-to-whole relationships for object entities through routing-by-agreement. We be-lieve this capability to model objects and their relations qualiﬁes capsules as a good choice for addressing the prob-lem of weakly-supervised grounding in VQA.
Current capsule-based methods follow the practice of adding capsule layers on top of convolutional features, and training them with object class supervision. A discrete and supervised masking operation, i.e. masking all capsules ex-cept the ground-truth class capsule, is often applied to re-construct or segment the object corresponding to the given class.
In case of weak VQA grounding, no class or ob-ject based supervision is available; only an embedding of a natural language question is given. Therefore, we pro-pose a “soft-masking” procedure which selects the cap-sule(s) based on the input question. For example, if the rea-soning operation is F ind(“blue spheres”), the soft-masking operation will mask all capsules not representing the “blue spheres”. Once the irrelevant capsules are masked, the cap-sule representations are passed to future reasoning opera-tions to complete the VQA task.
To evaluate VQA systems for their answer grounding ability, we consider two datasets, the recently proposed
GQA dataset [17] as well as the CLEVR dataset [23]. To allow the evaluation of grounding accuracy on CLEVR, we propose a new CLEVR validation set, named CLEVR-Answers. CLEVR-Answers provides VQA pairs with the respective ground truth bounding boxes for all objects that the answer is based on. Note that, as we are not interested in using any object annotations during training, we only need ground truth bounding boxes during evaluation, but not during training. The idea is, thus, to train on the stan-dard CLEVR training set and to learn visual representations of objects during this training without further annotation.
We use this new evaluation set to test current state-of-the-art frameworks, MAC [16] and Stacked NMN [13] with re-spect to their grounding abilities. We show that, although all frameworks perform at the same level with respect to VQA accuracy, there are major differences with respect to their grounding abilities. We show that using capsules with soft query-based masking signiﬁcantly improves existing meth-ods’ grounding abilities. 2.