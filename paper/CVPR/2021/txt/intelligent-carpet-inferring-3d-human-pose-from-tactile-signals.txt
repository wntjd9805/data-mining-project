Abstract
Daily human activities, e.g., locomotion, exercises, and resting, are heavily guided by the tactile interactions be-tween the human and the ground. In this work, leveraging such tactile interactions, we propose a 3D human pose es-timation approach using the pressure maps recorded by a tactile carpet as input. We build a low-cost, high-density, large-scale intelligent carpet, which enables the real-time recordings of human-ﬂoor tactile interactions in a seam-less manner. We collect a synchronized tactile and visual dataset on various human activities. Employing a state-of-the-art camera-based pose estimation model as supervision, we design and implement a deep neural network model to infer 3D human poses using only the tactile information.
Our pipeline can be further scaled up to multi-person pose estimation. We evaluate our system and demonstrate its po-tential applications in diverse ﬁelds. 1.

Introduction
Human pose estimation is critical in action recognition
[30, 52], gaming [26], healthcare [64, 36, 22], and robotics
[34]. Signiﬁcant advances have been made to estimate hu-man pose by extracting skeletal kinematics from images and videos. However, camera-based pose estimation re-mains challenging when occlusion happens, which is in-evitable during daily activities. Further, the rising demand for privacy also promotes development in non-vision-based human pose estimation systems [63, 62]. Since most hu-man activities are dependent on the contact between the human and the environment, we present a pose estima-tion approach using tactile interactions between humans and the ground. Recently, various smart ﬂoor or carpet systems have been proposed for human movement detec-tion [11, 2, 48, 7, 3, 60, 40, 16, 1], and posture recognition
[25, 50]. Previous work has also demonstrated the feasibil-ity of using pressure images for pose estimation [6, 9, 8].
However, these studies mainly focus on the estimation of poses where a large portion of the body is in direct con-tact with the sensing surface, e.g., resting postures. A more challenging task is to infer 3D human pose from the limited pressure imprints involved in complicated daily activities, e.g., using feet pressure distribution to reconstruct the pose of the head and limbs. So far, complex 3D human pose esti-mation and modeling using tactile information, spanning a diverse set of human activities including locomotion, rest-ing, and daily exercises, have not been available.
In this study, we ﬁrst develop an intelligent carpet – a large integrated tactile sensing array consisting of over 9,000 pressure sensors, covering over 36 square feet, which can be seamlessly embedded on the ﬂoor. Coupled with readout circuits, our system enables real-time recordings 11255
of high-resolution human-ground tactile interactions. With this hardware, we collect over 1,800,000 synchronized tac-tile and visual frames for 10 different individuals perform-ing a diverse set of daily activities, e.g., lying, walking, and exercising. Employing the visual information as supervi-sion, we design and implement a deep neural network to in-fer the corresponding 3D human pose using only the tactile information. Our network predicts the 3D human pose with the average localization error of less than 10 cm compared with the ground truth pose obtained from the visual infor-mation. The learned representations from the pose estima-tion model, when combined with a simple linear classiﬁer, allow us to perform action classiﬁcation with an accuracy of 98.7%. We also include ablation studies and evaluate how well our model generalizes to unseen individuals and unseen actions. Moreover, our approach can be scaled up for multi-person 3D pose estimation. Leveraging the tactile sensing modality, we believe our work opens up opportuni-ties for human pose estimation that is unaffected by visual obstructions in a seamless and conﬁdential manner. 2.