Abstract
Active stereo cameras that recover depth from structured light captures have become a cornerstone sensor modal-ity for 3D scene reconstruction and understanding tasks across application domains. Active stereo cameras project a pseudo-random dot pattern on object surfaces to extract disparity independently of object texture. Such hand-crafted patterns are designed in isolation from the scene statis-tics, ambient illumination conditions, and the reconstruc-tion method. In this work, we propose a method to jointly learn structured illumination and reconstruction, parame-terized by a diffractive optical element and a neural net-work, in an end-to-end fashion. To this end, we introduce a differentiable image formation model for active stereo, rely-ing on both wave and geometric optics, and a trinocular re-construction network. The jointly optimized pattern, which we dub “Polka Lines,” together with the reconstruction net-work, makes accurate active-stereo depth estimates across imaging conditions. We validate the proposed method in simulation and using with an experimental prototype, and we demonstrate several variants of the Polka Lines patterns specialized to the illumination conditions. 1.

Introduction
Active depth cameras have become essential for three-dimensional scene reconstruction and scene understanding, with established and emerging applications across disci-plines, including robotics, autonomous drones, navigation, driver monitoring, human-computer interaction, virtual and mixed reality, and remote conferencing. When combined with RGB cameras, depth-sensing methods have made it possible to recover high-ﬁdelity scene reconstructions [23].
Such RGB-D cameras also allowed researchers to collect large-scale RGB-D data sets that propelled work on funda-mental computer vision problems, including scene under-standing [44, 21] and action recognition [36]. However, while depth cameras under controlled conditions with low ambient light and little object motion are becoming reli-able [1, 42], depth imaging in strong ambient light, at long ranges, and for ﬁne detail and highly dynamic scenes re-mains an open challenge.
A large body of work has explored active depth sens-ing approaches to tackle this challenge [18, 27, 4, 41], with structure light and time-of-ﬂight cameras being the most successful methods. Pulsed time-of-ﬂight sensors emit pulses of light into the scene and measure the travel time of the returned photons directly by employing sensitive sili-con avalanche photo-diodes [51] or single-photon avalanche diodes [5]. Although these detectors are sensitive to a single photon, their low ﬁll factor restricts existing LiDAR sensors to point-by-point scanning with individual diodes, which prohibits the acquisition of dense depth maps. Correlation time-of-ﬂight sensors [18, 25, 27] overcome this challenge by indirectly estimating round-trip time from the phase of temporally modulated illumination. Although these cam-eras provide accurate depth for indoor scenes, they suf-fer from strong ambient illumination and multi-path inter-ference [45, 29], are limited to VGA resolution, and they require multiple captures, which makes dynamic scenes a challenge. Active stereo [55, 1, 2] has emerged as the only low-cost depth sensing modality that has the potential to overcome these limitations of existing methods for room-sized scenes. Active stereo cameras equip a stereo camera pair with an illumination module that projects a ﬁxed pat-tern onto a scene so that, independently of surface texture, stereo correspondence can be reliably estimated. As such, active stereo methods allow for single-shot depth estimates at high resolutions using low-cost diffractive laser dot mod-ules [1] and conventional CMOS sensor deployed in mass-market products including Intel RealSense cameras [1] and the Google Pixel 4 Phones [2]. However, although active stereo has become a rapidly emerging depth-sensing tech-nology, existing approaches struggle with extreme ambient illumination and complex scenes, prohibiting reliable depth estimates in uncontrolled in-the-wild scenarios.
These limitations are direct consequences of the pipeline design of existing active stereo systems, which hand-engineer the illumination patterns and the reconstruction al-gorithms in isolation. Typically, the illumination pattern is designed in a ﬁrst step using a diffractive optical element (DOE) placed in front of a laser diode. Existing dot pat-terns resulting from known diffractive gratings, such as the
Dammann grating [10], are employed with the assumption that generating uniform textures ensures robust disparity es-timation for the average scene. Given a ﬁxed illumination 5757
pattern, the reconstruction algorithm is then designed with the goal of estimating correspondence using cost-volume methods [7, 22] or learning-based methods [39, 12, 55, 38].
In this conventional design paradigm, the illumination pat-tern does not receive feedback from the reconstruction algo-rithm or the dataset of scenes, prohibiting end-to-end learn-ing of optimal patterns, reconstruction algorithms, and cap-ture conﬁgurations tailored to the scene.
In this work, we propose a method that jointly learns illumination patterns and a reconstruction algorithm, pa-rameterized by a DOE and a neural network, in an end-to-end manner. The resulting optimal illumination patterns, which we dub “Polka Lines”, together with the reconstruc-tion network, allow for high-quality scene reconstructions.
Moreover, our method allows us, for the ﬁrst time, to learn environment-speciﬁc illumination patterns for active stereo systems. The proposed method hinges on a differentiable image formation model that relies on wave and geometric optics to make the illumination and capture simulation ac-curate and, at the same time, efﬁcient enough for joint op-timization. We then propose a trinocular active stereo net-work that estimates an accurate depth map from the sensor inputs. Unlike previous methods that only use binocular inputs from the stereo cameras, our network exploits the known illumination pattern, resulting in a trinocular stereo setup which reduces reconstruction errors near occlusion boundaries. We train the fully differentiable illumination and reconstruction model in a supervised manner and ﬁne-tune the reconstruction for an experimental prototype in a self-supervised manner. The proposed Polka Lines pat-terns, together with the reconstruction network, allows us to achieve state-of-the-art active stereo depth estimates for a wide variety of imaging conditions.
Speciﬁcally, We make the following contributions:
• We introduce a novel differentiable image formation model for active stereo systems based on geometric and wave optics.
• We devise a novel trinocular active stereo network that uses the known illumination pattern in addition to the stereo inputs.
• We jointly learn optimal “Polka Lines” illumination patterns via differentiable end-to-end optimization, which can be specialized to speciﬁc illumination con-ditions.
• We validate the proposed method in simulation and with an experimental prototype. We demonstrate ro-bust depth acquisition across diverse scene scenarios from low light to strong illumination. 2.