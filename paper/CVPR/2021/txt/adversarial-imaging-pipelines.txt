Abstract
Adversarial attacks play a critical role in understand-ing deep neural network predictions and improving their robustness. Existing attack methods aim to deceive convo-lutional neural network (CNN)-based classiﬁers by manip-ulating RGB images that are fed directly to the classiﬁers.
However, these approaches typically neglect the inﬂuence of the camera optics and image processing pipeline (ISP) that produce the network inputs. ISPs transform RAW mea-surements to RGB images and traditionally are assumed to preserve adversarial patterns.
In fact, these low-level pipelines can destroy, introduce or amplify adversarial pat-terns that can deceive a downstream detector. As a result, optimized patterns can become adversarial for the classiﬁer after being transformed by a certain camera ISP or optical lens system but not for others.
In this work, we examine and develop such an attack that deceives a speciﬁc cam-era ISP while leaving others intact, using the same down-stream classiﬁer. We frame this camera-speciﬁc attack as a multi-task optimization problem, relying on a differentiable approximation for the ISP itself. We validate the proposed method using recent state-of-the-art automotive hardware
ISPs, achieving 92% fooling rate when attacking a speciﬁc
ISP. We demonstrate physical optics attacks with 90% fool-ing rate for a speciﬁc camera lens. 1.

Introduction
Deep neural networks have become a cornerstone method in computer vision [7, 20, 21, 25, 58] with diverse applications across ﬁelds, including safety-critical percep-tion for self-driving vehicles, medical diagnosis, video se-curity, medical imaging and assistive robotics. Although a wide range of high-stakes applications base their decision making on the output of deep networks, existing deep mod-els have been shown to be susceptible to adversarial attacks on the image that the network ingests. Speciﬁcally, existing adversarial attacks perturb the input image with carefully designed patterns to deceive the model while being imper-ceptible to a human viewer [34, 41, 44, 48, 37, 52]. As such, understanding and exploring adversarial perturbations offer
Figure 1: We illustrate and show the camera-speciﬁc attack. The image is tampered such that it becomes only adversarial for a spe-ciﬁc camera pipeline, even when the three pipelines deploy the same classiﬁer. insights into the failure cases of today’s models and it allows researchers to develop defense methods and models that are resilient against proposed attacks [3, 33, 34, 42, 55].
Existing adversarial attacks ﬁnd post-capture adver-saries, tampering with the image after capture before it is input to the deep network. Recently, a number of attack methods have been demonstrated in the form of physical objects that are placed in real-world scenes to generate ad-versarial patterns by capturing images of the physical ob-jects [2, 14, 29]. The most successful methods for com-puting adversarial perturbations rely on network gradients to form adversarial examples [48, 18, 29, 36, 41, 4] for each input image, that struggle to transfer to other net-works or images [48, 32, 39]. Alternative approaches rely 16051
only on the network predictions [24, 38, 47] and use surro-gate networks [40] or gradient approximations [1]. All of these methods, both physical and synthetic attacks, assume that the camera image processing pipeline (ISP) preserves the attack pattern. Although modern image processing pipelines implement complex algorithms, such as tonemap-ping, sharpening and denoising [27, 28], which transform
RAW measurements to RGB images on embedded camera processors, the inﬂuence of this pipeline is ignored by exist-ing attack methods. Some of the processing blocks in cam-era image processing pipelines have even been suggested as defenses against existing attacks [19, 31].
In this work, we close this gap between scene-based physical attacks and attacks on post-processed images.
Speciﬁcally, we propose a novel method that allows us to attack cameras with a speciﬁc ISP, while leaving the detec-tions of other cameras intact for the identical classiﬁer but a different ISP. As such, the attack mechanism proposed in this work is a camera-speciﬁc attack that not only targets the deep network but conventional hardware ISPs that tradi-tionally have not been considered susceptible to adversarial attacks. As a further camera-speciﬁc attack, we also attack the optical system of a camera. The proposed method can incorporate proprietary black-box ISP and complex com-pound optics, without accurate models, by relying on dif-ferentiable approximations as gradient oracles. We validate our method using recent automotive hardware ISP proces-sors and automotive optics, where the novel attack achieves a fooling rate of 92% on RAW images in experimental cap-tures.
Speciﬁcally, we make the following contributions
• We introduce the ﬁrst method for ﬁnding adversarial attacks that deceives a speciﬁc camera ISP and optics while leaving cameras with other ISPs or optics intact although they employ the same classiﬁer network.
• We demonstrate attacks for embedded hardware ISPs that are not differentiable and only available as black-box algorithms. To this end, we learn differentiable approximations of the image processing and sensing pipeline that serves as gradient oracles for our attack.
• We analyze and validate the attack on RAW input mea-surements for state-of-the-art hardware ISPs.
• We validate physical attacks of the proposed method on recent automotive camera ISPs and automotive op-tics, achieving more than 90% success rate. 2.