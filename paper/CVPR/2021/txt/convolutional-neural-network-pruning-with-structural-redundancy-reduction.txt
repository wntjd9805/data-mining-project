Abstract
Convolutional neural network (CNN) pruning has be-come one of the most successful network compression ap-proaches in recent years. Existing works on network prun-ing usually focus on removing the least important ﬁlters in the network to achieve compact architectures.
In this study, we claim that identifying structural redundancy plays a more essential role than ﬁnding unimportant ﬁlters, the-oretically and empirically. We ﬁrst statistically model the network pruning problem in a redundancy reduction per-spective and ﬁnd that pruning in the layer(s) with the most structural redundancy outperforms pruning the least impor-tant ﬁlters across all layers. Based on this ﬁnding, we then propose a network pruning approach that identiﬁes struc-tural redundancy of a CNN and prunes ﬁlters in the selected layer(s) with the most redundancy. Experiments on various benchmark network architectures and datasets show that our proposed approach signiﬁcantly outperforms the pre-vious state-of-the-art. 1.

Introduction
Convolutional neural networks (CNNs) [22] have devel-oped substantially in recent years and are widely used in various applications, such as object classiﬁcation [2, 21], image synthesis, [8, 42], super-resolution [4], and game-playing [34, 40]. State-or-the-art performance are achieved by designing wider and deeper CNNs [41, 13, 18]. How-ever, the over-parameterization problem of CNNs prevents them from being applied to resource-limited devices, such as mobile phones and robotics [39, 32]. Many approaches have been proposed to reduce the computation and storage cost of CNNs, such as quantization [10], matrix decomposi-tion [48], network pruning [11, 24, 46, 44, 15], and knowl-edge distillation [16]. Network pruning is one of the most popular methods and attracts enormous attention.
Generally, network pruning can be categorized into weight (unstructured) pruning [11] and channel (structured) pruning [24, 35, 43, 46]. Weight pruning zeros out spe-ciﬁc weights in ﬁlters and results in unstructured sparsities.
To accelerate the pruned CNNs, specialized hardware and software have to be developed [9]. Channel pruning, which removes the whole convolutional ﬁlters, is a more ﬂexible method without the need for special hardware. As the entire
ﬁlters are deleted, a considerable pruning ratio can usually be achieved with little performance degradation. Many of the existing channel pruning approaches rely on ﬁnding and pruning the least important ﬁlters, or the ﬁlters that share the most similarities with others across all layers [24, 35, 15, 3].
For example, [35] uses the Taylor series to estimate the loss change after each ﬁlter’s removal and prune the ﬁlters that cause minimal training loss change. It has been a common belief that with a better ﬁlter ranking criterion, there is a better chance to drop the least important ﬁlters and get a compact network with less performance loss.
However, our studies on channel pruning contradict this common belief. Using statistical modeling to measure the redundancy in each convolutional layer, we theoretically show that (in certain cases, even randomly) pruning ﬁlters in the layer with the most redundancy outperforms prun-ing the least important ﬁlters across all layers. To our best knowledge, this is the ﬁrst study that theoretically analyzes the rationale behind network pruning from a redundancy re-duction perspective. With this ﬁnding, we propose a layer-adaptive channel pruning approach based on structural re-dundancy reduction (SRR), which is achieved by establish-ing a graph for each convolutional layer of a CNN and us-ing two quantities associated with the graph, i.e., ℓ-covering number and quotient space size, as the measurement of the redundancy in each layer. After that, unimportant ﬁlters in the identiﬁed layer(s) with the most redundancy, rather than the least important ﬁlters across all layers, are pruned.
We summarize the contribution of this study as follows. (1) We theoretically analyze network pruning with statisti-cal modeling from a perspective of redundancy reduction.
We ﬁnd that pruning in the layer(s) with the most redun-dancy outperforms pruning the least important ﬁlters across all layers. (2) We propose a layer-adaptive channel pruning approach based on structural redundancy reduction, which 14913
builds a graph for each convolutional layer of a CNN to measure the redundancy existed in each layer. This ap-proach prunes unimportant ﬁlters in the most redundant layer(s), rather than the ﬁlters with the least importance across all layers. (3) We validate the proposed approach on various network architectures and datasets. Experiment re-sults demonstrate that our approach achieves state-of-the-art performance compared with recent channel pruning meth-ods. More speciﬁcally, our pruned ResNet50 model on Im-ageNet can reduce 44.1% FLOPs while losing only 0.37% top-1 accuracy. initialization. Surprisingly, comparable or even better per-formance can be achieved compared with ﬁne-tuning the pruned models. [33] reports that a network’s performance can be recovered even after random pruning. Related to these works, we also ﬁnd that pruning unimportant ﬁlters is not always essential. But beyond that, we theoretically show that pruning in the layers with large redundancy out-performs pruning the least important ﬁlters and propose to prune a network based on structural redundancy reduction. 3. A theoretic analysis of network pruning 2.