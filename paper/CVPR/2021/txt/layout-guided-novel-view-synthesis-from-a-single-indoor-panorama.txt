Abstract
Existing view synthesis methods mainly focus on the per-spective images and have shown promising results. How-ever, due to the limited Ô¨Åeld-of-view of the pinhole cam-era, the performance quickly degrades when large cam-In this paper, we make the era movements are adopted.
Ô¨Årst attempt to generate novel views from a single indoor panorama and take the large camera translations into con-sideration. To tackle this challenging problem, we Ô¨Årst use
Convolutional Neural Networks (CNNs) to extract the deep features and estimate the depth map from the source-view image. Then, we leverage the room layout prior, a strong structural constraint of the indoor scene, to guide the gen-eration of target views. More concretely, we estimate the room layout in the source view and transform it into the target viewpoint as guidance. Meanwhile, we also con-strain the room layout of the generated target-view images to enforce geometric consistency. To validate the effective-ness of our method, we further build a large-scale photo-realistic dataset containing both small and large camera translations. The experimental results on our challeng-ing dataset demonstrate that our method achieves state-of-the-art performance. The project page is at https:
//github.com/bluestyle97/PNVS. 1.

Introduction
With the popularity of 360‚ó¶ cameras, panoramas have been widely used in many emerging domains such as Vir-tual Reality (VR). In a typical VR application, the de-vice displays a 360‚ó¶ virtual scene, which can respond to 6 degree-of-freedom (DoF) head motion and give the user an immersive feeling. However, owing to the tedious im-age collection process, the panoramas are usually captured at a limited set of locations in practice, which restricts the
‚àóCorresponding author. source view target view source camera target camera
ùíï
Figure 1. Panoramic novel view synthesis. Our goal is to gener-ate a target-view panorama from the source-view panorama with camera translation t. The green, red, and blue lines represent the ceiling-wall boundaries, wall-wall boundaries, and Ô¨Çoor-wall boundaries of the room layout, respectively.
DoF of scene viewing. With the expectation of providing a free-viewpoint scene visualization experience, we make the Ô¨Årst attempt to address the problem of panoramic novel view synthesis from a single panorama.
In this paper, we constrain the panoramic view synthesis problem in the indoor scenario on account of its common-ness in typical applications. Previous work [30, 36, 39] has shown promising results on novel view synthesis from a sin-gle perspective image. However, the performance quickly degrades when larger camera rotations and translations are adopted. Due to the limited Ô¨Åeld-of-view (FoV) of a pinhole camera, it is arduous to extrapolate the large unseen areas caused by violent camera motion. In contrast, a panorama inherently supports the rotational viewpoint change. Thus, we only need to consider camera translations. Furthermore, 360‚ó¶ FoV provides omnidirectional information, making it possible to consider larger camera translations. By synthe-16438
sizing panoramic novel views, we can create new 360‚ó¶ con-tents to achieve 6-DoF scene viewing, which could poten-tially beneÔ¨Åt many applications, such as virtual house tours.
The main challenge of novel view synthesis lies in recov-ering the missing areas caused by viewpoint change, and the difÔ¨Åculty is ampliÔ¨Åed when considering large camera trans-lations. Fortunately, a panorama contains more structural information than a perspective image that can be exploited to reduce the difÔ¨Åculty. Previous work on image inpaint-ing [23, 28] has proven the effectiveness of structural in-formation to guide the content generation process. In the indoor scenario, the most common and easy-obtained struc-tural information is the room layout, i.e., the ceiling-wall boundaries, Ô¨Çoor-wall boundaries, and wall-wall bound-aries. The synthesized images have to keep the room layout reasonable, especially when large camera translations are adopted.
Inspired by the state-of-the-art view synthesis frame-work [39], we propose a novel method to tackle the panoramic view synthesis problem and exploit the room layout as a prior and geometric constraint. The proposed method is composed of three stages. In the Ô¨Årst stage, we use CNNs to extract a dense feature map, a depth map, and room layout from the source-view panorama. In the sec-ond stage, we transform the extracted feature map and room layout into the target view with a spherical geometric trans-formation process and fuse them to synthesize the target panorama.
In the Ô¨Ånal stage, we estimate the room lay-out of the synthesized panorama and enforce the estimated layout consistent with the transformed target-view layout in the preceding stage.
To validate the effectiveness of our method and facilitate the research on this novel task, we further build a large-scale photo-realistic dataset upon Structured3D dataset [53]. The rendered images are high-Ô¨Ådelity, making the dataset close to realistic application scenarios. Besides the typical set-tings of previous work, our dataset also considers large cam-era translations to push the boundaries of the view synthesis task. We split our dataset into an easy set and a hard set ac-cording to the camera translation. The easy set contains target panoramas with small camera translations ranging from 0.2 m to 0.3 m, including 13 080 training images and 1791 testing images. The hard set contains target panora-mas with large camera translations ranging from 1 m to 2 m, including 17 661 training images and 2279 testing images.
In summary, the main contributions of this paper are as follows: (i) We are the Ô¨Årst to tackle the problem of synthesizing panoramic novel views from a single indoor panorama. (ii) We propose a novel layout-guided method to tackle this challenging task, which is able to handle large camera translations. (iii) We build a new high-quality and challenging dataset for this novel task, which contains small and large camera translations. (iv) The experimental results demonstrate that our method achieves state-of-the-art per-formance on this novel task and can be generalized to real datasets. 2.