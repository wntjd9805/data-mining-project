Abstract
Cross-modal retrieval methods build a common repre-sentation space for samples from multiple modalities, typ-ically from the vision and the language domains. For im-ages and their captions, the multiplicity of the correspon-dences makes the task particularly challenging. Given an image (respectively a caption), there are multiple captions (respectively images) that equally make sense. In this paper, we argue that deterministic functions are not sufﬁciently powerful to capture such one-to-many correspondences. In-stead, we propose to use Probabilistic Cross-Modal Embed-ding (PCME), where samples from the different modalities are represented as probabilistic distributions in the com-mon embedding space. Since common benchmarks such as
COCO suffer from non-exhaustive annotations for cross-modal matches, we propose to additionally evaluate re-trieval on the CUB dataset, a smaller yet clean database where all possible image-caption pairs are annotated. We extensively ablate PCME and demonstrate that it not only improves the retrieval performance over its deterministic counterpart but also provides uncertainty estimates that render the embeddings more interpretable. Code is avail-able at https://github.com/naver-ai/pcme. 1.

Introduction
Given a query and a database from different modalities, cross-modal retrieval is the task of retrieving the database items which are most relevant to the query. Most research on this topic has focused on the image and text modali-ties [5, 9, 25, 51, 58]. Typically, methods estimate embed-ding functions that map visual and textual inputs into a com-mon embedding space, such that the cross-modal retrieval task boils down to the familiar nearest neighbour retrieval task in a Euclidean space [9, 51].
Building a common representation space for multiple modalities is challenging. Consider an image with a group of people on a platform preparing to board a train (Figure 1).
There is more than one possible caption describing this im-age. “People waiting to board a train in a train platform”
Figure 1. We propose to use probabilistic embeddings to rep-resent images and their captions as probability distributions in a common embedding space suited for cross-modal retrieval. These distributions gracefully model the uncertainty which results from the multiplicity of concepts appearing in a visual scene and im-plicitly perform many-to-many matching between those concepts. and “The metro train has pulled into a large station” were two of the choices from the COCO [5] annotators. Thus, the common representation has to deal with the fact that an image potentially matches with a number of different cap-tions. Conversely, given a caption, there may be multiple manifestations of the caption in visual forms. The multiplic-ity of correspondences across image-text pairs stems in part from the different natures of the modalities. All the different components of a visual scene are thoroughly and passively captured in a photograph, while language descriptions are the product of conscious choices of the key relevant con-cepts to report from a scene. All in all, a common repre-sentation space for image and text modalities is required to model the one-to-many mappings in both directions.
Standard approaches which rely on vanilla functions do not meet this necessary condition: they can only quan-tify one-to-one relationships [9, 51]. There have been at-tempts to introduce multiplicity. For example, Song and So-leymani [45] have introduced Polysemous Visual-Semantic 18415
Embeddings (PVSE) by letting an embedding function pro-pose K candidate representations for a given input. PVSE has been shown to successfully capture the multiplicity in the matching task and to improve over the baseline built upon one-to-one functions. Others [25] have computed re-gion embeddings obtained with a pre-trained object detec-tor, establishing multiple region-word matches. This strat-egy has led to signiﬁcant performance gains at the expense of a signiﬁcant increase in computational cost.
In this work, we propose Probabilistic Cross-Modal
Embedding (PCME). We argue that probabilistic map-ping is an effective representation tool that does not re-quire an explicit many-to-many representation as is done by detection-based approaches, and further offers a number of advantages. First, PCME yields uncertainty estimates that lead to useful applications like estimating the difﬁculty or chance of failure for a query. Second, the probabilistic representation leads to a richer embedding space where set algebras make sense, whereas deterministic ones can only represent similarity relations. Third, PCME is complemen-tary to the deterministic retrieval systems.
As harmful as the assumption of one-to-one correspon-dence is for the method, the same assumption has intro-duced confusion in the evaluation benchmarks. For exam-ple, MS-COCO [5] suffers from non-exhaustive annotations for cross-modal matches. The best solution would be to ex-plicitly and manually annotate all image-caption pairs for evaluation. Unfortunately, this process does not scale, espe-cially for a large-scale dataset like COCO. Instead, we pro-pose a smaller yet cleaner cross-modal retrieval benchmark using CUB [55] and more sensible evaluation metrics.
Our contributions are as follows. (1) We propose Proba-bilistic Cross-Modal Embedding (PCME) to properly rep-resent the one-to-many relationships in joint embedding spaces for cross-modal retrieval. (2) We identify shortcom-ings with existing cross-modal retrieval benchmarks and propose alternative solutions. (3) We analyse the joint em-bedding space using the uncertainty estimates provided by
PCME and show how intuitive properties arise. 2.