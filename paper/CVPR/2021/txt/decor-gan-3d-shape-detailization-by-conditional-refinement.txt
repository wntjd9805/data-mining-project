Abstract
We introduce a deep generative network for 3D shape de-tailization, akin to stylization with the style being geometric details. We address the challenge of creating large varieties of high-resolution and detailed 3D geometry from a small set of exemplars by treating the problem as that of geometric detail transfer. Given a low-resolution coarse voxel shape, our network reﬁnes it, via voxel upsampling, into a higher-resolution shape enriched with geometric details. The out-put shape preserves the overall structure (or content) of the input, while its detail generation is conditioned on an in-put “style code” corresponding to a detailed exemplar. Our 3D detailization via conditional reﬁnement is realized by a generative adversarial network, coined DECOR-GAN. The network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D PatchGAN discriminator to enforce local patches of the generated model to be similar to those in the training detailed shapes. During testing, a style code is fed into the generator to condition the reﬁnement. We demon-strate that our method can reﬁne a coarse shape into a va-riety of detailed shapes with different styles. The generated results are evaluated in terms of content preservation, plau-sibility, and diversity. Comprehensive ablation studies are conducted to validate our network designs. Code is avail-able at https://github.com/czq142857/DECOR-GAN. 1.

Introduction
Creating high-quality detailed 3D shapes for visual de-sign, AR/VR, gaming, and simulation is a laborious process that requires signiﬁcant expertise. Recent advances in deep generative neural networks have mainly focused on learning low-dimensional [1, 8, 18, 46, 47] or structural representa-tions [6, 26, 34, 43, 48] of 3D shapes from large collections of stock models, striving for plausibility and diversity of the generated shapes. While these techniques are effective at creating coarse geometry and enable the user to model rough objects, they lack the ability to represent, synthesize, and provide control over the ﬁner geometric details.
In this work, we pose the novel problem of 3D shape de-tailization, akin to stylization with the style deﬁned by geo-Figure 1: Our 3D detailization network, DECOR-GAN, re-ﬁnes a coarse shape (red, leftmost) into a variety of detailed shapes, each conditioned on a style code characterizing an exemplar detailed 3D shape (green, topmost). metric details. We wish to address the challenge of creating large varieties of high-resolution and detailed 3D geome-tries from only a small set of detailed 3D exemplars by treat-ing the problem as that of geometric detail transfer. Speciﬁ-cally, given a low-resolution coarse content shape and a de-tailed style shape, we would like to synthesize a novel shape that preserves the coarse structure of the content, while re-ﬁning its geometric details to be similar to that of the style shape; see Figure 1. Importantly, the detail transfer should not rely on any annotations regarding shape details.
Our conditional detailization task cannot be accom-plished by simply copying local patches from the style shape onto the content shape, since (a) it is unknown which patches represent details; (b) it may be difﬁcult to integrate copied patches into the content shape to ensure consistency.
To this end, we train a generative neural network that learns detailization priors over a collection of high-resolution de-tailed exemplar shapes, enabling it to reﬁne a coarse in-stance using a detail style code; see top of Figure 1.
To date, there has been little work on generating high-resolution detailed 3D shapes. For example, surface-based representations that synthesize details on meshes [21, 28] 15740
lack the ability to perform topological changes (as in several plant detailization results in Figure 1) and, due to complex-ities of mesh analysis, do not consider large shape contexts, limiting their notion of details to homogeneous geometric texture. To allow topological variations while leveraging a simpler domain for analyzing the content shape, we choose voxel representations for our task. However, 3D grids do not scale well with high resolution, which necessitates a net-work architecture that effectively leverages limited capac-ity. Also, regardless of the representation, careful choices of network losses must be made to balance the conﬂicting goals of detailization and (coarse) content preservation.
To tackle all these challenges, we design DECOR-GAN, a novel generative adversarial network that utilizes a 3D
CNN generator to locally reﬁne a coarse shape, via voxel upsampling, into a high-resolution detailed model, and a 3D PatchGAN discriminator to enforce local patches of the generated model to be similar to those in the training de-tailed shapes. Our generator learns local ﬁlters with limited receptive ﬁeld, and thus effectively allocates its capacity to-wards generating local geometry. We condition our reﬁne-ment method on a single detail style code that provides an intuitive way to control the model while also ensuring that generated details are consistent across the entire shape. We train our approach adversarially, using masked discrimina-tors to simultaneously ensure that local details are stylisti-cally plausible and coherent, while the global shape (i.e., downsampled version of the detailized shape) still respects the coarse source shape. Our method can be trained even with a small number of detailed training examples (up to 64 for all of our experiments), since our convolutional genera-tor only relies on learning local ﬁlters.
We demonstrate that DECOR-GAN can be used to re-ﬁne coarse shapes derived by downsampling a stock model or neurally generated by prior techniques. The user can con-trol the style of the shape detailization either by providing a style code from an exemplar or by interpolating between existing styles. Thus, our technique offers a complementary latent space for details which could be used jointly with existing techniques that provide latent spaces for coarse shapes. We quantitatively evaluate our method using mea-sures for content preservation, plausibility, and diversity.
We also provide a comprehensive ablation study to validate our network designs, while demonstrating that simpler ap-proaches (such as only using a reconstructive loss) do not provide the same quality of generated details. 2.