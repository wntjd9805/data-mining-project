Abstract
Traditional scene graph generation methods are trained using cross-entropy losses that treat objects and relation-ships as independent entities. Such a formulation, however, ignores the structure in the output space, in an inherently structured prediction problem. In this work, we introduce a novel energy-based learning framework for generating scene graphs. The proposed formulation allows for efﬁ-ciently incorporating the structure of scene graphs in the output space. This additional constraint in the learning framework acts as an inductive bias and allows models to learn efﬁciently from a small number of labels. We use the proposed energy-based framework † to train existing state-of-the-art models and obtain a signiﬁcant performance im-provement, of up to 21% and 27%, on the Visual Genome
[9] and GQA [5] benchmark datasets, respectively. Fur-thermore, we showcase the learning efﬁciency of the pro-posed framework by demonstrating superior performance in the zero- and few-shot settings where data is scarce. 1.

Introduction
A scene graph is a graph-based representation of an im-age which encodes objects along with the relationships be-tween them. Such a representation allows for a comprehen-sive understanding of images that is useful in several vision applications, including visual question answering [5, 22], image captioning [3, 28] and scene synthesis [4, 6].
A typical scene graph generation model comprises of the object detection network, which extracts object regions and corresponding features, and a message passing network with nodes initialized with these region features and edges accounting for the potential relations among them. The features are reﬁned, through context aggregation, and then
†Code and pre-trained models available at https://github. com/mods333/energy-based-scene-graph.
* Work done while interning at Amazon. rock wave surfboard near o n h oldin g riding man h as arm h a s hair (b) Cross entropy based training  rock wave surfboard near s t a n in front of din g o n carryin g h as arm man h a s hair (a) Input Image  (c) Energy based training
Figure 1. Scene Graph Generation: Figure shows scene graphs generated by a VCTree [22] model trained using conventional cross-entropy loss (purple) and our proposed energy-based frame-work (green). We make two crucial observations. First, the model trained using cross-entropy loss is incapable of consistent struc-tural reasoning (riding is not possible given the rest of the graph). Second, the trained model tends to be biased, favoring more frequent relations (e.g., on). Our proposed energy-based framework is designed, and able, to address these shortcomings. classiﬁed to produce both object (node) and relation (edge) labels. These networks are often trained end-to-end by min-imizing individual cross-entropy losses on both sets of la-bels. A major drawback of such an approach is that quality of prediction (loss) is simply proportional to the number of correctly predicted labels and ignores the rich structure of the scene graph output space (e.g., correlation or exclusion among object and relation label sets). In addition, the im-balance in the number of training samples for the relations results in dominant relations being heavily favored, leading to biased relation prediction at test time [21].
Figure 1 (b) illustrates the scene graph generated by a model [22] trained using the cross-entropy loss. Both the 13936
aforementioned drawbacks are apparent in the output. First, the model predicts a relation <man, riding, wave>.
A simple examination of the rest of the scene graph re-veals that such a relationship is impossible given that the man is on a rock and holding a surfboard. Second, the model leans towards making generic relation predictions such as <man, on, rock> as opposed to more informa-tive alternatives, e.g., <man, standing on, rock>.
The origin of these issues can be identiﬁed by examin-ing the likelihood term. Cross-entropy based training treats objects (O) and relationships (R) in a scene graph as inde-pendent entities. This amounts to factorizing the likelihood of a scene graph (SG), given an image (I), as the product of the likelihoods for the individual objects and relations: log p(SG|I) = log p(oi|I) +
X i∈O
X j∈R log p(rj|I). (1)
Eq.(1) brings to light the underlying cause of the prob-lem highlighted above. First, during loss computation, the loss for each relation term is independent of the re-lations predicted in the rest of the scene graph. Thus an incorrect relation such as <man, riding, wave> is penalized the same as <man, behind, wave> ir-respective of the other relations (<man, on, rock>).
However, using common sense reasoning, we can deter-mine that <man, riding, wave> is highly improbable given <man, carrying, surfboard> and should be penalized heavily as opposed to a likely, albeit incorrect, relation behind. Second, due to the summation over in-dividual relation terms, the model, in order to minimize the loss, is incentivized to predict relations which are more common in the training data.
While prior works have tried to address the issue of bi-ased predictions [14, 21] in the context of scene graph gen-eration, little progress has been made towards structured learning of scene graphs. In this work, we address both of these issues by proposing a novel generic loss formulation that incorporates the structure of scene graphs into the learn-ing framework using an energy-based learning framework.
This energy-based framework relies on graph message pass-ing algorithm for energy computation, that is learned to model the joint conditional density of a scene graph, given an image. Such a formulation transforms the problem from maximizing sum of the individual likelihood terms to that of directly maximizing the joint likelihood of the objects and relations. Furthermore, this added structure acts as an inductive bias for the learning, allowing the model to efﬁ-ciently learn relationship statistics from less data.
The proposed learning framework is general and hence can be used to train any off-the-shelf scene graph gener-ation model. We experiment with various state-of-the-art models and demonstrate that our energy-based formulation achieves signiﬁcant improvements in the performance over the corresponding models trained using the standard cross-entropy based formulation. We also demonstrate the en-hanced generalization capability of models trained using our framework by evaluating zero shot relation retrieval per-formance. Finally, we demonstrate the ability of our energy-based framework to learn from lesser amounts of training data by demonstrating an improvement in relative perfor-mance when evaluating on few-shot relation triplets.
Figure 1 (c), shows the scene graph generated by the pro-posed method. The generated scene graph is more granu-lar, predicting relations such as <man, standing on, rock> as opposed to the biased and generic variant <man, on, rock>. The model is also able to preclude improba-ble relations (e.g., <man, riding, wave>) and instead predicts in front of between man and wave.
Contribution: Our main contribution is a novel energy-based framework for scene graph generation that allows for direct incorporation of structure into the learning. We also propose a novel message passing algorithm that is used for computing the energy of scene graph conﬁgurations. This message passing algorithm is generic and can be used for other applications such as learning graph embeddings. Fi-nally, we demonstrate the efﬁcacy of our proposed frame-work by applying it to multiple state-of-the-art models and evaluating performance on two benchmark datasets - Visual
Genome [9] and GQA [5] - where we consistently outper-form the cross-entropy based counterparts by up to 21% on
Visual Genome and 27% on GQA. 2.