Abstract
Class center
The performance of face recognition system degrades when the variability of the acquired faces increases. Prior work alleviates this issue by either monitoring the face quality in pre-processing or predicting the data uncertainty along with the face feature. This paper proposes MagFace, a category of losses that learn a universal feature embed-ding whose magnitude can measure the quality of the given face. Under the new loss, it can be proven that the magni-tude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, Mag-Face introduces an adaptive mechanism to learn a well-structured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away.
This prevents models from overÔ¨Åtting on noisy low-quality samples and improves face recognition in the wild. Ex-tensive experiments conducted on face recognition, qual-ity assessments as well as clustering demonstrate its su-periority over state-of-the-arts. The code is available at https://github.com/IrvingMeng/MagFace. 1.

Introduction
Recognizing face in the wild is difÔ¨Åcult mainly due to the large variability exhibited by face images acquired in unconstrained settings. This variability is associated to the image acquisition conditions (such as illumination, back-ground, blurriness, and low resolution), factors of the face (such as pose, occlusion and expression) or biases of the deployed face recognition system [34]. To cope with these challenges, most relevant face analysis system under uncon-strained environment (e.g., surveillance video) consists of three stages: 1) face acquisition to select from a set of raw images or capture from video stream the most suitable face image for recognition purpose; 2) feature extraction to ex-tract discriminative representation from each face image; 3) facial application to match the reference image towards a given gallery or cluster faces into groups of same person.
Easy
Semi-hard
Hard
ùíç
ùúΩ
O
ùëô (b)
ùëêùëúùë†ùúÉ (a)
Figure 1: MagFace learns for (a) in-the-wild faces (b) a universal embedding by pulling the easier samples closer to the class center and pushing them away from the origin o. As shown in our exper-iments and supported by mathematical proof, the magnitude l be-fore normalization increases along with feature‚Äôs cosine distance to its class center, and therefore reveals the quality for each face.
The larger the l, the more likely the sample can be recognized.
To acquire the optimal reference image in the Ô¨Årst stage, a technique called face quality assessment [4, 24] is of-ten employed on each detected face. Although the ideal quality score should be indicative of the face recognition performance, most of early work [1, 2] estimates quali-ties based on human-understandable factors such as lumi-nances, distortions and pose angles, which may not directly favor the face feature learning in the second stage. Alter-natively, learning-based methods [4, 13] train quality as-sessment models with artiÔ¨Åcially or human labelled quality values. Theses methods are error-prone as there lacks of a clear deÔ¨Ånition of quality and human may not know the best characteristics for the whole systems.
To achieve high end-to-end application performances in the second stage, various metric-learning [25, 28] or classi-Ô¨Åcation losses [46, 23, 18, 38, 8] emerged in the past few years. These works learn to represent each face image as a deterministic point embedding in the latent space regard-less of the variance inherent in faces. In reality, however, low-quality or large-pose images like Fig. 1a widely exist and their facial features are ambiguous or absent. Given 14225
these challenges, a large shift in the embedded points is inevitable, leading to false recognition. For instance, per-formance reported by prior state-of-the-art [27] on IJB-C is much lower than LFW. Recently, conÔ¨Ådence-aware meth-ods [27, 6] propose to represent each face image as a Gaus-sian distribution in the latent space, where the mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. De-spite the performance improvement, these methods seek to separate the face feature learning from data noise modeling.
Therefore, additional network blocks are introduced in the architecture to compute the uncertainty level for each im-age. This complicates the training procedure and adds com-putational burden in inference. In addition, the uncertainty measure cannot be directed used in conventional metrics for comparing face features.
This paper proposes MagFace to learn a universal and quality-aware face representation. The design of MagFace follows two principles: 1) Given the face images of the same subject but in different levels of quality (e.g., Fig. 1a), it seeks to learn a within-class distribution, where the high-quality ones stay close to the class center while the low-quality ones are distributed around the boundary. 2) It should pose the minimum cost for changing existing infer-ence architecture to measure the face quality along with the computation of face feature. To achieve the above goals, we choose magnitude, the independent property to the direc-tion of the feature vector, as the indicator for quality assess-ment. The core objective of MagFace is to not only enlarge inter-class distance, but also maintain a cone-like within-class structure like Fig. 1b, where ambiguous samples are pushed away from the class centers and pulled to the origin.
This is realized by adaptively down-weighting ambiguous samples during training and rewarding the learned feature vector with large magnitude in the MagFace loss. To sum up, MagFace improves previous work in two aspects: 1. For the Ô¨Årst time, MagFace explores the complete set of two properties associated with feature vector, direc-tion and magnitude, in the problem of face recognition while previous works often neglect the importance of the magnitude by normalizing the feature. With exten-sive experimental study and solid mathematical proof, we show that the magnitude can reveal the quality of faces and can be bundled with the characteristics of recognition without any quality labels involved. 2. MagFace explicitly distributes features structurally in the angular direction (as shown in Fig. 1b). By dynam-ically assigning angular margins based on samples‚Äô hardness for recognition, MagFace prevents model from overÔ¨Åtting on noisy and low-quality samples and learns a well-structured distributions that are more suitable for recognition and clustering purpose. 2.