Abstract
In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similar-ity learns the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipu-lation. Our model can produce diverse and high-quality images with an unprecedented resolution at 10242. Us-ing a control mechanism based on style-mixing, our Tedi-GAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or with-out instance guidance.
To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the in-troduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN. 1.

Introduction
How to create or edit an image of the desired content without tedious manual operations is a difﬁcult but mean-∗Yujiu Yang is the corresponding author. This research was partially supported by the Key Program of National Natural Science Foundation of
China under Grant No. U1903213 and the Guangdong Basic and Applied
Basic Research Foundation (No. 2019A1515011387).
†Baoyuan Wu is supported by the Natural Science Foundation of China under Grant No. 62076213, the University Development Fund of the Chi-nese University of Hong Kong, Shenzhen under Grant No. 01001810, and the Special Project Fund of Shenzhen Research Institute of Big Data under grant No. T00120210003. s n o i t a r e p
O s u o u n i t n o
C
) a ( s i s e h t n y
S l a d o
M
-i t l u
M
) b (
This woman is  smiling. She has  short black hair.
He is a young man.
He is old.
He is wearing eyeglasses.
He is wearing  heavy makeup.
This young man  has red hair.
Figure 1. Our TediGAN is the ﬁrst method that uniﬁes text-guided image generation and manipulation into one same framework, leading to naturally continuous operations from generation to ma-nipulation (a), and inherently supports image synthesis with multi-modal inputs (b), such as sketches or semantic labels with or with-out instance (texts or real images) guidance. ingful task. To make image generation and manipulation more readily and user-friendly, recent studies have been fo-cusing on the image synthesis conditioned on a variety of guidance, such as sketch [9, 37], semantic label [11, 36], or textual description [26, 39]. Despite the success of its label and sketch counterparts, most state-of-the-art text-guided image generation and manipulation methods are only able to produce low-quality images [28, 8]. Those aiming at generating high-quality images from texts typically design a multi-stage architecture and train their models in a pro-gressive manner. To be more speciﬁc, there are usually three stages in the main module, and each stage contains a generator and a discriminator. Three stages are trained at the same time, and progressively generate images of three different scales, i.e., 642
→ 2562. The initial image with rough shape and color would be reﬁned to a high-resolution one. However, the multi-stage training pro-cess is time-consuming and cumbersome, making the afore-→ 1282 2256                    
StyleGAN inversion w1 wL
… mixing wv w1 wL
… wl wt wt*
… wt source
He is a young  man. text guidance wv wl w
He is a young man with  short black  hair.
Visual-linguistic similarity
W latent space of StyleGAN edited
Figure 2. Projecting Multi-Modal Embedding into the W Space of
StyleGAN. Taking visual and linguistic embedding for example, the left illustrates visual-linguistic similarity learning, where the visual embedding wv and linguistic embedding wl are expected to be close enough. The right demonstrates text-guided image ma-nipulation. Given a source image and a text guidance, we ﬁrst get their embedding wv and wl in W space through correspond-ing encoders. We then perform style mixing for target layers and get the target latent code wt. The ﬁnal wt∗ is obtained through instance-level optimization. The edited image can be generated from the StyleGAN generator. mentioned methods unfeasible for higher resolution. Fur-thermore, the pretrained text-image matching model they used fails to exploit attribute-level cross-modal information and leads to mismatched attributes when generating images from texts [39, 19, 47, 5], or undesired changes of irrelevant attributes when manipulating images [8, 26, 20, 21].
Recent progress on generative adversarial networks (GANs) has established an entirely different image gener-ation paradigm that achieves phenomenal quality, ﬁdelity, and realism. StyleGAN [16], one of the most notable GAN frameworks, introduces a novel style-based generator archi-tecture and can produce high-resolution images with un-matched photorealism. Some recent work [16] has demon-strated that the intermediate latent space W of StyleGAN, inducted from a learned piece-wise continuous mapping, yields less entangled representations and offers more fea-sible manipulation. The superior characteristics of W space appeal to numerous researchers to develop advanced GAN inversion techniques [38, 2, 1] to invert real images back into the StyleGAN’s latent space and perform meaningful manipulation. The most popular way [45, 29] is to train an additional encoder to map real images into the W space, which leads to not only faithful reconstruction but also se-mantically meaningful editing. Furthermore, it is easy to in-troduce the hierarchically semantic property of the W space to any GAN model by simply learning an extra mapping network before a ﬁxed, pretrained StyleGAN generator. We thoroughly investigated the existing GAN inversion meth-ods, and found all is about how to map images into the latent space of a well-trained GAN model. The other modalities like texts, however, have not received any attention.
In this paper, for the ﬁrst time, we propose a GAN in-version technique that can map multi-modal information, e.g., texts, sketches, or labels, into a common latent space of a pretrained StyleGAN. Based on that, we propose a very simple yet effective method for Text-guided diverse image generation and manipulation via GAN (abbreviated Tedi-GAN). Our proposed method introduces three novel mod-ules. The ﬁrst StyleGAN inversion module learns the inver-sion where an image encoder can map a real image to the
W space, while the second visual-linguistic similarity mod-ule learns linguistic representations that are consistent with the visual representations by projecting both into a common
W space, as shown in Figure 2. The third instance-level optimization module is to preserve the identity after edit-ing, which can precisely manipulate the desired attributes consistent with the texts while faithfully reconstructing the unconcerned ones. Our proposed method can generate di-verse and high-quality results with a resolution up to 10242, and inherently support image synthesis with multi-modal inputs, such as sketches or semantic labels with or without instance (texts or real images) guidance. Due to the uti-lization of a pretrained StyleGAN model, our method can provide the lowest effect guarantee, i.e., our method can al-ways produce pleasing results no matter how uncommon the given text or image is. Furthermore, to ﬁll the gaps in the text-to-image synthesis dataset for faces, we create the
Multi-Modal CelebA-HQ dataset to facilitate the research community. Following the format of the two popular text-to-image synthesis datasets, i.e., CUB [34] for birds and
COCO [22] for natural scenes, we create ten unique descrip-tions for each image in the CelebA-HQ [15]. Besides real faces and textual descriptions, the introduced dataset also contains the label map and sketch for the text-guided gener-ation with multi-modal inputs.
In summary, this work has the following contributions:
• We propose a uniﬁed framework that can generate di-verse images given the same input text, or text with image for manipulation, allowing the user to edit the appearance of different attributes interactively.
• We propose a GAN inversion technique that can map multi-modal information into a common latent space of a pretrained StyleGAN where the instance-level image-text alignment can be learned.
• We introduce the Multi-Modal CelebA-HQ dataset, consisting of multi-modal face images and correspond-ing textual descriptions, to facilitate the community. 2.