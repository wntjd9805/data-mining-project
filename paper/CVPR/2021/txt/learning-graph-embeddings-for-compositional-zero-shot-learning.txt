Abstract
In compositional zero-shot learning, the goal is to recog-nize unseen compositions (e.g. old dog) of observed visual primitives states (e.g. old, cute) and objects (e.g. car, dog) in the training set. This is challenging because the same state can for example alter the visual appearance of a dog drastically differently from a car. As a solution, we propose a novel graph formulation called Compositional Graph Em-bedding (CGE) that learns image features, compositional classiﬁers and latent representations of visual primitives in an end-to-end manner. The key to our approach is exploit-ing the dependency between states, objects and their com-positions within a graph structure to enforce the relevant knowledge transfer from seen to unseen compositions. By learning a joint compatibility that encodes semantics be-tween concepts, our model allows for generalization to un-seen compositions without relying on an external knowledge base like WordNet. We show that in the challenging gen-eralized compositional zero-shot setting our CGE signiﬁ-cantly outperforms the state of the art on MIT-States and
UT-Zappos. We also propose a new benchmark for this task based on the recent GQA dataset. Code is available at: https://github.com/ExplainableML/czsl 1.

Introduction
A “black swan” was ironically used as a metaphor in the 16th century for an unlikely event because the west-ern world had only seen white swans. Yet when the Eu-ropean settlers observed a black swan for the ﬁrst time in Australia in 1697, they immediately knew what it was.
This is because humans posses the ability to compose their knowledge of known entities to generalize to novel con-cepts. Since visual concepts follow a long tailed distribu-tion [43, 48], it is not possible to gather supervision for all concepts. Therefore, recognizing shared and discrimi-native properties of objects and reasoning about their vari-ous states has evolved as an essential part of human intel-ligence. Once familiar with the semantic meaning of these concepts, we can recognize unseen compositions of them without any supervision. While there is a certain degree
Figure 1: We aim to build a classiﬁer for a novel state of a known object (e.g. old dog) given the knowledge of the shared primitives state and object in the training set. of compositionality in modern vision systems, e.g. feature sharing, most models are not compositional in the classiﬁer space and treat every class as an independent entity requir-ing training for any new concept.
In this work, we study the state-object compositional-ity problem also known as Compositional Zero-Shot Learn-ing (CZSL)[34]. The goal is to learn the compositionality of observed objects and their states as visual primitives to generalize to novel compositions of them as shown in ﬁg-ure 1. Some notable existing works in this ﬁeld include learning a transformation network on top of individual clas-siﬁers [34], treating states as linear transformations of ob-ject vectors [35], learning modular networks conditioned on compositional classes [39] and learning object embeddings that are symmetric under different states [28]. However, these works treat each state-object composition indepen-dently, ignoring the rich dependency structure of different states, objects and their compositions. For example, learn-ing the composition old dog is not only dependent on the state old and object dog, but also can be supported by other compositions like cute dog, old car, etc. We argue that such dependency structure provides a strong reg-ularization which allows the network to better generalize to novel compositions. We therefore propose to exploit this dependency relationship by constructing a compositional graph to learn embeddings that are globally consistent.
Our contributions are as follows: (1) We introduce a novel graph formulation named Compositional Graph Em-bedding (CGE) to model the dependency relationship of vi-sual primitives and compositional classes. This graph can 953
be created independently of an external knowledge base like WordNet [32]. (2) Observing that visual primitives are dependent on each other and their compositional classes (ﬁgure 1), we propose a multimodal compatibility learning framework that learns to embed related states, objects and their compositions close to each other and far away from the unrelated ones. (3) We propose a new benchmark called C-GQA for the task of CZSL. This dataset is curated from the recent GQA[15] dataset with diverse compositional classes and clean annotations compared to datasets used in the com-munity. (4) Our model signiﬁcantly improves the state of the art on all the metrics on MIT-States, UT-Zappos and C-GQA datasets. 2.