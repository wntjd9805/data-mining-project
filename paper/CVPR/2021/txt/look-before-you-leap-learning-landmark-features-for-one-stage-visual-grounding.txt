Abstract
An LBYL (‘Look Before You Leap’) Network is proposed for end-to-end trainable one-stage visual grounding. The idea behind LBYL-Net is intuitive and straightforward: we follow a language’s description to localize the target ob-ject based on its relative spatial relation to ‘Landmarks’, which is characterized by some spatial positional words and some descriptive words about the object. The core of our LBYL-Net is a landmark feature convolution mod-ule that transmits the visual features with the guidance of linguistic description along with different directions. Con-sequently, such a module encodes the relative spatial po-sitional relations between the current object and its con-text. Then we combine the contextual information from the landmark feature convolution module with the target’s vi-sual features for grounding. To make this landmark feature convolution light-weight, we introduce a dynamic program-ming algorithm (termed dynamic max pooling) with low complexity to extract the landmark feature. Thanks to the landmark feature convolution module, we mimic the human behavior of ‘Look Before You Leap’ to design an LBYL-Net, which takes full consideration of contextual information.
Extensive experiments show our method’s effectiveness in four grounding datasets. Speciﬁcally, our LBYL-Net out-performs all state-of-the-art two-stage and one-stage meth-ods on ReferitGame. On RefCOCO and RefCOCO+, Our
LBYL-Net also achieves comparable results or even better results than existing one-stage methods. Code is available at https://github.com/svip-lab/LBYLNet. 1.

Introduction
Humans often refer to objects in an image by describing their relationships with other entities, e.g. “laptop on table”, and understanding their relationships is vital to compre-† Corresponding author
Query expression: the guy in brown on the right.
Figure 1. Illustration on how LBYL-Net uses contextual cues. On the left ﬁgure, the target location (green) perceives information
In this case, landmarks from landmarks (red) to localize itself. attend to the attribute brown to differ from the other guy. The right
ﬁgure shows our predicted result (blue box) and the ground-truth (yellow box). hend referential expressions. Visual grounding, aiming to localize the entities described by referential expressions, in-herently requires contextual information for grounding the target. By considering relationship of objects, several re-cent studies have achieved promising results [44, 24, 9, 43].
In particular, these methods usually leverage a two-stage paradigm, where they ﬁrst extract region proposals as can-didates and then rank the region-expression pairs as a way of metric learning.
Although effective, these two-stage methods have the (i) two stages bring time complexity, following defects: which hinders these methods from being real-time. (ii) since only objects in the pre-deﬁned categories are con-sidered, the contextual cues in the whole scene may not be fully exploited. Motivated by the success of one-stage detection [28, 19], one-stage based visual grounding has gained great interest, where the pipeline is simpliﬁed, and the inference is accelerated with a detecting and matching simultaneously paradigm [39, 29]. These detection-based one-stage approaches, however, still perform localization on grid features indivisually. The contextual information in the whole scene, especially relationships between objects, 16888
is not thoroughly investigated yet, making them inferior to their two-stage counterparts.
From this perspective, it is desirable to enable relation-ship modeling in one-stage visual grounding since the ob-ject requires perceiving the relational entities mentioned by the language to localize itself, e.g. “the chair with owl on it”. We enable the grid features to capture rich contextual cues for better localization by introducing the concepts of
Landmark Features and Landmark Feature Convolution.
To begin with, in our real life, we usually judge our lo-cation or positions of other buildings by using an easily no-ticed building, which is called Landmark. Similarly, in the image domain of visual grounding, the landmarks can be regarded as those locations that are helpful for object lo-calization. Figure 1 shows the visualization of landmarks in an image given the query language. These landmarks might fall on the background, other objects or the object itself to be located as long as they have helpful semantic cues. The network could extract the Landmark Features, which contains the global contextual information from these landmarks. To fully integrate this contextual information to improve the localization, these landmark features are propa-gated to the target object from different orientations to char-acterize relative positions by an efﬁcient dynamic program-ming algorithm, termed Dynamic Max Pooling. By aggre-gating landmark features with a standard convolution opera-tion, the grid features are equipped with (i) global receptive
ﬁeld (ii) direction-awareness. We call the whole process as
Landmark Feature Convolution.
Considering the long-range context, we propose a novel one-stage visual grounding framework. Our network ﬁrst applies feature pyramid network (FPN) [15] to extract vi-sual features of objects from different scales, of which ef-fectiveness has been proven for better object localization.
A landmark feature convolution is then employed to ex-tract contextual information of objects from different ori-entations, for a better characterizing relationship to objects mentioned by expressions. Since we mimic the ‘Look Be-fore You Leap’ behavior of us humans in visual grounding, we term our method as LBYL-Net.
We summarize our main contributions as follows:
• We propose a novel LBYL-Net for one-stage visual grounding, which combines the visual feature of ob-jects mentioned in the description as well as landmark features of the spatial relationships between different objects for target localization;
• A landmark features convolution is proposed, which has a global receptive ﬁeld but without introducing ex-tra parameter and complexity. We showcase it’s su-periority over related convolutional modules, i.e. di-lated convolution [41], deformable convolution [4] and
Non-Local module [34].
• Extensive experiments show the effectiveness and efﬁ-ciency of our LBYL-Net on four grounding datasets.
Especially, our method achieves state-of-the-art per-formance on ReferitGame. 2.