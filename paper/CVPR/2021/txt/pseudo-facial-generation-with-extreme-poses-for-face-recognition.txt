Abstract
Face recognition has achieved a great success in recent years, it is still challenging to recognize those facial im-ages with extreme poses. Traditional methods consider it as a domain gap problem. Many of them settle it by gen-erating fake frontal faces from extreme ones, whereas they are tough to maintain the identity information with high computational consumption and uncontrolled disturbances.
Our experimental analysis shows a dramatic precision drop with extreme poses. Meanwhile, those extreme poses just exist minor visual differences after small rotations. De-rived from this insight, we attempt to relieve such a huge precision drop by making minor changes to the input im-ages without modifying existing discriminators. A novel lightweight pseudo facial generation is proposed to relieve the problem of extreme poses without generating any frontal facial image. It can depict the facial contour information and make appropriate modiﬁcations to preserve the criti-cal identity information. Speciﬁcally, the proposed method reconstructs pseudo proﬁle faces by minimizing the pixel-wise differences with original proﬁle faces and maintaining the identity consistent information from their corresponding frontal faces simultaneously. The proposed framework can improve existing discriminators and obtain a great promo-tion on several benchmark datasets. 1.

Introduction
Face recognition aims to ﬁgure out the ground-truth identity for an unknown facial image. Inspired by the de-velopment of deep learning, the performance of face recog-nition has been improved rapidly [38, 40, 59, 60, 12, 10, 23].
LightCNN [47] is proposed as a superb model for face recognition recently and reaches new heights on several benchmark datasets. However, pose variations result in an unsolved problem in desired real-world applications. As is shown in Table 1, LightCNN recognizes faces well on the
*Jiwen Lu is the corresponding author.
+90˚
+75˚
--=
=
Figure 1. An illustration of the proposed pseudo facial generation.
From left to right, the input faces, residual faces and pseudo faces are visualized in sequence. +90◦ and +75◦ faces are respectively displayed in the ﬁrst and second lines.
Multi-PIE dataset [13] in the range of ±15◦ and ±45◦, and suffers a slight drop in ±60◦ and ±75◦. But for extreme poses such as ±90◦, the recognition rate declines dramati-cally, where the decrease is up to 35.09%.
Why does the recognition rate drop heavily when it comes to the extreme proﬁle faces? Deep learning methods are data-driven and usually extract features with a propen-sity [3]. Generally speaking, the frontal and proﬁle facial images lie in diverse domains. Models prefer to learn dis-criminative features from the dominant domain, and may fail in the subordinate domain. Both the imbalance of data distribution and domain gap devote to this aforementioned phenomenon, and researchers have tried several different paths to settle it [9]. The existing methods are mainly di-vided into two categories. One kind of them is to extract pose invariant embeddings from original faces to maintain the invariance [4, 30, 42]. The others rotate facial images to the frontal pose and recognize them directly [43, 52].
For those embedding based methods, metric learning and multi-view learning are usually applied to obtain pose in-variant embedding features. Florian et al. [36] train a CNN to optimize the desired embeddings directly rather than an intermediate bottleneck layer. Kan et al. [21] consider the imbalance of data distribution as large discrepancies be-tween views, and devise several adaptive sub-nets to release 1994
the discrepancies. Wu et al. [48] propose a couple deep learning approach to discover a shared feature subspace, and the heterogeneous recognition problem can be approx-imately considered as a homogeneous face matching. Con-sidering the imbalance of poses which may cause a long tail distribution problem, the performance of the obtained pose invariant embeddings are often unsatisﬁed.
For those rotation based methods, the ﬁrst attempt can date back to 1990s by Roberto et al. [1] and Alex et al.
[31]. Huang et al. [18] propose a Two-Pathway Genera-tive Adversarial Network (TP-GAN) to generate realistic frontal faces.
It is restricted by the huge computational consumption and the intention only for frontal images. Hu et al. [16] upgrade the TP-GAN to an arbitrary pose ro-tation as well as decreasing training and inference time.
But a GAN-based frontal generation method needs to learn plenty of parameters for approaching the ground-truth im-ages. Yin et al. [52] propose a multi-task problem to devise three sub-tasks: pose, illumination and emotion, then ad-just weights between the main recognition task and three sub-tasks adaptively. Although multi-task is a shortcut to achieve better performance, it needs hand-craft for adjust-ing weights of sub-task networks. Cao et al. [3] improve the proﬁle pose recognition rates by learning deep frontal resid-ual mappings. Compared with our proposed method, this method mainly focuses on the transform of learned residual features, whereas our method concentrates on generating pseudo proﬁle facial images from original inputs. Recently, a ﬂow-based method named FFWM [43] also frontalizes fa-cial images and reaches excellent scores on several bench-mark datasets. FFWM contains a Waro Attention Module (WAM) and an Illumination Preserving Module (IPM), that can synthesize realistic and illumination preserved frontal faces. Unlike those former pose invariant models, FFWM generates frontal faces by estimating ﬂows and can well rec-ognize facial images with extreme poses.
Different from the above face frontalization based meth-ods, we try to make minor pixel-wise changes to input fa-cial images. This innovation derives from the observation that there exists a dramatic precision drop between ±75◦ and ±90◦ in Table 1, but +75◦ and +90◦ facial images in Fig. 1 may not be visually distinguished apparently.
Compared with those traditional GAN-based methods, we reduce the number of parameters and ﬂops by designing a novel lightweight generator. As is shown in Fig. 1, residual images mainly describe facial contour information and the generated pseudo facial images can make appro-priate modiﬁcations to preserve the critical identity infor-mation. To illustrate its efﬁciency and expandable abil-ity, the lightweight generator is applied to LightCNN-29-v2 and relieve the aforementioned phenomenon successfully.
Our proposed framework can help any existing discrimina-tor obtain a great promotion on several benchmark datasets
Pose
±15◦
±30◦ ±45◦ ±60◦ ±75◦ ±90◦
Rank-1 100.00 100.00 99.94 98.83 92.91 57.82
Table 1. Rank-1 recognition rates (%) on the Multi-PIE dataset under Setting2 by the pretrained LightCNN-29-v2. without carrying too much burden. Quantitative and quali-tative experiments demonstrate the efﬁciency and effective-ness of our proposed method. Comparing with the baseline
LightCNN-29-v2, our method shows its appealing charm by its superb performance, and can be further added to any other high quality facial discriminator for a promising pro-motion. To conclude, our contributions can be summarized as the following points:
• We provide a novel, straight-forward and simple method to relieve the dramatic precision drop for ex-treme poses by generating pseudo proﬁle facial images under minor pixel-wise modiﬁcations rather than gen-erating fake frontalized faces.
• A lightweight pseudo proﬁle facial generator is pro-posed as the front-end input of any existing facial dis-criminator. The inherent identity information can be well preserved by the generator at a low computational consumption.
• Quantitative and qualitative experimental results con-ﬁrm that the proposed framework can perform better than the pre-trained discriminator. Speciﬁcally, it can achieve a surprising recognition rate of 93.68% for
±90◦ on the Multi-PIE dataset under Setting2. 2.