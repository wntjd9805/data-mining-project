Abstract
We present a novel method for predicting accurate depths from monocular images with high efﬁciency. This optimal efﬁciency is achieved by exploiting wavelet de-composition, which is integrated in a fully differentiable encoder-decoder architecture. We demonstrate that we can reconstruct high-ﬁdelity depth maps by predicting sparse wavelet coefﬁcients.
In contrast with previous works, we show that wavelet coefﬁcients can be learned without direct supervision on coefﬁcients. Instead we supervise only the ﬁnal depth im-age that is reconstructed through the inverse wavelet trans-form. We additionally show that wavelet coefﬁcients can be learned in fully self-supervised scenarios, without access to ground-truth depth. Finally, we apply our method to differ-ent state-of-the-art monocular depth estimation models, in each case giving similar or better results compared to the original model, while requiring less than half the multiply-adds in the decoder network. 1.

Introduction (a) Input color image – (320×1024) (b) Sparse estimation using wavelets. Our network up-samples and re-ﬁnes a 1/16-resolution depth map (bottom-right), by estimating wavelet coefﬁcients only in sparse regions.
Single-image depth estimation methods are useful in many real-time applications, for example robotics, au-tonomous driving and augmented reality. These areas are typically resource-constrained, so efﬁciency at prediction time is important.
Neural networks which estimate depth from a single im-age overwhelmingly use U-Net architectures, with skip con-nections between encoder and decoder layers [45]. Most work on single-image depth prediction has focused on improved depth accuracy, without focusing on efﬁciency.
Those that have cared about efﬁciency have typically bor-rowed tricks from the “efﬁcient network” world [24, 46] to make faster depth estimation, with the network using stan-dard convolutions all the way through [54, 41]. All these
∗Work done during an internship at Niantic. (c) Reconstruction of the output depth map using the inverse wavelet transform.
Figure 1: We can represent depth maps more efﬁciently with wavelets. Here the network takes image (a) as input and outputs a low resolution depth map, together with sparse wavelet coefﬁ-cients (b). We can reconstruct a high-resolution depth map (c) us-ing the inverse wavelet transform. In our model we predict multi-scale wavelet coefﬁcients with an image-to-image network, and we exploit sparseness of the output to save computation. approaches still use standard neural network components: convolutions, additions, summations and multiplications.
Inspired by sparse representations that can be achieved with wavelet decomposition, we propose an alternative net-work representation for more efﬁcient depth estimation, us-11089
ing wavelet decomposition. We call this system Wavelet-Monodepth. We make the observation that depth images of the man-made world are typically made up of many piece-wise ﬂat regions, with a few ‘jumps’ in depth between the
ﬂat regions. This structure lends itself well to wavelets. A low-frequency component can represent the overall scene structure, while the ‘jumps’ can be well captured in high-frequency components. Crucially, the high-frequency com-ponents are sparse, which means computation can be fo-cused only in certain areas. This has the effect of sav-ing run-time computation, while still enabling high-quality depths to be estimated.
To the best of our knowledge, we are the ﬁrst to train a single-image depth estimation network that reconstructs depth by predicting wavelet coefﬁcients. Furthermore, we show that our models can be trained with self-supervised loss on the ﬁnal depth signal, in contrast to other methods that directly supervise predicted wavelet coefﬁcients.
We evaluate on NYU and KITTI datasets, where we train supervised and self-supervised, respectively. We show that our approach allows us to effectively trade off depth accu-racy against runtime computation. 2.