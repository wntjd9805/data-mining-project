Abstract
Many adversarial attacks have been proposed to inves-tigate the security issues of deep neural networks. In the black-box setting, current model stealing attacks train a substitute model to counterfeit the functionality of the target model. However, the training requires querying the target model. Consequently, the query complexity remains high, and such attacks can be defended easily. This study aims to train a generalized substitute model called “Simulator”, which can mimic the functionality of any unknown target model. To this end, we build the training data with the form of multiple tasks by collecting query sequences generated during the attacks of various existing networks. The learn-ing process uses a mean square error-based knowledge-distillation loss in the meta-learning to minimize the dif-ference between the Simulator and the sampled networks.
The meta-gradients of this loss are then computed and ac-cumulated from multiple tasks to update the Simulator and subsequently improve generalization. When attacking a target model that is unseen in training, the trained Sim-ulator can accurately simulate its functionality using its limited feedback. As a result, a large fraction of queries can be transferred to the Simulator, thereby reducing query complexity. Results of the comprehensive experiments con-ducted using the CIFAR-10, CIFAR-100, and TinyImageNet datasets demonstrate that the proposed approach reduces query complexity by several orders of magnitude compared to the baseline method. The implementation source code is released online1. 1.

Introduction
Deep neural networks (DNNs) are vulnerable to adver-sarial attacks [3, 13, 38], which add human-imperceptible perturbations to benign images for the misclassiﬁcation of the target model. The study of adversarial attacks is crucial in the implementation of robust DNNs [28]. Adversarial at-tacks can be categorized into two types, namely, white-box
∗Corresponding author. 1https://github.com/machanic/SimulatorAttack
Figure 1: The procedure of the Simulator Attack, where q1 and q2 are the corresponding perturbations for generating query pairs in the attack (Algorithm 2). The queries of the
ﬁrst t iterations are fed into the target model to estimate the gradients. These queries and the corresponding outputs are collected to ﬁne-tune the Simulator, which is trained with-out using the target model. The ﬁne-tuned Simulator can ac-curately simulate the unknown target model, thereby trans-ferring the queries and improving overall query efﬁciency. and black-box attacks. In the white-box attack setting, the target model is fully exposed to the adversary. Thus, the per-turbation can be crafted easily by using gradients [4, 13]. In the black-box attack setting, the adversary only has partial information of the target model, and adversarial examples are crafted without any gradient information. Hence, black-box attacks (i.e., query- and transfer-based attacks) are more practical in real-world scenarios.
Query-based attacks focus on estimating gradients through queries [6, 40, 19, 20]. These attacks are considered highly effective because of their satisfactory attack success rate. However, despite their practical merits, high query complexity inevitably arises when estimating the approxi-mate gradient with high precision, resulting in costly pro-cedures. In addition, the queries are typically underutilized, 11835
i.e., the implicit but profound messages returned from the target model are overlooked, because they are abandoned after estimating the gradients. Thus, how to make full use of the feedback of the target model to enhance the query efﬁciency of attacks should be thoroughly investigated.
Transfer-based attacks generate adversarial examples by using a white-box attack method on a source model to fool the target model [24, 32, 10, 18]. Transfer-based attacks have two disadvantages: (1) they cannot achieve a high suc-cess rate, and (2) they are weak in a targeted attack. To improve transferability, model stealing attacks train a local substitute model to mimic the black-box model using a syn-thetic dataset, in which the labels are given by the target model through queries [39, 35, 33]. In this way, the differ-ence between the substitute and the target model is mini-mized, resulting in an increased attack success rate. How-ever, such a training requires querying the target model.
Consequently, the query complexity increases and such at-tacks can be defended easily by deploying a defense mecha-nism (e.g., [34, 23]). Furthermore, the inevitable re-training to substitute a new target model is an expensive process.
Hence, how to train a substitute model without the tar-get model requirement is worthy of further exploration.
To eliminate the target model requirement in training, we propose a novel meta-learning-based framework to learn a generalized substitute model (i.e., “Simulator”) over many different networks, thereby exploiting their characteristics to achieve fast adaptation. Once trained and ﬁne-tuned, the
Simulator can mimic the output of any target model that is unseen in training, enabling it to eventually replace the target model (Fig. 1). Speciﬁcally, the intermediate queries of the real black-box attack are moved to the training stage, thus allowing the Simulator to learn how to distinguish the subtle differences among queries. All the training data are reorganized into a format consisting of multiple tasks. Each task is a small data subset consisting of a query sequence of one network. In this system, a large number of tasks allow the Simulator to experience the attacks of various networks.
We propose three components to optimize the general-ization. First, a query-sequence level partition strategy is adopted to divide each task into meta-train and meta-test sets (Fig. 2) that match the iterations of ﬁne-tuning and simulation in the attack, respectively (Fig. 1). Second, the mean square error (MSE)-based knowledge-distillation loss carries out the inner and outer loops of meta-learning. Fi-nally, the meta-gradients of a batch of tasks are computed and then aggregated to update the Simulator and improve generalization. These strategies well address the problem of the target model requirement during training. In the at-tack (named “Simulator Attack”), the trained Simulator is
ﬁne-tuned using the limited feedback of the unknown target model to accurately simulate its output, thereby transfer-ring its query stress (Fig. 1). Therefore, the feedback of the target model is fully utilized to improve query efﬁciency.
In the proposed approach, the elimination of target models in training poses a new security threat, i.e., the adversary with the minimal information about the target model can also counterfeit this model for a successful attack.
In this study, we evaluate the proposed method us-ing the CIFAR-10 [22], CIFAR-100 [22], and TinyIma-geNet [37] datasets and compare it with natural evolution strategies (NES) [19], Bandits [20], Meta Attack [12], ran-dom gradient-free (RGF) [31], and prior-guided RGF (P-RGF) [8]. Experimental results show that the Simulator At-tack can signiﬁcantly reduce query complexity compared with the baseline method.
The main contributions of this work are summarized as follows: (1) We propose a novel black-box attack by training a generalized substitute model named “Simulator”. The train-ing uses a knowledge-distillation loss to carry out the meta-learning between the Simulator and the sampled networks.
After training, the Simulator only requires a few queries to accurately mimic any target model that is unseen in training. (2) We identify a new type of security threat upon elim-inating the target models in training: the adversary with the minimal information about the target model can also coun-terfeit this model for achieving the query-efﬁcient attack. (3) By conducting extensive experiments using the
CIFAR-10, CIFAR-100, and TinyImageNet datasets, we demonstrate that the proposed approach achieves similar success rates as those of state-of-the-art attacks but with an unprecedented low number of queries. 2.