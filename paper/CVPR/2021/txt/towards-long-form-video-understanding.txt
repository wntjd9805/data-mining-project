Abstract
Our world offers a never-ending stream of visual stimuli, yet today’s vision systems only accurately recognize pat-terns within a few seconds. These systems understand the present, but fail to contextualize it in past or future events.
In this paper, we study long-form video understanding.
We introduce a framework for modeling long-form videos and develop evaluation protocols on large-scale datasets.
We show that existing state-of-the-art short-term models are limited for long-form tasks. A novel object-centric transformer-based video recognition architecture performs signiﬁcantly better on 7 diverse tasks. It also outperforms comparable state-of-the-art on the AVA dataset. 1.

Introduction
Our world tells an endless story of people, objects, and their interactions, each person with its own goals, desires, and intentions. Video recognition aims to understand this story from a stream of moving pictures. Yet, top-performing recognition models focus exclusively on short video clips, and learn primarily about the present — objects, places, shapes, etc. They fail to capture how this present connects to the past or future, and only snapshot a very limited ver-sion of our world’s story. They reason about the ‘what’,
‘who’, and ‘where’ but struggle to connect these elements to form a full picture. The reasons for this are two fold: First, short-term models derived from powerful image-based ar-chitectures beneﬁt from years of progress in static image recognition [8,72]. Second, many current video recognition tasks require little long-term temporal reasoning [36,39,61].
In this paper, we take a step towards leveling the playing
ﬁeld between short-term and long-term models, and study long-form video understanding problems (Fig. 1). First, we design a novel object-centric long-term video recog-nition model. Our model takes full advantage of current image-based recognition architectures to detect and track all objects, including people, throughout a video, but ad-ditionally captures the complex synergies among objects across time in a transformer-based architecture [74], called
Object Transformers. Tracked instances of arbitrary length
Figure 1. Long-Form Video Understanding aims at understand-ing the “full picture” of a long-form video. Examples include un-derstanding the storyline of a movie, the relationships among the characters, the message conveyed by their creators, the aesthetic styles, etc. It is in contrast to ‘short-form video understanding’, which models short-term patterns to infer local properties. along with their visual features form basic semantic ele-ments. A transformer architecture then models arbitrary interactions between these elements. This object-centric design takes inspiration from early work that builds space-time instance representations [5,14,22,87], but further con-siders more complex inter-instance interactions over a long span of time. The model can be trained directly for a spe-ciﬁc end-task or pre-trained in a self-supervised fashion similar to models in image recognition [10, 26, 53, 55, 70] and language understanding [13, 40, 45, 86].
Second, we introduce a large-scale benchmark, which comprises of 9 diverse tasks on more than 1,000 hours of video. Tasks range from content analysis to predicting user engagement and higher-level movie metadata. On these long-form tasks, current short-term approaches fail to per-form well, even with strong (Kinetics-600 [6], AVA [25]) pre-training and various aggregation methods.
Our experiments show that Object Transformers outper-form existing state-of-the-art methods on most of the long-form tasks, and signiﬁcantly outperform the current state-of-the-art on existing datasets, such as AVA 2.2. The videos we use are publicly available and free. 1884
2.