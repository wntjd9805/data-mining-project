Abstract
We address the problem of uncertainty calibration.
While standard deep neural networks typically yield uncal-ibrated predictions, calibrated conﬁdence scores that are representative of the true likelihood of a prediction can be achieved using post-hoc calibration methods. However, to date, the focus of these approaches has been on in-domain calibration. Our contribution is two-fold. First, we show that existing post-hoc calibration methods yield highly over-conﬁdent predictions under domain shift. Second, we intro-duce a simple strategy where perturbations are applied to samples in the validation set before performing the post-hoc calibration step. In extensive experiments, we demonstrate that this perturbation step results in substantially better cal-ibration under domain shift on a wide range of architectures and modelling tasks. 1.

Introduction 1.1. Towards calibrated classiﬁers
Due to their high predictive power, deep neural networks are increasingly being used as part of decision making sys-tems in real world applications. However, such systems require not only high accuracy, but also reliable and cali-brated uncertainty estimates. A classiﬁer is calibrated, if the conﬁdence of predictions matches the probability of be-ing correct for all conﬁdence levels [4]. Especially in safety critical applications in medicine where average case perfor-mance is insufﬁcient, but also in dynamically changing en-vironments in industry, practitioners need to have access to reliable predictive uncertainty during the entire life-cycle of the model. This means conﬁdence scores (or predictive un-certainty) should be well calibrated not only for in-domain
* Work done for Siemens AG predictions, but also under gradual domain drift where the distribution of the input samples gradually changes from in-domain to truly out-of-distribution (OOD). Such domain drift scenarios commonly include changes in object back-grounds, rotations, and imaging viewpoints [1]; Fig. 1.
ObjectNet
ImageNet
ObjectNet
Methods 
Confidence (correct pred.)
Confidence (incorrect pred.)
Uncalibrated ResNet 0.99 (doormat)
Baseline  calibration
Proposed calibration 0.99 (doormat) 0.99 (doormat) 0.91 (hanky) 0.85 (hanky) 0.45 (hanky)
Figure 1: Re-calibrated neural networks make over-conﬁdent predictions under domain drift. Left: Only for our approach conﬁdence scores match model accuracy (dashed red line) across all predictions; all other approaches are over-conﬁdent under domain shift. Middle: For Imagenet, all models make a correct prediction with high conﬁdence (left). Right: Under domain drift (different viewpoint; Ob-jectnet) all models make wrong predictions, but only our approach has a low conﬁdence score reﬂecting model un-certainty.
Since deep neural networks typically only yield uncali-brated conﬁdence scores, a variety of different post-hoc cal-ibration approaches have been proposed [15, 4, 23, 22, 24].
These methods use the validation set to transform predic-tions returned by a trained neural network such that in-domain predictions are well calibrated. Such post-hoc un-certainty calibration approaches are particularly appealing since costly training of intrinsically uncertainty-aware neu-10124
ral networks can be avoided.
Current efforts to systematically quantify the quality of pre-dictive uncertainties have focused on assessing model cali-bration for in-domain predictions. Here, post-processing in form of temperature scaling has shown great promise and
Guo et al. [4] illustrated that this approach yields well cal-ibrated predictions for a wide range of model architectures.
More recently, more complex combinations of paramet-ric and non-parametric methods have been proposed [24].
However, little attention has been paid to uncertainty cal-ibration under domain drift and no comprehensive analy-sis of the performance of post-hoc uncertainty calibration methods under domain drift exists. 1.2. Contribution
In this work we focus on the task of post-hoc uncertainty calibration under domain drift scenarios and make the fol-lowing contributions:
• We ﬁrst show that neural networks yield overconﬁdent predictions under domain shift even after re-calibration using existing post-hoc calibrators.
• We generalise existing post-hoc calibration methods by transforming the validation set before performing the post-hoc calibration step.
• We demonstrate that our approach results in substan-tially better calibration under domain shift on a wide range of architectures and image data sets.
In addition to the contributions above, our code is made available at https://github.com/tochris/ calibration-domain-drift. 2.