Abstract
Weakly supervised temporal action localization aims to detect and localize actions in untrimmed videos with only video-level labels during training. However, without frame-level annotations, it is challenging to achieve localization completeness and relieve background interference. In this paper, we present an Action Unit Memory Network (AUMN) for weakly supervised temporal action localization, which can mitigate the above two challenges by learning an ac-tion unit memory bank.
In the proposed AUMN, two at-tention modules are designed to update the memory bank adaptively and learn action units speciﬁc classiﬁers. Fur-thermore, three effective mechanisms (diversity, homogene-ity and sparsity) are designed to guide the updating of the memory network. To the best of our knowledge, this is the
ﬁrst work to explicitly model the action units with a mem-ory network. Extensive experimental results on two stan-dard benchmarks (THUMOS14 and ActivityNet) demon-strate that our AUMN performs favorably against state-of-the-art methods. Speciﬁcally, the average mAP of IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is sig-niﬁcantly improved from 47.0% to 52.1%. 1.

Introduction
Temporal action localization (TAL) is an important yet challenging task for video understanding.
Its goal is to localize temporal boundaries of actions with speciﬁc cat-egories in untrimmed videos [13, 7]. Because of its broad applications in high-level tasks such as video surveil-lance [40], video summarization [17], and event detec-tion [15], TAL has recently drawn increasing attentions from the community. Up to now, deep learning based meth-ods have made impressive progresses in this area. How-ever, most of them handle this task in a fully supervised way, requiring massive temporal boundary annotations for actions [24, 51, 5, 42, 36]. Such manual annotations are ex-∗Corresponding Author (a) Illustration of “Sharing Units” characteristic. The
Figure 1. running (red box) is a shared action unit among high-jump, long-jump and cricket-bowling. (b) Illustration of “Sparsity” character-istic. An action usually occupies a small portion of untrimmed videos. (c) Illustration of “Smoothness” characteristic. CAS1 is more suitable for the action localization task because the CAS2 tends to divide a continuous action into multiple instances. pensive to obtain, which limits the development potential of fully-supervised methods in real-world scenarios.
To relieve this problem, the weakly supervised set-ting that only requires video-level category labels is pro-posed [37, 55, 39, 37, 55, 53, 29, 30, 45, 46].
It can be formulated as a multiple instance learning problem, where a video is treated as a bag of multiple segments and fed into a video-level classiﬁer to get a class activa-tion sequence (CAS). There are two primary challenges, named localization completeness and background interfer-ence. To solve the ﬁrst challenge, previous works usually adopt a well-designed erasing strategy [37, 55, 53] or a 9969
multi-branch architecture [21]. Both of them aim to force the model to concentrate on different parts of videos and hence discover the whole action without missing any rel-evant segments. To handle the second challenge, some methods [31, 28, 12] employ an attention-based per-class feature aggregation scheme, where the class-speciﬁc atten-tion is obtained by normalizing the CAS along the temporal axis. This scheme helps learn a compact intra-class fea-ture, which enables action segments to be more discrimi-native than the background. Furthermore, one popular way to handle both challenges is to learn a class-agnostic atten-tion mechanism [29, 30, 16, 34, 10], to highlight action seg-ments and suppress background segments.
By studying all previous TAL methods, we sum up the following three important observations (i.e., TAL Proper-ties): (1) Sharing Units. An action to be detected generally consists of some primary action units, which can be shared with other action classes. For example, as shown in Fig-ure 1 (a), a high-jump contains running and jumping up-ward while a long-jump consists of running and jumping forward, so running is a shared action unit. (2) Sparsity. In general, only a sparse set of video segments contains the meaningful target actions. As we can see from Figure 1 (b), an action only occupies a small portion of the video. (3) Smoothness. A smooth CAS is required for localiza-tion, because an action is continuous, as shown in Figure 1 (c). These three characteristics are critical for the success of action localization. Unfortunately, they have not been thor-oughly addressed by previous studies. To achieve accurate and complete action localization, these three observations should be taken into consideration when designing an action localization model. However, with only video-level labels, it is difﬁcult to model them jointly in a uniﬁed model.
To fully leverage the above three characteristics for ac-tion localization, we propose a novel end-to-end frame-work, called Action Unit Memory Network (AUMN), for more effective weakly supervised action localization. Our framework starts with the action unit templates learning.
According to the “Sharing Units” characteristic, we de-sign a sub-network as a memory bank of action unit tem-plates, which serve as our learning primary for action lo-calization. To exploit the templates for action classiﬁcation, we further design a Multi-Layer Perceptron (MLP) network to embed each template into the action class space. Ba-sically, the MLP network helps connect templates to ac-tion classes. Intuitively speaking, action unit templates will be projected onto a set of action classiﬁers. Afterwards, a cross-attention module is proposed to compute the relation-ships between a video segment and all templates. And ac-cording to the “Smoothness” characteristic, we introduce a self-attention module to compute the relationships between different segments in a video for aggregating context infor-mation. Leveraging both of the attention mechanisms, we can get reﬁned segment features and be able to dynamically select action classiﬁers for each video segment, which in turn, simultaneously contribute the adaptive learning of the memory bank.
However, the video-level ground-truth supervision alone is not enough for memory updating. Based on the property of action units and “Sparsity” characteristic, we further de-sign three effective mechanisms to guide the updating of the memory bank: (1) Since action units are different from each other, each template in the memory bank should be unique.
To achieve this goal, we design a diversity mechanism to encourage the differences among the templates in the mem-ory. (2) While the diversity mechanism can encourage each template in the memory to be unique, it does not guaran-tee that no template is useless, which means that a template may have low similarities with all video segments. To avoid this, we design a homogeneity mechanism to encourage a uniform distribution for the occurring probability of tem-plates. (3) In an untrimmed video, action segments only occupy a small portion of the whole video, and most of the video segments are background. Thus we design a sparsity mechanism to encourage that only a sparse set of video seg-ments can have high similarities with the templates in the memory. These three mechanisms together with the super-vision of video-level category label can guide the network to learn meaningful action units.
To sum up, the main contributions of our work are three-fold: (1) To the best of our knowledge, we are the ﬁrst to model the action units with a memory network for the (2) We propose two atten-weakly supervised TAL task. tion modules to ensure our memory to update adaptively and learn action units speciﬁc classiﬁers. Further, three ef-fective mechanisms (diversity, homogeneity and sparsity) (3) Extensive exper-are designed to guide the updating. imental results on two challenging benchmarks including
THUMOS14 [13] and ActivityNet [3] demonstrate that the proposed AUMN performs favorably against state-of-the-art weakly supervised TAL methods. 2.