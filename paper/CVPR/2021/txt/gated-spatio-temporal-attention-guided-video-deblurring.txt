Abstract
Video deblurring remains a challenging task due to the complexity of spatially and temporally varying blur. Most of the existing works depend on implicit or explicit alignment for temporal information fusion, which either increases the computational cost or results in suboptimal performance due to misalignment.
In this work, we investigate two key factors responsible for deblurring quality: how to fuse spatio-temporal information and from where to collect it.
We propose a factorized gated spatio-temporal attention module to perform non-local operations across space and time to fully utilize the available information without de-pending on alignment. First, we perform spatial aggre-gation followed by a temporal aggregation step. Next, we adaptively distribute the global spatio-temporal informa-tion to each pixel.
It shows superior performance com-pared to existing non-local fusion techniques while being considerably more efﬁcient. To complement the attention module, we propose a reinforcement learning-based frame-work for selecting keyframes from the neighborhood with the most complementary and useful information. Moreover, our adaptive approach can increase or decrease the frame usage at inference time, depending on the user’s need. Ex-tensive experiments on multiple datasets demonstrate the superiority of our method. 1.

Introduction
Video deblurring, as a primary problem in the vision and graphics communities, strives to predict latent frames from a blurred sequence. The camera shake and high-speed mo-tion in dynamic scenes often generate unwanted blur and produce blurry videos. Such videos not only deteriorate the visual quality but also hinder some high-level vision tasks such as tracking [13, 18], video stabilization [17], etc. As more videos are taken using hand-held and onboard video capturing devices, this problem has received great attention in the last decade. The blur in videos is usually a conse-quence of several interwoven factors like camera shake, ob-ject motion, depth variations, etc.
Unlike single-image deblurring, video deblurring meth-ods can utilize additional information that exists across neighboring frames. Early methods relied on motion com-pensation of the input frames, either explicitly [24, 16, 12] or implicitly [37, 36], to aggregate information at a partic-ular location from adjacent frames. [24, 12, 3] ﬁrst com-pute optical ﬂow between a reference frame and neighbor-ing frames and then use the aligned observations to deblur the reference frame. [32] utilizes deformable convolution to align feature maps using learnable offsets. Although these alignment methods are intended for increasing the temporal coherence, they have several disadvantages: 1) they intro-duce extra parameters, calculations and training difﬁculty, and 2) incorrect alignment may lead to undesired artifacts.
Implicit handling of motion using recurrent networks or 3D convolution has its own drawbacks. 3D convolution [35] is computationally heavy and introduces a large number of parameters. For recurrent architectures, the assumption that all previous frames will be automatically aligned and fused in the hidden state remains a problem for frames with large displacement. It is not very easy to extract only the rele-vant information from a single combined state. Also, due to recurrent connections it is not feasible to process multiple frames in parallel.
In this work, we address two critical aspects of video deblurring: how to gather spatio-temporal information ef-fectively and from where to gather this information. The key intuition is that a blurred region in the current frame would probably have complementary information in a dis-tant frame. Finding the spatio-temporal relation is critical while fusing information as not all parts of the neighbor-ing frames are equally informative for restoring the current frame due to varying factors such as occlusions, motion, etc. Fusion of incorrect information adversely affects re-construction performance. We explore the need for non-local operations for spatio-temporal fusion. A non-local self-attention module aims at computing the correlations between all possible pixels within and across frames, which directly resonates with the current goal of spatio-temporal fusion. By nature, such a block does not require any align-ment steps. [30] introduced self-attention based transformer 7802
network for natural language processing, [33] showed a similar non-local approach for classiﬁcation and recogni-tion. However, extending such approaches to generation tasks is non-trivial. Despite its exceptional non-local pro-cessing capabilities, even simpler spatial self-attention can be hard to implement due to its large memory requirement for the image domain. For spatio-temporal operation in videos, it will become signiﬁcantly more expensive. [33] downsamples the input by a large scale factor, even to 7 × 7 feature maps. But, for generation tasks, where every pixel accuracy matters, such downsampling will be harmful. For general video deblurring, the input size can be arbitrary and quite large.
In this paper, we present a factorized spatio-temporal self-attention mechanism that contains the essential prop-erties of non-local processing in spatio-temporal domain while being much more efﬁcient. We formulate the entire non-local operation as the composition of three lightweight operators: spatial aggregation, temporal aggregation, and pixelwise adaptive distribution. It requires signiﬁcantly less memory (almost 90% for 5 × 128 × 128 patch) compared to existing non-local blocks for the same spatio-temporal size while providing superior performance. Further, we in-corporate feature-gating to put more focus on aggregating sharp features from the neighborhood. We discuss the de-tails in Section 3 and advantages over existing video deblur-ring methods in Section 2.
Next, to complement the spatio-temporal attention mod-ule, we delve deeper into a rather unexplored area of video restoration - ﬁnding the temporal locations to fuse informa-tion from. Earlier works use the immediate neighborhood of the current frame (for example, ±2 frames) or sequential frame processing for recurrent methods. These approaches assume that the immediate vicinity will contain all the re-quired information for restoring the current one. Applying a ﬁxed neighborhood to all frames is a sub-optimal design choice. Each frame to be deblurred has a distinct appear-ance, and different parts of the frame can have complemen-tary information beyond the typical ﬁxed temporal window.
As shown in the example in Fig. 1, the (t + 7)th frame is the most useful one for deblurring the text whereas (t+6)th frame contains sharper features of the person. Therefore, we could focus on these frames while skipping the unneces-sary ones. Albeit being intuitive, the simple solution of ex-tending the neighborhood size in existing works will hardly solve the problem. On the one hand, it will signiﬁcantly in-crease the amount of computations, and on the other, distant frames will create more issues due to misalignment and fu-sion of wrong information. Also, depending on the severity of the blur in the current frame, we would ideally want to look into a varying number of frames for efﬁcient utilization of the available computing resources.
In contrast to the commonly used one-size-ﬁts-all t t+6
Figure 1. Varying amount of blur across frames. t+7 scheme, we would like to make these decisions individu-ally per input frame. Based on this intuition, we present a new perspective for video deblurring by deciding on-the-ﬂy which frames from the neighborhood to use on a per-frame basis. Empirically, we set a maximum temporal window, beyond which we observe that the scene content changes signiﬁcantly to be of any use. We train a lightweight rein-forcement learning agent (referred to as the frame selection network) to pick a certain number of keyframes within this large window. We design a novel reward function that al-lows the agent to look for more frames for severely blurred frames and skip unnecessary processing for easier ones.
Further, to adapt to different applications, we design the frame selection network to take input from the user at infer-ence time and increase or decrease the number of neighbor-ing frames usage adaptively while providing the best possi-ble restoration performance.
To summarize, our contributions are
• We introduce a factorized spatio-temporal attention as an effective non-local information fusion tool for video deblurring task.
• To the best of our knowledge, our work is the ﬁrst to present an approach for ﬁnding the key-frames with the most relevant information for video deblurring. It signiﬁcantly boosts the restoration performance when coupled with the proposed attention module.
• Extensive experiments and analysis are presented on several video deblurring benchmarks to show state-of-the-art accuracy and interpretability achieved by our architecture. 1.1. Language 2.