Abstract
Traditional evaluation metrics for learned models that report aggregate scores over a test set are insufﬁcient for surfacing important and informative patterns of failure over features and instances. We introduce and study a method aimed at characterizing and explaining failures by identi-fying visual attributes whose presence or absence results in poor performance. In distinction to previous work that relies upon crowdsourced labels for visual attributes, we leverage the representation of a separate robust model to ex-tract interpretable features and then harness these features to identify failure modes. We further propose a visualization method aimed at enabling humans to understand the mean-ing encoded in such features and we test the comprehensi-bility of the features. An evaluation of the methods on the
ImageNet dataset demonstrates that: (i) the proposed work-ﬂow is effective for discovering important failure modes, (ii) the visualization techniques help humans to understand the extracted features, and (iii) the extracted insights can assist engineers with error analysis and debugging. 1.

Introduction
It is critically important to understand the failure modes of machine learning (ML) systems, especially when they are employed in high-stakes applications. Aggregrate metrics in common use capture summary statistics on failure. While reporting overall performance is important, gaining an un-derstanding of the speciﬁcs of failure is a core responsibility in the ﬁelding of ML systems and components. For exam-ple, we need to understand situations where a self-driving car will fail to detect a pedestrian even when the system has high overall accuracy. Similarly, it is important to under-stand for which features misdiagnosis is most probable in chest x-rays even if the model has higher overall accuracy than humans. Such situation-speciﬁc insights can guide the iterative process of model development and debugging.
Model performance can be wildly non-uniform for dif-*Work carried out during a research internship at Microsoft Research. ferent clusterings of instances and such heterogeneity is not reﬂected by standard metrics such as AUC or accuracy. For example, it was shown in [19] that a commercial model for emotion detection from facial expressions systematically failed for young children. Buolamwini et al. [5] found that gender detection in multiple commercial models had signif-icantly higher error rates for women with darker skin tone.
These examples highlight the importance of identifying nat-ural clusters in the data with high failure rates. However, practical problems with these approaches still remain: (a) they require an expensive and time-consuming collection of metadata by humans, and (b) visual attributes that machine learning procedures pay attention to can be very different from the ones humans focus on (see Appendix Section F).
To resolve these issues, we propose to leverage the in-ternal representation of a robust model [25] to generate the metadata. The key property that makes robust representa-tions useful is that the features can be visualized more eas-ily than for a standard model [14, 39]. Our method, named
Barlow1, is inspired from Fault Tree Analysis [23] in safety engineering and uses robust representations as a building block.2 We demonstrate the results on the ImageNet dataset
[12] and ﬁnd that it reveals two types of failures:
Spurious correlations: A spurious correlation is a feature that is causally unrelated to the desired class but is likely to co-occur with the same class in the training/test data. For example, food is likely to co-occur with plates. However, the absence of the food from a plate image should not result in misclassiﬁcation (see examples in Figures 1a, 1b and 1e).
Overemphasized features: An overemphasized feature is a feature that is causally related to the desired class but where the model gives excessive importance for classiﬁcation, dis-regarding the other relevant features, and is unable to make a correct prediction when that feature is absent from the im-age. For example, a model may be likely to fail on a purse image if the buckle is absent (see Figures 1c and 1d).
While determining the type of failure (spurious vs. causal but overemphasized feature) and formulating mitiga-tion steps remains a task that depends on human expertise, 1In honor of perceptual psychologist, Horace Barlow [3]. 2Code repository: https://github.com/singlasahil14/barlow 12853
(a) class: maillot feature: water error increase: +9.72% (b) class: monastery feature: greenery error increase: +24.84% (c) class: purse feature: buckle error increase: +10.94% (d) class: syringe feature: markings error increase: +14.99% (e) class: rhodesian feature: dog collar error increase: +10.91%
Figure 1: Failure modes discovered using the proposed methodology for a standard Resnet-50 neural network trained on
ImageNet. In the top row, red denotes the region that a speciﬁc feature is paying attention to. In the bottom row, we show the image generated by visually amplifying the same feature. We observe that, due to the presence of spurious correlations, the failure rate of the model increases signiﬁcantly on the relevant class. Additional examples in Appendix Section F.
Barlow assists practitioners in this process by efﬁciently identifying and providing visualizations of failure modes, showing how input characteristics correlate with failures.
For example, failures identiﬁed in Figure 1 suggest the fol-lowing interventions: (a) add images of maillot/monastery with diverse backgrounds, Rhodesian Ridgebacks with-out collar (b) mask overemphasized features (buckle from purse, markings from syringe) in the training set.
In summary, we provide the following contributions: 1. An error analysis framework for discovering critical fail-ure modes for a given model. 2. A feature extraction and visualization method based on robust model representations to enable humans to under-stand the semantics of a learned feature. 3. A large-scale crowdsourcing study to evaluate the ef-fectiveness of the visualization technique and the inter-pretability of robust feature representations. 4. A user study with engineers with experience using ma-chine learning for vision tasks to evaluate the effective-ness of the methodology for model debugging. 2.