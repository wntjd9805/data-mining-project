Abstract
OCR-based image captioning aims to automatically de-scribe images based on all the visual entities (both visual objects and scene text) in images. Compared with conven-tional image captioning, the reasoning of scene text is re-quired for OCR-based image captioning since the gener-ated descriptions often contain multiple OCR tokens. Ex-isting methods attempt to achieve this goal via encoding the OCR tokens with rich visual and semantic representa-tions. However, strong correlations between OCR tokens may not be established with such limited representations. In this paper, we propose to enhance the connections between
OCR tokens from the viewpoint of exploiting the geometri-cal relationship. We comprehensively consider the height, width, distance, IoU and orientation relations between the
OCR tokens for constructing the geometrical relationship.
To integrate the learned relation as well as the visual and semantic representations into a uniﬁed framework, a Long
Short-Term Memory plus Relation-aware pointer network (LSTM-R) architecture is presented in this paper. Under the guidance of the geometrical relationship between OCR to-kens, our LSTM-R capitalizes on a newly-devised relation-aware pointer network to select OCR tokens from the scene text for OCR-based image captioning. Extensive experi-ments demonstrate the effectiveness of our LSTM-R. More remarkably, LSTM-R achieves state-of-the-art performance on TextCaps, with the CIDEr-D score being increased from 98.0% to 109.3%. 1.

Introduction
The task of OCR-based image captioning is to describe images with sentences based on the visual entities (visual objects and scene text) contained in images. Despite the signiﬁcant improvement made in conventional image cap-tioning [35, 38, 30, 4, 41] and Optical Character Recogni-*Corresponding author.
Dynamic pointer net h
Embed
OCR
Embed
Bilinear s
Relation-aware pointer net h
Embed
Bilinear s
OCR
Embed
Embed geo-rel
Embed
Concatenation (a) Without geo-rel:
A white van with the words  yes more federal airlines, on it. (b) With geo-rel:
A white van that says do pay  more federal income taxes.
Figure 1. Caption examples from the models with or without ge-ometrical relationship (geo-rel for short) and comparison between the conventional dynamic pointer network (e.g. [31]) and the pro-posed relation-aware pointer network. h and s denote the output state of the LSTM and the scores of the OCR tokens, respectively. tion (OCR) [9, 12, 19, 23, 33] in recent years, OCR-based image captioning still faces challenges and leaves much to be desired. To incorporate scene text for generating image descriptions, OCR-based image captioning needs to under-stand the words, recognize the visual entities of different modalities and reason among them.
Among the modalities contained in OCR-based image captioning, the scene text, which is closely related to both visual content and language descriptions, is of vital impor-tance. As has been veriﬁed, the model would witness a dra-matic decline in the generation performance [31] if the in-formation from scene text was ignored. To achieve better understanding and reasoning of the scene text in images, existing research [31, 37] jointly encodes visual feature, semantic feature and spatial feature of OCR tokens for a richer representation. Although promising results are ob-tained, the produced descriptions may still encounter prob-lems about the OCR tokens. For example, existing meth-ods (Figure 1(a)) may describe the upper-left image as “a white van with the words yes more federal airlines on it”, which reads the OCR tokens in an incorrect order. This in-1306
dicates that the relations between the OCR tokens learned by existing methods may be imprecise. One of the pos-sible causes is that the visual feature and semantic feature could not always reﬂect the correlations between the tokens.
This is because the spatially-adjacent OCR tokens may be semantically-unrelated or visually-unrelated to each other.
In this case, the communication between the OCR tokens is more likely to lie in the size relations and position relations, which are collectively known as the geometrical relation-ship. Unfortunately, existing methods [31, 37] are limited in the exploration of geometrical relationship. M4C-Captioner
[31] directly uses the bounding box coordinates of OCR to-kens as spatial feature. MMA-SR [37] tries to ﬁnd the next token for each OCR token according to the relative angles.
These practices can hardly provide strong geometrical rela-tionships for OCR tokens.
In this paper, we propose to improve OCR-based image captioning from the viewpoint of exploiting the geometrical relationship between OCR tokens. To establish the geomet-rical relationship, we consider the height/width difference, the IoU value, the shortest distance and the relative angle between OCR tokens. These elements constitute the intrin-sic relevance between OCR tokens and would be beneﬁcial for describing the images containing scene text. To take full advantage of the learned relation, we equip the caption-ing model with a newly-devised relation-aware pointer net-work, which selects OCR tokens under the guidance of the learned geometrical relationship. By this means, the model builds a more thorough understanding of the scene text and the OCR-centered problems appeared in the generated sen-tences are thus mitigated.
By consolidating the idea of incorporating the geomet-rical relationship between OCR tokens, we present a Long
Short-Term Memory plus Relation-aware pointer network (LSTM-R) architecture for OCR-based image captioning, as depicted in Figure 2. Speciﬁcally, the pretrained Faster
R-CNN [29] and OCR systems [1, 9] are ﬁrst adopted to detect object regions and OCR regions from the image. Af-ter that, the features of the objects and the OCR tokens are extracted. In the meantime, the geometrical relationships between OCR tokens are exploited and are encoded into
ﬁxed-length vectors. Given the features and the learned re-lation vectors, the image captioner selects a word from the common vocabulary or the OCR tokens according to their scores at each time step. Speciﬁcally, the relation-aware pointer network is exploited to determine the scores for the
OCR tokens under the guidance of the learned relation vec-tors. The entire model is optimized with the multi-label loss
In contrast to [31], we adopt in the teacher-forcing way. different target labels as the supervision, since the OCR to-ken appearing in captions may have multiple corresponding
OCR regions in the image.
In addition, considering that the words in captions are from different modalities (com-mon vocabulary or OCR tokens), we design a word encod-ing method to decide the type of each sentence word and further encode the words according to their types which en-courages the maximum use of the rich OCR representations and thus facilitates the model training.
The main contributions can be summarized as follows:
• We propose to explore the geometrical relationship be-tween OCR tokens which enables more thorough un-derstanding of OCR tokens as well as the entire image.
• We devise a relation-aware pointer network to select
OCR tokens under the guidance of the geometrical re-lationship, which also provides an elegant view of how to integrate the learned relationship for OCR-based im-age captioning.
• We conduct extensive experiments on the TextCaps dataset and achieve the state-of-the-art performance. 2.