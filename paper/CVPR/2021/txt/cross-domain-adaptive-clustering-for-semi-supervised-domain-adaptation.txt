Abstract
In semi-supervised domain adaptation, a few labeled samples per class in the target domain guide features of the remaining target samples to aggregate around them. How-ever, the trained model cannot produce a highly discrimi-native feature representation for the target domain because the training data is dominated by labeled samples from the source domain. This could lead to disconnection between the labeled and unlabeled target samples as well as mis-alignment between unlabeled target samples and the source domain. In this paper, we propose a novel approach called
Cross-domain Adaptive Clustering to address this problem.
To achieve both inter-domain and intra-domain adaptation, we ﬁrst introduce an adversarial adaptive clustering loss to group features of unlabeled target data into clusters and perform cluster-wise feature alignment across the source and target domains. We further apply pseudo labeling to unlabeled samples in the target domain and retain pseudo-labels with high conﬁdence. Pseudo labeling expands the number of “labeled" samples in each class in the target domain, and thus produces a more robust and powerful cluster core for each class to facilitate adversarial learn-ing. Extensive experiments on benchmark datasets, includ-ing DomainNet, Ofﬁce-Home and Ofﬁce, demonstrate that our proposed approach achieves the state-of-the-art perfor-mance in semi-supervised domain adaptation. 1.

Introduction
Semi-supervised domain adaptation (SSDA) is a vari-ant of the unsupervised domain adaptation (UDA) problem.
With a small number of labeled samples in the target do-main, SSDA has the potential to signiﬁcantly boost perfor-mance in comparison to UDA. In general, domain adapta-tion needs to reduce inter-domain gap (i.e., differences in feature distributions between two domains) and decrease intra-domain gap (i.e., differences among class-wise sub-*Corresponding Authors.
Label Source
Label Target
Source Cluster Core
Target Cluster Core
Unlabeled Target
Target with Pseudo-labels (a) (c) (b) (d)
Figure 1. Overview of our Cross-domain Adaptive Clustering (CDAC) approach. (a) Supervision on labeled data from both source and target domains to ensure partial cross-domain feature alignment. (b) Pseudo labeling for giving pseudo-labels on un-labeled target samples to form enhanced target cluster cores with higher robustness and power. (c) Minimization of the adversar-ial adaptive clustering loss for grouping features from the target domain into clusters. (d) Maximization of the adversarial adap-tive clustering loss to facilitate cross-domain cluster-wise feature alignment. distributions in the target domain) in order to achieve inter-domain adaptation and intra-domain adaptation simultane-ously [26].
Many existing domain adaptation approaches start with inter-domain adaptation, and guide their models to learn cross-domain sample-wise feature alignment [34, 6, 4], or
In the distribution-wise feature alignment [12, 22, 18]. semi-supervised learning setting, adversarial learning is employed in [32, 25] to improve sample-wise feature align-ment for inter-domain adaptation. However, such previous work ignores extra information indicated by class-wise sub-distributions in the target domain, and thus results in cross-domain feature mismatch during model training, thereby re-ducing model generalization performance on novel test data in the target domain.
Whereafter, much work on domain adaptation has turned to intra-domain adaptation [16, 13]. By optimizing class-wise sub-distributions within the target domain, the gen-eralization performance of adaptation models can be im-proved. In the context of semi-supervised domain adapta-tion, the presence of few labeled target samples is utilized to help features of unlabeled target samples from different 2505
classes be guided to aggregate in the corresponding clus-ters to form perfect class-wise sub-distributions in the target domain, which reduces the possibility of feature mismatch across domains. However, a model trained with supervision on few labeled target samples and labeled source data just can ensure partial cross-domain feature alignment because it only aligns the features of labeled target samples and their correlated nearby ones with the corresponding feature clus-ters in the source domain [17]. Also, the trained model can-not produce a highly discriminative feature representation for the target domain because the learned feature represen-tation is biased to the sample discrimination of the source domain due to the existence of a much larger scale of la-beled samples than those of the target domain [32]. These could lead to disconnection between the labeled and unla-beled target samples as well as misalignment between unla-beled target samples and the source domain.
In this paper, we propose a novel approach called Cross-domain Adaptive Clustering (CDAC), as Figure 1 shows, to address the aforementioned problem. It ﬁrst groups fea-tures of unlabeled target data into clusters and further per-forms cluster-wise feature alignment across the source and target domains rather than sample-wise or distribution-wise feature alignment. In this way, our approach achieves both inter-domain adaptation and intra-domain adaptation simul-taneously. More speciﬁcally, our proposed approach per-forms minimax optimization over the parameters of a fea-ture extractor and a classiﬁer. For intra-domain adaptation, the features of unlabeled target samples are guided by la-beled target samples to form clusters corresponding to the classes of labeled samples by minimizing an adversarial adaptive clustering loss with respect to the parameters of the feature extractor. For inter-domain adaptation, the classiﬁer is trained to maximize the same loss deﬁned on unlabeled target samples so that cluster-wise feature distribution in the target domain is aligned with the corresponding feature dis-tribution in the source domain.
In addition, we apply pseudo labeling to unlabeled sam-ples in the target domain and retain pseudo-labels with high conﬁdence. In the SSDA setting, since only a very small number (typically one or three) of target samples from each class are labeled, it is hard for such few samples to form a stable and accurate cluster core. Pseudo labeling expands the number of “labeled" samples in each class in the tar-get domain, and thus produces a more robust and power-ful cluster core for each class. Such an enhanced cluster core can attract unlabeled samples from the corresponding class towards itself in the target domain using the adversar-ial adaptive clustering loss. Therefore, our pseudo labeling technique assists adversarial learning, and helps our SSDA model reach higher performance. follows.
• We introduce an adversarial adaptive clustering loss to perform cross-domain cluster-wise feature alignment so as to solve the SSDA problem.
• We integrate an adapted version of pseudo labeling to enhance the robustness and power of cluster cores in the target domain to facilitate adversarial learning.
• Extensive experiments on benchmark datasets, in-cluding DomainNet [28], Ofﬁce-Home [39] and Of-ﬁce [31], demonstrate that our proposed CDAC ap-proach achieves the state-of-the-art performance in semi-supervised domain adaptation. 2.