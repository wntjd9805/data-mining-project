Abstract 1-bit detectors show great promise for resource-constrained embedded devices but often suffer from a sig-niﬁcant performance gap compared with their real-valued counterparts. The primary reason lies in the error during binarization. This paper presents a layer-wise searching (LWS) strategy to generate 1-bit detectors that maintain a performance very close to the original real-valued model.
The approach introduces angular and amplitude loss func-tions to increase detector capacity. At 1-bit layers, it ex-ploits a differentiable binarization search (DBS) to mini-mize the angular error in a student-teacher framework. We also learn the scale factor by minimizing the amplitude loss in the same student-teacher framework. Extensive experi-ments show that LWS-Det outperforms state-of-the-art 1-bit detectors by a considerable margin on the PASCAL VOC and COCO datasets. For example, the LWS-Det achieves 1-bit Faster-RCNN with ResNet-34 backbone within 2.0% mAP of its real-valued counterpart on the PASCAL VOC dataset. 1.

Introduction
Object detection is a fundamental task in computer vi-sion [8,22], and deep convolutional neural networks (CNN)
[31–33] dominate recent performance advancements. But
CNN models typically have millions of parameters and re-quire billions of ﬂoating-point operations (FLOPs) to com-pute, limiting their deployment on resource-limited plat-forms.
Substantial efforts have been made to compress and accelerate CNNs for efﬁcient online inference. Methods
*Baochang Zhang is the corresponding author.
Figure 1. Example layer-wise feature map distribution and detec-tion results of (a) a real-valued detector, (b) LWS-Det, and (c)
BiDet. We extract the feature maps of the ﬁrst, second, and ﬁ-nal binarized layers and illustrate their distributions based on the frequency-value histogram in rows 1-3. The last row shows the detection result. include compact network design [15, 27], network prun-ing [14, 19, 44], low-rank decomposition [7], quantization
[30, 41], and knowledge distillation [34]. Quantization is particularly suitable for deployment on AI chips because it reduces the bit-width of network parameters and activa-tions for efﬁcient inference. Binarization, an extreme form of quantization, compresses the weights and activations of
CNNs into a single bit, which can decrease the storage re-5682
Figure 2. Our LWS-Det. From left to right are the input, the search process, and the learning process. For a given 1-bit convolution layer, LWS-Det ﬁrst searches for the binary weight (+1 or -1) by minimizing the angular loss supervised by a real-valued teacher detector.
LWS-Det learns the real-valued scale factor α to enhance the feature representation ability. quirements by 32× and computation cost by up to 58× [30].
Binarized detectors have been an important contribution to object detection by speeding up the CNN feature extraction process to allow bounding box detection and object clas-siﬁcation in real-time [36, 38, 40]. For example, the 1-bit detectors can theoretically achieve 15× acceleration in an
SSD300 framework [24] with its purely logical computa-tion using XNOR operations on binary weights and binary activations. These are highly energy-efﬁcient for embed-ded devices and possess the potential for being deployed directly on next-generation memristor-based hardware.
Despite these attractive characteristics, the performance of 1-bit detectors typically degrades to the point where they are not broadly deployed on real-world embedded devices. For example, BiDet [38] only achieves 13.2% mAP@[.5, .95] on the COCO minival dataset [22], re-sulting in an accuracy gap of 10.0% below its real-valued counterpart (on SSD300 framework). The reason, we be-lieve, lies in the fact that the layer-wise binarization error signiﬁcantly affects 1-bit detector learning.
Fig. 1 shows the layer-wise feature map distribution and detection results of a real-valued detector, our LWS-Det, and BiDet [38] from left to right. The ﬁrst three rows show the distributions of feature maps. The feature map distribu-tion of BiDet has a variance less similar to the one of the real-valued detector, leading to a result with false positives and missed detection in the 4-th row. In comparison, our
LWS-Det can reduce the binarization error and provide bet-ter detection results.
In this paper, we present layer-wise search method to produce an optimized 1-bit detector (LWS-Det) using the student-teacher framework to narrow the performance gap.
As shown in Fig. 2, we minimize the binarization error by decoupling it into an angular error and an amplitude er-ror. We search for the binarized weight supervised by well-designed losses between real-valued convolution and 1-bit convolution under differentiable binarization search (DBS) framework, following the method DARTS [23, 45]. We for-mulate the binarization problem as the combination of −1 and 1, while a differentiable search can explore the binary space to signiﬁcantly improve the capacity of 1-bit detec-tors. To improve representation ability of LWS-Det, we de-sign two losses to supervise the 1-bit convolution layer from angular and amplitude perspective. In this way, we obtain a powerful 1-bit detector (LWS-Det) that can minimize both angular and amplitude errors in the same framework. Our contributions are summarized as:
• We introduce the differentiable search method for 1-bit detectors, which minimize the angular error under the supervision of the real-valued network.
• We learn the scale factor to minimize the amplitude loss based on the student-teacher framework. As a re-sult, the representation ability of our detector is signif-icantly improved.
• We evaluate our LWS-Det on the PASCAL VOC and the large-scale COCO datasets for a comprehensive comparison with state-of-the-art 1-bit detectors. The results show that our methods outperform other state-of-the-art BNN-based detectors by a sizable margin.
For example, on COCO, the 1-bit Faster-RCNN with
ResNet-50 backbone obtained by LWS-Det achieves 31.7% mAP, which outperforms all current 1-bit de-tectors. 2.