Abstract
Classical Fashion Search
Dialog-based Fashion Search
Conversational interfaces for the detail-oriented retail fashion domain are more natural, expressive, and user friendly than classical keyword-based search interfaces. In this paper, we introduce the Fashion IQ dataset to sup-port and advance research on interactive fashion image re-trieval. Fashion IQ is the ﬁrst fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual at-tribute labels for these images. We provide a detailed analy-sis of the characteristics of the Fashion IQ data, and present a transformer-based user simulator and interactive image retriever that can seamlessly integrate visual attributes with image features, user feedback, and dialog history, leading to improved performance over the state of the art in dialog-based image retrieval. We believe that our dataset will en-courage further work on developing more natural and real-world applicable conversational shopping assistants.1 1.

Introduction
Fashion is a multi-billion-dollar industry, with direct so-cial, cultural, and economic implications in the world. Re-cently, computer vision has demonstrated remarkable suc-cess in many applications in this domain, including trend forecasting [2], modeling inﬂuence relations [1], creation of capsule wardrobes [23], interactive product retrieval
[18, 68], recommendation [41], and fashion design [46]. In this work, we address the problem of interactive image re-trieval for fashion product search. High ﬁdelity interactive image retrieval, despite decades of research and many great strides, remains a research challenge. At the crux of the challenge are two entangled elements: empowering the user with ways to express what they want, and empowering the
* Equal contribution. 1Fashion IQ is available at: https://github.com/XiaoxiaoGuo/fashion-iq
Product Filtered by:
Red
White
Mini
Sleeveless
I want a mini sleeveless dress
I prefer stripes and more  covered around the neck
I want a little more red accent
Length
Short
Midi
Long
…
Color
Blue
White
Orange
…
Sleeves long 3/4
Sleeveless
…
Figure 1: A classical fashion search interface relies on the user selecting ﬁlters based on a pre-deﬁned fashion ontol-ogy. This process can be cumbersome and the search results still need manual reﬁnement. The Fashion IQ dataset sup-ports building dialog-based fashion search systems, which are more natural to use and allow the user to precisely de-scribe what they want to search for. retrieval machine with the information, capacity, and learn-ing objective to realize high performance.
To tackle these challenges, traditional systems have re-lied on relevance feedback [47, 68], allowing users to indi-cate which images are “similar” or “dissimilar” to the de-sired image. Relative attribute feedback (e.g., “more formal than these”, “shinier than these”) [33, 32] allows the com-parison of the desired image with candidate images based on a ﬁxed set of attributes. While effective, this speciﬁc form of user feedback constrains what the user can convey.
More recent work utilizes natural language to address this problem [65, 18, 55], with relative captions describing the differences between a reference image and what the user has in mind, and dialog-based interactive retrieval as a prin-cipled and general methodology for interactively engaging the user in a multimodal conversation to resolve their intent 11307
User Model / Relative Captioning
Single-shot Retrieval
“More ruffles on top  and is beige”
Relative 
Captioner
“The top has  stripes and is  long sleeved”
Retriever
Shopping Assistant
User model: “The  top has stripes and  is long sleeved”
Human: “The top is  orange in color and  more flowy”
Retriever
Retriever
…
“is strapless and more  fitted”
Fashion IQ
User model: “The  top has stripes and  is long sleeved”
Human: “The top is  orange in color  and more flowy”
User Model Feedback
Human Feedback
Tasks
User Model Feedback
Human Feedback
Application
Dialog-based Retrieval
Figure 2: Fashion IQ can be used in three scenarios: user modeling based on relative captioning, and single-shot as well as dialog-based retrieval. Fashion IQ uniquely provides both annotated user feedback (black font) and visual attributes derived from real-world product data (dashed boxes) for system training.
[18]. When empowered with natural language feedback, the user is not bound to a pre-deﬁned set of attributes, and can communicate compound and more speciﬁc details dur-ing each query, which leads to more effective retrieval. For example, with the common attribute-based interface (Fig-ure 1 left) the user can only deﬁne what kind of attributes the garment has (e.g., white, sleeveless, mini), however with interactive and relative natural language feedback (Figure 1 right) the user can use comparative forms (e.g., more cov-ered, brighter) and ﬁne-grained compound attribute descrip-tions (e.g., red accent at the bottom, narrower at the hips).
While this recent work represents great progress, several important questions remain. In real-world fashion product catalogs, images are often associated with side information, which in the wild varies greatly in format and information content, and can often be acquired at large scale with low cost. Descriptive representations such as attributes can of-ten be extracted from this data, and form a strong basis for generating stronger image captions [71, 66, 70] and more effective image retrieval [25, 5, 51, 34]. How such side in-formation interacts with natural language user inputs to im-prove the state of the art dialog-based image retrieval sys-tems are important open research questions. Furthermore, a challenge with implementing dialog-interface image search systems is that currently conversational systems typically require cumbersome hand-engineering and/or large-scale dialog data [35, 6]. In this paper, we investigate the extent to which side information can alleviate these issues, and ex-plore ways to incorporate side information in the form of visual attributes into model training to improve interactive image retrieval. This represents an important step toward the ultimate goal of constructing commercial-grade conver-sational interfaces with much less data and effort, and much wider real-world applicability.
Toward this end, we contribute a new dataset, Fashion In-teractive Queries (Fashion IQ). Fashion IQ is distinct from existing fashion image datasets (see Figure 4) in that it uniquely enables joint modeling of natural language feed-back and side information to realize effective and practical image retrieval systems. As we illustrate in Figure 2, there are two main settings to utilize Fashion IQ to drive progress on developing more effective interfaces for image retrieval: single-shot retrieval and dialog-based retrieval. In both set-tings, the user can communicate their ﬁne-grained search intent via natural language relative feedback. The differ-ence of the two settings is that dialog-based retrieval can progressively improve the retrieval results over the interac-tion rounds. Fashion IQ also enables relative captioning, which we leverage as a user model to efﬁciently generate a large amount of low-cost training data, to further improve training interactive fashion retrieval systems.2
To summarize, our main contributions are as follows:
• We introduce a novel dataset, Fashion IQ, a publicly available resource for advancing research on conversa-tional fashion retrieval. Fashion IQ is the ﬁrst fashion dataset that includes both human-written relative cap-tions that have been annotated for similar pairs of images, and the associated real-world product descriptions and at-tribute labels as side information.
• We present a transformer-based user simulator and inter-active image retriever that can seamlessly leverage mul-timodal inputs (images, natural language feedback, and attributes) during training, and leads to signiﬁcantly im-proved performance. Through the use of self-attention, these models consolidate the traditional components of user modeling and interactive retrieval, are highly exten-sible, and outperform existing methods for the relative captioning and interactive image retrieval of fashion im-ages on Fashion IQ. 2Relative captioning is also a standalone vision task [26, 57, 43, 15], which Fashion IQ serves as a new training and benchmarking dataset. 11308
• To the best of our knowledge, this is the ﬁrst study to in-vestigate the beneﬁt of combining natural language user feedback and attributes for dialog-based image retrieval, and it provides empirical evidence that incorporating at-tributes results in superior performance for both user modeling and dialog-based image retrieval. 2.