Abstract 6D pose estimation in space poses unique challenges that are not commonly encountered in the terrestrial set-ting. One of the most striking differences is the lack of at-mospheric scattering, allowing objects to be visible from a great distance while complicating illumination conditions.
Currently available benchmark datasets do not place a suf-ﬁcient emphasis on this aspect and mostly depict the target in close proximity.
Prior work tackling pose estimation under large scale variations relies on a two-stage approach to ﬁrst estimate scale, followed by pose estimation on a resized image patch.
We instead propose a single-stage hierarchical end-to-end trainable network that is more robust to scale variations.
We demonstrate that it outperforms existing approaches not only on images synthesized to resemble images taken in space but also on standard benchmarks. 1.

Introduction
Reliable 6D pose estimation is key to automating many spatial maneuvers, such as docking or capturing inert ob-jects as shown in Fig. 1. An important consequence of such maneuvers is that they dramatically change the scale and aspect of the observed target. Although 6D pose es-timation is an active area of research in computer vision and robotics, this important aspect has not received sig-niﬁcant attention thus far—for example, most benchmark datasets [8, 20, 42, 9] feature objects whose depth varies within a limited range. The lack of atmospheric scattering enabling observation from great distances also leads to other challenges: harsh contrast, under- and over-exposed areas, and signiﬁcant specular reﬂections from reﬂective materi-als used in space engineering (aluminium and carbon ﬁber panels, etc.).
To address such challenges, the European Space Agency (ESA) and Stanford University recently organized a satellite pose estimation challenge based on the Spacecraft Pose Es-timation Dataset (SPEED) [19]. The best-performing meth-ods in this competition use a two-step approach to handle (a) (b) (c)
Figure 1: Docking and space cleaning. (a, b) Two different views of the Agena target vehicle during the ﬁrst space docking. The appearance of Agena is strongly affected by the large scale and viewpoint changes, suggesting that different image features should be used for 6D pose estimation. In 1966, this docking procedure was controlled manually. (c) In 2025, the ClearSpace One chaser satellite will be launched to retrieve and de-orbit a non-operational satellite, so as to showcase the feasibility of removing space de-bris. In this case, the capture will be fully automated. The syn-thetic image shown here highlights the challenges the algorithm will have to handle, such as reﬂections, over-exposure of some parts of the images, and lack of details in others. large depth variation: a detector ﬁnds an axis-aligned box bounding the target, which is resampled to a uniform size and ﬁnally processed by a 6D pose estimator.
This approach is suboptimal in several ways. First, de-tection and pose estimation are treated as separate pro-cesses, which precludes joint training. Second, it provides supervisory signals only to the ﬁnal layer of the encoder-decoder architecture being used instead of to all levels of the decoding pyramid, which would increase robustness. Third, many similar feature extraction computations are performed by both processes, which results in an unnecessary dupli-cation of effort. Finally, these methods rely on the domi-nant approach to deep learning based 6D object pose esti-mation [33, 11, 2] consisting of training a network to mini-mize the 2D reprojection error of predeﬁned 3D keypoints, which cannot cope with large depth range variations: As shown in Fig. 2, reprojection error is strongly affected by the distance of individual keypoints to the camera, and not explicitly taking this into account degrades performance.
To address these shortcomings, we introduce a sin-gle hierarchical end-to-end trainable network depicted by
Fig. 3 that yields robust and scale-insensitive 6D poses. 15870
p1 object p1
Input Image
Hierarchical Processing
Multi-Scale Fusion
Pose Result p2
O
Z
O
Z image plane image plane (a) Sensitivity to different keypoints (b) Sensitivity to target positions
Figure 2: Problem with minimizing the 2D reprojection error. (a) The red lines denote the 2D reprojection errors for points p1 and p2. Because one is closer to the camera than the other, these 2D errors are of about the same magnitude even though the corre-sponding 3D errors, shown in blue, are very different. (b) For the same object at different locations, the same 2D error can generate different 3D errors. This makes pose accuracy dependent on the relative position of the target to the camera.
To use information across scales, it progressively down-scales the learned features, derives 3D-to-2D correspon-dences for each level of the resulting pyramid, and ﬁnally uses a RANSAC-based PnP strategy to infer a single re-liable pose from these sets of correspondences. This is a departure from most networks that estimate pose only from the ﬁnal layer. To address the issue in Fig. 2, we minimize a training loss based on 3D positions instead of 2D pro-jections, making the method invariant to the target distance.
We use a Feature Pyramid Network (FPN) [24] as our back-bone but, unlike in most approaches relying on such net-works, we assign each training instance to multiple pyramid levels to promote the joint use of multi-scale information.
In short, our contribution is a new 6D pose estimation architecture that reliably handles large scale changes under challenging conditions. We will show that it outperforms all state-of-the-art methods on the established SPEED dataset while also being much faster. Furthermore, we introduce a larger-scale satellite pose estimation dataset featuring more realistic and more complex images than SPEED, and we show that our method delivers the same beneﬁts in this more challenging scenario. Finally, we demonstrate that our method outperforms the state of the art even on images with smaller depth variations, such as those of the challenging
Occluded LINEMOD dataset. Our code and new dataset will be publicly released. 2.