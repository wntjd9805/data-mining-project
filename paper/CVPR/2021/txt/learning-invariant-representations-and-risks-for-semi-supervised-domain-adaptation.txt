Abstract
The success of supervised learning hinges on the as-sumption that the training and test data come from the same underlying distribution, which is often not valid in practice due to potential distribution shift. In light of this, most ex-isting methods for unsupervised domain adaptation focus on achieving domain-invariant representations and small source domain error. However, recent works have shown that this is not sufﬁcient to guarantee good generalization on the target domain, and in fact, is provably detrimen-tal under label distribution shift. Furthermore, in many real-world applications it is often feasible to obtain a small amount of labeled data from the target domain and use them to facilitate model training with source data.
In-spired by the above observations, in this paper we pro-pose the ﬁrst method that aims to simultaneously learn in-variant representations and risks under the setting of semi-supervised domain adaptation (Semi-DA). First, we provide a ﬁnite sample bound for both classiﬁcation and regres-sion problems under Semi-DA. The bound suggests a prin-cipled way to obtain target generalization, i.e., by align-ing both the marginal and conditional distributions across domains in feature space. Motivated by this, we then in-troduce the LIRR algorithm for jointly Learning Invariant
Representations and Risks. Finally, extensive experiments are conducted on both classiﬁcation and regression tasks, which demonstrate that LIRR consistently achieves state-of-the-art performance and signiﬁcant improvements com-pared with the methods that only learn invariant represen-tations or invariant risks. Our code will be released at
LIRR@github 1.

Introduction
The success of supervised learning hinges on the key as-sumption that test data should share the same distribution
*Equal contribution.
†Work done while at Carnegie Mellon University with the training data. Unfortunately, in most of the real-world applications, data are dynamic, meaning that there is often a distribution shift between the training (source) and test (target) domains. To this end, unsupervised domain adaptation (UDA) methods aim to approach this problem by adapting the predictive model from labeled source data to the unlabeled target data. Recent advances in UDA focus on learning domain-invariant representations that also lead to a small error on the source domain. The goal is to learn representations, along with the source predictor, that can generalize to the target domain [1, 2, 3, 4, 5, 6]. However, recent works [7, 8, 9] have shown that the above conditions are not sufﬁcient to guarantee good generalizations on the target domain. In fact, if the marginal label distributions are distinct across domains, the above method provably hurts target generalization [7].
On the other hand, while labeled target data is usually more difﬁcult or costly to obtain than labeled source data, it can lead to better accuracy [10]. Furthermore, in many prac-tical applications, e.g., vehicle counting, object detection, speech recognition, etc., it is often feasible to at least obtain a small amount of labeled data from the target domain so that it can facilitate model training with source data [11, 12].
Motivated by these observations, in this paper we focus on a more realistic setting of semi-supervised domain adaptation (Semi-DA). In Semi-DA, in addition to the large amount of labeled source data, the learner also has access to a small amount of labeled data from the target domain. Again, the learner’s goal is to produce a hypothesis that well general-izes to the target domain, under the potential shift between the source and the target. Semi-DA is a more-realistic set-ting that allows practitioners to design better algorithms that can overcome the aforementioned limitations in UDA. The key question in this scenario is: how to maximally exploit the labeled target data for better model generalization?
In this paper, we address the above question under the
Semi-DA setting. In order to ﬁrst understand how perfor-mance discrepancy occurs, we derive a ﬁnite-sample gener-alization bound for both classiﬁcation and regression prob-lems under Semi-DA. Our theory shows that, for a given 1104
predictor, the accuracy discrepancy between two domains depends on two terms: (i) the distance between the marginal feature distributions, and (ii) the distance between the opti-mal predictors from source and target domains. Our obser-vation naturally leads to a principled way of learning in-variant representations (to minimize discrepancy between marginal feature distributions) and risks (to minimize dis-crepancy between conditional distributions over the fea-tures) across domains simultaneously for a better general-ization on the target. In light of this, we introduce our novel bound minimization algorithm LIRR, a model of jointly
Learning Invariant Representations and Risks for such pur-poses. As a comparison, existing works either focus on learning invariant representations only [2, 3, 6, 5], or learn-ing invariant risks only [13, 14, 15, 16, 17, 18], but not both. However, these are not sufﬁcient to reduce the ac-curacy discrepancy for good generalizations on the target.
To our best knowledge, LIRR is the ﬁrst work that subtly combine above learning objectives with sound theoretical justiﬁcation. LIRR jointly learns invariant representations and risks, and as a result, better mitigates the accuracy dis-crepancy across domains. To better understand our method, we illustrate the proposed algorithm, LIRR, in Fig. 1.
In summary, our work provides the following contribu-tions:
• Theoretically, we provide ﬁnite-sample generalization bounds for Semi-DA on both classiﬁcation (Theorem 4.1) and regression (Theorem 4.2) problems. Our bounds in-form new directions for simultaneously optimizing both marginal and conditional distributions across domains for better generalization on the target. To the best of our knowledge, this is the ﬁrst generalization analysis in the
Semi-DA setting that takes into account both the shifts be-tween the marginal and the conditional distributions from source and target domains.
• To bridge the gap between theory and practice, we pro-vide an information-theoretic interpretation of our theo-retical results. Based on this perspective, we propose a bound minimization algorithm, LIRR, to jointly learn in-variant representations and invariant optimal predictors, in order to mitigate the accuracy discrepancy across do-mains for better generalizations.
• We systematically analyze LIRR with extensive experi-ments on both classiﬁcation and regression tasks. Com-pared with methods that only learn invariant representa-tions or invariant risks, LIRR demonstrates signiﬁcant im-provements on Semi-DA. We also analyze the adaptation performance with an increasing amount of labeled target data, which shows LIRR even surpasses oracle method
Full Target trained only on labeled target data, suggesting that LIRR can successfully exploit the structure in source data to improve generalization on the target domain. 2.