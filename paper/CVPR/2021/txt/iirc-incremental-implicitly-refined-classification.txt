Abstract
We introduce the “Incremental Implicitly-Reﬁned Classi-ﬁcation (IIRC)” setup, an extension to the class incremental learning setup where the incoming batches of classes have two granularity levels. i.e., each sample could have a high-level (coarse) label like “bear” and a low-level (ﬁne) label like “polar bear”. Only one label is provided at a time, and the model has to ﬁgure out the other label if it has already learned it. This setup is more aligned with real-life scenar-ios, where a learner usually interacts with the same family of entities multiple times, discovers more granularity about them, while still trying not to forget previous knowledge.
Moreover, this setup enables evaluating models for some important lifelong learning challenges that cannot be eas-ily addressed under the existing setups. These challenges can be motivated by the example ”if a model was trained on the class bear in one task and on polar bear in another task, will it forget the concept of bear, will it rightfully in-fer that a polar bear is still a bear? and will it wrongfully associate the label of polar bear to other breeds of bear?”.
We develop a standardized benchmark that enables evalu-ating models on the IIRC setup. We evaluate several state-of-the-art lifelong learning algorithms and highlight their strengths and limitations. For example, distillation-based methods perform relatively well but are prone to incorrectly predicting too many labels per image. We hope that the proposed setup, along with the benchmark, would provide a meaningful problem setting to the practitioners. 1.

Introduction
Deep learning algorithms have led to transformational breakthroughs in computer vision [12, 17], natural lan-guage processing [19, 50], speech processing [3, 5], rein-forcement learning [36, 44], robotics [16, 1], recommender systems [11, 18], etc. On several tasks, deep learning models have either matched or surpassed human perfor-mance. However, such super-human performance is lim-Figure 1. Humans incrementally accumulate knowledge over time.
They encounter new entities and discover new information about existing entities. In this process, they associate new labels with entities and reﬁne or update their existing labels, while ensuring the accumulated knowledge is coherent. ited to some narrow and well-deﬁned setups. Moreover, humans can continually learn and accumulate knowledge over their lifetime, while the current learning algorithms are known to suffer from several challenges when training over a sequence of tasks [33, 15, 8, 45]. These challenges are broadly studied under the domain of Lifelong Learn-ing [47], also called Incremental Learning [42], Continual
Learning [48], and Never Ending Learning [35].
In the general lifelong learning setup, the model experiences new knowledge, in terms of new tasks, from the same or differ-ent domains. The model is expected to learn and solve new tasks while retaining useful knowledge from previous tasks.
There are two popular paradigms in lifelong learn-ing [49]: i) task incremental learning, where the model has access to a task delimiter (say a task id), which distinguish between tasks. Models for this setup are generally multi-headed, where there exists a separate classiﬁcation layer ii) class incremental learning, where the for each task. model does not have access to a task delimiter, so it needs to discriminate between all classes from all tasks at infer-11038
Figure 2. IIRC setup showing how the model expands its knowledge and associates and re-associates labels over time. The top right label shows the label model sees during training, and the bottom label (annotated as “Target”) is the one that model should predict during evaluation. The right bottom panel for each task shows the set classes that model is evaluated on and the dashed line shows different tasks. ence time. Therefore, models developed for this paradigm are generally single-headed. The class incremental setup is more closely aligned with the real-life scenarios and is more challenging than the task incremental scenario.
Several useful benchmarks have been proposed for eval-uating models in the lifelong learning setting [4, 25]. While useful for measuring high-level aggregate quantities, these benchmarks take a narrow and limited view on the broad problem of lifelong learning. One common assumption that many class incremental setups make is “information about a given sample (say label) can not change across tasks”. For example, an image of a bear is always labeled as “bear”, no matter how much knowledge the model has acquired.
While this assumption appears to be “obviously correct” in the context of the supervised learning paradigm (where each sample generally has a ﬁxed label), the assumption is not always satisﬁed in real-life scenarios. We often inter-act with the same entities multiple times and discover new information about them. Instead of invalidating the previ-ous knowledge or outright rejecting the new information, we reﬁne our previous knowledge using the new informa-tion. Figure 1 illustrates an example where a child may rec-ognize all bears as “bear” (and hence label them as “bear”).
However, while growing up, they may hear different kinds of bear being called by different names, and so they update their knowledge as: “Some bears are brown bears, some bears are polar bears, and other bears are just bears. Brown bears and polar bears are both still bears but they are dis-tinct”. This does not mean that their previous knowledge was wrong (or that previous label “bear” was “incorrect”), but they have discovered new information about an entity and have coherently updated their knowledge. This is the general scheme of learning in humans.
A concrete instantiation of this learning problem is that two similar or even identical input samples have two dif-ferent labels across two different tasks. We would want the model to learn the new label, associate it with the old label without forgetting the old label. Evaluating lifelong learn-ing models for these capabilities is generally outside the scope of existing benchmarks. We propose the Incremen-tal Implicitly-Reﬁned Classiﬁcation (IIRC) setup to ﬁll this gap. We adapt the publicly available CIFAR100 and Ima-geNet datasets to create a benchmark for the IIRC setup and evaluate several well-known algorithms on this benchmark.
Our goal is not to develop a new state-of-the-art model but to surface the challenges posed by the IIRC setup.
The main contributions of our work are as follows: 1. We propose the Incremental Implicitly-Reﬁned Clas-siﬁcation (IIRC) setup, where the model starts training with some coarse, high-level classes and observes new,
ﬁne-grained classes as it trains over new tasks. During the lifetime of the model, it may encounter a new sam-ple or an old sample with a ﬁne-grained label. 2. We provide a standardized benchmark to evaluate a lifelong model in the IIRC setup. We adapt the com-monly used ImageNet and CIFAR datasets, and pro-vide a benchmark setup compatible with several major deep learning frameworks (PyTorch and Tensorﬂow)1. 3. We evaluate well-known lifelong learning algorithms on the benchmark and highlight their strengths and limitations, while ensuring that the models are com-pared in a fair and standardized setup. 2. Incremental Implicitly-Reﬁned Classiﬁca-tion (IIRC)
While class incremental learning is a challenging and close-to-real-life formulation of the lifelong learning setup, most existing benchmarks do not explore the full breadth 1https://chandar-lab.github.io/IIRC/ 11039
of the complexity. They tend to over-focus on catastrophic forgetting (which is indeed an essential aspect) at the ex-pense of several other unique challenges to the class in-cremental learning. In this work, we highlight those chal-lenges and propose the Incremental Implicitly-Reﬁned Clas-siﬁcation (IIRC) setting, an extension of the class incremen-tal learning setting, that enables us to study these under-explored challenges, along with the other well-known chal-lenges like catastrophic forgetting. We provide an instanti-ation of the setup, in the form of a benchmark, and evaluate several well-known lifelong learning algorithms on it. 2.1. Under explored challenges in class incremental learning setting
In class incremental learning, the model encounters new classes as it trains over new tasks. The nature of the class distributions and the relationship between classes (across tasks) can lead to several interesting challenges for the learning model: If the model is trained on a high-level label (say “bear”) in the initial task and then trained on a low-level label, which is a reﬁned category of the previous label (say “polar bear”), what kind of associations will the model learn and what associations will it forget? Will the model generalize and label the images of polar bear as both “bear” and “polar bear”? Will the model catastrophically forget the concept of “bear”? Will the model infer the spurious correlation: “all bears are polar bears”? What happens if the model sees different labels (at different levels of gran-ularity) for the same sample (across different tasks)? Does the model remember the latest label or the oldest label or does it remember all the labels? These challenges can not be trivially overcome by removing restrictions on memory or replay buffer capacity (as we show in Section 6). 2.2. Terminology
We describe the terminology used in the paper with the help of an example. As shown in Figure 2, at the start, the model trains on data corresponding to classes “bear”, “bus” and “dog”. Training the model on data corresponding to these three classes is the ﬁrst task. After some time, a new set of classes (“polar bear”, “lamp” and “whippet”) is en-countered, forming the second task. Since “whippet” is a type of “dog”, it is referred to as a subclass, while “dog” is referred to as a superclass. The “dog-whippet” pair is referred to as the superclass-subclass pair. Some classes do not have a superclass (example “lamp”), we refer to these classes as subclasses as well. When training the model on an example of a “whippet”, we may provide only “whippet” as the supervised learning label. This setup is referred to as the incomplete information setup, where if a task sample has two labels, only the label that belongs to the current task is provided. Alternatively, we may provide both “whippet” and “dog” as the supervised learning labels. This setup is referred as the complete information setup, where if a task sample has two labels, labels that belong to the current or previous tasks are provided. The majority of our experi-ments are performed in the incomplete information setup as it is closer to the real life setup, requiring the model to recall the previous knowledge when it encounters some new infor-mation about a known entity. We want to emphasize that the use of the word task in our setup refers to the arrival of a new batch of classes for the model to train on in a single-head setting, and so it is different from it’s use to indicate a distinct classiﬁcation head in task incremental learning.
As the model is usually trained in an incomplete infor-mation setup, it needs access to a validation set to monitor the progress in training that is also an incomplete informa-tion set, otherwise there would be some sort of labels leak-age. On the other hand, after training on a speciﬁc task, the model has to be evaluated on a complete information set, hence a complete information validation set is needed to be used during the process of model development and tweak-ing, so as to not overﬁt on the test set. We provide both in the benchmark. We call the ﬁrst one the in-task validation set, while the latter one the post-task validation set. 2.3. Setup
We describe the high-level design of the IIRC setup (for a visual illustration, see Figure 2). We have access to a se-ries of N tasks denoted as T1, · · · , TN . Each task comprises of three collections of datasets, for training, validation and testing. Each sample can have one or two labels associated with it. In the case of two labels, one label is a subclass and the other label is a superclass. For any superclass-subclass pair, the superclass is always introduced in an earlier task, with the intuition that a high-level label should be relatively easier to learn. Moreover, the number of samples for a su-perclass is always more than the number of samples for a subclass (it increases with the number of subclasses, up to a limit). During training, we always follow the incomplete information setup. During the ﬁrst task, only a subset of su-perclasses (and no sublcasses) are used to train the model.
The ﬁrst task has more classes (and samples), as compared to the other tasks and it can be seen as a kind of pretraining task. The subsequent tasks have a mix of superclasses and subclasses. During the training phase, the model is evalu-ated on the in-task validation set (with incomplete informa-tion), and during the evaluation phase, the model is eval-uated on the post-task validation set and the test set (both with complete information). 3.