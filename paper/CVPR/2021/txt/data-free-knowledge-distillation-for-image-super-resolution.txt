Abstract
Convolutional network compression methods require training data for achieving acceptable results, but train-ing data is routinely unavailable due to some privacy and transmission limitations. Therefore, recent works focus on learning efﬁcient networks without original training data, i.e., data-free model compression. Wherein, most of ex-isting algorithms are developed for image recognition or segmentation tasks.
In this paper, we study the data-free compression approach for single image super-resolution (SISR) task which is widely used in mobile phones and smart cameras. Speciﬁcally, we analyze the relationship between the outputs and inputs from the pre-trained net-work and explore a generator with a series of loss func-tions for maximally capturing useful information. The generator is then trained for synthesizing training sam-ples which have similar distribution to that of the origi-nal data. To further alleviate the training difﬁculty of the student network using only the synthetic data, we intro-duce a progressive distillation scheme. Experiments on var-ious datasets and architectures demonstrate that the pro-posed method is able to be utilized for effectively learn-ing portable student networks without the original data, e.g., with 0.16dB PSNR drop on Set5 for 2 super resolu-tion. Code will be available at https://github.com/huawei-noah/Data-Efﬁcient-Model-Compression.
× 1.

Introduction
Deep convolutional neural networks have achieved huge success in various computer vision tasks, such as image recognition [12], object detection [26], semantic segmen-tation [27] and super-resolution [7]. Such great progress largely relies on the advances of computing power and stor-age capacity in modern equipments. For example, ResNet-4G FLOPs. How-98MB storage and 50 [12] requires a ever, due to the heavy computation cost of these deep mod-∼
∼
*Corresponding author. els, they cannot be directly embedded into mobile devices with limited computing capacity, such as self-driving cars, micro-robots and cellphones. Therefore, how to compress
CNNs with enormous parameters and then apply them on resource-constrained devices, becomes a research hotspot.
In order to accelerate the pre-trained heavy convolutional networks, various attempts have been made recently, in-cluding quantization [24], NAS [5, 39], pruning [25, 34], knowledge distillation [37, 38] and etc. For example, Han et al. [11] utilize pruning, quantization and Huffman coding to compress a deep model with extremely higher compression and speed-up ratio. Hinton et al. [14] propose knowledge distillation, which learns a portable student network from a heavy teacher network. Luo et al. [29] propose ThiNet to perform ﬁlter pruning by solving an optimization prob-lem. Courbariaux et al. [6] propose binary neural network with only binary weights and activations to extremely re-duce the networks’ computation cost and storage consump-tion. Han et al. [10] introduce cheap operations in Ghost-Net to generate more features for lightweight convolutional models.
Although the compressed models with low computation complexity can be easily deployed in mobile devices, these techniques require original data to ﬁne-tune or train the compressed networks to achieve comparable performance with the pre-trained model. However, the original train-ing data is often unavailable due to some privacy or trans-mission constraints. For example, Google shared a series of excellent models trained on the JFT-300M dataset [20] which is still unpublic. In addition, there are considerable apps trained using privacy-related data such as face ID and voice assistant, which is often not provided. It is very hard to provide model compression and acceleration service for these models without the original training data. Therefore, existing network compression methods cannot be well per-formed.
To this end, recent works are devoted to compress and accelerate the pre-trained heavy deep models without the training dataset, i.e. data-free model compression. Lopes et al. [28] ﬁrst propose to use meta data to reconstruct the orig-7852
Figure 1. Framework of the proposed data-free knowledge distillation. The generator is trained with reconstruction loss and adversarial loss to synthesize images that similar with the original data. The student network is then obtained utilizing progressive distillation from the teacher network. inal dataset for knowledge distillation. Nayak et al. [32] present zero-shot knowledge distillation, which leverages the information of pre-trained model to synthesize useful training data. Chen et al. [4] exploit Generative Adversarial
Networks (GAN) to generate training samples which have similar distribution with the original images and achieve better performance. Yin et al. [40] propose DeepInversion and successfully generate training data on the ImageNet dataset. However, they only focus on image recognition tasks with sophisticated loss function, e.g., the one-hot loss for capturing features learned by the conventional cross-entropy loss. Differently, the training of SISR models does not involve the semantic information, the ground-truth im-ages are exactly the original high-resolution images. Thus, the existing data-free compression approaches cannot be di-rectly employed.
To this end, we propose a new data-free knowledge dis-tillation framework for super-resolution. A generator net-work is also adopted for approximating the original training data from the given teacher network. Different from the classiﬁcation networks whose outputs are probability dis-tributions, the inputs and outputs of SR models are images with similar patterns. Therefore, we develop the reconstruc-tion loss function by utilizing this relationship.
In prac-tice, we have to ensure that the synthetic images will not be distorted signiﬁcantly by the teacher SR network, i.e., the super-resolution results of these images should be similar to themselves. Moreover, an adversarial loss is combined to prevent the model collapse of the generator. Since SISR models are often required to capture and emphasize details such as edge and texture from the input images, the learn-ing on intermediate features is also very important. Thus, we propose to conduct the distillation progressively to alle-viate the training difﬁculty. We then conduct a series of ex-periments on several benchmark datasets and models. The results demonstrate that the proposed framework can effec-tively learn a portable network from a pre-trained model without any training data.
The rest of this paper is organized as follows. Section 2 investigates related work about model compression in super-resolution and data-free knowledge distillation meth-ods. Section 3 introduces our data-free distillation method for image super-resolution. Section 4 provides experimen-tal results on several benchmark datasets and models and
Section 5 concludes the paper. 2.