Abstract
Hand gesture-to-gesture translation is a signiﬁcant and interesting problem, which serves as a key role in many ap-plications, such as sign language production. This task in-volves ﬁne-grained structure understanding of the mapping between the source and target gestures. Current works fol-low a data-driven paradigm based on sparse 2D joint rep-resentation. However, given the insufﬁcient representation capability of 2D joints, this paradigm easily leads to blurry generation results with incorrect structure. In this paper, we propose a novel model-aware gesture-to-gesture translation framework, which introduces hand prior with hand meshes as the intermediate representation. To take full advantage of the structured hand model, we ﬁrst build a dense topology map aligning the image plane with the encoded embedding of the visible hand mesh. Then, a transformation ﬂow is calculated based on the correspondence of the source and target topology map. During the generation stage, we inject the topology information into generation streams by modu-lating the activations in a spatially-adaptive manner. Fur-ther, we incorporate the source local characteristic to en-hance the translated gesture image according to the trans-formation ﬂow. Extensive experiments on two benchmark datasets have demonstrated that our method achieves new state-of-the-art performance. 1.

Introduction
Hand gesture-to-gesture translation aims to convert the source gesture to the target one conditioned by the target posture, while preserving identity information. This prob-lem is of signiﬁcant importance with broad applications in sign language production, data augmentation, human-computer interactions, etc. Hand exhibits highly articulated joints and covers almost uniform appearance with fewer local characteristics compared with rigid human bodies or
*Contribute equally with the ﬁrst author.
†Corresponding author: Wengang Zhou and Houqiang Li.
Figure 1. Illustration of gesture-to-gesture translation. The ﬁrst and second row visualize the intermediate condition and generated image of the previous method [34] and our method, respectively.
The samples are chosen from the STB dataset. Our method ex-hibits more accurate spatial structure and ﬁne-grained details. faces. As a result, it is characterized by more ﬁne-grained texture with self-occlusion and poses a new challenge on learning the precise correspondence between the source and the target for the task of gesture-to-gesture translation.
Previous methods [34, 19] encode the gesture state via 2D sparse joint representation. The source hand images are translated according to the optical ﬂow learned from the source and target 2D joints. GestureGAN [34] attempts to learn the mapping under two novel losses in a cycle-consistency manner. As shown in Fig. 1, we visualize the intermediate representation and generated image by previ-ous work and our method. It can be observed that the gen-erated image by the comparison method contains incorrect and blurry structure, while our method produces the image with more ﬁdelity in ﬁne-grained details.
Previous methods follow a direct data-driven paradigm and suffer unsatisfactory results due to limited representa-tion capability and intrinsic ambiguity of 2D sparse hand pose [34, 26]. To tackle this issue, we incorporate hand prior and propose a model-aware gesture-to-gesture trans-lation framework. Since hand gesture-to-gesture translation needs ﬁne-grained understanding of hand pose, shape and texture, we attempt to depict the hand status more informa-tively. Speciﬁcally, we extract the hand representation in a model-aware way. The hand model provides a compact mapping from the latent pose and shape embedding to the high-dimensional hand mesh representation. It is a fully-16428
differentiable statistical model, which stores prior knowl-edge learned from a large variety of hand scans. With this model, the hand is reconstructed with more details, while irrational hand poses are ﬁltered out. During the extrac-tion process, we only need to estimate the latent embedding and camera parameter matching the image. To this end, our method provides two alternative effective ways, including direct regressing and iterative ﬁtting.
To fully exploit structure information contained in the hand mesh representation, we ﬁrst build the dense topology map for the source and target, and transformation ﬂow be-tween them. Speciﬁcally, we unravel the surface of the hand model and create its ﬂattened representation in 2D space.
Then each hand mesh face visible in the aligned image plane is encoded with its corresponding position embedding from the ﬂattened surface representation. The transforma-tion ﬂow is derived by calculating the correspondence be-tween the source and target. In this way, the dense topology map and their transformation ﬂow preserve abundant struc-ture information for the next stage. When turning to the gesture synthesis stage, we modulate the structure into gen-eration streams in a spatially-adaptive manner. To further enhance the translated hand gesture, we adaptively incorpo-rate local characteristics with the attention mechanism.
Our contributions are summarized as follows,
• To our best knowledge, we propose the ﬁrst model-aware gesture-to-gesture translation framework, con-sisting of three key modules, i.e., hand representation extraction, hand topology modeling and gesture syn-thesis.
• We introduce hand prior with hand meshes as the in-termediate representation, and propose an alternative hand representation extraction method based on itera-tive ﬁtting besides the direct regressing way.
• Extensive experiments on two widely-used bench-marks, i.e., STB and Senz3D, demonstrate the effec-tiveness of our proposed method, achieving new state-of-the-art performance. Our generated gesture images have more accurate spatial structure with better ﬁne-grained details. 2.