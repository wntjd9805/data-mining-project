Abstract
Unsupervised image clustering methods often introduce alternative objectives to indirectly train the model and are subject to faulty predictions and overconﬁdent results. To overcome these challenges, the current research proposes an innovative model RUC that is inspired by robust learn-ing. RUC’s novelty is at utilizing pseudo-labels of existing image clustering models as a noisy dataset that may include misclassiﬁed samples. Its retraining process can revise mis-aligned knowledge and alleviate the overconﬁdence prob-lem in predictions. The model’s ﬂexible structure makes it possible to be used as an add-on module to other cluster-ing methods and helps them achieve better performance on multiple datasets. Extensive experiments show that the pro-posed model can adjust the model conﬁdence with better calibration and gain additional robustness against adver-sarial noise. 1.

Introduction
Unsupervised clustering is a core task in computer vi-sion that aims to identify each image’s class membership without using any labels. Here, a class represents the group membership of images that share similar visual characteris-tics. Many studies have proposed deep learning-based algo-rithms that utilize distance in a feature space as the similar-ity metric to assign data points into classes [11, 44].
Training without ground-truth guidance, however, is prone to ﬁnding trivial solutions that are learned from low-level visual traits like colors and textures [22]. Several stud-ies have introduced innovative ways to guide the model’s training indirectly by setting alternative objectives. For ex-ample, Hu et al. [20] proposed to maximize the mutual in-formation between input and its hidden representations, and
Ji et al. [22] proposed to learn invariant features against data augmentation. Entropy-based balancing has often been adopted to prevent degenerate solutions [17, 22, 42].
Nevertheless, these alternative objectives are bound to
∗Equal contribution to this work.
Figure 1: Illustration for this work’s basic concept: robust learning is used to separate clean data from unclean data using pseudo-labels from off-the-shelf unsupervised clus-tering algorithm. producing overconﬁdent results, i.e., low-entropy predic-tions, due to the dense grouping among clusters. When uncertain samples are added to a wrong cluster at an early stage of training, the model gradually becomes overconﬁ-dent in its later predictions as the noise from misclassiﬁca-tion accumulates and degrades the overall performance.
This paper introduces a novel robust learning train-ing method, RUC (Robust learning for Unsupervised
Clustering), that runs in conjunction with existing cluster-ing models to alleviate the noise discussed above. Utiliz-ing and treating the existing clustering model’s results as a noisy dataset that may include wrong labels, RUC up-dates the model’s misaligned knowledge. Bringing insights from the literature, we ﬁlter out unclean samples and apply loss correction as in Fig. 1. This process is assisted by la-bel smoothing and co-training to reduce any wrong gradient signals from unclean labels. This retraining process with re-vised pseudo-labels further regularizes the model and pre-vents overconﬁdent results.
RUC comprises two key components: (1) extracting clean samples and (2) retraining with the reﬁned dataset.
We propose conﬁdence-based, metric-based, and hybrid strategies to ﬁlter out misclassiﬁed pseudo-labels. The ﬁrst strategy considers samples of high prediction conﬁdence from the original clustering model as a clean set; it ﬁlters out low conﬁdence samples. This strategy relies on the 12278
model’s calibration performance. The second strategy uti-lizes similarity metrics from unsupervised embedding mod-els to detect clean samples with non-parametric classiﬁers by checking whether the given instance shares the same la-bels with top k-nearest samples. The third strategy com-bines the above two and selects samples that are credible according to both strategies.
The next step is to retrain the clustering model with the sampled dataset. We use MixMatch [5], a semi-supervised learning technique; which uses clean samples as labeled data and unclean samples as unlabeled data. We then adopt label smoothing to leverage strong denoising effects on the label noise [29] and block learning from overconﬁdent sam-ples [22, 42]. Finally, a co-training architecture with two networks is used to mitigate noise accumulation from the unclean samples during training and increase performance.
We evaluate RUC with rigorous experiments on datasets, including CIFAR-10, CIFAR-20, STL-10, and ImageNet-50. Combining RUC to an existing clustering model out-performs the state-of-the-art results with the accuracy of 90.3% in CIFAR-10, 54.3% in CIFAR-20, 86.7% in STL-10, and 78.5% in ImageNet-50 dataset. RUC also enhances the baseline model to be robust against adversarial noise.
Our contributions are as follows:
• The proposed algorithm RUC aids existing unsuper-vised clustering models via retraining and avoiding overconﬁdent predictions.
• The unique retraining process of RUC helps existing models boost performance.
It achieves a 5.3pp in-crease for the STL-10 dataset when added to the state-of-the-art model (81.4% to 86.7%).
• The ablation study shows every component in RUC is critical, including the three proposed strategies (i.e., conﬁdence-based, metric-based, and hybrid) that excel in extracting clean samples from noisy pseudo-labels.
• The proposed training process is robust against adver-sarial noise and can adjust the model conﬁdence with better calibrations.
Implementation details of the model and codes are avail-able at https://github.com/deu30303/RUC. 2.