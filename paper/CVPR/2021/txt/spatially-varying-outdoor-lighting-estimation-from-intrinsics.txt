Abstract
We present SOLID-Net, a neural network for spatially-varying outdoor lighting estimation from a single outdoor image for any 2D pixel location. Previous work has used a uniﬁed sky environment map to represent outdoor lighting.
Instead, we generate spatially-varying local lighting envi-ronment maps by combining global sky environment map with warped image information according to geometric in-formation estimated from intrinsics. As no outdoor dataset with image and local lighting ground truth is readily avail-able, we introduce the SOLID-Img dataset with physically-based rendered images and their corresponding intrinsic and lighting information. We train a deep neural network to regress intrinsic cues with physically-based constraints and use them to conduct global and local lightings estimation.
Experiments on both synthetic and real datasets show that
SOLID-Net signiﬁcantly outperforms previous methods. 1.

Introduction
Estimating outdoor lighting from a single image is one of the fundamental problems in computer vision. By provid-ing outdoor scene properties from the physical aspect, it has huge impact on many applications, e.g., face/body relight-ing, scene understanding, augmented reality (AR), and so on. This task is rather challenging since images are formed by conﬂating lighting with complex surface reﬂectance dis-tribution and object geometry. In the outdoor scenario, ex-isting solutions usually employ low-dimensional parametric models such as the Hoˇsek-Wilkie (HW) sky model [11] with four parameters to ﬁt the sky illumination. The capacity of parametric models is not sufﬁcient to represent the com-plex real-world illumination, and a recent non-parametric
∗ Corresponding authors. † Part of this work was ﬁnished as a vis-iting student at Peking University. This work was supported by National
National Natural Science Foundation of China under Grant No. 61872012, 62088102, National Key R&D Program of China (2019YFF0302902), and
Beijing Academy of Artiﬁcial Intelligence (BAAI).
Figure 1: Given a single low-dynamic-range (LDR) im-age with limited FOV and a location in pixel coordinate (marked by numbers), SOLID-Net, for the ﬁrst time, infers a panoramic HDR illumination map representing the light arriving from all directions at the location. Note that the global environment map (could be estimated using existing method [10]) is only able to cover a small part of the local lighting (red contours). approach using an autoencoder to learn the sky illumination model from a large-scale sky panorama dataset and encod-ing the lighting information from a single limited Field-of-View (FOV) image shows more promising results [10].
However, as far as we know, all existing outdoor lighting estimation methods [11, 10, 24] only consider the outdoor illumination as a single global map without any spatially-varying consideration, i.e., the light probe is surrounded by an environment map that casts rays from inﬁnitely far away.
A spatially-varying lighting estimation has proved to be successful in indoor scenarios, which is achieved by mod-eling local indoor lighting using low-frequency parametric lighting represented by spherical harmonics (SH) [9, 5] or panoramic environment map [18].
Extending spatially-varying lighting estimation from in-door to outdoor is non-trivial in three aspects: 1) The ex-tremely high-dynamic-range (HDR) sunlight and the com-plicated sky light under different weather conditions make it more difﬁcult to parameterize outdoor than indoor light-ing [9, 5], while the existing non-parametric sky model [10] treats it as a pure deep learning task without considering 2) Non-parametric physics image formation constraint. 112834
lighting estimation is highly ill-spatially-varying local posed, since different 3D locations should have different lighting observations and the majority of local observation is missing [18]. 3) HDR and panoramic images capturing local lighting and geometry information in outdoor are not yet available, despite there are many datasets for such a purpose in the indoor scenario by synthetically generating scenes from SUNCG [19] and Matterport3D [4].
In this paper, we propose SOLID-Net, a neural net-work for Spatially-varying Outdoor Lighting estimation us-ing cues from Intrinsic image Decomposition, as shown in Figure 1. We tackle the three major challenges mentioned above by proposing a two-stage framework: 1) We train a single-in-multi-output CNN to decompose an input image into intrinsic parts: albedo (material-related), normal and plane distance (geometry-related), and shadow (lighting-related). These intrinsics provide a physically-based shad-ing constraint by ﬁtting SH-represented global lighting with low-frequency information, which is then combined with extracted sky features from the input image to generate a non-parametric sky model like [10]. 2) With the estimated geometry from decomposed intrinsics, we further warp the input image and estimated shadow map with limited FOV to a spherical projection centered at the target location, which provides panoramic observation to reduce the ill-posed is-sue. This is then combined with global sky lighting from the previous step as input to train a multi-input-single-output
CNN for complementing high-frequency local lighting esti-mation. 3) We use the Blender SceneCity [2] to create city models that contain a large set of outdoor scenes and render a synthetic outdoor lighting estimation dataset with labeled location information and corresponding lighting effects us-ing a physically-based path-tracer to facilitate the training of our network. SOLID-Net demonstrates signiﬁcant im-provements over other methods by making contributions in
• integrating shading constraint from intrinsic decompo-sition into the global sky lighting estimation;
• producing high-frequency local lighting estimation via panoramic warping and shadow map reference; and
• building the ﬁrst spatially-varying outdoor lighting es-timation dataset with ground truth labels. 2.