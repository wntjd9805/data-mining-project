Abstract
Instance segmentation is an active topic in computer vi-sion that is usually solved by using supervised learning ap-proaches over very large datasets composed of object level masks. Obtaining such a dataset for any new domain can be very expensive and time-consuming. In addition, models trained on certain annotated categories do not generalize well to unseen objects. The goal of this paper is to propose a method that can perform unsupervised discovery of long-tail categories in instance segmentation, through learning instance embeddings of masked regions. Leveraging rich relationship and hierarchical structure between objects in the images, we propose self-supervised losses for learning mask embeddings. Trained on COCO [34] dataset without additional annotations of the long-tail objects, our model is able to discover novel and more ﬁne-grained objects than the common categories in COCO. We show that the model achieves competitive quantitative results on LVIS [17] as compared to the supervised and partially supervised meth-ods. 1.

Introduction
Instance segmentation is a crucial problem that has a wide range of applications in various real-world applica-tions such as autonomous driving and medical imaging. Re-cent approaches [18, 19, 4, 35] have shown impressive re-sults on large-scale datasets in parsing diverse real-world scenes into informative semantics or instance maps. Most of the existing works so far assume a fully supervised set-ting, where the instance level segmentation masks are avail-able during training. However, relying on human annotated segmentation labels has a few obvious drawbacks. Since most of the existing segmentation datasets contain a small set of annotated categories (e.g. 20 in PASCAL VOC [10] and 80 in COCO [34]), models trained on these categories are not able to generalize well to novel and long-tail ob-jects present in the real world. Despite the ease of cap-Figure 1. We propose an instance segmentation method that is able to discover the long-tail objects through self-supervised represen-tation learning. Without having access to ground truth annotations of long-tail objects during training, our method is able to produce
ﬁne-grained segmentation result of novel objects. turing a large number of images nowadays, extending the set of annotated categories is still very expensive and time-consuming. Recently, a dataset for Large Vocabulary In-stance Segmentation (LVIS) [17] was released as an at-tempt to increase the coverage of the annotated categories in
COCO from 80 to over 1200 categories including the long-tail categories that appear rarely in the dataset.
To overcome the bottleneck in obtaining large annotated datasets, previous works have attempted instance segmen-tation with weaker forms of supervision such as the object bounding boxes [7, 39], points on instances [2] or image-level labels [49]. Recent works [31, 25, 12, 48] aim to improve the generalization capability of instance segmen-tation models by employing a partially supervised setting 12603
where only a subset of classes have instance-level mask an-notations during training; the remaining classes have only bounding box annotations. However, these methods still re-quire that the bounding boxes of all categories be known prior to training, which limits their ability in discovering novel objects that do not have bounding box annotations.
In this work we propose the ﬁrst instance segmentation method that performs unsupervised discovery of the long-tail objects through representation learning using hierarchi-cal self-supervision. To the best of our knowledge, this is the ﬁrst method that eliminates the need for any type of in-stance level mask or bounding box annotation for the long tail of categories during training. Our idea is that since instance segmentation models (e.g. Mask R-CNN [19]) trained on a small set of categories can already produce good class-agnostic mask proposals, we can leverage these masks and use representation learning to separate these pro-posals into distinct categories. Therefore, much of our ap-proach, after taking the mask proposals from a region pro-posal network, is focused on the representation learning.
We present an effective approach that exploits the inherent hierarchical visual structure of the objects and enables the embedded features of all proposed masks to be easily dif-ferentiated through unsupervised clustering.
Our method is motivated by the recent works [28, 36] that use hyperbolic embeddings to boost the performance of downstream computer vision tasks. These works draw inspiration from the key observation that objects in the real world exhibit hierarchical structure. To perform self-supervised learning in hyperbolic embedding space, we in-troduce three triplet losses for learning better mask features and capturing hierarchical relations between the masks. We show that our model outperforms the state-of-the-art par-tially supervised models [25, 31] even though they require box annotations of the long-tail objects during training and we do not. We also qualitatively show that our model is able to discover and segment additional novel object categories.
Compared to the the fully supervised setting, our model is competitive in terms of its capability of detecting and seg-menting rare and small objects.
We summarize our main contributions as the following:
• We propose an instance segmentation method that is able to discover the long-tail objects using hierarchical self-supervision. To the best of our knowledge, this is the ﬁrst instance segmentation method that eliminates the need for any type of ground truth annotation of the long-tail categories during training.
• We leverage hyperbolic embeddings to capture the hierarchical structure in the segmented objects and demonstrate the effectiveness of our learned embed-dings as compared to their Euclidean counterpart.
• We show that our method outperforms the state-of-the-art partially supervised instance segmentation methods although using less supervision. We also provide the
ﬁrst set of baseline numbers on LVIS for such self-supervised methods. 2.