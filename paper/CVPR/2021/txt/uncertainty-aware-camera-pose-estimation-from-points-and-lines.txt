Abstract
Perspective-n-Point-and-Line (PnPL) algorithms aim at fast, accurate, and robust camera localization with respect to a 3D model from 2D-3D feature correspondences, being a major part of modern robotic and AR/VR systems. Current point-based pose estimation methods use only 2D feature detection uncertainties, and the line-based methods do not take uncertainties into account. In our setup, both 3D co-ordinates and 2D projections of the features are considered uncertain. We propose PnP(L) solvers based on EPnP [20] and DLS [14] for the uncertainty-aware pose estimation.
We also modify motion-only bundle adjustment to take 3D uncertainties into account. We perform exhaustive syn-thetic and real experiments on two different visual odometry datasets. The new PnP(L) methods outperform the state-of-the-art on real data in isolation, showing an increase in mean translation accuracy by 18% on a representative sub-set of KITTI, while the new uncertain reﬁnement improves pose accuracy for most of the solvers, e.g. decreasing mean translation error for the EPnP by 16% compared to the standard reﬁnement on the same dataset. The code is avail-able at https://alexandervakhitov.github.io/uncertain-pnp/. 1.

Introduction
Camera localization using sparse feature correspon-dences is a major part of augmented or virtual reality and robotic systems. The Perspective-n-Point(-and-Line), or
PnP(L), methods can be successfully used to estimate the pose of a calibrated camera from sparse feature correspon-dences. Line features can increase localization accuracy in man-made self-similar environments which lack surfaces with distinctive textures [36, 43, 15, 29], motivating the use of PnP(L) methods [41].
While vision-based localization with respect to an au-This work has been partially funded by the Spanish government under projects HuMoUR TIN2017-90086-R, ERA-Net Chistera project IPALM
PCI2019-103386 and Mar´ıa de Maeztu Seal of Excellence MDM-2016-0656.
Figure 1. We propose globally convergent PnP(L) solvers leverag-ing a complete set of 2D and 3D uncertainties for camera pose es-timation. A 3D scene model with sparse features is reconstructed from images with known poses (right cameras), and we need to
ﬁnd a pose of a camera on the left. The point has 2D detection un-certainty (blue ellipsoid), and 3D model uncertainty (red ellipsoid in the scene). tomatically reconstructed map of sparse features is an im-portant part of current robotic and AR/VR systems, com-monly used PnP methods treat the features as absolutely accurate [47, 14, 20, 16]. The more recent 2D covariance-aware methods relax this assumption [40, 6], but only for the feature detections, still assuming perfect accuracy of the 3D feature coordinates.
In maps reconstructed with structure-from-motion, 3D feature coordinate accuracy can vary. Stereo triangula-tion errors grow quadratically with respect to the object-to-sensor distance, so the accuracy in estimating point depth can vary in several orders of magnitude, while the line stereo triangulation accuracy depends on the angle between the line direction and the baseline. Nevertheless, to the best of our knowledge, no prior method for PnP(L) was designed to take both 3D and 2D uncertainties into account.
We propose to integrate the feature uncertainty into the
PnP(L) methods, see Fig. 1, which is our main contribu-tion. We build on the classical DLS [14] and EPnP [20] 14659
methods. Additionally, we propose a modiﬁcation to the standard nonlinear reﬁnement, which is normally used after the PnP(L) solver, to take 3D uncertainties into account. An exhaustive evaluation on synthetic data and on the two real indoor and outdoor datasets demonstrates that new PnP(L) methods are signiﬁcantly more accurate than state-of-the-art, both in isolation and in a complete pipeline, e.g. the proposed DLSU method reduces the mean translation error on KITTI by 18%, see Section 4. The proposed uncertain pose reﬁnement can improve the pose accuracy by up to 16% in exchange of an extra 5-10% of the computational time. In a synthetic setting with noise in 2D feature detec-tions, the new methods have the same accuracy as the most accurate 2D uncertainty-aware methods [6, 40]. The code is available at https://alexandervakhitov.github.io/uncertain-pnp/. 2.