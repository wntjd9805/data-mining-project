Abstract
With the growing use of camera devices, the industry has many image datasets that provide more opportunities for collaboration between the machine learning commu-nity and industry. However, the sensitive information in the datasets discourages data owners from releasing these datasets. Despite recent research devoted to removing sen-sitive information from images, they provide neither mean-ingful privacy-utility trade-off nor provable privacy guar-antees. In this study, with the consideration of the percep-tual similarity, we propose perceptual indistinguishability (PI) as a formal privacy notion particularly for images. We also propose PI-Net, a privacy-preserving mechanism that achieves image obfuscation with PI guarantee. Our study shows that PI-Net achieves signiﬁcantly better privacy util-ity trade-off through public image data. 1.

Introduction
More and more facial image datasets are becoming avail-able in a wide variety of communities. The availability of these datasets presents enormous opportunities for collabo-ration between data owners and the machine learning com-munity (e.g., social relation recognition [43]). However, the inherent privacy risk of such datasets prevents data owners from sharing the data. For example, Microsoft’s facial im-age dataset MS-Celeb-1M, Duke’s MTMC, and Stanford’s
Brainwash were taken down due to the potential privacy concern [30, 36].
Facial Image Obfuscation. Many facial image obfus-cation (aka. face anonymization and face de-identiﬁcation) solutions have been proposed. An approach to mitigating privacy risk of releasing facial images is to use Generative
Adversarial Networks (GANs) to synthesize visually sim-ilar images [7, 25]. Recently, GANs that can manipulate semantics have been proposed to even have a ﬁne-grained control of attributes such as age and gender [17, 37, 50].
Based on inpainting [28, 42, 52], one can also anonymize the image, while retaining the facial semantics, by remov-ing the region of interest (ROI) with sensitive semantics in the image and restoring it. Image forgery methods such as
Deepfakes [45], if used to replace sensitive semantics, can also mitigate privacy risks for identity disclosure [3, 15].
However, all of the above methods share a common weakness of syntactic anonymity, or say, lack of formal privacy guarantee. Recent studies report that obfuscated faces can be re-identiﬁed through machine learning tech-niques [33, 19, 35]. Even worse, the above methods are not guaranteed to reach the analytical conclusions consis-tent with the one derived from original images, after manip-ulating semantics. To overcome the above two weaknesses, one might resort to differential privacy (DP) [9], a rigorous privacy notion with utility preservation. In particular, DP-GANs [1, 6, 23, 46] shows a promising solution for both the provable privacy and perceptual similarity of synthetic images. Unfortunately, DP-GANs can only be shallow, be-cause of its rapid noise accumulation, hindering model ac-curacy. One can also apply DP to the image, leading to pix-elation [10]. Consequently, the images generated in such a way are of low quality.
Key Insights. Basically, anonymizing facial images, while retaining necessary information for tasks such as de-tection, recognition, and tracking is very challenging. No-tably, our result is in possession of the following novelties.
First, based on metric privacy [5], we introduce percep-tual indistinguishability (PI), a variant of DP with a par-ticular consideration of perceptual similarity of the facial images. More speciﬁcally, PI, while retaining perceptual similarity, achieves the indistinguishability result that an ad-versary, when seeing an anonymized image, can hardly in-fer the original image, thereby protecting the privacy of the image content. On the other hand, inherited from DP, PI can also ensure high data utility (detection, classiﬁcation, tracking, etc.). As far as we know, this is the ﬁrst time perceptual similarity or more concretely, facial attributes, is used to deﬁne an indistinguishability notion from the im-age adjacency point of view in the context of DP, which enables the reconciliation among privacy and utility. Al-though facial attributes have been also exploited for face de-identiﬁcation [27, 51], in addition to relying on different privacy notions, our study is different from theirs in that (1)
[27] needs pre-processing for face alignment and cropping and (2) [51] learns privacy representation instead of releas-ing private image dataset. 6478
Second, we introduce PI-Net, a novel encoder-decoder architecture to achieve image obfuscation with PI. PI-Net is featured by its operations in latent space and can also be seen as a latent-coded autoencoder. In particular, we inject noise to the latent code derived from GAN inversion. PI-Net is also featured by the use of triplet loss that clusters the faces with similar facial attributes. This novel architecture enables the network to create anonymized images that look realistic and satisfy user-deﬁned facial attributes.
In summary, we make the following contributions:
•
•
We present a notion for image privacy, perceptual in-distinguishability (PI), deﬁning adjacent images by the perceptual similarity between images in latent space.
We propose PI-Net to anonymize faces with the se-lected semantic attributes manipulation. The architec-ture of PI-Net generates realistic looking faces with the selected attributes preservation. 2.