Abstract
Beyond achieving high performance across many vi-sion tasks, multimodal models are expected to be robust to single-source faults due to the availability of redun-dant information between modalities.
In this paper, we investigate the robustness of multimodal neural networks against worst-case (i.e., adversarial) perturbations on a single modality. We ﬁrst show that standard multimodal fusion models are vulnerable to single-source adversaries: an attack on any single modality can overcome the cor-rect information from multiple unperturbed modalities and cause the model to fail. This surprising vulnerability holds across diverse multimodal tasks and necessitates a solu-tion. Motivated by this ﬁnding, we propose an adversar-ially robust fusion strategy that trains the model to com-pare information coming from all the input sources, detect inconsistencies in the perturbed modality compared to the other modalities, and only allow information from the un-perturbed modalities to pass through. Our approach sig-niﬁcantly improves on state-of-the-art methods in single-source robustness, achieving gains of 7.8-25.2% on action recognition, 19.7-48.2% on object detection, and 1.6-6.7% on sentiment analysis, without degrading performance on unperturbed (i.e., clean) data. 1.

Introduction
Consider a multimodal neural network, illustrated in Fig-ure 1(a), that fuses inputs from k different sources to iden-tify objects for an autonomous driving system.
If one of the modalities (e.g., RGB) receives a worst-case or adver-sarial perturbation, does the model fail to detect the truck in the scene? Or does the model make a robust prediction using the remaining k − 1 unperturbed modalities (e.g., LI-DAR, audio, etc.)? This example illustrates the importance of single-source adversarial robustness [17] for avoiding
∗KY and MB completed work during an internship at BCAI. Work is sponsored by DARPA (Grant number HR11002020006). (a) Example of a single source, worst-case (i.e., adver-Figure 1. sarial) perturbation on a multimodal model. (b) Standard multi-modal models are vulnerable to worst-case perturbations on any single modality (“Vulnerable”). Our adversarially robust fusion strategy (“Robust”) leverages multimodal consistency to defend against such perturbations without degrading clean performance. catastrophic failures in real-world multimodal systems. In a realistic setting, any single modality may be affected by a worst-case perturbation, whereas multiple modalities usu-ally do not fail simultaneously particularly if the physi-cal sensors are not coupled. Since multimodal models are being increasingly developed for real-world vision tasks
[5, 35, 24, 18], it is imperative to investigate whether they are robust to worst-case errors that may affect any single modality and, if they are not, to develop strategies to im-prove robustness.
Despite the importance of this problem, we found to the best of our knowledge that empirical studies of single-source adversarial robustness are lacking. Previous empiri-cal works on multimodal robustness have so far only consid-ered single-source corruptions (e.g., dropout, blurring, etc.)
[16, 15, 17], and although Kim & Ghosh [17] formulate the problem for the adversarial setting, they do not perform an 3340
empirical study. In the ﬁeld of adversarial robustness, most studies have focused on the unimodal setting rather than the multimodal setting [20, 21]. An effective strategy for de-fending unimodal models against adversaries is adversarial training (i.e., end-to-end training of the model on adversar-ial examples).
In principle, adversarial training could be extended to multimodal models as well, but it has several downsides: (1) it is resource-intensive [31] and may not scale well to large, multimodal models that contain many more parameters than their unimodal counterparts; (2) it signiﬁcantly degrades performance on clean data [21]. For these reasons, end-to-end adversarial training may not be practical for multimodal systems used in real-world tasks.
Contributions. This paper presents, to our knowledge, the
ﬁrst empirical study of single-source adversarial robustness in multimodal systems. Our contributions are two-fold. (1) We investigate multimodal robustness against single-source adversaries on diverse benchmark tasks with three modalities (k = 3): action recognition on EPIC-Kitchens
[7], object detection on KITTI [9], and sentiment analysis on CMU-MOSI [40]. We ﬁnd that standard multimodal fusion practices are vulnerable to single-source adversar-ial perturbations, even when there are multiple unperturbed modalities that could yield a correct prediction; naive en-sembling of features from a perturbed modality with fea-tures from clean modalities does not automatically yield ro-bust prediction. As shown in Figure 1(b), a worst-case input at any single modality of a multimodal model can outweigh the other modalities and cause the model to fail. In fact, contrary to expectations, a multimodal model (k = 3) un-der a single-source perturbation does not necessarily out-perform a unimodal model (k = 1) under the same attack. (2) We propose an adversarially robust fusion strategy that can be applied to mid- to late- fusion models to defend against this vulnerability without degrading clean perfor-mance.
Inspired by recent works that detect correspon-dence between inputs to defend against image manipula-tion [13], we hypothesize that a multimodal model can be trained to detect correspondence (or lack thereof) between features from different modalities and use this information to perform a robust feature fusion that defends against the perturbed modality. Our approach extends existing work on adaptive gating strategies [16, 15, 34, 22] with a robust fusion training procedure based on odd-one-out learning
[8] to improve single-source adversarial robustness without degrading clean performance. Through extensive experi-ments, we demonstrate that our approach is effective even against adaptive, white-box attacks with access to the robust fusion strategy. We signiﬁcantly outperform state-of-the-art methods in single-source robustness [16, 15, 17], achiev-ing gains of 7.8-25.2% on action recognition on EPIC-Kitchens, 19.7-48.2% on 2D object detection on KITTI, and 1.6-6.7% sentiment analysis on CMU-MOSI.
Overall, this paper demonstrates that multimodal mod-els are not inherently robust to single-source adversaries, but that we can improve their robustness without the down-sides associated with end-to-end adversarial training in uni-modal models. The combination of robust fusion architec-tures with robust fusion training may be a practical strategy for defending real-world systems against adversarial attacks and establishes a promising direction for future research. 1.1.