Abstract
The dense video captioning task aims to detect and de-scribe a sequence of events in a video for detailed and co-herent storytelling. Previous works mainly adopt a “detect-then-describe” framework, which ﬁrstly detects event pro-posals in the video and then generates descriptions for the detected events. However, the deﬁnitions of events are di-verse which could be as simple as a single action or as com-plex as a set of events, depending on different semantic con-texts. Therefore, directly detecting events based on video information is ill-deﬁned and hurts the coherency and accu-racy of generated dense captions. In this work, we reverse the predominant “detect-then-describe” fashion, proposing a top-down way to ﬁrst generate paragraphs from a global view and then ground each event description to a video seg-ment for detailed reﬁnement. It is formulated as a Sketch,
Ground, and Reﬁne process (SGR). The sketch stage ﬁrst generates a coarse-grained multi-sentence paragraph to de-scribe the whole video, where each sentence is treated as an event and gets localised in the grounding stage. In the re-ﬁning stage, we improve captioning quality via reﬁnement-enhanced training and dual-path cross attention on both coarse-grained event captions and aligned event segments.
The updated event caption can further adjust its segment boundaries. Our SGR model outperforms state-of-the-art methods on ActivityNet Captioning benchmark under tradi-tional and story-oriented dense caption evaluations. Code will be released at github.com/bearcatt/SGR. 1.

Introduction
Video understanding is an important research topic in computer vision. Thanks to the development of deep learn-ing and large-scale datasets [3, 10], recent video under-standing approaches have achieved promising performance in action recognition [4, 5, 35] and temporal action local-isation [15, 25, 41]. However, an obvious limitation of these approaches is that their predictions are based on a pre-∗Equal contribution, †Corresponding author
⋆Work done while at the Renmin University of China.
Top-down	DVC
Captioning
Selection
Event	proposals
……
……
Sketching
Video-level	story
<Sentence1>	<Sentence2>	<Sentence3>
Grounding
Proposing
Story-oriented	event	segments
Bottom-up	DVC
Refining
Figure 1. Comparisons between top-down and bottom-up DVC frameworks. Bottom-up DVC breaks the DVC task into multi-ple “event captioning” sub-tasks with an event detection process.
While our SGR captions the whole video in a uniﬁed fashion and localises each event afterwards. deﬁned discrete set of action categories, which lack many
ﬁne-grained details of the video information.
In order to provide more detailed information in the video, the task of
Dense Video Captioning (DVC) [11] aims to discover a se-quence of key events in a video and describe them using a coherent story. Therefore, DVC can beneﬁt many real-world applications such as content-based video retrieval and recommendation, and has become an important task in language-based video understanding research.
DVC is a challenging task since the generated event cap-tions are expected to be correct, concise, and coherent in the context of a video-level story, so as to support video under-standing. Most existing methods [11, 13, 33, 43] in this ﬁeld adopt a common “detect-then-describe” framework which solves the problem from a straightforward bottom-up per-spective, i.e., ﬁrst detect a large set of event proposals (even up to 103), then caption each proposal to obtain dense de-scriptions. However, one obvious issue of this framework is that the event proposals are independently generated with-out considering their temporal correlations, thus making the resultant event captions highly redundant and incoherent.
To address this issue, Mun et al. [18] proposed a stream-lined DVC framework that learns a proposal selection mod-234
𝐸"
𝐸$
𝐸": A	woman	is	seen	speaking	to	the	camera	while	holding	an	 accordion	and	moving	her	hands	around.
Episode	1
𝐸$: She	demonstrates	how	to	play	the	instrument	while	still	speaking	to	 the	camera	and	moving	around.
𝐸$
𝐸"
𝐸": A	woman	is	standing	on	a	lit	stage.
𝐸$: She	is	holding	an	accordion	as	she	talks.
𝐸%: She	is	surrounded	by	two	other	accordions	as	she	instructs	on	how	to
𝐸%
Episode	2 play the	instrument.	
Figure 2. Illustration of the diverse deﬁnition of event proposals.
The ﬁrst episode consists of salient events, which are long and informative, while in the second episode, a simple action is also considered as an event, which is very short and indistinctive. ule to ﬁnd a small subset of the event proposals that oc-cur sequentially in the video. In this way, they explicitly consider the temporal correlations among the selected event proposals. However, their framework is still limited due to the ill-deﬁned event proposal generation. As shown in Fig-ure 2, the deﬁnition of events varies dramatically in prac-tice, from as simple as an action (E1, Episode 2) to as com-plicated as a combination of multiple salient events (E1,
Episode 1). Therefore, detecting the event proposals based solely on the video information like [18] and other bottom-up DVC approaches may have an ill-deﬁned target, i.e., the detector is not aware of which kinds of event proposals are suitable to be captioned. As a result, the quality of the gen-erated event captions may be negatively affected.
In this paper, we reverse the predominant “detect-then-describe” fashion and propose to solve DVC from a top-down perspective, i.e., generating a video-level story at ﬁrst and then ground each sentence in the story to video segment for detailed reﬁnement. By doing this, the event segments are predicted not only based on the visual information, but also the semantic coherence from the text. Thus the afore-mentioned issues are waived. Our model is formulated as a
Sketch, Ground, and Reﬁne (SGR) procedure.
In the Sketch stage, we focus on the structure coherency of the event captions. We ﬁrst leverage the entire video in-formation to generate a coherent video-level paragraph so as to describe the video from a global perspective. Note that, the sentences in this paragraph naturally deﬁnes a se-quence of story-oriented events in the video. Therefore, it is not necessary for our framework to generate a large number of event proposals as in bottom-up approaches. In fact, our top-down SGR contains no event proposal generation pro-cess. Instead, we use a video Grounding module to localise the sentences (i.e., events) in our video-level paragraph.
The sentences generated so far may lack some ﬁne-grained details since we did not explicitly consider their event-speciﬁc information. Thus, in the Reﬁne stage, we focus on the ﬁne-grained details of the event captions.
Speciﬁcally, we propose a Dual-Path Cross Attention mod-ule to dynamically focus on the event-level information and the coarse-grained sentence. A reﬁnement-enhanced train-ing scheme is also designed to facilitate the reﬁnement.
Based on the reﬁned descriptions, we further adjust their event segments by feeding them into the grounding module again. We illustrate the difference between our SGR and the previous DVC frameworks in Figure 1.
We evaluate our method on the benchmark ActivityNet
Captioning dataset [11] and YouCook2 dataset [42]. Our model achieves state-of-the-art performance under both tra-ditional and story-oriented dense caption evaluations. 2.