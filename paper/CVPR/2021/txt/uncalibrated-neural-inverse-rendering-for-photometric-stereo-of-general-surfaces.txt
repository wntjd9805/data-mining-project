Abstract
This paper presents an uncalibrated deep neural network framework for the photometric stereo problem. For training models to solve the problem, existing neural network-based methods either require exact light directions or ground-truth surface normals of the object or both. However, in practice, it is challenging to procure both of this informa-tion precisely, which restricts the broader adoption of pho-tometric stereo algorithms for vision application. To bypass this difﬁculty, we propose an uncalibrated neural inverse rendering approach to this problem. Our method ﬁrst es-timates the light directions from the input images and then optimizes an image reconstruction loss to calculate the sur-face normals, bidirectional reﬂectance distribution function value, and depth. Additionally, our formulation explicitly models the concave and convex parts of a complex surface to consider the effects of interreﬂections in the image forma-tion process. Extensive evaluation of the proposed method on the challenging subjects generally shows comparable or better results than the supervised and classical approaches. 1.

Introduction
Since Woodham’s seminal work [69], the photometric stereo problem has become a popular choice to estimate an object’s surface normals from its light varying images. The formulation proposed in that paper assumes the Lambertian reﬂectance model of the object, and therefore, it does not apply to general objects with unknown reﬂectance property.
While multiple-view geometry methods exist to achieve a similar goal [57, 20, 70, 76, 35, 24, 36, 37], photometric stereo is excellent at recovering ﬁne details on the surface, like indentations, imprints, and even scratches. Of course, the solution proposed in Woodham’s paper has some unre-alistic assumptions. Still, it is central to the development of several robust algorithms [71, 30, 55, 1, 22, 26] and also lies at the core of the current state-of-the-art deep photometric stereo methods [28, 65, 12, 10, 11, 42, 41, 27].
Generally, deep learning-based photometric stereo meth-ods assume a calibrated setting, where all the light source information is given both at the train and test time [28, 56, 12, 65]. Such methods attempt to learn an explicit relation between the reﬂectance map and the ground-truth surface normals. But, the exact estimation of light directions is a te-dious process and requires expert skill for calibration. Mo-tivated by that, Chen et al. [10, 11] recently proposed an un-calibrated photometric stereo method. Though it estimates light directions using image data, the proposed method re-quires ground-truth surface normals for training the neural network. Certainly, procuring ground-truth 3D surface ge-ometry is difﬁcult, if not impossible, which makes the ac-quisition task of correct surface normals strenuous. For 3D data acquisition, active sensors are mostly used, which is expensive and often needs post-processing of the data to re-move noise and outliers. Hence, the necessity of ground-truth surface normals limits the usage of such an approach.
Further, most photometric stereo methods, including cur-rent deep-learning methods, assume that each surface point is illuminated only by the light source, which generally holds for a convex surface [49]. However, objects, mainly from ancient architectures, have complex geometric struc-tures, where the shape may compose of convex, concave, and other ﬁne geometric primitives (see Fig.1(a)). When illuminated under a varying light source, certain concave parts of the surface might reﬂect light onto other parts of the object, depending on its position. Surprisingly, this phe-nomenon of interreﬂections is often ignored in the modeling and formulation of a photometric stereo problem, despite its vital role in the object’s imaging [28, 65, 12, 10, 11].
In this work, we overcome the above shortcomings by proposing an uncalibrated neural inverse rendering network.
We ﬁrst estimate all the light source directions and intensi-ties using image data. Computed light source information is then fed into the proposed neural inverse rendering net-work to estimate the surface normals. The idea is, those cor-rect surface normals, when provided to the rendering equa-tion, should reconstruct the input image as close as possi-ble. Consequently, we can bypass the requirement of the ground-truth surface normals at train time. Unlike recent methods, we model the effects of both the light source and the interreﬂections for rendering the image. Although one 3804
L i g h t
S o u r c e
L i g h t 
S o u r c e
I n ci d e n t  o n t h
R a e 
C y fr o o n v
I n c i o d n e n t t h e
R a
C y o f n r c o a m v e t h
R e e
S m  t h e x  e 
S
R e g i o o u n r c e g o i o u r n c e
Camera y t i s n e t n
I t h g i l f o t n e n o p m o
C o t e u d r o s n e
S e h t t a d e v i e c e r e c r u o s y r a d n o c e s e h t r o s n e
S e h t t a d e v i e c e r y t i s n e t n
I e c r u o s y r a m i r p e h t o t e u d
Illuminates the other  surface element. (a) Photometric Stereo Setup (b) Qualitative and Quantitative Comparison
Figure 1: (a) Example showing the interreﬂection effect due to concave geometric structure. The light from the primary source hits the concave region of the surface that illuminates the other surface points which then act as a secondary light source. (b) Comparison of our approach against the classical and deep-learning methods on the Vase dataset which shows that it performs better than others. We used Mean Angular Error (MAE) metric to report the results. can handle interreﬂection using classical methods [49, 9], the reﬂectance characteristics of different types of material are quite diverse. Hence, we want to leverage neural net-work’s powerful capability to learn complex reﬂectance be-havior from the input image data.
For evaluation, we performed experiments on DiLiGenT dataset [62]. We noticed that the objects present with this dataset are not apt for studying interreﬂections. To that end, we proposed a novel dataset to study the behavior and effect of interreﬂections on the object’s imaging §5. We observed that ignoring interreﬂections can dramatically affect the ac-curacy of the surface normals estimate (see Fig 1(b)). To sum up, our paper makes the following contributions:
• This paper presents an uncalibrated deep photometric stereo method that does not require ground-truth surface normals at train time to solve photometric stereo.
• Our work considers the contribution of both the source light and interreﬂections in the image formation process.
Consequently, our approach is more general and applica-ble to a wide range of objects.
• The proposed method leverages neural inverse rendering principles to infer the surface normals, depth, and spa-tially varying bidirectional reﬂectance distribution func-tion (BRDF) values from input images. Our method gen-erally provides comparable or better results than the clas-sical [49, 2, 60, 73, 45, 52, 44] and the recent supervised uncalibrated deep learning methods [12, 15, 11]. 2.