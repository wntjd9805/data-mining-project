Abstract
Recently, a large number of channel attention blocks are proposed to boost the representational power of deep convolutional neural networks (CNNs). These approaches commonly learn the relationship between global contexts and attention activations by fully-connected layers or linear transformations. However, we empirically ﬁnd that though many parameters are introduced, these attention blocks may not learn the relationship well. In this paper, we hypothesize that the relationship is predetermined. Based on this hy-pothesis, we propose a simple yet extremely efﬁcient chan-nel attention block, called Gaussian Context Transformer (GCT), which achieves contextual feature excitation using a Gaussian function that satisﬁes the presupposed relation-ship. According to whether the standard deviation of the
Gaussian function is learnable, we develop two versions of
GCT: GCT-B0 and GCT-B1. GCT-B0 is a parameter-free channel attention block by ﬁxing the standard deviation. It directly maps global contexts to attention activations with-out learning. In contrast, GCT-B1 is a parameterized ver-sion, which adaptively learns the standard deviation to en-hance the mapping ability. Extensive experiments on Im-ageNet and MS COCO benchmarks demonstrate that our
GCTs lead to consistent improvements across various deep
CNNs and detectors. Compared with a bank of state-of-the-art channel attention blocks, such as SE [17] and ECA [42], our GCTs are superior in effectiveness and efﬁciency. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved signiﬁcant progresses in many computer vision tasks, such as image classiﬁcation [21, 13], segmentation
*Corresponding author
Figure 1. Visualization of the sorted global contexts and the ac-cording attention activations of different channel attention blocks at different stages across 1000 classes on ImageNet validation set.
“SiBj” denotes the j-th attention block of stage i. In the second row, the semitransparent lines and the opaque lines represent the attention activations before and after a low-pass ﬁlter, respectively.
[28, 30], and object detection [9, 3]. However, the local con-text characteristics of convolutional kernel prevent CNNs from effectively capturing global context information in an image, which is often essential for semantically understand-ing. To tackle this problem, attention mechanisms are com-monly adopted. Their core is to arm CNNs with additional lightweight modules which can capture global long-range dependencies [43, 8, 43]. As one of them, channel atten-tion mechanism has become increasingly popular, owing to its simplicity and effectiveness. The most pioneering work in this scenario is squeeze-and-excitation networks (SENet)
[17], which aims to adaptively emphasize important chan-nels and suppress trivial ones by capturing channel-wise de-pendencies, bringing enormous beneﬁts for various CNNs. 15129
Methods
ResNet50
+SE
+LCT
+ECA
+GCT-B0
+GCT-B1
Pram-Free
-×
×
×
X
×
#Param
-2C 2/r 2C
|(log2C + 1)/2|odd 0 1
Top-1 76.15 77.18 77.45 77.48 77.51 77.55
Top-5 92.87 93.67 93.71 93.68 93.86 93.71
Table 1. Comparison of existing state-of-the-art channel attention blocks on ImageNet validation set. Param-Free denotes whether the attention block is free of parameters. #Param denotes the num-ber of parameters introduced in one channel attention block. C de-notes the number of channels. r denotes the reduction ratio of SE.
|·|odd indicates the nearest odd number of ·. Note that our GCTs outperform other channel attention blocks with fewer parameters introduced.
Several channel attention blocks have been thereafter proposed to improve the SE block with different per-spectives including simplifying feature transform module
[7, 42], changing fusion mode [4], and integrating with spatial attention mechanism [44, 31]. Despite the perfor-mance improvements, these approaches generally introduce large amounts of parameters to learn the relationship be-tween global contexts and attention activations. However, the learnt relationship may not be good enough.
As observed in linear context transform (LCT) block [7],
SE tends to learn a negative correlation that the more global contexts deviate from their mean, the smaller attention acti-vations are attached, as shown in Fig. 1. To learn this corre-lation more accurately, LCT uses a per-channel transforma-tion to replace two fully-connected layers of SE. However, we empirically ﬁnd that this negative correlation may not be well-learnt by LCT. As shown in Fig. 1, the attention activations learnt by LCT ﬂuctuate greatly.
To alleviate the above problem, we hypothesize a nega-tive correlation between global contexts and attention acti-vations. Based on this hypothesis, we propose a new chan-nel attention block, called Gaussian Context Transformer (GCT), which directly maps global contexts to attention ac-tivations with a Gaussian function that represents the pre-supposed negative correlation. The basic structure of GCT is illustrated in Fig. 2. Speciﬁcally, after global average pooling, GCT performs normalization to stabilize the global contextual distribution. Then a Gaussian function is used to excite (transform and activate) the normalized global con-texts to obtain the attention activations. When the Gaussian function is ﬁxed, we refer to this model as GCT-B0. Note that GCT-B0 is a parameter-free attention block that model global contexts without contextual feature transform learn-ing. As shown in Table 1, GCT-B0 yields signiﬁcant per-formance gains over baselines and outperforms other state-of-the-art channel attention blocks without introducing any parameters, indicating that the contextual feature transform learning is not essential. Further, we develop a learnable
GCT, called GCT-B1, which adaptively learns the standard deviation of Gaussian function. We empirically show that
GCT-B1 generally performs better than GCT-B0 on Ima-geNet. On object detection/segmentation tasks, GCT-B0 and GCT-B1 achieve similar performance.
In summary, our main contributions can be summarized as follows:
• Our work provides a new insight into the channel at-tention mechanism: parameterized contextual feature transform learning is not essential. This will inform further research progress in designing more efﬁcient channel attention blocks.
• We propose a simple yet extremely efﬁcient channel attention block (GCT), which hypothesizes the rela-tionship between global contexts and attention activa-tions and excites global contexts only using a Gaussian function. Our GCTs can signiﬁcantly boost various deep CNNs and detectors.
• Comprehensive experiments on ImageNet and MS
COCO consistently demonstrate the superiority and generalization ability of our proposed GCTs. In partic-ular, GCT-B0 generally outperforms other state-of-the-art channel attention blocks without introducing any parameters. 2.