Abstract
Different visual style of different images
Image composition plays a common but important role in photo editing. To acquire photo-realistic composite im-ages, one must adjust the appearance and visual style of the foreground to be compatible with the background. Ex-isting deep learning methods for harmonizing composite images directly learn an image mapping network from the composite to real one, without explicit exploration on vi-sual style consistency between the background and the fore-ground images. To ensure the visual style consistency be-tween the foreground and the background, in this paper, we treat image harmonization as a style transfer problem. In particular, we propose a simple yet effective Region-aware
Adaptive Instance Normalization (RAIN) module, which ex-plicitly formulates the visual style from the background and adaptively applies them to the foreground. With our set-tings, our RAIN module can be used as a drop-in module for existing image harmonization networks and is able to bring signiﬁcant improvements. Extensive experiments on the ex-isting image harmonization benchmark datasets shows the superior capability of the proposed method. Code is avail-able at https://github.com/junleen/RainNet. 1.

Introduction
Image composition is one of the most common opera-tions in image editing [39, 3] and data augmentation [6, 42], etc. However, generating a realistic composite image by taking an object from one image and combining it with a new background image usually requires professional com-positors to adjust the appearance of the foreground objects by photo editing software like Adobe Photoshop, and ensure the realism of the generated image. To alleviate this burden, image harmonization is introduced for adjusting the fore-ground and making it seamlessly integrated into the new im-age with less human involvement, especially for non-expert users.
However, what makes a composite image appear more realistic? In this paper, we present a new perspective for y c n e t s i s n o
C e l y t
S
Figure 1. Illustration of our motivation. If we want to put a police car into these images with different visual style , we must ensure that the car is compatible with the background images (small-sized images with red boundaries in the top row). Simple cut-and-paste operations introduce unrealistic results (top row). Our method aims to adaptively learn high-level visual style from different back-grounds and produce harmonious composite images (bottom row). image harmonization. Let us take Fig. 1 for example. Fig. 1 shows three different real photos (small-sized images with red border) that hold different visual properties. When an unbeﬁtting foreground object with special visual properties is pasted into a new image with incompatible visual fea-tures, we can easily distinguish it from real photos. This is an unsolved problem and has emerged for years, which we call visual style discrepancy. Speciﬁcally, in this paper, we deﬁne the visual style of an image as visual properties including illumination, color temperature, saturation, hue, texture etc., which varies from image to image. To make a composite image look more realistic, we must ensure a more consistent visual style between the foreground and the background.
Abundant image harmonization approaches have been proposed for improving the realism of composite images.
Traditional methods address the harmonization problem by transferring statistics of hand-crafted features between fore-ground and background regions, such as color [26, 27, 39, 30]. However, these methods only work in simple cases 9361  
where the foreground image is already consistent with the background image. Recently, more deep learning-based methods [2, 3, 32, 43] have been proposed for generating harmonious images in an end-to-end manner. Zhu et al. [43] propose to adopt a discriminative model to predict the real-ism of a compsite image and assist optimization of color adjustment. Tsai et al. [32] propose an end-to-end learning approach for image harmonization while only constraining semantic information learning in the encoder. Cun et al. [3] adopt a spatial-separated attention module to enforce the network to learn the foreground and background features separately, failing to ensure the style consistency between these two parts. To sum up, none of these methods really consider the realism from the perspective of visual style consistency. Cong et al. [2] propose to use a domain ver-iﬁcation discriminator and adversarial loss [10] to improve domain-consistency between foreground and background regions but neglect to explicitly transform the foreground features in the generator. However, performance improve-ment brought by such an auxiliary discriminator is limited (i.e., 0.27dB for PSNR, which is revealed in [2]).
To address these issues, in this work, we reframe image harmonization as a background-to-foreground style trans-fer problem, where we render the foreground image to hold similar visual style of the background image. Taking style guidance from background information is of great impor-tance because the foreground image should be converted to own different appearances when pasted into different back-ground images (as illustrated in Fig. 1). To generate style-consistent and realistic-looking composite images, we ex-pect a uniﬁed transferring operation to adaptively adjust the style of the foreground objects to be in perfect harmony with new background images even collected in different envi-ronments. Therefore, in this work, we propose a learnable layer, named Region-aware Adaptive Instance Normaliza-tion (RAIN) layer, to learn the style from background im-ages and apply it to the foreground objects. By taking con-volutional features and the foreground mask as input, the
RAIN layer aligns the channel-wise mean and variance of the foreground activation to match those learned from the background. The details of the proposed RAIN module are presented in Fig. 3. It is worth mentioning that our RAIN layer can be easily applied to existing image harmonization networks and encourage performance improvements.
The contributions of this work are as follows. 1) To the best of our knowledge, we are the ﬁrst to introduce the style concept of background images and regard the image har-monization task as a style transferring problem. 2) We pro-pose a novel Region-aware Adaptive Instance Normaliza-tion (RAIN) method, which captures the style information only from the background features and applies it to the fore-ground for image harmonization tasks. Our RAIN module is simple yet effective and can be used as a plug-and-play module for existing image harmonization networks to en-hance their performance. 3) Extensive experiments demon-strate that our method surpasses the state-of-the-art methods by a large margin. 2.