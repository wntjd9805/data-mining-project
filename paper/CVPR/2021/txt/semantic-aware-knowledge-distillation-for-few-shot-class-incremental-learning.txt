Abstract
Few-shot class incremental learning (FSCIL) portrays the problem of learning new concepts gradually, where only a few examples per concept are available to the learner.
Due to the limited number of examples for training, the tech-niques developed for standard incremental learning cannot be applied verbatim to FSCIL. In this work, we introduce a distillation algorithm to address the problem of FSCIL and propose to make use of semantic information during training. To this end, we make use of word embeddings as semantic information which is cheap to obtain and which facilitate the distillation process. Furthermore, we propose a method based on an attention mechanism on multiple par-allel embeddings of visual data to align visual and semantic vectors, which reduces issues related to catastrophic forget-ting. Via experiments on MiniImageNet, CUB200, and CI-FAR100 dataset, we establish new state-of-the-art results by outperforming existing approaches. 1.

Introduction
In a real world scenario, we may not get access to in-formation about all possible classes when the system is ﬁrst trained. It is more realistic to assume that we will obtain class-speciﬁc data incrementally as time goes by. There-fore, in such a scenario, we require that our model can be adapted with new information made available without ham-pering the performance on what has been learnt so far. Al-though a natural task for human beings, it is a difﬁcult task for an intelligent machine due to the possibility of catas-trophic forgetting [17]. A trained model tends to forget old tasks when learning new information. There are three dif-ferent streams of work in the literature addressing such an
*denotes equal contribution. output output output output (a) (c) (b) (d)
Figure 1: (a) Knowledge distillation as described in [16] does not work on few-shot class-incremental learning [32] since adding new tasks appends new trainable weights (Wn) to the network in addition to base weights (Wb). (b) The impact of using only a few instances of novel classes. As few samples are not sufﬁcient to learn new parameters, the network gets biased towards base classes, overﬁtted on few examples of novel classes, and not well-separated from base classes. (c) Our semantically guided network does not add new parameters while adding new classes incremen-tally. We only include word vectors of new tasks (sn) in addition to the base classes (sb) and keep ﬁne-tuning the base network (F ) (d) As a result, the knowledge distillation process can help the net-work, remembering base training, generalizing to novel classes, and ﬁnding well-separated representation of classes. incremental or continual learning paradigm [33]. Firstly, task-incremental learning divides all classes into different tasks, where each task contains a few classes, and then learns each task individually. The task labels of the test in-stances are made available during testing which means the model does not need to predict the correct label between all classes but only between classes that are deﬁned for a speciﬁc task. Secondly, domain-incremental learning does 2534
not reveal the task label at test time, but the model always solves the current task at hand without inferring the true class label. Thirdly, class-incremental learning predicts the class label between all classes during test time as the output of all tasks are merged into one uniﬁed classiﬁer without having access to the task label. Being the most realistic of the three, in this paper, we are interested in this third setting.
Furthermore, in many applications, new tasks (a set of novel classes) come with only a few examples per class, making the class-incremental learning even more challenging. This setting is called few-shot class-incremental learning (FS-CIL) [32]. The main challenges in FSCIL are catastrophic forgetting of already acquired knowledge and overﬁtting the network to novel classes. Challenges of that nature are ad-dressed by the work on knowledge distillation in [11]. How-ever, [32] showed that knowledge distillation is not the pre-ferred approach for FSCIL due to class imbalance in the few-shot scenario and the performance trade-off between novel classes and base classes. In this paper, we propose an augmented knowledge distillation approach suitable for the case of few-shot incremental learning.
In order to apply knowledge distillation to novel tasks, scores of the previously trained model are needed as well as many instances of the new classes to be learned. Those new instances help to learn the new trainable weights that are added while learning novel tasks. For incremental learn-ing with few-shot data, we can preserve previous scores but cannot provide enough samples to learn the corresponding weights for novel classes. For this reason, knowledge dis-tillation [32] becomes a difﬁcult problem in our case. Ad-dressing this issue, we take advantage of a semantic word vector (word2vec [18] or GloVe [22]) which provides a semantic representation for each class as auxiliary knowl-edge. Being inspired by the literature on zero-shot learn-ing [7, 6, 5, 20, 37, 24, 25], given an image as input, we estimate the semantic word vectors for the input instead of directly predicting its class label. Then, we measure the similarity of the predicted word vectors with the word vec-tors from the set of possible class labels, followed by a soft-max layer applied to the similarity values to get the ﬁnal score of the classes. One key beneﬁt of this approach is that adding new classes while training on novel tasks does not come with new weights to train because the model at-tempts to predict ﬁxed-length word vectors as an intermedi-ate representation. No matter how many classes are present during ﬁne-tuning, the network continues with its previous task of estimating the word vectors. In this new set-up, the distillation loss can easily accommodate new classes. One challenge of this approach is to obtain a good alignment of visual and semantic word vectors of few-shot instances. To address this issue, we employ automatically assigned su-perclass information of classes to train multiple embedding modules in parallel after the backbone network. The set of superclasses is attained from the semantic word vector space representations of the base task, and are then held
ﬁxed for the novel classes that follow. We determine an em-bedding for each superclass during training such that each embedding sees only the superclass set of classes. Hence, given a novel class, there is a selection of embeddings that each may be more or less suited. We employ an attention module [8] to merge multiple embedding outputs and a loss to train the alignment appropriately with few-shot instances.
It helps the network not to overﬁt on the few-shot instances as well as not becoming biased to the base classes. Figure 1 describes the key differences between conventional works and our method. With our proposed approach, we suc-cessfully beat the current state-of-the-art [32] on MiniIm-ageNet, CUB200, and the CIFAR100 datasets thanks to the combined effect of using the auxiliary semantic information from word vectors and knowledge distillation in concert.
In summary, the contributions of this paper are: (1) A semantically-guided knowledge distillation approach for few-shot class-incremental learning using semantic word vectors, (2) A new visual-semantic alignment strategy for few-shot class-incremental learning using automatically as-signed superclass annotations, (3) Extensive experiments validating the approach on MiniImageNet, CUB200, and
CIFAR100 while achieving new state-of-the-art results. 2.