Abstract
Generative Adversarial Networks (GANs) are able to gen-erate high-quality images, but it remains difﬁcult to explicitly specify the semantics of synthesized images. In this work, we aim to better understand the semantic representation of
GANs, and thereby enable semantic control in GAN’s gener-ation process. Interestingly, we ﬁnd that a well-trained GAN encodes image semantics in its internal feature maps in a surprisingly simple way: a linear transformation of feature maps sufﬁces to extract the generated image semantics. To verify this simplicity, we conduct extensive experiments on various GANs and datasets; and thanks to this simplicity, we are able to learn a semantic segmentation model for a trained GAN from a small number (e.g., 8) of labeled im-ages. Last but not least, leveraging our ﬁnding, we propose two few-shot image editing approaches, namely Semantic-Conditional Sampling and Semantic Image Editing. Given a trained GAN and as few as eight semantic annotations, the user is able to generate diverse images subject to a user-provided semantic layout, and control the synthesized image semantics. We have made the code publicly available1. 1.

Introduction
Recent years have witnessed the striking success of Gen-erative Adversarial Networks (GANs) [15] in various image synthesis tasks: to generate human faces, animals, cars, and interior scenes [28, 12, 17, 21]. Apart from improving the generated image quality, recent research has been directed toward the control of GAN’s image generation process—for example, to enforce the generated images having user speci-ﬁed attributes, colors, and layouts.
Toward this goal, a fundamental question remains unan-swered: how does a well-trained GAN encodes image se-mantics—such as the layout of such semantic classes as hair, nose, and hats—in its image generation process? Motivated by this question, we aim to extract image semantics from a GAN’s internal data, namely its feature maps. If we can
*Jianjin Xu is currently a research assistant at Panzhihua University. 1https://github.com/AtlantixJJ/LinearGAN well extract image semantics and understand the extraction process, we can develop insight on how the image semantics are encoded.
Our ﬁnding is surprisingly simple: a linear transforma-tion on the GAN’s internal feature maps sufﬁces to extract the generated image semantics. In stark contrast to GAN’s highly nonlinear image generation process, this simple linear transformation is easy to understand and has a clear geomet-ric interpretation (see Sec. 3.1).
We refer to this linear transformation process as linear se-mantic extraction (LSE). To verify its performance, we con-duct extensive experiments on various GANs and datasets, in-cluding PGGAN [20], StyleGAN [21] and StyleGAN2 [22] trained on FFHQ [21], CelebAHQ [25], and LSUN [37]’s bedroom and church dataset. We also compare the perfor-mance of LSE with other semantic extraction approaches which use learned nonlinear transformations. It turns out that LSE is highly comparable to those more complex, non-linear models, suggesting that image semantics are indeed represented in a linear fashion in GANs.
Related to our study of the linear encoding of image semantics in GANs is the work of GAN Dissection [5]. It identiﬁes feature maps that have causal manipulation ability for image semantics. Yet, most feature maps in that approach come from middle-level layers in the GAN, often having much lower resolution than the output image. Instead, we examine the GAN’s internal feature maps collectively. We upsample all feature maps to the resolution of ﬁnal output image and stack them into a tensor. This approach allows us to study per-pixel feature vectors, that is, feature values corresponding to a particular pixel across all internal layers, and we are able to classify every output pixel into a speciﬁc semantic class.
The linear transformation in our proposed LSE is learned under supervision. Its training requires image semantic an-notations, which are automatically generated using a pre-trained segmentation model (such as UNet [29]). Interest-ingly, thanks to the linearity of LSE, even a small number of annotations sufﬁce to train LSE well. For example, the LSE trained with 16 annotated images on StyleGAN2 (which it-self is trained on FFHQ dataset) achieves 88.1% performance 19351
relative to a fully trained LSE model. Not only does this result further support our ﬁnding about the linear representa-tion of semantics in GANs, it also inspires new approaches for controlling image generation through few-shot training.
In particular, we explore two controlled image generation tasks: (1) Semantic Image Editing (SIE) and (2) Semantic-Conditional Sampling (SCS). The former aims to update images based on the user’s edit on the semantics of a GAN’s output (e.g., generate images in which the hair region is reshaped); the latter is meant to generate images subject to a user speciﬁcation of desired semantic layout (e.g., produce images of an interior room where the furnitures are laid out according to user speciﬁcation). We demonstrate few-shot
SIE and SCS models both trained with small number of annotated images.
Behind both SCS and SIE is the core idea of matching the generated image semantics with a target semantic speciﬁca-tion. This is done by formulating an optimization problem, one that ﬁnds a proper latent vector for the GAN’s image generation while respecting the user speciﬁcation. We also consider baselines of both tasks, which are implemented by using carefully trained, off-the-shelf semantic segmentation models rather than our few-shot LSE. In comparison to the baselines, our approach with 8-shot LSE is able to generate comparable (and sometimes even better) image quality.
In summary, our technical contributions are twofold: (i)
Through extensive experiments, we show that GANs rep-resent the image’s pixel-level semantics in a linear fashion. (ii) We propose an LSE with few-shot learning, which fur-ther enables two image synthesis applications with semantic control, namely SCS and SIE under few-shot settings. 2.