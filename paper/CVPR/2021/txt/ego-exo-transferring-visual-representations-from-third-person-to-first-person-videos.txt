Abstract (cid:40)(cid:74)(cid:82)(cid:70)(cid:72)(cid:81)(cid:87)(cid:85)(cid:76)(cid:70)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3) (cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81) (cid:43)(cid:68)(cid:81)(cid:71)(cid:16)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:85)(cid:72)(cid:74)(cid:76)(cid:82)(cid:81)(cid:86)
We introduce an approach for pre-training egocentric video models using large-scale third-person video datasets.
Learning from purely egocentric data is limited by low dataset scale and diversity, while using purely exocentric (third-person) data introduces a large domain mismatch.
Our idea is to discover latent signals in third-person video that are predictive of key egocentric-speciﬁc properties. In-corporating these signals as knowledge distillation losses during pre-training results in models that beneﬁt from both the scale and diversity of third-person video data, as well as representations that capture salient egocentric proper-ties. Our experiments show that our “Ego-Exo” framework can be seamlessly integrated into standard video models; it outperforms all baselines when ﬁne-tuned for egocen-tric activity recognition, achieving state-of-the-art results on Charades-Ego and EPIC-Kitchens-100. 1.

Introduction
Egocentric video captured by wearable cameras offers a unique perspective into human behavior.
It is the sub-ject of a recent surge in research interest in ﬁrst-person ac-tivity recognition [35, 77], anticipation [21, 1], and video summarization [36, 74, 13] with many valuable future ap-plications in augmented reality and robotics. Compared to third-person videos, egocentric videos show the world through a distinct viewpoint, encode characteristic egocen-tric motion patterns due to body and head movements, and have a unique focus on hands, objects, and faces, driven by the camera wearer’s attention and interaction with their sur-roundings.
However, these unique properties also present a fun-damental challenge for video understanding. On the one hand, learning models purely from egocentric data are lim-ited by dataset scale. Current egocentric video datasets are small (e.g., 90k clips in EPIC-Kitchens-100 [11] vs. 650k in Kinetics-700 [34]) and lack diversity (e.g., videos only (cid:51)(cid:11)(cid:72)(cid:74)(cid:82)(cid:12)(cid:3)(cid:32)(cid:3)(cid:19)(cid:17)(cid:27) (cid:33) (cid:51)(cid:11)(cid:72)(cid:74)(cid:82)(cid:12)(cid:3)(cid:32)(cid:3)(cid:19)(cid:17)(cid:20) (cid:3)(cid:69)(cid:82)(cid:90)(cid:79)(cid:3)(cid:83)(cid:88)(cid:80)(cid:83)(cid:78)(cid:76)(cid:81)(cid:3)(cid:87)(cid:68)(cid:69)(cid:79)(cid:72) (cid:39)(cid:76)(cid:86)(cid:87)(cid:76)(cid:79)(cid:79)(cid:3)(cid:78)(cid:72)(cid:92)(cid:3)(cid:72)(cid:74)(cid:82)(cid:70)(cid:72)(cid:81)(cid:87)(cid:85)(cid:76)(cid:70)(cid:3)(cid:70)(cid:88)(cid:72)(cid:86) (cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:87)(cid:75)(cid:76)(cid:85)(cid:71)(cid:16)(cid:83)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:89)(cid:76)(cid:71)(cid:72)(cid:82)(cid:86) (cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:79)(cid:68)(cid:69)(cid:72)(cid:79) (cid:47)(cid:68)(cid:85)(cid:74)(cid:72)(cid:16)(cid:86)(cid:70)(cid:68)(cid:79)(cid:72)(cid:3)(cid:87)(cid:75)(cid:76)(cid:85)(cid:71)(cid:16)(cid:83)(cid:72)(cid:85)(cid:86)(cid:82)(cid:81)(cid:3)(cid:89)(cid:76)(cid:71)(cid:72)(cid:82)(cid:3) (cid:71)(cid:68)(cid:87)(cid:68)(cid:86)(cid:72)(cid:87)(cid:3)(cid:11)(cid:72)(cid:17)(cid:74)(cid:17)(cid:3)(cid:46)(cid:76)(cid:81)(cid:72)(cid:87)(cid:76)(cid:70)(cid:86)(cid:12) (cid:22)(cid:39)(cid:16)(cid:38)(cid:49)(cid:49)(cid:3)(cid:72)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85) (cid:57)(cid:76)(cid:71)(cid:72)(cid:82)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:3)(cid:83)(cid:85)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)
Figure 1: Main idea. We extract key egocentric signals from large-scale third-person data and distill them into the video backbone during pre-training to guide feature learn-ing for egocentric video tasks with wearable camera video. in kitchen scenes). On the other hand, a purely exo-centric approach that uses more readily available third-person video—the status-quo for pre-training video mod-els [19, 67, 76, 69]—ignores the unique properties of ego-centric video and faces a major domain mismatch. Prior work has shown that this latter strategy, though popular, is insufﬁcient: pre-training egocentric action recognition models with third-person data alone produces signiﬁcantly worse results than pre-training with ﬁrst-person data [62].
In an attempt to bridge the domain gap, prior work explores traditional embedding learning [61, 75] or domain adapta-tion approaches [10], but they require paired egocentric and third-person videos that are either concurrently recorded or annotated for the same set of activities, which are difﬁcult to collect and hence severely limit their scope.
Despite their differences, we hypothesize that the exo-centric view of activity should in fact inform the egocen-tric view. First, humans are able to watch videos of other people performing activities and map actions into their own (egocentric) perspective; babies in part learn new skills in just this manner [48, 55]. Second, exocentric video is not devoid of person-centered cues. For example, a close-up in-structional video captured from the third-person view may 6943
nonetheless highlight substantial hand-object interactions; or video captured with a hand-held phone may follow an event (e.g., a parade) as it unfolds with attentional cues re-lated to a head-mounted camera.
Building on this premise, in this work we ask: “How can we best utilize current video datasets to pre-train egocentric video models?” Our key idea is to discover latent signals in third-person video that approximate egocentric-speciﬁc properties. To that end, we introduce a feature learning approach in which ego-video features are guided by both exo-video activity labels and (unlabeled) ego-video cues, to better align traditional third-person video pre-training with downstream egocentric video tasks.
Speciﬁcally, we introduce a series of ego-inspired tasks that require the video model to be predictive of manipulated objects, spatiotemporal hand-object interaction regions, and general egocentricity. Then we incorporate these tasks into training as knowledge-distillation losses to supplement an action classiﬁcation pre-training objective on third-person video. See Fig. 1.
By design, our video models can continue to enjoy large amounts of labeled third-person training data, while simul-taneously embedding egocentric signals into the learned features, making them a suitable drop-in replacement for traditional video encoders for egocentric video tasks. Fi-nally, our approach does not assume any paired or activity-labeled egocentric videos during pre-training; the egocen-tric signals are directly inferred from third-person video.
Our experiments on three challenging egocentric video datasets show that our “Ego-Exo” framework learns strong egocentric feature representations from third-person video.
On Charades-Ego [62], our model improves over models pre-trained on Kinetics—the standard pre-training and ﬁne-tuning paradigm—by +3.26 mAP, and outperforms meth-ods that speciﬁcally aim to bridge the domain gap between viewpoints. Finally, our pre-trained model achieves state-of-the-art results on EPIC-Kitchens-100 [11], the largest available ﬁrst-person dataset. 2.