Abstract 1.

Introduction 3D video avatars can empower virtual communications by providing compression, privacy, entertainment, and a sense of presence in AR/VR. Best 3D photo-realistic AR/VR avatars driven by video, that can minimize uncanny effects, rely on person-speciﬁc models. However, existing person-speciﬁc photo-realistic 3D models are not robust to lighting, hence their results typically miss subtle facial behaviors and cause artifacts in the avatar. This is a major drawback for the scalability of these models in communication systems (e.g., Messenger, Skype, FaceTime) and AR/VR. This paper addresses previous limitations by learning a deep learning lighting model, that in combination with a high-quality 3D face tracking algorithm, provides a method for subtle and robust facial motion transfer from a regular video to a 3D photo-realistic avatar. Extensive experimental validation and comparisons to other state-of-the-art methods demon-strate the effectiveness of the proposed framework in real-world scenarios with variability in pose, expression, and illumination. Our project page can be found at this website.
Currently, video conferencing (e.g., Zoom, Skype, Mes-senger) is the best 2D available technology for internet com-munication. To allow for more advance levels of communi-cation and sense of presence, Augmented Reality (AR) and
Virtual Reality (VR) technologies aim to build 3D person-alized avatars, and superimpose virtual objects in the real space. If successful, this new form of face-to-face interac-tion will allow extended remote work experiences that can improve productivity, reducing cost and stress of commut-ing, have a huge impact on the environment, and overall improving the work/life balance.
Today most real-time systems for avatars in AR are cartoon-like (e.g., Apple Animoji, Tiktok FaceAnimation,
Hyprsense, Loom AI); on the other hand, digital creators in movies have developed uncanny digital humans using ad-vanced computer graphics technology and person-speciﬁc (PS) models (e.g., Siren). While some of these avatars can be driven in real-time from egocentric cameras (e.g., Doug character made by digital domain), building the PS model is an extremely time-consuming and hand-tuned process that 13059
3D face photo-realistic prevents democratizing this technology. This paper con-tributes toward this direction, and it proposes new algo-rithms to robustly and accurately drive 3D video-realistic avatars from monocular cameras to be consumed by AR/VR displays (see Fig. 1).
Model-based reconstruc-tion/animation from a video has been a core area of research in computer vision and graphics in the last thirty years [3, 4, 6, 8, 14, 16, 23, 28, 33, 49, 44, 59, 2].
While different versions of morphable models or active appearance models have provided good facial animation results, the existing 3D models do not provide the quality that is needed it for a good immersive viewing experience in AR/VR. In particular, the complex lighting, motion, and other in-the-wild conditions do result in artifacts in the avatar due to poor decouple of rigid and non-rigid motion, as well as, no accurate texture reconstruction. To tackle this problem, we build on recent work on Deep Appearance
Model (DAM) [27] that learns a person-speciﬁc model from a multi-view capture setup. [27] can render photo-realistic avatars in a VR headset by inferring 3D geometry and view-dependent texture from egocentric cameras .
This paper extends DAM [27] with a new deep light-ing adaptation method to recover subtle facial expressions from monocular videos in-the-wild and transfer them to a 3D video-realistic avatar. The method is able to decouple rigid and non-rigid facial motions, as well as, shape, ap-pearance and lighting from videos in-the-wild. Our method combines a prior lighting model learned in a lab-controlled scenario and adapts it to the in-the-wild video, recovering accurate texture and geometric details in the avatar from images with complex illuminations and challenging poses (e.g. proﬁle). There are two main contributions of our work.
First, we provide a framework for ﬁtting a non-linear ap-pearance model (based on a variational auto-encoder) to in-the-wild videos. Second, we propose a new lighting trans-fer module to learn a global illumination model. Experi-mental validation shows that our proposed algorithm with deep lighting adaptation outperforms state-of-the-art meth-ods and provides robust solutions in realistic scenarios. 2.