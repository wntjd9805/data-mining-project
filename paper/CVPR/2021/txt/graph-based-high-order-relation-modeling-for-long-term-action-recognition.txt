Abstract
T
Long-term actions involve many important visual con-cepts, e.g., objects, motions, and sub-actions, and there are various relations among these concepts, which we call ba-sic relations. These basic relations will jointly affect each other during the temporal evolution of long-term actions, which forms the high-order relations that are essential for long-term action recognition. In this paper, we propose a
Graph-based High-order Relation Modeling (GHRM) mod-ule to exploit the high-order relations in the long-term ac-tions for long-term action recognition. In GHRM, each ba-sic relation in the long-term actions will be modeled by a graph, where each node represents a segment in a long video. Moreover, when modeling each basic relation, the information from all the other basic relations will be in-corporated by GHRM, and thus the high-order relations in the long-term actions can be well exploited. To bet-ter exploit the high-order relations along the time dimen-sion, we design a GHRM-layer consisting of a Temporal-GHRM branch and a Semantic-GHRM branch, which aims to model the local temporal high-order relations and global semantic high-order relations. The experimental results on three long-term action recognition datasets, namely, Break-fast, Charades, and MultiThumos, demonstrate the effec-tiveness of our model. 1.

Introduction
In the computer vision community, there have been many studies on action recognition. Approaches such as two-stream networks [24], Inﬂated-3D networks (I3D) [2], and temporal segment networks (TSN) [28] have shown their effectiveness for action recognition. The actions these stud-ies mainly focus on are from pre-trimmed clips of videos
*Corresponding author open the book grab a book close the book connections put the book back find a picture stare at the picture hold the picture
T
T
Figure 1. 10 frames selected from the long video Q948H in Cha-rades [23] show “A person sits on a bed, grabs a book off of a table, ﬁnds a picture in the book, puts the book back, and stares at the picture”. The purple and cyan cues show two basic relation instances, and there are some connections between them. As the woman opens the book but then closes it quickly without reading it, we can infer that the picture is found from this book. Addition-ally, the woman does not put the book and picture back together, so she may going to look at the picture. Therefore, the information in these two basic relation instances can affect each other. and last only a few seconds, which we call short-term ac-tions. However, the videos we are usually exposed to in daily life are more complex long videos, so trimming short-term actions from those long videos is very time-consuming and labor-intensive. More importantly, trimming short-term actions from long videos without considering the possible internal relations between them will prevent action recog-nition research from achieving the goal of understanding complex human behaviors.
In contrast to short-term actions, long-term actions [10, 11, 32] are actions in untrimmed videos that have a very long execution time. A long-term action generally contains multiple sub-actions, among which some complicated rela-tions may exist. The goal of long-term action recognition is to identify the long-term action or to identify all sub-actions that occur during the execution of the long-term ac-8984
tion. Thus, only long-term action category labels or all sub-action category labels need to be provided for long-term ac-tion recognition task, which will largely reduce the cost of dataset annotation. Therefore, long-term action recognition provides a feasible way for us to understand complex hu-man behaviors.
In the long-term actions, there are many important vi-sual concepts, e.g., objects, motions, and sub-actions. Var-ious relations may exist among these visual concepts, such as relations between humans and objects, relations between sub-actions, etc., which we call basic relations. These ba-sic relations will jointly affect each other during the tempo-ral evolution of long-term actions, which forms the high-order relations in the long-term actions. Therefore, ex-ploiting the high-order relations in long-term actions from these basic relations is the key to recognizing long-term ac-tions. In the example from the Charades [23] dataset shown in Figure 1, we can ﬁnd two visual cues in this long video.
The purple cue shows an instance of the basic relation, i.e.,
“grab a book → open the book → close the book → put the book back”. The cyan cue shows another instance of the basic relation, i.e., “ﬁnd a picture → hold the picture
→ stare at the picture”. There are some connections be-tween these two basic relation instances, which forms the high-order relations. For example, there is a connection be-tween the actions open the book, close the book in the
ﬁrst basic relation instance and the action ﬁnd a picture in the second basic relation instance, as ﬁnding a picture from a book makes it reasonable that a person would open the book but then close it quickly without reading it. Simi-larly, the actions close the book, put the book back in the
ﬁrst instance can provide a clue for the action stare at the picture in the second instance. Therefore, for long-term ac-tion recognition, it is important to model each basic relation well. More importantly, to exploit the high-order relations in the long-term actions, the information from all the other basic relations should be incorporated while modeling each basic relation.
In this work, we propose a novel Graph-based High-order Relation Modeling (GHRM) module, which exploits the high-order relations from the basic relations in the long-term actions for recognition. In GHRM, each basic relation in the long-term actions will be modeled by a graph con-volutional module, with each segment in a long video as a graph node. As these basic relations can affect each other and then form the high-order relations during the temporal evolution of long-term actions, GHRM will incorporate the information from all the other basic relations when mod-eling each basic relation, thus the high-order relations in the long-term actions can be well exploited. To better ex-ploit the high-order relations in the long-term actions along the time dimension, our model constructs the GHRM-layer based on the GHRM module, which considers both the local temporal high-order relations and the global semantic high-order relations by using a Temporal-GHRM branch and a Semantic-GHRM branch, respectively. The Temporal-GHRM branch reasons the graph locally to leverage the temporal context between neighboring segments, while the
Semantic-GHRM branch will reason the graph globally to leverage the semantic context between segments without the limitation of temporal distance.
To summarize, the main contributions of this work are:
• A novel GHRM module is proposed to exploit the high-order relations from the basic relations in the long-term actions.
• A GHRM-layer consisting of a Temporal-GHRM branch and a Semantic-GHRM branch is designed to model the local temporal high-order relations and the global semantic high-order relations in the long-term actions.
Our proposed model achieves state-of-the-art perfor-mance on three popular long-term action recognition datasets: Breakfast [15], Charades [23], and MultiThumos
[31], which demonstrates the effectiveness of our model. 2.