Abstract
Current model extraction attacks assume that the adver-sary has access to a surrogate dataset with characteris-tics similar to the proprietary data used to train the vic-tim model. This requirement precludes the use of existing model extraction techniques on valuable models, such as those trained on rare or hard to acquire datasets. In con-trast, we propose data-free model extraction methods that do not require a surrogate dataset. Our approach adapts techniques from the area of data-free knowledge transfer for model extraction. As part of our study, we identify that the choice of loss is critical to ensuring that the extracted model is an accurate replica of the victim model. Further-more, we address difﬁculties arising from the adversary’s limited access to the victim model in a black-box setting.
For example, we recover the model’s logits from its prob-ability predictions to approximate gradients. We ﬁnd that the proposed data-free model extraction approach achieves high-accuracy with reasonable query complexity – 0.99⇥ and 0.92⇥ the victim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries respectively. 1.

Introduction
Machine learning (ML) and deep learning, in particu-lar, often require large amounts of training data to achieve high performance on a particular task [39]. Curating such data necessitates signiﬁcant time and monetary invest-ment [17, 12]. Thus, the resulting ML model becomes valu-able intellectual property, especially when considering the computing resources and human expertise required [5, 13].
Often to monetize these models, companies make them available as a service via APIs over the web (MLaaS). These models are also deployed to end-user devices, making their
*equal contribution
†Work done while an intern at the Vector Institute. predictions directly accessible to customers. However, the exposure of the model’s predictions represents a signiﬁcant risk as an adversary can leverage this information to steal the model’s knowledge [26, 41, 7, 32, 31, 11, 28, 19]. The threat of such model extraction attacks is two-fold: adver-saries may use the stolen model for monetary gains or as a reconnaissance step to mount further attacks [33, 37].
While model extraction is in many ways similar to model distillation, it differs in that the victim’s proprietary train-ing set is not accessible to the adversary. To stage a model extraction attack, the adversary typically queries the victim using samples from a surrogate dataset with semantic or dis-tributional similarity to the original training set [31]. In the classiﬁcation setting, the victim’s response may be limited to the most-likely label [6] or include conﬁdence values for different class labels [19]. The number of queries—i.e., the query complexity—is also an important consideration for the adversary. The greater the query complexity, the higher the cost of the attack—unless the victim model is available ofﬂine (e.g., deployed on-device).
In this work, we ﬁrst demonstrate in Section 3 that the success of current established practices for model extrac-tion, which often take the form of distillation, depends on the closeness of the surrogate distribution to the victim’s proprietary training distribution. This ﬁnding has important implications for the practicality of existing model extraction techniques.
To remedy this, we propose techniques for data-free model extraction (DFME). In short, we demonstrate the fea-sibility of extracting ML models without any knowledge of the distribution of the proprietary training data. In practice, gathering a surrogate dataset for the purpose of model ex-traction can be a very expensive process, both in terms of the time and money required to curate it. In particular, the most valuable models are often those for which it is most challenging to curate an appropriate surrogate dataset, i.e., when the victim model’s value arises from its proprietary 4771
dataset. Our work builds on recent advances in data-free knowledge distillation, which involve a generative model to synthesize queries that maximize disagreement between the student and teacher models [27, 14]. Here, the teacher is the victim model whereas the student is the stolen extracted model. We innovate on two fronts: the choice of loss to quantify student-teacher disagreement and an approach for training the generator without the ability to backpropagate through the teacher to compute its gradients (because we only have black-box access to the victim/teacher predictions in our setting). We observe that it is essential to ensure the stability of the loss computed, and ﬁnd that the `1 norm loss is particularly conducive to data-free model extraction. We also demonstrate that using inexpensive gradient approxi-mation (based on the victim model’s outputs) is sufﬁcient to train a generative model that produces queries relevant to distill the knowledge of a victim to a student model. In summary, our main contributions are:
• We demonstrate in Section 3 that successful distillation-based model extraction attacks require the adversary to sample queries from a surrogate dataset whose distribution is close to the victim training data.
• In Section 4, we propose data-free model extraction (DFME) to extract ML models without knowledge of private training data, and only using the victim’s black-box predictions. As a by-product of DFME needing to approximate gradients of the victim, this leads us to present a method for recovering per-example logits out of the probability vector output by a ML model.
• We validate1 our DFME technique in Section 5 on the
SVHN and CIFAR10 datasets and successfully extract a model with 0.99x the victim accuracy with only 2M queries for SVHN, and 0.92x the victim accuracy with 20M queries for CIFAR10.
• An ablation study of our approach in Section 6 pro-vides two key insights: (1) measuring disagreement between the victim and extracted models with the `1 norm achieves higher extraction accuracy than losses previously considered in the literature; (2) weak gradi-ent estimates yield sufﬁcient signal to train a generator despite only having access to the victim’s predictions. 2.