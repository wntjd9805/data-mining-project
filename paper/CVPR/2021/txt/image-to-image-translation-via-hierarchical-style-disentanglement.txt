Abstract
Original Labels
Tags
Attributes
Styles
Recently, image-to-image translation has made signiﬁ-cant progress in achieving both multi-label (i.e., translation conditioned on different labels) and multi-style (i.e., gener-ation with diverse styles) tasks. However, due to the unex-plored independence and exclusiveness in the labels, exist-ing endeavors are defeated by involving uncontrolled ma-nipulations to the translation results. In this paper, we pro-pose Hierarchical Style Disentanglement (HiSD) to address this issue. Speciﬁcally, we organize the labels into a hierar-chical tree structure, in which independent tags, exclusive attributes, and disentangled styles are allocated from top to bottom. Correspondingly, a new translation process is de-signed to adapt the above structure, in which the styles are identiﬁed for controllable translations. Both qualitative and quantitative results on the CelebA-HQ dataset verify the ability of the proposed HiSD. The code has been released at https://github.com/imlixinyang/HiSD. 1.

Introduction
Recently, deep learning based methods have achieved promising results in image-to-image translation area. Early works [47, 41, 21, 35] learn a deterministic mapping be-tween two domains, which give rise to two emergent issues: translating the inputs conditioned on multiple labels, and generating diverse outputs with multiple styles. The former is termed the multi-label task, and the latter is termed the multi-style (or multi-modal) task. For the multi-label task, methods [5, 11, 20, 38] combine the labels into the trans-lator. For the multi-style task, methods [15, 18, 1, 48] in-corporate latent codes drawn from Gaussian noise into the translator. Recent uniﬁed solutions for these tasks can be classiﬁed into two categories. (i). Works [34, 36, 42, 19] learn the shared style by injecting the style code concate-∗ Corresponding Author.
With_Bangs
Bangs
With_Glasses
Blond_Hair
Black_Hair
Brown_Hair
Glasses
Hair color with without with without blond black brown unsupervised
Figure 1: Hierarchical Style Disentanglement. The origi-nal labels are organized into independent tags and exclusive attributes. We aim to disentangle the styles to represent the clear manifestations in attributes, in an unsupervised way. nated with the target labels into the generator. The shared style code does not have an explicit effect on the source im-age without changed labels, which is shown in Figure 3(a). (ii). StarGANv2 [6] learns the mixed style by using the tar-get label to index the mapped style code. It continues to use the hypothesis of StarGAN [5] that an image domain is the set of images sharing the same labels. The translations frequently involve unnecessary manipulations like changing facial identity and affecting background, as shown in Fig-ure 3(b). In addition, they cannot independently learn the respective styles for bangs, glasses, and hair color. These uncontrollable translations severely limit their practical use.
We propose a novel framework, called Hierarchical Style
Disentanglement, to solve the above limitations. We notice the general independence and exclusiveness among most la-bel annotations. For example, in CelebA, original binary labels ‘With Bangs’ and ‘With Glasses’ are independent, while ‘Blond Hair’ and ‘Black Hair’ are exclusive. Ac-cordingly, as shown in Figure 1, we organize the original labels into a hierarchical structure, including independent tags and exclusive attributes. The tags represent different accordance of attributes, and every image is relabeled to 8639
Input
‘Glasses’ to ‘with’
‘Hair color’ to ‘…’
Input
‘Glasses’ to reference
‘Bangs’ to reference e c n e r e f e
R
Input
Input
‘black’
‘brown’
Input
‘blond’
‘brown’
Input
‘black’
‘blond’ e c n e r e f e r o t
’ s g n a
B
‘ (a) Multi-style task  (b) Multi-attribute task  (c) Multi-tag task 
Figure 2: Selected results of our method on CelebA-HQ. (a). The multi-style task, which aims to generate diverse tag-relevant styles. The styles in our framework can be either generated by random latent codes or extracted from reference images. (b). The multi-attribute task, which aims to translate images into multiple possible attributes. (c). The multi-tag task, which aims to manipulate multiple tags of images simultaneously and independently.
Input
Tags and attributes
B
G
H separate
Z
Noise
Z organize
Z
Labels b g h hh index map
B
G
H concat
Shared
Mixed
Bangs
Glasses
Hair color diverse translations. For example, the style for tag ‘Glasses’ can disentangle different glasses, such as myopic glasses, sunglasses, and reading glasses in images, without super-vised annotations. We introduce different modules to gen-erate, extract, and efﬁciently manipulate the disentangled tag-relevant styles. In the cycle-translation path, we consis-tently optimize both generated and extracted styles to ma-nipulate images realistically and accurately. Through cy-cle consistency and style consistency, the generated and ex-tracted styles are guaranteed to include the detailed mani-festations for tags. To guarantee the disentanglement, we introduce a local translator, which uses the attention mask to avoid the global manipulations; and a tag-irrelevant con-ditional discriminator, which uses redundant labels in the annotations to prevent that these implicit conditions are ma-nipulated by the translations. In Figure 2, we show some selected results of our method on CelebA-HQ. (a) SDIT (b) StarGANv2 (c) Our method
Our contributions include:
Figure 3: Comparison of different style codes. Our style codes are identiﬁed to the hierarchical structure. one of the attributes for each tag. For example, accord-ing to the tag ‘Glasses’, the attribute of images can be ei-ther ‘with’ or ‘without’. Thus the multi-label issue is di-vided into two sub-tasks: multi-attribute task, which trans-lates a tag to multiple possible attributes; and multi-tag task, which manipulates multiple tags simultaneously. However, the human-annotated attributes cannot represent the clear manifestations in images for tags.
In this paper, we take the clear manifestations in images for tags as tag-relevant styles. The tag-relevant styles, which are identiﬁed to the tags and attributes, provide a more controllable manner for
• We propose HiSD to address the issues in recent multi-label and multi-style image-to-image translation meth-ods by organizing the labels into a hierarchical struc-ture, where independent tags, exclusive attributes, and disentangled styles are allocated from top to bottom.
• To make the styles identiﬁed to the tags and attributes, we carefully redesign the modules, phases, and objec-tives. For unsupervised style disentanglement, we in-troduce two architectural improvements to avoid the global manipulations and implicit attributes to be ma-nipulated during the translations.
• We conduct extensive experiments to prove the effec-tiveness of our model. 8640    
2.