Abstract
Over the last few years, we have witnessed tremendous progress on many subtasks of autonomous driving includ-ing perception, motion forecasting, and motion planning.
However, these systems often assume that the car is accu-rately localized against a high-deﬁnition map. In this paper we question this assumption, and investigate the issues that arise in state-of-the-art autonomy stacks under localization error. Based on our observations, we design a system that jointly performs perception, prediction, and localization.
Our architecture is able to reuse computation between the three tasks, and is thus able to correct localization errors efﬁciently. We show experiments on a large-scale autonomy dataset, demonstrating the efﬁciency and accuracy of our proposed approach. 1.

Introduction
Many tasks in robotics can be broken down into a series of subproblems that are easier to study in isolation, and facil-itate the interpretability of system failures [75]. In particular, it is common to subdivide the self-driving problem into ﬁve critical subtasks: (i) Localization: placing the car on a high-deﬁnition (HD) map with centimetre-level accuracy. (ii)
Perception: estimating the number and location of dynamic objects in the scene. (iii) Prediction: forecasting the trajecto-ries and actions that the observed dynamic objects might do in the next few seconds. (iv) Motion planning: coming up with a desired trajectory for the ego-vehicle, and (v) Control: using the actuators (i.e., steering, brakes, throttle, etc.) to execute the planned motion.
Moreover, it is common to solve the above problems se-quentially, such that the output of one sub-system is passed as input to the next, and the procedure is repeated iteratively over time. This classical approach lets researchers focus on well-deﬁned problems that can be studied independently, and these areas tend to have well-understood metrics that measure progress on their respective sub-ﬁelds. For sim-plicity, researchers typically study autonomy subproblems under the assumption that its inputs are correct. For exam-ple, state-of-the-art perception-prediction (P2) and motion planning (MP) systems often take HD maps as input, thereby assuming access to accurate online localization. We focus our attention on this assumption and begin by studying the effect of localization errors on modern autonomy pipelines.
Here, we observe that localization errors can have serious consequences for P2 and MP systems, resulting in missed detections and prediction errors, as well as bad planning that leads to larger discrepancies with human trajectories, and in-creased collision rates. Please refer to Fig. 1 for an example of an autonomy error caused by inaccurate localization.
In contrast to the classical formulation, recent systems have been designed to perform multiple autonomy tasks jointly. This joint formulation often comes with a shared neural backbone that decreases computational and system complexity, while still producing interpretable outputs that make it easier to diagnose system failures. However, these approaches have so far been limited to jointly performing perception and prediction (P2) [11,13,41,43], P2 and motion planning (P3) [52, 69, 70], semantic segmentation and local-ization [51] or road segmentation and object detection [60].
In this paper, and informed by our analysis of the effects of localization error, we apply the joint design philosophy to the tasks of localization, perception, and prediction; we refer to this joint setting as LP2. We design an LP2 system that shares computation between the tasks, which makes it possible to perform localization with as little as 2 ms of computational overhead while still producing intepretable localization and P2 outputs. We evaluate our proposed sys-tem on a large-scale dataset in terms of motion planning metrics, and show that the proposed approach matches the performance of a traditional system with separate localiza-tion and perception components, while being able to correct localization errors online, and having reduced run time and engineering complexity. 4679
Plan for GT pose: reality
Plan for incorrect pose: if pose were correct
Plan for incorrect pose: reality
Figure 1: A scenario where a small amount of localization error results in a collision. The top row visualizes the ﬁrst time step, and the bottom row visualizes a later time step where a collision occurs. GT labels are black rectangles, and the pale blue rectangles are forecasted object trajectories. The SDV is the red rectangle, with its GT trajectory in a dark blue rectangle. The samples predicted by the motion planner are shown as orange lines. The 3 columns visualize different variants of the same scenario. (Left) The planned trajectory of the SDV when there is no localization error. (Middle) What the SDV “thinks” is happening, based on its estimated pose that has error (x, y, yaw) = (10 cm, 0 cm, 1.5 deg). (Right) What the SDV is actually doing when subject to the pose error; this is the same trajectory as shown in the middle image, but rigidly transformed so that the initial pose agrees with the GT pose. The collision (red circle) occurs because the yellow vehicle is not perceived at t = 0 due to occlusion (by the cyan vehicle); the localization error then causes the SDV to go into the lane of opposite trafﬁc which results in a collision. 2.