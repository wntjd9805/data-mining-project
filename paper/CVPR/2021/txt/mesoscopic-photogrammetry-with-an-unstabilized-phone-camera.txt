Abstract
We present a feature-free photogrammetric technique that enables quantitative 3D mesoscopic (mm-scale height variation) imaging with tens-of-micron accuracy from se-quences of images acquired by a smartphone at close range (several cm) under freehand motion without addi-tional hardware. Our end-to-end, pixel-intensity-based ap-proach jointly registers and stitches all the images by es-timating a coaligned height map, which acts as a pixel-wise radial deformation ﬁeld that orthorectiﬁes each cam-era image to allow plane-plus-parallax registration. The height maps themselves are reparameterized as the out-put of an untrained encoder-decoder convolutional neu-ral network (CNN) with the raw camera images as the in-put, which effectively removes many reconstruction arti-facts. Our method also jointly estimates both the camera’s dynamic 6D pose and its distortion using a nonparametric model, the latter of which is especially important in meso-scopic applications when using cameras not designed for imaging at short working distances, such as smartphone cameras. We also propose strategies for reducing computa-tion time and memory, applicable to other multi-frame reg-istration problems. Finally, we demonstrate our method us-ing sequences of multi-megapixel images captured by an un-stabilized smartphone on a variety of samples (e.g., painting brushstrokes, circuit board, seeds). 1.

Introduction
The photogrammetric problem of reconstructing 3D rep-resentations of an object or scene from 2D images taken from multiple viewpoints is common and well studied, fea-turing prominently in techniques such as multi-view stereo (MVS) [15], structure from motion (SfM) [51, 57, 46], and simultaneous localization and mapping (SLAM) [13]. Im-plicit in these 3D reconstructions is knowledge of the cam-era parameters, such as camera position, orientation, and distortions, which are jointly estimated in SfM and SLAM.
Photogrammetry tools have been developed and applied
Figure 1. Our method jointly stitches multi-megapixel images ac-quired under freehand motion at close range and reconstructs high-accuracy height maps without precalibration of camera distortion. to both long-range, macro-scale applications [41], such as building-scale reconstructions or aerial topographical map-ping, and close-range, meter-scale applications [33], such as industrial metrology. However, comparatively less work has been done to push photogrammetry to mesoscopic (mm variation) and microscopic scales, where additional issues arise, such as more limited depths of ﬁeld and increased impact of camera distortion. Existing approaches at smaller scales typically require very careful camera distortion pre-calibration, expensive cameras, dedicated setups that allow well-controlled camera or sample motion (e.g., with a dedi-cated rig), or attachment of control points to the object [33].
Here, we show that a smartphone is capable of obtain-ing quantitative 3D mesoscopic images of objects with 100
µm- to mm-scale height variations at tens-of-micron accura-cies with unstabilized, freehand motion and without precal-ibration of camera distortion (Fig. 1). To achieve this, we present a new photogrammetric reconstruction algorithm that simultaneously stitches the multi-perspective images after warping to a common reference frame, reconstructs 7535
sample’s 3D height proﬁle, and estimates the camera’s posi-tion, orientation, and distortion (via a piecewise linear, non-parametric model) in an end-to-end fashion without relying on feature point extraction and matching. Our careful mod-elling of distortions is especially important for mesoscopic applications. Our method also features a reparameteriza-tion of the camera-centric height maps as the outputs of a single untrained convolutional neural network (CNN) with the raw camera images at the input (akin to the deep image prior (DIP) [52]), which is optimized instead of the height map itself. Since the camera-centric height maps are by de-sign coaligned with the camera images, they are automati-cally registered once the camera images are registered. As we will demonstrate, both the use of an untrained CNN and careful modeling of distortion substantially reduce recon-struction artifacts, thus allowing high-accuracy height map estimation without camera precalibration or stabilization. 2.