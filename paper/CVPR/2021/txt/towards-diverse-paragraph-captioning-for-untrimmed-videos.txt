Abstract
Video paragraph captioning aims to describe multiple events in untrimmed videos with descriptive paragraphs.
Existing approaches mainly solve the problem in two steps: event detection and then event captioning. Such two-step manner makes the quality of generated paragraphs highly dependent on the accuracy of event proposal detection which is already a challenging task.
In this paper, we propose a paragraph captioning model which eschews the problematic event detection stage and directly generates paragraphs for untrimmed videos. To describe coherent and diverse events, we propose to enhance the conventional temporal attention with dynamic video memories, which progressively exposes new video features and suppresses over-accessed video contents to control visual focuses of the model.
In addition, a diversity-driven training strat-egy is proposed to improve diversity of paragraph on the language perspective. Considering that untrimmed videos generally contain massive but redundant frames, we fur-ther augment the video encoder with keyframe awareness to improve efﬁciency. Experimental results on the ActivityNet and Charades datasets show that our proposed model sig-niﬁcantly outperforms the state-of-the-art performance on both accuracy and diversity metrics without using any event boundary annotations. Code will be released at https:
//github.com/syuqings/video-paragraph. 1.

Introduction
Describing videos with natural language sentences, a.k.a. video captioning, has attracted increasing research attentions due to the rapid emergence of videos in our lives.
The dominant video captioning task [19, 46, 31, 42, 45] fo-cuses on generating a single sentence to describe a care-fully trimmed video which mainly contains one major event within short duration such as 10-20 seconds [40, 37]. How-ever, the videos in the wild are mostly untrimmed with
∗Equal contribution. This work was performed when Shizhe Chen was at Renmin University of China.
†Corresponding author. rich temporal event structures. A single sentence is insufﬁ-cient to convey ﬁne-grained information in such untrimmed videos. Therefore, recent works [39, 21, 13] have attempted to generate a story-oriented paragraph with multiple sen-tences to comprehensively describe video contents.
Existing works [39, 21, 13] mainly adopt a two-stage framework for video paragraph captioning: ﬁrstly detect-ing event segments in the video, and then generating the event description for each segment. Despite being reason-able, the framework requires temporal segment coordinates for descriptions in the paragraph to train the model, which are expensive to annotate. Moreover, since event categories are extremely diverse in open-domain untrimmed videos, it is quite challenging to detect precise event segments com-pared with the action detection task [25, 47, 4], which has a
ﬁxed category list. The poorly detected events greatly harm the performance of paragraph captioning in existing frame-works. As a result, several works [21, 13] use ground-truth event segments to generate video paragraphs, which cannot generalize to videos without such event annotations.
However, is event detection really necessary for video paragraph captioning? Let’s review a simpler task of im-age paragraph captioning. The state-of-the-art approaches
[16, 17] directly generate sentences from images without predicting sequences of image coordinates. The generated paragraphs have shown good capability to capture descrip-tive logic such as from foreground to background. Moti-vated by these works, we aim to eschew the costly event segment detection process, and efﬁciently generate video paragraph descriptions in a single stage.
Compared with the image counterpart, there are mainly three challenges for video paragraph captioning when event segments are unavailable. Firstly, an untrimmed video gen-erally consists of hundreds or thousands of frames, while an image contains much fewer region candidates to be attended to. Therefore, it consumes more computation resources during description generation. Secondly, the large number of frame candidates also makes it hard for the captioning model to learn an effective attention mechanism to form a coherent descriptive logic and describe diverse events in the video, especially when the training examples are limited. 11245
Thirdly, the captioning model usually tends to generate re-dundant words and phrases that are of high frequency in the dataset especially for the long paragraph generation.
In this work, we propose an one-stage framework to tackle the above challenges for diverse and efﬁcient video paragraph generation. Considering that there are many re-dundant frames in untrimmed videos, we propose to auto-matically select keyframes during the video encoding via additional video semantic summary loss and sparsity loss.
In this way, only keyframes are used to generate the long paragraph during inference, which improves the computa-tional efﬁciency. To guide the model in effective description logic learning for diverse and coherent events, we propose to improve conventional temporal attention with dynamic video memory which tracks and controls visual focuses in the video. It includes an “add” operation to progressively expose new video frames to the model, and an “erase” op-eration to suppress over-accessed video contents. To fur-ther improve diversity of generated paragraphs from lan-guage perspective, we improve the training objective with token-level and sequence-level high-frequency penalties to encourage generating more unique expressions. Experi-ments show that our model outperforms two-stage methods which even utilize ground-truth event segments on Activi-tyNet dataset, and also achieves the state-of-the-art result on
Charades dataset which does not have temporal annotations.
The main contributions of this work are as follows:
• To the best of our knowledge, we are the ﬁrst to eschew event detection stage and directly generate paragraphs for untrimmed videos, which avoids the dependence on expensive event temporal annotations.
• We propose an attention mechanism with dynamic video memories and diversity-driven training objec-tives to generate coherent and diverse paragraph from video and language perspectives, and improve genera-tion efﬁciency via keyframe-aware video encoder.
• Our model achieves state-of-the-art results on both
ActivityNet and Charades datasets without using any event boundary annotations. 2.