Abstract
We introduce WyPR, a Weakly-supervised framework for
Point cloud Recognition, requiring only scene-level class tags as supervision. WyPR jointly addresses three core 3D recognition tasks: point-level semantic segmentation, 3D proposal generation, and 3D object detection, coupling their predictions through self and cross-task consistency losses. We show that in conjunction with standard multiple-instance learning objectives, WyPR can detect and segment objects in point cloud data without access to any spatial labels at training time. We demonstrate its efﬁcacy us-ing the ScanNet and S3DIS datasets, outperforming prior state of the art on weakly-supervised segmentation by more than 6% mIoU. In addition, we set up the ﬁrst benchmark for weakly-supervised 3D object detection on both datasets, where WyPR outperforms standard approaches and estab-lishes strong baselines for future work. 1.

Introduction
Recognition (i.e., segmentation and detection) of 3D ob-jects is a key step towards scene understanding. With the recent development of consumer-level depth sensors (e.g.,
LiDAR [13, 43]) and the advances of computer vision al-gorithms, 3D data collection has become more convenient and inexpensive. However, existing 3D recognition sys-tems often fail to scale as they rely on strong supervi-sion, such as point level semantic labels or 3D bounding
⇤Work partly done during an internship at Facebook AI Research. boxes [9, 29, 32], which are time consuming to obtain. For example, while the popular large-scale indoor 3D dataset
ScanNet [10] was collected by only 20 people, the anno-tation effort involved more than 500 annotators spending nearly 22.3 minutes per scan. Furthermore, due to the high annotation cost, existing 3D object detection datasets have limited themselves to a small number of object classes. This time consuming labeling process is a major bottleneck pre-venting the community from scaling 3D recognition.
Motivated by this observation, we study 3D weakly-supervised learning with only scene-level class tags avail-able as supervision to train semantic segmentation and ob-ject detection models. Scene-level tags are very efﬁcient to annotate, taking only a second or less for each object in the scene [36]. Hence, methods that rely on such supervision can be scaled more easily than those that rely on box-level supervision.
For this we develop the novel weakly-supervised frame-work called WyPR, shown in Fig. 1. Using just scene level tags, it jointly learns both segmentation of point cloud and detection of 3D boxes. Why should joint learning of seg-mentation and detection perform better than independently learning the two tasks? First, since these two tasks are re-lated, joint training is mutually beneﬁcial for representation learning. Second, these tasks naturally constrain each other, leading to effective self-supervised objectives that further improve performance. For example, the semantic labels of points within a bounding box should be consistent, and vice versa. Lastly, directly learning to regress to dimen-sions of 3D bounding boxes, as common in supervised ap-113204
Methods
Weak labels
Tasks
Dataset
[46] 2D boxes det indoor
[56] 2D inst seg det outdoor
[59] sparse label seg indoor & objects
[51] 2D sem seg seg indoor
[53] region & scene tags seg indoor
[33] scene tags det outdoor
WyPR scene tags det + seg indoor
Table 1: Summary of closely related work in weakly-supervised 3D recognition. Compared to prior work, our proposed method (WyPR) uses the readily available scene tags, and jointly learns detection and segmentation in the more challenging indoor room setting. proaches [28, 29, 39], is extremely challenging using weak labels. Learning weakly-supervised segmentation ﬁrst per-mits a two-stage detection framework, where object propos-als are generated bottom-up conditioned on segmentation prediction and further classiﬁed using a weakly-supervised detection algorithm.
To achieve this, WyPR operates on point cloud data of complex indoor scenes and combines a weakly-supervised semantic segmentation stage (§ 3.1) with a weakly-supervised object detection stage (§ 3.2). The latter takes as input the geometric representation of the input scene and a set of computed 3D proposals from GSS, our novel
Geometric Selective Search algorithm (§ 3.3). GSS uses local geometric structures (e.g., planes) and the previously computed segmentation, for bottom-up proposal genera-tion. Due to the uninformative nature of weak labels, weakly-supervised frameworks often suffer from noisy pre-diction and high variance. We address this by encourag-ing both cross-task and cross-transformation consistency through self-supervised objectives. We evaluate WyPR on standard 3D datasets, i.e., ScanNet and S3DIS (§ 4), im-proving over prior work on weakly-supervised 3D segmen-tation by more than 6% mIoU, and establishing new bench-marks and strong baselines for weakly-supervised 3D de-tection.
Our contributions are as follows: 1) a novel point cloud framework to jointly learn weakly-supervised semantic seg-mentation and object detection, which signiﬁcantly out-performs single task baselines; 2) an unsupervised 3D proposal generation algorithm, geometric selective search (GSS), for point cloud data; and 3) state-of-the-art results on weakly-supervised semantic segmentation, and bench-marks on weakly-supervised proposal generation and object detection. 2.