Abstract
Deep Neural Networks (DNN) could forget the knowl-edge about earlier tasks when learning new tasks, which is known as catastrophic forgetting. To learn new task with-out forgetting, recently, the mask-based learning method (e.g. piggyback [10]) is proposed to address this issue by learning only a binary element-wise mask, while keeping the backbone model ﬁxed. However, the binary mask has limited modeling capacity for new tasks. A more recent work [5] proposes a compress-grow-based method (CPG) to achieve better accuracy for new tasks by partially train-ing backbone model, but with order-higher training cost, which makes it infeasible to be deployed into popular state-of-the-art edge-/mobile-learning. The primary goal of this work is to simultaneously achieve fast and high-accuracy multi task adaption in continual learning setting. Thus mo-tivated, we propose a new training method called Kernel-wise Soft Mask (KSM), which learns a kernel-wise hybrid binary and real-value soft mask for each task. Such a hybrid mask can be viewed as a superposition of a binary mask and a properly scaled real-value tensor, which offers a richer representation capability without low-level kernel support to meet the objective of low hardware overhead. We val-idate KSM on multiple benchmark datasets against recent state-of-the-art methods (e.g. Piggyback, Packnet, CPG, etc.), which shows good improvement in both accuracy and training cost. 1.

Introduction
It is well-known that human and animals can learn new tasks without forgetting old ones. However, a practical lim-itation of Deep Neural Network (DNN) is their high degree of specialization to a single task and domain. For example, given a backbone DNN model, conventional ﬁne-tuning of the model for new tasks could easily result in the forget-ting of old knowledge upon earlier tasks, thus degrading the performance. Such phenomenon is known as catastrophic forgetting, which widely exists in continual learning [6].
The continual learning refers that a model is incrementally updated over a sequence of tasks, performing knowledge transfer from old tasks to the new one.
The typical way to alleviate the catastrophic forgetting issue is to ﬁne-tune the backbone model w.r.t the new task with regularization [8, 6, 14, 1], thus preventing drastic weight update. Nevertheless, such method has limited suc-cess when many new tasks need to be learned. Different from that, Piggyback [10], a mask-based continual learn-ing method, is proposed to address this issue by learning only binary (i.e., 0,1) element-wise masks w.r.t the weights, while keeping the backbone model ﬁxed. Such mask is then multiplied by the ﬁxed network weights, determining rele-vant or irrelevant for the current task. Since it only updates binary masks for each new task during training, it can be trained in fast manner, but with limited modeling capac-ity. To further improve the adaption capacity without for-getting, the compress-grow-based approach (e.g., CPG [5]) compresses (via pruning) and selectively expands the model iteratively. After punning, it utilizes the Piggyback method to learn a mask for the preserved weights as shown in Fig.1, and also retrains the released weights for current task. If the accuracy goal is not attained, it will expand the model by adding new ﬁlters. Such method outperforms Piggy-back [10], as it involves additional task-speciﬁc parameters, but with order-higher training cost.
Motivation: Although Piggyback could learn new tasks in a fast manner, the binary mask has limited representation capacity, which gets non-ideal accuracy gain. As the coun-termeasure, CPG improves the representation capacity via combining the mask learning and additional task-speciﬁc parameters retraining. However, such complex training pro-cedure suffers from extremely high training cost (i.e., train-ing time and computing resources) that makes it impossible to deploy into state-of-the-art popular edge based or mobile computing based continual learning domain. These limi-tations motivate us to explore a new mask-based learning method that can rich the representation capacity, and more importantly, without involving additional training cost.
Contribution: In this work, we propose a new learn-ing method called Kernel-wise Soft Mask (KSM), which learns a kernel-wise hybrid binary and real-value soft mask 113845
Weight:
Trainable
Fixed
Mask: 1 0
Real value
T1
T2
T1
T2
T1
T2
T1 (cid:521)
T2 (cid:521)
T2
T1 a) retraining with regularization b) unchanged weights  with network extension c) selective retraining  with expansion d) hard mask method e) the proposed soft  mask method
Figure 1: Overview of neural network approaches to overcome catastrophic forgetting, we consider the setting where each task retrains a new classiﬁer. Except that, for the backbone model: a) retraining while regularizing to prevent catastrophic forgetting with previously learned tasks; b) unchanged weights with network extension for representing new tasks; c) selective retraining with possible expansion[5]; d) the hard mask method[10]; e) the proposed soft mask method. for each new task, while keeping the backbone model
ﬁxed. The KSM method has the capability to mitigate the well-known catastrophic forgetting issue, to better transfer knowledge from old tasks, and more importantly, to im-prove the training efﬁciency. Our method is distinguished from prior works in the following aspects: 1. Kernel-wise mask sharing. To reduce the mask size and improve the computation efﬁciency in hardware, we design the mask in kernel-wise, instead of element-wise. For instance, only a single mask value is utilized to represent a 3 by 3 kernel, thus the mask size would reduce by 9 times. 2. Soft mask. To boost the knowledge representation ability without involving additional training cost, we decompose the mask into a binary mask and partial real-value scaling coefﬁcient tensor. 3. Softmax trick. To obtain a better binary mask, we pro-pose to leverage the softmax trick to relax the gradient approximation for mask during training.
Beneﬁting from the techniques above, the proposed KSM method could achieve similar to CPG (or even better) ac-curacy, while keeping similar to Piggyback (or even better) training speed. 2.