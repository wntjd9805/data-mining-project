Abstract
We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to ﬁrst establish pixel-wise dense matches at a coarse level and later reﬁne the good matches at a ﬁne level.
In contrast to dense methods that use a cost volume to search corre-spondences, we use self and cross attention layers in Trans-former to obtain feature descriptors that are conditioned on both images. The global receptive ﬁeld provided by Trans-former enables our method to produce dense matches in low-texture areas, where feature detectors usually strug-gle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outper-forms state-of-the-art methods by a large margin. LoFTR also ranks ﬁrst on two public benchmarks of visual local-ization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/. 1.

Introduction
Local feature matching between images is the corner-stone of many 3D computer vision tasks, including structure from motion (SfM), simultaneous localization and mapping (SLAM), visual localization, etc. Given two images to be matched, most existing matching methods consist of three separate phases: feature detection, feature description, and feature matching. In the detection phase, salient points like corners are ﬁrst detected as interest points from each im-age. Local descriptors are then extracted around neigh-borhood regions of these interest points. The feature de-tection and description phases produce two sets of interest points with descriptors, the point-to-point correspondences of which are later found by nearest neighbor search or more sophisticated matching algorithms.
The use of a feature detector reduces the search space of matching, and the resulted sparse correspondences are sufﬁ-cient for most tasks, e.g., camera pose estimation. However, a feature detector may fail to extract enough interest points
Figure 1: Comparison between the proposed method
LoFTR and the detector-based method SuperGlue [37].
This example demonstrates that LoFTR is capable of ﬁnd-ing correspondences on the texture-less wall and the ﬂoor with repetitive patterns, where detector-based methods struggle to ﬁnd repeatable interest points.1 that are repeatable between images due to various factors such as poor texture, repetitive patterns, viewpoint change, illumination variation, and motion blur. This issue is espe-cially prominent in indoor environments, where low-texture regions or repetitive patterns sometimes occupy most areas in the ﬁeld of view. Fig. 1 shows an example. Without re-peatable interest points, it is impossible to ﬁnd correct cor-respondences even with perfect descriptors.
Several recent works [34, 33, 19] have attempted to rem-edy this problem by establishing pixel-wise dense matches.
Matches with high conﬁdence scores can be selected from the dense matches, and thus feature detection is avoided.
However, the dense features extracted by convolutional neu-ral networks (CNNs) in these works have limited receptive
ﬁeld which may not distinguish indistinctive regions. In-stead, humans ﬁnd correspondences in these indistinctive regions not only based on the local neighborhood, but with a larger global context. For example, low-texture regions in
∗The ﬁrst three authors contributed equally. The authors are afﬁliated with the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D
Vision. †Corresponding author: Xiaowei Zhou. 1Only the inlier matches after RANSAC are shown. The green color indicates a match with epipolar error smaller than 5 × 10−4 (in the nor-malized image coordinates). 8922
Fig. 1 can be distinguished according to their relative po-sitions to the edges. This observation tells us that a large receptive ﬁeld in the feature extraction network is crucial.
Motivated by the above observations, we propose Lo-cal Feature TRansformer (LoFTR), a novel detector-free approach to local feature matching.
Inspired by seminal work SuperGlue [37], we use Transformer [48] with self and cross attention layers to process (transform) the dense local features extracted from the convolutional backbone.
Dense matches are ﬁrst extracted between the two sets of transformed features at a low feature resolution (1/8 of the image dimension). Matches with high conﬁdence are se-lected from these dense matches and later reﬁned to a sub-pixel level with a correlation-based approach. The global receptive ﬁeld and positional encoding of Transformer en-able the transformed feature representations to be context-and position-dependent. By interleaving the self and cross attention layers multiple times, LoFTR learns the densely-arranged globally-consented matching priors exhibited in the ground-truth matches. A linear transformer is also adopted to reduce the computational complexity to a man-ageable level.
We evaluate the proposed method on several image matching and camera pose estimation tasks with indoor and outdoor datasets. The experiments show that LoFTR out-performs detector-based and detector-free feature matching baselines by a large margin. LoFTR also achieves state-of-the-art performance and ranks ﬁrst among the published methods on two public benchmarks of visual localization.
Compared to detector-based baseline methods, LoFTR can produce high-quality matches even in indistinctive regions with low-textures, motion blur, or repetitive patterns. 2.