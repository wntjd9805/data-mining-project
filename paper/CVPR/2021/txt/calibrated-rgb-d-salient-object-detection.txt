Abstract
Complex backgrounds and similar appearances between objects and their surroundings are generally recognized as challenging scenarios in Salient Object Detection (SOD).
This naturally leads to the incorporation of depth infor-mation in addition to the conventional RGB image as in-put, known as RGB-D SOD or depth-aware SOD. Mean-while, this emerging line of research has been consider-ably hindered by the noise and ambiguity that prevail in raw depth images. To address the aforementioned issues, we propose a Depth Calibration and Fusion (DCF) frame-work that contains two novel components: 1) a learning strategy to calibrate the latent bias in the original depth maps towards boosting the SOD performance; 2) a simple yet effective cross reference module to fuse features from both RGB and depth modalities. Extensive empirical ex-periments demonstrate that the proposed approach achieves superior performance against 27 state-of-the-art methods.
Moreover, our depth calibration strategy alone can work as a preprocessing step; empirically it results in notice-able improvements when being applied to existing cutting-edge RGB-D SOD models. Source code is available at https://github.com/jiwei0921/DCF. 1.

Introduction
Salient Object Detection (SOD) is an important com-puter vision problem that aims to identify and segment the most prominent object in a scene.
It has found success-ful applications in a variety of tasks such as object recog-nition [59], image retrieval [38, 61], SLAM [37] and video analysis [25, 19, 14]. To tackle the innate challenges in ad-dressing difﬁcult scenes with low texture contrast or in the presence of cluttered backgrounds, depth information has been incorporated as a complementary input source. The
Wei Ji and Jingjing Li have equal contributions. Wei Ji contributes to this work during internship at Tencent Jarvis Lab.
Shuang Yu and Li Cheng are the corresponding authors. e v i t a g e
N
) a ( e v i t i s o
P
) b ( l a r t u e
N
) c ( y c a r u c c a
)
↑
!"
! ( e r u s a e m
-F d e t h g i e
W 0.875 0.870 0.865 0.860 0.855 0.850 0.0
Image
GT
Depthraw
Depthest (CoNet)
Depthest (Ours)
Depthcal
D3Net
D3Net(+Cal)
DMRA
DMRA(+Cal)
+1.4%
+1.3%
+1.3%
+1.4% y c a r u c c a
)
↑
!
! ( e r u s a e m
-F 0.885 0.880 0.875 0.870 0.865 0.860 0.0
)
↓
&
%
$ ( r o r r
E e t u l o s b
A n a e
M 0.052 0.050 0.048 0.046 0.044 0.042 0.0
−7.8%
−8.5% (d) Models with Depthraw vs. Depthcal on NJU2K benchmark
Figure 1. Top: Examples of different depth qualities; GT de-notes the ground-truth saliency map; Depthraw denotes the orig-inal depth map; Depthest in the 4th and 5th columns are the esti-mated depth produced by CoNet [34] and our DCF, respectively;
Depthcal of the last column is generated by our proposed depth cal-ibration strategy. Bottom: Accuracy of two representative RGB-D
SOD models (D3Net [24] and DMRA [54]) trained with original and calibrated depth (‘+Cal’), respectively. growing interests in the development of RGB-D SOD meth-ods [12, 42, 48] are especially boosted by the rapid progress and ﬂourish of varied 3D imaging sensors [29], ranging from the traditional stereo imaging that produces disparity maps, to the more recent structured lighting [76, 30], time-of-ﬂight, light ﬁeld [63, 71, 72] and LIDAR cameras that directly generate depth images. As showcased by the recent cross-modality fusion schemes [7, 10, 44], adding depth-map on top of RGB image as an extra input leads to supe-rior performance in localizing salient objects on challenging scenes.
In essence, the actual value of depth in SOD lies in its capability of discerning the object silhouette from back-ground. Nevertheless, practical examination as presented in
Fig. 1 implies two main issues that hinder the full exploita-9471                      
012 ,*-(./
· · ·
!!"#
!"#$%"&
")*&
⨁
!"#$%"&
!"#$%& '() &"*"
CRM 34,567
!"#!
!"#"
!"##
· · ·
· · ·
!$%&'(
!"#$#%&' ()*+ℎ
-&'#."&+)/ ()*+ℎ dif$iculty
!"#$% &'()*+'$),-!"#$%"&
'()*+ ,*-(./
Figure 2. An overview of the proposed Depth Calibration and
Fusion (DCF) framework. tion of depth-map: 1) The depth maps are often exceed-ingly noisy at the object boundaries, as shown in Fig. 1(a), which may be hampered by the limitation of depth sen-sors and scene conﬁgurations such as occlusion [64], reﬂec-tion [3, 43] and viewing distance [2]; 2) Even with correct depth, as exampled by Fig. 1(c), the foreground object often differs only slightly from the surrounding background in the depth maps. This severely limits the potential performance gain of incorporating depth maps compared to using RGB image as the sole input.
To tackle the above two challenges, a Depth Calibration and Fusion (DCF) framework is proposed. As illustrated in Fig. 2, our DCF generates an optimal calibration of the depth values that directly promotes salient object detection.
Our approach contains the following main contributions:
• A two-step calibration & fusion pipeline is developed: step one involves calibrating the depth image and cor-recting the latent bias in the original depth maps; step two introduces an effective Cross Reference Module (CRM) to fuse the feature representations from RGB and calibrated depth streams. The performance of the proposed approach is demonstrated with extensive em-pirical experiments, and compared with 27 state-of-the-art RGB-D SOD methods.
• Our depth calibration module can serve as a pre-is directly applicable to ex-processing step that isting RGB-D SOD methods. By introducing the depth calibration module to the existing RGB-D based
SOD methods, the MAE metric of D3Net [24] and
DMRA [54] are decreased by 8.5% and 7.8%, re-spectively, when being evaluated on the widely-used
NJU2K benchmark. 2.