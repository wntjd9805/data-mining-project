Abstract the dataset. 2. The output, characterizing which requires labels for
The problem of expert model selection deals with choos-ing the appropriate pretrained network (“expert”) to trans-fer to a target task. Methods, however, generally depend on two separate assumptions: the presence of labeled images and access to powerful “probe” networks that yield useful features. In this work, we demonstrate the current reliance on both of these aspects and develop algorithms to operate when either of these assumptions fail. In the unlabeled case, we show that pseudolabels from the probe network provide discriminative enough gradients to perform nearly-equal task selection even when the probe network is trained on imagery unrelated to the tasks. To compute the embedding with no probe network at all, we introduce the Task Tangent
Kernel (TTK) which uses a kernelized distance across mul-tiple random networks to achieve performance over dou-ble that of other methods with randomly initialized models.
Code is available at https://github.com/BramSW/ task_characterization_cvpr_2021/. 1.

Introduction
Transfer learning is key to the success and popularity of computer vision. The features from a convolutional neural network (CNN) trained on one task can be incredibly useful across a broad variety of tasks[36, 32, 34, 24]. Even larger performance gains can be obtained by selecting a pretrained model more specially suited to the task at hand. This be-havior has been studied in past work[6], but only recently has attention turned to how to determine which specialized model is appropriate for a given task[2].
The key to such model selection is characterizing tasks and their relationships. What does one need to characterize a task? A priori, it seems that we need to characterize two things: 1. The input, characterizing which requires a dataset of images, and features to represent them, and
∗ The ﬁrst two authors contributed equally to this work.
Current approaches to characterizing tasks synthesize both sources of information. Task2Vec[2], for example, trains a linear head on top of a pretrained feature extractor and uses the Fisher information associated with the result-ing model to generate a vectorized embedding. Decisions such as choosing the best pretraining task for a target task can then effectively be made using retrieval-like techniques with this embedding.
But how much of these accurate decisions come from characterizing the input domain alone, and how much comes from knowledge of the precise task? This question has important practical considerations. For example, sup-pose we want to choose a pretrained representation for an-alyzing x-ray images. We may not yet know what we want to recognize in x-ray images. In fact, we may want a pre-trained representation suitable for any kind of x-ray image analysis, even those we haven’t conceived yet. In such cases we are interested in characterizing only the general problem domain, and do not have particular labels (yet) that we are interested in. This raises the question: Can we character-ize tasks without labels?
As our ﬁrst contribution, we answer this question in the afﬁrmative. To address this problem, we introduce Pseu-doTask: a modiﬁcation of the Task2Vec algorithm that re-places labels with pseudolabels output by an image classi-ﬁer trained in a generic source domain. While these pseu-dolabels are deﬁnitely incorrect due to domain misalign-ment, they prove discriminative enough to be quite useful in characterizing tasks. Empirically, we ﬁnd that PseudoTask embeddings are as accurate as supervised Task2Vec embed-dings, indicating that one can characterize tasks effectively even without labels.
Key to this performance, as also to the performance of
Task2Vec, is the inductive bias provided by the pre-trained feature representation. However, this inductive bias may prove harmful as the task domains move farther away from the domain where the feature extractor is pretrained [36], making it risky to rely so heavily on such feature represen-tations. This raises a second question: can we characterize 11245
tasks without features?
We answer this too in the afﬁrmative. We design a method called Task Tangent Kernel (TTK) that measures task similarity using the gradients of randomly initialized networls. TTK does not use pretrained probe networks at all. Despite this lack of inductive bias, it provides useful selections as well, at less than half the error of PseudoTask and Task2Vec with random feature extractors.
In sum, this paper introduces two new techniques for characterizing tasks that lift some of the restrictive assump-tions of prior work (Figure 2): 1. We introduce PseudoTask, a new way of characteriz-ing tasks without labels, allowing one to characterize problem domains in general. We ﬁnd PseudoTask per-forms almost the same as Task2Vec, indicating that la-bels are in fact not necessary. 2. To avoid the potentially mismatched inductive bias of pretrained feature extractors, we introduce Task Tan-gent Kernels, which characterizes tasks effectively even without such feature extractors. 2.