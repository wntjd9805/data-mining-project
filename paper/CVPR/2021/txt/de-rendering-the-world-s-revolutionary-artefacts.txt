Abstract 1.

Introduction
Recent works have shown exciting results in unsupervised image de-rendering—learning to decompose 3D shape, ap-pearance, and lighting from single-image collections with-out explicit supervision. However, many of these assume sim-plistic material and lighting models. We propose a method, termed RADAR, that can recover environment illumination and surface materials from real single-image collections, re-lying neither on explicit 3D supervision, nor on multi-view or multi-light images. Speciﬁcally, we focus on rotationally symmetric artefacts that exhibit challenging surface prop-erties including specular reﬂections, such as vases. We in-troduce a novel self-supervised albedo discriminator, which allows the model to recover plausible albedo without requir-ing any ground-truth during training. In conjunction with a shape reconstruction module exploiting rotational sym-metry, we present an end-to-end learning framework that is able to de-render the world’s revolutionary artefacts. We conduct experiments on a real vase dataset and demonstrate compelling decomposition results, allowing for applications including free-viewpoint rendering and relighting. More re-sults and code at: https://sorderender.github.io/.
*The work was primarily done during an internship at Google Research.
Consider one of the vases shown in Fig. 1. From just a sin-gle image, we can tell a lot about the underlying properties of that vase. Despite the image’s ﬂatness, we can perceive an instance of a 3D surface with various lights cast upon it. We can distinguish between areas where the underlying color of the vase changes and regions that reﬂect light, revealing the glossiness of the surface and its local geometry.
We introduce a model that aims to de-render a sin-gle image into these factors—geometry, material, and illumination—which we call RADAR (Revolutionary
Artefact De-rendering And Re-rendering). In particular, our approach can decompose real images of vase-like objects un-der complex illumination and with glossy materials. Notably, our approach can learn this ability just from collections of single images (i.e., where each object is pictured once), with-out explicit 3D supervision or multiple images. This allows us to analyze images obtained in real world settings, such as artefact collections in museums, and subsequently apply modiﬁcations including relighting, as illustrated in Fig. 1.
Making de-rendering tractable involves simplifying as-sumptions. In some methods, this means requiring explicit supervision, e.g., with synthetic [24, 26] or specially cap-tured data [25]. An alternative to direct supervision is to 6338
observe an object under multiple viewpoints [6, 43] or mul-tiple lights [42, 7], but for many existing image collections, such multiple views are unavailable. Hence, learning to de-render from single image collections has been of growing interest [41, 35]. However, these approaches assume sim-plistic shading or lighting models, such as Lambertian, and are not applicable to realistic scenarios with complex illumi-nation effects.
In contrast, our objective in this paper is to explore un-supervised de-rendering in the presence of more complex illumination effects. To make our task tractable, we consider simplifying assumptions on the 3D shape. We draw inspira-tion from recent work [41] that leverages symmetry priors for self-supervised decomposition. Speciﬁcally, we focus on de-rendering objects whose shapes are described by solids of revolution (SoRs, or “revolutionary” objects)—such ob-jects include many categories of man-made objects such as vases. This allows us to derive a simple yet effective method for recovering the 3D geometry and camera viewpoint from only single images with 2D silhouettes.
Our model de-renders a single image of a revolutionary object into 3D geometry, viewpoint, albedo, material shini-ness, and environment lighting. Even with this strong as-sumption on SoR shape and inductive bias on the rendering process, this is still an extremely under-constrained problem.
As with most ill-posed inverse problems, we must prevent degenerate solutions where the model learns no disentangle-ment at all. Another major challenge is to predict realistic diffuse albedo in regions saturated by specular reﬂections.
To ensure realistic disentanglement, we incorporate novel components into our model. In particular, we propose a new adversarial module that we call a Self-supervised Albedo
Discriminator (SAD). The key insight is that the distribution of diffuse albedo patches should be independent of observed specular effects—it should not be possible to tell from the albedo alone whether a particular surface region exhibits a specular reﬂection or not. Unlike existing adversarial frame-works, a key feature of SAD is that the discriminator always takes its inputs from the predicted albedo and never requires a ‘real’ albedo, hence the label self-supervised.
In summary, we propose RADAR, an end-to-end frame-work for de-rendering single images into shape, complex lighting, and materials, learning only from single-image collections with 2D silhouettes. We evaluate our approach numerically on a synthetic dataset, and demonstrate effec-tive results on real images of revolutionary artefacts from museum collections, where our approach allows for applica-tions such as free-viewpoint rendering and relighting. 2.