Abstract
Learning to model and predict how humans interact with objects while performing an action is challenging, and most of the existing video prediction models are in-effective in modeling complicated human-object interac-tions. Our work builds on hierarchical video prediction models, which disentangle the video generation process into two stages: predicting a high-level representation, such as pose sequence, and then learning a pose-to-pixels transla-tion model for pixel generation. An action sequence for a human-object interaction task is typically very complicated, involving the evolution of pose, person’s appearance, object locations, and object appearances over time. To this end, we propose a Hierarchical Video Prediction model using
Relational Layouts. In the ﬁrst stage, we learn to predict a sequence of layouts. A layout is a high-level representation of the video containing both pose and objects’ information for every frame. The layout sequence is learned by mod-eling the relationships between the pose and objects using relational reasoning and recurrent neural networks. The layout sequence acts as a strong structure prior to the sec-ond stage that learns to map the layouts into pixel space.
Experimental evaluation of our method on two datasets,
UMD-HOI and Bimanual, shows signiﬁcant improvements in standard video evaluation metrics such as LPIPS, PSNR, and SSIM. We also perform a detailed qualitative analysis of our model to demonstrate various generalizations. 1.

Introduction
Video prediction is a challenging task of predicting fu-ture frames conditioned on one or more past frames. Videos in the real world are extremely complex. An everyday ac-tion, such as drinking a coffee, results from complicated interactions among various objects. For example, ﬁrst, the person might reach the coffee pot and pour coffee into their cup. Next, they might start drinking from the cup while browsing their cell phone. Observe that this particular ac-tion involves interactions among various objects such as a coffee pot, cup, and cell phone. Each of the objects has its relative motion with respect to other objects and the person performing the action. While it is effortless for human be-ings to imagine such events, existing computer vision mod-els often fail at these tasks.
Existing video prediction methods broadly fall into two categories: 1) models that directly predict the video in the pixel space, and 2) models that use hierarchical prediction.
Hierarchical prediction methods are a preferred over di-rectly predicting the video in pixel space as they learn a good intermediate representation which is then mapped to pixel space. Disentangling the prediction into simpler steps helps the models focus on smaller tasks and, hence, learn an improved frame prediction model. A natural choice of intermediate representation for videos is optical ﬂow [1].
Similarly, for videos involving human actions, human-pose is typically used as an intermediate representation. Villegas et al. [2] and Walker et al. [3] have disentangled the video prediction by ﬁrst predicting the pose sequence and then mapping the pose sequence to pixel space.
While pose is a great choice for videos involving human actions, pose alone is not sufﬁcient to capture various dy-namics in a Human-Object interaction sequence. For com-plex actions, such as human-object interactions where mul-tiple objects evolve over time, pose alone does not fully cap-ture the complex scene dynamics. Since the pose does not contain any information about the objects, the models fail to capture the object’s motion and appearance faithfully. To mitigate this issue, we propose to learn a layout sequence as an intermediate representation. A layout sequence is a combination of pose and object sequences that not only cap-tures the person’s pose while performing an action, but also explicitly learns the locations of various objects at differ-ent times while the action is being performed. The naive way of learning these pose and object sequences indepen-dently is also not sufﬁcient since the spatio-temporal evolu-tion of an object is dependent on how the pose is evolving and vice versa. Hence, we propose a Human-Object Rela-tional Network (HORN) to model these complex interac-tions among objects and poses.
Our key contributions for video prediction for human-12146
object interactions are: 1) we model the full-body mo-tion for humans, 2) our intermediate representation cap-tures both pose and object locations, and thus learns a better structure prior for the frame prediction stage. 3) our model can generate videos for novel interactions, e.g., it can gen-eralize well to new people performing actions that were not part of the training set. 2.