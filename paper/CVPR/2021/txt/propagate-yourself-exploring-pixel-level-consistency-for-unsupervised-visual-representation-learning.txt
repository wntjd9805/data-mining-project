Abstract
Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of con-trastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The ﬁrst task directly ap-plies contrastive learning at the pixel level. We addition-ally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Speciﬁcally, it achieves 60.2
AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred to
Pascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using a
ResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pre-training not only regular backbone networks but also head networks used for dense downstream tasks, and are com-plementary to instance-level contrastive methods. These results demonstrate the strong potential of deﬁning pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning. Code is avail-able at https://github.com/zdaxie/PixPro. 1.

Introduction
According to Yann LeCun, “if intelligence is a cake, the bulk of the cake is unsupervised learning”. This quote reﬂects his belief that human understanding of the world
*Equal Contribution. The work is done when Zhenda Xie and Yutong
Lin are interns at Microsoft Research Asia.
Figure 1. An illustration of the proposed PixPro method, which is based on a pixel-to-propagation consistency pretext task for pixel-level visual representation learning. In this method, two views are randomly cropped from an image (outlined in black), and the fea-tures from the corresponding pixels of the two views are encour-aged to be consistent. For one of them, the feature comes from a regular pixel representation (illustrated as orange crosses). The other feature comes from a smoothed pixel representation (shown as green dots) built by propagating the features of similar pixels (illustrated as the light green region). Note that this hard selec-tion of similar pixels is for illustration only. In implementation, all pixels on the same view will contribute to propagation, with the propagation weight of each pixel determined by its feature simi-larity to the center pixel. is predominantly learned from the tremendous amount of unlabeled information within it. Research in machine in-telligence has increasingly moved in this direction, with substantial progress in unsupervised and self-supervised learning[34, 18, 25, 8, 30]. In computer vision, recent ad-vances can largely be ascribed to the use of a pretext task called instance discrimination, which treats each image in a training set as a single class and aims to learn a feature representation that discriminates among all the classes.
Although self-supervised learning has proven to be re-markably successful, we argue that there remains signiﬁcant 116684
untapped potential. The self-supervision that guides repre-sentation learning in current methods is based on image-level comparisons. As a result, the pre-trained represen-tation may be well-suited for image-level inference, such as image classiﬁcation, but may lack the spatial sensitivity needed for downstream tasks that require pixel-level pre-dictions, e.g., object detection and semantic segmentation.
How to perform self-supervised representation learning at the pixel level is a problem that until now has been rela-tively unexplored.
In this paper, we tackle this problem by introducing pixel-level pretext tasks for self-supervised visual repre-sentation learning. Inspired by recent instance discrimina-tion methods, our ﬁrst attempt is to construct a pixel-level contrastive learning task, where each pixel in an image is treated as a single class and the goal is to distinguish each pixel from others within the image. Features from the same pixel are extracted via two random image crops containing the pixel, and these features are used to form positive train-ing pairs. On the other hand, features obtained from differ-ent pixels are treated as negative pairs. With training data collected in this self-supervised manner, a contrastive loss is applied to learn the representation. We refer to this ap-proach as PixContrast.
In addition to this contrastive approach, we present a method based on pixel-to-propagation consistency, where positive pairs are obtained by extracting features from the same pixel through two asymmetric pipelines instead. The
ﬁrst pipeline is a standard backbone network with a pro-jection head. The other has a similar form but ends with a proposed pixel propagation module, which ﬁlters the pixel’s features by propagating the features of similar pixels to it.
This ﬁltering introduces a certain smoothing effect, while the standard feature maintains spatial sensitivity. A differ-ence of this method from the contrastive approach of Pix-Contrast is that it encourages consistency between positive pairs without consideration of negative pairs. While the per-formance of contrastive learning is known to be inﬂuenced heavily by how negative pairs are handled [18, 8], this is-sue is avoided in this consistency-based pretext task. Em-pirically, we ﬁnd that this pixel-to-propagation consistency method, which we call PixPro, signiﬁcantly outperforms the PixContrast approach over various downstream tasks.
Besides learning good pixel-level representations, the proposed pixel-level pretext tasks are found to be effective for pre-training on not only backbone networks but also head networks used for dense downstream tasks, contrary to instance-level discrimination where only backbone net-works are pre-trained and transferred. This is especially beneﬁcial for downstream tasks with limited annotated data, as all layers can be well-initialized. Moreover, the proposed pixel-level approach is complementary to existing instance-level methods, where the former is good at learning a spa-tially sensitive representation and the latter provides better categorization ability. A combination of the two methods capitalizes on both of their strengths, while also remaining computationally efﬁcient in pre-training as they both can share a data loader and backbone encoders.
The proposed PixPro achieves state-of-the-art transfer performance on common downstream benchmarks requir-ing dense prediction. Speciﬁcally, with a ResNet-50 back-bone, it obtains 60.2 AP on Pascal VOC object detection us-ing a Faster R-CNN detector (C4 version), 41.4 / 40.5 mAP on COCO object detection using a Mask R-CNN detector (both the FPN / C4 versions, 1× settings), and 77.2 mIoU
Cityscapes semantic segmentation using an FCN method, which are 2.6 AP, 0.8 / 1.0 mAP, and 1.0 mIoU better than the leading unsupervised/supervised methods. Though past evaluations of unsupervised representation learning have mostly been biased towards linear classiﬁcation on Ima-geNet, we advocate a shift in attention to performance on downstream tasks, which is the main purpose of unsuper-vised representation learning and a promising setting for pixel-level approaches. 2.