Abstract
We present SCANimate, an end-to-end trainable frame-work that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that
ﬁtting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate signiﬁcantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle-consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives.
In contrast to commonly used global pose embeddings, our local pose conditioning signiﬁcantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing
∗Currently at Facebook Reality Labs. types with different amounts of training data, outperforming existing solutions and other variants in terms of ﬁdelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de. 1.

Introduction
Parametric models of 3D human bodies are widely used for the analysis and synthesis of human shape, pose, and motion. While existing models typically represent “mini-mally clothed” bodies [4, 26, 43, 52, 66], many applications require realistically clothed bodies. Our goal is to make it easy to produce a realistic 3D avatar of a clothed person that can be reposed and animated as easily as existing models like
SMPL [43]. In particular, the model must support clothing that moves and deforms naturally, with detailed 3D wrinkles, and the rendering of realistically textured images.
To that end, we introduce SCANimate (Skinned Clothed
Avatar Networks for animation), which creates high-quality animatable clothed humans, called Scanimats, from raw 3D scans. SCANimate has the following properties: (1) we learn an articulated clothed human model directly from raw scans, completely eliminating the need for surface registration of a custom template or synthetic clothing simulation data, (2) our parametric model retains the complex and detailed deformations of clothing present in the original scans such as wrinkles and sliding effects of garments with arbitrary topology, (3) a Scanimat can be animated directly using
SMPL pose parameters, and (4) our approach predicts 2886
pose-dependent clothing deformations based on local pose parameters, providing generalization to unseen poses.
Recent data-driven approaches have shown promise for learning parametric models of clothed humans from real-world observations [39, 46, 54, 56]. However, these approaches typically limit the supported clothing types and topology because they require accurate surface regis-tration of a common template mesh to 3D training scans
[39, 46, 56]. Concurrent work by Ma et al. [45] learns clothing deformation without surface registration, yet it is unclear if the method works on raw scans with noise and holes. Learning from real-world observations is essentially challenging because raw 3D scans are un-ordered point clouds with missing data, changing topology, multiple clothing layers, and sliding motions between the body and garments. Although one can learn from synthetic data generated by physics-based clothing simulation [23, 25, 54], the results are less realistic, the data preparation is time consuming and non-trivial to scale to the real-world clothing.
To address these issues, SCANimate learns directly from raw scans of people in clothing. Body scanning is becoming common, and scans can be obtained from a variety of devices. Scans contain high-frequency details, capture varied clothing topology, and are inherently realistic. To make learning from scans possible, we make several contributions: canonicalization, implicit skinning ﬁelds, cycle consistency, and implicit shape learning.
Canonicalization and Implicit Skinning Fields. The ﬁrst step involves transforming the raw scans to a common pose so we can learn to model pose-dependent surface defor-mations (e.g. bulging, stretching, wrinkling, and sliding), i.e. pose “correctives”. But we are not seeking a traditional
“registration” of the scans to a common mesh topology, since this is, in general, not feasible with clothed bodies. Instead, we learn continuous functions of 3D space that allow us to transform posed scans to a canonical pose and back again.
The key idea is to build this on linear blend skinning (LBS), which traditionally deﬁnes weights on the surface of a mesh that encode how much each vertex is inﬂuenced by the rotation of a body joint. To deal with raw scans of unknown topology, we extend this notion by deﬁning skinning weights implicitly everywhere in 3D space. Speciﬁcally, given a 3D location x, we regress a continuous vector function g represented by a neural network, g(x) : R3 → RJ , which deﬁnes the skinning weights. An inverse LBS function uses the regressed skinning weights to “undo” the pose of the body and transforms the points into the canonical space. As this representation makes no assumptions about the topology or resolution of input scans, we can canonicalize arbitrary non-watertight meshes. Furthermore, we can easily generate animations of the parametric clothed avatar by applying forward LBS to the clothed body in the canonical pose with the learned pose correctives.
Cycle Consistency. Despite the desirable properties of canonicalization, learning the skinning function is ill-posed since we do not have ground truth training data that speciﬁes the weights. To address this, we exploit two key observations.
First, as demonstrated in previous work [27, 69, 72], ﬁtting a parametric human body model such as SMPL [43] to 3D scans is more tractable than surface registration. We leverage SMPL’s skinning weights, which are deﬁned only on the body surface, to regularize our more general skinning function. Second, the transformations between the posed space and the canonical space should be cycle-consistent.
Namely, inverse LBS and forward LBS together should form an identity mapping as illustrated in Fig. 3, which provides a self-supervision signal for training the skinning function. After training the skinning function, we obtain the canonicalized scans (all in the same pose).
Learning Implicit Pose Correctives. Given the canoni-calized scans, we learn a model that captures the pose-dependent deformations. However a problem remains: the original raw scans often contain holes, and so do the canonicalized scans. To deal with this and with the arbitrary topology of clothing, we use an implicit surface representation [13, 47, 53]. As multiple canonicalized scans will miss different regions, with this approach, they complement each other, while retaining details present in the original inputs. Furthermore, unlike traditional approaches
[39, 46, 54, 70], where pose-dependent deformations are conditioned on entire pose parameters, we spatially ﬁlter out irrelevant pose features from the input conditions by leveraging the learned skinning weights. In this way, we effectively prune long-range spurious correlations between garment deformations and body joints, achieving plausible pose correctives for unseen poses even from a small number of training scans. The resulting learned Scanimat can be easily reposed and animated with SMPL pose parameters.
In summary, our main contributions are (1) the ﬁrst end-to-end trainable framework to build a high-quality parametric clothed human model from raw scans, (2) a novel weakly-supervised formulation with geometric cycle-consistency that disentangles articulated deformations from the local pose correctives without requiring ground-truth training data, and (3) a locally pose-aware implicit surface representation that models pose-dependent clothing deformation and gen-eralizes to unseen poses. Our results show that SCANimate is superior to existing solutions in terms of generality and accuracy. Furthermore, we perform an extensive study to evaluate the technical contributions that are critical for success. The code and example Scanimats can be found at https://scanimate.is.tue.mpg.de. 2.