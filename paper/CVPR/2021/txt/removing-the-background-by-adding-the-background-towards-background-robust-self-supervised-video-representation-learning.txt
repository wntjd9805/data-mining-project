Abstract
Self-supervised learning has shown great potentials in improving the video representation ability of deep neural networks by getting supervision from the data itself. How-ever, some of the current methods tend to cheat from the background, i.e., the prediction is highly dependent on the video background instead of the motion, making the model vulnerable to background changes. To mitigate the model reliance towards the background, we propose to remove the background impact by adding the background. That is, given a video, we randomly select a static frame and add it to every other frames to construct a distracting video sam-ple. Then we force the model to pull the feature of the dis-tracting video and the feature of the original video closer, so that the model is explicitly restricted to resist the back-ground inﬂuence, focusing more on the motion changes. We term our method as Background Erasing (BE). It is worth noting that the implementation of our method is so sim-ple and neat and can be added to most of the SOTA meth-ods without much efforts. Speciﬁcally, BE brings 16.4% and 19.1% improvements with MoCo on the severely biased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased dataset Diving48. 1.

Introduction
Convolutional neural networks (CNNs) have achieved competitive accuracy on a variety of video understand-ing tasks, including action recognition [20], temporal ac-tion detection [63] and spatio-temporal action localization
[55]. Such success relies heavily on manually annotated datasets, which are time-consuming and expensive to ob-tain. Meanwhile, there are numerous unlabeled data that are instantly available on the Internet, drawing more and
*The ﬁrst two authors contributed equally. This work was done when
Jinpeng was in Tencent Youtu Lab.
†Corresponding Author. Email: majh8@mail.sysu.edu.cn.
Figure 1: Illustration of the background cheating. In the real open world, an action can happen at various locations.
Current models trained on the mainstream datasets tend to give predictions simply because it sees some background cues, neglecting the fact that motion pattern is what actually deﬁnes an “action”. more researchers’ attention from the community to utilize off-the-shelf unlabeled data to improve the performance of
CNNs by self-supervised learning.
Recently, self-supervised learning methods have been developed from the image ﬁeld to the video ﬁeld. However, there are big differences between the mainstream video dataset and the mainstream image dataset. Li et al.[29] and
Girdhar et al.[14] point out that the current commonly used video datasets usually exist large implicit biases over scene and object structure , making the temporal structure become less important and the prediction tends to have a high depen-dence on the video background. We name this phenomenon as background cheating, as is shown in Figure 1. For exam-ple, a trained model may classify an action as playing soccer simply because it sees the ﬁeld, without really understand-ing the cartwheel motion. As a result, the model is easily to overﬁt the training set, and the learned feature representa-tion is likely to be scene-biased. Li et al.[29] reduce the bias by resampling the training set, and Wang et al.[53] propose to pull actions out of the context by training a binary clas-siﬁer to explicitly distinguish action samples and conjugate samples that are contextually similar to action samples but contains different action.
In this work, to hinder the model from background cheat-ing and make the model generalize better, we present to 111804
reduce the impact of the background by adding the back-ground and encourage the model to learn consistent fea-ture w/ or w/o the operation. Speciﬁcally, given a video, we randomly select a static frame and add it to every other frames to construct a distracting video, as is shown in Fig-ure 3. Then we force the model to pull the feature of the distracting video and the feature of the original video to-gether by consistency regularization. In this way, we made a disturbance to the video background and require its feature to be consistent with the original video, achieving the pur-pose of making the model not be excessively dependent on the background, thereby alleviating the background cheat-ing problem.
Experimental results demonstrate that the proposed method can effectively reduce the inﬂuence of the back-ground cheating, and the extracted representation is more robust to the background bias and have stronger generaliza-tion ability. Our approach is simple and incorporate it into existing self-supervised video learning methods can bring signiﬁcant gains.
In summary, our main contributions are twofold:
• We propose a simple yet effective video representation learning method that is robust to the background.
• The proposed approach can be easily incorporated with existing self-supervised video representation learn-ing methods, bringing further gains on UCF101[41],
HMDB51 [27] and Diving48[30] datasets. 2.