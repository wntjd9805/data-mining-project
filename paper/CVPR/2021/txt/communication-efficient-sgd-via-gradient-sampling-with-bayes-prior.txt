Abstract
Gradient compression has been widely adopted in data-parallel distributed training of deep neural networks to reduce communication overhead. Some literatures have demonstrated that large gradients are more important than small ones because they contain more information, such as Top-k compressor. Other mainstream methods, like random-k compressor and gradient quantization, usually treat all gradients equally. Different from all of them, we regard large and small gradients selection as the exploita-tion and exploration of gradient information, respectively.
And we ﬁnd taking both of them into consideration is the key to boost the ﬁnal accuracy. So, we propose a novel gra-dient compressor: Gradient Sampling with Bayes Prior in this paper. Speciﬁcally, we sample important/large gradi-ents based on the global gradient distribution, which is pe-riodically updated across multiple workers. Then we intro-duce Bayes Prior into distribution model to further explore the gradients. We prove the convergence of our method for smooth non-convex problems in the distributed system.
Compared with methods that running after high compres-sion ratio at the expense of accuracy, we pursue no loss of accuracy and the actual acceleration beneﬁt in prac-tice. Experimental comparisons on a variety of computer vision tasks (e.g. image classiﬁcation and object detec-tion) and backbones (ResNet, MobileNetV2, InceptionV3 and AlexNet) show that our approach outperforms the state-of-the-art techniques in terms of both speed and accuracy, with the limitation of 100× compression ratio. 1.

Introduction
Recently, deep learning has achieved great success in many tasks, such as image classiﬁcation [37, 45, 36], ob-ject detection [23, 32, 28], video understanding [46, 8, 47] and so on. By taking advantage of data-parallel distributed training equipped with more and more GPU resources, the expensive computational time-consuming of training deep
*Equal Contribution.
Figure 1. The core idea of our gradient compression method. The left subﬁgure illustrates that we sample important gradients based on gradient distribution. The right subﬁgure indicates that we also select some “trivial” ones as the exploration of gradient informa-tion. neural networks have been dramatically reduced. However, the communication cost keeps rising with the increasing number of computing nodes. Even in All-Reduce archi-tecture [31, 40], the transmission delay for communication is obvious. To handle this problem, gradient compression techniques [44, 22, 6, 14] have been put forward to reduce gradients transfer data size, by substituting partial gradients for full ones.
Although gradient compression has been well studied in
[44, 22, 6], it is still a great challenge to maintain the accu-racy at a high compression ratio. [44] combines error reset with partial synchronization to scale the compression ratio up to 1024× successfully. But the random sampling strat-egy adopted in partial synchronization overlooks the im-portance of large gradients. In [14], two adaptive gradient quantization schemes: ALQ and AMQ are proposed to im-prove the ﬁnal accuracy. Unfortunately, it still suffers from some accuracy loss (more than 1.2% on ImageNet dataset), due to the limited representations of low-bit.
To better capture the gradient statistics, in this paper, we propose a new gradient compression method Gradient
Sampling with Bayes Prior to further improve the accuracy.
Compared with a very high compression ratio, we pursue no loss of accuracy and the actual acceleration beneﬁt in practice. The major contributions of our work are outlined 12065
as follows:
• Based on the global gradient distribution, we propose a novel gradient compression method called Gradient
Sampling to efﬁciently capture the large gradients.
• We improve Gradient Sampling scheme with Bayes
Prior to trade off the exploration and exploitation of gradients information, which boosts the ﬁnal accuracy further.
• We prove the convergence bound of our proposed methods, and verify the convergence rate of our meth-ods are the same as SGD under common assumptions.
• Experimental results on a variety of computer vision tasks and backbones show that our method is superior to the state-of-the-art techniques in terms of both speed and accuracy. 2.