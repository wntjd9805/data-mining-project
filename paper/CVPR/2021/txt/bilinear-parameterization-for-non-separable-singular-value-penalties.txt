Abstract
Low rank inducing penalties have been proven to suc-cessfully uncover fundamental structures considered in computer vision and machine learning; however, such methods generally lead to non-convex optimization prob-lems. Since the resulting objective is non-convex one of-ten resorts to using standard splitting schemes such as Al-ternating Direction Methods of Multipliers (ADMM), or other subgradient methods, which exhibit slow convergence in the neighbourhood of a local minimum. We propose a method using second order methods, in particular the variable projection method (VarPro), by replacing the non-convex penalties with a surrogate capable of converting the original objectives to differentiable equivalents. In this way we beneﬁt from faster convergence.
The bilinear framework is compatible with a large fam-ily of regularizers, and we demonstrate the beneﬁts of our approach on real datasets for rigid and non-rigid structure from motion. The qualitative difference in reconstructions show that many popular non-convex objectives enjoy an ad-vantage in transitioning to the proposed framework.1 1.

Introduction
Low rank approximation and factorization methods are classical approaches for solving various computer vision problems, such as structure from motion [38, 5, 21, 11, 24, 25], photometric stereo [2, 7, 29] image segmenta-tion [15], image restoration [9, 29, 20, 27, 45], back-ground/foreground segmentation [43, 7], etc.
There are two main approaches when it comes to solving these problems, and which one is used largely depends on properties of the particular problem being addressed. The 1This work was supported by the Swedish Research Council (grants no. 2015-05639, 2016-04445 and 2018-05375), the strategic research project
ELLIIT, the Swedish Foundation for Strategic Research (Semantic Map-ping and Visual Navigation for Smart Robots) and the Wallenberg AI, Au-tonomous Systems and Software Program (WASP) funded by the Knut and
Alice Wallenberg Foundation. 2Department of Electrical Engineering
Chalmers University of Technology
Figure 1: Top row: Weighted nuclear norm penalty (left) and the corresponding relaxation rh, for a1 = 0 and a2 = 1, considered in this paper. Bottom row: Level sets, corre-sponding to the red lines in the top images. classical problem of low rank recovery with missing data min rank(X)≤k k
W
⊙ (X
−
M ) 2
F , k (1) is a core step in many structure from motion formula-tions [6]. Here M is a measurement matrix which is only partially known and W is a binary matrix removing residu-als corresponding to unknown elements. The traditional ap-proach, which is typically used when the rank of the sought matrix is known, enforces a particular rank by restricting the number of columns of the factors B and C and searches over the bilinear parameterization of the unknown matrix
X = BC T . Since the resulting objective is a least squares problem in both B and C, alternating updates of B and C can be used. While being extremely simple, this approach has been shown to be prone to “ﬂatlining: requiring exces-sive numbers of iterations before convergence” [6]. Instead
[6] proposed a damped newton approach and empirically 3897
veriﬁed that this outperforms the alternation approach. In a number of recent papers Hong et al. [17, 19, 18, 21] showed that the so called VarPro method is remarkably resilient to local minima. For example [18] reports convergence to the best solution from random initialization in 94% of the cases on the dinosaur sequence which is an admittedly difﬁcult dataset with 77% missing data.
An alternative approach is to optimize directly over the elements of X while applying penalties to the singular val-ues. This is typically applied to problems of the more gen-eral class min
X p(σ(X)) +
X b 2. k
− kA (2)
Here p is some penalty function encouraging a desired dis-tribution of singular values σi(X) of the matrix X, see e.g.
[29, 20, 27]. This way of directly optimizing over the ele-ments of X has been made popular by the work on nuclear norms [34, 8] and their generalizations [26, 28, 12, 14, 13], which has shown that with an appropriate choice of regu-larizer (2) can be made convex. While convex regularizers can be sufﬁcient for applications such as image restoration, where a relatively high rank is acceptable, they are typically rather weak and do not give solutions with low enough rank for structure from motion problems. Consequently, they have to be combined with thresholding schemes to gener-ate satisfactory solutions [7, 11].
To achieve better results, non-convex penalty functions
[25, 29, 16, 9] are also frequently used in (2). Since these formulations are typically not differentiable, optimization relies on splitting methods such as ADMM [4]. These are essentially ﬁrst order methods and, as such, convergence near the minimum can be slow. Indeed, [4] recommends to use these when an approximate solution is sufﬁcient, but suggests to switch to second order methods when accuracy is needed.
In this paper, we derive such second order methods for a general class of objectives of the form (2). Our class cov-ers commonly used regularizers, such as weighted nuclear norms, soft rank penalties and hard rank constraints. Note that these functions can be both non-convex and discontinu-ous. We show how to reformulate these into bilinear objec-tives, that can be accurately approximated with quadratic functions, allowing rapid convergence with second order methods such as VarPro or Levenberg–Marquardt. 1.1. Framework and Contributions
In this paper we consider a general framework of non-separable objectives the form where fh(X) = h(σ(X)) +
X b 2, k
− kA rank(X) h(σ(X)) = aiσi(X) + bi. (3) (4) i=1
X 0, we get the weighted nuclear norm i=1 and (bi)k
Here the sequences (ai)k i=1 are both assumed to be non-decreasing. For different choices of a and b the gen-eral regularizer h reduces to commonly used singular value 0 we get the penalties. For example, with bi = µ and ai
≡ k and 0 if i soft rank penalty µ rank(X), with ai
∞
≤ otherwise, we get the hard constraint rank(X) k. With
≤ i aiσi(X), bi but the framework is large and several other regularizers are possible. We aim to optimize objectives including all such regularizers using second order methods, such as VarPro.
This requires ﬁnding a good approximation—which is two times differentiable—of the objective function. For this pur-pose, we propose to use a relaxation rh(σ) of h(σ) devel-oped in [40] and consider
P
≡
≡ rh(σ(X)) +
X 2. b k
− kA (5)
This results in a continuous and almost everywhere differ-entiable objective (see Section 2 for details, Figures 1 and 2 for examples). When introducing the terms γi(B, C) =
)/2, where Bi and Ci are columns i of B (kBik and C, respectively, we obtain the bilinear formulation
+ kCik 2 2 min
B,C rh(γ(B, C)) + (BC T ) b 2. k
− kA (6)
The main contributions of this paper are: i) We show that (5) is equivalent to (6) by proving that rh(σ(X)) = min
X=BCT rh(γ(B, C)), (7)
< 1 then the see Theorem 1. Furthermore, if relaxation (5) is guaranteed to have the same global optimizers as the original (3), see [10]. kAk ii) We show that (6) can be accurately approximated by quadratic functions opening up the possibility of ap-plying second order methods to the problem. iii) We propose a modiﬁed VarPro algorithm and show that it provides superior performance for difﬁcult ob-jectives common in computer vision applications. 1.2.