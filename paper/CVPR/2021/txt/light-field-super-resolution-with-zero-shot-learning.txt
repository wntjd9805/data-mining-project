Abstract
Deep learning provides a new avenue for light ﬁeld super-resolution (SR). However, the domain gap caused by drastically different light ﬁeld acquisition conditions poses a main obstacle in practice. To ﬁll this gap, we propose a zero-shot learning framework for light ﬁeld SR, which learns a mapping to super-resolve the reference view with examples extracted solely from the input low-resolution light ﬁeld itself. Given highly limited training data under the zero-shot setting, however, we observe that it is difﬁcult to train an end-to-end network successfully.
Instead, we divide this challenging task into three sub-tasks, i.e., pre-upsampling, view alignment, and multi-view aggregation, and then conquer them separately with simple yet efﬁcient
CNNs. Moreover, the proposed framework can be read-ily extended to ﬁnetune the pre-trained model on a source dataset to better adapt to the target input, which further boosts the performance of light ﬁeld SR in the wild. Exper-imental results validate that our method not only outper-forms classic non-learning-based methods, but also gener-alizes better to unseen light ﬁelds than state-of-the-art deep-learning-based methods when the domain gap is large. 1.

Introduction
The 4D light ﬁeld that records both angular and spatial information of light has been playing an increasing role in computer vision [33,36,43]. The commercialized light ﬁeld cameras generally adopt micro-lens-array in front of the sensor, which poses an essential trade-off between the angu-lar and spatial resolutions [16, 24]. The limited spatial res-olution restricts the capability of light ﬁeld in practical ap-plications. Therefore, light ﬁeld super-resolution (SR) has been an important and popular topic in the research com-munity and attracts a lot of attention since the emergence of light ﬁeld cameras [1, 2]. Recently, due to the prosper-ity of deep learning techniques, convolutional neural net-work (CNN) based methods have demonstrated promising performance for light ﬁeld SR [10, 13, 23, 38, 39, 44, 45, 46,
*Correspondence should be addressed to zwxiong@ustc.edu.cn 49,52], and the state-of-the-art methods exceed classic non-learning-based methods [4, 18, 30] with notable gains. Such performance boost is obtained by training well-engineered
CNNs on a large external dataset to explore the 4D light
ﬁeld structures. However, these deep-learning-based meth-ods inevitably face the domain shift problem [6], which hin-ders their capability of generalizing to unseen light ﬁelds with a large domain gap from the training set.
Domain shift, which means the performance drop when a deep neural network is trained on one dataset (source) but tested on another dataset (target), is much more severe in light ﬁeld SR than that in single image SR. The underlying reason is that, light ﬁeld SR exploits not only the 2D spatial correlation within each view, but also the 2D angular corre-spondence among different views (also called across-view redundancy). Such a 4D spatio-angular structure varies a lot between light ﬁelds captured by different acquisition sys-tems and conﬁgurations. Take cameras using micro-lens-array for example, the baselines between the micro lens can be quite different in different types of cameras, resulting in distinct angular correspondences. Therefore, the network trained with a certain light ﬁeld dataset could easily overﬁt to the spatio-angular structure within the given dataset and thus may not perform well on light ﬁelds in the wild.
To address this problem, we propose a zero-shot learn-ing framework for light ﬁeld SR, which learns a mapping to super-resolve the reference view with examples extracted solely from the input low-resolution (LR) light ﬁeld it-self. This work is inspired by the recently proposed zero-shot single image SR (ZSSR) method [34], which exploits across-scale recurrence within a single image and trains an
SR network with paired examples extracted from the in-put LR image and its downscaled version.
In this way, the input-speciﬁc model can generalize well on real images with unknown acquisition process, where abundant data for external training are not available. However, given highly limited training data under the zero-shot setting, we ob-serve that it is difﬁcult to train end-to-end SR networks successfully. Through a comparative study, we then ﬁnd that a divide-and-conquer strategy, which explicitly divides the SR task into several sub-tasks and conquers them sepa-rately, can facilitate the learning of the SR mapping. 10010
Speciﬁcally, our proposed zero-shot light ﬁeld SR frame-work consists of three sub-tasks, i.e., pre-upsampling, view alignment, and multi-view aggregation. We select the
VDSR [15] network pre-trained on a 2D image dataset for preliminary upsampling. For view alignment, we design an alignment-oriented disparity estimation network follow-ing a plane-sweep volume generator, which can be readily trained in an unsupervised manner. After disparity-guided warping, an aggregation network is designed to aggre-gate the aligned views for high-frequency detail restoration, which can be trained with light ﬁeld patch pairs extracted from the input LR light ﬁeld and its downscaled version in a self-supervised manner. In this way, an input-speciﬁc
SR model can be trained given an LR light ﬁeld without any external light ﬁeld dataset. Thanks to the divide-and-conquer strategy, the obtained model produces impressive high-resolution (HR) results even with highly limited train-ing data, which outperforms state-of-the-art light ﬁeld SR models when the domain gap is large.
The proposed zero-shot framework paves a way for light
ﬁeld SR in the wild, where abundant external training data that match the target input are not available. Moreover, this framework can be readily extended to ﬁnetune the pre-trained model on a source dataset to better adapt to the tar-get input. Speciﬁcally, we propose an error-guided ﬁnetun-ing algorithm to handle the regions where the pre-trained model is less effective by selecting complementary training samples from the target input.
In this way, new state-of-the-art results are generated for light ﬁeld SR in the wild, which again validates the effectiveness of our method in closing the domain gap. We believe the zero-shot frame-work introduced in this paper could also inspire other in-verse problems where high dimensional data acquiring with customized hardware are involved. 2.