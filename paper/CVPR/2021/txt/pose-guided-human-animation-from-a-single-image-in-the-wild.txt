Abstract
We present a new pose transfer method for synthesizing a human animation from a single image of a person con-trolled by a sequence of body poses. Existing pose transfer methods exhibit signiﬁcant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and fail-ures in preserving the identity and textures of the person. To address these limitations, we design a compositional neu-ral network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a uniﬁed representation of appearance and its labels in UV coordinates, which remains constant across poses. The uni-ﬁed representation provides an incomplete yet strong guid-ance to generating the appearance in response to the pose change. We use the trained network to complete the appear-ance and render it with the background. With these strate-gies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a tem-porally coherent way without any ﬁne-tuning of the network on the testing scene. Experiments show that our method out-performs the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability. 1.

Introduction
Being able to animate a human in everyday apparel with an arbitrary pose sequence from just a single still image opens the door to many creative applications. For example, animated photographs can be much more memorable than static images. Furthermore, such techniques not only sim-plify and democratize computer animation for non-experts, they can also expedite pre-visualization and content cre-ation for more professional animators who may use single image animations as basis for further reﬁnement.
Tackling this problem using classical computer graphics techniques is highly complex and time consuming. A high-quality 3D textured human model needs to be reconstructed from a single image and then sophisticated rigging methods are required to obtain an animatable character. An alterna-tive is to apply 2D character animation methods [17, 19] to animate the person in the image. However, this approach cannot visualize the occluded parts of the character.
In this paper, we approach this problem using a pose transfer algorithm that synthesizes the appearance of a per-son at arbitrary pose by transforming the appearance from an input image without requiring a 3D animatable textured human model. Existing works on pose transfer have demon-strated promising results only when training and testing take place on the same dataset (e.g., DeepFashion dataset [28]), and some require even more restrictive conditions that test-ing is performed on the same person in the same environ-ment as training. [8, 25, 26]. However, the domain differ-ence between training and testing data in real applications introduces substantial quality degradation.
A core challenge of pose transfer lies in lack of data that span diverse poses, shapes, appearance, viewpoints, and background. This leads to limited generalizability to a test-ing scene, resulting in noticeable visual artifacts as shown in 15039
Figure 2. The pose transfer results synthesized by a state-of-the-art method [44] on an unconstrained real-world scene, where the network is trained on the Deep Fashion dataset [28]. The target body pose is shown in the inset (black). Each box represents the type of the observed artifacts such as loss of identity (red), mis-classiﬁed body parts (blue), background mismatch (yellow), and temporal incoherence (green).
Fig. 2. We address this challenge by decomposing the pose transfer task into modular subtasks predicting silhouette, garment labels, and textures where each task can be learned from a large amount of synthetic data. This modularized de-sign makes training tractable and signiﬁcantly improves re-sult quality. Explicit silhouette prediction further facilitates animation blending with arbitrary static scene backgrounds.
In inference phase, given the trained network from the synthetic data, we introduce an efﬁcient strategy for synthe-sizing temporally coherent human animations controlled by a sequence of body poses. We ﬁrst produce a uniﬁed rep-resentation of appearance and its labels in UV coordinates, which remains constant across different poses. This uniﬁed representation provides an incomplete yet strong guidance to generating the appearance in response to the pose change.
We use the trained network to complete the appearance and render it with the background. Experiments show that our method signiﬁcantly outperforms the state-of-the-art meth-ods in terms of synthesis quality, temporal consistency, and generalization ability.
Our technical contributions include (1) a novel ap-proach that can generate a realistic animation of a person from a single image controlled by a sequence of poses, which shows higher visual quality and temporal coherence, and generalizes better to new subjects and backgrounds; (2) a new compoositional pose transfer framework that pro-duces the silhouette mask, garment labels, and textures, which makes the pose transfer tractable; (3) an effective in-ference method by generating a uniﬁed representation of ap-pearance and its labels for image synthesis, enforcing tem-poral consistency and preserving identity in the presence of occlusion and shape deformation. 2.