Abstract
Recent study on detecting facial action units (AU) has utilized auxiliary information (i.e., facial landmarks, rela-tionship among AUs and expressions, web facial images, etc.), in order to improve the AU detection performance. As of now, no semantic information of AUs has yet been ex-plored for such a task. As a matter of fact, AU semantic de-scriptions provide much more information than the binary
AU labels alone, thus we propose to exploit the Semantic
Embedding and Visual feature (SEV-Net) for AU detec-tion. More speciﬁcally, AU semantic embeddings are ob-tained through both Intra-AU and Inter-AU attention mod-ules, where the Intra-AU attention module captures the rela-tion among words within each sentence that describes indi-vidual AU, and the Inter-AU attention module focuses on the relation among those sentences. The learned AU semantic embeddings are then used as guidance for the generation of attention maps through a cross-modality attention network.
The generated cross-modality attention maps are further used as weights for the aggregated feature. Our proposed method is unique in that the semantic features are exploited as the ﬁrst of this kind. The approach has been evaluated on three public AU-coded facial expression databases, and has achieved a superior performance than the state-of-the-art peer methods. 1.

Introduction
Facial action units (AUs) deﬁned in the Facial Action
Coding System (FACS)[5] has been widely used for de-scribing and measuring facial behavior. Automatic action unit detection has been an essential task for facial analysis, with a variety of applications in psychological and behav-ioral research, mental health assessment and human-robot interaction.
Beneﬁted from the great progress in deep learning re-the performance of AU detection has been im-search,
Figure 1. An example of the individual AUs, related facial ar-eas and the corresponding AU semantic descriptions. Red: facial area/position, Green: action, Yellow: motion direction, and Blue: motion intensity. As we can see, the AU related facial areas and their actions are clearly explained in each AU semantic descrip-tion. The facial area/position, action, motion direction and inten-sity, and relation of AUs will be automatically encoded in the AU semantic embedding, as described in Section 3.2. proved using the deep-model based methods in recent years the deep-model
[28][30][12][3][19][22][15]. However, based methods are starved for labeled data, whereas AU annotation is a highly labor intensive and time consum-ing process, thus many existing works seek to exploit the auxiliary information for AU detection, which include, for example, domain knowledge (e.g., probabilistic dependen-cies between expressions and AUs as well as dependencies among AUs) [17][18][26]; facial landmarks and expression labels [13][15], and freely web face images [29]. Although the performance has a certain improvement when utilizing those auxiliary information, the AU semantic descriptions have not yet been explored by any of the previous methods. 10482
FACS provides a complete set of textual descriptions for
AU deﬁnition, such a set of AU descriptions provide rich semantic information, such as which facial area is related to the individual AU, what intensity and type of action can be considered as the occurrence of an AU, and what is the relation among AUs, etc. Such a unique ﬁnding motivates us to explore the textual descriptions as an auxiliary infor-mation along with the visual information for AU detection.
Figure 1 illustrates an example of AU semantic descriptions on three AUs. As we can ﬁnd that two facial areas (chin boss and lower lip) and two corresponding actions (wrinkle chin boss and push up the lower lip) involve in the occurrence of AU17. Besides, we can also obtain the potential relation among AUs. For example, AU23 occurs in the area of lips, which share the lower lip with AU17, but they rarely appear together as different actions applied to the lip (tighten vs push up). A similar example is AU12 (lip corner puller) vs
AU15 (lip corner depressor). Those semantic information (i.e., facial area/position, action, motion direction/intensity, and relation of AUs) will be automatically encoded in the
AU semantic embedding, which will be described in Sec-tion 3.2.
Two recent works [2][24] have been developed to explic-itly model the label relationships from the semantic label embeddings using a graph convolutional network (GCN) based method for multi-label image recognition. These two works have demonstrated that explicitly modeling the label relationships from the label embeddings is beneﬁcial for the discovery of meaningful locations and discriminative fea-tures. However, both of them rely on the manually deﬁned label relation graph, as used in the GCN module, making them incapable of applications without the ground truth la-bel relation graph.
Inspired by the self-attention mechanism from trans-former [20] and Inter/Intra attention modules in [7], we pro-pose a novel framework to exploit the Semantic Embedding and Visual feature (SEV-Net) for AU detection, which will automatically learn the AU relations from the AU semantic descriptions. First of all, in order to capture the seman-tic relations among AUs, we introduce two new attention modules, which are so-called Intra-AU and Inter-AU atten-tion module, where the Intra-AU attention module targets at the word-level attention among the AU semantic descrip-tions (i.e., <lip corner> −<raised>), while the Inter-AU attention module focuses on the relation among sentences (i.e., both AU12 and AU15 occur at the lip corner, but they cannot happen concurrently because opposite actions are associated with the corresponding AUs (puller vs depres-sor)). Second, the learned AU semantic embeddings are fur-ther combined with the visual features to generate the atten-tion map through a cross-modality attention module. Unlike the traditional self-attention methods, the cross-modality at-tention module beneﬁts from the rich semantic information (i.e.,facial area/position, action, motion direction/intensity, and relation of AUs), hence being able to learn more useful and discriminative features from more meaningful facial ar-eas. The attention maps are further utilized as weights for the aggregated feature for AU classiﬁcation.
In summary, the contributions of this work are two-fold: 1. We proposed a uniﬁed framework that applying the attention into three different levels to capture differ-ent AU semantic relations: Intra-AU attention (Words level: location, action type/intensity, etc), Inter-AU at-tention (Sentence level: AU relations, can two AUs happen concurrently?) and cross-modality attention (Modality level: connecting the AU semantic embed-ding to visual features). As a result, the model is able to learn more discriminative features from more mean-ingful areas. 2. To the best of our knowledge, this is the ﬁrst work to introduce AU semantic description as an auxiliary in-formation for AU detection, achieving signiﬁcant im-provement for AU detection than SOTA in three widely used datasets. 2.