Abstract
History Poses
Pose Estimation (HRNet)
Ours
Multi-person pose estimation and tracking serve as cru-cial steps for video understanding. Most state-of-the-art ap-proaches rely on ﬁrst estimating poses in each frame and only then implementing data association and reﬁnement.
Despite the promising results achieved, such a strategy is inevitably prone to missed detections especially in heavily-cluttered scenes, since this tracking-by-detection paradigm is, by nature, largely dependent on visual evidences that are absent in the case of occlusion. In this paper, we pro-pose a novel online approach to learning the pose dynam-ics, which are independent of pose detections in current fame, and hence may serve as a robust estimation even in challenging scenarios including occlusion. Speciﬁcally, we derive this prediction of dynamics through a graph neural network (GNN) that explicitly accounts for both spatial-temporal and visual information. It takes as input the his-torical pose tracklets and directly predicts the correspond-ing poses in the following frame for each tracklet. The predicted poses will then be aggregated with the detected poses, if any, at the same frame so as to produce the ﬁnal pose, potentially recovering the occluded joints missed by the estimator. Experiments on PoseTrack 2017 and Pose-Track 2018 datasets demonstrate that the proposed method achieves results superior to the state of the art on both hu-man pose estimation and tracking tasks. 1.

Introduction
Multi-person pose estimation and tracking ﬁnd their ap-plications in a wide spectrum of scenarios including behav-ior analysis and action recognition, and have therefore re-ceived increasing attention in recent years [45, 32, 19]. De-spite often coupled together, they focus on slightly differ-ent aspects: the former aims to locate human joints in each
*The work is partially done when the author is an internship at Worm-pex AI Research.
†Corresponding author. t-2 t-1 t t
Figure 1. By modeling the pose dynamics from history poses through a graph neural network, our method learns a pose predic-tion that is robust to challenging scenes, such as motion blur (top) and occlusion (bottom).
In both cases, the visual-based HR-Net [37] fails to locate the joints, yet our approach delivers de-pendable pose estimations. frame of an input video, while the latter one aims to asso-ciate joints that belong to the same human across frames. It has been long considered as a challenging task due to var-ious factors, including but not limited to camera motions, complex backgrounds, and mutual occlusions.
Thanks to the recent advances of deep learning tech-niques, pose estimation and tracking have witnessed un-precedented results in the past years. Existing methods can be broadly categorized into two streams, bottom-up meth-ods [32, 19, 55, 18] and top-down methods [45, 53, 40].
Bottom-up methods ﬁrst generate joint candidates and then group the joints into a person detection. The grouped joints are then associated across frames to generate the ﬁnal pose tracking results. Top-down methods, on the other hand, ﬁrst detect human candidates in a single frame and then estimate the human poses for each candidate. The estimated human poses are associated across frames to achieve pose track-ing. Methods from both streams have produced promising 8074
results on various scenarios [53, 40].
In spite of the encouraging results, state-of-the-art pose estimation and tracking approaches remain prone to missed detections especially in highly-cluttered and fast-motion scenes. This is not totally unexpected, since by nature they rely on ﬁrst detecting either joints or human bodies in a scene using a visual-based detector, and only then carry-ing out data association to link the detections into tracks. In challenging scenarios such as crowded or blurred scenes, the joint- or human-detector would inevitably fail due to the absent image evidences. Although some succeeding re-ﬁnement steps would mildly remedy the ﬂawed estimations, they are are still largely dependent on visual cues and hence incompetent to fully tackle missed detections.
We propose in this paper a novel approach by explic-itly looking into the dynamics of human poses within image sequences.
In contrast to state-of-the-art approaches that rely on ﬁrst detecting human or joints in each frame, which is again prone to failures in the absence of detection evi-dences, our approach ﬁrst predicts poses in a frame from a track of history without looking at any detection cue. This strategy allows us to free our dependency on the detection evidences and consequently produce a legitimate state of human pose at the very ﬁrst place. Speciﬁcally, in our ap-proach this prediction step is accomplished through a graph neural network (GNN) that takes as input a track of history poses in previous frames. Next, the predicted pose is aggre-gated with the detected poses, if any, in the same frame to produce the ﬁnal pose, in which way both dynamical and visual information are exploited. At a conceptual level, our approach follows a similar spirit of Bayesian ﬁlters, expect that in our approach all parameters and features are learned end to end. A qualitative example is shown in Figure 1, where our dynamic-based approach yields dependable pose estimation results in the cases of motion blur and occlusion.
Apart from the strength of recovering missed poses from predictions, the proposed approach also enjoys other mer-its. First, prior approaches match poses between two con-secutive frames, which is brittle to identify switches due to factors such as intersection of poses and fast motion. Our approach, by contrast, aggregates poses within the same frame, thanks to our prediction-based nature, allowing us to signiﬁcantly reduce the mismatched rate. Second, as com-pared to state-of-the-art methods, our approach tackles pose tracking from an additional perspective, i.e. the motion dy-namics, which complements the visual cues that are in many cases absent, resulting in gratifying ﬁnal poses.
We evaluate the effectiveness of the proposed method on two widely used benchmark datasets, PoseTrack 2017 and
PoseTrack 2018. Empirical evaluations showcase that our method outperforms state-of-the-art approaches by a con-siderably large margin on both pose estimation and tracking tasks. We also provide extensive analyses on the impact of each component in the proposed method, and demonstrate the superiority of learning pose dynamics using our method. 2.