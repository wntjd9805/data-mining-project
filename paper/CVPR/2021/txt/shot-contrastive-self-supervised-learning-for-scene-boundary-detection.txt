Abstract 1.

Introduction
Scenes play a crucial role in breaking the storyline of movies and TV episodes into semantically cohesive parts.
However, given their complex temporal structure, ﬁnding scene boundaries can be a challenging task requiring large amounts of labeled training data. To address this challenge, we present a self-supervised shot contrastive learning ap-proach (ShotCoL) to learn a shot representation that maxi-mizes the similarity between nearby shots compared to ran-domly selected shots. We show how to apply our learned shot representation for the task of scene boundary detection to offer state-of-the-art performance on the MovieNet [33] dataset while requiring only ∼25% of the training labels, using 9× fewer model parameters and offering 7× faster runtime. To assess the effectiveness of ShotCoL on novel applications of scene boundary detection, we take on the problem of ﬁnding timestamps in movies and TV episodes where video-ads can be inserted while offering a minimally disruptive viewing experience. To this end, we collected a new dataset called AdCuepoints with 3, 975 movies and TV episodes, 2.2 million shots and 19, 119 minimally disrup-tive ad cue-point labels. We present a thorough empirical analysis on this dataset demonstrating the effectiveness of
ShotCoL for ad cue-points detection.
*Equal contribution.
In ﬁlmmaking and video production, shots and scenes play a crucial role in effectively communicating a storyline by dividing it into easily interpretable parts. A shot is deﬁned as a series of frames captured from the same camera over an uninterrupted period of time [40], while a scene is deﬁned as a series of shots depicting a semantically cohesive part of a story [23] (see Figure 1 for an illustration). Localizing shots and scenes is an important step towards building se-mantic understanding of movies and TV episodes, and of-fers a broad range of applications including preview gen-eration for browsing and discovery, content-driven video search, and minimally disruptive video-ads insertion.
Unlike shots which can be accurately localized using low-level visual cues [38] [6], scenes in movies and TV episodes tend to have complex temporal structure of their constituent shots and therefore pose a signiﬁcantly more difﬁcult challenge for their accurate localization. Existing unsupervised approaches for scene boundary detection [3]
[34] [2] do not offer competitive levels of accuracy, while supervised approaches [33] require large amounts of labeled training data and therefore do not scale well. Recently, several self-supervised learning approaches have been ap-plied to learn generalized visual representations for im-ages [22] [1] [16] [18] [28] [48] [51] [43] and short video 9796
clips [32] [12] [46] [41], however it has been mostly unclear how to extend these approaches to long-form videos. This is primarily because the relatively simple data augmentation schemes used by previous self-supervised methods cannot encode the complex temporal scene-structure often found in long-form movies and TV-episodes.
To address this challenge, we propose a novel shot con-trastive learning approach (ShotCoL) that naturally makes use of the underlying production process of long-form videos where directors and editors carefully arrange differ-ent shots and scenes to communicate the story in a smooth and believable manner. This underlying process gives rise to a simple yet effective invariance, i.e., nearby shots tend to have the same set of actors enacting a semantically cohesive story-arch, and are therefore in expectation more similar to each other than a set of randomly selected shots. This in-variance enables us to consider nearby shots as augmented versions of each other where the augmentation function can implicitly capture the local scene-structure signiﬁcantly bet-ter than the previously used augmentation schemes. Specif-ically, given a shot, we try to: (a) maximize its similarity with its most similar neighboring shot, and (b) minimize its similarity with a set of randomly selected shots (see Fig-ure 1 for an illustration).
We show how to use our learned shot representation for the task of scene boundary detection to achieve state-of-the-art results on MovieNet dataset [33] while requiring only
∼25% of the training labels, using 9× fewer model param-eters, and offering 7× faster runtime. Besides these perfor-mance beneﬁts, our single-model based approach is signiﬁ-cantly easier to maintain in a production setting compared to previous approaches that make use of multiple models [33].
As a practical application of scene boundary detection, we explore the problem of ﬁnding timestamps in movies and TV episodes for minimally disruptive video-ads inser-tion. To this end, we present a new dataset called AdCue-points with 3, 975 movies and TV episodes, 2.2 million shots, and 19, 119 manually labeled minimally disruptive ad cue-points. We present a thorough empirical analysis on this dataset demonstrating the generalizability of ShotCoL on the task of ad cue-points detection. 2.