Abstract
Analysis of human sketches in deep learning has ad-vanced immensely through the use of waypoint-sequences rather than raster-graphic representations. We further aim to model sketches as a sequence of low-dimensional parametric curves. To this end, we propose an inverse graphics framework capable of approximating a raster or waypoint based stroke encoded as a point-cloud with a variable-degree B´ezier curve. Building on this module, we present Cloud2Curve, a generative model for scalable high-resolution vector sketches that can be trained end-to-end using point-cloud data alone. As a consequence, our model is also capable of deterministic vectorization which can map novel raster or waypoint based sketches to their corresponding high-resolution scalable B´ezier equivalent.
We evaluate the generation and vectorization capabilities of our model on Quick, Draw! and K-MNIST datasets. 1.

Introduction
The analysis of free-hand sketches using deep learning
[40] has ﬂourished over the past few years, with sketches now being well analysed from classiﬁcation [43, 42] and retrieval [27, 12, 4] perspectives. Sketches for digital anal-ysis have always been acquired in two primary modalities
- raster (pixel grids) and vector (line segments). Raster sketches have mostly been the modality of choice for sketch recognition and retrieval [43, 27]. However, generative sketch models began to advance rapidly [16] after focus-ing on vector representations and generating sketches as sequences [7, 37] of waypoints/line segments, similarly to how humans sketch. As a happy byproduct, this paradigm leads to clean and blur-free image generation as opposed to direct raster-graphic generations [30]. Recent works have studied creativity in sketch generation [16], learning to sketch raster photo input images [36], learning efﬁcient hu-g n i l p m a
S
Pen	Sketch
Preprocessing
Point	Cloud
Cloud2Curve
Generated	Vector	Sketches g n i l p m a
S
Figure 1. Cloud2Curve capability teaser. Top: Our trained model can vectorize a pen-on-paper sketch using scalable parametric curves. Bottom: Cloud2Curve can be trained on raster datasets such as KMNIST, where existing generative models cannot. man sketching strategies [3], exploiting sketch generation for photo representation learning [39], and the interaction between sketch generation and language [18].
We present Cloud2Curve, a framework that advances the generative sketch modeling paradigm on two major axes: (i) by generating parametric sketches, i.e., a compositions of its constituent strokes as scalable parametric curves; and (ii) providing the ability to sample such sketches given point-cloud data, which is trivial to obtain either from raster-graphic or waypoint sequences. Altogether, our framework uniquely provides the ability to generate or deterministi-cally vectorize scalable parametric sketches based on point clouds, as illustrated in Figure 1 and explained below.
First, we note that although existing frameworks like
SketchRNN [16] and derivatives generate vector sketches, they do so via generation of a dense sequence of short straight line segments. Consequently, (i) the output sketches are not spatially scalable as required, e.g., for digital art ap-plications, (ii) they struggle to generate long sketches due to the use of recurrent generators [28], and (iii) the generative process suffers from low interpretability compared to hu-man sketching, where the human mental model relies more on composing sketches with smooth strokes. While there 7088
have been some initial attempts at scalable vector sketch generative models [10], they were limited by the need for a two step training process, and to generate parametric B´ezier curves of a ﬁxed complexity, which poorly represent the di-verse kinds of strokes in human sketches. In contrast, we introduce a machinery to dynamically determine the opti-mal complexity of each generated curve by introducing a continuous degree parameter; and our model can further be trained end-to-end.
Second, existing generative sketch frameworks require purely sequential data in order to train their sequence-to-sequence encoder-decoder modules. This necessitates the usage of specially collected sequential datasets like Quick,
Draw! and not general purpose raster sketch datasets. In contrast, we introduce a framework that encodes point-cloud training data and decodes a sequence of paramet-ric curves. We achieve this through an inverse-graphics
[22, 33] approach of reconstructing training images/clouds by rendering its constituent strokes individually through a white-box B´ezier decoder over several time-steps. To train this framework we compare reconstructed curves with the original segmented point cloud strokes with an Optimal
Transport (OT) based objective. We show that such objec-tives, when coupled with appropriate regularizers, can lead to controllable abstraction of sketches.
To summarize our contributions: 1) We introduce a novel formulation of B´ezier curves with a continuous degree pa-rameter that is automatically inferred for each individual stroke of a sketch. 2) We develop Cloud2Curve, a genera-tive model capable of training and inference on point cloud data to produce spatially scalable parametric sketches. (3)
We demonstrate scalable parametric curve generation from point-clouds, using Quick, Draw! and a subset of K-MNIST
[9] datasets. 2.