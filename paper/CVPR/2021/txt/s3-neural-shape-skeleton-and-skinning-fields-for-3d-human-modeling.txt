Abstract
Constructing and animating humans is an important com-ponent for building virtual worlds in a wide variety of ap-plications such as virtual reality or robotics testing in sim-ulation. As there are exponentially many variations of hu-mans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian’s shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without ex-plicitly ﬁtting a human parametric body model, allowing us to handle a wider range of human geometries and topologies.
We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform exist-ing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human anima-tions at scale from a single RGB image (and/or an optional
LiDAR sweep) as input. 1.

Introduction
Realistic, articulated human simulation is an important task with a wide range applications. It helps to bring char-acters to life in video games and movies [61], provides re-alistic AR/VR experiences in the context of sports [71, 67] and social media [12], and is starting to play an important role in realistic simulations for testing robotic systems in both indoor [51, 58, 39] and outdoor environments [20, 64].
Traditionally, human reconstruction and animation is a time-consuming manual process. An artistic designer needs to create a scale-appropriate "joint estimation" that speciﬁes the human’s degrees of freedom, "reconstruct" the human 3D mesh geometry, and "skin" the mesh by describing how the vertex positions deform as a function of the skeleton joints pose. Finally, the artist must specify the pose sequence that enacts an animation. This manual approach is neither cost-effective nor efﬁcient if we want to reconstruct and animate 3D humans at scale, for example by digitizing millions of different pedestrians observed in urban city scenes.
Automating the human reconstruction and animation is very challenging as there are large variations in pedestrian shape, pose, clothing, and accoutrement. Most existing an-imatable human modeling methods are pipelined systems consisting of a set of modules that run separately [59, 53, 61].
Typically, the ﬁrst module performs joint estimation us-ing marker [17] or markerless [11] motion capture. From either dense 3D scans or images, the system then recon-structs shape [14, 6, 34, 30, 66, 5, 25, 4, 21, 49] and tex-ture [56, 55, 41]. The estimated pose skeleton is further associated with the mesh typically via closed-form skinning models [27, 28] with hand-designed or automatic weight painting [8, 19]. Unfortunately, for existing approaches to work successfully, they typically rely on an expensive 3D 13284
scanner in a controlled environment, a multi-camera cage
[59, 53], or alternatively require relatively controlled view-points, typical frontal views with small variations [61]. Few works so far have performed fully automatic end-to-end re-construction and animation of clothed humans [22, 62, 9], but they all require ﬁtting to a parametric body model, mak-ing it difﬁcult to animate humans that deviate signﬁcantly in geometry (i.e. non-tight clothing such as skirts and dresses).
In this paper, we propose a scalable solution by recon-structing 3D animatable humans in the wild that takes ad-vantage of sensory data captured around our cities. However, this setting brings new challenges: in-the-wild data lacks ground-truth 3D shape and pose, making supervision for deep-learning based models challenging. Furthermore, the captured sensory data can be noisy, of low-resolution, under non-canonical views and poses, and with various lighting conditions. These conditions are especially challenging for skinning, which requires accurate correspondence of each surface location to its corresponding body part. Inaccurate as-signments will cause large aberrations during animation. To address these challenges, we propose a novel approach that takes sensor data captured at a single viewpoint (i.e. image and/or LiDAR sweep) of a pedestrian and jointly predicts 3D mesh, skeleton joints, and skinning weights, all with a single network (see Figure 1). The resulting animatable pedestrian can be directly deformed to novel poses and placed into new scenarios using either motion capture data or artist-created animations. Inspired by the recent success of implicit model-ing [42, 7, 15] and neural radiance ﬁelds [38], we represent a 3D human as a continuous multi-dimensional neural ﬁeld, which outputs the occupancy, human joint probability as well as skinning weights given each input 3D location in continuous space. This representation is very ﬂexible and can capture ﬁne details of clothed humans, handle different surface topologies (i.e. skirts), and adapt well to unseen human shapes as it is not constrained by a parametric model with a ﬁxed (mesh) topology. In addition, our end-to-end architecture overcomes the challenge of error propagation in conventional pipelines.
We demonstrate the effectiveness of our approach on both photorealistic synthetic human 3D data and a large-scale real-world self-driving dataset. Our approach achieves better quantitative and qualitative performance compared to state-of-the-art methods in terms of shape reconstruction quality.
Importantly, we also show that we can reliably re-animate the reconstructed 3D human given novel poses. 2.