Abstract
Existing deep deraining models are mainly learned via directly minimizing the statistical differences between rainy images and rain-free ground truths. They emphasize learn-ing a mapping from rainy images to rain-free images with supervision. Despite the demonstrated success, these meth-ods do not perform well on restoring the ﬁne-grained local details or removing blurry rainy traces. In this work, we aim to exploit the intrinsic priors of rainy images and de-velop intrinsic loss functions to facilitate training derain-ing networks, which decompose a rainy image into a rain-free background layer and a rainy layer containing intact rain streaks. To this end, we introduce the quasi-sparsity prior to train network so as to generate two sparse lay-ers with intact textures of different objects. Then we ex-plore the low-value prior to compensate sparsity, forcing all rain streaks to enter into one layer while non-rain con-tents into another layer to restore image details. We in-troduce a multi-decoding structure to specially supervise the generation of multi-type deraining features. This helps to learn the most contributory features to deraining in re-spective spaces. Moreover, our model stabilizes the feature values from multi-spaces via information sharing to allevi-ate potential artifacts, which also accelerates the running speed. Extensive experiments show that the proposed de-raining method outperforms the state-of-the-art approaches in terms of effectiveness and efﬁciency. 1.

Introduction
The rainy artifacts on images usually include visible rain streaks and the haze-like veiling effect. This work focuses on rain streaks removal as it is still an unsolved challenging task. Conventional methods usually remove rain via learn-ing an over-complete dictionary [26, 1, 10]. The rain-free image is reconstructed by the non-rain dictionary atoms, which are identiﬁed from all the learned dictionary atoms
*C. Ma and B. Zeng are corresponding authors. by means of the heuristic appearance characteristics of rain.
These methods do not perform well when the patterns of rain streaks are complex, e.g., rain streaks overlap and in-terweave. Without using high-level features, complex rain streaks cannot be identiﬁed, causing heavy performance bottleneck.
Recently, deep learning based methods explore high-level features of rainy images through pretrained CNN mod-els. They achieve the state-of-the-art deraining performance in not only restoration quality but also the running speed
[27, 19, 8, 31, 15, 5, 4, 30, 35, 17, 25]. However, the limitation appears when rain streaks are wide or blurry.
Fig. 1 shows an example that, when the middle parts of rain streaks become brighter, while the edges are blurry, ex-isting deep learning based methods only remove the bright parts, but the blurry edges still remain. The reason lies in that these CNN models are trained by means of minimiz-ing the statistical differences between deraining results and ground truth images, as formulated by the MSE [35, 5, 30] or MAE [14] loss functions. MSE and MAE converge at the arithmetic mean and median of the observations respec-tively, which correlate poorly with image details restoration
[12, 37]. Therefore, exploring losses beyond the traditional formulation is urged for further performance improvement.
In this paper, we introduce the intrinsic priors to con-struct loss functions. We ﬁrst study the sparsity of rainy images to achieve sparse image decomposition, i.e., mak-ing each sparse layer contain as intact (not split) textures of objects as possible [13, 3, 20]. Based on such property, rain streaks will not be split during deraining, as well as other image contents. Note that the concept of sparsity here is an intrinsic property of images, which is different from the one indicating the majority of elements in a matrix are zero or close to zero [29]. However, existing sparse stochastic distributions lead to too complex derivation of maximum likelihood (ML) to train CNNs [13].
To obtain tractable loss functions via ML, we relax the sparsity degree and develop quasi-sparsity prior to approx-imate the sincere sparsity. Quasi-sparsity with simpler for-mula keeps the property of sparsity and can be used to train 13375
(a) (c)
Figure 1. (a) Input rainy image. (b) Deraining result with quasi-sparsity. (c) Rain layer with quasi-sparsity. (d) Deraining result without quasi-sparsity. (e) Rain layer without quasi-sparsity. Without quasi-sparsity, a few traces of rain streaks remain in the deraining result. (b) (d) (e)
CNN to decompose images sparsely, meaning that the tex-tures belonging to one object will not appear in different layers. Moreover, an auxiliary decoder loss and a low-value prior loss are proposed to force rain streaks to enter into the rainy layer during deraining. Fig. 1 compares our networks with and without quasi-sparsity. We observe that quasi-sparsity produces clearer deraining result and removes rain more thoroughly.
Deep features from different spaces have not be fully studied before [35, 30]. We introduce novel auxiliary de-coders to respectively decode deep features in multi-spaces.
Auxiliary decoders play roles in two aspects: 1) they gener-ate rain-free images from multi-spaces respectively, which helps to compare the effectiveness of different types of deep features; 2) they boost encoder to generate optimal features via respectively imposing supervision in each space. Fea-tures from different spaces usually possess large numerical gap that may cause undesired artifact [31]. We propose to implement information sharing among multi-type features before feeding them into the main decoder. Ablation studies illustrate that information sharing alleviate the undesired ar-tifact by stabilizing the feature values from different spaces.
In summary, the main contributions are listed below:
• We explore the intrinsic properties of rainy images, i.e., quasi-sparsity and low-value priors, for deraining.
• We introduce a multi-decoding structure that enables each feature space to generate optimal deraining fea-tures by respectively imposing supervision. We intro-duce information sharing to stabilize feature values of multi-spaces to alleviate artifact.
• We conduct extensive experiments in comparison with the state of the arts. We push forward the state-of-the-art detaining performance by large margins in terms of both effectiveness and efﬁciency. 2.