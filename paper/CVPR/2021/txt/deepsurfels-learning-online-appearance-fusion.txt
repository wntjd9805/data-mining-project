Abstract
We present DeepSurfels, a novel hybrid scene represen-tation for geometry and appearance information. DeepSur-fels combines explicit and neural building blocks to jointly encode geometry and appearance information. In contrast to established representations, DeepSurfels better repre-sents high-frequency textures, is well-suited for online up-dates of appearance information, and can be easily com-bined with machine learning methods. We further present an end-to-end trainable online appearance fusion pipeline that fuses information from RGB images into the proposed scene representation and is trained using self-supervision imposed by the reprojection error with respect to the input images. Our method compares favorably to classical tex-ture mapping approaches as well as recent learning-based techniques. Moreover, we demonstrate lower runtime, im-proved generalization capabilities, and better scalability to larger scenes compared to existing methods. 1.

Introduction
Realistic 3D model reconstruction from images and depth sensors has been a central and long-studied problem in computer vision. Appearance mapping is often treated as a separate post-processing step that follows 3D surface reconstruction and is usually approached using batch-based optimization methods [18, 19, 23, 87] that are unsuitable for many applications that do not have access to the en-tire dataset at processing time, for instance, robot naviga-tion [8, 9, 26], augmented reality [51, 68], and virtual re-ality [14, 43, 44] applications, Simultaneous Localization and Mapping (SLAM) systems [93], online scene percep-tion methods [28, 69], and many others.
Common online fusion methods like KinectFusion [50] are well suited for online geometry fusion and can efﬁ-ciently handle noise and topological changes. However, due to their high memory requirements at high voxel res-olutions, they have strong limitations when it comes to encoding high-frequency appearance details on the sur-face. On the other hand, meshes with high-resolution tex-ture maps [19, 23, 87] are well-suited for encoding high-frequency appearance information in an efﬁcient manner, but they have difﬁculties in handling topology changes in an online reconstruction setting. Moreover, recent learning-based approaches [49, 54, 72, 73] have achieved high-quality results by learning geometry and texture mapping directly from RGB images. However, they are not well suited for local online updates, do not scale to large-scale scenes, and easily overﬁt to the training data.
In this paper, we approach the problem of online appear-ance reconstruction from RGB-D images by combining the advantages of 1) implicit grids, which easily handle topo-logical changes and where low resolution is often sufﬁcient to encode the scene topology, 2) scalable high-frequency appearance along the surface via texture maps or learned feature maps, and 3) a learned scene representation to build a framework for learning-based appearance fusion that al-lows for online processing and scalability to large scenes.
To this end, we propose a novel scene representation Deep-Surfels and an efﬁcient learning-based online appearance fusion pipeline which is illustrated in Figure 1.
Our DeepSurfels representation is a hybrid between an implicit surface that encodes the topology and low-frequency geometric details and a surfel representation that encodes high-frequency geometry and appearance informa-tion in form of surface-aligned patches. These patches are arranged in a sparse grid and consist of surface-aligned tex-els that encode appearance information either in the classi-cal form of RGB color values or, as proposed, via learned feature vectors. The sparse grid allows for efﬁcient vol-umetric rendering and enables explicit scene updates that are crucial for online fusion, while the 2D patches enable quadratic memory storage complexity like meshes or sparse grid structures. Depending on the DeepSurfel parameters it can approximate between simple colored voxels (high grid resolution, 1 × 1 patches) and textured meshes with high texture atlas resolutions (lower grid resolution, higher patch resolution). Our online appearance fusion pipeline it-eratively fuses RGB-D frames into estimated DeepSurfels geometry and is optimized by using a differentiable ren-derer for self-supervision and the reprojection error as train-14524
Figure 1. Overview of our online appearance fusion pipeline and the DeepSurfel scene representation. The Appearance Fusion network efﬁciently aggregates appearance information from a stream of camera views into the proposed DeepSurfel representation St−1 that maintains high-frequency geometric and appearance information. DeepSurfels is a sparse grid of 2D patches that consist of surface-aligned texels, which encode appearance information either as RGB color values or learned feature vectors. The proposed Appearance
Rendering network interprets aggregated and interpolated geometric and appearance information stored in DeepSurfels for rendering novel viewpoints. In this example we used DeepSurfels with a sparse 643 patch grid with 8 × 8 resolution surfel patches. ing signal. In this way, the pipeline does not require any ground-truth texture maps and the training procedure allows for efﬁcient transfer to new sensors and scenes without the need for acquiring costly ground-truth data.
While we eventually target full online reconstruction of both geometry and texture from monocular video, we only focus on online appearance estimation in this paper. Even in a setting with known geometry, our online approach has scalability advantages: We can fuse arbitrary numbers of input frames and the grid-aligned surfels have performance advantages during feature aggregation across local neigh-bors and for controlling the sampling density. Our grid-aligned surfel patches can also be seen as a spatial align-ment of per-voxel sub-features being anchored along the surface. In contrast to works that only save a single fea-ture vector per voxel, e.g. DeepVoxels [72], we can directly relate sub-features with particular image pixels via projec-tive mapping and as such simplify the network learning task and improve output accuracy. As opposed to many novel view synthesis works [49, 72, 73], we do not overﬁt onto a single scene, but train a network that generalizes over multi-ple scenes without re-training. While those methods iterate many times over each input image in a slow optimization process, our method processes every image only once with a single network forward pass and is thus much faster. From the application point of view, our approach is thus closer to classical texture mapping methods like [18, 19, 23, 87].
We compare our novel scene representation and appear-ance fusion pipeline to existing methods on single and multi-object datasets and show that our scene representa-tion better captures high-frequency textures. Moreover, our method generalizes well and compares favorably even to ex-isting texture optimization methods that jointly optimize all images together. This is a crucial step towards a fully end-to-end appearance fusion method that can be deployed to real-world applications. In sum, our key contributions are:
• DeepSurfels. A novel scalable and memory-efﬁcient 3D scene representation closing the gap between traditional interpretable and modern learned representations.
• Online Appearance Fusion Pipeline. An end-to-end dif-ferentiable and efﬁcient online appearance fusion pipeline compatible with classical and learned texture mapping.
The method yields competitive texturing results without heavy optimization as every input frame is processed only once with a single network forward pass.
• Generalized Novel View Synthesis. Contrary to other learning-based methods [49, 72, 73] that overﬁt onto a single scene, our method generalizes to new scenes with-out retraining. 2.