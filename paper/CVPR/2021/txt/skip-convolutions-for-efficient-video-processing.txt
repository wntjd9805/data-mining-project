Abstract
We propose Skip-Convolutions to leverage the large amount of redundancies in video streams and save compu-tations. Each video is represented as a series of changes across frames and network activations, denoted as residu-als. We reformulate standard convolution to be efﬁciently computed on residual frames: each layer is coupled with a binary gate deciding whether a residual is important to the model prediction, e.g. foreground regions, or it can be safely skipped, e.g. background regions. These gates can ei-ther be implemented as an efﬁcient network trained jointly with convolution kernels, or can simply skip the residuals based on their magnitude. Gating functions can also in-corporate block-wise sparsity structures, as required for ef-ﬁcient implementation on hardware platforms. By replac-ing all convolutions with Skip-Convolutions in two state-of-the-art architectures, namely EfﬁcientDet and HRNet, we reduce their computational cost consistently by a factor of 3 ∼ 4× for two different tasks, without any accuracy drop.
Extensive comparisons with existing model compression, as well as image and video efﬁciency methods demonstrate that Skip-Convolutions set a new state-of-the-art by effec-tively exploiting the temporal redundancies in videos. 1.

Introduction
Is a video a sequence of still images or a continuous se-ries of changes? We see the world by sensing changes, and process information whenever the accumulated differences in our neurons exceed some threshold. This trait has in-spired many efforts to develop neuromorphic sensors and processing algorithms, such as event-based cameras [40] and spiking neural networks [12]. Despite their efﬁciency for video processing, spiking nets have not been as success-ful as conventional models, mostly due to the lack of ef-ﬁcient training algorithms. There have been several works on mapping spiking nets to conventional networks, but these works have been mostly limited to shallow architectures and
*Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc.
Figure 1: Skip-Convolution illustration for the input layer.
Convolutions are computed only on a few locations in the residual features determined by a gate function (blue dots).
In other locations, output features are copied from the pre-vious time step (yellow dots). Frames taken from [65]. simple problems, such as digit classiﬁcation [66, 36, 35].
Representing videos by changes through residual frames is common in video compression codecs, such as HEVC [46], because residual frames normally have less information en-tropy and therefore require fewer bits to be compressed.
For stream processing applications, that require spatially dense predictions for each input frame, deep convolutional networks still process a sequence of still images as input.
Each frame is processed entirely by sliding convolutional
ﬁlters all over the frame, layer by layer. As a result, the overall computational cost grows linearly with the number of input frames, even though there might be not much new information in the subsequent frames. This inherent inefﬁ-ciency prohibits using accurate but expensive networks for real-time tasks, such as object detection and pose estima-tion, on video streams.
This paper proposes Skip-Convolutions, in short, Skip-Convs, to speed up any convolutional network for infer-ence on video streams. Instead of considering a video as a sequence of still images, we represent it as a series of changes across frames and network activations, denoted as residual frames. We reformulate standard convolution to be efﬁciently computed over such residual frames by limiting the computation only to the regions with signiﬁcant changes while skipping the others. Each convolutional layer is cou-2695
pled with a gating function learned to distinguish between the residuals that are important for the model accuracy and background regions that can be safely ignored (Fig. 1).
By applying the convolution kernel on sparse locations,
Skip-Convs allow to adjust efﬁciency depending on the in-put, in line with recent studies on conditional computation in images [25, 7, 42, 51, 53]. However, we hereby ar-gue that distinguishing the important and non-important re-gions is more challenging in still images. Indeed, residual frames provide a strong prior on the relevant regions, eas-ing the design of effective gating functions. As a result,
Skip-Convs achieve a much higher cost reduction in videos (300 ∼ 400%), compared to what has been previously re-ported for images (15 ∼ 60% in [51], 27 ∼ 41% in [53]).
To summarize, the main contributions of this work are: i) a simple reformulation of convolution, which computes features on highly sparse residuals instead of dense video frames. ii, iii) Two gating functions, Norm gate and Gum-bel gate, to effectively decide whether to process or skip each location. Norm gates do not have any trainable pa-rameter, thus can be easily plugged into any trained net-work obviating the need for further ﬁne-tuning. On the con-trary, Gumbel gates are trainable: they are learned jointly with the backbone model with the Gumbel reparametriza-tion [20, 32], and allow to achieve even more efﬁciency.
We extend these gates to generate structured sparsity as re-iv) A gen-quired for efﬁcient hardware implementations. eral formulation of Skip-Conv, which extends the idea to a broader range of transformations and operations. v) ex-tensive experiments on two different tasks and state-of-the-art network architectures, showing a consistent reduction in cost by a factor of 3 ∼ 4×, without any accuracy drop. 2.