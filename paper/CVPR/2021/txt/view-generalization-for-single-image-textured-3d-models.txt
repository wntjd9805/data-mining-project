Abstract
Humans can easily infer the underlying 3D geometry and texture of an object only from a single 2D image. Current computer vision methods can do this, too, but suffer from view generalization problems – the models inferred tend to make poor predictions of appearance in novel views. As for generalization problems in machine learning, the difﬁ-culty is balancing single-view accuracy (cf. training error; bias) with novel view accuracy (cf. test error; variance).
We describe a class of models whose geometric rigidity is easily controlled to manage this tradeoff. We describe a cycle consistency loss that improves view generalization (roughly, a model from a generated view should predict the original view well). View generalization of textures requires that models share texture information, so a car seen from the back still has headlights because other cars have head-lights. We describe a cycle consistency loss that encourages
∗work done while interning with NVIDIA model textures to be aligned, so as to encourage sharing.
We compare our method against the state-of-the-art method and show both qualitative and quantitative improvements. 1.

Introduction
People can easily infer the 3D geometry and texture of an object from a single 2D image. Likely, we can reason about the shape and the appearance of an object seen from only a single view because we have seen many objects from many views. Experience enables us to infer a reasonable estimate of shape and appearance of a novel object from a familiar category, even though there is within category vari-ation. Current algorithms can recover 3D shape and texture from a single image, but suffer from severe view general-ization problems: the recovered representation, viewed in a new camera, tends to look unrealistic. Either the inferred 3D mesh is squashed when viewed from other viewpoints
[16, 4] or the inferred texture are muddled [16], and also not sharp enough with missing details [4]. For a faithful re-16081
Image Input
DOD = 4
DOD = 16
DOD = 64
Increasing Deformation
DOD = 256
DOD = 1024
Figure 2: Controllable Deformation. Our approach is built upon a 2D convolutional deformation where one can easy control the extent of deformation by changing degrees of deformation (DOD) or spatial resolution of our predicted 2D deformation map. A higher DOD generally yields a more ﬂexible model (eg the beak of the gull), with better ﬁt to the source view but the possibility of view generalization problems (eg gull’s eye in 1024). We report results for a ﬁxed DOD, but note that speciﬁc instances might perform better at different DOD’s – an artist might choose the best DOD for a particular source image. covery of 3D models, current state-of-the-art (SOTA) meth-ods rely on a ﬁnely curated multi-view datasets or use of synthetic images [3, 6] which do not translate well to real image domains. Therefore, in this work, we focus on in-ferring realistic textured 3D models – meshes and texture maps; from a collection of single-view real images and by using only available 2D supervision. We not only synthe-size superior results from the original image view but also our synthesis generalizes across views with a coherent ge-ometry and convincing texture prediction (Fig.1).
Good view generalization requires a geometric deforma-tion model that can balance bias (source view) and variance (novel views). A highly ﬂexible model may ﬁt the original image well, but look bad from other directions; too rigid a model may generalize moderately well, but not ﬁt the origi-nal image. Furthermore, good view generalization requires texture models that can share details across instances as ap-propriate: the way to know that this car has (say) headlights, even though they are not visible in the original view, is to know that all cars have headlights, and borrow headlights from some other model.
Our geometric model uses a controllable convolutional deformation approach that predicts a single UV deforma-tion map over a spherical surface. This allows us to control the extent of deformation by varying the predicted spatial map resolution. The convolutional deformation is previ-ously proposed by [32], here we use it for controllability by changing the degrees of deformation 3( Fig. 2). Our texture model is, again, on a single UV map, allowing us to use cy-cle consistency losses to share texture features. Our geom-etry and texture models are controlled with two consistency losses: i) Rotation GAN cycle consistency to deal with de-formation from canonical views, encouraging their appear-ance and shape to be realistic as the original view images. ii) Texture mesh alignment consistency loss that shares ap-pearance information across images providing strong spa-tial cues for inferring occluded texture.
Our experiments show that our framework built upon these philosophies and principles strongly improves the overall ﬁdelity of synthesized textured 3D models (Fig. 1).
We show that our framework allows us to control the ﬂexi-bility of our deformation that is speciﬁcally useful for de-forming non-rigid object categories like birds where dif-ferent images undergo different degrees of deformation (Fig. 2). We also show that leveraging a few handfuls of multiple mesh templates instead of a single mesh template can improve overall recovered 3D inference quality without requiring the need of ﬁnely curated multi-view dataset.
Current SOTA methods are evaluated on intersection over union (IoU) compared to the ground truth mask from the original view. But a good mIoU from the original view may generate meshes that look poor from other views. The evaluation misses the point of 3D models - they are intended to predict appearance from new viewpoints. We evaluate mIoU for comparison, but evaluate view generalization by comparing multiple renderings from novel viewpoints with real images of real objects (using Fr´echet Inception Dis-tance (FID) scores [12]). We also conduct user studies com-paring our inferred 3D models with the baseline methods.
In summary, our main contributions are:
• A controllable convolutional deformation approach for better recovery of 3D geometry of the objects.
• Two novel cycle consistency losses that strongly im-proves overall inferred textured 3D models.
• High-ﬁdelity textured 3D model synthesis from a single image both qualitatively and quantitatively. 26082
2.