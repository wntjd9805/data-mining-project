Abstract
Invertible networks have various beneﬁts for image de-noising since they are lightweight, information-lossless, and memory-saving during back-propagation. However, apply-ing invertible models to remove noise is challenging be-cause the input is noisy, and the reversed output is clean, following two different distributions. We propose an invert-ible denoising network, InvDN, to address this challenge.
InvDN transforms the noisy input into a low-resolution clean image and a latent representation containing noise.
To discard noise and restore the clean image, InvDN re-places the noisy latent representation with another one sam-pled from a prior distribution during reversion. The de-noising performance of InvDN is better than all the existing competitive models, achieving a new state-of-the-art result for the SIDD dataset while enjoying less run time. More-over, the size of InvDN is far smaller, only having 4.2% of the number of parameters compared to the most recently proposed DANet. Further, via manipulating the noisy la-tent representation, InvDN is also able to generate noise more similar to the original one. Our code is available at: https://github.com/Yang-Liu1082/InvDN.git. 1.

Introduction
Image denoising aims to restore clean images from noisy observations. Traditional approaches model denoising as a maximum a posteriori (MAP) optimization problem, with assumptions on the distribution of noise [38, 57, 12], and natural image priors [16, 39, 47]. Although these algo-rithms achieve satisfactory performance on removing syn-thetic noise, their effectiveness on real-world noise is com-promised since their assumptions deviate from those in real-world scenarios. Recently, convolutional neural net-works (CNNs) have achieved superior denoising perfor-mance [53, 54]. These CNNs learn the features of images from a large number of clean and noisy image pairs. How-ever, since real noise is very complex, to achieve better de-noising accuracy, CNN denoising models have become in-∗Corresponding authors: Zhenyue Qin (zhenyue.qin@anu.edu.au) and
Yang Liu (yang.liu3@anu.edu.au).
Noisy
CBDNet [24]
RIDNet [5]
VDN [50]
DANet [51]
InvDN (Ours)
Figure 1: A real noisy image from the SIDD [2] dataset.
Compared with RIDNet [5] and DANet [51], InvDN does not over-smooth.
In addition, in comparison with all the other methods, InvDN restores more crisp edges and pro-duces fewer artifacts. The examples are best viewed in color on a high-resolution display with zooming in. creasingly large and complicated [50, 51, 52]. Thus, al-though some methods can achieve very impressive denois-ing results, they may not be practical in realistic scenarios such as deploying the model on edge equipment like smart-phones and motion sensing devices.
Currently, a substantial amount of research has been devoted to developing neural networks that are invert-ible [18, 41, 25, 9]. For image denoising, invertible net-works are advantageous from the following three aspects: (1) the model is light, as encoding and decoding use the same parameters; (2) they preserve details of the input data since invertible networks are information-lossless [36]; (3) they save memory during back-propagation because they use a constant amount of memory to compute gradients, re-gardless of the depth of the network [23]. Hence, invert-ible models are suitable for small devices like smartphones.
We thus study employing invertible networks to address the problem of image denoising. However, applying such net-works to remove noise is non-trivial. The original inputs 13365
and the reversed results of the traditional invertible models follow the same distribution [19, 31, 46]. In contrast, for image denoising, the input is noisy, and the restored im-age is clean, following two different distributions. There-fore, invertible denoising networks are required to abandon the noise in the latent space before the reversion. Due to this difﬁculty, noise removal has not previously been stud-ied and deployed in invertible literature and models.
In this paper, we propose an invertible denoising net-work, InvDN, to resolve the above difﬁculties. Unlike pre-vious invertible models, two different latent variables are involved; one incorporates noise and high-frequency clean contents while the other only encodes the clean part. Dur-ing the forward pass, InvDN transforms the input image to a downscaled latent representation with an increased num-ber of channels. We train InvDN to make the ﬁrst three channels of the latent representation the same as the low-resolution clean image. Since invertible networks preserve all the information of the input [36], noisy signals are in the rest of the channels. To remove noise completely, we dis-card all the channels that contain noise. However, as a side-effect, we also lose some information corresponding to the high-resolution clean image. To reconstruct such missing information, we sample a new latent variable from a prior distribution and combine it with the low-resolution image to restore the clean image.
Our contributions are as follows:
• We are the ﬁrst to design invertible networks for real image denoising to the best of our knowledge.
• The latent variable of traditional invertible networks follows a single distribution. Instead, InvDN has two latent variables following two different distributions.
Thus, InvDN can not only restore clean images but also generate new noisy images.
• We achieve a new state-of-the-art (SOTA) result on the
SIDD test set, using far fewer parameters and less run time than the previous SOTA methods.
• InvDN is able to generate new noisy images that are more similar to the original noisy ones. 2.