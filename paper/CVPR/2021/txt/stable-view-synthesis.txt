Abstract
We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and cor-responding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which direc-tional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features syn-thesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It sup-ports spatially-varying view-dependent importance weight-ing and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the tar-get view; and synthesis of view-dependent effects such as specular reﬂection. Experimental results demonstrate that
SVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes.
Code is available at https://github.com/intel-isl/StableViewSynthesis 12216
1.

Introduction
Photorealistic view synthesis can allow us to explore magniﬁcent sites in faraway lands without leaving the com-fort of our homes. This requires advancing the technology towards two key goals. First, the synthesized images should be photorealistic: indistinguishable from reality. Second, the user should be free to move through the scene, as in the real world, exploring it from any physically realizable viewpoint.
In this paper, we present a new method for photorealistic view synthesis that brings these two goals closer. Our in-put is a set of images that can be taken for example from a handheld video of the scene. From these images, we con-struct a 3D geometric scaffold via off-the-shelf structure-from-motion, multi-view stereo, and meshing. Input images are encoded by a convolutional network and the resulting deep features are mapped onto the geometric scaffold. As a result, for any point on the scaffold, we can obtain a col-lection of view rays with associated feature vectors, which correspond to input images that see this point.
The core of our method is an approach to synthesizing arbitrary new views given this representation of the scene.
Each pixel in the new view is mapped onto the geometric scaffold to obtain the set of input rays with associated fea-ture vectors, and an output ray towards the new view. The feature vectors from the input rays are then aggregated, tak-ing the geometry of the input and output rays into account, by a differentiable module that produces a feature vector for the output ray. Together, the feature vectors synthesized for all pixels form a feature tensor. The new image is rendered from this feature tensor by a convolutional network.
All steps of the method are differentiable and the com-plete pipeline can be trained end-to-end to maximize pho-torealism. All steps can be implemented efﬁciently, lever-aging parallelism across pixels. Crucially, the computation of a feature vector for a new output ray does not require any heuristic selection of input rays. The computation ag-gregates information from all input rays in a differentiable module that is informed by the spatial layout of the rays and is optimized end-to-end. This supports temporal stabil-ity for smoothly moving viewpoints.
We evaluate the presented method on three diverse datasets of real scenes and objects: Tanks and Temples [18],
FVS [29], and DTU [1]. Tanks and Temples and FVS pro-vide handheld video sequences of large real-world scenes; the objective is to use these video sequences as input to en-able photorealistic rendering of the scenes from new views.
DTU provides regularly-spaced outside-in images of chal-lenging real objects. On all three datasets, SVS convinc-ingly outperforms the state of the art. On Tanks and Tem-ples, our method reduces the LPIPS error for new views by up to 10 absolute percentage points (a reduction of roughly 30% on average) relative to the prior state of the art, while also improving PSNR and SSIM. On the FVS dataset, our method likewise outperforms the state of the art on all met-rics, reducing LPIPS by 7 absolute percentage points on av-erage relative to the best prior method. On DTU, we set the new state of the art for novel view synthesis, attaining an average LPIPS error of 4.5% over the test scenes in extrap-olation mode and 1.6% for view interpolation. A number of our synthesized images for new views in Tanks and Temples and FVS scenes are shown in Figure 1, and video sequences are provided in the supplementary video. 2.