Abstract
We propose a semi-supervised approach for contempo-rary object detectors following the teacher-student dual model framework. Our method 1 is featured with 1) the ex-ponential moving averaging strategy to update the teacher from the student online, 2) using plenty of region proposals and soft pseudo-labels as the student’s training targets, and 3) a light-weighted detection-speciﬁc data ensemble for the teacher to generate more reliable pseudo-labels. Compared to the recent state-of-the-art – STAC, which uses hard la-bels on sparsely selected hard pseudo samples, the teacher in our model exposes richer information to the student with soft-labels on many proposals. Our model achieves COCO-style AP of 53.04% on VOC07 val set, 8.4% better than
STAC, when using VOC12 as unlabeled data. On MS-COCO, it outperforms prior work when only a small per-centage of data is taken as labeled. It also reaches 53.8%
AP on MS-COCO test-dev with 3.1% gain over the fully supervised ResNet-152 Cascaded R-CNN, by tapping into unlabeled data of a similar size to the labeled data. 1.

Introduction
We address the problem of semi-supervised object de-tection in this paper. Large curated datasets have driven the recent progress in vision tasks like image classiﬁcation, but data remain scarce for object detection [14, 31, 26, 5, 22, 30]. MS-COCO [25], for example, offers 118,287 anno-tated images, a relatively small fraction compared to over 14 million labeled images in ILSVRC [35]. Annotation ac-quisition for detection is also much more costly.
Much effort has been made to solve the semi-supervised learning problem for image classiﬁcation, where an object always exists and dominates the image. Not all progress for image classiﬁcation can beneﬁt the detection task signiﬁ-cantly as the existence and locations of objects are unknown without bounding box annotations. For example, a di-rect application of classiﬁcation-based pretraining [15, 7] is
*Work conducted during internship at Amazon Web Services. 1Project page: http://yihet.com/humble-teacher
Figure 1: Comparing CSD [19], STAC [40], and our ap-proach trained on full MS-COCO train 2017 with 1%, 2%, 5%, and 10% labeled over ﬁve runs using the splits in
Sec. 4.1. Our approach consistently outperforms others. shown to be not so effective in our experiments (Sec. 4.4.2).
In this work, we propose a teacher-student approach called Humble Teacher, which ﬁts modern object detec-tion frameworks better. The line of work on teacher-student models has many variants, including self-training [37, 48, 32, 39, 47], the exponential moving average (EMA) based mean teacher [44], and various ways to obtain pseudo-labels and different views of data for consistency regularization
[49, 21, 36, 39] between the teacher and student. Recently,
Sohn et al. [40] proposed a Self-Training method based on an Augmentation driven Consistency regularization (STAC) via hard pseudo-labels. It adopted FixMatch [39], one of the most successful recent methods for semi-supervised image classiﬁcation, directly to the classiﬁcation head of the Faster
R-CNN [31] detector, yielding improved semi-supervised detection results.
Our method further advances the semi-supervised ob-ject detection for Faster-R-CNN-like models in a few as-pects. Unlike self-training with a ﬁxed teacher model, our method updates the teacher model dynamically using EMA updates for object detectors. The teacher and student model use asymmetric data augmentation – stronger augmenta-tions for the student [46, 3, 39, 40] – to process different views of the same image [38]. In this framework, the key to our model’s strong performance is to use soft pseudo-3132
labels on a reasonable number of region proposals, striking a good balance between covering the entire image and fo-cusing more on learning useful foreground instances. It al-lows the student to distill much richer information from the teacher, compared to sparsely hard-selected high-conﬁdent pseudo ground truths in the existing work [40]. The use of soft-labels also keeps the model from over-ﬁtting to the teacher model’s potential missing and wrong predictions, which can occur often when using a hard decision thresh-old. In addition, we ensemble the teacher model under a light-weighted detection-speciﬁc data augmentation to ob-tain more reliable pseudo-labels. Through our study, we
ﬁnd the wisdom from FixMatch and STAC – hard pseudo-labels with sample selection – is not as effective. As our method avoids hard training signals, looks at abundant box instances, seeks for multi-teacher consensus, and uses run-ning average weights as in the mean teacher, we name our method a Humble Teacher.
The humble teacher signiﬁcantly closes the gap between semi-supervised learning and their fully supervised coun-terpart on VOC. It signiﬁcantly outperforms the state-of-the-art STAC [40] on MS-COCO (Fig. 1) and VOC by large margins. It also improves the ResNet-152 Cascade R-CNN [5] supervised on MS-COCO train signiﬁcantly with the additional similar-size unlabeled data.
In summary, we propose the humble teacher for semi-supervised object detection.
It outperforms the previous state-of-the-art in both low-data and high-data regimes. Its use of soft-labels are pivotal to enable learning with abun-dant proposals and also make the EMA and teacher ensem-ble more effective for detection. 2.