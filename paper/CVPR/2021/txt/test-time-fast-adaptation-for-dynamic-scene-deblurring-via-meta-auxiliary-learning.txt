Abstract
In this paper, we tackle the problem of dynamic scene deblurring. Most existing deep end-to-end learning ap-proaches adopt the same generic model for all unseen test images. These solutions are sub-optimal, as they fail to utilize the internal information within a speciﬁc image.
On the other hand, a self-supervised approach, SelfDe-blur, enables internal training within a test image from scratch, but it does not fully take advantage of large ex-ternal datasets.
In this work, we propose a novel self-supervised meta-auxiliary learning to improve the perfor-mance of deblurring by integrating both external and inter-nal learning. Concretely, we build a self-supervised auxil-iary reconstruction task that shares a portion of the network with the primary deblurring task. The two tasks are jointly trained on an external dataset. Furthermore, we propose a meta-auxiliary training scheme to further optimize the pre-trained model as a base learner, which is applicable for fast adaptation at test time. During training, the performance of both tasks is coupled. Therefore, we are able to exploit the internal information at test time via the auxiliary task to en-hance the performance of deblurring. Extensive experimen-tal results across evaluation datasets demonstrate the effec-tiveness of test-time adaptation of the proposed method. 1.

Introduction
Images taken in dynamic scenes are often degraded by objectionable blur caused by object motions and camera shake. Restoring latent clean images from such cases is challenging due to the spatially non-uniform property of the blur kernels. Despite its highly ill-posed characteristic, ex-tensive research efforts have been devoted to remove the no-torious blurry artifacts in the past decades [7, 6, 44, 46, 8, 9].
Recently, deep neural networks (DNNs) have been popular in this ﬁeld. Various network structures have been proposed to achieve reliable quantitative results and generate visually pleasing clean images for dynamic scene deblurring [37, 1, (a) Input blurry image (b) SelfDeblur [29] (2,500 updates) (c) Meta-auxiliary learned (ours, no updates) (d) Meta-auxiliary learned (ours, ﬁve updates)
Figure 1: Sample deblurring results. Given a blurry input image (a), SelfDeblur [29] requires thousands of iterations to learn the internal information of the input image (b). Our approach uses meta-auxiliary learning to learn to adapt to the unique properties of the given image. Before adaption, the output has some artifacts due to the distribution shift be-tween the training dataset and the test image (c). After ﬁve updates, our model quickly adapts to the internal informa-tion of the test image and removes the artifacts (d). 31, 24]. In particular, the end-to-end learning approaches have set the state-of-the-art by directly learning the statisti-cal correlation between blurry and latent images on large-scale training datasets [40, 13, 27, 36, 47, 25, 23, 5, 2].
The main shortcoming for most of the existing DNN-9137
based methods is that the same set of trained weights are adopted for all unseen test images. However, the features learned from the external training data may not be opti-mal for the given test image [26]. The failure of exploit-ing the unique internal properties, such as depth variations and motion trajectories, leads to non-optimal solutions [29].
Therefore, the generalization highly depends on the distri-bution of training data and will likely deteriorate under dis-tribution shift. To overcome this, SelfDeblur [29] explicitly captures the internal statistics of the given test image in a self-supervised manner. However, this method has some limitations. First, it assumes the blur is spatially uniform, which does not apply in dynamic scenes. Second, it fails to utilize broad external information.
Our proposed approach combines the ideas from two dif-ferent machine learning paradigms, namely meta-learning (also known as learning to learn) and auxiliary-learning.
Meta-learning enables fast adaptation at test time via a few training examples [14, 42, 39, 28, 21, 3, 33, 20].
In particular, model agnostic meta-learning (MAML) [3] has been successfully adopted to other image restoration prob-lems, such as super-resolution (SR) [34, 26]. The methods in [34, 26] ﬁrst conduct large-scale training on external SR datasets. Then the meta-learning scheme further optimizes the pre-trained model so that it can quickly adapt to unseen images via internal learning. The key point is that the super-vision at test time can be simulated by further downscaling the low-resolution images. Thus, internal learning can be achieved to explore the speciﬁc patch-recurrence property.
However, for deblurring problem, such setting is impracti-cal as patch-recurrence diminishes across scales [19]. One could re-blur the blurry image and treat the original blurry image as the clean counterpart. However, the re-blurring process requires accurate blur kernel estimation, which is challenging. Moreover, it may break the mapping between blurry image and latent clean image. So the application of
MAML to the deblurring problem is not as straightforward.
Another practicable approach is to introduce auxiliary-learning by deﬁning an auxiliary task alongside the primary deblurring task [16, 41, 50, 10]. These two tasks can share some parameters. The auxiliary task is often designed in a self-supervised way so that the auxiliary loss can be used to update the model weights at test time. Ideally, the up-dated shared weights can also improve the performance of the primary task [38]. However, we empirically observe that naively updating the model via the auxiliary task on the pre-trained model can lead to catastrophic forgetting [18] where the performance of the primary deblurring task drops.
We propose to integrate meta-learning and auxiliary-learning to exploit their respective strength for the deblur-Inspired by [17], we propose to use self-ring problem. supervised image reconstruction as the auxiliary task. Its loss can be deﬁned at test time as it does not require any manual labeling. During meta-training, we have access to a labeled dataset consisting of pairs of blurring images and their ground-truth clean counterparts. We consider each im-age pair as a “task” using the meta-learning terminology [3].
For each task, we update the model parameters using the auxiliary loss deﬁned on the blurry image. The performance of the updated parameters is measured by the deblurring quality via the primary loss. The goal of meta-learning is to learn the model parameters so that the deblurring output us-ing the updated parameters better matches the ground-truth clean image. Note that for different input images, the cor-responding updated parameters will be different. In other words, our model is adapted to each input image to better capture its internal information. See Fig. 1 for a qualitative example of our method compared with other alternatives.
The contributions of this paper are manifold. First, we propose to use self-reconstruction as an auxiliary task for the primary deblurring task. The jointly trained model al-ready outperforms existing state-of-the-art. Second, we in-troduce novel meta-auxiliary learning to enable effective and fast test-time model adaptation. During testing, the model is updated using the self-supervised auxiliary task, which does not require extra labels. Third, our model is learned in a way that facilitates fast adaption with only a few gradient updates during testing. To the best of our knowledge, this is the ﬁrst attempt to apply meta-auxiliary learning to low-level computer vision problems. Unlike
SR [34, 26] using meta-learning, our approach does not re-quire surrogate training pairs during testing. Although we focus on dynamic scene deblurring in this paper, our method can potentially be applied in other image restorations where surrogate training pairs cannot be obtained at test time. 2.