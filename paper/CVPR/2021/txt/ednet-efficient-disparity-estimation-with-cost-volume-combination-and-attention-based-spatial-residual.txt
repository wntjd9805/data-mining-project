Abstract
Existing state-of-the-art disparity estimation works mostly leverage the 4D concatenation volume and construct a very deep 3D convolution neural network (CNN) for dis-parity regression, which is inefﬁcient due to the high mem-ory consumption and slow inference speed. In this paper, we propose a network named EDNet for efﬁcient disparity estimation. Firstly, we construct a combined volume which incorporates contextual information from the squeezed con-catenation volume and feature similarity measurement from the correlation volume. The combined volume can be next aggregated by 2D convolutions which are faster and re-quire less memory than 3D convolutions. Secondly, we pro-pose an attention-based spatial residual module to generate attention-aware residual features. The attention mechanism is applied to provide intuitive spatial evidence about inac-curate regions with the help of error maps at multiple scales and thus improve the residual learning efﬁciency. Extensive experiments on the Scene Flow and KITTI datasets show that EDNet outperforms the previous 3D CNN based works and achieves state-of-the-art performance with signiﬁcantly faster speed and less memory consumption. 1.

Introduction
Accurate and fast depth estimation is of great signiﬁ-cance to many applications like robot navigation, 3D recon-struction and autonomous driving. Instead of depth regres-sion from a single-view RGB image, stereo matching is to conduct correspondence analysis between pixels of stereo images and compute the disparity d for each pixel. Depth can be then calculated by ( f B d ), where f is the camera’s focal length and B is the distance between two camera cen-ters, also called baseline in stereo vision.
*Corresponding author.
While traditional methods based on hand-crafted fea-ture extraction and matching cost aggregation tend to fail on those textureless and repetitive regions in the images, convolutional neural networks (CNNs) have been widely adopted to conquer those difﬁculties in stereo matching.
Several recent methods [19, 3, 13, 42] have achieved state-of-the-art performance by constructing a 4D concatenation volume which follows 3D convolution blocks for aggrega-tion. Although the 4D concatenation cost volume can pre-serve the rich contextual information in conjunction with the strong regularization ability of 3D convolutions, it sig-niﬁcantly increases the computation cost and usually cannot perform real-time disparity inference. Moreover, the con-catenation volume incorporates no feature similarity mea-surement, which means that the model has to learn corre-spondence from scratch. Besides, DispNetC [22] formed a low-cost correlation layer with 2D convolutions to conduct correspondence analysis between the left and right feature maps. The following works [21, 38, 26] adopted the similar method as it keeps a good balance between speed and accu-racy. However, as the correlation map is produced with only one single feature channel for each disparity level, the per-formance is less competitive. Thus, this raises the question of how to make full use of the complementary advantages of the concatenation volume and correlation volume.
Since ResNet [15] has revealed that the residual convo-lution block can improve the training efﬁciency by learning a residual mapping instead of the desired underlying one, it has been adopted as a popular approach to reﬁne the dis-parity estimation [34, 31, 21, 37]. To be speciﬁc in stereo matching, learning an additive correction to the coarse dis-parity map is easier and more efﬁcient than directly learning the ﬁne-grained one. However, some works failed to pro-vide the residual learning module with the ﬁtting error in-formation [33, 40], or computed the estimated error at only one scale [34, 26, 17] but adopted it to learn the disparity maps at multiple scales. The error map from one single 5433
Figure 1: The ﬁrst row is the visualization of residual learning process from scale 3 to scale 2. The residual scale2 is learned from the disparity scale3 for correcting the disparity scale2. With our proposed modules, sharp edges and overall structures can be recovered. Other state-of-the-art methods fail to generate the accurate disparity estimation in low-texture regions as shown in the second row. Please pay more attention to regions pointed by the red arrow. scale cannot provide the precise error information at other scales, which makes the residual learning method less effec-tive. Furthermore, even if the error map is provided at each corresponding scale [31], the conventional residual learn-ing method has no explicit spatial guidance about where to intervene. As the regions with inaccurate estimation de-serve more attention, we argue that residual learning could be more efﬁcient if the spatial attention about the learning errors is provided.
To address the above issues, we propose EDNet which is composed of a combined volume to generate robust fea-ture representations, and an attention-based residual mod-ule to learn the disparity reﬁnement. Firstly, the proposed combined volume alleviates the information loss by em-ploying the squeezed concatenation volume and preserves the feature similarity measurement with the correlation vol-ume. We then adopt 2D convolutions for further aggrega-tion so that the signiﬁcant memory consumption and com-putational complexity of 3D CNNs can be avoided. Sec-ondly, inspired by the attention mechanism, we adopt a spa-tial attention module to generate the attention-aware resid-ual features. Therefore, the residual learning module can have intuitive spatial evidence about inaccurate regions to compute a speciﬁc correction. We follow the coarse-to-ﬁne strategy and compute the attention-aware residuals across different scales. With the error maps provided at each scale, the residual module can learn a corresponding correction accordingly and improve the learning efﬁciency. As shown in Figure 1, our network can generate an accurate and con-tinuous disparity map even in low-texture regions. The con-tributions of our work can be summarized as follows:
• We propose a low-cost but effective method to aggre-gate the 3D correlation features and 4D concatenation volume together by constructing a combined volume, which can be further processed by fast 2D convolu-tions. Compared with others, our correspondence anal-ysis can preserve both the contextual information and feature similarities even with 2D convolutions.
• We design an Attention-based Residual (AR) module to learn the disparity reﬁnement at each scale. In the
AR module, the attention mechanism is applied to the concatenated maps of RGB image, estimated disparity and estimated error to improve the learning efﬁciency.
• Compared to those existing methods based on 3D
CNNs, our proposed EDNet achieves state-of-the-art accuracy on the public Scene Flow [22] and KITTI
[24, 10] datasets with less than 25% running memory requirement compared with Bi3D [1] and is 45× faster than GANet [42].
The rest of the paper is organized as follows. We intro-duce some related studies about stereo matching based on
CNNs in Section 2. Section 3 introduces the methodology and implementation of our proposed EDNet. We demon-strate our experimental results in Section 4. We ﬁnally con-clude the paper in Section 5. 2.