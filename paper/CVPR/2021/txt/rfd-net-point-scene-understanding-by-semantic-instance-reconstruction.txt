Abstract
Semantic scene understanding from point clouds is par-ticularly challenging as the points reﬂect only a sparse set of the underlying 3D geometry. Previous works often con-vert point cloud into regular grids (e.g. voxels or bird-eye view images), and resort to grid-based convolutions for scene understanding. In this work, we introduce RfD-Net that jointly detects and reconstructs dense object surfaces directly from raw point clouds.
Instead of representing scenes with regular grids, our method leverages the spar-sity of point cloud data and focuses on predicting shapes that are recognized with high objectness. With this de-sign, we decouple the instance reconstruction into global object localization and local shape prediction. It not only eases the difﬁculty of learning 2-D manifold surfaces from sparse 3D space, the point clouds in each object proposal convey shape details that support implicit function learn-ing to reconstruct any high-resolution surfaces. Our exper-iments indicate that instance detection and reconstruction present complementary effects, where the shape prediction head shows consistent effects on improving object detection with modern 3D proposal network backbones. The quali-tative and quantitative evaluations further demonstrate that our approach consistently outperforms the state-of-the-arts and improves over 11 of mesh IoU in object reconstruction. 1.

Introduction
Semantic scene reconstruction has received increasing attention in applications such as robot navigation and inte-∗ Corresponding Email: hanxiaoguang@cuhk.edu.cn rior design. It focuses on recovering the object labels, poses and geometries of objects in a 3D scene from partial ob-servations (e.g. images or 3D scans). With the advance of 2D CNNs, instance reconstruction from images achieves appealing results [20, 45, 36, 59, 33] but still bottlenecked by the depth ambiguity thus resulting in defective object lo-cations. Compared to images, point clouds provide surface geometry that largely alleviates the object locating issues
[49, 2, 65, 7, 31]. However, its inherent sparseness and ir-regularity challenge the direct usage of grid-based CNNs on point clouds for semantic instance reconstruction.
Scanning 3D scenes usually results in missing geome-tries due to occlusions, view constraints and weak illumina-tion where individual objects cannot be covered in all views.
Prior works have explored various strategies to recover the missing shapes, e.g. depth inpainting, voxel/TSDF predic-tion, and shape retrieval. Depth inpainting [23, 47, 28, 55] aims to complete the depth maps in single views. With 2D
CNNs, these methods achieve pleasing results in surface points recovery. To complete a scene with occluded con-tents, many methods extend the 2D CNNs to 3D and nat-urally represent 3D scenes with voxel/TSDF grids [57, 63, 37, 16, 13, 26, 43, 14]. This strategy also enables decoding voxel labels to complete scenes at the semantic or instance level. However, the expensive 3D convolutions in the scene level make them suffer from the resolution problem. Shape retrieval [3, 4, 5, 30] provides an alternative method to pre-dict shapes by searching for a CAD model as similar to the incomplete object as possible. However, the accuracy and the computation efﬁciency depends on the model dataset scale. Compared to voxels/TSDFs, point clouds present sparsity and are more scalable for efﬁcient learning. To 4608
our knowledge, few works have attempted to learn object meshes at the semantic-instance level directly from points.
In this work, we provide a reconstruction-from-detection framework, RfD-Net, for end-to-end semantic instance re-construction directly from raw point clouds (see Figure 1).
Our design is based on the insight that object detections pro-vide spatial alignment that enables better local shape recon-struction. On the other hand, object shapes in sparse point clouds indicate local geometry that should back improve 3D
It decouples the problem of scene reconstruc-detection. tion into global localization and local shape prediction. Our method embeds the shape reconstruction head with a 3D detector backbone. It leverages the sparsity of point clouds and focuses on predicting shapes that are detected with high objectness and ignores the free scene space. With this de-sign, our method allows implicit function learning to recon-struct surfaces with much higher resolution. In our exper-iments, we observe that joint shape reconstruction and 3D detection presents complementary effects. Deploying the shape prediction head is shown consistently effective on im-proving the modern point-based detectors, and vice versa, which makes our method consistently outperforms the prior arts in 3D detection and instance reconstruction. In sum-mary, we list our contributions as follows:
• We provide a novel learning modality for semantic in-stance reconstruction. To our knowledge, it is the ﬁrst learning method to predict instance semantics with ge-ometry directly from point clouds. While prior methods heavily rely on 3D CNNs to learn from voxelized scenes.
• We propose a new end-to-end architecture, namely RfD-Net, to learn object semantics and shapes from sparse point clouds.
It disentangles semantic instance recon-struction into global object localization and local shape prediction, which are bridged with a skip propagation module to facilitate joint learning. With this manner, our shape generator supports implicit learning, which directly overcomes the resolution bottleneck in the prior art [26].
• Joint learning object poses and shapes presents comple-mentary beneﬁts. It also shows consistent effects on mod-ern detection backbones, and makes our method achieve the state-of-the-art in instance detection&completion and improve over 11 of mesh IoU in object reconstruction. 2.