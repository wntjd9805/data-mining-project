Abstract
Search space
S a m p l e  
Super-net
Weight sharing has become a de facto standard in neu-ral architecture search because it enables the search to be done on commodity hardware. However, recent works have empirically shown a ranking disorder between the perfor-mance of stand-alone architectures and that of the corre-sponding shared-weight networks. This violates the main assumption of weight-sharing NAS algorithms, thus limit-ing their effectiveness. We tackle this issue by proposing a regularization term that aims to maximize the correla-tion between the performance rankings of the shared-weight network and that of the standalone architectures using a small set of landmark architectures. We incorporate our regularization term into three different NAS algorithms and show that it consistently improves performance across al-gorithms, search-spaces, and tasks. 1.

Introduction
Modern algorithms for neural architecture search (NAS) can now ﬁnd architectures that outperform the human-designed ones for many computer vision tasks [21, 42, 7, 34]. A driving factor behind this progress was the intro-duction of parameter sharing [27], which reduces the search time from thousands of GPU hours to just a few and has thus become the backbone of most state-of-the-art NAS frame-works [3, 49, 5, 23]. At the heart of all these methods is a shared network, a.k.a. super-net, that encompasses all ar-chitectures within the search space.
To train the super-net, NAS algorithms essentially sam-ple individual architectures from the super-net and train
The sampling can be them for one or a few steps. done explicitly, with strategies such as reinforcement learn-ing [27, 6], evolutionary algorithms [15, 39], or random sampling [52, 18], or implicitly, by relying on a differen-tiable parameterization of the architecture space [19, 22, 5,
∗This work was partially done during an internship at Intel, and sup-ported in part by the Swiss National Science Foundation.
Super-net
Rank
Stand-alone
Rank
Evaluate 
U p d a t e    
Poor correlation
NAS  algorithm
S a m p le
Regularize
>
>
Landmark architectures
Stand-alone Rank 
Super-net
NAS  algorithm
Evaluate  a t e    d p
U
Better correlation
Figure 1: Traditional super-net training leads to poor correlation between relative stand-alone performance and super-net perfor-mance (top). We sample landmark architectures and use their rel-ative performance to guide training towards an improved ranking and show that this improves the search performance (bottom). 42, 53]. Whether explicit or implicit, the underlying as-sumption of these methods is that the relative performance of the individual architectures in the super-net is highly cor-related with the performance of the same architectures when they are trained in a stand-alone fashion. If this were the case, one could then safely choose the best individual ar-chitecture from the super-net after the search and use it for evaluation. However, this assumption was disproved in [52, 54], who showed a correlation close to zero be-tween the two rankings on complex search spaces [47, 22].
The major reason behind this is fairly intuitive: To be opti-mal, different individual architectures should have different parameter values, which they cannot because the parame-ters are shared. Super-net training will thus not produce the same results as stand-alone training. More importantly, there is no guarantee that even the relative ranking of the architectures will be maintained. While for simple, linear search spaces the ranking can be improved by using a care-fully crafted sampling strategy [8, 9], addressing the rank-ing disorder for more realistic, complex search spaces re-mains an open problem [50].
In this paper, we propose to explicitly encourage archi-13723  
tectures represented by the super-net to have a similar rank-ing to their counterparts trained in a stand-alone fashion. As illustrated by Figure 1, we leverage a set of landmark archi-tectures, that is, architectures with known stand-alone per-formance, to deﬁne a regularization term that guides super-net training towards this goal. We show that a small set of landmark architectures sufﬁces to signiﬁcantly improve the global ranking correlation, so that the overall search pro-cedure, including the independent training of the landmark architectures, remains tractable.
Our regularization term is general and does not make as-sumptions about the speciﬁc sampling algorithm used for super-net training. As such, it can easily be combined with many popular weight-sharing NAS algorithms. We demon-strate this by integrating it into three different algorithms
[15, 24, 11] that are representative of three different cat-egories of weight-sharing NAS algorithms: i) Algorithms that sample architectures from the super-net in an unbiased manner throughout the super-net training [18, 52, 2, 15, 8]; ii) approaches that employ learning-based samplers, which are updated during the training based on the performance of the partially-trained super-net [27, 19, 24, 40, 55]; and iii) algorithms that rely on differentiable architecture search [22, 6, 44, 25, 45].
Our extensive experiments on CIFAR-10 and ImageNet landmark regularization signiﬁcantly reduces show that the ranking disorder that occurs in these algorithms and that they are consequently able to consistently ﬁnd better-performing architectures. To further showcase the effec-tiveness and generality of our approach, we study its use in the context of architecture search for monocular depth estimation. To the best of our knowledge, this is the ﬁrst at-tempt at performing NAS for this task. We, therefore, con-struct a dedicated search space and show that a landmark-regularized NAS algorithm can ﬁnd novel architectures that improve upon the state of the art in this ﬁeld. 2.