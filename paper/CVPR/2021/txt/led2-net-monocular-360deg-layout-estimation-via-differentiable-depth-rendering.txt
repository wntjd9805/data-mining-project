Abstract
Although signiﬁcant progress has been made in room layout estimation, most methods aim to reduce the loss in the 2D pixel coordinate rather than exploiting the room structure in the 3D space. Towards reconstructing the room layout in 3D, we formulate the task of 360◦ layout estima-tion as a problem of predicting depth on the horizon line of a panorama. Speciﬁcally, we propose the Differentiable
Depth Rendering procedure to make the conversion from layout to depth prediction differentiable, thus making our proposed model end-to-end trainable while leveraging the 3D geometric information, without the need of providing the ground truth depth. Our method achieves state-of-the-art performance on numerous 360◦ layout benchmark datasets.
Moreover, our formulation enables a pre-training step on the depth dataset, which further improves the generalizabil-ity of our layout estimation model. 1.

Introduction
Inferring the geometric structure such as depth, layout, etc. from a single image has been studied for years. With the advance of deep learning, convolutional neural networks are widely used in these tasks. In addition, with the increas-ing popularity of consumer-level 360◦ cameras, approaches dealing with 360◦ panoramas start to play a crucial role in virtual and augmented reality (VR/AR) and robotic vision.
In order to support the indoor use case of these applica-tions, the task of room layout estimation from a single 360◦ panorama becomes important.
Generally, the room layout can be constructed by con-necting the adjacent room corners or directly ﬁnding the 1National Tsing Hua University 2National Chiao Tung University 3NEC Labs America 4
MOST Joint Research Center for AI Technology and All Vista Healthcare
∗The authors contribute equally to this paper.
Figure 1. Our LED2-Net takes the (a) single panorama as input and infers the (c) 3D room layout. We propose the (b) Differentiable
Depth Rendering technique to incorporate the geometry-aware in-formation into our model. boundary between walls, ﬂoor, and ceiling. Hence, most methods directly estimate the layout boundary and corners from the input panorama, e.g., HorizonNet [18]. Despite signiﬁcant progress being achieved, the 3D reconstruction of the room layout is often not as good as expected from observing the results overlaid on the 2D panorama. The main issue is that these methods are trained with the loss in the pixel coordinates of the 2D panorama rather than in the coordinate of the 3D reconstruction. In particular, 2D pixel loss disregards the fact that pixels with different depths from the camera should contribute differently to the loss in the 3D coordinate (see Figure 2). Additional losses such as binary segmentation loss in the ceiling and ﬂoor perspec-tive views have been introduced [24]. However, segmen-tation loss tends to focus on the correctness of the major-ity of the segment rather than the boundary of the segment.
On the other hand, although several progresses have been made for monocular 360◦ depth estimation given a single 2D panorama [11, 25, 20, 28] where the loss is deﬁned to reduce errors in 3D, none of the existing works aims at ap-plying depth-based constraints to layout estimation frame-12956
Figure 2. For the panorama shown on the left, we visualize sev-eral corners/boundary points where the layout estimation methods generally aim to ﬁnd, in which their objective is mostly based on the errors in the 2D pixel coordinate on the equirectangular im-age (e.g., two arrows indicate the same error). However, as illus-trated in the ceiling perspective view on the right, these two corner errors actually associate with different depth values (denoted as black arrows) from the camera, and such depth difference cannot be reﬂected in the intersection-over-union (IoU) error metric. work. Hence, we are inspired to leverage the depth predic-tion loss to improve room layout estimation, which provides us the geometric information in the 3D space.
To this end, we re-formulate the 360◦ layout estimation into a unique 360◦ depth estimation problem. First, instead of trying to estimate the full depth map of the panorama, we only estimate the depth values on the horizon line of a panorama, which we call “horizon-depth” (see Figure 3), which is already sufﬁcient to recover the layout. To this end, we propose a differentiable L2D (Layout-to-Depth) proce-dure to transform the layout into a “layout-depth”. As a re-sult, we can adopt the widely-used objective functions in depth estimation to train our model on layout estimation datasets, which also enables the possibility to pre-train our model on depth estimation datasets and further improve the model generalizability.
Our proposed layout-to-depth procedure is based on ray-casting (i.e., casting the rays from a unit sphere as illus-trated in Figure 1(b)). The depth is recovered by computing the distances of each ray. Ideally, we can predict the depth for every pixel on the horizon line, but it would reduce the model efﬁciency. Also, for layout estimation, we simply need to know at least the depth values of corner points in the room. To consider the balance between efﬁciency and accuracy, we propose a “Grid Re-sample” strategy which is able to approximate the horizon-depth map by a ﬂexi-ble number of casting rays (see Figure 1(b)). We name our method LED2-Net, which can be efﬁciently trained in an end-to-end fashion.
To demonstrate the effectiveness of our proposed model based on the novel technique of Differentiable Depth Ren-dering for 360◦ layout estimation, we conduct extensive experiments on four benchmark datasets, including Matter-port3D [30], Realtor360 [24], PanoContext [26], and Stan-ford2D3D [1]. We show that our method performs favor-ably against state-of-the-art approaches in both the within-Figure 3. (a) The layout-depth generated from layout annotation.
The horizontal red line indicates the horizon-depth, in which we use it as the supervisory signal for the network. (b) The horizon-depth aligned with the RGB panorama. dataset and cross-dataset settings. More interestingly, we leverage the property of our depth estimation objective to enable depth pre-training using a synthetic dataset, Struc-ture3D [27], which further improves the generalization abil-ity of our model. Our supplementary material, source code, and pre-trained models are available to the public on our project website1. We summarize our contributions as fol-lows: 1. We reformulate the task of 360◦ layout estimation to a unique 360◦ depth estimation problem that optimizes a loss in 3D while maintaining the simplicity of layout estimation. 2. We propose a differentiable layout-to-depth procedure to convert the layout into horizon-depth through ray-casting of a few points, which enables the end-to-end training on layout estimation datasets. 3. We show that our framework can be seamlessly pre-trained by 360◦ depth datasets, which further improves the generalizability on cross-dataset evaluations. 2.