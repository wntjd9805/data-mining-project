Abstract
Few-shot segmentation has been attracting a lot of atten-tion due to its effectiveness to segment unseen object classes with a few annotated samples. Most existing approaches use masked Global Average Pooling (GAP) to encode an an-notated support image to a feature vector to facilitate query image segmentation. However, this pipeline unavoidably loses some discriminative information due to the average operation. In this paper, we propose a simple but effective self-guided learning approach, where the lost critical in-formation is mined. Speciﬁcally, through making an initial prediction for the annotated support image, the covered and uncovered foreground regions are encoded to the primary and auxiliary support vectors using masked GAP, respec-tively. By aggregating both primary and auxiliary support vectors, better segmentation performances are obtained on query images. Enlightened by our self-guided module for 1-shot segmentation, we propose a cross-guided module for multiple shot segmentation, where the ﬁnal mask is fused us-ing predictions from multiple annotated samples with high-quality support vectors contributing more and vice versa.
This module improves the ﬁnal prediction in the inference stage without re-training. Extensive experiments show that our approach achieves new state-of-the-art performances on both PASCAL-5i and COCO-20i datasets. Source code is available at https://github.com/zbf1991/SCL . 1.

Introduction
Semantic segmentation has been making great progress with recent advances in deep neural network especially
Fully Convolutional Network (FCN) [18]. Requiring suf-ﬁcient and accurate pixel-level annotated data, state-of-the-art semantic segmentation approaches can produce satisfy-ing segmentation masks. However, these approaches heav-ily rely on massive annotated data. Their performance drops dramatically on unseen classes or with insufﬁcient anno-∗Corresponding author 1The work was supported by National Natural Science Foundation of
China under 61972323.
Figure 1. Motivation of our approach. Even using the same im-age as both support and query input, previous approaches cannot generate accurate segmentation under the guide of its ground-truth mask. tated data [33].
Few-shot segmentation [8, 14, 20, 24] is a promising method to tackle this issue. Compared to fully supervised semantic segmentation [3, 5, 11, 13] which can solely seg-ment the same classes in the training set, the objective of few-shot segmentation is to utilize one or a few annotated samples to segment new classes. Speciﬁcally, the data in few-shot segmentation is divided into two sets: support set and query set. This task requires to segment images from the query set given one or several annotated images from the support set. Thus, the key challenge of this task is how to leverage the information from the support set. Most ap-proaches [6, 17, 30, 35, 32, 26] adopt a Siamese Convolu-tional Neural Network (SCNN) to encode both support and query images. In order to apply the information from sup-port images, they mainly use masked Global Average Pool-ing (GAP) [38] or other strengthened methods [19] to ex-tract all foreground [30, 35, 16] or background [30] as one feature vector, which is used as a prototype to compute co-sine distance [36] or make dense comparison [35] on query images.
Using a support feature vector extracted from the sup-port image does facilitate the query image segmentation, but it does not carry sufﬁcient information. Fig. 1 shows an extreme example where the support image and query im-age are exactly the same. However, even the existing best 8312
performing approaches fail to accurately segment the query image. We argue that when we use masked GAP or other methods [19] to encode a support image to a feature vector, it is unavoidable to lose some useful information due to the average operation. Using such a feature vector to guide the segmentation cannot make a precise prediction for pixels which need the lost information as support. Furthermore, for the multiple shot case such as 5-shot segmentation, the common practice is to use the average of predictions from 5 individual support images as the ﬁnal prediction [36] or the average of 5 support vectors as the ﬁnal support vector [30].
However, the quality of different support images is differ-ent, using an average operation forces all support images to share the same contribution.
In this paper, we propose a simple yet effective Self-Guided and Cross-Guided Learning approach (SCL) to overcome the above mentioned drawbacks. Speciﬁcally, we design a Self-Guided Module (SGM) to extract compre-hensive support information from the support set. Through making an initial prediction for the annotated support image with the initial prototype, the covered and uncovered fore-ground regions are encoded to the primary and auxiliary support vectors using masked GAP, respectively. By ag-gregating both primary and auxiliary support vectors, better segmentation performances are obtained on query images.
Enlightened by our proposed SGM, we propose a Cross-Guided Module (CGM) for multiple shot segmentation, where we can evaluate prediction quality from each sup-port image using other annotated support images, such that the high-quality support image will contribute more in the
ﬁnal fusion, and vice versa. Compared to other compli-cated approaches such as the attention mechanism [35, 34], our CGM does not need to re-train the model, and di-rectly applying it during inference can improve the ﬁnal per-formance. Extensive experiments show that our approach achieves new state-of-the-art performances on PASCAL-5i and COCO-20i datasets.
Our contributions are summarized as follows:
• We observe that it is unavoidable to lose some useful critical information using the average operation to ob-tain the support vector. To mitigate this issue, we pro-pose a self-guided mechanism to mine more compre-hensive support information by reinforcing such easily lost information, thus accurate segmentation mask can be predicted for query images.
• We propose a cross-guided module to fuse multiple predictions from different support images for the mul-tiple shot segmentation task. Without re-training the model, it can be directly used during inference to im-prove the ﬁnal performance.
• Our approach can be applied to different baselines to improve their performance directly. Using our ap-proach achieves new state-of-the-art performances on
PASCAL-5i (mIoU for 1-shot: 61.8%, 5-shot: 62.9%) and COCO-20i datasets (mIoU for 1-shot: 37.0%, 5-shot: 39.9%) for this task. 2.