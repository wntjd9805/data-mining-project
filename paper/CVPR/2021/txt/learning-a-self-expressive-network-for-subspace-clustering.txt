Abstract
State-of-the-art subspace clustering methods are based on the self-expressive model, which represents each data point as a linear combination of other data points. How-ever, such methods are designed for a ﬁnite sample dataset and lack the ability to generalize to out-of-sample data.
Moreover, since the number of self-expressive coefﬁcients grows quadratically with the number of data points, their ability to handle large-scale datasets is often limited.
In this paper, we propose a novel framework for subspace clus-tering, termed Self-Expressive Network (SENet), which em-ploys a properly designed neural network to learn a self-expressive representation of the data. We show that our
SENet can not only learn the self-expressive coefﬁcients with desired properties on the training data, but also han-dle out-of-sample data. Besides, we show that SENet can also be leveraged to perform subspace clustering on large-scale datasets. Extensive experiments conducted on syn-thetic data and real world benchmark data validate the ef-fectiveness of the proposed method.
In particular, SENet yields highly competitive performance on MNIST, Fashion
MNIST and Extended MNIST and state-of-the-art perfor-mance on CIFAR-10. 1.

Introduction
With technological advances in data acquisition, storage and processing, there is a surge in the availability of large-scale databases in computer vision. While the development of modern machine learning techniques, such as deep learn-ing, has led to great success in analyzing big data, such methods require a large amount of annotated data which is often costly to obtain. Extracting patterns and clusters from unlabeled big data has become an important open problem.
We consider the problem of clustering large-scale un-labeled data under the assumption that each cluster is ap-proximated by a low-dimensional subspace of the high-dimensional ambient space, a.k.a. subspace clustering [62, 63]. This problem has wide applications in image clustering
[23, 17], motion segmentation [12, 9], hybrid system iden-tiﬁcation [61, 5], cancer subtype clustering [44, 32], hyper-spectral image segmentation [86] and so on.
Self-expressive model [16] is one of the most popular and successful methods for subspace clustering. Given a data matrix X = [x1, · · · , xN ] ∈ IRD×N whose columns are drawn from a union of n subspaces, the self-expressive model expresses each data point xj ∈ IRD as a linear com-bination of other data points, i.e., xj =
X i6=j cijxi, (1) where {cij}i6=j are self-expressive coefﬁcients. A remark-able property of the self-expressive model is that solu-tions to (1) that minimize certain regularization function on the coefﬁcients have the subspace-preserving property, i.e., nonzero coefﬁcients cij occur only between xi and xj lying in the same subspace [16, 17, 37, 41, 56, 68, 83, 79, 76, 40].
Consequently, correct clustering can be obtained by deﬁn-ing an afﬁnity between any pair of data points xi and xj as, e.g., |cij| + |cji|, and applying spectral clustering to the afﬁnity. Recent developments further extend the applicabil-ity of self-expressive models to the case where the data are corrupted by noise [66, 57, 67] and outliers [56, 82], are im-balanced over classes [77], or possess missing entries [59].
Despite its great empirical performance and broad theo-retical guarantees for correctness, the self-expressive model suffers from the limitation that it requires solving for a self-expressive matrix of size N × N , which is computationally prohibitive for large-scale data. Although scalable subspace clustering methods based on subsampling [51], sketching
[58] or learning a compact dictionary [3, 54] already exist, they do not have broad theoretical guarantees for correct-ness and sacriﬁce accuracy for scalability. In addition, the self-expressive coefﬁcients computed for a set of data can-not be used to produce self-expressive coefﬁcients for pre-viously unseen data, posing challenges for learning in an online setting and for out-of-sample data.
In this work, we introduce the self-expressive network (SENet) to learn a self-expressive model for subspace clus-12393
tering, which can be leveraged to handle out-of-sample data and large-scale data. Our method is based on learning a function f (xi, xj; Θ) : RD × RD → R, implemented as a neural network with parameters Θ, that is designed to sat-isfy the self-expressive model xj =
X i6=j f (xi, xj; Θ) · xi. (2)
In principle, the number of network parameters does not need to scale with the number of points in the dataset, hence
SENet can effectively handle large scale data. Moreover, an
SENet trained on a certain dataset can be used to produce self-expressive coefﬁcients for another dataset drawn from the same data distribution, therefore the method can handle out-of-sample data effectively. We present a network archi-tecture for f (xi, xj; Θ) as well as a training algorithm that allow us to learn self-expressive coefﬁcients with desired subspace-preserving properties. Our experiments showcase the effectiveness of our method as summarized below: 1. We show that the self-expressive coefﬁcients computed by a trained SENet closely approximate those computed by solving for them directly without the network. This il-lustrates the ability of SENet to approximate the desired self-expressive coefﬁcients. 2. We show that a SENet trained on (part of) the training set of MNIST and Fashion MNIST can be used to pro-duce self-expressive coefﬁcients on the test set that give a good clustering performance. This illustrates the abil-ity of SENet to handle out-of-sample data. 3. We show that SENet can be used to cluster datasets con-taining 70,000+ data poins, such as MNIST, Fashion
MNIST and Extended MNIST, very efﬁciently, achiev-ing a performance that closely matches (for MNIST,
Fashion MNIST and Extended MNIST) or surpasses (for
CIFAR-10) the state of the art. 2.