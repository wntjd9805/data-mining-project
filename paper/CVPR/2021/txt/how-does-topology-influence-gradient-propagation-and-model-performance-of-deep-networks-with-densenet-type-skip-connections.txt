Abstract
DenseNets introduce concatenation-type skip connections that achieve state-of-the-art accuracy in several computer vision tasks. In this paper, we reveal that the topology of the concatenation-type skip connections is closely related to the gradient propagation which, in turn, enables a pre-dictable behavior of DNNs’ test performance. To this end, we introduce a new metric called NN-Mass to quantify how effectively information ﬂows through DNNs. Moreover, we empirically show that NN-Mass also works for other types of skip connections, e.g., for ResNets, Wide-ResNets (WRNs), and MobileNets, which contain addition-type skip connec-tions (i.e., residuals or inverted residuals). As such, for both DenseNet-like CNNs and ResNets/WRNs/MobileNets, our theoretically grounded NN-Mass can identify models with similar accuracy, despite having signiﬁcantly different size/compute requirements. Detailed experiments on both synthetic and real datasets (e.g., MNIST, CIFAR-10, CIFAR-100, ImageNet) provide extensive evidence for our insights.
Finally, the closed-form equation of our NN-Mass enables us to design signiﬁcantly compressed DenseNets (for CIFAR-10) and MobileNets (for ImageNet) directly at initialization without time-consuming training and/or searching.1 1.

Introduction
DenseNets [7] and their variants have been widely adopted by the deep learning community to achieve ex-cellent performance in many computer vision tasks such as image classiﬁcation, object detection, image segmenta-tion, super resolution, among many others [39, 8, 38]. One of the main contributions of DenseNets is the introduction of concatenation-type skip connections where the output channels from all previous layers are concatenated at the input of the current convolutional layer. The concatenation-type skip connections2 have been particularly valuable to
*Equal Contribution 1Code at https://github.com/SLDGroup/NN_Mass. 2Also referred to as DenseNet-type skip connections. the deep learning literature. For instance, in addition to signiﬁcant accuracy and efﬁciency gains in computer vi-sion applications, many state-of-the-art Neural Architecture
Search (NAS) techniques have exploited the concatenation-type skip connections into their search space to obtain high-performance models [24, 16, 26, 17, 40]. However, to the best of our knowledge, the properties of concatenation-type skip connections such as their gradient propagation and the resulting effect on model performance has not been explored.
Recently, an important method called Dynamical Isom-etry emerged in order to quantify gradient ﬂow through
DNNs [29, 31, 23, 13]. When DNNs achieve “dynamical isometry”, the signal ﬂows through such networks without signiﬁcant ampliﬁcation or attenuation. This, in turn, helps the learning process and, hence, quantiﬁes the gradient prop-erties of DNNs. The role of Dynamical Isometry in trainabil-ity has been demonstrated for networks such as ResNets [5] which have addition-type skip connections.
Due to concatenation of channels, DenseNet-type skip connections enforce strong structural/topological constraints on the gradient propagation (i.e., the gradients can only fol-low speciﬁc paths during training). In general, the topology (or structure) of graphs/networks directly inﬂuences the pro-cess taking place over them [21]. For instance, how closely the users of a social network are connected to each other completely determines how fast the information propagates through the network [14, 9]. Consequently, the structural constraints imposed by concatenation of channels must also affect the learning dynamics of DNNs. Motivated by this observation, we study the relationship between topology, gra-dient ﬂow, and model performance of such deep networks.
Note that, to study these topological properties, we do not use the original DenseNets which contain all-to-all connec-tions [7], but rather a generalized version where we can vary the density of skip connections (more details in Section 3).
To this end, we ﬁrst deﬁne our setup of DNNs with concatenation-type skip connections. Then, we propose a new metric called NN-Mass to quantify the topological prop-erties of DNNs considered within this setup. Next, we show 13498
the relationship between NN-Mass (a topological property) and Layerwise Dynamical Isometry (LDI) [13], a property that indicates the faithful gradient propagation through the network [29]. Speciﬁcally, we show that irrespective of number of parameters/FLOPS/layers, models with similar
NN-Mass and width should have similar LDI, and thus a similar gradient ﬂow that results in comparable accuracy.
To support these theoretical insights, we conduct exten-sive experiments to show that models with the same width and NN-Mass indeed achieve a similar accuracy irrespective of their depth, number of parameters (#Params), and FLOPS.
Moreover, we empirically show that NN-Mass also works for other types of skip connections, e.g., for ResNets, Wide-ResNets (WRNs), and MobileNets which contain addition-type skip connections (ATSC), i.e., residuals or inverted residuals. Finally, we show how the closed-form expression for NN-Mass can be used to directly design compressed
DNNs, that is, without any time-consuming training and (manual or automatic) searching for compressed models.
Overall, we make the following key contributions: (i) We reveal how topological constraints imposed by
DenseNet-type skip connections inﬂuence gradient propaga-tion and resulting accuracy; (ii) For this setup, we propose a new topological metric called NN-Mass that is theoreti-cally linked to Layerwise Dynamical Isometry and quantiﬁes how efﬁciently information propagates in neural networks; (iii) Our experiments encompass multilayer perceptron as well as CNNs with DenseNet-type skip connections on sev-eral datasets (MNIST, CIFAR-10, CIFAR-100, Imagenet).
Our results demonstrate that NN-Mass is an excellent indi-cator of accuracy and support our theory. We further em-pirically show that NN-Mass also works for ATSC-based networks (ResNets, WRNs, and MobileNets); (iv) Finally,
NN-Mass allows us to directly design models with up to 3
× compression rate (for DenseNets on CIFAR-10), and up to 34%-40% compression rate (for MobileNet-v2 on ImageNet) in #Params/FLOPS while losing minimal accuracy.
The rest of the paper is organized as follows: Section 2 discusses the related work and some preliminaries. Then,
Section 3 describes our proposed metric and its theoretical analysis. Section 4 presents detailed experimental results.
Finally, Section 5 summarizes our work and contributions. 2.