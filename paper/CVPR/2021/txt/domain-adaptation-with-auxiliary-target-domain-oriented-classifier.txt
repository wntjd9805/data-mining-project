Abstract
Domain adaptation (DA) aims to transfer knowledge from a label-rich but heterogeneous domain to a label-scare domain, which alleviates the labeling efforts and attracts considerable attention. Different from previous methods focusing on learning domain-invariant feature representa-tions, some recent methods present generic semi-supervised learning (SSL) techniques and directly apply them to DA tasks, even achieving competitive performance. One of the most popular SSL techniques is pseudo-labeling that as-signs pseudo labels for each unlabeled data via the classiﬁer trained by labeled data. However, it ignores the distribution shift in DA problems and is inevitably biased to source data.
To address this issue, we propose a new pseudo-labeling framework called Auxiliary Target Domain-Oriented Clas-siﬁer (ATDOC). ATDOC alleviates the classiﬁer bias by introducing an auxiliary classiﬁer for target data only, to improve the quality of pseudo labels. Speciﬁcally, we em-ploy the memory mechanism and develop two types of non-parametric classiﬁers, i.e. the nearest centroid classiﬁer and neighborhood aggregation, without introducing any addi-tional network parameters. Despite its simplicity in a pseudo classiﬁcation objective, ATDOC with neighborhood aggrega-tion signiﬁcantly outperforms domain alignment techniques and prior SSL techniques on a large variety of DA bench-marks and even scare-labeled SSL tasks. 1.

Introduction
Despite remarkable progress in classiﬁcation tasks over the past decades, deep neural network models still suffer poor generalization performance to another new domain e.g. classifying real-world object images using a classiﬁcation model trained on simulated object images [54], due to the well-known dataset shift [55] or domain shift [66] problem.
Hence, to avoid expensive human labeling and utilize prior related labeled datasets, lots of research efforts have been devoted to developing domain adaptation (DA) methods
*This work was partially supported by AISG-100E-2019-035,
MOE2017-T2-2-151, NUS_ECRA_FY17_P08 and CRP20-2017-0006.
[21, 20, 27, 67, 59, 29, 40] to transfer knowledge in the label-rich dataset to a label-scare scenario. Depending on the availability of labeled data in the target domain, one can further divide existing DA methods into two categories, i.e., unsupervised DA [20] and semi-supervised DA [59].
This paper mainly focuses on unsupervised DA where no labeled data is available in the target domain during training.
Recently, deep unsupervised DA approaches have almost dominated this ﬁeld with promising results [47, 20, 48, 37, 31, 14], and most of them focus on learning domain-invariant feature representations that achieve a small error on the source domain at the same time. With the assumption about covariate shift in [2], the learned representations together with the classiﬁer built on the source domain are able to gen-eralize well to the target domain. However, the strict assump-tion does not always hold in real-world applications. Another line of research ignores transferable representation learning but directly resorts to semi-supervised learning (SSL) tech-niques for the DA problems [18, 8, 57, 16, 30], where the target domain could be readily treated as the unlabeled set in
SSL. For instance, MixMatch [3], a popular SSL approach, has been successfully applied by [57] that wins prizes of the
VisDA 2019 Challenge. But these SSL-based DA methods may fail to classify target samples far away from the source domain due to the ignorance of domain shift.
One of the most popular SSL techniques—pseudo label-ing [38] iteratively assigns pseudo labels corresponding to the maximum prediction scores for each unlabeled data in the target domain and then retrains the network with the pseudo-labeled data in a supervised manner. However, the network is inevitably biased to the labeled source data during training, giving low-quality pseudo labels and propagating errors in the target domain. To tackle this issue, we pro-pose a new pseudo-labeling framework termed Auxiliary
Target Domain-Oriented Classiﬁer (ATDOC) for DA prob-lems. Generally, ATDOC attempts to alleviate the labeling bias by introducing an auxiliary classiﬁer for target data only.
In particular, we design two types of non-parametric classi-ﬁers, i.e., nearest centroid classiﬁer (NC) and neighborhood aggregation (NA), to avoid additional network parameters.
Both class centroids and local neighborhood structures are 16632
Figure 1. The pipeline of our proposed framework ATDOC with neighborhood aggregation (NA) for UDA. Different from exist-ing methods that mostly rely on feature-level domain alignment,
ATDOC addresses domain shift by alleviating the classiﬁer bias via an auxiliary classiﬁer for target data only during adaptation.
ATDOC-NA employs a memory bank and develops neighborhood aggregation to help build the domain-speciﬁc classiﬁer Ft, and expects to generate unbiased accurate pseudo labels along with conﬁdence weights for unlabeled data. capable of representing the target domain, thus the generated target-oriented pseudo labels are fairly unbiased and reliable.
To enable global structure learning with mini-batch opti-mization, we introduce a memory module to store informa-tion over all the unlabeled target samples. Besides, since no labeled data is available in the target domain, noisy pseudo labels are directly exploited as an alternative. Speciﬁcally, concerning the NC classiﬁer training, we follow [72, 75] and construct a memory bank to store feature representations of the class centroids. The exponential moving average strategy is adopted to dynamically update centroids in the memory bank. Built on the centroids, the NC classiﬁer readily offers a target-oriented pseudo-label for each unlabeled data. Com-pared with coarse class centroids, training a NA classiﬁer needs to construct a large memory bank that consists of the features along with soft predictions over all the target sam-ples. Through simple aggregation, NA feasibly generates conﬁdence as an instance weight besides the one-hot pseudo label. An overview of ATDOC-NA is shown in Fig. 1. Gen-erally, the network at ﬁrst several iterations is still biased to source data, but as the number of training iterations increases, retraining with target-oriented pseudo labels gradually ad-justs the network to unlabeled target data, achieving domain alignment via optimizing two classiﬁcation objectives.
Our contributions are summarized as follows: (i) we propose ATDOC, a new framework to combat classiﬁer bias that provides a new perspective of addressing domain shift. (ii) we exploit the memory bank and develop two types of non-parametric classiﬁers, not involving complicated net-work architectures with extra parameters. (iii) despite its sim-plicity, ATDOC achieves competitive or better results than prior state-of-the-arts under a variety of DA settings, e.g., partial-set unsupervised DA [5] and semi-supervised DA.
ATDOC can be seamlessly integrated into existing domain-invariant feature learning methods and further boost their adaptation performance. Besides, we study an SSL setting with only a few annotated data points available and ﬁnd AT-DOC also performs better than other SSL techniques even without domain shift. 2.