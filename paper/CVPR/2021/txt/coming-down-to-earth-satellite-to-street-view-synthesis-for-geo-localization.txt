Abstract
The goal of cross-view image based geo-localization is to determine the location of a given street view image by matching it against a collection of geo-tagged satellite im-ages. This task is notoriously challenging due to the drastic viewpoint and appearance differences between the two do-mains. We show that we can address this discrepancy ex-plicitly by learning to synthesize realistic street views from satellite inputs. Following this observation, we propose a novel multi-task architecture in which image synthesis and retrieval are considered jointly. The rationale behind this is that we can bias our network to learn latent feature rep-resentations that are useful for retrieval if we utilize them to generate images across the two input domains. To the best of our knowledge, ours is the ﬁrst approach that cre-ates realistic street views from satellite images and local-izes the corresponding query street-view simultaneously in an end-to-end manner. In our experiments, we obtain state-of-the-art performance on the CVUSA and CVACT bench-marks. Finally, we show compelling qualitative results for satellite-to-street view synthesis. 1.

Introduction
Estimating the geographic location of an image is a fundamental problem in computer vision with applications in autonomous driving, robotics, and augmented reality. the problem was cast as an image retrieval
Originally, task [27, 9, 37, 6, 2, 31, 38, 26], where the goal is to de-termine the geographic location of a query street view im-age by comparing it against a database of GPS-tagged street images. The main limitation of this approach is that, even though there are large databases available for this type of imagery, the coverage varies a lot between different regions of the world, and it is generally sparse in rural areas.
Satellite imagery, on the other hand, is broadly available for most parts of the world with services like Google maps.
This encouraged researchers to focus on cross-view image-based geo-localization [36, 16, 33, 17, 28, 4, 30, 29] as a more general and inclusive alternative. The overall idea is to predict the latitude and longitude of a street-level image by matching it against a GPS-tagged satellite database. Even though this approach helps to cover vast parts of the world, the signiﬁcant domain gap between a pair of street view and top-view satellite images, shown in Figure 1, makes cross-view image based geo-localization extremely challenging.
For instance, the appearance of the two images can vary signiﬁcantly as they are typically taken at different times and with different cameras, leading to illumination changes.
The biggest challenge, however, comes from the dramat-ically different viewpoints of street and satellite images – even for human eyes, it is far from obvious that two images show the same location. Satellite images cover a broader area in comparison to the ego-centric viewpoint of the street images. On the other hand, there are a lot of additional fea-6488
tures in street view images, like facades, that are not visible in the top-view satellite images which would otherwise be extremely useful for precise location retrieval.
In order to alleviate the difﬁculty of learning cross-view
[28, 29] use a simple polar coordinate transfor-features,
In-mation as a preprocessing step for image retrieval. tuitively, this mimics the real viewpoint transformation from the overhead view to the ground-view. Nevertheless, there is still a signiﬁcant appearance gap between polar-transformed and real street images. The two views do not overlap perfectly, which limits the retrieval performance.
In the last few years, Generative Adversarial Networks (GANs) [8] have proven to be a powerful tool for gener-ating realistic looking images. Recent works [22, 23, 43] applied them for cross-view image synthesis between aerial and ground-level images but they do not evaluate their ef-fectiveness for the geo-localization task. [24] is the ﬁrst to use pre-trained synthesized images [22] to train a retrieval network for geo-localization. However, this is done in two stages and therefore does not allow for end-to-end training.
They obtained less accurate retrieval results than methods based on polar transformations [28, 29]. This suggests that, while GANs create images that look more realistic, polar-transformation is more suitable to map the content of the images across the two domains.
In this work, our goal is to address the drastic viewpoint difference of the two domains by synthesizing realistic-looking and content-preserving street images from their satellite counterparts for geo-localization. To that end, we integrate a cross-view synthesis module and a geo-localization branch in a single architecture. The main in-sight here is that these two network components mutu-ally reinforce each other: Learning to generate street im-ages from satellite inputs naturally helps the image retrieval branch, since our network learns to extract local features that are useful across the two input domains. Vice versa, the retrieval branch incentivizes our network to create realistic street views that replicate the content of a given satellite image. Additionally, our network uses polar transformed satellite images as a starting point (i.e. as an input to the
GAN). This makes the image generation easier, since the spatial layout of the polar transformed image and the street view is approximately the same.
Contribution We propose a novel geo-localization method that is trained jointly for the multi-task setup of both synthesizing ground images from satellite images and retrieving cross-view image matches. We devise a single network for both of these tasks which can be trained in an end-to-end manner. Our method shows strong empiri-cal results, both in terms of the retrieval accuracy and syn-thesis quality. For geo-localization, we obtain state-of-the-art performance on standard large-scale cross-view retrieval benchmarks. Moreover, our pipeline generates highly re-alistic street views that strongly resemble real, panoramic street images. Remarkably, our method outperforms exist-ing cross-view synthesis approaches that use semantic la-bels as supervision during training. 2.