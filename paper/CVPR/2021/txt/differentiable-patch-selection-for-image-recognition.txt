Abstract
Neural Networks require large amounts of memory and compute to process high resolution images, even when only a small part of the image is actually informative for the task at hand. We propose a method based on a differentiable
Top-K operator to select the most relevant parts of the input to efﬁciently process high resolution images. Our method may be interfaced with any downstream neural network, is able to aggregate information from different patches in a
ﬂexible way, and allows the whole model to be trained end-to-end using backpropagation. We show results for traf-ﬁc sign recognition, inter-patch relationship reasoning, and
ﬁne-grained recognition without using object/part bound-ing box annotations during training. 1.

Introduction
High-resolution imagery has become ubiquitous nowa-days: both consumer devices and specialized sensors rou-tinely capture images and videos with resolution in tens of megapixels. Processing these high-quality images with computer vision models remains challenging: analyzing the images at full resolution can be prohibitively computation-ally expensive, while simply downsampling them before processing may remove important ﬁne details and substan-tially hurt performance. It would be desirable to save com-pute, while retaining the capability to recognize ﬁne details.
Compute can be saved by exploiting the following prop-erty of many practical vision tasks: not all parts of the image are equally important for ﬁnding the answer. Figure 1 shows examples of tasks where only a small fraction of the full im-age needs to be processed in detail. Being able to quickly discard uninformative parts of the image would have sev-eral beneﬁts. It would reduce the overall computational and memory complexity of the model, and the regions of inter-est could be processed in more detail and by a more power-∗Work done during internship at Google Research.† Equal contribution.
Figure 1: Examples of large images where patch extraction allows (top-left) to focus on details for ﬁne-grained recog-nition, (bottom-left) to reason across patches, and (right) to efﬁciently capture very localized information. ful model than otherwise.
Determining which parts of the image to retain and which to discard is usually nontrivial and highly task de-pendent. In some applications the solution might be as sim-ple as taking the center crop of the image, but in most cases relevant regions need to be detected ﬁrst. For instance, in a self-driving car setting, it would be permissible to ignore the sky, but all trafﬁc signs in sight should be correctly identi-ﬁed and must not be ignored. One may formulate this as fol-lows: Given a regular grid of equally sized image patches, decide for each patch whether to process or discard it. This decision is however discrete, which makes it unsuitable for end-to-end learning.
To overcome this limitation, inspired by the work of
Katharopoulos & Fleuret [22], we formulate patch selec-tion as a ranking problem, where per-patch relevance scores are predicted by a small ConvNet and the Top K scor-ing patches are selected for downstream processing. We make this end-to-end trainable with backpropagation using the perturbed maximum method of Berthet et. al [5]. We present this as a generic module for patch selection. Our approach is most effective when the majority of patches in the image are irrelevant to the target, but the model a priory 12351
does not know where in the image the important patches are present. Hence, we do not aim to achieve image coverage such as in semantic segmentation and object detection.
In the remainder of this paper, we will formulate patch selection for image recognition as a Top-K selection prob-lem in section 3, apply the perturbed maximum method to construct an end-to-end model trainable via backpropaga-tion in section 4.2, and demonstrate wide applicability of this method via empirical results in three different domains: (1) street sign recognition, (2) inter-patch relationship rea-soning on synthetic data, and (3) ﬁne-grained classiﬁcation without using object/part bounding box annotations during training and evaluation (Section 5). 2.