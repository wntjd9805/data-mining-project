Abstract
Despite remarkable progress achieved, most neural ar-chitecture search (NAS) methods focus on searching for one single accurate and robust architecture. To further build models with better generalization capability and perfor-mance, model ensemble is usually adopted and performs better than stand-alone models. Inspired by the merits of model ensemble, we propose to search for multiple diverse models simultaneously as an alternative way to ﬁnd pow-erful models. Searching for ensembles is non-trivial and has two key challenges: enlarged search space and poten-tially more complexity for the searched model. In this paper, we propose a one-shot neural ensemble architecture search (NEAS) solution that addresses the two challenges. For the
ﬁrst challenge, we introduce a novel diversity-based met-ric to guide search space shrinking, considering both the potentiality and diversity of candidate operators. For the second challenge, we enable a new search dimension to learn layer sharing among different models for efﬁciency purposes. The experiments on ImageNet clearly demon-strate that our solution can improve the supernet’s capacity of ranking ensemble architectures, and further lead to better search results. The discovered architectures achieve supe-rior performance compared with state-of-the-arts such as
MobileNetV3 and EfﬁcientNet families under aligned set-tings. Moreover, we evaluate the generalization ability and robustness of our searched architecture on the COCO de-tection benchmark and achieve a 3.1% improvement on AP compared with MobileNetV3. Codes and models are avail-able here. 1.

Introduction
The emergence of deep neural networks greatly re-lieves the need for feature engineering. Previous studies have shown that the design of neural network architecture
[11, 25, 32, 41] is essential to the performance for varied
Figure 1. Comparison of our method with state-of-the-art ap-proaches on ImageNet under mobile settings. tasks in computer vision. However, the number of possi-ble architectures is enormous, making the manual design very difﬁcult. Neural Architecture Search (NAS) [48] aims to automate the design process. Recently, NAS methods have achieved state-of-the-arts on varied tasks such as im-age classiﬁcation [48], semantic segmentation [23], object detection [4], etc. Despite great progress achieved, most of the NAS methods focus on searching for optimal architec-tures of single models. However, the generalization ability and performance of single models are usually affected by different initialization, noisy data, and training recipe mod-iﬁcation.
Model ensemble has been proved to be a universally ef-fective method to build more robust and accurate models compared with single models.
Implicit ensemble meth-ods like Dropout [34], Dropconnect [40], StochDepth [15],
Shake-Shake [9] are already widely used in neural archi-tecture design. On the contrary, although explicit ensem-ble methods like averaging, bagging, boosting, and stack-ing have been commonly adopted in large competitions and real-world scenarios. The use of explicit ensemble methods in designing efﬁcient models is not fully explored due to the extra computation they brought.
*This work is done when Minghao is an intern at Microsoft.
Inspired by the effectiveness of ensemble, we propose to 16530
search for multiple models instead of one simultaneously to form a robust, accurate and efﬁcient ensemble model.
However, the combination of NAS and ensemble faces two challenges: (1) efﬁcient search and supernet optimization over a large search space (2) reducing the extra complexity brought by model ensemble. Addressing these challenges, in this paper, we propose a one-shot neural ensemble archi-tecture search (NEAS) approach searching for lightweight ensemble models.
To solve the ﬁrst challenge caused by the enlarged space of ensemble models compared with single models, we pro-pose a novel metric called diversity score to progressively drop inferior candidates during the supernet training pro-cess, thus reduce the difﬁculty of ﬁnding promising ensem-ble models. This metric explicitly quantiﬁes the diversity between the operators, which is commonly believed to be a key factor in building models with better feature expression capability.
To solve the second challenge, we introduce the layer sharing mechanism to reduce the model complexity. We allow the ensemble components share some shallow layers and search for the best architectures of the shared layers together with the architectures of the rest layers. We fur-ther introduce a new search dimension called split point to automatically ﬁnd optimal layers for sharing under a given
FLOPs constraint.
Comprehensive experiments verify the effectiveness of the proposed diversity score and layer sharing strategy.
They improve the ranking ability of trained supernet and lead to better searched architectures under same complexity constraint. The searched architectures generate new state-of-the-art performance on ImageNet [8]. For instance, as shown in Fig. 1, our search algorithm ﬁnds a 314M FLOPs model that achieves 77.9% top-1 accuracy on ImageNet, which is 19% smaller and 1.6% better than EfﬁcientNet-B0
[37]. The architecture discovered by NEAS transfers well to downstream object detection task, suggesting the gener-alization ability of the searched models. We obtain an AP of 33.0 on COCO validation set, which is superior to the state-of-the-art backbone, MobileNetV3 [12].
In summary, we make the following contributions:
• We propose a pipeline, NEAS, searching for diverse models under certain resource constraints. Our ap-proach could search for both homogeneous and het-erogeneous ensemble models.
• We design a new metric, diversity score, to guide the shrinking process of search space. We evaluate its su-periority on supernet training and the performance of searched models by enormous experiments.
• We propose a layer-sharing strategy to reduce the com-plexity of ensemble models and enlarge the search space to search for an optimal split point.
• We compare the searched architectures to state-of-the-art NAS methods on the image classiﬁcation task and achieve state-of-the-art results. Furthermore, we eval-uate our searched model on the downstream object de-tection task, showing their generalization ability. 2.