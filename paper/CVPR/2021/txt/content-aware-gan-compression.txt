Abstract
Generative adversarial networks (GANs), e.g., Style-GAN2, play a vital role in various image generation and synthesis tasks, yet their notoriously high computational cost hinders their efﬁcient deployment on edge devices.
Directly applying generic compression approaches yields poor results on GANs, which motivates a number of re-cent GAN compression works. While prior works mainly accelerate conditional GANs, e.g., pix2pix and Cycle-GAN, compressing state-of-the-art unconditional GANs has rarely been explored and is more challenging. In this pa-per, we propose novel approaches for unconditional GAN compression. We ﬁrst introduce effective channel pruning and knowledge distillation schemes specialized for uncon-ditional GANs. We then propose a novel content-aware method to guide the processes of both pruning and distil-lation. With content-awareness, we can effectively prune channels that are unimportant to the contents of interest, e.g., human faces, and focus our distillation on these re-gions, which signiﬁcantly enhances the distillation qual-ity. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the state-of-the-art compression method.
Notably, we reduce the FLOPs of StyleGAN2 by 11× with visually negligible image quality loss compared to the full-size model. More interestingly, when applied to various image manipulation tasks, our compressed model forms a smoother and better disentangled latent manifold, making it more effective for image editing. 1.

Introduction
Generative adversarial networks (GANs) [16] are the leading model for several crucial computer vision tasks like image generation [7, 30] and image editing [3, 4, 18, 47].
Due to their growing popularity and convincing perfor-mance, there is an increasing interest in deploying them on edge devices like mobile phones. However, state-of-the-art GANs often require large storage space, high computa-tional cost, and great memory utility, which disallows them for efﬁcient deployment. For example, StyleGAN2 [30] re-quires 45.1B/74.3B FLOPs to generate a 256px/1024px im-age, around 150×/250× more than MobileNet [43].
A number of network compression techniques have been developed for classiﬁcation models, including weight quan-tization [12, 26], network pruning [17, 24, 34, 55], and knowledge distillation [22, 41]. Nonetheless, these meth-ods are not directly applicable for GANs. For example, al-though removing channels with low activations [24] is ef-fective for classiﬁer compression, we ﬁnd it not better than training a smaller GAN from scratch (Tab. 1).
As such, several specialized GAN compression mecha-nisms are introduced to learn efﬁcient GAN models with the techniques of channel pruning and knowledge distilla-tion [45, 10, 48, 8, 35]. For example, Wang et al. [48] pro-pose GAN-Slimming (GS), which uniﬁes losses of channel pruning and knowledge distillation and achieves the state-of-the-art compression results. However, these methods mainly target on conditional GANs (pix2pix [25], Cycle-GAN [58], etc.) compression, and there is little study to compress unconditional GANs (StyleGAN2, etc.). While conditional GANs normally have paired training data and perform translation from images to images, unconditional
GANs are trained under completely unpaired setting and have much different source domains (white noises), which adds extra challenges for the compression. Therefore, a redesign of channel pruning and knowledge distillation schemes is required for effective unconditional GAN com-pression. In addition, these works also miss a signiﬁcant trait of GANs that the output of GANs are images with strong spatial correlation and meaningful semantic con-tents. While they prune channels by weights norm or scal-ing factors and distill images over all spatial locations, they pay no attention on the generated contents and just treat out-put images as normal 3D tensors.
To combat these issues, we propose novel approaches to effectively compress unconditional GANs. We ﬁrst develop an effective pruning metric to remove redundant channels and explore several distillation losses for unconditional
GANs compression. Different from prior works [10, 8, 35, 48] where either a norm-based loss or a perceptual loss is used for knowledge distillation, we ﬁnd that a combination 12156
Figure 1: We demonstrate the advantage of our compression approach on StyleGAN2 over two baseline methods: (1) a conventional classiﬁcation compression (Class. Comp.) approach with low activation based channel pruning [24] and norm-based knowledge distillation [41]. (2) the state-of-the-art GAN compression method, GAN-Slimming [48] (GS). Left:
Images generated by full-size model and three compression approaches. Our results show the least artifacts and the best distillation quality. Right: Model statistics for three compression schemes. Our model achieves the best FID performance with the highest FLOPs acceleration ratio from the full-size model compared to two baseline methods. of these losses improves GAN compression results. With our new pruning and distillation scheme, we achieve a ma-jor improvement in quantitative measurements over GS.
We then make the ﬁrst attempt of leveraging the seman-tic contents in the generated images to guide the GAN com-pression process of both pruning and distillation. Specif-ically, we leverage a content-parsing network to identify contents of interest (COI), a set of spatial locations with salient semantic concepts, within the generated images. We design a content-aware pruning metric to remove channels that are least sensitive to COI in the generated images. For knowledge distillation, we focus our distillation region only to COI of the teacher’s outputs which further enhances tar-get contents’ distillation. The advantage of the content-aware scheme over conventional method is not only demon-strated by a clear improvement in numerical statistics, but also visually explained in Fig. 7 and 9. Compared to a clas-siﬁcation compression approach and GS, our compressed model enjoys better generation quality and higher computa-tional acceleration, as shown in Fig. 1.
Our contributions are four-fold: (1) We develop a new framework of channel pruning and knowledge dis-tillation for unconditional GANs compression, which achieves a clear improvement over prior methods quanti-(2) We propose a novel content-aware compres-tatively. sion paradigm, which leverages generated contents to guide the process of pruning and distillation. Such a scheme fur-ther enhances both visual quality and numerical statistics of the compressed generators. (3) Compared to the state-of-the-art GAN compression method, GAN-Slimming [48], our method shows a major advancement in image gen-eration, embedding, and editing on SN-GAN and Style-GAN2. (4) We ﬁnd that our compressed generators not only have a better resource-performance tradeoff, but also own a smoother latent space manifold compared to the uncom-pressed model, which is beneﬁcial to image editing tasks. 2.