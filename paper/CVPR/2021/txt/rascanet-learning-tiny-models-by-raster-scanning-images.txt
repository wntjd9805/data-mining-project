Abstract
Deploying deep convolutional neural networks on ultra-low power systems is challenging due to the extremely lim-ited resources. Especially, the memory becomes a bottle-neck as the systems put a hard limit on the size of on-chip memory. Because peak memory explosion in the lower lay-ers is critical even in tiny models, the size of an input im-age should be reduced with sacriﬁce in accuracy. To over-come this drawback, we propose a novel Raster-Scanning
Network, named RaScaNet, inspired by raster-scanning in image sensors. RaScaNet reads only a few rows of pixels at a time using a convolutional neural network and then se-quentially learns the representation of the whole image us-ing a recurrent neural network. The proposed method oper-ates on an ultra-low power system without input size reduc-tion; it requires 15.9–24.3× smaller peak memory and 5.3– 12.9× smaller weight memory than the state-of-the-art tiny models. Moreover, RaScaNet fully exploits on-chip SRAM and cache memory of the system as the sum of the peak memory and the weight memory does not exceed 60 KB, improving the power efﬁciency of the system.
In our ex-periments, we demonstrate the binary classiﬁcation perfor-mance of RaScaNet on Visual Wake Words and Pascal VOC datasets. 1.

Introduction
With recent advances in deep learning, complex com-puter vision applications run on edge devices, which re-sults in various beneﬁts, including improving privacy, re-ducing power consumption, and personalizing predictions.
There has been emerging interest in expanding the scope of machine learning to ultra-low power systems, called
TinyML [2]. The goal of TinyML is to perform various always-on use-cases, typically in the mW range and below, powered by general purpose microcontroller units (MCUs) or application speciﬁc integrated circuits (ASICs) [1].
Figure 1. Comparison between RaScaNet and existing tiny models in terms of memory usage. RaScaNet not only ﬁts into SRAM but also takes advantage of cache memory.
Despite many successes of deep neural networks in com-puter vision applications [13, 20, 38], it is still challeng-ing to deploy a convolutional neural network (CNN) on an
MCU due to the on-chip memory restrictions and the prob-lems related to power consumption and latency [12, 21].
Typical microcontrollers only have limited on-chip memory of 100–320 KB (SRAM), and the peak memory of TinyML models should not exceed the on-chip SRAM. Moreover, it is highly recommended to maintain the sum of the peak memory and the weight memory below the size of on-chip SRAM. Otherwise, we have to fetch the partial model weights layer by layer from the ﬂash storage (256 KB–1
MB), which increases both read access time and cache miss ratio.
While previous works [22, 30] focused only on reducing the peak memory below the on-chip SRAM (320 KB), our method reduces both peak and weight memory signiﬁcantly even below 60 KB (Fig. 1). Such ultra-low memory foot-print is crucial for low-power platforms, as it reduces the number of accesses to memory.
Considering the objective of running TinyML with the limited on-chip memory and power, the conventional CNNs fetching a full-frame image at once would be inappropriate. 13673
(a) Conventional (b) Proposed
Figure 2. The pixels in image sensor are ﬁrst digitized by an analog-to-digital converter (ADC) in a raster-scan order.
Then, the converted pixel rows are stored in the buffer that can hold only k rows for image signal processing (ISP) al-gorithms. To process conventional CNNs, (a) the processed rows are transmitted to the applications, which have to wait until the whole image is received. On the other hand, (b)
RaScaNet can operate on sensor with minimum peak mem-ory by exploiting the data stream of an image sensor.
Even a small model is not free from the peak memory explo-sion due to the quadratic increase of memory requirements with respect to image size.
To tackle this problem, we propose a novel architecture, named RaScaNet, which processes non-overlapping sub-images sequentially instead of processing full-frame images as a whole. This aligns well with the modern image sensors that read a small number of rows at a time sequentially. By exploiting the data stream of an image sensor, RaScaNet can operate on sensor with minimum peak memory (Fig. 2).
To be speciﬁc, the proposed algorithm scans an input im-age from top to bottom using a CNN. Next, a series of the features obtained from the CNN are sequentially fed into a recurrent neural network (RNN) to learn the ﬁnal represen-tation of the image. In this model, the RNN learns semantic representations based on the low-level information encoded by the CNN.
Our contributions are summarized as below: 1. We propose a novel deep neural network architecture to ﬁt in the ultra-low power systems, called RaScaNet, which processes input images in a raster-scan manner. 2. RaScaNet achieves state-of-the-art Pareto efﬁciency in accuracy vs. memory; it requires 15.9–24.3× smaller peak memory and 5.3–12.9× smaller weight memory than the state-of-the-art tiny models while still present-ing competitive accuracy. 3. We design three components in RaScaNet—multi-head CNN, attention mechanisms, and conﬁdence loss—for its effectiveness on practical computer vision tasks. We also introduce an early termination scheme for further acceleration. 2.