Abstract
We present a learning-based method for synthesizing novel views of complex scenes using only unstructured col-lections of in-the-wild photographs. We build on Neural
Radiance Fields (NeRF), which uses the weights of a multi-layer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illu-mination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collec-tions taken from the internet. We apply our system, dubbed
NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view render-ings that are signiﬁcantly closer to photorealism than the prior state of the art. 1.

Introduction
Synthesizing novel views of a scene from a sparse set of captured images is a long-standing problem in computer vision, and a prerequisite to many AR and VR applications.
Though classic techniques have addressed this problem using structure-from-motion [11] or image-based rendering [30], this ﬁeld has recently seen signiﬁcant progress due to neural rendering techniques — learning-based modules embedded within a 3D geometric context, and trained to reconstruct observed images. The Neural Radiance Fields (NeRF) ap-proach [25] models the radiance ﬁeld and density of a scene with the weights of a neural network. Volume rendering is then used to synthesize new views, demonstrating a hereto-fore unprecedented level of ﬁdelity on a range of challenging scenes. However, NeRF has only been demonstrated to work
∗Denotes equal contribution. (a) Photos (b) Renderings
Figure 1: Given only an internet photo collection (a), our method is able to render novel views with variable illumination (b). Photos by Flickr users dbowie78, vasnic64, punch / CC BY. well in controlled settings: the scene is captured within a short time frame during which lighting effects remain con-stant, and all content in the scene is static. As we will demonstrate, NeRF’s performance degrades signiﬁcantly when presented with moving objects or variable illumina-tion. This limitation prohibits direct application of NeRF to large-scale in-the-wild scenarios, where input images may be taken hours or years apart, and may contain pedestrians and vehicles moving through them.
The central limitation of NeRF that we address here is its assumption that the world is geometrically, materially, and photometrically static — that the density and radiance of the world is constant. NeRF therefore requires that any two photographs taken at the same position and orientation must be identical. This assumption is severely violated in many real-world datasets, such as large-scale internet photo collec-tions of tourist landmarks. Two photographers may stand in the same location and photograph the same landmark, but in the time between those two photographs the world can change signiﬁcantly: cars and people may move, construc-tion may begin or end, seasons and weather may change, the sun may move through the sky, etc. Even two photos 7210
taken at the same time and location can exhibit considerable variation: exposure, color correction, and tone-mapping all may vary depending on the camera and post-processing. We will demonstrate that naively applying NeRF to in-the-wild photo collections results in inaccurate reconstructions that exhibit severe ghosting, oversmoothing, and further artifacts.
To handle these demanding scenarios, we present NeRF-W, an extension of NeRF that relaxes its strict consistency as-sumptions. First, we model per-image appearance variations such as exposure, lighting, weather, and post-processing in a learned low-dimensional latent space. Following the frame-work of Generative Latent Optimization [3], we optimize an appearance embedding for each input image, thereby grant-ing NeRF-W the ﬂexibility to explain away photometric and environmental variations between images by learning a shared appearance representation across the entire photo collection. The learned latent space provides control of the appearance of output renderings as illustrated in Figure 1, (b). Second, we model the scene as the union of shared and image-dependent elements, thereby enabling the unsuper-vised decomposition of scene content into “static” and “tran-sient” components. Our approach models transient elements using a secondary volumetric radiance ﬁeld combined with a data-dependent uncertainty ﬁeld, where the latter captures variable observation noise and further reduces the effect of transient objects on the static scene representation. Because optimization is able to identify and discount transient im-age content, we can synthesize realistic renderings of novel views by rendering only the static component.
We apply NeRF-W to several challenging in-the-wild photo collections of cultural landmarks and show that it can produce detailed, high-ﬁdelity renderings from novel view-points, surpassing the prior state of the art by a large margin on PSNR and MS-SSIM. Unlike prior work, renderings from our model exhibit smooth appearance interpolation and tem-poral consistency, even for wide camera trajectories. We ﬁnd that NeRF-W signiﬁcantly improves quality over NeRF in the presence of appearance variation and transient occluders while achieving similar quality in controlled settings. 2.