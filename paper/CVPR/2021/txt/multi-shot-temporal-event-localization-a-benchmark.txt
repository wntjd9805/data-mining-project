Abstract
Current developments in temporal event or action lo-calization usually target actions captured by a single cam-era. However, extensive events or actions in the wild may be captured as a sequence of shots by multiple cameras at different positions.
In this paper, we propose a new and challenging task called multi-shot temporal event lo-calization, and accordingly, collect a large-scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours. The core nature of MUSES is the frequent shot cuts, for an average of 19 shots per instance and 176 shots per video, which in-duces large intra-instance variations. Our comprehensive evaluations show that the state-of-the-art method in tem-poral action localization only achieves an mAP of 13.1% at IoU=0.5. As a minor contribution, we present a sim-ple baseline approach for handling the intra-instance vari-ations, which reports an mAP of 18.9% on MUSES and 56.9% on THUMOS14 at IoU=0.5. To facilitate research in this direction, we release the dataset and the project code at https://songbai.site/muses/. 1.

Introduction
Driven by the increasing number of videos generated, shared and consumed every day, video understanding has attracted greater attention in computer vision especially in recent years. As one of the pillars in video understand-ing, temporal event (or action) localization [10, 47, 61, 77, 84, 85, 87, 88] is a challenging task that aims to pre-dict the semantic label of an action, and in the meantime, locate its start time and end time in a long video. Au-tomating this process is of great importance for many ap-plications, e.g., security surveillance, home care, human-computer interaction, and sports analysis.
*Work done during an internship at Alibaba Group.
†Corresponding author (a) (c) (b) (d)
Figure 1. Different types of shot cuts, such as cutting on action (a), dissolve (b), cut-in (c), and cross-cut (d). Clickable: best viewed with Adobe Acrobat Reader; click to watch the animation.
While fruitful progress has been made in this ﬁeld, our work focuses on a real-world scenario that has not been se-riously treated so far, that is, localizing events in TV shows and movies. We cast it as a new research topic named multi-shot temporal event localization, which is detailed below: 1) Motivation: Our work is motivated by some commer-cial applications of multi-shot temporal event localization, where truncated videos are automatically generated from
TV shows and movies. For example, a trailer is generated for presenting the highlight to tease the audience, a video summary is generated to help the viewer grasp the impor-tant plot points and characters, or a mashup is generated for blending multiple clips of the same theme. To achieve this, a basic but time-consuming step is to ﬁnd the materi-als, i.e., the video segments of interest, while the develop-ment of multi-shot temporal event localization will enable efﬁcient material extraction and greatly improve the produc-tivity of video content generation. 2) Characteristic: Compared with user-generated videos or surveillance videos, a unique characteristic of TV shows and movies is the highly frequent shot cuts. Herein, 12596
Conversation
Stroll
Cooking
…
…
…
✂
✂
✂
…
…
…
✂
✂
✂
…
…
…
Shot 2
Figure 2. Examples of multi-shot events. In each row, we show three consecutive shots in an instance and select two frames per shot for illustration. The scissor icons indicate the shot boundaries.
Shot 1
Shot 3 a shot means a single sequence of video frames taken by one camera without interruption1. Because of the use of multi-camera shooting and professional editing techniques, a complete action or event in such videos is usually ex-pressed as a sequence of meaningful short shots connected by cuts of various types, such as cutting on action, dissolve, cut-in, and cross-cut (see Fig. 1 for examples).
In other words, the termination of a shot does not mean the end of the corresponding event or action. 3) Challenge: The key challenge of localizing events in
TV shows and movies is the large intra-instance variation, induced by the nature of shot cuts. As can be observed in
Fig. 2, the view angles and the depth of ﬁelds across shots vary dramatically. Meanwhile, due to the existence of shot cuts, some side effects occur, such as scene change, actor change, and heavy occlusions. With such large variations within a single instance, the difﬁculty of localizing a com-plete event across shots is signiﬁcantly increased. 4) Our Contribution: To facilitate the study of multi-shot temporal event localization, the main contribution of this work is a collection of a large-scale dataset named
MUlti-Shot EventS (MUSES). Unlike existing datasets (e.g., THUMOS14 [30], ActivityNet-1.3 [8], HACS Seg-ments [86]) that are mainly built upon user-generated videos, the data source of MUSES is drama videos pro-cessed by professional editing techniques of the entertain-ment industry. Each instance is deﬁned and annotated as an individual event that may take place across multiple shots.
As a consequence, the number of shots in MUSES is as great as 19 per instance and 176 per video. MUSES is also a large-scale dataset that is suitable for training deep learn-1Note that the meaning of “shot” in our work is different from those in one-shot/zero-shot/few-shot learning [16]. ing models, which contains 31,477 instances for a total of 716 video hours.
We conduct a comprehensive evaluation of state-of-the-art methods on MUSES, including P-GCN [85], G-TAD [76] and MR [87]. The results show that current meth-ods cannot well handle the large intra-instance variations.
The best performance is achieved by P-GCN [85], which is 13.1% mAP at IoU=0.5. It reveals the difﬁculty of our dataset in temporal event localization, and in the meantime, dramatically exposes the necessity of exploring methods speciﬁcally focusing on capturing intra-instance variations.
As a minor contribution of this work, we present a simple baseline approach for multi-shot temporal event localiza-tion. The proposed baseline achieves an mAP of 18.9% on
MUSES and 56.9% on THUMOS14 [30] at IoU=0.5. The dataset and the project code are publicly available for fellow researchers. 2.