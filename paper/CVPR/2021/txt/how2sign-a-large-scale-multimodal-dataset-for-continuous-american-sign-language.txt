Abstract 1.

Introduction
One of the factors that have hindered progress in the ar-eas of sign language recognition, translation, and produc-tion is the absence of large annotated datasets. Towards this end, we introduce How2Sign, a multimodal and mul-tiview continuous American Sign Language (ASL) dataset, consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modal-ities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic stu-dio enabling detailed 3D pose estimation. To evaluate the potential of How2Sign for real-world impact, we conduct a study with ASL signers and show that synthesized videos us-ing our dataset can indeed be understood. The study further gives insights on challenges that computer vision should ad-dress in order to make progress in this ﬁeld.
Dataset website: http://how2sign.github.io/
*Corresponding authors: {amanda.duarte,xavier.giro}@upc.edu
Sign languages (SL) are the primary means of commu-nication for an estimated 466 million deaf 1 or hard-of-hearing people worldwide [1]. Like any other natural lan-guage, sign languages are consistently evolving and have structure directed by a set of linguistic rules [3]. They dif-fer from spoken languages and do not have standard written forms, e.g. American Sign Language (ASL) is not a sign form of English. Although sign languages are used by mil-lions of people everyday to communicate, the vast majority of communications technologies nowadays are designed to support spoken or written language, but not sign languages.
At the same time, most hearing people do not know a sign language; as a result, many communication barriers exist for deaf sign language users [6, 7, 14]. 1We follow the recognized convention of using the upper-cased word
Deaf which refers to the culture and describes members of the community of sign language users and the lower-cased word deaf describes the hearing status[37]. 12735
Promising recent works in sign language processing2
[12, 30, 33, 41, 40, 19] have shown that modern computer vision and machine learning architectures can help break down these barriers for sign language users.
Improving such models could make technologies that are primarily designed for non-sign language users, e.g. voice-activated services, text-based systems, spoken-media based content, etc., more accessible to the Deaf community. Other pos-sibilities include automatic transcription of signed content, which would help facilitating the communication between sign and non-sign language users, as well as real-time in-terpreting when human interpreters are not available, and many other educational tools and applications [6].
However, training such models requires large amounts of data. The availability of public large-scale datasets suit-able for machine learning is very limited, especially when it comes to continuous sign language datasets, i.e., where the data needs to be segmented and annotated at the sentence level. Currently, there is no ASL dataset large enough to be used with recent deep learning approaches.
In order to instigate the advance in the area of re-search that involves sign language processing, in this pa-per we introduce the How2Sign dataset. How2Sign is a large-scale collection of multimodal and multiview sign lan-guage videos in American Sign Language (ASL) for over 2500 instructional videos selected from the existing How2 dataset [27]. Figure 1 shows samples of the data contained in the dataset. Working in close collaboration with na-tive ASL signers and professional interpreters, we collected more than 80 hours of multi-view and multimodal (recorded with multiple RGB and a depth sensor) ASL videos, and corresponding gloss annotations [22]. In addition, a three-hour subset was further recorded at the Panoptic studio [17], a geodesic dome setup equipped with hundreds of cameras and sensors, which enables detailed 3D reconstruction and pose estimation. This subset paves the way for vision sys-tems to understand the 3D geometry of sign language.
Our contributions can be summarized as follows: a) We present How2Sign, a large-scale multimodal and multiview continuous American Sign Language dataset that consists of more than 80 hours of American Sign Language videos, with sentence-level alignment for more than 35k sentences.
It features a vocabulary of 16k English words that represent more than two thousand instructional videos from a broad range of categories; b) Our dataset comes with a rich set of annotations including gloss, category labels, as well auto-matically extracted 2D keypoints for more than 6M frames.
What is more, a subset of the dataset was re-recorded in the
Panoptic studio with more than 500 cameras that enabled high quality 3D keypoints estimation for around 3 hours of 2For brevity, we follow [6] and use the term sign language processing to refer to the set of sign language recognition, translation and production tasks. videos; c) We conduct a study with ASL signers that showed that videos generated using our dataset can be understood to a certain extent, and at the same time gave insights on chal-lenges that the research community can address in this ﬁeld. 2.