Abstract 2D image representations are in regular grids and can be processed efﬁciently, whereas 3D point clouds are unordered and scattered in 3D space. The information inside these two visual domains is well complementary, e.g., 2D images have
ﬁne-grained texture while 3D point clouds contain plentiful geometry information. However, most current visual recog-nition systems process them individually. In this paper, we present a bidirectional projection network (BPNet) for joint 2D and 3D reasoning in an end-to-end manner. It contains 2D and 3D sub-networks with symmetric architectures, that are connected by our proposed bidirectional projection mod-ule (BPM). Via the BPM, complementary 2D and 3D infor-mation can interact with each other in multiple architectural levels, such that advantages in these two visual domains can be combined for better scene recognition. Extensive quan-titative and qualitative experimental evaluations show that joint reasoning over 2D and 3D visual domains can beneﬁt both 2D and 3D scene understanding simultaneously. Our
BPNet achieves top performance on the ScanNetV2 bench-mark for both 2D and 3D semantic segmentation. Code is available at https://github.com/wbhu/BPNet. 1.

Introduction
Scene understanding is a fundamental while challenging problem in computer vision. Multiple sensors are used to capture the scene information. The 2D camera is the most common sensor in our daily life. It projects the 3D space to image planes with plenty of ﬁne-grained textures captured. 2D images with pixels densely arranged in regular grids can be processed efﬁciently with deep convolutional neural net-works. We have witnessed remarkable improvements on 2D visual reasoning, e.g., image classiﬁcation [28, 53, 48, 17] and semantic segmentation [35, 4, 66, 72]. 3D sensors, on the other hand, can provide important geometry information of the scene. 3D data is usually represented in points that
*Equal contribution.
†Corresponding author. are unordered and irregularly scattered in 3D space. Con-ventional convolution that relies on ordered grids can not be directly adapted to 3D data. Hence, several tailor-made neu-ral networks [41, 32, 12] have been proposed for 3D scene recognition and understanding.
We observe that the information inside 2D and 3D data is well complementary. 2D images provide detailed texture and color information while 3D point clouds contain strong shape and geometry knowledge. Although the techniques for indi-vidual 2D and 3D reasoning are studied a lot in the literature, the exploration of combining both 2D and 3D data for recog-nition is very limited. Existing methods that utilize both 2D and 3D data for recognition mostly adopt the unidirectional scheme. For example, to incorporate 3D information for 2D scene understanding, some methods either encode depth into geocentric inputs [13] or incorporate it into convolution operations [44, 60]. But depth map only contains limited geometry information as it is view-dependent. The occluded part under the viewpoint together with the global context is missing. In the other aspect, 3DMV [8] and MVPNet [24] utilize 2D information to assist the 3D recognition by ﬁrst extracting multi-view image features and then lifting them into 3D space for fusing with the 3D features. However, we argue that the unidirectional scheme cannot fully leverage the complementary information inside the 2D and 3D data, bidirectionally interacting and fusing 2D and 3D features can better combine the advantages of these two visual domains as evidenced by our experiments.
In this paper, we present a Bidirectional Projection Net-work (BPNet) to enable information inside the 2D and 3D domains to ﬂow bidirectionally at the network architectural level. Such that the complementary information can be well combined for joint 2D and 3D scene understanding in an end-to-end manner. Our method adopts two similar U-Net structures to process 2D and 3D data and introduces a Bidi-rectional Projection Module (BPM) to bidirectionally fuse the multi-view 2D and 3D features. In this way, both 2D and 3D sides can beneﬁt from each other. The overall frame-work is shown in Figure 1. To be mentioned, we employ
BPM at multiple pyramid levels, such that the features from 14373
2D and 3D domains can be aggregated in a coarse-to-ﬁne manner and the BPNet can harvest both low- and high-level complementary information. At each level, BPM builds the projection link matrix between 2D and 3D, and then trans-fers the features bidirectionally according to the link matrix, i.e., 2D features are projected into 3D space for boosting the recognition of 3D and vice the verse.
We evaluated our model on ScanNetV2 [7] dataset for both 2D and 3D semantic segmentation tasks. BPNet achieves top performance on the benchmark in terms of mIoU and consistently outperforms the baseline with a sin-gle 2D/3D network. Also, the qualitative results show the effectiveness of combining 2D and 3D information. BPNet can distinguish objects without much shape difference (e.g.,
“wall” and “picture”) in 3D segmentation. Meanwhile, 2D objects are better segmented with sharper boundaries thanks to the underlying geometric clues provided by 3D features.
Besides, we evaluated the generalization ability of BPNet for 2.5D data on the typical RGB-D dataset, NYUv2 [38], and the results show BPNet performs favorably against the typical RGB-D and joint 2D-3D baselines. We believe the proposed BPM is also advantageous to other tasks where both 2D and 3D resources are available, e.g., classiﬁcation, detection, and instance segmentation. Our contributions are summarized below.
• We argue that 2D and 3D information is complementary for the understanding of each other and joint optimiza-tion over both 2D and 3D scenes is applicable and proved to be beneﬁcial.
• We propose a Bidirectional Projection Module (BPM) that enables information interacting between the 2D and 3D representations. And such bidirectional projec-tion operation can be adopted at multiple levels in the decoder stage.
• We present a novel framework named Bidirectional Pro-jection Network (BPNet) for jointly reasoning over 2D and 3D scenes. Our method achieves top performance on the challenging large-scale ScanNetv2 benchmark for both 2D and 3D semantic segmentation tasks, which demonstrates its effectiveness. 2.