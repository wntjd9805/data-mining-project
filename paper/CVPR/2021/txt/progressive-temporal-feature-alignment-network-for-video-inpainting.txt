Abstract
Video inpainting aims to ﬁll spatio-temporal “cor-rupted” regions with plausible content. To achieve this goal, it is necessary to ﬁnd correspondences from neigh-bouring frames to faithfully hallucinate the unknown con-tent. Current methods achieve this goal through attention,
ﬂow-based warping, or 3D temporal convolution. However,
ﬂow-based warping can create artifacts when optical ﬂow is not accurate, while temporal convolution may suffer from spatial misalignment. We propose ‘Progressive Temporal
Feature Alignment Network’, which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical ﬂow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual qual-ity and temporal consistency of the inpainted videos. Us-ing the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github.com/MaureenZOU/TSAM . 1.

Introduction
Video inpainting is a task that aims to ﬁll missing regions in video frames with plausible content [3]. It has a wide range of applications including corrupted video restora-tion, watermark/logo removal, object removal, etc. To ﬁll the “holes”, it is ideal to inpaint them with corresponding known content from neighbouring frames, which can well approximate the missing region. For any missing pixels that lack good correspondence due to e.g., occlusion, the video inpainting method must hallucinate reasonable content.
Existing state-of-the-art video inpainting methods rely on extracting useful information from neighbouring frames, and are based on three main directions: 3D temporal convo-lution [6, 7, 14], optical ﬂow [8, 37], and attention [39, 26].
The general structure of existing 3D convolution mod-els for video inpainting consists of a fully convolutional
*Work mostly done during an internship at ByteDance Inc. generator to predict the inpainted result holistically and a Temporal-Patch GAN discriminator to enforce temporal smoothness and frame realism [7, 6]. However, these meth-ods simply stack feature maps from neighboring frames to encode 3D temporal information as an additional axis, without considering the movement of objects across those frames. This leads to spatial misalignment in the features, which can cause issues for video inpaiting. For example, as shown in the pink circle on the second row of Fig. 1, the panda leg is not successfully inpainted when using 3D convolutions [6]. In order to accurately predict structural (i.e., edge/shape) details, video inpainting models require spatially-aligned feature maps for each timestamp. This motivates us to propose a feature alignment framework to overcome these challenges, with inspirations from ﬂow-based approaches.
Recent ﬂow-based approaches [8, 37] ﬁrst compute op-tical ﬂow from the corrupted video frames, and then com-plete the unknown regions using ﬂow-based inpainting tech-niques. Using the computed ﬂow, pixels in corrupted re-gions are propagated from adjacent frames. Further, im-age inpainting techniques such as [38] are applied to com-plete the remaining content. Although optical ﬂow meth-ods are good at spatial content alignment and are able to inpaint video frames with higher resolution compared to at-tention or 3D convolution models, any errors in optical ﬂow (especially in the missing regions) can prevent the mod-els from capturing ﬁne-grained structural details. For ex-ample, as shown in Fig. 1, the telephone pole in the ﬁrst row using FGVC [8], a state-of-the-art optical ﬂow based approach, is not straight compared to 3D convolution ap-proaches (FFVI [6] and ours). In addition, image inpainting techniques might generate unwanted content that does not match the ground truth content. As shown in the ﬁrst row of Fig. 1, the purple circle denotes an area that could not be handled by propagated pixels. FGVC generates a car on the grass that does not exist in the original video.
We hereby propose a novel framework called Progres-sive Temporal Feature Alignment Network to combine the advantages and offset the weaknesses of temporal con-volution frameworks and optical ﬂow based warping ap-16448
(a) FGVC (b) FFVI (c) Ours (d) GT
Figure 1. This ﬁgure shows a qualitative comparison of our approach with ﬂow-based method FGVC [8] and 3D convolution based method
FFVI [6]. We try to inpaint the gray regions shown in (d). The results show that our method generates content that is both more structure-preserving and visually appealing. proaches. Our method is an end-to-end deep network with a novel temporal shift-and-aligned module (TSAM), in which features between neighbouring frames are aligned using optical ﬂow. In order to extract aligned feature rep-resentations across different scales, we progressively ap-ply TSAM to feature maps in different scales at different network depths in a coarse-to-ﬁne manner. Fig. 1 shows that our method produces satisfactory results in challenging cases in terms of spatial alignment, resolution, and coarse to
ﬁne-grained structures. Through extensive experiments, we demonstrate that our method achieves state-of-the-art per-formance on two video inpainting benchmarks FVI (subset of YoutubeVOS) [6] and DAVIS [4]. 2.