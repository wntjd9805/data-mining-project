Abstract
Learning based representation has become the key to the success of many computer vision systems. While many 3D representations have been proposed, it is still an unad-dressed problem how to represent a dynamically changing 3D object. In this paper, we introduce a compositional rep-resentation for 4D captures, i.e. a deforming 3D object over a temporal span, that disentangles shape, initial state, and motion respectively. Each component is represented by a latent code via a trained encoder. To model the motion, a neural Ordinary Differential Equation (ODE) is trained to update the initial state conditioned on the learned motion code, and a decoder takes the shape code and the updated state code to reconstruct the 3D model at each time stamp.
To this end, we propose an Identity Exchange Training (IET) strategy to encourage the network to learn effectively de-coupling each component. Extensive experiments demon-strate that the proposed method outperforms existing state-of-the-art deep learning based methods on 4D reconstruc-tion, and signiﬁcantly improves on various tasks, including motion transfer and completion. 1.

Introduction
Shape representation is one of the core topics in 3D com-puter vision, especially in the era of deep learning. Early work uses explicit representation, e.g. volume [13, 17, 58], point cloud [46, 16, 45, 1], and mesh [18, 23, 57] for 3D related tasks, such as shape reconstruction, synthesis, and completion. Recently, deep implicit representation
[33, 40, 22] shows promising performance in producing ac-curate geometry with appealing surface details. However, arguably, we, humans, stay in a 3D world with an addi-tional temporal dimension, and the majority of data we per-ceive everyday are moving or deforming 3D objects and scenes. Many existing applications also require understand-∗ indicates equal contributions.
Boyan Jiang Xingkui Wei and Xiangyang Xue are with the School of
Computer Science, Fudan University.
Yanwei Fu is with the School of Data Science, MOE Frontiers Cen-ter for Brain Science, and Shanghai Key Lab of Intelligent Information
Processing, Fudan University. geometry  template initial state temporal  deformation
Figure 1: We present a compositional representation for 4D object dynamics, through which the input point cloud se-quence is disentangled into semantically meaningful rep-resentations in three latent spaces for geometry template, initial state, and temporal deformation. ing or reconstruction of 4D data, such as autonomous driv-ing, robotics, and virtual or augmented reality. But the deep representation for 4D data, i.e. a deforming 3D object over a time span, is barely missing in the literature. As a pio-neer work, Niemeyer et al. [37] propose to predict velocity
ﬁeld of the 3D motion via a Neural ODE [11]. However, the method mainly focuses on recovering and integrating local
ﬂow for 4D reconstruction, which might accumulate error and thus produce sub-optimal quality.
In this work, we propose a novel deep compositional representation for 4D captures. This representation can be used to reconstruct 4D captures, and it also extracts key understanding that supports high-level tasks, such as mo-tion transfer, 4D completion, or future prediction. This is achieved by an encoder that takes a 4D capture as input and produces latent codes representing the geometry template, 5340
initial state, and temporal deformation respectively. Taking human as an example, these three key factors are commonly understood as the identity, initial body pose, and motion1.
To reconstruct the 4D capture, we design a novel archi-tecture taking three latent codes as inputs. First, we keep the geometry template code (i.e. the identity) unchanged over time since it is not affected by the motion. Then, we propose a novel conditional latent Neural ODE to update the initial state code (i.e. the initial body pose) conditioned on the de-formation code (i.e. the motion). The temporally varying state code is further concatenated with the geometry tem-plate code, and fed into a decoder to reconstruct an implicit occupancy ﬁeld for each time frame, which recovers the 3D shape over time. Mostly similar to us, Occupancy Flow [37] also use Neural ODE [11] to update the position of each 3D point for 4D reconstruction. In contrast, our method applies the Neural ODE to update the latent state code that controls the shape globally, which is empirically more stable.
To learn our compositional representation, we propose a training strategy to enable the encoder to decouple the geometry template and deformation, inspired by He et al.
[19]. Speciﬁcally, we take two 4D captures from different subjects and extract their latent codes respectively. We then swap their geometry template code and train the network to reconstruct the motion with swapped geometry template.
The training is fully supervised by synthetic data, where the parametric model is used to generate 4D captures with the same motion but different geometry template, e.g. SMPL model [28] for humans. We found this training strategy is effective in separating geometry template from the motion, which naturally supports motion transfer. The representa-tion also enables 4D completion from captures with either missing frames or partial geometry by solving an optimiza-tion to update the latent codes until the partial observation is best explained.
Our contributions can be summarized as follows. First, we design a novel deep representation for 4D captures that understands the geometry template, initial state, and tem-poral deformation, and propose a novel training strategy to learn it. Second, we propose a novel decoder to recon-struct 4D captures from the learned representation, which includes, as a key component, a conditional Neural ODE to recover varying pose codes under the guidance of the mo-tion code; and these codes are then translated into an occu-pancy ﬁeld in implicit representation to recover the varying shape. Finally, we show that our model outperforms state-of-the-art methods on 4D reconstruction, and our composi-tional representation is naturally suitable for various appli-cations, including motion transfer and 4D completion. 1Since the experiment is mainly conducted on 4D human captures, we use these terms interchangeably. 2.