Abstract
In this paper, we present DRANet, a network architecture that disentangles image representations and transfers the visual attributes in a latent space for unsupervised cross-domain adaptation. Unlike the existing domain adaptation methods that learn associated features sharing a domain,
DRANet preserves the distinctiveness of each domain’s characteristics. Our model encodes individual representa-tions of content (scene structure) and style (artistic appear-ance) from both source and target images. Then, it adapts the domain by incorporating the transferred style factor into the content factor along with learnable weights spec-iﬁed for each domain. This learning framework allows bi-/multi-directional domain adaptation with a single encoder-decoder network and aligns their domain shift. Addition-ally, we propose a content-adaptive domain transfer mod-ule that helps retain scene structure while transferring style.
Extensive experiments show our model successfully sepa-rates content-style factors and synthesizes visually pleasing domain-transferred images. The proposed method demon-strates state-of-the-art performance on standard digit clas-siﬁcation tasks as well as semantic segmentation tasks. 1.

Introduction
The use of deep neural networks (DNN) has led to sig-niﬁcant performance improvements in a variety of areas, including computer vision [6], machine learning [13], and natural language processing [7]. However, problems re-main, particularly domain gaps between data, which can signiﬁcantly degrade model performance. Extensive efforts have been made to generalize the models across domains using unsupervised domain adaptation [1, 38, 23, 36, 9, 32, 37, 21, 2, 15, 39]. Unsupervised domain adaptation attempts to align the distribution shift in labeled source data with un-labeled target data. Various strategies have been explored to bridge the gap across domains, for example, by feature learning and generative pixel-level adaptation.
*Corresponding author. (a) Traditional domain adaptation [9, 15] (b) Linear feature separation [42] and domain adaptation (c) Our feature separation and domain adaptation (DRANet)
Figure 1. Illustration of DRANet and the competitive methods (do-main adaptation [9, 15], representation disentanglement [42]. Note that E, S, and G are an encoder, a separator, and a generator.
Feature-level methods [38, 23, 32, 36, 9, 32, 37] learn features that combine task-discrimination and domain-invariance, where both domains are mapped into a com-mon feature space. Domain invariance typically involves minimizing some feature distance metric [38, 23, 32] or adversarial discriminator accuracy [9]. Pixel-level ap-proaches [21, 2] perform a similar distribution alignment, not in the raw pixel space by leveraging the power of Generative Adversarial Networks (GANs) [14, 24, 28, 30, 4]. They adapt source domain im-ages so that they appear as if drawn from the target domain. in a feature space but 15252
Some studies [15, 35, 39] incorporate both pixel-level and feature-level approaches to achieve complementary bene-ﬁts.
Recently, the ﬁeld of study has been further advanced by learning disentangled representations into the exclusive and shared components in a latent feature space [3, 12, 22, 45].
They demonstrate that representation disentanglement im-proves a model’s ability to extract domain invariant features, as well as the domain adaptation performance. However, these methods still focus on the associated features between two domains such as shared and exclusive components, so they require multiple encoders and generators specialized in individual domains. Moreover, the network training relies heavily on a task classiﬁer with ground-truth class labels, in addition to domain classiﬁers.
To tackle these issues, we propose DRANet, a single feed-forward network, that does not require any ground-truth task labels for cross-domain adaptation. In contrast to previous approaches in Fig. 1-(a) that map all domain im-ages into a shared feature space, we focus on extracting the domain-speciﬁc features that preserve individual domain characteristics in Fig. 1-(c). Then, we disentangle the dis-criminative features of individual domains into the content and style components using a separator, which are later used to generate the domain-adaptive features. Unlike the previ-ous feature separation work [42], which linearly divides la-tent vectors into two components in Fig. 1-(b), our separa-tor is tailored to disentangle latent variables in a nonlinear manifold. Our intuition behind the network design is that different domains may have different distributions for their contents and styles, which cannot be effectively handled by the linear separation of latent vectors. Thus, to handle such difference, our network adopts the non-linear separation and domain-speciﬁc scale parameters that are dedicated to han-dle such inter-domain difference.
To the best of our knowledge, DRANet is the ﬁrst ap-proach based solely on the individual domain characteris-tics for unsupervised cross-domain adaptation. It enables us to apply a single encoder-decoder network for a multi-directional domain transfer from fully unlabeled data. The distinctive points of our approach are summarized as fol-lows:
• We present DRANet, which disentangles image rep-resentation and adapts the visual attributes in a latent space to align the domain shift.
• We propose a content-adaptive domain transfer mod-ule that helps to synthesize realistic images of com-plex segmentation datasets, such as CityScapes [5] and
GTA5 [29].
• We demonstrate that images synthesized by our ap-proach boost the task performances and achieve state-of-the-art performance on standard digit classiﬁcation tasks as well as semantic segmentation tasks. 2.