Abstract
Few-shot learning is a challenging task, which aims to learn a classiﬁer for novel classes with few examples. Pre-training based meta-learning methods effectively tackle the problem by pre-training a feature extractor and then ﬁne-tuning it through the nearest centroid based meta-learning.
However, results show that the ﬁne-tuning step makes very
In this paper, 1) we ﬁgure out marginal improvements. the key reason, i.e., in the pre-trained feature space, the base classes already form compact clusters while novel classes spread as groups with large variances, which im-plies that ﬁne-tuning the feature extractor is less meaning-ful; 2) instead of ﬁne-tuning the feature extractor, we focus on estimating more representative prototypes during meta-learning. Consequently, we propose a novel prototype com-pletion based meta-learning framework. This framework
ﬁrst introduces primitive knowledge (i.e., class-level part or attribute annotations) and extracts representative attribute features as priors. Then, we design a prototype completion network to learn to complete prototypes with these priors.
To avoid the prototype completion error caused by primitive knowledge noises or class differences, we further develop a Gaussian based prototype fusion strategy that combines the mean-based and completed prototypes by exploiting the unlabeled samples. Extensive experiments show that our method: (i) can obtain more accurate prototypes; (ii) out-performs state-of-the-art techniques by 2% 9% in terms of classiﬁcation accuracy. Our code is available online 1.
∼ 1.

Introduction
Humans can adapt to a novel task from only a few ob-servations, because our brains have the excellent capability of learning to learn. In contrast, modern artiﬁcial intelli-gence (AI) systems generally require a large amount of an-notated samples to make the adaptations. However, prepar-∗Corresponding author 1https://github.com/zhangbq-research/Prototype_
Completion_for_FSL (a) Base Classes (σ2 = 0.086)
Incomplete 
Complete  (b) Novel Classes (σ2 = 0.099)
Figure 1. The distribution of base and novel class samples in the pre-trained feature space. “σ2” denotes the averaged variance. ing sufﬁcient annotated samples is often laborious, expen-sive, or even unrealistic in some applications, for example, cold-start recommendation [25] and drug discovery [1]. To equip the AI systems with such human-like ability, few-shot learning (FSL) becomes an important and widely studied problem. Different from conventional machine learning,
FSL aims to learn a classiﬁer from a set of base classes with abundant labeled samples, then adapt to a set of novel classes with few labeled data [28].
Existing studies on FSL roughly fall into four categories, namely the metric-based methods [4], optimization-based methods [8], graph-based methods [21], and semantics-based methods [29]. Though their methodologies are to-tally different, almost all methods address the FSL prob-lem by a two-phase meta-learning framework, i.e., meta-training and meta-test phases. Recently, Chen et al. [6] ﬁnd that introducing an extra pre-training phase can signiﬁcantly boost the performance. In this method, a feature extractor
ﬁrst is pre-trained by learning a classiﬁer on the entire base classes. Then, the metric-based meta-learning is adopted to
ﬁne-tune it. In the meta-test phase, the mean-based proto-types are constructed to classify novel classes via a nearest neighbor classiﬁer with cosine distance.
Though the pre-training based meta-learning method achieves promising improvements, Chen et al. ﬁnd that the
ﬁne-tuning step indeed makes very marginal contributions
[6]. However, the reason is not revealed in [6]. To ﬁgure out 3754
the reason, we visualize the distribution of base and novel class samples of the miniImagenet in the pre-trained feature space in Figure 1. We ﬁnd that the base class samples form compact clusters while the novel class samples spread as groups with large variances. It means that 1) ﬁne-tuning the feature extractor to gather the base class samples into more compact clusters is less meaningful, because this enlarges the probability to overﬁt the base tasks; 2) the given few labeled samples may be far away from its ground-truth cen-ters in the case of large variances for novel classes, which poses a great challenge for estimating representative proto-types. Hence, in this paper, instead of ﬁne-tuning the fea-ture extractor, we focus on how to estimate representative prototypes from the few labeled samples, especially when these samples are far away from its ground-truth centers.
Recently, Xue et al. [30] also attempt to address a similar problem by learning a mapping function from noisy sam-ples to their ground-truth centers. However, learning to re-cover representative prototypes from noisy samples without any priors is very difﬁcult. Moreover, the method does not leverage the pre-training strategy. Thus, the performance
In this paper, we improvement of the method is limited.
ﬁnd that the samples deviated from its ground-truth cen-ters are often incomplete, i.e., missing some representative attribute features. As shown in Figure 1(b), the meerkat sample nearby the class center contains all the representa-tive features, e.g., the head, body, legs and tail, while the ones far away may miss some representative features. This means that the prototypes estimated by the samples deviated from its centers may be incomplete.
Based on this fact, we propose a novel prototype com-pletion based meta-learning framework. Our framework works in a pre-training manner and introduces some prim-itive knowledge, e.g., whether a class object should have ears, legs or eyes, as priors to achieve the prototype com-pletion. Speciﬁcally, we ﬁrst extract the visual features for each part/attribute, by aggregating the pre-trained feature representations of all the base class samples that have the corresponding attribute in our primitive knowledge. Sec-ond, we mimic the setting of few-shot classiﬁcation task and construct a set of prototype completion tasks. A Prototype
Completion Network (ProtoComNet) is then designed to learn to complete representative prototypes with the prim-itive knowledge and visual attribute features. To avoid the prototype completion error caused by primitive knowledge noises or base-novel class differences, we further design a
Gaussian-based prototype fusion strategy, which effectively combines the mean-based and completed prototypes by ex-ploiting the unlabeled data. Finally, the few-shot classiﬁca-tion is achieved via a nearest neighbor classiﬁer. Our main contributions of this paper can be summarized as follows:
•
We reveal the reason why the feature extractor ﬁne-tuning step contributes marginally to the pre-training based meta-learning methods, and point out that repre-sentative prototype estimation is a more critical issue.
•
•
We propose a novel prototype completion based meta-learning framework, which can effectively learn to re-cover representative prototypes by leveraging primi-tive knowledge and unlabeled data.
We have conducted comprehensive experiments on three real-world data sets. The experimental results demonstrate that the proposed method outperforms the state-of-the-art techniques by 2% 9% in terms of classiﬁcation accuracy.
∼ 2.