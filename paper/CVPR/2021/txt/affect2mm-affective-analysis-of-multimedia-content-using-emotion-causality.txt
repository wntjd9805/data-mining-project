Abstract
We present Affect2MM, a learning method for time-series emotion prediction for multimedia content. Our goal is to automatically capture the varying emotions depicted by characters in real-life human-centric situations and behav-iors. We use the ideas from emotion causation theories to computationally model and determine the emotional state evoked in clips of movies. Affect2MM explicitly models the temporal causality using attention-based methods and
Granger causality. We use a variety of components like fa-cial features of actors involved, scene understanding, visual aesthetics, action/situation description, and movie script to obtain an affective-rich representation to understand and perceive the scene. We use an LSTM-based learning model for emotion perception. To evaluate our method, we analyze and compare our performance on three datasets, SENDv1,
MovieGraphs, and the LIRIS-ACCEDE dataset, and ob-serve an average of 10 − 15% increase in the performance over SOTA methods for all three datasets. 1.

Introduction
In affective computing, perceiving the emotions con-veyed in images and videos has found applications in digi-tal content management [30, 73], digital marketing [42, 25, 80], education [17, 2], and healthcare [14]. Such applica-tions have resulted in automated ranking systems, indexing systems [72, 75, 32], and more personalized movie recom-mendation systems [49]. Affective analysis of movies has been a problem of interest in the community [58, 29], along similar lines. In our work, we explore the problem of affec-tive analysis of movies with the goal of understanding the emotions that the movies invoke in the audience.
There has been a growing interest [51] in dynami-cally modeling the emotions over time (‘time series emo-tion recognition’ among the affective computing commu-nity. This underlying problem uses temporally continuous data (facial features, speech feature, or other modality fea-tures) from multimedia content as input and predicts the emotion labels at multiple timestamps (clips) of the input.
Figure 1: Time-Series Emotion Perception Model: We present
Affect2MM, a learning model for time-series emotion perception for movies. We input a multimedia content in the form of multi-ple clips and predict emotion labels for each clip. Affect2MM is based on the theory of emotion causation and also borrows idea for temporal causality. We show some example clip-frames from the movie ‘Titanic’, a part of the MovieGraphs Dataset and corre-sponding emotion labels.
To aid in solving this time-series problem, several time-series emotion datasets have been proposed [44, 25, 69, 35, 6, 51]. While these datasets focus more on single-person emotional narratives recorded in controlled settings, multi-media datasets (movie databases) like LIRIS-ACCEDE [8] and MovieGraphs [71] (annotated for per-clip emotion la-bels) are also being explored for time-series emotion per-ception tasks.
There have been various efforts to understand how hu-mans reason and interpret emotions resulting in various the-ories of emotion causation based on physiological, neuro-logical, and cognitive frameworks. One such theory is the
“emotional causality” [15] that has been developed from the
Causal Theory of Perception [27] and Conceptual Metaphor
Theory [4, 48, 37]. “Emotional Causality” refers to the un-derstanding that an experience of emotion is embedded in a chain of events comprising of an (a) outer event; (b) an emo-tional state; and (c) a physiological response. Few works have explored such emotional causality for emotion percep-tion in multimedia tasks.
Movies, as a time-series multimedia content, model mul-15661
tiple human-centric situations and are temporally very long, but coherent sequences. To be able to reason about emo-tions invoked at various clips of the movie, it is important to develop a causal understanding of the story. Generic meth-ods of handling such temporality include recurrent neu-ral networks [38, 63], attention-mechanisms [12], graph modeling [23], and statistical methods like Granger causal-ity [21]. Explicit modeling of causality in the context of time-series emotion perception has been relatively unex-plored.
Emotion labels have been explored extensively, both as discrete [36] and continuous [46], in affective analysis. The
Valence-Arousal-Dominance (VAD) model [46] is used for representing emotions in a continuous space on a 3D plane with independent axes for valence, arousal, and dominance values. The Valence axis indicates how pleasant (vs. un-pleasant) the emotion is; the Arousal axis indicates how high (or low) the physiological intensity of the emotion is, and the dominance axis indicates how much the emotion is tied to the assertion of high (vs. low) social status. A combination of 3 values picked from each axis represents a categorical emotion like ‘angry’ or ‘sad’, much like how an (x, y, z) point represents a physical location in 3-D Eu-clidean space. Various transformations [45, 24] can be used to map discrete emotion labels to the VAD space. In this work, we work with both continuous emotion labels and discrete emotion labels.
Main Contributions: The following are the novel con-tributions of our work. 1. We present Affect2MM, a learning-based method for capturing the dynamics of emotion over time. Af-fect2MM aligns with the psychological theory of
“emotional causality” to better model the emotions evoked by each clip of a movie. 2. To better model the temporal causality in movies for long-range multimedia content like movies, we use attention methods and Granger causality to explic-itly model the temporal causality (between clips in movies). Our approach can be used for predicting both continuous emotion labels (valence and arousal) and also discrete class labels.
We evaluate our method on two movie datasets,
MovieGraphs [71] and the LIRIS-ACCEDE [8] dataset. To showcase our method’s generalizability, we also evaluate and compare our method on the SENDv1 [51] dataset, a single-person emotional narratives dataset. 2.