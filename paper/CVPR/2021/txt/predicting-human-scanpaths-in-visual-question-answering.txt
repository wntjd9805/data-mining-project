Abstract
Image
Question:	Is	the	vase	the	same	color	as	the	scarf?
Attention has been an important mechanism for both humans and computer vision systems. While state-of-the-art models to predict attention focus on estimating a static probabilistic saliency map with free-viewing behavior, real-life scenarios are ﬁlled with tasks of varying types and com-plexities, and visual exploration is a temporal process that contributes to task performance. To bridge the gap, we con-duct a ﬁrst study to understand and predict the temporal se-quences of eye ﬁxations (a.k.a. scanpaths) during perform-ing general tasks, and examine how scanpaths affect task performance. We present a new deep reinforcement learn-ing method to predict scanpaths leading to different perfor-mances in visual question answering. Conditioned on a task guidance map, the proposed model learns question-speciﬁc attention patterns to generate scanpaths. It addresses the exposure bias in scanpath prediction with self-critical se-quence training and designs a Consistency-Divergence loss to generate distinguishable scanpaths between correct and incorrect answers. The proposed model not only accurately predicts the spatio-temporal patterns of human behavior in visual question answering, such as ﬁxation position, dura-tion, and order, but also generalizes to free-viewing and vi-sual search tasks, achieving human-level performance in all tasks and signiﬁcantly outperforming the state of the art. 1.

Introduction
Visual attention plays an essential role in everyday tasks.
While existing works focus on stimulus-driven attention with free-viewing behavior, underlying daily tasks is an-other form of attention, i.e., task-driven attention, that se-lects task-relevant information to make a decision or to ac-complish a task. Besides, beyond the static saliency map that highlights the relative importance of a visual input, tem-poral sequences of eye ﬁxations encode a more comprehen-sive and natural representation of attention. Understand-ing and predicting visual scanpaths in general tasks will not only shed light on the decision-making process but also be a useful tool for a variety of computer vision applications.
Correct	Answer:	no
Incorrect	Answer:	failed
Figure 1. Visual scanpaths of humans can reveal their decision-making strategies and explain their performance. Those who pay attention to relevant visual cues can achieve high levels of task per-formance. This example compares the scanpaths of people who succeed or fail to answer a question, where the dots represent ﬁx-ations. The number and radius indicate the ﬁxation order and du-ration, respectively. The blue and red dots indicate the beginning and the end of the scanpath, respectively.
Task-driven visual scanpaths reﬂect the visual explo-ration to accomplish the task, which also strongly correlates with task performance. As an example (Fig. 1), to answer the question “Is the vase the same color as the scarf?” while exploring the scene, humans need to actively explore the scene and search for the vase and the scarf. While looking at the right places at the right time would usually lead to correct answers (Fig. 1, middle), failing to do so may result in incorrect answers (Fig. 1, right).
As a step toward understanding and modeling general task-driven attention, we propose a novel deep reinforce-ment learning method leveraging task guidance as an im-portant modality to predict the visual exploration behav-ior of humans performing general tasks. We ﬁrst intro-duce a task guidance map to specify task-relevant image regions. The map is designed and demonstrated to gener-alize across tasks. To address the exposure bias that arises between training- and test-time contexts, we introduce a re-inforcement learning method that directly optimizes non-differentiable test-time evaluation metrics [14]. To differ-entiate eye-movement patterns that lead to different perfor-10876
mances, we further introduce a novel loss function to ac-count for the consistency and divergence between correct and incorrect scanpaths.
Our work has three distinctions from previous scanpath prediction studies: (1) While state-of-the-art scanpath pre-diction studies focus on free-viewing [4, 5, 13, 40] or well-structured tasks such as visual search [52], this paper for the ﬁrst time studies the complex scanpath patterns in gen-eral decision-making tasks, and investigates the correlation of scanpaths and performances in this context. (2) Scanpath prediction has not been as popular (compared with saliency prediction) or achieved excellent performance (compared with humans), partly due to the exposure bias – the discrep-ancy between training-time and test-time contexts. Here we close the gap using self-critical sequence training in the reinforcement learning method, leading to signiﬁcantly boosted performance that is better than humans. (3) We go beyond a single task and design a new mechanism to encode general task-relevant information that is easily adaptable to other tasks with varying nature and levels of complexity.
The proposed method has been demonstrated by three tasks with human-level performance.
In sum, this work makes the following contributions: 1. We develop a deep reinforcement learning model to understand and predict scanpaths in the general task-driven context with visual question answering (VQA).
Task performance is for the ﬁrst time taken into ac-count to predict scanpaths. 2. We propose to explicitly integrate attention maps from task-speciﬁc deep neural network models, allowing the encoding of task-relevant information as well as pro-viding an alternative to measure the interpretability of task-speciﬁc models through analyzing model vs. hu-man attention. 3. To address the discrepancy between training and test-ing that may have limited the development of scanpath prediction methods, we apply self-critical sequence training to directly optimize non-differentiable evalu-ation metrics. We further introduce a novel loss func-tion to learn discriminative features and differentiate correct and incorrect scanpaths. 4. The proposed method signiﬁcantly outperforms the state-of-the-art and shows human-level performance on three tasks: VQA, free-viewing, and visual search, demonstrating the generalizability of the method. 2.