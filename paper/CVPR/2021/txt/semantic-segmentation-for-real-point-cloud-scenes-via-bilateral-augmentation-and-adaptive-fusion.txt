Abstract
Given the prominence of current 3D sensors, a ﬁne-grained analysis on the basic point cloud data is worthy of further investigation. Particularly, real point cloud scenes can intuitively capture complex surroundings in the real world, but due to 3D data’s raw nature, it is very challeng-ing for machine perception. In this work, we concentrate on the essential visual task, semantic segmentation, for large-scale point cloud data collected in reality. On the one hand, to reduce the ambiguity in nearby points, we augment their local context by fully utilizing both geometric and seman-tic features in a bilateral structure. On the other hand, we comprehensively interpret the distinctness of the points from multiple resolutions and represent the feature map follow-ing an adaptive fusion method at point-level for accurate semantic segmentation. Further, we provide speciﬁc abla-tion studies and intuitive visualizations to validate our key modules. By comparing with state-of-the-art networks on three different benchmarks, we demonstrate the effective-ness of our network. 1.

Introduction
As 3D data acquisition techniques develop rapidly, dif-ferent types of 3D scanners, e.g. LiDAR scanners [22] and
RGB-D cameras [10] are becoming popular in our daily life. Basically, 3D scanners can capture data that enables
AI-driven machines to better see and recognize the world.
As a fundamental data representation, point clouds can be easily collected using 3D scanners, retaining abundant in-formation for further investigation. Therefore, point cloud analysis is playing an essential role in 3D computer vision.
Research has shown great success in terms of basic clas-siﬁcation of small-scale point clouds (i.e., objects contain-ing a few thousand points): for example, face ID [16] is now a widely used bio-identiﬁcation for mobile devices.
Researchers have recently been investigating a ﬁne-grained analysis of large and complex point clouds [44, 26, 19, 48]
Figure 1: Examples of semantic segmentation for real point cloud scenes, where the main differences are highlighted and zoomed-in. The upper row shows an indoor working environment with
∼0.9 million points: RandLA-Net [19] falsely classiﬁes the wall around the room corner, while our result is much closer to the ground-truth. The lower row is an outdoor trafﬁc scene containing
∼32 thousand points, where a small bike on the right is correctly identiﬁed by our network (in blue), while RandLA-Net mislabels it as vegetation (in green). because of the tremendous potential in applications such as autonomous driving, augmented reality, robotics, etc. This paper focuses on the semantic segmentation task to identify each point’s semantic label for real point cloud scenes.
Although there are many notable works [41, 35, 55] ad-dressing the semantic segmentation of 2D images which have a simpler structure, point clouds are scattered, irregu-lar, unordered, and unevenly distributed in 3D space, mak-ing the corresponding task much more challenging, espe-cially for large scenes made of millions or even billions of points collected from the real world. To deal with the 3D data, many papers try to build data-driven models using deep learning. Speciﬁcally, Guo et al. [13] sum-marizes the Convolutional Neural Network (CNN) mod-els targeting point clouds into three streams: projection-based, discretization-based, and point-based methods. As a projection-based example, Lawin et al. [27] virtually projects 3D point clouds onto images and applies a con-ventional FCN [35] to analyze the 2D multi-view repre-sentations. Similarly, the discretization-based approaches model point clouds as voxels [20] or lattices [42] for CNN processing, and ﬁnally interpolate the semantic results back 1757
to the original input. However, the mentioned methods are not optimal for real applications due to some common is-sues: ﬁrstly, they require several time-consuming pre/post-processing steps to make predictions; and secondly, the gen-erated intermediate representations may partially lose the context of the surroundings.
To avoid the above issues, in this paper, we prefer point-based networks (details in Sec. 2) that directly process the points for ﬁne-grained analysis. Moreover, for an accurate semantic segmentation on real point cloud scenes, we en-deavor to resolve the major drawbacks of existing works:
Ambiguity in close points. Most current solutions [45, 11, 40] represent a point based on its pre-deﬁned neighbors via a ﬁxed metric like Euclidean distance. However, outliers and overlap between neighborhoods during the neighbor-hood’s construction are difﬁcult to avoid, especially when the points are closely distributed near the boundaries of dif-ferent semantic classes. To alleviate possible impacts, we attempt to augment the local context by involving a dense region. Moreover, we introduce a robust aggregation pro-cess to reﬁne the augmented local context and extract useful neighboring information for the point’s representation.
Redundant features. We notice an increasing number of works [19, 50, 39] combine similar features multiple times to enhance the perception of the model. In fact, this pro-cess causes redundancy and increases the complexity for the model to process large-scale point clouds. To avoid the above problems, we propose to characterize the input in-formation as geometric and semantic clues and then fully utilize them through a bilateral structure. More compactly, our design can explicitly represent complex point clouds.
Inadequate global representations. Although some ap-proaches [38, 34, 29] apply an encoder-decoder [3] struc-ture to learn the sampled point cloud; the output feature map is inadequate for a ﬁne-grained semantic segmentation anal-ysis since the global perception of the original data would be damaged during the sampling process. In our method, we intend to rebuild such perception by integrating information from different resolutions. Moreover, we adaptively fuse multi-resolutional features for each point to obtain a com-prehensive representation, which can be directly applied for semantic prediction.
To conclude, our contributions are in these aspects:
• We introduce a bilateral block to augment the local context of the points.
• We adaptively fuse multi-resolutional features to ac-quire comprehensive knowledge about point clouds.
• We present a novel semantic segmentation network using our proposed structures to deal with real point cloud scenes.
• We evaluate our network on three large-scale bench-marks of real point cloud scenes. The experimental results demonstrate that our approach achieves com-petitive performances against state-of-the-art methods. 2.