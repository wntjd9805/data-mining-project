Abstract
Despite substantial progress in applying neural networks (NN) to a wide variety of areas, they still largely suffer from a lack of transparency and interpretability. While re-cent developments in explainable artiﬁcial intelligence at-tempt to bridge this gap (e.g., by visualizing the correlation between input pixels and ﬁnal outputs), these approaches are limited to explaining low-level relationships, and cru-cially, do not provide insights on error correction. In this work, we propose a framework (VRX) to interpret classiﬁ-cation NNs with intuitive structural visual concepts. Given a trained classiﬁcation model, the proposed VRX extracts relevant class-speciﬁc visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the rea-soning process of NNs and provide logical, concept-level explanations for ﬁnal model decisions. With extensive ex-periments, we empirically show VRX can meaningfully an-swer “why” and “why not” questions about the prediction, providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN’s performance. 1.

Introduction
With the use of machine learning increasing dramati-cally in recent years in areas ranging from security [3] to medicine [31], it is critical that these neural network (NN) models are transparent and explainable as this relates di-rectly to an end-user’s trust in the algorithm [12, 1]. Con-sequently, explainable AI (xAI) has emerged as an impor-tant research topic with substantial progress in the past few years. Most recent xAI approaches attempt to explain NN decision reasoning process with visualizations depicting the correlation between input pixels ( or low-level features) and
Figure 1. An example result with the proposed VRX. To explain the prediction (i.e., ﬁre engine and not alternatives like ambu-lance), VRX provides both visual and structural clues. Colors of visual concepts (numbered circles) and structural relationships (ar-rows) represent the positive or negative contribution computed by
VRX to the ﬁnal decision (see color scale inset). (a): The four de-tected concepts (1-engine grill, 2-bumper, 3-wheel, 4-ladder) and their relationships provide a positive contribution (blue) for ﬁre engine prediction. (b, c): Unlike (a), the top 4 concepts, and their relationships, for ambulance/school bus are not well matched and contribute negatively to the decision (green/yellow/red colors). the ﬁnal output [40, 23, 42, 37, 30, 35, 18, 4, 32, 29], with perturbation-based [32, 29] and gradient-based [30, 4] methods receiving particular attention in the community.
Despite impressive progress, we identify some key limita-tions of these methods that motivate our work. First, the resulting explanations are limited to low-level relationships and are insufﬁcient to provide in-depth reasoning for model inference. Second, these methods do not have systematic 2195
processes to verify the reliability of the proposed model ex-planations [17, 9]. Finally, they do not offer guidance on how to correct mistakes made by the original model.
We contend that explaining the underlying decision rea-soning process of the NN is critical to addressing the afore-mentioned issues. In addition to providing in-depth under-standing and precise causality of a model’s inference pro-cess, such a capability can help diagnose errors in the orig-inal model and improve performance, thereby helping take a step towards building next-generation human-in-the-loop
AI systems. To take a step towards these goals, we propose the visual reasoning explanation framework (VRX) with the following key contributions:
• To understand what an NN pays attention to, given an input image, we use high-level category-speciﬁc visual concepts and their pairwise relationships to build struc-tural concepts graphs (SCGs) that help to highlight spatial relationships between visual concepts. Further-more, our proposed method can in-principle encode higher-order relationships between visual concepts.
• To explain an NN’s reasoning process, we propose a
GNN-based graph reasoning network (GRN) frame-work that comprises a distillation-based knowledge transfer algorithm between the original NN and the
GRN. With SCGs as input, the GRN helps optimize the underlying structural relationships between concepts that are important for the original NN’s ﬁnal decision, providing a procedure to explain the original NN.
• Our proposed GRN is designed to answer inter-pretability questions such as why and why not as they relate to the original NN’s inference decisions, helping provide systematic veriﬁcation techniques to demonstrate the causality between our explanations and the model decision. We provide qualitative and quantitative results to show efﬁcacy and reliability.
• As a useful by-product, in addition to visual reasoning explanations, our method can help take a step towards diagnosing reasons for any incorrect predictions and guide the model towards improved performance. 2.