Abstract
While mesh saliency aims to predict regional importance of 3D surfaces in agreement with human visual perception and is well researched in computer vision and graphics, lat-est work with eye-tracking experiments shows that state-of-the-art mesh saliency methods remain poor at predicting human ﬁxations. Cues emerging prominently from these ex-periments suggest that mesh saliency might associate with the saliency of 2D natural images. This paper proposes a novel deep neural network for learning mesh saliency using image saliency ground truth to 1) investigate whether mesh saliency is an independent perceptual measure or just a derivative of image saliency and 2) provide a weakly super-vised method for more accurately predicting mesh saliency.
Through extensive experiments, we not only demonstrate that our method outperforms the current state-of-the-art mesh saliency method by 116% and 21% in terms of linear correlation coefﬁcient and AUC respectively, but also re-veal that mesh saliency is intrinsically related with both im-age saliency and object categorical information. Codes are available at https://github.com/rsong/MIMO-GAN . 1.

Introduction
Mesh saliency, ﬁrst proposed by the seminal paper of
Lee et al. [16], measures regional importance of 3D surfaces in accordance with human visual perception. While many methods [5, 23, 25, 26, 17] for mesh saliency have been presented since then, recent eye-tracking work [34, 33, 15] shows that state-of-the-art mesh saliency methods are poor at predicting human ﬁxations. In particular, Lavou´e et al.
[15] found that even a simple centre-bias model, a prior
*Corresponding author widely used for predicting saliency of 2D natural images, generated better results for various 3D meshes than the state-of-the-art mesh saliency methods including [16, 26, 19, 17]. Apart from the centre bias, mesh saliency and im-age saliency also have other characteristics in common. For instance, it was found that some features such as facial areas of people or animals always attract human ﬁxations no mat-ter whether they are expressed by 2D images or 3D meshes.
Image saliency is mainly driven by colour and texture while the detection of mesh saliency relies largely on object geometry. But the ﬁndings above give us an impression that despite such a fundamental difference, mesh saliency might be a derivative of image saliency rather than an independent perceptual measure. To explore this proposition, we pro-poses to learn mesh saliency from ground-truth saliency of general 2D images. In addition, it has been shown that 3D objects of the same category usually have similar saliency distributions [2, 15]. One explanation is that the information vital for object classiﬁcation is usually also important for saliency as it can help humans to recognise an object swiftly without the need for scrutinizing its details [27]. There-fore, considering that there already exist large-scale public datasets for image saliency (e.g. SALICON Dataset [10],
MIT Saliency Benchmark [1] and DUT-OMRON Dataset
[38]) and 3D object classiﬁcation (e.g. ModelNet [37] and
ShapeNet [21]), we present a weakly supervised deep neu-ral network for mesh saliency trained jointly with saliency maps of 2D images and category labels of 3D objects.
Importantly, such a weakly supervised method is poten-tially of broad interest as gathering eye-ﬁxation data for 3D objects is a notoriously laborious task [12, 34, 33, 15]. To the best of our knowledge, all existing ﬁxation datasets for mesh saliency are very small (e.g. 5 objects in [12], 15 ob-jects in [34], 16 objects in [33] and 32 objects in [15]). The 8853
consequence of using such a small dataset to train a neural network that cannot be sufﬁciently deep (for avoiding over-ﬁtting) is that it usually failed to generalise across a diver-sity of objects [33]. In this paper, we shall demonstrate that with the training data of image saliency and object category labels, our weakly supervised method accurately predicts ground-truth ﬁxations of various 3D objects. Speciﬁcally, in the view-dependent set-up, our method outperforms the state-of-the-art mesh saliency method by 116% and 21% in terms of linear correlation and AUC respectively on the cur-rently largest ﬁxation dataset [15] for mesh saliency.
The core of the proposed method is a Multi-Input Multi-Output Generative Adversarial Network (MIMO-GAN). It contains two input-output paths: a regression path for pixel-level saliency prediction and a classiﬁcation path for object-level recognition. The two paths essentially enable transfer learning from image saliency and 3D object classiﬁcation to mesh saliency. And, since projected 2D views of 3D meshes appear highly different from 2D natural scene images, we propose to use a GAN architecture so that transfer learning is compelled to minimise the gap between image saliency and mesh saliency as much as possible.
Overall, the contribution of our work is threefold:
• We propose a novel method for mesh saliency trained with image saliency and object category labels in a weakly supervised manner and thus does not need the expensive collection of human ﬁxations for 3D objects.
• We reveal and validate that 1) image saliency helps predict mesh saliency even though 2D natural images appear highly different from projected 2D views of 3D meshes and 2) mesh saliency also associates with class membership of meshes.
• We demonstrate that our method signiﬁcantly out-performs existing state-of-the-art approaches to mesh saliency on publicly available datasets in both view-dependent and independent set-ups. 2.