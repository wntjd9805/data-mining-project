Abstract
In real-world image enhancement, it is often challeng-ing (if not impossible) to acquire ground-truth data, pre-venting the adoption of distance metrics for objective qual-ity assessment. As a result, one often resorts to subjec-tive quality assessment, the most straightforward and re-liable means of evaluating image enhancement. Conven-tional subjective testing requires manually pre-selecting a small set of visual examples, which may suffer from three sources of biases: 1) sampling bias due to the extremely sparse distribution of the selected samples in the image space; 2) algorithmic bias due to potential overﬁtting the selected samples; 3) subjective bias due to further potential cherry-picking test results. This eventually makes the ﬁeld of real-world image enhancement more of an art than a sci-ence. Here we take steps towards debiasing conventional subjective assessment by automatically sampling a set of adaptive and diverse images for subsequent testing. This is achieved by casting sample selection into a joint maxi-mization of the discrepancy between the enhancers and the diversity among the selected input images. Careful visual inspection on the resulting enhanced images provides a de-biased ranking of the enhancement algorithms. We demon-strate our subjective assessment method using three popu-lar and practically demanding image enhancement tasks: dehazing, super-resolution, and low-light enhancement. 1.

Introduction
For many years, image enhancement has been investi-gated in an unrealistic setting, with the assumption that the original images of perfect quality exist to help evaluate vi-sual quality of the enhanced images. This promotes the use of full-reference image quality metrics [69] to compute an average distance between a large set of enhanced and orig-inal image pairs as an indication of enhancement perfor-mance. Along this path, many full-reference metrics have been proposed [71, 54, 82, 9], trying to measure this dis-tance more perceptually.
However, in real-world image enhancement, it is of-ten difﬁcult (if not impossible) to specify desired outputs.
Moreover, there may be multiple diverse outputs that are desirable, as in the case of super-resolution [75]. Therefore, full-reference models that rely on a single “ideal” image are not applicable. Some attempts have been made to adopt no-reference models [70] for performance assessment of real-world enhancement. However, no-reference objective as-sessment is still in its infancy, and accurate models for (spe-ciﬁc or general) image enhancement applications are largely lacking. Currently, the most widely used no-reference met-ric - NIQE [45] - was empirically proven to correlate poorly with human quality judgments of the enhanced images [45], which exhibit unique and algorithm-speciﬁc artifacts that are often non-overlapping with natural distortions.
Alternatively, one may refer to subjective quality assess-ment, which is so far the most straightforward and reliable way of evaluating real-world image enhancement because the ultimate receiver in most such applications is the human eye. Conventional subjective assessment typically takes a four-step approach. First, pre-select a number of images from the input domain of a given image enhancement prob-lem. Second, pick a set of competing enhancers, and gener-ate the corresponding output images. Third, ask humans to rate the perceived quality of the enhanced images. Fourth, compare the enhancers according to the subjective results.
Unfortunately, conventional subjective assessment may suffer from three sources of biases. The ﬁrst is the sam-pling bias. The underlying principle of conventional sub-jective assessment is to prove an enhancement method to be correct. This would require the set of pre-selected im-ages to be large enough to sufﬁciently represent the input domain of interest. However, subject testing is an expen-sive and time-consuming endeavor. In practice, the number of images being examined is limited to a few hundreds (if not fewer), casting doubt on the assumption of sufﬁcient sampling in the high-dimensional image space. The second is the algorithmic bias. It is important to note that the se-lection of test images precedes the selection of competing methods. One may take advantage of this (intentionally or unintentionally), and tunes her/his enhancer to overﬁt the pre-selected images, drawing overly optimistic conclusions 711
on the real-world generalization performance. The third is the subjective bias. That is, the test results may further be cherry-picked to bias towards certain methods. In summary, it is sad, but not uncommon, to see that a “state-of-the art” image enhancer produces superior results in its original pa-per, but remains particularly weak at handling examples ap-peared in subsequent work.
In this paper, we contribute to debiasing conventional subjective assessment by injecting an automated, adap-tive and sample-efﬁcient mechanism to select input domain samples. Our inspirations are drawn from interdisciplinary prior work on “model falsiﬁcation as model comparison”, a renowned philosophy in the ﬁelds of computational vision
[72], software testing [43] and computer vision [47, 40, 62].
Speciﬁcally, we start from a large-scale image set as a ﬁnite approximation to the input space of an image enhancement application. According to the available human labelling budget, our method automatically selects a set of adaptive and diverse images for subsequent subjective testing. The selected images are optimal in terms of discriminating be-tween the enhancers, while having the maximum within-group variation in a latent space to ensure content diversity.
Subjective results of the corresponding enhanced images reveal the advantages and disadvantages of the competing methods, and provide a debiased ranking of their relative performance. Our subjective assessment method is appli-cable to a wide variety of image processing and computer photography subﬁelds, and we choose three real-world im-age enhancement applications as demonstration: 1) single image dehazing, 2) single image super-resolution, and 3) low-light image enhancement. 2.