Abstract
There have been many successful implementations of neural style transfer in recent years. In most of these works, the stylization process is conﬁned to the pixel domain. How-ever, we argue that this representation is unnatural because paintings usually consist of brushstrokes rather than pixels.
We propose a method to stylize images by optimizing param-eterized brushstrokes instead of pixels and further introduce a simple differentiable rendering mechanism.
Our approach signiﬁcantly improves visual quality and en-ables additional control over the stylization process such as controlling the ﬂow of brushstrokes through user input.
We provide qualitative and quantitative evaluations that show the efﬁcacy of the proposed parameterized repre-sentation. Code is available at https://github. com / CompVis / brushstroke - parameterized -style-transfer. 1.

Introduction
Style and texture transfer have been research topics for decades [17, 9]. More recently, the seminal work by Gatys
[11] reformulated style transfer as the synthesis of et al. an image combining content of one image with style of another image. Since then, a plethora of approaches have explored different aspects of the original problem. There are papers on feed-forward architectures [23, 48], universal feed-forward models [18, 32, 33, 31], disentanglement of style and content [44, 28, 29], ultra-resolution models [50], meta-learning techniques [45, 56], and video style transfer
[4]. Yet, the initial approach suggested by Gatys et al. [11] remains one of the best in terms of image quality, especially in the artistic style transfer scenario, with one style image and one content image.
Recent works have advanced the ﬁeld of style trans-fer and produced impressive results by introducing novel losses [34, 43, 44], adopting more suitable architectures
[23, 48, 18, 32], imposing regularizations on the ﬁnal im-age and intermediate latent representation [44, 28, 29, 47],
*Both authors contributed equally to this work.
Figure 1: Stylization results. Top artwork: “Girl on a Di-van” by Ernst Ludwig Kirchner. Bottom artwork: “Red
Cabbages and Onions” by Vincent van Gogh. and even using different training paradigms [45, 56]. How-the stylization pro-ever, they share a key commonality: 12196
cess is conﬁned to the pixel domain, almost as if style transfer is a special case of image-to-image translation
[21, 58, 51, 36, 19, 37, 6, 7, 25]. We argue that the pixel rep-resentation is unnatural for the task of artistic style transfer: artists compose their paintings with brushstrokes, not with individual pixels. While position, color, shape, placement and interaction of brushstrokes play an important role in the creation of an artwork, small irregularities appearing on the pixel level like bristle marks, canvas texture or pigments are to some extent arbitrary and random.
With this in mind, we take a step back and rethink the original approach by suggesting a representation that inher-ently aligns with these characteristics by design.
Just like learning to walk in the reinforcement learning set-ting starts with deﬁning the set of constraints and degrees of freedom for individual joints, we restrict our representation to a collection of brushstrokes instead of pixels. Speciﬁ-cally, we parameterize a brushstroke with a B´ezier curve and additional parameters for color, width, and location.
To map these parameterized brushstrokes into the pixel do-main we propose a lightweight, explicit, differentiable ren-derer which serves as a mapping between brushstroke pa-rameters and pixels. Thus, this reparameterization can be seamlessly combined with other style transfer approaches.
One crucial property that this rendering mechanism offers is a spatial relocation ability of groups of pixels. Standard optimization on the pixel level cannot directly move pix-els across the image - instead it dims pixels in one area and highlights them in another area. Our model, however, parameterizes brushstrokes with location and shape, thus moving brushstrokes becomes a more natural transforma-tion.
We validate the effectiveness of this reparameterization by coupling the renderer with the model Gatys et al. [11] have suggested, see Fig. 4. We show that this simple shift of representation along with our rendering mechanism can outperform modern style transfer approaches in terms of stylization quality. This is measured using 1) the decep-tion rate - how similar is the stylized image to the style of an artist 2) human deception rate - whether a human sub-ject can distinguish cropouts of real artworks from cropouts of our stylization. In addition, we illustrate that the brush-stroke representation offers more control. A user can con-trol brushstrokes, change the ﬂow of strokes in a neighbour-hood.
We further conduct experiments on reconstructions of an image using our rendering mechanism. Huang et al.
[20] train a neural network that successively ﬁts colored quadratic B´ezier curves (brushstrokes) that approximate a target image. Our renderer can be applied to this task as well. It achieves almost 2 times smaller mean squared er-ror (MSE) in the pixel space for a large number of strokes (1000 strokes) and 20% smaller MSE using 200 strokes. 2.