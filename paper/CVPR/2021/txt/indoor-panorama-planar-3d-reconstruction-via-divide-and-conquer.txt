Abstract (cid:43)(cid:82)(cid:85)(cid:76)(cid:93)(cid:82)(cid:81)(cid:87)(cid:68)(cid:79) (cid:57)(cid:72)(cid:85)(cid:87)(cid:76)(cid:70)(cid:68)(cid:79)
Indoor panorama typically consists of human-made struc-tures parallel or perpendicular to gravity. We leverage this phenomenon to approximate the scene in a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this end, we propose an effective divide-and-conquer strategy that divides pixels based on their plane orientation estimation; then, the succeeding instance segmentation module conquers the task of planes clustering more easily in each plane ori-entation group. Besides, parameters of V-planes depend on camera yaw rotation, but translation-invariant CNNs are less aware of the yaw change. We thus propose a yaw-invariant
V-planar reparameterization for CNNs to learn. We create a benchmark for indoor panorama planar reconstruction by extending existing 360 depth datasets with ground truth
H&V-planes (referred to as “PanoH&V” dataset) and adopt state-of-the-art planar reconstruction methods to predict
H&V-planes as our baselines. Our method outperforms the baselines by a large margin on the proposed dataset. 1.

Introduction
Reconstructing planar surfaces from single-view images have many applications such as interior modeling, AR/VR, robot navigation, and scene understanding. Generally, an indoor scene consisting of human-made structures can be approximated by a small set of dominant planes, making planar reconstruction suitable for 3D indoor modeling.
Recent works on planar reconstruction [17, 18, 20] em-ploy state-of-the-art instance segmentation methods and achieve promising results. However, these works are mostly trained on the planar datasets derived from ScanNet [6] or
NYUv2 [23] with a small ﬁeld-of-view (FoV). Data of this kind require multiple images to reconstruct entire scenes, 1National Tsing Hua University 2ASUS AICS Department 3Joint Research Center for AI Technology and All Vista Healthcare 4Aeolus Robotics (cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:53)(cid:42)(cid:37) (cid:51)(cid:79)(cid:68)(cid:81)(cid:72)(cid:3)(cid:44)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:54)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:14)(cid:3)(cid:51)(cid:79)(cid:68)(cid:81)(cid:72)(cid:3)(cid:51)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)
Figure 1: Planar surfaces of human-made structures are mostly horizontal or vertical with respect to the gravity di-rection. Given an RGB panorama, we propose to model a 3D scene as horizontal or vertical planes (the H&V-planes). which would cost more computational time and resources.
As 360° devices get popularized, the amount of 360° data has signiﬁcantly increased, with many panorama datasets be-ing released to facilitate learning-based methods. By taking input in 360° format, 3D reconstruction of an entire scene can be done with only one snapshot. Considering the beneﬁt of real-world 360° data in planar reconstruction and the gap of existing literature in this research ﬁeld, we believe the planar reconstruction task from panorama imagery is worthy of investigation.
In this work, we construct the ﬁrst real-world indoor 360° H&V-plane dataset (PanoH&V dataset), where we sim-plify the planar reconstruction task by focusing on horizontal and vertical planes (illustrated in Fig. 1). To this end, we extend existing large-scale 360° datasets by ﬁtting the pro-vided depth modality with horizontal and vertical planes.
The H&V-planes are similar to the concept of the Manhattan world [5] and the Atlanta world [22], but we only constrain the extracted planes to be vertical or horizontal without as-suming other inter-plane relationship. With the extended modality from large-scale H&V-planes, we train two cur-rent state-of-the-art planar reconstruction methods, PlaneR-CNN [17] and PlaneAE [31], and report their performance to serve as the strong baselines in our benchmark.
We ﬁnd existing planar reconstruction methods subopti-mal when applied to the presented PanoH&V dataset. First, existing methods employ instance segmentation to detect planes from visual cue with less consideration about the estimated geometry. In practice, some planes are easier to differentiate through geometry instead of visual appearance 11338
(e.g., walls with similar appearance). Second, plane parame-ters depend on camera poses, but 360° camera yaw rotations are left-right circular shifts on equirectangular images and hard for the translation-invariant CNNs to observe. In con-trast to the existing works, our method addresses the above issues appropriately: i) We use a divide-and-conquer strategy.
The proposed surface orientation grouping distinguishes pix-els of different plane orientations, so the succeeding instance segmentation module applied to each group can focus on a simpler subproblem. ii) We then propose a residual form yaw-invariant parameterization for V-planar geometry such that it is independent of the camera yaw rotation. We show that the yaw-invariant parameterization brings signiﬁcant improvements in V-plane orientations estimation, which also beneﬁts other methods.
We summarize the contribution of this work in two as-In terms of technical contribution, the proposed pects. method consists of i) a divide-and-conquer strategy for the task of plane instance segmentation, which exploits the esti-mated plane orientations to divide the task into multiple sim-pler subproblems; ii) a yaw-invariant vertical plane param-eterization addressing the 360° yaw ambiguity, which can also boost other existing methods. In terms of system con-tribution, we construct a new real-world 360° piece-wise planar benchmark, which focuses on evaluating horizontal and vertical planes. Finally, our approach outperforms the two strong baselines adapted from existing state-of-the-art planar reconstruction methods on the new benchmark. 2.