Abstract
Style transfer between images is an artistic application of CNNs, where the ‘style’ of one image is transferred onto another image while preserving the latter’s content. The state of the art in neural style transfer is based on Adaptive
Instance Normalization (AdaIN), a technique that transfers the statistical properties of style features to a content im-age, and can transfer a large number of styles in real time.
However, AdaIN is a global operation; thus local geometric structures in the style image are often ignored during the transfer. We propose Adaptive Convolutions (AdaConv), a generic extension of AdaIN, to allow for the simultaneous transfer of both statistical and structural styles in real time.
Apart from style transfer, our method can also be readily extended to style-based image generation, and other tasks where AdaIN has already been adopted. 1.

Introduction
In recent years, convolutional neural networks (CNNs) have been used to explore and manipulate the style of im-ages. Image style is often deﬁned by image features such as overall color and local structure of brush strokes, in the context of paintings, or the pose and expression of a face in generative image applications. Style is also deﬁned at different resolutions, and thus can include both the global identity of a face as well as the local structure of freckles on the skin. Research in this area gained a lot of momentum with the advent of neural style transfer, originally proposed by Gatys et al. [8], where a CNN is trained to reproduce the content of one input image, but rendered with the style of another image. In a similar spirit, generative adversarial networks (GANs) have been used to produce realistic syn-thetic images with style deﬁned by a random vector input, for example in the creation of synthetic face images [18].
The widespread approach for manipulating style is through adaptive instance normalization (AdaIN), a method that transforms the mean and variance of image features.
For example, AdaIN is often used to transfer feature statis-tics of a style image onto a content image. Since its deﬁ-nition by Huang et al. in 2017 [13], this operation has al-ready become commonplace in CNN-based image manipu-lation literature. One major drawback of AdaIN, however, 7972
is that the statistic computation is a global operation; thus localized spatial structure in the style cannot be effectively captured and transferred. A concrete example is shown in
Fig. 1 (row 1) where the style image has distinct features like black and white circles and squares. The AdaIN result transfers the statistics of that image to the content images, but the result lacks any structure of the style. A similar phe-nomenon can be seen in row 2, for a different style image.
In this work we introduce an extension to AdaIN called
Adaptive Convolutions (AdaConv), which allows for the simultaneous adaptation of both statistical and structural style. In the context of style transfer, instead of transfer-ring a simple pair of global statistics (mean and standard deviation) from each style feature, our approach estimates full convolution kernels and bias values from the style im-age, which are then convolved on the features of the con-tent image. As these kernels better capture localized spatial structure in the style, AdaConv can more faithfully transfer structural elements of the style image to the content image, as illustrated in Fig. 1 (columns 4 and 7).
The concept of predicting convolution kernels for deep learning tasks has already shown some promise in ﬁelds such as video frame interpolation [26, 27, 28] and denois-ing [1, 35]. Here we leverage this idea to extend AdaIN for more general image style manipulation. AdaConv can re-place AdaIN in virtually every application where the latter has been adopted, providing a new, generic building block in CNN-based image generation and style manipulation. To illustrate the generality of AdaConv, we demonstrate its ap-plication in both style transfer as well as style-based gener-ative face modeling (StyleGAN [18]). 2.