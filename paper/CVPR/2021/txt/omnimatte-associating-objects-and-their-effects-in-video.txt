Abstract 1.

Introduction
Computer vision is increasingly effective at segmenting objects in images and videos; however, scene effects re-lated to the objects—shadows, reﬂections, generated smoke, etc.—are typically overlooked.
Identifying such scene ef-fects and associating them with the objects producing them is important for improving our fundamental understanding of visual scenes, and can also assist a variety of applica-tions such as removing, duplicating, or enhancing objects in video. In this work, we take a step towards solving this novel problem of automatically associating objects with their ef-fects in video. Given an ordinary video and a rough seg-mentation mask over time of one or more subjects of inter-est, we estimate an omnimatte for each subject—an alpha matte and color image that includes the subject along with all its related time-varying scene elements. Our model is trained only on the input video in a self-supervised manner, without any manual labels, and is generic—it produces om-nimattes automatically for arbitrary objects and a variety of effects. We show results on real-world videos contain-ing interactions between different types of subjects (cars, animals, people) and complex effects, ranging from semi-transparent elements such as smoke and reﬂections, to fully opaque effects such as objects attached to the subject.1 1Project page: https://omnimatte.github.io/
“And ﬁrst he will see the shadows best, next the reﬂec-tions of men and other objects in the water, and then the objects themselves, then he will gaze upon the light of the moon and the stars and the spangled heaven ... Last of all he will be able to see the sun.” – Plato
Is it possible to automatically determine all the effects caused by a subject in a video? Reﬂect for a moment on the difﬁculty of the task: a subject, such as a human wander-ing through a scene, can cast shadows on the ﬂoor and dis-tant walls, and be reﬂected in windows and other surfaces.
These ‘effects’ are non-local. However, they are correlated with the subject’s shape, motion and, in the case of reﬂec-tions, appearance.
Tackling this problem is the objective of this paper. More speciﬁcally, given an input video and (possibly rough) seg-mentations over time of subjects of interest in the video, we seek to produce an output opacity matte (alpha matte) for each subject that includes the subject and their effects in the scene (Figure 1). We call this the “omnimatte” of the subject. We additionally produce a color background image containing the static background elements in the video. We achieve this by proposing a network and training framework that is able to automatically determine and segment regions that are correlated with the given subject (Figure 2). The model is trained in a self-supervised way only on the in-4507
Figure 2. Estimating omnimattes from video. The input to the model is an ordinary video with multiple moving objects, and a rough segmentation mask M for each object (left). In a pre-processing step, we compute an optical ﬂow ﬁeld F between consecutive frames using [28]. For each object, we pass the mask, estimated ﬂow in the object’s region, and a sampled noise image Zt (representing the background) to our model, producing an omnimatte (color + opacity) and an optical ﬂow ﬁeld for the object (right). In addition, the model predicts a single background color image for the entire video (top), given a spatial texture noise image ¯Z as input. See Sec. 3 for details. put video, without observing any additional examples. Our solution is inspired by the recent work of Lu et al. [17] that presented a method to decompose a video into a set of human-speciﬁc RGBA layers. We generalize this technique to support arbitrary objects, by relying only on binary input masks (no object-speciﬁc representation or processing) and incorporating general optical ﬂow to account for motion and frame-to-frame correspondence.
Associating objects with their effects not only improves our fundamental understanding of visual scenes and events captured in video, it can also support a range of applica-tions. Consider for example the problem of removing a person or other types of objects from a video. As is well known, a common error in person removal, e.g., by inpaint-ing, is that a shadow or reﬂection of the person remains, resulting in a video left with just a ‘shadow of the former self’. The erroneous missing of a reﬂection is a central plot point in the ﬁlm ‘Rising Sun’ (1993), and the converse, a lack of reﬂection, a common trope of vampire movies. The important point is that manipulating an object in a video requires dealing not only with the object; its effects in the scene need to be adjusted together with the object in order to create realistic and faithful renditions.
We demonstrate results of inferring omnimattes for dif-ferent objects such as animals, cars, and people, capturing a variety of complex scene effects including shadows, re-ﬂections, dust and smoke. We evaluate the resulting om-nimattes qualitatively and quantitatively, and also demon-strate how omnimattes can be useful for video editing ap-plications such as object removal, background replacement,
“color pop”, and stroboscopic photography. 2.