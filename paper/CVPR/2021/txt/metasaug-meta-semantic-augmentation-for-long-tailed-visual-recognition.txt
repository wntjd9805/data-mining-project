Abstract w/o Augmentation
Real-world training data usually exhibits long-tailed dis-tribution, where several majority classes have a signiﬁ-cantly larger number of samples than the remaining mi-nority classes. This imbalance degrades the performance of typical supervised learning algorithms designed for bal-anced training sets. In this paper, we address this issue by augmenting minority classes with a recently proposed im-plicit semantic data augmentation (ISDA) algorithm [37], which produces diversiﬁed augmented samples by translat-ing deep features along many semantically meaningful di-rections. Importantly, given that ISDA estimates the class-conditional statistics to obtain semantic directions, we ﬁnd it ineffective to do this on minority classes due to the insuf-ﬁcient training data. To this end, we propose a novel ap-proach to learn transformed semantic directions with meta-learning automatically. In speciﬁc, the augmentation strat-egy during training is dynamically optimized, aiming to minimize the loss on a small balanced validation set, which is approximated via a meta update step. Extensive empirical results on CIFAR-LT-10/100, ImageNet-LT, and iNaturalist 2017/2018 validate the effectiveness of our method. 1.

Introduction
Deep convolutional neural networks (CNNs) have achieved remarkable success in recent years [22, 15, 17].
Their state-of-the-art performance is typically demonstrated on the benchmarks such as ImageNet [31] and MS COCO
[24]. While these datasets are established by ideally col-lecting a similar and sufﬁcient number of samples for each class, real-world training data is usually imbalanced, as shown in Fig. 1(a). For example, in automatic medical di-agnosis, a few common diseases may dominate the training set, with scarce cases for the remaining classes. This long-tailed distribution, unfortunately, degrades the performance of networks severely if using a standard training strategy (e.g., supervised learning with the cross-entropy loss).
†C. Liu is the corresponding author.
Majority classes
Minority classes (a)  w/ Augmentation
Majority
Minority
Samples
Augmentation
Classifier boundaries (b) 
Conventional data augmentation 
Original 
Flipping 
Rotation 
Semantic data augmentation 
Original 
Changing color 
Changing visual angle (c) 
Figure 1. (a): In the data distribution of the real-world dataset iNat-uralist 2018, a few majority classes account for the most samples, while the minority classes are under-represented. (b): Motivation of this work. Facilitating data augmentation for long-tailed prob-lems to ameliorate the classiﬁer performance. (c): Illustration of traditional data augmentation and semantic data augmentation.
To address the issue of data imbalance, a natural solution might be to augment the minority classes for more training samples as shown in Fig. 1(b), e.g., by leveraging the data augmentation technique [15, 17, 42, 27]. However, conven-tional data augmentation techniques like cropping, mirror-ing and mixup are typically performed on the inputs. As a result, the diversity of augmented samples is inherently lim-ited by the small amount of training data in minority classes.
Fortunately, this problem can potentially be solved by a recently proposed implicit semantic data augmentation (ISDA) technique [37]. ISDA performs class identity pre-serving semantic transformation (e.g., changing the color 5212
of an object and changing the visual angles) by translat-ing deep features towards certain meaningful semantic di-rections as shown in Fig. 1(c). The deep feature space extracted by CNNs tends to be linearized and has signiﬁ-cantly smaller complexity than the pixel space. Therefore, the minority classes will be effectively augmented for more diversity as long as proper semantic directions are found.
ISDA estimates class-wise covariance matrices from deep features and sample semantic directions from a Gaussian distribution. Nevertheless, we ﬁnd that this leads to inferior performance in the long-tailed scenario, since scarce data in minority classes is insufﬁcient to obtain reasonable covari-ance matrices.
In this paper, we propose a meta semantic augmentation (MetaSAug) approach, aiming to perform effective seman-tic data augmentation for long-tailed problems via learning more meaningful class-wise covariance. Our major insight here is that if the appropriate covariance matrices are used for semantic augmentation, the loss on a balanced valida-tion set should be minimized. At every training iteration, we perform validation on a small balanced validation set, and update the class-wise covariance by minimizing the vali-dation loss. Speciﬁcally, we ﬁrst fulﬁll the augmentation procedure using current class-wise covariance. Then, we calculate the loss on the validation set with respect to the class-wise covariance. By optimizing the validation loss, we can obtain the updated class-wise covariance that con-tains rich semantic directions. With it, we train the models on the augmentation set with sufﬁcient semantically aug-mented samples. In addition, our method can be treated as a plug-in module and be uniﬁed with previous methods. We further improve the classiﬁcation ability of focal loss [23] and LDAM loss [7] by combining them with MetaSAug.
We conduct extensive experiments on several long-tailed datasets, including the artiﬁcially long-tailed CIFAR-10/100 [21, 9], ImageNet [31, 26], and the naturally long-tailed dataset inaturalist 2017 and 2018 [36, 1]. The results demonstrate the effectiveness of our method. 2.