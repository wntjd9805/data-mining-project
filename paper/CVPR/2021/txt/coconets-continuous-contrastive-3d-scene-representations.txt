Abstract
This paper explores self-supervised learning of amodal 3D feature representations from RGB and RGB-D posed images and videos, agnostic to object and scene semantic content, and evaluates the resulting scene representations in the downstream tasks of visual correspondence, object tracking, and object detection. The model infers a latent 3D representation of the scene in the form of 3D feature points, where each continuous world 3D point is mapped to its corresponding feature vector. The model is trained for contrastive view prediction by rendering 3D feature clouds in queried viewpoints and matching against the 3D feature point cloud predicted from the query view. Notably, the rep-resentation can be queried for any 3D location, even if it is not visible from the input view. Our model brings together three powerful ideas of recent exciting research work: 3D feature grids as a neural bottleneck for view prediction, im-plicit functions for handling resolution limitations of 3D grids, and contrastive learning for unsupervised training of feature representations. We show the resulting 3D vi-sual feature representations effectively scale across objects and scenes, imagine information occluded or missing from the input viewpoints, track objects over time, align seman-tically related objects in 3D, and improve 3D object detec-tion. We outperform many existing state-of-the-art methods for 3D feature learning and view prediction, which are ei-ther limited by 3D grid spatial resolution, do not attempt to build amodal 3D representations, or do not handle com-binatorial scene variability due to their non-convolutional bottlenecks. 1.

Introduction
Understanding the three-dimensional structure of objects and scenes may be a key for success of machine percep-tion and control in object detection, tracking, manipulation and navigation. Exciting recent works have explored learn-*Equal contribution
Project page: https://mihirp1998.github.io/project pages/coconets/ ing representations of objects and scenes from multiview imagery and capture the three-dimensional scene structure implicitly or explicitly with 3D binary or feature grids
[46, 45, 40], 3D point feature clouds [50], implicit func-tions that map continuous world coordinates to 3D point occupancy [4, 9, 41, 25, 31, 3], as well as 1D or 2D feature maps [6]. These methods typically evaluate the accuracy of the inferred 3D scene occupancy [4, 30, 46, 25, 31, 3] and the ﬁdelity of image views rendered from the 3D rep-resentation [22, 26, 6, 50, 41], as opposed to the suitability of representations for downstream semantic tasks. Methods that indeed focus on rendering photo-realistic images often give up on cross-scene generalization [26, 36], or focus on single-object scenes [41]. Methods that instead focus on learning semantically relevant scene representations are ex-pected to generalize across scenes, and handle multi-object scenes. In the 2D image space, contrastive predictive cod-ing has shown to generate state-of-the-art visual features for correspondence and recognition [49, 12], but does not en-code 3D scene structure. In 3D voxel feature learning meth-ods [14, 13], convolutional latent 3D feature grids encode the 3D structure and a view contrastive objective learns se-mantically useful 3D representations, but the grid resolution limits the discriminability of the features learnt. Recent ex-citing works combine 3D voxel grids and implicit functions and learn to predict 3D scene and object 3D occupancy from a single view with unlimited spatial resolution [32, 33]. The model proposed in this work brings together these two pow-erful ideas: 3D feature grids as a 3D-informed neural bot-tleneck for contrastive view prediction [14], and implicit functions for handling the resolution limitations of 3D grids
[32].
We propose Continuous Contrastive 3D Networks (Co-CoNets), a model that learns to map RGB-D images to inﬁnite-resolution 3D scene feature representations by con-trastively predicting views, in an object and scene agnostic way. Our model is trained to predict views of static scenes given 2.5D (color and depth; RGB-D) video streams as in-put, and is evaluated on its ability to detect and recognize objects in 3D. CoCoNets map the 2.5D input streams into 3D feature grids of the depicted scene. Given a target view 12487
and its viewpoint, the model ﬁrst warps its inferred 3D fea-ture map from the input view to a target view, then queries point features using their continuous coordinates, and pulls these features closer to the point features extracted from the target view at the same 3D locations (Figure 1). We use a contrastive loss to measure the matching error, and back-propagate gradients end-to-end to our differentiable modu-lar architecture. At test time, our model forms plausible 3D completions of the scene given a single RGB-D image as input: it learns to ﬁll in information behind occlusions, and infer the 3D extents of objects.
We demonstrate the advantages of combining 3D neu-ral bottleneck, implicit functions and contrastive learning for 3D representation learning by comparing our model against state-of-the-art self-supervised models, such as i) contrastive learning for pointclouds [51], which shares a similar loss but not the amodal predictive ability of our ii) contrastive neural mapping [13], which can model, amodally inpaint a 3D discrete feature grid but suffers from limited spatial grid resolution, and iii) dense ObjectNets [7], which self-learns 2D (instead of 3D) feature representations with a triangulation-driven supervision similar to (i). Our experimental results can be summarized as follows: (1) 3D object tracking and re-identiﬁcation (Figure 3): We show that scene representations learnt by CoCoNets can detect objects in 3D across large frame gaps better than the base-lines [51, 13, 7]. (2) Supervised 3D object detection: Us-ing the learnt 3D point features as initialization boosts the performance of the state-of-the-art Deep Hough Voting de-tector of [34]. (3) 3D cross-view and cross-scene object 3D alignment: We show that the learnt 3D feature represen-tations can infer 6DoF alignment between the same object in different viewpoints, and across different objects of the same category, better than [51, 13, 7]. We further show that our model can predict image views (with or without depth as input) and 3D occupancies that outperform or are on par with the state-of-the-art view and occupancy predic-tion models [33, 47, 6].
In summary, the main contribution of this paper is a model that learns inﬁnite-resolution 3D scene representa-tions from RGB-D posed images, useful for tracking and corresponding objects in 3D, pre-training 3D object detec-tors, and predicting views and 3D occupancies. We set a new state-of-the-art in self-supervision of 3D feature repre-sentations. 2.