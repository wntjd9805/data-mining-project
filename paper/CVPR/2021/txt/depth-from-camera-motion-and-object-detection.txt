Abstract
This paper addresses the problem of learning to esti-mate the depth of detected objects given some measure-ment of camera motion (e.g., from robot kinematics or ve-hicle odometry). We achieve this by 1) designing a recur-rent neural network (DBox) that estimates the depth of ob-jects using a generalized representation of bounding boxes and uncalibrated camera movement and 2) introducing the
Object Depth via Motion and Detection Dataset (ODMD).
ODMD training data are extensible and conﬁgurable, and the ODMD benchmark includes 21,600 examples across four validation and test sets. These sets include mobile robot experiments using an end-effector camera to locate objects from the YCB dataset and examples with perturba-tions added to camera motion or bounding box data. In ad-dition to the ODMD benchmark, we evaluate DBox in other monocular application domains, achieving state-of-the-art results on existing driving and robotics benchmarks and es-timating the depth of objects using a camera phone. 1.

Introduction
With the progression of high-quality datasets and sub-sequent methods, our community has seen remarkable ad-vances in image segmentation [27, 54], video object seg-mentation [35, 38], and object detection [4, 16, 32], some-times with a focus on a speciﬁc application like driving
[5, 11, 43]. However, applications like autonomous vehi-cles and robotics require a three-dimensional (3D) under-standing of the environment, so they frequently rely on 3D sensors (e.g., LiDAR [10] or RGBD cameras [8]). Although 3D sensors are great for identifying free space and motion planning, classifying and understanding raw 3D data is a challenging and ongoing area of research [25, 28, 33, 39].
On the other hand, RGB cameras are inexpensive, ubiqui-tous, and interpretable by countless vision methods.
To bridge the gap between 3D applications and progress in video object segmentation, in recent work [14], we devel-oped a method of video object segmentation-based visual servo control, object depth estimation, and mobile robot
Figure 1. Depth from Camera Motion and Object Detection.
Object detectors can reliably place bounding boxes on target ob-jects in a variety of settings. Given a sequence of bounding boxes and camera movement distances between observations (e.g., from robot kinematics, top), our network (DBox) estimates each ob-ject’s depth (bottom). This result is from the ODMD Robot Set. grasping using a single RGB camera. For object depth esti-mation, speciﬁcally, we used the optical expansion [22, 46] of segmentation masks with z-axis camera motion to an-alytically solve for depth [14, VOS-DE (20)].
In subse-quent work [15], we introduced the ﬁrst learning-based method (ODN) and benchmark dataset for estimating Ob-ject Depth via Motion and Segmentation (ODMS), which includes test sets in robotics and driving. From the ODMS benchmark, ODN improves accuracy over VOS-DE in mul-tiple domains, especially those with segmentation errors.
Motivated by these developments [14, 15], this paper ad-1397
dresses the problem of estimating the depth of objects us-ing uncalibrated camera motion and bounding boxes from object detection (see Figure 1), which has many advan-tages. First, a bounding box has only four parameters, can be processed quickly with few resources, and has less domain-speciﬁc features than an RGB image or segmenta-tion mask. Second, movement is already measured on most autonomous hardware platforms and, even if not measured, structure from motion is plausible to recover camera motion
[23, 34, 45]. Third, as we show with a pinhole camera and box-based model (see Figure 2) and in experiments, we can use x-, y-, or z-axis camera motion to estimate depth using optical expansion, motion parallax [9, 42], or both. Finally, our detection-based methods can support more applications by using boxes or segmentation masks, which we demon-strate in multiple domains with state-of-the-art results on the segmentation-based ODMS benchmark [15].
The ﬁrst contribution of our paper is deriving an analyt-ical model and corresponding solution (BoxLS) for uncali-brated motion and detection-based depth estimation in Sec-tion 3.1. To the best of our knowledge, this is the ﬁrst model or solution in this new problem space. Furthermore, BoxLS achieves the best analytical result on the ODMS benchmark.
A second contribution is developing a recurrent neural network (RNN) to predict Depth from motion and bounding
Boxes (DBox) in Section 3.2. DBox sequentially processes observations and uses our normalized and dimensionless input-loss formulation, which improves performance across domains with different movement distances and camera pa-rameters. Thus, using a single DBox network, we achieve the best Robot, Driving, and overall result on the ODMS benchmark and estimate depth from a camera phone.1
Inspired by ODMS [15], a ﬁnal contribution of our pa-per is the Object Depth via Motion and Detection (ODMD) dataset in Section 3.3.2 ODMD is the ﬁrst dataset for mo-tion and detection-based depth estimation, which enables learning-based methods in this new problem space. ODMD data consist of a series of bounding boxes, x, y, z camera movement distances, and ground truth object depth.
For ODMD training, we continuously generate synthetic examples with random movements, depths, object sizes, and three types of perturbations typical of camera motion and object detection errors. As we will show, training with perturbations improves end performance in real applica-tions. Furthermore, ODMD’s distance- and box-based in-puts are 1) simple, so we can generate over 300,000 train-ing examples per second, and 2) general, so we can transfer from synthetic training data to many application domains.
Finally, for an ODMD evaluation benchmark, we create four validation and test sets with 21,600 examples, includ-ing mobile robot experiments locating YCB objects [3]. 1Supplementary video: https://youtu.be/GruhbdJ2l7k 2Dataset website: https://github.com/griffbr/ODMD 2.