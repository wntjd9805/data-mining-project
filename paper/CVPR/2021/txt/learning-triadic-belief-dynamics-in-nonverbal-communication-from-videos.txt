Abstract
Humans possess a unique social cognition capabil-ity [43, 20]; nonverbal communication can convey rich so-cial information among agents. In contrast, such crucial so-cial characteristics are mostly missing in the existing scene understanding literature. In this paper, we incorporate dif-ferent nonverbal communication cues (e.g., gaze, human poses, and gestures) to represent, model, learn, and infer agents’ mental states from pure visual inputs. Crucially, such a mental representation takes the agent’s belief into account so that it represents what the true world state is and infers the beliefs in each agent’s mental state, which may differ from the true world states. By aggregating different beliefs and true world states, our model essentially forms
“ﬁve minds” during the interactions between two agents.
This “ﬁve minds” model differs from prior works that in-fer beliefs in an inﬁnite recursion; instead, agents’ beliefs are converged into a “common mind” [31, 47]. Based on this representation, we further devise a hierarchical energy-based model that jointly tracks and predicts all ﬁve minds.
From this new perspective, a social event is interpreted by a series of nonverbal communication and belief dynam-ics, which transcends the classic keyframe video summary.
In the experiments, we demonstrate that using such a so-cial account provides a better video summary on videos with rich social interactions compared with state-of-the-art keyframe video summary methods. 1.

Introduction
“The human body is the best picture of the human soul.”
— Ludwig Wittgenstein [32]
We live in a world with a plethora of animate and goal-directed agents [60], or at least it is how humans perceive and construct [49] the world in our mental state [24]. The iconic Heider-Simmel display [19] is a quintessential stim-ulus, wherein human participants are given videos of sim-ple shapes roaming around the space. In this experiment,
*Lifeng Fan and Shuwen Qiu contributed equally. humans have a strong inclination to interpret the observed featureless motions composed of simple shapes as a story-telling description, such as a hero saving a victim from a bully. This social cognition account of human vision is largely missing in the computational literature of scene un-derstanding or, more broadly, the ﬁeld of computer vision.
In the ﬁeld of social cognition, researchers have identi-ﬁed two unique components that distinguish human adults from infants and other primates [43]. The ﬁrst component is “representational Theory of Mind (ToM),” the ability to attribute mental states to oneself and others, to under-stand that others have perspectives and mental states differ-ent from one’s own, as well as using these abilities to recog-nize false belief [39]. In the theoretical construct of mental states, mainstream psychology and related disciplines have traditionally treated belief as one simplest form, and there-fore one of the building blocks of conscious thought [23].
Belief can be constructed as mental objects with seman-tic attributes; cognitive states and processes are consti-tuted by the occurrence, transformation, and storage of such information-bearing structure [38]. The second component is the triadic relations: You, and Me, collaboratively look-ing at, working on, or talking about This [47]. Much power of human social cognition depends on the ability to form representations with a triadic structure [43].
To promote social cognition in computer vision, we fo-cus on belief dynamics in nonverbal communication. Here, belief is deﬁned as an entity and its attributes (e.g., loca-tion), and belief dynamics (i.e., the change of belief) are naturally and completely summarized using four categories: occur indicates an agent becomes aware of an object at a certain location, update means an agent knows the object’s attribute was updated, disappear denotes that an agent loses track of the object’s attribute, and null is no change. We em-phasize on triadic relations emerged during nonverbal com-munication, including No Communication, Attention Fol-lowing, and Joint Attention [12, 1]: No Communication in-dicates no social interaction between the two agents, Atten-tion Following is a one-way observation, and Joint Attention means that two agents have the same intention to share at-tention on a common stimulus and both know that they are sharing the attention [47]; see an illustration in Fig. 1. 7312
m1 m12 m(cid:2563) m21 m2 m1 m12 m(cid:2563) m21 m2 m1 m12 m(cid:2563) m21 m2 m1 m12 m(cid:2563) m21 m2 m1 m12 m(cid:2563) m21 m2 (cid:2)(cid:3)(cid:4)(cid:5)(cid:3)(cid:6)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:9)(cid:3)(cid:8) (cid:13)(cid:12)(cid:12)(cid:14)(cid:8)(cid:12)(cid:9)(cid:3)(cid:8)(cid:4)(cid:15)(cid:3)(cid:16)(cid:16)(cid:3)(cid:17)(cid:9)(cid:8)(cid:18) (cid:19)(cid:3)(cid:9)(cid:8)(cid:12)(cid:4)(cid:13)(cid:12)(cid:12)(cid:14)(cid:8)(cid:12)(cid:9)(cid:3)(cid:8)
Figure 1: Triadic belief dynamics in nonverbal communication. Three types of communication events emerge from social interactions (bottom) and causally construct agents’ belief dynamics (top). In this paper, we propose a novel structural mind representation “ﬁve minds” and a learning and inference algorithm for belief dynamics based on a hierarchical energy-based model that tracks (i) each agent’s mental state (m1 and m2), (ii) their estimated belief about other agent’s mental state (m12 and m21), and (iii) the common mind (mc). Of note, some events have two phases connected by three arrows.
To account for the two social components computation-ally, we propose a novel structural mind representation, termed “ﬁve minds,” that includes two ﬁrst-order self men-tal states (i.e., the ground-truth mental state), two second-order estimated mental states of each other’s mind (may deviate from the ground-truth mental states), and the third-level “common mind.” Note that the proposed “ﬁve minds” differs from prior models that attempt to infer mental states among agents recursively with potentially inﬁnite loops; in-stead, the “common mind” considers what the two agents share completely transparently without inﬁnite recursion and corresponds to the concept of “common ground” [47].
The proposed “ﬁve minds” model is well-grounded to visual inputs, especially in terms of nonverbal communica-tion. For instance, gaze communication uses eye gazes as portals inward to provide agents with glimpses into the in-ner mental world [12], and pointing gesture serves as “the
ﬁrst uniquely human forms of communication” to ground and reshape mental states [47]. We bring these crucial so-cial components into representing, modeling, learning, and inference of belief dynamics in the computer vision com-munity. Intuitively, the spatiotemporal parsing of social in-teractions affords the emergence of communication events; these events causally affect belief dynamics. Thus, a hierar-chical energy-based model with Bayesian inference is natu-rally derived to track, maintain, and predict the mental states of all “ﬁve minds.” To demonstrate the model’s efﬁcacy, we collect a new 3D video dataset with eye-tracking devices to facilitate ground-truth labeling. We verify the proposed method on this new 3D video dataset focusing on rich non-verbal social interactions and triadic belief dynamics.
This paper makes four contributions: (i) By incorporat-ing crucial social cognition components, we address a new task of triadic belief dynamics learning and inference from nonverbal communication in natural scenes with rich social interactions. We propose a novel structural mental represen-tation “ﬁve minds” by introducing a “common mind,” with well-deﬁned and quantized belief and belief dynamics, as well as nonverbal communication events. To the best of our knowledge, ours is the ﬁrst to tackle such challenging prob-lems in the ﬁeld of computer vision. (ii) We collect a new 3D video dataset with rich social interactions using eye-tracking devices to facilitate ground-truth labeling; nonver-bal communication events and belief dynamics are densely annotated. Such a setup goes beyond toy and symbolic ex-amples presented in the literature, which we believe will serve as a modern benchmark for high-level social learning based on pixel inputs. (iii) We devise a hierarchical energy-based model and a beam-search-based algorithm to simul-taneously optimize the learning and inference of nonverbal communication events and belief dynamics. (iv) We provide a benchmark and demonstrate the efﬁcacy of the proposed method in a keyframe-based video summary. 2.