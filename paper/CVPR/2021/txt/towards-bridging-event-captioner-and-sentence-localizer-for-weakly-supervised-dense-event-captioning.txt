Abstract (a) Previous WS-DEC method
Dense Event Captioning (DEC) aims to jointly local-ize and describe multiple events of interest in untrimmed videos, which is an advancement of the conventional video captioning task (generating a single sentence description for a trimmed video). Weakly Supervised Dense Event Cap-tioning (WS-DEC) goes one step further by not relying on human-annotated temporal event boundaries. However, there are few methods trying to tackle this task, and how to connect localization and description remains an open prob-lem. In this paper, we demonstrate that under weak supervi-sion, the event captioning module and localization module should be more closely bridged in order to improve descrip-tion performance. Different from previous approaches, in our method, the event captioner generates a sentence from a video segment and feeds it to the sentence localizer to re-construct the segment, and the localizer produces word im-portance weights as a guidance for the captioner to improve event description. To further bridge the sentence localizer and event captioner, a concept learner is adopted as the basis of the sentence localizer, which can be utilized to con-struct an induced set of concept features to enhance video features and improve the event captioner. Finally, our pro-posed method outperforms state-of-the-art WS-DEC meth-ods on the ActivityNet Captions dataset. 1.

Introduction
A conventional video captioning task [6, 53] refers to generating a single sentence description for a trimmed video (usually around 10 seconds long), and it has been exten-sively studied in recent years [45, 58, 34, 17, 46, 61, 67].
Since natural videos contain multiple events, the Dense
Event Captioning (DEC) task [20] is later introduced, which aims at generating multiple temporally localized event de-scriptions for untrimmed videos with the help of temporal
âˆ—Corresponding author.
GT sentence segment
ğ‘…ğ‘… sentence
ğ‘…ğ‘… segment (b) Proposed method
ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶
ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
GT sentence segment
ğ‘…ğ‘… sentence
ğ‘…ğ‘… segment
Reconstruction Loss
Network Module
ğ‘…ğ‘…
ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶
ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
Figure 1. Comparison of the previous the workï¬‚ows of
ğ¼ğ¼ğ¼ğ¼ğ¼ğ¼ğ¼ğ¼
ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ method [11] (a) and our proposed method (b). â€˜Cptâ€™ and â€˜ISABâ€™ denote concept learner and induced set attention block. The yel-low connections highlight our advantages, and they bridge the cap-tioner and localizer more closely by allowing richer bidirectional information passing between the two modules. boundary annotations for each event. Weakly Supervised
Dense Event Captioning (WS-DEC) [11] goes one step fur-ther by not relying on the resource-consuming temporal boundary annotations, and this also makes the task more challenging.
For strongly supervised dense event captioning, the event localization module can receive strong supervision signals to learn to predict precise temporal boundaries and the cap-tioning module can focus on more accurate video segments during both training and testing. Thus most existing meth-ods [47, 22, 68, 32] design a localization module similar to temporal action detection networks [5, 24] to ï¬rstly gener-ate events of interest and then describe them with a cap-tioning module. But for weakly supervised dense event captioning, other than the sentence annotation, the local-ization and captioning modules can only receive informa-tion from each other. As shown in Fig. 1 (a), the ï¬rst WS-DEC method [11] adopted an iterative approach, in which the event captioner and sentence localizer in turn feed out-puts to each other. Its underlying assumption is that by op-8425
timizing the reconstruction losses, the sentence localizerâ€™s output will converge to a point that is optimal for the event captioner. To the best of our knowledge, there are only two published methods [11, 39] of WS-DEC and both adopted the same workï¬‚ow. In this paper, we propose a new WS-DEC method with its simpliï¬ed workï¬‚ow shown in Fig. 1 (b). We argue that directly stacking the event captioner and sentence localizer in a feedforward fashion is not sufï¬cient, since there is no pathway for information communication between these two subtasks other than input-output con-nections. Thus the goal of this work is introducing addi-tional information communication to more closely bridge these two subtasks. Technically, we base our sentence lo-calizer on a multiple instance concept learner, which can capture localized semantic concepts (although not perfect) for aligning with the sentence features in a frame-to-word level. The localizer can then pass two types of information to the captioner: 1) the word importances learned during video-text alignment to guide the caption generation, 2) the concept features to construct an induced set of concept fea-tures to enhance the original video features via an Induced
Set Attention Block. In this way, both the language decoder and visual encoder are more closely connected with the sen-tence localizer and can receive richer information compared to previous methods. Note that we bridge the event cap-tioner and sentence localizer but not deeply couple them, so that it is possible to adopt more sophisticated captioner or localizer models in the future for further improvement with-out changing the proposed workï¬‚ow. Our proposed method is abbreviated as EC-SL.
The main contribution of this paper is a new way to in-tegrate the temporal localization and description for events in untrimmed videos under the weakly supervised setting, where temporal boundary annotations are not available. The proposed method is also an exploration of what type of in-formation communication pathway should be built between the two subtasks in order to better bridge and unify them, which is a problem not yet fully investigated in the litera-ture. 2.