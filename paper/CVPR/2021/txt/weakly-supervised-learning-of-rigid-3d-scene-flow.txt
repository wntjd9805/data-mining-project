Abstract
We propose a data-driven scene ﬂow estimation algo-rithm exploiting the observation that many 3D scenes can be explained by a collection of agents moving as rigid bod-ies. At the core of our method lies a deep architecture able to reason at the object-level by considering 3D scene ﬂow in conjunction with other 3D tasks. This object level ab-straction enables us to relax the requirement for dense scene
ﬂow supervision with simpler binary background segmenta-tion mask and ego-motion annotations. Our mild supervi-sion requirements make our method well suited for recently released massive data collections for autonomous driving, which do not contain dense scene ﬂow annotations. As out-put, our model provides low-level cues like pointwise ﬂow and higher-level cues such as holistic scene understanding at the level of rigid objects. We further propose a test-time optimization reﬁning the predicted rigid scene ﬂow. We showcase the effectiveness and generalization capacity of our method on four different autonomous driving datasets.
We release our source code and pre-trained models under github.com/zgojcic/Rigid3DSceneFlow. 1.

Introduction
Understanding dynamic 3D environments is a core chal-lenge in computer vision and robotics. In particular, appli-cations such as self-driving and robot navigation rely upon a robust perception of dynamically changing 3D scenes.
To equip autonomous agents with the ability to infer spa-tiotemporal geometric properties, there has recently been an increased interest in 3D scene ﬂow as a form of low-level dynamic scene representation [37, 67, 73, 51, 49, 54].
Scene ﬂow is the 3D motion ﬁeld of points in the scene [69] and is a generalization of 2D optical ﬂow.
In fact, opti-cal ﬂow [3, 24] can be understood as the projection of the scene ﬂow onto a camera image plane [14]. Such dense mo-tion ﬁelds can serve bottom-up approaches for high-level dynamic scene understanding tasks like semantic segmen-tation [38] or motion perception. However, representing dynamics via a free form velocity ﬁeld has two major disad-vantages. First, in most applications of interest, dynamics are attributed to rigid object motion [9, 42]. This notion
Figure 1: Our network takes two successive frames as input (a), and outputs a set of transformation parameters for each segmented rigid agent (c) which are used to recover per-point rigid scene ﬂow. After applying the predicted ﬂow to the ﬁrst point cloud, the two frames are aligned (b, d). has been extensively exploited in robotics [8, 13, 7, 6] and holds especially for vehicles in autonomous driving. Pre-dicting unconstrained per-point ﬂow may lead to non-viable results, e.g. parts of the same car might move in different di-rections. Second, accurately learning direct ﬂow estimation necessitates dense supervision that is expensive to acquire and prone to annotation errors. As a result, many meth-ods have resorted to training on simulated data [41, 73, 51], yet this comes at the price of a non-negligible domain gap.
Other methods have attempted to solve the problem in a completely unsupervised manner [67, 75, 44], however they fail to provide competitive performance. In Fig. 2 we illus-trate these two extremes of no- and full-supervision while spanning the many intermediate possibilities, sorted accord-ing to the annotation effort1.
Based on this observation, we seek a sweet spot between supervision effort and performance. To this end, we pro-pose a scene abstraction approach that uses rigid objects as the basic components. More speciﬁcally, by splitting the scene into foreground (movable objects) and background (static objects), we explain the background (BG) ﬂow as 1Note that no prior work exists at different points on the spectrum given in Fig. 2. This leaves ample room for future exploration. 5692
JGWTF [44]
PointPwcNet [75]
EgoFlow [67]
Ours
PointFlowNet [4]
FlowNet3D [37]
HPLFlowNet [22]
MeteorNet [38]
PointPwcNet [75]
Flot [51], EgoFlow [67] no labels ego-motion
FG/BG semantic instance scene ﬂow
Figure 2: Recent scene ﬂow methods either use full super-vision (and suffer from domain gap) or no-supervision (and suffer from reduced performance). Instead, our method uses weak supervision and beneﬁts from the best of both worlds. the sensor ego-motion and the foreground (FG) ﬂow as clus-ters of rigidly moving entities. As a result, we tackle both aforementioned challenges at the same time: (1) we en-force a rigidity constraint to get meaningful and more accu-rate (foreground and background) ﬂow, (2) we can relax the requirement for dense ﬂow supervision with a much sim-pler binary mask annotation and ego-motion that can often be extracted directly from the agent’s IMU. The result is a weakly supervised method for accurate ﬂow estimation that, unlike completely unsupervised approaches, outper-forms the previous state of the art (SoTA) by a signiﬁcant margin. For example, reducing the end-point-error on the li-darKITTI dataset by more than 30 cm relative to the SoTA.
At the same time, our result provides an interpretable and
In brief, readily usable object-level scene representation. our contributions are:
• We exploit the geometry of the rigid scene ﬂow problem to introduce an inductive bias into our network. This allows us to learn from weak supervision signals: back-ground masks and ego-motion.
• Our data-driven method decomposes the scene into rigidly moving agents enabling us to reason on the level of objects rather than points. We use this notion to pro-pose a new test-time optimization, further reﬁning the
ﬂow predictions.
• Our method is backed by a novel, ﬂexible, scene ﬂow backbone, which can be adapted to solve various tasks.
As a result of these contributions, our method greatly out-performs the SoTA on several benchmarks: FT3D [41], stereoKITTI [42], lidarKITTI [20], and semanticKITTI [5], while generalizing to the waymo-open dataset [64] without additional ﬁne-tuning. 2.