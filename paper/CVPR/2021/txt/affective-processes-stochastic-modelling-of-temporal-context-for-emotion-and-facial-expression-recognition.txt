Abstract
Temporal context is key to the recognition of expressions of emotion. Existing methods, that rely on recurrent or self-attention models to enforce temporal consistency, work on the feature level, ignoring the task-speciﬁc temporal depen-dencies, and fail to model context uncertainty. To alleviate these issues, we build upon the framework of Neural Pro-cesses to propose a method for apparent emotion recog-nition with three key novel components: (a) probabilis-tic contextual representation with a global latent variable model; (b) temporal context modelling using task-speciﬁc predictions in addition to features; and (c) smart tempo-ral context selection. We validate our approach on four databases, two for Valence and Arousal estimation (SEWA and AffWild2), and two for Action Unit intensity estimation (DISFA and BP4D). Results show a consistent improvement over a series of strong baselines as well as over state-of-the-art methods. 1.

Introduction in particular,
In this paper, we address the problem of facial behaviour recognition from video, the problem of recognising apparent emotions in terms of Valence and
Arousal [49], and facial expressions in terms of Action
Unit intensity [9]. This is a longstanding problem in video recognition which has been extensively studied by the com-puter vision community [24, 62, 39, 19, 31, 26, 65, 72, 75, 69, 53, 51, 52, 37, 44]. Nevertheless, even recent methods [25, 31, 75] struggle to achieve high accuracy on the most difﬁcult datasets including SEWA [32], Aff-Wild2 [27], BP4D [73] and DISFA [40]. Hence, there is still great progress to be made towards solving this challenging problem. In this paper, we show that effective temporal con-text modelling is a key feature for signiﬁcantly advancing the state-of-the-art.
Figure 1. Different ways of modelling temporal context in the liter-ature of emotion recognition: Recurrent Models (top), Attention-based Models (middle), and our proposed approach based on
Neural Processes (bottom). Our method proposes a probabilis-tic modelling of temporal context through a global latent variable z ∼ N (µ, σ), and includes a novel method for automatic context selection. The global latent variable allows to sample temporal functions fθ(., z) that are consistent with the captured context.
But what makes emotion (and facial expression) recogni-tion such a difﬁcult problem? There are many, often inter-related, reasons for this including: (a) annotation of emo-tions is often both subjective and laborious making it hard to annotate it consistently [54]; (b) there exist only small- and medium-size emotion video datasets and, due to point (a) above, the annotations for these datasets are often noisy; (c) emotions are subtle, acting as unobserved, latent variables that only partially explain facial appearance over time [2].
They often require temporal as well as multi-modal context 9074
in order to be robustly recognised. From these challenges, this paper focuses on solving problem (c). In particular, it proposes a completely unexplored perspective for effective incorporation of temporal context into emotion recognition.
Previous work in temporal modelling of emotions has primarily focused on modelling facial expression dynam-ics, i.e., the way that facial expressions evolve over time for recognition purposes. A typical deep learning pipeline for this has been a Convolution Neural Network (CNN) fol-lowed by a Recurrent Neural Network (RNN), usually an
LSTM or GRU [30, 26, 24, 69]. Although these dynamics can be particularly useful in recognising speciﬁc facial ex-pressions (e.g. [61, 62, 19, 39, 68]), we believe that their importance has been over-emphasised for the problem of emotion recognition.
We posit that temporal context is more important than fa-cial dynamics for emotion recognition. The cues for infer-ring a person’s apparent emotion from their facial behaviour are often sparsely and non-regularly distributed over a tem-poral window of interest and collecting such distributed contextual cues is critical to inferring emotions robustly. In addition, due to the person-speciﬁc variability of facial ex-pressions but, most importantly, due to their subjective an-notation, context must be modelled in a stochastic manner.
To address the aforementioned challenges, in this work, and for the ﬁrst time to the best of our knowledge, we build upon the framework of Neural Processes [14, 15] to propose a model for emotion recognition with 3 key components: (1) stochastic contextual representation with a global latent variable model; (2) task-aware temporal context modelling not only by using features but also task-speciﬁc predictions; and (3) effective temporal context selection.
Fig. 1 depicts an overview of the working ﬂow our method (bottom), RNN-based methods (top), and methods based on self-attention [64] (middle). Note methods for context modelling based on self-attention do not satisfy any of (1)-(3): They are deterministic and model long-term de-pendencies not globally, but by using pair-wise feature sim-ilarities. Moreover, they do not use task-aware context mod-elling and do not perform context selection.
Overall, we make the following 3 contributions: 1. We propose Affective Processes: the very ﬁrst model for emotion recognition with three key properties: (a) global stochastic contextual representation; (b) task-aware temporal context modelling; and (c) temporal context selection. We conduct a large number of abla-tion studies illustrating the contribution of each of our model’s key features and components. 2. We show that our model is more effective in modelling temporal context not only than CNNs+RNNs but also than a strong baseline based on self-attention. Our model outperforms both baselines by large margin. 3. We validated our approach on the most difﬁcult databases for emotion recognition in terms of Valence and Arousal estimation, namely SEWA [32] and Af-fWild2 [27]. We further show that our approach can be effective for the problem of Action Unit intensity esti-mation on DISFA [40] and BP4D [73] datasets. On all datasets used, we show a consistent and often signiﬁ-cant improvement over state-of-the-art methods. 2.