Abstract
With increasing ﬁelds of application for neural networks and the development of neural networks, the ability to ex-plain deep learning models is also becoming increasingly important. Especially, prior to practical applications, it is crucial to analyze a model’s inference and the process of generating the results. A common explanation method is Class Activation Mapping(CAM) based method where it is often used to understand the last layer of the convolu-tional neural networks popular in the ﬁeld of Computer Vi-sion. In this paper, we propose a novel CAM method named
Relevance-weighted Class Activation Mapping(Relevance-CAM) that utilizes Layer-wise Relevance Propagation to obtain the weighting components. This allows the expla-nation map to be faithful and robust to the shattered gradi-ent problem, a shared problem of the gradient based CAM methods that causes noisy saliency maps for intermediate layers. Therefore, our proposed method can better explain a model by correctly analyzing the intermediate layers as well as the last convolutional layer. In this paper, we visu-alize how each layer of the popular image processing mod-els extracts class speciﬁc features using Relevance-CAM, evaluate the localization ability, and show why the gradi-ent based CAM cannot be used to explain the intermedi-ate layers, proven by experimenting the weighting compo-nent. Relevance-CAM outperforms other CAM-based meth-ods in recognition and localization evaluation in layers of any depth. The source code is available at: https:
//github.com/mongeoroo/Relevance-CAM 1.

Introduction
Recently, deep learning is producing remarkable results with the development in GPUs and the advancements of new neural net architectures[10, 12, 13].
In this context, the ﬁeld of interpretable deep learning is also being ar-dently studied[1, 24, 36, 4], especially in medical image processing [15, 7]. There are many methods for analyzing models in Computer Vision, and such as Class Activation
*Corresponding author.
Figure 1. Visualization results of Grad-CAM[25], Grad-CAM++[6], Score-CAM[30] and proposed Relevance-CAM. The label of the input is a admiral butterﬂy. Relevance-CAM makes the high resolution heatmaps even in the shallow layer.
Map(CAM) based methods [36, 25, 6, 30] and the decom-position based methods [27, 33, 9, 14, 19, 34, 4, 21, 3, 26].
CAM-based method calculates the weighted linear summa-tion of the last convolutional feature map to visualize a model decision. The weights of fully connected layer be-tween global average pooling output and target class out-put nodes are multiplied to the activation maps of the cor-responding channels, and the sum of the weighted activa-tion maps along the channel axis is the Class Activation
Map. It can be understood as generating heatmaps describ-ing the model decision by localizing where the model looks at. However, although CAM[36] can localize a target class, it is heavily constrained to the model architecture where the model must consist of Global Average Pooling(GAP) and one fully connected layer as its classiﬁer. Grad-CAM[25] and Grad-CAM++[6] explain a model without the con-straint to the model architecture. These gradient based
CAMs use gradients of the target class output scores with respect to the last convolutional layer as the weighting com-ponents of activation maps. These methods are based on the 14944
idea that the sensitivity of activation to a target class can be understood as the importance of the activation map to the class. is
Layer-wise Relevance Propagation(LRP)[4] a decomposition-based method. LRP redistributes the model class output scores into input image through speciﬁc rele-vance propagation rule. It has a theoretical background on
Deep Taylor Decomposition[19], and with this theoretical base, it is proved that LRP is robust to the shattered gradient problem[5] and show great performances[4, 20]. 2.2. Grad CAM
Grad-CAM[25] is similar to CAM except for the method of calculating the weight values. It is designed to generalize
CAM and can be applicable to all CNN models. The mo-tivation of Grad-CAM is that activation maps are the fea-ture maps extracted by a certain convolutional layer, and the importance of each activation map to a class can be de-ﬁned as the gradients of the activation maps. Grad-CAM,
Lc
Grad−CAM , is deﬁned as:
In this paper, we propose the novel explanation method which can analyze the model not only at the last convo-lutional layer but also at intermediate layers. Speciﬁcally, we propose the Relevance-weighted Class Activation Map (Relevance-CAM). As shown in Fig 1,Relevance-CAM out-performs other methods in visualizing target objects.
The contributions of this paper are as follows. where
Lc
Grad−CAM =
αc kAk k
X
αc k = GP (
∂yc
∂Ak
) (1) (2) 1. We propose the novel CAM-based method which is faithful and robust to shattered gradient problem.
Therefore, it can operate well even at the intermedi-ate layers. Relevance-CAM helps you to analyze the model in detail that other CAM-based method cannot.
With Relevance-CAM, we ﬁnd the surprising fact that the shallow layers which have small receptive ﬁelds can even extract the class speciﬁc features in some net-works. 2. Through visualization using heatmaps, we show that our Relevance-CAM works effectively at any layer.
Relevance-CAM especially outperforms other meth-ods in localizing target objects in shallow layers. 3. We objectively evaluate faithfulness and localization ability of Relevance-CAM through Average Drop, Av-erage Increase, and Intersection over Union. The proposed method outperforms the other CAM-based methods especially at the intermediate layers. 4. We demonstrate class sensitivity not only in deep lay-ers but also in shallow layers. Relevance-CAM shows that even shallow layers can extract class speciﬁc in-formation. 2.