Abstract
In this paper, we investigate Universal Domain Adap-tation (UniDA) problem, which aims to transfer the knowl-edge from source to target under unaligned label space. The main challenge of UniDA lies in how to separate common classes (i.e., classes shared across domains), from private classes (i.e., classes only exist in one domain). Previous works treat the private samples in the target as one generic class but ignore their intrinsic structure. Consequently, the resulting representations are not compact enough in the latent space and can be easily confused with common samples. To better exploit the intrinsic structure of the target domain, we propose Domain Consensus Clustering (DCC), which exploits the domain consensus knowledge to discover discriminative clusters on both common samples and private ones. Speciﬁcally, we draw the domain con-sensus knowledge from two aspects to facilitate the clus-tering and the private class discovery, i.e., the semantic-level consensus, which identiﬁes the cycle-consistent clus-ters as the common classes, and the sample-level consen-sus, which utilizes the cross-domain classiﬁcation agree-ment to determine the number of clusters and discover the private classes. Based on DCC, we are able to separate the private classes from the common ones, and differentiate the private classes themselves. Finally, we apply a class-aware alignment technique on identiﬁed common samples to minimize the distribution shift, and a prototypical reg-ularizer to inspire discriminative target clusters. Experi-ments on four benchmarks demonstrate DCC signiﬁcantly outperforms previous state-of-the-arts. 1.

Introduction
Deep convolutional neural networks have achieved sig-niﬁcant progress in many ﬁelds, such as image classiﬁca-tion [55, 24], semantic segmentation [7, 8], etc. However, as a data-driven technique, the severe reliance on anno-tated in-domain data greatly limits its application to cross-†Corresponding author.
Code available at: https://git.io/JY86C
Target Domain
Source Domain
Previous
Ours
Common Samples
Decision Boundaries
Private Samples
Cluster Boundaries
Figure 1. A comparison between previous methods and ours. Pre-vious methods simply treat private samples as one general class and ignore its intrinsic data structure. Our approach aims to better exploit the diverse distribution of private samples via forming dis-criminative clusters on both common samples and private samples. domain tasks. As a feasible solution, unsupervised domain adaptation (UDA) [45] tries to solve this by transferring the knowledge from an annotated domain to an unlabeled domain, and has achieved signiﬁcant progress in multiple tasks [33, 34, 27, 29, 35, 37]. Despite UDA’s achievement, most UDA solutions assume that two domains share identi-cal label set, which is hard to satisfy in real-world scenarios.
In light of this, several works considering the unaligned label set have been proposed: open set domain adaptation, partial domain adaptation, and universal domain adaptation.
Open set domain adaptation (OSDA) [54] assumes the tar-get domain possesses private classes that are unknown to the source domain. Analogously, partial domain adaptation (PDA) [4] describes a setting where only the source domain holds private classes. However, both OSDA and PDA still require prior knowledge where the private classes lie in. As a result, they are limited to one scenario and fail to gener-alize to other scenarios. For example, an OSDA solution 9757
would fail in the PDA scenario as it only seeks private sam-ples in the target domain. To solve this, [62] takes a step further to propose a more general yet practical setting, uni-versal domain adaptation (UniDA), which allows both do-mains to own private classes.
The main challenge of transferring over unaligned la-bel space is how to effectively separate common samples from private samples in both domains. To achieve this goal, many efforts have been devoted to performing com-mon sample discovery from different perspectives, such as designing new criteria [4, 3, 62, 17, 53] or introducing ex-tra discriminator [63, 5, 38, 9]. However, previous practices mainly focus on identifying common samples but treat pri-vate samples as a whole, i.e., unknown class (Bottom left in Fig. 1). Despite making progress, the intrinsic structure (i.e., the variations within each semantic class and the rela-tionships between different semantic classes) of the private samples is not fully exploited. As the private samples in na-ture belong to distinct semantic classes, treating them as one general class is arguably sub-optimal, which further induces lower compactness and less discriminative target represen-tations.
In this paper, we aim to better exploit the intrinsic struc-ture of the target domain via mining both common classes and individual private classes. We propose Domain Consen-sus Clustering (DCC), which utilizes the domain consen-sus knowledge to form discriminative clusters on both com-mon samples and private samples (Bottom right in Fig. 1).
Speciﬁcally, we mine the domain consensus knowledge from two aspects, i.e., semantic-level and sample-level, and integrate them into two consecutive steps. Firstly, we lever-age Cycle-Consistent Matching (CCM) to mine the seman-tic consensus among cluster centers so that we could iden-If two cluster tify common clusters from both domains. centers reach consensus, i.e., both centers act as the other’s nearest center simultaneously, this pair will be regarded as common clusters. Secondly, we propose a metric, do-main consensus score, to acquire cross-domain classiﬁca-tion agreements between identiﬁed common clusters. Con-cretely, domain consensus score is deﬁned as the proportion of samples that hold corresponding cluster label across do-mains. Intuitively, more samples reach consensus, the dis-tribution shift between matched clusters is smaller. There-fore, domain consensus score could be regarded as a con-straint that ensures the precise matching of CCM. More-over, domain consensus score also offers a necessary guid-ance that determines the number of target clusters, and en-courages the samples to be grouped into clusters of both common and private classes. Finally, for those common clusters with high domain consensus scores, we exploit a class-aware alignment technique on them to mitigate the distribution shift. As for those centers that fail to ﬁnd their consensus counterparts, we also enhance their cluster-based consistency. To be speciﬁc, we employ a prototypical reg-ularizer to encourage samples to approach their attached cluster centers.
In this way, those samples belonging to different private categories will be encouraged to be distin-guishable from each other, which also contributes to learn-ing better representations.
Our contribution can be summarized as: 1) We tackle the
UniDA problem from a new perspective, i.e., differentiat-ing private samples into different clusters instead of treating them as whole. 2) We propose Domain Consensus Clus-tering (DCC), which mines domain consensus knowledge from two levels, i.e., semantic-level and sample-level, and guides the target clustering in the absence of prior knowl-edge. 3) Extensive experiments on four benchmarks ver-ify the superior performance of proposed method compared with previous works. 2.