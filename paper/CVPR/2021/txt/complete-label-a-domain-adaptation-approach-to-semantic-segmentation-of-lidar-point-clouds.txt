Abstract
We study an unsupervised domain adaptation problem for the semantic labeling of 3D point clouds, with a particular focus on domain discrepancies induced by different LiDAR sensors. Based on the observation that sparse 3D point clouds are sampled from 3D surfaces, we take a Complete and Label approach to recover the underlying surfaces be-fore passing them to a segmentation network. Speciﬁcally, we design a Sparse Voxel Completion Network (SVCN) to complete the 3D surfaces of a sparse point cloud. Unlike semantic labels, to obtain training pairs for SVCN requires no manual labeling. We also introduce local adversarial learning to model the surface prior. The recovered 3D sur-faces serve as a canonical domain, from which semantic labels can transfer across different LiDAR sensors. Exper-iments and ablation studies with our new benchmark for cross-domain semantic labeling of LiDAR data show that the proposed approach provides 6.3-37.6% better performance than previous domain adaptation methods. 1.

Introduction
Semantic segmentation of LiDAR point clouds is important for many applications, including autonomous driving, se-mantic mapping, and construction site monitoring to name a few. Given a LiDAR sweep (frame), the goal is to produce a semantic label for each point.
Although there is a great potential for deep neural net-works on this semantic segmentation task, their performance is limited by the availability of labeled training data. Acquir-ing manual labels for 3D points is very expensive. Several datasets have recently been released by autonomous driving companies [1, 4, 5, 13, 14, 25, 27, 51]. However, each has a different conﬁguration of LiDAR sensors, which produce different 3D sampling patterns (Figure 1), and each cov-ers distinct geographic regions with distinct distributions of scene contents. As a result, deep networks trained on one dataset do not perform well on others. (a) captured by a 64-beam LiDAR (b) captured by a 32-beam LiDAR
Figure 1. The sampling discrepancy between point clouds captured by two LiDAR sensors. All ﬁgures are best viewed in color.
There is a domain adaptation problem. While the mis-match of scene contents is similar to those studied in 2D visual domain adaptation [36, 8], the sampling mismatch is unique to 3D point clouds. Each time a new LiDAR sensor conﬁguration is selected, data is acquired with a different 3D sampling pattern, so models trained on the old data are no longer effective, and new labeled data must be acquired for supervised training in the conventional machine learning paradigm. In contrast, domain adaptation aims to take better advantage of the old labeled data by revealing unlabeled data of the new LiDAR conﬁguration to a machine learner so that it can account for the new scenarios.
To address the sampling caused domain gap, we observe that LiDAR samples have an underlying geometric structure, and domain adaptation can be performed more effectively with a 3D model leveraging that structure. Speciﬁcally, as-suming the physical world is composed of 3D surfaces, and that LiDAR sensor samples come from those surfaces, we address the domain adaption problem by transforming it into a 3D surface completion task. That is, if we can recover the underlying complete 3D surfaces from sparse LiDAR point samples, and train networks that operate on the completed surfaces, then we can leverage the labeled data from any
LiDAR scanner to work on the data from any other.
The motivation for this approach is that surface comple-tion is an easier task than semantic segmentation. First, there are strong priors on the shapes of 3D surfaces encountered in the real world, and thus a network trained to densify a point cloud can learn and leverage those priors with relatively little 15363
training data. Second, surface completion can be learned from self-supervision (e.g., from multi-view observations) and/or from synthetic datasets (e.g., from sampled computer graphics models). Unlike semantic segmentation, no man-ual labels are required. We train our completion network with supervision from complete surfaces reconstructed from multiple frames of LiDAR data.
Our network architecture is composed of two phases: surface completion and semantic labeling. In the ﬁrst phase, we use a sparse voxel completion network (SVCN) to recover the 3D surface from a LiDAR point cloud. In the second phase, we use a sparse convolutional U-Net to predict a semantic label for each voxel on the completed surface.
Extensive experiments with different autonomous vehi-cle driving datasets verify the effectiveness of our domain adaptation approach to the semantic segmentation of 3D point clouds. For example, using a network trained on the
Waymo open dataset [51] to perform semantic segmenta-tion on the nuScenes dataset [4] provides an absolute mIoU improvement of 6.0 over state-of-the-art domain adaptation methods. Similarly, training on nuScenes and testing on
Waymo provides an absolute mIoU improvement of 10.4 over prior arts.
Our contributions are three-fold. First and foremost, we identify the cross-sensor domain gap for LiDAR point clouds caused by sampling differences, and we propose to recover complete 3D surfaces from the point clouds to eliminate the discrepancies in sampling patterns. Second, we present a novel sparse voxel completion network, which efﬁciently processes sparse and incomplete LiDAR point clouds and completes the underlying 3D surfaces with high resolution.
Third, we provide thorough quantitative evaluations to vali-date our design choices on three datasets. 2.