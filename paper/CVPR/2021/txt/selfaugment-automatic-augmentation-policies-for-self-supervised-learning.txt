Abstract
Fully Unsupervised Training
A common practice in unsupervised representation learn-ing is to use labeled data to evaluate the quality of the learned representations. This supervised evaluation is then used to guide critical aspects of the training process such as selecting the data augmentation policy. However, guid-ing an unsupervised training process through supervised evaluations is not possible for real-world data that does not actually contain labels (which may be the case, for ex-ample, in privacy sensitive ﬁelds such as medical imaging).
Therefore, in this work we show that evaluating the learned representations with a self-supervised image rotation task is highly correlated with a standard set of supervised evalua-tions (rank correlation > 0.94). We establish this correlation across hundreds of augmentation policies, training settings, and network architectures and provide an algorithm (Self-Augment) to automatically and efﬁciently select augmenta-tion policies without using supervised evaluations. Despite not using any labeled data, the learned augmentation poli-cies perform comparably with augmentation policies that were determined using exhaustive supervised evaluations. 1.

Introduction
Self-supervised learning, a type of unsupervised learning that creates target objectives without human annotation, has led to a dramatic increase in the ability to capture salient fea-ture representations from unlabeled visual data. So much so, that in an increasing number of cases these representations outperform representations learned from the same data with labels [1, 2, 3]. At the center of these advances is a form of instance contrastive learning where a single image is aug-mented using two separate data augmentations, and then a network is trained to distinguish which augmented images originated from the same image when contrasted with other randomly sampled augmented images, see [1, 3, 4, 5].
As illustrated in Figure 1, recent works [1, 4, 6] have used extensive supervised evaluations to determine which
*equal contribution; correspondence to cjrd@cs.berkeley.edu
D
Data Augmentation
Policy
T ∗
T (D)
T ∗
Self-Supervised Evaluation (this paper)
Encoder
Network
H
Projection
Head
Supervised Evaluation (prior work)
Figure 1: The blue box highlights our fully unsupervised training pipeline for instance contrastive representation learn-ing: data D are augmented with policy T , then encoded into representations, H, which are fed into a projection head yield-ing features that determine the InfoNCE loss (Eq. 1). As shown by the red arrow, prior work uses supervised evalua-tions of the representations, F to inform the training process, e.g. to update the augmentation policy T → T ∗. In this paper, we show that self-supervised evaluation can be used in lieu of supervised evaluation and show how to use this evaluation for automatic and efﬁcient augmentation selection. augmentation policies to use for training. The best policies obtain a sweet spot, where the augmentations make it difﬁcult for the contrastive task to determine the corresponding image pairs while retaining salient image features; ﬁnding this sweet spot can be the difference between state-of-the-art performance or poor performance for various tasks [6].
However, it is often difﬁcult or impossible to obtain ac-curately labeled data in privacy sensitive ﬁelds (e.g. medi-cal imaging [7]), applications with highly ambiguous label deﬁnitions (e.g. fashion or retail categorization [8]), or not practical when one set of representations is used for a diverse set of downstream tasks (e.g. in autonomous driving sys-tems [9]). This leads to the open question: How can we eval-uate self-supervised models, especially to efﬁciently select augmentation policies, when labeled data is not available?
We address this question via the following contributions:
• We show that a linear, image-rotation-prediction evalua-tion task is highly correlated with the downstream super-vised performance (rank correlation ρ > 0.94) on six stan-2674
dard recognition datasets (CIFAR-10 [10], SVHN [11], Im-ageNet [12], PASCAL [13], COCO [14], Places-205 [15]) and tasks (image classiﬁcation, object detection, and few-shot variants) across hundreds of learned representations, spanning three types of common evaluation techniques: linear separability performance, semi-supervised perfor-mance, and transfer learning performance.
• Using self-supervised evaluation, we adapt two automatic data augmentation algorithms for instance contrastive learning. Without using labeled evaluations, these algo-rithms discover augmentation policies that match or out-perform policies obtained using supervised feedback and only use a fraction of the compute.
• We further show that using linear image rotation prediction to evaluate the representations works across network archi-tectures, and that image rotation prediction has a stronger correlation with supervised performance than a jigsaw [16] or color prediction [17] evaluation task.
Based on these contributions and experiments, we conclude that image rotation prediction is a strong, unsupervised evalu-ation criteria for evaluating and selecting data augmentations for instance contrastive learning. 2.