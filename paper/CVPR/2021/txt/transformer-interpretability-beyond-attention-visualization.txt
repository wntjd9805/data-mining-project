Abstract
Self-attention techniques, and speciﬁcally Transformers, are dominating the ﬁeld of text processing and are becom-ing increasingly popular in computer vision classiﬁcation
In order to visualize the parts of the image that tasks. led to a certain classiﬁcation, existing methods either rely on the obtained attention maps or employ heuristic prop-agation along the attention graph.
In this work, we pro-pose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then prop-agates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a speciﬁc formulation that is shown to maintain the to-tal relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classiﬁcation problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila-chefer/Transformer-Explainability. 1.

Introduction
Transformers and derived methods [41, 9, 22, 30] are currently the state-of-the-art methods in almost all NLP benchmarks. The power of these methods has led to their adoption in the ﬁeld of language and vision [23, 40, 38].
More recently, Transformers have become a leading tool in traditional computer vision tasks, such as object detec-tion [4] and image recognition [6, 11]. The importance of
Transformer networks necessitates tools for the visualiza-tion of their decision process. Such a visualization can aid in debugging the models, help verify that the models are fair and unbiased, and enable downstream tasks.
The main building block of Transformer networks are self-attention layers [29, 7], which assign a pairwise atten-tion value between every two tokens. In NLP, a token is typically a word or a word part. In vision, each token can be associated with a patch [11, 4]. A common practice when trying to visualize Transformer models is, therefore, to con-sider these attentions as a relevancy score [41, 43, 4]. This is usually done for a single attention layer. Another option is to combine multiple layers. Simply averaging the attentions obtained for each token, would lead to blurring of the sig-nal and would not consider the different roles of the layers: deeper layers are more semantic, but each token accumu-lates additional context each time self-attention is applied.
The rollout method [1] is an alternative, which reassigns all attention scores by considering the pairwise attentions and assuming that attentions are combined linearly into subse-quent contexts. The method seems to improve results over the utilization of a single attention layer. However, as we show, by relying on simplistic assumptions, irrelevant to-kens often become highlighted.
In this work, we follow the line of work that assigns rel-evancy and propagates it, such that the sum of relevancy is maintained throughout the layers [27]. While the ap-plication of such methods to Transformers has been at-tempted [42], this was done in a partial way that does not propagate attention throughout all layers.
Transformer networks heavily rely on skip connection and attention operators, both involving the mixing of two activation maps, and each leading to unique challenges.
Moreover, Transformers apply non-linearities other than
ReLU, which result in both positive and negative features.
Because of the non-positive values, skip connections lead, if not carefully handled, to numerical instabilities. Meth-ods such as LRP [3] for example, tend to fail in such cases.
Self-attention layers form a challenge since a naive propa-gation through these would not maintain the total amount of relevancy.
We handle these challenges by ﬁrst introducing a rele-vancy propagation rule that is applicable to both positive and negative attributions. Second, we present a normal-ization term for non-parametric layers, such as “add” (e.g. skip-connection) and matrix multiplication. Third, we in-tegrate the attention and the relevancy scores, and combine the integrated results for multiple attention blocks.
Many of the interpretability methods used in computer vision are not class-speciﬁc in practice, i.e., return the same 782
visualization regardless of the class one tries to visualize, even for images that contain multiple objects. The class-speciﬁc signal, especially for methods that propagate all the way to the input, is often blurred by the salient regions of the image. Some methods avoid this by not propagating to the lower layers [32], while other methods contrast differ-ent classes to emphasize the differences [15]. Our method provides the class-based separation by design and it is the only Transformer visualization method, as far as we can as-certain, that presents this property.
Explainability, interpretability, and relevance are not uni-formly deﬁned in the literature [26]. For example, it is not clear if one would expect the resulting image to contain all of the pixels of the identiﬁed object, which would lead to better downstream tasks [21] and for favorable human impressions, or to identify the sparse image locations that cause the predicted label to dominate. While some meth-ods offer a clear theoretical framework [24], these rely on speciﬁc assumptions and often do not lead to better perfor-mance on real data. Our approach is a mechanistic one and avoids controversial issues. Our goal is to improve the performance on the acceptable benchmarks of the ﬁeld.
This goal is achieved on a diverse and complementary set of computer vision benchmarks, representing multiple ap-proaches to explainability.
These benchmarks include image segmentation on a sub-set of the ImageNet dataset, as well as positive and negative perturbations on the ImageNet validation set. In NLP, we consider a public NLP explainability benchmark [10]. In this benchmark, the task is to identify the excerpt that was marked by humans as leading to a decision. 2.