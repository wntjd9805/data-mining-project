Abstract
Space-time video super-resolution (STVSR) aims to increase the spatial and temporal resolutions of low-resolution and low-frame-rate videos. Recently, deformable convolution based methods have achieved promising STVSR performance, but they could only infer the intermediate frame pre-deﬁned in the training stage. Besides, these meth-ods undervalued the short-term motion cues among adja-cent frames. In this paper, we propose a Temporal Modu-lation Network (TMNet) to interpolate arbitrary interme-diate frame(s) with accurate high-resolution reconstruc-tion. Speciﬁcally, we propose a Temporal Modulation Block (TMB) to modulate deformable convolution kernels for con-trollable feature interpolation. To well exploit the temporal information, we propose a Locally-temporal Feature Com-parison (LFC) module, along with the Bi-directional De-formable ConvLSTM, to extract short-term and long-term motion cues in videos. Experiments on three benchmark datasets demonstrate that our TMNet outperforms previ-ous STVSR methods. The code is available at https:
//github.com/CS-GangXu/TMNet. 1.

Introduction
Nowadays, ﬂat-panel displays using liquid-crystal dis-play (LCD) or light-emitting diode (LED) technologies can broadcast Ultra High Deﬁnition Television (UHD TV) videos with 4K (3840 × 2160) or 8K (7680 × 4320) full-color pixels, at the frame rate of 120 frames per second (FPS) or 240 FPS [39]. However, currently available videos are commonly in Full High Deﬁnition (FHD) with a reso-lution of 2K (1920 × 1080) at 30 FPS [45]. To broadcast
FHD videos on UHD TVs, it is necessary to increase their
∗Jun Xu is (email: nankaimathxu-corresponding author jun@gmail.com). This work is supported by National Natural Science
Foundation of China under Grant 62002176 and 61922046. the
Figure 1: Flexible STVSR performance by our TMNet.
Given input frames at moments 0 (begin) and 1 (end), [45] could only interpolate pre-deﬁned intermediate frame at moment 0.5 (a), while our TMNet can generate interme-diate frames at arbitrary moments (e.g., 0.3, 0.5, 0.7) (b). space-time resolutions comfortably with the broadcasting standard of UHD TVs. Although it is possible to increase the spatial resolution of videos frame-by-frame via single image super-resolution methods [4,22], the perceptual qual-ity of the enhanced videos would be deteriorated by tempo-ral distortion [17]. To this end, the space-time video super-resolution (STVSR) methods [31, 45] are developed to si-multaneously increase the spatial and temporal resolutions of low-frame-rate and low-resolution videos.
Previous model-based STVSR methods [30–32] rely heavily on precise spatial and temporal registration [38], and would produce inferior reconstruction results when the registration is inaccurate. Besides, they usually require huge computational costs on solving complex optimization problems, resulting in low inference efﬁciency [21, 25].
Later, deep convolutional neural networks [12, 13, 34, 44] have been widely employed in video restoration tasks such as video super-resolution (VSR) [3, 37], video frame in-terpretation (VFI) [1, 27, 46], and the more challenging
STVSR [17, 45]. A straightforward solution for STVSR is to perform VFI and VSR successively on low-resolution and low-frame-rate videos, to increase their spatial resolu-tions and frame rates [45]. However, these two-stage meth-ods ignore the inherent correlation between temporal and spatial dimensions. That is, the videos with high-resolution frames contain richer details on moving object(s) and back-6388
ground, while those in high-frame-rate provide ﬁner pixel alignment between adjacent frames [8]. Therefore, these two-stage STVSR methods would suffer from the temporal inconsistency problem [45] and produce artifacts, e.g., “the attentional blink phenomenon” [36] on STVSR.
To well exploit the correlation between the temporal and spatial dimensions in videos, several one-stage STVSR methods [8, 17, 45] have been proposed to simultaneously perform VFI and VSR reconstruction on low-frame-rate and low-resolution videos. The work of STARnet [8] es-timates the motion cues with an additional optical ﬂow branch [5], and performs feature warping of two adja-cent frames to interpolate the intermediate frame. But this ﬂow-based method [8] needs to learn an extra branch for optical ﬂow estimation, which consumes expensive costs on computation and memory. To alleviate this prob-lem, Xiang et al. [45] employed the deformable convolu-tion backbone [41], and directly performed STVSR on the feature space. Though with promising performance, cur-rent STVSR networks could only generate the intermediate frames pre-deﬁned in the network architecture, and thus are restricted to highly-controlled application scenarios with
ﬁxed frame-rate videos. However, in many commercial sce-narios, such as sports events, it is very common for the user to ﬂexibly adjust the intermediate video frames for better visualization. Thus, it is necessary to develop controllable
STVSR methods for smooth motion synthesizing.
To fulﬁll the versatile requirements of broadcasting sce-narios, in this paper, we propose a Temporal Modulation
Network (TMNet) to interpolate an arbitrary number of in-termediate frames for STVSR, as shown in Figure 1. But current deformable convolution based methods [45] could only generate pre-deﬁned intermediate frame(s). To tackle this problem, we propose a Temporal Modulation Block (TMB) to incorporate motion cues into the feature inter-polation of intermediate frames. Speciﬁcally, we ﬁrst es-timate the motion between two adjacent frames under the deformable convolution framework [41], and learn control-lable interpolation at an arbitrary moment deﬁned by a tem-poral parameter.
In addition, we also propose a Locally-temporal Feature Comparison module to fuse multi-frame features for effective spatial alignment and feature warping, and a globally-temporal feature fusion to explore the long-term variations of the whole video. This two-stage temporal feature fusion scheme accurately interpolates the interme-diate frames for STVSR. Extensive experiments on three benchmarks [23,35,46] demonstrate that our TMNet is able to interpolate an arbitrary number of intermediate frames, and achieves state-of-the-art performance on STVSR.
The contribution of this work are three-fold:
• We propose a Temporal Modulation Network (TM-Net) to perform controllable interpolation of arbi-trary frame-rates for ﬂexible STVSR performance.
This is achieved by our Temporal Modulation Block under the deformable convolution framework.
• We present a two-stage temporal feature fusion scheme for effective STVSR. Speciﬁcally, we pro-pose a locally-temporal feature comparison module to exploit the short-term motion cues of adjacent frames, and perform globally-temporal feature fusion by ex-ploring the long-term variations over the whole video.
• Experiments on three benchmarks show that our TM-Net is able to perform controllable frame interpola-tion at arbitrary frame-rate, and outperforms state-of-the-art STVSR methods. 2.