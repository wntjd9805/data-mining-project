Abstract
This paper addresses the problem of 3D human body shape and pose estimation from RGB images. Recent progress in this ﬁeld has focused on single images, video or multi-view images as inputs. In contrast, we propose a new task: shape and pose estimation from a group of multi-ple images of a human subject, without constraints on sub-ject pose, camera viewpoint or background conditions be-tween images in the group. Our solution to this task predicts distributions over SMPL body shape and pose parameters conditioned on the input images in the group. We proba-bilistically combine predicted body shape distributions from each image to obtain a ﬁnal multi-image shape prediction.
We show that the additional body shape information present in multi-image input groups improves 3D human shape es-timation metrics compared to single-image inputs on the
SSP-3D dataset and a private dataset of tape-measured hu-mans. In addition, predicting distributions over 3D bodies allows us to quantify pose prediction uncertainty, which is useful when faced with challenging input images with sig-niﬁcant occlusion. Our method demonstrates meaningful pose uncertainty on the 3DPW dataset and is competitive with the state-of-the-art in terms of pose estimation metrics. 1.

Introduction 3D human body shape and pose estimation from RGB images is a challenging problem with potential applications in augmented and virtual reality, healthcare and ﬁtness tech-nology and virtual retail. Recent solutions have focused on three types of inputs: i) single images [7, 48, 19, 27, 28, 57, 36, 45, 38, 41, 50], ii) video [26, 20, 47, 40, 16] with temporal constraints on pose, camera viewpoint and back-ground conditions and iii) multi-view images [32, 46] with a ﬁxed subject pose captured from multiple viewpoints. In contrast, we aim to estimate 3D body shape and pose from a group of images of the same human subject without any constraints on the subject’s pose, camera viewpoint or back-Figure 1: Example shape and pose predictions from a group of input images. Probabilistic shape combination results in a more accurate body shape estimate than both individual single-image predictions (visualised here from SPIN [27] and STRAPS [45]) and naively-averaged single-image pre-dictions, as our experiments show in Section 5. ground conditions between the images, as illustrated in Fig-ure 1. This task is motivated by the intuition that multiple images of the same subject should contain additional visual information about their body shape compared to a single im-age, regardless of whether the subject’s pose or surrounding environment change between images. A suitable shape and pose estimator should leverage this information to improve shape prediction accuracy over single-image methods.
We present a probabilistic body shape and pose estima-tion method from a group of unconstrained images of the same subject. Inference occurs in three stages (see Figure 2). First, we predict a proxy representation from each in-put image in the group, consisting of the subject’s silhouette and 2D joint location heatmaps, using off-the-shelf segmen-tation and 2D keypoint detection CNNs [14, 13, 25, 53].
Then, each proxy representation is passed through a 3D distribution prediction network that outputs a probability distribution over SMPL [33] body shape and pose pa-rameters conditioned on the input representation. Lastly, body shape distributions from each input image are prob-16094
abilistically combined to procure a ﬁnal shape prediction.
This yields a better estimate of the subject’s body shape than current single-image body shape and pose estimators
[19, 27, 28, 57, 45], which may be inaccurate or inconsis-tent, as shown in Figure 1.
Moreover, most single-image body model parameter re-gressors [19, 27, 38, 57, 45, 54, 10] do not consider the un-certainty associated with each pose parameter estimate. If certain body parts are occluded or out-of-frame in the input image, the estimator can only guess about the pose parame-ters corresponding to these body parts. Such situations fur-ther motivate our approach of predicting a distribution over body pose, since the variance of the distribution quantiﬁes the uncertainty associated with each pose parameter predic-tion, as shown in Figures 3 and 4.
Training body model parameter regressors to accurately predict body shape is challenging due to the lack of suitable training datasets of in-the-wild images paired with accurate and diverse body shape labels. Collecting such data is prac-tically difﬁcult, particularly for our proposed task of shape estimation from a group of unconstrained images. Recent works [45, 46, 51] propose using synthetic input-label pairs to overcome the lack of suitable training datasets. We adopt the same synthetic training approach as STRAPS [45] to train our 3D distribution prediction network, but extend the data augmentations used to bridge the gap between syn-thetic and real inputs. In particular, our synthetic training data better models occluded and out-of-frame body parts in silhouettes and joints such that the domain gap to real occluded data is smaller. This allows our method to es-timate pose prediction uncertainty and also results in im-proved single-input pose prediction metrics on challenging evaluation datasets, such as 3DPW [52].
In summary, our main contributions are as follows:
• We propose a novel task: predicting body shape from a group of images of the same human subject, with-out imposing any constraints on subject pose, camera viewpoint or backgrounds between the images.
• We present a solution to the proposed task which pre-dicts a distribution over 3D human body shape and pose parameters conditioned on the input images in the group. Body shape distributions from each image are probabilistically combined to yield a ﬁnal body shape estimate which leverages multi-image shape informa-tion, resulting in a more accurate body shape estimate compared to single-input methods.
• To the best of our knowledge, our method is the
ﬁrst to output uncertainties alongside associated SMPL
[33] shape and pose parameter predictions, which are shown to be useful when input images contain oc-cluded or out-of-frame body parts.
• We extend the synthetic training framework introduced by [45] to better model occlusion and missing body parts, allowing our synthetically-trained distribution prediction neural network to yield better 3D shape and pose metrics. 2.