Abstract
We introduce an inversion based method, denoted as
IMAge-Guided model INvErsion (IMAGINE), to generate high-quality and diverse images from only a single train-ing sample. We leverage the knowledge of image seman-tics from a pre-trained classiﬁer to achieve plausible gen-erations via matching multi-level feature representations in the classiﬁer, associated with adversarial training with an external discriminator. IMAGINE enables the synthesis procedure to simultaneously 1) enforce semantic speciﬁcity constraints during the synthesis, 2) produce realistic images without generator training, and 3) give users intuitive con-trol over the generation process. With extensive experimen-tal results, we demonstrate qualitatively and quantitatively
*Work done during internship at Adobe Research that IMAGINE performs favorably against state-of-the-art
GAN-based and inversion-based methods, across three dif-ferent image domains (i.e., objects, scenes, and textures). 1.

Introduction
Consider a reference image from the left-most column of
Fig. 1, we humans could easily imagine its variants in other formats, e.g., patterns with different spatial arrangements, objects in different positions or viewpoints. Can we give machines such abilities to automatically synthesize seman-tically meaningful variations of a reference image? Such a system can generate interesting variations of assets for de-sign ideations and can also be utilized as a data augmenta-tion technique to beneﬁt downstream data-hungry tasks.
With the popularity of generative adversarial networks 3681
Repetitive
Non-repetitive repetitive non-repetitive
SinGAN
Ours e g a m i g n i n i a r
T
N
A
G n i
S
I
E
N
G
A
M
I
Figure 2: Image generation by SinGAN [29] and our IMAGINE.
SinGAN only works well on repetitive images that contain repet-itive structures such as mountains, but fails on non-repetitive im-ages containing large semantic structures such as the ostrich and the goldﬁsh. IMAGINE can generate higher quality and more di-verse images on both types. (GANs) [12, 3, 21, 24, 6], two categories of GAN-based approaches have been proposed for editing a speciﬁc image and/or synthesizing its variations. The ﬁrst one is based on
GAN projection (or inversion) [28, 32, 5, 16], which con-sists of projecting the target image into the latent space of the generator, jittering the projected latent code, and synthe-sizing a new image. However, there are several limitations to this type of approach. First, the projection step is far from accurate often resulting in unrealistic images due to the pro-jected latent code falling outside of the learning distribu-tion [32]. Second, the projection step can only handle the type of images that the generator is trained on. Since exist-ing GANs are trained on speciﬁc domains, such as faces, or datasets containing a few classes [21, 32, 27, 47, 4, 16], the projection-based approaches cannot generalize. The sec-ond category is to learn individual GANs for individual tar-get images, e.g., SinGAN [29]. SinGAN leverages image patches from the target image to learn image internal statis-tics. While enforcing patch consistency across resolutions enables the synthesis of multi-scale structures, SinGAN is best suited for the synthesis of repetitive images, like the mountain in Fig. 2. However, as the patch-based learning is less effective in capturing high-level semantics, it struggles to synthesize images of non-repetitive objects. As shown in the second row of Fig. 2, properties like object identity, shape, or consistency among parts are lost in their results.
This problem can be solved by model inversion of a pre-trained deep classiﬁer which can capture multi-scale ab-stract features. Model inversion seeks to determine the im-age responsible for a particular classiﬁer prediction [23, 43, 26, 42]1. Given the prediction, an optimization is performed to synthesize the image that best explains it. Although sev-1In the remainder of the paper, we use “model inversion” to refer to
“classiﬁer inversion”. eral works have shown that introducing strong regulariz-ers [23, 26, 42] can improve the quality of synthesized im-ages, the quality is still not comparable to GANs. More im-portantly, all these methods suffer from lack of speciﬁcity.
They simply generate random images with the appearance characteristics of the target class, but they cannot synthesize variations of a speciﬁc image.
In this work, to overcome the limitations of these previ-ous methods, we introduce the concept of IMAge-Guided model INvErsion (IMAGINE). IMAGINE leverages the knowledge of image semantics from a pre-trained classiﬁer (e.g., ResNet-50 pre-trained on ImageNet [8]) to generate semantically meaningful variations of a target. Speciﬁcally, we feed the target image into the classiﬁer and optimize for a new image that best explains its class prediction, under the constraint that the features of the synthesized image must match those of the target at various network layers. This is then complemented by the adversarial training to further improve the realism of the synthesized images. However, unlike GANs, our adversarial training is conducted by al-ternating the optimization of the synthesized image and that of the discriminator weights.
The resulting algorithm enables the synthesis procedure to simultaneously 1) enforce semantic constraints during the synthesis, 2) produce realistic images, without the need for a separate generator training, and 3) allow multifaceted control over the image to be synthesized. When compared to the SinGAN, it captures much more abstract high-level image semantics, such as object shape or identity. This enables the successful synthesis of non-repetitive images, such as the objects of Fig. 1 and 2. Because the feature matching constraints are based on the matching of distribu-tions, rather than minimizing geometric distances between feature maps [28, 34], IMAGINE leads to more visual vari-ations in the synthesized results (Fig. 1). We show that, by manipulating the target feature statistics at the different levels of the network, it is possible to recreate objects with modiﬁed semantics, such as different shapes (Fig. 7). Fur-thermore, through the use of attribution functions [31, 37], it is even possible to manipulate the location of the target objects in the synthesized image (Fig. 6). Several other ap-plications of IMAGINE are also discussed.
Our paper makes four main contributions. (1) To the best of our knowledge, this is the ﬁrst attempt to utilize model inversion for image-guided synthesis problem. (2) We in-troduce adversarial training to improve the image quality of optimization-based model inversion. (3) We perform exten-sive quantitative and qualitative evaluations to demonstrate the superiority of IMAGINE over GAN counterparts and its generalizability across different image domains includ-ing single objects in front of simple backgrounds, complex scenes, and non-stationary textures. (4) We show multiple multifaceted image control applications including a newly 3682  
proposed object position control task. 2.