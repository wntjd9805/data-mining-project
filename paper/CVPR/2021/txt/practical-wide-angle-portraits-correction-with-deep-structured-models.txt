Abstract
Wide-angle portraits often enjoy expanded views. How-ever, they contain perspective distortions, especially notice-able when capturing group portrait photos, where the back-ground is skewed and faces are stretched. This paper in-troduces the ﬁrst deep learning based approach to remove such artifacts from freely-shot photos. Speciﬁcally, given a wide-angle portrait as input, we build a cascaded net-work consisting of a LineNet, a ShapeNet, and a transition module (TM), which corrects perspective distortions on the background, adapts to the stereographic projection on fa-cial regions, and achieves smooth transitions between these two projections, accordingly. To train our network, we build the ﬁrst perspective portrait dataset with a large diversity in identities, scenes and camera modules. For the quantita-tive evaluation, we introduce two novel metrics, line consis-tency and face congruence. Compared to the previous state-of-the-art approach, our method does not require camera distortion parameters. We demonstrate that our approach signiﬁcantly outperforms the previous state-of-the-art ap-proach both qualitatively and quantitatively. 1.

Introduction
With the popularity of wide-angle cameras on smart-phones, photographers can take pictures with broad vision.
However, a wider ﬁeld-of-view often introduces a stronger perspective distortion. All wide-angle cameras suffer from distortion artifacts that stretch and twist buildings, road ridges and faces, as shown in Fig. 1 (a).
There are relatively few works targeting on the perspec-tive distortion correction in portrait photography[8, 7]. Pre-vious methods apply perspective undistortion using cam-*Equal contribution
†Corresponding author (a) Input Image (b) Projection Image (c) Shih’s Result (d) Our Result
Figure 1. Examples of distorted and corrected photographs. (a) the original distortion image with curved background and distorted faces. (b) projection image with straight lines. (c) result by Shih et al. [21], and (d) result of the proposed deep learning method. Both the background and faces are corrected in (d). era calibrated distortion parameters [18, 23, 2, 6], which projects the image onto a plane for undistortion, as shown in Fig. 1 (b). Compared with Fig. 1 (a), the lines at the background become straight. Unfortunately, the faces are also projected as a plane, becoming unnaturally wider and asymmetric. It is then evident that background and faces re-quire different types of corrections, to be separately handled with different strategies. As traditional calibration-based methods[1, 12, 19, 32] can only correct distortion in back-ground regions, we need new ways to process faces.
Recently, Shih et al. [21] proposes to deform a mesh which adapts to the stereographic projection [22] on facial regions, and applies perspective projection over the back-ground, enabling different handling of the background and faces. However, a new problem arises, where the smooth transition between faces and background regions is non-trivial. In addition, the method [21] requires camera distor-tion parameters as well as the portrait segmentation mask as additional inputs. Fig. 1 (c) shows the result, where the face 3498
in the corner has been over corrected and appear deformed.
In contrast, our approach does not rely on any prior calibrated parameters, thus being more ﬂexible to various conditioned portraits. Compared to the mesh-based energy minimization [21], our deep solution works well in balanc-ing the perspective projection on the background and stere-ographic projection on the faces, delivering smooth transi-tions between them. Fig. 1 (d) shows our result.
To this end, we propose a deep structured network to generate a content-aware warping ﬂow ﬁeld, which both straightens the background lines through perspective undis-tortion, and adapts to the stereographic projection on facial regions, notably achieving smooth transitions between these two projections. Our cascaded network includes a Line Cor-rection Network (LineNet) and a Portrait Correction Net-work (ShapeNet). Speciﬁcally, given an input image, the
LineNet is ﬁrst applied to produce a ﬂow ﬁeld to undistort the perspective effects for line correction, where a Line At-tention Module (LAM) is introduced to facilitate the local-ization of lines. Second, the projected image is fed into the ShapeNet for face correction, within which a Face At-tention Module (FAM) is introduced for face localization.
Furthermore, we design a Transition Module (TM) between
LineNet and ShapeNet to ensure smooth transitions.
As there is no proper dataset readily available for train-ing, we build a high-quality wide-angle portrait dataset.
Speciﬁcally, we capture portrait photos by smartphones with various wide-angle lenses and then interactively cor-rect them with a specially designed content-aware mesh warping tool, yielding 5, 854 pairs of input and output im-ages for training. Moreover, for quantitative evaluations, we introduce two novel metrics, Line Straightness Metric (Lin-eAcc) and Shape Congruence Metric (ShapeAcc) to eval-uate the line straightness and face correctness accordingly.
Previously, evaluation can only be made qualitatively.
Experimental results show that our approach can cor-rect distortions in wide-angle portraits. Compared with calibration-based opponents, our method can rectify the faces faithfully without camera parameters. Compared with
Shih’s method [21], our method is calibration-free, and achieves good transitions between background and face re-gions. Both qualitative and quantitative evaluations are pro-vided to validate the effectiveness of our method. Our main contributions are:
• We provide a new perspective portrait dataset for im-age undistortion with a wide range of subject identities, scenes and camera modules. In addition, two universal metrics are designed for the quantitative evaluation. 2.