Abstract
Transfer learning across heterogeneous data distribu-tions (a.k.a. domains) and distinct tasks is a more gen-eral and challenging problem than conventional transfer learning, where either domains or tasks are assumed to be the same. While neural network based feature transfer is widely used in transfer learning applications, ﬁnding the optimal transfer strategy still requires time-consuming ex-periments and domain knowledge. We propose a transfer-ability metric called Optimal Transport based Conditional
Entropy (OTCE), to analytically predict the transfer per-formance for supervised classiﬁcation tasks in such cross-domain and cross-task feature transfer settings. Our OTCE score characterizes transferability as a combination of do-main difference and task difference, and explicitly evalu-ates them from data in a uniﬁed framework. Speciﬁcally, we use optimal transport to estimate domain difference and the optimal coupling between source and target distribu-tions, which is then used to derive the conditional entropy of the target task (task difference). Experiments on the largest cross-domain dataset DomainNet and Ofﬁce31 demonstrate that OTCE shows an average of 21% gain in the correlation with the ground truth transfer accuracy compared to state-of-the-art methods. We also investigate two applications of the OTCE score including source model selection and multi-source feature fusion. 1.

Introduction
Transfer learning is a useful learning paradigm to im-prove the performance on target tasks with the help of related source tasks (or source models), especially when only few labeled target data are available for supervi-sion [30, 37]. A Transferability metric can quantitatively reveal how easy it is to transfer knowledge learned from a source task to the target task [13, 4, 36, 24].
It indeed provides a road map for conducting transfer learning in
B Corresponding author. This research is funded by Natural Science
Foundation of China 62001266.
Figure 1. Illustration of three different transfer learning scenar-ios, i.e., transductive domain adaptation [27], cross-task trans-fer [5, 24] and the cross-domain cross-task transfer we investi-gating. We take the digital number classiﬁcation as an example, where the cross-domain cross-task transfer setting suffers both do-main difference and task difference. practice, e.g., selecting highly transferable tasks for joint training [41], or understanding task relationships for source model selection [5, 1, 38, 24].
While theoretical results in transfer learning such as
[6, 8, 9, 21] suggest that task relationship can be modeled by certain divergence between the source and target data gener-ating distributions, they are difﬁcult to estimate in practice when target training data is limited. Previous transferability metrics [41, 40, 1] empirically calculate the task relation-ships indicated by training loss or validation accuracy, thus they need to retrain the source model involving expensive computation. Recent analytical metrics [5, 38, 24] are lim-ited by strict assumptions on data. NCE [38] assumes that source and target tasks share the same input instances; H-score [5] assumes that source and target data are distributed in the same domain. Although LEEP [28] does not make any assumptions on source and target data except for hav-ing the same input size, it does not work sufﬁciently well under the cross-domain setting.
In this paper, we investigate the transferability estimation problem for classiﬁcation tasks under the more challenging 15779
cross-domain cross-task setting, as illustrated in Figure 1.
For most transfer learning problems we encounter in prac-tice, we cannot assume the source and target data are gen-erated from the same distribution (domain) since domain gaps commonly exist in real life due to different acquisi-tion devices and different physical environments. Mean-while, we also cannot always assume no task difference ex-ists in a transfer learning application as in transductive do-main adaptation [27], i.e., source and target tasks have the same category set. We emphasize that the cross-domain cross-task setting is more challenging compared to previ-ous settings that require shared input data or same domain, since both domain difference and task difference deteriorate the transfer performance [27, 5, 26].
To this end, we propose a novel cross-domain cross-task transferability metric called the Optimal Transport based
Conditional Entropy, abbreviated as OTCE score. On one hand, compared to the empirical methods [41, 40, 1] that need to retrain the source model using gradient descent to estimate the empirical transfer error, our metric is more ef-ﬁcient (about 75x faster) to compute. On the other hand, our OTCE score explicitly learns the domain difference and task difference in a uniﬁed framework, providing a more interpretable result compared to recent analytical met-rics [24, 38, 5].
More speciﬁcally, we measure the domain difference between source and target data using Wasserstein dis-tance computed by solving the classic Optimal Transport (OT) [18, 29] problem. The OT problem also estimates the joint probability between source and target samples, which allows us to derive the task difference in terms of the con-ditional entropy between the source and target task labels.
Finally, we learn a linear model of transfer accuracy on do-main difference and task difference, drawing transfer expe-rience evaluated on a few auxiliary tasks. Albeit its simplic-ity, the learned model makes it easier to decompose trans-ferability into different factors through model coefﬁcients.
Extensive experiments on the largest cross-domain dataset DomainNet [28] and Ofﬁce31 [34] demonstrate that our OTCE score shows signiﬁcantly higher correlation with transfer accuracy, i.e., predicting the transfer performance more accurately with an average of 21% gain compared to state-of-the-art metrics [24, 38, 5]. In addition, we fur-ther investigate two applications of transferability in source model selection and multi-source feature fusion. In sum-mary, our contributions are follows: 1) To our knowledge, we are the ﬁrst to analytically in-vestigate the transferability estimation problem for super-vised classiﬁcation tasks under the more general and chal-lenging cross-domain cross-task setting. 2) We propose a novel cross-domain cross-task transfer-ability metric OTCE score which can explicitly evaluate do-main difference and task difference in a uniﬁed framework, and predict the transfer performance in advance. 3) We show consistent superior performance in predict-ing transfer performance compared to state-of-the-art met-rics and also investigate the applications of OTCE score in source model selection and multi-source feature fusion. 2.