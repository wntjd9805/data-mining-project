Abstract
Image-to-image translation aims to preserve source con-tents while translating to discriminative target styles be-tween two visual domains. Most works apply adversarial learning in the ambient image space, which could be com-putationally expensive and challenging to train. In this pa-per, we propose to deploy an energy-based model (EBM) in the latent space of a pretrained autoencoder for this task. The pretrained autoencoder serves as both a latent code extractor and an image reconstruction worker. Our model, LETIT1, is based on the assumption that two do-mains share the same latent space, where latent represen-tation is implicitly decomposed as a content code and a domain-speciﬁc style code. Instead of explicitly extracting the two codes and applying adaptive instance normaliza-tion to combine them, our latent EBM can implicitly learn to transport the source style code to the target style code while preserving the content code, an advantage over exist-ing image translation methods. This simpliﬁed solution is also more efﬁcient in the one-sided unpaired image trans-lation setting. Qualitative and quantitative comparisons demonstrate superior translation quality and faithfulness for content preservation. Our model is the ﬁrst to be ap-plicable to 1024×1024-resolution unpaired image trans-lation to the best of our knowledge. Code is available at https://github.com/YangNaruto/latent-energy-transport. 1.

Introduction
The unpaired image-to-image translation aims to learn pairwise domain mappings without being aware of any paired-image information. Suppose a task of translating between two domains of male and female, denoted as X and Y and illustrated in Figure 1. Ideally, one should be able to retain the shared contents, e.g., the irrelevant back-ground and the rough facial skeleton, and only focus on transferring discriminative styles, e.g., hair and beard. Most 1Latent Energy Transport for Image Translation existing models adopt generative adversarial nets (GANs)
[12, 17] to enforce the translated style of source instances to be indistinguishable from that of the target domain, which typically relies on an explicit cycle consistency regularizer
[42, 46, 22] to maintain the content. However, the enforced cycle consistency is often restrictive, and the learning of two roughly invertible mappings, to some extent, can hin-der model optimization efﬁciency. CUT [31] resorts to con-trastive learning as an alternative in the one-sided transla-tion setting. Still, GAN-based approaches need to learn at least one set of an encoder-decoder structured generator and an encoder-based discriminator, which is usually computa-tionally expensive to train [45].
Apart from GAN-based solutions, CF-EBM [45], one of the most recent works, applies the energy-based model (EBM) to realize implicit image translation by direct maxi-mum likelihood estimation (MLE). However, EBM learning leverages the Langevin dynamics for Markov Chian Monte
Carlo (MCMC) sampling in the ambient data space, which usually distorts image pixels and is challenging to scale up.
Besides, it is unclear whether EBM can learn a disentan-gled representation of the content and style for better image translation [25].
To overcome the above issues, we propose a plug-and-play EBM-based model in the latent space. Speciﬁcally, we
ﬁrst pretrain an autoencoder (or use an existing one) and then plug the EBM into the latent space to manipulate the extracted latent code to realize image translation. Our latent
EBM models an explicit density distribution of latent vari-ables by training a bottom-up latent energy function, which always assigns lower energy to latent variables of the tar-get domain data and higher energy to those of the source domain data. Hence, when sampling from the EBM via
Langevin dynamics, the energy gradient (score function) describes a path to transport the latent codes from source to target domains. Most interestingly, we demonstrate that the score function can implicitly and automatically sepa-rate both the content and style codes from the whole latent embedding. Thus image translation corresponds to simply evolving the style code. As can be seen in Figure 1, only the style appearance is translated while the content information 16418
Figure 1. Unpaired image-to-image translation on 1024×1024-resolution images. (Left) female to male; (Right) male to female. (the background) is preserved. 2.