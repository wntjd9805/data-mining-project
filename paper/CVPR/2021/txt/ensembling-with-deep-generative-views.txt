Abstract
Recent generative models can synthesize “views” of ar-tiﬁcial images that mimic real-world variations, such as changes in color or pose, simply by learning from unla-beled image collections. Here, we investigate whether such views can be applied to real images to beneﬁt downstream analysis tasks such as image classiﬁcation. Using a pre-trained generator, we ﬁrst ﬁnd the latent code correspond-ing to a given real input image. Applying perturbations to the code creates natural variations of the image, which can then be ensembled together at test-time. We use StyleGAN2 as the source of generative augmentations and investigate this setup on classiﬁcation tasks involving facial attributes, cat faces, and cars. Critically, we ﬁnd that several design decisions are required towards making this process work; the perturbation procedure, weighting between the augmen-tations and original image, and training the classiﬁer on synthesized images can all impact the result. Currently, we
ﬁnd that while test-time ensembling with GAN-based aug-mentations can offer some small improvements, the remain-ing bottlenecks are the efﬁciency and accuracy of the GAN reconstructions, coupled with classiﬁer sensitivities to arti-facts in GAN-generated images. 1.

Introduction
Image datasets are the backbone of learning-based vi-sion problems, but images are only sparsely-sampled dis-crete snapshots of the underlying continuous world. How-ever, recent generative adversarial networks (GANs) [18] have shown promise in learning to imitate the real-image manifold, mapping random samples from a latent distribu-tion to realistic image outputs. A heavily exploited property of these models is that the latent space is locally smooth: samples nearby in latent space will appear perceptually sim-ilar in image space [42]. Therefore, GANs can be viewed as a type of “interpolating mechanism” that can blend and recombine images in a continuous manner. From individ-ual image samples, can we use a GAN to generate nearby alternatives on the image manifold, or “views,” giving us unlimited variants of a given image?
Input Image
C
Generator Samples
Σ
G
C
C
C
Generator
Classifier
Latent 
Space
Figure 1: We project an input image into the latent space of a pre-trained GAN and perturb it slightly to obtain modiﬁcations of the input image. These alternative views from the GAN are ensem-bled at test-time, together with the original image, in a downstream classiﬁcation task.
Here, we investigate using GAN outputs as test-time augmentation for classiﬁcation tasks. In the standard classi-ﬁcation pipeline, passing an image through a trained classi-ﬁer yields predictions of the image belonging to one of sev-eral classes. However, performance is often improved using more than one sample – if we have multiple views of an im-age, we can use the classiﬁer to obtain predictions for each view and average the results together as an ensemble. The classic approach to generating additional views has been to crop the image at different locations before classiﬁcation.
Using a GAN, we have an orthogonal, data-driven method of generating additional views of a given image, such as al-tering its pose, shape, or color based on the directions of variation that a GAN learns.
A secondary advantage of unconditional GANs is that they can be trained on image collections without requiring image labels. As data labeling is often vastly more expen-sive than data collection, GANs learn from much larger datasets compared to tasks involving manual annotation, such as classiﬁcation. Training on large datasets allows sev-114997
eral interesting properties to emerge, where the generator learns meaningful variations in data without requiring an explicit training objective to do so [21, 53, 12].
A challenge in using GANs to generate augmented sam-ples is the potential domain gap between real images and
GAN outputs – the generated samples must be of sufﬁcient quality to be used in classiﬁcation tasks, and must ade-quately reconstruct the target image sample while preserv-ing the relevant visual patterns for accurate discrimination.
If this condition is not met, the classiﬁer may behave dif-ferently on the generated samples than the natural images, which is undesirable for data augmentation. To reconstruct image samples using the GAN, we use a hybrid encoder and optimization approach [69]: the encoder network initializes the latent code, which is further optimized to improve the similarity between the target image and the reconstruction.
In addition to high-quality reconstruction, we also desire that the generated variations of an image do not cross clas-siﬁer boundaries; i.e., it cannot modify visual appearance that affect its classiﬁcation. To this end, we experiment with a variety of possible image modiﬁcations using the GAN generator, ranging from coarse pose and shape changes to
ﬁner-grained color changes.
Using the recent StyleGAN2 generator [28], we apply our method on several classiﬁcation tasks involving facial attributes [33, 26], cat faces [39], and cars [29]. Given the relative simplicity of the face domain, we ﬁnd test-time en-sembling with GAN samples helps even when the classiﬁer is trained only on real images; however, training the classi-ﬁer on generated samples offers further improvements, par-ticularly for the more difﬁcult car and cat domains. Code is available on our website: https://chail.github. io/gan-ensembling/. 2.