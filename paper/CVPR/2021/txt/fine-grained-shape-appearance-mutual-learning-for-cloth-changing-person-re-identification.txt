Abstract (cid:28636)(cid:28631)(cid:28595)(cid:28612) (cid:28660)(cid:28675)(cid:28675)(cid:28664)(cid:28660)(cid:28677)(cid:28660)(cid:28673)(cid:28662)(cid:28664) (cid:28636)(cid:28631)(cid:28595)(cid:28612) (cid:28636)(cid:28631)(cid:28595)(cid:28612) (cid:28672)(cid:28660)(cid:28678)(cid:28670) (cid:28636)(cid:28631)(cid:28595)(cid:28612)
Recently, person re-identiﬁcation (Re-ID) has achieved great progress. However, current methods largely depend on color appearance, which is not reliable when a person changes the clothes. Cloth-changing Re-ID is challenging since pedestrian images with clothes change exhibit large intra-class variation and small inter-class variation. Some signiﬁcant features for identiﬁcation are embedded in unob-vious body shape differences across pedestrians. To explore such body shape cues for cloth-changing Re-ID, we propose a Fine-grained Shape-Appearance Mutual learning frame-work (FSAM), a two-stream framework that learns ﬁne-grained discriminative body shape knowledge in a shape stream and transfers it to an appearance stream to com-plement the cloth-unrelated knowledge in the appearance features. Speciﬁcally, in the shape stream, FSAM learns
ﬁne-grained discriminative mask with the guidance of iden-tities and extracts ﬁne-grained body shape features by a pose-speciﬁc multi-branch network. To complement cloth-unrelated shape knowledge in the appearance stream, dense interactive mutual learning is performed across low-level and high-level features to transfer knowledge from shape stream to appearance stream, which enables the appear-ance stream to be deployed independently without extra computation for mask estimation. We evaluated our method on benchmark cloth-changing Re-ID datasets and achieved the start-of-the-art performance. 1.

Introduction
Person re-identiﬁcation (Re-ID) aims at matching the same person across different cameras. Advanced meth-# Equal contribution. Work done during the internship at Huya Inc.
* Corresponding author. (cid:28636)(cid:28631)(cid:28595)(cid:28613) (cid:28636)(cid:28631)(cid:28595)(cid:28613) (cid:28603)(cid:28660)(cid:28604)(cid:28595)(cid:28631)(cid:28668)(cid:28678)(cid:28662)(cid:28677)(cid:28668)(cid:28672)(cid:28668)(cid:28673)(cid:28660)(cid:28679)(cid:28668)(cid:28681)(cid:28664)(cid:28595)(cid:28678)(cid:28667)(cid:28660)(cid:28675)(cid:28664)(cid:28595)(cid:28663)(cid:28668)(cid:28665)(cid:28665)(cid:28664)(cid:28677)(cid:28664)(cid:28673)(cid:28662)(cid:28664)(cid:28595)(cid:28668)(cid:28673)(cid:28595)(cid:28665)(cid:28668)(cid:28673)(cid:28664)(cid:28608)(cid:28666)(cid:28677)(cid:28660)(cid:28668)(cid:28673)(cid:28664)(cid:28663)(cid:28595)(cid:28672)(cid:28660)(cid:28678)(cid:28670)(cid:28678) (cid:28621)(cid:28663)(cid:28668)(cid:28678)(cid:28678)(cid:28668)(cid:28672)(cid:28668)(cid:28671)(cid:28660)(cid:28677) (cid:28621)(cid:28678)(cid:28668)(cid:28672)(cid:28668)(cid:28671)(cid:28660)(cid:28677) (cid:28636)(cid:28631)(cid:28595)(cid:28614) (cid:28636)(cid:28631)(cid:28595)(cid:28615) (cid:28603)(cid:28612)(cid:28604)(cid:28595)(cid:28646)(cid:28668)(cid:28672)(cid:28668)(cid:28671)(cid:28660)(cid:28677)(cid:28595)(cid:28678)(cid:28667)(cid:28660)(cid:28675)(cid:28664)(cid:28595)(cid:28661)(cid:28664)(cid:28679)(cid:28682)(cid:28664)(cid:28664)(cid:28673)(cid:28595)(cid:28675)(cid:28664)(cid:28663)(cid:28664)(cid:28678)(cid:28679)(cid:28677)(cid:28668)(cid:28660)(cid:28673)(cid:28678) (cid:28636)(cid:28631)(cid:28595)(cid:28616) (cid:28636)(cid:28631)(cid:28595)(cid:28616) (cid:28603)(cid:28613)(cid:28604)(cid:28595)(cid:28643)(cid:28674)(cid:28678)(cid:28664)(cid:28595)(cid:28681)(cid:28660)(cid:28677)(cid:28668)(cid:28660)(cid:28679)(cid:28668)(cid:28674)(cid:28673) (cid:28603)(cid:28614)(cid:28604)(cid:28595)(cid:28640)(cid:28668)(cid:28678)(cid:28678)(cid:28668)(cid:28673)(cid:28666)(cid:28595)(cid:28675)(cid:28660)(cid:28677)(cid:28679)(cid:28678) (cid:28603)(cid:28661)(cid:28604)(cid:28595)(cid:28643)(cid:28677)(cid:28674)(cid:28661)(cid:28671)(cid:28664)(cid:28672)(cid:28678)(cid:28595)(cid:28674)(cid:28665)(cid:28595)(cid:28660)(cid:28675)(cid:28675)(cid:28671)(cid:28684)(cid:28668)(cid:28673)(cid:28666)(cid:28595)(cid:28664)(cid:28678)(cid:28679)(cid:28668)(cid:28672)(cid:28660)(cid:28679)(cid:28664)(cid:28663)(cid:28595)(cid:28672)(cid:28660)(cid:28678)(cid:28670)(cid:28678)(cid:28595)(cid:28682)(cid:28668)(cid:28679)(cid:28667)(cid:28595)(cid:28664)(cid:28677)(cid:28677)(cid:28674)(cid:28677)(cid:28678)
Figure 1. Examples of images and masks in cloth-changing Re-ID.
Color appearance under cloth-changing suffers from large intra-class variation and small inter-class variation, while body shape contains cloth-unrelated clues. ods have achieved high performance with deep learning
[1, 34, 42, 45, 40, 53]. However, most current works largely rely on color appearance based on the assumption that the same person wears the same clothes in short term. Such limitation brings dramatic performance decrease in the sit-uation that persons change their clothes, since different per-sons with similar clothes may be wrongly matched. To address this problem, we study the cloth-changing Re-ID problem [46, 29, 48, 13].
As the color appearance becomes unreliable in cloth-changing Re-ID, it is critical to learn cloth-unrelated fea-tures. Under moderate clothing change, the body shape does not change signiﬁcantly for the same person and it is an important cue for identiﬁcation. As shown in Figure 1 (a), the manually labeled ﬁne-grained masks can capture detailed shape differences between pedestrians. In practi-cal situations without manual label, human mask can be estimated by pretrained human parsing models. However, 10513
as shown in Figure 1 (b), error of estimated masks in-curs difﬁculties for exploiting accurate body shape cues.
First, the shapes of estimated coarse masks in correspond-ing body parts of different pedestrians may be highly simi-lar and non-discriminative, since the human parsing models are not learned for identiﬁcation (Figure 1 (b)(1)). Some-times there are even missing parts in the mask because of domain gap caused by scene variations (Figure 1 (b)(3)).
Second, the estimated mask of the same pedestrian suffers from large deformations when the poses changes, which causes large intra-class variation for utilizing body shape (Figure 1 (b)(2)).
To solve the above problems and mine discriminative body shape knowledge, we propose a Fine-grained Shape-Appearance Mutual learning framework (FSAM) that con-sists of a shape stream and an appearance stream as shown in Figure 2.
To extract discriminative ﬁne-grained body shape fea-tures, in the shape stream, we learn ﬁne-grained human masks under the guidance of identity while also preserv-ing the prior knowledge of human parsing. To alleviate the impact of mask deformation brought by pose variation, we introduce a pose-speciﬁc multi-branch feature extractor to extract pose-speciﬁc ﬁne-grained body shape features.
The cloth-unrelated appearance for cloth-changing Re-ID includes body shape, face, hair style, etc. Fine-grained body shape feature is important among the appearance cues but is hard to be mined from color image because of the dominant color-based appearance.
In the appearance stream, to complement discriminative body shape knowl-edge for appearance feature learning, we propose dense in-teractive mutual learning to transfer the ﬁne-grained body shape knowledge from shape stream to appearance stream in logit level and across different intermediate layer level.
Meanwhile, mutual learning enables the appearance stream to be deployed independently for inference without extra computation of mask estimation and feature extraction in the shape stream.
Current works on cloth-changing Re-ID [46, 29] typi-cally employ human poses or contour by off-the-shelf esti-mators, which can only capture limited discriminative shape knowledge and requires large estimation computation of poses or contours. Compared to them, our method instead learns the ﬁne-grained masks with discriminative shape de-tails and saves the mask estimation cost in inference.
In summary, our contributions are listed as follows: (1) We learn ﬁne-grained body shape features for cloth-changing Re-ID by estimating masks with discriminative shape details and extracting pose-speciﬁc features. (2) A dense interactive mutual learning framework is proposed to transfer the ﬁne-grained body shape knowl-edge to learn robust cloth-unrelated appearance features in an end-to-end fashion. (3) Our Fine-grained Shape-Appearance Mutual learn-ing framework (FSAM) achieves state-of-the-arts results on several benchmark cloth-changing Re-ID datasets including
PRCC [46], LTCC [29] and VC-Clothes [37]. 2.