Abstract more diverse and realistic images than previous methods.
Training generative models, such as GANs, on a tar-get domain containing limited examples (e.g., 10) can eas-ily result in overﬁtting. In this work, we seek to utilize a large source domain for pretraining and transfer the di-versity information from source to target. We propose to preserve the relative similarities and differences between instances in the source via a novel cross-domain distance consistency loss. To further reduce overﬁtting, we present an anchor-based strategy to encourage different levels of realism over different regions in the latent space. With ex-tensive results in both photorealistic and non-photorealistic domains, we demonstrate qualitatively and quantitatively that our few-shot model automatically discovers correspon-dences between source and target domains and generates 1.

Introduction
Consider 10 portrait paintings by the incomparable
Amedeo Modigliani [34], shown in Fig. 1 (middle). Given only these 10 paintings, would it be possible to train a model which can generate inﬁnitely many paintings in the style of
Modigliani? Unfortunately, contemporary generative mod-els [11, 12, 13, 27, 3] require thousands of images to train properly, not 10. This problem is of practical importance, since many such domains of interest have a very limited col-lection of images (e.g., there are just 10 examples per artist in the Artistic-Faces dataset [34]).
Transfer learning serves as an alternative to training from scratch and has been explored in the context of generative 10743
adversarial networks (GANs) to address the limited data regime. The key idea is to start with a source model, pre-trained on a large dataset, and adapt it to a target domain with limited data by either making only small changes to the network parameters to preserve as much information as possible [32, 21, 30, 19, 15], or by synthetically increas-ing the training data via data augmentation [37, 11]. Most of these methods, however, are designed for scenarios with more than 100 training images. When the number of avail-able images is lowered to just a few [15], results often over-ﬁt to the training samples or are of poor quality.
In this work, we explore transferring a different kind of information from the source domain, namely how images relate to each other, to address the limited data setting. Intu-itively, if the model can preserve the relative similarities and differences between instances in the source domain, then it has the chance to inherit the diversity in the source do-main while adapting to the target domain. To capture this notion, we introduce a novel cross-domain distance con-sistency loss, which enforces similarity in the distribution of pairwise distances of generated samples before and af-ter adaptation. Unlike domain adaptation approaches like image-to-image translation, here we are adapting models, not images.
Interesting properties emerge when enforcing this structure-level alignment between the two domains. Specif-ically, when the source and target domains are related (e.g., faces and caricatures), our approach automatically discov-ers a one-to-one correspondence between them and is able to more faithfully model the true target distribution in terms of both diversity and image realism, as shown in Fig. 1.
When the two domains are unrelated (e.g., cars and cari-catures), our approach is unable to model the target distri-bution but still discovers interesting part-level correspon-dences to generate diverse samples.
Since the few training samples only form a small sub-set of the target distribution we seek to approximate, we
ﬁnd it necessary to enforce realism in two different ways, to not inordinately penalize the diversity among the gener-ated images. We apply an image-level adversarial loss on the synthesized images which should map to one of the real samples. For all other synthesized images, we only enforce a patch-level adversarial loss. In this way, only a small sub-set of our generated samples need to look like one of the few-shot training images, while the rest are only forced to capture their patch-level texture.
Contributions. Our main contribution is a novel GAN adaptation framework, which enforces cross-domain cor-respondence for few-shot image generation. Through ex-tensive qualitative and quantitative results, we demonstrate that our model automatically discovers correspondences be-tween related source and target domains to generate diverse and realistic images. 2.