Abstract
We present Magic Layouts; a method for parsing screen-shots or hand-drawn sketches of user interface (UI) layouts.
Our core contribution is to extend existing detectors to ex-ploit a learned structural prior for UI designs, enabling ro-bust detection of UI components; buttons, text boxes and similar. Speciﬁcally we learn a prior over mobile UI lay-outs, encoding common spatial co-occurrence relationships between different UI components. Conditioning region pro-posals using this prior leads to performance gains on UI layout parsing for both hand-drawn UIs and app screen-shots, which we demonstrate within the context an interac-tive application for rapidly acquiring digital prototypes of user experience (UX) designs. 1.

Introduction
User interface (UI) layout is a critical component in user experience (UX) design. UI Layouts are commonly ideated and developed through sketched (‘wireframe’) designs, or by mocking up screenshots. Digital prototypes are then built using sequences of such layouts, to evaluate the UX and rapidly iterate on layout design. The ability to quickly move from such prototypes ( sketches or screenshots) to digital prototypes in which components may be modiﬁed or rearranged, is valuable in expediting the design process.
This paper presents Magic Layouts; a technique for pars-ing existing UI layouts (for example wireframe sketches, or UI screenshots) into their UI components. Our techni-cal contribution is a deep learning method for detecting UI components within UI layouts that exploits common spatial relationships of components as a learned prior knowledge to improve detection accuracy.
For example, UI elements often occur together and have a meaning underpinning that co-occurrence relationship. A
‘text input ﬁeld’ and a ‘button’ occuring side-by-side in a
UI is often a query-text and a response-button. We propose to explore the use of such co-occurrence information as an external knowledge graph to learn these component rela-Figure 1. Magic Layouts parses UI layouts from sketched designs or app screenshots, exploiting learned prior knowledge of common component arrangements to improve recognition accuracy. In this parsed example, colour indicates different component classes. tionships, and incorporate this learning knowledge to boost the performance of state of the art detection algorithms.
We conduct experiments on two publicly available datasets of UI layouts; the RICO dataset of mobile app
UX designs, and the DrawnUI dataset comprising hand-sketched UX wireframes. Our proposed approach yields improvements in detection for modalities, demonstrating that co-occurrences of UI components is a useful prior upon which to condition component detection and recog-nition when parsing UI layouts. We incorporate our de-tection model into an interactive tool dubbed ‘Magic Lay-outs’ capable of parsing UI layouts from mobile camera photographs of sketches (Fig. 1), or screenshots from mo-bile app stores. Additionally, Magic Layouts incorporates sketch based image search to replace sketched graphics with higher ﬁdelity artwork. 2.