Abstract
In this paper, we address the problem of separating indi-vidual speech signals from videos using audio-visual neural processing. Most conventional approaches utilize frame-wise matching criteria to extract shared information be-tween co-occurring audio and video. Thus, their perfor-mance heavily depends on the accuracy of audio-visual syn-chronization and the effectiveness of their representations.
To overcome the frame discontinuity problem between two modalities due to transmission delay mismatch or jitter, we propose a cross-modal afﬁnity network (CaffNet) that learns global correspondence as well as locally-varying afﬁnities between audio and visual streams. Given that the global term provides stability over a temporal sequence at the
∗ Both authors contributed equally to this work
† Corresponding authors
This work was supported by the National Research Foundation of
Korea (NRF) grant funded by the Korea government (MSIT). (NRF-2021R1A2C2006703). utterance-level, this resolves the label permutation problem characterized by inconsistent assignments. By extending the proposed cross-modal afﬁnity on the complex network, we further improve the separation performance in the com-plex spectral domain. Experimental results verify that the proposed methods outperform conventional ones on vari-ous datasets, demonstrating their advantages in real-world scenarios. 1.

Introduction
Humans have a remarkable auditory system that can perceive sound sources separately in their conversations even in the presence of many surrounding sounds, in-thumping cluding background noise, crowded babbling, music, and sometimes other loud voices [1, 2]. How-ever, reliably separating a target speech signal for human-computer interaction (HCI) systems such as speech recog-nition [3, 4, 5], speaker recognition [6, 7, 8], and emotion recognition [9, 10] is still a challenging task because it is an ill-posed problem. 1336
With the impressive advent of deep learning technologies that utilize high-dimensional embeddings [11, 12, 13], it is possible nowadays to simultaneously analyze the unique acoustic characteristics of different speakers even from mixed signals. Although these deep learning-based meth-ods are effective compared to conventional statistical sig-nal processing-based ones, they are prone to a label per-mutation (or ambiguity) error due to their frame-by-frame or short segment-based processing paradigm [11, 14].
In order to address this problem, permutation invariant train-ing [15, 16] that utilizes a permutation loss criterion was presented, but the label ambiguity problem still occurs at the inference stage, especially for unseen speakers.
Leveraging the visual streams of target speech signals can be one of the best alternatives.
In psychology, sev-eral experiments have proved that looking at speakers’ faces is helpful for auditory perception under background noise environments [17, 18]. For example, lip reading, which matches lip movements onto utterances, is widely used to recognize others’ words better [19].
In audio-visual speech separation (AVSS) systems, audio and visual fea-tures are used together or complement each other to derive unique characteristics [20, 21, 22, 23, 24, 25, 26]. Mostly,
AVSS ﬁrst extracts the common correspondence features between speaker/linguistic information of speech signals and face/articulatory lip movements of video signals, after which the extracted features are exploited for the following source separation task. Consequently, the AVSS problem can be viewed as a local matching (i.e. frame-wise match-ing) task, where segmented visual features are matched with frames of speciﬁc sounds. Thus, the separation perfor-mance highly depends on the alignment accuracy between audio and video streams.
In real-world scenarios, however, audio and video are recorded from different devices with their own speciﬁca-tions, and they are transmitted through independent com-munication channels and saved with different codec pro-tocols. These practical issues frequently cause mutually unaligned states in talking videos. Fig. 1 shows an ex-ample of a video with a speech that has physical errors in its video contents, where sometimes audio plays ahead of video and vice versa. When there are even subtle data transformations caused by jitters, omissions, and out-of-synchronization in video streams, conventional local match-ing strategies [20, 23, 25] are vulnerable. This issue can be detrimental to the performance of AVSS systems in video-telephony, broadcasting, video conferencing, or ﬁlming.
In this paper, we highlight those limitations and tackle the alignment problems in AVSS processing. We propose a novel cross-modal afﬁnity network for robust speech sepa-ration, referred to as CaffNet, by utilizing visual cues in con-sideration of relative timing information. Afﬁnity, i.e. mu-tual correlation, learned in CaffNet compensates for abrupt discontinuities in audio-visual data without external infor-mation or additional supervision. Furthermore, we propose an afﬁnity regularization module that tiles the diagonal term of the afﬁnity matrix to match audio-visual sequences at the utterance level. Since the afﬁnity regularization provides a global positional constraint, it avoids the label permuta-tion problem that occurred by inconsistent assignment over time of the speech signals to the visual target. In addition, considering the estimation of the magnitude mask in tan-dem with the phase mask is one of the keys to reasonable speech reconstruction because such factors are correlated with each other [27, 28]. To accomplish this, we extend
CaffNet to have a complex-valued convolution network ar-chitecture [29, 30, 31] such that speech quality is indeed increased by restoring the mask of the magnitude and phase spectrum together. We demonstrate the effectiveness of the proposed networks with extensive experiments, achieving large improvements in unconditioned scenarios on several benchmark datasets [32, 33, 34]. 2.