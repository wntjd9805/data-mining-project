Abstract
The Capsule Network is widely believed to be more ro-bust than Convolutional Networks. However, there are no comprehensive comparisons between these two networks, and it is also unknown which components in the CapsNet affect its robustness. In this paper, we ﬁrst carefully exam-ine the special designs in CapsNet that differ from that of a ConvNet commonly used for image classiﬁcation. The examination reveals ﬁve major new/different components in CapsNet: a transformation process, a dynamic rout-ing layer, a squashing function, a marginal loss other than cross-entropy loss, and an additional class-conditional re-construction loss for regularization. Along with these ma-jor differences, we conduct comprehensive ablation studies on three kinds of robustness, including afﬁne transforma-tion, overlapping digits, and semantic representation. The study reveals that some designs, which are thought critical to CapsNet, actually can harm its robustness, i.e., the dy-namic routing layer and the transformation process, while others are beneﬁcial for the robustness. Based on these ﬁnd-ings, we propose enhanced ConvNets simply by introduc-ing the essential components behind the CapsNet’s success.
The proposed simple ConvNets can achieve better robust-ness than the CapsNet. 1.

Introduction
The Capsule network (CapsNet) [24] was proposed to address the intrinsic limitations of convolutional networks (ConvNet) [14], such as the exponential inefﬁciency and the lack of robustness to afﬁne transformations. In recent years, It has been sugested that CapsNets have the poten-tial to surpass the dominant convolutional networks in these aspects [24, 8, 21, 3, 2, 16]. However, there lack compre-hensive comparisons to support this assumption, and even for some reported improvements, there are no solid abla-tion studies to ﬁgure out which ones of the components in
CapsNets are, in fact, effective. ferences in design between the capsule networks and the common convolutional networks adopted for image classi-ﬁcation. A common convolutional network follows a simple algorithm ﬂow, using a backbone convolutional network to extract image features, a global average pooling layer plus a linear layer to produce the classiﬁcation logits (or optionally several fully connected layers [13]), and an N -way Soft-Max loss to drive the learning. To be better aligned with the capsule (vector) representations, the capsule networks introduce several special components. These components involve (see Fig. 1 for detailed architectures):
• a non-shared transformation module, in which the pri-mary capsules are transformed to execute votes by non-shared transformation matrices;
• a dynamic routing layer to automatically group input capsules to produce output capsules with high agree-ments in each output capsule;
• a squashing function, which is applied to squash the capsule vectors such that their lengths distribute in the range of [0, 1);
• a marginal classiﬁcation loss to work together with the squashed capsule representations;
• a class-conditional reconstruction sub-network with a reconstruction loss, targeting at recovering the origi-nal image from the capsule representations. This sub-network acts as a regularization force, in complemen-tary to the classiﬁcation loss.
Unlike previous studies [24, 8] which usually takes Cap-sNet as a whole to test its robustness, we instead try to study the effects of each of the above components in their effec-tiveness on robustness. We consider the three different as-pects shown in [24]:
• the robustness to afﬁne transformations,
• the ability to recognizing overlapping digits,
In this paper, we ﬁrst carefully examine the major dif-• the semantic representation compactness. 14309
Our investigations reveal that some widely believed ben-eﬁts of Capsule networks could be wrong: 1. The ConvNets baseline adopted in comparison with
CapsNets is weak [24]. Concretely, there is no global average pooling layer before the classiﬁcation head in this baseline, which sacriﬁces the ability of spatial in-variance to some extent and is harmful for generaliza-tion to novel views.
In fact, a ConvNet with an ad-ditional global average pooling layer can outperform
CapsNet by a large margin in the robustness to afﬁne transformation; 2. The dynamic routing actually may harm the robustness to input afﬁne transformation, in contrast to the com-mon belief; 3. The high performance of CapsNets to recognize over-lapping digits can be mainly attributed to the extra modeling capacity brought by the transformation ma-trices. 4. Some components of CapsNets are indeed beneﬁcial for learning semantic representations, e.g., the condi-tional reconstruction and the squashing function, but they are mainly auxiliary components and can be ap-plied beyond CapsNets.
In addition to these ﬁndings, we also enhance com-mon ConvNets by the useful components of CapsNet, and achieve greater robustness. The paper is organized as fol-lows: Sec. 2 introduces the CapsNet and related work. In
Sec. 3, we examine the behavior of CapsNets and ConvNets on three kinds of robustness, one by one, and component by component. The last section concludes our work and dis-cusses future work. 2.