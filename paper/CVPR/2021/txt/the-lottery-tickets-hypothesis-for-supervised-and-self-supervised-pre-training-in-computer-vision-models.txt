Abstract
The computer vision world has been re-gaining en-thusiasm in various pre-trained models, including both classical ImageNet supervised pre-training and recently emerged self-supervised pre-training such as simCLR [10] and MoCo [40]. Pre-trained weights often boost a wide range of downstream tasks including classiﬁcation, detection, and segmentation. Latest studies suggest that pre-training beneﬁts from gigantic model capacity [11]. We are hereby curious and ask: after pre-training, does a pre-trained model indeed have to stay large for its downstream transferability?
In this paper, we examine supervised and self-supervised pre-trained models through the lens of the lottery ticket hy-pothesis (LTH) [31]. LTH identiﬁes highly sparse matching subnetworks that can be trained in isolation from (nearly) scratch yet still reach the full models’ performance. We extend the scope of LTH and question whether matching subnetworks still exist in pre-trained computer vision mod-els, that enjoy the same downstream transfer performance.
Our extensive experiments convey an overall positive mes-sage: from all pre-trained weights obtained by ImageNet classiﬁcation, simCLR, and MoCo, we are consistently able to locate such matching subnetworks at 59.04% to 96.48% sparsity that transfer universally to multiple downstream tasks, whose performance see no degradation compared to using full pre-trained weights. Further analyses reveal that subnetworks found from different pre-training tend to yield diverse mask structures and perturbation sensitivities. We conclude that the core LTH observations remain generally relevant in the pre-training paradigm of computer vision, but more delicate discussions are needed in some cases. Codes and pre-trained models will be made available at: https:
//github.com/VITA-Group/CV_LTH_Pre-training. 1.

Introduction
Deep neural networks pre-trained on large-scale datasets prevail as general-purpose feature extractors [23]. Moving beyond the most traditional greedy unsupervised pre-training
[2], the most popular pre-training in computer vision (CV)
Transfer Learning
Pre-training
Classification
Cat ?
Detection
Iterative 
Pruning
Segmentation
Dense Model
Matching Subnetworks
Figure 1. Overview of our work paradigm: from pre-trained CV models (both supervised and self-supervised), we study the exis-tence of matching subnetworks that are transferable to many down-stream tasks, with little performance degradation compared to using full pre-trained weights. We ﬁnd task-agnostic, universally trans-ferable subnetworks at pre-trained initialization, for classiﬁcation, detection, and segmentation tasks. is arguably to train the model for supervised classiﬁcation on ImageNet [18]. Such supervised pre-training enables the network to learn a hierarchy of generalizable features
[46]; it is widely acknowledged [36] to not only beneﬁt the subsequent ﬁne-tuning on other visual classiﬁcation datasets (especially in small datasets and few-shot learning [71, 74]), but also to accelerate/improve the training for different, more complicated types of downstream vision tasks, such as object detection and semantic segmentation [63, 41].
Several state-of-the-art self-supervised pre-training, such as simCLR [10, 11] and MoCo [40, 16], have demonstrated that it is instead possible to use unlabeled data in pre-training.
Their methods refer to no actual labels in pre-training, but instead leverage self-generated pseudo labels [22, 25] or con-trasting augmented views [10]. Impressively, self-supervised pre-training yields pre-trained weights with comparable or even better transferability and generalization, for various downstream tasks, compared to their supervised pre-training counterparts.
A few recent efforts have shown to successfully scale up pre-training in CV. That is perhaps most natural for self-16306
supervised pre-training, since unlabeled images are cheap and easily accessible. Chen et al. [11] investigated to boost simCLR with massive unlabeled data in a task-agnostic way, and pointed out the key ingredient to be the use of big (deep and wide) networks during pretraining and ﬁne-tuning. The authors found that, the fewer the labels, the more this ap-proach (task-agnostic use of unlabeled data) beneﬁts from a bigger network. After ﬁne-tuning, the big network is reduced into a much smaller one with little performance loss by us-ing task-speciﬁc distillation. We additionally note the latest works suggesting that supervised ﬁne-tuning can also scale up to larger models and datasets beyond ImageNet [24].
The extraordinary cost of pre-training can be amortized by transferring to many downstream tasks. However, such explosive sizes of pre-trained models can even make ﬁne-tuning computationally demanding, urging us to ask: can we aggressively trim down the complexity of pre-trained models, without damaging their downstream transferability?
Note that, the question asked is drastically different from the conventional scope of model compression [38] in CV, where a model is trained, compressed and/or tuned on the same dataset and speciﬁc task. In comparison, any simpliﬁcation for a pre-trained model has to ensure its intact transferability to a variety of possible downstream tasks.
To address this research gap, we turn our attention to lottery ticket hypothesis (LTH) [20, 27, 31, 50, 76, 81], a fast-rising ﬁeld that investigates the sparse trainable sub-networks within full dense networks. The original LTH
[31, 32] demonstrated small-scale networks contain sparse matching subnetworks capable of training in isolation from initialization to full accuracy. In other words, we could have trained smaller networks from the start if only we had known which subnetworks to choose. Recent investigations [59, 58] showed those matching subnetworks to transfer between related classiﬁcation tasks. However, no study has closely examined the tantalizing possibility of universal transfer-ability in LTH for CV models, i.e., if we treat the pre-trained weights as our initialization, whether matching subnetworks still exist in the pre-training models, that also enjoy the same downstream transfer performance? Are there universal sub-networks that can transfer to many tasks with no degradation in performance?
The paper carries out the ﬁrst comprehensive exper-imental study to seek these desired universal matching subnetworks, from both supervised and self-supervised pre-trained CV models. Our principled methodology i) bridges pre-training and LTH from two perspectives:
Initialization via pre-training. In the previous larger-scale settings of LTH for CV [31, 69], the matching subnetworks are found at an early point in training.
Instead, we aim to identify these matching subnetworks from dense pre-trained models (self-supervised or supervised), which cre-ates an initialization directly amenable to sparsiﬁcation. ii)
Transfer learning. Finding the matching subnetwork is an expensive investment, usually costing multiple rounds of pruning and re-training. To justify this extra investment, the found subnetwork must be able to be reused by various downstream tasks, as illustrated in Figure 1.
The course of this study presents the following ﬁndings:
• Using iterative unstructured magnitude pruning [31], we identify matching sub-networks up to 67.23%, 59.04%, 95.60% sparsity, at pre-trained weights from
ImageNet-equipped supervised pre-training, simCLR and MoCo, respectively. We also ﬁnd matching sub-networks at pre-trained initialization with sparsity from 73.79% to 98.20% in a variety of classiﬁcation, detec-tion and segmentation downstream tasks.
• Subnetworks at 67.23%, 59.04% and 59.04% sparsity, found respectively using supervised ImageNet, simCLR and MoCo pre-training, are universally transferable to diverse downstream classiﬁcation tasks with nearly the same accuracies.
• Subnetworks at 73.79%/48.80%, 48.80%/36.00% and 73.79%/83.22% sparsity, found respectively by super-vised ImageNet, simCLR and MoCo, can transfer to downstream detection/segmentation tasks without sac-riﬁcing performance.
• Unlike previous matching subnetworks found at ran-dom initialization or early in training, we show that those identiﬁed at pre-trained initialization are more sensitive to structure perturbations. Also, different pre-training ways tend to yield diverse mask structures and perturbation sensitivities.
• Lastly, pruning from larger pre-trained models can also produce better transferable matching subnetworks.
Practically speaking, this work sets the ﬁrst step toward replacing large pre-trained models with smaller subnetworks, enabling much more efﬁcient downstream tuning without inhibiting transfer performance. As pre-training becomes increasingly central in the CV ﬁeld, our results shed light on the relevance of LTH in this new paradigm. 2.