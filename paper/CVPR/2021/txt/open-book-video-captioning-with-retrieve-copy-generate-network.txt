Abstract
In this paper, we convert traditional video captioning task into a new paradigm, i.e., Open-book Video Caption-ing, which generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address the open-book video captioning prob-lem, we propose a novel Retrieve-Copy-Generate network, where a pluggable video-to-text retriever is constructed to retrieve sentences as hints from the training corpus effec-tively, and a copy-mechanism generator is introduced to extract expressions from multi-retrieved sentences dynam-ically. The two modules can be trained end-to-end or sepa-rately, which is ﬂexible and extensible. Our framework co-ordinates the conventional retrieval-based methods with or-thodox encoder-decoder methods, which can not only draw on the diverse expressions in the retrieved sentences but also generate natural and accurate content of the video.
Extensive experiments on several benchmark datasets show that our proposed approach surpasses the state-of-the-art performance, indicating the effectiveness and promising of the proposed paradigm in the task of video captioning. 1.

Introduction
Video captioning is one of the most important vision-language tasks, and it seeks to automatically describe what has happened in the video according to the visual content.
Recently, many promising methods [36, 22, 24, 34, 2] have been proposed to address this task. These methods mainly focus on learning the spatial-temporal representations of videos to fully tap visual information and devising novel decoders to achieve visual-textual alignment or controllable decoding. In general, there exist some drawbacks for most of the existing work: ﬁrst, since the video content is the only source of input, the generation process lacks appropri-*Corresponding author.
Existing
Methods
Generator
Video-to-Text
Retriever
Text    
Corpus   
Our
Method
Text1: A girl is on a mat
...doing a backflip
Text2: ... does somersaults
... other little girls watch
A little girl practices in a room
Copy-Mechanism
Generator
A kid doing a somersault on a mat while a boy watches
Figure 1. Pipeline comparison of the existing methods and our method.
Our generation is produced based on not only video content but also the cues of multi-retrieved sentences searched from text corpus by a cross-modal retriever. Pluggable retriever provides guidance and expansion for the generating model. ate guidance, resulting in the generations of more generic sentences; second, the memory or knowledge domain of the model is ﬁxed after training and cannot be expanded or re-visited unless retraining.
To address these issues, we propose an Open-book Video
Captioning paradigm. We ﬁrst compare the two cross-modal tasks for better illustration: Video-Text Retrieval (VTR) and Video Captioning (VC). VTR is a discrimina-tive task that can access all the information of visual and textual modalities all the time; VC as a generative task can only produce words based on current generated words and visual information, which is more challenging than VTR.
Instead of performing the VC task directly, we propose to convert it into two-stages: we ﬁrst perform VTR to search for sentences relevant to the given video from the text cor-pus; then, we leverage the retrieval sentences as extra hints or guidance for caption generation. During the inference, the generator can generate words based on the video con-tent or directly copy expressions from retrieved sentences. 9837
The ﬂexible VTR and the changeable corpus provide the possibility for the model’s extension or revision.
The inspiration for the proposed paradigm comes from the Open-domain Question Answering task [18, 10, 15], which requires a system to answer any questions utilizing large-scale documents. Open means not providing the sys-tem with documents containing the correct answers directly but requires the system to retrieve documents related to the question from massive corpus and then generate the cor-rect answer based on them. We claim that the open-domain mechanism is also effective in cross-modal interaction area and can thus be applied to the video captioning task.
This mechanism essentially extends the knowledge do-main of the model learned only from the labeled data. It is known that producing large-scale, high-quality labeled data is extremely laborious and time-consuming. Instead, a model learns to collect related references, distinguish use-ful hints, abstract and summarize information from external weakly labeled or unlabeled documents breaks through the limitations of labeled data. This is valuable especially for the industry-scale video platforms where hundreds of mil-lions weakly labeled or unlabeled data are generated every-day. Unlike traditional semi-supervised learning that uti-lizes ﬁxed weakly labeled or unlabeled examples directly for training, the proposed paradigm makes the model learn to extract useful information from changeable weakly la-beled or unlabeled corpus directly for inference.
To realize the aforementioned open-book video caption-ing, we introduce a novel Retrieve-Copy-Generate (RCG) network. We introduce a Video-to-Text Retriever to search for video-content-relevant sentences from the corpus con-taining the whole sentences of the training set. Our retriever follows the Bi-encoders [14] structure and utilizes both mo-tion and appearance features to search desired sentences ef-ﬁciently and effectively. For the example in Fig.1, the top retrieved sentences contain expressions “on a mat”, “does somersaults”, and “someone watches”, which describe the given video accurately. Then, the retrieved sentences and the visual features are passed to the generator. A novel
Copy-Mechanism Generator is introduced, which dynami-cally decides whether to copy the expressions directly from multi-retrieved sentences or generate new words from the video contents. The model combines the information from the video content and the words copied from retrievals to generate the ﬁnal caption “A kid doing a somersault on a mat while a boy watches”, which is much better than the generic caption “A little girl practices in a room”.
The contributions of this work are listed as follows: (1) We propose to solve the video captioning task with an open-book paradigm, which generates captions under the guidance of video-content-retrieval sentences, not limited to the video itself. (2) We introduce a novel Retrieve-Copy-Generate net-work to tackle this task, where an improved cross-modal retriever is utilized to provide hints for gener-ator, and a copy-mechanism generator is proposed for dynamical copying and better generation. (3) The extensive experimental results highlight the ben-eﬁts of combining cross-modal retrieval with copy-mechanism generation for the video caption task. The proposed approach achieves state-of-the-art results on
VATEX and superior performance on MSR-VTT. 2.