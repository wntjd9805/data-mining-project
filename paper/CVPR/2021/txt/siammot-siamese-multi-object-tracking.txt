Abstract
In this paper, we focus on improving online multi-object tracking (MOT). In particular, we introduce a region-based
Siamese Multi-Object Tracking network, which we name
SiamMOT. SiamMOT includes a motion model that esti-mates the instance’s movement between two frames such that detected instances are associated. To explore how the motion modelling affects its tracking capability, we present two variants of Siamese tracker, one that implicitly mod-els motion and one that models it explicitly. We carry out extensive quantitative experiments on three different
MOT datasets: MOT17, TAO-person and Caltech Roadside
Pedestrians, showing the importance of motion modelling for MOT and the ability of SiamMOT to substantially out-perform the state-of-the-art. Finally, SiamMOT also outper-forms the winners of ACM MM’20 HiEve Grand Challenge on HiEve dataset. Moreover, SiamMOT is efﬁcient, and it runs at 17 FPS for 720P videos on a single modern GPU. 1.

Introduction
Multi-object tracking (MOT) is the problem of detect-ing object instances and then temporally associating them to form trajectories. Early works [1, 3, 4, 17, 29, 33, 46, 49, 53, 53, 59, 64, 65, 71, 72] formulate instance association as a graph-based optimization problem under the “tracking-by-detection” paradigm, in which a node represents a de-tection and an edge encodes the likelihood of two nodes
In practice, they use a combination of vi-being linked. sual and motion cues to represent each node, which often requires expensive computation. Furthermore, they usu-ally construct a large ofﬂine graph, which is non-trivial to solve, making them inapplicable for real-time tracking. Re-cently, online trackers [5,7,60,76] started to emerge, as they are more desirable in real-time tracking scenarios. They focus on improving local linking over consecutive frames rather than building an ofﬂine graph to re-identify instances across large temporal gaps. Among these, some recent works [5, 75] have pushed online MOT into state-of-the-art territory, making them very competitive.
In this work, we explore the importance of modelling motion in online MOT by building upon “Simple and On-line Realtime Tracking” (SORT) [7,60] that underlies recent state-of-the-art models [5, 76].
In SORT, a better motion model is the key to improving its local linking accuracy.
For example, SORT [7] uses Kalman Filters [31] to model the instance’s motion with simple geometric features, while the more recent state-of-the-art trackers [5, 76] learn a deep network to predict the displacement (motion) of instances based on both visual and geometric features, signiﬁcantly outperforming the simpler SORT.
We conduct our motion modelling exploration by lever-aging a region-based Siamese Multi-Object Tracking net-work, which we name SiamMOT. We combine a region-based detection network (Faster-RCNN [45]) with two mo-tion models inspired by the literature on Siamese-based single-object tracking [6, 18, 22, 35, 36]: an implicit mo-tion model (IMM) and an explicit motion model (EMM).
Differently from CenterTrack [76] that implicitly infers the motion of instances with point-based features [16, 44, 56], SiamMOT uses region-based features and develops (explicit) template matching to estimate instance motion, which is more robust to challenging tracking scenarios, such as fast motion.
We present extensive ablation analysis on three differ-ent multi-person tracking datasets. Our results suggest that instance-level motion modelling is of great importance for robust online MOT, especially in more challenging tracking scenarios. Furthermore, we show that the motion models of
SiamMOT can improve tracking performance substantially, especially when cameras are moving fast and when people’s poses are deforming signiﬁcantly. On the popular MOT17
Challenge [42] SiamMOT with EMM achieves 65.9 MOTA
/ 63.3 IDF1 with a DLA-34 [69] backbone by using public detection, outperforming all previous methods. Moreover, on the recently introduced large-scale TAO-person dataset
[14], SiamMOT substantially improves over the state-of-the-art Tracktor++ [5] from 36.7 to 41.1 TrackAP [14, 66].
Finally, we benchmark SiamMOT on the Human In Events (HiEve) dataset [41], where it outperforms the winner of the
ACM MM’20 grand HiEve challenge [40]. 12372
2.