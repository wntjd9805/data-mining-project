Abstract
Weight pruning is an effective technique to reduce the model size and inference time for deep neural networks in real-world deployments. However, since magnitudes and relative importance of weights are very different for differ-ent layers of a neural network, existing methods rely on ei-ther manual tuning or handcrafted heuristic rules to ﬁnd appropriate pruning rates individually for each layer. This
In approach generally leads to suboptimal performance. this paper, by directly working on the probability space, we propose an effective network sparsiﬁcation method called probabilistic masking (ProbMask), which solves a natural sparsiﬁcation formulation under global sparsity constraint.
The key idea is to use probability as a global criterion for all layers to measure the weight importance. An appealing feature of ProbMask is that the amounts of weight redun-dancy can be learned automatically via our constraint and thus we avoid the problem of tuning pruning rates individu-ally for different layers in a network. Extensive experimen-tal results on CIFAR-10/100 and ImageNet demonstrate that our method is highly effective, and can outperform previous state-of-the-art methods by a signiﬁcant margin, especially in the high pruning rate situation. Notably, the gap of Top-1 accuracy between our ProbMask and existing methods can be up to 10%. As a by-product, we show ProbMask is also highly effective in identifying supermasks, which are sub-networks with high performance in a randomly weighted dense neural network. 1.

Introduction
Weight pruning [9] is a popular technique for alleviating the weight redundancy in deep neural networks (DNNs) to improve inference efﬁciency and decrease computation de-mands. Typical pruning algorithms usually prune the unim-portant weights by developing proper criteria. It is repeat-edly reported in the literature [8, 22, 39, 21] that by prun-ing one can reduce the neural network size and improve
*Equal contribution the inference efﬁciency signiﬁcantly with quite slight or even negligible loss on performance, which makes deploy-ing large-scale DNNs on equipment with limited computa-tional and memory budget possible.
What can serve as a suitable global comparator to mea-sure weight importance and identify sparsity level for differ-ent layers is a long-standing problem [7] though impressive results have been achieved. We know that the core module in pruning is the explicit or implicit criterion for identify-ing the redundant weights, and it is difﬁcult to develop a global criterion for the weights in all the layers. For exam-ple, in [9], the authors propose a simple yet effective cri-terion, i.e., for each layer it prunes all the weights below a certain threshold in a fully trained network. The threshold is obtained by sorting weight by its magnitude and retriev-ing the weight magnitude at the target pruning rate. The criterion is weight magnitude in this case. Notice that the magnitudes of the weights across layers could be quite dif-ferent and different layers could have different amount of redundancy.
If we use a global threshold for all the lay-ers, then almost all the weights in certain layers could be pruned in order to achieve high enough pruning ratio, which will be veriﬁed in Section 5. Thus, we need to set a proper threshold or pruning ratio for each layer individually. In the networks with numerous layers, it is very difﬁcult and even impossible to ﬁnd the optimal thresholds or pruning ratios for all the layers manually. One reasonable compromise for such dilemma is to set sparsity level uniformly for different layers. However, this results in imperfect weight allocation obviously and gives unsatisfactory results on high pruning rates.
In this paper, to address the above limitations, we pro-pose an effective network sparsiﬁcation method called prob-abilistic masking (ProbMask). Firstly, we know that net-work pruning can be naturally formulated into a problem of ﬁnding a sparse binary mask m as well as the weights at the same time to minimize the empirical loss (1). If the component mi is equal to 0, it means that the correspond-ing weight is pruned. However, it is a discrete optimization problem and hard to solve. We notice that if we view the components mi in the mask as independent Bernoulli ran-3599
dom variables with probability si being 1 and probability (1 − si) being 0 and reparameterize them w.r.t. its probabil-ity, then the loss in problem (1) would become continuous over the probability space. Due to the nature of probabil-ity, probability can be used as a global criterion in all the layers. Therefore, we can control the model size via forc-ing the sum of all the probabilities si of the mask smaller than a proper value, leading to a global sparsity constraint in the probability space. In this way, the discrete optimization problem (1) is transformed into a constrained expected loss minimization problem (2) over a probability space, which is continuous. Finally, we adopt the Gumbel-Softmax trick to solve the continuous problem. As the optimizer goes on, the probabilities si would converge to either 0 or 1, i.e., m would become close to a deterministic sparse mask. Thus, a fully trained mask would have quite low variance, mak-ing the loss of the sampled sparse network according to m close to the excepted loss in problem (2). Another appeal-ing feature of our proposed method is that the amount of weight redundancy in each layer can be identiﬁed automat-ically by our global sparsity constraint and thus we do not need to choose different pruning ratios for different layers.
Experimental results on network pruning and supermask
[40] ﬁnding demonstrate that our method is much more ef-fective than the state-of-the-art methods on both small scale datasets and large scale datasets and can outperform them with a signiﬁcant margin when the pruning rate is high.
The contribution and novelty of ProbMask can be sum-marized as follows: 1) We provide evidence showing that probability can serve as a suitable global comparator to measure weight importance and identify sparsity level for different layers, which is a long-standing problem [7]. 2) We present a natural formulation of global sparsity constraint, and an optimization method that is practically effective. Our solution ﬁxes the training and testing perfor-mance discrepancy problem observed in practice, which led to the failure of previous methods [23] on ImageNet [7]. 3) We demonstrate the effectiveness of using probability as global comparator on small-scale and large-scale prob-lems and various models and achieve state-of-the-art results on Top-1 accuracy and accuracy-versus-FLOPS curve. 4) We show ProbMask can also serve as a powerful tool for identifying supermasks, which are subnetworks with high performance in a randomly weighted dense neural net-work, and we achieve state-of-the-art results on Top-1 ac-curacy on CIFAR-100 under high pruning rates.
Notations: Let k · k0, k · k1 and k · k2 be the ℓ0, ℓ1 and
ℓ2 norm of a real valued vector, respectively. We denote 1 ∈ Rn to be a vector with all components equal to 1. In addition, {0, 1}n is the set of n-dimensional vectors with each coordinate valued in {0, 1}. 2.