Abstract Reasoning
Yaniv Benny1
Niv Pekar1
Lior Wolf1,2 1The School of Computer Science, Tel Aviv University 2Facebook AI Research (FAIR)
Abstract
We consider the abstract relational reasoning task,
Since which is commonly used as an intelligence test. some patterns have spatial rationales, while others are only semantic, we propose a multi-scale architecture that pro-cesses each query in multiple resolutions. We show that in-deed different rules are solved by different resolutions and a combined multi-scale approach outperforms the existing state of the art in this task on all benchmarks by 5-54%.
The success of our method is shown to arise from multiple novelties. First, it searches for relational patterns in multi-ple resolutions, which allows it to readily detect visual re-lations, such as location, in higher resolution, while allow-ing the lower resolution module to focus on semantic rela-tions, such as shape type. Second, we optimize the reason-ing network of each resolution proportionally to its perfor-mance, hereby we motivate each resolution to specialize on the rules for which it performs better than the others and ig-nore cases that are already solved by the other resolutions.
Third, we propose a new way to pool information along the rows and the columns of the illustration-grid of the query.
Our work also analyses the existing benchmarks, demon-strating that the RAVEN dataset selects the negative exam-ples in a way that is easily exploited. We, therefore, propose a modiﬁed version of the RAVEN dataset, named RAVEN-FAIR. Our code and pretrained models are available at https://github.com/yanivbenny/MRNet. 1.

Introduction
Raven’s Progressive Matrices (RPM) is a widely-used intelligence test [13, 3], which does not require prior knowl-edge in language, reading, or arithmetics. While IQ mea-surements are often criticized [21, 4], RPM is highly corre-lated with other intelligence-based properties [17] and has
Its wide acceptance a high statistical reliability [12, 11]. by the psychological community led to an interest in the AI community. Unfortunately, as pointed out by [20, 25, 8], applying machine learning models to it can sometimes re-sult in shallow heuristics that have little to do with actual intelligence. Therefore, it is necessary to study the pitfalls of RPMs and the protocols to eliminate these exploits.
In Sec. 2.1, we present an analysis of the most pop-ular machine learning RPM benchmarks, PGM [15] and
RAVEN [26], from the perspective of biases and exploits. It is shown that RAVEN is built in a way that enables the se-lection of the correct answer with a high success rate with-out observing the question itself. To mitigate this bias, we construct, in Sec. 3, a fair variant of the RAVEN bench-mark. To further mitigate the identiﬁed issues, we propose a new evaluation protocol, in which every choice out of the multiple-choice answers is evaluated independently. This new evaluation protocol leads to a marked deterioration in the performance of the existing methods and calls for the development of more accurate ones that also allows for a better understating of abstract pattern recognition. In Sec. 4, we propose a novel neural architecture, which, as shown in
Sec. 5, outperforms the existing methods by a sizable mar-gin. The performance gap is also high when applying the network to rules that were unseen during training. Further-more, the structure of the new method allows us to separate the rules into families that are based on the scale in which reasoning occurs.
The success of the new method stems mostly from two components: (i) a multi-scale representation, which is shown to lead to a specialization in different aspects of the
RPM challenge across the levels, and (ii) a new form of in-formation pooling along the rows and columns of the chal-lenge’s 3 × 3 matrix.
To summarize, the contributions of this work are as fol-lows: (i) An abstract multi-scale design for relational rea-soning. (ii) A novel reasoning network that is applied on each scale to detect relational patterns between the rows and between the columns of the query grid. (iii) An improved loss function that balances between the single positive ex-ample and the numerous negative examples. (iv) A multi-head attentive loss function that prioritizes the different res-olutions to specialize in solving different rules. (v) A new 112557
balanced version of the existing RAVEN dataset, which we call RAVEN-FAIR. 2.