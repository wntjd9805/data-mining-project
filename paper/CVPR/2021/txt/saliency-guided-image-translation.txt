Abstract
In this paper, we propose a novel task for saliency-guided image translation, with the goal of image-to-image translation conditioned on the user speciﬁed saliency map.
To address this problem, we develop a novel Generative Ad-versarial Network (GAN)-based model, called SalG-GAN.
Given the original image and target saliency map, SalG-GAN can generate a translated image that satisﬁes the tar-get saliency map. In SalG-GAN, a disentangled represen-tation framework is proposed to encourage the model to learn diverse translations for the same target saliency con-dition. A saliency-based attention module is introduced as a special attention mechanism for facilitating the developed structures of saliency-guided generator, saliency cue en-coder and saliency-guided global and local discriminators.
Furthermore, we build a synthetic dataset and a real-world dataset with labeled visual attention for training and eval-uating our SalG-GAN. The experimental results over both datasets verify the effectiveness of our model for saliency-guided image translation. 1.

Introduction
Conditional image generation has gained signiﬁcant at-tention in recent years, especially in light of the progress in Generative Adversarial Network (GAN)-based, and, to a lesser extent, Variational Auto Encoder (VAE)-based gen-erative methods. Impressive results have been achieved in generating high-quality images from different (condition-ing) information such as text [6, 11], sketches [13], lay-outs [38], facial attributes [5, 34] and scene graphs [18].
Image-to-image translation [13, 20, 33, 40] has been a par-ticularly successful sub-class of these methods.
Image-to-image translation focuses on producing images that are structurally similar to the original inputs but deviate in stylistic [20] or texture detail [33]. This allows models such as CycleGAN [40] and alternatives to produce images of zebras from horses, or Picasso painting renditions from ev-eryday photographs. More recent models [1] also provide
*Corresponding author. ability to modify the image more structurally by, for exam-ple, adding speciﬁc objects. This of course requires a user to select and place an object in a desired location. None of these methods, however, allow for the ability to model more abstract translations or image modiﬁcations that alter the way in which the original image is perceived.
Consider someone taking a photo of a person outdoors.
In addition to the person, the image may contain other back-ground or foreground objects (e.g., cars, motorcycle) that distract attention of the viewer. How can this be mitigated in “post-production”? Many techniques and strategies can be employed. For example, distracting objects may sim-ply be removed, using image inpainting techniques [36] on the object regions. Alternatively, good bokeh (good qual-ity blur) could be computationally applied to all pixels but those belonging to the main subject, effectively modeling shallow depth of ﬁeld which is a common technique in pro-fessional photography. Note that a bad bokeh (a distracting blur) may actually have an adverse effect toward the desired goal. Further, a color pallet of either distracting objects or the subject itself maybe altered to make the subject more distinctive. These are just some of the multitude of ways that an image maybe altered to achieve a desired effect. Lets consider what all of these strategies have in common, in ef-fect they are trying to modify the saliency distribution of the input image, by modifying image itself, to achieve a certain visual effect. We posit that ability to manipulate an image to achieve a desired saliency distribution is a core task for a variety of high-level applications including image retarget-ing [26], object enhancement [27], distractor removal [7] and intelligent advertisement [31]. To this end we propose a novel task of saliency-guided image translation and cor-responding benchmark datasets.
The goal of saliency-guided image translation is to per-form image-to-image translation conditioned on the (user speciﬁed) target image saliency map.
Some examples of saliency-guided image translation are shown in Fig-ure 1. Despite long history of saliency in computer vi-sion [14], few approaches exist that carry ability to per-form saliency-driven image adjustments; most focus on 16509
(a) Saliency-driven Image Editing (b) Saliency-Guided Image Trnslation
Figure 1. In traditional saliency-driven image editing, the modiﬁcation is pixel aligned; while for our saliency-guided image trans-lation, the composition of the image itself can be changed, allowing spatial transformations or shifts, addition, removal of objects as a whole. Meanwhile, instead of an accurate mask, our image translation method is directly guided by the ﬁxation map, which can be easily acquired by mouse-contingent tool or eye-tracker. In (b), we present two potential applications of saliency-guided image translation: object enhancement (ﬁrst row) and object removal (second row). In the ﬁrst row, the mug in the original image attracts little human attention, mainly because it is far away from the camera. We can make the mug more focal by a suitable target saliency map. The saliency-guided translated image can be seen on the right. Similarly, in the second row, the distracting objects can be removed by giving the target saliency map. Note that the results in (a) and (b) are from [27] and our proposed method, respectively. saliency prediction. Saliency-driven image editing methods
[4, 7, 27, 31, 35, 8, 28], that come closest, are a special case of the proposed, and much more broadly deﬁned, saliency-guided image translation. Saliency driven image manipu-lation approaches are limited to low-level pixel modiﬁca-tions such as color, luminance, saturation and sharpness; while our task also allows for object removal, creation and even motion within the image. As shown in Figure 1, saliency-driven image editing methods are limited to low-level pixel modiﬁcations such as color, luminance, satu-ration and sharpness; while our task also allows for ob-ject removal, creation and even motion within the image. while our task also allows for object removal, creation and even motion within the image. Meanwhile, instead of an accurate mask, our image translation method is directly guided by the ﬁxation map, which can be easily acquired by mouse-contingent tool or eye-tracker. Thus, beyond saliency-driven image editing, saliency-guided image trans-lation offers more ﬂexible and vast potential real-world ap-plications, such as the go-to tools for product designers, market researchers and consumer behavior modeling, in-cluding in advertising.
Compared to traditional image-to-image translation tasks, the saliency-guided image translation is much more challenging.
Impoverished content and ambiguity of the saliency are the core challenges. For example, saliency is object and content agnostic, meaning same added level of saliency in a given image location can be achieved by in-serting a variety of objects that adhere to the correct pro-portions. Also, there are multiple conceptual solutions that can satisfy the same saliency change. For example, the saliency of a object can be enhanced by changing its appear-ance or removing other salient objects around it. Last, the saliency of same object can be different across images, due to the inﬂuence of surrounding objects. Thus, the models for saliency-guided image translation should be inherently capable of both generating real images and understanding of human attention and how it can be manipulated.
Contributions:
In this paper, we take the ﬁrst step to-wards the saliency-guided image translation, by proposing a novel GAN-based model, namely SalG-GAN. To address the challenge of saliency ambiguity, a disentangled repre-sentation framework is developed in SalG-GAN, in order to encourage the model to learn diverse translations for the same target saliency map. Besides, a saliency-based atten-tion module is introduced as a special attention mechanism 16510
for facilitating the developed structures of saliency-guided generator, saliency cue encoder and saliency-guided global and local discriminators. Additionally, a light but effective saliency detector is developed as part of the framework, to help the generator understand and modify human attention.
For training and testing our SalG-GAN, we build a syn-thetic dataset (SGIT-S) consisting of 53,000 images and a real-world dataset (SGIT-R). Both datasets are labeled by 7 subjects with attention; datasets will be released. The ex-periments over these two datasets show the effectiveness of our method for saliency-guided image generation. 2.