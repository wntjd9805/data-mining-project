Abstract
Existing studies in weakly-supervised semantic segmen-tation (WSSS) using image-level weak supervision have sev-eral limitations: sparse object coverage, inaccurate ob-ject boundaries, and co-occurring pixels from non-target objects.
To overcome these challenges, we propose a novel framework, namely Explicit Pseudo-pixel Supervision (EPS), which learns from pixel-level feedback by combin-ing two weak supervisions; the image-level label provides the object identity via the localization map and the saliency map from the off-the-shelf saliency detection model offers rich boundaries. We devise a joint training strategy to fully utilize the complementary relationship between both infor-mation. Our method can obtain accurate object boundaries and discard co-occurring pixels, thereby signiﬁcantly im-proving the quality of pseudo-masks. Experimental results show that the proposed method remarkably outperforms ex-isting methods by resolving key challenges of WSSS and achieves the new state-of-the-art performance on both PAS-CAL VOC 2012 and MS COCO 2014 datasets. The code is available at https://github.com/halbielee/EPS.
Figure 1. Motivating example of utilizing both the saliency map and the localization map for WSSS. (a) Groundtruth, (b) saliency map via PFAN [51], (c) localization map via CAM [52] and (d) our EPS utilizing both the saliency map and the localization map for training a classiﬁer. Note that the saliency map cannot capture person and car while our result can correctly restore them, and the localization map overly captures two objects. 1.

Introduction
Weakly-supervised semantic segmentation (WSSS) uti-lizes weak supervision (e.g., image-level labels [36, 37], scribbles [29], or bounding boxes [22]) and aims at achieving competitive performances to the fully-supervised model, which requires pixel-level labels. Most existing studies adopt image-level labels as the weak supervision of the segmentation model. The overall pipeline of WSSS con-sists of two stages. Firstly, pseudo-masks are generated for
∗indicates an equal contribution.
†Hyunjung Shim is a corresponding author. target objects using an image classiﬁer. Then, the segmen-tation model is trained using the pseudo-masks as supervi-sion. The prevalent technique for generating pseudo-masks is class activation mapping (CAM) [52], which provides ob-ject localization maps corresponding to their image-level labels. Because of the supervision gap between the fully (i.e., pixel-level annotations) and weakly (i.e., image-level labels) supervised semantic segmentation, WSSS has the following key challenges: 1) the localization map only cap-tures a small fraction of target objects [52], 2) it suffers from the boundary mismatch of the objects [23], and 3) it hardly separates co-occurring pixels from target objects (e.g., the railroad from a train) [25]. 5495
To address these problems, existing studies can be cat-egorized into three pillars. The ﬁrst approach expands ob-ject coverage to capture the full extent of objects by eras-ing pixels [9, 23, 28], ensembling score maps [21, 27], or using self-supervised signal [41]. However, they fail to de-termine accurate object boundaries of the target object be-cause they have no clue to guide the object’s shape. The second approach focuses on improving the object bound-aries of pseudo-masks [13, 32]. Since they effectively learn object boundaries, they naturally expand pseudo-masks un-til boundaries. However, they still fail to distinguish co-incident pixels of non-target objects from a target object.
It is because the strong correlation between the foreground and the background (i.e., co-occurrence) is almost indistin-guishable from an inductive bias (i.e., the frequency of ob-serving the target object and its coincident pixels), as shown in [10]. Lastly, the third approach aims to mitigate the co-occurrence problem using extra groundtruth masks [24], or the saliency map [35, 47]. However, [24, 28] require strong pixel-level annotations, which are far from a weakly super-vised learning paradigm. [35] is sensitive to the errors of the saliency map. Also, [47] does not cover the full extent of objects and suffers from the boundary mismatch.
In this paper, our goal is to overcome the three challenges of WSSS by fully utilizing both the localization map (i.e.,
CAM from the image classiﬁer trained with image-level la-bels) and the saliency map (i.e., the output of the off-the-shelf saliency detection model [18, 34, 51]). We focus on a complementary relationship in the localization map and the saliency map. As illustrated in Figure 1, the localization map can distinguish different objects but does not separate their boundaries effectively. Contrarily, while the saliency map provides rich boundary information, it does not reveal object identity. In this sense, we argue that our method us-ing two complementary pieces of information can resolve the performance bottleneck of WSSS.
To this end, we propose a novel framework for WSSS, called Explicit Pseudo-pixel Supervision (EPS). To fully utilize the saliency map (i.e., both the foreground and the background), we design a classiﬁer to predict C + 1 classes, consisting of C target classes and the background class. We leverage C localization maps and the background localiza-tion map to estimate a saliency map. Then, the saliency loss is deﬁned as the pixel-wise difference between the saliency map and our estimated saliency map. By introducing the saliency loss, the model can be supervised by pseudo-pixel feedback across all classes. We also use the multi-label clas-siﬁcation loss to predict image-level labels. Therefore, we train the classiﬁer to optimize both the saliency loss and the multi-label classiﬁcation loss, synergizing the predictions for both the background and foreground pixels– we ﬁnd that our strategy can improve both the saliency map (Section 3.3 and Figure 3) and the pseudo-mask (Section 5.1 and Fig-ure 4).
We stress that, because the saliency loss penalizes boundary mismatches via pseudo-pixel feedback, it can en-force our method to learn the object’s accurate boundaries.
As a byproduct, we can also capture the entire object by ex-panding the map until the boundaries. Because the saliency loss helps separate the foreground (e.g., a train) from the background, our method can assign the co-occurring pixels (e.g., a railroad) to the background class. Experimental re-sults show that our EPS achieves remarkable segmentation performances, recording new state-of-the-art accuracies on
PASCAL VOC 2012 and MS COCO 2014 datasets. 2.