Abstract
Class-Incremental Learning (CIL) aims to learn a classi-ﬁcation model with the number of classes increasing phase-by-phase. An inherent problem in CIL is the stability-plasticity dilemma between the learning of old and new classes, i.e., high-plasticity models easily forget old classes, but high-stability models are weak to learn new classes.
We alleviate this issue by proposing a novel network ar-chitecture called Adaptive Aggregation Networks (AANets) in which we explicitly build two types of residual blocks at each residual level (taking ResNet as the baseline architec-ture): a stable block and a plastic block. We aggregate the output feature maps from these two blocks and then feed the results to the next-level blocks. We adapt the aggregation weights in order to balance these two types of blocks, i.e., to balance stability and plasticity, dynamically. We conduct extensive experiments on three CIL benchmarks: CIFAR-100, ImageNet-Subset, and ImageNet, and show that many existing CIL methods can be straightforwardly incorpo-rated into the architecture of AANets to boost their perfor-mances1. 1.

Introduction
AI systems are expected to work in an incremental manner when the amount of knowledge increases over time. They should be capable of learning new concepts while maintaining the ability to recognize previous ones.
However, deep-neural-network-based systems often suffer from serious forgetting problems (called “catastrophic for-getting”) when they are continuously updated using new coming data. This is due to two facts: (i) the updates can override the knowledge acquired from the previous data [19, 27, 28, 33, 40], and (ii) the model can not replay the entire previous data to regain the old knowledge.
To encourage solving these problems, [34] deﬁned a 1Code: https://class-il.mpi-inf.mpg.de/ class-incremental learning (CIL) protocol for image clas-siﬁcation where the training data of different classes grad-ually come phase-by-phase. In each phase, the classiﬁer is re-trained on new class data, and then evaluated on the test data of both old and new classes. To prevent trivial algo-rithms such as storing all old data for replaying, there is a strict memory budget due to which a tiny set of exemplars of old classes can be saved in the memory. This memory constraint causes a serious data imbalance problem between old and new classes, and indirectly causes the main problem of CIL – the stability-plasticity dilemma [29]. In particular, higher plasticity results in the forgetting of old classes [27], while higher stability weakens the model from learning the data of new classes (that contain a large number of samples).
Existing CIL works try to balance stability and plasticity us-ing data strategies. For example, as illustrated in Figure 1 (a) and (b), some early methods train their models on the imbalanced dataset where there is only a small set of exem-plars for old classes [23, 34], and recent methods include a
ﬁne-tuning step using a balanced subset of exemplars sam-pled from all classes [4, 11, 16]. However, these data strate-gies are still limited in terms of effectiveness. For example, when using the models trained after 25 phases, LUCIR [16] and Mnemonics [25] “forget” the initial 50 classes by 30% and 20%, respectively, on the ImageNet dataset [37].
In this paper, we address the stability-plasticity dilemma by introducing a novel network architecture called Adaptive
Aggregation Networks (AANets). Taking the ResNet [14] as an example of baseline architectures, we explicitly build two residual blocks (at each residual level) in AANets: one for maintaining the knowledge of old classes (i.e., the stabil-ity) and the other for learning new classes (i.e., the plastic-ity), as shown in Figure 1 (c). We achieve these by allowing these two blocks to have different levels of learnability, i.e., less learnable parameters in the stable block but more in the plastic one. We apply aggregation weights to the output fea-ture maps of these blocks, sum them up, and pass the result maps to the next residual level.
In this way, we are able to dynamically balance the usage of these blocks by updat-2544
ing their aggregation weights. To achieve auto-updating, we take the weights as hyperparameters and optimize them in an end-to-end manner [12, 25, 48].
Technically, the overall optimization of AANets is bilevel. Level-1 is to learn the network parameters for two types of residual blocks, and level-2 is to adapt their aggre-gation weights. More speciﬁcally, level-1 is the standard optimization of network parameters, for which we use all the data available in the phase. Level-2 aims to balance the usage of the two types of blocks, for which we optimize the aggregation weights using a balanced subset (by downsam-pling the data of new classes), as illustrated in Figure 1 (c).
We formulate these two levels in a bilevel optimization pro-gram (BOP) [41] that solves two optimization problems al-ternatively, i.e., update network parameters with aggrega-tion weights ﬁxed, and then switch. For evaluation, we con-duct CIL experiments on three widely-used benchmarks,
CIFAR-100, ImageNet-Subset, and ImageNet. We ﬁnd that many existing CIL methods, e.g., iCaRL [34], LUCIR [16],
Mnemonics Training [25], and PODNet [11], can be di-rectly incorporated in the architecture of AANets, yield-ing consistent performance improvements. We observe that a straightforward plug-in causes memory overheads, e.g., 26% and 15% respectively for CIFAR-100 and ImageNet-Subset. For a fair comparison, we conduct additional exper-iments under the settings of zero overhead (e.g., by reducing the number of old exemplars for training AANets), and vali-date that our approach still achieves top performance across all datasets.
Our contribution is three-fold: 1) a novel and generic network architecture called AANets specially designed for tackling the stability-plasticity dilemma in CIL tasks; 2) a
BOP-based formulation and an end-to-end training solution for optimizing AANets; and 3) extensive experiments on three CIL benchmarks by incorporating four baseline meth-ods in the architecture of AANets. 2.