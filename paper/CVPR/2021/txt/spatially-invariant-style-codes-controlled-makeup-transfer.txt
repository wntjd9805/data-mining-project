Abstract
Transferring makeup from the misaligned reference im-age is challenging. Previous methods overcome this barrier by computing pixel-wise correspondences between two im-ages, which is inaccurate and computational-expensive. In this paper, we take a different perspective to break down the makeup transfer problem into a two-step extraction-assignment process. To this end, we propose a Style-based
Controllable GAN model that consists of three components, each of which corresponds to target style-code encoding, face identity features extraction, and makeup fusion, respec-tively. In particular, a Part-speciﬁc Style Encoder encodes the component-wise makeup style of the reference image into a style-code in an intermediate latent space W . The style-code discards spatial information and therefore is in-variant to spatial misalignment. On the other hand, the style-code embeds component-wise information, enabling
ﬂexible partial makeup editing from multiple references.
This style-code, together with source identity features, is in-tegrated into a Makeup Fusion Decoder equipped with mul-tiple AdaIN layers to generate the ﬁnal result. Our proposed method demonstrates great ﬂexibility on makeup transfer by supporting makeup removal, shade-controllable makeup transfer, and part-speciﬁc makeup transfer, even with large spatial misalignment. Extensive experiments demonstrate
∗The ﬁrst two authors contributed equally.
†Corresponding author (hesfe@scut.edu.cn). the superiority of our approach over state-of-the-art meth-ods. Code is available at https://github.com/ makeuptransfer/SCGAN . 1.

Introduction
Makeup is one of the best ways to make people attrac-tive. However, applying makeup is time-consuming and it even takes a much longer time to seek suitable makeup for each individual. Thus, it is practical to automatically transfer the makeup from reference images onto our own faces. Deep learning approaches, especially GAN-based models, have been widely exploited in makeup transfer task [19, 1, 6, 2]. They mainly model the makeup trans-fer and recovery processes as a closed-loop to combat the problem of lacking paired data.
Notwithstanding the demonstrated success, these meth-ods are constrained by the input condition, i.e., both the source and reference images must be well-aligned frontal faces. This is because enforcing cycle-consistency [29] cannot guarantee correct spatial transformation. On the other hand, their CycleGAN-based solutions [1, 19] lack
ﬂexibility and controllability of makeup styles. A recent work, PSGAN [13], is proposed to address these problems.
It computes the dense correspondence attention between two images to consolidate component-to-component trans-fer. However, apart from the large computational overhead of pixel-wise correspondence, PSGAN suffers from two main issues. First, the predicted pixel-to-region attention is 6549
ambiguous and thus leading to the color bleeding problem around facial components (see results in Sec. 4). Second, it is cumbersome to implement local transfer from multiple references, as it requires computing dense correspondences for every image and reconstructs a new attentive matrix in a pixel-wise manner.
In this paper, we overcome the spatial misalignment bar-rier from a completely different perspective. We aim to ex-tract the spatially invariant 1D style-code from the reference image and re-assign it to the source one. Our two-step prin-ciple gets over the challenging pixel-wise matching, lead-ing to a simple learning emphasis of makeup assignment.
To this end, we propose a new model, called Style-based controllable GAN (SCGAN), that consists of two “extrac-tion” and one “assignment” modules. Particularly, a Part-speciﬁc Style Encoder (PSEnc) is designed to extract the makeup style of each part (e.g., lip, skin, and eyes) from one (or multiple) face(s) with the given face parsing maps.
The extracted style-code is encoded in a component-wise manner, enabling ﬂexible local manipulation in the down-stream applications. On the other hand, inspired by Style-GAN [15], the makeup style is mapped into an intermediate style space instead of using the linearly projected vectors, thus the extracted attributes are less entangled to different factors of variation. Meanwhile, we propose a Face Iden-tity Encoder (FIEnc) to extract the face identity features from the source image. Then, a Makeup Fusion Decoder (MFDec) is presented to progressively fuses the style-code and the face identity features in different feature levels us-ing AdaIN layers [11] and generates the ﬁnal makeup trans-fer results. Our proposed model demonstrates great ﬂexi-bility in makeup transfer, as shown in Fig. 1. Our model achieves shade-controllable makeup transfer by linear com-bining the style-codes from the source and the reference im-ages (Fig. 1(a) and (b)). Partial transfer from different ref-erence images can be easily achieved by integrating their style-codes (Fig. 1(c)). More importantly, our proposed model is invariant to pose variations (Fig. 1(d)). Extensive experiments and comparisons demonstrate the superiority of our proposed model comparing with the state-of-art ap-proaches.
Our contributions are three-fold:
• We propose a fully automatic makeup transfer model with the best ﬂexibility comparing with existing ap-proaches. Global/local makeup transfer and removal with shade-control can be easily realized by editing the style-codes without extra computational efforts.
• We break down the makeup transfer problem into a two-step extraction-assignment process. A style-based network PSEnc is proposed to map the makeup style into a component-wise style-code. This design elimi-nates the spatial misalignment problem.
• Our proposed model achieves state-of-the-art perfor-mance even with large spatial misalignment between the source and the reference images. 2.