Abstract
Neural image compression leverages deep neural net-works to outperform traditional image codecs in rate-distortion performance. However, the resulting models are also heavy, computationally demanding and generally opti-mized for a single rate, limiting their practical use. Focus-ing on practical image compression, we propose slimmable compressive autoencoders (SlimCAEs), where rate (R) and distortion (D) are jointly optimized for different capacities.
Once trained, encoders and decoders can be executed at different capacities, leading to different rates and complex-ities. We show that a successful implementation of Slim-CAEs requires suitable capacity-speciﬁc RD tradeoffs. Our experiments show that SlimCAEs are highly ﬂexible models that provide excellent rate-distortion performance, variable rate, and dynamic adjustment of memory, computational cost and latency, thus addressing the main requirements of practical image compression. 1.

Introduction
Visual information (e.g. images, videos) plays a central role in human content creation, communication and interac-tion. Efﬁcient storage and transmission through constrained channels requires compression. We can thus consider the basic (lossy) image compression problem, where the goal is to obtain the shortest binary representation (i.e. lowest rate bitstream) that can represent the input image at a cer-tain level of ﬁdelity (i.e. minimum distortion). Thus, low rate and low distortion are fundamentally opposing objec-tives, in practice involving a rate-distortion (RD) tradeoff.
The more challenging problem of practical image compres-sion further includes real-world constraints such as mem-ory, computation and latency, related with their implemen-tation in resource-constrained devices (e.g. mobile phones) and networks. Similarly, video compression addresses the same problem for sequences of images, where low com-plexity and latency become even more critical [41]. Many applications also require dynamic control of the RD tradeoff
Rate-dist. perform.
Total memory
Variable
Rate Memory FLOPs
Method
JPEG,JP2K
BPG
Single CAE
Low Very low Yes
Yes
High
Optimal Medium No
High Yes
Medium Medium Yes
MAE[42],cAE[10] High Medium Yes
Optimal Medium Yes
Multiple CAEs Optimal
BScale[36]
Low
--No
Yes
No
No
Yes
Training time
--Low
High
Low
Low
Low
--No
Yes
No
No
Yes
SlimCAE
Table 1: Comparison of compression methods.
Encoder
Bitstream
Decoder
Optimal
Naive y i t x e l
C o m p ( M B , F L O P
) s
Rate (bpp)
)
R
N
S
P ( n o i t r o t s i
D
Figure 1: Variable rate and complexity adaptive image com-pression with a slimmable compressive autoencoder. to adapt to speciﬁc rate requirements (i.e. variable rate).
Traditional methods (e.g.
JPEG [40], JPEG2000 [25, 31], BPG [32]) follow the transform coding paradigm with carefully designed linear transforms and coding tools, to ef-fectively address practical image compression. Encoding is block-based and iterative. RD is optimized during encoding by exhaustively searching optimal block partitions and cod-ing and prediction modes. Complexity can be controlled by limiting the range of coding and prediction modes, or using heuristics. The rate can be estimated from previous blocks, and controlled by adjusting quantization parameters.
A more recent paradigm is neural image compression (NIC) [37, 6, 11, 38, 36, 15, 20, 21, 22, 19] (or learned im-age compression), which exploits ﬂexible nonlinear trans-forms and entropy models parametrized as deep neural net-works. The framework typically consists of an autoen-coder followed by quantization and entropy coding (hence-forth a compressive autoencoder -CAE- [36]). While they can outperform traditional image codecs in RD, they are 4998  
typically only optimal for a single target rate, and at the cost of much heavier and computationally expensive mod-els that require specialized hardware (e.g. GPUs), making them unattractive in practical resource-limited scenarios. In contrast to block-based codecs, NIC approaches are typi-cally image-based and feed-forward, resulting in a constant processing cost. While some approaches address variable rate [36, 8, 42, 10], practical concerns related with efﬁ-ciency still remain largely unaddressed (see Table 1).
The challenge of deploying deep neural networks in resource-limited devices (e.g. smartphones, tablets) has motivated research on lightweight architectures [12, 29], in-teger and binary networks [16, 27, 13] and automatic ar-chitecture search [34]. However, only few works have ad-dressed efﬁciency in NIC [28, 14]. We borrow the idea of slimmable neural networks [44], where the width of the lay-ers (i.e. number of channels) of a classiﬁer can be adjusted to trade off accuracy for computational efﬁciency.
In this paper, we propose the slimmable compressive autoencoder (SlimCAE) framework, where we show that the slimming mechanism can enable both variable rate and adaptive complexity (see Fig. 1). We propose and study dif-ferent variants of slimmable generalized divisive normaliza-tion [5] (GDN) layers, and slimmable probability models.
Naive training of SlimCAEs, with the different subnetworks (i.e. subCAEs) optimizing the same loss on all widths, re-sults in suboptimal performance. We crucially observe that each RD tradeoff has an corresponding minimum capacity.
This suggest that, in contrast to other slimmable networks, each width should have different objectives, i.e. different
D + λR, determined by the corresponding tradeoff λ. This characteristic makes SlimCAEs more difﬁcult to train, and unlikely to beneﬁt from implicit or explicit distillation [44].
Addressing this problem, we propose λ-scheduling an algo-rithm that alternates between training the model and adjust-ing the different λs. Via slimming, SlimCAEs can address the main requirements of practical neural image compres-sion (PNIC) in a simple and integrated way.
Our main contributions are: (1) a novel rate and com-plexity control mechanism via layer widths, motivated by a key insight connecting optimal RD tradeoffs and capacity; (2) the SlimCAE framework, which enables control of com-putation, memory and rate, required for PNIC; (3) an efﬁ-cient training algorithm for SlimCAEs; (4) novel slimmable modules (i.e. GDNs, entropy models). In addition, Slim-CAEs can be easily adapted to obtain scalable bitstreams. 2.