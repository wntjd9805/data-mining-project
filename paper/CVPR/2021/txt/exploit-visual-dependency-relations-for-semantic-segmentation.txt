Abstract
Dependency relations among visual entities are ubiq-uity because both objects and scenes are highly structured.
They provide prior knowledge about the real world that can help improve the generalization ability of deep learning ap-proaches. Different from contextual reasoning which fo-cuses on feature aggregation in the spatial domain, visual dependency reasoning explicitly models the dependency re-lations among visual entities. In this paper, we introduce a novel network architecture, termed the dependency network or DependencyNet, for semantic segmentation.
It uniﬁes dependency reasoning at three semantic levels. Intra-class reasoning decouples the representations of different object categories and updates them separately based on the inter-nal object structures. Inter-class reasoning then performs spatial and semantic reasoning based on the dependency re-lations among different object categories. We will have an in-depth investigation on how to discover the dependency graph from the training annotations. Global dependency reasoning further reﬁnes the representations of each object category based on the global scene information. Extensive ablative studies with a controlled model size and the same network depth show that each individual dependency rea-soning component beneﬁts semantic segmentation and they together signiﬁcantly improve the base network. Experi-mental results on two benchmark datasets show the Depen-dencyNet achieves comparable performance to the recent states of the art. 1.

Introduction
Semantic segmentation aims at assigning a categorical label to each pixel to partition an image into multiple mean-ingful segments. It is a fundamental task in computer vision and has many practical applications, such as autonomous driving, image editing, and medical image analysis. In the past decade, convolutional neural networks (CNNs) have become a dominant solution to it [26, 11].
*Corresponding author.
Figure 1. Visual dependence relations are ubiquity since both ob-jects and scenes are highly structured. They provide prior knowl-edge about the real world that can be used to improve the gener-alization ability of a learning model. We consider three levels of visual dependency. Intra-class dependency means parts of an ob-ject are related due to the object’s internal structural patterns, e.g., a bicycle consists of wheels, pedals and a frame. Inter-class depen-dency means objects are related as certain objects (e.g., a rider and a bicycle) co-occur more frequently than the others or by chance.
Global dependency means the global scene type enforces strong prior on the categories of objects that should appear in it.
Recent works [54, 56, 51, 9, 39] have achieved great im-provement by leveraging contextual information in CNNs.
The context of a pixel refers to the collection of its sur-rounding pixels. It provides rich visual cues to resolve am-biguities in pixel classiﬁcation. For example, the presence of water in the context suggests that a pixel is more likely to belong to a boat than other objects like a car or a bed. Ex-isting approaches like ASPP [1] and PSPNet [57] explore optimal strategies for multi-scale context aggregation. The non-local network [46] and its variants [61, 21, 20] employ the self-attention mechanism [44] to capture the long-range context in an image. These methods effectively leverage the context to enrich the representations of each pixel. How-ever, they focus on feature aggregation in the spatial do-main, and the explicit dependence relations among different semantic categories are largely ignored.
Visual dependence relations are ubiquity since both ob-jects and scenes are highly structured, and they occur at var-ious semantic levels. Parts are related as they compose into objects, e.g., a car consists of wheels and a frame. Objects 9726
are related as certain objects (e.g., a desk and a chair) co-occur more frequently than the others (e.g., a bed and a car).
At the image level, the type of a scene enforces strong prior on the categories of objects that should appear in it. For example, it is unlikely to see a bed in an outdoor scene.
Different from contextual reasoning which focuses on aggregating pixel features from the context, visual depen-dency reasoning puts more emphasis on exploiting the de-pendency relations among semantic entities, e.g., parts, ob-jects, and a scene. This makes it possible to inject explicit prior knowledge of the real world into a learning model and thus promotes its generalization ability.
In this paper, we introduce a novel approach termed the dependency network or DependencyNet for semantic segmentation.
It explicitly models visual dependency re-lations in a CNN. As shown in Figure 1, we divide vi-sual dependency into three levels, i.e., intra-class, inter-class and global dependency. Accordingly, the Dependen-cyNet performs three levels of dependency reasoning.
It
ﬁrst decouples the representations of different object cate-gories so that each representation contains spatial and se-Intra-class rea-mantic information of only one category. soning means to update the representations of each object category based on their respective internal structures, e.g., a person is composed of body parts. Inter-class reasoning per-forms spatial and semantic reasoning based on the depen-dency relations among different object categories. We ﬁrst mine prior knowledge about dependency relations from the training annotations and encode it via a dependency graph.
Two objects are strongly related if they co-occur frequently in images. Then, the DependencyNet performs reasoning via group weighted convolutions, wherein category-speciﬁc representations interact with each other according to the de-pendency graph. Unlike the attention mechanisms [37, 19] which compute the feature correlations across the spatial lo-cations or feature channels, our graph does not depend on the input image and acts as prior knowledge. The inter-class reasoning is also different from graph convolution networks (GCNs) [18, 2, 30]. GCNs take as input feature vectors of positioned objects while we perform both spatial and se-mantic reasoning to localize objects. Global dependency reasoning further reﬁnes the representations of each cate-gory based on the scene information. Specially, we encode a scene via probabilities of the presence of each category and use them to enrich the object representations. The con-tributions of this paper are summarized below.
• We introduce a novel DependencyNet to explicitly ex-ploit visual dependency relations for semantic segmen-tation. It is the ﬁrst neural architecture to unify three levels of dependency reasoning. The research is im-portant as it bridges CNNs and dependency modeling commonly achieved via graphical models.
• We introduce intra-class, inter-class, and global depen-dency reasoning modules, which are the core compo-nents of the DependencyNet. They effectively utilize the internal object structures, object-object relations, and scene information to perform dependency reason-ing. We also have an in-depth investigation of mining prior knowledge of dependency relations from training annotations.
• We perform extensive ablation studies with a con-trolled model size and the same depth on the three lev-els of dependency reasoning. Results show that each individual component beneﬁts semantic segmentation and they together lead to signiﬁcant improvement over the base network. Experimental results on two datasets demonstrate the effectiveness of our approach. 2.