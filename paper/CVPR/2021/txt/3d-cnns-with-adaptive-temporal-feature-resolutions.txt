Abstract
While state-of-the-art 3D Convolutional Neural Net-works (CNN) achieve very good results on action recogni-tion datasets, they are computationally very expensive and require many GFLOPs. While the GFLOPs of a 3D CNN can be decreased by reducing the temporal feature reso-lution within the network, there is no setting that is opti-mal for all input clips. In this work, we therefore introduce a differentiable Similarity Guided Sampling (SGS) module, which can be plugged into any existing 3D CNN architec-ture. SGS empowers 3D CNNs by learning the similarity of temporal features and grouping similar features together.
As a result, the temporal feature resolution is not anymore static but it varies for each input video clip. By integrat-ing SGS as an additional layer within current 3D CNNs, we can convert them into much more efﬁcient 3D CNNs with adaptive temporal feature resolutions (ATFR). Our evalu-ations show that the proposed module improves the state-of-the-art by reducing the computational cost (GFLOPs) by half while preserving or even improving the accuracy.
We evaluate our module by adding it to multiple state-of-the-art 3D CNNs on various datasets such as Kinetics-600, Kinetics-400, mini-Kinetics, Something-Something V2,
UCF101, and HMDB51. 1.

Introduction
In recent years, there has been a tremendous progress for video processing in the light of new and complex deep learning architectures, which are based on variants of 3D
Convolutional Neural Networks (CNNs) [24, 9, 7, 4, 6, 12, 8]. They are trained for a speciﬁc number of input frames,
⋆Mohsen Fayyaz and Emad Bahrami equally contributed to this work.
Emad Bahrami contributed to this project while he was a visiting re-searcher at the Computer Vision Group of the University of Bonn. typically between 16 to 64 frames. For classifying a longer video, they slide over the video and the outputs are then ag-gregated. These networks, however, are often very expen-sive to train and heavy to deploy for inference task. In order to reduce the inference time, [15, 20] proposed to process not all parts of a video with the same 3D CNN. While [15] trains a second network that decides for each chunk of in-put frames if it should be processed by the more expensive 3D CNN, [20] uses a ﬁx scheme where a subset of the in-put chunks are processed by an expensive 3D CNN and the other chunks by a less expensive 3D CNN. The latter then uses an RNN to fuse the outputs of the different 3D CNNs.
Although both approaches effectively reduce the GFLOPS during inference, they increase the training time since two instead of one network need to be trained. Furthermore, they do not reduce the computational cost of the 3D CNNs themselves.
In this work, we propose an approach that makes 3D
CNNs more efﬁcient for training and inference. Our pro-posal is based on the observation that the computational cost of a 3D CNN depends on the temporal resolution it operates on at each stage of the network. While the temporal reso-lution can be different at different stages, the schemes that deﬁne how the temporal resolution is reduced is hard-coded and thus the same for all videos. However, it is impossible to deﬁne a scheme that is optimal for all videos. If the tem-poral resolution is too much reduced, the network is forced to discard important information for some videos. This re-sults in a decrease of the action recognition accuracy per-formance. Vice versa, a high temporal resolution results in highly redundant feature maps and increases the computa-tional time, which makes the 3D CNN highly inefﬁcient for most videos. In this work, we therefore address the question of how a 3D CNN can dynamically adapt its computational resources in a way such that not more resources than neces-sary are used for each input chunk.
In order to address this question, we propose to ex-ploit the redundancy within temporal features such that 3D 14731
Figure 1: The difﬁculty of recognizing actions varies largely across videos. For videos with slow motion (top), the temporal features that are processed within a 3D CNN can be highly redundant. However, there are also very challenging videos where all features are required to understand the content (bottom). While previous 3D CNNs use ﬁx down-sampling schemes that are independent of the input video, we propose a similarity guided sampler that groups and aggregates redundant information of temporal features into B′ ≤ T feature maps. The core aspect is that this process adapts the internal temporal resolution to the input video such that B′ is small if the input features are redundant (top) and large (bottom) if most of the features are required.
CNNs process and select the most valuable and informative temporal features for the action classiﬁcation task. In con-trast to previous works, we propose to dynamically adapt the temporal feature resolution within the network to the input frames such that on one hand important informa-tion is not discarded and on the other hand no computa-tional resources are wasted for processing redundant infor-mation. To this end, we propose a Similarity Guided Sam-pling (SGS) mechanism that measures the similarity of tem-poral feature maps, groups similar feature maps together, and aggregates the grouped feature maps into a single out-put feature map. The similarity guided sampling is designed such that it is differentiable and number of output feature maps varies depending on the redundancy of the temporal input feature maps as shown in Fig. 1. By integrating the similarity guided sampling as an additional module within any 3D CNN, we convert the 3D CNN with ﬁxed temporal feature resolutions into a much more efﬁcient dynamic 3D
CNN with adaptive temporal feature resolutions (ATFR).
Note that this approach is complementary to [15, 20] and the two static 3D CNNs used in these works can be replaced by adaptive 3D CNNs. However, even with just a single 3D
CNN with adaptive temporal feature resolutions, we already achieve a higher accuracy and lower GFLOPs performance compared to [15, 20].
We demonstrate the efﬁciency of 3D CNNs with adap-tive temporal feature resolutions by integrating the simi-larity guided sampler into the current state-of-the-art 3D
CNNs such as R(2+1)D [25], I3D [3], and X3D [8].
It drastically decreases the GFLOPs by about half in aver-age while the accuracy remains nearly the same or gain improvements. In summary, the similarity guided sampler is capable of signiﬁcantly scaling down the computational cost of off-the-shelf 3D CNNs and therefore plays a crucial role for real-world video-based applications. 2.