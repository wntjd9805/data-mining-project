Abstract
Training of Convolutional Neural Networks (CNNs) with data with noisy labels is known to be a challenge. Based on the fact that directly providing the label to the data (Positive
Learning; PL) has a risk of allowing CNNs to memorize the contaminated labels for the case of noisy data, the indirect learning approach that uses complementary labels (Nega-tive Learning for Noisy Labels; NLNL) has proven to be highly effective in preventing overﬁtting to noisy data as it reduces the risk of providing faulty target. NLNL further employs a three-stage pipeline to improve convergence. As a result, ﬁltering noisy data through the NLNL pipeline is cumbersome, increasing the training cost. In this study, we propose a novel improvement of NLNL, named Joint Neg-ative and Positive Learning (JNPL), that uniﬁes the ﬁlter-ing pipeline into a single stage. JNPL trains CNN via two losses, NL+ and PL+, which are improved upon NL and
PL loss functions, respectively. We analyze the fundamental issue of NL loss function and develop new NL+ loss func-tion producing gradient that enhances the convergence of noisy data. Furthermore, PL+ loss function is designed to enable faster convergence to expected-to-be-clean data.
We show that the NL+ and PL+ train CNN simultaneously, signiﬁcantly simplifying the pipeline, allowing greater ease of practical use compared to NLNL. With a simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classiﬁcation based on the superior ﬁltering ability. 1.

Introduction
Convolutional Neural Networks (CNNs) have led to great improvements in many supervised tasks. However,
CNNs’ performance relies heavily on the quality of labels, and accurately labeling a huge amount of data is expen-sive and time-consuming. Furthermore, accurate labeling is done by hand, which can eventually lead to mismatched labeling. Therefore, the robust training of CNNs with noisy data is of great practical importance. There are many ap-proaches regarding this issue. For example, there are meth-ods that design noise-robust loss [4, 3, 29, 18], use two neu-ral networks to select clean labels [6, 33, 30], and utilize label correction [22, 31]. These existing approaches com-monly use the given labels in a direct manner, i.e., “input image belongs to this label” (Positive Learning; PL). This behavior carries the risk of providing faulty information to the CNNs when noisy labels are involved.
Motivated by this reason, Negative Learning for Noisy
Labels; NLNL [12], which is an indirect learning method for training CNNs, has been proposed recently. Negative
Learning (NL) uses randomly chosen complementary la-bels and trains the CNN that “input image does not belong to this complementary label,” reducing the risk of providing the wrong information because of the high chance of not selecting a true label as a complementary label. Addition-ally, NLNL proposed three-stage pipeline for ﬁltering noisy data from training data (Figure 1 (a)). Each stage is com-posed of NL → NL while discarding data of low conﬁdence (Selective NL; SelNL) → PL while only retaining data of high conﬁdence (Selective PL; SelPL), enabling more con-vergence after NL. However, the fundamental problem that
NL loss function causes underﬁtting to the overall training data still remains. This is the reason that NL requires an additional sequential step, SelNL. Furthermore, the three-stage pipeline for ﬁltering noisy data is quite inefﬁcient, ex-tending the time for training CNNs.
In this study, we propose a novel version of NLNL: Joint
Negative Learning and Positive Learning; JNPL which has a uniﬁed single-stage pipeline for ﬁltering noisy data (Fig-ure 1 (b)). JNPL is composed of two losses to train CNN,
NL+ and PL+ losses, dedicated to ﬁltering noisy data from training data. Each is developed from NL and PL loss func-tions, respectively. Firstly, our paper focuses on analyzing the NL loss function to understand the cause for underﬁt-ting. Then we develop a new loss function NL+ that re-solves the issue, which produces a gradient appropriate for convergence on a noisy training dataset. Our study demon-strates the effectiveness of NL+, showing improved conver-gence across various label noise types and noise rates. Sec-ondly, while we utilize PL to aid in training with noisy data,
PL+ loss function is also newly designed to enable faster 9442
NL selNL selPL
NL selNL selPL
JNPL
JNPL (a) NLNL (b) JNPL
Figure 1: Comparison between Negative Learning for Noisy Labels (NLNL) and Joint Negative and Positive Learning (JNPL) for ﬁltering noisy data from training data, demonstrated with histograms showing the distribution of noisy training data . (a):
NLNL is a 3-stage pipeline (NL→SelNL→SelPL). (b): JNPL is a single-stage pipeline, in which two loss functions (NL+ and PL+) train CNN simultaneously. training with expected-to-be-clean data. Our paper shows the effectiveness of the PL+ loss function compared to the previous PL loss function. Finally, as both loss functions of our method (NL+ and PL+) jointly train the model through a single stage, it is simple and easier to use than NLNL. Our experiments show that JNPL successfully ﬁlters noisy data in a single stage, thereby providing signiﬁcantly faster train-ing of CNN as well as better ﬁltering compared to NLNL.
After ﬁltering noisy data from the training data we per-form pseudo-labeling for noisy data classiﬁcation. We achieve state-of-the-art accuracy across various settings in
CIFAR10, CIFAR100 [13], and Clothing1M [31] datasets, proving the superior ﬁltering ability of JNPL.
The main contributions of this paper are as follows:
• We propose an improved version of NLNL, named “Joint
Negative and Positive Learning (JNPL),” featuring a single-stage pipeline for ﬁltering noisy data, therefore enabling easier usage compared to NLNL.
• Two novel loss functions are newly designed, each named
NL+ loss and PL+ loss. NL+ solves the underﬁtting problem of the NL loss, and provides better convergence on various types and ratios of label noises in the training data. Moreover, PL+ enables faster training compared to the previous PL loss function.
• Our method ﬁlters noisy data, more robust across differ-ent types and ratios of noise than NLNL. Our method also achieves state-of-the-art noisy data classiﬁcation re-sults when used along with pseudo-labeling.
• Prior knowledge of the type or number of noisy data is not required for our method.
It does not require any hyper-parameter tuning that depend on prior knowledge, allowing our method to be applicable in practice.
The remainder of this paper is organized as follows. Sec-tion 3 describes NLNL method in depth, which is targeted throughout the whole paper, and discusses the cause of the underﬁtting problem of the method. Section 4 describes our proposed method, JNPL, and explains in detail on NL+ loss and PL+ loss terms. Section 5 demonstrates the overall comparison between JNPL and NLNL, showing the distinct advantages of JNPL over NLNL. Section 6 discusses the evaluations of our method in comparison to baseline meth-ods. Finally, we summarize and conclude in Section 7. 2.