Abstract 1.

Introduction
Image inpainting is the task of plausibly restoring miss-ing pixels within a hole region that is to be removed from a target image. Most existing technologies exploit patch sim-ilarities within the image, or leverage large-scale training data to ﬁll the hole using learned semantic and texture infor-mation. However, due to the ill-posed nature of the inpaint-ing task, such methods struggle to complete larger holes containing complicated scenes. In this paper, we propose
TransFill, a multi-homography transformed fusion method to ﬁll the hole by referring to another source image that shares scene contents with the target image. We ﬁrst align the source image to the target image by estimating multi-ple homographies guided by different depth levels. We then learn to adjust the color and apply a pixel-level warping to each homography-warped source image to make it more consistent with the target. Finally, a pixel-level fusion mod-ule is learned to selectively merge the different proposals.
Our method achieves state-of-the-art performance on pairs of images across a variety of wide baselines and color dif-ferences, and generalizes to user-provided image pairs.
Image inpainting is an image restoration task where the goal is to ﬁll in speciﬁc regions of the image while making the entire image visually realistic. The regions to be ﬁlled are called hole regions, and could contain undesired fore-ground objects or small distracting elements, or corrupted regions of the image. Much research has been devoted to improving image inpainting either by image self-similarity
[3]) or deep generative models (e.g. [67, 60, 66]). (e.g.
Such methods synthesize realistic semantics and textures by reusing similar patches from non-hole regions or learn-ing from large collections of images, respectively. However, those methods still struggle in cases when holes are large, or the expected contents inside hole regions have complicated semantic layout, texture, or depth.
These problems can be addressed if there happens to be a second reference image of the same scene that exposes some desired image content that can be copied to the hole.
This task is referred to as reference-guided image inpainting in the literature [37], but this topic is less explored. In our paper, we call the image with the hole indicated for removal the target image. In general, there could be multiple other source images used as references. These could be taken by the photographer for the same scene after objects have moved or the photographer moved the camera to a different 12266
viewpoint to expose the background. Alternatively, a source image could be collected from the Internet [57]. If one such source image contains new desired appearance for the target hole region, then we can copy the pixels from the source to
ﬁll in the target hole regions. In this paper we assume that the user has identiﬁed a particular source image with the new desired appearance, so we refer to this as the source image. We imagine that dedicated apps might be created for aiding the photographer in this process, or for automatically retrieving suitable such source images from the Internet.
Although the reference source image makes the inpaint-ing task easier, reference-guided inpainting is still quite challenging for several reasons. First, the hole regions could be very large, which makes the task of guessing the pixel colors in the hole region less well-posed. Second, we wish for our task to be as general as possible, so we allow an uncalibrated camera to freely translate to different 3D po-sitions for the source and target image, because this can al-low the photographer to intentionally reveal regions behind a foreground object to be removed. Such translations, how-ever, can induce large parallax, which cannot be modeled in image space by a simple 2D warp such as a global homogra-phy. Unlike video inpainting or multi-view Structure-from-Motion (SfM), we assume the system will not have access to more than two photos. Thus, it is harder in our setting to reliably estimate 3D structures, depth, and point corre-spondences. Third, depending on the camera and photog-raphy setup, the photographs may have substantially differ-ent exposure, white balance, or lighting environment, and if one photograph comes from the Internet, then it will have different camera response curves. Existing methods based purely on warping cannot resolve the resulting complex is-sues of color matching. Finally, there may exist regions in the source image that do not exist after warping due to pix-els being out of the image or occluded.
To address these challenges, we propose a multi-homography fusion pipeline combined with deep warping, color harmonization, and single image inpainting. We ad-dress the issue of parallax by assuming that there may be multiple depth planes inside the hole. Loosely inspired by recent work on multiplane images [9, 75, 33, 53], we pro-pose multiple homographic registrations of the source im-age to the target, each corresponding to an assumption that the scene geometry lies on a different 3D plane. Given a target and a source image, we ﬁrst estimate the matched feature points between the two images, cluster the inliers according to their estimated depths in the target image, and for each cluster estimate a single homography to perform an initial image registration. We call each of these candi-date alignment images a proposal. For each proposal, we then tackle the challenge of color matching by using a deep bilateral color transformation, and we address parallax is-sues by reﬁning the warp using a learned per-pixel spatial transformation. We then merge all the transformed source image proposals by learning a set of fusion masks. Finally, we address the last challenge regarding regions which do not exist in the source image by using a state-of-the-art sin-gle image inpainting method to complete missing regions, and learn to merge it as well.
In summary, the main contributions of our method are: (1) We propose TransFill, a multi-homography estimation pipeline to obtain multiple transformations of the source im-age, where each aligns a speciﬁc region to the target image; (2) We propose to learn a color and spatial transformer to simultaneously perform a color matching and make a per-pixel spatial transformation to address any residual differ-ences after the initial alignment; (3) We learn weights suit-able for combining all ﬁnal proposals with a single image inpainting result. 2.