Abstract
We introduce RoboPose, a method to estimate the joint angles and the 6D camera-to-robot pose of a known articu-lated robot from a single RGB image. This is an important problem to grant mobile and itinerant autonomous systems the ability to interact with other robots using only visual information in non-instrumented environments, especially in the context of collaborative robotics. It is also challeng-ing because robots have many degrees of freedom and an inﬁnite space of possible conﬁgurations that often result in self-occlusions and depth ambiguities when imaged by a single camera. The contributions of this work are three-fold.
First, we introduce a new render & compare approach for es-timating the 6D pose and joint angles of an articulated robot that can be trained from synthetic data, generalizes to new unseen robot conﬁgurations at test time, and can be applied to a variety of robots. Second, we experimentally demon-strate the importance of the robot parametrization for the iterative pose updates and design a parametrization strategy that is independent of the robot structure. Finally, we show experimental results on existing benchmark datasets for four different robots and demonstrate that our method signiﬁ-cantly outperforms the state of the art. Code and pre-trained models are available on the project webpage [1]. 1.

Introduction
The goal of this work is to recover the state of a known ar-ticulated robot within a 3D scene using a single RGB image.
The robot state is deﬁned by (i) its 6D pose, i.e. a 3D transla-tion and a 3D rotation with respect to the camera frame, and (ii) the joint angle values of the robot’s articulations. The problem set-up is illustrated in Figure 1. This is an important problem to grant mobile and itinerant autonomous systems the ability to interact with other robots using only visual information in non-instrumented environments. For instance, in the context of collaborative tasks between two or more robots, having knowledge of the pose and the joint angle 1Inria Paris and D´epartement d’informatique de l’ENS, ´Ecole normale sup´erieure, CNRS, PSL Research University, 75005 Paris, France. 2LIGM, ´Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-vall´ee,
France. 3Czech Institute of Informatics, Robotics and Cybernetics at the Czech
Technical University in Prague.
Figure 1: RoboPose. (a) Given a single RGB image of a known articulated robot in an unknown conﬁguration (left), RoboPose estimates the joint angles and the 6D camera-to-robot pose (rigid translation and rotation) providing the complete state of the robot within the 3D scene, here illustrated by overlaying the articulated
CAD model of the robot over the input image (right). (b) When the joint angles are known at test-time (e.g. from internal measure-ments of the robot), RoboPose can use them as an additional input to estimate the 6D camera-to-robot pose to enable, for example, visually guided manipulation without ﬁducial markers. values of all other robots would allow better distribution of the load between robots involved in the task [5].
The problem is, however, very challenging because robots can have many degrees of freedom (DoF) and an inﬁnite space of admissible conﬁgurations that often result in self-occlusions and depth ambiguities when imaged by a single camera. The current best performing methods for this prob-lem [28, 61] use a deep neural network to localize in the image a ﬁxed number of pre-deﬁned keypoints (typically located at the articulations) and then solve a 2D-to-3D opti-mization problem to recover the robot 6D pose [28] or pose 11654
and conﬁguration [61]. For rigid objects, however, methods based on 2D keypoints [34, 3, 7, 6, 45, 52, 23, 50, 44, 43, 18] have been recently outperformed by render & compare meth-ods that forgo explicit detection of 2D keypoints but instead use the entire shape of the object by comparing the rendered view of the 3D model to the input image and iteratively re-ﬁning the object’s 6D pose [59, 31, 25]. Motivated by this success, we investigate how to extend the render & compare paradigm for articulated objects. This presents signiﬁcant challenges. First, we need to estimate many more degrees of freedom than the sole 6D pose. Articulated robots we consider in this work can have up to 15 degrees of freedom in addition to their 6D rigid pose in the environment. Sec-ond, the space of conﬁgurations is continuous and hence there are inﬁnitely many conﬁgurations in which the object can appear. As a result, it is not possible to see all conﬁg-urations during training and the method has to generalize to unseen conﬁgurations at test time. Third, the choice of transformation parametrization plays an important role for 6D pose estimation of rigid objects [31] and ﬁnding a good parametrization of pose updates for articulated objects is a key technical challenge.
Contributions. To address these challenges, we make the following contributions. First, we introduce a new render & compare approach for estimating the 6D pose and joint an-gles of an articulated robot that can be trained from synthetic data, generalizes to new unseen robot conﬁgurations at test time, and can be applied to a large variety of robots (robotic arms, bi-manual robots, etc.). Second, we experimentally demonstrate the importance of the robot pose parametriza-tion for the iterative pose updates and design an effective parametrization strategy that is independent of the robot.
Third, we apply the proposed method in two settings: (i) with known joint angles (e.g. provided by internal measure-ments from the robot such as joint encoders), only predicting the camera-to-robot 6D pose, and (ii) with unknown joint an-gles, predicting both the joint angles and the camera-to-robot 6D pose. We show experimental results on existing bench-mark datasets for both settings that include a total of four different robots and demonstrate signiﬁcant improvements compared to the state of the art. 2.