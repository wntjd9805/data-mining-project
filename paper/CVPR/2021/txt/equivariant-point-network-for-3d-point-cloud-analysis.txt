Abstract
Features that are equivariant to a larger group of sym-metries have been shown to be more discriminative and powerful in recent studies [4, 40, 5]. However, higher-order equivariant features often come with an exponentially-growing computational cost. Furthermore, it remains rel-atively less explored how rotation-equivariant features can be leveraged to tackle 3D shape alignment tasks. While many past approaches have been based on either non-equivariant or invariant descriptors to align 3D shapes, we argue that such tasks may beneﬁt greatly from an equiv-ariant framework.
In this paper, we propose an effective and practical SE(3) (3D translation and rotation) equivari-ant network for point cloud analysis that addresses both problems. First, we present SE(3) separable point convo-lution, a novel framework that breaks down the 6D con-volution into two separable convolutional operators alter-natively performed in the 3D Euclidean and SO(3) spaces respectively. This signiﬁcantly reduces the computational cost without compromising the performance. Second, we in-troduce an attention layer to effectively harness the expres-siveness of the equivariant features. While jointly trained with the network, the attention layer implicitly derives the intrinsic local frame in the feature space and generates at-tention vectors that can be integrated with different align-ment tasks. We evaluate our approach through extensive studies and visual interpretations. The empirical results demonstrate that our proposed model outperforms strong baselines in a variety of benchmarks. Code is available at https://github.com/nintendops/EPN PointCloud. 1.

Introduction
The success of 2D CNNs stems in large part from the ability of exploiting the translational symmetries via weight sharing and translation equivariance. Recent trends strive
Figure 1: The core of our network is a convolution opera-tor on point clouds, termed SE(3) separable point convolu-tion (SPConv), that consumes features deﬁned in the SE(3) space and outputs per-point features that are SE(3) equivari-ant. When the output feature is spatially pooled over the Eu-clidean space, it becomes SO(3) equivariant, as visualized above by projecting onto the spherical domain. Our method also supports a faithful conversion from the equivariant fea-ture to its invariant counterpart by using a novel attentive fusing mechanism. Thereby, we offer a general framework that can generate equivariant or invariant point feature de-pending on the nature of downstream applications. to duplicate this success to 3D domain in order to shed new light on the 3D learning tasks. With the 3D scanning tech-nology being the mainstream manner of measuring the real world, point cloud arises naturally as one of the most promi-nent 3D representations. Yet, despite its simple and uniﬁed structure, it remains a nuisance to extend the CNN archi-tecture to analyzing point clouds.
In addition, the group of transformations in 3D data is more complex compared to 2D images, as 3D entities are often transformed by ar-bitrary 3D translations and 3D rotations when observed.
Although group-invariant operators could render identical 14514
features even under different group transforms, it fails to distinguish distinct instances with internal symmetries (e.g. the counterparts of “6” and “9” in 3D scenarios regarding rotational symmetry). In contrast, equivariant features are much more expressive thanks to their ability to retain in-formation about the input group transform on the feature maps throughout the neural layers. As a result, it could be very beneﬁcial for point cloud features to be equivariant to the SE(3) group of transformations while being invariant to point permutations.
Despite the importance of deriving SE(3)-equivariant features for point clouds, progress in this regard remains highly sparse. The main obstacles arise in two aspects.
First, the cost of computing convolutions between 6-dimensional functions over the entire SE(3) spaces is pro-hibitive especially in the presence of bulky 3D raw scans.
Second, it remains challenging to fully harness the expres-siveness of equivariant features without losing important structural information at a low computational cost. In par-ticular, matching any two group-equivariant features is the prerequisite of many applications like correspondence com-putation, pose estimation, etc. One common practice is to compute the best relative group transformation that maxi-mizes the similarity of the input features when the trans-formation is applied. This typically requires solving a PnP optimization which is quite costly considering the high di-mensionality of the features. Another option is to fuse the equivariant features into invariant ones via pooling opera-tion and directly compare the invariant features to obtain similarity. However, we argue that the naive pooling opera-tions will inevitably discard useful features and damage the equivariant structure of the feature.
In this paper, we strive to address both of the problems by introducing an effective and practical framework for learn-ing SE(3)-equivariant features of point clouds. In particular, inspired by the spirit of “going wider” in the Inception mod-ule [35], we ﬁrst propose SE(3) separable convolution, a novel paradigm that breaks down the naive 6D convolution into two separable convolutional operators alternatively per-formed in the 3D Euclidean and SO(3) spaces. Due to the non-commutative and non-compact nature of SE(3) group, it is non-trivial to factorize SE(3) convolution into two sepa-rable sub-operators. We achieve this goal by ﬁrst lifting the input points to the homogeneous space. We then take ad-vantage of the ﬁnite rotation groups such as the icosahedral and aggregate spatially-convoluted features as functions on the rotation groups that are processed via group convolu-tion. The proposed SE(3) separable convolution signiﬁ-cantly reduces the computational cost of a SE(3) convolu-tion and leads to practical solutions that can be deployed in the commodity hardware.
Second, we present an attention mechanism specially tai-lored for fusing SE(3)-equivariant features. We observe that while the commonly used pooling operations, such as max or mean pooling, work well in translation equivariant net-works like 2D CNNs, they are not best suited for fusing equivariant features in SO(3) groups. This is mostly due to the highly sparse and non-linear structure of SO(3) features which poses additional challenges for max/mean pooling to maintain its unique pattern without losing too much infor-mation. We introduce group attentive pooling (GA pooling) to adaptively fuse rotation-equivariant features into their in-variant counterparts. Trained together with the network, the
GA pooling layer implicitly learns an intrinsic local frame of the feature space and generates attention weights to guide the pooling of rotation-equivariant features.
Third, compared to invariant features, equivariant fea-tures preserves, rather than discards, spatial structure and therefore can be seen as a more discriminative represen-tation. It is for this reason that translational equivariance has been the premise for convolutional approaches for de-tection and instance segmentation [14]. Similarly, through the attention mechanism, the equivariant framework can be utilized for inferring 3D rotations. We demonstrate in the experiments that this structure signiﬁcantly outperforms a non-equivariant framework in a shape alignment task.
We validate our proposed framework on a variety of tasks. Experimental results show that our approach con-sistently outperforms strong baselines. We also perform ab-lation analysis and qualitative visualization to evaluate the effectiveness of each algorithmic component. 2.