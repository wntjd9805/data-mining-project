Abstract
Estimating scene geometry from data obtained with cost-effective sensors is key for robots and self-driving cars. In this paper, we study the problem of predicting dense depth from a single RGB image (monodepth) with optional sparse measurements from low-cost active depth sensors. We in-troduce Sparse Auxiliary Networks (SANs), a new module enabling monodepth networks to perform both the tasks of depth prediction and completion, depending on whether only RGB images or also sparse point clouds are available at inference time. First, we decouple the image and depth map encoding stages using sparse convolutions to process only the valid depth map pixels. Second, we inject this in-formation, when available, into the skip connections of the depth prediction network, augmenting its features. Through extensive experimental analysis on one indoor (NYUv2) and two outdoor (KITTI and DDAD) benchmarks, we demon-strate that our proposed SAN architecture is able to simul-taneously learn both tasks, while achieving a new state of the art in depth prediction by a signiÔ¨Åcant margin. 1.

Introduction
Dense scene geometry can be directly measured using active sensors (e.g., LiDAR, structured light) or estimated from RGB cameras (e.g., via stereo matching, structure from motion, monocular depth networks). Both approaches have complementary strengths and failure modes (e.g., rain or low light). Consequently, a robust perception system must leverage both modalities while still retaining function-ality when only one is available. In this paper, we propose a learning algorithm and model that can satisfy these desider-ata with a simple sensor suite: a single monocular RGB camera combined with any low-cost active depth sensor re-turning only a few 3D points per scene.
Monocular depth prediction is becoming a cornerstone capability for a wide range of robotic applications where
RGB cameras are ubiquitous [23, 49, 51]. Recently, self-Figure 1: Our proposed joint task learning SAN archi-tecture produces state of the art monocular depth estimates from a single image (prediction), which can be further im-proved by also providing a sparse depth map (completion) without changing the model. supervised methods trained only on raw videos demon-strated that robots with a single camera can learn and predict dense depth information [2, 14, 15, 17, 58, 60], especially as the quantity of data increases [18]. However, in prac-tice an active range sensor is often available, and can be used to either provide further supervision at training time
[7, 9, 10, 30, 53] or also during inference [24, 33, 39], in a task known as depth completion. Even though sparse, re-cent works [20] have shown that even a few pixels contain-ing valid depth information is enough to boost performance, and therefore should not be discarded. Importantly, these two tasks, depth prediction and completion, are treated as separate problems with different architectures. No method to date tackles the issue of using all the information avail-able from both modalities at both training and inference time, including if only partially available (e.g., due to sensor blackout, occlusion, or environmental conditions).
Our main contribution is a novel architecture, Sparse
Auxiliary Networks (SANs, cf. Fig. 2), that enables a monocular depth prediction network to also perform depth completion in the presence of optional sparse 3D measure-ments at inference time. Note that the same architecture 111078
and weights can dynamically perform either task at infer-ence time, depending on the presence or not of sparse depth measurements. Our model relies on a sparse depth convo-lutional encoder to inject depth information, when avail-able, into the skip connections of state-of-the-art encoder-decoder networks for depth prediction. Our second contri-bution is a thorough experimental evaluation on three chal-lenging outdoor (KITTI [12] and DDAD [18]) and indoor (NYUv2 [40]) datasets, demonstrating that our SAN archi-tecture boosts monocular depth prediction performance and sets a new state of the art in this task. 2.