Abstract
Vehicle detection with visual sensors like lidar and cam-era is one of the critical functions enabling autonomous driving. While they generate ﬁne-grained point clouds or high-resolution images with rich information in good weather conditions, they fail in adverse weather (e.g., fog) where opaque particles distort lights and signiﬁcantly re-duce visibility. Thus, existing methods relying on lidar or camera experience signiﬁcant performance degradation in rare but critical adverse weather conditions. To remedy this, we resort to exploiting complementary radar, which is less impacted by adverse weather and becomes prevalent on vehicles. In this paper, we present Multimodal Vehicle
Detection Network (MVDNet), a two-stage deep fusion de-tector, which ﬁrst generates proposals from two sensors and then fuses region-wise features between multimodal sen-sor streams to improve ﬁnal detection results. To evalu-ate MVDNet, we create a procedurally generated training dataset based on the collected raw lidar and radar signals from the open-source Oxford Radar Robotcar. We show that the proposed MVDNet surpasses other state-of-the-art methods, notably in terms of Average Precision (AP), espe-cially in adverse weather conditions. The code and data are available at https://github.com/qiank10/MVDNet. 1.

Introduction
As the holy grail of autonomous driving technology, Full
Driving Automation (Level 5) [20] relies on robust all-weather object detection, which provides accurate bound-ing boxes of surrounding objects even in the challenging adverse foggy weather condition. Nowadays, autonomous vehicles are equipped with multiple sensor modalities, such as camera, lidar, and radar [12, 6, 48, 3]. Fusing multimodal sensors overcomes any individual sensor’s occasional fail-ures and potentially yields more accurate object detection than using only a single sensor. Existing object detec-tors [10, 21, 52, 38] mainly fuse lidar and camera, which normally provide rich and redundant visual information.
However, these visual sensors are sensitive to weather con-ditions and are not expected to work fully in harsh weather like fog [4, 26], making the autonomous perception systems unreliable. For example, Fig. 1a shows an example of a driving scenario with ground-truth vehicles labeled. Fig. 1b shows the detected vehicles using only lidar point cloud that (a) Ground-truth (clear weather) (b) MVDNet (lidar-only) (c) MVDNet (radar-only) (d) Complete MVDNet (Ours)
Figure 1. Performance overview of our proposed MVDNet. (a) 360◦ bird’s eye view of the 3D lidar point cloud and ground-truth labels (colors represent different vehicles). The vehicle equipped with lidar and radar is at the center. In foggy weather, (b) lidar-only MVDNet misses vehicles at farthest range due to fog occlu-sion and misclassiﬁes background points as vehicles; (c) radar-only MVDNet produces false alarms and inaccurate bounding boxes due to noisy radar data; (d) By deeply fusing lidar and radar, the complete MVDNet correctly detects vehicles. is deteriorated by fog. Two farthest vehicles at the top are missing due to the occlusion of fog.
Aside from lidar and camera, radar has been widely de-ployed on autonomous vehicles [6, 3, 58] and has the po-tential to overcome foggy weather. Speciﬁcally, radar uses millimeter-wave signals whose wavelength is much larger than the tiny particles forming fog, rain, and snow [14, 1], and hence easily penetrates or diffracts around them. How-ever, radars in the existing autonomous driving datasets are still underexplored, mainly due to their signiﬁcant data spar-sity, as compared with camera and lidar. For example, the nuScenes dataset [6] has about 35K lidar points but only 200 radar points on average in each data frame. The main reason is that its radars use conventional electronically steerable 444
antenna array, which tends to generate beam patterns with wide beamwidth (3.2◦-12.3◦). In the DENSE [3] dataset, a proprietary radar is mounted on the front bumper of the vehicle. However, its angular ﬁeld of view is only 35◦.
Fortunately, the recent Oxford Radar Robotcar [2] (ORR) deploys a radar with a rotating horn antenna, which has high directionality and much ﬁner spatial resolution of 0.9◦, and is mechanically rotated to achieve 360◦ ﬁeld of view.
The ORR radar generates dense intensity maps, as shown in Fig. 1c, where each pixel represents the reﬂected signal strength. It creates a new opportunity for object detection in foggy weather condition.
Despite the richer information, the ORR radar is still sig-niﬁcantly coarser and noisier than its visual counterpart, i.e., lidar, as showcased in Fig. 1a and 1c. As a result, if it is pro-cessed in the same way as the lidar point cloud, then false alarms and large regression errors show up. To robustly de-tect vehicles in foggy weather, one should take advantage of both lidar (ﬁne granularity within visible range) and radar (immunity to foggy weather) while overcoming their short-comings. To this end, we propose MVDNet, a multimodal deep fusion model for vehicle detection in adverse foggy weather condition. MVDNet consists of two stages. The
ﬁrst stage generates proposals from the lidar and radar sep-arately. The second stage employs the adaptive fusion of the two sensors’ features via attention and the temporal fusion using 3D convolutions. Such a late fusion scheme allows the model to generate sufﬁcient proposals while focusing the fusion within the regions of interest (ROI). As shown in
Fig. 1d, MVDNet can not only detect the vehicles occluded by fog in the lidar point clouds but also reject false alarms in the noisy radar intensity maps.
To validate MVDNet, we create a procedurally gener-ated training dataset based on the raw lidar and radar sig-nals from ORR. Speciﬁcally, we manually generate oriented bounding boxes for vehicles in the lidar point clouds, syn-chronize the radar and lidar with the knowledge of visual odometry, and simulate random fog effects using an accu-rate fog model proposed in DEF [3]. We compare MVDNet with the state-of-the-art lidar-alone detectors [55, 24, 46], or lidar and radar fusion [3]. Evaluation results show that
MVDNet achieves notably better performance on vehicle detection in foggy weather condition while requiring 10× less computing resource.
Our core contributions are two folds. First, we propose a deep late fusion detector that effectively exploits lidar and radar’s complementary advantages. To our knowledge,
MVDNet represents the ﬁrst vehicle detection system that fuses lidar and high-resolution 360◦ radar signals for vehi-cle detection. Second, we introduce a labeled dataset with
ﬁne-grained lidar and radar point cloud in foggy weather condition. We assess MVDNet on the proposed dataset and demonstrate the effectiveness of the proposed fusion model. 2.