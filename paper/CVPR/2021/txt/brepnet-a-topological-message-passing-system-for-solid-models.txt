Abstract
Boundary representation (B-rep) models are the stan-dard way 3D shapes are described in Computer-Aided De-sign (CAD) applications. They combine lightweight para-metric curves and surfaces with topological information which connects the geometric entities to describe mani-folds. In this paper we introduce BRepNet, a neural net-work architecture designed to operate directly on B-rep data structures, avoiding the need to approximate the model as meshes or point clouds. BRepNet deﬁnes convolutional kernels with respect to oriented coedges in the data struc-ture.
In the neighborhood of each coedge, a small col-lection of faces, edges and coedges can be identiﬁed and patterns in the feature vectors from these entities detected by speciﬁc learnable parameters. In addition, to encour-age further deep learning research with B-reps, we pub-lish the Fusion 360 Gallery segmentation dataset. A col-lection of over 35,000 B-rep models annotated with infor-mation about the modeling operations which created each face. We demonstrate that BRepNet can segment these mod-els with higher accuracy than methods working on meshes, and point clouds.
Figure 1: BRepNet convolutional kernels are deﬁned with respect to topological entities called coedges (dashed ar-rows). Feature vectors from a small collection of faces (grey), edges (black) and coedges (blue) adjacent to each coedge (red) are multiplied by the learnable parameters in the kernel. The hidden states arising from the convolution can then be pooled to perform face segmentation. 1.

Introduction
Boundary representation (B-rep) models are the de facto standard for describing 3D objects in commercial Computer
Aided Design (CAD) software. They consist of collections of trimmed parametric surfaces along with the adjacency relationships between them [44]. Prismatic shapes can be represented using lightweight primitive curves and surfaces while free-form objects can be deﬁned using NURBS [33].
Although this makes the representation both compact and expressive, the complexity of the data structures and limited availability of labelled datasets has presented a high barrier to entry for researchers.
The problem of segmenting B-rep models, based on learned patterns, is of particular interest as it allows the au-tomation of many laborious manual tasks in CAD, Com-puter Aided Engineering (CAE) and Computer Aided Pro-cess Planning (CAPP) [6, 45, 1, 38]. Currently these require a user to repeatedly select groups of faces and/or edges as input for the modeling or manufacturing operation. Exam-ples include model simpliﬁcation in preparation for ﬁnite element analysis [12] and segmenting a model according to the manufacturing process or machining toolpath strategy required to make the object [1, 45].
In addition, parametric feature history is often lost when models are exchanged between different CAD applications
[23] and many commercial CAD systems use segmentation algorithms to recover this information [5, 13]. 12773
Although attempts were made in the 90s to apply neural networks to the task of B-rep segmentation [19, 11, 30, 14, 41, 38], the absence of machine learning frameworks and large labelled datasets caused progress to stall until very re-cently [20, 10]. In this paper we introduce BRepNet, a novel neural network architecture designed speciﬁcally to operate directly on the faces and edges of B-rep data structures and take full advantage of the topological relationships between them. In addition, we hope to revitalize interest in the prob-lem of B-rep segmentation with the publication of the Fu-sion 360 Gallery segmentation dataset. For the ﬁrst time we provide a collection of over 35,000 3D models, in multiple representations, annotated with segmentation labels reveal-ing the modeling operations used to create them.
The BRepNet approach is motivated by the observation that in convolutional neural networks for image processing, the weights operate on pixels with known locations within the ﬁlter window. A similar arrangement can be achieved with B-reps, where a small collection of faces, edges and coedges can be identiﬁed at well deﬁned locations relative to each coedge in the data structure (see Figure 1). Feature vectors can be extracted from these neighbouring entities and concatenated in a known order, allowing convolution to take place as a matrix/vector multiplication [18, 21]. As in image convolution, speciﬁc entities relative to each coedge map to speciﬁc learnable parameters in our convolutional kernels, allowing patterns in the input data to be easily rec-ognized [32, 9]. The key contributions are as follows:
• We introduce BRepNet, a network architecture using a novel convolution technique which takes full advan-tage of the topological information the B-rep stores.
• We publish the Fusion 360 Gallery segmentation dataset that contains over 35,000 segmented 3D mod-els in B-rep, mesh, and point cloud format.
• We provide experimental results on the Fusion 360
Gallery segmentation task, including ablation studies and comparisons to other representations and methods.
Our results demonstrate that direct use of B-rep data solves the Fusion 360 Gallery segmentation problem with higher performance and parameter efﬁciency than other techniques based on point cloud and mesh representations. 2.