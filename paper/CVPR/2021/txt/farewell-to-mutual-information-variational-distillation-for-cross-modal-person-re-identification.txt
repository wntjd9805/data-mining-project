Abstract
The Information Bottleneck (IB) provides an information theoretic principle for representation learning, by retaining all information relevant for predicting label while minimiz-ing the redundancy. Though IB principle has been applied to a wide range of applications, its optimization remains a challenging problem which heavily relies on the accurate estimation of mutual information. In this paper, we present a new strategy, Variational Self-Distillation (VSD), which provides a scalable, ﬂexible and analytic solution to essen-tially ﬁtting the mutual information but without explicitly estimating it. Under rigorously theoretical guarantee, VSD enables the IB to grasp the intrinsic correlation between representation and label for supervised training. Further-more, by extending VSD to multi-view learning, we in-troduce two other strategies, Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML), which sig-niﬁcantly improve the robustness of representation to view-changes by eliminating view-speciﬁc and task-irrelevant in-formation. To verify our theoretically grounded strategies, we apply our approaches to cross-modal person Re-ID, and conduct extensive experiments, where the superior perfor-mance against state-of-the-art methods are demonstrated.
Our intriguing ﬁndings highlight the need to rethink the way to estimate mutual information. 1.

Introduction
The Information Bottleneck (IB) [35] has made remark-able progress in the development of modern machine per-ception systems such as computer vision [6], speech pro-cessing [21], neuroscience [30] and natural language pro-cessing [18]. It is essentially an information-theoretic prin-ciple that transforms raw observation into a, typically lower-dimensional, representation and this principle is naturally extended to representation learning or understanding Deep
Neural Networks (DNNs) [31, 24, 9].
By ﬁtting mutual information (MI), IB allows the learned representation to preserve complex intrinsic correlation structures over high dimensional data and contain the in-formation relevant with downstream task [35]. However, despite successful applications, there is a signiﬁcant draw-back in conventional IB hindering its further development (i.e., estimation of mutual information).
In practice, mutual information is a fundamental quan-tity for measuring the statistical dependencies [2] between random variables, but its estimation remains a challenging problem. To address this, traditional approaches [7, 5, 33, 16, 23, 15, 8, 14, 32, 22] mainly resort to non-parametric es-timator under very limited problem setting where the prob-ability distributions are known or the variables are discrete
[29, 2]. To overcome this constraint, some works [13, 6, 29] adopt the trainable parametric neural estimators involving reparameterization, sampling, estimation of posterior dis-tribution [2], which, unfortunately, practically has very poor scalability. Besides, the estimation of posterior distribution would become intractable when the network is complicated.
Another obvious drawback of the conventional IB is that, the optimization of IB is essentially a tradeoff between having a concise representation and one with good pre-dictive power, which makes it impossible to achieve both high compression and accurate prediction [35, 1, 28, 38].
Consequently, the optimization of conventional IB becomes tricky, and its robustness is also seriously compromised due to the mentioned reasons.
In this paper, we propose a new strategy for the in-formation bottleneck named as Variational Self-Distillation (VSD), which enables us to preserve sufﬁcient task-relevant information while simultaneously discarding task-irrelevant distractors. We should emphasize here that our approach essentially ﬁts the mutual information but without ex-plicitly estimating it. To achieve this, we use variational inference to provide a theoretical analysis which obtains an analytical solution to VSD. Different from traditional meth-ods that attempt to develop estimators for mutual informa-tion, our method avoids all complicated designs and allows 1522
the network to grasp the intrinsic correlation between the data and label with theoretical guarantee.
Furthermore, by extending VSD to multi-view learn-ing, we propose Variational Cross-Distillation (VCD) and
Variational Mutual-Learning (VML), the strategies that im-prove the robustness of information bottleneck to view-changes. VCD and VML eliminate the view-speciﬁc and task-irrelevant information without relying on any strong prior assumptions. More importantly, we implement VSD,
VCD and VML in the form of training losses and they can beneﬁt from each other, boosting the performance. As a re-sult, two key characteristics of representation learning (i.e., sufﬁciency and consistency) are kept by our approach.
To verify our theoretically grounded strategies, we ap-ply our approaches to cross-modal person re-identiﬁcation1, a cross-modality pedestrian image matching task. Exten-sive experiments conducted on the widely adopted bench-mark datasets demonstrate the effectiveness, robustness and impressive performance of our approaches against state-of-the-arts methods. Our main contributions are summarized as follows:
• We design a new information bottleneck strategy (VSD) for representation learning. By using varia-tional inference to reconstruct the objective of IB, we can preserve sufﬁcient label information while simul-taneously getting rid of task-irrelevant details.
• A scalable, ﬂexible and analytical solution to ﬁtting mutual information is presented through rigorous the-oretical analysis, which fundamentally tackle the difﬁ-culty of estimation of mutual information.
• We extend our approach to multi-view representation learning, and it signiﬁcantly improve the robustness to view-changes by eliminating the view-speciﬁc and task-irrelevant information. 2.