Abstract
Recently, numerous algorithms have been developed to tackle the problem of vision-language navigation (VLN), i.e., entailing an agent to navigate 3D environments through following linguistic instructions. However, current VLN agents simply store their past experiences/observations as latent states in recurrent networks, failing to capture envi-ronment layouts and make long-term planning. To address these limitations, we propose a crucial architecture, called
Structured Scene Memory (SSM). It is compartmentalized enough to accurately memorize the percepts during navi-gation. It also serves as a structured scene representation, which captures and disentangles visual and geometric cues in the environment. SSM has a collect-read controller that adaptively collects information for supporting current deci-sion making and mimics iterative algorithms for long-range reasoning. As SSM provides a complete action space, i.e., all the navigable places on the map, a frontier-exploration based navigation decision making strategy is introduced to enable efﬁcient and global planning. Experiment results on two VLN datasets (i.e., R2R and R4R) show that our method achieves state-of-the-art performance on several metrics. 1.

Introduction
As a crucial step towards building intelligent embod-ied agents, autonomous navigation has long been studied in robotics. Since Anderson et al. [3] extended prior efforts[9, 37] in instruction based navigation into photo-realistic sim-ulated scenes [6], vision-language navigation (VLN) has recently attracted increasing attention in computer vision community. Towards the goal of enabling an agent to ex-ecute navigation instructions in 3D environments, current representative VLN methods made great advances in: i) de-veloping more powerful learning paradigms[50, 48]; ii) ex-ploring extra supervision signals from synthesized data[13, 45, 14] or auxiliary tasks [48, 25, 33, 55]; iii) designing more efﬁcient multi-modal embedding schemes[24, 40, 49]; and iv) making more intelligent path planning [28, 34, 47].
*Corresponding author: Wenguan Wang.
Figure 1: In the partially observed environment (b), our agent builds SSM (c) for persistent memorization and topological scene representation. SSM supports long-range planning over the global action space, thus our agent can easily change its navigation di-rection (i.e., P1) by directly ‘jumping’ to any previously visited position (e.g., P2), resulting in robust navigation.
Despite these progresses, current VLN models typically follow[3] to tackle the task through a sequence-to-sequence (Seq2Seq) framework, which maps instructions and online observations to navigation actions. With such a design, the perceived information are embedded and mixed in the in-ternal recurrent units. This prohibits the agent from di-rectly accessing to its past observations and understanding the environment layout. Taking Fig. 1 as an example. VLN agents execute instructions in a partially observed environ-ment (Fig. 1(b)). As current VLN agents simply encode past perceptions (blue area in Fig. 1(b)) as hidden network states, their decision-making is only limited within a lo-cal action space (i.e., currently navigable directions – green area in Fig.1(b)). Thus they tend to make sub-optimal nav-igation decisions, failing to perform path planning over the explored environment (i.e., global action space in Fig.1(b)).
In robotics, map-building is long studied for facilitating path planning [46, 15]. Inspired by classic simultaneous lo-calization and mapping (SLAM) models [46, 19], recent ef-forts make use of deep learning techniques for constructing more efﬁcient space representations [17, 12, 7, 8]. On the other hand, making path planning in VLN, in essence, re-quires efﬁciently modeling long-term dependencies within 8455
online observation sequences. However, the states (inter-nal memory) in recurrent networks are latent and shown in-herently unstable over long timescales [44]. Alternatively, neural networks with external memories provide a feasible solution, which uses a compartmentalized memory to pre-serve and recall long-term information and has been shown successful in language modeling [44] and deep reinforce-ment learning (RL) agents in 3D environments [38, 39].
Built off these two lines of previous efforts, we propose an essential, outside memory architecture, called Structured
Scene Memory (SSM). SSM online collects and precisely stores the percepts during navigation (see Fig. 1(b-c)).
It is graph-structured, yielding a topological representation of the environment. Thus SSM is able to perform long-term memorization and capture the environment layouts, allow-ing our agent to make efﬁcient planning. In addition, SSM delivers a global action space – all the navigable locations within the explored area, enabling ﬂexible decision making.
As shown in Fig.1(c), when necessary, our agent can simply move off its current navigation direction (i.e., P1) and travel to another previously visited far location (i.e., P2).
Speciﬁcally, SSM consists of nodes that embed visual information in explored locations and edges that represent geometric relations between connected locations. As both visual and geometric cues are captured and disentangled, in-structions can be better grounded onto the visual world, i.e., correlating perception and action related descriptions with nodes and edges in SSM respectively. SSM is equipped with a collect-read controller, which adaptively reads con-tent from the memory, depending on current navigation con-text. The controller also mimics iterative algorithms for comprehensive information gathering and long-range rea-soning. Hence, SSM brings a global action space but its gradually increased scale will make policy learning harder.
We propose a frontier-exploration based decision making strategy that addresses this issue elegantly and efﬁciently.
Extensive experiments on Room-to-Room (R2R)[3] and
Room-for-Room (R4R)[26] datasets demonstrate the effec-tiveness of the full approach and core model designs. 2.