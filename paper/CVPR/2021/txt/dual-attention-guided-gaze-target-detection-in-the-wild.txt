Abstract
Gaze target detection aims to infer where each person in a scene is looking. Existing works focus on 2D gaze and 2D saliency, but fail to exploit 3D contexts. In this work, we propose a three-stage method to simulate the human gaze inference behavior in 3D space. In the ﬁrst stage, we intro-duce a coarse-to-ﬁne strategy to robustly estimate a 3D gaze orientation from the head. The predicted gaze is decom-posed into a planar gaze on the image plane and a depth-channel gaze. In the second stage, we develop a Dual Atten-tion Module (DAM), which takes the planar gaze to produce the ﬁled of view and masks interfering objects regulated by depth information according to the depth-channel gaze. In the third stage, we use the generated dual attention as guid-ance to perform two sub-tasks: (1) identifying whether the gaze target is inside or out of the image; (2) locating the target if inside. Extensive experiments demonstrate that our approach performs favorably against state-of-the-art meth-ods on GazeFollow and VideoAttentionTarget datasets. 1.

Introduction
Gaze cues indicate what a person is interested in, and thus function as an important means to evaluate intentions and predict human behaviors in various social contexts [12].
For these reasons, gaze analysis has widely been used in neurophysiology studies [36, 10], relevant saliency predic-tion [8, 34], and social awareness tracking [7, 29, 30]. How-ever, most existing works need particular equipment (e.g., eye tracker [13], VR/AR device, or costly RGB-D cam-eras [42]) or specialized settings (e.g., human-robot inter-action [31, 40], or constrained subject locations [32, 1]). In contrast, we are concerned with gaze target detection from a more readily available source in daily life, i.e., a single im-age in the wild. As depicted in Figure 1 (a), given a scene and the head location for each person (bounding box), we
∗Equal contribution. †Corresponding anthor. aim to predict where they are looking, including identifying out-of-frame targets and locating inside-frame targets (dot).
Existing methods [37, 28, 5, 6, 46] typically reason about salient objects in the scene conditioned on an estimated gaze orientation. While signiﬁcant advances have been made, there are still three critical problems to be considered. (1)
Most prior works explore the gaze direction in 2D represen-tations and barely encode the depth-channel gaze. They fail to capture whether the marked person is looking forward, backward, or sideward (see Figure 1 (b)). An intuitive so-lution proposed by Chong et al. [5] simply incorporates the 3D gaze as an additional feature channel but does not rea-sonably combine with scene contexts. Thus, we need an explicit 3D gaze representation coupling with a more ef-fective way to exploit it. (2) Previous methods search for salient objects mainly from 2D visual cues. It is hard for them to capture exact spatial information for lack of scene depth understanding. For instance, two or more candidate objects at different depths may exist along the subject’s gaze direction (see Figure 1 (c)). Thus, we need to model the person-relative depth of surroundings for 3D scene under-standing. (3) Existing approaches directly learn a mapping function from head features to gaze direction. They are hard to cope with the ﬁxation inconsistency between the eyes and the head (see Figure 1 (d), e.g., facing forward but look-ing downward). Thus, we need to learn the dependency be-tween eyes and the head for a more accurate prediction.
Based on the above observations, we propose a three-stage scheme to simulate the human gaze inference behav-ior in 3D space. When one infers the gaze target of another person, he/she ﬁrst predicts a gaze orientation and then esti-mates the target by analyzing the 3D geometry of the scene along the gaze direction. Similarly, in the ﬁrst stage, we learn to estimate a 3D gaze direction from the head image.
The predicted gaze is decomposed into a planar gaze on the 2D image plane and a depth-channel gaze. Then we propose the Dual Attention Module (DAM) to model the person’s depth-aware perspective in the scene as the sec-ond stage. Speciﬁcally, we aggregate two parallel atten-tion components. One is a ﬁeld of view (FOV) generator 11390
(a) Gaze target detection examples. (b) Depth-channel gaze angle. (c) Scene depth understanding. (d) Inconsistent head pose & eyeball orientation.
Figure 1. Examples of gaze target detection ((a)) and failure cases of existing methods [37, 28, 5, 6] ((b)-(d)). Given an image and the ground truth location of a head (bounding box), our approach learns to predict where the person is looking, including identifying out-of-frame targets and locating inside-frame targets (dot). Yellow lines and red lines in (b)-(d) represent ground truth gaze and the predicted gaze, respectively. (b) shows misjudgement (e.g., located at the front woman in the ﬁrst example) by existing methods [5, 6] resulting from the lack of depth-channel gaze (e.g., the marked baby is looking backward not forward). (c) shows defect of these methods [5, 28] in scene depth understanding. The ﬁrst example is a cycling man looking at the side ground, while these methods wrongly predict at the front cow.
Existing methodology [5, 37] is incapable to precisely estimate gaze when inconsistency occurs between head pose (e.g., the girl is facing forward in the ﬁrst example) and eyeball orientation (e.g., the girl is actually looking downward). Our novel network architecture, guided by a Dual Attention Module, well solves above problems and achieves accurate detection results. for FOV attention, and the other is a Depth Rebasing com-ponent for depth attention. The FOV Generator takes the planar gaze to produce a perspective scope on the image plane. The Depth Rebasing segments the scene into front-, middle-, back-grounds based on the pre-estimated depth priori, and subsequently deduces the focused ground from the dept-channel gaze. The depth attention effectively as-sists in masking 2D-salient interfering objects at unmatched depths in the FOV attention, bridging the 2D cues with 3D relations.
In the last stage, we take the dual attention as guidance for two sub-tasks. We formulate the ﬁrst sub-task, i.e., identifying out-of-frame targets, as a binary classiﬁca-tion problem, and the second sub-task, i.e., locating inside-frame targets, as a heatmap regression problem.
Speciﬁcally for the ﬁrst stage, we introduce a coarse-to-ﬁne strategy to estimate 3D gaze in the wild. It is a challeng-ing task for large camera-to-subject distances, diversities in illumination, free subject motions and the resulting varia-tions of appearance in unconstrained images. We present a coarse-grained component to estimate a coarse gaze from the head image. This component does not rely on facial key points but on visible head features. Even with completely occluded eyes and faces, the model outputs a relatively ac-curate prediction. To cope with the possible inconsistency between the eyes and the head, we take an eye detector and reﬁne the coarse gaze with ﬁne-grained eye features using a transform layer. The model learns the intrinsic correlations between eyes and the face explicitly when eyes are visible.
In this way, the proposed 3D gaze estimator can manage in-the-wild images and boost the gaze estimation accuracy.
Our contributions can be summarized as follows:
• We design a novel Dual Attention Module (DAM) that explicitly embodies the person’s ﬁeld of view regu-lated by depth information in 3D space. To the best of our knowledge, we are among the ﬁrst to incorporate scene depth understanding in 2D gaze target detection.
• We introduce a coarse-to-ﬁne strategy to estimate 3D gaze orientation. The robust gaze estimator shows competitive generalization properties on images in the wild, especially for eye-included or occluded cases.
• We demonstrate that the proposed method performs fa-vorably against state-of-the-art methods on the Gaze-Follow [37] benchmark and the VideoAttentionTar-get [6] benchmark. 2.