Abstract
Original Scenario
AdvSim Scenario
As self-driving systems become better, simulating sce-narios where the autonomy stack may fail becomes more important. Traditionally, those scenarios are generated for a few scenes with respect to the planning module that takes ground-truth actor states as input. This does not scale and cannot identify all possible autonomy failures, such as per-ception failures due to occlusion.
In this paper, we pro-pose AdvSim, an adversarial framework to generate safety-critical scenarios for any LiDAR-based autonomy system.
Given an initial trafﬁc scenario, AdvSim modiﬁes the ac-tors’ trajectories in a physically plausible manner and up-dates the LiDAR sensor data to match the perturbed world.
Importantly, by simulating directly from sensor data, we ob-tain adversarial scenarios that are safety-critical for the full autonomy stack. Our experiments show that our approach is general and can identify thousands of semantically mean-ingful safety-critical scenarios for a wide range of modern self-driving systems. Furthermore, we show that the robust-ness and safety of these systems can be further improved by training them with scenarios generated by AdvSim. 1.

Introduction
Self-driving vehicles (SDV) are safety critical applica-tions in which the comprehensive testing is necessary before real-world deployment. As the performance of self-driving systems becomes better on natural and well-behaved scenar-ios, it becomes of key importance to ﬁnd scenarios where the system is likely to fail. However, exhaustively searching over all possible scenarios to identify safety critical ones is computationally unfeasible, as there are exponentially many scenario variations due to the combinatorial number of pos-sible lane topologies, actor conﬁgurations, trajectories, ve-locity proﬁles, appearance of actors and background, etc.
Conventional practice in industry for comprehensive testing is a semi-autonomic process that relies on human expertise to create an initial scenario set, where each sce-Perturbed Trajectory
Ego-vehicle
Perturbed vehicle
Collided actor
Figure 1: AdvSim modiﬁes trafﬁc scenarios in a physically plausible manner to produce autonomy system failure. Here the SDV collides with a crossing pedestrian after the behav-ior of a nearby bus is modiﬁed by AdvSim. The collision is indicated by the red arrow. nario contains at most 1 or 2 “actors of interest” (e.g., ve-hicles that interact with the SDV’s planned path) with spec-iﬁed initial locations and trajectories [34, 26]. Scenario variations are then programmatically created by varying the actors’ locations and velocity proﬁles. While such sce-narios are valuable, they only evaluate simple interactions with the SDV and do not test complex multi-actor inter-actions, such as lane-merging and unprotected left-turns in dense trafﬁc scenes. They do not test the autonomy sys-tem on the wide variety of scenarios that the SDV may en-counter. Moreover, human involvement makes this process time-consuming and difﬁcult to scale. Manual design may also result in missing testing conﬁgurations that identify un-expected failure modes, as it is difﬁcult to assess coverage.
To address this problem, recent works aim to automate the scenario generation procedure by searching over the set of possible scenarios and identifying high-risk ones ac-cording to a speciﬁed cost function. Most previous works
[14, 43, 23, 11, 13] only consider a motion planning module that has access to the ground-truth state of the actors in the scene. This overlooks the fact that many adversarial sce-narios often involve actors that are hard to identify due to occlusion, or that have trajectory plans that can be difﬁcult to localize and forecast. Such issues in the perception and motion forecasting modules of the autonomy system can 9909
Point Clouds
Adversarial
Behavior
+
LiDAR 
Simulation
Simulated Point Clouds
Cost Measure 
!$%&
Autonomy 
System
Black-box Optimization
Safety-Critical Scenarios
Perturbed 
Vehicle
Collided 
Vehicle
Perturbed 
Trajectory
Planned 
Trajectory
Figure 2: Overview of our proposed adversarial scenario generation pipeline. Our goal is to perturb the maneuvers of interactive actors in an existing scenario with adversarial behaviors that cause realistic autonomy system failures. Given an existing scenario and its original sensor data, we perturb the scenario and update accordingly how the SDV would observe the
LiDAR sensor data based on the new scene conﬁguration. We then evaluate the autonomy system on the modiﬁed scenario, compute an adversarial objective, and update the proposed perturbation using a search algorithm. generate compounding errors that ultimately cause planning failures. While [30, 29, 1] test end-to-end image-based self-driving systems, the adversarial scenarios they generate are either generated at a small scale [1] or with respect to sim-pliﬁed imitation learning models that do not reﬂect the au-tonomy system of modern self-driving vehicles [30, 29].
Additionally, most past works either modify the scenario by changing high-level actor behavior [14, 13, 22, 30, 29, 1], or create physically unrealizable trajectories [43]. This does not allow for physically-plausible ﬁne-grained control of the actor trajectory, such as creating a nudging actor.
In contrast, we want to ﬁnd complex and realistic safety-critical scenarios at scale for the full self-driving system.
Towards this goal, we frame the generation of worst-case scenarios as a black box adversarial attack that can test any
LiDAR-based autonomy system. We explore adversarial perturbations with respect to physically feasible changes in actor behavior, since such perturbations provide insight into the different types of driving situations that are challenging.
This contrasts previous works on black box attacks for per-ception systems [46, 24, 50, 42, 47] that perturb appearance and texture, but do not perturb actor behavior.
In this paper, we leverage real world trafﬁc scenarios available in standard self-driving datasets and optimize the actors’ trajectories jointly to increase the risk of an auton-omy system failure. As our perturbation modiﬁes the ac-tors’ trajectories, we need to adjust the sensor data to accu-rately reﬂect the actors’ new locations. We therefore adopt a high-ﬁdelity LiDAR simulator [27] that modiﬁes the sensor data accordingly taking into account occlusions. After run-ning the black-box autonomy system with modiﬁed sensor data as input, we obtain the planned trajectory and evaluate how adversarial the scenario was. Our adversarial objec-tive captures multiple safety factors such as collisions, vio-lations in trafﬁc rules, and uncomfortable driving behaviors.
We demonstrate the ﬂexibility and scalabity of our approach by generating over 4000 adversarial scenarios for a wide range of modern autonomy systems. Finally, we leverage
AdvSim-generated safety-critical scenarios in training and further improve the safety of autonomy systems. 2.