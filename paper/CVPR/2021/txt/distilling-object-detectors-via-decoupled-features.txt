Abstract without KD
COCO mAP = 37.7%
Bed 6.9% 5.4%
KD: object regions
COCO mAP = 39.9% 5.0% 5.9% 6.4% 4.0% 8.8% 5.7% 3.2% 10.0% 68.5% 70.2% 70.9%
KD: background regions
COCO mAP = 39.9% 6.9% 4.2% 4.8% 3.2% 10.0%
Knowledge distillation is a widely used paradigm for in-heriting information from a complicated teacher network to a compact student network and maintaining the strong per-formance. Different from image classiﬁcation, object detec-tors are much more sophisticated with multiple loss func-tions in which features that semantic information rely on are tangled. In this paper, we point out that the information of features derived from regions excluding objects are also essential for distilling the student detector, which is usu-ally ignored in existing approaches. In addition, we eluci-date that features from different regions should be assigned with different importance during distillation. To this end, we present a novel distillation algorithm via decoupled fea-tures (DeFeat) for learning a better student detector. Specif-ically, two levels of decoupled features will be processed for embedding useful information into the student, i.e., decou-pled features from neck and decoupled proposals from clas-siﬁcation head. Extensive experiments on various detectors with different backbones show that the proposed DeFeat is able to surpass the state-of-the-art distillation methods for object detection. For example, DeFeat improves ResNet50 based Faster R-CNN from 37.4% to 40.9% mAP, and im-proves ResNet50 based RetinaNet from 36.5% to 39.7% mAP on COCO benchmark. Code will be released1,2. 1.

Introduction
As one of the fundamental computer vision tasks, ob-ject detection has attracted increasing attention in various real-world applications including autonomous driving and surveillance video analysis. Recent advances of deep learn-ing introduce many convolutional neural network based solutions to object detection. The backbone of a detec-tor is often composed of heavy convolution operations to produce intensive features that is critical to the detection
∗Corresponding author. 1https://github.com/huawei-noah/noah-research/tree/master/DeFeat 2https://www.mindspore.cn/resources/hub
Cor
Sandwich
Loc
Sim Oth BG FN 26.8% 29.7% 7.6% 14.7% 16.8% 4.4%
Cor
Loc
Sim Oth BG FN
Cor
Loc
Sim Oth BG FN 27.3% 27.3% 27.7% 4.7% 27.7% 3.6% 16.1% 18.8% 5.4% 17.1% 16.3% 8.0%
Figure 1. Error analyses of different distillation methods on COCO minival. KD via background regions alleviates the false positive rate and achieves comparable result with KD via object regions.
Cor: correct class (IoU > 0.5). Loc: correct class but misaligned box (0.1 < IoU < 0.5). Sim: wrong class but correct supercate-gory (IoU > 0.1). Oth: wrong class (IoU > 0.1). BG: background false positives (IoU < 0.1). FN: false negatives (remaining errors). accuracy. But doing so inevitably results in a sharp in-crease in the cost of computing resource and an apparent decrease in detection speed. Techniques such as quanti-zation [19, 58, 31, 57, 62], pruning [2, 17, 20], network design [55, 49, 15, 18] and knowledge distillation [56, 6] have been developed to overcome this dilemma and achieve an efﬁcient inference on detection task. We are particu-larly interested in knowledge distillation [24], as it provides an elegant way to learn a compact student network when a performance proven teacher network is available. Clas-sical knowledge distillation methods are ﬁrstly developed for the classiﬁcation task to decide which category the im-age belongs to. The information from soft label outputs
[24, 28, 38, 13] or intermediate features [1, 23, 66] of a well-optimized teacher network have been well exploited to learn the student networks, but these methods cannot be di-rectly extended to the detection task which needs to further
ﬁgure out where the objects are.
There are a few attempts investigating knowledge distil-lation in the object detection task. For example, FGFI [56] asks the student network to imitate the teacher network on 2154
(a) (e) (b) (c) (d) (f) (g)
Figure 2. Left: L2-Norm of the gradient in intermediate neck feature during back propagation, the darkest blue indicates the largest norm value. Images are randomly selected from COCO training set, and object regions are marked with the green box. Right: Average L2-Norm of object and background regions. Avg (train) indicates the average norm of all images from COCO training set. the near object anchor locations. TADF [47] distills the stu-dent via Gaussian masked object regions in neck features and positive samples in detection head. These works only distilled knowledge from object regions, as background re-gions were supposed to be not of interests in the detec-tion task. Intuitively, during the distillation, background re-gions might introduce a large amount of noise and they have rarely been explored. But there lacks a thorough analysis of background regions when conducting the distillation. The hasty decision of throwing away background regions thus might not be wise. Most importantly, background informa-tion has already been proven to be helpful for visual recog-nition [53, 46, 9, 16]. Instead of guessing that background regions are useless or even harmful for distillation, it is time to have a fair and thorough analysis of the background and let the facts speak for themselves.
We ﬁrst examine the roles of object and background regions in knowledge distillation by comparing two ap-proaches: (i) distilling only via object-region FPN features and (ii) distilling only via background-region FPN features.
It was taken for granted that the student would not be en-hanced signiﬁcantly when distilled via the background re-gions from teacher detector, since the background is less in-formative and noisy [56]. However, after extensive experi-ments on various models and datasets, we observe a surpris-ing result that distilling student only via background-region features can also enhance the student remarkably and even, achieve comparable results with that of distillation via ob-ject regions (Figure 4). We further explore where the perfor-mance improvement comes from by distilling background features. Taking two classes from COCO as an example (see Figure 1), we conduct the error analysis [25] and ﬁnd that distillation via background regions effectively reduces the number of background false positives.
The above evidence points to the conclusion that back-ground regions can actually be a complementary to that dis-tillation on object regions. Except that, prior literature has shown that there is a strong relationship between objects and background [53, 69]. The object likelihood [53] can be written as P (O|Vo, Vb)=P (O|Vb) P (Vo|O,Vb) (Vo and Vb are
P (Vo|Vb) features of object region and background, respectively). All probabilities are related to background information which provides an estimate of the likelihood of ﬁnding an object (for example, one is unlikely to ﬁnd a car in the room). The background-based priors vary from different images [69], thus we need to learn background feature for better predic-tion. However, the promising expectation above was failed to be justiﬁed by previous works [6, 30] that take both ob-ject and background regions into account. Although they leveraged both types of regions, the student was not sig-niﬁcantly improved compared to those only using object regions, which seems to agree with the phenomenon indi-cated by [56]. Either the object or background regions can independently beneﬁt the object detection through the dis-tillation, but once they are integrated together, the perfor-mance drops unexpectedly. The reason could be that their methods integrate these two types of regions directly. From the gradient point of view, we illustrate the discordance be-tween object and background regions in Figure 2. Images in the left column are randomly selected from COCO training set, and images in the right column are their corresponding gradients of neck features in student detector. We can ob-serve that the magnitude of gradients from object regions are consistently larger than that from background regions.
This therefore reminds us of different importance of object regions and background regions during the distillation.
Based on these insightful observations, we propose to decouple the features used for knowledge distillation and highlight their unique importance during the distillation.
Two levels of features are included, i.e., FPN features and
RoI-aligned features. The FPN features are split into ob-ject and background parts using the ground-truth mask, and the mean square error loss is applied between teacher and student. The RoI-aligned features are also decoupled into positive and negative parts using teacher’s predicted re-gion proposals. The classiﬁcation logits generated based on these decoupled RoI-aligned features are distilled us-ing the KL divergence loss. The resulting DeFeat algo-rithm can be adaptively incorporated into both one-stage and two-stage detectors to improve the detection accuracy. 2155
To validate our method, we conduct extensive experiments on Faster R-CNN [43] and RetinaNet [34] under various scenarios including distillation on shallow student and nar-row student on two common detection benchmarks PAS-CAL VOC [12] and COCO [35]. In particular, our DeFeat improves ResNet50 based FPN from 37.4% to 40.9% mAP, and ResNet50 based RetinaNet from 36.5% to 39.7% mAP on COCO benchmark. 2.