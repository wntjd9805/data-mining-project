Abstract
We consider the task of 3D pose estimation and tracking of multiple people seen in an arbitrary number of camera feeds. We propose TesseTrack1, a novel top-down approach that simultaneously reasons about multiple individuals’ 3D body joint reconstructions and associations in space and time in a single end-to-end learnable framework. At the core of our approach is a novel spatio-temporal formula-tion that operates in a common voxelized feature space ag-gregated from single- or multiple camera views. After a per-son detection step, a 4D CNN produces short-term person-speciﬁc representations which are then linked across time by a differentiable matcher. The linked descriptions are then merged and deconvolved into 3D poses. This joint spatio-temporal formulation contrasts with previous piece-wise strategies that treat 2D pose estimation, 2D-to-3D lift-ing, and 3D pose tracking as independent sub-problems that are error-prone when solved in isolation. Furthermore, un-like previous methods, TesseTrack is robust to changes in the number of camera views and achieves very good results even if a single view is available at inference time. Quan-titative evaluation of 3D pose reconstruction accuracy on standard benchmarks shows signiﬁcant improvements over the state of the art. Evaluation of multi-person articulated 3D pose tracking in our novel evaluation framework demon-strates the superiority of TesseTrack over strong baselines. 1.

Introduction
This paper addresses the problem of tracking and re-constructing in 3D articulated poses of multiple individ-uals seen in an arbitrary number of camera feeds. This task requires identifying the number of people in the scene, reconstructing their 3D body joints into consistent skele-tons, and associating 3D body joints over time. We do not make any assumption on the number of available cam-era views and focus on real-world scenarios that often in-∗Work done during DR internship at Amazon
†Equal Contribution 1Webpage can be found at http://www.cs.cmu.edu/˜ILIM/ projects/IM/TesseTrack/
Frame 0
Frame 100
Frame 200
Figure 1: We illustrate the output of Tessetrack on the Tagging sequence.
The top two row potray the projections of keypoints on two views, while the bottom row shows the 3D pose tracking. Observe smooth tracking of people in the wild with moving cameras for long duration of time. clude multiple close-by interacting individuals, fast mo-tions, self- and person-person occlusions. A key challenge in such scenarios is that people might strongly overlap and expose only a subset of body joints due to occlusions or truncations by image boundaries (c.f . Fig. 1), which makes it harder to reliably reconstruct and track articulated 3D human poses. Most multi-view strategies rely on multi-stage inference [9, 13, 20, 7, 8, 21, 15, 35] to ﬁrst estimate 2D poses in each frame, cluster same person poses across views, reconstruct 3D poses from clusters based on triangu-lation, and ﬁnally link 3D poses over time [9, 8]. Solving each step in isolation is sub-optimal and prone to errors that cannot be recovered in later stages. This is even more true for monocular methods [4, 26, 33, 25, 42] where solving each step in isolation often represents an ill-posed problem.
We propose TesseTrack, a top-down approach that si-multaneously addresses 3D body joint reconstructions and associations in space and time of multiple persons. At the core of our approach is a novel spatio-temporal formu-lation that operates in a common voxelized feature space obtained by casting per-frame deep learning features from single or multiple views into a discretized 3D voxel vol-ume. First, a 3D CNN is used to localize each person in 115190
Figure 2: The complete pipeline of tessetrack has been illustrated. Initially, the video feed from multiple cameras is passed through shared HRNet to compute the features required for detection and 3D pose tracking. The ﬁnal layer of the HRNet is passed through a 3D convolution to regress to the center of the human 3D bounding boxes. Each of the hypotheses is combined with the HRNet ﬁnal layer to create a spatio-temporal Tube called tesseract. We use a learnable 3D tracking framework for a person association over time using spatio-temporal person descriptors. Finally, the associated descriptors are passed through deconvolution layers to infer the 3D pose. Note that the framework is end-to-end trainable except for the NMS layer in the detection network. the voxel volume. Then, a ﬁxed spatio-temporal volume around each person detection is processed by a 4D CNN to compute short-term person-speciﬁc representations. Over-lapping representations at neighboring time steps are fur-ther scored based on attention aggregation and linked using a differentiable matcher. Finally, 3D body joints of the same person are consistently predicted at each time step based on merged person-speciﬁc representations. Notably, all com-ponents are implemented as layers in a single feed-forward neural network and are thus jointly learned end-to-end.
Our main contribution is a novel spatio-temporal formu-lation that allows simultaneous 3D body joint reconstruc-tion and tracking of multiple individuals. In contrast to the multi-person 3D pose estimation approach of [46] who sim-ilarly aggregate per frame information in 3D voxel space, we address a more challenging problem of multi-person 3D pose tracking and propose end-to-end person-speciﬁc rep-resentation learning. TesseTrack does not make assump-tions on the available number of camera views and performs reasonably well even in the purely monocular setting. Re-markably, using only a single view allows achieving simi-lar MPJPE 3D joint localization error compared to the ﬁve-view setting of [46], while using the same ﬁve-view set-ting results in 2.4× reduction in MPJPE error (c.f . Sec. 4).
In contrast to the multi-person 2D pose tracking method of [49] who rely on short-term spatio-temporal represen-tation learning, our approach operates on the aggregated spatio-temporal voxel volume and provides a richer hypoth-esis comprising of tracked 3D skeletons.
Our second contribution is a novel learnable track-ing formulation that allows extending person-speciﬁc spatio-temporal representation learning to arbitrary-long se-quences. In contrast to [49] who use a heuristic pairwise tracking score based on pose distance and perform match-ing using the Hungarian method, we rely on an attention aggregation layer and a differentiable representation match-ing layer based on the Sinkhorn algorithm. Importantly, we match person-speciﬁc representations instead of the deter-mined body pose tracklets, which allows to learn more ex-pressive representations. In Sec. 4 we demonstrate that the proposed learnable tracking formulation not only improves tracking accuracy but also improves joint localization.
Our third contribution is a novel framework for the eval-uation of multi-person articulated 3D pose tracking. Exper-imental evaluation on the Panoptic dataset [21] shows that
TesseTrack achieves signiﬁcant improvements in per-joint tracking accuracy compared to strong baselines.
Finally, our fourth contribution is an in-depth ablation study of the proposed approach and thorough comparisons to current methods on several standard benchmarks.
In
Sec. 4 we demonstrate that proposed design choices result in signiﬁcant accuracy gains, thereby establishing a new state of the art on multiple datasets. 2.