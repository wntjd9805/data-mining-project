Abstract
Neural network ensembles are gaining popularity by harnessing the complementary wisdom of multiple base models. Ensemble teams with high diversity promote high failure independence, which is effective for boosting the overall ensemble accuracy. This paper provides an in-depth study on how to design and compute ensemble diversity, which can capture the complementary decision capacity of ensemble member models. We make three original contri-butions. First, we revisit the ensemble diversity metrics in the literature and analyze the inherent problems of poor correlation between ensemble diversity and ensemble ac-curacy, which leads to the low quality ensemble selection using such diversity metrics. Second, instead of computing diversity scores for ensemble teams of different sizes using the same criteria, we introduce focal model based ensemble diversity metrics, coined as FQ-diversity metrics. Our new metrics signiﬁcantly improve the intrinsic correlation be-tween high ensemble diversity and high ensemble accuracy.
Third, we introduce a diversity fusion method, coined as the
EQ-diversity metric, by integrating the top three most repre-sentative FQ-diversity metrics. Comprehensive experiments on two benchmark datasets (CIFAR-10 and ImageNet) show that our FQ and EQ diversity metrics are effective for se-lecting high diversity ensemble teams to boost overall en-semble accuracy. 1.

Introduction
Ensemble learning aims to produce a strong model by harnessing the combined and complementary wisdom of multiple base models. There are two broad categories of approaches to construct high quality ensemble teams: (1) data driven or model driven training of multiple models to form an ensemble and (2) selecting ensemble teams from a given pool of diverse base models (learners). The for-mer is represented by boosting algorithms [3, 17], bagging methods [1], and random forests [2]. The latter is repre-sented by ensembles of base models, which are trained us-ing diverse neural network structures and diverse settings of hyperparameters [7, 11, 19, 23, 24, 27], including those pre-trained models in public domains. This paper is dedi-cated to the second category, namely the problem of select-ing high quality ensemble teams from a base model pool.
Given a pool of M diverse base models, there are M ex-ponential number of possible candidate ensemble teams, a large portion of which may not offer high ensemble perfor-mance due to insufﬁcient failure independence among their member models [13, 14, 15, 18, 25]. Ensemble diversity metrics are widely regarded as representative methods for capturing failure independence among member models of ensemble teams and expected to have stable correlation with ensemble accuracy. 1.1.