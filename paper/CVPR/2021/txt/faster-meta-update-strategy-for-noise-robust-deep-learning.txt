Abstract
It has been shown that deep neural networks are prone to overﬁtting on biased training data. Towards address-ing this issue, meta-learning employs a meta model for correcting the training bias. Despite the promising per-formances, super slow training is currently the bottleneck in the meta learning approaches. In this paper, we intro-duce a novel Faster Meta Update Strategy (FaMUS) to re-place the most expensive step in the meta gradient compu-tation with a faster layer-wise approximation. We empir-ically ﬁnd that FaMUS yields not only a reasonably accu-rate but also a low-variance approximation of the meta gra-dient. We conduct extensive experiments to verify the pro-posed method on two tasks. We show our method is able to save two-thirds of the training time while still maintain-ing the comparable or achieving even better generalization performance. In particular, our method achieves the state-of-the-art performance on both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recognition on standard benchmarks. Code are released at https://github.com/youjiangxu/FaMUS. 1.

Introduction
Deep neural networks (DNNs) have achieved impressive results in various computer vision applications such as im-age classiﬁcation [23, 13], object detection [41, 39, 29], and semantic segmentation [12]. A notable issue is that DNNs are prone to memorizing the training data [60, 47], aggra-vating training set bias such as noisy training labels [60] or imbalanced class distributions [11, 64]. This signiﬁ-cantly degrades the generalization capabilities and results in skewed classiﬁers or degenerated feature representations.
Numerous works have been proposed to tackle this issue (e.g. [20, 9, 40, 24, 28]). Among them, meta-learning [40, 43, 51] has recently emerged as an effective framework to mitigate the training data bias. In a nutshell, it employs a meta-model to correct bias by providing a more precise es-timation of the training data. The meta-model is updated by stochastic gradient descent using the meta gradient (or (a) Top-1 Accuracy vs. Training time (b) Cost in One Iteration (c) Gradient Variance
Figure 1: (a) Top-1 accuracy vs. Training time (in hours) on the WebVision dataset [26]. We apply our method on the
MW-Net model [43] and train them using the identical hard-ware platform of four NVIDIA V100 GPUs. (b) The aver-age GPU running time (in seconds) of each step in MW-Net per training iteration. Inception-ResNet V2 is used as the backbone. (c) The meta gradient during the training pro-cess. The solid line denotes the mean and the shaded region show the standard deviation. the high-order gradient) computed on a small proportion of validation data that is assumed available during training1.
Recently, meta-learning approaches such as L2R [40], MW-Net [43], and MLC [51] have shown superior performance on several public benchmarks such as CIFAR [22], WebVi-sion [26], and Clothing1M [54].
Despite the promising empirical results [50, 44], slow training is currently the bottleneck that prevents meta-learning from being applied in many applications. The training time of the meta-learning model is approximately 1The extra validation dataset is not a requirement in meta-learning. As in our experiments, we can use a subset of pseudo-labeled training data as the validation data. In this case, no extra labels or data are used. 144
3∼7 times more than the regular DNN training time. For in-stance, it could take 4 days with 4 NVIDIA V100 GPUs to train MW-Net [43] on a mini subset of WebVision [26, 20] of only ∼50K images.
To understand why the meta-learning approaches are computationally intensive, we may divide the training into three stages: Virtual-Train, Meta-Train, and Actual-Train [51], where each stage consists of a forward and a backward step. Figure 1(b) summarizes the GPU time for each stage using a representative meta-learning model called MW-Net [43]. We ﬁnd more than 80% of the total computation comes from the Meta-Train backward step in which the meta gradient is computed with respect to the loss on the validation data. In this step, the meta gradient is back-propagated through every layer of the network all the way back to the meta-model to update its parameters.
Since the regular training does not have such a step, this overhead cost rapidly becomes signiﬁcant as the number of layers grows in the deep networks.
In this work, we aim at improving the training efﬁciency of meta-learning while maintaining the generalization ca-pability. We propose a new Meta-Train step, named Faster
Meta Update Strategy (FaMUS), to efﬁciently compute the meta gradient. The plausibility of our method relies on the important ﬁnding that the total meta gradient can be reason-ably approximated by the meta gradient accumulated from only a few network layers. As a result, instead of accu-mulating meta gradients from all layers in the Meta-Train step, we design a gradient sampler that is learned to de-cide, whether or not, to aggregate the meta gradient for each layer. When the learnable gradient sampler is turned off, the meta gradient computation is hence circumvented for the corresponding layer. This saves a considerable amount of computation especially when the gradient samplers for lower layers are turned off.
More importantly, we ﬁnd the meta gradient yielded by the FaMUS has lower variance. Figure 1(c) shows the to-tal meta gradient of the ground-truth (blue curve) and the approximation by the FaMUS (red curve).
It shows that our approximation is reasonably close to the mean but has a much lower variance. We hypothesize this is because the
FaMUS learns to select a small number of most informative layers which hence reduces the noisy or redundant signals in the meta gradient. As shown in [35, 34], reduction in gradi-ent variance results in faster and more stable optimization.
We observe similar results in our experiments where our method is able to improve the generalization performance of the recent meta-learning methods on noisy training data.
We conduct extensive experiments to verify the efﬁ-ciency and efﬁcacy of the proposed method. We demon-strate two beneﬁts of our method in overcoming corrupted training labels. First, it speeds up the recent meta-learning methods [40, 43, 51] by at least three times while main-taining the comparable or even better generalization perfor-mance. For example, Figure 1(a) shows a faster and better convergence when we applied our method on the MW-Net model. Second, our method achieves new state-of-the-art performance on multiple benchmarks for both synthetic la-bel noise and realistic label noise, including the challeng-ing CNWL benchmark [18]. The comparison is fair as our meta-model is learned without using any extra data.
In addition, we also validate our method on the long-tailed recognition task. On the long-tailed CIFAR dataset [6], our method yields competitive performance compared to the re-cent strong baseline methods.
The contributions of this paper are three-fold. (1) We propose a new Faster Meta Update Strategy to efﬁciently learn to approximate the meta gradient, which halves two-thirds of the training time of the recent meta-learning meth-ods [40, 43, 51]. (2) We empirically show our approach reduces the variance of the meta gradient and improves the generalization performance of the meta-learning model. (3)
Our method achieves state-of-the-art performance on sev-eral benchmarks with noisy labels. 2.