Abstract
In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks. These two tasks aim at reading and understanding scene text in images for question answering and image caption generation, respec-tively.
In contrast to conventional vision-language pre-training that fails to capture scene text and its relationship with the visual and text modalities, TAP explicitly incorpo-rates scene text (generated from OCR engines) during pre-training. With three pre-training tasks, including masked language modeling (MLM), image-text (contrastive) match-ing (ITM), and relative (spatial) position prediction (RPP), pre-training with scene text effectively helps the model learn a better aligned representation among the three modali-ties: text word, visual object, and scene text. Due to this aligned representation learning, even pre-trained on the same downstream task dataset, TAP already boosts the ab-solute accuracy on the TextVQA dataset by +5.4%, com-pared with a non-TAP baseline. To further improve the per-formance, we build a large-scale scene text-related image-text dataset based on the Conceptual Caption dataset, named OCR-CC, which contains 1.4 million images with scene text. Pre-trained on this OCR-CC dataset, our ap-proach outperforms the state of the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA, +8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps. 1.

Introduction
The Vision-language tasks incorporating scene text [7, 18, 49, 46], e.g., Text-VQA [49, 8, 40, 56] and Text-Caption [46], pose new challenges to vision-language mod-els of reading and understanding scene text in image con-text. Extended from Visual Question Answering (VQA) [6],
Text-VQA aims to answer questions by understanding the
∗This work was done while Z.Yang was an intern at Microsoft.
Figure 1. (a) Text-VQA and Text-Caption tasks aim at reading and understanding scene text in images for question answering and im-age caption generation, respectively. We highlight the scene text-related words in bold. (b) By explicitly incorporating scene text in pre-training, Text-Aware Pre-training (TAP) signiﬁcantly outper-forms both the non-TAP baseline and previous state of the art on multiple tasks (bars shown in red and blue colors, respectively). scene text in the image-question context. Text-Caption seeks to generate an image caption [54, 4] that describes both the visual and scene text information in the image, as shown in Figure 1 (a). These tasks have many potential applications, including robotics [5], document understand-ing [40], assisting visually-impaired people [7, 18], etc.
A typical Text-VQA/Text-Caption framework consists of 1) a feature encoder for each single modality (text word, visual object, and scene text), 2) a multi-modal fusion module, and 3) a decoding module for prediction gener-ation. Previous studies [49, 17, 16, 20, 25, 46, 55] im-prove the model’s performance by designing stronger net-work architectures. Among them, LoRRA [49] added an
OCR attention branch for scene text encoding to a VQA model [24]. M4C [20, 46] proposed a transformer-based multi-modal fusion module [52] and a multi-step multi-choice decoding module. Despite the effective network de-sign, most previous models are optimized with a sole ob-18751
jective directly towards the correct answer/caption. Such a single answer/caption loss tries to predict each word in the ground-truth but is less effective in learning a joint representation among text word, visual object, and scene text. Without a good joint representation, directly optimiz-ing for question-answering/image-captioning could be chal-lenging. Inspired by the success of Vision-Language Pre-training (VLP) [37, 32, 12, 51, 34, 23, 11] in image-text joint representation learning, we leverage the effective Text-VQA/Text-Caption network designs and explore to further improve Text-VQA/Text-Caption by pre-training.
Vision-Language Pre-training (VLP) shows its effective-ness in learning task-agnostic joint representations of image and text. The main idea is to ﬁrst pre-train the model with pre-training tasks on image-caption datasets [45, 29, 54, 41, 43], and then ﬁne-tune the model for a speciﬁc vision-language task [6, 60, 28, 54]. However, conventional VLP methods are designed intuitively for vision-language tasks and do not include scene text in pre-training. Therefore, previous methods fail to capture the scene text modality and its relationship with the visual and text modalities, and are thus less effective in Text-VQA/Text-Caption.
In this study, we propose Text-Aware Pre-training (TAP), which incorporates the scene text modality in pre-training to learn a joint representation of text word, visual object, and scene text. In TAP, we design text-aware pre-training tasks to better fuse scene text (including both scene text words and their visual regions detected by OCR) with the text words and visual objects. For the former, we reﬁne the pre-training tasks in VLP [37, 34] to support the extra scene text input. We ﬁnd it particularly important to include the detected scene text words as extra language inputs. The ex-tra inputs anchor the scene text and language modalities and make the aligned representation learning easier. For the lat-ter, previous studies [25, 55] show that the spatial relation-ships between scene text and object regions are important, e.g., the relationship “left” in Figure 1 (a). Therefore, we propose a “relative (spatial) position prediction” task that learns regions’ spatial relationships by predicting their rela-tive spatial positions in pre-training.
The extra scene text modality, together with the specially designed pre-training tasks, effectively helps the model learn a better aligned representation among the three modal-ities: text word, visual object, and scene text. This aligned representation learning, even pre-trained and ﬁne-tuned on the same downstream task dataset, leads to signiﬁcant im-provement over the non-TAP baseline and helps the TAP model achieve the new state of the art.
To further unleash the power of TAP, we clean and gen-erate a large-scale scene text-related image-caption dataset for pre-training. In general image-caption datasets [45, 29, 54, 41, 43], many image-text pairs contain either no scene text-related visual regions or no scene text-related language referring, and are thus less helpful to Text-VQA/Text-Caption. On the visual side, we run an OCR detector to ﬁl-ter out images with no scene text. On the language side, we include the detected OCR text tokens as the additional cap-tion input to obtain scene text-related language descriptions.
In the end, we build a large-scale dataset named OCR-CC with around 1.4 million scene text-related image-text pairs based on the Conceptual Captioning dataset [45]. By using this large-scale dataset for pre-training, we observe further improvement on the Text-VQA and Text-Caption tasks.
We experiment with the TAP approach on the M4C network architecture [20] and benchmark it on the
TextVQA [49], ST-VQA [8], and TextCaps [46] datasets.
With the identical network architecture and training data,
TAP improves the accuracy on the TextVQA dataset [49] from 44.50% to 49.91%, compared with a non-TAP base-line. Our ﬁnal model ranks No.11 on multiple Text-VQA/Text-Caption challenges, and outperforms previous methods by large margins: TextVQA [49] (+8.3% in abso-lute accuracy), ST-VQA [8] (+8.6% in absolute accuracy), and TextCaps [46] (+10.2 in CIDEr score).
Our main contributions are:
• To the best of our knowledge, we are the ﬁrst to explore pre-training for Text-VQA and Text-Caption.
• By explicitly incorporating scene text with three spe-cially designed pre-training tasks, Text-Aware Pre-training (TAP) effectively learns a better aligned rep-resentation that leads to signiﬁcant performance im-provement on Text-VQA/Text-Caption.
• We build a large-scale dataset named OCR-CC with around 1.4 million scene text-related image-text pairs.
TAP with OCR-CC leads to the new state of the art on multiple tasks: TextVQA [49] (+8.3% in absolute accuracy), ST-VQA [8] (+8.6% in absolute accuracy), and TextCaps [46] (+10.2 in CIDEr score). We will release the dataset and the models. 2.