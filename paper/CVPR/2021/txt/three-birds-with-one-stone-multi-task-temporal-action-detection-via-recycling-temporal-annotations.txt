Abstract
Temporal action detection on unconstrained videos has seen signiﬁcant research progress in recent years. Deep learning has achieved enormous success in this direction.
However, collecting large-scale temporal detection datasets to ensuring promising performance in the real-world is a la-borious, impractical and time consuming process. Accord-ingly, we present a novel improved temporal action local-ization model that is better able to take advantage of limited labeled data available. Speciﬁcally, we design two auxil-iary tasks by reconstructing the available label information and then facilitate the learning of the temporal action de-tection model. Each task generates their supervision sig-nal by recycling the original annotations, and are jointly trained with the temporal action detection model in a multi-task learning fashion. Note that the proposed approach can be pluggable to any region proposal based temporal ac-tion detection models. We conduct extensive experiments on three benchmark datasets, namely THUMOS’14 [15], Cha-rades [35] and ActivityNet [14]. Our experimental results conﬁrm the effectiveness of the proposed model. 1.

Introduction
Video collections have been proliferating with the ad-vance of devices with video recording capabilities. Most of these videos are untrimmed and only a small part contains events of interest, while the major part is background. In continuous videos, temporal action detection (TAD) refers to the task of simultaneously recognizing actions and pre-cisely localizing them in time. Due to its apparent com-plexity and enormous usefulness in real-world applications including video surveillance, video summarization and skill assessment, TAD has drawn attention from researchers in the machine learning and computer vision communi-ties [41, 32]. When sufﬁcient labeled training data ex-ists, deep convolutional neural networks can achieve re-markable performance [18, 11, 48]. However, it is costly, time-consuming and tedious to acquire a large amount of segmentation-level label information in continuous videos for real-world applications.
Researchers have explored different ways to address the problem of labeled training data shortages for deep learn-ing approaches. Among these approaches, multi-task learn-ing (MTL) [25, 28, 4] is one the most representative exam-ples. MTL mitigates the label shortage problem by training multiple relevant tasks at the same time [25, 17]. Its goal is to jointly train multiple relevant tasks with limited su-pervision information in order to improve the performance of each task [25].
Its effectiveness has been widely ex-plored. As the number of related tasks increases, MTL is able to decrease the upper bound of the amount of labeled training data required, thereby allowing better generaliza-tion. MTL approaches can be broadly grouped into two categories. The ﬁrst category aims to maximise task-wise performance by optimising the structures of weight shar-ing, while the second focuses on weight clustering based on task-similarity. Both of these approaches have been widely applied in the ﬁeld of computer vision tasks including per-son re-identiﬁcation [36], depth estimation and scene pars-ing [40, 44], etc. For example, we can employ Mask R-CNN [12] to improve the performance of object detection by jointly training an instance segmentation task. However, due to the expensive cost of segmentation mask labels, this approach is of limited practical beneﬁt.
In this paper, we propose a novel temporal action de-tection framework that leverages the beneﬁts of multi-task learning. More speciﬁcally, we build the proposed model from a widely used supervised temporal action detection framework [41], where a temporal proposal based detector is provided along with segmentation label information. Us-ing the limited supervision information provided, we con-struct two auxiliary tasks (e.g. multi-action classiﬁcation and localisation conﬁdence estimation), which are used to improve the performance of temporal action detection in a multi-task learning fashion. These two tasks generate their 4751
own supervision information by recycling the given lim-ited temporal annotations. Different from the principles of traditional multi-task learning, we are here only concerned with the performance of the main temporal action detection task. We generate the ground truth information for these auxiliary tasks by exploring the temporal segmentation in-formation provided, after which we jointly train the tem-poral action detection with these auxiliary tasks. To evalu-ate the performance of the proposed approach, we conduct extensive experiments on several publicly available bench-mark datasets, including THUMOS’14 [15], Charades [35] and ActivityNet [14]. The experimental results conﬁrm that auxiliary tasks contribute to the improvement of temporal action detection.
To summarize, we make the following contributions:
• To mitigate the label shortage problem of temporal ac-tion detection, we propose a novel multi-task temporal action detection algorithm via reusing temporal annota-tions. The proposed approach can be applied to any re-gion proposal based temporal action detection models.
• We construct two auxiliary tasks by recycling the tempo-ral segmentation information, thereby improving the per-formance of temporal action localisation in a multi-task learning fashion.
• To demonstrate the performance improvement achieved by the proposed method, we conduct extensive experi-ments on three benchmark datasets: THUMOS’14 [15],
Charades [35] and ActivityNet [14]. The experimental results conﬁrm the effectiveness of the proposed method. 2.