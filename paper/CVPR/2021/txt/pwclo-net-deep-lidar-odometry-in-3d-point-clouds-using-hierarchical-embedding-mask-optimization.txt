Abstract
Input  Point Cloud PC1
Input  Point Cloud PC2
A novel 3D point cloud learning model for deep LiDAR odometry, named PWCLO-Net, using hierarchical embed-ding mask optimization is proposed in this paper. In this model, the Pyramid, Warping, and Cost volume (PWC) struc-ture for the LiDAR odometry task is built to reﬁne the esti-mated pose in a coarse-to-ﬁne approach hierarchically. An attentive cost volume is built to associate two point clouds and obtain embedding motion patterns. Then, a novel train-able embedding mask is proposed to weigh the local mo-tion patterns of all points to regress the overall pose and
ﬁlter outlier points. The estimated current pose is used to warp the ﬁrst point cloud to bridge the distance to the second point cloud, and then the cost volume of the resid-ual motion is built. At the same time, the embedding mask is optimized hierarchically from coarse to ﬁne to obtain more accurate ﬁltering information for pose reﬁnement. The trainable pose warp-reﬁnement process is iteratively used to make the pose estimation more robust for outliers. The superior performance and effectiveness of our LiDAR odom-etry model are demonstrated on KITTI odometry dataset.
Our method outperforms all recent learning-based meth-ods and outperforms the geometry-based approach, LOAM with mapping optimization, on most sequences of KITTI odometry dataset. Our source codes will be released on https://github.com/IRMVLab/PWCLONet. 1.

Introduction
The visual/LiDAR odometry is one of the key technolo-gies in autonomous driving. This task uses two consec-utive images or point clouds to obtain the relative pose transformation between two frames, and acts as the base of the subsequential planning and decision making of mobile robots [13]. Recently, learning-based odometry methods have shown impressive accuracy on datasets compared with conventional methods based on hand-crafted features. It
*Corresponding Author. The ﬁrst three authors contributed equally.
Feature 
Pyramid
Attentive cost-volume  
Feature 
Pyramid
Sampled points in PC1
Initial Embedding Mask
Initial Pose
Pose
Iterative Pose Warp-Refinement
-t e
N
O
L
C
W
P
Sampled points in PC1
Final Embedding Mask
Final Pose
Pose
Weight
Output Pose
Low
High
Figure 1. The Point Feature Pyramid, Pose Warping, and Attentive
Cost Volume (PWC) structure in our proposed PWCLO-Net. The pose is reﬁned layer by layer through iterative pose warp-reﬁnement.
The whole process is realized end-to-end by making all modules differentiable. In the LiDAR point clouds, the small red points are the whole point cloud. The big black points are the sampled points in P C1. Distinctive colors of big points in embedding mask measure the contribution of sampled points to the pose estimation. is found that learning-based methods can deal with sparse features and dynamic environments [9, 14], which are usu-ally difﬁcult for conventional methods. To our knowledge, most learning-based methods are on the 2D visual odometry
[26, 33, 15, 29, 23, 25] or utilize 2D projection of LiDAR
[16, 22, 27, 10, 11] and the LiDAR odometry in 3D point clouds is underexplored. This paper aims to estimate the
LiDAR odometry directly through raw 3D point clouds.
For the LiDAR odometry in 3D point clouds, there are three challenges: 1) As the discrete LiDAR point data are ob-tained in two consecutive frames separately, it is hard to ﬁnd 15910
an precisely corresponding point pair between two frames; 2) Some points belonging to an object in a frame may not be seen in the other view if they are occluded by other ob-jects or are not captured because of the limitation of LiDAR resolution; 3) Some points belonging to dynamic objects are not suitable to be used for the pose estimation because these points have uncertainty motions of the dynamic objects.
For the ﬁrst challenge, Zheng et al. [32] use matched keypoint pairs judged in 2D depth images. However, the correspondence is rough because of the discrete perception of LiDAR. The cost volume for 3D point cloud [28, 24] is adopted in this paper to obtain a weighted soft correspon-dence between two consecutive frames. For the second and third challenges, the mismatched points or dynamic ob-jects, which do not conform to the overall pose, need to be ﬁltered. LO-Net [10] trains an extra mask estimation network [33, 30] by weighting the consistency error of the normal of 3D points. In our network, an internal trainable embedding mask is proposed to weigh local motion patterns from the cost volume to regress the overall pose. In this way, the mask can be optimized for more accurate pose esti-mation rather than depending on geometry correspondence.
In addition, the PWC structure is built to capture the large motion in the layer of sparse points and reﬁne the estimated pose in dense layers. As shown in Fig. 1, the embedding mask is also optimized hierarchically to obtain more accurate
ﬁltering information to reﬁne pose estimation.
Overall, our contributions are as follows:
• The Point Feature Pyramid, Pose Warping, and Cost
Volume (PWC) structure for the 3D LiDAR odometry task is built to capture the large motion between two frames and accomplish the trainable iterative 3D feature matching and pose regression.
• In this structure, the hierarchical embedding mask is proposed to ﬁlter mismatched points and convert the cost volume embedded in points to the overall ego-motion in each reﬁnement level. Meanwhile, the em-bedding mask is optimized and reﬁned hierarchically to obtain more accurate ﬁltering information for pose reﬁnement following the density of points.
• Based on the characteristic of the pose transformation, the pose warping and pose reﬁnement are proposed to reﬁne the estimated pose layer by layer iteratively.
A totally end-to-end framework, named PWCLO-Net, is established, in which all modules are fully differen-tiable so that each process is no longer independent and combinedly optimized.
• Finally, our method is demonstrated on KITTI odom-etry dataset [8, 7]. The evaluation experiments and ablation studies show the superiority of the proposed method and the effectiveness of each design. To the best of our knowledge, our method outperforms all recent learning-based LiDAR odometry and even outperforms the geometry-based LOAM with mapping optimization
[31] on most sequences. 2.