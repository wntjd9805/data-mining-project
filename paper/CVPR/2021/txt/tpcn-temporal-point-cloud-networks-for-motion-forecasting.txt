Abstract
Spatial Module
We propose the Temporal Point Cloud Networks (TPCN), a novel and ﬂexible framework with joint spatial and tem-poral learning for trajectory prediction. Unlike existing ap-proaches that rasterize agents and map information as 2D images or operate in a graph representation, our approach extends ideas from point cloud learning with dynamic tem-poral learning to capture both spatial and temporal infor-mation by splitting trajectory prediction into both spatial and temporal dimensions. In the spatial dimension, agents can be viewed as an unordered point set, and thus it is straightforward to apply point cloud learning techniques to model agents’ locations. While the spatial dimension does not take kinematic and motion information into ac-count, we further propose dynamic temporal learning to model agents’ motion over time. Experiments on the Argov-erse motion forecasting benchmark show that our approach achieves state-of-the-art results. 1.

Introduction
Motion forecasting in autonomous driving concerns fu-ture trajectories of agents, including vehicles and pedestri-ans. For a self-driving car, the predicted future trajectories of surrounding trafﬁc participants serve as key information to plan its future trajectories. A self-driving car should be able to predict the distribution or a few possible future tra-jectories of each agent as the future is full of uncertainty, given the relevant sensor input information in the past.
Traditional methods for motion forecasting [15, 28, 34, 39] are based on kinematic constraints and road map infor-mation with handcrafted rules. Though these approaches are sufﬁcient in many simple situations, they fail to capture the rich behavior strategies and interaction in complex ur-ban scenarios. Great progress has been made to explore the power of data-driven methods in motion forecasting with deep learning [2, 4, 6, 7, 25]. These methods encode the agents (e.g., vehicles, pedestrians, and cyclists) and high-deﬁnition map (HD map) information by rasterizing the cor-responding elements (lanes, crosswalks) as lines and poly-‡Part of the work was done during an internship at DEEPROUTE.AI.
Temporal Module
Predicted trajectories
Input (agents + map)
Figure 1. A high-level illustration of our approach. Red points represent the agent history trajectory points, while blue points are discrete map lane points. We use a spatial module based on point cloud learning to extract geometric features and a temporal mod-ule to extract sequential features. Both modules utilize the output of the other module and propagate mutually to output future tra-jectory points. gons with different colors. A standard image backbone net-work [12, 31] is then applied to the rasterized image to ex-tract the map and agent features and perform motion pre-diction.
However, the rasterized image is an overly complex rep-resentation for environment and agent history and requires signiﬁcantly more computation and data to train and deploy.
More succinct representations have been explored to avoid this heavy process. VectorNet [10] proposes a vector rep-resentation to exploit the spatial locality of individual road components with graph neural networks. LaneConv [19] constructs a lane graph from vectorized map data and pro-poses LaneGCN to capture the topology and long depen-dency of the agents and map information. Both Vector-Net [10] and LaneConv [19] can be viewed as extensions of graph neural network in prediction with strong capability to extract spatial locality. However, both works fail to fully utilize the temporal information of agents with less focus on temporal feature extraction.
In this work, we extend ideas from 3D point cloud learn-ing to the motion forecasting task. Previous works on point cloud networks focus on spatial points. We extend the met-11318
ric space to the joint spatial-temporal space and represent the agents’ history observations and map data as points in this space. Since the raw input data of prediction is a set of points that contain different agents with historical ob-servations and map data, spatial and temporal learning will be two key components in prediction learning. Ignoring ei-ther information will lead to information loss and reduce the model’s capability of context learning.
In order to combine spatial and temporal learning in a ﬂexible and uniﬁed framework, we propose Temporal
Point Cloud Networks (TPCN). Compared with GCN based methods [10, 19], our TPCN does not manually specify the interaction structures (e.g., connectivity in the graph) and avoid the complex correlation learning process. TPCN models the prediction learning task as joint learning be-tween a spatial module and a temporal module. In the spa-tial module, note that the waypoints and map points have very similar properties as point clouds, both being sparse and permutation invariant, and have a strong geometric cor-relation. Thus, point cloud learning strategies can be ef-fective for spatial feature extractions.
Instead of directly applying works [26, 27, 33, 36] whose computation cost is high, we propose our novel spatial learning layer, namely
Dual-representation Spatial Learning to obtain pointwise and voxelwise features through point cloud learning. Mean-while, we propose Dynamic Temporal Learning in the temporal module to effectively extract the time-series infor-mation and motion estimation. Compared with traditional
Hard Temporal Learning [1, 18, 19], our dynamic tempo-ral learning layer naturally handles variant time lengths of different agents in the same sequence without the need to pad the history. By switching between the two modules, the spatial features and temporal features from these two modules are propagated mutually, each module taking the features of the other module as input. As such, spatial learn-ing will utilize the temporal information (e.g., motion state), while the temporal learning will have some spatial guidance (e.g., map information), namely Joint Learning. Fig. 1 il-lustrates the overall architecture of our approach. Note that we model the selection of multi-modal trajectories problem as displacement regression rather than classiﬁcation.
Our contributions are summarized as follows:
• We propose a novel and ﬂexible architecture for pre-diction learning, which models the complex pro-cess as joint spatial and temporal learning. Dual-representation Spatial Learning for feature extraction of waypoints and map data is proposed as the spatial module. Meanwhile, we propose novel Dynamic Tem-poral Learning, consisting of Multi-interval Learning and Instance Pooling.
• We propose displacement prediction for multi-modal trajectories selection, which alleviates the hard assign-ment in classiﬁcation through regression.
• Extensive experiments are conducted on the large-scale Argoverse motion forecasting benchmark to show the effectiveness of our approach. 2.