Abstract
Action segmentation refers to inferring boundaries of se-mantically consistent visual concepts in videos and is an important requirement for many video understanding tasks.
For this and other video understanding tasks, supervised approaches have achieved encouraging performance but re-quire a high volume of detailed frame-level annotations.
We present a fully automatic and unsupervised approach for segmenting actions in a video that does not require any training. Our proposal is an effective temporally-weighted hierarchical clustering algorithm that can group semanti-cally consistent frames of the video. Our main ﬁnding is that representing a video with a 1-nearest neighbor graph by taking into account the time progression is sufﬁcient to form semantically and temporally consistent clusters of frames where each cluster may represent some action in the video. Additionally, we establish strong unsupervised base-lines for action segmentation and show signiﬁcant perfor-mance improvements over published unsupervised methods on ﬁve challenging action segmentation datasets. Our code is available.1 1.

Introduction
Human behaviour understanding in videos has tradi-tionally been addressed by inferring high-level semantics such as activity recognition [12, 3]. Such works are of-ten limited to tightly clipped video sequences to reduce the level of labelling ambiguity and thus make the prob-lem more tractable. However, a more ﬁne-grained under-standing of video content, including for un-curated content that may be untrimmed and therefore contain a lot of ma-terial unrelated to human activities, would be beneﬁcial for many downstream video understanding applications. Con-sequently, the less-constrained problem of action segmen-tation in untrimmed videos has received increasing atten-tion. Action segmentation refers to labelling each frame of 1https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH a video with an action, where the sequence of actions is usually performed by a human engaged in a high-level ac-tivity such as making coffee (illustrated in Figure 1). Action segmentation is more challenging than activity recognition of trimmed videos for several reasons, including the pres-ence of background frames that don’t depict actions of rel-evance to the high-level activity. A major challenge is the need for signiﬁcantly more detailed annotations for super-vising learning-based approaches. For this reason, weakly-and unsupervised approaches to action segmentation have gained popularity [34, 27, 18, 32]. Some approaches have relied on natural language text extracted from accompany-ing audio to provide frame-based action labels for training action segmentation models [2]. This of course makes the strong assumption that audio and video frames are well-aligned. Other approaches assume some a priori knowl-edge of the actions, such as the high-level activity label or the list of actions depicted, in each video [11, 34]. Even this level of annotation however, requires signiﬁcant anno-tation effort for each training video as not all activities are performed using the same constituent actions.
Most weakly- and unsupervised methods, whatever their degree of a priori knowledge, focus on acquiring pseudo-labels that can be used to supervise training of task-speciﬁc feature embeddings [22, 32, 9, 26, 28, 34]. As pseudo-labels are often quite noisy, their use may hamper the efﬁ-cacy of the learned embeddings. In this work, we adopt the view that action segmentation is fundamentally a grouping problem, and instead focus on developing clustering meth-ods that effectively delineate the temporal boundaries be-tween actions. This approach leads to an illuminating ﬁnd-ing: for action segmentation, a simple clustering (e.g., with
Kmeans) of appearance-based frame features achieves per-formance on par with, and in some cases superior to, SoTA weakly-supervised and unsupervised methods that require training on the target video data (please refer to section 3 for details). This ﬁnding indicates that a sufﬁciently discrimi-native visual representation of video frames can be used to group frames into visually coherent clusters. However, for action segmentation, temporal coherence is also critically 11225
Figure 1. Segmentation output example from Breakfast Dataset [14]: P46 webcam02 P46 milk. Colors indicate different actions in chrono-logical order: ϕ, take cup, spoon powder, pour milk, stir milk, ϕ, where ϕ is background shown in while color. important. Building on these insights, we adapt a hierarchi-cal graph-based clustering algorithm to the task of temporal video segmentation by modulating appearance-based graph edges between frames by their temporal distances. The re-sulting spatio-temporal graph captures both visually- and temporally-consistent neighbourhoods of frames that can be effectively extracted. Our work makes the following main contributions:
• We establish strong appearance-based clustering base-lines for unsupervised action segmentation that outper-form SoTA models;
• We propose to use temporally-modulated appearance-based graphs to represent untrimmed videos;
• We combine this representation with a hierarchical graph-based clustering algorithm in order to perform temporal action segmentation.
Our proposed method outperforms our strong baselines and existing SOTA unsupervised methods by a signiﬁcant mar-gin on 5 varied and challenging benchmark datasets. 2.