Abstract
Understanding the nutritional content of food from vi-sual data is a challenging computer vision problem, with the potential to have a positive and widespread impact on public health. Studies in this area are limited to existing datasets in the ﬁeld that lack sufﬁcient diversity or labels required for training models with nutritional understand-ing capability. We introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes with corresponding video streams, depth images, component weights, and high accuracy nutritional content annotation. We demonstrate the potential of this dataset by training a computer vision algorithm capable of predicting the caloric and macronu-trient values of a complex, real world dish at an ac-curacy that outperforms professional nutritionists. Fur-ther we present a baseline for incorporating depth sensor data to improve nutrition predictions. We release Nutri-tion5k in the hope that it will accelerate innovation in the space of nutritional understanding. The dataset is avail-able at https : / / github . com / google - research -datasets/Nutrition5k. 1.

Introduction
The nutritional composition of a person’s diet is inextri-cably linked to health, happiness, and longevity. Making it easier to understand and track the nutritional breakdown of the food we eat enables us to make better dietary choices and potentially live longer and healthier lives. Despite the considerable impact that what we eat has on us, the tools for trying to understand food are extremely cumbersome and limited.
Currently, the main approach for an individual who wants to record the nutritional content of their food intake is to utilize a tracking app such as MyFitnessPal. These types
∗Work done while Wade Norris was at Google.
Figure 1. tion5k.
Example representation of data contained in Nutri-of apps enable a user to set intake goals for total calories, as well as for speciﬁc macronutrients (carbohydrate, pro-tein, and fat), and provide an interface to log and aggregate meals to see if these goals are met. When doing this type of tracking, each ingredient or dish consumed must be individ-ually logged along with the exact portion size. These logs can then be automatically converted to nutritional content using average per portion values saved in a database. Many apps require a scale to individually weigh and log each in-gredient the user eats, but this can be a tedious and time 8903
consuming process. Short of using a scale, users can also at-tempt to perform visual portion size estimation themselves, but this can be highly error-prone step [10, 18]. MyFitness-Pal boasted over 19M monthly unique active users in early 2018 [20] despite these difﬁculties in data entry. The ability to streamline food logging with a camera would not only make this process easier for current active users, but it may also unlock another large demographic of potential users.
As the impact of nutrition on our health grows more ap-parent, more progress has been made in making this data more accessible. Many chain restaurants now post the nutri-tional content of menu items online and in stores. A variety of promising techniques have leveraged this publicly avail-able source of data to show the potential for classiﬁcation models to help with food logging. Such approaches demon-strate a dramatic reduction in effort when logging meals by enabling a user to take a photo of their dish rather than man-ually entering nutritional content. However, this requires that the dish belong to a set of known menu items with pre-sumably ﬁxed portion sizes and nutritional content. Fre-quently, though, our meals are not on a list of known dishes and current capabilities in this space are extremely limited.
We use the term generic food for dishes that are not nec-essarily from a pre-determined set with known nutritional content. Understanding the content of generic food requires the ability to estimate portion sizes in addition to recogniz-ing the ingredients present, making it a signiﬁcantly more difﬁcult problem than classiﬁcation alone.
One of the largest challenges to furthering progress in this space is data collection. For many computer vision problems, researchers are able to leverage the abundance of data readily available on the internet to train their models.
Unfortunately for the nutritional understanding space, data on the internet is sparse and often inaccurate. Images from many sources are often plated and shot with an artistic in-tention to increase appeal rather than represent realism. The few large scale and diverse datasets, such as Recipe1M [12], are mined from recipe websites. While these contain valu-able dish level, ingredient level, and preparation attribute annotations, they almost always lack annotations for the portion sizes shown in the photos. Without accurate portion size annotations, learning to predict the nutritional content from the associated images is difﬁcult and error prone. A potential solution for building a nutrition dataset could be to send web-mined images to human annotators and have them estimate the portion size. However, our ﬁndings show that this annotation task is extremely difﬁcult, even for nu-trition experts, and produces highly inaccurate labels.
This paper explores an alternative approach to dataset construction: incrementally weighing, scanning, and log-ging each item as it’s added to a plate in real world cafete-rias immediately before consumption. A variety of weight and imagery sensors are used for the scanning process and the ingredient breakdown is logged via the item recipe, en-abling the calculation of near exact nutrition annotations.
The end result is Nutrition5k, a dataset of ﬁve thousand unique, real world, generic food dishes and their associated video captures, depth images, component weights, and high accuracy nutritional information. We demonstrate the util-ity of this dataset by training deep CNNs to predict nutri-tional content from a single RGB image, achieving an accu-racy that even surpasses a trained nutritionist’s ability to do so visually. Additionally, we make use of depth sensor data as an additional signal to greatly improve our portion size predictions and nutrition regression accuracy. 2.