Abstract
Feature-based student-teacher learning, a training method that encourages the student’s hidden features to mimic those of the teacher network, is empirically success-ful in transferring the knowledge from a pre-trained teacher network to the student network. Furthermore, recent em-pirical results demonstrate that, the teacher’s features can boost the student network’s generalization even when the student’s input sample is corrupted by noise. However, there is a lack of theoretical insights into why and when this method of transferring knowledge can be successful be-tween such heterogeneous tasks. We analyze this method theoretically using deep linear networks, and experimen-tally using nonlinear networks. We identify three vital fac-tors to the success of the method: (1) whether the student is trained to zero training loss; (2) how knowledgeable the teacher is on the clean-input problem; (3) how the teacher decomposes its knowledge in its hidden features. Lack of proper control in any of the three factors leads to failure of the student-teacher learning method. 1.

Introduction 1.1. What is student teacher learning?
Student-teacher learning is a form of supervised learning that uses a well-trained teacher network to train a student network for various low-level and high-level vision tasks.
Inspired by the knowledge distillation work of Hinton et al.
[14], Romero et al. [24] started a major line of experimen-tal work demonstrating the utility of feature-based student-teacher training [1, 7, 9, 13, 15, 17, 19, 20, 26–28, 31–33].
Figure 1 shows an illustration of the scheme. Suppose that we want to perform classiﬁcation (or regression) where the input image is corrupted by noise. In student-teacher learning, the teacher is a model trained to classify clean images. We assume that the teacher’s prediction quality is acceptable, and the features extracted by the teacher are meaningful. However, the teacher cannot handle noisy im-ages because it has never seen one before. Student-teacher
Figure 1. The student-teacher training loss, computed by measur-ing the difference between the hidden features of the student and the teacher networks. During training, the input signal to the stu-dent network is the noisy version of the teacher’s. learning says that, given a pair of clean-noisy input, we can train a student by forcing the noisy features extracted by the student to be similar to those clean features extracted by the teacher, via a loss term known as the student-teacher loss. In some sense, the training scheme forces the student network to adjust its weights so that the features are “de-noised”. During testing, we drop the teacher and use the student for inference.
The success of student-teacher learning from clean in-puts to corrupted inputs has been demonstrated in recent pa-pers, including classiﬁcation with noisy input [9], low-light denoising [7], and image dehazing [15]. However, on the theory side, there is very little analysis of why and when the hidden features of the teacher can boost the generalization power of the student. Most of the explanations in the exper-imental papers boil down to stating that the hidden features contain rich and abstract information about the task which the teacher solves, which could be difﬁcult for the student network to discover on its own.
In this paper, we provide the ﬁrst insights into the mech-anism of feature-based student teacher learning from clean inputs to noisy inputs, for classiﬁcation and regression 112075
tasks. The questions we ask are: When will student-teacher learning succeed? When will it fail? What are the con-tributing factors? What is the generalization capability of the student?
The main results of our theoretical and experimental
ﬁndings can be summarized in the three points below:
• The student should not be trained to zero training loss.
• A knowledgeable teacher is generally preferred, but there are limitations.
• Well-decomposed knowledge leads to better knowl-edge transfer.
To verify these ﬁndings, we prove several theoretical re-sults, including showing how missing one or more of those can lead to failure, by studying deep linear networks. We experimentally verify these ﬁndings by studying wide non-linear networks. 1.2.