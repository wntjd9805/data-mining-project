Abstract
Human behavior understanding with unmanned aerial vehicles (UAVs) is of great signiﬁcance for a wide range of applications, which simultaneously brings an urgent de-mand of large, challenging, and comprehensive benchmarks for the development and evaluation of UAV-based models.
However, existing benchmarks have limitations in terms of the amount of captured data, types of data modalities, cat-egories of provided tasks, and diversities of subjects and environments. Here we propose a new benchmark - UAV-Human - for human behavior understanding with UAVs, which contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identiﬁcation, and 22,263 frames for attribute recogni-tion. Our dataset was collected by a ﬂying UAV in mul-tiple urban and rural districts in both daytime and night-time over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlu-sions, camera motions, and UAV ﬂying attitudes. Such a comprehensive and challenging benchmark shall be able to promote the research of UAV-based human behavior un-derstanding, including action recognition, pose estimation, re-identiﬁcation, and attribute recognition. Furthermore, we propose a ﬁsheye-based action recognition method that mitigates the distortions in ﬁsheye videos via learning un-bounded transformations guided by ﬂat RGB videos. Exper-iments show the efﬁcacy of our method on the UAV-Human dataset. 1.

Introduction
Given the ﬂexibility and capability of long-range track-ing, unmanned aerial vehicles (UAVs) equipped with cam-eras are often used to collect information from remote for
*Corresponding Authors the scenarios where it is either impossible or not sensible to use ground cameras [24, 35, 22]. One particular area where
UAVs are often deployed is human behavior understanding and surveillance in the wild, where video sequences of hu-man subjects can be collected for analysis (such as action recognition, pose estimation, human re-identiﬁcation, and attribute analysis), and for subsequent decision making.
Compared to videos collected by common ground cam-eras, the video sequences captured by UAVs generally present more diversiﬁed yet unique viewpoints, more ob-vious motion blurs, and more varying resolutions of the subjects, owing to the fast motion and continuously chang-ing attitudes and heights of the UAVs during ﬂight. These factors lead to signiﬁcant challenges in UAV-based human behavior understanding, clearly requiring the design and development of human behavior understanding methods speciﬁcally taking the unique characteristics of UAV appli-cation scenarios into consideration.
Existing works [8] have demonstrated the great impor-tance of leveraging large, comprehensive, and challeng-ing benchmarks to develop and evaluate the state-of-the-art deep learning methods for handling various computer vi-sion tasks. However, in the UAV-based human behavior un-derstanding area, existing datasets [3, 26] have limitations in multiple aspects, including: (1) Very limited number of samples, while a large scale of the dataset is often important for mitigating over-ﬁtting issues and enhancing the general-ization capability of the models developed on it. (2) Lim-ited number and limited diversity of subjects, while the di-versities of human ages, genders, and clothing are crucial for developing robust models for analyzing the behaviors of various subjects. (3) Constrained capturing conditions. In practical application scenarios, UAVs often need to work in various regions (e.g., urban, rural, and even mountain and river areas), under diversiﬁed weathers (e.g., windy and rainy weathers), in different time periods (e.g, daytime and nighttime). However, samples in existing datasets are usu-16266
ally collected under similar conditions, obviously simplify-ing the challenges in real-world UAV application scenarios. (4) Limited UAV viewpoints and ﬂying attitudes. UAVs can experience frequent (and sometimes abrupt) position shifts during ﬂying, which not only cause obvious viewpoint vari-ations and motion blurs, but also lead to signiﬁcant resolu-tion changes. However, the UAVs in most of the existing datasets only present slow and slight motions with limited
ﬂying attitude variations. (5) Limited types of data modal-ities. In practical scenarios, we often need to deploy dif-ferent types of sensors to collect data under different condi-tions. For example, infrared (IR) sensors can be deployed on UAVs for human search and rescue in the nighttime, while ﬁsheye cameras are often used to capture a broad area.
This indicates the signiﬁcance of collecting different data modalities, to facilitate the development of various models for analyzing human behaviors under different conditions.
However, most of the existing UAV datasets provide con-ventional RGB video samples only. (6) Limited categories of provided tasks and annotations. As for UAV-based hu-man behavior understanding, various tasks, such as action recognition, pose estimation, re-identiﬁcation (ID), and at-tribute analysis, are all of great signiﬁcance, which indicates the importance of providing thorough annotations of vari-ous tasks for a comprehensive behavior analysis. However, most of the existing datasets provide annotations for one or two tasks only.
The aforementioned limitations in existing datasets clearly show the demand of a larger, more challenging, and more comprehensive dataset for human behavior analysis with UAVs. Motivated by this, in this work, we create UAV-Human, the ﬁrst large-scale multi-modal benchmark in this domain. To construct this benchmark, we collect samples by ﬂying UAVs equipped with multiple sensors in both day-time and nighttime, over three different months, and across multiple rural districts and cities, which thus brings a large number of video samples covering extensive diversities w.r.t human subjects, data modalities, capturing environments, and UAV ﬂying attitudes and speeds, etc.
Speciﬁcally, a total of 22,476×3 video sequences (con-sisting of three sensors: Azure DK, ﬁsheye camera and night-vision camera) with 119 distinct subjects and 155 different activity categories are collected for action recog-nition; 22,476 frames with 17 major keypoints are anno-tated for pose estimation; 41,290 frames with 1,144 identi-ties are collected for person re-identiﬁcation; and 22,263 frames with 7 distinct characteristics are labelled for at-tribute recognition, where the captured subjects present a wide range of ages (from 7 to 70) and clothing styles (from summer dressing to fall dressing). Meanwhile the captur-ing environments contain diverse scenes (45 sites, includ-ing forests, riversides, mountains, farmlands, streets, gyms, and buildings), different weather conditions (sunny, cloudy, rainy, and windy), and various illumination conditions (dark and bright). Besides, different types of UAV ﬂying atti-tudes, speeds, and trajectories are adopted to collect data, and thus our dataset covers very diversiﬁed yet practical viewpoints and camera motions in UAV application sce-narios. Furthermore, the equipped different sensors enable our dataset to provide rich data modalities including RGB, depth, IR, ﬁsheye, night-vision, and skeleton sequences.
Besides introducing the UAV-Human dataset, in this pa-per, we also propose a method for action recognition in ﬁsh-eye UAV videos. Thanks to the wide angle of view, ﬁsh-eye cameras can capture a large area in one shot and thus are often deployed on UAVs for surveillance. However, the provided wide angle in turn brings large distortions into the collected videos, making ﬁsheye-based action recogni-tion quite challenging. To mitigate this problem, we design a Guided Transformer I3D model to learn an unbounded transformation for ﬁsheye videos under the guidance of ﬂat
RGB images. Experimental results show that such a design is able to boost the performance of action recognition using
ﬁsheye cameras. 2.