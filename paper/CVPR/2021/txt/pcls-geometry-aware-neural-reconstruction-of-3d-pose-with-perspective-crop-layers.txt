Abstract
Local processing is an essential feature of CNNs and other neural network architectures—it is one of the rea-sons why they work so well on images where relevant in-formation is, to a large extent, local. However, perspec-tive effects stemming from the projection in a conventional camera vary for different global positions in the image.
We introduce Perspective Crop Layers (PCLs)—a form of perspective crop of the region of interest based on the camera geometry— and show that accounting for the per-spective consistently improves the accuracy of state-of-the-art 3D pose reconstruction methods. PCLs are modu-lar neural network layers, which, when inserted into ex-isting CNN and MLP architectures, deterministically re-move the location-dependent perspective effects while leav-ing end-to-end training and the number of parameters of the underlying neural network unchanged. We demon-strate that PCL leads to improved 3D human pose recon-struction accuracy for CNN architectures that use cropping operations, such as spatial transformer networks (STN), and, somewhat surprisingly, MLPs used for 2D-to-3D key-point lifting. Our conclusion is that it is important to uti-lize camera calibration information when available, for classical and deep-learning-based computer vision alike.
PCL offers an easy way to improve the accuracy of exist-ing 3D reconstruction networks by making them geometry-aware. Our code is publicly available at github.com/yu-frank/PerspectiveCropLayers. 1.

Introduction
Convolutional neural networks (CNNs) have proven highly effective for image-based prediction tasks because of their translation invariance and the locality of the computa-tion they perform. For 3D pose estimation, this allows them to focus on image locations that carry information about the r a l u g n a t c e
R
L
C
P p o r
C l e b a
L p o r
C l e b a
L
M i s m a t c h
M a t c h
Figure 1: Perspective effects and correction with PCL crops. The skier looks as if she turns, from front to back-wards facing, although recorded with a static camera and going straight. PCLs correct the stretching originating from the projection onto the image plane and matches the 3D pose label to the local view direction of the crop. pose while discarding other ones [43, 30, 6, 27, 32, 39, 44, 42, 20, 51, 17, 47]. 9064
Convolutions in the image plane, however, ignore the perspective effects caused by projecting a 3D scene in 2D.
For example, as shown in Figure 1, a person captured by static camera in a ﬁxed pose and moving in a constant di-rection is seen from different angles as their image location changes. Applying the same convolutional ﬁlter at the top-left image corner and at the bottom-right one will therefore yield different features, even though the pose is the same.
In practice, this is typically tackled by increasing the width and depth of the network, so that different ﬁlters and lay-ers can model the same 3D pose perspective-distorted in different ways. The effectiveness of this procedure, how-ever, strongly depends on the availability of large amounts of training data, which is far from being a given for pose estimation in the wild. Notably, two-stage approaches that lift 3D pose from 2D pose estimates using multilayer per-ceptrons (MLPs) [2, 28, 8, 23, 22, 34, 31, 26] rely also on translational invariance by centering the 2D pose on a root joint, thereby losing important cues on perspective distor-tion too.
In this paper, we therefore introduce Perspective Crop
Layers (PCLs) to explicitly account for perspective distor-tion within CNNs and other neural networks. Speciﬁcally, we use a homography to map the input image to a virtual camera with pre-deﬁned intrinsic parameters that point to the region of interest (RoI). The homography parameters are functions of the RoI’s location and scale. Hence, this yields a synthetic view in which the location-dependent perspec-tive deformations are undone. The 3D pose inferred from this synthetic view can then be projected into the original image. This requires a priori knowledge about the intrinsic camera parameters, which is rarely a problem in real-world situations because they either are readily available from the camera speciﬁcations or can be inferred from the input im-ages alone [45, 11]. We will further show that our PCLs are robust to calibration inaccuracies. Ultimately, all the oper-ations performed by our PCLs are differentiable, and thus amenable to end-to-end learning, while removing the need for the CNN to learn the already known perspective geom-etry. Our contributions can be summarized as follows:
•
•
•
We showcase the inﬂuence of perspective effects on 3D pose estimates that increases for poses away from the image center, which is disregarded by virtually all state-of-the-art algorithms;
We derive the equations to compensate for these effects across the image in a location-dependent manner;
We encapsulate our formalism into generic NN lay-ers, dubbed PCLs, that naturally integrate into existing deep learning frameworks.
We demonstrate the beneﬁts of our PCLs for 3D human pose estimation of both rigid objects and articulated people.
−
PCLs yield a consistent boost in performance, of 2 10% on average and up to 25% at the image boundary where per-spective effects are strongest. Notably, the improvements attributable to our PCLs are consistent across the baseline we seek to improve, which validates our claim that even the most-advance deep networks do not learn these perspective effects on the existing datasets. This includes a PCL variant that undoes the perspective effect on 2D keypoints, thus al-lowing us to showcase the beneﬁts of our approach on state-of-the-art 3D pose estimation methods that lift 2D keypoint detections to 3D poses [22, 33]. Our code is publicly avail-able at github.com/yu-frank/PerspectiveCropLayers. 2.