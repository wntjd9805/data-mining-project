Abstract
Quantizing deep neural networks is an effective method for reducing memory consumption and improving inference speed, and is thus useful for implementation in resource-constrained devices. However, it is still hard for extremely low-bit models to achieve accuracy comparable with that of full-precision models. To address this issue, we propose learnable companding quantization (LCQ) as a novel non-uniform quantization method for 2-, 3-, and 4-bit models.
LCQ jointly optimizes model weights and learnable com-panding functions that can ﬂexibly and non-uniformly con-trol the quantization levels of weights and activations. We also present a new weight normalization technique that al-lows more stable training for quantization. Experimental results show that LCQ outperforms conventional state-of-the-art methods and narrows the gap between quantized and full-precision models for image classiﬁcation and ob-ject detection tasks. Notably, the 2-bit ResNet-50 model on
ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%, allowing LCQ to further exploit the poten-tial of non-uniform quantization. 1.

Introduction
Deep neural networks (DNNs) have been successfully applied to image-based tasks such as image classiﬁcation and object detection, but their implementation in resource-constrained mobile or edge devices remains difﬁcult, ow-ing to the large number of required multiply–accumulate (MAC) operations and parameters. To mitigate this prob-lem, various techniques for compressing DNNs while main-taining performance have been proposed, such as prun-ing [9], knowledge distillation [15], low-rank approxima-tion [10], and network quantization [16]. Among these, network quantization is important as a way to effectively improve both memory consumption and inference speed.
However, network quantization is known to degrade perfor-mance of the original model in proportion to the amount of bit-width reduction.
In network quantization, the weights or activations of
DNNs are typically discretized by a quantization function.
Weight/Activation distribution
Quantized distribution
Non-uniform quantization
Clipping to [-1,1]
Compressing
Rounding
Expanding
Learnable companding
Figure 1: An overview of the proposed method. Our non-uniform quantizer quantizes weights or activations with four functions, those for clipping, compressing, rounding, and expanding. In particular, a composite function consist-ing of those except for the clipping function is generally called the companding function. We formulate the com-panding function in a learnable form with a set of parame-ters Θ and jointly optimize it with clipping parameter α and the other parameters in the model.
Although the quantization function is not differentiable, a straight-through estimator (STE) [1] can be used to approx-imate the gradient calculation for backpropagation. Quanti-zation functions are divided into two types: uniform and non-uniform quantization, in which input values are re-spectively linearly and nonlinearly discretized. Because the weight or activation distribution is empirically dissimilar to the uniform distribution, non-uniform quantization can be expected to further reduce quantization and prediction er-rors than can uniform quantization via proper optimization.
For example, previous works on non-uniform quantization have attempted to use ﬁxed and logarithmic quantization levels [16,22] or learnable quantization levels that minimize quantization errors [30].
However, it is not easy to estimate effective quantiza-tion levels accurately, especially in low-bit models, where accuracy is often inferior to that of uniform quantization methods. This paper thus aims to exploit the potential of non-uniform quantizers and further bridge the accuracy gap 5029
between quantized and full-precision models.
We propose a non-uniform quantization method called learnable companding quantization (LCQ). Figure 1 shows an overview of LCQ. Our method is based on a companding (compressing and expanding) technique [21] that is widely used in telecommunication and signal processing to reduce the bit-width of input signals by nonlinearly limiting their dynamic range. We assume that companding is effective for quantization of DNNs in two aspects. The ﬁrst is that the scale remains unchanged between inputs and outputs by using a nonlinear function and its inverse function, and that maintaining scale reduces quantization error and stabilizes training via backpropagation. The second is that if the com-panding function is differentiable, its parameters can be op-timized to directly minimize task loss. Then, since the pa-rameters are updated with a sum of the two gradients from the paths of before and after rounding, they can be trained with a large quantization inﬂuence. Speciﬁcally, we for-mulate a learnable piecewise linear function as a nonlinear compressing function, allowing ﬂexible and non-uniform control of quantization levels by optimization.
We also propose a new weight normalization method that improves accuracy by restoring the standard deviation of quantized weights to its original level, and we discuss a training trick for efﬁcient inference with lookup tables.
Our main contributions are summarized as follows:
• We propose a LCQ method as a novel non-uniform quantization method, which optimizes non-uniform quantization levels to minimize task loss by training the learnable companding function.
• We present a simple normalization method for weight quantizers called limited weight normalization (LWN) that results in stable training and better solutions.
• We evaluate LCQ on various networks for image clas-siﬁcation and object detection tasks, and the results show promising performance on the CIFAR-10/100,
ImageNet, and COCO datasets. 2.