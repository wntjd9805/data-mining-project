Abstract
The detection of 3D objects from LiDAR data is a criti-cal component in most autonomous driving systems. Safe, high speed driving needs larger detection ranges, which are enabled by new LiDARs. These larger detection ranges re-quire more efﬁcient and accurate detection models. Towards this goal, we propose Range Sparse Net (RSN) – a sim-ple, efﬁcient, and accurate 3D object detector – in order to tackle real time 3D object detection in this extended detection regime. RSN predicts foreground points from range images and applies sparse convolutions on the selected foreground points to detect objects. The lightweight 2D convolutions on dense range images results in signiﬁcantly fewer selected foreground points, thus enabling the later sparse convolu-tions in RSN to efﬁciently operate. Combining features from the range image further enhance detection accuracy. RSN runs at more than 60 frames per second on a 150m × 150m detection region on Waymo Open Dataset (WOD) while be-ing more accurate than previously published detectors. As of 11/2020, RSN is ranked ﬁrst in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based pedestrian and vehicle detection, while being several times faster than alternatives. 1.

Introduction
Concurrent with steady progress towards improving the accuracy and efﬁciency of 3D object detector algorithms
[37, 24, 21, 16, 42, 32, 22, 30, 5, 35, 3], LiDAR sensor hardware has been improving in maximum range and ﬁdelity, in order to meet the needs of safe, high speed driving. Some of the latest commercial LiDARs can sense up to 250m [12] and 300m [36] in all directions around the vehicle. This large volume coverage places strong demands for efﬁcient and accurate 3D detection methods.
Grid based methods [43, 16, 42, 35, 8] divide the 3D space into voxels or pillars, each of these being optionally en-coded using PointNet [25]. Dense convolutions are applied
Figure 1. Accuracy (3D AP/L1 on WOD validation set) vs Latency (ms). RSN models signiﬁcantly outperform others. See Table 1 and
Table 2 for more details. on the grid to extract features. This approach is inefﬁcient for large grids which are needed for long range sensing or small object detection. Sparse convolutions [30] scale better to large detection ranges but are usually slow due to the inef-ﬁciencies of applying to all points. Range images are native, dense representations, suitable for processing point clouds captured by a single LiDAR. Range image based methods
[21, 3] perform convolutions directly over the range in order to extract point cloud features. Such models scale well with distance, but tend to perform less well in occlusion handling, accurate object localization, and for size estimation. A sec-ond stage, reﬁning a set of initial candidate detections, can help mitigate some of these quality issues, at the expense of signiﬁcant computational cost.
To address the shortcomings of existing approaches, we 5725
introduce a novel 3D object detection model – Range Sparse
Net (RSN) – which boosts the 3D detection accuracy and efﬁciency by combining the advantages of methods based on both dense range images and grids. RSN ﬁrst applies a lightweight 2D convolutional network to efﬁciently learn semantic features from the high-resolution range image. Un-like existing range image methods, which regress boxes directly from their underlying features, RSN is trained for high recall foreground segmentation. In a subsequent stage, sparse convolutions are applied only on the predicted fore-ground voxels and their learned range image features, in order to accurately regress 3D boxes. A conﬁgurable sparse convolution backbone and a customized CenterNet [41] head designed for processing sparse voxels is introduced in or-der to enable end-to-end, efﬁcient, accurate object detection without non-maximum-suppression. Figure 1 summarizes the main gains obtained with RSN models compared to oth-ers on the WOD validation set to demonstrate RSN’s efﬁ-ciency and accuracy.
RSN is a novel multi-view fusion method, as it trans-fers information from perspective view (range image) to the 3D view (sparse convolution on the foreground points). Its fusion approach differs from existing multi-view detection methods [42, 35] in that 1) RSN’s ﬁrst stage directly operates on the high resolution range image while past approaches
[42, 35] perform voxelization (in a cylindrical or spherical coordinate system) that may lose some resolution, especially for small objects at a distance. 2) RSN’s second stage pro-cesses only 3D points selected as foreground by the ﬁrst stage, which yields improvements in both feature quality and efﬁciency.
RSN’s design combines several insights that make the model very efﬁcient. The initial stage is optimized to rapidly discriminate foreground from background points, a task that is simpler than full 3D object detection and allows a lightweight 2D image backbone to be applied to the range image at full resolution. The downstream sparse convolution processing is only applied on points that are likely to belong to a foreground object, which leads to additional, signiﬁcant savings in compute. Furthermore, expensive postprocessing such as non-maximum suppression are eliminated by gather-ing local maxima center-ness points on the output, similar to
CenterNet [41].
In this work, we make the following main contributions:
• We propose a simple, efﬁcient and accurate 3D LiDAR detection model RSN, which utilizes LiDAR range im-ages to perform foreground object segmentation, fol-lowed by sparse convolutions to efﬁciently process the segmented foreground points to detect objects.
• We propose a simple yet effective temporal fusion strat-egy in RSN with little additional inference cost.
• In experiments on the Waymo Open Dataset [34] (WOD), we demonstrate the state of art accuracy and efﬁciency for vehicle and pedestrian detection. Experi-ments on an internal dataset further demonstrate RSN’s scalability for long-range object detection.
• We conduct ablation studies to examine the effective-ness of range image features and the impact of aspects like foreground point selection thresholds, or end-to-end model training, on both latency and accuracy. 2.