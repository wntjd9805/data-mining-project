Abstract
We propose an online multi-view depth prediction ap-proach on posed video streams, where the scene geometry in-formation computed in the previous time steps is propagated to the current time step in an efﬁcient and geometrically plausible way. The backbone of our approach is a real-time capable, lightweight encoder-decoder that relies on cost vol-umes computed from pairs of images. We extend it by placing a ConvLSTM cell at the bottleneck layer, which compresses an arbitrary amount of past information in its states. The novelty lies in propagating the hidden state of the cell by accounting for the viewpoint changes between time steps. At a given time step, we warp the previous hidden state into the current camera plane using the previous depth prediction.
Our extension brings only a small overhead of computa-tion time and memory consumption, while improving the depth predictions signiﬁcantly. As a result, we outperform the existing state-of-the-art multi-view stereo methods on most of the evaluated metrics in hundreds of indoor scenes while maintaining a real-time performance. Code available: https://github.com/ardaduz/deep-video-mvs 1.

Introduction
Obtaining dense 3D information about the environment is key for a wide range of applications such as naviga-tion for autonomous vehicles (e.g., robots, drones [44]), mixed reality [1, 2, 24, 42], 3D modelling and industrial control. Compared to active depth sensing with LiDAR [41], time-of-ﬂight [19] or structured-light cameras [13], camera-based passive sensing has the advantage of being energy and cost efﬁcient, compact in size and operating in a wide range of conditions [33]. Among passive depth sensing ap-proaches, monocular systems can offer highly mobile, low-maintenance solutions, while stereo devices often require baseline sizes that are infeasible for mobile devices [46].
One common denominator of the aforementioned appli-cations is that the data is acquired as a video stream instead of sparse instances in time, and the depth is often recon-structed for selected keyframes. In this work, we assume a calibrated camera and known poses between acquisitions and focus on the dense depth recovery for each keyframe.
Such pose information can, for instance, be obtained through visual-inertial odometry techniques [6, 40], which are read-ily available in mobile platforms (e.g. Apple ARKit and
Android ARCore) [20] or mixed reality headsets such as Mi-crosoft HoloLens. The presence of camera poses enables the computation of triangulation-based metric reconstructions, as opposed to the popular learning-based single image depth prediction methods [3,12,14,29,55] that have been extended to video [39, 50, 52, 58]. Finally, the real-time aspect of the applications and the potential of an on-device solution, imply targeting a lightweight online multi-view stereo (MVS) sys-tem that is memory and compute efﬁcient. Therefore, similar to [20, 32], we speciﬁcally aim to harness the advantages of video, with limited variation in viewpoint at consecutive time steps, instead of pursuing unstructured MVS.
In this work, we present a framework that can extend many existing MVS methods, such that, when processing 15324
video streams, partial scene geometry information from the past contributes to the prediction at the current time step, leading to improved and consistent depth outputs. We use convolutional long short-term memory (ConvLSTM) [45] and a hidden state propagation scheme to achieve such infor-mation ﬂow in the latent space. Our approach is inﬂuenced by [20], where latent cost volume encodings are weakly cou-pled through a Gaussian Process, and by [39] where latent encodings of image features and sparse depth cues are fused through ConvLSTM to achieve temporally consistent depth predictions. However, we leverage geometry and explicitly account for the implications of perspective projection when propagating the latent encodings.
Our key contributions are as follows: (i) We propose a compact, cost-volume-based, stereo depth estimation net-work that solely relies on 2D convolutions to obtain real-time and memory efﬁcient processing. (ii) We extend our model with a ConvLSTM cell, an explicit hidden state propagation scheme, and a training/inference strategy to enable spatio-temporal information ﬂow. This extension signiﬁcantly im-proves the depth estimation accuracy, while creating only a small computational overhead, c.f . Fig. 1. (iii) We set a new state-of-the-art on ScanNet [11], 7-Scenes [16], TUM RGB-D [47] and ICL-NUIM [18], c.f . Tab. 1, while obtaining the lowest runtime and a small memory footprint, c.f . Fig. 4. 2.