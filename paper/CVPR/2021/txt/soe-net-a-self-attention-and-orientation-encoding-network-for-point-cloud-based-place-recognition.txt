Abstract
Scan sequence
We tackle the problem of place recognition from point cloud data and introduce a self-attention and orientation encoding network (SOE-Net) that fully explores the rela-tionship between points and incorporates long-range con-text into point-wise local descriptors. Local information of each point from eight orientations is captured in a
PointOE module, whereas long-range feature dependen-cies among local descriptors are captured with a self-attention unit. Moreover, we propose a novel loss func-tion called Hard Positive Hard Negative quadruplet loss (HPHN quadruplet), that achieves better performance than the commonly used metric learning loss. Experiments on various benchmark datasets demonstrate superior perfor-mance of the proposed network over the current state-of-the-art approaches. Our code is released publicly at https://github.com/Yan-Xia/SOE-Net. 1.

Introduction
Place recognition and scene localization in large-scale and complex environments is a fundamental challenge with applications ranging from autonomous driving [13, 14, 22] and robot navigation [11, 29] to augmented reality [19].
Given a query image or a LiDAR scan, the aim is to re-cover the closest match and its location by traversing a pre-built database. In the past decade, a variety of image-retrieval based solutions have shown promising perfor-mance [18, 20, 21]. However, the performance of image-based methods often degrades when facing drastic vari-ations in illumination and appearance caused by weather and seasonal changes [1]. As a possible remedy, 3D point clouds acquired from LiDAR offer accurate and detailed 3D information that is inherently invariant to illumination changes. As a consequence, place recognition from point cloud data is becoming an increasingly attractive research topic. Fig. 1 (Top) shows a typical pipeline for point cloud
†Corresponding author.
Retrieval of the best match
Query scan
Location 1
Location 2
Location N
Figure 1. (Top) Place recognition from 3D point clouds: the street scene of a route map (shown in blue line) is denoted by a set of real-scan point clouds tagged with UTM coordinates. Given a query scan, we retrieve the closest match in this map (shown in the green box) to get the location of the query scan. (Bottom) Com-paring the average recall at top 1 retrieval, SOE-Net signiﬁcantly outperforms all published methods on various datasets. based place recognition: One ﬁrst constructs a database with LiDAR scans tagged with UTM coordinates acquired from GPS/INS readings. Given a query LiDAR scan, we then retrieve the closest match and its corresponding loca-tion from the database.
The main challenge of point cloud based place recogni-tion lies in how to ﬁnd a robust and discriminative global 11348
descriptor for a local scene point cloud. While there ex-ist abundant works on learning image descriptors, learning from point cloud data is far less developed. To date, only a few networks have been proposed for point cloud based place recognition in large-scale scenarios. PointNetVLAD
[1] is a pioneering work, which ﬁrst extracts the local fea-tures from 3D point clouds using PointNet [31] and then fuses them into global descriptors using the NetVLAD [2] layer. PCAN [40] adapts the PointNet++ [32] architecture to generate an attention map which re-weights each point during the local descriptors aggregation stage. Both meth-ods use PointNet [31] to extract local descriptors, which ignores the geometric relationship among points. As of late, the authors of DH3D [8] and DAGC [36] noticed this shortcoming and designed advanced local feature descrip-tion networks. While this results in better local descriptors, both approaches simply aggregate these local descriptors to a global descriptor using the PCAN or PointNetVLAD fusion architecture, without considering the long-range de-pendencies of different features.
Similar to these previous studies [8, 36], we also notice the importance of better utilizing the neighborhood context of each point when extracting its geometric representation.
To tackle this problem, we adopt a point orientation encod-ing (PointOE) module to encode the neighborhood informa-tion of various orientations. The orientation-encoding unit is integrated with PointNet [31], taking its advantages in feature representation learning provided by the multi-layer perceptrons. Another observation is, when aggregated into a global descriptor, different local descriptors should tac-tically contribute unevenly. To achieve this, we develop a self-attention unit to introduce long-range contextual de-pendencies, encoding the spatial relationships of the local descriptors for weighting. Combining the principals above, we propose a novel network named SOE-Net (Self-attention and Orientation Encoding Network). It is an end-to-end ar-chitecture that explores the relationship among the raw 3D points and the different importance of local descriptors for large-scale point cloud based retrieval. Speciﬁcally, SOE-Net combines local descriptor extraction and aggregation, which enables one-stage training to generate a discrimina-tive and compact global descriptor from a given 3D point cloud. Additionally, we propose a novel “Hard Positive
Hard Negative quadruplet” (HPHN quadruplet) loss, which addresses some of the limitations of the widely used lasy quadruplet loss. To summarize, main contributions of this work include:
• We propose a novel point orientation encoding (PointOE) module to effectively extract local descrip-tors from a given point cloud, considering the relation-ship between each point and its neighboring points.
We further design a self-attention unit to differentiate the importance of different local descriptors to a global descriptor.
• We present a new loss function termed HPHN quadru-plet loss that is more effective for large-scale point cloud based retrieval. Comparing with previous loss functions, it can achieve more versatile global descrip-tors by relying on the maximum distance of positive pairs and the minimum distance of negative pairs.
• We conduct experiments on four benchmark datasets, including Oxford RobotCar [28] and three in-house datasets to demonstrate the superiority of SOE-Net over other state-of-the-art methods. Notably, the per-formance on Oxford RobotCar reaches a recall of 89.37% at top 1 retrieval. 2.