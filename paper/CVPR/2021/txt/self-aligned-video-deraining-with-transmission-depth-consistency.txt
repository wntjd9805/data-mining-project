Abstract
In this paper, we address the problem of rain streaks and rain accumulation removal in video, by developing a self-alignment network with transmission-depth consis-tency. Existing video based deraining methods focus only on rain streak removal, and commonly use optical ﬂow to align the rain video frames. However, besides rain streaks, rain accummulation can considerably degrade visibility; and, optical ﬂow estimation in a rain video is still erro-neous, making the deraining performance tend to be inac-curate. Our method employs deformable convolution lay-ers in our encoder to achieve feature-level frame alignment, and hence avoids using optical ﬂow. For rain streaks, our method predicts the current frame from its adjacent frames, such that rain streaks that appear randomly in the tempo-ral domain can be removed. For rain accumulation, our method employs a transmission-depth consistency loss to resolve the ambiguity between the depth and water-droplet density. Our network estimates the depth from consecu-tive rain-accumulation-removal outputs, and calculates the transmission map using a commonly used physics model.
To ensure photometric-temporal and depth-temporal con-sistencies, our method estimates the camera poses, so that it can warp one frame to its adjacent frames. Experimental results show that our method is effective in removing both rain streaks and rain accumulation, outperforming those of state-of-the-art methods quantitatively and qualitatively. 1.

Introduction
Rain is a common outdoor weather condition, and de-grades visibility in video. The degradation are mainly caused by: rain streaks and rain accumulation (or rain veil-ing effect). Rain streaks partially occlude a background scene, change image appearance, cause the scene to look blurred. Rain accumulation, which is like fog or mist, washes out the scene colors, reduces the overall contrast and generates a veiling effect. Both rain streaks and accu-†This work is supported by MOE2019-T2-1-130.
Rain Image
Our Result
Li et al.
Yang et al.
Figure 1: Top left: Input image. Top right: Our result. Bot-tom left: Li et al.’s result [30]. Bottom right: Yang et al.’s result [51]. Zoom-in for better visualization. mulation are visibly present and thus degrades the visibility of a scene. Hence, to obtain better background scene visual information, we need to remove both rain streaks and rain accumulation in videos.
A series of rain removal methods for videos have been proposed [45, 39, 52, 53, 49, 18, 30, 33, 48, 51]. Most of them focus on rain streaks alone, and thus cannot deal with rain accumulation, which is unfortunately commonly present in any rainy situations, particularly in heavy rain.
Many of these methods, e.g., [25, 33, 48, 51] rely on op-tical ﬂow for aligning adjacent frames. Yet, optical ﬂow estimation in rainy conditions is still unstable and challeng-ing, since its main constraint (the brightness constancy con-11966
straint) generally does not hold.
For rain accumulation, to our knowledge, there is no video-based method dealing with the problem. There are rain-accumulation removal methods for single images, e.g.
[49, 30, 18]. Unfortunately, they suffer from the ambigu-ity between depth and water-droplet density, i.e.: a thick veiling effect can be triggered either by a relatively sparse droplet density but a distant scene, or by a relatively dense droplet but a nearby scene. This ambiguity applies to all surfaces, yet particularly to achromatic surfaces (i.e., white, gray), causing the over-saturation or under-saturation effect in the rain removal results.
In this paper, we address the problem of daytime rain removal from video by focusing on both rain streaks and rain accumulation. To accomplish this task, ﬁrst, we align a few consecutive input frames using a feature-based align-ment network; and thus unlike many existing methods, we do not rely on optical ﬂow. Second, our network removes rain-steaks in every frame based on the aligned features of its adjacent frames, as most likely rain streaks randomly ap-pear along the temporal domain. We train our network with both synthetic rain videos with ground-truths and real rain videos without ground-truths. Third, to deal with rain ac-cumulation, we exploit the depth cues that can be obtained from the input video. Given the estimated rainstreak-free images from the previous step, our accumulationNet gen-erates a rain-accumulation free images, which is our ﬁnal output. To train the accumulationNet, we employ a few losses: depth-transmission consistency loss, depth-temporal consistency loss, and photo-temporal consistency loss.
As a summary, our contributions are as follows:
• We introduce a video deraining method that can re-move both rain streaks and rain accumulation in one end-to-end framework. To our knowledge, this is the
ﬁrst attempt in video deraining dealing with both rain streaks and rain accumulation.
• We provide a video-based deraining method with feature-level alignment. Many existing deraining methods use optical ﬂow, which brings many issues due to the degradation in the input video. By using de-formable convolution layers in encoder, we avoid us-ing optical ﬂow in our method.
• We propose a few losses that combines depth, trans-mission map, and camera pose to deal with rain accu-mulation. The use of depth and camera pose enable our method to handle the depth and water-droplet am-biguity problem, and thus improving our results.
Using these novel ideas, our experimental results show the effectiveness of our method compared to the-state-of-the-art methods qualitatively and quantitatively. 2.