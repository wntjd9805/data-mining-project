Abstract
When translating text inputs into layouts or images, ex-isting works typically require explicit descriptions of each object in a scene, including their spatial information or the associated relationships. To better exploit the text in-put, so that implicit objects or relationships can be prop-erly inferred during layout generation, we propose a Lay-outTransformer Network (LT-Net) in this paper. Given a scene-graph input, our LT-Net uniquely encodes the seman-tic features for exploiting their co-occurrences and implicit relationships. This allows one to manipulate conceptually diverse yet plausible layout outputs. Moreover, the decoder of our LT-Net translates the encoded contextual features into bounding boxes with self-supervised relation consis-tency preserved. By ﬁtting their distributions to Gaussian mixture models, spatially-diverse layouts can be addition-ally produced by LT-Net. We conduct extensive experiments on the datasets of MS-COCO and Visual Genome, and con-ﬁrm the effectiveness and plausibility of our LT-Net over recent layout generation models. Codes will be released at
LayoutTransformer. 1.

Introduction
Text-to-image (T2I) generation takes the input text de-scriptions and converts them into realistic images, which would beneﬁt a massive number of applications including computer-aided design, art generation and image editing emerging, it attracts the attention from researchers in com-puter vision and deep learning communities.
In addition to the need to output high-quality images [19, 26, 27], the main challenge in T2I lies in the generation of plausible im-ages, with semantic relationships preserved across different objects. Thus, how to bridge the gap between semantic and perceptual information requires the efforts from researchers in the ﬁelds of computer vision and machine learning.
*Equal Contribution
Generated layouts
Generated image
Input scene graph
Spatially diverse  layout generation
Conceptually diverse  layout generation
Generated  scene graph
Inferred
Object
Predicate
Generated image
Figure 1. Layout generation with conceptual and spatial diversity.
Given a scene graph input, we aim to produce diverse plausible layouts with the ability to exploit implicit objects/relations.
A common challenge in text-to-layout or image is that, objects and their relations described in text sentences may not be easily described, which would result less plausible outputs. Therefore, Johnson et al. [8] choose to convert tex-tual input into scene graph (SG) as an intermediate text rep-resentation, which explicitly deﬁnes the objects and their relationships in a scene. Generally, the task of SG-to-image can be decomposed into the following two steps: SG-to-layout [6, 9, 12] and layout-to-image [13, 25, 21, 1] gen-eration. The former focuses on modeling the geometric properties of objects while retaining their semantic char-acteristics. The latter targets at synthesizing realistic im-ages conditioned on the given layout conﬁguration. To ad-dress text-to-layout generation, [9] apply a variational au-13732
VAE based
LayoutVAE Sg2Im Grid2Im NDN CanonicalSg2Im
GCN based
Layout Conﬁguration
Inferring spatial relation
Generating semantic-equivalent relation
Complex scene graph
Spatially-diverse Layout Generation
Conceptually-diverse Layout Generation
----X
-X
-----X
X
----X
X
--X
-X
X
X
X
X
-Transformer based
Ours
X
X
X
X
X
X
Table 1. Comparisons of recent approaches on scene graph to layout generation. toencoder (VAE) to model the layout distribution of objects.
Alternatively, [12, 6] utilize Graph Convolution Network (GCN) [5] to extract the semantic information of the input
SG and utilize VAE to regress the output layouts.
Although the use of SG allows possible inference of im-plicit relationships between objects in a scene, it cannot eas-ily exploit and handle static objects or background, and thus the resulting layout might not be satisfactory. For example, the SG triplet of Man-walks-Dog implies the potential ob-jects of tree, grass, pavement, since such static objects or those in the background are related to walk and dog. Such co-occurrences are conceptually realistic and can be pos-sibly exploited during training. Moreover, given a prop-erly derived SG, how to produce diverse layouts (i.e., those with different yet plausible compositions) and avoid possi-ble mode collapse problems would be among the difﬁculties in text-to-layout generation.
To overcome the above challenges, we propose a novel
LayoutTransformer Network (LT-Net) in this paper. Fol-lowing [8], our LT-Net starts from SG as the input text de-scription. In order to exploit the implicit objects or relations in a SG input, we present a masked language model (MLM) based on BERT [4]. Preserving the input graph characteris-tics, our MLM uniquely learns the contextual representation with object co-occurrence information exploited, allowing generation of conceptually related yet diverse outputs. As for layout synthesis, we advance the transformer decoder which sequentially produces the bounding boxes for each object/relation, whose distribution is modeled by Gaussian
Mixture Models [20]. This decoder design allows spatially diverse layout components. Finally, a visual-textual co-attention module is deployed for layout reﬁnement.
We now highlight the contributions as follows:
• We propose a novel framework of LayoutTransformer
Network (LT-Net), which takes scene-graph inputs for layout generation, with the ability to produce concep-tually and spatially diverse outputs.
• Our LT-Net jointly encodes object/relation, pair and sentence-wise information from SG inputs, which not only learns to recover the semantic information also exploits implicit objects and relations for conceptual diversity guarantees.
• Our decoder utilizes Gaussian mixture models to de-scribe spatial outputs for each object/relation, model-ing the desirable distributions.
• Visual-textual co-attention is performed to reﬁne the layout output, which jointly observes the conditions of derived semantic and spatial representations. 2.