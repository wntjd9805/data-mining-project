Abstract
The 3D world limits the human body pose and the hu-man body pose conveys information about the surrounding objects. Indeed, from a single image of a person placed in an indoor scene, we as humans are adept at resolving am-biguities of the human pose and room layout through our knowledge of the physical laws and prior perception of the plausible object and human poses. However, few computer vision models fully leverage this fact. In this work, we pro-pose a holistically trainable model that perceives the 3D scene from a single RGB image, estimates the camera pose and the room layout, and reconstructs both human body and object meshes. By imposing a set of comprehensive and sophisticated losses on all aspects of the estimations, we show that our model outperforms existing human body mesh methods and indoor scene reconstruction methods. To the best of our knowledge, this is the ﬁrst model that outputs both object and human predictions at the mesh level, and performs joint optimization on the scene and human poses. 1.

Introduction
Holistic scene perception is key to our human ability to accurately interpret and interact with the 3D world. The hu-man visual system naturally integrates context from actors, objects, and scene layout to infer realistic, robust estima-tions of the world. Suppose a human is partially included in an image because they are positioned behind a desk. We can still effortlessly extract rich information from the static scene to resolve ambiguities due to the occlusion. Like-wise, the appearance of humans also provides useful infor-mation about scenes, such as the ground plane and depth of surrounding objects. Humans and objects in scenes jointly manifest spatial occupancies that constrain their relative po-sitions. For computer vision systems to achieve high accu-racy in recognizing and interpreting complex scenes, it is therefore important to develop approaches for holistic scene perception and reasoning.
In recent years, holistic scene understanding from sin-gle view images has gained increasing interest from com-Figure 1. Given a single view RGB image of an indoor scene, our model is able to (i) predict all aspects of the scene (3D ob-ject bounding boxes, object and human meshes, 3D room layout, camera pose), and (ii) jointly optimize over a comprehensive set of global consistency losses. The ﬁnal result is more physically plausible and accurate.
[34] [14] proposed methods for puter vision researchers. joint reasoning over inanimate scenes, and recovered room layout and 3D object bounding boxes using consistency losses such as a constraint for objects to be enclosed within
[4] additionally discouraged in-the room bounding box. tersection between object bounding box estimations, and was the ﬁrst model to bring 3D human pose estimation into the holistic scene understanding problem. It incorporated human-object interaction priors to reason about approxi-mate relations between humans and objects. However all of these works still operate at the relatively coarser level of bounding boxes and joint key points, and are therefore lim-ited in their ability to use precise shapes, surfaces, and phys-ical occupancies to design holistic scene constraints and im-prove estimation accuracy.
In this work we propose the ﬁrst single-view, holistic scene understanding method that jointly optimizes over all aspects of 3D human pose, objects, and room layout at the mesh level, to produce state-of-the-art mesh estimations of the scene. Our approach builds on recent advances in mesh prediction. [5] [7] [29] proposed methods for reconstruct-ing the individual object meshes with varying topological 1334
structures. [27] builds on [29] and proposed the ﬁrst holistic 3D scene understanding method with mesh reconstruction at the instance level, however they did not consider humans.
Recently, [11] introduced a method for 3D mesh-based hu-man pose estimation, that utilizes physical occupancy in-formation of the static scene to discourage body penetration into the scene. However, [11] requires the ground truth 3D scans of the scene, and does not perform joint human and scene estimation.
Given a single RGB image, our method simultaneously reconstructs the human body mesh and multiple aspects of the scene – 3D object meshes and bounding boxes, room layout, and camera pose – all in 3D (Figure 1). Our ap-proach outputs the SMPL-X (SMPL eXpressive) [30] hu-man mesh model, which fully parameterizes the 3D surface of the human body. It also leverages a variant of the Topol-ogy Modiﬁcation Network (TMN) [5], proposed in [27], as the base model for static object mesh and scene recon-struction.
Importantly, we introduce a joint optimization process that incorporates a comprehensive set of physical constraints and priors including 2D/3D reprojection con-straints, object-object mesh constraints, object-human mesh constraints, and object/human - room layout constraints, to obtain robust, physically plausible predictions. We perform experimental evaluation on the PiGraphs [33] and PROX
[11] datasets and demonstrate that our model outperforms state-of-the-art methods on either 3D scene understanding or 3D human pose estimation.
In summary, our contributions are the following:
• We propose a holistic trainable model for jointly re-constructing 3D human body meshes and static scene elements (3D object meshes and bounding boxes, room layout, and camera pose) from monocular RGB images. To the best of our knowledge, we are the ﬁrst to jointly estimate this rich scene understanding at the mesh level.
• Our model does not require any ground truth anno-tations of the 3D scene or the human poses, and can be directly used on any indoor dataset to produce high quality mesh reconstructions.
• Through our joint optimization process that incorpo-rates a comprehensive set of physical constraints and priors, we show that our model outperforms prior state-of-the-art methods on either 3D scene understanding or 3D human pose estimation, on the PiGraphs and
PROX Quantitative datasets. 2.