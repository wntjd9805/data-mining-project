Abstract
We present a generic image-to-image translation frame-work, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a se-ries of style vectors which are fed into a pretrained Style-GAN generator, forming the extended W+ latent space. We
ﬁrst show that our encoder can directly embed real images into W+, with no additional optimization. Next, we pro-pose utilizing our encoder to directly solve image-to-image translation tasks, deﬁning them as encoding problems from some input domain into the latent domain. By deviating from the standard “invert ﬁrst, edit later” methodology used with previous StyleGAN encoders, our approach can han-dle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solv-ing translation tasks through StyleGAN signiﬁcantly sim-pliﬁes the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal syn-thesis via the resampling of styles. Finally, we demon-strate the potential of our framework on a variety of fa-cial image-to-image translation tasks, even when compared to state-of-the-art solutions designed speciﬁcally for a sin-gle task, and further show that it can be extended beyond the human facial domain. Code is available at https:
//github.com/eladrich/pixel2style2pixel. 1.

Introduction
In recent years, Generative Adversarial Networks (GANs) have signiﬁcantly advanced image synthesis, par-ticularly on face images. State-of-the-art image genera-tion methods have achieved high visual quality and ﬁdelity, and can now generate images with phenomenal realism.
Most notably, StyleGAN [20, 21] proposes a novel style-2287
based generator architecture and attains state-of-the-art vi-sual quality on high-resolution images. Moreover, it has been demonstrated that it has a disentangled latent space,
W [39, 7, 35], which offers control and editing capabilities.
Recently, numerous methods have shown competence in controlling StyleGAN’s latent space and performing mean-ingful manipulations in W [17, 35, 36, 13]. These methods follow an “invert ﬁrst, edit later” approach, where one ﬁrst inverts an image into StyleGAN’s latent space and then ed-its the latent code in a semantically meaningful manner to obtain a new code that is then used by StyleGAN to generate the output image. However, it has been shown that invert-ing a real image into a 512-dimensional vector w ∈ W does not lead to an accurate reconstruction. Motivated by this, it has become common practice [1, 2, 4, 42, 3] to encode real images into an extended latent space, W+, deﬁned by the concatenation of 18 different 512-dimensional w vectors, one for each input layer of StyleGAN. These works usually resort to using per-image optimization over W+, requiring several minutes for a single image. To accelerate this opti-mization process, some methods [4, 42] have trained an en-coder to infer an approximate vector in W+ which serves as a good initial point from which additional optimization is required. However, a fast and accurate inversion of real images into W+ remains a challenge.
In this paper, we ﬁrst introduce a novel encoder architec-ture tasked with encoding an arbitrary image directly into
W+. The encoder is based on a Feature Pyramid Net-work [24], where style vectors are extracted from differ-ent pyramid scales and inserted directly into a ﬁxed, pre-trained StyleGAN generator in correspondence to their spa-tial scales. We show that our encoder can directly recon-struct real input images, allowing one to perform latent space manipulations without requiring time-consuming op-timization. While these manipulations allow for extensive editing of real images, they are inherently limited. That is because the input image must be invertible, i.e., there must exist a latent code that reconstructs the image. This require-ment is a severe limitation for tasks, such as conditional image generation, where the input image does not reside in the same StyleGAN domain. To overcome this limita-tion we propose using our encoder together with the pre-trained StyleGAN generator as a complete image-to-image translation framework.
In this formulation, input images are directly encoded into the desired output latents which are then fed into StyleGAN to generate the desired output images. This allows one to utilize StyleGAN for image-to-image translation even when the input and output images are not from the same domain.
While many previous approaches to solving image-to-image translation tasks involve dedicated architectures spe-ciﬁc for solving a single problem, we follow the spirit of pix2pix [16] and deﬁne a generic framework able to solve a wide range of tasks, all using the same architecture. Besides the simpliﬁcation of the training process, as no adversary discriminator needs to be trained, using a pretrained Style-GAN generator offers several intriguing advantages over previous works. For example, many image-to-image archi-tectures explicitly feed the generator with residual feature maps from the encoder [16, 38], creating a strong locality bias [33]. In contrast, our generator is governed only by the styles with no direct spatial input. Another notable advan-tage of the intermediate style representation is the inherent support for multi-modal synthesis for ambiguous tasks such as image generation from sketches, segmentation maps, or low-resolution images. In such tasks, the generated styles can be resampled to create variations of the output image with no change to the architecture or training process. In a sense, our method performs pixel2style2pixel translation, as every image is ﬁrst encoded into style vectors and then into an image, and is therefore dubbed pSp.
The main contributions of this paper are: (i) A novel
StyleGAN encoder able to directly encode real images into the W+ latent domain; and (ii) A new methodology for utilizing a pretrained StyleGAN generator to solve image-to-image translation tasks. 2.