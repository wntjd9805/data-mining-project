Abstract
Rotation is among the long prevailing, yet still unre-solved, hard challenges encountered in visual object track-ing. The existing deep learning-based tracking algorithms use regular CNNs that are inherently translation equivari-ant, but not designed to tackle rotations.
In this paper, we ﬁrst demonstrate that in the presence of rotation in-stances in videos, the performance of existing trackers is severely affected. To circumvent the adverse effect of ro-tations, we present rotation-equivariant Siamese networks (RE-SiamNets), built through the use of group-equivariant convolutional layers comprising steerable ﬁlters. SiamNets allow estimating the change in orientation of the object in an unsupervised manner, thereby facilitating its use in rel-ative 2D pose estimation as well. We further show that this change in orientation can be used to impose an addi-tional motion constraint in Siamese tracking through im-posing restriction on the change in orientation between two consecutive frames. For benchmarking, we present Rota-tion Tracking Benchmark (RTB), a dataset comprising a set of videos with rotation instances. Through experiments on two popular Siamese architectures, we show that RE-SiamNets handle the problem of rotation very well and out-perform their regular counterparts. Further, RE-SiamNets can accurately estimate the relative change in pose of the target in an unsupervised fashion, namely the in-plane ro-tation the target has sustained with respect to the refer-ence frame. Code and data can be accessed at https:
//github.com/dkgupta90/re-siamnet. 1.

Introduction
The task of visual object tracking with Siamese net-works
[1, 29], also referred as Siamese tracking, trans-forms the problem of tracking into similarity estimation be-tween a template frame and sampled regions from a can-didate frame. Siamese trackers have recently gained pop-ularity in the ﬁeld of visual object tracking, especially be-cause of their strong discriminative power obtained from
Figure 1: Example demonstrating rotation non-equivariance in regular CNN models used in object tracking, ψθ(f (·)) 6= f (ψθ(·)). Here f (·) and ψθ(·) denote the neural network encoding function and rotation transform, respectively. similarity matching. This is the primary reason most of the state-of-the-art trackers are based on this framework
[1, 11, 18, 19, 29].
Although Siamese trackers are generally shown to work well, they are still prone to failure under challenges such as partial occlusion [16], scale change [27] or when one of the two inputs is rotated.
This paper focuses on handling the adverse affects of in-plane rotation of objects on the performance of Siamese trackers. Object rotation is considered to be amongst the hardest challenges of tracking with no effective solution till date.
It can commonly occur in real-life scenarios, espe-cially when the camera records from the top, as in drones, where either the object is rotating or the camera itself. Ego-centric videos are another example, where large head rota-tions can cause the target to rotate.
The CNN architectures used in Siamese trackers are not inherently equivariant to in-plane rotations of the target.
The implication is that the model may perform well on ob-ject orientations that are represented in the training set, but may fail on other previously unseen orientations. This hap-pens because the latent encoding obtained from the network for such cases might not be representative of the input im-age itself. Example demonstrating this issue is shown in
Figure 1. Further, even if it were equivariant, the cross-12362
correlation step in traditional Siamese trackers would still fail to perform an accurate matching between the template and candidate images due to rotational shift between them.
A straightforward approach to enforce learning of ro-tated variants is to use training datasets where in-plane ro-tations occur naturally or through data augmentation. How-ever, as highlighted in [17], there are several limitations of data augmentation. First, such procedures would require learning separate representations for different rotated vari-ants of the data. Second, the more variations are consid-ered, the more ﬂexible tracker model needs to be to capture them all. This means a signiﬁcant increase in training data and computational budget. Further, such an approach would make the model invariant to rotations, thus making the pre-dictions unreliable when the target is surrounded by similar objects, e.g., tracking a ﬁsh in a school of ﬁshes.
This paper aims at incorporating the property of rotation equivariance in the existing Siamese trackers. This built-in feature would then allow the trackers to capture the rota-tion variations from the start itself without the need of ad-ditional data augmentation. Rotation equivariant networks have been studied widely in the context of image classiﬁ-cation [3, 4, 34, 35, 36]. Drawing inspiration from these works, we introduce rotation equivariance for the task of ob-ject localization in videos. We exploit the concept of group-equivariant CNNs [3], and use steerable ﬁlters [35] to make the Siamese trackers equivariant to rotations. This way of incorporating rotation equivariance induces built-in sharing of weights among the different groups of rotations and adds an internal notion of rotation in the model (referred further as RE-SiamNet).
Interpreting the template image as the static memory of the tracking model, RE-SiamNets know beforehand how the encoding should be represented for a discrete set of rota-tions. In the absence of other challenges such as illumina-tion variation and occlusion, the target appearance would match exactly at one of the discrete rotations, and is ex-pected to contain only small errors for other intermediate angles. This property increases the discriminative power of the trackers towards differences in orientation (in-plane rotation) of the target. Beyond this, RE-SiamNet can be used for relative 2D pose estimation of objects in videos, interchangeably also referred in this paper as relative orien-tation estimation of objects. RE-SiamNets are equivariant to translations and rotations, and these properties combined with the structure of Siamese networks allow capturing the change in pose of the target in 2D. Further, we propose an additional motion constraint on the rotational degree of freedom and demonstrate that it allows to obtain better tem-poral correspondence in videos.
It is important to note that most current datasets, espe-cially in tracking, contain very limited to no instances of rotation. Thus, for benchmarking the performance of mod-els in presence of in-plane rotations, we present Rotating
Object Benchmark (ROB), a set of videos focusing on in-plane rotations. Annotations include bounding boxes of the target object as well as its orientation in every frame. To further summarize, the contributions of this paper are:
• We give a brief introduction to equivariant convolu-tions networks. We then extend the theory to ob-tain rotation-equivariant Siamese architectures (RE-SiamNets) that feature in-plane rotation equivariance.
• We show that RE-SiamNets estimate the relative 2D pose of any rotating object in a unsupervised manner.
Further, we introduce an additional motion constraint to improve temporal correspondence in videos.
• For benchmarking, we present Rotating Object Bench-mark (ROB), a novel dataset comprising sequences with signiﬁcant in-plane rotations of the target.
• Through incorporating in two existing Siamese track-ing methods, we show that rotation equivariance can provide signiﬁcant improvements in tracking perfor-mance and accurately estimate the orientation changes. 2.