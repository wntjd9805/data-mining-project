Abstract
Human pose estimation from single images is a challeng-ing problem in computer vision that requires large amounts of labeled training data to be solved accurately. Unfortu-nately, for many human activities (e.g. outdoor sports) such training data does not exist and is hard or even impossible to acquire with traditional motion capture systems. We pro-pose a self-supervised approach that learns a single image 3D pose estimator from unlabeled multi-view data. To this end, we exploit multi-view consistency constraints to disen-tangle the observed 2D pose into the underlying 3D pose and camera rotation.
In contrast to most existing meth-ods, we do not require calibrated cameras and can there-fore learn from moving cameras. Nevertheless, in the case of a static camera setup, we present an optional extension to include constant relative camera rotations over multiple views into our framework. Key to the success are new, unbi-ased reconstruction objectives that mix information across views and training samples. The proposed approach is eval-uated on two benchmark datasets (Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset. 1.

Introduction
Human pose estimation from single images is an ongo-ing research topic in computer vision. There exist a large amount of supervised deep learning solutions in the litera-ture. These approaches achieve remarkable results in a su-pervised setting, i.e. having 2D to 3D annotations, but heav-ily rely on a vast amount of available training data. How-ever, there are many activities a person can perform which are not present in common datasets. For instance, human motions performed during outdoor and/or sports activities, e.g. as shown in Fig. 1, are hard or even impossible to cap-ture with a commercial motion capture systems. There-fore, the acquisition of training data is a major practical challenge. To this end, we propose a novel self-supervised training procedure that does not require any 2D or 3D an-Figure 1. CanonPose learns a monocular 3D human pose estimator from multi-view self-supervision. By estimating 3D poses from different views in a canonical form together with the respective camera rotations we exploit multi-view consistency in the training data. Even for challenging outdoor datasets with moving cameras we achieve convincing 3D pose estimates from single images after training. notations in the multi-view training dataset and works with uncalibrated cameras. To acquire 2D joint predictions from images we use a 2D human joint estimator [7] that is pre-trained on a different dataset with only 2D joint annotations.
The only requirements for our method are at least two tem-porally synchronised cameras that observe the person of in-terest from different angles. No further knowledge about the scene, camera calibration and intrinsics is required. Sev-eral related works consider a sparse set of 3D annotations
[36, 34, 29], unpaired 3D data [47, 48, 16], or known cam-era positions [36, 34] to solve this problem. However, such data rarely exists for outdoor settings with moving cameras.
To the best of our knowledge, there are only three compet-ing methods [2, 14, 11] that apply to our setting. They either require additional knowledge about the scene or observed person, such as scene geometry [2] and bone lengths con-straints [11], or sophisticated traditional computer vision al-gorithms that produce a pseudo ground truth pose [14].
We propose a self-supervised training method which mixes outputs of multiple weight-sharing neural networks.
Fig. 2 shows our training pipeline when using two cameras.
Each individual network takes a single image as input and outputs a 3D pose in a canonical rotation, which gives our 13294
method its name CanonPose. This representation allows for the projection of all estimated 3D poses to any camera of the setup. Our approach splits into two stages. The ﬁrst stage predicts the 2D human pose from an image using a neural network pretrained on the MPII dataset [24], in our case AlphaPose [7, 17]. The second stage lifts these 2D detections to a 3D pose represented in a learned canonical coordinate system. In a separate path it predicts the cam-era orientation to rotate the predicted 3D pose back into the camera coordinate system. Combining the 3D pose from a
ﬁrst view with the rotation predicted from a second view, re-sults in a rotated pose in the second camera coordinate sys-tem. In other words, both 3D poses in the pose coordinate system should be equal and the predicted rotations project it back into the respective camera coordinate systems. This enables the deﬁnition of a reprojection loss for each orig-inal and newly combined reprojection. For static camera setups we propose an optional reprojection loss that is com-puted by mixing relative camera rotations between samples in a training batch. Additionally, in contrast to existing self-supervised approaches, we make use of the conﬁdences that are typically provided by 2D pose estimators for each pre-dicted 2D joint by including them into the 2D input vector as well as into the reprojection loss formulation.
We evaluate our approach on the two benchmark datasets
Human3.6M [10] and MPI-INF-3DHP [24] and set the new state of the art in several metrics for self-supervised 3D pose estimation. Notably, this is without assuming any camera calibration or static cameras. Our results are competitive to the fully supervised approach of Martinez et al. [23] which sets the baseline for single image pose estimation from 2D detections. Additionally, we show results for the SkiPose
[39, 36] dataset. This dataset represents all challenges that arise when activities are captured that cannot be performed in the restricted setting of a standard motion capture system.
It consists of outdoor scenes captured on a ski slope and includes fast motions, a large capture volume and pan-tilt-zoom cameras.
The code is available on GitHub 1.
Summarizing, our contributions are:
• We present CanonPose: a self-supervised approach to train a single image 3D pose estimator from unlabeled multi-view images by mixing poses across views.
• Our approach requires no prior knowledge about the scene, 3D skeleton or camera calibration.
• The proposed method directly employs multi-view im-ages without any laborious pre-processing, such as camera calibration or multi-view geometry estimation.
• We integrate the conﬁdences from the 2D joint estima-tor into the training pipeline. 1https://github.com/bastianwandt/CanonPose
Figure 2. Network structure to learn single image 3D pose estima-tion from multi-view self-supervision. Each lifting network pre-dicts a 3D pose and a camera rotation which is used to project the 3D pose back to 2D. Both networks observe the same 3D pose from different angles. We exploit this fact by applying the cam-era rotation to the respective other pose. This projects a predicted 3D pose into the other camera and gives an additional reprojection error. At inference time only one view (gray box) is applied. 2.