Abstract
Recently, the group maximum differentiation competition (gMAD) has been used to improve blind image quality assess-ment (BIQA) models, with the help of full-reference metrics.
When applying this type of approach to troubleshoot “best-performing” BIQA models in the wild, we are faced with a practical challenge: it is highly nontrivial to obtain stronger competing models for efﬁcient failure-spotting. Inspired by recent ﬁndings that difﬁcult samples of deep models may be exposed through network pruning, we construct a set of
“self-competitors,” as random ensembles of pruned versions of the target model to be improved. Diverse failures can then be efﬁciently identiﬁed via self-gMAD competition. Next, we ﬁne-tune both the target and its pruned variants on the human-rated gMAD set. This allows all models to learn from their respective failures, preparing themselves for the next round of self-gMAD competition. Experimental results demonstrate that our method efﬁciently troubleshoots BIQA models in the wild with improved generalizability. 1.

Introduction
Over the years, researchers and engineers in the ﬁeld of image processing and computer vision have realized the im-portance of blind image quality assessment (BIQA) [42].
Numerous BIQA models [3, 30, 33, 34, 47, 48] have been proposed, focusing mainly on boosting performance on ex-isting IQA datasets of ﬁxed sizes. However, the superior correlation numbers on closed test sets may not translate in a reliable way to generalization in the open visual world
[44, 29, 36, 50]. Therefore, computational methods for prob-ing and improving the generalizablity of BIQA models are highly desirable.
In 2008, Wang and Simoncelli [44] described a maximum differentiation (MAD) competition procedure to compare
IQA models in the space of all possible images. Ma et al.
[29] proposed gMAD, a discrete instantiation of the MAD method, by restricting the search space to some speciﬁc do-main of interest. Both methods are able to automatically and efﬁciently expose failures of a relatively weak IQA model,
Best Ensemble
Best UNIQUE
Fixed UNIQUE
Fixed Ensemble
Worst Ensemble
Worst UNIQUE (a) (b)
Figure 1: Failure cases of a “top-performing” BIQA method
- UNIQUE [52] spotted by an ensemble of its pruned ver-sions. (a) Best/worst-quality images according to the en-semble, with near-identical quality reported by UNIQUE. (b) Best/worst-quality images according to UNIQUE with near-identical quality reported by the ensemble. by letting it compete with a set of strong models. Wang and
Ma [43] took advantage of gMAD to identify the counterex-amples of a BIQA model [52] using a set of stronger full-reference IQA metrics. Furthermore, they demonstrated that harnessing gMAD-selected failures signiﬁcantly improves the BIQA generalizability.
Despite demonstrated success, the progressive failure identiﬁcation and model rectiﬁcation pipeline proposed in
[43] have two drawbacks. First, it can only be applied to the synthetic distortion scenario, where full-reference IQA mod-els are computable. For BIQA models in the wild with input images containing realistic camera distortions, it is highly nontrivial to obtain a list of stronger methods to falsify a state-of-the-art model. Second, the competing full-reference models are ﬁxed throughout model development, rendering failure-spotting less effective as the target model becomes stronger [43]. 16256
In this paper, we present an innovative extension of the pipeline proposed in [43], to troubleshooting BIQA mod-els in the wild. We start with a “top-performing” BIQA method based on deep neural networks (DNNs) as the target model. Instead of ﬁnding strong external methods, the key step in our approach is to compress the target model using network pruning techniques [15, 16, 24, 27], to construct strong “self-competitors” from the target model. The critical underlying rationale takes root in the recent ﬁnding [17] in image classiﬁcation. The authors observed that, network pruning, which usually removes smallest-magnitude weights in a trained network, does not affect all learned classes or samples equally. Rather, it tends to disproportionally hamper the network memorization and generalization on the long-tailed and most difﬁcult images from the training distribution.
In other words, those images are not “memorized” well by the current model, and therefore easily “forgotten” when pruning the model. In short, network pruning can effectively spot the samples not yet well learned or represented, hence exposing the weakness of the trained model.
Inspired by this prior wisdom [17], we propose to lever-age network pruning in revealing superﬁcial “shortcuts” in (either original or pruned) BIQA models. In order to encour-age spotting diverse failures of the target model, we create ensembles of subsets of pruned models [53] to compete with the target model in gMAD [29] (see Figure 1). We then jointly ﬁne-tune the target and all pruned variants on the combination of the human-rated gMAD images and previ-ously trained data. This allows all competing models to learn from their respective failures, and prepare themselves for the next round of gMAD competition.
Our method is the ﬁrst of its kind to troubleshoot BIQA models in the wild. The ﬁne-tuned model shows improved aggressiveness and resistance [29] in gMAD, comparing with itself in previous rounds. In addition, we ﬁnd that the images in the gMAD sets exhibit increasing transferability to falsify existing BIQA models. Our code is publicly avail-able at https://github.com/wangzhihua520/ troubleshooting_BIQA. 2.