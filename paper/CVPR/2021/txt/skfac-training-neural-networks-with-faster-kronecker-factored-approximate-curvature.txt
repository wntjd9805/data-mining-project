Abstract 1.

Introduction
The bottleneck of computation burden limits the widespread use of the 2nd order optimization algorithms for training deep neural networks. In this paper, we present a computationally efﬁcient approximation for natural gradi-ent descent, named Swift Kronecker-Factored Approximate
Curvature (SKFAC), which combines Kronecker factoriza-tion and a fast low-rank matrix inversion technique. Our re-search aims at both fully connected and convolutional lay-ers. For the fully connected layers, by utilizing the low-rank property of Kronecker factors of Fisher information matrix, our method only requires inverting a small matrix to ap-proximate the curvature with desirable accuracy. For con-volutional layers, we propose a way with two strategies to save computational efforts without affecting the empirical performance by reducing across the spatial dimension or receptive ﬁelds of feature maps. Speciﬁcally, we propose two effective dimension reduction methods for this purpose:
Spatial Subsampling and Reduce Sum. Experimental results of training several deep neural networks on Cifar-10 and
ImageNet-1k datasets demonstrate that SKFAC can capture the main curvature and yield comparative performance to
K-FAC. The proposed method bridges the wall-clock time gap between the 1st and 2nd order algorithms.
*Equal Contribution
†Corresponding Author
This work was supported in part by the National Natural Science
Foundation of China under Grant 62036006 and Grant 61906146, in part by CAAI Huawei MindSpore Open Fund, and in part by the Key Research and Development Program of Shaanxi Province under Grant 2018ZDXM-GY-045.
Deep learning, whose success is inseparable from the support of enormous computing power, has achieved fruit-ful results in many ﬁelds, such as computer vision [6][21] and natural language processing [1][11]. However, for large-scale tasks, effective training of neural networks is usually time-consuming [8], resulting in a new request for fast and efﬁcient training methods.
In general, the training of neural networks is a process to optimize the network parameters θ by minimizing the reg-ularized empirical loss function L(θ), and the parameters are updated as: θ ← θ−ηG−1∇θL(θ), where G is a posi-tive deﬁnite preconditioner of the gradient and η represents a positive learning rate.
In particular, if G is an identity matrix, it will degrade to the classical 1st order optimiza-tion algorithm, namely Stochastic Gradient Descent (SGD).
It and its variants with momentum term are current main-stream in neural network training. SGD adopts consistent update step size for all parameters. As for 2nd order opti-mization, in this case, G contains more curvature or corre-lation information, such as the Hessian matrix in Newton’s method or Fisher Information Matrix (FIM) in natural gra-dient method (NG). For a network with massive parameters,
G has a gigantic size of nθ ×nθ, where nθ is the dimensions of θ, which renders it impractical to compute and invert it directly. Only if we can get huge time savings at the ex-pense of a little information integrity, 2nd order algorithms will show more potential than the 1st order ones to fasten the training.
This has motivated researches into ﬁnding the approxi-mation of G and its inverse G−1 with low computational cost. For instance, the well-known and widely used Ada-113479
grad [2], Adadelta [26], RMSProp [25], and Adam [9] in-volve a simple diagonal approximation to the covariance matrix of the gradients. Moreover, more sophisticated al-gorithms are not limited to diagonalization, but try to pre-serve more important correlations between parameters in-stead. Le Roux et al. proposed a block-diagonal approxi-mation of empirical FIM named TONGA [23], where each block corresponds to the weights of each neuron. It com-putes the inverse matrix of each block by maintaining a low rank plus diagonal term. In 2013, Ollivier et al. also pro-posed a similar block-diagonal approach [17], except that it used the standard FIM instead of the empirical one1. In ad-dition, some methods leverage the hierarchical structure of neural network to perform block-diagonal approximation, such as K-FAC [14], and methods in [7][19]. Interestingly, in addition to similar approximations, they [7][19][14] all
Kronecker-factorized the block matrices, which can further reduce the computational cost. However, for models that have thousands or even tens of thousands of neurons per layer, it is still too time-consuming to efﬁciently compute the inverse matrices of those factors, which also results in the consequence—compared with carefully tuned SGD and adaptive algorithms such as Adam—that these algorithms still have no signiﬁcant advantage in overall time cost.
We present a much faster approximation algorithm for natural gradient descent, namely Swift Kronecker-Factored
Approximate Curvature (SKFAC). Consider this case where a large batch of samples are distributed to multiple nodes, each node only processes a mini batch whose size might be smaller than the number of layers’ inputs and outputs.
The covariance matrices of activations and pre-activations are induced by the low-rank factors. Inspired by this, we propose a low-rank formulation of K-FAC by reformulating the FIM, such that the computations of preconditioners have moderate time which is sublinear in the number of layers’ inputs and outputs. For convolutional layers, additional spa-tial structures of feature maps will bring out a sharp increase in computational burden in the proposed method. There-fore, we further propose two corresponding dimensionality reduction methods for the convolutional layers. We train canonical neural networks to verify the advancement of SK-FAC on the Cifar/ImageNet datasets. The results show that
SKFAC can not only retain improved convergence proper-ties, but also tremendously improve the efﬁciency of com-puting natural gradients in large-scale networks. The con-tributions of our work can be summarized as follows:
• We attempt to reformulate the Kronecker-factored ap-proximation of natural gradient for accelerating the in-version.
• Two effective dimensionality reduction methods are 1Please refer to [13] and [10] for detailed discussions of the difference between the standard Fisher and the empirical one. proposed to address the issue of too large dimension in convolutional layers.
• We propose to only transmit the activations and pre-activation derivations in distributed training to reduce communication latency.
• The experiments of training neural networks demon-strate that our method can retain the good convergence property, like that of K-FAC. 2.