Abstract
In deep learning applications, the architectures of deep neural networks are crucial in achieving high accuracy.
Many methods have been proposed to search for high-performance neural architectures automatically. However, these searched architectures are prone to adversarial at-tacks. A small perturbation of the input data can render the architecture to change prediction outcomes signiﬁcantly. To address this problem, we propose methods to perform dif-ferentiable search of robust neural architectures.
In our methods, two differentiable metrics are deﬁned to measure architectures’ robustness, based on certiﬁed lower bound and Jacobian norm bound. Then we search for robust ar-chitectures by maximizing the robustness metrics. Different from previous approaches which aim to improve architec-tures’ robustness in an implicit way: performing adversar-ial training and injecting random noise, our methods ex-plicitly and directly maximize robustness metrics to harvest robust architectures. On CIFAR-10, ImageNet, and MNIST, we perform game-based evaluation and veriﬁcation-based evaluation on the robustness of our methods. The experi-mental results show that our methods 1) are more robust to various norm-bound attacks than several robust NAS base-lines; 2) are more accurate than baselines when there are no attacks; 3) have signiﬁcantly higher certiﬁed lower bounds than baselines. 1.

Introduction
In deep learning applications, the architectures of neu-ral models play a crucial role in improving performance.
For example, on the ImageNet [15] benchmark, the im-age classiﬁcation error is reduced from 16.4% to 3.57%, when the architecture is evolved from AlexNet [26] to
ResNet [20]. Previously, neural architectures are mostly designed by humans, which is time-consuming to obtain a highly-performant architecture. Recently, automated neural architecture search [49, 50, 36, 37, 41, 42] which develops algorithms to ﬁnd out the optimal architecture that yields the best performance on the validation datasets, has raised much attention and achieved promising results. For exam-ple, on the CIFAR-10 dataset, an automatically searched ar-chitecture [32] achieves an image classiﬁcation error rate of 2.76% while the error achieved by state-of-the-art human-designed architecture is 3.46%.
As we will show in the experiments, the architectures searched by existing methods are prone to adversarial at-tacks. A small perturbation (which is not perceivable by humans) of the input data can render the architecture to change prediction outcomes signiﬁcantly. Many ap-proaches [18, 4, 33, 12, 28] have been proposed to improve the robustness of DNNs.
In these approaches, the archi-tecture of a DNN is provided by humans, and the defense method focuses on training the weights in this architecture in a robust way. However, the robustness of a DNN is not only relevant to its weight parameters, but also determined by the architecture. It is important to search for architec-tures that are robust to adversarial attacks as well.
In this paper, we develop a novel approach for robust
NAS. We deﬁne two differentiable metrics to measure the robustness of architectures and formulate robust NAS as an optimization problem that aims to ﬁnd out an optimal ar-chitecture by maximizing the robustness metrics. The ﬁrst metric is deﬁned based on certiﬁed lower bound [2]. Linear bounding methods are applied to individual building blocks in the differentiable architecture search space and these in-dividual bounds are composed to obtain global bounds for the entire neural architecture. The second metric is based on the Jacobian norm bound [21], where the robustness is measured by how much the output shifts as the input is per-turbed. The shift is upper bounded by the norms of row vec-tors in the Jacobian matrix of the neural architecture. Our approach is applicable to various forms of differentiable ar-chitecture search methods (e.g., DARTS [32], PC-DARTS
[46], P-DARTS [9], etc. and is robust against adversarial at-tacks in various norm choices. Previously, robust NAS has been investigated in [19, 8], based on adversarial training of randomly sampled sub-architectures [19] and differen-tiable architecture variables [8]. Unlike these methods that achieve robustness implicitly via adversarial training, our method explicitly deﬁnes robustness metrics and directly 6196
optimizes these metrics to obtain robust architectures.
On CIFAR-10, ImageNet, and MNIST, we perform game-based evaluation and veriﬁcation-based evaluation on the robustness of our methods. The experimental results show that our methods 1) are more robust to various norm-bound attacks than several robust NAS baselines; 2) are more accurate than baselines when there are no attacks; 3) have signiﬁcantly higher certiﬁed lower bounds than base-lines.
The major contributions of this paper include:
• We propose a novel robust NAS method, which searches robust architectures by maximizing differen-tiable robustness metrics, deﬁned based on certiﬁed lower bound and Jacobian norm bound. Our meth-ods have strong guarantees in obtaining robust archi-tectures by explicitly and directly maximizing robust-ness measures. In contrast, previous approaches per-form implicit robustiﬁcation of architectures via adver-sarial training, which is not guaranteed to yield robust architectures. Besides, our methods can be applied to robustify any differentiable NAS methods, in a princi-pled and uniﬁed way.
• Experiments on ImageNet, CIFAR-10, and MNIST show that the architectures searched by our methods are robust to various forms of adversarial attacks and are as accurate as state-of-the-art NAS methods when there are no attacks. Our methods are consistently more robust than previous approaches against various attacks. In contrast, previous approaches are effective for certain types of attacks, but ineffective for other types.
The rest of the paper is organized as follows. Section 2 reviews related works. Section 3 and 4 present the method and experiments. Section 5 concludes the paper. 2.