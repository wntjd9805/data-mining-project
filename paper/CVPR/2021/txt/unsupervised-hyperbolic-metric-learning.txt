Abstract
Learning feature embedding directly from images with-out any human supervision is a very challenging and essen-tial task in the ﬁeld of computer vision and machine learn-ing. Following the paradigm in supervised manner, most existing unsupervised metric learning approaches mainly focus on binary similarity in Euclidean space. However, these methods cannot achieve promising performance in many practical applications, where the manual information is lacking and data exhibits non-Euclidean latent anatomy.
To address this limitation, we propose an Unsupervised Hy-perbolic Metric Learning method with Hierarchical Simi-larity. It considers the natural hierarchies of data by taking advantage of Hyperbolic metric learning and hierarchical clustering, which can effectively excavate richer similar-ity information beyond binary in modeling. More impor-tantly, we design a new loss function to capture the hier-archical similarity among samples to enhance the stability of the proposed method. Extensive experimental results on benchmark datasets demonstrate that our method achieves state-of-the-art performance compared with current unsu-pervised deep metric learning approaches. 1.

Introduction
Learning a precise distance metric for similarity mea-surement is a key ingredient of various computer vision tasks, such as face recognition [13, 50], image classiﬁcation
[5, 53, 6], and person re-identiﬁcation [52]. Therefore, met-ric learning has aroused much attention and many classical methods have been proposed in the past decades [6, 45, 13].
With the resurgence of deep neural networks, Deep Metric
Learning (DML) has emerged as a powerful tool in many practical applications [34, 30, 2, 29, 44]. It targets at seek-ing a reliable embedding space by virtue of nonlinear deep neural networks, where a well-designed metric loss func-*J.Y. and L.L. made equal contributions, C.D. is corresponding author. tion brings positive samples closer to anchors, but pushes negative samples far away from the anchors.
Most of the existing DML methods usually use large-scale data for training. They can be roughly divided into two categories: structure-learning methods and hard min-ing methods. For the former, the crucial point is to con-struct a proper loss function that plays a key role in many well-known DML methods. To this end, numbers of objec-tives [5, 34, 30, 40, 35, 29, 32, 20], including commonly-used contrastive loss [5], triplet loss [34] and lifted structure loss [30], have been reported to mine underlying similarity relationships among training data in the literature. While the second category, i.e., hard mining approaches, intends to enhance the discriminative ability of the learned embed-ding by sampling meaningful hard examples. Since training with numerous easy examples may suffer from inefﬁciency and poor performance, hard sample mining has become a prevalent technique in DML [30, 15, 12, 10, 9, 36].
However, in real-world tasks, supervised DML methods are often inapplicable since the labeled data is not avail-able. To address this issue, many unsupervised deep learn-ing algorithms have been introduced [48, 51, 17, 49], which attempt to learn the inherent structure of training data with-out using explicitly-provided labels. A common unsuper-vised DML manner mines potential sample relationship by an auxiliary algorithm such as clustering, and then utilizes the learned pair-wise information as input to perform the
DML task. For example, MOM [18] exploits a random walk process to discover the neighborhood of unlabeled data in the manifold space and the Euclidean space to excavate the pairwise information. Compared with the ground truth, the learned pairwise information usually contains label noise, which makes the DML stage unstable. Therefore, how to discover more semantic information as supervision is still a big challenge. Moreover, inspired by self-supervised learning [17, 49], TAC-CCL [24] integrates self-supervised module into the common unsupervised DML framework to boost the performance. Nonetheless, this algorithm ignores
It is not the latent metric information of unlabeled data. 12465
Figure 1. The brief description of different ways to excavate and use similarity information in CUB dataset. In conventional deep metric learning (bottom), given an anchor, we can only address positive pairs and negative pairs as supervision, and all negative pairs will get the same similarity. In fact, due to the complicated hierarchy, negative samples of the same anchor have different similarity degrees. To tackle this problem, our method (top) excavates this hierarchical information and give different negative pairs different similarity degrees. so much an unsupervised DML method as a plug-and-play self-supervised module extended to DML.
To the best of our knowledge, as shown in Figure 1, images often contain representatives of multiple classes in real-world applications. For example, suppose an anchor in
Green Jay, instance in Florida Jay and Ovenbird are both negative samples, but the anchor is more similar to the point of Florida Jay than one of Ovenbird. Because all samples in Florida Jay and Green Jay belong to the same second class Jay. Previous approaches typically relying on binary labels indicating whether the image pairs are similar or not only address a small subset of similarity relations [21]. Due to the powerful performance of deep learning with labeled data, such supervised DML methods can sometimes obtain good enough results. However, lacking explicitly-provided labels, the performance of these unsupervised DML meth-ods with binary supervision is severely degraded in facing some speciﬁc scenarios. On the other hand, most existing
DML methods prefer to use Euclidean embeddings to facil-itate calculation. However, recent research has proven that many types of data from a multitude of ﬁelds (e.g. Net-work Science and Computer Vision) exhibit a highly non-Euclidean latent anatomy [1]. In such cases, these DML methods based on Euclidean space obviously do not pro-vide the most powerful or meaningful geometrical repre-sentations of data. As a result, to improve the model perfor-mance, it is extremely important and challenging to capture the complicated structure that implicitly exists in real data.
In this work, we propose a novel unsupervised DML method, dubbed Unsupervised Hyperbolic Metric Learn-ing with Hierarchical Similarity, which can effectively ex-cavate the inherent semantic information from unlabeled data. Considering the hierarchical relations between images shown in Figure 1, we ﬁrst embed the data points from orig-inal Euclidean space into Hyperbolic space, which induces a new Hyperbolic DML framework. Speciﬁcally, we use hi-erarchical clustering to generate pseudo hierarchical labels rather than binary labels as supervision for DML task as il-lustrated in Figure 1. And then, we design a novel loss func-tion to enhance the stability of the model using the inher-ent richer similarity information discovered by hierarchical clustering. It should be noted that the proposed loss takes the similarity degrees of data pairs into account. Thus, it can well characterize the multi-level relations in the learned hyperbolic embedding space, which is suitable for dealing with triplet supervision task. Our contributions can be sum-marized as follows:
• We propose the ﬁrst hyperbolic unsupervised deep metric learning framework, which can well capture the hierarchical structure of data by conducting hierarchi-cal clustering in Hyperbolic embedding space.
• We design a new metric loss function for hierarchical relations. Unlike existing metric losses which are only interested in binary similarity, our loss aims to discover richer similarity information in unsupervised manner by taking full advantages of the learned hierarchical labels.
• Our proposed model achieves the state-of-the-art per-formances on clustering and retrieval tasks over three benchmark datasets, including CARS196, CUB-200-2011 and Stanford Online Products. 2.