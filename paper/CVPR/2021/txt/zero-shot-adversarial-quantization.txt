Abstract
Model quantization is a promising approach to compress deep neural networks and accelerate inference, making it possible to be deployed on mobile and edge devices. To retain the high performance of full-precision models, most existing quantization methods focus on ﬁne-tuning quantized model by assuming training datasets are accessible. How-ever, this assumption sometimes is not satisﬁed in real situa-tions due to data privacy and security issues, thereby mak-ing these quantization methods not applicable. To achieve zero-short model quantization without accessing training data, a tiny number of quantization methods adopt either post-training quantization or batch normalization statistics-guided data generation for ﬁne-tuning. However, both of them inevitably suffer from low performance, since the for-mer is a little too empirical and lacks training support for ultra-low precision quantization, while the latter could not fully restore the peculiarities of original data and is often low efﬁcient for diverse data generation. To address the above is-sues, we propose a zero-shot adversarial quantization (ZAQ) framework, facilitating effective discrepancy estimation and knowledge transfer from a full-precision model to its quan-tized model. This is achieved by a novel two-level discrep-ancy modeling to drive a generator to synthesize informative and diverse data examples to optimize the quantized model in an adversarial learning fashion. We conduct extensive exper-iments on three fundamental vision tasks, demonstrating the superiority of ZAQ over the strong zero-shot baselines and validating the effectiveness of its main components. Code is available at https://git.io/Jqc0y. 1.

Introduction
Although deep neural networks (DNNs), especially deep convolutional networks (DCNs), have achieved remark-able performance in a broad range of computer vision tasks [20, 40, 24, 34], their ever-growing complexities — a large number of model parameters — inhibit the appli-*Corresponding author.
Figure 1. Overview of our framework. The methods based on sample reconstruction are shown as part (a), and part (b) is the overview of our framework. BNS is short for batch normalization statistics stored in the BN layers. cations on cloud and edge devices. As a consequence, model quantization, converting high-precision parameters to low-precision ones, becomes one of the main paradigms in model compression and acceleration [10]. To mitigate the performance degradation issue due to model quantiza-tion, quantization-aware ﬁne-tuning approaches have been extensively studied to optimize quantized models on the full training datasets [41, 16, 38]. However, in real situations, original training data is sometimes inaccessible due to pri-vacy and security issues. For instance, electronic health records usually contain patients’ private information. As such, the quantization-aware ﬁne-tuning methods are no longer applicable.
Post-training quantization methods [2, 30, 47] therefore emerge to quantize weights and activations in DNNs through correction strategies, without ﬁne-tuning. However, there is a negligible gap between the strategies and the goals of target tasks, causing the quantized models to suffer from performance degradation. This issue is even ampliﬁed for the ultra-low precision situation. To address this, batch nor-malization statistics (BNS)-guided data generation is lever-aged by recent methods [4, 42]. They aim at synthesizing data samples that match the real-data statistics encoded in the batch normalization layers of full-precision deep mod-els. The synthetic data is further leveraged to ﬁne-tune the 1512
quantized models by directly optimizing on target tasks su-pervised by its full-precision model, as shown in Figure 1(a).
Although the performance of ultra-low precision models is boosted to some extent, thanks to ﬁne-tuning, data generated by batch normalization statistics is hard to fully recover the peculiarities of training data and the generation process itself is time-consuming due to data redundancy. These issues make the results still far from satisfactory.
This paper seeks to promote the development of data-free model quantization by addressing the above-mentioned is-sues. We, therefore, present a novel learning framework named Zero-shot Adversarial Quantization (ZAQ) to per-form model quantization without utilizing any sample from training data. Speciﬁcally, we devise a two-level discrepancy modeling strategy for ZAQ to measure the gap between a quantized model and its corresponding full-precision model.
We consider not only the output discrepancy from models’ top layers, just similar as existing data-free model quantiza-tion methods, but also fuses a new intermediate inter-channel discrepancy based on feature maps. A generator in ZAQ is responsible for generating informative and diverse data examples in an adversarial learning manner [15] — opti-mization based on a minimax game — to enable effective discrepancy estimation and knowledge transfer, as depicted
Figure 1(b). In addition, activation regularization is adopted to facilitate the generator to obtain examples more sensitive to the network. To sum up, our contributions are as follows:
• We propose a zero-shot adversarial quantization frame-work to support effective data generation and knowl-edge transfer. To our best knowledge, it represents the
ﬁrst effort to apply adversarial learning to data-free model quantization.
• A novel two-level discrepancy modeling strategy is devised to measure the discrepancy between a quantized model and its full-precision model, thereby guiding the training of the quantized model and generator.
• We conduct extensive experiments on image classiﬁca-tion, segmentation, and object detection tasks, showing our ZAQ framework achieves state-of-the-art results in data-free situation, works well for ultra-low precision scenarios, and is efﬁcient compared to the approaches of BNS-guided data generation for model quantization. 2.