Abstract
Neural Architecture Search (NAS) aims to automatically discover optimal architectures. In this paper, we propose an hourglass-inspired approach (HourNAS) for extremely fast NAS. It is motivated by the fact that the effects of the architecture often proceed from the vital few blocks. Act-ing like the narrow neck of an hourglass, vital blocks in the guaranteed path from the input to the output of a deep neural network restrict the information ﬂow and inﬂuence the network accuracy. The other blocks occupy the major volume of the network and determine the overall network complexity, corresponding to the bulbs of an hourglass. To achieve an extremely fast NAS while preserving the high ac-curacy, we propose to identify the vital blocks and make them the priority in the architecture search. The search space of those non-vital blocks is further shrunk to only cover the candidates that are affordable under the computational re-source constraints. Experimental results on ImageNet show that only using 3 hours (0.1 days) with one GPU, our Hour-NAS can search an architecture that achieves a 77.0% Top-1 accuracy, which outperforms the state-of-the-art methods. 1.

Introduction
In the past decade, progress in deep neural networks has resulted in the advancements in various computer vi-sion tasks, such as image classiﬁcation [6, 67, 7, 52], ob-ject detection [20, 41], and segmentation [25]. The big success of deep neural networks is mainly contributed to the well-designed cells and sophisticated architectures. For example, VGGNet [51] suggested the use of smaller con-volutional ﬁlters and stacked a series of convolution layers for achieving higher performance, ResNet [26] introduced the residual blocks to beneﬁt the training of deeper neu-ral networks, and DenseNet [29] designed the densely con-∗Corresponding author. nected blocks to stack features from different depths. Be-sides the efforts on the initial architecture design, extensive experiments [59, 24, 10] are often required to determine the weights and hyperparameters of the lightweight deep neural network [2, 11, 69, 35, 36, 37].
To automatically and efﬁciently search for neural net-works of desireable properties (e.g., model size and FLOPs) from a predeﬁned search space, a number of Neural Architec-ture Search (NAS) algorithms [40, 65, 33, 70, 68, 18, 58, 61] have been recently proposed. Wherein, Evolutionary Algo-rithm (EA) based methods [48] maintain a set of architec-tures and generate new architectures using genetic operations like mutation and crossover. Reinforcement Learning (RL) based methods [75, 76] sample architectures from the search space and train the controllers accordingly. The differen-tiable based methods [40, 62, 63, 50] optimize the shared weights and architecture parameters, which signiﬁcantly re-duces the demand for computation resources and makes the search process efﬁcient.
These methods have made tremendous efforts to greatly accelerate the search process of NAS. Nevertheless, given the huge computation cost on the large-scale dataset [62, 23, 64, 27, 3], e.g., 9 GPU days for NAS on the ImageNet benchmark, most methods execute NAS in a compromised way. The architecture is ﬁrst searched on a smaller dataset (e.g., CIFAR-10 [32]), and then the network weight of the derived architecture is trained on the large dataset. An obvi-ous disadvantage of this concession is that the performance of the selected architecture on the CIFAR-10 may not be well extended to the ImageNet benchmark [71]. We tend to use the minimal number of parameters without discrimination to construct an architecture that would achieve the maximal accuracy. But just as the popular 80-20 rule1 goes, only a few parts could be critical to the architecture’s success, and 1The 80/20 rule (a.k.a Pareto principle, the law of vital few) is an aphorism that states, for many events, roughly 80% of the effects come from 20% of the causes. 10896
Figure 1. Blocks in the residual network are either “vital” or “non-vital”, and they form the neck or bulb parts in the hourglass network.
Two-stage search scheme speed up architecture search by 9× and the resource constrained search further accelerates architecture search by 72×. we need to give them the most focus while balancing the parameter volume for other parts.
In this paper, we propose HourNAS for an accurate and efﬁcient architecture search on the large-scale ImageNet dataset. Blocks in an architecture are not created equally.
Given all the possible paths for the information ﬂow from the input to the output of the network, blocks shared by these paths are vital, just as the neck of an hourglass to regulate the
ﬂow. We identify these vital blocks and make them the prior-ity in the architecture search. The other non-vital blocks may not be critical to the accuracy of the architecture, but they often occupy the major volume of the architecture (like the bulb of an hourglass) and will carve up the resource budget left by the vital blocks. Instead of directly working in a large search space ﬂooding with architectures that obviously do not comply with the constraints during deployment, we de-velop a space proposal method to screen out and optimize the most promising architecture candidates under the constraints.
By treating the architecture search through an hourglass lens, the limited computation resource can be well allocated across vital and non-vital blocks. We design toy experiments on residual networks to illustrate the varied inﬂuence of vital and non-vital blocks on the network accuracy. The resulting
HourNAS costs only about 3 hours (0.1 GPU days) on the entire ImageNet dataset to achieve a 77.0% Top-1 accuracy, which outperforms the state-of-the-art methods. 2.