Abstract
Deep denoiser, the deep network for denoising, has been the focus of the recent development on image denoising. In the last few years, there is an increasing interest in devel-oping unsupervised deep denoisers which only call unorga-nized noisy images without ground truth for training. Nev-ertheless, the performance of these unsupervised deep de-noisers is not competitive to their supervised counterparts.
Aiming at developing a more powerful unsupervised deep denoiser, this paper proposed a data augmentation tech-nique, called recorrupted-to-recorrupted (R2R), to address the overﬁtting caused by the absence of truth images. For each noisy image, we showed that the cost function de-ﬁned on the noisy/noisy image pairs constructed by the R2R method is statistically equivalent to its supervised counter-part deﬁned on the noisy/truth image pairs. Extensive ex-periments showed that the proposed R2R method noticeably outperformed existing unsupervised deep denoisers, and is competitive to representative supervised deep denoisers. 1.

Introduction
Image denoising is one fundamental problem in im-age processing which receives an enduring interest in last decades.
It aims at removing random noise from the in-put images to improve their signal-to-noise-ratios (SNRs).
Image denoising is not only an important problem itself but also serves as a basic module in many image recovery meth-ods. A noisy image is usually formulated as y = x + n, (1) where y denotes the noisy image, x the noise-free image for recovery, and n measurement noise. The noise n is often assumed to be the instance drawn from some distribution.
In recent years, deep learning is the main driving force in the development of image denoisers. A majority of ex-isting deep-learning-based denoisers (e.g. [31, 36, 37]) are supervised, which learn the mapping from the noisy input to its clean counterpart by training a deep neural network (DNN) on many clean/noisy image pairs. However, in or-der to have a trained model that generalizes well, a large number of such noisy/clean image pairs are needed to suf-ﬁciently cover the variations on image content and mea-surement noise. Fulﬁlling such a demanding requirement on training samples may be costly and sometimes chal-lenging. For example, it is non-trivial to collect real-world noisy/clean image pairs; see e.g. [25, 33, 3]. For scientiﬁc images and medical images, the task is more challenging.
Recently, it is receiving an increasing interest on relax-ing the prerequisite of supervised learning on training sam-ples. Lehtinen et al. [21] presented a weakly supervised learning method, the so-called Noise2Noise method, which directly trains the DNN on the pairs of two noisy images of the same scene. As the noise of such image pairs is indepen-dent, the expectation of the cost function of Noise2Noise is then the same as that of the supervised one deﬁned on the noisy/truth image pairs. However, collecting noisy image pairs of the same scene remains highly non-trivial as im-age alignment can be an issue, and it is not possible for the images of dynamic scenes. More recent works on un-supervised deep denoisers have been focusing on training
DNNs using a noisy image dataset without pair-wise cor-respondence, or even training DNNs only using the input noisy image itself. These methods can be categorized to two classes.
• Data augmentation methods. Noise2Void [17] and
Noise2Self [5] adopt the blind-spot strategy to avoid overﬁtting (convergence to identity map) when train-ing a DNN to map a noisy image to itself, while
Noiser2Noise [23] and Noise-as-Clean [32] add addi-tional noise to the original noisy image to generate im-age pairs which are then used to train the DNN.
• Regularized denoising DNN. The Stein’s Unbiased Risk
Estimator (SURE) [29, 22] regularizes the DNN by pe-nalizing the divergence of the prediction. Deep image prior [30] uses early-stopping to avoid the overﬁtting. In
Self2Self [26], a dropout-based training/testing scheme is introduced to reduce the bias and variance of the pre-2043
diction from the DNN trained on a single noisy image.
Indeed, we have 1.1. Motivation
Despite the great progress in last few years, the per-formance of unsupervised learning methods for denoising is still not comparable to that of their supervised coun-terparts, e.g. DnCNN [36] trained on noisy/clean pairs or Noise2Noise trained on noisy/noisy pairs.
Indeed, many of them cannot compete well against classical non-local denoising methods such as BM3D [11].
So far,
SURE [29] provided the state-of-the-art (SOTA) perfor-mance among dataset-based unsupervised denoisers, and
Self2Self [26] provided the SOTA performance among single-image-based unsupervised denoisers. In summary,
• Unsupervised learning has its value in many real-world applications, since it remains useful when no ground-truth image is available.
• Most existing unsupervised learning methods have a no-ticeable performance gap to their supervised counter-parts, especially for denoising real-world images.
This paper aims at developing an unsupervised learning method for denoising that works on a set of unorganized noisy images without truth images. The proposed method not only provides the SOTA performance among existing unsupervised learning methods, but also is very competitive to many supervised learning methods including DnCNN. 1.2. Main Idea
Revisiting Noise2Noise.
Before proceeding, we take a revisit to Noise2Noise, the ﬁrst attempt that relaxes the requirement of supervised denoising methods on training dataset: from noisy/clean image pairs to noisy/noisy image pairs. It is shown in [21] that the performance of a denoising network trained on noisy/noisy image pairs is roughly the same as that trained on noisy/clean image pairs of the same scene. Mathematically speaking, in the setting of additive white Gaussian noise (AWGN), a pair of noisy images of the same scene can be expressed as n ∼ N (0, σ2 y = x + n, y′ = x + n′, n′ ∼ N (0, σ2 1I), 2I).
En,n′ {kFθ(y) − y′k2 2}
=En,n′ {kFθ(y) − x − n′k2 2}
=En,n′ {kFθ(y) − xk2
=En,n′ {kFθ(y) − xk2 2 − 2(n′)⊤(Fθ(y) − x) + (n′)⊤n′} 2} − 2En,n′ {(n′)⊤Fθ(y)} + const.
As long as the noise n and n′ are independent, which gives
En,n′ {(n′)⊤Fθ(y)} = 0, the expectation of the loss func-tion deﬁned on (y, y′) will be equivalent to the supervised one deﬁned on (y, x) up to a constant. This is the reason why Noise2Noise can perform comparably to its supervised counterparts.
Re-corrupting both the input image and target image for training on unorganized noisy images. Different from the dataset required by Noise2Noise, we only assume the availability of a set of unorganized noisy images without pairwise correspondence. In order to achieve comparable performance to Noise2Noise, the question is then about how y) with independent to construct a pair of noisy images ( noise from a single noisy image y = x + n such that y,
E{kFθ( y) − yk2 2} = E{kFθ( e b y) − xk2 2} + const.
In the setting of AWGN: n ∼ N (0, σ2I), our answer to the above question is to recorrupt the noisy image y as follows: b b e y = y + D⊤z, y = y − D−1z, z ∼ N (0, σ2I), (4) b e where D can be any invertible matrix. We showed in Corol-y are indepen-lary 2 (Section 3) that the noise in dent from each other, and thus the squared-ℓ2 loss function trained on the image pair ( y) satisﬁes y and y, e b
En,z{kFθ( y) − yk2 e 2} = Ebn{kFθ(x + 2} + const, (5) n = n + D⊤z. Consider a dataset of un-organized n) − xk2 b b e b where noisy images b yk = xk + nk, xk ∼ X , nk ∼ N (0, σ2I), k ∈ N. yk)}k∈N con-The cost function deﬁned on the pairs {( structed by (4) is then equivalent to the following cost func-tion: yk, e b
Ex,bn{kFθ(x + n) − xk2 2} + const,
Let Fθ(·) denote the denoising DNN. Then, Noise2Noise trains the DNN by minimizing the squared-ℓ2 loss:
En,n′ {kFθ(y) − y′k2 2}. (2)
Such a loss function is closely related to the one used in supervised learning:
En{kFθ(y) − xk2 2}. (3) i.e., the one used in the supervised learning on a set of b n, xk)}k∈N. noisy/truth image pairs {(xk +
Discussion.
From (5), it can be seen that the proposed b y) leads to a loss function y, scheme (4) of the image pair ( in the same form as that of Noise2Noise. Therefore, the network trained using the proposed scheme can be expected to have comparable performance to those supervised learn-ing methods. Through this paper, the training scheme (5) e b 2044
built on the construction scheme of image pair (4) is called
Recorrupted-to-Recorrupted, abbreviated as R2R.
Moreover, the proposed R2R scheme also works for the noise which is signal-dependent. Suppose the noise follows a normal distribution N (0, Σx) with the x-dependent co-variance matrix Σx. Then, one only needs to modify the recorruption scheme as follows: y = y+
ΣxD⊤z, y = y−
ΣxD−1z, z ∼ N (0, I). p p b
Note that since the covariance matrix Σx is positive deﬁ-nite, its square root matrix is well deﬁned and satisﬁes e
⊤
Σx
=
Σx,
Σx
Σx = Σx. p p
The modiﬁed scheme above still leads to the same result as (5); see Section 3 for more details. p p 1.3. Contributions
In this paper, we proposed an unsupervised deep learn-ing method for image denoising, named as R2R, which is trained on a dataset of un-organized noisy images, without truth or pair-wise correspondence. The contributions are summarized as follows:
• With rigorous mathematical treatment, this paper pre-sented a so-called R2R unsupervised learning technique for image denoising, which is statistically equivalent to the supervised learning on noisy/clean image pairs.
• In comparison to other unsupervised learning methods for denoising, the proposed R2R is simple and ﬂexible.
It can be trained on external training samples or directly trained on noisy images for processing.
• Extensive experiments on synthetic noisy images show that the proposed R2R method performs better than all compared non-learning and unsupervised learning methods, and is comparable to representative supervised denoisers. For denoising real-world images, it is also very competitive to the top performers among the non-learning and unsupervised learning methods. 2.