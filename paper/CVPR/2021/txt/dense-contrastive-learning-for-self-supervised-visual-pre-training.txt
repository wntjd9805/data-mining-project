Abstract
To date, most existing self-supervised learning methods are designed and optimized for image classiﬁcation. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level predic-tion and pixel-level prediction. To ﬁll this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning (DenseCL), which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images.
Compared to the baseline method MoCo-v2, our method introduces negligible computation overhead (only <1% slower), but demonstrates consistently superior perfor-mance when transferring to downstream dense prediction tasks including object detection, semantic segmentation and instance segmentation; and outperforms the state-of-the-art methods by a large margin. Speciﬁcally, over the strong
MoCo-v2 baseline, our method achieves signiﬁcant im-provements of 2.0% AP on PASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO in-stance segmentation, 3.0% mIoU on PASCAL VOC seman-tic segmentation and 1.8% mIoU on Cityscapes semantic segmentation.
Code and models are available at: https://git.io/
DenseCL 1.

Introduction
Pre-training has become a well-established paradigm in many computer vision tasks.
In a typical pre-training paradigm, models are ﬁrst pre-trained on large-scale datasets and then ﬁne-tuned on target tasks with less train-ing data. Speciﬁcally, the supervised ImageNet pre-training has been dominant for years, where the models are pre-trained to solve image classiﬁcation and transferred to downstream tasks. However, there is a gap between im-*Corresponding author. (a) Object Detection (b) Semantic Segmentation
Figure 1 – Comparisons of pre-trained models by ﬁne-tuning on object detection and semantic segmentation datasets. ‘Sup.
IN’ denotes the supervised pre-training on ImageNet. ‘COCO’ and ‘ImageNet’ indicate the pre-training models trained on (a): The object detec-COCO and ImageNet respectively. tion results of a Faster R-CNN detector ﬁne-tuned on VOC trainval07+12 for 24k iterations and evaluated on VOC test2007; (b): The semantic segmentation results of an FCN model ﬁne-tuned on VOC train aug2012 for 20k iterations and evaluated on val2012. The results are averaged over 5 independent trials. age classiﬁcation pre-training and target dense prediction tasks, such as object detection [9, 25] and semantic segmen-tation [5]. The former focuses on assigning a category to an input image, while the latter needs to perform dense classi-ﬁcation or regression over the whole image. For example, semantic segmentation aims to assign a category for each pixel, and object detection aims to predict the categories and bounding boxes for all object instances of interest. A straightforward solution would be to pre-train on dense pre-diction tasks directly. However, these tasks’ annotation is notoriously time-consuming compared to the image-level labeling, making it hard to collect data at a massive scale to pre-train a universal feature representation.
Recently, unsupervised visual pre-training has attracted much research attention, which aims to learn a proper vi-sual representation from a large set of unlabeled images. A few methods [17, 2, 3, 14] show the effectiveness in down-stream tasks, which achieve comparable or better results compared to supervised ImageNet pre-training. However, the gap between image classiﬁcation pre-training and target 3024
dense prediction tasks still exists. First, almost all recent self-supervised learning methods formulate the learning as image-level prediction using global features. They all can be thought of as classifying each image into its own version, i.e., instance discrimination [41]. Moreover, existing ap-proaches are usually evaluated and optimized on the image classiﬁcation benchmark. Nevertheless, better image clas-siﬁcation does not guarantee more accurate object detec-tion, as shown in [18]. Thus, self-supervised learning that is customized for dense prediction tasks is on demand. As for unsupervised pre-training, dense annotation is no longer needed. A clear approach would be pre-training as a dense prediction task directly, thus removing the gap between pre-training and target dense prediction tasks.
Inspired by the supervised dense prediction tasks, e.g., semantic segmentation, which performs dense per-pixel classiﬁcation, we propose dense contrastive learn-for self-supervised visual pre-training. ing (DenseCL)
DenseCL views the self-supervised learning task as a dense pairwise contrastive learning rather than the global image classiﬁcation. First, we introduce a dense projection head that takes the features from backbone networks as input and generates dense feature vectors. Our method naturally pre-serves the spatial information and constructs a dense output format, compared to the existing global projection head that applies a global pooling to the backbone features and out-puts a single, global feature vector for each image. Second, we deﬁne the positive sample of each local feature vector by extracting the correspondence across views. To construct an unsupervised objective function, we further design a dense contrastive loss, which extends the conventional InfoNCE loss [29] to a dense paradigm. With the above approaches, we perform contrastive learning densely using a fully con-volutional network (FCN) [26], similar to target dense pre-diction tasks.
Our main contributions are thus summarized as follows.
• We propose a new contrastive learning paradigm, i.e., dense contrastive learning, which performs dense pair-wise contrastive learning at the level of pixels (or local features).
• With the proposed dense contrastive learning, we de-sign a simple and effective self-supervised learning method tailored for dense prediction tasks, termed
DenseCL, which ﬁlls the gap between self-supervised pre-training and dense prediction tasks.
• DenseCL signiﬁcantly outperforms the state-of-the-art
MoCo-v2 [3] when transferring the pre-trained model to downstream dense prediction tasks, including object detection (+2.0% AP), instance segmentation (+0.9%
AP) and semantic segmentation (+3.0% mIoU), and far surpasses the supervised ImageNet pre-training. 1.1.