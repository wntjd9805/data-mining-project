Abstract
In this paper, we propose to make a systematic study on machines’ multisensory perception under attacks. We use the audio-visual event recognition task against multi-modal adversarial attacks as a proxy to investigate the ro-bustness of audio-visual learning. We attack audio, visual, and both modalities to explore whether audio-visual inte-gration still strengthens perception and how different fusion mechanisms affect the robustness of audio-visual models.
For interpreting the multimodal interactions under attacks, we learn a weakly-supervised sound source visual localiza-tion model to localize sounding regions in videos. To mit-igate multimodal attacks, we propose an audio-visual de-fense approach based on an audio-visual dissimilarity con-straint and external feature memory banks. Extensive ex-periments demonstrate that audio-visual models are sus-ceptible to multimodal adversarial attacks; audio-visual integration could decrease the model robustness rather than strengthen under multimodal attacks; even a weakly-supervised sound source visual localization model can be successfully fooled; our defense method can improve the in-vulnerability of audio-visual networks without signiﬁcantly sacriﬁcing clean model performance. The source code and pre-trained models are released in https://github. com/YapengTian/AV-Robustness-CVPR21. 1.

Introduction
Our daily perceptual experiences are speciﬁed by mul-tiple cooperated senses with multisensory integration [50].
When we are talking with a person, we can learn her/his spoken words and emotions from the seen lip movements, gestures, facial expressions, and heard speech sounds. Nu-merous psychological and cognitive studies show that the availability of sensory inputs from several modalities en-sures the robustness of the human perception system [66, 29, 75]. However, the robustness highly depends on the re-liability of multisensory inputs. For our human perception
Audio-Visual
Adversarial Attack
Violin
Guitar
Figure 1: Adding imperceptible perturbations into audio and visual inputs by an audio-visual adversarial attack, our joint perception model predicts a wrong event class: Guitar and tend to localize visual regions without the sound source. system, it might fail if certain senses are attacked. For ex-ample, the McGurk effect1 [46] indicates a perceptual illu-sion, which occurs when a speech sound is paired with the visual component of another sound, leading to the percep-tion of a third speech sound.
For computation models, our community indeed has devoted to develop data-driven approaches in lip read-ing [15, 58, 14], visually indicated sound separation [20, 25, 53, 87, 86, 81, 22], audio-visual event localization [71, 42, 77, 61, 62], audio-visual video parsing [70], audio-visual embodied navigation [9, 23], and audio-visual action recog-nition [28, 37, 78] to achieve robust auditory or visual per-ception by integrating audio and visual information. How-ever, whether these computational perception models still 1https://www.youtube.com/watch?v=2k8fHR9jKVM 5601
exhibit robustness under attacks or they are vulnerable to corrupted sensory inputs as in human perception, these have not been systematically evaluated in previous work.
Inspired by the auditory-visual illusion [46] in human perception, we present a systematic study on machines’ multisensory integration under attacks. We use the audio-visual event recognition task against multimodal adver-sarial attacks as a proxy to investigate the robustness of audio-visual learning. Adversarial examples are generated with several different attack methods for audio, visual, and both modalities to evaluate the robustness of our models.
In addition, different audio-visual fusion methods are ex-plored to validate the correlation between model robust-ness and multisensory integration. To visually interpret the audio-visual interactions under attacks, we learn a weakly-supervised sound source visual localization model to local-ize sounding regions in videos. To mitigate the adversar-ial multimodal attacks, we propose an audio-visual defense method. It uses external feature memory banks to denoise corrupted features from each modality and learns compact unimodal embeddings by enforcing audio-visual dissimilar-ity to strengthen invulnerability. For fairly evaluating differ-ent defense approaches, we propose a relative improvement (RI) metric that considers results from both clean and attack models and can penalize modality-biased defense models.
One audio-visual attack example is illustrated in Fig. 1.
Extensive experiments can validate that our audio-visual models are susceptible to adversarial perturbations, audio-visual integration could weaken model robustness rather than strengthen under multimodal attacks, even a weakly-supervised sound source visual localization model can be successfully fooled, and the proposed audio-visual defense method can improve network invulnerability without signif-icantly sacriﬁcing clean model performance.
The main contributions of our work are: (1) system-atically investigating the robustness of audio-visual event recognition models against the adversarial multimodal at-tack with different attackers and fusion methods; (2) quali-tatively interpreting the robustness over multimodal attacks in terms of the sound source spatial localization; (3) propos-ing a novel audio-visual defense method that uses clean ex-ternal feature memory banks to denoise adversarial audio and visual features and enforces the multimodal dispersion and unimodal embedding compactness to strengthen invul-nerability. (4) ﬁnding a shortcut of audio-visual defense originating from the modality bias issue and proposing a new evaluation metric: RI. 2.