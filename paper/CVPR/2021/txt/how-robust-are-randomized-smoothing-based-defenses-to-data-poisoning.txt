Abstract
Predictions of certiﬁably robust classiﬁers remain con-stant in a neighborhood of a point, making them resilient to test-time attacks with a guarantee. In this work, we present a previously unrecognized threat to robust machine learning models that highlights the importance of training-data qual-ity in achieving high certiﬁed adversarial robustness. Speciﬁ-cally, we propose a novel bilevel optimization based data poi-soning attack that degrades the robustness guarantees of cer-tiﬁably robust classiﬁers. Unlike other poisoning attacks that reduce the accuracy of the poisoned models on a small set of target points, our attack reduces the average certiﬁed radius (ACR) of an entire target class in the dataset. Moreover, our attack is effective even when the victim trains the models from scratch using state-of-the-art robust training methods such as Gaussian data augmentation[8], MACER[36], and
SmoothAdv[29] that achieve high certiﬁed adversarial ro-bustness. To make the attack harder to detect, we use clean-label poisoning points with imperceptible distortions. The effectiveness of the proposed method is evaluated by poison-ing MNIST and CIFAR10 datasets and training deep neural networks using previously mentioned training methods and certifying the robustness with randomized smoothing. The
ACR of the target class, for models trained on generated poi-son data, can be reduced by more than 30%. Moreover, the poisoned data is transferable to models trained with different training methods and models with different architectures. 1.

Introduction
Data poisoning [3, 17, 31, 32, 37] is a training-time attack where the attacker is assumed to have access to the training data on which the victim will train the model. The attacker can modify the training data in a manner that the model trained on this poisoned data performs as the attacker desires.
The data hungry nature of modern machine learning methods make them vulnerable to poisoning attacks. Attackers can place the poisoned data online and wait for it to be scraped by victims trying to increase the size for their training sets.
Another easy target for data poisoning is data collection by crowd sourcing where malicious users can corrupt the data they contribute. In most cases, an attacker can modify only certain parts of the training data such as change the features or labels for a speciﬁc class or modify a small subset of the data from all classes. In this work, we assume the attacker wants to affect the performance of the victim’s models on a target class and modiﬁes only the features of the points be-longing to that class (without affecting the labels). To evade detection, the attacker is constrained to only add impercepti-bly small perturbations to the points of the target class. Many previous works [26, 31, 16, 19, 37, 7, 18, 33] have shown the effectiveness of poisoning in affecting the accuracy of models trained on poisoned data compared to the accuracy achievable by training with clean data. In most works, the victim is assumed to use standard training by minimizing the empirical loss on the poisoned data to train the models and thus the attack is optimized to hurt the accuracy of standard training. However, recent research on test-time evasion at-tacks [5, 1, 34, 4] suggests that models trained with standard training are not robust to adversarial examples, making the assumption of victim relying on standard training to train the models for deployment questionable.
Thus, in a realistic scenario, where the aim of the vic-tim is to deploy the model, it’s better to assume that the victim will rely on training procedures that yield classiﬁers which are provably robust to test-time attacks. Several recent works have proposed methods for training certiﬁably robust models whose predictions are guaranteed to be constant in a neighbourhood of a point. However, many of these meth-ods [28, 13, 15, 35] do not scale to deep neural networks or large datasets, due to their high complexity. Moreover, the effect of training data quality on the performance of these certiﬁed defenses at test time remains largely unexplored.
Recently, randomized smoothing (RS) based certiﬁcation methods [20, 21, 8] were shown to be scalable to deep neural networks and high dimensional datasets enabling researchers to propose training procedures [29, 36] that lead to models with high certiﬁed robustness. Thus, we assume that a victim will rely on RS based certiﬁcation methods to measure the certiﬁed robustness and use RS based training procedures to train the models. The fact that a victim can train with a method that improves certiﬁed adversarial robustness is 13244
Figure 1. Overview of our poisoning against certiﬁed defenses (PACD) attack which generates poisoned data to reduce the certiﬁed robustness of the victim’s model trained with methods such as Gaussian data augmentation(GA)[8], SmoothAdv[29] and MACER[36] on a target class.
Table 1. Failure of traditional data poisoning attacks optimized against standard training in affecting the test accuracy (of target class) of models trained with certiﬁably robust training procedures.
Details of the experiment are present in Appendix D.1. Certiﬁably robust training methods [8, 29, 36] are trained with σ = 0.25 and accuracy of their base classiﬁers are reported.
Training method
Standard
GA[8]
SmoothAdv [29]
MACER [36]
Standard
GA [8]
SmoothAdv [29]
MACER [36]
T
S
I
N
M 0 1
R
A
F
I
C
Model trained on clean data
Model trained on poison data 99.28±0.01 98.99±0.14 99.18±0.23 99.21±0.56 92.71±1.31 88.84±2.39 79.48±2.69 87.12±1.17 60.08±12.6 98.31±1.65 99.31±0.29 98.31±0.58 0.36±0.37 88.38±2.13 74.95±3.45 88.54±4.52 an immediate challenge for current poisoning attacks which optimize the poison data to affect the accuracy of models trained with standard training. Table 1 shows that poisons optimized against standard training can signiﬁcantly reduce the accuracy of the victim’s model (left to right) when the victim also uses standard training (1st and 5th row). How-ever, this poison data is rendered ineffective when the victim uses a certiﬁably robust training method such as [8, 29, 36].
Are certiﬁed defenses robust to data poisoning? We study this question and demonstrate that data poisoning is a seri-ous concern even for certiﬁed defenses. We propose a novel data poisoning attack that can signiﬁcantly compromise the certiﬁed robustness guarantees achievable from training with robust training procedures. We formulate the Poisoning
Against Certiﬁed Defenses (PACD) attack as a constrained bilevel optimization problem and theoretically analyze its solution for the case when the victim uses linear classiﬁers.
Our theoretical analysis and empirical results suggests that the decision boundary of the smoothed classiﬁers (used for
RS) learned from the poisoned data is signiﬁcantly different from the one learned from clean data there by causing a reduction in certiﬁed radius. Our bilevel optimization based attack formulation is general since it can generate poisoned data against a model trained with any certiﬁably robust train-ing method (lower-level problem) and certiﬁed with any certiﬁcation procedure (upper-level problem). Fig. 1 shows the overview of the proposed PACD attack.
Unlike previous poisoning attacks that aim to reduce the accuracy of the models on a small subset of data, our at-tack can reduce the certiﬁed radius of an entire target class.
The poison points generated by our attack have clean la-bels and imperceptible distortion making them difﬁcult to detect. The poison data remains effective when the victim trains the models from scratch or uses data augmentation or weight regularization during training. Moreover, the attack points generated against a certiﬁed defense are transferable to models trained with other RS based certiﬁed defenses and to models with different architectures. This highlights the importance of training-data quality and curation for obtain-ing meaningful gains from certiﬁed defenses at test time, a factor not considered by current certiﬁed defense research.
Our main contributions are as follows
• We study the problem of using data poisoning attacks to affect the robustness guarantees of classiﬁers trained using certiﬁed defense methods. To the best of our knowledge, this is the ﬁrst clean label poisoning attack that signiﬁcantly reduces the certiﬁed robustness guar-antees of the models trained on the poisoned dataset.
• We propose a bilevel optimization based attack which can generate poison data against several robust training and certiﬁcation methods. We speciﬁcally use the attack to highlight the vulnerability of randomized smoothing based certiﬁed defenses to data poisoning.
• We demonstrate the effectiveness of our attack in reduc-ing the certiﬁable robustness obtained using random-ized smoothing on models trained with state-of-the-art certiﬁed defenses [8, 29, 36]. Our attack reduces the
ACR of the target class by more than 30%. 13245
2.