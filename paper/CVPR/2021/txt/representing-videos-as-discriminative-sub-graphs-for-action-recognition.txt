Abstract
Human actions are typically of combinatorial structures or patterns, i.e., subjects, objects, plus spatio-temporal in-teractions in between. Discovering such structures is there-fore a rewarding way to reason about the dynamics of inter-actions and recognize the actions. In this paper, we intro-duce a new design of sub-graphs to represent and encode the discriminative patterns of each action in the videos.
Speciﬁcally, we present MUlti-scale Sub-graph LEarning (MUSLE) framework that novelly builds space-time graphs and clusters the graphs into compact sub-graphs on each scale with respect to the number of nodes. Technically,
MUSLE produces 3D bounding boxes, i.e., tubelets, in each video clip, as graph nodes and takes dense connectivity as graph edges between tubelets. For each action cate-gory, we execute online clustering to decompose the graph into sub-graphs on each scale through learning Gaussian
Mixture Layer and select the discriminative sub-graphs as action prototypes for recognition. Extensive experiments are conducted on both Something-Something V1 & V2 and
Kinetics-400 datasets, and superior results are reported when comparing to state-of-the-art methods. More remark-ably, our MUSLE achieves to-date the best reported accu-racy of 65.0% on Something-Something V2 validation set. 1.

Introduction
The recognition of human actions is to analyze a video and identify the actions taking place in the video. In gen-eral, an action arises from interactive motion, and involves actors, objects and functional interactions in between. This characteristics motivate us to represent the structure of a video as a spatio-temporal graph. The graph nodes corre-spond to the volumes of actor or object regions in space-time and the edges capture the interactions between the vol-umes. Then, a valid question is how to reason about such
*This work was performed at JD AI Research.
Figure 1. Two example videos of the action of “skateboarding” and
“assembling computer,” respectively. graph structure to recognize the action. The difﬁculty orig-inates from two aspects: 1) is it necessary to capitalize on the whole graph for reasoning? 2) considering the fact that the complexity of different actions is various, how to dis-cover and model the discriminative patterns for each action in a uniﬁed framework?
In an effort to answer the two questions, let’s look at the two action instances illustrated in Figure 1. In the up-per video clip of action “skateboarding,” the objects con-tain “building,” “pole,” “plants,” and “skateboard.” Never-theless, the action of “skateboarding” only involves the ac-tor (person), the object of “skateboard,” and the occurring interactions. In other words, the action is irrelevant to the other objects and only necessitates using a part of video structure (graph). As a result, we propose to mitigate this is-sue through learning discriminative sub-graphs as the proto-types of each action category rather than performing reason-ing on the whole graph. The design of such prototypes takes the advantages of clustering on sub-graphs, on one hand, of-fers greater discriminative power in recognition, and on the other, is helpful for encoding intra-class variance. Com-pared to the upper video in which the action is related to only one object “skateboard,” the lower one of action “as-sembling computer” correlates with more objects, including
“screwdriver,” “computer parts,” and “computer.” From this view, the action “assembling computer” is relatively more complex and thus its prototypical sub-graphs should include more nodes. To take the complexity of actions into account, the framework should enable ﬂexibility in the scale of sub-graphs, i.e., the number of nodes in sub-graphs, to better reﬂect the inherent properties of different actions. 3310
To consolidate the idea of modeling discriminative sub-graphs in videos for various actions, we present MUlti-scale Sub-graph LEarning (MUSLE) framework for action recognition. Speciﬁcally, we evenly divide an input video into a set of ﬁxed-length video clips, which are fed into
Tubelet Proposal Networks (TPN) to produce space-time actor/object tubelets. Technically, we leverage Faster R-CNN pre-trained on COCO dataset to initialize the region proposals in the ﬁrst frame, and then estimate the move-ment of each proposal frame by frame in each clip to ﬁ-nally link the volume of proposals across frames as tubelets.
The representation of each tubelet is the concatenation of visual features plus the coordinates of its constituent pro-posals. Our MUSLE takes all the tubelets from clips in a video as graph nodes and exploits dense connectivity as graph edges, which measure both semantic similarity and relative coordinate changes between every two nodes. Next,
MUSLE decomposes the whole graph into multiple scales of sub-graphs. Each scale corresponds to a ﬁxed number of nodes in the sub-graphs. We capitalize on one Gaussian
Mixture Layer to interpret the distribution of all the sub-graphs from an identical action on each scale and learn K
Gaussian kernels. Each kernel is regarded as the discrimi-native sub-graph or action prototype on that scale. Note that we optimize our MUSLE framework in an end-to-end man-ner. During inference, we compute the similarity between sub-graphs extracted from the test video and action proto-types across all the scales and actions, and take the class of action prototype with the highest similarity as prediction.
The main contribution of this work is the proposal of rep-resenting video structure as a space-time graph and even-tually discovering the discriminative sub-graphs for action recognition. This also leads to the elegant views of how to perform end-to-end learning of the discriminative sub-graphs, and how to nicely present the complexity of differ-ent actions in the reasoning process, which are problems not yet fully understood. We demonstrate the effective-ness of our design, i.e., MUSLE framework, on Something-Something V1&V2 and Kinetics-400 datasets, and superior performances are reported in the experiments. 2.