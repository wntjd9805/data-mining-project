Abstract
To calculate the model accuracy on a computer vision task, e.g., object recognition, we usually require a test set composing of test samples and their ground truth labels.
Whilst standard usage cases satisfy this requirement, many real-world scenarios involve unlabeled test data, render-ing common model evaluation methods infeasible. We in-vestigate this important and under-explored problem, Au-tomatic model Evaluation (AutoEval). Speciﬁcally, given a labeled training set and a classiﬁer, we aim to estimate the classiﬁcation accuracy on unlabeled test datasets. We construct a meta-dataset: a dataset comprised of datasets generated from the original images via various transforma-tions such as rotation, background substitution, foreground scaling, etc. As the classiﬁcation accuracy of the model on each sample (dataset) is known from the original dataset labels, our task can be solved via regression. Using the feature statistics to represent the distribution of a sample dataset, we can train regression models (e.g., a regression neural network) to predict model performance. Using syn-thetic meta-dataset and real-world datasets in training and testing, respectively, we report a reasonable and promising prediction of the model accuracy. We also provide insights into the application scope, limitation, and potential future direction of AutoEval. 1.

Introduction
Model evaluation is an indispensable step in almost ev-ery computer vision task. Using a test set that is un-seen during training, the goal of evaluation is to estimate a model’s (hopefully) unbiased accuracy when deployed in real-world scenarios. In most cases, we are provided with a labeled test set, allowing us to calculate the accuracy of a model by comparing the predicted labels with the ground truth labels (e.g., Fig. 1(a)). In the community, there are many well-established benchmarks (e.g., ImageNet [7] and
COCO [26]) that provide various types of evaluation met-rics. For example, top-1 error, commonly used in image classiﬁcation, indicates whether the predicted class is the same as the ground truth. There are some other metrics such as mean average precision in object detection [26] and panoptic quality [22] in panoptic segmentation.
Compared with the evaluation on these benchmarks, evaluating model performance for real-world deployment is not that straightforward. Often, real-world data follow distributions that differ from the original training distribu-tion. In this case, a model’s performance on the test sets in a benchmark may not reﬂect that achieved during deploy-ment. If we still need to have an estimation of the model’s accuracy in this scenario, we have to re-evaluate it on the real-world data. However, we often face scenarios where annotations of test samples are not provided. Furthermore, it can be very complex and expensive to manually gather la-bels. Even if acquired, these samples may only cover a very limited set of conditions, adding bias to the evaluated per-formance. For example, it is very expensive to annotate test samples for license plate recognition systems; even label is gathered for every car, it still can not capture the diversity of real-world circumstances such as lighting and weather con-dition. This raises an interesting question: can we estimate model performance on a test set without test labels?
To answer this question, this paper introduces the Auto-matic model Evaluation (AutoEval) problem. Given a clas-siﬁer trained on a training set, the goal is to estimate its accuracy on an unlabeled test set. Here, we introduce an example in Fig. 1(b). Given a digit classiﬁer trained on
MNIST [23], we want to predict the classiﬁcation accuracy on a test set without ground truths. This problem is chal-lenging, as a test set contains many images, and each image has varied and rich visual contents. However, by visually inspecting the obvious differences between test and train-ing sets, we can infer that the accuracy on the test set is low.
From this observation, we study AutoEval by consider-ing the distribution difference between training and test sets and how it effects classiﬁer accuracy. Existing literature gives us important hints. Dataset distributions can be repre-sented by ﬁrst and second-order statistics of the mean vector of output image feature representations [36, 32, 15]. For ex-ample, distribution difference can be estimated via Fr´echet
Distance (FD) [12] or maximum mean discrepancy (MMD) 15069
Figure 1. Problem illustration. Given a classiﬁer trained on a training set, we can gain a hopefully unbiased estimate of its real-world performance by evaluating it on an unseen labeled test dataset, as shown in (a). However, in many real-world deployment scenarios, we are presented with unlabeled test datasets (b), and as such are unable to evaluate our classiﬁer using common metrics. This inspired us to explore the problem of Automatic model Evaluation.
Sample
Label
Train Set
Test Set
Loss
Task
Image Classiﬁcation
Image
Sample class ground truth
Set of labeled images
Set of unseen labeled images
AutoEval
Dataset (sample set)
Accuracy of model on sample set
Set of synthetic labeled sample sets (meta set)
Set of unseen labeled real-world datasets
Class cross-entropy Predicted accuracy RMSE
Predict accuracy of model from statistics of dataset
Classify images
Table 1. Analogies between standard image classiﬁcation terms and their AutoEval equivalents. The analogy shows that the image classiﬁcation is an image-based task, while the AutoEval problem in this work is dataset-based. metric [15]. In addition, domain adaptation literature shows that a smaller distribution difference leads to higher target domain accuracy and implies that a large domain gap causes a low test accuracy [14, 37, 38].
In this work, we explicitly show that there is a very strong negative correlation between accuracy and distribu-tion difference (the Spearman’s Rank Correlation [35] is
−0.9). This observation indicates that it is feasible to es-timate classiﬁer accuracy with distribution statistics. With this, we attempt to quantitatively estimate the test accuracy by studying the underlying relationship between dataset dis-tribution and classiﬁer performance. We propose to learn this relationship via a meta-dataset (dataset of datasets). We use the terms meta set and meta-dataset interchangeably.
Unlike most existing datasets that treat each image as a sam-ple, we focus on the dataset level: in the meta-dataset, each dataset is treated as a sample, which we term “sample set”.
The analogy between standard image classiﬁcation and Au-toEval task is shown in Table 1. The sample sets should possess an appropriate number of images, exhibit a diverse spread of distributions, and in the case of image classiﬁca-tion, have the same set of classes.
It is difﬁcult to collect sufﬁcient real-world sample sets that meet the above mentioned requirements. In this work, we propose to construct the meta set by data synthesis. Ev-ery sample set in the meta set is generated from a seed set that follows the same distribution as the original training set. This is achieved via various geometric and photo-metric transformation operations on the seed set, including blur-ring, background substitution, foreground rotation, transla-tion, etc. Note that, the synthetic sample sets are fully la-beled because they are transformed versions of the seed set.
Using these labels, we can obtain the recognition accuracy of the classiﬁer on each sample set. Sample set i can thus be denoted by (fi, ai), where ai is a recognition accuracy, and fi is the vector representation of the dataset, e.g., the mean vector of image features in this dataset. With this meta set denoted as {(fi, ai)}, i = 1, ..., N , where N is the number of sample sets, we can train a regression model that takes input as the f of a sample set and returns the predicted clas-siﬁer accuracy on this set.
In conclude, the main contributions of this paper include:
• We introduce the AutoEval task, aiming to estimate the recognition accuracy of a trained classiﬁer on a test set without any human annotated label.
• We validate the feasibility of estimating classiﬁer ac-curacy from dataset-level feature statistics. With this, we propose to learn an accuracy regression model from a synthetic meta-dataset (a dataset comprised of many datasets) and obtain promising accuracy predictions for real-world test datasets. 15070
2. Automatic Model Evaluation
We are interested in predicting the recognition accuracy of a trained classiﬁer on an unlabeled test set. 2.1. Problem Deﬁnition
We ﬁrst deﬁne a labeled dataset, Dl = {(xi, yi)} where i ∈ [1, ..., M ], xi is an image, yi is its class label, and M is the number of images. Consider a source domain S, from which we sample an original training dataset Dori. We use
Dori to train a classiﬁer fθ : xi → ˆyi, which is param-eterized by θ and maps an image xi to its predicted class
ˆyi. Given Dl, we obtain its classiﬁcation accuracy by com-paring the class predictions ˆyi with the ground truths yi to obtain accuracy, 3. Methods 3.1. Formulation
Motivated by the implications in domain adaptation, we propose to address AutoEval by measuring the distribution difference between the original training set and the test set, and explicitly learning a mapping function from the distri-bution shift to the classiﬁer accuracy.
Under this consideration, we formulate AutoEval as a dataset-level regression problem. In this problem, we view a dataset as a sample, and its label is the recognition ac-curacy on the dataset itself. Suppose we have N sample sets. We denote the j-th sample set Dj as (fj, aj), where fj is some vector representation for Dj, and aj ∈ [0, 1] is the recognition accuracy of classiﬁer fθ on Dj. We aim to learn a regression model (accuracy predictor), written as, astandard = P
M i=1J ˆyi == yiK
M
, (1) aj = A(fj).
We use a standard squared loss function for this model, where J·K is an indicator function returning 1 if argument is true and 0 otherwise.
In AutoEval, given fθ and an unlabeled dataset Du =
{xi} for i ∈ [1, ..., M ], we use an accuracy predictor
A : (fθ, Du) → a, which outputs an estimated classiﬁer accuracy a ∈ [0, 1] on this test set, aauto = A(fθ, Du). (2)
Note that in image classiﬁcation, Dori and Du share the same label space. 2.2. An Intuitive Solution
We ﬁrst present an intuitive solution to the AutoEval problem, which is not learning based. This solution is mo-tivated by the pseudo labeling strategy in many vision tasks
[17, 41, 28]. The basic assumption is: if a class prediction is made with a high softmax output score, this prediction is likely to be correct. Formally, let us consider a K-way classiﬁcation problem. When feeding a test image xi to a trained classiﬁer fθ, we obtain si ∈ RK, which is the out-put of the softmax layer. The k-th entry in si characterizes the probability of xi belonging to class k. The ℓ1 norm ksik1 = 1. If the maximum entry of si is greater than a threshold τ , image xi is considered to be correctly classi-ﬁed. The accuracy predictor is written as, (4) (5)
L = 1
N
N
X j=1 ( ˆaj − aj)2, where ˆaj is the predicted accuracy of the j-th sample set
Dj, and aj is the ground truth classiﬁer accuracy of Dj.
During testing, we extract the dataset representation f u for unlabeled test set Du, and obtain estimated classiﬁcation accuracy using a = A(f u).
To learn regression models deﬁned in Eq. 4 and Eq. 5, we need to specify the design of 1) dataset representation fi, 2) regression model A, and 3) N sample sets (meta-dataset). 3.2. Regression Model and Dataset Representation
Linear regression. We ﬁrst introduce a simple linear re-gression model, alinear = Alinear(f ) = w1flinear + w0, (6) where flinear ∈ R is the representation of sample set D, and w0, w1 ∈ R are parameters of this linear regression model. Based on the intuition that the domain gap impacts classiﬁer accuracy, we deﬁne flinear as the quantiﬁed do-main gap between dataset D and the original training set
Dori. Speciﬁcally, we use the Fr´echet distance [12] to mea-sure the domain gap, and thus, 2 flinear = FD(Dori, D) = kµori − µk 2 +
T r(Σori + Σ − 2(ΣoriΣ) 1 2 ), (7) amax = Amax(fθ, Du) = P
M i=1Jmax(si) > τ K
M
, (3) where M is the number of images in Du. We will evalu-ate Amax in the experiment and show that it does not work consistently well across datasets. where µori and µ are the mean feature vectors of Dori and
D, respectively. Σori and Σ are the covariance matrices of Dori and D, respectively. They are calculated from the image features in Dori and D, which are extracted using the classiﬁer fθ trained on Dori. Other measurements of the domain gap can also be used, such as MMD [15]. 15071
Figure 2. Relationship between the distribution shift and classiﬁer accuracy on digits and natural image classiﬁcation. Each point represents a sample set of the meta set. The Spearman’s Rank Correlation (ρ) [35] between distribution shift and classiﬁer accuracy is around −0.91 on two scenarios, indicating they have a very strong negative correlation. The red straight line is ﬁt with robust linear regression [18].
Proof of concept. Given a meta set and a classiﬁer trained on the training dataset Dori from a source domain S, we study the relationship between classiﬁer’s accuracy and dis-tribution shift. In Fig. 2, we show the accuracy as a function of the distribution shift. The distribution shift is measured by Fr´echet distance (FD) with the features extracted from the trained classiﬁer. In practice, we use the activations in the penultimate of the classiﬁer as features.
In both digits and natural image classiﬁcation, we ob-serve a very strong negative correlation between accuracy and distribution shift in both digits and natural image classi-ﬁcation: the Spearman’s Rank Correlation (ρ) [35] is about
−0.91. That is, the classiﬁer tends to achieve a low accu-racy on the sample set which has a high distribution shift with training set Dori. This indicates it is feasible to learn a regression model to predict classiﬁer performance based on distribution difference between training and test sets.
Neural network regression. Besides the linear regres-sion, we also propose a neural network regression model, aneural = Aneural(fneural), which has the same formula-tion as Eq. 4. In practice, we use a simple fully connected neural network for regression. The input of the model is the dataset representation fneural, and the output is the es-timated classiﬁer accuracy aneural.
With the observation in the proof of concept, we propose to use distribution-related statistics to represent a dataset.
In this work, we use its ﬁrst-order and second-order feature statistics, i.e., mean vector and covariance matrix. More-over, we also include a 1-dim FD score as an auxiliary in-formation to the representation. Compared with linear re-gression, the neural network regression has a richer dataset representation. The dataset representation is written as, fneural = [flinear; µ; σ], where flinear ∈ R is the Fr´echet distance between D and
Dori, µ and Σ are calculate the same way as Eq. 7. Covari-ance Σ ∈ Rd×d is very high-dimensional, making training difﬁcult. Dimension reduction is thus necessary. Speciﬁ-cally, we calculate σ by taking a weighted summation of (8)
Figure 3. Visual examples of transformations. Here we show au-toContrast, rotation, translation, brightness, and color. For other used transformations, we refer readers to [6]. each row of Σ to produce a single vector, using learned col-umn speciﬁc coefﬁcients that are shared across all rows. For example, if the feature extracted from fθ is d-dim, the di-mensionality of fneural is 1 + 2d. 3.3. Constructing Training Meta dataset
Meta-datasets for training. The regression model (Eq. 4,
Eq. 5, Eq. 8) takes the dataset representation as input and outputs a classiﬁcation accuracy. To train it, we need to prepare a meta-dataset in which each sample is a dataset.
In classiﬁcation, the diversity of the samples in the train-ing set should ideally be sufﬁcient such that test scenario is represented in its distribution. In this work, we seek to create a diverse meta set that (hopefully) contains the test distributions. To construct such a meta set, we should col-lect sample sets that are 1) large in number, 2) diverse in the data distribution, and 3) have the same label space with the training set. There are very few real-world datasets that satisfy these requirements, so we resort to data synthesis.
For each classiﬁcation task (digits or natural images), we synthesize sample sets from a single seed dataset. The 15072
Figure 4. The seed set and examples of three sample sets. The seed set is from the same distribution with the original training set; they share the same classes but do not have image overlap. The sam-ple sets are generated from the seed by background replacement and image transformations. The sample sets exhibit distinct data distributions, but inherit the foreground objects from the seed, and thus are fully labeled. Many sample sets form a meta-dataset from which an accuracy regression model is trained. seed Ds is sampled from source domain S, and thus has the same distribution as Dori. Given Ds, we apply various visual transformation and obtain N different sample sets
Dj, j = 1, ..., N . Since Ds is fully labeled, these sample sets inherent the labels from Ds.
To create a sample set Dj, we adopt a two-step proce-dure: perform background change, and then image transfor-mations. In the ﬁrst step, we keep the foreground / object unchanged and replace the background. For each sample set, we randomly select an image from the COCO dataset
[26], from which we randomly crop a patch and use it as the background. The patch scale and position in that image are both random. In the second step, for the background-replaced images, we use six image transformations deﬁned in [6], including autoContrast, rotation, color, brightness, sharpness, and translation. Examples of some transforma-tions are shown in Fig. 3. For each sample set, we randomly select and combine three out of the six transformations, with the magnitude of each transformation being random on per-sample basis. As such, each sample set is generated by background replacement and a combination of three image transformations. Fig. 4 presents examples of sample sets in natural image classiﬁcation, where background replace-ment can be observed. In the supplementary materials, we present the detailed transformation parameters and more vi-sual examples of the training meta set. Note that a sample set inherits all the image labels from the seed set and is fully labeled. As such, we can calculate the recognition accuracy
Figure 5. Sample images from real-world test datasets, including
SVNH, USPS, Pascal, Caltech and ImageNet. The former two are for digit classiﬁcation, and the latter three are for natural image classiﬁcation. We predict the classiﬁer accuracy on these datasets. of classiﬁer fθ on each sample set. Sample set Dj can be denoted as (fj, aj), which is used as a training sample to optimize the regression model.
Real-world datasets for testing. This is an early attempt for the AutoEval problem. To our knowledge, we could only ﬁnd few real-world datasets that have different distri-butions but contain the same classes. To clarify the Au-toEval problem, we conduct extensive analyses with these dataset. For digits classiﬁcation, we use USPS [19] and
SVHN [30], both with 10 classes. For natural image classi-ﬁcation, we use three existing datasets, i.e., PASCAL [13],
Caltech [16], and ImageNet [7], all with 12 classes. Details of the test meta sets are provided in Sec. 4.1. 4. Experiment and Analysis 4.1. Experimental Settings
We study the AutoEval problem on two classiﬁcation tasks: digit classiﬁcation and natural image classiﬁcation.
Digit classiﬁcation. The original training set contains all the training images of MNIST. We use the testing images of MNIST as the seed to generate the training meta set. Be-cause MNIST images are binary, the foreground can be sep-arated from the background. When generating a meta set, we randomly select an image from the COCO training set, and the background of each image is replaced with a ran-dom patch of the sampled COCO image. Then, we apply three out of six image transformations to images. We gener-ate 3, 000 sample sets, of which we use 3,000 and 1,000 for the training and the validation meta set, respectively. More-over, we use two real datasets for testing, i.e., USPS [19] and SVHN [30] datasets.
Natural image classiﬁcation. We use COCO [26] training set as the original training set, and COCO validation set as the seed set to build meta set. When generating meta set for training, we use instance mask annotations of COCO vali-dation set to get foreground regions. Similar to digit clas-siﬁcation, for each sample set, we replace the background with a random patch of an image from COCO test set. We then use image transformations to introduce more visual changes. We create 1,600 sample sets from the seed set, of which we use 1,000 and 600 for the training and the val-15073
Train Set
Unseen Test Set
Ground-truth accuracy
Predicted score (τ = 0.7)
Predicted score (τ = 0.8)
Predicted score (τ = 0.9)
Linear reg.
Neural network reg.
SVHN 25.46 10.09 7.97 7.03 26.28 27.52
Digits
USPS 64.08 43.60 37.22 32.94 50.14 64.11
RMSE↓
-18.11 22.66 25.59 9.87 1.46
Pascal 86.13 88.34 84.32 78.61 83.87 87.76
Natural images
Caltech 93.40 93.28 90.78 87.71 79.77 89.39
ImageNet 88.83 94.67 86.50 81.33 83.19 91.82
RMSE↓
-1.49 2.28 6.96 8.62 3.04
Table 2. Method comparison in predicting classiﬁcation accuracy. Results on digit classiﬁcation (SVHN and USPS datasets) and natural image classiﬁcation (Pascal, Caltech, and ImageNet) are shown. We compare three methods, i.e., predicted score based (Section 2.2), linear regression and neural network regression (Section 3.2). For each dataset, we report the estimated classiﬁcation accuracy (%). For both digit and natural image classiﬁcation, RMSE (%) is reported. The original training sets are MNIST and COCO, respectively. The ground-truth recognition accuracy (%) is presented. idation meta set, respectively. In testing, we use PASCAL
[13], Caltech [16], and ImageNet [7]. For each dataset, we select images of 12 common classes, i.e., aeroplane, bike, bird, boat, bottle, bus, car, dog, horse, monitor, motorbike, and person. We reduce the “person” class to 600 images to balance the overall number of images per class.
Classiﬁer architecture. For digit classiﬁcation, we use
LeNet-5 [23] as classiﬁer. Since the all images are mapped to the RGB space, we modify the number of input channel of LeNet-5 to 3. For natural image classiﬁcation, we use the
ResNet-50 pretrained on ImageNet [7] which is adapted to the 12-way classiﬁcation.
Metrics. This paper estimates the recognition accuracy of a model on a test set. To evaluate the performance of such es-timate, we use root mean squared error (RMSE) and mean absolute error (MAE) as metrics. RMSE measures the aver-age squared difference between the estimated classiﬁer ac-curacy and ground-truth accuracy. MAE measures the aver-age magnitude of the errors. Small RMSE and MAE corre-spond to good predictions and vice versa. 4.2. Classiﬁer Accuracy Prediction
This paper introduces three possible methods to esti-mate the recognition accuracy, including the conﬁdence-based method, linear regression and neural network regres-sion. We report the estimations of these methods in Table 2.
For the predicted score based method, three thresholds(i.e.,
τ = 0.7, 0.8and 0.9 in Eq. 3) are used.
The predicted score based method is very sensitive to the threshold. Under a speciﬁc threshold (τ = 0.7), this method makes accuracy prediction on natural image datasets (RMSE=1.49%), but its prediction quality drops signiﬁcantly (from 1.49% to 6.96%) when we increase value of τ to 0.9. What is more, its performance is very poor when considering the digit classiﬁcation task. Under two values of τ , the RMSE is consistently high, i.e., 22.66% and 25.59%, respectively. Note that, it is infeasible to select the optimal threshold because 1) test labels are unavailable
Figure 6. Comparing linear regression and neural network regres-sion when test data undergo new image transformations such as
Cutout [9, 43], Shear, Equalize and ColorTemperature [6]. The transformed datasets are denoted by “-A” and “-B”. We report the absolute error (%) of predictions and the ground truth accuracy is also shown below each dataset. (-) / (+) means the predicted accu-racy is lower / higher than the ground-truth accuracy, respectively. and 2) the test domain keeps changing. Our method does not depend on such a parameter and yields much more sta-ble results. That said, it would be interesting to address this drawback in the context of AutoEval.
Regression methods achieve better predictions than pre-dicted score based method. In digit datasets, the RMSE values of linear regression and neural network regression are 9.87% and 1.46%, respectively. A similar trend can be observed in natural image datasets. Their RMSE scores are generally lower and more stable than the conﬁdence-based method. This indicates the effectiveness of learning-based methods: the distribution difference between the original training and test sets is a critical feature.
Neural network regression is generally better than lin-ear regression. As shown in Table 2, the neural network regression is more accurate than linear regression in both digit and natural image datasets. For example, RMSE of the former is 8.41% lower than the latter on digit datasets.
In fact, the RMSE of neural network regression is as small as 1.46%: the predicted classiﬁer accuracy is very close to the ground truth accuracy. 15074
Figure 7. The impact of meta set size (ﬁrst row) and sample set size (second row) on the performance of regression methods. We report the absolute errors (%) between estimated classiﬁer accuracy and the ground-truth accuracy. We observe that linear regression is relatively stable with different sample set and meta set size. In comparison, neural network needs more and large sample sets for training.
We note that linear regression is signiﬁcantly inferior to neural network regression on Caltech datasets, where lin-ear regression gives errors higher than 10%. Caltech is an interesting dataset.
Its images have relatively simple backgrounds and salient foregrounds, implying that they are
“easy” to classify. However, such simple background con-trasts signiﬁcantly with the original training set (COCO), so the FD score between Caltech and COCO is very large.
Only looking at the FD score, linear regression tends to pre-dict low accuracy on Cltech. In comparison, neural network regression considers the data statistics of Caltech, such that it can make more accurate prediction. Furthermore, the meta-dataset might already contain sample sets with such
“simple backgrounds” (large FD), and high recognition ac-curacy. Under such circumstances, the network has learned to overrule the large FD and instead resort to the “simple background” when making predictions.
Regression models robustness. To further examine the two regression methods, we perform image transformations to real datasets (ImageNet, Pascal and Caltach) and assess the performance of the two regression methods on these “edited real-world datasets”. Note that image transformations we use here are not applied in meta-dataset generation. Thus, this experiment assesses some generalization ability of the regression methods. From Fig. 6, we ﬁrst observe the ground truth accuracy on the edited datasets is lower than that on the original sets. It suggests that the image trans-formations are introducing visual differences that hinder the classiﬁer performance. The results show that the two regres-sion methods could also achieve reasonably good estimated results. For example, linear regression can make promising predictions on 6 out of 9 datasets (It has the same issue dis-cussed above on the 3 Caltech sets). Our network regression methods gives lower errors on all 9 datasets. This suggest that our network can learn from diverse and various sets of the meta-set to make accurate performance prediction. 4.3. Analysis of the Training Meta Dataset
The synthetic meta-dataset is a key component of our system, allowing us to obtain labeled samples sets in a large scale. We analyze its impact on the regression methods from two aspects, i.e., meta set size and sample set size.
Meta set size. We ﬁrst study the impact of meta set size on the regression methods. Meta set contains training sam-ples/datasets for regression models. In Fig. 7 (ﬁrst row).
We observe the results of linear regression are relatively stable with different meta set size. It can achieve good per-formance even with 50 sample sets. This is because linear regression only has two parameters (Eq. 6), which can be learned with relatively few samples [18].
In comparison, neural work cannot achieve good results when the number of sample sets is small. When provided adequate sample sets, the neural network can learn effectively from rich and diverse sample datasets and surpasses the linear regression.
Sample set size. By default, the number of images in each sample set is equal to that of seed Ds. We study the impact of sample set size on the regression methods. In the experi-ment, we set the meta set size 1000, and vary the sample set 15075
size. In Fig. 7 (second row), we observe linear regression is stable under different sample set sizes. In comparison, the neural network needs more images in each sample set for training. We think more images in each sample set makes the distribute-related representations more accurate. This is beneﬁcial for regression learning of network. 5.