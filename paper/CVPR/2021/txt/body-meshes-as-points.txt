Abstract
We consider the challenging multi-person 3D body mesh estimation task in this work. Existing methods are mostly two-stage based—one stage for person localization and the other stage for individual body mesh estimation, lead-ing to redundant pipelines with high computation cost and degraded performance for complex scenes (e.g., occluded person instances).
In this work, we present a single-stage model, Body Meshes as Points (BMP), to simplify the pipeline and lift both efﬁciency and performance. In par-ticular, BMP adopts a new method that represents multiple person instances as points in the spatial-depth space where each point is associated with one body mesh. Hinging on such representations, BMP can directly predict body meshes for multiple persons in a single stage by concurrently local-izing person instance points and estimating the correspond-ing body meshes. To better reason about depth ordering of all the persons within the same scene, BMP designs a simple yet effective inter-instance ordinal depth loss to ob-tain depth-coherent body mesh estimation. BMP also in-troduces a novel keypoint-aware augmentation to enhance model robustness to occluded person instances. Compre-hensive experiments on benchmarks Panoptic, MuPoTS-3D and 3DPW clearly demonstrate the state-of-the-art ef-ﬁciency of BMP for multi-person body mesh estimation, to-gether with outstanding accuracy. Code can be found at: https://github.com/jfzhang95/BMP. 1.

Introduction 3D human body mesh recovery aims to reconstruct the 3D full-body mesh of the person instance from images or videos. As a fundamental yet challenging task, it has been widely applied for action recognition [63], virtual try-on [41], motion retargeting [35], etc. With the recent no-table progress in single-person based full-body mesh recov-ery [24, 27, 3, 25], a more realistic and challenging set-ting has attracted increasing attention, i.e. to estimate body meshes for multiple persons from a single image.
Existing methods for multi-person mesh recovery are
Figure 1. Our single-stage solution. The proposed model repre-sents each person instance as the center point of its body. Instance localization and body mesh recovery are then directly predicted from the center point features, enabling simultaneous reconstruc-tion of multiple persons in a single stage. Best viewed in color. mainly two-stage solutions, including top-down [20] and bottom-up [69] approaches. The former ﬁrst localizes per-son instances via a person detector, based on which it then recovers the 3D meshes individually; the bottom-up ap-proach estimates person keypoints at ﬁrst, and then jointly reconstructs multiple 3D human bodies in the image via constrained optimization [69]. Though with notable ac-curacy, the above paradigms are inefﬁcient with computa-tional redundancy. For instance, the former one estimates body mesh for each person separately, and consequently the total computation cost linearly grows with the number of persons in the image, while the latter requires grouping the keypoints into corresponding persons and inferring the body meshes iteratively, leading to high computational cost.
Targeted at a more efﬁcient and compact pipeline, we consider exploring a single-stage solution. Despite the re-cent popularity and promising performance of single-stage methods on 2D keypoints estimation [43] and object detec-tion tasks [75, 59], a single-stage pipeline for multi-person mesh recovery is barely explored as it remains unclear how to effectively integrate both person localization and mesh recovery steps within a single stage. In this work, we pro-pose a new instance representation for multi-person body mesh recovery that represents multiple person instances as points in the spatial-depth space where each point is asso-ciated with one body mesh. Such an representation allows effective parallelism of person localization and body mesh recovery. Based on it, we develop a new model architec-ture that exploits shareable features for both localization and mesh recovery and thus achieve a single-stage solution.
In particular, the model has two parallel branches, one 546
for instance localization and the other for body mesh recov-ery. In the localization branch, we model each person in-stance as a single point in a 3-dimensional space, i.e. spatial (2D) and depth (1D), where each localized point (detected person) is associated with a body mesh in the body mesh branch represented by the SMPL parametric model [36].
This in turn converts the multi-person mesh recovery into a single-shot regression problem (Fig. 1). Speciﬁcally, the spatial location is represented by discrete coordinates w.r.t. regular grids over the input image. Similarly, we discretize depth into several levels to obtain the depth representa-tion. To learn better feature representation to differentiate instances at different depth, motivated by the phenomenon that a person closer to the camera tends to seem larger in the image, we adopt the feature pyramid network (FPN) [30] to extract multi-scale features and use features from the lower scales to represent the closer (and larger) instances. In this way, each instance is represented as one point, whose as-sociated features (extracted from its corresponding spatial location and FPN scale) are used to effectively estimate its body mesh. We name this Body Meshes as Points (BMP).
Applying the BMP model for estimating multi-person body mesh simultaneously faces two challenges in realis-tic scenarios: how to coherently reconstruct instances with correct depth ordering, and how to handle the common oc-clusion issue (e.g., overlapping instances and partial obser-vations). For the ﬁrst challenge, we consider explicitly us-ing the ordinal relations among all the persons in the scene to supervise the model to learn to output body meshes with correct depth order. However, obtaining such ordinal rela-tions is non-trivial for the scenes captured in the wild, since there is no 3D annotation available. Inspired by the recent success of depth estimation for human body joints [42, 73], we propose to take the depth of each person (center point) predicted by a model pre-trained on 3D datasets with depth annotations as the pseudo ordinal relation for model train-ing on the in-the-wild data, which is experimentally proved beneﬁcial to depth-coherent body mesh reconstruction.
Also, to tackle the common occlusion issue, we propose a novel keypoint-aware occlusion augmentation strategy to improve the model robustness to occluded person instances.
Different from the previous method [55] that randomly sim-ulates occlusion in images, we generate synthetic occlusion based on the position of skeleton keypoints. Such keypoint-aware occlusion explicitly forces the model to focus on body structure, making it more robust to occlusion.
Comprehensive experiments on 3D pose benchmarks
Panoptic [23], MuPoTS-3D [38] and 3DPW [64] evidently demonstrate the high efﬁciency of the proposed model.
Moreover, it achieves new state-of-the-art on Panoptic and
MuPoTS-3D datasets, and competitive performance on 3DPW dataset. Our contributions are summarized as fol-lows: 1) To our best knowledge, we are among the ﬁrst to explore the single-stage solution to multi-person mesh recovery. We introduce a new person instance representa-tion that enables simultaneous person localization and body mesh recovery for all person instances in an image within a single stage, and design a novel model architecture accord-ingly. 2) We propose a simple yet effective inter-instance ordinal relation supervision to encourage depth-coherent re-construction. 3) We propose a keypoint-aware occlusion augmentation strategy that takes body structure into con-sideration, to improve model robustness to occlusion. 2.