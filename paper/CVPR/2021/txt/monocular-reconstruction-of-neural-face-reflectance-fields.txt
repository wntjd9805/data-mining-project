Abstract
The reﬂectance ﬁeld of a face describes the reﬂectance properties responsible for complex lighting effects includ-ing diffuse, specular, inter-reﬂection and self shadowing.
Most existing methods for estimating the face reﬂectance from a monocular image assume faces to be diffuse with very few approaches adding a specular component. This still leaves out important perceptual aspects of reﬂectance such as higher-order global illumination effects and self-shadowing. We present a new neural representation for face reﬂectance where we can estimate all components of the reﬂectance responsible for the ﬁnal appearance from a monocular image. Instead of modeling each component of the reﬂectance separately using parametric models, our neural representation allows us to generate a basis set of faces in a geometric deformation-invariant space, parame-terized by the input light direction, viewpoint and face ge-ometry. We learn to reconstruct this reﬂectance ﬁeld of a face just from a monocular image, which can be used to ren-der the face from any viewpoint in any light condition. Our method is trained on a light-stage dataset, which captures 300 people illuminated with 150 light conditions from 8 viewpoints. We show that our method outperforms existing monocular reﬂectance reconstruction methods due to bet-ter capturing of physical effects, such as sub-surface scat-tering, specularities, self-shadows and other higher-order effects. 1.

Introduction
Monocular face reconstruction (i.e. dense reconstruction of 3D face geometry, reﬂectance and illumination) has ap-plications in visual effects, telepresence, portrait relighting, facial reenactment, and interactions in virtual environments.
It has been an active area of research with tremendous progress in all aspects of reconstruction, including both ge-ometry and reﬂectance [7]. Our focus is on the reconstruc-tion of the face reﬂectance, which captures the interaction between the face and scene illumination, playing a very im-portant role in perception.
In the literature, one category of methods [10, 37, 40], approximates faces as a Lamber-tian surface. Many of them use analysis-by-synthesis opti-mization to estimate the face geometry, spherical harmon-ics lighting, and diffuse face reﬂectance; the latter is a stark simpliﬁcation of the true face reﬂectance. This type of rep-4791
resentation fails to capture important specularities and sub-surface effects in face reﬂectance, which prevents truly pho-torealistic reconstruction. While some approaches [31, 2] use ambient occlusion and precomputed radiance transfer to model shadows in an inverse rendering framework, they still assume simple reﬂectance properties of the face, which limits photorealism. Another category of methods [42, 22] reconstruct diffuse and a specular face albedos from an im-age using machine learning methods. While being more complete, this still leaves out important components of the reﬂectance, such as self shadowing and other higher-order view-dependent effects and sub-surface effects.
We present the ﬁrst monocular face reconstruction algo-rithm that estimates a full face reﬂectance ﬁeld, represent-ing both view direction- and light direction-dependent re-ﬂectance properties, from a single face image. We train a
CNN that infers the face reﬂectance ﬁeld from a single im-age, and represents it as a basis set of images showing the illuminated face in a normalized space. The images, and thus the reﬂectance ﬁeld, are parameterized by the light di-rection, view direction and face geometry. This is similar to the representations used by image-based techniques for acquiring reﬂectance ﬁelds [6, 25, 33, 8]. However, the cru-cial difference to our work is that they only capture light-dependent, not view-dependent effects; they can only re-light the given input camera view. While Debevec et al. [6] can render the face from a different viewpoint, doing so re-quires an assumption of the BRDF model of the face, and ignores effects such as self-shadowing in the reﬂectance.
Our method goes signiﬁcantly further by estimating the full reﬂectance ﬁeld, including view-dependent effects. We can change both the light source and viewpoint in the image.
We do this by jointly estimating the 3D face geometry from the monocular image, and representing the basis images in the UV space [4] of the template face mesh. This also offers other advantages, such as generalization outside of the training data space. Our method is trained on a light-stage dataset, which captures 300 people illuminated with 150 point light sources one at a time, and from 8 viewpoints.
While all faces in the dataset are in a neutral expression with mouth closed, our method still generalizes to real images with general facial expression, since the training is done in the normalized expression-invariant UV space.
In summary we make the following contributions:
• A monocular method for estimating neural face re-ﬂectance ﬁelds. We show that the neural reﬂectance
ﬁeld, directly learned from real data can model com-plex real phenomena, unlike commonly used paramet-ric reﬂectance models.
• Generalization to in-the-wild images after training on a light stage dataset. This generalization is obtained by the virtue of explicit use of a canonical space invariant to head pose, identity and expressions, i.e., UV space, as well as training with data synthesized by natural en-vironment maps. 2.