Abstract (cid:52)(cid:88)(cid:72)(cid:85)(cid:92)(cid:29)(cid:3)(cid:51)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:81)(cid:3)(cid:87)(cid:75)(cid:85)(cid:82)(cid:90)(cid:76)(cid:81)(cid:74)(cid:3)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:83)(cid:82)(cid:81)(cid:74)(cid:3)(cid:69)(cid:68)(cid:79)(cid:79)(cid:86)(cid:3)(cid:76)(cid:81)(cid:87)(cid:82)(cid:3)(cid:69)(cid:72)(cid:72)(cid:85)(cid:3)(cid:73)(cid:76)(cid:79)(cid:79)(cid:72)(cid:71)(cid:3)(cid:70)(cid:88)(cid:83)(cid:86)(cid:17)
Video grounding aims to localize a moment from an untrimmed video for a given textual query. Existing ap-proaches focus more on the alignment of visual and lan-guage stimuli with various likelihood-based matching or regression strategies, i.e., P (Y |X). Consequently, these models may suffer from spurious correlations between the language and video features due to the selection bias of the dataset. 1) To uncover the causality behind the model and data, we ﬁrst propose a novel paradigm from the per-spective of the causal inference, i.e., interventional video grounding (IVG) that leverages backdoor adjustment to deconfound the selection bias based on structured causal model (SCM) and do-calculus P (Y |do(X)). Then, we present a simple yet effective method to approximate the unobserved confounder as it cannot be directly sampled from the dataset. 2) Meanwhile, we introduce a dual con-trastive learning approach (DCL) to better align the text and video by maximizing the mutual information (MI) be-tween query and video clips, and the MI between start/end frames of a target moment and the others within a video to learn more informative visual representations. Experiments on three standard benchmarks show the effectiveness of our approaches. 1.

Introduction
Video grounding [3, 24], which aims to automatically lo-cate the temporal boundaries of the target video span for a given textual description, is a challenging multimedia infor-mation retrieval task due to the ﬂexibility and complexity of text descriptions and video content. It has been widely used
*Equally contributed
†Work done during internship at StatNLP group, SUTD
‡Corresponding Authors (cid:54)(cid:87)(cid:68)(cid:85)(cid:87)(cid:3)(cid:55)(cid:76)(cid:80)(cid:72)(cid:29)(cid:3)(cid:25)(cid:20)(cid:17)(cid:23)(cid:86) (cid:11)(cid:68)(cid:12) (cid:52)(cid:88)(cid:72)(cid:85)(cid:92)(cid:20)(cid:29)(cid:3)(cid:51)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:75)(cid:82)(cid:79)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:3)(cid:89)(cid:68)(cid:70)(cid:88)(cid:88)(cid:80)(cid:17) (cid:171)(cid:3)(cid:17)(cid:17)(cid:17) (cid:52)(cid:88)(cid:72)(cid:85)(cid:92)(cid:21)(cid:29)(cid:3)(cid:51)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:73)(cid:76)(cid:91)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:3)(cid:89)(cid:68)(cid:70)(cid:88)(cid:88)(cid:80)(cid:17) (cid:40)(cid:81)(cid:71)(cid:3)(cid:55)(cid:76)(cid:80)(cid:72)(cid:29)(cid:3)(cid:25)(cid:23)(cid:17)(cid:24)(cid:86) (cid:43)(cid:76)(cid:74)(cid:75)(cid:3)(cid:38)(cid:82)(cid:85)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:55)(cid:85)(cid:68)(cid:76)(cid:81) (cid:47)(cid:82)(cid:90)(cid:3)(cid:38)(cid:82)(cid:85)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:3) (cid:80) (cid:88) (cid:88) (cid:70) (cid:68) (cid:57) (cid:16) (cid:71) (cid:82) (cid:43) (cid:79) (cid:79) (cid:81) (cid:82) (cid:76) (cid:87) (cid:68) (cid:72) (cid:85) (cid:85) (cid:82) (cid:70) (cid:3) (cid:80) (cid:88) (cid:88) (cid:70) (cid:68) (cid:57) (cid:16) (cid:91) (cid:41) (cid:76) (cid:79) (cid:81) (cid:82) (cid:76) (cid:87) (cid:68) (cid:72) (cid:85) (cid:85) (cid:82) (cid:70) (cid:11)(cid:69)(cid:12) (cid:55)(cid:85)(cid:68)(cid:76)(cid:81)
Figure 1: (a) Illustration of video grounding. (b) Spuri-ous correlations between object “people” and “vacuum” and the activity “people are holding a vacuum” in the Charades-TA [24] dataset. in many applications, such as video question answering [40] and video summarization [74]. As shown in Figure 1 (a), the query “People are shown throwing ping pong balls into beer-ﬁlled cups” involves two actions “shown” and “throw-ing”, one role “people”, and three objects “ping pong balls”,
“beer” and “cups”, which will be located in the video with a start time (61.4s) and an end time (64.5s). To retrieve the most relevant segment, a model needs to well understand the complex interactions among these actions and objects from both language and video context, and then properly align the two sides semantic information for a prediction.
Extensive works have been proposed for the aforemen-tioned challenging video grounding task, which can be mainly categorized into three groups: 1) ranking meth-ods that typically count on a two-stage propose-and-rank pipeline [24, 3, 87], or attention-based localization ap-proach [82] to ﬁnd out the target video span among can-didates with the best matching score. 2) regression meth-2765
ods [46, 86, 50] that directly predict the start and end time of the target moment to avoid the heavy computations for a large number of candidates. 3) reinforcement learning
[70, 30, 11] approaches that dynamically ﬁlter out a se-quence of video frames conditioned on the given textual query and ﬁnally outputs temporal boundaries. The above studies keep pushing the boundary of state-of-the-art perfor-mance for the moment localization, and have made it possi-ble to beneﬁt the downstream applications [40, 23, 74].
Despite the enormous success of the current neural mod-els, we argue that these approaches may suffer from the spu-rious correlations between textual and visual features due to the selection bias of the dataset. As shown in Figure 1 (b), a dataset involves many relevant training instances for some queries that contain the action description word “hold” and two objects “people” and “vacuum’. Meanwhile, there are a very limited number of queries with the action “ﬁx” and two objects “people” and “vacuum’. We can draw a conclu-sion that the tokens “people” and “vacuum’ are highly cor-related to the video moments relevant to “people are holding a vacuum”, and have low correlations to the video moments
“people are ﬁxing a vacuum”. Hence, a query that contains the description “people are ﬁxing a vacuum” may be incor-rectly located to the video segment “people are holding a vacuum” with high probability. We observe that such bi-ased distributions commonly exist in many datasets, such as Charades-STA [24] and TACoS [57].
Many methods [47, 18, 69, 90] attempt to address the above issue. However, these re-sampling [18] and re-weighting [47] methods are mainly designed for the clas-siﬁcation tasks, and it can be difﬁcult for them to be ap-plied to temporal localization. One may also consider that pre-trained knowledge is immune to the issue as they are learned from large-scale datasets. However, as highlighted by Zhang et al. [84], the pre-trained models, such as BERT
[20] and ResNET [32], may still suffer from the biased is-sue. From a very different direction, causal inference [54] based on structured causal model (SCM) [54] and potential outcomes [58] have recently shown great promise, achiev-ing state-of-the-art performance on many applications such as scene graph generation [65], data clustering [71], image classiﬁcation [84], and visual question answering [2, 56].
Despite these success, directly applying these causal meth-ods to the video grounding task may not yield good results, due to the more complex interactions for moment retrieval compared with the image-based bi-modal alignment.
To address the above problem, this paper presents a novel paradigm named interventional video grounding (IVG) based on Pear’s SCM [54] from the perspective of causal inference. Theoretically, SCM uses the graphical formalism to treat nodes as random variables and directed edges as the direct causal dependencies between variables. We borrow the idea of the backdoor adjustment and do-calculus theory
P (Y |do(X)) [54] to deconfound the aforementioned spu-rious correlations. The main challenge here is to get the unobserved confounders Z that inﬂuences both bi-modal representations and the predictions, leading to the unex-pected correlations between language and video features by only learning from the likelihood P(Y |X). Previous studies on image-based tasks treat the latent confounder as the prior knowledge or dataset distribution, and approxi-mate them with static probabilities [56, 68] of the image objects from the dataset, or the probabilities predicted by pre-trained classiﬁers [84]. We propose a simple yet effec-tive method to approximate the prior P (Z) and then ob-tain P (Y |do(X)). Meanwhile, we introduce a dual con-trastive learning method (DCL) to learn more informative visual representations by maximizing the MI between query and video clips to better align the bi-modal features, and the MI between the start/end time of the target moment and other clips in the video. With the above two key compo-nents IVG and DCL, our proposed IVG-DCL can alleviate the confounder bias and learn high-quality representations for the challenging video grounding task. Experiments on three public datasets show the effectiveness of our proposed
IVG-DCL. Speciﬁcally, our main contributions are:
• We propose IVG, a novel model for video ground-ing by introducing causal interventions P (Y |do(X)) to mitigate the spurious correlations from the dataset.
We also present a novel approximation method for the latent confounder based on SCM.
• We propose a dual contrastive learning method DCL based on MI maximization to learn more informative feature representations in an unsupervised manner.
• We conduct quantitative and qualitative analyses on three benchmark datasets and show interesting ﬁnd-ings based on our observations. 2.