Abstract
This paper studies the single image super-resolution problem using adder neural networks (AdderNets). Com-pared with convolutional neural networks, AdderNets uti-lize additions to calculate the output features thus avoid massive energy consumptions of conventional multiplica-tions. However, it is very hard to directly inherit the existing success of AdderNets on large-scale image classiﬁcation to the image super-resolution task due to the different calcu-lation paradigm. Speciﬁcally, the adder operation cannot easily learn the identity mapping, which is essential for im-age processing tasks. In addition, the functionality of high-pass ﬁlters cannot be ensured by AdderNets. To this end, we thoroughly analyze the relationship between an adder oper-ation and the identity mapping and insert shortcuts to en-hance the performance of SR models using adder networks.
Then, we develop a learnable power activation for adjust-ing the feature distribution and reﬁning details. Experi-ments conducted on several benchmark models and datasets demonstrate that, our image super-resolution models using
AdderNets can achieve comparable performance and visual quality to that of their CNN baselines with an about 2.5× reduction on the energy consumption. The codes are avail-able at: https://github.com/huawei-noah/AdderNet. 1.

Introduction
Single image super-resolution (SISR) is a typical com-puter vision task which aims at reconstructing a high-resolution (HR) image from a low-resolution (LR) image.
SISR is a very popular image signal processing task in real-world applications such as smart phones and mobile cam-eras. Due to the hardware constrains of these portable de-vices, it is necessary to develop SISR models with low com-putation cost and high visual quality.
Recently, deep convolutional neural network (CNN) has dramatically boosted the performance of SISR. The
ﬁrst super-resolution convolutional neural network (SR-∗Equal contribution
Figure 1. PSNR and energy trade-off with the SOTA SR methods on Urban100 database for ×2 scale. AdderSR networks achieve superior performance with moderate energy.
CNN) [4] contains only three convolutional layers with about 57K parameters. Then, the capacity of DCNN was ampliﬁed with the increasing of depth and width (chan-nel number), resulting in notable improvement of super-resolution. The parameters and computation cost of recent
DCNN are increased accordingly. For example, the residual dense network (RDN) [41] contains 22M parameters and re-quires about 10,192G FLOPs (ﬂoating-number operations) for processing only one image. Compared with neural net-works for visual recognition (e.g., ResNet-50 [11]), models for SISR have much higher computational complexities due to the larger feature map sizes. These massive calculations will consume much energy and reduce the enduration time of mobile devices.
In order to address the aforementioned problem, a series of approaches have been proposed to compress and accel-erate deep convolutional neural networks. The prominent compression methods such as ﬁlter pruning [9, 35, 14, 22] and knowledge distillation [12, 7, 6, 38] reduce the com-putation by narrowing or shallowing the network. On the other hand, quantization methods [3, 23, 20, 37] devote to reducing the computation complexity of multiplications while preserving the architecture of the original neural net-work. Wherein, binarization is a speciﬁc case that weights and activations in networks are represented as {+1, −1}, 15648
which can signiﬁcantly reduce the energy and memory con-sumptions. However, the binarized network often cannot maintain the accuracy of full precision network, especially for super-resolution task [23]. Recently, Chen et al. [2] pro-posed a novel AdderNet which replaces the multiplication operations by additions. Since the complexity of additions is much lower than that of multiplications, this work mo-tivates us to utilize AdderNet for constructing energy efﬁ-cient SISR models.
To maximally excavate the potential for exploiting
AdderNets to establish SISR models, we ﬁrst analyze the theoretical difﬁculties for applying the additions into SISR tasks. Speciﬁcally, input and output features in any two neighbor layers in SISR models are very close with the sim-ilar global texture and color information as shown in Fig-ure 2. However, the identify mapping cannot be learned by a one-layer adder network. Thus, we suggest to insert self-shortcuts and formulate new adder models for the SISR task. Moreover, we ﬁnd that the high-pass ﬁlter is also hard to approximate by adder units. We then develop a learnable power activation. By exploiting these two techniques, we replace the conventional convolution ﬁlters in modern SISR networks by adder ﬁlters and establish AdderSR models ac-cordingly. The effectiveness of the proposed SISR networks using additions is veriﬁed on several benchmark datasets.
We can obtain comparable performance (i.e., PSNR val-ues and visual quality) using AdderSR models with that of the CNN baselines. Meanwhile, we can reduce more than 50% of the overall energy consumptions of these neu-ral networks. Fig. 1 indicates the superior performance of
AdderSR networks with moderate energy.
The rest of this paper is organized as follows. We brieﬂy investigate the related works on neural network compres-sion and energy-efﬁcient approaches in Section 2. In Sec-tion 3, we present the motivation of using additions in SISR and establish AdderSR models. Section 4 illustrates both the quantitative and qualitative results on benchmarks and
Section 5 concludes the paper. 2.