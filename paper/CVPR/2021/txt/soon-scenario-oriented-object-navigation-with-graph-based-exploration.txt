Abstract
The ability to navigate like a human towards a language-guided target from anywhere in a 3D embodied environment is one of the ‘holy grail’ goals of intelligent robots. Most visual navigation benchmarks, however, focus on navigat-ing toward a target from a ﬁxed starting point, guided by an elaborate set of instructions that depicts step-by-step.
This approach deviates from real-world problems in which human-only describes what the object and its surrounding look like and asks the robot to start navigation from any-where. Accordingly, in this paper, we introduce a Scenario
Oriented Object Navigation (SOON) task. In this task, an agent is required to navigate from an arbitrary position in a 3D embodied environment to localize a target following a scene description. To give a promising direction to solve this task, we propose a novel graph-based exploration (GBE) method, which models the navigation state as a graph and in-troduces a novel graph-based exploration approach to learn knowledge from the graph and stabilize training by learning sub-optimal trajectories. We also propose a new large-scale benchmark named From Anywhere to Object (FAO) dataset.
To avoid target ambiguity, the descriptions in FAO provide rich semantic scene information includes: object attribute, object relationship, region description, and nearby region description. Our experiments reveal that the proposed GBE outperforms various state-of-the-arts on both FAO and R2R datasets. And the ablation studies on FAO validates the quality of the dataset. 1.

Introduction
Recent research efforts [49, 19, 17, 47, 33, 45, 32] have achieved great success in embodied navigation tasks. The agent is able to reach the target by following a variety of instructions, such as a word (e.g. object name or room name) [49, 40], a question-answer pair [11, 18], a natural
*Corresponding author. language sentence [3] or a dialogue consisting of multiple sentences [45, 55].
However, these navigation approaches are still far from real-world navigation activities. Current vision language based navigation tasks such as Vision-language Navigation (VLN) [3], Navigation from Dialog History (NDH) [45] focus on navigating to a target by a ﬁxed trajectory, guided by an elaborate set of instructions that outlines every step.
These approaches fail to consider the case in which the complex instruction provided only target description while the starting point is not ﬁxed. In real-world applications, people often do not provide detailed step-by-step instructions and expect the robot to be capable of self-exploration and autonomous decision-making. We claim that the ability to navigate towards a language-guided target from anywhere in a 3D embodied environment like human would be of great importance to an intelligent robot.
To address these problems, we propose a new task, named
Vision Situated Object Navigation (SOON), where an agent is instructed to ﬁnd a thoroughly described target object inside a house. The navigation instructions in SOON are target-oriented rather than step-by-step babysitter as in pre-vious benchmarks. There are two major features that makes our task unique: target orienting and starting independence.
A brief example of a navigation process in SOON is illus-trated in Fig. 1. Firstly, different from conventional object navigation tasks deﬁned in [49, 40], instructions in SOON play a guidance role in addition to distinguish a target ob-ject class. An instruction contains thorough descriptions to guide the agent to ﬁnd a unique object from anywhere in the house. After receiving an instruction in SOON, the agent
ﬁrst searches a larger-scale area according to the region de-scriptions in the instruction, and then gradually narrows the search space to the target area. Compared with step-by-step navigation settings [3] or object-goal navigation settings [49], this kind of coarse-to-ﬁne navigation process is more closely resembles a real-world situation. Moreover, the SOON task is starting-independent. Since the language instructions con-tain geographic region descriptions rather than trajectory 12689
I want to find a cylindrical, metallic and tall lamp, which is set in the bright living room. 
The lamp is on the cabinet which is on the  left of the television and next to the window. 
The living room is on the first floor , next to the dining room and next to the kitchen.
Living room but  not kitchen, exit.
Not this  chamber, exit. 
I find the lamp. 
I am close to area. 
Living room but not  dining room, exit.  object name object attribute object relationship target area neighbor areas
Target Area
Neighbor Areas
Figure 1: An example of the navigation process in SOON. An agent receives a complex natural language instruction consisting of multiple kinds of descriptions (left-hand side). During the agent navigates among different rooms, it searches a larger-scale area ﬁrst, then gradually narrows down the search scope according to the visual scene and the instructions. speciﬁc descriptions, they do not limit how the agent ﬁnds the target. By contrast, in step-by-step navigation tasks such as Vision Language Navigation [3] or Cooperative Vision-and-Dialog Navigation [45], any deviation from the directed path may be considered as an error [25]. We present an overall comparison between the SOON task and existing embodied navigation tasks in Tab. 1.
In this work, We propose a novel Graph-based Semantic
Exploration (GBE) method to suggest a promising direction in approaching SOON. The proposed GBE has two advan-tages compared with previous navigation works [3, 17, 47].
Firstly, GBE models the navigation process as a graph, which enables the navigation agent to obtain a comprehensive and structured understanding of observed information. It adopts graph action space to signiﬁcantly merge the multiple actions in conventional sequence-to-sequence models [3, 17, 47] into one-step decision. Merging actions reduces the num-ber of predictions in a navigation process, which makes the model training more stable. Secondly, different from other graph-based navigation models [14, 9] that use either imi-tation learning or reinforcement to learn navigation policy, the proposed GBE combines the two learning approaches and proposes a novel exploration approach to stabilize train-ing by learning from sub-optimal trajectories. In imitation learning, the agent learns to navigate step by step under the supervision of ground truth label. It causes severe overﬁt-ting problem since labeled trajectories occupy only a small proportion of the large trajectory space. In reinforcement learning, the navigation agent explores large trajectory space, and learn to maximize the discounted reward. Reinforce-ment learning leverages sub-optimal trajectories to improve the generalizability. However, the reinforcement learning is not an end-to-end optimization method, which is difﬁcult for the agent to converge and learn a robust policy. We propose to learn the optimal actions in trajectories sampled from im-perfect GBE policy to stabilize training while exploration.
Different from other RL exploration methods, the proposed exploration method is based on the semantic graph, which is dynamically built during the navigation. Thus it helps the agent to learn a robust policy while navigating based on a graph.
To investigate the SOON task, we propose a large-scale
From Anywhere to Object (FAO) benchmark. This bench-mark is built on the Matterport3D simulator, which com-prises 90 different housing environments with real image panoramas. FAO provides 4K sets of annotated instructions with 40K trajectories. As Fig. 1 (left) shows, one set of the instruction contains three sentences, including four levels of description: i) the color and shape of the object; ii) the surrounding objects along with the relationships between these objects and the target object; iii) the area in which the target object is located and the neighbour areas. Then, the average word number of the instructions is 38 (R2R is 26), and the average hop of the labeled trajectories is 9.6 (R2R is 6.0). Thus our dataset is more challenging than other tasks.
We present experimental analyses on both R2R and FAO datasets to validate the performance of the proposed GBE and the quality of FAO dataset. The proposed GBE signiﬁ-cantly outperforms previous previous VLN methods without pretraining or auxiliary tasks on R2R and SOON tasks. We further provide human performance on the test set of FAO to quantify the human-machine gap. Moreover, by ablating vision and language modals with different granularity, we validate that our FAO dataset contains rich information that enables the agent to successfully locate the target. 12690
Dataset
House3D [49]
MINOS [40]
EQA [11], IQA [18]
MARCO [31], DRIF [5]
R2R [3]
TouchDown [10]
VLNA [37], HANNA [36]
TtW [13]
CVDN [45]
REVERIE [39]
FAO (Ours)
Human
✗
✗
✗
✓
✓
✓
✗
✓
✓
✓
✓
Instruction Context
Content
Room Name
Ojbect Name
QA
Instruction
Instruction
Instruction
Dialog
Dialog
Dialog
Instruction
Instruction
Visual Context
Unamb. Real-world Temporal
✗
Dynamic
✓
Dynamic
✗
Dynamic
✗
Dynamic
✓
Dynamic
✓
Dynamic
✓
Dynamic
✓
Dynamic
✓
Dynamic
✓
Dynamic
✓
Dynamic
✓
✓
✓
✓
✓
✓
✗
✓
✗
✓
✓
Starting
Target
Independent Oriented
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✓
✓
✓
✓
✓
✓
✓
✓
Table 1: Compared with existing datasets involving embodied vision and language tasks. 2.