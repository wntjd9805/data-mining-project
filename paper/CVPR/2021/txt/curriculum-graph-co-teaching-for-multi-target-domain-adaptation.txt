Abstract
In this paper we address multi-target domain adaptation (MTDA), where given one labeled source dataset and mul-tiple unlabeled target datasets that differ in data distribu-tions, the task is to learn a robust predictor for all the tar-get domains. We identify two key aspects that can help to alleviate multiple domain-shifts in the MTDA: feature ag-gregation and curriculum learning. To this end, we pro-pose Curriculum Graph Co-Teaching (CGCT) that uses a dual classiﬁer head, with one of them being a graph con-volutional network (GCN) which aggregates features from similar samples across the domains. To prevent the clas-siﬁers from over-ﬁtting on its own noisy pseudo-labels we develop a co-teaching strategy with the dual classiﬁer head that is assisted by curriculum learning to obtain more re-liable pseudo-labels. Furthermore, when the domain la-bels are available, we propose Domain-aware Curriculum
Learning (DCL), a sequential adaptation strategy that ﬁrst adapts on the easier target domains, followed by the harder ones. We experimentally demonstrate the effectiveness of our proposed frameworks on several benchmarks and ad-vance the state-of-the-art in the MTDA by large margins (e.g. +5.6% on the DomainNet). 1.

Introduction
Deep learning models suffer from the well known draw-back of failing to generalize well when deployed in the real world. The gap in performance arises due to the differ-ence in the distributions of the training (a.k.a source) and the test (a.k.a target) data, which is popularly referred to as domain-shift [46]. Since, collecting labeled data for ev-ery new operating environment is prohibitive, a rich line of research, called Unsupervised Domain Adaptation (UDA), has evolved to tackle the task of leveraging the source data to learn a robust predictor on a desired target domain.
In the literature, UDA methods have predominantly been designed to adapt from a single source domain to a single target domain (STDA). Such methods include optimizing
*Equal contribution
†Corresponding author statistical moments [48, 26, 45, 35, 4, 5, 6, 36], adversar-ial training [11, 47, 25], generative modelling [39, 17, 23], to name a few. However, given the proliferation in unla-beled data acquisition, the need to adapt to just a single tar-get domain has lost traction in the real world scenarios. As the number of target domains grows, the number of models that need to be trained also scales linearly. For this rea-son, the research focus has very recently been steered to ad-dress a more practical scenario of adapting simultaneously to multiple target domains from a single source domain.
This adaptation setting is formally termed as Multi-target
Domain Adaptation (MTDA). The goal of the MTDA is to learn more compact representations with a single predictor that can perform well in all the target domains. Straight-forward application of the STDA methods for the MTDA may be sub-optimal due to the presence of multiple domain-shifts, thereby leading to negative transfer [54, 9]. Thus, the desideratum to align multiple data distributions makes the
MTDA considerably more challenging.
In this paper we build our framework for the MTDA piv-oted around two key concepts: feature aggregation and cur-riculum learning. Firstly, we argue that given the intrinsic nature of the task, learning robust features in a uniﬁed space is a prerequisite for attaining minimum risk across multiple target domains. For this purpose we propose to represent the source and the target samples as a graph and then lever-age Graph Convolutional Networks [19] (GCN) to aggre-gate semantic information from similar samples in a neigh-bourhood across different domains. For the GCN to be op-erative, partial relationships among the samples (nodes) in the graph must at least be known apriori in the form of class labels. However, this information is absent for the target samples. To this end, we design a co-teaching framework where we train two classiﬁers: a MLP classiﬁer and a GCN classiﬁer that provide target pseudo-labels to each other.
On the one hand, the MLP classiﬁer is utilized to make the GCN learn the pairwise similarity between two nodes in the graph. While, on the other hand, the GCN classi-ﬁer, due to its feature aggregation property, provides better pseudo-labels to assist the training of the MLP classiﬁer.
Given that co-teaching works on the assumption that differ-5351
Method
AMEAN [9]
DADA [34]
MTDA-ITA [12]
HGAN [53]
CGCT (Ours)
D-CGCT (Ours)
Domain labels
✗
✗
✓
✓
✗
✓
Feature aggregation
✗
✗
✗
✓
✓
✓
Curriculum learning
✗
✗
✗
✗
✓
✓
Co-teaching
✗
✗
✗
✗
✓
✓
Table 1. Comparison with recent the state-of-the-art MTDA methods in terms of the operating regimes. ent networks capture different aspects of learning [2], it is beneﬁcial for suppressing noisy pseudo-labels. his feature aggregation and/or co-teaching aspects are largely missing in existing MTDA methods [9, 12, 34, 53] (see Tab. 1).
Secondly, we make a crucial observation, very peculiar to the MTDA setting, i.e., during training as the network tries to adapt to multiple domain-shifts of varying degree, pseudo-labels obtained on-the-ﬂy from the network for the target samples are very noisy. Self-training the network with unreliable pseudo-labeled target data further deterio-rates the performance. To further combat the impact of noisy pseudo-labels, we propose to obtain pseudo-labels in an episodic fashion, and advocate the use of curriculum learning in the context of MTDA. In particular, when the domain labels of the target are latent, each episode or cur-riculum step consists of a ﬁxed number of training itera-tions. Fairly consistent and reliable pseudo-labels are ob-tained from the GCN classiﬁer at the end of each curricu-lum step. We call this proposed framework as Curriculum
Graph Co-Teaching (CGCT) (see Fig. 1 (a)).
Furthermore, when the domain labels of the target are available, we propose an Easy-To-Hard Domain Selection (EHDS) strategy where the feature alignment process be-gins with the target domain that is closest to the source and then gradually progresses towards the hardest one. This makes adaptation to multiple targets smoother.
In this case, each curriculum step involves adaptation with a sin-gle new target domain. The CGCT when combined with this proposed Domain-aware Curriculum Learning (DCL) (see Fig. 1 (b)) is referred to as D-CGCT. The Tab. 1 high-lights the operating regimes of our frameworks versus the state-of-the-art MTDA methods. To summarize, the contri-butions of this work are threefold:
• We propose Curriculum Graph Co-Teaching (CGCT) for MTDA that exploits the co-teaching strategy with the dual classiﬁer head, together with the curriculum learning, to learn more robust representations across multiple target domains.
• To better utilize the domain labels, we propose a
Domain-aware Curriculum Learning (DCL) strategy to make the feature alignment process smoother.
• In the MTDA setting, we outperform the state-of-the-art for several UDA benchmarks by signiﬁcant margins (including +5.6% on the large scale DomainNet [33]). 2.