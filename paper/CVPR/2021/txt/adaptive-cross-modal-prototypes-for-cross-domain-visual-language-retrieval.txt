Abstract
Visual Content (Composition of objects)
Cross-Modal Embedding
Space for retrieval
Text Description (Composition of words)
In this paper, we study the task of visual-text retrieval in the highly practical setting in which labelled visual data with paired text descriptions are available in one domain (the “source”), but only unlabelled visual data (without text descriptions) are available in the domain of interest (the “target”). We propose the ADAPTIVE CROSS-MODAL
PROTOTYPES framework which seeks to enable target do-main retrieval by learning cross-modal visual-text represen-tations while minimising both uni-modal and cross-modal distribution shift across the source and target domains.
Our approach is built upon two key ideas: ﬁrst, we en-code the inductive bias that the learned cross-modal rep-resentations should be compositional with respect to con-cepts in each modality—this is achieved through clustering pretrained uni-modal features across each domain and de-signing a careful regularisation scheme to preserve the re-sulting structure. Second, we employ mutual information maximisation between cross-modal representations in the source and target domains during learning—this provides a mechanism that preserves commonalities between the do-mains while discarding signal in each that cannot be in-ferred from the other. We showcase our approach for the task of cross-domain visual-text retrieval, outperforming ex-isting approaches for both images and videos. 1.

Introduction
Large-scale datasets of visual content paired with cor-responding text descriptions have driven recent advances in cross-modal retrieval tasks such as image-text retrieval and video-text retrieval. In the last few years, approaches trained with such data have achieved a steady and signif-icant improvement under retrieval tasks within the single-domain setting (in which training and inference take place
*Equally contributed ﬁrst and corresponding authors.
Here we can see a person wearing ski boards on his legs and skating in the is snow and also gloves, helmet, wearing goggles and we can see trees present he (a) Compositions of multiple unimodal concepts
Source Domain (MSRVTT)
Amazon Mechanical Turk
Target Domain (LSMDC)
Description Video Services
A dog is holding binoculars.
At Clinic, SOMEONE reads through paperwork on a clipboard, then brings it to a reception desk where a dark haired nurse takes it. (b) Cross domain shift
Figure 1: Three challenges of cross-domain visual lan-guage retrieval. Top (a): Retrieval systems must be capa-ble of generalising to novel compositions of multiple con-cepts, represented in both the visual and text domains; Bot-tom (b): Retrieval systems must be robust to signiﬁcant cross-domain shifts in both the visual and text distributions;
Top and bottom ((a) and (b)): Retrieval systems must ac-count for “reporting bias” (across both images and videos) in which only a subset of visual concepts are described in the corresponding text. on the same domain). However, in real-world applications, manual collection of paired visual content and text descrip-tions is a labour-intensive and time-consuming process, cre-ating a signiﬁcant barrier for the application of cross-modal retrieval methods to new domains.
In this paper, we investigate the pragmatic question of how we can best learn knowledge on the “source” domain with paired data to generalize to other “target” domains 114954
without the prohibitive cost of additional data collection.
Such a study sheds light on how well machines can un-derstand visual and textual information in their general-ity, rather than learning and exploiting with domain-speciﬁc knowledge of the pairing.
The task of transferring a model that has been learned on a labelled source domain to an unlabelled target domain is known as Unsupervised Domain Adaptation (UDA). There has been a great deal of progress in this vein for uni-modal analysis, i.e., image classiﬁcation [41], image segmenta-tion [59], text sentiment classiﬁcation [51], etc. How-ever, relatively few works have attempted UDA for cross-modal tasks involving vision and free-form natural language descriptions—the topic we study in this paper.
To prosper in the UDA setting, a visual-text retrieval model must address three challenges (shown in Fig. 1): (1) Compositionality. The model needs to encode complex semantic features with compositions of multiple visual enti-ties (multiple words) as well as their relationships (as shown in Fig 1(a), which depicts an image from MS COCO [36] with its corresponding description provided by [53]). Effec-tive retrieval on the target domain requires representations that enable novel combinations of visual entities and text which may not have been observed in the source domain. (2) Reporting bias. Retrieval requires the model to solve a challenging set-to-set cross-modal matching problem (where multiple visual entities correspond to various words contained in free-form sentences), in which information across modality is only partial matched [28]. Examples of this effect can be seen in Fig 1(a) and Fig 1(b). Even for relatively dense descriptions such as the one associated to the image in Fig 1(a), the description is not exhaustive (in this case, the ﬂag to the left of the skier is not described). (3) Visual and text domain shifts. The retrieval model must be robust to domain shift in both visual content and written descriptions. For example, consider Fig 1(b), where we ob-serve samples from strikingly different visual domains (car-toons and movies sourced from [70] and [55], resp.). In ad-dition to “visual shifts”, valid text queries can differ signiﬁ-cantly in detail and manner: while both describe videos, the description on the left describes a single interaction while the description on the right conveys an ongoing set of inter-actions between people, objects and their environment.
To tackle these challenges, we propose the ADAPTIVE
CROSS-MODAL PROTOTYPES (ACP) framework. The two key ideas underpinning this framework as follows. (1) To address the need for compositionality and achieve robust-ness to reporting bias, we propose to learn a cross-modal representation with carefully designed regularisation. Since data samples for text-video retrieval lack a natural discrete semantic class structure (unlike traditional UDA for classiﬁ-cation, in which each visual input is mapped to one or more
ﬁnite predeﬁned categories), we ﬁrst perform clustering on off-the-shelf uni-modal embeddings for visual content in the target domain and text in the source domain. We then attach prototypical networks to the cross-modal represen-tation and task them with predicting, for each sample, the assignment probability of its uni-modal embedding to each of the cluster centres for samples within the same modality.
The goal is to ensure that the relationships between cate-gories discovered by the clustering are not lost in the cross-modal representation when it is trained with paired data on the source domain.This design is inspired in part by recent works highlighting the powerful generalisation capabilities of pretrained vision models to out of distribution data [40] and the ability of large-scale language models as few-shot learners [4], suggesting that knowledge of a vast array of concepts are likely already encoded among these features in a manner that enables their composition. (2) To minimise the inﬂuence of visual and text distribution shifts across do-mains, we employ mutual information maximization [29] between the predictions of the prototypical networks on the source and target domains. This aims to preserve common-alities between the domains while discarding signal in each that cannot be inferred from the other.
The contributions of this paper are as follows: (1) We propose a new framework, ADAPTIVE CROSS-MODAL
PROTOTYPES, for cross-modal retrieval in the UDA setting by preserving semantic structure of compositional concepts from uni-modal data; (2) We demonstrate that maximising mutual information of the co-occurrence between source and target cross-modal prototype cluster assignment predic-tion is an effective mechanism to reduce domain shifts for both visual and text data; (3) Our method achieves improve-ments on three image-retrieval datasets and three video-retrieval datasets compared to a retrieval system trained only on the source domain, as well as alternative domain adaptation strategies, such as variants of maximum mean discrepancy [42], adversarial learning strategy [24] and transportation modelling [17]. 2.