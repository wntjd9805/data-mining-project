Abstract
Faces	w/	gaze	labels
'Looking	At	Each	Other	(LAEO)' collection
A major challenge for physically unconstrained gaze es-timation is acquiring training data with 3D gaze annota-tions for in-the-wild and outdoor scenarios.
In contrast, videos of human interactions in unconstrained environ-ments are abundantly available and can be much more eas-ily annotated with frame-level activity labels. In this work, we tackle the previously unexplored problem of weakly-supervised gaze estimation from videos of human interac-tions. We leverage the insight that strong gaze-related ge-ometric constraints exist when people perform the activity of “looking at each other” (LAEO). To acquire viable 3D gaze supervision from LAEO labels, we propose a training algorithm along with several novel loss functions especially designed for the task. With weak supervision from two large scale CMU-Panoptic and AVA-LAEO activity datasets, we show signiﬁcant improvements in (a) the accuracy of semi-supervised gaze estimation and (b) cross-domain gener-alization on the state-of-the-art physically unconstrained in-the-wild Gaze360 gaze estimation benchmark. We open source our code at https://github.com/NVlabs/weakly-supervised-gaze. 1.

Introduction
Much progress has been made recently in the task of remote 3D gaze estimation from monocular images, but most of these methods are constrained to largely frontal subjects viewed by cameras located within a meter of them [46, 20]. To go beyond frontal faces, a few recent works explore the more challenging problem of so-called
“physically unconstrained gaze estimation”, where larger camera-to-subject distances and higher variations in head pose and eye gaze angles are present [17, 44, 8]. A signif-icant challenge there is in acquiring training data with 3D gaze labels, generally and more so outdoors. Fortunately, several 3D gaze datasets with large camera-to-subject dis-*Rakshit Kothari was an intern at NVIDIA during the project.
Supervised	losses
LAEO	constraints
Input
Output
Weakly-supervised training
Gaze	estimation	network e c n e r e f n
I
Figure 1. Overview of our weakly-supervised gaze estimation ap-proach. We employ large collections of videos of people “looking at each other” (LAEO) curated from the Internet without any ex-plicit 3D gaze labels, either by themselves or in a semi-supervised manner to learn 3D gaze in physically unconstrained settings. tances and variability in head pose have been collected re-cently in indoor laboratory environments using specialized multi-cameras setups [43, 8, 44, 28].
In contrast, the re-cent Gaze360 dataset [17] was collected both indoors and outdoors, at greater distances to subjects. While the ap-proach of Gaze360 advances the ﬁeld signiﬁcantly, it never-theless requires expensive hardware and many co-operative subjects and hence can be difﬁcult to scale.
Recently “weakly-supervised” approaches have been demonstrated on various human perception tasks, such as body pose estimation via multi-view constraints [35, 14], hand pose estimation via bio-mechanical constraints [37], and face reconstruction via differentiable rendering [6].
Nevertheless, little attention has been paid to exploring methods with weak supervision for frontal face gaze esti-mation [42] and none at all for physically unconstrained gaze estimation. Eye gaze is a natural and strong non-verbal form of human communication [27]. For instance, babies detect and follow a caregiver’s gaze from as early as four months of age [38]. Consequently, videos of hu-9980
man interactions involving eye gaze are commonplace and are abundantly available on the Internet [10]. Thus we pose the question: “Can machines learn to estimate 3D gaze by observing videos of humans interacting with each other?”.
In this work, we tackle the previously unexplored prob-lem of weakly supervising 3D gaze learning from videos of human interactions curated from the Internet (Fig. 1).
We target the most challenging problem within this domain of physically unconstrained gaze estimation. Speciﬁcally, to learn 3D gaze we leverage the insight that strong gaze-related geometric constraints exist when people perform the commonplace interaction of “looking at each other” (LAEO), i.e., the 3D gaze vectors of the two people interact-ing are oriented in opposite directions to each other. Videos of the LAEO activity can be easily curated from the Inter-net and annotated with frame-level labels for the presence of the LAEO activity and with 2D locations of the persons performing it [26, 25]. However, estimating 3D gaze from just 2D LAEO annotations is challenging and ill-posed be-cause of the depth ambiguity of the subjects in the scene.
Furthermore, naively enforcing the geometric constraint of opposing gaze vector predictions for the two subjects per-forming LAEO is, by itself, insufﬁcient supervision to avoid degenerate solutions while learning 3D gaze.
To solve these challenges and to extract viable 3D gaze supervision from weak LAEO labels, we propose a train-ing algorithm that is especially designed for the task. We enforce several scene-level geometric 3D and 2D LAEO constraints between pairs of faces, which signiﬁcantly aid in accurately learning 3D gaze information. While train-ing, we also employ a self-training procedure and compute stronger pseudo 3D gaze labels from weak noisy estimates for pairs of faces in LAEO in an uncertainty-aware man-ner. Lastly, we employ an aleatoric gaze uncertainty loss and a symmetry loss to supervise learning. Our algorithm operates both in a purely weakly-supervised manner with
LAEO data only or in a semi-supervised manner along with limited 3D gaze-labeled data.
We evaluate the real-world efﬁcacy of our approach on the large physically unconstrained Gaze360 [17] bench-mark. We conduct various within- and cross-dataset ex-periments and obtain LAEO labels from two large-scale (a) the CMU Panoptic [16] with known 3D datasets: scene geometry and (b) the in-the-wild AVA-LAEO activity dataset [25] containing Internet videos. We show that our proposed approach can successfully learn 3D gaze infor-mation from weak LAEO labels. Furthermore, when com-bined with limited (in terms of the variability of subjects, head poses or environmental conditions) 3D gaze-labeled data in a semi-supervised setting, our approach can signiﬁ-cantly help to improve accuracy and cross-domain general-ization. Hence, our approach not only reduces the burden of acquiring data and labels for the task of physically uncon-strained gaze estimation, but also helps to generalize better for diverse/naturalistic environments.
To summarize, our key contributions are:
• We propose a novel weakly-supervised framework for learning 3D gaze from in-the-wild videos of people performing the activity of “looking at each other”. To our understanding, we are the ﬁrst to employ videos of humans interacting to supervise 3D gaze learning.
• To effectively derive 3D gaze supervision from weak
LAEO labels, we introduce several novel training ob-jectives. We learn to predict aleatoric uncertainty, use it to derive strong pseudo-3D gaze labels, and further propose geometric LAEO 3D and 2D constraints to learn gaze from LAEO labels.
• Our experiments on the Gaze360 benchmark show that
LAEO data can effectively augment data with strong 3D gaze labels both within and across datasets. 2.