Abstract
Standard methods for video recognition use large CNNs designed to capture spatio-temporal data. However, training these models requires a large amount of labeled training data, containing a wide variety of actions, scenes, settings and camera viewpoints. In this paper, we show that current convolutional neural network models are unable to recognize actions from camera viewpoints not present in their training data (i.e., unseen view action recognition). To address this, we develop approaches based on 3D representations and introduce a new geometric convolutional layer that can learn viewpoint invariant representations. Further, we introduce a new, challenging dataset for unseen view recognition and show the approaches ability to learn viewpoint invariant representations. 1.

Introduction
Activity recognition with convolutional neural networks (CNNs) has been very successful [2, 41, 13] when provided sufﬁcient diverse labeled data, like Kinetics [21]. However, one major limitation of these CNNs is that they are unable to recognize actions/data that are outside of the training data distribution. This is most notably observed for unseen classes (objects, activities, etc.) which has been heavily studied in zero-shot and few-shot learning literature. In this work, we look at a related, but different problem of unseen viewpoint activity recognition, where the actions are the same, but occur from different camera angles.
To motivate this problem, let us consider an example.
Given a labeled dataset of a person performing actions with one camera angle, we train a CNN to recognize this action.
Now, suppose we have new videos to recognize, but from a different camera view. This could be as simple as a different camera placement in the environment, or an entirely different camera and setting (e.g., Fig. 1). In this case, a trained CNN, in general, fails to recognize the action. As a simple experi-ment, we use the Human3.6M dataset [14], which contains videos of a person performing an action from 4 different camera angles. As shown in Table 1, when training on one
Figure 1: Examples of the seen, static broadcast camera in
MLB-YouTube and examples of the new, unseen viewpoints of the same actions. This dataset is quite challenging, adding new views, people, etc. view and testing on another, the model is unable to recognize the action. However, humans are able to recognize these actions regardless of viewpoint and studies have found that this is likely because humans build invariant representations of actions in their minds [39] .
Further, this problem frequently occurs in real data (e.g.,
YouTube videos). Existing smaller datasets such as Toyota
SmartHome [8], Charades-Ego [38], NTU [35] and others all provide videos in multiple viewpoints to study this effect.
Large video datasets like Kinetics [21] naturally contain many views, however, there is no annotation of the view and each video only provides a single view. Other datasets like MLB-YouTue [31] only contains the single broadcast camera view baseball games. As collecting video data is already challenging, designing CNNs that generalize to un-seen viewpoints is critical, especially for applications where diverse view data is limited or unavailable. It would be practi-cally impossible to build datasets for many desirable settings that enumerate all possible (or sufﬁciently large number of) viewpoints to fully model activities.
There are many potential ways to address this problem.
One hypothesis is that by training on a large-scale video datasets, such as Kinetics, the model could implicitly learn multi-view representations of actions. However, as shown in Table 1, we empirically ﬁnd that while it improves per-14124
Method
Random 2D ResNet-50 3D ResNet-50 3D ResNet-50 + Kinetics
Ground Truth 3D Pose
Seen
Unseen 9.1% 9.1% 86.4% 9.1% 100% 9.1% 100% 38.2% 100% 100%
Table 1: Experiments on Human3.6M with unseen view-points. Standard CNNs are unable to recognize actions with different viewpoints, however, using global 3D pose allows the models to recognize the actions. formance, it is still lacking. A second hypothesis is that by using 3D human pose information, we can recognize actions in a global representation space, unconstrained by camera views. A key drawback to this approach is estimating 3D pose from video itself is a challenging problem, especially when multiple people are present. It further requires estimat-ing camera pose in order to build a ‘world’/global camera invariant 3D representation. Further, it is unclear what the right representation of 3D pose is (e.g., coordinates of joints, limbs, motion difference of joints between frames, etc.).
Building on this hypothesis and observation, we present and evaluate several approaches for recognizing actions in unseen viewpoints. The basic approach relies on estimating 3D pose directly from the videos, then explores using dif-ferent representations of it for recognition. Since directly estimating accurate real-world 3D pose is often difﬁcult, we also present an approach of learning latent 3D representa-tions of an action and its multi-view 2D projections. This is done by imposing the latent action representations to follow 3D geometric transformations and projections, in addition to minimizing the action classiﬁcation loss. We learn such view invariant action representations without any 3D or view ground truth labels.
We also introduce a challenging dataset building on the
MLB-YouTube dataset [31]. The MLB-YouTube dataset contains actions from a single camera and these actions are all in the same environment (e.g., a professional baseball stadium). Our extended dataset contains evaluation samples of the same actions, but from many different viewpoints and a variety of different settings: batting cages, little league (children’s baseball games), high school games, etc. These use different camera (e.g., cell phones), in very different environments. The goal is to learn a representation from the single view dataset that generalizes to these challenging, unseen viewpoints. Examples are shown in Fig. 1.
To summarize, the contributions of this paper are:
• A computationally efﬁcient, geometric-based layer and learning to learn view invariant representation.
• Thorough evaluation of multiple approaches to unseen viewpoint action recognition.
• A challenging new dataset for unseen viewpoint recog-nition in unseen environments. 2.