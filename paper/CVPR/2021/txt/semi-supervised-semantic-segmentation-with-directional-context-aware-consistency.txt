Abstract
Semantic segmentation has made tremendous progress in recent years. However, satisfying performance highly de-pends on a large number of pixel-level annotations. There-fore, in this paper, we focus on the semi-supervised seg-mentation problem where only a small set of labeled data is provided with a much larger collection of totally unlabeled images. Nevertheless, due to the limited annotations, mod-els may overly rely on the contexts available in the training data, which causes poor generalization to the scenes un-seen before. A preferred high-level representation should capture the contextual information while not losing self-awareness. Therefore, we propose to maintain the context-aware consistency between features of the same identity but with different contexts, making the representations robust to the varying environments. Moreover, we present the Direc-tional Contrastive Loss (DC Loss) to accomplish the consis-tency in a pixel-to-pixel manner, only requiring the feature with lower quality to be aligned towards its counterpart.
In addition, to avoid the false-negative samples and ﬁlter the uncertain positive samples, we put forward two sam-pling strategies. Extensive experiments show that our sim-ple yet effective method surpasses current state-of-the-art methods by a large margin and also generalizes well with extra image-level annotations. 1.

Introduction
Semantic segmentation, as a fundamental tool, has prof-ited many downstream applications, and deep learning fur-ther boosts this area with remarkable progress. However, training a strong segmentation network highly relies on suf-ﬁcient ﬁnely annotated data to yield robust representations for input images, and dense pixel-wise labeling is rather time-consuming, e.g., the annotation process costs more than 1.5h on average for a single image in Cityscapes [12].
*Equal Contribution
Input
SupOnly
Figure 1. Grad-CAM [46] visualizations of the regional contribu-tion to the feature of interest (i.e., the yellow cross shown in the input). The red region corresponds to high contribution. SupOnly: the model trained with only 1/8 labeled data. More illustrations are shown in the supplementary.
Ours
To alleviate this problem, weaker forms of segmentation annotation, e.g., bounding boxes [13, 48], image-level la-bels [56, 1, 44] and scribbles [34, 51, 52], have been ex-ploited to supplement the limited pixel-wise labeled data.
Still, collecting these weak labels requires additional hu-man efforts. Instead, in this paper, we focus on the semi-supervised scenario where the segmentation models are trained with a small set of labeled data and a much larger collection of unlabeled data.
Segmentation networks can not predict a label for each pixel merely based on its RGB values. Therefore, the con-textual information is essential for semantic segmentation.
Iconic models (e.g., DeepLab [7] and PSPNet [60]) have also shown satisfying performance by adequately aggregat-ing the contextual cues to individual pixels before making
ﬁnal predictions. However, in the semi-supervised setting, models are prone to overﬁt the quite limited training data, which results in poor generalization on the scenes unseen during training. In this case, models are easy to excessively rely on the contexts to make predictions. Empirically, as shown in Fig. 1, we ﬁnd that after training with only the la-beled data, features of train and person overly focus on the contexts of sky and dog but overlook themselves. There-fore, to prevent the model abusing the contexts and also 1205
Figure 2. Crop1 and Crop2 are randomly cropped from the same image with an overlapping region. The consistency (represented by the solid white line) is maintained between representations for the overlapping region in the two crops under different contexts (represented by the dashed white line), in a pixel-to-pixel manner. help enhance self-awareness, our solution in this work is to make the representations more robust to the changing en-vironments, which we call the context-aware consistency.
Speciﬁcally, as shown in Fig. 2, we crop two random patches from an unlabeled image and they are conﬁned to have an overlapping region, which can be deemed that the overlapping region is placed into two different envi-ronments, i.e., contextual augmentation. Even though the ground-truth labels are unknown, the consistency of high-level features under different environments can still be maintained because there exists a pixel-wise one-to-one re-lationship between the overlapping regions of the two crops.
To accomplish the consistency, we propose the Directional
Contrastive Loss that encourages the feature to align to-wards the one with generally higher quality, rather than bi-laterally in the vanilla contrastive loss. Also, we put forward two effective sampling strategies that ﬁlter out the com-mon false negative samples and the uncertain positive sam-ples respectively. Owing to the context-aware consistency and the carefully designed sampling strategies, the proposed method brings signiﬁcant performance gain to the baseline.
The proposed method is simple yet effective. Only a few additional parameters are introduced during training and the original model is kept intact for inference, so it can be easily applied to different models without structural con-straints. Extensive experiments on PASCAL VOC [15] and
Cityscapes [12] show the effectiveness of our method.
In sum, our contributions are three-fold:
• To alleviate the overﬁtting problem, we propose to maintain context-aware consistency between pixels under different environments to make models robust to the contextual variance.
• To accomplish the contextual alignment, we design the
Directional Contrastive Loss, which applies the con-trastive learning in a pixel-wise manner. Also, two ef-fective sampling strategies are proposed to further im-prove performance.
• Extensive experiments demonstrate that our proposed model surpasses current state-of-the-art methods by a large margin. Moreover, our method can be extended to the setting with extra image-level annotations. 2.