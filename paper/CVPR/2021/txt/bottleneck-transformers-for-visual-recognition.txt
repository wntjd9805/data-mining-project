Abstract
We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classiﬁca-tion, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the ﬁnal three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines signiﬁ-cantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in la-tency. Through the design of BoTNet, we also point out how
ResNet bottleneck blocks with self-attention can be viewed as
Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO
Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classiﬁcation, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in “compute”1 time than the popu-lar EfﬁcientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2 1.

Introduction
Deep convolutional backbone architectures [37, 54, 28, 66, 56] have enabled signiﬁcant progress in image classiﬁ-cation [52], object detection [17, 40, 21, 20, 50], instance segmentation [25, 13, 27]. Most landmark backbone archi-tectures [37, 54, 28] use multiple layers of 3×3 convolutions.
While the convolution operation can effectively capture local information, vision tasks such as object detection, in-stance segmentation, keypoint detection require modeling long range dependencies. For example, in instance segmen-1Forward and backward propagation for batch size 32 2Please refer to https://arxiv.org/abs/2101.11605 for a longer version.
Figure 1: Left: A ResNet Bottleneck Block, Right: A Bot-tleneck Transformer (BoT) block. The only difference is the replacement of the spatial 3 × 3 convolution layer with
Multi-Head Self-Attention (MHSA). The structure of the self-attention layer is described in Figure 4. tation, being able to collect and associate scene information from a large neighborhood can be useful in learning relation-ships across objects [32]. In order to globally aggregate the locally captured ﬁlter responses, convolution based archi-tectures require stacking multiple layers [54, 28]. Although stacking more layers indeed improves the performance of these backbones [67], an explicit mechanism to model global (non-local) dependencies could be a more powerful and scal-able solution without requiring as many layers.
Modeling long-range dependencies is critical to natural language processing (NLP) tasks as well. Self-attention is a computational primitive [61] that implements pairwise entity interactions with a content-based addressing mecha-nism, thereby learning a rich hierarchy of associative features across long sequences. This has now become a standard tool in the form of Transformer [61] blocks in NLP with promi-nent examples being GPT [46, 5] and BERT [14, 42] models.
A simple approach to using self-attention in vision is to replace spatial convolutional layers with the multi-head self-attention (MHSA) layer proposed in the Transformer [61] (Figure 1). This approach has seen progress on two seem-ingly different approaches in the recent past. On the one hand, we have models such as SASA [49], AACN [4], 16519
Figure 2: A taxonomy of deep learning architectures using self-attention for visual recognition. Our proposed architecture
BoTNet is a hybrid model that uses both convolutions and self-attention. The speciﬁc implementation of self-attention could either resemble a Transformer block [61] or a Non-Local block [63] (difference highlighted in Figure 4). BoTNet is different from architectures such as DETR [10], VideoBERT [55], VILBERT [44], CCNet [34], etc by employing self-attention within the backbone architecture, in contrast to using them outside the backbone architecture. Being a hybrid model, BoTNet differs from pure attention models such as SASA [49], LRNet [33], SANet [68], Axial-SASA [31, 62] and ViT [15]. AA-ResNet [4] also attempted to replace a fraction of spatial convolution channels with self-attention.
SANet [68], Axial-SASA [62], etc that propose to replace spatial convolutions in ResNet botleneck blocks [28] with different forms of self-attention (local, global, vector, axial, etc). On the other hand, we have the Vision Transformer (ViT) [15], that proposes to stack Transformer blocks [61] operating on linear projections of non-overlapping patches.
It may appear that these approaches present two different classes of architectures. We point out that it is not the case.
Rather, ResNet botteneck blocks with the MHSA layer can be viewed as Transformer blocks with a bottleneck struc-ture, modulo minor differences such as the residual connec-tions, choice of normalization layers, etc. (Figure 3). Given this equivalence, we call ResNet bottleneck blocks with the
MHSA layer as Bottleneck Transformer (BoT) blocks.
Here are a few challenges when using self-attention in vision: (1) Image sizes are much larger (1024 × 1024) in ob-ject detection and instance segmentation compared to image classiﬁcation (224 × 224). (2) The memory and computa-tion for self-attention scale quadratically with spatial dimen-sions [58], causing overheads for training and inference.
To overcome these challenges, we consider the following design: (1) Use convolutions to efﬁciently learn abstract and low resolution featuremaps from large images; (2) Use global (all2all) self-attention to process and aggregate the informa-tion contained in the featuremaps captured by convolutions.
Such a hybrid design [4] (1) uses existing and well optimized primitives for both convolutions and all2all self-attention; (2) can deal with large images efﬁciently by having convolutions do the spatial downsampling and letting attention work on smaller resolutions. Here is a simple practical instantiation of this hybrid design: Replace only the ﬁnal three bottle-neck blocks of a ResNet with BoT blocks without any other changes. Or in other words, take a ResNet and only replace the ﬁnal three 3 × 3 convolutions with MHSA layers (Fig 1, Table 1). This simple change improves the mask AP by 1.2% on the COCO instance segmentation benchmark [40] over our canonical baseline that uses ResNet-50 in the Mask
R-CNN framework [27] with no hyperparameter differences and minimal overheads for training and inference. Moving forward, we call this simple instantiation as BoTNet given its connections to the Transformer through the BoT blocks.
While we note that there is no novelty in its construction, we believe the simplicity and performance make it a useful reference backbone architecture that is worth studying.
Using BoTNet, we demonstrate signiﬁcantly improved re-sults on instance segmentation without any bells and whistles such as Cascade R-CNN [7], FPN changes [41, 19, 43, 57], hyperparameter changes [56], etc. A few key results from
BoTNet are: (1) Performance gains across various training conﬁgurations (Section 4.1), data augmentations (Section 4.2) and ResNet family backbones (Section 4.4); (2) Signif-icant boost from BoTNet on small objects (+2.4 Mask AP and +2.6 Box AP) (Appendix); (3) Performance gains over
Non-Local layers (Section 4.6); (4) Gains that scale well with larger images resulting in 44.4% mask AP, competitive with state-of-the-art performance among entries that only study backbone architectures with modest training schedules (up to 72 epochs) and no extra data or augmentations.3. 3SoTA is based on https://paperswithcode.com/sota/ instance-segmentation-on-coco-minival. 16520
Lastly, we scale BoTNets, taking inspiration from the training and scaling strategies in [56, 49, 38, 51, 48, 67, 3], after noting that BoTNets do not provide substantial gains in a smaller scale training regime. We design a family of BoT-Net models that achieve up to 84.7% top-1 accuracy on the
ImageNet validation set, while being upto 1.64x faster than the popular EfﬁcientNet models in terms of compute time on TPU-v3 hardware. By providing strong results through
BoTNet, we hope that self-attention becomes a widely used primitive in future vision architectures. 2.