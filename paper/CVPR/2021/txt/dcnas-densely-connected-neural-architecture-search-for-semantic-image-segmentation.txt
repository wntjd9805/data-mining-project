Abstract
Existing NAS methods for dense image prediction tasks usually compromise on restricted search space or search on proxy task to meet the achievable computational de-mands. To allow as wide as possible network architectures and avoid the gap between realistic and proxy setting, we propose a novel Densely Connected NAS (DCNAS) frame-work, which directly searches the optimal network struc-tures for the multi-scale representations of visual informa-tion, over a large-scale target dataset without proxy. Specif-ically, by connecting cells with each other using learnable weights, we introduce a densely connected search space to cover an abundance of mainstream network designs. More-over, by combining both path-level and channel-level sam-pling strategies, we design a fusion module and mixture layer to reduce the memory consumption of ample search space, hence favoring the proxyless searching. Compared with contemporary works, experiments reveal that the prox-yless searching scheme is capable of bridging the gap be-tween searching and training environments. Further, DC-NAS achieves new state-of-the-art performances on public semantic image segmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC 2012. We also retain leading performances when evaluating the ar-chitecture on the more challenging ADE20K and PASCAL-Context dataset. 1.

Introduction
The heavy computation overheads of early Neural Ar-chitecture Search (NAS) methods [81] hinder their appli-cations in real-world problems. Recently, limited search space strategies [82, 37, 38] signiÔ¨Åcantly shorten the search-ing time of NAS algorithms, which make NAS approaches achieve superhuman performance in image classiÔ¨Åcation task. However, to meet the low consumption of searching time, NAS methods with a constrained search space throw down the multi-scale representation of high-resolution im-‚àócorresponding author, zhangxiong@yy.com
Method
Proxyless GPU Days FLOPs(G)
DPC [3]
Auto-DeepLab [36]
CAS [73]
GAS [35]
FasterSeg [8]
Fast-NAS [44]
SparseMask [59]
Ours (DCNAS)
‚úó
‚úó
--‚úó
‚úó
‚úó
‚úì 2600 3
-6.7 2 8 4.2 5.6 684.0 695.0
--28.2 435.7 36.4 294.6
œÅ ‚Üë 0.46 0.31
--0.35 0.42 0.49 0.73
œÑ ‚Üë mIoU(%) ‚Üë 0.37 0.21
--0.25 0.33 0.38 0.55 82.7 82.1 72.3 73.5 71.5 78.9 68.6 84.3
Table 1. Comparisons with state of the arts. The table presents the comparison results of [3, 36, 73, 35, 8, 66, 59] and our DC-In which, œÅ and œÑ represent the Pear-NAS comprehensively. son Correlation CoefÔ¨Åcient and the Kendall Rank Correlation Co-efÔ¨Åcient, respectively, both of which are measured according to the performances of the super-net and the Ô¨Åne-tuned stand-alone model, Proxyless applies a proxyless searching paradigm, GPU
Days presents the searching cost, mIoU(%) and FLOPs(G) re-port the accuracy and the Ô¨Çops of the best model. age. As a result, those methods are not suitable for dense image prediction tasks (e.g. semantic image segmentation, object detection, and monocular depth estimation).
To efÔ¨Åciently search appropriate network structure that can combine both the local and global clues of the se-mantic features, researches recently focus on improving
NAS frameworks by designing new search spaces to han-dle multi-scale features. For instance, DPC [3] introduces a recursive search space, and Auto-DeepLab [36] proposes a hierarchical search space. However, designing new search space for dense image prediction tasks proves challenging: one has to delicately compose a Ô¨Çexible search space that covers as much as possible optimal network architectures.
Meanwhile, efÔ¨Åciently address memory consumption and the heavy computation problems accompanied with ample search space for the high-resolution imagery.
In this work, we propose an efÔ¨Åcient and proxyless NAS framework to search the optimal model structure for se-mantic image segmentation. Our approach is based on two principal considerations. Firstly, the search space should be comprehensive enough to handle most of the main-stream architecture designs, even some undiscovered high-quality model structures. As shown in Figure 1, we design a reticular-like and fully densely connected search space, which contains various paths in the search space. Con-13956
Scale
Input
Densely Connected Search Space
STEM 0 1 2 3 4 5 6
L-2
L-1
L 1 1 4& 1 8& 1 16& 1 32&
ùëì! ùë†!‚ÅÑ
ùëì# ùë†#‚ÅÑ
ùëì% ùë†%‚ÅÑ
ùëì" ùë†"‚ÅÑ
Fusion Module
Mixture Layer
ùë†!
‚®Å
ùëì$ ùë†$‚ÅÑ split 3x3 sep 5x5 sep
√ó	ùëù"
√ó	ùëù# concat 7x7 sep
√ó	ùëù$
ùëù% =
ùëí&!
$
‚àë ùëí&"
‚Äô("
Output
Upsample 3x3 conv 1x1 conv
Cat
Down
Keep
Up
‚®Å
Elem-wise Add
Fusion Module
Fixed operator
Figure 1. Framework of DCNAS. Top: The densely connected search space (DCSS) with layer L and max downsampling rate 32. To stable the searching procedure, we keep the beginning STEM and Ô¨Ånal Upsample block unchanged. Dashed lines represent candidate connections in DCSS, to keep clarity, we only demonstrate several connections among all the candidates. Bottom Left: Fusion module targets at aggregating feature-maps derived by previous layers, and solving the intensive GPU memory requirement problem by sampling a portion of connections from all possible ones. Bottom Right: Mixture layer may further save GPU memory consumption and accelerating the searching process by sampling and operating on a portion of features while bypassing the others. sequently, DCNAS may derive whichever model architec-ture designs by selecting appropriate paths among the whole set of these connections. Additionally, DCNAS aggregates contextual semantics encoded in multi-scale imageries to derive long-range context information, which has been re-cently found to be vital in the state-of-the-art manually de-signed image segmentation models [40, 74, 5, 7, 70]. Sec-ondly, we observe that previous NAS approaches [3, 36] heavily rely on the proxy task or proxy dataset to reduce the cost of GPU hours and to alleviate the high GPU memory consumption problem. However, architectures optimized over the proxy are not guaranteed to be suitable in realistic setting [2], because of the gap between the proxy and the target conÔ¨Åguration. Regarding this concern, we relax the discrete architectures into continuous representation to save
GPU hours and design a fusion module which applies both path-level and channel-level sampling strategies during the searching procedure to reduce memory demand. Based on that, one may employ the stochastic gradient descent (SGD) to perform the proxyless searching procedure to select the optimal architecture from all the candidate models without the help of proxy datasets or proxy tasks. The searching procedure takes about 5.6 GPU days on Cityscapes [11].
We apply our approach to semantic image segmentation tasks on several public benchmarks, our model achieves the best performance compared with state-of-the-art hand-craft models [51, 74, 80, 7, 24, 12, 17] and other contempo-rary NAS approaches [36, 3, 59, 44, 8]. We also evaluate the optimal model identiÔ¨Åed by DCNAS on PASCAL VOC 2012 [15], the model outperforms other leading approaches
[26, 17, 74, 68, 69, 27, 70, 21] and advances the state-of-the-art performance. Transferring the model to ADE20K
[78] and PASCAL-Context [43] datasets, our model obtains the best result compared with state-of-the-art approaches
[12, 17, 22, 24, 71, 69, 31], according to the correspond-ing evaluation metrics.
Further, considering NAS methods depend on agent met-rics to explore promising networks efÔ¨Åciently. If the agent metric provides a strong relative ranking of models, this en-ables the discovery of high performing models when trained to convergence. However, studies about the relative ranking of models in the searching period are less explored in litera-ture [44, 59, 8, 3, 36]. In this work, we enrich this Ô¨Åeld and investigate most related methods on how well the accuracy in searching phase for speciÔ¨Åc architectures correlates with that of the Ô¨Åne-tuned stand-alone model.
To summarize, our main contributions are as follows:
‚Ä¢ We design a novel entirely densely connected search space, allowing to explore various existing designs and to cover arbitrary model architecture patterns.
‚Ä¢ A novel proxyless searching paradigm is implemented to efÔ¨Åciently and directly explore the most promising model among all the candidates encoded in DCNAS, on large-scale segmentation datasets (e.g., Cityscapes [11]).
‚Ä¢ The DCNAS outperforms contemporary NAS methods
[44, 59, 8, 3, 36] and demonstrates new state-of-the-art performance on Cityscapes [11], PASCAL-VOC 2012
[15], PASCAL-Context [43] and ADE20K [78] datasets. 13957
‚Ä¢ We conduct a large scale experiment to understand the correlation of model performance between the searching and training periods for most contemporary works [36, 3, 59, 44, 8] focusing on semantic segmentation task. 2.