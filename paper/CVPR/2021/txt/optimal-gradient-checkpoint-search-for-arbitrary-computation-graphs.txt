Abstract
Deep Neural Networks(DNNs) require huge GPU mem-ory when training on modern image/video databases. Un-fortunately, the GPU memory is physically ﬁnite, which lim-its the image resolutions and batch sizes that could be used in training for better DNN performance. Unlike solutions that require physically upgrade GPUs, the Gradient Check-Pointing(GCP) training trades computation for more mem-ory beyond existing GPU hardware. GCP only stores a subset of intermediate tensors, called Gradient Checkpoints (GCs), during forward. Then during backward, extra local forwards are conducted to compute the missing tensors. The total training memory cost becomes the sum of (1) the mem-ory cost of the gradient checkpoints and (2) the maximum memory cost of local forwards. To achieve maximal mem-ory cut-offs, one needs optimal algorithms to select GCs.
Existing GCP approaches rely on either manual input of
GCs or heuristics-based GC search on Linear Computation
Graphs (LCGs), and cannot apply to Arbitrary Computa-tion Graphs(ACGs). In this paper, we present theories and optimal algorithms on GC selection that, for the ﬁrst time, are applicable to ACGs and achieve the maximal memory cut-offs. Extensive experiments show that our approach not only outperforms existing approaches (only applicable on
LCGs), and is applicable to a vast family of LCG and ACG networks, such as Alexnet, VGG, ResNet, Densenet, Incep-tion Net and highly complicated DNNs by Network Archi-tecture Search. Our work enables GCP training on ACGs, and cuts off up-to 80% of training memory1 with a moderate time overhead ( 30%-50%). Codes are available2.
∼ 1.

Introduction
Deep Neural Networks(DNNs) require huge GPU mem-ory when training on modern image/video databases. image fea-For popular backbone DNNs used for 1Cutting off 80% of training memory means one can double the input image size or quadruple the batch size on the same GPUs. 2https://github.com/lordfjw/OptimalGradCheckpointing
Figure 1: Regular Training vs. Gradient CheckPoint-ing(GCP) Training. (a) The regular training stores all ten-sors during forward, and uses these tensors to compute gra-dients during backward. (b) GCP stores a subset of ten-sors during the ﬁrst forward, and conducts extra local re-forwards to compute tensors and gradients during back-ward. Our approach automatically searches the optimal set of Gradient Checkpoints (GCs) for memory cut-off.
Such that on the same physical GPU memory (e.g., in 4
RTX2080Ti GPUs), GCP training can accommodate mod-els that require 2+ times extra GPU memory. such as AlexNet the memory cost
For example, given a median size input
[17], VGG [29] ture extraction, increases quadrat-and ResNet [14], image resolution and network ically with the input ten-depth. sor of [BatchSize
Height] =
[32, 3, 224, 224], ResNet101 requires around 4 GB memory only to store feature tensors and gradients in training (soft-ware overheads not included). In more challenging tasks,
DNNs that detect small objects and large number of ob-Channel
W idth
×
×
× 111433
× ject categories require input image resolution of more than 600 [25, 30, 24] and can easily consume more than 600 30 GB with the same batch size. The memory issue is even worse for video-based DNNs, such as CDC [28], C3D [16] and 3D-ResNet [13]. To recognize complex activities in video, the input video clips would be as long as 64 frames and could easily go beyond 10 GB using a moderate net-work. Memory issue also occurs in training DNN compo-sitions, such as Generative Adversarial Networks (GANs), where multiple generator and discriminator networks are si-multaneously stored in GPU memory.
Existing efforts to address memory issues presented three main approaches: (1) Better single GPUs. Recent
GPUs provide larger memory at the expense of exponen-tially growing price and power consumption. For instance, from TitanXp, Quadro P6000, RTX 3090 to Tesla V100, for 1-2.7 times increase in memory, the prices increase (2) Parallelization among multiple GPUs 2.8-8.5 times.
[10, 27, 18, 21, 20, 34, 2, 3], which requires expensive clus-ters, introduces substantial I/O cost, and does not reduce the total memory cost. (3) Gradient CheckPointing (GCP)
[8, 12], which focuses on trading computation for mem-ory and reduces the total memory cost without any upgrade in hardware. Note that recent affordable GPUs (e.g., RTX 2080 Ti , RTX 3080), although limited in memory (around 11GB), provide exceptional improvement in GPU cores and
FLOPS. Trading computation costs for memory is a very attractive solution that make it possible to train very heavy
DNNs with ﬁnite GPU memory.
The regular DNN training approach consists of two al-ternated stages: forward and backward. Fig. 1 (a) illustrates an example of feed-forward neural networks.
In the for-ward stage, the network takes an input tensor, and computes tensors at every layer to the ﬁnal output. In the backward stage, the difference between the output and ground truth is passed back along the network to compute the gradients at each layer. The regular training approach saves tensors at all layers computed during forward. The total memory cost is the sum of cost over all these intermediate tensors.
GCP is a high-level training approach that trade extra computation time for substantial saving of GPU memory.
Fig. 1 (b) illustrates its main idea. During GCP training, only a subset of intermediate tensors (which are called Gra-dient Checkpoints (GCs)) are stored in the ﬁrst forward, and the missing tensors needed during backward are computed via extra local re-forwards. The total memory cost is the sum of the cost at the subset of intermediate tensors and the maximum memory cost among local re-forwards. Training with GCP can lead to substantial memory reduction, with the time overhead of local re-forwards. To achieve maximal memory cut-offs, one needs optimal algorithms to search for GCs. The GC searching algorithm is a preprocessing step of GCP training, and only needs to be run once for one computation graph.
In this paper, we propose sophisticate theories and efﬁ-cient algorithms that, for the ﬁrst time, automatically ﬁnd the optimal GCs in Arbitrary Computation Graphs(ACG), opens the gate of GCP training to a vast family of DNNs from ResNet to the Neural Architecture Search(NAS) net-works. Compared to existing GC searching (only applica-ble to Linear Computation Graphs(LCG) such as VGG), the optimality of our approach does not pose any assumption on computation graph, thus applicable to ACGs. Our op-timal GCs lead to the smallest memory cost in GCP train-ing. Using our GC searching algorithm, the GCP training can accommodate much larger models, on the same phys-ical GPU memory (see the table in Fig. 1). For instance, on 4 RTX2080Ti GPUs, regular training can typically train a ResNet50 image classiﬁcation model of 3 224 input size with 256 batch size. 224
×
× 2.