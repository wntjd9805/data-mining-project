Abstract 2D
_dense correspondence_ 3D image RGB object crd. map  (xyz as RGB) rendered as  point cloud
Reconstruct
Reproject
Object localization in 3D space is a challenging as-pect in monocular 3D object detection. Recent advances in 6DoF pose estimation have shown that predicting dense 2D-3D correspondence maps between image and object 3D model and then estimating object pose via Perspective-n-Point (PnP) algorithm can achieve remarkable localization accuracy. Yet these methods rely on training with ground truth of object geometry, which is difﬁcult to acquire in real outdoor scenes. To address this issue, we propose
MonoRUn, a novel detection framework that learns dense correspondences and geometry in a self-supervised man-ner, with simple 3D bounding box annotations. To regress the pixel-related 3D object coordinates, we employ a re-gional reconstruction network with uncertainty awareness.
For self-supervised training, the predicted 3D coordinates are projected back to the image plane. A Robust KL loss is proposed to minimize the uncertainty-weighted reprojec-tion error. During testing phase, we exploit the network uncertainty by propagating it through all downstream mod-ules. More speciﬁcally, the uncertainty-driven PnP algo-rithm is leveraged to estimate object pose and its covari-ance. Extensive experiments demonstrate that our proposed approach outperforms current state-of-the-art methods on
KITTI benchmark.1 1.

Introduction
Monocular 3D object detection is an active research area in computer vision. Although deep-learning-based 2D ob-ject detection has achieved remarkable progress [3, 30], the 3D counterpart still poses a much greater challenge on accu-rate object localization, since a single image cannot provide explicit depth information. To address this issue, a large number of works leverage geometrical priors and solve the object pose (position and orientation in camera frame) via
*Corresponding author: Wei Tian. 1Code: https://github.com/tjiiv-cprg/MonoRUn. image crd. Map (uv as RG) i n T e a s r t
T uncertainty map (large uncertainty in  occluded region) camera object
Pose  (with es!mated covariance)
Figure 1. 3D reconstruction is conducted by regressing the pixel-related object coordinate map, which can be visualized as local point cloud in object space. For self-supervision, the object coor-dinates are reprojected to recover the image coordinate map. To focus on foreground pixels, we estimate the aleatoric uncertainty of network prediction. The coordinate uncertainty can be further propagated to estimate the pose covariance. 2D-3D constraints. These constraints either require extra keypoint annotations [4, 13], or exploit centers, corners and edges of ground truth bounding boxes [21, 27]. Yet the ac-curacy largely depends on the number and quality of avail-able constraints, and the performance degrades in occlusion and truncation cases with fewer visible keypoints. A more robust approach is using dense 2D-3D correspondences, in which every single foreground pixel is mapped to a 3D point in the object space. This has proven successful in monocu-lar 6DoF pose estimation tasks [15].
Current state-of-the-art dense correspondence meth-ods [22, 28, 40] rely on both ground truth pose and 3D object model, so that target 3D coordinate map and ob-ject mask can be rendered for training supervision. This requirement restricts the training data to synthetic or sim-ple laboratory scenes, where exact or pre-reconstructed 3D models are readily available. However, 3D object detec-tion in real scenes (e.g., driving scenes) mostly deals with category-level objects, where acquiring accurate 3D models for all instances is impractical. An intuitive idea could be 10379
using LiDAR points to generate sparse coordinate maps for supervision. However, the persisting challenge is the deﬁ-ciency of LiDAR points on speciﬁc objects or parts, e.g., on distant objects or reﬂective materials.
A workaround to the lack of ground truth is leveraging self-supervision. Typically, Wang et al. [35] adopted a self-supervised network to directly learn object pose with the given ground truth 3D geometry. Our work, on the con-trary, adopts the opposite idea: learning the 3D geometry from ground truth pose in a self-supervised manner during training, and then solving the object pose via 2D-3D corre-spondences during testing.
In this paper, we propose the MonoRUn, a novel monocular 3D object detection method using self-supervised reconstruction with uncertainty propagation.
MonoRUn can extend off-the-shelf 2D detectors by append-ing a 3D branch to the region of interest (RoI) within each predicted 2D box. The 3D branch regresses dense 3D ob-ject coordinates in the RoI, which effectively builds up the geometry and 2D-3D correspondences.
To overcome the need for supervised foreground seg-mentation, we estimate the uncertainty of the predicted co-ordinates and adopt an uncertainty-driven PnP algorithm, which focuses on the low-uncertainty foreground. Further-more, by forward propagating the uncertainty through PnP module, we can estimate the pose covariance matrix, which is used for scoring the detection conﬁdence.
Self-supervision is conducted by projecting the predicted 3D coordinates back to the image via ground truth object pose and camera intrinsic parameters. To minimize the re-projection error with uncertainty awareness, we propose the
Robust KL loss that minimizes the KL divergence between the predicted Gaussian distribution and the ground truth
Dirac distribution. This novel loss function is the key to the state-of-the-art performance for the MonoRUn network.
To summarize, our main contributions are as follows:
• We propose a novel monocular 3D object detection network with uncertainty awareness, which can be trained without extra annotations (e.g., keypoints, 3D models, masks). To the best of our knowledge, this is the ﬁrst dense correspondence method employed for 3D detection in real driving scenes.
• We propose the Robust KL loss for general deep re-gression with uncertainty awareness, and demonstrate its superiority over the plain KL loss in previous work.
• Extensive evaluation on KITTI [10] benchmark shows signiﬁcant improvement of our approach compared to current state-of-the-art methods. 2.