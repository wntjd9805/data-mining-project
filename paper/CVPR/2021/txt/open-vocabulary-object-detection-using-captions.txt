Abstract
Despite the remarkable accuracy of deep neural net-works in object detection, they are costly to train and scale due to supervision requirements. Particularly, learn-ing more object categories typically requires proportionally more bounding box annotations. Weakly supervised and zero-shot learning techniques have been explored to scale object detectors to more categories with less supervision, but they have not been as successful and widely adopted as supervised models. In this paper, we put forth a novel formulation of the object detection problem, namely open-vocabulary object detection, which is more general, more practical, and more effective than weakly supervised and zero-shot approaches. We propose a new method to train object detectors using bounding box annotations for a lim-ited set of object categories, as well as image-caption pairs that cover a larger variety of objects at a signiﬁcantly lower cost. We show that the proposed method can detect and localize objects for which no bounding box annotation is provided during training, at a signiﬁcantly higher accuracy than zero-shot approaches. Meanwhile, objects with bound-ing box annotation can be detected almost as accurately as supervised methods, which is signiﬁcantly better than weakly supervised baselines. Accordingly, we establish a new state of the art for scalable object detection. 1.

Introduction
Object detection is one of the most prominent applica-tions of artiﬁcial intelligence, and one of the most suc-cessful tasks for deep neural networks. However, despite the tremendous progress in deep object detection, such as
Faster R-CNN [32] and its impressive accuracy, training such models requires expensive and time-consuming human supervision. Particularly, one needs to manually annotate at least thousands of bounding boxes for each object category of interest. Although such efforts have been already made and there are valuable datasets publicly available, such as
Open Images [21] and MSCOCO [25], these datasets cover
Figure 1. An overview of Open-Vocabulary Object Detection. We propose a two-stage training framework where we ﬁrst (1) con-struct a visual-semantic space using low-cost image-caption pairs, and then (2) learn object detection using object annotations for a set of base classes. During test (3), the goal is to detect object categories beyond base classes, by exploiting the semantic space. a limited set of object categories (e.g. 600), despite requir-ing extensive resources. Extending object detection from 600 to 60,000 categories requires 100 times more resources, which makes versatile object detection out of reach.
Nevertheless, humans learn to recognize and localize ob-jects effortlessly through natural supervision, i.e., exploring the visual world and listening to others describing situa-tions. Their lifelong learning of visual patterns and asso-ciating them with spoken words results in a rich visual and semantic vocabulary that can be used not only for detecting objects, but for other tasks too, such as describing objects and reasoning about their attributes and affordances. Al-though drawing bounding boxes around objects is not a task that humans naturally learn, they can quickly learn it using few examples, and generalize it well to all types of objects, without needing examples for each object class.
In this paper, we imitate this human ability, by design-ing a two-stage framework named Open-Vocabulary ob-ject Detection (OVD). We propose to ﬁrst use a corpus of image-caption pairs to acquire an unbounded vocabulary of 14393
concepts, simulating how humans learn by natural supervi-sion, and then use that knowledge to learn object detection (or any other downstream task) using annotation for only some object categories. This way, costly annotation is only needed for some categories, and the rest can be learned us-ing captions, which are much easier to collect, and in many cases freely available on the web [33]. Figure 1 illustrates the proposed OVD framework, which is novel and efﬁcient, enables versatile real-world applications, and can be gener-alized to other computer vision tasks.
More speciﬁcally, we train a model that takes an image and detects any object within a given target vocabulary VT .
To train such a model, we use an image-caption dataset cov-ering a large variety of words denoted as VC as well as a much smaller dataset with localized object annotations from a set of base classes VB. Note that in this task, target classes are not known during training, and can be any subset of the entire language vocabulary VΩ. This is in contrast with most existing object detection settings including weakly super-vised transfer learning methods, where VT should be known beforehand [37]. The most similar task to OVD is zero-shot object detection, which also generalizes to any given target set, but cannot utilize captions. Figure 2 illustrates an intu-itive abstraction of our proposed task compared to zero-shot and weakly supervised detection. Despite close connections to those well-known ideas, OVD is novel and uniquely po-sitioned in the literature, as we elaborate in Section 2.
To address the task of OVD, we propose a novel method based on Faster R-CNN [32], which is ﬁrst pretrained on an image-caption dataset, and then ﬁne-tuned on a bounding box dataset, in a particular way that maintains the rich vo-cabulary learned during pretraining, enabling generalization to object categories without annotation. Through extensive experiments, we evaluate our method, Open Vocabulary R-CNN (OVR-CNN), and show that it achieves signiﬁcantly higher performance than the state of the art in zero-shot learning (27% mAP compared to 10%). We also show that it outperforms weakly supervised object detectors by a sig-niﬁcant margin in generalized zero-shot settings (40% mAP compared to 26%). We supplement the paper with compre-hensive open-source code to reproduce results.1 2.