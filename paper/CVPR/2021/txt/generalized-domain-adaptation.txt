Abstract
Class Labeled
Class Unlabeled
Unknown
Many variants of unsupervised domain adaptation (UDA) problems have been proposed and solved individ-ually.
Its side effect is that a method that works for one variant is often ineffective for or not even applicable to an-other, which has prevented practical applications. In this paper, we give a general representation of UDA problems, named Generalized Domain Adaptation (GDA). GDA cov-ers the major variants as special cases, which allows us to organize them in a comprehensive framework. Moreover, this generalization leads to a new challenging setting where existing methods fail, such as when domain labels are un-known, and class labels are only partially given to each domain. We propose a novel approach to the new setting.
The key to our approach is self-supervised class-destructive learning, which enables the learning of class-invariant rep-resentations and domain-adversarial classiﬁers without us-ing any domain labels. Extensive experiments using three benchmark datasets demonstrate that our method outper-forms the state-of-the-art UDA methods in the new setting and that it is competitive in existing UDA variations as well. 1.

Introduction
Deep learning is data-hungry.
It performs remarkably well in a domain that has a sufﬁcient amount of labeled data, but its performance suffers signiﬁcantly in one that does not. Unsupervised domain adaptation (UDA) aims to resolve this problem by transferring a model learned for a label-rich source domain to a label-less target domain.
Besides the standard UDA, which assumes a fully la-beled source domain and a completely unlabeled target do-main, a number of variants have been proposed to address more complex and practical problems. Major variants are illustrated in Fig. 1. One representative example is multi-source domain adaptation (MSDA) or multi-target domain adaptation (MTDA), an extension to the case where there is more than one source or target domain [45, 26, 12, 12].
Open set domain adaptation (OSDA) and partial domain
UDA
MSDA
MTDA
OSDA
PDA
GDA
Figure 1: Schematic overview of Generalized Domain
Adaptation (GDA). GDA covers major existing UDA problems as its special cases by imposing some constraints on classes and/or domains (represented in the shapes and colors of symbols, respectively). Moreover, it provides new challenging settings where domain labels are unknown, and class labels are given to only a subset in each domain. adaptation (PDA) address the case where the class sets of the source and target do not match, i.e., there exist unknown classes [25, 32, 16, 2, 3]. Some extensions of these variants have also been studied [20, 7, 43].
Most of these variants have been proposed independently and solved individually. A negative side of this history is that a method that works for one variant may not work for or even be applicable to another. In reality, it is rarely possi-ble to identify which variant of the problems one is facing, which requires a costly trial and error to ﬁgure out the type of problem or ﬁnd a satisfactory solution. Moreover, a real problem is often a combination of these variants, in which case none of the methods will eventually be applicable.
In this work, we aimed to overcome this problem. Our approach is to ﬁrst consider giving a new general represen-tation of the UDA problems that covers all these major UDA variants and then implement a method for solving them.
Deﬁnitions of the most existing UDA problems discussed above assume a clear distinction between the source and tar-get domains, and whether or not class labels are available
Instead, in is determined on a domain-by-domain basis. 11084
our generalized representation, which we call Generalized
Domain Adaptation (GDA), everything is determined on a sample-by-sample basis; each sample is given a class label, a domain label, and indexes indicating whether or not these labels are available. In Sec. 3, we will show that this slight difference in perspective allows us to represent all the major variants and their combinations as GDA’s special cases.
Moreover, GDA brings a brand new challenge as illus-trated in Fig. 1, which is not just a straightforward combi-nation of the existing variants. The key properties of this setting are that the domain labels are completely unknown for all the samples, and the class labels are given to only a subset of the classes of each domain. Indeed, such a set-ting arises in practice, for example, for data coming from multiple institutions where the acquisition processes are un-known. Nonetheless, it has not received much attention so far. As shown later in our experiments (Sec. 5), the state-of-the-art UDA methods applicable to this setting, if forced, suffer from severe performance degradations.
We propose self-supervised class-destructive learning to accurately estimate the domain of each sample, which is es-sential missing ingredient to solve the new problem. The assumptions behind our approach are that (1) class infor-mation of an image is strongly dependent on its local struc-tural information (e.g., shape and part connection); and that (2) domain and class information are independent of each other. Based on these assumptions, our method ﬁrst trans-forms an image into a “class-indistinguishable” form by randomly shufﬂing the positions of its pixel blocks to break its local structure and then performs self-supervised learn-ing to capture class-independent information. This enables learning of class-invariant and domain-variant representa-tions without using any domain labels, making it possible to train a domain-invariant classiﬁer with a simple domain-adversarial learning approach. Our method works even for the case where the class labels are only partially available for each domain. Furthermore, our method can readily cope with open set settings by integrating a joint label-network optimization framework [38]. Thorough experiments on three datasets demonstrate that our method outperforms the state-of-the-art methods in the new setting and is highly competitive in the existing UDA problems.
Our main contributions include: (1) a general representa-tion of UDA problems named Generalized Domain Adapta-tion (GDA); (2) new UDA settings where existing methods fail; and (3) a novel domain label estimation method based on self-supervised class-destructive learning. 2.