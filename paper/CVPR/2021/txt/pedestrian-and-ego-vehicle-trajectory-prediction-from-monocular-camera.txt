Abstract
Predicting future pedestrian trajectory is a crucial com-ponent of autonomous driving systems, as recognizing crit-ical situations based only on current pedestrian position may come too late for any meaningful corrective action (e.g. breaking) to take place. In this paper, we propose a new method to predict future position of pedestrians, with respect to a predicted future position of the ego-vehicle, thus giv-ing a assistive/autonomous driving system sufﬁcient time to respond. The method explicitly disentangles actual move-ment of pedestrians in real world from the ego-motion of the vehicle, using a future pose prediction network trained in self-supervised fashion, which allows the method to observe and predict the intrinsic pedestrian motion in a normalised view, that captures the same real-world location across mul-tiple frames.
The method is evaluated on two public datasets, where it achieves state-of-the-art results in pedestrian trajectory prediction from an on-board camera. 1.

Introduction
Predicting the future behavior of objects in images and videos is of considerable importance in applications, espe-cially in areas such as robotics or automotive systems. For example, predicting future trajectory of pedestrians is a cru-cial component of autonomous and assistive driving systems, as recognizing critical situations only based on the current pedestrian position (e.g. a child at the margin of the road) may come too late for any meaningful corrective action (e.g. breaking) to be effective. For any such prediction algorithm to be widely adopted, however, it is crucial that its sensing hardware requirements are as low as possible. This is the reason why methods based on a single camera mounted on the vehicle — also known as ﬁrst-person view monocular methods — are raising a lot of attention [2, 5, 10, 11, 26, 28].
Pedestrian trajectory prediction has been extensively stud-ied in a static or a bird-eye view camera setup [1, 12, 18, 23], but these methods typically fail in dynamic scenes captured by an on-board camera due to constantly changing camera viewpoint, occlusions and other scene dynamics. Moreover, these methods typically rely on discovering pedestrian mo-tion patterns to infer future trajectory, which is not possible in the context of on-board videos, where the movement of the car, not the pedestrian movement, is the main observed effect: even a pedestrian who stands still appears in different image position in every frame, creating an apparent motion in the 2D pixel space.
In this paper, we thus propose a method1 that explicitly 1The source code is available at https : / / gitlab . com / lukeN86/pedFutureTracking 10204
disentangles the two sources of motion — the actual move-ment of the pedestrian in real world (e.g. walking, running) and the ego-motion of the vehicle, as it drives around. The key observation is that the motion of a pedestrian only af-fects a speciﬁc part of the image, whereas the motion of the car (ego-motion) affects the appearance of the whole scene. Using a self-supervised training paradigm, we train a ego-motion prediction network, which infers the ego-motion of the vehicle where the camera is mounted. This network is trained in a similar manner to current self-supervised monoc-ular depth estimator systems [10, 11, 30], but with two differ-ences: we are as much interested in recovering egomotion as in recovering depth; and we predict the egomotion deep into the future. In this manner, we can “subtract” the predicted motion of the vehicle and observe and predict the intrinsic pedestrian motion in a normalised view, which captures the same real-world location across multiple frames.
The view normalization then allows us to use a very sim-ple model to predict the intrinsic motion of pedestrians and yet achieve state-of-the-art results on two public datasets, suggesting that indeed properly disentangling ego-motion is a crucial component in these systems. Compared to previous designs that used complex predictors such as LSTMs, our predictor is much simpler. Furthermore, our method does not require additional annotations compare to these base-lines as it learns to interpret the vehicle’s egomotion in an unsupervised fashion.
As for the practical impact, the resulting method requires solely on a monocular camera, which expands its possible applications, because it can be used either as a standalone method in a current-generation vehicles which do not have any advanced LiDAR sensors, or it can be used as a redun-dancy system in the new generation of autonomous cars.
Because of the pedestrian view normalization, the method could also be incorporated into more sophisticated trajectory prediction methods that work with stationary camera.
To summarise, we make three key contributions in this paper: (i) we introduce a new self-supervised framework for ego-vehicle movement prediction, (ii) we use the latter to dis-entangle the motion of the vehicle from the intrinsic motion of pedestrians, allowing to predict pedestrian trajectories from a normalised sequence of patches observing the same part of the scene regardless of the variable viewpoint and (iii) we show that, when the pedestrian viewpoint is normalised in this manner, a simple linear model for trajectory predic-tion outperforms the traditional LSTM sequence output used in literature. 2.