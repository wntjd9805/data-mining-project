Abstract embedding space
We introduce a weakly supervised method for represen-tation learning based on aligning temporal sequences (e.g., videos) of the same process (e.g., human action). The main idea is to use the global temporal ordering of latent cor-respondences across sequence pairs as a supervisory sig-nal.
In particular, we propose a loss based on scoring the optimal sequence alignment to train an embedding net-work. Our loss is based on a novel probabilistic path ﬁnd-ing view of dynamic time warping (DTW) that contains the following three key features: (i) the local path routing de-cisions are contrastive and differentiable, (ii) pairwise dis-tances are cast as probabilities that are contrastive as well, and (iii) our formulation naturally admits a global cycle-consistency loss that veriﬁes correspondences. For evalua-tion, we consider the tasks of ﬁne-grained action classiﬁca-tion, few shot learning, and video synchronization. We re-port signiﬁcant performance increases over previous meth-ods. In addition, we report two applications of our temporal alignment framework, namely 3D pose reconstruction and
ﬁne-grained audio/visual retrieval. 1.

Introduction
Temporal sequences (e.g., videos) are an appealing data source as they provide a rich source of information and ad-ditional constraints to leverage in learning. By far the main focus on temporal sequence analysis has been on learn-ing representations targeting distinctions at the global sig-nal level, e.g., action classiﬁcation, where abundant labeled data is available for training.
In this paper, we target a weakly-supervised training regime for representation learn-ing, capable of making ﬁne-grained temporal distinctions.
Most previous approaches to temporally ﬁne-grained un-derstanding of sequential signals have considered fully-supervised training methods (e.g., [56]), where labels are provided at the sub-sequence level, e.g., frames. The ma-jor drawback of these methods is the expense in acquiring dense labels, and their subjective nature. In contrast, a key consideration in our work is the selection of training signals video 1 video 2
Figure 1. We introduce a representation learning approach based on (globally) aligning pairs of temporal sequences (e.g., video) de-picting the same process (e.g., human action). Our training objec-tive is to learn an element-wise embedding function that supports the alignment process. For example, here we illustrate the align-ment (denoted by black dashed lines) in the embedding space be-tween videos of the same human action (i.e., tennis forehand) con-taining signiﬁcant variations in their appearances and dynamics.
Empirically, we show that our learned embeddings are sensitive to both human pose and ﬁne-grained temporal distinctions, while being invariant to appearance, camera viewpoint, and background. capable of scaling up to large amounts of data yet support-ing ﬁner-grained video understanding.
As outlined in Fig. 1, given a set of paired sequences capturing the same process (e.g., tennis forehand) but highly varied (i.e., different participants, action executions, scenes, and camera viewpoints), our method trains an embedding network to support the recovery of their latent temporal alignment. We refer to our method as weakly supervised, as only readily available sequence-level labels (e.g., tennis forehand) are required to construct training pairings con-taining the same process. Given such a pair of sequences, we use their latent temporal alignment as a supervisory sig-nal to learn ﬁne-grained temporal distinctions.
Key to our proposed method is a novel dynamic time warping (DTW) formulation to score global alignments be-tween paired sequences. DTW enforces a stronger con-straint over simply considering local (soft) nearest neigh-bour correspondences [15], since the temporal ordering of the matches in the sequences are taken into account. We de-part from previous differentiable DTW methods [30, 8, 6] 11068
by taking a probabilistic path ﬁnding view of DTW that encompasses the following three key features. First, we introduce a differentiable smoothMin operator that effec-tively selects each successive path extension. Moreover, we show that this operator has a contrastive effect across paths which is missing in previous differentiable DTW formula-tions [30, 8, 6]. Second, the pairwise embedding similar-ities that form our cost function are deﬁned as probabil-ities, using the softmax operator. Optimizing our loss is shown to correspond to ﬁnding the maximum probability of any feasible alignment between the paired sequences. The softmax operator over element pairs also provides a con-trastive component which we show is crucial to prevent the model from learning trivial embeddings. This forgoes the need for a downstream discriminative loss and the corre-sponding non-trivial task of deﬁning negative alignments, e.g., [8, 6]. Third, as an additional supervisory signal, our probabilistic framework admits a straightforward global cycle-consistency loss that matches the alignments recov-ered through a cycle of sequence pairings. Collectively, our method takes into account long-term temporal information that allows us to learn embeddings sensitive to ﬁne-grained temporal distinctions (e.g., human pose), while being in-variant to nuisance variables, e.g., camera viewpoint, back-ground, and appearance.
Contributions. We make the following key contributions:
• A novel weakly supervised method for representa-tion learning tasked with discovering the alignment between sequence pairings for the purpose of ﬁne-grained temporal understanding.
• A differentiable DTW formulation with two novel fea-tures: (i) a smoothMin operation that admits a proba-bilistic path interpretation and is contrastive across al-ternative paths, and (ii) a probabilistic data term that is contrastive across alternative data pairs.
• A global cycle consistency loss to further enforce the temporal alignment.
• An extensive set of evaluations, ablations, and com-parisons with previous methods. We report signiﬁcant performance increases on several tasks requiring ﬁne-grained temporal distinctions.
• Two downstream applications, namely 3D pose recon-struction and audio-visual retrieval.
Our code and trained models will be available at: https://github.com/hadjisma/VideoAlignment. 2.