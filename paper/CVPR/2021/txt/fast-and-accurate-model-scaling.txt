Abstract
In this work we analyze strategies for convolutional neu-ral network scaling; that is, the process of scaling a base convolutional network to endow it with greater computa-tional complexity and consequently representational power.
Example scaling strategies may include increasing model width, depth, resolution, etc. While various scaling strate-gies exist, their tradeoffs are not fully understood. Existing analysis typically focuses on the interplay of accuracy and
ﬂops (ﬂoating point operations). Yet, as we demonstrate, various scaling strategies affect model parameters, activa-tions, and consequently actual runtime quite differently. In our experiments we show the surprising result that numer-ous scaling strategies yield networks with similar accuracy but with widely varying properties. This leads us to pro-pose a simple fast compound scaling strategy that encour-ages primarily scaling model width, while scaling depth and resolution to a lesser extent. Unlike currently popular scal-ing strategies, which result in about O(s) increase in model activation w.r.t. scaling ﬂops by a factor of s, the proposed fast compound scaling results in close to O(√s) increase in activations, while achieving excellent accuracy. Fewer ac-tivations leads to speedups on modern memory-bandwidth limited hardware (e.g., GPUs). More generally, we hope this work provides a framework for analyzing scaling strate-gies under various computational constraints. 1.

Introduction
Advances in modern hardware for training and running convolutional neural networks over the past several years have been impressive. Highly-parallel hardware accelera-tors, such as GPUs and TPUs, allow for training and de-ploying ever larger and more accurate networks.
Interestingly, this rapid advancement has greatly bene-ﬁted our ability to optimize models for the low-compute regime. In particular, whether via manual design, random search, or more complex neural architecture search strate-gies [37], it has become feasible to train a large number of small models and select the best one, in terms of both accu-racy and speed. At intermediate-compute regimes, efﬁcient search [17] or efﬁcient design spaces [22, 23] can still pro-Figure 1. An analysis of four model scaling strategies: width scaling (w), in which only the width of a base model is scaled; compound scaling (dwr), in which the width, depth, and resolu-tion are all scaled in roughly equal proportions; depth and width scaling (dw); and the proposed fast compound scaling (dW r), which emphasizes scaling primarily, but not only, the model width. (Top): We apply the four scaling strategies to two base models (EfﬁcientNet-B0 and RegNetZ-500MF). Compound and fast scal-ing result in highest accuracy models, and both outperform width scaling. (Bottom-left): The scaling strategies have asymptotically different behavior in how they affect model activations. Given a scale factor of s, activations increase with about O(√s) for w and dW r scaling compared to almost O(s) for dwr and dw scaling. (Bottom-right): Runtime of a model (EfﬁcientNet-B0) scaled us-ing the four scaling strategies. Fast scaling results in models nearly as fast as w scaling (but with higher accuracy), and much faster than dwr and dw scaling, closely reﬂecting model activations. vide the ability to directly optimize neural networks. How-ever, regardless of computational resources, there will nec-essarily exist a high-compute regime where it may only be feasible to train a handful of models, or possibly even only a single model. This regime motivates our work.
In the high-compute regime, network scaling, the pro-cess by which a lower-complexity model is enlarged by expanding one or more of its dimensions (e.g., depth or width), becomes essential. Scaling has proven effective in terms of obtaining larger models with good accuracy [32].
However, existing work on model scaling focuses on model accuracy. In this work, we are interested in large, accurate models that are fast enough to deploy and use in practice. 924
The concept of network scaling emerged naturally in deep learning, with early work focused on scaling networks by increasing depth [27, 29, 10]. However, gains from depth scaling plateaued, leading to explorations of scaling width [35] and resolution [12]. More recently scaling multi-ple dimensions at once, coined compound scaling [32], has been shown to achieve excellent accuracy.
Existing explorations of model scaling typically focus on maximizing accuracy versus ﬂops. Yet as we will show, two scaled models with the same ﬂops can have very different runtime on modern accelerators. This leads us to the cen-tral question explored in our work: can we design scaling strategies that optimize both accuracy and model runtime?
Our ﬁrst core observation is that there exists multiple scaling strategies that can yield similar accuracy models at
In Figure 1, top, we show that there ex-the same ﬂops. ist multiple scaling strategies that can result in models with high accuracy. We will expand on this result in §6.
However, scaling a model to a ﬁxed target ﬂops using two scaling strategies can result in widely different run-times, see Figure 1, bottom-right. To better understand this behavior at a more fundamental level, in §3 we develop a framework for analyzing the complexity of various scaling strategies, in terms of not just ﬂops, but also parameters and activations. In particular, we show that different strate-gies scale activations at different asymptotic rates relative to
ﬂops. E.g., when scaling a model from f ﬂops to sf ﬂops by scaling width, activations increase by O(√s), compared to nearly O(s) for compound scaling. Figure 1, bottom-left, shows this asymptotic behavior for a few select strategies.
In §4 we will show that within a ﬂop range of practi-cal interest, on modern accelerators the runtime of a scaled model is more strongly correlated with activations than
ﬂops. We emphasize that this correlation holds over a di-verse set of scaling strategies, which enables us to use acti-vations as a proxy for predicting a scaled model’s runtime.
Based on our analysis, in §5 we introduce a new fam-ily of scaling strategies parameterized by a single param-eter α that controls the relative scaling along model width versus other dimensions. This lets us carefully control the asymptotic rate at which model activations scale. We show
α < 1 yields models that are both fast and accurate. 0
We refer to this scaling strategy as fast compound model scaling, or simply fast scaling for brevity.
≪
As we will show in §6, fast scaling allows us to obtain large models that are as accurate as the state-of-the-art but faster. As a concrete example, we apply fast scaling to scale a RegNetY-4GF [23] model to 16GF (gigaﬂops), and ﬁnd it uses less memory and is faster (and more accurate) than
EfﬁcientNet-B4 [32] – a model with 4 fewer ﬂops.
In order to facilitate future research we will release all
× code and pretrained models introduced in this work.1 1https://github.com/facebookresearch/pycls 2.