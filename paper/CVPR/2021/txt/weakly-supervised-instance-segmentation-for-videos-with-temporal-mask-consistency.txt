Abstract
Weakly supervised instance segmentation reduces the cost of annotations required to train models. However, existing approaches which rely only on image-level class labels predominantly suffer from errors due to (a) par-tial segmentation of objects and (b) missing object predic-tions. We show that these issues can be better addressed by training with weakly labeled videos instead of images.
In videos, motion and temporal consistency of predictions across frames provide complementary signals which can help segmentation. We are the Ô¨Årst to explore the use of these video signals to tackle weakly supervised instance segmentation. We propose two ways to leverage this in-formation in our model. First, we adapt inter-pixel rela-tion network (IRN) [1] to effectively incorporate motion information during training. Second, we introduce a new
MaskConsist module, which addresses the problem of miss-ing object instances by transferring stable predictions be-tween neighboring frames during training. We demonstrate that both approaches together improve the instance seg-mentation metric AP50 on video frames of two datasets:
Youtube-VIS and Cityscapes by 5% and 3% respectively. 1.

Introduction
Instance segmentation is a challenging task, where all object instances in an image have to be detected and segmented. This task has seen rapid progress in recent years [13, 28, 5], partly due to the availability of large datasets like COCO [26]. However, it can be forbiddingly expensive to build datasets at this scale for a new domain of images or videos, since segmentation boundaries have to be annotated for every object in an image.
Alternatively, weak labels like classiÔ¨Åcation labels can be used to train instance segmentation models [54, 7, 55, 1,
‚àóThis work is done during Qing Liu‚Äôs internship at Facebook. flowIRN (a) Partial instance segmentation
Frame t
Frame t +ùõø
MaskConsist (b) Missing object instance
Figure 1. Two types of error for IRN [1] trained with still images: (a) partial segmentation and (b) missing instance. We observe opti-cal Ô¨Çow is able to capture pixels of the same instance better (circles in (a)) and we propose Ô¨ÇowIRN to model this information. In (b), a Ô¨Åsh is missed on one frame. We propose MaskConsist to lever-age temporal consistency and transfer stable mask predictions to neighboring frames during training. 21, 12, 37, 2]. While weak labels are signiÔ¨Åcantly cheaper to annotate, training weakly supervised models can be far more challenging. They typically suffer from two sources of error: (a) partial instance segmentation and (b) miss-ing object instances, as shown in Fig. 1. Weakly super-vised methods often identify only the most discriminative object regions that help predict the class label. This results in partial segmentation of objects, as shown in Fig. 1(a).
For instance, the recent work on weakly supervised in-stance segmentation IRN [1] relies on class activation maps (CAMs) [53], which suffer from this issue as also observed in other works [20, 48, 52]. Further, CAMs do not differ-entiate between overlapping instances of the same class. It can also miss object instances when multiple instances are present in an image, as shown in Fig. 1(b). In particular, an instance could be segmented in one image but not in another image where it is occluded or its pose alters.
Interestingly, these issues are less severe in videos, where object motion provides an additional signal for in-13968
stance segmentation. As shown in Fig. 1, optical Ô¨Çow in a video is tightly coupled with instance segmentation masks.
This is unsurprising since pixels belonging to the same (rigid) object move together and have similar Ô¨Çow vectors.
We incorporate such video signals to train weakly super-vised instance segmentation models, in contrast to existing methods [1, 21, 37, 2] only targeted at images.
Typical weakly supervised approaches involve two steps: (a) generating pseudo-labels, comprising noisy instance segmentation masks consistent with the weak class labels, and (b) training a supervised model like Mask R-CNN based on these pseudo-labels. We leverage video informa-tion in both stages. In the Ô¨Årst step, we modify IRN to as-sign similar labels to pixels with similar motion. This helps in addressing the problem of partial segmentation. We refer to the modiÔ¨Åed IRN as Ô¨ÇowIRN. In the second step, we in-troduce a new module called MaskConsist, which counters the problem of missing instances by leveraging temporal consistency between objects across consecutive frames. It matches prediction between neighboring frames and trans-fers the stable predictions to obtain additional pseudo-labels missed by Ô¨ÇowIRN during training. This is a generic mod-ule that can be used in combination with any weakly super-vised segmentation methods as we show in our experiments.
To the best of our knowledge, we are the Ô¨Årst work to uti-lize temporal consistency between frames to train a weakly supervised instance segmentation model for videos. We show that this leads to more than 5% and 3% improve-ment in average precision compared to image-centric meth-ods, like IRN, on video frames from two challenging video datasets: Youtube-VIS (YTVIS) [51] and Cityscapes [8], respectively. We also observe similar gains on the recently introduced video instance segmentation task [51] in YTVIS. 2.