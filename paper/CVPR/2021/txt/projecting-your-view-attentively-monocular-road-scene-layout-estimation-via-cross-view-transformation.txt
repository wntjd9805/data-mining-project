Abstract
HD map reconstruction is crucial for autonomous driv-ing. LiDAR-based methods are limited due to the de-ployed expensive sensors and time-consuming computation.
Camera-based methods usually need to separately perform road segmentation and view transformation, which often causes distortion and the absence of content. To push the limits of the technology, we present a novel framework that enables reconstructing a local map formed by road lay-out and vehicle occupancy in the bird’s-eye view given a front-view monocular image only. In particular, we propose a cross-view transformation module, which takes the con-straint of cycle consistency between views into account and makes full use of their correlation to strengthen the view transformation and scene understanding. Considering the relationship between vehicles and roads, we also design a context-aware discriminator to further reﬁne the results.
Experiments on public benchmarks show that our method achieves the state-of-the-art performance in the tasks of road layout estimation and vehicle occupancy estimation.
Especially for the latter task, our model outperforms all competitors by a large margin. Furthermore, our model runs at 35 FPS on a single GPU, which is efﬁcient and ap-plicable for real-time panorama HD map reconstruction. 1.

Introduction
With the rapid progress of autonomous driving technolo-gies, many recent efforts have been spent on the related re-search topics, e.g., scene layout estimation [11, 22, 24, 36, 38, 42, 48], 3D object detection [5, 6, 21, 37, 40, 41], ve-hicle behavior prediction [15, 19, 26, 30], and lane detec-tion [16, 33, 57], etc.
Among these tasks, high-deﬁnition map (HD map) re-construction is fundamental and critical for perception, pre-diction, and planning of autonomous driving. Its major is-sues are concerned with the estimation of a local map in-*Wenxi Liu and Yuanlong Yu are the corresponding authors.
Figure 1. Given a frontal view monocular image, we propose to leverage a cycle structure that bridges the features of frontal view and top view in their respective domains, as well as a cross-view transformer that correlates views attentively in order to facilitate the road layout estimation. cluding the road layout as well as the occupancies of nearby vehicles in the 3D world. Existing techniques rely on ex-pensive sensors like LiDAR and require time-consuming computation for cloud point data. Besides, the camera-based techniques usually need to separately perform road segmentation and view transformation, which thus causes distortion and the absence of content.
To push the limits of the technology, our work aims to address this realistic yet challenging problem of estimat-ing the road layout and vehicle occupancy in top view or bird’s-eye view (BEV), given a single monocular front-view image (see Fig. 1). However, due to the large view gap and severe view deformation, understanding and estimat-ing the top-view scene layout from the front-view image is an extremely difﬁcult problem even for a human observer.
Particularly, the same scene has signiﬁcantly different ap-pearances in the images of bird’s-eye view and frontal view.
Thus, parsing and projecting the road scenes of frontal view to top view require the ability of fully exploiting the infor-mation of the frontal view image and innate reasoning the unseen regions.
Traditional methods (e.g. [23,45]) focus on investigating the perspective transformation by estimating the camera pa-rameters and performing image coordinate transformation, but but gaps in the resulting BEV feature maps caused by geometric warping lead to poor results. Recent deep learn-ing based approaches [35, 56] mainly rely on the halluci-nation capability of deep Convolutional Neural Networks 115536
(CNNs) to infer the unseen regions between views. In gen-eral, instead of modeling the correlation between views, these methods directly leverage CNNs to learn the view projection models in a supervised manner. These models require deep network structures to propagate and transform the features of frontal view through multiple layers to spa-tially align with the top-view layout. However, due to the locally conﬁned receptive ﬁelds of convolutional layers, it causes the difﬁculty of ﬁtting a view projection model and identifying the vehicles of small scales. Moreover, road layout provides the crucial context information to infer the position and orientation of vehicles, e.g., vehicles parked alongside the road. Yet, the prior road scene parsing meth-ods usually ignore the spatial relationship between vehicles and roads.
To address the aforementioned concerns, we derive a novel GAN-based framework to estimate the road layout and vehicle occupancies from top view, given a single monocular front-view image. To handle the large discrep-ancy between views, we present a cross-view transforma-tion module in the generator network, which is composed of two sub-modules: Cycled View Projection (CVP) mod-ule bridges the view features in their respective domains and Cross-View Transformer (CVT) correlates the views, as shown in Fig. 1. Speciﬁcally, the CVP utilizes a multi-layer perceptron (MLP) to project views, which overtakes the standard information ﬂow passing through convolutional layers, and involves the constraint of cycle consistency to retain the features relevant for view projection.
In other word, transforming frontal views to top views requires a global spatial transformation over the visual features. Yet, standard CNN layers only allow local computation over fea-ture maps, which thus takes several layers to obtain a sufﬁ-ciently large receptive ﬁeld. On the other hand, fully con-nected layers can better facilitate the cross-view transfor-mation. Then, CVT explicitly correlates the features of the views before and after projection obtained from CVP, which can signiﬁcantly enhance the features after view projection.
In particular, we involve a feature selection scheme in CVT which leverages the associations of both views to extract the most relevant information. Furthermore, to exploit the spa-tial relationship between vehicles and roads, we present a context-aware discriminator that evaluates not only the es-timated masks of vehicles but also their correlation.
In experimental results, we show that our cross-view transformation module and the context-aware discriminator can elevate the performance of road layout and vehicle oc-cupancy estimation. For both tasks, we compare our model against the state-of-the-art methods on public benchmarks and demonstrate that our model is superior to all the other methods. It is worth noting that, for the estimation of vehi-cle occupancies, our model achieves a signiﬁcant advantage over the other comparison methods by at least 28.5% in the
KITTI 3D Object dataset and by at least 48.8% in the Argo-verse dataset. We also show that our framework is able to process 1024 × 1024 images in 35 FPS using a single GPU, and it is applicable for real-time reconstruction of panorama
HD map. The contributions of our paper are summarized as:
• We propose a novel framework that reconstructs a local map formed by top-view road scene layout and vehicle occupancy using a single monocular front-view image only. In particular, we propose a cross-view transfor-mation module which leverages the cycle consistency between views and their correlation to strengthen the view transformation.
• We also propose a context-aware discriminator that considers the spatial relationship between vehicles and roads in the task of estimating vehicle occupancies.
• On public benchmarks, it is demonstrated that our model achieves the state-of-the-art performance for the tasks of road layout and vehicle occupancy estimation. 2.