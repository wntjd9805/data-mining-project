Abstract
Human pose estimation is a major computer vision prob-lem with applications ranging from augmented reality and video capture to surveillance and movement tracking. In the medical context, the latter may be an important biomarker for neurological impairments in infants. Whilst many meth-ods exist, their application has been limited by the need for well annotated large datasets and the inability to gen-eralize to humans of different shapes and body composi-tions, e.g. children and infants. In this paper we present a novel method for learning pose estimators for human adults and infants in an unsupervised fashion. We ap-proach this as a learnable template matching problem fa-cilitated by deep feature extractors. Human-interpretable landmarks are estimated by transforming a template con-sisting of predeﬁned body parts that are characterized by 2D Gaussian distributions. Enforcing a connectivity prior guides our model to meaningful human shape representa-tions. We demonstrate the effectiveness of our approach on two different datasets including adults and infants. Project page: infantmotion.github.io 1.

Introduction
In today’s digitized world, images and videos are an al-most endless source of unlabeled, but inherently structured data. Tapping into this reserve of information and knowl-edge requires the ability to reason in an unsupervised ca-pacity; one of the most compelling and fundamental open problems in machine learning and computer vision.
Self-supervision approaches have shown evidence that they can provide a good supervisory signal for video data [24]. In video recordings an object usually maintains its intrinsic feature distribution but changes its predomi-nantly linear relationships between localized features [23].
In this paper we consider the problem of human pose es-timation. Motivated by a wide range of applications includ-Figure 1. Schematic overview of our approach. Top: We deﬁne a part-based human template consisting of 2D Gaussian ellipses and estimate the transformation parameters to estimate the pose of humans with any body composition. Anchor-points are deﬁned between adjacent body parts in order to enforce a connectivity constraint. Bottom: We also evaluate downstream applicability by estimating 3D body poses. ing motion capture, visual surveillance and robot control, a continuous effort has been put into generating datasets and models where a manually annotated ground truth pose and key points are available as labels for full supervision. These are currently the most attractive approaches for industrial applications due to their promise of higher accuracy.
However, ground truth generation is laborious and often limited to a narrow domain, for example, standard poses of healthy adults. In domains with limited demand or special requirements, extensive labeling efforts are often not jus-2484
tiﬁed. Such domains include medical applications, where key point deﬁnitions may vary according to diagnostic aims and body shapes might not comply with the learned expec-tations from a standard training set. Indeed, motion track-ing has a variety of applications in medicine, for example to examine the progression of neurological disease and to evaluate treatment success [29], the assessment of injury [5] or for the early diagnosis of impaired neurological develop-ment in infants [50, 13]. None of these applications allow for excessive data collection and annotation, often because of a limited number of subjects, restrictions on recording in the clinical environment and economical considerations.
Moreover, the direct application of models trained on com-mon benchmark datasets [22, 3, 17] is often challenging.
To tackle this issue, unsupervised and weakly supervised pose estimation methods [23, 34, 30] decompose images into appearance, which encodes individual differences such as clothing or body height and pose, describing the individ-ual’s positioning and conﬁguration of limbs and joints with a canonical latent code.
In this context, self-supervision tasks, such as image reconstruction or translation, have been shown to be powerful tools to estimate pose as a factor of variation across images instead of relying on strong, man-ual supervision signals.
We therefore aim at learning the 2D geometry of ob-ject categories such as humans and infants with no addi-tional supervision. We exploit the structured information provided by raw videos of continuous pose changes and propose to control inductive bias directly for arbitrary ob-ject categories through the manual deﬁnition of very simple templates. Thus, we intend to automatically train a neural representation that can predict the 2D pose from a single input image. We show that if such a 2D pose prediction is accurate and compliant with an expected shape prior, these estimations can be extrapolated to 3D poses with a lower error than other existing methods.
We present a method for the unsupervised estimation of 2D keypoints requiring only a simple template and an unan-notated video of a single human performing actions in front of a static background to learn a meaningful pose represen-tation. Inspired by previous work [24], where this problem is framed as a conditional image generation and translation approach, pose information is utilized to recover a particu-lar frame of a video from any other randomly chosen time-point. Despite the effective use of self-supervision and rep-resentational bottleneck, this approach still requires another prior in the form of unpaired labels and introduces suscep-tibility to domain shift if these labels come from a different datset. Our model however does not require an additional dataset of unpaired 2D pose examples and relies solely on a simple 2D template consisting of connected body parts modeled as 2D Gaussians. The update of these Gaussians can be learned as afﬁne transformations. Even though im-ages are 2D representations of 3D information, afﬁne trans-formations allow to model all possible projected conﬁgu-rations of body parts. Motivated by part-based approaches such as [64] we introduce anchor-points in order to enforce connectivity between body parts and regularize model train-ing and prediction.
In summary, we make the following contributions:
• We introduce a conceptually simple but effective method to learn 2D human-interpretable keypoints based on transforming a single manually deﬁned 2D template.
• Our proposed approach is capable of performing 2D human pose estimation without any additional need for labeled data, either paired or unpaired.
• We demonstrate the high adaptability of our approach by evaluating it on benchmark data and in the wild on a challenging infant pose estimation dataset. 2.