Abstract
In this paper, we proposed a novel Style-based Point
Generator with Adversarial Rendering (SpareNet) for point cloud completion. Firstly, we present the channel-attentive
EdgeConv to fully exploit the local structures as well as the global shape in point features. Secondly, we observe that the concatenation manner used by vanilla foldings limits its potential of generating a complex and faithful shape. En-lightened by the success of StyleGAN, we regard the shape feature as style code that modulates the normalization lay-ers during the folding, which considerably enhances its ca-pability. Thirdly, we realize that existing point supervisions, e.g., Chamfer Distance or Earth Moverâ€™s Distance, cannot faithfully reï¬‚ect the perceptual quality of the reconstructed points. To address this, we propose to project the com-pleted points to depth maps with a differentiable renderer and apply adversarial training to advocate the perceptual realism under different viewpoints. Comprehensive experi-ments on ShapeNet and KITTI prove the effectiveness of our method, which achieves state-of-the-art quantitative perfor-mance while offering superior visual quality. 1.

Introduction
As the 3D scanning devices such as depth camera and
LiDAR become ubiquitous, point clouds get easier to ac-quire and have recently attracted a surge of research interest in the vision and robotics community. However, raw points directly captured by those devices are usually sparse and incomplete due to the limited sensor resolution and occlu-sions. Hence, it is essential to infer the complete shape from the partial observation so as to facilitate various downstream tasks [12] such as classiï¬cation and shape manipulation as required in real-world applications.
âˆ—Equal contribution. Authors did this work during the internship at
Microsoft Research Asia.
Due to the irregularity and unorderedness of point clouds, one workaround is to leverage intermediate repre-sentations, e.g., depth map [15] or voxels [35], that are more amenable to neural networks. However, the represen-tation transform may result in information loss, so the de-tailed structures can not be well preserved. With the emer-gence of point-based networks [25, 26, 29, 32, 12], predom-inant methods [37, 39, 11, 33, 21, 28, 5, 31, 17, 41] nowa-days digest the partial inputs directly and estimate the com-plete point clouds in an end-to-end manner. These methods typically follow the encoder-decoder paradigm and adopt permutation invariant losses [8], e.g., Chamfer Distance or
Earth Moverâ€™s Distance, for regressing the groundtruth.
Ideally, the point completion network should simultane-ously meet the following needs: 1) The output is desired to faithfully preserve the detailed structures of the partial in-put; 2) The network has a strong imaginative power to infer the global shape from the partial clue; 3) the local struc-ture should be sharp, accurate, and free from the corruption by the noise points. Nonetheless, existing methods fail to achieve the above goals because of the neglecting of the global context during the feature extraction, the insufï¬cient capability of modeling the structural details, and the lack of perceptual metrics for measuring the visual quality.
In this paper, we propose Style-based Point generator with Adversarial REndering, i.e., the SpareNet, to circum-vent the above issues. We have made improvements from encoder, generator, and loss function, and proposed 3 new modules: channel-attentive EdgeConv, Style-based Point
Generator, and Adversarial Point Rendering. Firstly, while previous works employ PointNet or PointNet++ to learn point-wise or local features, we propose channel-attentive
EdgeConv (Section 3.1), which not only considers the local information within the k-nearest neighbors but also wisely leverages the global context by aggregating the global fea-tures and weighting the feature channel attention for each point accordingly. The fusion of local and global context 4619
SpareNet
D
D
Image Domain Losses
D
Renderer 
E 3
Ã—
ğ‘€
ğ‘”
ğ¶
G
R 3
Ã—
ğ‘
R 3
Ã—
ğ‘
Renderer 
ğ…
Point Domain Losses 3
Ã—
ğ‘
ğ… 3
Ã—
ğ‘
ğ‘‹
Style-based Point Generation
ğ‘Œğ‘
ğ’ˆ 1
ğ‘Œğ‘Ÿ
ğ‘Œğ‘”ğ‘¡ 2
ğ‘Œğ‘Ÿ
Refinement with Adversarial Point Rendering
= ğ‘Œ
ğ‘Œğ‘”ğ‘¡
Figure 1: The architecture of SpareNet. An encoder E encodes the partial points X into a shape code g, leveraged by a style-based generator G to synthesize a coarse completion Yc, which is recurrently improved with reï¬ner R into the ï¬nal result Y .
Adversarial point rendering is applied to advocate the perceptual realism of completed points under different views.
ğ‘Œğ‘
ğ‘Œ
ğ‘‹
ğ‘Œğ‘”ğ‘¡ enriches the learnt representation, so the network is more powerful to characterize ï¬ne structures of the input.
Further, we claim that the vanilla folding module [37] in conventional methods, which outputs the 3D shapes by morphing the 2D surfaces through multilayer perceptrons (MLP), has limited modeling capability due to the improper usage of the features, i.e., the features are tiled and con-catenated to each location of the 2D lattice. Drawn by the success of StyleGAN [19] in image synthesis, we boost the folding capability by regarding the learnt features as style codes, which can be used to modulate the feature normal-ization within the folding MLPs. The resulting style-based generator, as elaborated in Section 3.2, shows considerably improved capability of modeling structural details.
Last but not least, in order to generate visually-pleasing results, we propose to project the generated point clouds to view images (Section 3.3), whose realism is further exam-ined by adversarial discriminators. Since the renderer we use is differentiable, the gradient from the discriminators will guide the network to learn the completion with high perceptual quality when viewed at different angles. We con-duct extensive experiments on ShapeNet [4] and KITTI [9] datasets, and our SpareNet performs favorably over state-of-the-art methods both quantitatively and qualitatively. 2.