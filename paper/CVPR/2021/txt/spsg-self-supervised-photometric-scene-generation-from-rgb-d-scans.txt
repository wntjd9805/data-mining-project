Abstract 1.

Introduction
We present SPSG, a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene ge-ometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D ge-ometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in or-der to achieve high-resolution, high-quality colored recon-structions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by inform-ing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.
The wide availability of consumer range cameras has propelled research in 3D reconstruction of real-world en-vironments, with applications ranging from content cre-ation to indoor robotic navigation and autonomous driving.
While state-of-the-art 3D reconstruction approaches have now demonstrated robust camera tracking and large-scale reconstruction [21, 15, 31, 6], occlusions and sensor lim-itation lead these approaches to yield reconstructions that are incomplete both in geometry and in color, making them ill-suited for use in the aforementioned applications.
In recent years, geometric deep learning has made sig-niﬁcant progress in learning to reconstruct complete, high-ﬁdelity 3D models of shapes from RGB or RGB-D obser-vations [19, 7, 25, 20, 23], leveraging synthetic 3D shape data to provide supervision for the geometric completion task. Recent work has also advanced generative 3D ap-proaches towards operating on larger-scale scenes [28, 8, 5].
However, producing complete, colored 3D reconstructions of real-world environments remains challenging – in partic-ular, for real-world observations, we do not have complete ground truth data available.
We introduce SPSG, a generative 3D approach to cre-1747
ate high-quality 3D models of real-world scenes from par-tial RGB-D scan observations in a self-supervised fashion.
Our self-supervised approach leverages incomplete RGB-D scans as target by generating a more incomplete version as input by removing frames. This allows correlation of more-incomplete to less-incomplete scans while ignoring unobserved regions. However, the target scan reconstruc-tion from the given RGB-D scan suffers from inconsisten-cies in camera alignments and view-dependent effects, re-sulting in signiﬁcant color artifacts. Moreover, the success of adversarial approaches in 2D image generation [10, 16] cannot be directly adopted when the target scan is incom-plete, as this results in the ‘real’ examples for the discrim-inator taking on incomplete characteristics. Our key ob-servation is that while a 3D scan is incomplete, each indi-vidual 2D frame is complete from its viewpoint. Thus, we leverage the 2D signal provided by the raw RGB-D frames, which provide high-resolution, self-consistent observations as well as photo-realistic examples for adversarial and per-ceptual losses in 2D.
Thus, our generative 3D model predicts a 3D scene reconstruction represented as a truncated signed distance function with per-voxel colors (TSDF), where we leverage a differentiable renderer to compare the predicted geometry and color to the original RGB-D frames. In addition, we employ a 2D adversarial and 2D perceptual loss between the rendering and the original input in order to achieve sharp, high-quality, complete colored 3D reconstructions.
Our experiments show that our 2D-based self-supervised approach towards inferring complete geometric and colored 3D reconstructions produces signiﬁcantly improved perfor-mance in comparison to state of state-of-the-art methods, both quantitatively and qualitatively on both synthetic and real data.
In summary, we present the following contributions:
• We introduce the ﬁrst self-supervised approach to infer a complete, colored reconstruction of 3D scenes from
RGB-D scan observations. This enables training solely on incomplete real-world scan data, without requiring domain adaptation from a synthetic regime.
• We present a view-based synthesis for differentiable rendering of both TSDF geometry and color, and show that this view-based synthesis outperforms supervision relying on 3D reconstruction of the RGB-D scan data. 2.