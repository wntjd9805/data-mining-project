Abstract
We present a method that learns a spatiotemporal neural ir-radiance ﬁeld for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradi-ance ﬁeld from a single video poses signiﬁcant challenges because the video contains only one observation of the
∗ This work was done while Wenqi was an intern at Facebook. scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appear-ance and vice versa. We address this ambiguity by con-straining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from indi-vidual frames into a single global representation. We pro-vide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results. 9421
1.

Introduction
This paper addresses the problem of rendering a video from novel viewpoints. Speciﬁcally, we learn a globally consis-tent, dynamic scene representation that can later be rendered from a novel viewpoint. We learn such a representation from a casually captured single video from everyday de-vices such as smartphones, without the assistance of multi-camera rigs or other dedicated hardware (which are typi-cally not accessible to casual users).
Free-viewpoint video rendering typically requires a com-plicated hardware setup consisting of multiple cameras to capture the scene of interest from different viewpoints [78, 11, 14, 6]. The multi-camera setup in existing methods is required because conventional 3D reconstruction algo-rithms (multi-view stereopsis) assume a fully static scene and thus can perform reconstruction using the multiple cap-tured viewpoints of a dynamic scene at any time. Most methods represent the geometric reconstructions as some form of per-frame representation (e.g., depth maps [78] or meshes[6]). Rendering from a novel viewpoint can then be achieved, e.g., by warping available views using their depth maps to the new viewpoint.
Following the success of single-image depth estimation, recent monocular video depth estimation methods allow for the acquisition of consistent per-frame depth estimates from only a single video [38, 74]. While still at an early stage, this line of work opens up new possibilities, where monoc-ular scene depth estimates can be directly used for view synthesis. However, naïve approaches such as per-frame depth-based warping would lead to unnatural stretches and reveal holes in disoccluded regions (even with perfect depth estimates). One can alleviate this by post-processing the incomplete rendering [74]. However, such per-frame pro-cessing methods often lead to temporal ﬂickers. The core problem lies in the use of a frame-wise representation (e.g., depth maps associated with the input images), and therefore suffer from issues ranging from temporal inconsistency to high redundancy and thus excessive storage requirements and data transfer bandwidth.
In this work, we build on recent monocular video depth estimation methods and aggregate the entire spatiotemporal aspects of a dynamic scene in a single global representa-tion. While fusing multiple depth maps into a single, global representation has a long tradition, most work on volumet-ric depth integration has focused on static scenes [12, 46] or geometry alone without textures [45]. These methods typically use discrete representations such as voxel grids, meshes, or point clouds. Consequently, these methods often suffer from premature hard decisions on geometry estima-tion and limited resolution due to high storage requirements.
In this paper, we, instead, turn to the recent advances in neural implicit representations, which allow for con-tinuous representations of a scene without resolution loss.
Recent work has shown that these representations achieve high-quality view interpolation of complex static scenes while retaining their advantages over discrete representa-tions [44, 75, 34]. The current approaches to learn them, however, require either multiple posed images of a fully static scene [44, 75, 34] or ground truth 3D representa-tions [58, 59]. While videos often contain appearances of a scene seen from multiple viewpoints, they only contain ex-actly one viewpoint at any given time. Combined with the dynamic nature of video, this renders it nontrivial to extend current approaches to learn spatiotemporal representations from a single video.
Speciﬁcally, we learn neural irradiance ﬁelds as a func-tion of both space and time for each video. We do not model view dependency, hence we use the term irradiance. Us-ing supervision from only color frames of the input video as in [44] is futile, since the variations between frames can be explained with either a change of appearance or geom-etry, or a combination of both. We resolve this ambiguity using the per-frame scene depth estimated from monocular video depth estimation. Our depth supervision constrains the scene’s geometry at any moment and disambiguates it from appearance variations. While this enables us to encode physically correct appearance and geometry in a global rep-resentation, it fails to ﬁll the holes that could be seen at other time steps in the video. We address this by encourag-ing the color and volume density to propagate across time whenever spatial locations are not supervised otherwise.
The resulting representation allows us to render the video from novel viewpoints and time: our implicit model can be queried at any spatiotemporal location and rendered using standard volume rendering†.
Our technical contributions include the following:
• We aggregate frame-wise 2.5D representations into a globally consistent spatiotemporal representation from a single monocular video.
• We address the inherent motion–appearance ambiguity using video depth supervision and constrain the disoc-cluded contents by propagating the color and volume density across time.
• We demonstrate a compelling free-viewpoint video rendering experience on various casual videos shot from smartphones, preserving motion and texture de-tails while conveying a vivid sense of 3D. 2.