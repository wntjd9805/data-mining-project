Abstract
The objective of this work is to localize sound sources that are visible in a video without using manual annota-tions. Our key technical contribution is to show that, by training the network to explicitly discriminate challenging image fragments, even for images that do contain the ob-ject emitting the sound, we can signiﬁcantly boost the lo-calization performance. We do so elegantly by introducing a mechanism to mine hard samples and add them to a con-trastive learning formulation automatically. We show that our algorithm achieves state-of-the-art performance on the popular Flickr SoundNet dataset. Furthermore, we intro-duce the VGG-Sound Source (VGG-SS) benchmark, a new set of annotations for the recently-introduced VGG-Sound dataset, where the sound sources visible in each video clip are explicitly marked with bounding box annotations. This dataset is 20 times larger than analogous existing ones, contains 5K videos spanning over 200 categories, and, dif-ferently from Flickr SoundNet, is video-based. On VGG-SS, we also show that our algorithm achieves state-of-the-art performance against several baselines. Code and datasets can be found at http://www.robots.ox.ac.uk/
˜vgg/research/lvs/. 1.

Introduction
While research in computer vision largely focuses on the visual aspects of perception, natural objects are character-ized by much more than just appearance. Most objects, in particular, emit sounds, either in their own right, or in their interaction with the environment — think of the bark of a dog, or the characteristic sound of a hammer striking a nail. A full understanding of natural objects should not ignore their acoustic characteristics. Instead, modelling ap-pearance and acoustics jointly can often help us understand them better and more efﬁciently. For example, several au-thors have shown that it is possible to use sound to discover and localize objects automatically in videos, without the use of any manual supervision [1, 2, 14, 17, 24, 30].
In this paper, we consider the problem of localizing ‘vi-sual sounds’, i.e. visual objects that emit characteristics sounds in videos. Inspired by prior works [2, 14, 30], we formulate this as ﬁnding the correlation between the visual and audio streams in videos. These papers have shown that not only can this correlation be learned successfully, but that, once this is done, the resulting convolutional neural networks can be ‘dissected’ to localize the sound source spatially, thus imputing it to a speciﬁc object. However, other than in the design of the architecture itself, there is lit-tle in this prior work meant to improve the localization capa-bilities of the resulting models. In particular, while several models [1, 2, 30] do incorporate a form of spatial attention which should also help to localize the sounding object as a byproduct, these may still fail to provide a good coverage of the object, often detecting too little or too much of it.
In order to address this issue, we propose a new training scheme that explicitly seeks to spatially localize sounds in 16867
video frames. Similar to object detection [35], in most cases only a small region in the image contains an object of inter-est, in our case a ‘sounding’ object, with the majority of the image often being ‘background’ which is not linked to the sound. Learning accurate object detectors involves explic-itly seeking for these background regions, prioritizing those that could be easily confused for the object of interest, also called hard negatives [7, 13, 21, 28, 31, 35]. Given that we lack supervision for the location of the object making the sound, however, we are unable to tell which boxes are pos-itive or negative. Furthermore, since we seek to solve the localization rather than the detection problem, we do not even have bounding boxes to work with, as we seek instead a segmentation of the relevant image area.
In order to incorporate hard evidence in our unsupervised (or self-supervised) setting, we propose an automatic back-ground mining technique through differentiable threshold-ing, i.e. regions with low correlation to the given sound are incorporated into a negatives set for contrastive learning. In-stead of using hard boundaries, we note that some regions may be uncertain, and hence we introduce the concept of a Tri-map into the training procedure, leaving an ‘ignore’ zone for our model. To our knowledge, this is the ﬁrst time that background regions have been explicitly consid-ered when solving the sound source localization problem.
We show that this simple change signiﬁcantly boosts sound localization performance on standard benchmarks, such as
Flickr SoundNet [30].
To further assess sound localization algorithms, we also introduce a new benchmark, based on the recently-introduced VGG-Sound dataset [4], where we provide high-quality bounding box annotations for ‘sounding’ objects, i.e. objects that produce a sound, for more than 5K videos spanning 200 different categories. This dataset is 20× larger and more diverse than existing sound localization benchmarks, such as Flickr SoundNet (the latter is also based on still images rather than videos). We believe this new benchmark, which we call VGG-Sound Source, or
VGG-SS for short, will be useful for further research in this area. In the experiments, we establish several baselines on this dataset, and further demonstrate the beneﬁts of our new algorithm. 2.