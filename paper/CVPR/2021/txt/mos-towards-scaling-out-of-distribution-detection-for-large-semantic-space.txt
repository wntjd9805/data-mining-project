Abstract
Detecting out-of-distribution (OOD) inputs is a central challenge for safely deploying machine learning models in the real world. Existing solutions are mainly driven by small datasets, with low resolution and very few class la-bels (e.g., CIFAR). As a result, OOD detection for large-scale image classiﬁcation tasks remains largely unexplored.
In this paper, we bridge this critical gap by proposing a group-based OOD detection framework, along with a novel
OOD scoring function termed MOS. Our key idea is to decompose the large semantic space into smaller groups with similar concepts, which allows simplifying the deci-sion boundaries between in- vs. out-of-distribution data for effective OOD detection. Our method scales substantially better for high-dimensional class space than previous ap-proaches. We evaluate models trained on ImageNet against four carefully curated OOD datasets, spanning diverse se-mantics. MOS establishes state-of-the-art performance, re-ducing the average FPR95 by 14.33% while achieving 6x speedup in inference compared to the previous best method. 1.

Introduction
Out-of-distribution (OOD) detection has become a cen-tral challenge in safely deploying machine learning mod-els in the open world, where the test data may be distri-butionally different from the training data. A plethora of literature has emerged in addressing the problem of OOD detection [3, 14, 16, 24, 27, 29, 35, 4, 20, 31]. However, ex-isting solutions are mainly driven by small, low-resolution datasets such as CIFAR [23] and MNIST [25]. Deployed systems like autonomous vehicles often operate on images that have far greater resolution and perceive environments with far more categories. As a result, a critical research gap exists in developing and evaluating OOD detection al-gorithms for large-scale image classiﬁcation tasks.
While one may be eager to conclude that solutions for small datasets should transfer to a large-scale setting, we argue that this is far from the truth. The main challenges posed in OOD detection stem from the fact that it is impos-sible to comprehensively deﬁne and anticipate anomalous data in advance, resulting in a large space of uncertainty.
As the number of semantic classes increases, the plethora of ways that OOD data may occur increases exponentially. For example, our analysis reveals that the average false positive rate (at 95% true positive rate) of a common baseline [16] would rise from 17.34% to 76.94% as the number of classes increases from 50 to 1,000 on ImageNet-1k [8]. Very few works have studied OOD detection in the large-scale set-ting, with limited evaluations and effectiveness [41, 15].
This begs the following question: how can we design an
OOD detection algorithm that scales effectively for classiﬁ-cation with large semantic space?
Motivated by this, we take an important step to bridge this gap and propose a group-based OOD detection frame-work that is effective for large-scale image classiﬁcation.
Our key idea is to decompose the large semantic space into smaller groups with similar concepts, which allows simpli-fying the decision boundary and reducing the uncertainty space between in- vs. out-of-distribution data. Intuitively, for OOD detection, it is simpler to estimate whether an im-age belongs to one of the coarser-level semantic groups than to estimate whether an image belongs to one of the ﬁner-grained classes. For example, consider a model tasked with classifying 200 categories of plantations and another 200 categories of marine animals. A truck image can be eas-ily classiﬁed as OOD data since it does not resemble either the plantation group or the marine animal group.
Formally, our proposed method leverages group soft-max and derives a novel OOD scoring function. Speciﬁ-cally, the group softmax computes probability distributions within each semantic group. A key component is to uti-lize a category others in each group, which measures the probabilistic score for an image to be OOD with respect to the group. Our proposed OOD scoring function, Min-imum Others Score (MOS), exploits the information car-ried by the others category. As illustrated in Figure 1,
MOS is higher for OOD inputs as they will be mapped to others with high conﬁdence in all groups, and is lower for in-distribution inputs. 8710
ImageNet-1k iNaturalist
SUN
Places
Textures
Animal Group x a m t f o s others kit fox
English settler 0.900 0.012 0.015 sea cucumber 0.020
In-distribution
Artifact Group
Out-of-distribution x a m t f o s others revolver warplane dumbell 0.850 0.008 0.013 0.025
Plant Group x a m t f o s others daisy yellow lady-slipper 0.950 0.020 0.030
MOS (Minimum Others Score)
Min 0.850 0.012
Min 0.900 0.850 0.950 0.800 0.012 0.900
?
Out-of-distribution
In-distribution 0.800 0.016 0.022 0.012 0.012 0.020 0.900 0.008 0.900 0.044 0.056
Feature Extraction
Group Softmax
OOD Detection
Figure 1: Top: Examples of in-distribution images sampled from ImageNet-1k (in green) and OOD images sampled from 4 datasets described in Section 4.1 (in orange). Bottom: Overview of the proposed group-based OOD detection framework. The key idea is to de-compose the large semantic space into smaller groups, which allows simplifying the decision boundary between in- and out-of-distribution data. A category others is added to each group. An OOD image is mapped to others with high conﬁdence for all groups, whereas an in-distribution image will have a lower score for others in the group it belongs to (e.g., artifact group). The minimum score on category others among all groups, MOS, allows effective differentiation of OOD data.
We extensively evaluate our approach on models trained with the ImageNet-1k dataset, leveraging the state-of-the-art pre-trained BiT-S models [22] as backbones. We explore label space of size 10-100 times larger than that of previous works [16, 29, 27, 31, 4, 17]. Compared to the best base-line [15], our method improves the average performance of
OOD detection by 14.33% (FPR95) over four diverse OOD test datasets. More importantly, our method achieves im-proved OOD detection performance while preserving the classiﬁcation accuracy on in-distribution datasets. We note that while group-based learning has been used for improv-ing tasks such as long-tail object detection [28], our ob-jective and motivation are very different—we are inter-ested in reducing the uncertainty between in- and out-of-distribution data, rather than reducing the confusion among in-distribution data themselves. Below we summarize our key results and contributions:
• We propose a group-based OOD detection frame-work, along with a novel OOD scoring function MOS, that scales substantially better for large label space.
Our method establishes the new state-of-the-art perfor-mance, reducing the average FPR95 by 14.33% while achieving 6x speedup in inference time compared to the best baseline.
• We conduct extensive ablations which improve under-standings of our method for large-scale OOD detection under (1) different grouping strategies, (2) different sizes of semantic class space, (3) different backbone architectures, and (4) varying ﬁne-tuning capacities.
• We curate diverse OOD evaluation datasets from four real-world high-resolution image databases, which en-ables future research to evaluate OOD detection meth-ods in a large-scale setting1. 2. Preliminary and Analysis
Preliminaries We consider a training dataset drawn i.i.d. from the in-distribution PX , with label space Y =
{1, 2, · · · , C}. For OOD detection problem, it is common to train a classiﬁer f (x) on the in-distribution PX , and eval-uate on samples that are drawn from a different distribution
QX . An OOD detector G(x) is a binary classiﬁer: in, out, if S(x) ≥ γ if S(x) < γ,
G(x) = ( 1Code and data for reproducing our results are available at: https:
//github.com/deeplearning-wisc/large_scale_ood 8711
where S(x) is the scoring function, and γ is the threshold chosen so that a high fraction (e.g., 95%) of in-distribution data is correctly classiﬁed.
PC
Effect of Number of Classes on OOD Detection We
ﬁrst revisit the common baseline approach [16], which uses the maximum softmax probability (MSP), S(x) = efi (x) j=1 efj (x) , for OOD detection. We investigate the maxi effect of label space size on the OOD detection perfor-mance.
In particular, we use a ResNetv2-101 architec-ture [13] trained on different subsets2 of ImageNet with varying numbers of classes C. As shown in Figure 2, the performance (FPR95) degrades rapidly from 17.34% to 76.94% as the number of in-distribution classes increases from 50 to 1,000. This trend signiﬁes that current OOD de-tection methods are indeed challenged by the increasingly large label space, which motivates our work.
C
O
R
U
A 95 90 85 80 75 80 60 40 20 5 9
R
P
F 1000 200 400 800
Number of classes C 600 iNaturalist
SUN
Places
Textures 1000 200 400 800
Number of classes C 600
Figure 2: OOD detection performance of a common baseline
MSP [16] decreases rapidly as the number of ImageNet-1k classes increases (left: AUROC; right: FPR95). 3. Method
Our novel group-based OOD detection framework is il-lustrated in Figure 1. In what follows, we ﬁrst provide an overview and then describe the group softmax training tech-nique in Section 3.1. We introduce our proposed OOD de-tection algorithm MOS in Section 3.2, followed with group-ing strategies in Section 3.3.
Method Overview: A Conceptual Example As afore-mentioned, OOD detection performance can suffer notably from the increasing number of in-distribution classes. To mitigate this issue, our key idea is to decompose the large semantic space into smaller groups with similar concepts, which allows simplifying the decision boundary and reduc-ing the uncertainty space between in- vs. out-of-distribution data. We illustrate our idea with a toy example in Figure 3, where the in-distribution data consists of class-conditional
Gaussians. Without grouping (left), the decision bound-ary between in- vs. OOD data is determined by all classes and becomes increasingly complex as the number of classes 2To create the training subset, we ﬁrst randomly select C (C ∈ {50, 200, 300, 400, 500, 600, 700, 800, 900, 1000}) labels from the 1,000 ImageNet classes. For each of the chosen label, we then sample 700 images for training.
C3
C5
C1
After
Grouping
C3
C1
C5
C2
C4
C6
C2
C6
C4
OOD samples
In-distribution classes
Decision boundaries between in- vs. out-of-distribution data
Figure 3: A toy example in 2D space of group-based OOD de-tection framework. Left: without grouping, the decision bound-ary between in- vs. out-of-distribution data becomes increasingly complex with more classes. Right: our group-based method sim-pliﬁes the decision boundary and reduces the uncertainty space for
OOD data.
In contrast, with grouping (right), the decision grows. boundary for OOD detection can be signiﬁcantly simpliﬁed, as shown by the dotted curves.
In other words, by way of grouping, the OOD detector only needs to make a small number of relatively simple es-timations about whether an image belongs to this group, as opposed to making a large number of hard decisions about whether an image belongs to this class. An image will be classiﬁed as OOD if it belongs to none of the groups.
We proceed with describing the training mechanism that achieves our novel conceptual idea. 3.1. Group based Learning
We divide the total number of C categories into K groups, G1, G2, ..., GK. We calculate the standard group-wise softmax for each group Gk: c (x) ef k c (x) = pk
, c ∈ Gk, c′ (x) ef k (1) c′∈Gk c (x) and pk where f k max probability for class c in group Gk, respectively. c (x) denote the output logit and the soft-P
Category “Others” Standard group softmax is insufﬁ-cient as it can only discriminate classes within the group, but cannot estimate the OOD uncertainty between inside vs. outside the group. To this end, a new category others is introduced to every group, as shown in Figure 1. The model can predict others if the input x does not belong to this group. In other words, the others category allows explic-itly learning the decision boundary between inside vs. out-side the group, as illustrated by the dashed curves surround-ing classes C1/C2/C3 in Figure 3. This is desirable for OOD detection, as an OOD input can be mapped to others for all groups, whereas an in-distribution input will be mapped to one of the semantic categories in some group with high conﬁdence.
Importantly, our use of the category others creates
“virtual” group-level outlier data without relying on any ex-ternal data. Each training example x not only helps esti-mate the decision boundary for the classiﬁcation problem, 8712
but also effectively improves the OOD uncertainty estima-tion for groups to which it does not belong. We show the formulation can in fact achieve the dual objective of in-distribution classiﬁcation, as well as OOD detection. from the animal group in the ImageNet-1k dataset. The minimum others score among all groups is signiﬁcantly lower for in-distribution data than that for OOD data, allow-ing for effective differentiation between them.
Training and Inference During training, the ground-truth labels are re-mapped in each group. In groups where c is not included, class others will be deﬁned as the ground-truth class. The training objective is a sum of cross-entropy losses in each group:
LGS = − 1
N
N
K n=1
X c∈Gk k=1 X
X c log(pk yk c (x)), (2) c and pk where yk c represent the label and the softmax prob-ability of category c in Gk, and N is the total number of training samples.
We denote the set of all valid (non-others) classes in each group as G′ k = Gk\{others}. During inference time, we derive the group-wise class prediction in the valid set for each group:
ˆpk = max c∈G′ k c (x), ˆck = arg max pk c∈G′ k c (x). pk
Then we use the maximum group-wise softmax score and the corresponding class for ﬁnal prediction: k∗ = arg max 1≤k≤K
ˆpk.
The ﬁnal prediction is category ˆck∗ from group Gk∗ . 3.2. OOD Detection with MOS
For a classiﬁcation model trained with the group softmax loss, we propose a novel OOD scoring function, Minimum
Others Score (MOS), that allows effective differentiation between in- vs. out-of-distribution data. Our key observa-tion is that category others carries useful information for how likely an image is OOD with respect to each group.
As discussed in Section 3.1, an OOD input will be mapped to others with high conﬁdence in all groups, whereas an in-distribution input will have a low score on category others in the group it belongs to. Therefore, the lowest others score among all groups is crucial for distin-guishing between in- vs. out-of-distribution data. This leads to the following OOD scoring function, termed as Minimum
Others Score:
SMOS(x) = − min 1≤k≤K others(x). pk (3)
Note that we negate the sign to align with the conventional notion that SMOS(x) is higher for in-distribution data and lower for out-of-distribution.
To provide an interpretation and intuition behind MOS, we show in Figure 4 the average scores for the category others in each group for both in-distribution and OOD images. For in-distribution, we select all validation images
In-distribution - Animal Group
OOD - iNaturalist s e r o c s s r e h t o f o e g a r e v
A 1.0 0.8 0.6 0.4 0.2 0.0 s e r o c s s r e h t o f o e g a r e v
A 1.0 0.8 0.6 0.4 0.2 0.0 ni m
A al
A rtifa ct
F o r m u s g n a tio n
F u
M is c
N a t u r al O bj
P e rs o n
Pla n t ni m
A al
A rtifa ct
F o r m u s g n a tio n
F u
M is c
N a t u r al O bj
P e rs o n
Pla n t
Figure 4: Average of others scores in each group for both in-distribution data (left) and OOD data (right). 3.3. Grouping Strategies
Given the dependency on the group structure, a natural question arises: how do different grouping strategies affect the performance of OOD detection? To answer this, we systematically consider three grouping strategies: (1) tax-onomy, (2) feature clustering, and (3) random grouping.
Taxonomy The ﬁrst grouping strategy is applicable when the taxonomy of the label space is known. For example, in the case of ImageNet, each class is associated with a synset in WordNet [34], from which we can build the taxonomy as a hierarchical tree. In particular, we adopt the 8 super-classes deﬁned by ImageNet3 as our groups and map each category into one of the 8 groups: animal, artifact, geological formation, fungus, misc, natural object, person, and plant.
Feature Clustering When taxonomy is not available, we can approximately estimate the structure of semantic classes through feature clustering. Speciﬁcally, we extract feature representations for each training image from a pre-trained feature extractor. Then, the feature representation of each class is the average of feature embeddings in that class. Finally, we perform a K-Means clustering [32] on categorical feature representations, one for each class.
Random Grouping Lastly, we contrast the taxonomy and the feature clustering strategies with random grouping, where each class is randomly assigned to a group. This al-lows us to estimate the lower bound of OOD detection per-formance with MOS.
By default, we use taxonomy as the grouping strategy if not speciﬁed otherwise. In Section 4.3.3, we experimentally compare the OOD detection performance using all three grouping strategies. 3http://image-net.org/explore 8713
Method
MSP [16]
ODIN [29]
Mahalanobis [27]
Energy [31]
KL Matching [15]
MOS (ours)
Test
Time (min) 3.1 23.6 145.4 3.1 20.6 3.2 iNaturalist
SUN
Places
Textures
Average
AUROC
↑
FPR95
↓
AUROC
↑
FPR95
↓
AUROC
↑
FPR95
↓
AUROC
↑
FPR95
↓
AUROC
↑
FPR95
↓ 87.59 89.36 46.33 88.48 93.00 98.15 63.69 62.69 96.34 64.91 27.36 9.28 78.34 83.92 65.20 85.32 78.72 92.01 79.98 71.67 88.43 65.33 67.52 40.63 76.76 80.67 64.46 81.37 76.49 89.06 81.44 76.27 89.75 73.02 72.61 49.54 74.45 76.30 72.10 75.79 87.07 81.23 82.73 81.31 52.23 80.87 49.70 60.43 79.29 82.56 62.02 82.74 83.82 90.11 76.96 72.99 81.69 71.03 54.30 39.97
Table 1: OOD detection performance comparison between MOS and baselines. All methods are ﬁne-tuned from the same pre-trained
BiT-S-R101x1 backbone with ImageNet-1k as in-distribution dataset. The description of 4 OOD test datasets is provided in Section 4.1. ↑ indicates larger values are better, while ↓ indicates smaller values are better. All values are percentages. Bold numbers are superior results.
Test time for all methods are evaluated with the same in- and out-of-distribution datasets (60k images in total). 4. Experiments
We ﬁrst describe the evaluation datasets (Section 4.1) and experimental setups (Section 4.2). In Section 4.3, we show that MOS achieves state-of-the-art OOD detection performance, followed by extensive ablations that improve the understandings of MOS for large-scale OOD detection. 4.1. Datasets
In-distribution Dataset 4.1.1
We use ImageNet-1k [8] as the in-distribution dataset, which covers a wide range of real-world objects. ImageNet-1k has at least 10 times more labels compared to CIFAR datasets used in prior literature. In addition, the image reso-lution is also signiﬁcantly higher than CIFAR (32×32) and
MNIST (28×28). 4.1.2 Out-of-distribution Datasets
To evaluate our approach, we consider a diverse collec-tion of OOD test datasets, spanning various domains in-cluding ﬁne-grained images, scene images, and textural im-ages. We carefully curate the OOD evaluation benchmarks to make sure concepts in these datasets do not overlap with
ImageNet-1k. Below we describe the construction of each evaluation dataset in detail. Samples of each OOD dataset are provided in Figure 1. We provide the list of concepts chosen for each OOD dataset in Appendix A. iNaturalist iNaturalist [46] is a ﬁne-grained dataset con-taining 859,000 images across more than 5,000 species of plants and animals. All images are resized to have a max dimension of 800 pixels. We manually select 110 plant classes not present in ImageNet-1k, and randomly sample 10,000 images for these 110 classes.
SUN SUN [48] is a scene database of 397 categories and 130,519 images with sizes larger than 200 × 200. SUN and
ImageNet-1k have overlapping categories. Therefore, we carefully select 50 nature-related concepts that are unique in SUN, such as forest and iceberg. We randomly sample 10,000 images for these 50 classes.
Places Places365 [50] is another scene dataset with sim-ilar concept coverage as SUN. All images in this dataset have been resized to have a minimum dimension of 512. We manually select 50 categories from this dataset that are not present in ImageNet-1k and then randomly sample 10,000 images for these 50 categories.
Textures Textures [6] consists of 5,640 images of textural patterns, with sizes ranging between 300 × 300 and 640 × 640. We use the entire dataset for evaluation. 4.2. Experiment Setup
Pre-trained Backbone We use Google BiT-S mod-els [22] as our feature extractor in all experiments. The models are trained on ImageNet-1k, with ResNetv2 archi-tectures [13] at varying capacities. Pre-trained models allow extracting high-quality features with minimal time and en-ergy consumption. In practice, one can always choose to train from scratch.
For the main results, we use the BiT-S-R101x1 model with depth 101 and width factor 1, unless speciﬁed other-wise. We provide a comparison of using feature extractors of varying model sizes in Section 4.3.4. For efﬁciency, we
ﬁx the backbone and only ﬁne-tune the last fully-connected (FC) layer in the main experiments. We additionally ex-plore the effect of ﬁne-tuning more layers beyond the last
FC layer in Section 4.3.5.
Training Details We follow the procedure in BiT-HyperRule [22] and ﬁne-tune the pre-trained BiT-S model for 20k steps with a batch size of 512. We use SGD with an initial learning rate of 0.003 and a momentum of 0.9. The learning rate is decayed by a factor of 10 at 30%, 60%, and 90% of the training steps. During training, all images are resized to 512 × 512 and randomly cropped to 480 × 480.
At test time, all images are resized to 480 × 480. A learning rate warm-up is used for the ﬁrst 500 steps. We perform all experiments on NVIDIA GeForce RTX 2080Ti GPUs.
Evaluation Metrics We measure the following metrics that are commonly used for OOD detection: (1) the false positive rate of OOD examples when the true positive rate 8714
C
O
R
U
A 5 9
R
P
F 98 96 94 92 90 88 60 50 40 30 20 10 iNaturalist
SUN
Places
Textures
C
O
R
U
A 95 90 85 80 95 90 85 80
C
O
R
U
A
C
O
R
U
A 95 90 85 80 75 200 400 600 800 1000 200 400 600 800 1000 200 400 600 800 1000 200 400 600 800 1000
Number of classes C iNaturalist
Number of classes C
SUN
Number of classes C
Places
Number of classes C
Textures 5 9
R
P
F 80 70 60 50 40 30 20 10 80 70 60 50 40 30 20 5 9
R
P
F 80 70 60 50 40 30 5 9
R
P
F 200 400 600 800 1000 200 400 600 800 1000 200 400 600 800 1000 200 400 600 800 1000
Number of classes C
Number of classes C
Number of classes C
Number of classes C
MSP (baseline)
MOS (ours)
Figure 5: OOD detection performance of MOS (blue) and the MSP baseline (gray). MOS exhibits more stabilized performance as the number of in-distribution classes increases. For each OOD dataset, we show AUROC (top) and FPR95 ( bottom). of in-distribution examples is at 95% (FPR95); (2) the area under the receiver operating characteristic curve (AUROC).
We additionally report the area under the precision-recall curve (AUPR) in Appendix D. 4.3. Results 4.3.1 MOS vs. Existing Methods
The main results are shown in Table 1. We report perfor-mance for each dataset described in Section 4.1, as well as the average performance. For fair evaluation, we compare with competitive methods in the literature that derive OOD scoring functions from a model trained on in-distribution data and do not rely on auxiliary outlier data. We ﬁrst com-pare with approaches driven by small datasets, including
MSP [16], ODIN [29], Mahalanobis [27], as well as En-ergy [31]. All these methods rely on networks trained with
ﬂat softmax. Under the same network backbone (BiT-S-R101x1), MOS outperforms the best baseline Energy [31] by 31.06% in FPR95. It is also worth noting that ﬁne-tuning with group softmax maintains competitive classiﬁcation ac-curacy (75.16%) on in-distribution data compared with its
ﬂat softmax counterpart (75.20%).
We also compare our method with KL matching [15], a competitive baseline evaluated on large-scale image classi-ﬁcation. MOS reduces FPR95 by 14.33% compared to KL matching. Note that for each input, KL matching needs to calculate its KL divergence to all class centers. Therefore, the running time of KL matching increases linearly with the number of in-distribution categories, which can be compu-tationally expensive for a very large label space. As shown in Table 1, our method achieves a 6x speedup compared to
KL matching. 4.3.2 MOS with Increasing Numbers of Classes
In Figure 5, we show the OOD detection performance as we increase the number of in-distribution classes
C ∈ {50, 200, 300, 400, 500, 600, 700, 800, 900, 1000} on
ImageNet-1k. For each C, we create training data by ﬁrst randomly sampling C labels from the entire 1k classes, and then sampling 700 images for each chosen label.
Impor-tantly, we observe that MOS (in blue) exhibits more stabi-lized performance as C increases, compared to MSP [16] (in gray). For example, on the iNaturalist OOD dataset,
FPR95 rises from 21.02% to 63.36% using MSP, whilst
MOS degrades by only 4.76%. This trend signiﬁes that
MOS is an effective approach for scaling OOD detection towards a large semantic space.
We also explore an alternative setting where we ﬁx the total number of training images, as we vary the number of classes C.
In this setting, the model is trained on fewer images per class as the number of classes increases, making the problem even more challenging. We report those results in Appendix B.1. Overall, MOS remains less sensitive to the number of classes compared to the MSP baseline. 4.3.3 MOS with Different Grouping Strategies
In this ablation, we contrast the performance of three dif-ferent grouping strategies described in Section 3.3. For a fair comparison, we use the number of groups K = 8 for all methods, since ImageNet taxonomy has 8 super-classes.
For feature clustering, we ﬁrst extract the feature vector from the penultimate layer of the pre-trained BiT-S model for each training image. The feature representation for each category is the average feature vector among all images in that category. We then perform a K-Means clustering on the 1,000 categorical feature vectors (one for each class) with 8715
100 98.2
C
O
R
U
A 5 9
R
P
F 95 90 85 80 75 70 80 60 40 20 0 93.3 90.6 87.6 92.0 84.8 89.1 81.8 78.3 77.0 76.8 76.6 80.6 81.2 79.0 74.5 iNaturalist
SUN
Places
Textures 81.9 80.0 81.4 78.5 82.7 63.7 63.1 47.5 41.1 40.6 66.3 49.5 65.2 64.6 60.4 9.3 iNaturalist
SUN
Places
Textures
MSP (baseline)
MOS (random)
MOS (cluster)
MOS (taxonomy)
Figure 6: OOD detection performance comparison between MOS with different grouping strategies and the MSP baseline on 4 OOD datasets. (top: AUROC; bottom: FPR95).
K = 8. For random grouping, we randomly split 1,000 classes into 8 groups with equal sizes (125 classes each).
We compare the performance of MOS under different grouping strategies in Figure 6. We observe that feature clustering works substantially better than the MSP base-line [16] while maintaining similar in-distribution classiﬁ-cation accuracy (-0.16%) to the taxonomy-based grouping.
Interestingly, random grouping achieves better performance than the MSP baseline [16] on 3 out of 4 OOD datasets.
However, we do observe a drop of in-distribution classiﬁca-tion accuracy (-0.98%) using random grouping, compared to the taxonomy-based grouping. We argue that feature clustering is a viable strategy when taxonomy is unavail-able, as it outperforms MSP by 18.2% (FPR95) on average.
We additionally report how different numbers of groups K affect the OOD detection performance for all three grouping strategies in Appendix B.2. 4.3.4 MOS with Different Feature Extractors
We investigate how the performance of OOD detection changes as we employ different pre-trained feature extrac-tors. In Figure 7, we compare the performance of using a family of 5 feature extractors (in increasing size): BiT-S-R50x1, BiT-S-R101x1, BiT-S-R50x3, BiT-S-R152x2, BiT-S-R101x34. All models are ResNetv2 architectures with varying depths and width factors.
It is important to note that since we ﬁx the entire backbone and only ﬁne-tune the last FC layer, this ablation is about the effect of the quality of feature extractors rather than model capacities.
As we use feature extractors trained on larger capacities, the classiﬁcation accuracy increases, with comparable per-4https : / / github . com / google - research / big _ transfer formance between using the ﬂat vs. group softmax. Overall the OOD detection performance improves as the capacity of feature extractors increases. More importantly, MOS con-sistently outperforms MSP [16] in all cases. These results suggest that using pre-trained models with better feature representations will not only improve classiﬁcation accu-racy but also beneﬁt OOD detection performance. 4.3.5 MOS with Varying Fine-tuning Capacities
In this ablation, we explore the efﬁcacy of ﬁne-tuning more layers. Concretely, we go beyond the FC layer and ﬁne-tune different numbers of residual blocks in BiT-S-R101x1. Fig-ure 8 shows the classiﬁcation accuracy and OOD detection performance under different ﬁne-tuning capacities. Notice-ably, MOS consistently outperforms MSP [16] in OOD de-tection under all ﬁne-tuning capacities. As expected, we observe that ﬁne-tuning more layers leads to better clas-siﬁcation accuracy. However, increasing the number of
ﬁne-tuning layers would adversely affect OOD detection in some cases. We hypothesize that ﬁne-tuning with more lay-ers will result in more label-overﬁtted predictions, and un-desirably produce higher conﬁdence scores for OOD data.
This suggests that only ﬁne-tuning the top FC layers is not only computationally efﬁcient but also in fact desirable for
OOD detection performance. 5.