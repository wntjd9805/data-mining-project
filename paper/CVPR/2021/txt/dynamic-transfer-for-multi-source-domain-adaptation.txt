Abstract
Recent works of multi-source domain adaptation focus on learning a domain-agnostic model, of which the param-eters are static. However, such a static model is difﬁcult to handle conﬂicts across multiple domains, and suffers from a performance degradation in both source domains and tar-get domain. In this paper, we present dynamic transfer to address domain conﬂicts, where the model parameters are adapted to samples. The key insight is that adapting model across domains is achieved via adapting model across sam-ples. Thus, it breaks down source domain barriers and turns multi-source domains into a single-source domain. This also simpliﬁes the alignment between source and target do-mains, as it only requires the target domain to be aligned with any part of the union of source domains. Further-more, we ﬁnd dynamic transfer can be simply modeled by aggregating residual matrices and a static convolution ma-trix. Experimental results show that, without using domain labels, our dynamic transfer outperforms the state-of-the-art method by more than 3% on the large multi-source do-main adaptation datasets – DomainNet. Source code is at https://github.com/liyunsheng13/DRT. 1.

Introduction
Multi-source domain adaptation addresses the adaptation from multiple source domains to a target domain. It is chal-lenging because a clear domain discrepancy exists not only between source and target domains, but also among multi-ple source domains (see exemplar images in Figure 2). This suggests that successful adaptation requires signiﬁcant elas-ticity of the model to adapt. A nature way to achieve this elasticity is to make model dynamic i.e. the mapping im-plemented by the model should vary with the input sample.
This hypothesis has not been explored by existing work, e.g. [22, 28], which instead aims to learn a domain agnostic model fθc , of static parameters θc, that works well for all source {S1, S2, ..., SN } and target T domains. We refer to this approach as static transfer. As illustrated in Figure 1 (a), the model implements a ﬁxed mapping across all do-Model
Samples from different domains
𝒮#
𝒮"
𝑓𝜽(&)
𝑓𝜽(&)
𝑓𝜽!
𝒯
𝒮$
𝒮#
𝒯
𝒮"
𝒮$
𝑓𝜽(&)
𝑓𝜽(&)
𝒆𝒟𝜃
𝒙
𝑓(!
𝑦 = 𝑓(!(𝒙)
𝒙
𝑦 = 𝑓((𝒙)(𝒙)
𝒆𝒟𝑓((𝒙)
Model parameter flow
Data flow (a) Static Transfer (b) Dynamic Transfer
Figure 1: Static Transfer vs. Dynamic Transfer. (a)
‘Static Transfer’ implements domain adaptation with a static model fθc , which has ﬁxed parameters θc to average domain conﬂict. (b) ‘Dynamic Transfer’ (fθ(x)) adapts the model parameters θ(x) according to samples, which gen-erates a different model per sample and turns multi-source domain adaptation into single-source domain adaptation. mains. However, learning a domain agnostic model is difﬁ-cult, since different domains can give rise to very different image distributions. When forcing a model to be domain agnostic, it essentially averages the domain conﬂict. Thus the performance drops on each source domain. This is vali-dated by our preliminary study. As shown in Figure 2, com-pared to the optimal model per domain, the static transfer model consistently degrades in each source domain.
In this paper, we propose dynamic transfer to address this issue. As shown in Figure 1(b), it contains a param-eter predictor that changes the model parameters on a per-sample basis, i.e.
It has the advantage of not requiring the deﬁnition of domains or the
In fact, it uniﬁes the prob-collection of domain labels. lems of single-source and multi-source domain adaptation.
By breaking down source domain barriers, it turns multiple implements mapping fθ(x). 10998
i n a g 2 1.5 1 0.5 0
-0.5
-1
-1.5
-2
-2.5
-3 1.6 static transfer dynamic transfer 0.3 0.3 0.1
-0.2
-0.4
-0.6
-1.8
-1.9
-2.2
-2.1
-2.5
-1.3
-1.9 clipart infograph painting quickdraw real sketch average
Figure 2: Static Transfer vs. Dynamic Transfer on the performance degradation of source domains compared to the oracle results. Both transfer models are tested across source domains. A clear performance degradation (1.9% in average) exists when using static transfer, indicating the conﬂicts across domains. The degradation is signiﬁcantly reduced when using dynamic transfer, as it handles the do-main variations well. (Best view in color) source domain adaptation into a single-source domain prob-lem. The only difference is the complexity of this domain.
The key insight is that adapting model according to do-mains is achieved statistically by adapting model per sam-ple, since each domain is viewed as a distribution of im-age samples. The dynamic transfer learns how to adapt the model’s parameters and ﬁt to the union of source domains.
Thus the alignment between source domains and target do-main is signiﬁcantly simpliﬁed, as it is no longer necessary to pull all source domains together with the target domain.
In this case, as long as the target domain is aligned to any part of the source domains, the model can be easily adapted to the target samples.
When compared to the domain adaption literature, dy-namic transfer introduces a signiﬁcant paradigm shift. In the literature, most works assume the static network of Fig-ure 1(a) and focus on loss functions. The goal is to de-ﬁne losses that somehow “pull all the domains” together into a shared latent representation. The problem is that the domains are usually very different at the network in-put. Hence, the force introduced by the loss at the output, to bring them together, is counter-balanced by an input force to keep them apart. This usually leads to a difﬁcult optimiza-tion and compromises adaptation performance. The intro-duction of a dynamic network, as in Figure 1(b), enables a more elastic mapping. In this case, it is not necessary to pull all domains together. The model adaptation given by the dynamic transfer can be generalized to target domain easily when the target domain is shifted to the space formed by entire source domain. In this way, dynamic transfer shifts the focus of the domain adaptation problem from the de-sign of good loss functions to the design of good network architectures for dynamic transfer.
An immediate difﬁculty is that the architecture of Fig-ure 1(b) can be very hard to train, since the parameter pre-dictor cannot generate all parameters for a large model. The question is whether it is possible to perform the model adap-tation by only modifying a small subset of parameters on a sample basis. In this work, we show that this is indeed pos-sible by the addition of dynamic residuals to the convolu-tion kernels of a static network. Since the residual blocks can be much smaller than the static ones, this has both very low additional computational cost (less than 0.1%) to aggregate dynamic residuals with static kernel and lit-tle tendency to overﬁt. However, it is shown to signiﬁcantly enhance the domain adaptation performance. Experimen-tal results show that the proposed dynamic residual trans-fer (DRT) can model domain variation in source domains (see Figure 2) and outperform its static counterpart (MCD
[25] method) by a large margin (11.2% on DomainNet).
Compared to state-of-the-art multi-source domain adapta-tion methods [28], it achieves a sizeable gain (3.9%) with a much simpler loss function and training algorithm. 2.