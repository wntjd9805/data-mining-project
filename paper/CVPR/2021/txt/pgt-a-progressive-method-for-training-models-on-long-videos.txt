Abstract
Convolutional video models have an order of magni-tude larger computational complexity than their counter-part image-level models. Constrained by computational resources, there is no model or training method that can train long video sequences end-to-end. Currently, the main-stream method is to split a raw video into clips, lead-ing to incomplete fragmentary temporal information ﬂow.
Inspired by natural language processing techniques deal-ing with long sentences, we propose to treat videos as se-rial fragments satisfying Markov property, and train it as a whole by progressively propagating information through the temporal dimension in multiple steps. This progres-sive training (PGT) method is able to train long videos end-to-end with limited resources and ensures the effec-tive transmission of information. As a general and robust training method, we empirically demonstrate that it yields signiﬁcant performance improvements on different models and datasets. As an illustrative example, the proposed method improves SlowOnly network by 3.7 mAP on Cha-rades and 1.9 top-1 accuracy on Kinetics with negligible parameter and computation overhead. Code is available at: https://github.com/BoPang1996/PGT. 1.

Introduction
Semantic information often ﬂows across a long time in videos. However, end-to-end modeling a long video as a whole is not feasible for current convolutional meth-ods since their computational complexities linearly increase with the number of frames [51]. The main-stream solution is splitting a video into multiple short clips [59, 60, 6, 34], but in this way, video models can only access local fragmen-tary temporal information, thus, fail to model long seman-tics [64, 51]. Is this trade-off between computational com-plexity and semantic integrity unavoidable, or might there be a speciﬁc training method tailored for video tasks that can model long semantics with acceptable complexity?
The main cause of this problem is that 3D convolutional
∗Equal contribution.
†Cewu Lu is the corresponding author, member of Qing Yuan Research
Insitute, MoE Key Lab of Artiﬁcial Intelligence, AI Institute, CS depart-ment of Shanghai Jiao Tong University, and Qi Zhi Institute. task loss original local  convolution  operator
Markov convolution  operator tg task loss original local  convolution  operator tg (cid:9)(cid:2869) (cid:9)(cid:2870) (cid:9)(cid:2871) (cid:4594) (cid:9)(cid:2904) step 1 (cid:4594) (cid:12)(cid:2869) (cid:3404) (cid:4668)(cid:9)(cid:2869)(cid:481) (cid:9)(cid:2870)(cid:481) (cid:485) (cid:481) (cid:9)(cid:2904) (cid:4669) (cid:4594) (cid:9)(cid:2904) (cid:2878)(cid:2869) (cid:9)(cid:2904) (cid:4594) (cid:2878)(cid:2870) (cid:4594) (cid:2878)(cid:2871) (cid:9)(cid:2904) (cid:4594) (cid:9)(cid:2870)(cid:2904) (cid:4594) (cid:12)(cid:2870) (cid:3404) (cid:4668)(cid:9)(cid:2904) step 2 (cid:4594) (cid:2878)(cid:2869)(cid:481) (cid:9)(cid:2904) (cid:4594) (cid:2878)(cid:2870)(cid:481) (cid:485) (cid:481) (cid:9)(cid:2870)(cid:2904) (cid:4669)
Figure 1: Progressive training (PGT) treats videos as serial fragments and optimizes a CNN model with multiple pro-gressive steps on long videos. The Markov convolutional operator designed to transfer temporal features among steps is adopted on the ﬁrst and last frames of each step, and the gradient is truncated between them. “tg” denotes truncating gradients. Fi is the ith video frame and Ip is the input of the pth progressive step containing multiple frames. models [53, 2, 54] treat a video signal I(x, y, t) as an inte-grated information block and have to process it as a whole.
With the video growing longer, the information block be-comes larger and the processing complexity increases to an infeasible point. Since in the temporal dimension the devel-opment of video semantics has high-order Markov property, violently splitting long videos and processing short clips with convolutions to model local features will hurt seman-tic integrity. For example, the model will never know an action of “pour milk” is making latte, unless it also sees the previous action of “grinding coffee beans”.
To avoid the trade-off between computational complex-ity and semantic integrity — i.e., to end-to-end train a model on long videos with much lower complexity, in this pa-per, we propose the progressive training (PGT) method (see
Fig. 1). Inspired by Truncated Back-Propagation through
Time (TBPTT) [63] originally designed for recurrent neu-ral networks to model long natural language sequences, the central idea of PGT is to 1) treat a video as serial fragments satisfying high-order Markov property instead of an inte-grated signal block, 2) disassemble the integrated forward and backward propagation into multiple serial portions like
TBPTT (see §2), which doesn’t break the Markov depen-dency of the calculation ﬂow. Modeling a long video in 11379
multiple steps won’t lead to high resource consumption and the Markov property ensures the integrity of temporal se-mantics after disassembling, akin to how TBPTT enables training RNN on long sequences.
Because the common convolutional operator is a kind of local operator which does not satisfy the Markov prop-erty, we design several Markov convolutional operators with only a few modiﬁcations on the original convolutional operator so that they can easily replace the original one in modern video models when training. With these operators, the progressive training schedule mixing local and Markov features is proposed (see Fig. 2 and Fig. 3): The temporal information is propagated progressively forward in multi-ple steps where within each progressive step, local opera-tors capture current features together with those transferred from previous steps through the Markov operators. In this schedule, the Markov operators propagate temporal infor-mation among the progressive steps throughout the tempo-ral dimension and the serial multi-step splitting reduces the computational resource requirements.
The proposed PGT method is effective and pretty sim-It is easy to implement and typically requires small ple. changes to a video model with negligible parameter or com-plexity overhead. Empirically, it works with default learn-ing rate schedules and hyper-parameters already in use ex-cept for weight decay rates (longer inputs need stronger regularization). Extensive experiments show that the pro-gressive training method works robustly out-of-the-box for different models (RegNet3D [40], ResNet [13], Slow-Fast [6]), datasets (Kinetics-200 [66], Kinetics-400 [20],
Charades [42], AVA [11]), and training settings (e.g. from scratch or pre-trained). We observe consistent performance improvements without tuning. As an example, the progres-sive method improves SlowOnly network by 3.7 mAP on
Charades and 1.9 top-1 accuracy on Kinetics. We hope this simple and effective method will provide the community with new insights into modeling long videos. 2.