Abstract
We show that the way inference is performed in few-shot segmentation tasks has a substantial effect on performances—an aspect often overlooked in the literature in favor of the meta-learning paradigm. We introduce a transductive inference for a given query image, leveraging the statistics of its unlabeled pixels, by optimizing a new loss containing three complementary terms: i) the cross-entropy on the labeled support pixels; ii) the Shannon en-tropy of the posteriors on the unlabeled query-image pix-els; and iii) a global KL-divergence regularizer based on the proportion of the predicted foreground. As our infer-ence uses a simple linear classiﬁer of the extracted fea-tures, its computational load is comparable to inductive in-ference and can be used on top of any base training. Forego-ing episodic training and using only standard cross-entropy training on the base classes, our inference yields compet-itive performances on standard benchmarks in the 1-shot scenarios. As the number of available shots increases, the gap in performances widens: on PASCAL-5i, our method brings about 5% and 6% improvements over the state-of-the-art, in the 5- and 10-shot scenarios, respectively. Fur-thermore, we introduce a new setting that includes domain shifts, where the base and novel classes are drawn from dif-ferent datasets. Our method achieves the best performances in this more realistic setting. Our code is freely avail-https://github.com/mboudiaf/ able online:
RePRI-for-Few-Shot-Segmentation. 1.

Introduction
Few-shot learning, which aims at classifying instances from unseen classes given only a handful of training ex-∗Corresponding author: malik.boudiaf.1@etsmtl.net amples, has witnessed a rapid progress in the recent years.
To quickly adapt to novel classes, there has been a sub-stantial focus on the meta-learning (or learning-to-learn) paradigm [27, 31, 35]. Meta-learning approaches popular-ized the need of structuring the training data into episodes, thereby simulating the tasks that will be presented at in-ference. Nevertheless, despite the achieved improvements, several recent image classiﬁcation works [2, 4, 6, 12, 32, 44] observed that meta-learning might have limited generaliza-tion capacity beyond the standard 1- or 5-shot classiﬁcation benchmarks. For instance, in more realistic settings with domain shifts, simple classiﬁcation baselines may outper-form much more complex meta-learning methods [4, 12].
Deep-learning based semantic segmentation has been generally nurtured from the methodological advances in image classiﬁcation. Few-shot segmentation, which has gained popularity recently [10, 17, 19, 23, 25, 33, 36, 37, 38, 39, 41, 42], is no exception. In this setting, a deep segmenta-tion model is ﬁrst pre-trained on base classes. Then, model generalization is assessed over few-shot tasks and novel classes unseen during base training. Each task includes an unlabeled test image, referred to as the query, along with a few labeled images (the support set). The recent literature in few-shot segmentation follows the learning-to-learn paradigm, and substantial research efforts focused on the design of specialized architectures and episodic-training schemes for base training. However, i) episodic training it-self implicitly assumes that testing tasks have a structure (e.g., the number of support shots) similar to the tasks used at the meta-training stage; and ii) base and novel classes are often assumed to be sampled from the same dataset.
In practice, those assumptions may limit the applicability of the existing few-shot segmentation methods in realistic scenarios [3, 4]. In fact, our experiments proved consistent with ﬁndings in few-shot classiﬁcation when going beyond the standard settings and benchmarks. Particularly, we ob-113979
served among state-of-the-art methods a saturation in per-formances [3] when increasing the number of labeled sam-ples (See Table 3). Also, in line with very recent obser-vations in image classiﬁcation [4], existing meta-learning methods prove less competitive in cross-domains scenarios (See Table 4). This casts doubts as to the viability of the cur-rent few-shot segmentation benchmarks and datasets; and motivates re-considering the relevance of the meta-learning paradigm, which has become the de facto choice in the few-shot segmentation litterature.
Contributions
In this work, we forego meta-learning, and re-consider a simple cross-entropy supervision during training on the base classes for feature extraction. Additionally, we pro-pose a transductive inference that better leverages the support-set supervision than the existing methods. Our con-tributions can be summarized as follows:
• We present a new transductive inference–RePRI (Re-gion Proportion Regularized Inference)–for a given few-shot segmentation task. RePRI optimizes a loss integrating three complementary terms: i) a standard cross-entropy on the labeled pixels of the support im-ages; ii) the entropy of the posteriors on the query pix-els of the test image; and iii) a global KL divergence regularizer based on the proportion of the predicted foreground pixels within the test image. RePRI can be used on top of any trained feature extractor, and uses exactly the same information as standard induc-tive methods for a given few-shot segmentation task.
• Although we use a basic cross-entropy training on the base classes, without complex meta-learning schemes, RePRI yields highly competitive perfor-mances on the standard few-shot segmentation bench-marks, PASCAL-5i and COCO-20i, with gains around 5% and 6% over the state-of-the-art in the 5- and 10-shot scenarios, respectively.
• We introduce a more realistic setting where, in addi-tion to the usual shift on classes between training and testing data distributions, a shift on the images’ feature distribution is also introduced. Our method achieves the best performances in this scenario.
• We demonstrate that a precise region-proportion infor-mation on the query object improves substantially the results, with an average gain of 13% on both datasets.
While assuming the availability of such information is not realistic, we show that inexact estimates can still lead to drastic improvements, opening a very promis-ing direction for future research. 2.