Abstract dicted from input images, giving a clear perspective of the scene.
Autonomous robotic systems and self driving cars rely on accurate perception of their surroundings as the safety of the passengers and pedestrians is the top priority. Se-mantic segmentation is one of the essential components of road scene perception that provides semantic information of the surrounding environment. Recently, several methods have been introduced for 3D LiDAR semantic segmentation.
While they can lead to improved performance, they are ei-ther afﬂicted by high computational complexity, therefore are inefﬁcient, or they lack ﬁne details of smaller object in-stances. To alleviate these problems, we propose (AF)2-S3Net, an end-to-end encoder-decoder CNN network for 3D
LiDAR semantic segmentation. We present a novel multi-branch attentive feature fusion module in the encoder and a unique adaptive feature selection module with feature map re-weighting in the decoder. Our (AF)2-S3Net fuses the voxel-based learning and point-based learning meth-ods into a uniﬁed framework to effectively process the po-tentially large 3D scene. Our experimental results show that the proposed method outperforms the state-of-the-art approaches on the large-scale nuScenes-lidarseg and Se-manticKITTI benchmark, ranking 1st on both competitive public leaderboard competitions upon publication. 1.

Introduction
Understanding of the surrounding environment has been one of the most fundamental tasks in autonomous robotic systems. With the challenges introduced with recent tech-nologies such as self-driving cars, a detailed and accurate understanding of the road scene has become a main part of any outdoor autonomous robotic system in the past few years. To achieve an acceptable level of road scene un-derstanding, many frameworks beneﬁt from image seman-tic segmentation, where speciﬁc pixel-wise classes are pre-authors the at
∗All
Canada ryan.razani, ehsan.taghavi, thomas.enxu.li, liu.bingbing}@huawei.com
Lab,
Ark
{ran.cheng1, with of writing. are time
Huawei
Noah’s
Figure 1: Comparison of our proposed method with Sal-saNext [11] and MinkNet42 [10] on SemanticKITTI bench-mark [3].
Although image semantic segmentation is an important step in realizing self driving cars, the limitations of a vision sensor such as the inability to record data in poor lighting conditions, variable sensor sensitivity, lack of depth infor-mation and limited ﬁeld-of-view (FOV) makes it difﬁcult for vision sensors to be the sole primary source for scene un-derstanding and semantic segmentation. In contrast, Light
Detection and Ranging (LiDAR) sensors can record accu-rate depth information regardless of the lighting conditions with high density and frame rate, making it a reliable source of information for critical tasks such as self driving.
LiDAR sensor generates point cloud by scanning the environment and calculating time-of-ﬂight for the emitted laser beams. In doing so, LiDARs can collect valuable in-formation, such as range (e.g., in Cartesian coordinates) and intensity (a measure of reﬂection from the surface of the ob-jects). Recent advancement in LiDAR technology makes it possible to generate high quality, low noise and dense scans 12547
from desired environments, making the task of scene un-derstanding a possibility using LiDARs. Although rich in information, LiDAR data often comes in an unstructured format and partially sparse at far ranges. These characteris-tics make the task of scene understating challenging using
LiDAR as primary sensor. Nevertheless, research in scene understanding and in speciﬁc, semantic segmentation using
LiDARs, has seen an increase in the past few years with the availability of datasets such as semanticKITTI [3].
The unstructured nature and partial sparsity of LiDAR data brings challenges to semantic segmentation. However, a great effort has been put by researchers to address these obstacles and many successful methods have been proposed in the literature (see Section 2). From real-time methods which use projection techniques to beneﬁt from the avail-able 2D computer vision techniques, to fully 3D approaches which target higher accuracy, there exist a range of meth-ods to build on. To better process LiDAR point cloud in 3D and to overcome limitations such as non-uniform point densities and loss of granular information in voxelization step, we propose (AF)2-S3Net, a next generation method based on S3Net [9] and S3CNet [8] which is built upon
Minkowski Engine [10], to suit varying levels of sparsity in LiDAR point clouds, achieving state-of-the-art accuracy in semantic segmentation methods on SemanticKITTI [3].
Fig. 1 demonstrates qualitative results of our approach com-pared to SalsaNext [11] and MinkNet42 [10]. We summa-rize our contributions as,
• An end-to-end encoder-decoder 3D sparse CNN that achieves state-of-the-art accuracy in semanticKITTI benchmark [3] and nuScenes-lidarseg competition [5];
• A multi-branch attentive feature fusion module in the encoder to learn both global contexts and local details;
• An adaptive feature selection module with feature map re-weighting in the decoder to actively emphasize the contextual information from a feature fusion module to improve the generalizability;
• A comprehensive analysis on semantic segmenta-tion and classiﬁcation performance of our model as opposed to existing methods on three bench-marks, semanticKITTI [3], nuScenes-lidarseg [5], and
ModelNet40 [38] through ablation studies, qualitative and quantitative results. 2.