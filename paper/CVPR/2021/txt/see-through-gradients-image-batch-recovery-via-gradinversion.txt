Abstract
Private Data (Client)
Training deep neural networks requires gradient estima-tion from data batches to update parameters. Gradients per parameter are averaged over a set of data and this has been presumed to be safe for privacy-preserving training in joint, collaborative, and federated learning applications.
Prior work only showed the possibility of recovering input data given gradients under very restrictive conditions – a single input point, or a network with no non-linearities, or a small 32 ˆ 32 px input batch. Therefore, averaging gra-dients over larger batches was thought to be safe. In this work, we introduce GradInversion, using which input images from a larger batch (8 – 48 images) can also be recovered for large networks such as ResNets (50 layers), on complex datasets such as ImageNet (1000 classes, 224 ˆ 224 px). We formulate an optimization task that converts random noise into natural images, matching gradients while regularizing image ﬁdelity. We also propose an algorithm for target class label recovery given gradients. We further propose a group consistency regularization framework, where multiple agents starting from different random seeds work together to ﬁnd an enhanced reconstruction of the original data batch. We show that gradients encode a surprisingly large amount of information, such that all the individual images can be recov-ered with high ﬁdelity via GradInversion, even for complex datasets, deep networks, and large batch sizes. 1.

Introduction
Sharing weight updates or gradients during training is the central idea behind collaborative, distributed, and fed-erated learning of deep networks [1, 22, 24, 25, 28]. In the basic setting of federated stochastic gradient descent, each device learns on local data, and shares gradients to update a global model. Alleviating the need to transmit training data offers several key advantages. This keeps user data private, allaying concerns related to user privacy, security, and other proprietary concerns. Further, this eliminates the need to store, transfer, and manage possibly large datasets. With this framework, one can train a model on medical data without access to any individual’s data [3, 32], or perception model
Client
Model
Averaged
Gradients
Server
GradInversion dataset boundary (presumed safe) (a) Inverting averaged gradients to recover original image batches
Recovery from Averaged Gradients (Server)
Input random seed 1
Image generator shared optimizer
Intermediate image random seed 1
Final
Result
R
ﬁdelity
Optimizer
Optimizer
.
.
.
Model
R grad
Gradient matching loss
.
.
.
Rgroup
Group
Registration
Input random seed N
Averaged
Gradients
Label recovery
Intermediate image random seed N
Registered average image (b) Overview of our proposed GradInversion method
Figure 1: We propose (a) GradInversion to recover hidden training image batches with high ﬁdelity via inverting averaged gradients.
GradInversion formulates (b) an optimization process that trans-forms noise to input images (Sec. 3.1). It starts with label restora-tion from the gradient of the fully connected layer (Sec. 3.2), then optimizes inputs to match target gradients under ﬁdelity regular-ization (Sec. 3.3) and registration-based group consistency regular-ization (Sec. 3.4) to improve reconstruction quality. This enables recovery of 224 ˆ 224 pixel ImageNet samples from ResNet-50 batch gradients, which was previously impossible (please zoom into examples above. More in Sec. 4). for autonomous driving without invasive data collection [41].
While this setting might seem safe at ﬁrst glance, a few recent works have begun to question the central premise of federated learning - is it possible for gradients to leak private information of the training data? Effectively serving as a
“proxy” of the training data, the link between gradients to the data in fact offers potential for retrieving information: from revealing the positional distribution of original data [33, 44], to even enabling pixel-level detailed image reconstruction from gradients [13, 53, 55]. Despite remarkable progress, in-16337
verting an original image through gradient matching remains a very challenging task – successful reconstruction of images of high resolution for complex datasets such as ImageNet [9] has remained elusive for batch sizes larger than one.
Emerging research on network inversion techniques offers insights into this task. Network inversion enables noise-to-image conversion via back-propagating gradients on appro-priate loss functions to the learnable inputs. Initial solutions were limited to shallow networks and low-resolution syn-thesis [11, 39], or creating an artistic effect [37]. However, the ﬁeld has rapidly evolved, enabling high-ﬁdelity, high-resolution image synthesis on ImageNet from commonly trained classiﬁers, making downstream tasks data-free for pruning, quantization, continual learning, knowledge trans-fer, etc. [5, 17, 42, 48]. Among these, DeepInversion [48] yields state-of-the-art results on image synthesis for Ima-geNet. It enables the synthesis of realistic data from a vanilla pretrained ResNet-50 [19] classiﬁer by regularizing feature distributions through batch normalization (BN) priors.
Building upon DeepInversion [48], we delve into the prob-lem of batch recovery via gradient inversion. We formulate the task as the optimization of the input data such that the gradients on that data match the ones provided by the client, while ensuring realism of the input data. However, since the gradient is also a function of the ground-truth label, one of the main challenges is to identify the ground-truth label for each data point in the batch. To tackle this, we propose a one-shot batch label restoration algorithm that uses gradients from the last fully connected layer.
Our goal is to recover the exact images that the client possesses. By starting from noisy inputs generated by differ-ent random seeds, multiple optimization processes are likely to converge to different minimas. Due to the inherently spatially-invariant nature of convolutional neural networks (CNNs), these resulting images share spatial information but differ in the exact location and arrangement. To allow for improved convergence towards the ground truth images, we compute a registered mean image from all candidates and introduce a group consistency regularization term on every optimization process to reduce deviation. We ﬁnd that the proposed approach and group consistency regularization provide superior better image recovery compared to prior optimization approaches [13, 55].
Our non-learning based image recovery method recovers more speciﬁc details of the hidden input data when com-pared to the state-of-the-art generative adversarial networks (GAN), such as BigGAN [4]. More importantly, we demon-strate that a full recovery of individual images of 224 ˆ 224 px resolution with high ﬁdelity and visual details, by invert-ing gradients of the batch, is now made feasible even up to batch size of 48 images.
Our main contributions are as follows:
• We introduce GradInversion to recover hidden original images from random noise via optimization given batch-averaged gradients.
• We propose a label restoration method to recover ground truth labels using ﬁnal fully connected layer gradients.
• We introduce a group consistency regularization term, based on multi-seed optimization and image registration, to improve reconstruction quality.
• We demonstrate that a full recovery of detailed individual images from batch-averaged gradients is now feasible for deep networks such as ResNet-50.
• We introduce a new Image Identiﬁability Precision met-ric to measure the ease of inversion over varying batch sizes, and identify samples vulnerable to inversion. 2.