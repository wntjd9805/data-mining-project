Abstract
Deep implicit functions (DIFs), as a kind of 3D shape representation, are becoming more and more popular in the 3D vision community due to their compactness and strong representation power. However, unlike polygon mesh-based templates, it remains a challenge to reason dense corre-spondences or other semantic relationships across shapes represented by DIFs, which limits its applications in texture transfer, shape analysis and so on. To overcome this limi-tation and also make DIFs more interpretable, we propose
Deep Implicit Templates, a new 3D shape representation that supports explicit correspondence reasoning in deep im-plicit representations. Our key idea is to formulate DIFs as conditional deformations of a template implicit function.
To this end, we propose Spatial Warping LSTM, which de-composes the conditional spatial transformation into mul-tiple point-wise transformations and guarantees general-ization capability. Moreover, the training loss is carefully designed in order to achieve high reconstruction accuracy while learning a plausible template with accurate corre-spondences in an unsupervised manner. Experiments show that our method can not only learn a common implicit tem-plate for a collection of shapes, but also establish dense correspondences across all the shapes simultaneously with-out any supervision. 1.

Introduction
Representing 3D objects effectively and efﬁciently in neural networks is fundamental for many tasks in computer vision, including 3D model reconstruction, matching, ma-nipulation and understanding. In the pioneering studies, re-searchers have adopted various traditional geometry repre-sentations, including voxel grids [53, 55, 52, 54, 23, 46], point clouds [62, 1, 58, 33] and meshes [63, 2, 50, 51, 22, 14, 5].
In the past several years, deep implicit functions (DIFs) have been proposed as an alternative [40, 38, 11, 18, 20, 56, 26, 9, 24, 47, 15]. Compared to traditional represen-tations, DIFs show expressive and ﬂexible capacity for rep-resenting complex shapes and ﬁne geometric details, even in challenging tasks like human digitization [43].
Unfortunately, the implicit nature of DIFs is also its
Achilles’ Heels: although DIFs are good at approximating individual shapes, they provide no information about the re-lationship between two different ones. One can easily estab-lish vertice-to-vertice correspondences between two shapes when using mesh templates [21, 50, 14], but that is difﬁcult in DIFs. The lack of semantic relationship in DIFs poses signiﬁcant challenges for using DIFs in downstream appli-cations such as shape understanding and editing.
To overcome this limitation, we propose Deep Implicit
Templates, a new way to interpret and implement DIFs. The key idea is to decompose a conditional deep implicit func-tion into two components: a template implicit function and a conditional spatial warping function. The template im-plicit function represents the “mean shape” for a category of objects, while the spatial warping function deforms the tem-plate implicit function to form speciﬁc object instances. On one hand, as both the template and the warping ﬁeld are de-ﬁned in an implicit manner, the advantages of deep implicit representations (compactness and efﬁciency) are preserved.
On the other hand, with the template implicit function as an intermediate shape, the warping function automatically establishes dense correspondences across different object.
More recently, some techniques use a set of primitives to represent 3D shapes in order to capture structure-level semantics [19, 18, 22, 14]. The primitives can be either manually deﬁned [19, 22] or learned from data [18, 14].
We emphasize that our method is essentially different from them in two ways. First, our method decomposes the im-plicit representations into a template implicit function and a continuous warping ﬁeld. Compared to element-based methods, our decomposition not only provides a complete, global template for the training data, enabling many appli-cations such as uv mapping and keypiont labeling, but also makes the latent shape space more interpretable as we can inspect how shapes deform. Second, our method directly builds up accurate correspondences in the whole 3D space, while element-based methods rely on interpolation or fea-ture matching to compute dense correspondences. Overall, our method provides more ﬂexibility and scalability to con-trol the template and/or its deformation: one can, for exam-ple, replace the template in our framework with a custom designed one without losing the representation power, or 1429
Figure 1: Example results of our representation. Our approach is able to factor out a plausible template (middle) from a set of shapes (surroundings), and builds up dense correspondences (color-coded) across all the shapes automatically without any supervision. apply additional semantic constraints on the spatial defor-mation for speciﬁc problems such as dynamic human mod-eling (Sec.6).
Training deep implicit templates is not straight-forward, because we have no access to either the ground-truth map-ping between the templates and shape instances, or dense correspondence annotations across different shapes. Our ultimate goal is to make deep implicit templates an effec-tive representation that can: 1) represent training shapes accurately, 2) establish plausible correspondences across shapes and 3) generalize to unseen data. However, with-out proper design and regularization, the network may not be able to learn such a representation in an unsupervised manner. We make several technical contributions to address these challenges. In terms of network architecture, we pro-pose Spatial Warping LSTM, which decomposes the con-ditional spatial warping into multi-step point-wose trans-formation, guaranteeing the generalization capacity and the representation power of our warping function. In addition, we introduce a progressive reconstruction loss for our Spa-tial Warping LSTM, which further improves the reconstruc-tion accuracy. Two-level regularization is also proposed to obtain plausible templates with accurate correspondences in an unsupervised manner. As shown in the experiments, our method can learn a plausible implicit template for a set of shapes, with conditional warping ﬁelds that accurately represent shapes while establishing dense correspondences among them without any supervision (See Fig.1). Overall, the proposed Deep Implicit Templates signiﬁcantly expands the capability of DIFs without losing its advantages, mak-ing it a more effective implicit representation for 3D learn-ing tasks. Code is available at https://github.com/
ZhengZerong/DeepImplicitTemplates. 2.