Abstract
Referring image segmentation aims to segment the ob-jects referred by a natural language expression. Previ-ous methods usually focus on designing an implicit and re-current feature interaction mechanism to fuse the visual-linguistic features to directly generate the ﬁnal segmenta-tion mask without explicitly modeling the localization infor-mation of the referent instances. To tackle these problems, we view this task from another perspective by decoupling it into a “Locate-Then-Segment” (LTS) scheme. Given a lan-guage expression, people generally ﬁrst perform attention to the corresponding target image regions, then generate a
ﬁne segmentation mask about the object based on its con-text. The LTS ﬁrst extracts and fuses both visual and textual features to get a cross-modal representation, then applies a cross-model interaction on the visual-textual features to lo-cate the referred object with position prior, and ﬁnally gen-erates the segmentation result with a light-weight segmen-tation network. Our LTS is simple but surprisingly effective.
On three popular benchmark datasets, the LTS outperforms all the previous state-of-the-arts methods by a large margin (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg).
In addition, our model is more interpretable with explicitly locating the object, which is also proved by visualization ex-periments. We believe this framework is promising to serve as a strong baseline for referring image segmentation. 1.

Introduction
Jointly learning vision and language is a signiﬁcant task in machine learning and pattern recognition community, which has drawn great attention in recent years. In this pa-per, we study the challenging task of language-instructed object segmentation [11, 38, 37] which aims to generate a segmentation mask of the object in image referred by
*This work was done when Ya Jing was an intern at ByteDance AI Lab.
Man in green  sweater
Kid in white jacket  being held by man
Figure 1. The illustration of referring image segmentation. Given a referring expression and an image, the model aims to generate a segmentation mask of the corresponding object in image referred by the language expression. Best viewed in color. a natural language expression.
It has wide applications, e.g., interactive image editing and language-guided human-robot interaction. Beyond traditional semantic segmenta-tion, language-referring image segmentation is more chal-lenging due to the semantic gap between image and lan-guage. In addition, the textual expression is not just limited to entities (e.g., “person”, “horse”). It may contain descrip-tive words, such as object properties (e.g., “red”, “young”), actions (e.g., “standing”, “hold”), and positional relation-ships (e.g., “right”, “above”).
Given the image and referring sentence, there are two essential issues affecting the overall performance of a re-ferring image segmentation model. First, the model must highlight the most discriminative candidate area in image corresponding to the given language. Second, the model must generate a ﬁne segmentation result. The existing refer-ring image segmentation methods could be generally sum-9858
marized as follows: (1) Utilizing a Convolutional Neural
Network (CNN) and a Recurrent Neural Network (RNN) to represent the image feature fv(I) and language feature ftext(X), respectively. (2) Cross-modal attention and re-current ConvLSTM are used to fuse fv(I) and ftext(X) to get a coarse mask. (3) Dense CRF (DCRF) is further used as post-processing to get the ﬁnal ﬁne segmentation M (I).
Previous works mainly focus on how to fuse the image feature and language feature. A straightforward solution
[11] is to utilize a concatenation-and-convolution method to fuse visual and linguistic representations to produce the
ﬁnal segmentation result. However, this method cannot model the alignment between image and language effec-tively due to the fact that the visual and textual information is modeled individually. To further model the context be-tween multi-modal features, some prior methods [32, 4, 37] propose cross-modal attention by adaptively focusing on important regions in the image and informative keywords in the language expression. Recently, to exploit different types of informative keywords in the language and learn the aligned multi-modal representations, some works [13, 14] either perceive all the entities that are referred by the ex-pression or utilize the linguistic structure as guidance to segment the referent. Although great progress has been made, the network architecture and experimental practice have steadily become more and more complex. This makes the algorithm analysis and comparison more and more dif-ﬁcult. In addition, they do not explicitly locate the referred object guided by language expression and only utilize time-consuming post-processing DCRF to generate the ﬁnal re-ﬁned segmentation.
In this paper, we consider solving this problem from an-other perspective. We decouple the referring image segmen-tation task into two sub-sequential tasks: (a) referring object position prediction, and (b) object segmentation mask gen-eration. In our model, we ﬁrst fuse the visual and linguis-tic features to get a cross-modal feature. Then for (a), we propose a localization module to directly obtain the visual contents prior corresponding to the expression. Such object prior will be used as a visual positional guidance for the subsequent segmentation module. For (b), we concatenate the object prior with the cross-modal features and utilize a light-weight ConvNets to get the ﬁnal segmentation mask (Fig. 2).
Our solution is very simple but surprisingly effective. On three challenging benchmarks, i.e., RefCOCO [15], Ref-COCO+ [15] and RefCOCOg [27], our model outperforms the state-of-the-arts methods by a large margin (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg). Extensive abla-tion studies also verify the effectiveness of each component of our method. 2.