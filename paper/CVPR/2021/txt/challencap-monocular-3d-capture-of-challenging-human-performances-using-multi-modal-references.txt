Abstract
Capturing challenging human motions is critical for nu-merous applications, but it suffers from complex motion pat-terns and severe self-occlusion under the monocular setting.
In this paper, we propose ChallenCap — a template-based approach to capture challenging 3D human motions using a single RGB camera in a novel learning-and-optimization framework, with the aid of multi-modal references. We propose a hybrid motion inference stage with a generation network, which utilizes a temporal encoder-decoder to ex-tract the motion details from the pair-wise sparse-view ref-erence, as well as a motion discriminator to utilize the un-paired marker-based references to extract speciﬁc challeng-ing motion characteristics in a data-driven manner. We fur-ther adopt a robust motion optimization stage to increase the tracking accuracy, by jointly utilizing the learned mo-tion details from the supervised multi-modal references as well as the reliable motion hints from the input image refer-ence. Extensive experiments on our new challenging motion dataset demonstrate the effectiveness and robustness of our approach to capture challenging human motions. 1.

Introduction
The past ten years have witnessed a rapid development of markerless human motion capture [14, 24, 60, 68], which beneﬁts various applications such as immersive VR/AR ex-perience, sports analysis and interactive entertainment.
Multi-view solutions [60, 40, 29, 12, 30, 63] achieve high-ﬁdelity results but rely on expensive studio setup which are difﬁcult to be deployed for daily usage. Recent learning-based techniques enables robust human attribute prediction from monocular RGB video [31, 35, 2, 80, 55].
The state-of-the-art monocular human motion capture ap-proaches [22, 75, 74] leverage learnable pose detections [9, 44] and template ﬁtting to achieve space-time coherent re-sults. However, these approaches fail to capture the speciﬁc challenging motions such as yoga or rolling on the ﬂoor,
Figure 1. Our ChallenCap approach achieves robust 3D capture of challenging human motions from a single RGB video, with the aid of multi-modal references. which suffer from extreme poses, complex motion patterns and severe self-occlusion under the monocular setting.
Capturing such challenging human motions is essential for many applications such as training and evaluation for gymnastics, sports and dancing. Currently, optical marker-based solutions like Vicon [66] are widely adopted to cap-ture such challenging professional motions. However, di-rectly utilizing such marker-based reference into markerless capture is inapplicable since the actor needs to re-perform the challenging motion which is temporally unsynchronized to the maker-based capture. Some data-driven human pose estimation approaches [32, 35] utilize the unpaired refer-ence in an adversarial manner, but they only extract general motion prior from existing motion capture datasets [28, 43], which fails to recover the characteristics of speciﬁc chal-lenging motion. The recent work [23] inspires to utilize the markerless multi-view reference in a data-driven man-ner to provide more robust 3D prior for monocular capture.
However, this method is weakly supervised on the input im-ages instead of the motion itself, leading to dedicated per-performer training. Moreover, researchers pay less atten-tion to combine various references from both marker-based systems and sparse multi-view systems for monocular chal-lenging motion capture. 11400
In this paper, we tackle the above challenges and present
ChallenCap – a template-based monocular 3D capture approach for challenging human motions from a single
RGB video, which outperforms existing state-of-the-art ap-proaches signiﬁcantly (See Fig. 1 for an overview). Our novel pipeline proves the effectiveness of embracing multi-modal references from both temporally unsynchronized marker-based system and light-weight markerless multi-view system in a data-driven manner, which enables robust human motion capture under challenging scenarios with ex-treme poses and complex motion patterns, whilst still main-taining a monocular setup.
More speciﬁcally, we introduce a novel learning-and-optimization framework, which consists of a hybrid motion inference stage and a robust motion optimization stage. Our hybrid motion inference utilizes both the marker-based ref-erence which encodes the accurate spatial motion charac-teristics but sacriﬁces the temporal consistency, as well as the sparse multi-view image reference which provides pair-wise 3D motion priors but fails to capture extreme poses.
To this end, we ﬁrst obtain the initial noisy skeletal motion map from the input monocular video. Then, a novel gen-eration network, HybridNet, is proposed to boost the ini-tial motion map, which utilizes a temporal encoder-decoder to extract local and global motion details from the sparse-view reference, as well as a motion discriminator to uti-lize the unpaired marker-based reference. Besides the data-driven 3D motion characteristics from the previous stage, the input RGB video also encodes reliable motion hints for those non-extreme poses, especially for the non-occluded regions. Thus, a robust motion optimization is further pro-posed to reﬁne the skeletal motions and improve the track-ing accuracy and overlay performance, which jointly uti-lizes the learned 3D prior from the supervised multi-modal references as well as the reliable 2D and silhouette infor-mation from the input image reference. To summarize, our main contributions include:
• We propose a monocular 3D capture approach for chal-lenging human motions, which utilizes multi-modal reference in a novel learning-and-optimization frame-work, achieving signiﬁcant superiority to state-of-the-arts.
• We propose a novel hybrid motion inference module to learn the challenging motion characteristics from the supervised references modalities, as well as a robust motion optimization module for accurate tracking.
• We introduce and make available a new challeng-ing human motion dataset with both unsynchro-nized marker-based and light-weight multi-image ref-erences, covering 60 kinds of challenging motions and 20 performers with 120k corresponding images. 2.