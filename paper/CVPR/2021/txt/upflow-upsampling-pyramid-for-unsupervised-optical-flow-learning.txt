Abstract
We present an unsupervised learning approach for opti-cal ﬂow estimation by improving the upsampling and learn-ing of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the ﬁnest ﬂow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised op-tical ﬂow learning on multiple leading benchmarks, includ-ing MPI-SIntel, KITTI 2012 and KITTI 2015. In particu-lar, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on
KITTI 2015, which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively. 1.

Introduction
Optical ﬂow estimation has been a fundamental com-puter vision task for decades, which has been widely used in various applications such as video editing [14], behavior recognition [31] and object tracking [3]. The early solutions focus on minimizing a pre-deﬁned energy function with op-timization tools [4, 33, 30]. Nowadays deep learning based approaches become popular, which can be classiﬁed into two categories, the supervised [11, 26] and unsupervised ones [29, 37]. The former one uses synthetic or human-labelled dense optical ﬂow as ground-truth to guide the motion regression. The supervised methods have achieved leading performance on the benchmark evaluations. How-ever, the acquisition of ground-truth labels are expensive.
In addition, the generalization is another challenge when trained on synthetic datasets. As a result, the latter category, i.e. the unsupervised approaches attracts more attentions re-cently, which does not require the ground-truth labels. In unsupervised methods, the photometric loss between two images is commonly used to train the optical ﬂow estima-∗Corresponding author
Figure 1. An example from Sintel Final benchmark. Compared with previous unsupervised methods including SelFlow [22], Epi-Flow [44], ARFlow [20], SimFlow [12] and UFlow [15], our ap-proach produces sharper and more accurate results in object edges. tion network. To facilitate the training, the pyramid network structure [34, 10] is often adopted, such that both global and local motions can be captured in a coarse-to-ﬁne man-ner. However, there are two main issues with respect to the pyramid learning, which are often ignored previously. We refer the two issues as bottom-up and top-down problems.
The bottom-up problem refers to the upsampling module in the pyramid. Existing methods often adopt simple bi-linear or bicubic upsampling [20, 15], which interpolates cross edges, resulting in blur artifacts in the predicted op-tical ﬂow. Such errors will be propagated and aggregated when the scale becomes ﬁner. Fig. 1 shows an example. The top-down problem refers to the pyramid supervision. The previous leading unsupervised methods typically add guid-ance losses only on the ﬁnal output of the network, while the intermediate pyramid levels have no guidance. In this condition, the estimation errors in coarser levels will accu-mulate and damage the estimation at ﬁner levels due to the lack of training guidance.
To this end, we propose an enhanced pyramid learning framework of unsupervised optical ﬂow estimation. First, we introduce a self-guided upsampling module that sup-ports blur-free optical ﬂow upsampling by using a self-learned interpolation ﬂow instead of the straightforward in-1045
terpolations. Second, we design a new loss named pyramid distillation loss that supports explicitly learning of the inter-mediate pyramid levels by taking the ﬁnest output ﬂow as pseudo labels. To sum up, our main contributions include:
• We propose a self-guided upsampling module to tackle the interpolation problem in the pyramid net-work, which can generate the sharp motion edges.
• We propose a pyramid distillation loss to enable ro-bust supervision for unsupervised learning of coarse pyramid levels.
• We achieve superior performance over the state-of-the-art unsupervised methods with a relatively large mar-gin, validated on multiple leading benchmarks. 2.