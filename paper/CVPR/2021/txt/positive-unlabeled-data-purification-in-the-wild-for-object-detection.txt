Abstract
Deep learning based object detection approaches have achieved great progress with the beneﬁt from large amount of labeled images. However, image annotation remains a laborious, time-consuming and error-prone process. To fur-ther improve the performance of detectors, we seek to ex-ploit all available labeled data and excavate useful sam-ples from massive unlabeled images in the wild, which is rarely discussed before. In this paper, we present a positive-unlabeled learning based scheme to expand training data by purifying valuable images from massive unlabeled ones, where the original training data are viewed as positive data and the unlabeled images in the wild are unlabeled data. To effectively utilized these puriﬁed data, we propose a self-distillation algorithm based on hint learning and ground truth bounded knowledge distillation. Experimental results verify that the proposed positive-unlabeled data puriﬁcation can strengthen the original detector by mining the massive unlabeled data. In particular, our method boosts the mAP of FPN by +2.0% on COCO benchmark. 1.

Introduction
As a fundamental but challenging task in computer vi-sion, object detection has attracted increasing attention from both academia and industry due to its signiﬁcant role in nu-merous ﬁelds, including autonomous driving and surveil-lance video analysis. Recently, many object detection meth-ods [4, 42, 28, 31, 41, 15, 9, 25, 14] have been proposed and achieved great progress, pushing the related applica-tions forward to the real world. However, most existing methods rely on a great volume of ﬁne annotated images as training data to obtain a high-performance detector as illus-trated in Figure 1(c) and keep beneﬁting from more avail-able labeled data [30, 10, 24]. When the labeled data are scarce, detectors are easy to severely overﬁt and fail to gen-∗Corresponding author. eralize. However, it is a time-consuming and expensive pro-cess to build a qualiﬁed object detection dataset. According to [45, 50], it takes annotators about 42 seconds to perform one annotation task after they receive a thorough training on that project. A label composed of bounding box coordi-nates and corresponding category needs to be annotated for multiple objects in one image, each with a different label.
Many works have observed heavy dependence of ob-ject detection task on huge amount of training data. Some of them try to tackle it from few-shot learning perspec-tive [59, 22, 47, 11, 12, 53], which eliminates this phe-nomenon by transferring knowledge from data-abundant base classes to the data-scarce novel classes as illustrated in
Figure 1(a). However, the inherent data imbalance problem proposes an even greater challenge for these novel meth-ods, e.g. the “train2017” set of COCO benchmark [30] con-tains 26k annotation boxes for category ”person” while only 198 for “hair drier”. Such cases of data scarce problem can hardly be handled by few-shot learning.
Another trend of eliminating label dependence in ob-ject detection belongs to semi-supervised learning [44, 40, 46, 48, 34, 49, 52]. This line of work either generates la-beled/unlabeled data by splitting a fully annotated dataset, or directly using unlabeled data that have the same distribu-tion with labeled ones as shown in Figure 1(b). The former usually needs a predeﬁned data scale and inevitably limit the performance of detectors, while the latter often results in the largely neglected problem of dataset bias, which is a well known limitation of semi-supervised learning [35].
In contrast to the existing methods which only utilize limited data, this paper aims at both improving the per-formance of object detectors and eliminating label depen-dence by exploiting massive unlabeled data from the wild.
As shown in Figure 1(d), the detector not only exploits as much well annotated data (e.g. COCO [30] and VOC [10]) as possible, but also is provided with unlimited unlabeled data from the wild (e.g. Flickr [19], ImageNet1 [5] and 1Although ImageNet has image-level labels, there is no extra box-level annotation, we choose to ignore all labels and treat the images as unlabeled. 2653
base (abundant) support (few)
Sub labeled train2017
Sub labeled train2017
All labeled train2017
All unlabel2017 (a) Few-shot learning  (b) Semi-supervised learning
All labeled train2017
All labeled train2017
Massive unlabeled data from the wild (c) Supervised learning  (d) Positive-Unlabeled learning
Figure 1. Different types of object detection setting and corresponding training images. The “train2017” and “unlabel2017” indicate the 118k images and 123k images provided by COCO benchmark [30], respectively. Images marked with red-dotted boxes denote the positive images from the unlabeled set.
Open Images [24]).
In such setting, data from the wild may contain potentially positive images, in which objects are the same with that appeared in original detection bench-mark, and these data can further help enlarge the training set. Nevertheless, the trained detector could also be con-taminated by other negative images that contain unrelated objects from the unlabeled data. A popular solution to utilize the unlabeled data is the self-training based semi-supervised methods [40, 44]. However, directly training the detector under the self-training framework [26] which uses a teacher model to generate pseudo labels on unlabeled images may inject extra noises to the augmented dataset.
To tackle this challenge, we propose a two-stage frame-work to adaptively utilize the positive data and meanwhile, avoid noise from negative data for improving the detector’s performance. Firstly, a PU (Positive-unlabeled) classiﬁer for data puriﬁcation is trained with the given labeled data and massive unlabeled data from the wild. Then the ex-panded training set is constructed by combining the original training data and the positive data from massive unlabeled data puriﬁed by the trained PU classiﬁer. In addition, we develop a self-distillation based training scheme to utilize the expanded dataset where the teacher’s architecture is the same with the student’s. We demonstrate the efﬁciency of our positive-unlabeled data puriﬁcation for object detection (PUDet) on COCO benchmark [30] in general two-stage de-In particular, the proposed tection framework FPN [28]. method can improve the mAP of baseline FPN from 37.4% to 39.4% without annotating any data manually. 2.