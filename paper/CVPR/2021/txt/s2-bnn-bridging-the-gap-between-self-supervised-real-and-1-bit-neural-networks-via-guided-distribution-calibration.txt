Abstract
Real-valued
Previous studies dominantly target at self-supervised learning on real-valued networks and have achieved many promising results. However, on the more challenging bi-nary neural networks (BNNs), this task has not yet been fully explored in the community.
In this paper, we focus on this more difﬁcult scenario: learning networks where both weights and activations are binary, meanwhile, with-out any human annotated labels. We observe that the commonly used contrastive objective is not satisfying on
BNNs for competitive accuracy, since the backbone net-work contains relatively limited capacity and representa-tion ability. Hence instead of directly applying existing self-supervised methods, which cause a severe decline in performance, we present a novel guided learning paradigm from real-valued to distill binary networks on the ﬁnal pre-diction distribution, to minimize the loss and obtain desir-able accuracy. Our proposed method can boost the sim-ple contrastive learning baseline by an absolute gain of 5.5∼15% on BNNs. We further reveal that it is difﬁcult for BNNs to recover the similar predictive distributions as real-valued models when training without labels. Thus, how to calibrate them is key to address the degradation in performance. Extensive experiments are conducted on the large-scale ImageNet and downstream datasets. Our method achieves substantial improvement over the simple contrastive learning baseline, and is even comparable to many mainstream supervised BNN methods. Code is avail-able at https://github.com/szq0214/S2-BNN . 1.

Introduction
The recent advances and breakthroughs in 1-bit convo-lutional neural networks (1-bit CNNs), also known as bi-nary neural networks [7, 30] mainly lie in supervised learn-ing [21, 23]. With the binary nature of BNNs, such net-works have been recognized as one of the most efﬁcient and
Contrastive loss
Contrastive loss? 1
-1
Binary y t i l i b a b o r
P y t i l i b a b o r
P 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.4 0.3 0.2 0.1 0 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10
Figure 1. Illustration of the motivation for self-supervised BNNs.
As the representation capability of BNNs is relatively limited, the output prediction is less conﬁdent than that of real-valued net-works. Hence, we argue that the conventional self-supervised methods in real-valued networks may not be optimal for BNNs. promising deep compression techniques for deploying mod-els in resource-limited devices. Generally, as introduced in [30], BNNs can produce up to 32× compressed mem-ory and 58× practical computational reduction on a CPU or mobile device. Considering the immense potential of being directly deployed in intelligent devices or low-power hard-ware, it is well worth further studying the behaviors of self-supervised BNNs (S2-BNN), i.e., BNNs without human-annotated labels, both to better understand the properties of
BNNs in academia, as well as to extend the scope of their usage in industry and real-world applications.
The goal of this paper is to study the mechanisms and properties of BNNs under the self-supervised learning sce-nario, then deliver practical guidelines on how to establish a strong self-supervised framework for them. To achieve this purpose, we start from exploring the widely used self-supervised contrastive learning in real-valued networks.
Is the well-Hence, our ﬁrst question in this paper is: performing contrastive learning in real-valued networks still suitable for self-supervised BNNs? Intuitively, binary networks are quite different from real-valued networks on both learning optimization and back-propagation of gradi-ents since the weights and activations in BNNs are dis-2165
Table 1. A brief overview of our improvement over a variety of different architectures for binarizing models. We choose XNOR Net [30],
Bi-Real Net [22] and ReActNet [21] as our backbone networks.
Supervised BNN
Contrastive Learning (MoCo V2) - Real-valued
Contrastive Learning (MoCo V2) - BNN (Baseline)
Contrastive Learning w/ Adam + lite aug. + progressive binarizing etc. (Ours) - BNN
+ Guided Learning (Ours) - BNN
Guided Learning Only (Ours) - BNN
XNOR (Re-impl.) [30] Bi-Real Net [22]
Top-5
Top-1 79.500 51.200 75.206 – 67.712 23.880 – – – – 75.890 36.996
Top-1 56.400 50.296 42.816 – – 51.242
Top-5 73.200 – 44.690 – – 61.416
ReActNet [21]
Top-5
Top-1 88.600 69.400 82.830 60.776 70.712 46.922 76.080 52.452 79.168 56.022 83.512 61.506 crete, causing dissimilar predictions between the two dif-ferent types of networks, as illustrated in Fig. 1. We answer this question by exploring the optimizers (SGD or the adap-tive Adam optimizer), learning rate schedulers, data aug-mentation strategies, etc., and give optimal designs for self-supervised BNNs. These non-trivial studies enable us to build a base solution which brings about 5.5% improvement over the na¨ıve contrastive learning of the baseline.
Subsequently, we empirically observe that the real-valued networks always achieve much better performance than BNNs on self-supervised learning (the comparison will be given later). Many recent studies [21, 23] have shown that BNNs demonstrate sufﬁcient capability to achieve ac-curacy as high as the real-valued counterparts in supervised learning, but an appropriate learning strategy is required to unleash the potential of binary networks. Our second ques-tion is thus: What are the essential causes for the perfor-mance gap between real-valued and binary neural networks in self-supervised learning? It is natural to believe that if we can expose the causes behind the inferior results and also
ﬁnd a proper method for training self-supervised BNNs to mitigate the obliterated/poor accuracy, we can categorically obtain more competitive performance for self-supervised
BNNs. Our discovery on this perspective is interesting: we observe that the distributions of predictions from BNNs and real-valued networks are signiﬁcantly different but after us-ing a frustratingly simple method through a teacher-student paradigm to calibrate the latent representation on BNNs, the performance of BNNs can be boosted substantially, with an extra ∼4% improvement.
Concretely, to address the issue of how to maximize the representation ability of self-supervised BNNs, we propose to add an additional self-supervised real-valued network branch to guide the target binary network learning. This is somewhat like knowledge distillation but the slight differ-ence is that our teacher is a self-supervision learned network and the class for the ﬁnal output is agnostic. We force the
BNNs to mimic the ﬁnal predictions of real-valued models after the projection MLP head and the softmax operation.
In our framework, we introduce a strategy that enables the
BNNs to mimic the distribution of a real-valued reference network smoothly. This procedure is called guided distil-lation in our method. Combining contrastive and guided learning is a spontaneous idea for tackling this problem, while intriguingly, we further observe that solely employ-ing guided learning without contrastive loss can dramati-cally boost the performance of the target model by an ad-ditional 5.5%. This is surprising since, intuitively, com-bining both of them seems a better choice. To shed fur-ther light on this observation, i.e., contrastive learning is not necessary for directly training self-supervised BNNs, we study the learning mechanism behind contrastive and guided/distilled techniques and derive the insights that con-trastive and guided learning basically focus on different as-pects of feature representations. Distillation forces BNNs to mimic the reference network’s predictive probability, while contrastive learning tries to discover and learn the latent pat-terns from the data itself. This paper does not argue that learning the isolated patterns by contrastive learning is not good, but from our experiments, it shows that recovering knowledge from a well-learned real-valued network with extremely high accuracy is more effective and practical for self-supervised BNNs. An overview of our improvement over various architectures is shown in Table 1.
To summarize, our contributions in this paper are:
• We are the ﬁrst to study the problem of self-supervised binary neural networks. We provide many practical de-signs, including optimizer choice, learning rate sched-uler, data augmentation, etc., which are useful to es-tablish a base framework of self-supervised BNNs.
• We further propose a guided learning paradigm to boost the performance of self-supervised BNNs. We discuss the roles of contrastive and guided learning in our framework and study the way to use them.
• Our proposed framework improves the na¨ıve con-trastive learning by 5.5∼15% on ImageNet, and we further verify the effectiveness of our learned models on the downstream datasets through transfer learning. 2.