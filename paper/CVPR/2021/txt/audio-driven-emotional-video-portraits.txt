Abstract
Despite previous success in generating audio-driven talking heads, most of the previous studies focus on the cor-relation between speech content and the mouth shape. Fa-cial emotion, which is one of the most important features on natural human faces, is always neglected in their methods.
∗Corresponding authors.
In this work, we present Emotional Video Portraits (EVP), a system for synthesizing high-quality video portraits with vivid emotional dynamics driven by audios. Speciﬁcally, we propose the Cross-Reconstructed Emotion Disentanglement technique to decompose speech into two decoupled spaces, i.e., a duration-independent emotion space and a duration-dependent content space. With the disentangled features, dynamic 2D emotional facial landmarks can be deduced.
Then we propose the Target-Adaptive Face Synthesis tech-14080
nique to generate the ﬁnal high-quality video portraits, by bridging the gap between the deduced landmarks and the natural head poses of target videos. Extensive experiments demonstrate the effectiveness of our method both qualita-tively and quantitatively.1 1.

Introduction
Generating audio-driven photo-realistic portrait video is of great need to multimedia applications, such as ﬁlm-making [22], telepresence [2] and digital human anima-tion [25, 14, 47]. Previous works have explored gener-ating talking heads or portraits whose lip movements are synced with the input speech contents. Generally, these techniques can be divided into two categories: 1) image-based methods that animate one or few frames of cropped faces [11, 44, 33, 9, 29, 46, 7], and 2) video-based editing methods that directly edit target video clips [34, 32, 35].
Nevertheless, most of the previous studies did not model emotion, a key factor for the naturalism of portraits.
Only few image-based works have discussed emotional information in talking head generation. Due to the lack of appropriate audio-visual datasets with emotional anno-tations, Vougioukas et al. [36] do not model emotions ex-plicitly. Simply encoding emotion and audio content infor-mation into a single feature, they produce preliminary re-sults with low quality. Most recently, Wang et al. [37] col-lect the MEAD dataset, which contains high-quality talk-ing head videos with annotations of both emotion category and intensity. Then they set emotion as an one-hot condi-tion to control the generated faces. However, all of these image-based methods render only minor head movements with ﬁxed or even no backgrounds, making them impracti-cal in most real-world scenarios.
Whereas, video-based editing methods, which are more applicable as discussed in [34, 15, 32, 35], have not consid-ered emotion control. Most of them only edit the mouth and keep the upper half of the video portraits unaltered, making free emotion control unaccessible.
In this study, we propose a novel algorithm named Emo-tional Video Portraits (EVP), aiming to endow the video-based editing talking face generation with the ability of emotion control from audio. We animate full portrait with emotion dynamics that better matches the speech intona-tion, leading to more vivid results. However, it is non-trivial to achieve this. There exist several intricate chal-lenges: 1) The extraction of emotion from audio is rather difﬁcult, since the emotion information is stickily entangled with other factors like the speech content. 2) The blending of the edited face and the target video is difﬁcult while syn-thesizing high ﬁdelity results. Audio does not supply any 1All materials are available at https://jixinya.github.io/ projects/evp/. cues for head poses and the global movements of a head, thus the edited head inferred from audio may have large head pose and movement variances with the target videos.
To tackle the challenges mentioned above, we manage to achieve audio-based emotion control in the proposed Emo-tional Video Portraits system with two key components, namely Cross-Reconstructed Emotion Disentanglement , and Target-Adaptive Face Synthesis. To perform emotion control on the generated portraits, we ﬁrstly propose the
Cross-Reconstructed Emotion Disentanglement technique on audios to extract two separate latent spaces: i) a duration-independent space, which is a content-agnostic encoding of the emotion; ii) a duration-dependent space, which en-codes the audio’s speech content. Once extracted, features from these latent spaces are recombined to yield a new au-dio representation, allowing a cross-reconstruction loss to be computed and optimized. However, to enable the cross-reconstructed training, paired sentences with the same con-tent but different emotions at the same length should be pro-vided. This is nearly unreachable in real-world scenarios.
To this end, we adopt Dynamic Time Warping (DTW) [3], a classic algorithm in time series analysis, to help form pseudo training pairs with aligned uneven-length speech corpus.
Following previous methods [34, 9], an audio-to-landmark animation module is then introduced with the de-composed features to deduce emotional 2D landmark dy-namics. As no pose information is provided in audio, there is a gap to be bridged between the generated landmarks and the large variances of head pose and movement in target video. To this end, we propose the Target-Adaptive Face
Synthesis technique to bridge the pose gap between the in-ferred landmarks and the target video portraits in 3D space.
With a carefully designed 3D-aware keypoint alignment al-gorithm, we are able to project 2D landmarks into the tar-get video. Finally, we train an Edge-to-Video translation network to generate the ﬁnal high-quality emotional video portraits. Extensive experiments demonstrate the superior performance of our method and the effectiveness of several key components.
Our contributions are summarized as follows:
• We propose the Emotional Video Portraits (EVP) sys-tem, which is the ﬁrst attempt to achieve emotional control in video-based editing talking face generation methods.
• We introduce Cross-Reconstructed Emotion Disentan-glement technique, to distill content-agnostic emotion features for free control.
• We introduce Target-Adaptive Face Synthesis, to syn-thesize high quality portrait by making the generated face adapt to the target video with natural head poses and movements. 14081
fa
P
L
M
Landmark Encoding 
Target Video
Aligned Landmarks la
Ec(x)
Cross-Reconstructed
Emotion
Disentanglement (Sec.3.2)
Input Audio x
Content Encoding
Ee(x)
Emotion Encoding
Audio
To
Landmark 3D-Aware Keypoint Alignment
Expression metry
Geometry se
Pose ssion
Expression metry
Geometry
Pose
Monocular  Reconstruction
Monocular  Reconstruction
Detected Landmarks lt
Predicted Landmarks  (cid:1864)(cid:4632)
Edge
Detection n o i t c e j o r
P
Reconstructed Mesh
Ed
Edge Maps
Edge-to-Video
Output Video
Target-Adaptive Face Synthesis (Sec.3.3)
Figure 2: Overview of our Emotional Video Portrait algorithm. We ﬁrst extract disentangled content and emotion information from the audio signal. Then we predict landmark motion from audio representations. The predicted motion is transferred to the edge map of the target video via a 3D-aware keypoint alignment module. Finally, the rendering network gives us photo-realistic animations of the target portrait based on the target video and edge maps. 2.