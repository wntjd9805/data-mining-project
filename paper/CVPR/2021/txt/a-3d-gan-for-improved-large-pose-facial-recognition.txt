Abstract
Facial recognition using deep convolutional neural net-works relies on the availability of large datasets of face im-ages. Many examples of identities are needed, and for each identity, a large variety of images are needed in order for the network to learn robustness to intra-class variation. In practice, such datasets are difﬁcult to obtain, particularly those containing adequate variation of pose. Generative
Adversarial Networks (GANs) provide a potential solution to this problem due to their ability to generate realistic, syn-thetic images. However, recent studies have shown that cur-rent methods of disentangling pose from identity are inade-quate. In this work we incorporate a 3D morphable model into the generator of a GAN in order to learn a nonlinear texture model from in-the-wild images. This allows genera-tion of new, synthetic identities, and manipulation of pose, illumination and expression without compromising the iden-tity. Our synthesised data is used to augment training of facial recognition networks with performance evaluated on the challenging CFP and CPLFW datasets. 1.

Introduction
State-of-the-art facial recognition (FR) algorithms are trained using millions of images. With the internet as a re-source, face-images are relatively easy to come by. How-ever, the distribution of semantics throughout these images is usually highly unbalanced. For example, the majority of available photographs are frontal portraits of smiling sub-jects, with images containing large poses being relatively scarce. Robustness to pose is currently considered to be one of the largest challenges for FR. Some researchers have attempted to avoid the problem by ﬁrst frontalising probe images [17, 34, 45], whilst others have attempted to learn additional robustness to pose by synthetically augmenting training datasets [7, 8, 30, 43]. We advocate this second ap-proach since it does not require additional resources during inference.
Synthetic augmentation of poses in training data has typ-ically been achieved by ﬁtting some 3D face model to in-Figure 1. Instances of the FLAME 3D morphable model rendered at various poses with nonlinear textures (and background) learned by our 3D GAN from the MultiPIE dataset. (The model instances correspond to the Neutral expression column of Figure 4.) put images, extracting textures, and then re-projecting those textures at modiﬁed poses [7, 43]. With recent advances in the development of Generative Adversarial Networks (GANs), however, a viable alternative has emerged. GANs have been shown to be capable of generating realistic im-ages of new identities and so restricting data-augmentation to existing identities is not necessary.
In order to gener-ate fully synthetic training data, however, disentanglement of identity from other characteristics, such as pose, is nec-essary. Recent studies have shown that 2D GAN methods struggle to achieve this disentanglement [29]. In this work we incorporate a 3D morphable model (3DMM) [26] into a
GAN so that images of new, synthetic identities can be gen-erated, and the pose modiﬁed without identity being com-promised. Our contributions are: 1. Introduction of a method of learning a nonlinear tex-ture model from in-the-wild images that can be used to generate images of synthetic identities with fully dis-entangled pose. No specially captured scans of facial texture are required. 2. Demonstration of improvements to large-pose facial recognition by augmenting datasets with synthetic, 3D
GAN images, and a state-of-art accuracy for CPLFW. 13445
2.