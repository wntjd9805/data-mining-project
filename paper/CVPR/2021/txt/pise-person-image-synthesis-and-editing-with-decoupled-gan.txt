Abstract
Person image synthesis, e.g., pose transfer, is a challeng-ing problem due to large variation and occlusion. Existing methods have difﬁculties predicting reasonable invisible re-gions and fail to decouple the shape and style of clothing, which limits their applications on person image editing. In this paper, we propose PISE, a novel two-stage generative model for Person Image Synthesis and Editing, which is able to generate realistic person images with desired poses, textures, or semantic layouts. For human pose transfer, we
ﬁrst synthesize a human parsing map aligned with the target pose to represent the shape of clothing by a parsing gener-ator, and then generate the ﬁnal image by an image genera-tor. To decouple the shape and style of clothing, we propose joint global and local per-region encoding and normaliza-tion to predict the reasonable style of clothing for invisi-ble regions. We also propose spatial-aware normalization to retain the spatial context relationship in the source im-age. The results of qualitative and quantitative experiments demonstrate the superiority of our model on human pose transfer. Besides, the results of texture transfer and region editing show that our model can be applied to person im-age editing. The code is available for research purposes at https://github.com/Zhangjinso/PISE. 1.

Introduction
Person image synthesis is a challenging problem in com-puter vision and computer graphics, which has great appli-cation potentials in image editing, video generation, virtual try-on, etc. Human pose transfer [16, 20, 23, 24, 32], i.e., synthesizing a new image for the same person in a target pose, is an active topic in person image synthesis.
Recently, Generative Adversarial Networks (GANs) [4] achieve great success in human pose transfer. Many meth-ods directly learn the mapping from the source image and pose to the target image using neural networks [32, 23, 24, 12]. Most of these methods utilize a two-branch (pose branch and image branch) framework to transfer the feature of the source image from the source pose to the target pose.
However, by taking keypoints as the pose representation,
∗Corresponding author
Figure 1. Our model PISE allows to transfer new pose or texture to a single person image, and also enables region editing. it is difﬁcult to predict a sharp and reasonable image with sparse correspondences when the source pose and the tar-get pose have large differences. To deal with this problem,
ﬂow-based methods [13, 20] estimate an appearance ﬂow to obtain denser correspondences, which is used to warp the source image or its feature to align with the target pose.
The ﬁnal image delivered by reﬁning the warped image or decoding the warped image feature is a rearrangement of the source image elements. Thus, the generated image can preserve details of the source image, but the invisible region due to occlusion is not satisfactorily recovered. To predict invisible regions, some methods [16, 29] introduce human parsing maps to human pose transfer. The human parsing map provides semantic correspondence to synthesize the ﬁ-nal image and enables applications of person image editing.
However, these methods cannot disentangle the shape and style information (e.g., the category and texture of cloth-ing) and fail to preserve spatial context relationships. For 7982
ﬂexible and detailed editing, it is better to disentangle the shape and style. Meanwhile, preserving spatial information of the source image and reasonable prediction of invisible regions are also important for producing the desired output for human pose transfer.
The aforementioned methods encounter three challenges to synthesize satisfactory images: 1) the coupling of the shape and style of clothing, 2) potential uncertainties in in-visible regions, and 3) loss of spatial context relationships.
To address these problems, we propose a novel Decou-pled GAN for person image synthesis and editing. Instead of directly learning a mapping from the source to the target, we take the human parsing map as the intermediate result to provide semantic guidance to predict a reasonable shape of clothing. We propose joint global and local per-region encoding and normalization to control the texture style on a semantic region basis, better utilizing information for both visible and invisible regions. Speciﬁcally, for the region visible in the source image, we use the local feature of the corresponding region to predict the style of clothing. For the region invisible in the source image but visible in the target image, we obtain the global feature of the source im-age to predict the reasonable style of clothing, which can well deal with the generation of invisible regions. Bene-ﬁting from the human parsing map and per-region texture control, the shape and the style of clothing are disentan-gled for more ﬂexible editing. Besides, to preserve the spa-tial context relationship, we propose a novel spatial-aware normalization to transfer spatial information of the source image to the generated image. After per-region normaliza-tion and spatial-aware normalization, the generated target feature passes through a decoder to output the ﬁnal image.
Figure 1 shows some applications of our model.
The main contributions of this work are summarized as follows:
• We propose a two-stage model with per-region control to decouple the shape and style of clothing. Exper-imental results on human pose transfer, texture trans-fer, and region editing show the ﬂexibility and superior performance of our person image synthesis and editing method.
• We propose joint global and local per-region encod-ing and normalization to predict the reasonable style of clothing for invisible regions, and preserve the orig-inal style of clothing in the target image.
• We propose a spatial-aware normalization to retain the spatial context relationship in the source image, and transfer it by modulating the scale and bias of the gen-erated image feature. 2.