Abstract
A common classiﬁcation task situation is where one has a large amount of data available for training, but only a small portion is annotated with class labels. The goal of semi-supervised training, in this context, is to improve clas-siﬁcation accuracy by leverage information not only from labeled data but also from a large amount of unlabeled data.
Recent works [2, 1, 26] have developed signiﬁcant improve-ments by exploring the consistency constrain between dif-ferently augmented labeled and unlabeled data. Following this path, we propose a novel unsupervised objective that focuses on the less studied relationship between the high conﬁdence unlabeled data that are similar to each other.
The new proposed Pair Loss minimizes the statistical dis-tance between high conﬁdence pseudo labels with similar-ity above a certain threshold. Combining the Pair Loss with the techniques developed by the MixMatch family [2, 1, 26], our proposed SimPLE algorithm shows signiﬁcant perfor-mance gains over previous algorithms on CIFAR-100 and
Mini-ImageNet [31], and is on par with the state-of-the-art methods on CIFAR-10 and SVHN. Furthermore, SimPLE also outperforms the state-of-the-art methods in the transfer learning setting, where models are initialized by the weights pre-trained on ImageNet[15] or DomainNet-Real[23]. The code is available at github.com/zijian-hu/SimPLE. 1.

Introduction
Deep learning has recently achieved state-of-the-art per-formance on many computer vision tasks. One major factor in the success of deep learning is the large labeled datasets.
However, labeling large datasets is very expensive and of-ten not feasible, especially in domains that require exper-tise to provide labels. Semi-Supervised Learning (SSL), on the other hand, can take advantage of partially labeled data, which is much more readily available, as shown in ﬁgure 1.
A critical problem in semi-supervised learning is how to generalize the information learned from limited label data
∗Equal contributions; names ordered alphabetically.
Labeled 
Data
Unlabeled 
Data
Figure 1: Illustration of an image set with a limited amount of labeled images among a large number of unlabeled im-ages. Unlike unsupervised learning methods that only ex-ploit the structure from unlabeled data, and supervised learning methods that only look at the limited amount of labeled data, semi-supervised learning utilizes information from both labeled and unlabeled data. to unlabeled data. Following the continuity assumption that close data have a higher probability of sharing the same la-bel [4], many approaches have been developed [27, 37, 8], including the recently proposed Label Propagation [14].
Another critical problem in semi-supervised learning is how to directly learn from the large amount of unlabeled data. Maintaining consistency between differently aug-mented unlabeled data has been recently studied and proved to be an effective way to learn from unlabeled data in both self-supervised learning [5, 11] and semi-supervised learn-ing [16, 25, 2, 1, 26, 22, 28, 36, 32]. Other than consistency regularization, a few other techniques have also been devel-oped for the semi-supervised learning to leverage the unla-beled data, such as entropy minimization [21, 17, 10] and generic regularization [13, 19, 35, 34, 30].
The recently proposed MixMatch[2] combined the above techniques and designed a uniﬁed loss function to let the model learn from differently augmented labeled and unla-beled data, together with the mix-up [35] technique, which encourages convex behavior between samples to increase models’ generalization ability. ReMixMatch [1] further im-proves the MixMatch by introducing the Distribution Align-ment and Augmentation Anchoring techniques, which al-15099
Labeled Train Images
Labels
Labels
Weak Augmentations
Fish
Dog
Predictions
Predictions
Supervised
Loss
LX
Pair Loss
LP
Weak Augmentations
Strong Augmentations
Unlabeled Train Images
???
???
Classification
Network
Predictions
Predictions
Pseudo
Labels
Pseudo
Labels
Label 
Guessing 
Predictions
Predictions
Unsupervised
Loss
LU
Figure 2: An overview of the proposed SimPLE algorithm. SimPLE optimizes the classiﬁcation network with three training
LX for augmented labeled data; 2) unsupervised loss objectives: 1) supervised loss
LU that aligns the strongly augmented
LP that minimizes the statistical unlabeled data with pseudo labels generated from weakly augmented data; 3) Pair Loss distance between predictions of strongly augmented data, based on the similarity and conﬁdence of their pseudo labels. lows the model to accommodate and leverage from the heavily augmented samples. FixMatch [26] simpliﬁes its previous works by introducing a conﬁdence threshold into its unsupervised objective function and achieves state-of-the-art performance over the standard benchmarks.
However, while Label Propagation [14] mainly focuses on the relationship between labeled data to unlabeled data, and the MixMatch family [2, 1, 26] primarily focuses on the relationship between differently augmented unlabeled sam-ples, the relationship between different unlabeled samples is less studied.
In this paper, we propose to take advantage of the rela-tionship between different unlabeled samples. We introduce a novel Pair Loss, which minimizes the distance between similar unlabeled samples of high conﬁdence.
Combining the techniques developed by the MixMatch family [2, 1, 26], we propose the SimPLE algorithm. As shown in ﬁgure 2, the SimPLE algorithm generates pseudo labels of unlabeled samples by averaging and sharpening the predictions on multiple weakly augmented variations of the same sample. Then, we use both the labels and pseudo labels to compute the supervised cross-entropy loss and un-supervised L2 distance loss. These two terms push the de-cision boundaries to go through low-density areas and en-courage consistency among different variations of the same samples. Finally, with the newly proposed Pair Loss, we harness the relationships among the pseudo labels of dif-ferent samples by encouraging consistency among different unlabeled samples which share a great similarity.
Our contribution can be described in four folds:
•
We propose a novel unsupervised loss term that lever-ages the information from high conﬁdence similar un-labeled data pairs.
•
•
•
Combining the techniques from MixMatch family [2, 1, 26] with the new Pair Loss, we developed the novel
SimPLE algorithm for semi-supervised learning.
We performed extensive experiments on the standard benchmarks and demonstrated the effectiveness of the proposed Pair Loss. SimPLE outperforms the state-of-the-art methods on CIFAR100 and Mini-ImageNet and on par with the state-of-the-art methods on CIFAR10,
SVHN.
We also evaluated our algorithm in a realistic setting where SSL methods are applied on pre-trained mod-els, where the new proposed SimPLE algorithm also outperforms the current state-of-the-art methods. 2.