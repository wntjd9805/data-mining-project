Abstract
One of the most challenging question types in VQA is when answering the question requires outside knowledge not present in the image. In this work we study open-domain knowledge, the setting when the knowledge required to an-swer a question is not given/annotated, neither at training nor test time. We tap into two types of knowledge represen-tations and reasoning. First, implicit knowledge which can be learned effectively from unsupervised language pretrain-ing and supervised training data with transformer-based models. Second, explicit, symbolic knowledge encoded in knowledge bases. Our approach combines both—exploiting the powerful implicit reasoning of transformer models for answer prediction, and integrating symbolic representa-tions from a knowledge graph, while never losing their ex-plicit semantics to an implicit embedding. We combine di-verse sources of knowledge to cover the wide variety of knowledge needed to solve knowledge-based questions. We show our approach, KRISP (Knowledge Reasoning with
Implicit and Symbolic rePresentations), signiﬁcantly out-performs state-of-the-art on OK-VQA, the largest available dataset for open-domain knowledge-based VQA. We show with extensive ablations that while our model successfully exploits implicit knowledge reasoning, the symbolic answer module which explicitly connects the knowledge graph to the answer vocabulary is critical to the performance of our method and generalizes to rare answers. 1 1.

Introduction
Consider the example shown in Fig. 1. To answer this question, we not only need to parse the question and understand the image but also use external knowledge.
Early work in VQA focused on image and question pars-ing [2, 6, 23, 49, 50] assuming all required knowledge can be learned from the VQA training set. However, learn-*Work done during internship at Facebook 1Code and more are available at https : / / github . com / facebookresearch/krisp
Symbolic Knowledge fruit juice apple juice drink citrus liquid is a used for orange orange juice
Implicit Knowledge
… In English, the color orange is named after the  appearance of the ripe orange fruit … Orange  juice or grapefruit juice is a common breakfast  beverage … 
BERT
Which liquid here comes from a citrus fruit?
?
Orange juice
Figure 1. An OK-VQA [51] example that requires external knowl-edge. Our KRISP model uses a symbolic knowledge graph as well as the implicit knowledge learned from large-scale BERT training to answer the question. ing knowledge from image-question-answer triplets in the training data is not scalable and is liable to biases in the training data. We should exploit other external knowledge sources such as Wikipedia or knowledge graphs. The recent
OK-VQA dataset [51] consists of these types of questions and allows us to study open-domain knowledge in VQA.
We can deﬁne two types of knowledge representation that can be useful for these types of questions: First we have implicit knowledge, knowledge which is embedded into some non-symbolic form such as the weights of a neu-ral network derived from annotated data or large-scale un-supervised language training. Recently, transformer- and speciﬁcally BERT- [16] based multi-modal VQA models have been proposed [40, 46, 47], which incorporate large scale language pretraining, implicitly capturing language based, as well as multimodal knowledge. This type of knowledge can be quite useful, but we ﬁnd this form of im-plicitly learned knowledge is not sufﬁcient to answer many knowledge-based questions as we will show. Perhaps this is not surprising if one considers that many facts are rare such as “Thomas Newcomen invented the steam engine” and learning them with implicit representations might be less efﬁcient while there are external sources and knowl-edge bases that state it explicitly.
The other type of knowledge typically studied is ex-plicit or symbolic knowledge, often in the form of knowl-14111
edge graphs. Approaches that use this form of knowledge either take the symbolic knowledge and then embed-and-fuse them into a larger VQA model before answer pre-diction which no longer maintains the well-deﬁned knowl-edge structures [51, 39], or by relying on a closed set of knowledge facts with strong annotation of source knowl-edge [54, 74, 77]. In the second case, the VQA dataset it-self has ground truth “facts” associated with the question, so solving these questions often ends up being the problem of retrieving a fact from the closed set. In our method, we pre-serve the symbolic meaning of our knowledge from input until answer prediction. This allows us to use knowledge that is rare or is about rare entities as learning the reason-ing logic with symbols is shared across all symbols. And unlike other work, we do not have a closed set or ground truth knowledge, so we must build a large diverse knowl-edge base for use by our model.
In this work, we develop an architecture, KRISP (Knowl-edge Reasoning with Implicit and Symbolic rePresenta-tions), to successfully combine the implicit and symbolic knowledge. Speciﬁcally, KRISP uses (i) a multi-modal
BERT-pretrained transformer to process the question and image, and take advantage of the implicit knowledge in
BERT, and (ii) a graph network to make use of symbolic knowledge bases. To cover the wide variety of knowl-edge required in OK-VQA, we draw on four very different knowledge sources to construct our knowledge graph: DB-Pedia [7], ConceptNet [44], VisualGenome [36] and hasPart
KB [10]. This covers crowdsourced data, visual data, ency-clopedic data, knowledge about everyday objects, knowl-edge about science and knowledge about speciﬁc people, places and events. Finally, our method preserves the sym-bolic meaning of the knowledge by making predictions based on the hidden state of individual nodes in the knowl-edge graph and using a late-fusion strategy to combine the implicit and symbolic parts of the model. 2.