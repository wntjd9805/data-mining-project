Abstract 1.

Introduction
The domain of Embodied AI has recently witnessed sub-stantial progress, particularly in navigating agents within their environments. These early successes have laid the building blocks for the community to tackle tasks that require agents to actively interact with objects in their environment. Object manipulation is an established re-search domain within the robotics community and poses several challenges including manipulator motion, grasping and long-horizon planning, particularly when dealing with oft-overlooked practical setups involving visually rich and complex scenes, manipulation using mobile agents (as op-posed to tabletop manipulation), and generalization to un-seen environments and objects. We propose a framework for object manipulation built upon the physics-enabled, vi-sually rich AI2-THOR framework and present a new chal-lenge to the Embodied AI community known as ArmPoint-Nav. This task extends the popular point navigation task [2] to object manipulation and offers new challenges including 3D obstacle avoidance, manipulating objects in the pres-ence of occlusion, and multi-object manipulation that ne-cessitates long term planning. Popular learning paradigms that are successful on PointNav challenges show promise, but leave a large room for improvement.
Embodied AI, the sub-specialty of artiﬁcial intelligence at the intersection of robotics, computer vision, and natural language processing continues to gain popularity amongst researchers within these communities. This has expedited progress on several fronts – open source simulators are get-ting faster, more robust, and more realistic via photoreal-ism and sophisticated physics engines, a variety of tasks are being worked on such as navigation and instruction fol-lowing, new algorithms and models are inching us towards more powerful and generalizable models and the recent de-velopment of multiple sim-to-real environments with paired worlds in simulation and real is enabling researchers to study the challenges of overcoming the domain gap from virtual to physical spaces. A notable outcome has been the development of near-perfect pure learning-based Point Nav-igation [33] agents, far outperforming classical approaches.
Most of the focus and progress in Embodied AI has re-volved around the task of navigation – including navigating to coordinates, to object instances, and to rooms. Navigat-ing around in an environment is a critical means to an end, not an end in itself. The aspiration of the Embodied AI community remains the development of embodied agents that can perform complex tasks in the real world, tasks that 4497
involve actively manipulating objects in one’s environment.
The early successes and interest in Embodied AI have laid a foundation for the community to tackle the myriad of chal-lenges that lie within the problem of object manipulation.
Object manipulation has long posed daunting challenges to roboticists. Moving manipulators within an environment requires estimating free spaces and avoiding obstacles in the scene, tasks which are rendered even harder due to the un-wieldy nature of robotic arms. Generalizing to novel en-vironments and objects is another important challenge. Fi-nally, real-world tasks often involve manipulating multiple objects in succession in cluttered scenes, which requires fairly complex visual reasoning and planning. Besides, de-veloping simulators for object manipulation poses a unique set of challenges. In contrast to navigation tasks that require camera translation and fairly rudimentary collision check-ing, object manipulation requires ﬁne grained collision de-tection between the agent, its arms, and surrounding ob-jects, and the usage of advanced physics emulators to com-pute the resulting displacements of the constituent entities.
In particular, these computations are expensive and require signiﬁcant engineering efforts to produce effective simula-tions at reasonably high frame rates.
We extend the AI2-THOR [20] framework by adding arms to its agents, enabling these agents to not only navi-gate around their environments but also actively manipulate objects within them. The newly introduced arm rig is de-signed to work with both forward and inverse kinematics, which allows one to control the arm using both joint actu-ations or by specifying the desired wrist translation. This
ﬂexibility allows Embodied AI practitioners to train poli-cies requiring ﬁne-grained actuator controls for all joints if they so desire, or instead use inbuilt kinematics functional-ities and focus solely on the desired positioning of the end of the arm and manipulator.
As a ﬁrst step towards generalizable object manipula-tion, we present the task of ARMPOINTNAV– moving in the scene towards an objects, picking it up and moving it to the desired location (Figure 1). ARMPOINTNAV builds upon the navigation task of PointNav [2] in that it is an atomic locomotive task, a key component of more complex downstream goals, speciﬁes source and target locations us-ing relative coordinates as opposed to other means such as language or images and utilizes compass as part of its sensor suite. But in contrast, it offers signiﬁcant new challenges.
Firstly, the task requires the motion of both the agent and the arm within the environment. Secondly, it frequently en-tails reaching behind occluding obstacles to pick up objects which requires careful arm manipulation to avoid collisions with occluding objects and surfaces. Thirdly, it may also re-quire the agent to manipulate multiple objects in the scene as part of a successful episode, to remove objects, or make space to move the target object, which requires long-term planning with multiple entities. Finally, the motion of the arm frequently occludes a signiﬁcant portion of the view, as one may expect, which is in sharp contrast to PointNav that only encounters static unobstructed views of the world.
The end-to-end ARMPOINTNAV model provides strong baseline results and shows an ability to not just generalize to new environments but also to novel objects within these en-vironments – a strong foundation towards learning general-izable object manipulation models. This end-to-end model is superior to a disjoint model that learns a separate policy for each skill within an episode.
In summary, we (a) introduce a novel efﬁcient frame-work (ManipulaTHOR) for low level object manipulation, (b) present a new dataset for this task with new challenges for the community, and (c) train an agent that generalizes to manipulating novel objects in unseen environments. Our framework, dataset and code will be publicly released. We hope that this new framework encourages the Embodied
AI community towards solving complex but exciting chal-lenges in visual object manipulation. 2.