Abstract
In the animation industry, cartoon videos are usually produced at low frame rate since hand drawing of such frames is costly and time-consuming. Therefore, it is desir-able to develop computational models that can automatically interpolate the in-between animation frames. However, ex-isting video interpolation methods fail to produce satisfying results on animation data. Compared to natural videos, animation videos possess two unique characteristics that make frame interpolation difﬁcult: 1) cartoons comprise lines and smooth color pieces. The smooth areas lack tex-tures and make it difﬁcult to estimate accurate motions on animation videos. 2) cartoons express stories via exagger-ation. Some of the motions are non-linear and extremely large. In this work, we formally deﬁne and study the an-imation video interpolation problem for the ﬁrst time. To address the aforementioned challenges, we propose an effec-tive framework, AnimeInterp, with two dedicated modules in a coarse-to-ﬁne manner. Speciﬁcally, 1) Segment-Guided
Matching resolves the “lack of textures” challenge by ex-ploiting global matching among color pieces that are piece-wise coherent. 2) Recurrent Flow Reﬁnement resolves the
“non-linear and extremely large motion” challenge by recur-rent predictions using a transformer-like architecture. To facilitate comprehensive training and evaluations, we build a large-scale animation triplet dataset, ATD-12K, which comprises 12,000 triplets with rich annotations. Extensive experiments demonstrate that our approach outperforms ex-∗Equal contributions; BCorresponding author. isting state-of-the-art interpolation methods for animation videos. Notably, AnimeInterp shows favorable perceptual quality and robustness for animation scenarios in the wild.
The proposed dataset and code are available at https:
//github.com/lisiyao21/AnimeInterp/. 1.

Introduction
In the animation industry, cartoon videos are produced from hand drawings of expert animators using a complex and precise procedure. To draw each frame of an animation video manually would consume tremendous time, thus lead-ing to a prohibitively high cost. In practice, the animation producers usually replicate one drawing two or three times to reduce the cost, which results in the actual low frame rate of animation videos. Therefore, it is highly desirable to develop computational algorithms to interpolate the interme-diate animation frames automatically.
In recent years, video interpolation has made great progress on natural videos. However, in animations, ex-isting video interpolation methods are not able to produce satisfying in-between frames. An example from the ﬁlm
Children Who Chase Lost Voices is illustrated in Figure 1, where the current state-of-the-art methods fail to generate a piece of complete luggage due to the incorrect motion esti-mation, which is shown in the lower-left corner of the image.
The challenges here stem from the two unique characteristics of animation videos: 1) First, cartoon images consist of ex-plicit sketches and lines, which split the image into segments of smooth color pieces. Pixels in one segment are similar, which yields insufﬁcient textures to match the corresponding 6587
(a) (b)
Figure 2: Two challenges in animation video interpola-tion. (a) Piece-wise smooth animations lack of textures. (b)
Non-linear and extremely large motions. pixels between two frames and hence increases the difﬁculty to predict accurate motions. 2) Second, cartoon animations use exaggerated expressions in pursuit of artistic effects, which result in non-linear and extremely large motions be-tween adjacent frames. Two typical cases are depicted in
Figure 2 (a) and (b) which illustrate these challenges re-spectively. Due to these difﬁculties mentioned above, video interpolation in animations remains a challenging task.
In this work, we develop an effective and principled novel method for video interpolation in animations. In particular, we propose an effective framework, AnimeInterp, to ad-dress the two aforementioned challenges. AnimeInterp con-sists of two dedicated modules: a Segment-Guided Match-ing (SGM) module and a Recurrent Flow Reﬁnement (RFR) module, which are designed to predict accurate motions for animations in a coarse-to-ﬁne manner. More speciﬁcally, the proposed SGM module computes a coarse piece-wise optical ﬂow using global semantic matching among color pieces split by contours. Since the similar pixels belonging to one segment are treated as a whole, SGM can avoid the local minimum caused by mismatching on smooth areas, which resolves the “lack of textures” problem. To tackle the “non-linear and extremely large motion” challenge in animation, the piece-wise ﬂow estimated by SGM is further enhanced by a Transformer-like network named Recurrent
Flow Reﬁnement. As shown in Figure 1, our proposed ap-proach can better estimate the ﬂow of the luggage in large displacement and generate a complete in-between frame.
A large-scale animation triplet dataset, ATD-12K, is built to facilitate comprehensive training and evaluations of video interpolation methods on cartoon videos. Unlike other ani-mation datasets, which consists of only single images, ATD-12K contains 12,000 frame triplets selected from 30 ani-mation movies in different styles with a total length over 25 hours. Apart from the diversity, our test set is divided into three difﬁculty levels according to the magnitude of the motion and occlusion. We also provide annotations on movement categories for further analysis.
The contributions of this work can be summarized as fol-lows: 1) We formally deﬁne and study the animation video interpolation problem for the ﬁrst time. This problem has signiﬁcance to both academia and industry. 2) We propose an effective animation interpolation framework named Ani-meInterp with two dedicated modules to resolve the “lack of textures” and “non-linear and extremely large motion” challenges. Extensive experiments demonstrate that Ani-meInterp outperforms existing state-of-the-art methods both quantitatively and qualitatively. 3) We build a large-scale cartoon triplet dataset called ATD-12K with large content diversity representing many types of animations to test ani-mation video interpolation methods. Given its data size and rich annotations, ATD-12K would pave the way for future research in animation study. 2.