Abstract
User-intended visual content ﬁlls the hole regions of an input image in the image editing scenario. The coarse low-level inputs, which typically consist of sparse sketch lines and color dots, convey user intentions for content creation (i.e., free-form editing). While existing methods combine an input image and these low-level controls for CNN inputs, the corresponding feature representations are not sufﬁcient to convey user intentions, leading to unfaithfully generated content. In this paper, we propose DeFLOCNet which re-lies on a deep encoder-decoder CNN to retain the guid-ance of these controls in the deep feature representations.
In each skip-connection layer, we design a structure gener-ation block. Instead of attaching low-level controls to an in-put image, we inject these controls directly into each struc-ture generation block for sketch line reﬁnement and color propagation in the CNN feature space. We then concatenate the modulated features with the original decoder features for structure generation. Meanwhile, DeFLOCNet involves
*Y. Song is the corresponding author. The results and code are available at https://github.com/KumapowerLIU/DeFLOCNet. another decoder branch for texture generation and detail enhancement. Both structures and textures are rendered in the decoder, leading to user-intended editing results. Exper-iments on benchmarks demonstrate that DeFLOCNet effec-tively transforms different user intentions to create visually pleasing content. 1.

Introduction
The investigation on image editing is growing as it re-duces signiﬁcant manual efforts during image content gen-eration. Beneﬁting from the realistic image representations brought by convolutional neural networks (CNNs), image editing is able to create meaningful while visually pleas-ant content. As shown in Fig. 1, users can draw arbitrary holes in a natural image as inputs to indicate the regions to be edited. If there are no further inputs given as shown in (a), image editing degenerates to image inpainting, where
CNNs automatically ﬁll hole regions by producing coher-ent image content as shown in (b). If there are additional inputs from users (e.g., lines in (c) and both lines and colors in (e)), CNNs will create meaningful content accordingly 10765
while maintaining visual pleasantness. Deep image editing provides ﬂexibility for users to generate diversiﬁed content, which can be widely applied in the areas of data enhance-ment, occlusion removal, and privacy protections.
The ﬂexibility of user controls and the quality of user-intended content generation are challenging to achieve si-multaneously in practice. The main difﬁculty resides on how to transform ﬂexible controls into user-intended con-tent. Existing attempts utilize high-level inputs (e.g., se-mantic parsing map [8], attributes [22], latent code [1], lan-guage [2], and visual context [21]) for semantic content generation, but ﬂexibility hinges on the predeﬁned seman-tics.
On the other hand, utilizing coarse low-level controls (i.e., sketch lines and colors) makes the editing more inter-active and ﬂexible. And in this paper, we mainly focus on incorporating such user inputs for image editing, in which we observe two main challenges: (1) Most prior investiga-tions [34, 10, 23] simply combine an input image and low-level controls together in the image level for CNN inputs.
The guidance from these low-level inputs gradually dimin-ishes in the CNN feature space, weakening their inﬂuence on generating user-intended contents. Fig. 6 (c)-(f) show such examples where facial components are not effectively produced, (2) Since users only provide sparse color strokes to control the generated colors, the model needs to prop-agate these spatially sparse signals to the desired regions guided by sketches (i.e., colors should ﬁll in the regions in-dicated by the sketches and not be wrongly rendered across sketch lines) as illustrated in Figs. 5 and 7.
To resolve these issues, we propose DeFLOCNet (i.e.,
Deep image editing via Flexible LO-level Control) to retain the guidance of low-level controls for reinforcing user in-tentions. Fig. 2 summarizes DeFLOCNet, which is built on a deep encoder-decoder for structure and texture gener-ations on the hole regions. At the core of our contribution is a novel structure generation block (Fig. 3 and Sec. 3.1), which is plugged into each skip connection in the network.
Low-level controls are directly injected into these blocks for sketch line generation and color propagation in the feature space. The structure features from these blocks are con-catenated to the original decoder features accordingly for user-intended structure generation in the hole regions.
Moreover, we introduce another decoder for texture gen-eration (Sec. 3.2). Each layer of the texture generation de-coder is concatenated to the original decoder for texture en-hancement. Thus, both structure and texture are effectively produced in the CNN feature space. They supplement orig-inal decoder features to bring coarse-to-ﬁne user-intended guidance in the CNN feature space and output visually pleasing editing results. Experiments on the benchmark datasets demonstrate the effectiveness of our DeFLOCNet compared to state-of-the-art approaches. 2.