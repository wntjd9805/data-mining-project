Abstract
Video stabilization is an essential component of visual quality enhancement. Early methods rely on feature track-ing to recover either 2D or 3D frame motion, which suffer from the robustness of local feature extraction and track-ing in shaky videos. Recently, learning-based methods seek to ﬁnd frame transformations with high-level information via deep neural networks to overcome the robustness issue of feature tracking. Nevertheless, to our best knowledge, no learning-based methods leverage 3D cues for the trans-formation inference yet; hence they would lead to artifacts on complex scene-depth scenarios. In this paper, we pro-pose Deep3D Stabilizer, a novel 3D depth-based learning method for video stabilization. We take advantage of the recent self-supervised framework on jointly learning depth and camera ego-motion estimation on raw videos. Our ap-proach requires no data for pre-training but stabilizes the input video via 3D reconstruction directly. The rectiﬁca-tion stage incorporates the 3D scene depth and camera mo-tion to smooth the camera trajectory and synthesize the sta-bilized video. Unlike most one-size-ﬁts-all learning-based methods, our smoothing algorithm allows users to manipu-late the stability of a video efﬁciently. Experimental results on challenging benchmarks show that the proposed solution consistently outperforms the state-of-the-art methods on al-most all motion categories. 1.

Introduction
Video stabilization removes the undesired shaky motion and preserves the primary motion from a video, which is a critical module to video acquisition and pre-processing.
Video stabilization methods often estimate the motion be-tween frames at ﬁrst, and then the inter-frame motion esti-mated can be used in a smoothing algorithm for stabilizing the input video. Traditional video stabilization methods can be categorized into two types according to the dimensional-ity of the motion model. The 2D-motion methods model the tracked features with 2D transformations (e.g. afﬁnity, ho-mography). Earlier 2D-based approaches (such as [15, 10]) adopt full-frame 2D transformations and show promising stabilization qualities. Nonetheless, the full-frame 2D mo-tion would suffer from spatially variant motion (e.g. paral-lax effect) caused by complex 3D scene structures. Some 2D-based methods [21, 19] divide the frame into grids and extract the 2D local motion of each grid to handle paral-lax effect. Despite the local motion models show better re-sults than full-frame models, the local motion methods are still challenging since it is difﬁcult to coordinate the grid-wise transformations to avoid the local distortions thus in-troduced. The 3D approaches [16, 14, 8, 20] acquire 3D in-formation to model the frame motion in 3D space by using
Structure-from-Motion (SfM) or additional sensors such as inertial measurement unit (IMU) and depth sensors. Hence, the 3D approach often requires expensive costs but shows superior results in contrast to the 2D approach.
The performance of earlier 2D and 3D approaches often rely on successful feature tracking for motion estimation.
Yet it is challenging to obtain robust and long feature tracks, particularly from shaky videos. This restricts the perfor-mance of early approaches. The convolution neural net-work (CNN)-based video stabilization methods [29, 31, 35] learn to extract the frame motion in CNN parameter space and infer the stabilization transformation directly. Some learning-based methods train the networks on synchronized stable/unstable video pairs [29] to ﬁnd frame transforma-tions from ground-truth stable video directly. However, the application would be limited by the generalization capabil-ity as it is demanding to collect all types of unsteady videos for training in advance. On the other hand, these prior stud-ies are based on 2D transformation and thus they still suffer from parallax effect and introduce distortions.
We introduce a novel 3D-based learning method for video stabilization. To the best of our knowledge, our pro-posed Deep3D Stabilizer is the ﬁrst learning method that handles the stabilization process with learned 3D informa-tion. The pipeline of our method is provided in Figure 1.
The CNNs jointly learn the scene depth and 3D camera mo-tion for the input video during test time in an optimization 10621
Figure 1. Pipeline of the proposed method: The pipeline consists of two stages. Firstly, the 3D geometry optimization stage estimates the 3D camera trajectory and the dense scene depth of the input RGB sequence with PoseNet and DepthNet, respectively, via test-time training. The optimization stage takes the input sequence and the corresponding optical ﬂows as the guidance signal for learning the 3D scene. Secondly, the frame rectiﬁcation stage takes the estimated camera trajectory and scene depth as input to perform view synthesis on a smoothed trajectory. The smoothing process enable users to manipulate the parameter of smoothing ﬁlter to attain different levels of stability of the resulted video, which is then wrapped and cropped to produce the stabilized video. framework without needing training data. Our solution only requires input frames and the precomputed optical ﬂow be-tween consecutive frames as guidance signals for 3D ge-ometry optimization. Consequently, the rectiﬁcation stage takes the optimized 3D information to perform stabilization via smoothing the camera motion and reprojecting the in-put frames with scene depth. Our method can model the 3D scene structure and lead to low distortion, particularly when dealing with the videos with parallax effect. Moreover, our smoothing algorithm enables users to manipulate or alter the stability of the stabilized video online, which is a criti-cal functionality for video stabilization while is overlooked by most previous learning-based studies since their meth-ods are difﬁcult to provide such freedom of manipulation.
In sum, main contributions of our approach include:
• We introduce the ﬁrst 3D-based deep CNN method for video stabilization without needing training data.
• Our approach can handle parallax effect more properly leveraging 3D motion model.
• Our stabilization solution allows users to manipulate the stability of a video in real-time (34.5 fps). 2.