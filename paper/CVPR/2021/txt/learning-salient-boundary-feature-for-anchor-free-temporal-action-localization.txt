Abstract
Temporal action localization is an important yet chal-lenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video. While most current models achieve good results by using pre-deﬁned anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors.
Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention.
In this paper, we propose the ﬁrst purely anchor-free localization method, which is both efﬁcient temporal and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based reﬁnement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can ﬁnd the accurate boundary given arbitrary propos-als. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Code is available at https://github.com/
TencentYoutuResearch / ActionDetection -AFSD. 1.

Introduction
Recently, with the progress of technology, a dramatically increasing number of videos have been stored and acces-sible from various daily activities. Temporal Action Lo-∗ indicates equal contributions. This work was done when Chengming
Xu was an intern at Tencent Youtu Lab. Yanwei Fu is the corresponding author. video
⋯ feature
⋯ action: throw
“throw”
Anchor adjust
“throw”
“throw”
“throw”
Anchor-free
Anchor-based extra classifier
“throw”
Actionness
Figure 1. Compared with actionness and anchor-based methods, anchor-free method is more efﬁcient and ﬂexible to produce fewer proposals without any extra classiﬁer and pre-deﬁned anchors. calization (TAL), as a fundamental aspect of video under-standing, thus plays an important role in real life, extending in several practical applications such as video analysis and summarization, human interaction, etc. Compared with ac-tion recognition that takes medium-range videos as input and only requires class labels as prediction, TAL is aimed at not only classifying every activity instance in each video, but also looking for the accurate temporal locations of them.
Current TAL models mainly focus on learning action-ness of each frame [20, 18, 17, 26, 27, 24] or adjustment of pre-deﬁned anchors [38, 23, 22], named actionness-guided methods and anchor-based methods, as shown in Fig. 1.
In spite of reasonable good results on benchmark datasets, such methods are still limited to the following points: (1)
Both methods will produce a bunch of redundant proposals.
For example, given a video with T frames, we have to pro-duce O(T 2) and C ·T proposals for the “actionness-guided”
BSN [20], and “anchor-based” R-C3D [38], individually.
Here C is the number of pre-deﬁned anchors. These pro-posals lead to prohibitive computational cost in both calcu-3320
lating the training loss and post-processing for testing. (2)
Actionness-guided methods can solely provide predictions of temporal boundary, while they have to rely on the extra model such as S-CNN [33] and P-GCN [41] for classiﬁca-tion. Nevertheless, the models of two stages are isolated and thus incapable of sharing information for the end-to-end up-date. (3) Typically, anchor-based methods are very sensitive to some critical hyper-parameters, such as the number and size of pre-deﬁned anchors; and it is very non-trivial to tune these hyper-parameters in the real-world applications.
Alternatively, an efﬁcient recipe for localization is to re-sort to the anchor-free method, which does not require pre-deﬁned anchors. Typically, this type of method only gener-ates one proposal for each temporal location in the form of a pair of values representing the distance between the start and end moments to the current location, individually.
In contrast to the existed methods, anchor-free model saves huge amount of pre-deﬁned anchors, while assem-bling both boundary regression and classiﬁcation in one model, thus being productive. Furthermore, even though some pilot studies, e.g., Yang et. al. [40] observed rela-tively weak results for anchor-free methods, the supporting evidence in object detection [42] shows that such methods with well designed network structure and training strategy should, in principle, be comparable with anchor-based ones.
To this end, in this paper we propose a novel purely anchor-free TAL framework dubbed Anchor-Free Saliency-based Detector (AFSD). Essentially, we ﬁrst build a naive anchor-free predictor containing an end-to-end trainable backbone network, a feature pyramid network and a sim-ple prediction network which outputs the action class and the temporal distance of the start and end from each loca-tion. To learn a more accurate boundary, we refer to former
TAL methods [20, 18] indicating the importance of bound-ary or context feature. These works obtain such features mainly by merging the neighbor of start and end moments with convolutions or mean pooling. However, we claim that in fact moment-level feature is more valuable than region-level feature for distinguishing whether an action starts or ends. As shown in Fig. 2, the background regions near the start and end moments are showing other irrelevant scenes, while regions inside the action are almost the same, which cannot provide any information for judging if the action starts or ends. Such an example indicates the importance of a moment-level feature.
Therefore, we propose a novel boundary pooling which, instead of aggregating the whole region, tries to ﬁnd the most salient moment-level feature for both start and end re-gions. We further equip the boundary pooling with a newly proposed Boundary Consistency Learning (BCL) strategy to regularize the pooling operation to provide the correct boundary features for each action. In detail, we employ a modiﬁed ground truth signal indicating start and end mo-Figure 2. An action instance of cliff diving. Note that the start and end moments of the movement are more salient than others, which can bring us signiﬁcant information to judge the boundary and completeness of the action. ments to guide the model. Then, we rearrange the video clips to help model discriminate background and action fea-tures by self-supervised contrastive learning. We conduct extensive experiments on THUMOS14 and ActivityNet1.3.
On THUMOS14 our model attains 3.7% improvement on mAP@0.5 against the state-of-the-art methods. The results on ActivityNet1.3 are also comparable.
In summary, our paper has the following contributions: 1. We, for the ﬁrst time, propose a purely anchor-free temporal action localization model. This model enjoys not only less hyper-parameter to tune and less outputs to process, but also better performance, thus making the best of both worlds. 2. To make full use of the anchor-free framework, we discuss the impact of boundary features and pro-pose novel boundary pooling method whose output is used along with the coarse proposals to generate ﬁne-grained predictions. Moreover, we introduce a novel
Boundary Consistency Learning strategy which can constrain the model to learn better boundary feature. 2.