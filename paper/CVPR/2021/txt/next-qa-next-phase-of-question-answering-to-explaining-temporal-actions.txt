Abstract
We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the tempo-ral actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reason-ing, temporal action reasoning, and common scene com-prehension. Through extensive analysis of baselines and es-tablished VideoQA techniques, we ﬁnd that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these mod-els to reason and highlights possibilities for improvement.
With detailed results for different question types and heuris-tic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superﬁcial description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git) 1.

Introduction
Actions in videos are often not independent but rather related with causal and temporal relationships [3]. For ex-ample, in the video in Figure 1, a toddler cries because he falls, and a lady runs to the toddler in order to pick him up.
Recognizing the objects “toddler”, “lady” and describing the independent action contents like “a toddler is crying” and “a lady picks the toddler up” in a video are now possible with advanced neural network models [14, 28, 63]. Yet be-ing able to reason about their causal and temporal relations and answer natural language questions (e.g., “Why is the toddler crying?”, “How did the lady react after the toddler fell?”), which lies at the core of human intelligence [39], remains a great challenge for computational models and is also much less explored by existing video understanding tasks [22, 37, 49, 52, 56].
In this work, we study causal and temporal action rea-Figure 1: NExT-QA is a question answering benchmark tar-geting the explanation of video contents. It challenges QA models to reason about causal and temporal actions and un-derstand the rich object interactions in daily activities. soning in video question answering (VideoQA) and con-tribute NExT-QA, a benchmark to foster the Next genera-tion of VQA models to Explain Temporal actions. NExT-QA contains 5,440 videos and about 52K manually anno-tated question-answer pairs grouped into causal, temporal and descriptive questions. An overview of the typical ques-tions and their distributions are found in Figure 1. To em-body the reasoning challenges and provide effective diag-nostics for video QA models, we set up two tasks at dif-ferent difﬁculty levels. At the ﬁrst level, multi-choice QA provides ﬁve candidate answers for each question and re-quires the models to pick out the correct one. At the second level, open-ended QA requires the models to generate the answers in short phrases with cues only from the videos and the questions (i.e., no candidate options).
Using NExT-QA, we evaluate several state-of-the-art (SOTA) video QA techniques [9, 11, 18, 19, 23, 26]. While the top-performing methods achieve compelling results on their performances on commonly descriptive questions, causal and temporal questions are far from satisfactory. Fur-thermore, when adapting the models that are effective on 9777
multi-choice QA to open-ended QA, we ﬁnd that they strug-gle to automatically answer the questions. This prompts a fundamental concern that these models do not truly under-stand the causal and temporal structure over the actions. As such, NExT-QA offers new challenges and ample opportu-nities to spark future research for a deeper understanding of video content.
To summarize our contributions: 1) we explore causal and temporal action reasoning in VideoQA to advance video understanding beyond shallow description towards deeper explanation; 2) we contribute NExT-QA, a rigorously cu-rated VideoQA benchmark with manual annotations to fos-ter research on causal and temporal action reasoning; and 3) we extensively analyze the baselines as well as the estab-lished video reasoning techniques on NExT-QA, providing detailed results for different question types and heuristic ob-servations for future works. 2.