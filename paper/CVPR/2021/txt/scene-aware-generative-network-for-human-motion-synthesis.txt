Abstract
We revisit human motion synthesis, a task useful in vari-ous real-world applications, in this paper. Whereas a num-ber of methods have been developed previously for this task, they are often limited in two aspects: 1) focus on the poses while leaving the location movement behind, and 2) ignore the impact of the environment on the human motion. In this paper, we propose a new framework, with the interaction between the scene and the human motion taken into ac-count. Considering the uncertainty of human motion, we formulate this task as a generative task, whose objective is to generate plausible human motion conditioned on both the scene and the human’s initial position. This framework fac-torizes the distribution of human motions into a distribution of movement trajectories conditioned on scenes and that of body pose dynamics conditioned on both scenes and trajec-tories. We further derive a GAN-based learning approach, with discriminators to enforce the compatibility between the human motion and the contextual scene as well as the 3D-to-2D projection constraints. We assess the effectiveness of the proposed method on two challenging datasets, which cover both synthetic and real-world environments. 1.

Introduction
The ability to synthesize human motions is beneﬁcial to many real-world applications, including virtual reality, ﬁlm-making, and stochastic action forecasting. Previous meth-ods [1, 2, 6, 9, 17, 20, 31, 33] for human motion synthesis often focus only on the movements of human bodies, while the scene context is neglected. Basically, people move their bodies for interacting with the outside world and are re-stricted by the outside world. It is hard to execute reason-able movements without observing the surrounding envi-ronment. And thus, the problem is worth further exploring.
Inspired by the importance of scene context, in this pa-∗Work done at The Chinese University of Hong Kong. (a) (b) (c)
Figure 1. Visualization of human motions in different scenes.
For the same starting point, the human not only can go to different goals, as in (a) and (c), but also the same goals under different tra-jectories and body movements, as in (a) and (b). All these human motions are sample from our generated results. per, we aim at synthesizing human motions under scene in-ﬂuence. Actually, human motions in the scene consists of two components, namely body movements and the trajec-tory of human in the surrounding scene. This trajectory controls the human movement in the scene, and the body movements always represent the action of humans, such as walking or sitting. Thus, there are two major challenges to handle when involving scene context. The ﬁrst challenge is how to effectively reﬂect the semantic guidance provided by the scene context, e.g. do sitting action on a chair. The second challenge is how to model the complicated physical relationship between scenes and action sequences. Specif-ically, we need to know the geometric conﬁguration of the scene context to avoid the collision, e.g. where the ﬂoor is.
To solve these problems, there is an early attempt [3] in-troducing the scene context into motion forecasting, which supposes human actions are deterministic predictions when the history and the destination are given. This method treats the distribution of human motion as the distribution of end-points in the scene. While in the motion synthesis task, 12206
we argue that such this treatment may lead to gaps between the learned distribution of human motion and the one in the real world since there could be inﬁnite ways for a person to move from one place to another. They are all valid human motions, as shown in Figure 1.
Therefore, we propose our scene-aware fully genera-tive framework to close this gap in motion synthesis. This framework can learn the distribution of human motion in given scenes directly, rather than predicting human motions deterministically. Following [3], we represent scene context using an RGB image, which is relatively easy to acquire in real scenarios. Speciﬁcally, we divide the joint distribution as the trajectory prior in the scene and the conditional dis-tribution of body movements given a trajectory. Inspired by the success of convolutional sequence generation networks (CSGN) [31] in skeleton-based action synthesis, we intro-duce the scene context into CSGN to respectively model the trajectories and ﬁne-grained body movements. The dis-tribution of trajectories is ﬁrst learned by the trajectory gen-erator under the condition of given scenes· Intuitively, with the guidance of the scene context and the trajectory, it is easier for the pose generator to model the distribution of semantic compatible body movements than direct synthe-sis (e.g. human always do sitting action with the static tra-jectory and context information of chair). In this way, our method is fully generative and is capable of capturing the diversity of human motions at various levels.
To fulﬁll the physical compatibility, it is crucial to in-troduce the geometry structure of the scene as prior knowl-edge into our synthesis framework. Therefore, we super-vise the encoder to extract geometry context from the scene by the depth map. Under this supervision, the encoder can provide geometry-aware features of the scene, and we do not need to provide depth information of the scene during inference, which can be used more easily. Moreover, we also propose a projection discriminator and a context dis-criminator as geometry discriminators to further encourage the compatibility between synthesized human motions and the surrounding scene context. We deploy the projection discriminator on the 2D human motion in image coordi-nate space projected by the 3D human motion, because the abnormal human motion can be clearly exhibited by scale changes of humans in 2D space with the scene, as shown in
Figure 4. Therefore, this discriminator encourages the gen-erator to synthesize trajectories following the global struc-ture of the scene, such as the ﬂoor of the scene. To prevent the collision between the synthesized human motions to the objects in the scene, we deploy this context discriminator to the relative depth sequence of the human motion to the local environment at each time step. This discriminator en-courages humans to move to the correct places surrounding these objects, which are shown in Figure 5.
We choose two challenging datasets to evaluate the ef-fectiveness of our proposed geometry-aware fully genera-tive framework, covering both a synthesized environment (GTA-IM [3]) and a real environment (PROX [10]). On both datasets, the proposed framework is capable of synthesizing promising human motions, in terms of the ﬁdelity of each independent sequence, the diversity of multiple sampled se-quences, as well as the consistency between synthesized se-quences and their corresponding scenes. To better quantita-tively assess different methods, we also propose a series of new metrics for human motion synthesis with scene context, including Motion FID, which is inspired by FID for image synthesis, and Non-collision Rate, which borrows insights from 3D computer games and examines the potential colli-sions between human motions and the scenes.
To summarize our contributions: 1) We reformulate the task of human motion synthesis with scene context as a con-ditional generation problem to avoid the limitations of de-terministic prediction in previous works. We further pro-pose a series of quantitative metrics to enhance the evalua-tion protocol of this task. 2) We develop a novel geometry-aware fully generative framework for this task, which ex-plicitly takes the scene geometry into consideration and captures the diversity of human motions in a scene from multiple levels. 3)We propose two geometry-aware dis-criminators to encourage the compatibility between synthe-sized human motions and their corresponding scenes. 2.