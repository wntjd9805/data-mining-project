Abstract
We propose Joint-DetNAS, a uniÔ¨Åed NAS framework for object detection, which integrates 3 key components: Neu-ral Architecture Search, pruning, and Knowledge Distilla-Instead of naively pipelining these techniques, our tion.
Joint-DetNAS optimizes them jointly. The algorithm con-sists of two core processes: student morphism optimizes the student‚Äôs architecture and removes the redundant pa-rameters, while dynamic distillation aims to Ô¨Ånd the opti-mal matching teacher. For student morphism, weight in-heritance strategy is adopted, allowing the student to Ô¨Çex-ibly update its architecture while fully utilize the predeces-sor‚Äôs weights, which considerably accelerates the search;
To facilitate dynamic distillation, an elastic teacher pool is trained via integrated progressive shrinking strategy, from which teacher detectors can be sampled without additional cost in subsequent searches. Given a base detector as the input, our algorithm directly outputs the derived student detector with high performance without additional train-ing. Experiments demonstrate that our Joint-DetNAS out-performs the naive pipelining approach by a great mar-gin. Given a classic R101-FPN as the base detector, Joint-DetNAS is able to boost its mAP from 41.4 to 43.9 on MS
COCO and reduce the latency by 47%, which is on par with the SOTA EfÔ¨ÅcientDet while requiring less search cost. We hope our proposed method can provide the community with a new way of jointly optimizing NAS, KD and pruning. 1.

Introduction
Finding the optimal tradeoff between model perfor-mance and complexity has always been a core problem for the community. The mainstream approaches aiming at addressing this issue are: Neural Architecture Search (NAS) [13, 8, 40, 15] is proposed to automatically search for promising model architectures; pruning [16, 18, 26] removes redundant parameters from a model while main-taining its performance; and Knowledge Distillation (KD)
[14, 5, 17, 38, 9] aims to transfer the learnt knowledge from
*Equal contribution
‚Ä†Corresponding author: xbjxh@live.com
Search space
Optimal arch 
Structure
Search
NAS (Parameter-agnostic)
‚Ä¶
ùê¥%
ùê¥!
ùê¥"#$
A pretrained network  with given structure  Prune weights
Pruning (Structure-agnostic)
Retrain
Finetune
Distillation (Fixed teacher-student pair)
A fixed student
A fixed teacher
ùëÉ!
Imitation
ùëÉ"
Figure 1: Limitation of NAS, pruning and KD. NAS is parameter-agnostic: The model search and training pro-cesses are decoupled, the searched architecture is retrained from scratch; Pruning is structure-agnostic: the pre-trained model has a Ô¨Åxed architecture; KD transfers knowledge between a Ô¨Åxed student-teacher pair while neglecting the structural dependence between the student and the teacher.
Our work aims to jointly optimize all three methods. a cumbersome teacher model to a more compact student model. These methods share the same ultimate goal: boost-ing the model‚Äôs performance while making it more com-pact. However, jointly optimizing them is a challenging task, especially for detection, which is much more complex than classiÔ¨Åcation. In this paper, we propose Joint-DetNAS, a uniÔ¨Åed framework for detection which jointly optimizes
NAS, pruning and KD.
The aforementioned methods each have some limita-tions, as illustrated in Figure 1. NAS and pruning only fo-cus on one aspect while neglecting the other: The current de facto paradigm of NAS considers the architecture to be the sole factor that impacts the model‚Äôs performance, while pruning only takes parameters into account and is structure-agnostic. A recent work [11] has observed an interesting phenomenon: the pruned model‚Äôs Ô¨Ånal performance highly depends on its retraining initialization. This observation in-dicates that the architecture and parameters are closely cou-10175
pled with each other, both of them play important roles in the model‚Äôs Ô¨Ånal performance, which motivates us to opti-mize them jointly.
On the other hand, the architecture of student-teacher pair is arbitrary and Ô¨Åxed during training in conventional
KD. However, recent works [9, 23] have pointed out the ex-istence of structural knowledge in KD, which implies that the teacher‚Äôs architecture has to match with the student to facilitate knowledge transfer. Therefore, we are inspired to incorporate dynamic KD into our framework, where the teacher is dynamically sampled to Ô¨Ånd the optimal match-ing for the student.
We propose Joint-DetNAS, a uniÔ¨Åed framework consist-ing of two integrated processes: student morphism and dy-namic distillation. Student morphism aims to optimize the student‚Äôs architecture while remove the redundant pa-rameters. To this end, an action space along with a weight inheritance training strategy are carefully designed, which eliminates the prerequisite of backbone‚Äôs ImageNet pre-training and allows the student to Ô¨Çexibly adjust its archi-tecture while fully utilize the predecessor‚Äôs weights. Dy-namic distillation targets at Ô¨Ånding the optimal matching teacher and transferring its knowledge to the student. To fa-cilitate teacher search without repeated training, an elastic teacher pool is built to provide sufÔ¨Åcient powerful detec-tors, which trains a super-network only once and obtains all the sub-networks with competitive performances. During the search, we adopt a neat hill climbing strategy to evolve the student-teacher pair. Thanks to weight inheritance and the elastic teacher pool, each student-teacher pair can be evaluated at the cost of fewer epochs and the Ô¨Ånal obtained student detector requires no additional training
Our framework enables further exploration on the rela-tionship between the architectures of student-teacher pair.
We observe two interesting phenomena: (1) a more power-ful detector does not necessarily make a better teacher; (2) the capacities of the student and teacher are highly corre-lated. These facts indicate the existence of structural knowl-edge and architecture matching in KD for detection.
We conduct extensive experiments to verify the effec-tiveness of each component (i.e., KD, pruning and the pro-posed elastic teacher pool) on detection task. Our Joint-DetNAS presents clear performance enhancement over 1) the input FPN baseline, 2) pipelining NAS->pruning->KD.
Given a classic R101-FPN as the base detector, our frame-work is able to boost its AP from 41.4 to 43.9 on MS COCO and reduce its latency by 47%, which is on par with the
SOTA EfÔ¨ÅcientDet [35] while requiring less search cost.
Our contributions are as follows: 1) We investigate KD and pruning for detection and carefully analyze their effec-tiveness. 2) We propose an elastic teacher pool containing sufÔ¨Åcient powerful detectors which can be directly sampled without training. 3) We develop a uniÔ¨Åed framework which jointly optimizes NAS, pruning and dynamic KD. 4) Exten-sive experiments are conducted to investigate the matching pattern between the student-teacher pair and verify the per-formance of our proposed framework. 2.