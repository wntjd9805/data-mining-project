Abstract
Pseudo-labeling is a key component in semi-supervised learning (SSL). It relies on iteratively using the model to generate artiﬁcial labels for the unlabeled data to train against. A common property among its various methods is that they only rely on the model’s prediction to make la-beling decisions without considering any prior knowledge about the visual similarity among the classes. In this paper, we demonstrate that this degrades the quality of pseudo-labeling as it poorly represents visually similar classes in the pool of pseudo-labeled data. We propose SemCo, a method which leverages label semantics and co-training to address this problem. We train two classiﬁers with two dif-ferent views of the class labels: one classiﬁer uses the one-hot view of the labels and disregards any potential similarity among the classes, while the other uses a distributed view of the labels and groups potentially similar classes together.
We then co-train the two classiﬁers to learn based on their disagreements. We show that our method achieves state-of-the-art performance across various SSL tasks includ-ing 5.6% accuracy improvement on Mini-ImageNet dataset with 1000 labeled examples. We also show that our method requires smaller batch size and fewer training iterations to reach its best performance. We make our code available at https://github.com/islam-nassar/semco. 1.

Introduction
Deep neural models require large amounts of labeled data to achieve their high performance. This quickly be-comes prohibitive and non-scalable especially when label-ing data is expensive and/or non practical. Semi-supervised learning (SSL) [5, 33] has hence emerged to explore a di-verse set of methods which aim to leverage unlabeled data to enable learning from a smaller set of labeled data.
In the context of image classiﬁcation, recent methods use unlabeled data to guide learning in different ways.
*corresponding author: islam.nassar@monash.edu
Figure 1: A conceptual diagram of our co-training solution
Some methods primarily focus on consistency regulariza-tion [29, 17], where the model is enforced to produce con-sistent predictions for different perturbed versions of the same unlabeled input image. While others focus on pseudo-labeling [1, 18, 13], where the model is used to produce artiﬁcial labels for the unlabeled data that are then used to further train the model. Evidently, combining the two ap-proaches has shown the state-of-the-art results on various image classiﬁcation tasks [30].
When it comes to pseudo-labeling, a common problem which hinders the SSL performance is the so-called con-ﬁrmation bias [32]. This takes place when the model re-assures its wrong predictions by retraining on them, lead-ing to an accumulation of the error from which the model can not recover. To mitigate this behaviour, some methods use a warm-up phase until the model becomes more reli-able [13, 32], or limit the number of pseudo-labeled sam-ples in each mini-batch [1]. Other strategies include using a conﬁdence threshold whereby a sample is only considered for pseudo-labeling if the model is highly conﬁdent about its prediction [30, 18]. One property shared by all these methods, however, is that they only rely on the model’s output and disregard any prior knowledge about potential similarities among the classes. As we show in Section 2, visually similar classes are expected to confuse the model and therefore get poorly represented in the pseudo-labeled data pool. This fact is even more exacerbated in conﬁdence-7241
based methods [18, 30] as it leads to discarding most of the visually similar samples simply because the model is rarely conﬁdent about their predictions. We show that this leads to a class imbalance in the pseudo-labeled pool, and thereby, misguides the training.
In this paper, we demonstrate that by exploiting class la-bels semantics, we can account for such similarity among the classes. We draw inspiration from few-shot learning methods [9, 41] where we use distributed embeddings to represent class labels. We present two methods to generate label embeddings in a way which encodes a weak prior on the visual similarity among the classes. One such method is based on knowledge graph embeddings [31], while an-other is based on visual attributes annotation [35]. Having such embeddings provides basis to group the class labels into visually similar concepts and allow considering such grouping while making pseudo-labeling decisions.
The beneﬁt of using label embeddings goes beyond la-bel grouping. Earlier work [9] has shown that using em-beddings as training targets (as opposed to one-hot labels) allows the model to map the image features to a more mean-ingful semantic space, and thereby, enables few shot trans-fer. In our work, we leverage this idea in a co-training [4] style approach to improve SSL performance. We propose to train two classiﬁers with the two different views of the class labels, i.e. one-hot and distributed. One of the classiﬁers makes use of the label grouping during pseudo-labelling, while the other does not. We then allow the two classiﬁers to learn from their disagreements via a shared consistency regularization loss on the unlabeled data.
We show that our method achieves new state-of-the-art results across ﬁve different datasets, while using smaller batch size with fewer training iterations. To summarize, our contributions are: 1. We propose an approach which leverages the seman-tic similarity among the classes to improve pseudo-labeling quality by addressing the confusion events. 2. We present a co-training-based SSL method which in-volves two classiﬁers co-operating via pseudo-labels obtained using their different views of the class label. 3. We show our approach outperforms the state-of-the-art in SSL by a large margin on 5 different datasets including 5.6% on Mini-Imagenet with 1000 labeled point, i.e. 10 labels per class. 2.