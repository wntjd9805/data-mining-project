Abstract
Sequence label: jump over obstacle
=
[BABEL, others]
Understanding the semantics of human movement – the what, how and why of the movement – is an important prob-lem that requires datasets of human actions with seman-tic labels. Existing datasets take one of two approaches.
Large-scale video datasets contain many action labels but do not contain ground-truth 3D human motion. Alterna-tively, motion-capture (mocap) datasets have precise body motions but are limited to a small number of actions. To address this, we present BABEL, a large dataset with lan-guage labels describing the actions being performed in mo-cap sequences. BABEL consists of language labels for over 43 hours of mocap sequences from AMASS, containing over 250 unique actions. Each action label in BABEL is precisely aligned with the duration of the corresponding action in the mocap sequence. BABELalso allows overlap of multiple ac-tions, that may each span different durations. This results in a total of over 66000 action segments. The dense annota-tions can be leveraged for tasks like action recognition, tem-poral localization, motion synthesis, etc. To demonstrate the value of BABEL as a benchmark, we evaluate the per-formance of models on 3D action recognition. We demon-strate that BABEL poses interesting learning challenges that are applicable to real-world scenarios, and can serve as a useful benchmark for progress in 3D action recogni-tion. The dataset, baseline methods, and evaluation code are available and supported for academic research pur-poses at https://babel.is.tue.mpg.de/. 1.

Introduction
A key goal in computer vision is to understand human movement in semantic terms. Relevant tasks include pre-dicting semantic labels for a human movement, e.g., action recognition [12], video description [37], temporal localiza-tion [26, 39], and generating human movement that is con-ditioned on semantics, e.g., motion synthesis conditioned
∗ Denotes equal contribution. stand transition run transition
Frame labels 
[BABEL] support body with  right hand jump over  obstacle
Figure 1. People moving naturally often perform multiple actions simultaneously, and sequentially, with transitions between them.
Existing large-scale 3D mocap datasets, however, describe an en-tire sequence with only a single action label. In BABEL, all frames and all actions are labeled. Each frame label is precisely aligned with the frames representing the action (colored wedge). This in-cludes simultaneous actions (overlapping wedges) and transitions between actions (gray wedges). on actions [10], or sentences [3, 17].
Large-scale datasets that capture variations in human movement and language descriptions that express the se-mantics of these movements, are critical to making progress on these challenging problems. Existing datasets contain detailed action descriptions for only 2D videos, e.g., Activ-ityNet [26], AVA [9] and HACS [39]. The large scale 3D datasets that contain action labels, e.g., NTU RGB+D 60
[27] and NTU RGB+D 120 [40] do not contain ground truth 3D human motion but only noisy estimates. On the other hand, motion-capture (mocap) datasets [2, 8, 11, 14] are small in scale and are only sparsely labeled with very few actions. We address this shortcoming with BABEL, a large dataset of diverse, densely annotated, actions with labels for all the actions in a motion capture (mocap) sequence.
We acquire action labels for sequences in BABEL, at two different levels of resolution. Similar to existing mo-cap datasets, we collect a sequence label that describes the action being performed in the entire sequence, e.g., jump over obstacle in Fig. 1. At a ﬁner-grained resolution, the frame labels describe the action being performed at each frame of the sequence, e.g., stand, run, etc. The frame 722
labels are precisely aligned with the corresponding frames in the sequence that represent the action. BABEL also cap-tures simultaneous actions, e.g., jump over obstacle and support body with right hand. We ensure that all frames in a sequence are labeled with at least one ac-tion, and all the actions in a frame are labeled. This results in dense action annotations for high-quality mocap data.
BABEL leverages the recently introduced AMASS dataset [20] for mocap sequences. AMASS is a large corpus of mocap datasets that are uniﬁed with a common represen-tation. It has > 43 hours of mocap data (13220 sequences) performed by over 346 subjects. The scale and diversity of
AMASS presents an opportunity for data-driven learning of semantic representations for 3D human movement.
Most existing large-scale datasets with action labels
[2, 11, 19, 21, 26, 27, 39, 40] ﬁrst determine a ﬁxed set of actions that are of interest. Following this, actors per-forming these actions are captured (3D datasets), or videos containing the actions of interest are mined from the web (2D datasets). While this ensures the presence of the action of interest in the sequence, all other actions remain unla-beled. The sparse action label for a sequence, while useful, serves only as weak supervision for data-driven models that aim to correlate movements with semantic labels. This is suboptimal. 3D datasets such as NTU RGB+D [19, 27] and
HumanAct12 [40] handle this shortcoming by cropping out segments that do not correspond to the action of interest from natural human movement sequences. While the action labels for the short segments are accurate, the cropped seg-ments are unlike the natural, continuous human movements in the real-world. Thus, the pre-segmented movements are less suitable as training data for real-world applications.
Our key idea with BABEL is that natural human move-ment often involves multiple actions and transitions be-tween them. Thus, understanding the semantics of natu-ral human movement not only involves modeling the re-lationship between an isolated action and its correspond-ing movement but also the relationship between different actions that occur simultaneously and sequentially. With
BABEL, our goal is to provide accurate data for statistical learning, which reﬂects the variety, concurrence and tempo-ral compositions of actions in natural human movement.
The current version of BABEL contains dense action an-notations for about 43.5 hours of mocap from AMASS, with 9421 unique language labels. Via a semi-automatic process of semantic clustering followed by manual categorization, we organize these into 252 actions such as greet, hop, scratch, dance, play instrument, etc. These ac-tions belong to 8 broad semantic categories involving sim-ple actions (throw, jump), complex activities (martial arts, dance), body part interactions (scratch, touch face), etc. (see Sec. 3.4). There are a total of 66289 action segments, and a single mocap sequence has 5.01 segments on average, with 3.4 unique actions. We collect the action labels and alignments by adapting an existing web annota-tion tool, VIA [6] (see Sec. 3.1). Labelling was done by using Amazon Mechanical Turk [1].
We benchmark the performance of models on BA-BEL for the 3D action recognition task [27]. The goal is to predict the action category, given a segment of mocap that corresponds to a single action span. Unlike existing datasets containing carefully constructed segments for the actions of interest, action recognition with BABEL more closely re-sembles real-world applications due to the long-tailed dis-tribution of classes in BABEL. We demonstrate that BA-BEL presents interesting learning challenges for an exist-ing action recognition model that performs well on NTU
RGB+D 60. In addition to being a useful benchmark for ac-tion recognition, we believe that BABEL can be leveraged by the community for tasks like pose estimation, motion synthesis, temporal localization, few shot learning, etc.
In this work, we make the following contributions: (1)
We provide the largest 3D dataset of dense action labels that are precisely aligned with their corresponding move-ment spans in the mocap sequence. (2) We categorize the raw language labels into over 250 action classes that can be leveraged for tasks requiring categorical label sets such as 3D action recognition. (3) We analyze the actions oc-curring in BABEL sequences in detail, furthering our se-mantic understanding of mocap data that is already widely used in vision tasks. (4) We benchmark the performance of baseline 3D action recognition models on BABEL, demon-strating that the distribution of actions that resembles real-world scenarios, poses interesting learning challenges. (5)
The dataset, baseline models and evaluation code are pub-licly available for academic research purposes at https:
//babel.is.tue.mpg.de/. 2.