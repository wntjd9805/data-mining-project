Abstract
Learning based video compression attracts increasing attention in the past few years. The previous hybrid coding approaches rely on pixel space operations to reduce spatial and temporal redundancy, which may suffer from inaccu-rate motion estimation or less effective motion compensa-tion. In this work, we propose a feature-space video cod-ing network (FVC) by performing all major operations (i.e., motion estimation, motion compression, motion compensa-tion and residual compression) in the feature space. Specif-ically, in the proposed deformable compensation module, we ﬁrst apply motion estimation in the feature space to produce motion information (i.e., the offset maps), which will be compressed by using the auto-encoder style net-work. Then we perform motion compensation by using de-formable convolution and generate the predicted feature.
After that, we compress the residual feature between the fea-ture from the current frame and the predicted feature from our deformable compensation module. For better frame reconstruction, the reference features from multiple previ-ous reconstructed frames are also fused by using the non-local attention mechanism in the multi-frame feature fusion module. Comprehensive experimental results demonstrate that the proposed framework achieves the state-of-the-art performance on four benchmark datasets including HEVC,
UVG, VTL and MCL-JCV. 1.

Introduction
There is an increasing research interest in develop-ing the next generation video compression technologies.
While the traditional video compression methods [37, 27] have achieved promising performance by using the hand-designed modules (e.g., block-based motion estimation,
Discrete Cosine Transform (DCT)) to reduce spatial and temporal redundancy, these modules cannot be optimized in an end-to-end fashion based on large-scale video data.
The recent deep video compression works [22, 38, 2, 10,
∗ Guo Lu is the corresponding author. 14, 21, 19] have achieved impressive results by applying the deep neural networks within the hybrid video compres-sion framework. Currently, most works only rely on pixel-level operations (e.g., motion estimation or motion com-pensation) for reducing redundancy. For example, pixel-level optical ﬂow estimation and motion compensation are used in [22, 21, 14, 23] to reduce temporal redundancy, and pixel-level residual is further compressed by using the auto-encoder style network. However, this pixel-level paradigm suffers from the following three limitations. First, it is dif-ﬁcult to produce accurate pixel-level optical ﬂow informa-tion, especially for videos with complex non-rigid motion patterns. Second, even we can extract sufﬁciently accurate motion information, the pixel-level motion compensation process may introduce additional artifacts. Third, it is also a non-trivial task to compress pixel-level residual informa-tion.
Given robust representation ability of deep features for various applications, it is desirable to perform motion com-pensation or residual compression in the feature space and more effectively reduce spatial or temporal redundancy in videos. Furthermore, the recent progress in deformable convolution [9, 35] has shown it is feasible to align two con-secutive frames in the feature space. Based on the learned offset maps of convolution kernels (i.e., the so-called dy-namic kernels), deformable convolution can decide which positions are sampled from the input feature. By using de-formable convolution based on dynamic kernels, we can better cope with more complex non-rigid motion patterns between two consecutive frames, which can thus improve the motion compensation results and also alleviate the bur-den for the subsequent residual compression module. Un-fortunately, it is still a non-trivial task to seamlessly incor-porate the feature space operations and deformable convolu-tion into the existing hybrid deep video compression frame-work and train the whole network in an end-to-end fashion.
In our work, instead of following the existing works [22, 14, 3, 19] to adopt the pixel-level operations as in the traditional video codecs, we propose a new feature-space video coding network (referred to as FVC) by reducing the spatial-temporal redundancy in the feature space. Speciﬁ-1502
cally, we ﬁrst estimate motion information (i.e., the offset maps for convolution kernels in deformable convolution), based on the extracted representations from two consecu-tive frames. Then the offset maps are compressed by using the auto-encoder style network and the reconstructed offset maps will be used in the subsequent deformable convolu-tion operation to generate the predicted feature for more ac-curate motion compensation. After that, we adopt another auto-encoder style network to compress the residual feature between the original input feature and the predicted feature.
Finally, a multi-frame feature fusion module is proposed to combine a set of reference features from multiple previous frames for better frame reconstruction.
When compared with the state-of-the-art learning based video compression methods [21, 14], we perform all oper-ations in the feature space for more accurate motion esti-mation and motion compensation, in which we can seam-lessly incorporate deformable convolution into the hybrid deep video compression framework. As a result, we can alleviate the errors introduced by inaccurate pixel-level op-erations like motion estimation/compensation and achieve better video compression performance. Our contributions are summarized as follows:
• We propose a new learning based video compression framework, which performs all operations including motion estimation, motion compensation and residual compression in the feature space.
• For effective motion compensation in the feature space, we use deformable convolution to “warp” the reference feature from the previous reconstructed frame and more accurately predict the feature of the current frame, in which the corresponding offset maps are compressed by using the auto-encoder style net-work.
• We propose a new multi-frame feature fusion module based on the non-local attention mechanism, which combines the features from multiple previous frames for better reconstruction of the current frame.
• Our framework FVC achieves the state-of-the-art performance on four benchmark datasets including
HEVC, UVG, VTL and MCL-JCV, which demon-strates the effectiveness of our proposed framework. 2.