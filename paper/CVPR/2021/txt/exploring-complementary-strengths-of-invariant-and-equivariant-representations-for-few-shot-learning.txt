Abstract
In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the ob-jective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predom-inantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the signiﬁcance of pow-erful feature representations with a simple embedding net-work that can outperform existing sophisticated FSL algo-rithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been em-ployed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Si-multaneous optimization for both of these contrasting ob-jectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transfor-mations. These complementary sets of features help gener-alize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive exper-imentation shows that even without knowledge distillation our proposed method can outperform current state-of-the-art FSL methods on ﬁve popular benchmark datasets. 1.

Introduction
In recent years, deep learning methods have made great strides on several challenging problems [29, 71, 28, 6, 7].
This success can be partially attributed to the availability of large-scale labeled datasets [14, 6, 81, 43]. However, acquiring large amounts of labeled data is infeasible in sev-eral real-world problems due to practical constraints such as the rarity of an event or the high cost of manual anno-tation. Few-shot learning (FSL) targets this problem by
Figure 1. Approach Overview: Shapes represent different trans-formations and colors represent different classes. While the in-variant features provide better discrimination, the equivariant fea-tures help us learn the internal structure of the data manifold.
These complimentary representations help us generalize better to new tasks with only a few training samples. By jointly leverag-ing the strengths of equivariant and invariant features, our method achieves signiﬁcant improvement over baseline (bottom row). learning a model on a set of base classes and studies its adaptability to novel classes with only a few samples (typ-ically 1-5) [19, 76, 65, 70]. Remarkably, this setting is dif-ferent from transfer and self/semi-supervised learning that assumes the availability of pretrained models [63, 79, 36] or large-amounts of unlabeled data [17, 9, 3].
FSL has been predominantly solved using ideas from meta-learning. The two most dominant approaches are optimization-based meta-learning [19, 32, 61] and metric-learning based methods [65, 70, 1]. Both sets of approaches attempt to train a base learner which can be quickly adapted in the presence of a few novel class examples. However, re-cently it has been shown in [55] that the quick adaptation of the base learner crucially depends on feature reuse. Other recent works [72, 15, 10] have also shown that a baseline 110836
feature extractor trained on all the meta-train set can achieve comparable performance to the state-of-the-art meta learn-ing based methods. This brings in an interesting question:
How much further can FSL performance be pushed by sim-ply improving the base feature extractor?
To answer this question, ﬁrst, we take a look at the in-ductive biases in machine learning (ML) algorithms. The optimization of all ML algorithms takes advantage of dif-ferent inductive biases for hypothesis selection; as the so-lutions are never unique. The generalization of these algo-rithms often relies on the effective design of inductive bi-ases, since they encode our priori preference for a particular set of solutions. For instance, regularization methods like
ℓ1/ℓ2-penalties [73], dropout [66], or early stopping [52] implicitly impose Occam’s razor in the optimization pro-cess by selecting simpler solutions. Likewise, convolutional neural networks (CNN) by design impose translation invari-ance [2] which makes the internal embeddings translation equivariant. Inspired by this, several methods [12, 20, 16] have attempted to generalize CNNs by imposing equivari-ance to different geometric transformations so that the inter-nal structure of data can be modeled more efﬁciently. On the other hand, methods like [37] try to be robust against nui-sance variations by learning transformation invariant fea-tures. However, such inductive biases do not provide opti-mal generalization on FSL tasks and the design of efﬁcient inductive designs for FSL is relatively unexplored.
In this paper, we propose a novel feature learning ap-proach by designing an effective set of inductive biases.
We observe that the features required to achieve invariance against input transformations can provide better discrim-ination, but do not ensure optimal generalization. Simi-larly, features that focus on transformation discrimination are not optimal for class discrimination but learn equiv-ariant properties that help in learning the data structure leading to better transferability. Therefore, we propose to combine the complementary strengths of both feature types through a multi-task objective that simultaneously seeks to retain both invariant and equivariant features. We argue that learning such generic features encourages the base feature extractor to be more general. We validate this claim by performing extensive experimentation on multiple bench-mark datasets. We also conduct thorough ablation studies to demonstrate that enforcing both equivariance and invari-ance outperforms enforcing either of these objectives alone (see Fig. 1).
Our main contributions are:
• We enforce complimentary equivariance and invariance to a general set of geometric transformations to model the underlying structure of the data, while remaining dis-criminative, thereby improving generalization for FSL.
• Instead of extensive architectural changes, we propose a simple alternative by deﬁning self-supervised tasks as auxiliary supervision. For equivariance, we introduce a transformation discrimination task, while an instance dis-crimination task is developed to learn transformation in-variant features.
• We demonstrate additional gains with cross-task knowl-edge distillation that retains the variance properties. 2.