Abstractions with Invertible Neural Networks
Despoina Paschalidou1,5,6 Angelos Katharopoulos3,4 Andreas Geiger1,2,5
Sanja Fidler6,7,8 1Max Planck Institute for Intelligent Systems T¨ubingen 2University of T¨ubingen 3Idiap Research Institute, Switzerland 4 ´Ecole Polytechique F´ed´erale de Lausanne (EPFL) 5Max Planck ETH Center for Learning Systems 6NVIDIA 7University of Toronto 8Vector Institute
{firstname.lastname}@tue.mpg.de angelos.katharopoulos@idiap.ch sfidler@nvidia.com
Abstract
Impressive progress in 3D shape extraction led to rep-resentations that can capture object geometries with high
ﬁdelity. In parallel, primitive-based methods seek to repre-sent objects as semantically consistent part arrangements.
However, due to the simplicity of existing primitive repre-sentations, these methods fail to accurately reconstruct 3D shapes using a small number of primitives/parts. We ad-dress the trade-off between reconstruction quality and num-ber of parts with Neural Parts, a novel 3D primitive repre-sentation that deﬁnes primitives using an Invertible Neural
Network (INN) which implements homeomorphic mappings between a sphere and the target object. The INN allows us to compute the inverse mapping of the homeomorphism, which in turn, enables the efﬁcient computation of both the implicit surface function of a primitive and its mesh, with-out any additional post-processing. Our model learns to parse 3D objects into semantically consistent part arrange-ments without any part-level supervision. Evaluations on
ShapeNet, D-FAUST and FreiHAND demonstrate that our primitives can capture complex geometries and thus simul-taneously achieve geometrically accurate as well as inter-pretable reconstructions using an order of magnitude fewer primitives than state-of-the-art shape abstraction methods. (a) Convexes (b) Neural Parts
Figure 1: Expressive Primitives. We address the trade-off between reconstruction quality and sparsity (i.e. number of parts) in primitive-based methods. Prior work [83, 61, 63, 16] has considered convex shapes as primitives, which due to their simplicity, require a large number of parts to accu-rately represent complex shapes. This results in less inter-pretable shape abstractions (i.e. primitives are not identiﬁ-able parts e.g. legs, arms etc.). In this work, we propose
Neural Parts, a novel 3D primitive representation that can represent arbitrarily complex genus-zero shapes and thus yields geometrically more accurate and semantically more meaningful parts compared to simpler primitives. 1.

Introduction
Recovering the geometry of a 3D shape from a single
RGB image is a fundamental task in computer vision and graphics. Existing shape reconstruction models utilize a neural network to learn a parametric function that maps the input image into a mesh [47, 33, 41, 84, 89, 59], a point-cloud [24, 66, 1, 40, 80, 89], a voxel grid [8, 15, 26, 68, 70, 77, 87] or an implicit surface [51, 13, 60, 73, 88, 52].
An alternative line of research focuses on compact low-dimensional representations that reconstruct 3D objects by decomposing them into simpler parts, called primitives
[83, 61, 16, 30]. Primitive-based representations seek to infer semantically consistent part arrangements across dif-ferent object instances and provide a more interpretable al-ternative, compared to representations that only focus on capturing the global object geometry. Primitives are partic-ularly useful for various applications where the notion of 13204
parts is necessary, such as shape editing, physics-based ap-plications, graphics simulation etc.
Existing primitive-based methods rely on simple shapes for decomposing complex 3D objects into parts.
In the early days of computer vision, researchers explored various shape primitives such as 3D polyhedral shapes [71], gener-alized cylinders [6] and geons [5] for representing 3D ge-ometries. More recently, the primitive paradigm has been revisited in the context of deep learning and [83, 61, 36, 16] have demonstrated the ability of neural networks to learn part-level geometries using 3D cuboids [83, 56, 92], su-perquadrics [61], spheres [36] or convexes [16]. Due to these primitives have lim-their simple parametrization, ited expressivity and cannot capture complex geometries.
Therefore, existing part-based methods require a large num-ber of primitives for extracting geometrically accurate re-constructions. However, using more primitives comes at the expense of the interpetability of the reconstruction.
To address this, we devise Neural Parts, a novel 3D primitive representation that is more expressive and inter-pretable, in comparison to alternatives that are limited to convex shapes. We argue that a primitive should be a non trivial genus-zero shape with well deﬁned implicit and ex-plicit representations. These characteristics allow us to ef-ﬁciently combine primitives and accurately represent arbi-trarily complex geometries. To this end, we pose the task of primitive learning as the task of learning a family of homeo-morphic mappings between the 3D space of a simple genus-zero shape (e.g. sphere, cube, ellipsoid) and the 3D space of the target object. We implement this mapping using an
Invertible Neural Network (INN) [19]. Being able to map 3D points in both directions allows us to efﬁciently com-pute the explicit representation of each primitive, namely its tesselation as well as the implicit representation, i.e. the relative position of a point wrt. the primitive’s surface. In contrast to prior work [83, 61, 16, 63] that directly predict the primitive parameters (i.e. centroids and sizes for cuboids and superquadrics and hyperplanes for convexes), we em-ploy the INN to fully deﬁne each primitive. Note that while a homeomorphism preserves the genus of the shape it does not constrain it in any other way. As a result, while exist-ing primitives are constrained to a speciﬁc family of shapes (e.g. ellipsoids), our primitives can capture arbitrarily com-plicated genus-zero shapes (see Fig. 1). We demonstrate that Neural Parts can be learned in an unsupervised fashion (i.e. without any primitive annotations), directly from un-structured 3D point clouds by ensuring that the assembly of predicted primitives accurately reconstructs the target.
In summary, we make the following contributions: We propose the ﬁrst model that deﬁnes primitives as a homeo-morphic mapping between two topological spaces through conditioning an INN on an image. Since the homeomor-phism does not impose any constraints on the primitive shape, our model effectively decouples geometric accuracy from parsimony and as a result captures complex geome-tries with an order of magnitude fewer primitives. Experi-ments on ShapeNet [9], D-FAUST [7] and FreiHAND [91] demonstrate that our model can parse objects into more expressive and semantically meaningful shape abstractions compared to models that rely on simpler primitives. Code and data is available at https : / / paschalidoud . github. io / neural parts. 2.