Abstract
We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural net-works are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via vol-ume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved.
In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance ﬁelds. We show that without any additional human speciﬁed supervi-sion, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neu-*Work done while the author was an intern at FRL Research. ral representation. We achieve this by jointly optimizing the parameters of two neural radiance ﬁelds and a set of rigid poses which align the two ﬁelds at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where nov-elty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion. 1.

Introduction
Recent years have seen an explosion of novel scene representations which leverage multi-layer perceptrons (MLPs) as an implicit representation to encode spatially-varying properties of a scene. While these implicit rep-resentations are optimized via stochastic gradient descent, they are not really “learning” in the traditional sense. In-stead, they exploit MLPs as a compressed representation of scene content. They can act as a substitute for traditional explicit volumetric grids), but can representations (e.g. 13144
adaptively distribute their limited capacity over the scene to enable high-ﬁdelity scene representation. Representa-tive examples include DeepSDF [16], Scene Representation
Networks [20] and Neural Radiance Fields (NeRF) [13].
Among all of these representations, NeRF [13] and its variants [8, 24, 11] show enormous potential in their ability to photorealistically reconstruct a scene from only a sparse set of images. However, these works assume the scene is static, or at least that the dynamic content of the scene is uninteresting and can be discarded as in the work by Martin-Brualla et al. [11]. When objects in a scene move, these methods can no longer correctly render novel views. It is possible to represent time-varying scenes by dedicating one
NeRF volume per frame or by extending the input to four dimensions by including time. The former is unnecessarily expensive, and neither can be used to render the object in novel poses (or remove it entirely) because they have no object-level understanding of the scene.
In this work, we aim to learn an interpretable and ed-itable representation of a dynamic scene by simply observ-ing an object in motion from multiple viewpoints. As an initial effort to tackle this challenge, we start from a simpli-ﬁed setting that assumes the scene contains only one mov-ing object and the motion is fully rigid. We accomplish this by rendering a compositional neural radiance ﬁeld with density and radiance deﬁned via a composition of a static and a dynamic neural radiance ﬁeld. Under this model, the only way all observations in a video can accurately be pre-dicted is by segmenting the scene into the two volumes and correctly estimating the pose of the object in each frame.
To achieve this goal, our paper provides two main techni-cal contributions. First, we present the ﬁrst self-supervised neural rendering based representation that can simultane-ously reconstruct a rigid moving scene as well as its back-ground from videos only. Our approach enables photo-realistic spatial-temporal novel view rendering as well as novel scene animation. Second, we present an optimization scheme that can effectively tackle the ambiguity and local optima during training.
Our experiments show that it is possible to recover the segmentation of static and dynamic contents as well as mo-tion trajectory without any supervision other than multi-view RGB observations. Compared to NeRF and its ex-tensions, our approach achieves more photorealistic recon-struction in complex synthetic as well as real world scenes.
Further, our factorized representation can be edited to po-sition the object in novel locations never observed during training, which no existing method can achieve without 3D ground truth or supervision. 2.