Abstract
Compact video super-resolution (VSR) networks can be easily deployed on resource-limited devices, e.g., smart-phones and wearable devices, but have considerable per-formance gaps compared with complicated VSR networks that require a large amount of computing resources.
In this paper, we aim to improve the performance of compact
VSR networks without changing their original architec-tures, through a knowledge distillation approach that trans-fers knowledge from a complicated VSR network to a com-pact one. Speciﬁcally, we propose a space-time distillation (STD) scheme to exploit both spatial and temporal knowl-edge in the VSR task. For space distillation, we extract spa-tial attention maps that hint the high-frequency video con-tent from both networks, which are further used for transfer-ring spatial modeling capabilities. For time distillation, we narrow the performance gap between compact models and complicated models by distilling the feature similarity of the temporal memory cells, which are encoded from the se-quence of feature maps generated in the training clips using
ConvLSTM. During the training process, STD can be easily incorporated into any network without changing the origi-nal network architecture. Experimental results on standard benchmarks demonstrate that, in resource-constrained sit-uations, the proposed method notably improves the perfor-mance of existing VSR networks without increasing the in-ference time. 1.

Introduction
Video super-resolution (VSR) aims to generate a high-resolution (HR) video from its corresponding low-resolution (LR) observation.
In the deep learning era, a variety of elaborately designed VSR networks can achieve promising super-resolution performance, yet at the cost of a large amount of computing resources. It is thus difﬁcult to deploy complicated VSR networks on resource-limited de-vices, e.g., smartphones and wearable devices. On the other hand, compact VSR networks can be easily deployed on these devices due to their lightweight architectures. How-ever, their ability to model spatial-temporal correlations is meanwhile limited due to simple architectures, which fur-ther limits their super-resolution performance.
∗Correspondence should be addressed to zwxiong@ustc.edu.cn
Figure 1: Comparisons on the runtime and the reconstruction qual-ity (PSNR) of different methods (both SISR and VSR methods are included). The running time is the average execution time for super-resolving a video clip of spatial resolution 180 × 120 with the scale factor equal to 4 on an NVIDIA 1080Ti GPU. The PSNR value refers to the average over Vid4-Walk [41].
Different from single image super-resolution (SISR) [5, 39, 42, 40, 4, 18, 48, 2], a key step in VSR is to align different frames, either explicitly or implicitly. A major-ity of VSR methods contain the motion compensation mod-ule. For example, Kappeler et al. [17] slightly modify SR-CNN [4] and extract features from frames that are aligned by optical ﬂow. However, estimating optical ﬂow itself is a challenging and time-consuming task [13, 31].
Inaccu-rate estimated optical ﬂow leads to artifacts in the ﬂow-based VSR methods. To avoid explicitly calculating op-tical ﬂow, some recent methods exploit the motion infor-mation in an implicit manner. For example, the dynamic upsampling ﬁlters [16] and the progressive fusion residual blocks [43] are designed to explore ﬂow-free motion com-pensation.
It is especially worth mentioning that, as the winner of the NTIRE2019 challenges on video restoration and enhancement [24, 25], EDVR [37] utilizes a combina-tion of pyramid, cascading, and deformable structures for multi-frame alignment. Together with temporal and spatial attention modules for information fusion, EDVR achieves state-of-the-art VSR results. Although implicit frame align-ment improves computational efﬁciency to a certain extent, these large and complex networks still require considerable computing resources and are not competent to resource-2113
constrained scenarios.
To reduce the computational cost and/or required mem-ory, a few works use recurrent schemes to introduce cost-effective network architectures for VSR [6, 14, 28]. By simply propagating the output and hidden state of previous steps with a recurrent unit, these methods achieve promis-ing reconstruction performance and greatly reduce infer-ence time. Although a good tradeoff between effectiveness and efﬁciency can be obtained, designing such recurrent networks requires tremendous efforts.
In this paper, we explore a new direction for effective and efﬁcient VSR. Instead of pursuing more advanced network design, we introduce knowledge distillation (KD) [10] to the VSR task for the ﬁrst time, which leverages intrinsic information of a teacher network to train a student network.
Without changing the original architecture or increasing the inference time of the student network (a compact one), its performance is expected to be elevated towards its teacher (a complicated one). The proposed method is especially suitable for resource-limited devices, e.g., smartphones and wearable devices. Once a more powerful teacher network is available, we only need to retrain the student network instead of deploying a new one.
To narrow the performance gap between compact mod-els and complicated models, we propose a novel space-time distillation (STD) scheme to help the training of compact networks. Speciﬁcally, for spatial-related information pro-cessing, we design a space distillation (SD) scheme to uti-lize the spatial attention maps derived from the teacher net-work as the training target of the student network. This SD scheme allows a simple student network to imitate the abil-ity of a powerful teacher network in capturing and model-ing the spatial correlation. For temporal-related informa-tion processing, the powerful teacher network has a strong ability to capture temporal correlation and maintain tem-poral consistency. Therefore, we design a time distillation (TD) scheme to narrow the gap between the temporal mem-ory cells of the teacher and student networks, which are encoded using a ConvLSTM from the sequence of feature maps with a sliding-window mechanism. This TD scheme not only improves the temporal consistency but also boosts the reconstruction accuracy. All these operations are only applied during training, and the network structures remain unchanged during inference. Compared to only using a re-construction loss (i.e., the Charbonnier loss [14, 15, 37]) for training, the proposed STD scheme can obtain additional performance gains from the teacher network.
Fig. 1 demonstrate that, with the proposed STD scheme and using EDVR as the teacher, notably improved VSR re-sults can be achieved without extra runtime for a number of existing compact networks. More comprehensive exper-iments are conducted on two VSR benchmarks: Vid4 [21] and Vimeo90K-Test [41], where three typical compact VSR networks, i.e., VESPCN [1], VSRNet [17] and FastDVD-net [34] are included for evaluation. It is veriﬁed that our proposed STD scheme improves both the reconstruction quality and the temporal consistency of VSR results while maintaining the high inference efﬁciency of these networks.
Due to its high ﬂexibility and generalizability, we believe the proposed STD method could greatly facilitate VSR on resource-limited devices. 2.