Abstract
In this paper, we address Novel Class Discovery (NCD), the task of unveiling new classes in a set of unlabeled sam-ples given a labeled dataset with known classes. We exploit the peculiarities of NCD to build a new framework, named
Neighborhood Contrastive Learning (NCL), to learn dis-criminative representations that are important to clustering performance. Our contribution is twofold. First, we ﬁnd that a feature extractor trained on the labeled set gener-ates representations in which a generic query sample and its neighbors are likely to share the same class. We exploit this observation to retrieve and aggregate pseudo-positive pairs with contrastive learning, thus encouraging the model to learn more discriminative representations. Second, we notice that most of the instances are easily discriminated by the network, contributing less to the contrastive loss.
To overcome this issue, we propose to generate hard nega-tives by mixing labeled and unlabeled samples in the feature space. We experimentally demonstrate that these two in-gredients signiﬁcantly contribute to clustering performance and lead our model to outperform state-of-the-art meth-ods by a large margin (e.g., clustering accuracy +13% on
CIFAR-100 and +8% on ImageNet). 1.

Introduction
Learning from labeled data has been a widely studied topic in the ﬁeld of machine learning, and more recently in deep learning [15, 21, 26]. Despite tremendous success, supervised learning techniques largely rely on the avail-ability of massive amounts of annotated data [8]. To get rid of the difﬁculty and expensive cost of annotating, the machine learning community has shifted the attention to techniques that can learn with limited or completely non-annotated data. To this end, many semi-supervised [5, 35] and unsupervised learning [4, 7, 14, 32] methods have been proposed, which achieve promising results compared to su-pervised learning methods. Nonetheless, not much effort has been made to exploit prior knowledge from existing la-*Equal contribution
†Corresponding author a t a d d e l e b a
L a t a d d e l e b a l n
U horse horse horse
Novel Class Discovery
Joint
Training
Model dog dog dog
？
？
？
？
？
？
Hard Negative
Generation
Classify Novel Classes
Ranking list of the query in the feature space pull push
…
…
…
… query pseudo positive augmented positive synthetic hard negative negative negative
Neighborhood Contrastive Learning
Figure 1. Illustration of novel class discovery (NCD) and the pro-posed neighborhood contrastive learning (NCL). In NCD, we are given two datasets, a labeled one and an unlabeled one, with dis-joint class sets. NCD aims to leverage all data to learn a model that can cluster the unlabeled data. NCL tries to learn discriminative representations by enforcing a query to be close to its correlated view (augmented-positive) and its pseudo-positives (neighbors), as well as to be far from the negatives. We also generate hard neg-atives by mixing between labeled and unlabeled features, which can further facilitate our NCL. beled datasets and use it to discover unknown classes that are not present in the labeled data.
In this paper, we address one such relevant problem, called Novel Class Discovery (NCD) [12, 13], where we are given a labeled dataset and an unlabeled dataset, dif-fering in class label space. The goal of NCD is to learn a model that can cluster the unlabeled data by exploiting the latent commonalities from the labeled data (see the top half of Fig. 1). Importantly, the availability of labeled data does not guarantee transferability because the patterns learned from the labeled data with off-the-shelf models might not be useful for the unlabeled data. This poses NCD apart from semi-supervised learning paradigm, where the label space is shared between labeled and unlabeled data, and also makes it more challenging. The NCD task ﬁnds relevance in many real-world scenarios where the volume of unlabeled data 10867
keeps growing (e.g., multimedia). It is desirable to leverage the existing annotated data (collected from known classes) to explore the new unlabeled data from novel classes, rather than in a completely unsupervised fashion from scratch.
With that goal in mind, this work proposes a holistic learning framework that uses contrastive loss [14, 27] for-mulation to learn discriminative features from both the la-beled and unlabelled data, which is absent in most NCD methods [12, 13, 16, 17]. Subsequently, we introduce two key ideas in the paper. The ﬁrst idea is to exploit the fact that the local neighborhood of a sample (query) in the em-bedding space will contain samples which most likely be-long to the same semantic category of the query, and can be considered as pseudo-positives. Note that this is spe-ciﬁc to the NCD setting, where we can pre-train a fea-ture extractor with supervision. We exploit this obser-vation in the context of contrastive learning to bring the query closer to its pseudo-positives, which is termed as
Neighborhood Contrastive Learning (NCL) (see the bottom half of Fig. 1). These numerous positives allow us to ob-tain a much stronger learning signal when compared to the traditional contrastive formulation realized with only two views [7, 14]. Our second idea is to address the better se-lection of negatives to further improve the contrastive learn-ing. Peculiar to the NCD task where we are given labeled samples of the known classes (a.k.a true negatives of any unlabeled instance), we exploit them, together with the un-labeled samples, to generate synthetic samples in the feature space using a mixing strategy and treat them as hard nega-tives (see Fig. 1). This circumvents the problem of falsely treating the true positives as negatives [14, 18]. We call this process as Hard Negative Generation (HNG), which is effective and can produce a boost in performance when em-ployed together with NCL.
To summarize, our contributions are threefold:
• We propose Neighborhood Contrastive Learning (NCL) for NCD, which exploits the local neighbor-hood in the embedding space of a given query. Our
NCL recruits more positive samples for the contrastive loss formulation, signiﬁcantly improving the cluster-ing accuracy.
• We propose to aid the contrastive learning by leverag-ing the labeled samples to generate hard negative sam-ples through feature mixing. With labeled data from various classes, the proposed Hard Negative Genera-tion (HNG) can obtain consistent improvement.
• Extensive experiments on three NCD benchmarks demonstrate the effectiveness of our method and show that we advance the state-of-the-art approaches by large margins (e.g., clustering accuracy +13% on
CIFAR-100 and +8% on ImageNet). 2.