Abstract
We present a high-performance method that can achieve mask-level instance segmentation with only bounding-box annotations for training. While this setting has been studied in the literature, here we show signiﬁcantly stronger perfor-mance with a simple design (e.g., dramatically improving previous best reported mask AP of 21.1% [13] to 31.6% on the COCO dataset). Our core idea is to redesign the loss of learning masks in instance segmentation, with no mod-iﬁcation to the segmentation network itself. The new loss functions can supervise the mask training without relying on mask annotations. This is made possible with two loss terms, namely, 1) a surrogate term that minimizes the dis-crepancy between the projections of the ground-truth box and the predicted mask; 2) a pairwise loss that can exploit the prior that proximal pixels with similar colors are very likely to have the same category label.
Experiments demonstrate that the redesigned mask loss can yield surprisingly high-quality instance masks with only box annotations. For example, without using any mask an-notations, with a ResNet-101 backbone and 3× training
*Corresponding author. schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1% of the fully supervised counterpart). Our ex-cellent experiment results on COCO and Pascal VOC indi-cate that our method dramatically narrows the performance gap between weakly and fully supervised instance segmen-tation.
Code is available at: https://git.io/AdelaiDet 1.

Introduction
Instance segmentation requires the algorithm to predict the pixel-wise masks and categories of instances of interest, and is one of the most fundamental tasks in computer vision.
The performance of instance segmentation has been signif-icantly advanced by a number of recent successful meth-ods [5, 6, 11, 15, 28, 30, 31]. These methods have almost made the previously much more challenging instance seg-mentation task be as simple and fast as bounding-box object detection. For example, built on the detector FCOS [29],
CondInst [28] only adds very compact dynamic mask heads to predict instance masks, and thus only introduces less than 10% computation time, compared to FCOS. Instance seg-mentation is able to provide more accurate and ﬁner mask-5443
projection loss term pairwise loss term y0 y1 image mask x0 x1 box mask
… pairwise relationship the 8 consistency maps
Figure 2: The two proposed loss terms. Top row: the projections onto x-axis and y-axis of the mask and the box, and the projections should be the same, where (x0, y0) and (x1, y1) are the two cor-ners of the box. Bottom row: the pairwise term. For each pixel, we compute the pairwise label consistency between the pixel and its 8 neighbours (with dilation rate 2). Thus each pixel has 8 edges and we have 8 consistency maps in the right. The white locations in the right ﬁgure are the edges we have the supervision derived from the color similarity, and other edges are discarded in the loss computation. level object location than detection. Thus, given that the extra computation cost is negligible, instance segmentation should be preferred over bounding box detection in many cases. For example, if a robot wants to grasp an object, an accurate mask will be much more helpful than a box. Now the main obstacle that impedes instance segmentation re-placing box detection is the signiﬁcantly heavier pixel-wise mask annotations. Compared to box-level annotations re-quired by object detection, annotating pixel-level masks is notoriously time-consuming, as shown in [3,9,19]. Here we aim to eliminate this obstacle by training instance segmen-tation using box annotations only.
A few works [2, 7, 13, 17, 19, 22, 24, 26] attempted to ob-tain (semantic or instance-level) mask prediction with box-level annotations. Among them, most methods such as Box-Sup [7] and Box2Seg [19] rely on the region proposals that are generated by MCG [23] or GrabCut [25]. One draw-back might be the slow training procedure since these algo-rithms are hard to be parallelized by modern GPUs. More-over, in order to achieve good performance, some meth-ods often require iterative training, resulting in a compli-cated training pipeline and more hyper-parameters. Most importantly, none of these methods is able to show strong weakly-supervised performance on large benchmarks such as COCO [21]. Thus almost all of them are only evaluated on small datasets such as Pascal VOC [9].
In this work, we propose a simple, single-shot and high-performance box-supervised instance segmentation method, built upon the recent fully convolutional instance segmentation framework—CondInst [28]. Our core idea is to replace the original pixel-wise mask losses in CondInst with a carefully designed mask loss consisting of two terms.
The ﬁrst term minimizes the discrepancy between the hor-izontal and vertical projections of the predicted mask and the ground-truth box (see Fig. 2 top). This essentially ensures that the tightest box covering the predicted mask matches the ground-truth box. Since the ground-truth mask and ground-truth box have the same projections on the two axes1, this can be also viewed as a surrogate term that mini-mizes the discrepancy between the projections of the pre-dicted mask and ground-truth mask. This loss term can be computed when we only have box annotations. Clearly, with this projection term, multiple masks can be projected to a same box. Therefore the projection loss alone would not sufﬁce. Thus, we introduce the second loss term, su-pervising the pairwise label consistency in proximal pixels, i.e., if two pixels have the same labels or not (Fig. 2 bot-tom). At ﬁrst glance, supervising the pairwise consistency still requires mask annotations. With only box annotations available, in principle this pairwise supervision signal is in-evitably noisy. However, an important observation is that the proximal pixels with similar colors are very likely to have the same label. Thus, we show that it is empirically plausible to determine a color similarity threshold such that only conﬁdent pairs of pixels having a same label are used in the loss computation (the white regions in the bottom right of Fig. 2), thus largely eliminating supervision noises.
Using these two loss terms, we achieve stunning instance segmentation results without using any mask annotations.
Some qualitative results are shown in Fig. 1.
Even though ideas that are relevant to either of our two observations mentioned above were studied more or less in the literature, ranging from non-deep learning methods such as CRF [18] and GrabCut [25] to deep learning-based methods such as Box2Seg [19] and BBTP [13], none of these works effectively incorporates them into a simple and appropriate framework. As a result, and more impor-tantly, performance of existing methods on large challeng-ing datasets (e.g., COCO) is far away from that of the full potential of box-supervised instance segmentation that is achievable, as we are going to reveal here. In summary, our method, termed BoxInst, enjoys the following advantages.
• The proposed method can achieve instance seg-mentation with box supervision by introducing two loss terms to the instance segmentation framework
CondInst [28]. BoxInst is simple as it does not modify the network model of CondInst at all, only using dif-ferent loss terms. This means that the inference pro-cess of the proposed BoxInst is exactly the same as 1This may not hold if the instance mask consists multiple disjointed regions. 5444
CondInst, thus naturally inheriting all desirable prop-erties of CondInst.
• BoxInst attains excellent instance segmentation perfor-mance on the large-scale benchmark COCO. With the
ResNet-101 backbone and 3× training schedule, our
BoxInst achieves 33.2% mask AP on COCO with no mask annotations used in training, outperforming a few recent fully supervised methods using the same back-bone and trained with mask annotations, including
YOLACT [4] (31.2% AP) and PolarMask [32] (32.1%
AP). We empirically show that in the semi-supervised setting, mask AP of BoxInst can be further improved, as expected (§3.6).
• Since instance masks can provide much more precise localization than boxes, we envision that BoxInst can be used in many downstream tasks to boost their per-formance without extra effort of annotating ground-truth masks. For example, we can obtain text masks using BoxInst (see the supplementary), which often help text recognition. BoxInst can also help annotate the mask-level training data for the fully-supervised settings.
Instance segmentation has long been believed to be much more challenging to solve than bounding box detection. Our strong performance of instance segmentation using only box supervision shows that it may not necessarily be the case. 1.1.