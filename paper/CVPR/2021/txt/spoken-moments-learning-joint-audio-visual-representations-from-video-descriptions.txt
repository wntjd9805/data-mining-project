Abstract 1.

Introduction
When people observe events, they are able to abstract key information and build concise summaries of what is hap-pening. These summaries include contextual and seman-tic information describing the important high-level details (what, where, who and how) of the observed event and ex-clude background information that is deemed unimportant to the observer. With this in mind, the descriptions people generate for videos of different dynamic events can greatly improve our understanding of the key information of inter-est in each video. These descriptions can be captured in captions that provide expanded attributes for video labeling (e.g. actions/objects/scenes/sentiment/etc.) while allowing us to gain new insight into what people ﬁnd important or necessary to summarize speciﬁc events. Existing caption datasets for video understanding are either small in scale or restricted to a speciﬁc domain. To address this, we present the Spoken Moments (S-MiT) dataset of 500k spoken cap-tions each attributed to a unique short video depicting a broad range of different events. We collect our descriptions using audio recordings to ensure that they remain as natu-ral and concise as possible while allowing us to scale the size of a large classiﬁcation dataset. In order to utilize our proposed dataset, we present a novel Adaptive Mean Mar-gin (AMM) approach to contrastive learning and evaluate our models on video/caption retrieval on multiple datasets.
We show that our AMM approach consistently improves our results and that models trained on our Spoken Moments dataset generalize better than those trained on other video-caption datasets. http://moments.csail.mit.edu/spoken.html
*equal contribution
Video understanding has typically been focused on ac-tion recognition and object tracking as the temporal as-pect of videos lends itself strongly to the task of repre-senting motion, a key component of an action. Breaking down video analysis to simple tasks, such as action recog-nition, allows for efﬁcient data annotation for building large datasets to train deep learning models [31, 45, 21] which has been extremely successful for images with object an-notations [34]. A main difﬁculty is that, in contrast to an image, a video often captures an interaction between agents and objects that evolves over time. These interactions can be as simple as “a person picking up a glass of water”, but even in this case three different objects (“person”, “glass” and “water”) are included in the interaction. Additionally, the video may also continue to depict the “person drink-ing from a glass” and the “person putting the glass back down on the table”. These sequential events present ad-ditional challenges for video datasets where single annota-tions may not be sufﬁcient to explain the events depicted.
Multi-label approaches to video annotation have attempted to address this problem by labeling multiple actions in a video [46, 22, 72]. However, these methods focus on sin-gle domain annotations, such as actions or objects, and do not capture additional contextual information, such as “per-son angrily putting down the dirty glass on a rusted table”, which can change the interpretation of an event and how it
ﬁts into a sequence of observations.
A solution for capturing more fully the content of video is to annotate multiple actions or objects in each video
[22, 71, 46, 49]. However labels like “drinking”, “glass”, only provide a portion of the information needed to inter-pret the veracity of the event. Additional narratives may include intuitive descriptions and intentions, such as “an ex-hausted man picks up a dirty glass of water and drinks from 14871
it before angrily putting it down on a table” which would dramatically change the event interpretation. The full lin-gual description combines these actions with adjectives and nouns (objects) that contextualize the events depicted lead-ing to a better understanding of the video. This is our goal in providing a new large scale dataset for training models for full video understanding.
We introduce a large scale video caption dataset, Spo-ken Moments in Time (S-MiT), to allow large deep learn-ing models for video understanding to learn contextual in-formation. Most existing video description datasets [70, 59, 32, 20, 79] are limited in size when compared to the large datasets for action recognition [31, 45, 21]. A likely cause is the increased cost of collecting full text descrip-tions for videos compared to single label annotations. Re-cent work in image captioning [25] addressed this problem by collecting audio descriptions for a large set of images from the Places dataset [76]. Collecting spoken captions is faster and more efﬁcient due to the low overhead of speak-ing compared to typing. In addition, recording of sponta-neous speech rather than typed text can produce more nat-ural descriptions of an event.An automatic speech recogni-tion (ASR) system was then used to transcribe the spoken descriptions to text captions. In this work, both audio, text and video models were jointly trained via contrastive learn-ing to learn joint cross-modal representations. We build on this approach and compare models that learn directly from the spoken captions to models that include a trained
ASR model which feeds generated text transcriptions into an NLP language model. We then jointly train caption and visual models (based on concatenated video and image fea-tures) using a novel Adaptive Mean Margin (AMM) ap-proach to contrastive learning to align the visual and cap-tion representations. We evaluate our models on multiple datasets for video/caption retrieval and show that a model trained using AMM on S-MiT achieves the best general per-formance across four datasets.
Altogether, our novel contributions include: 1. The large-scale Spoken Moments in Time dataset (S-MiT) which includes 500k pairs of video clips and cor-responding audio descriptions. This new dataset rep-resents the largest video description dataset available and will serve as a new benchmark for the community. 2. Benchmark models with aligned spoken caption and video representations learned via contrastive learning.
We compare approaches that learn directly from the spoken descriptions as well as approaches that include
ASR transcriptions that feed into different language models to generate caption representations. 3. An Adaptive Mean Margin (AMM) approach to cross-entropy based contrastive learning. 2.