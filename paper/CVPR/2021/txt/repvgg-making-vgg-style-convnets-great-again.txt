Abstract
We present a simple but powerful architecture of convo-lutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3 × 3 con-volution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a struc-tural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy, which is the ﬁrst time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU,
RepVGG models run 83% faster than ResNet-50 or 101% faster than ResNet-101 with higher accuracy and show fa-vorable accuracy-speed trade-off compared to the state-of-the-art models like EfﬁcientNet and RegNet. The code and trained models are available at https://github. com/megvii-model/RepVGG. 1.

Introduction
A classic Convolutional Neural Network (ConvNet),
VGG [31], achieved huge success in image recognition with a simple architecture composed of a stack of conv, ReLU, and pooling. With Inception [33, 34, 32, 19], ResNet [12] and DenseNet [17], a lot of research interests were shifted to well-designed architectures, making the models more and more complicated. Some recent architectures are based on
∗This work is supported by The National Key Research and Develop-ment Program of China (No. 2017YFA0700800), the National Natural
Science Foundation of China (No.61925107, No.U1936202) and Beijing
Academy of Artiﬁcial Intelligence (BAAI). Xiaohan Ding is funded by the
Baidu Scholarship Program 2019. This work is done during Xiaohan Ding and Ningning Ma’s internship at MEGVII Technology.
†Corresponding author.
Figure 1: Top-1 accuracy on ImageNet vs. actual speed.
Left: lightweight and middleweight RepVGG and baselines trained in 120 epochs. Right: heavyweight models trained in 200 epochs. The speed is tested on the same 1080Ti with a batch size of 128, full precision (fp32), single crop, and measured in examples/second. The input resolution is 300 for EfﬁcientNet-B3 [35] and 224 for the others. automatic [44, 29, 23] or manual [28] architecture search, or a searched compound scaling strategy [35].
Though many complicated ConvNets deliver higher ac-curacy than the simple ones, the drawbacks are signiﬁcant. 1) The complicated multi-branch designs (e.g., residual-addition in ResNet and branch-concatenation in Inception) make the model difﬁcult to implement and customize, slow down the inference and reduce the memory utilization. 2)
Some components (e.g., depthwise conv in Xception [3] and MobileNets [16, 30] and channel shufﬂe in ShufﬂeNets
[24, 41]) increase the memory access cost and lack sup-ports of various devices. With so many factors affecting the inference speed, the amount of ﬂoating-point opera-tions (FLOPs) does not precisely reﬂect the actual speed.
Though some novel models have lower FLOPs than the old-fashioned ones like VGG and ResNet-18/34/50 [12], they 13733
kernel tensor. If the parameters of a certain structure can be converted into another set of parameters coupled by another structure, we can equivalently replace the former with the latter, so that the overall network architecture is changed.
Speciﬁcally, we construct the training-time RepVGG us-ing identity and 1×1 branches, which is inspired by ResNet but in a different way that the branches can be removed by structural re-parameterization (Fig. 2,4). After training, we perform the transformation with simple algebra, as an iden-tity branch can be regarded as a degraded 1×1 conv, and the latter can be further regarded as a degraded 3 × 3 conv, so that we can construct a single 3 × 3 kernel with the trained parameters of the original 3 × 3 kernel, identity and 1 × 1 branches and batch normalization (BN) [19] layers. Conse-quently, the transformed model has a stack of 3 × 3 conv layers, which is saved for test and deployment.
Notably, the body of an inference-time RepVGG only has one single type of operator: 3 × 3 conv followed by
ReLU, which makes RepVGG fast on generic computing devices like GPUs. Even better, RepVGG allows for spe-cialized hardware to achieve even higher speed because given the chip size and power consumption, the fewer types of operators we require, the more computing units we can integrate onto the chip. Consequently, an inference chip specialized for RepVGG can have an enormous number of 3×3-ReLU units and fewer memory units (because the plain topology is memory-economical, as shown in Fig. 3). Our contributions are summarized as follows.
• We propose RepVGG, a simple architecture with favorable speed-accuracy trade-off compared to the state-of-the-arts.
• We propose to use structural re-parameterization to de-couple a training-time multi-branch topology with an inference-time plain architecture.
• We show the effectiveness of RepVGG in image classi-ﬁcation and semantic segmentation, and the efﬁciency and ease of implementation. 2.