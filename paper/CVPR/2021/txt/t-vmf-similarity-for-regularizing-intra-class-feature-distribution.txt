Abstract
Deep convolutional neural networks (CNNs) leverage large-scale training dataset to produce remarkable perfor-mance on various image classiﬁcation tasks. It, however, is difﬁcult to effectively train the CNNs on some realis-tic learning situations such as regarding class imbalance, small-scale and label noises. Regularizing CNNs works well on learning with such deteriorated training datasets by mitigating overﬁtting issues. In this work, we propose a method to effectively impose regularization on feature rep-resentation learning. By focusing on the angle between a feature and a classiﬁer which is embedded in cosine similar-ity at the classiﬁcation layer, we formulate a novel similarity beyond the cosine based on von Mises-Fisher distribution of directional statistics. In contrast to the cosine similar-ity, our similarity is compact while having heavy tail, which contributes to regularizing intra-class feature distribution to improve generalization performance. Through the exper-iments on some realistic learning situations such as of im-balance, small-scale and noisy labels, we demonstrate the effectiveness of the proposed method for training CNNs, in comparison to the other regularization methods. Codes are available at https://github.com/tk1980/tvMF. 1.

Introduction
Deep convolutional neural networks (CNNs) are fun-damental methods to produce promising performance on various computer vision tasks including visual recogni-tion [16, 27]. A large amount of parameters in CNNs are effectively optimized in an end-to-end manner on a large-scale dataset which contains plenty of image samples with detailed annotation; in other words, high-performance
CNNs demand such a healthy dataset of large-scale and clean-labeled samples. For example, ImageNet [10], a stan-dard benchmark dataset for image classiﬁcation, is com-posed of a large number of training samples, each of which is assigned one of 1000 class labels, and those samples are uniformly distributed across classes without severe bias
+1 0
+1
+1 0
+1 (a) cos θ (c) t-vMF (κ = 16) (b) t-vMF (κ = 4)
Figure 1. t-vMF similarity (7) compared to cosine similarity cos θ.
The proposed t-vMF produces compact-support similarity func-tion around the classiﬁer weight w with the parameter κ to control the compactness. It orients features x toward w as a implicit reg-ularization to enhance compact intra-class distribution. Colored line indicates similarity values in [−1, +1] over the angle θ. toward speciﬁc class categories. Such a data-hunger na-ture hinders CNNs from being applied to various real-world tasks. Due to the laborious procedure of collecting and an-notating data, real-world tasks are frequently equipped with rather deteriorated training datasets which are subject to such as class imbalance, small-scale and label noises. The
CNNs trained on those poor datasets degrade performance, e.g., due to overﬁtting.
The bottleneck of CNNs could be alleviated by re-ducing their parameter size from the architectural view-point [19, 51] and data-augmentation techniques would contribute to virtually enlarge the training data by means of injecting perturbation into real image samples [12, 50].
On the other hand, as a rather general approach, some reg-ularizations can be effectively introduced to CNNs for im-proving generalization performance [40, 46, 20, 31, 11].
A crucial feature representation is found in the neuron activations produced by the penultimate layer which are fed into the ﬁnal classiﬁer. Thus, regularization on those fea-tures contributes to enhancing feature representation learn-ing even on the deteriorated datasets where training sam-ples are too poorly collected to well model the intrinsic fea-ture distribution.
In the literature of deep learning, there are some regularization techniques for feature representa-tion such as center loss [46] to reduce within-class vari-ance and DropOut [40, 29] to inject stochastic perturba-tion. It is also possible to regularize features at a classiﬁ-6616
cation layer through end-to-end learning. A representative approach would be large-margin loss [31, 30, 45, 11] by em-bedding large-margin criterion into a softmax cross-entropy loss. The large-margin criterion renders the classiﬁer of high generalization performance [43] as well as favorable feature representation through the end-to-end learning. The large-margin methods modify logits of ground-truth class based on a cosine similarity between an input feature vector and the classiﬁer weight at the classiﬁcation.
In this work, we focus on the cosine similarity, a funda-mental metric in the classiﬁer, to impose regularization on features for improving performance especially on deterio-rated training datasets. The cosine similarity is built on the angle between two vectors which is geometrically depicted on a unit hyper-sphere, and thus we leverage von Mises-Fisher (vMF) distribution [35], one of directional statisti-cal models, to propose a novel similarity beyond the cosine similarity. The proposed similarity is a compact-support function over angles which enables us to implicitly regu-larize intra-class feature distribution (Fig. 1). While the method can be related to the regularization loss [46] and the large-margin methods [31, 11] which touch cosine sim-ilarity, the proposed method exhibits clear difference from those prior works in the following points: (1) the proposed similarity regulates features without introducing additional regularization loss, and (2) it is equally applied to all the classes without paying special attention to the ground-truth class. (3) It is also noteworthy that the proposed similarity can simply substitute the cosine similarity in a computation-ally efﬁcient form implemented by only one-line code. 1.1.