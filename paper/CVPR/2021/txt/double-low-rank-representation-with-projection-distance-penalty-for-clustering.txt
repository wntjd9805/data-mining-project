Abstract
This paper presents a novel, simple yet robust self-representation method, i.e., Double Low-Rank Representa-tion with Projection Distance penalty (DLRRPD) for clus-tering. With the learned optimal projected representations,
DLRRPD is capable of obtaining an effective similarity graph to capture the multi-subspace structure. Besides the global low-rank constraint, the local geometrical structure is additionally exploited via a projection distance penalty in our DLRRPD, thus facilitating a more favorable graph.
Moreover, to improve the robustness of DLRRPD to noises, we introduce a Laplacian rank constraint, which can fur-ther encourage the learned graph to be more discriminative for clustering tasks. Meanwhile, Frobenius norm (instead of the popularly used nuclear norm) is employed to enforce the graph to be more block-diagonal with lower complexity.
Extensive experiments have been conducted on synthetic, real, and noisy data to show that the proposed method out-performs currently available alternatives by a margin of 1.0%∼10.1%. 1.

Introduction
Clustering is one of the most fundamental unsupervised problems, aiming to group samples into categories such that samples in the same category are similar in some sense and differentiate from those of other categories in the same sense. It has been widely used in many areas (e.g., image processing [31], image segmentation [31], camera source identiﬁcation [15], and data mining [29, 30, 34, 35]). Many clustering methods (e.g., kmeans based methods [1, 24], density based methods [4, 3], and graph based methods [2]) have been proposed. Among these methods, graph based clustering methods (e.g., Ncut [22]), which classify the samples according to a similarity graph, have attracted lots of attention because of their good performance and solid mathematical foundation. Therefore, constructing a good
∗Corresponding author similarity graph is important for clustering algorithm to ob-tain good clustering results.
Self-representation models are effective to construct the similarity graph because they are proposed to exploit the subspace structure of data. These methods base on the as-sumption that a database with k clusters is drawn from a union of k independent low-dimensional subspaces. Fur-thermore, self-representation theory shows that a sample in a subspace can be linearly represented by the other points in the same subspace [5]. Hence, self-representation meth-ods use the database as the dictionary and learn a similarity graph by capturing the subspace structure.
Sparse subspace clustering (SSC) [6] is the ﬁrst self-representation method proposed to represent each data point with a few neighbors, hence it can capture the local neigh-bor relationship. In fact, SSC can’t learn the global struc-ture, and the learned graph may be too sparse for clustering
[14]. In order to capture the global structure, low-rank rep-resentation [17] was proposed to learn the low-dimensional subspace structure with a global low-rank constraint. Com-pared with SSC, LRR can learn a more denser similarity graph and capture more global information [28]. How-ever, some elements of the graphs obtained by SSC and
LRR are negative, while the similarity should be nonneg-ative. To overcome this problem, Zhuang et al. proposed a non-negative low-rank learning method which adopted both low-rank and sparse constraint and could show the similar-ity among samples directly [37]. Recently, some evidence showed that using nuclear norm to capture the subspace structure could lose the local intrinsic structure [10, 12].
Motivated by this, some Laplacian regularized LRR meth-ods [11, 19, 33] were proposed to preserve the local in-formation by learning the manifold structure embedded in the data space. Futhermore, these LRR methods use the original features which may contain some redundancy and noise. To address this issue, Wen et al. proposed an adaptive weighted nonnegative low-rank representation that used a sparse weighted matrix to reduce the bad inﬂuence of noise and redundancy information [28]. 5320
The LRR methods mentioned above use the observed data as the dictionary. When the observed data is insufﬁ-cient or corrupted by noise, the performance of these meth-ods may deteriorate [18]. Therefore, the latent low-rank representation (LatLRR) [18] was proposed to represent the data using both observed and unobserved data. Although
LatLRR performs better than LRR in matrix recovering, it still ignores the structure of the feature. In order to learn the relationship among samples and features sufﬁciently, the double low-rank representation was proposed [32], which performs better than LRR and LatLRR in face recognition.
In fact, DLRR uses a global low-rank constraint to learn the structure among the samples and ignores the local structure; in addition, DLRR may not learn the optimal projection since it doesn’t take the class information into account. To address these issues, a novel double low-rank representation model, i.e., double low-rank representation with projection distance penalty (DLRRPD), is proposed in this paper, and the major highlights of our method are as follows,
• We develop DLRRPD: a Double Low-Rank Represen-tation with Projection Distance penalty, to improve the discrimination and robustness of similarity graph for clustering tasks.
• An additional projection distance penalty is intro-duced to capture both the global and local geometrical structures, thus facilitating a sparse and discriminative graph.
• With a Laplacian rank constraint, the robustness of
DLRRPD to noises is guaranteed, and meanwhile, the effectiveness of the learned graph is further enhanced.
• Frobenius norm (instead of the widely used nuclear norm) is employed to enforce the graph to be more block-diagonal with a lower complexity, so as to im-prove the clustering performance. 2.