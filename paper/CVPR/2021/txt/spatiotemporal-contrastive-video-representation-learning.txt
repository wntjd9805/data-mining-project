Abstract
We present a self-supervised Contrastive Video Repre-sentation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our rep-resentations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentations for video self-supervised learning and
ﬁnd that both spatial and temporal information are crucial.
We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on clips that are distant in time. On Kinetics-600, a linear classiﬁer trained on the representations learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inﬂated R3D-50.
The performance of
CVRL can be further improved to 72.9% with a larger
R3D-152 (2⇥ ﬁlters) backbone, signiﬁcantly closing the gap between unsupervised and supervised video represen-tation learning. Our code and models will be available at https://github.com/tensorﬂow/models/tree/master/ofﬁcial/. 1.

Introduction
Representation learning is of crucial importance in com-puter vision tasks, and a number of highly promising recent developments in this area have carried over successfully from the static image domain to the video domain. Clas-sic hand-crafted local invariant features (e.g., SIFT [44])
⇤ The ﬁrst two authors contributed equally. This work was performed while Rui Qian worked at Google.
Figure 1. Kinetics-600 top-1 linear classiﬁcation accuracy of different spatiotemporal representations. CVRL outperforms Ima-geNet supervised [33] and SimCLR unsupervised [10] pre-training using the same 3D inﬂated ResNets, closing the gap between un-supervised and supervised video representation learning. for images have their counterparts (e.g., 3D SIFT [55]) in videos, where the temporal dimension of videos gives rise to key differences between them. Similarly, state-of-the-art neural networks for video understanding [63, 9, 31, 71, 17, 16] often extend 2D convolutional neural networks [33, 36] for images along the temporal dimension. More recently, unsupervised or self-supervised learning of representations from unlabeled visual data [32, 10, 26, 7] has gained mo-mentum in the literature partially thanks to its ability to model the abundantly available unlabeled data.
However, self-supervised learning gravitates to different dimensions in videos and images, respectively.
It is nat-ural to engineer self-supervised learning signals along the temporal dimension in videos. Examples abound, including models for predicting the future [58, 43, 28], changing tem-poral sampling rates [73], sorting video frames or clips [42, 39, 72] and combining a few tasks [5]. Meanwhile, in the domain of static images, some recent work [32, 10, 26, 7] that exploits spatial self-supervision has reported unprece-dented performance on image representation learning.
The long-standing pursuit after temporal cues for self-6964
supervised video representation learning has left self-supervision signals in the spatial subspace under-exploited for videos. To promote the spatial self-supervision sig-nals in videos, we build a Contrastive Video Representa-tion Learning (CVRL) framework to learn spatiotemporal representations from unlabeled videos. Figure 2 illustrates our framework, which contrasts the similarity between two positive video clips against those of negative pairs using the
InfoNCE contrastive loss [48]. Since there is no label in self-supervised learning, we construct positive pairs as two augmented video clips sampled from the same input video.
We carefully design data augmentations to involve both spatial and temporal cues for CVRL. Simply applying spa-tial augmentation independently to video frames actually hurts the learning because it breaks the natural motion along the time dimension. Instead, we propose a temporally con-sistent spatial augmentation method by ﬁxing the random-It is simple and yet vital as demon-ness across frames. strated in our experiments. For temporal augmentation, we take visual content into account by a sampling strategy tai-lored for the CVRL framework. On the one hand, a pair of positive clips that are temporally distant may contain very different visual content, leading to a low similarity that could be indistinguishable from those of the negative pairs.
On the other hand, completely discarding the clips that are far in time reduces the temporal augmentation effect. To this end, we propose a sampling strategy to ensure the time difference between two positive clips follows a monoton-ically decreasing distribution. Effectively, CVRL mainly learns from positive pairs of temporally close clips and sec-ondarily sees some temporally distant clips during training.
The efﬁcacy of the proposed spatial and temporal augmen-tation methods is veriﬁed by extensive ablation studies.
We primarily evaluate the learned video representations on both Kinetics-400 [38] and Kinetics-600 [8] by train-ing a linear classiﬁer following [10, 32] on top of frozen backbones. We also study semi-supervised learning, down-stream action classiﬁcation and detection to further assess
CVRL. We next summarize our main ﬁndings.
Mixing spatial and temporal cues boosts the perfor-mance. Relying on spatial or temporal augmentation only yields relatively low performance, as shown in Table 9. In contrast, we achieve an improvement of 22.9% top-1 accu-racy by combining both augmentations in the manner we proposed above, i.e., temporally consistent spatial augmen-tation and the temporal sampling strategy.
Our representations outperform prior arts. The linear evaluation of CVRL achieves more than 15% gain over competing baselines, as shown in Figure 1 and Table 2.
On Kinetics-400, CVRL achieves 12.6% improvement over
ImageNet pre-training, which were shown competitive in previous work [73, 24]. For semi-supervised learning (Ta-ble 3), CVRL surpasses all other baselines especially when there is only 1% labeled data, indicating the advantage of our self-learned feature is more profound with limited la-bels. For downtream action classiﬁcation on UCF-101 [57] and HMDB-51 [41], CVRL has obvious advantages over other methods based on the vision modality and is compet-itive with state-of-the-art multimodal methods (Table 4).
Our CVRL framework beneﬁts from larger datasets and networks. We study the effect of more training data in CVRL. We design an evaluation protocol by ﬁrst pre-training models on different amounts of data with same iterations, and then comparing the performance on the same validation set. As shown in Figure 4, a clear improvement is observed by using 50% more data, demonstrating the potential of CVRL to scale to larger unlabeled datasets. We also conduct experiments with wider & deeper networks and observe consistent improvements (Table 2), demon-strating that CVRL is more effective with larger networks. 2.