Abstract
Support Images Query Image
Prediction
Ground Truth
Conventional deep learning based methods for object de-tection require a large amount of bounding box annotations for training, which is expensive to obtain such high qual-ity annotated data. Few-shot object detection, which learns to adapt to novel classes with only a few annotated exam-ples, is very challenging since the ﬁne-grained feature of novel object can be easily overlooked with only a few data available. In this work, aiming to fully exploit features of annotated novel object and capture ﬁne-grained features of query object, we propose Dense Relation Distillation with
Context-aware Aggregation (DCNet) to tackle the few-shot detection problem. Built on the meta-learning based frame-work, Dense Relation Distillation module targets at fully ex-ploiting support features, where support features and query feature are densely matched, covering all spatial locations in a feed-forward fashion. The abundant usage of the guid-ance information endows model the capability to handle common challenges such as appearance changes and oc-clusions. Moreover, to better capture scale-aware features,
Context-aware Aggregation module adaptively harnesses features from different scales for a more comprehensive fea-ture representation. Extensive experiments illustrate that our proposed approach achieves state-of-the-art results on
PASCAL VOC and MS COCO datasets. Code will be made available at https://github.com/hzhupku/DCNet. 1.

Introduction
With the success of deep convolutional neural works, object detection has made great progress these years [20, 23, 8]. The success of deep CNNs, however, heavily re-lies on large-scale datasets such as ImageNet [2] that en-able the training of deep models. When the labeled data becomes scarce, CNNs can severely overﬁt and fail to gen-eralize. While in contrast, human beings have exhibited
* Corresponding authors.
” t a c
“
” w o c
“
” r a c
“
” s u b
“
Figure 1. Two challenges for few-shot object detection. a) Ap-pearance changes between support and query images are com-mon, which results in a misleading manner. b) Occlusion prob-lem brings about incomplete feature representation, causing false classiﬁcation and missing detection. strong performance in learning a new concept with only a few examples available. Since some object categories natu-rally have scarce examples or bounding box annotations are laborsome to obtain such as medical data. These problems have triggered increasing attentions to deal with learning models with limited examples. Few-shot learning aims to train models to generalize well with a few examples pro-vided. However, most existing few-shot learning works fo-cus on image classiﬁcation [29, 26, 27] problem and only a few focus on few-shot object detection problem. Since object detection not only requires class prediction, but also demands localization of the object, making it much more difﬁcult than few-shot classiﬁcation task.
Prior studies in few-shot object detection mainly consist of two groups. Most of them [13, 35, 34] adopt a meta-learning [5] based framework to perform feature reweight-ing for a class-speciﬁc prediction. While Wang et al.
[31] adopt a two-stage ﬁne-tuning approach with only ﬁne-tuning the last layer of detectors and achieve state-of-the-art 10185
performance. Wu et al. [33] also use similar strategy and focus on the scale variation problem in few-shot detection.
However, aforementioned methods often suffer from several drawbacks due to the challenging nature of few-shot object detection. Firstly, relations between support fea-tures and query feature are hardly fully explored in previ-ous few-shot detection works, where global pooling opera-tion on support features is mostly adopted to modulate the query branch, which is prone to loss of detailed local con-text. Speciﬁcally, appearance changes and occlusions are common for objects, as shown Fig. 1. Without enough dis-criminative information provided, the model is obstructed from learning critical features for class and bounding box predictions. Secondly, although scale variation problem has been widely studied in prior works [17, 15, 33], it remains a serious obstacle in few-shot detection tasks. Under few-shot settings, feature extractor with scale-aware modiﬁca-tions is inclined to overﬁtting, leading to a deteriorated per-formance for both base and novel classes.
In order to alleviate the above issues, we ﬁrst propose the dense relation distillation module to fully exploit sup-port set. Given a query image and a few support images from novel classes, the shared feature learner extracts query feature and support features for subsequent matching pro-cedure.
Intuitively, the criteria that determines whether query object and support object belong to the same category mainly measures how much feature similarity they share in common. When appearance changes or occlusions occur, local detailed features are dominant for matching candi-date objects and template ones. Hence, instead of obtaining global representations of support set, we propose a dense re-lation distillation mechanism where query and support fea-tures are matched in a pixel-wise level. Speciﬁcally, key and value maps are produced from features, which serve as encoding visual semantics for matching and containing detailed appearance information for decoding respectively.
With local information of support set effectively retrieved for guidance, the performance can be signiﬁcantly boosted, especially in extremely low-shot scenarios.
Furthermore, for the purpose of mitigating the scale vari-ation problem, we design the context-aware feature aggre-gation module to capture essential cues for different scales during RoI pooling. Since directly modifying feature ex-tractor could result in overﬁtting, we choose to perform ad-justment from a more ﬂexible perspective. Recognition of objects with different scales requires different levels of con-textual information, while the ﬁxed pooling resolution may bring about loss of substantial context information. Hence, an adaptive aggregation mechanism that allocates speciﬁc attention to local and global features simultaneously could help preserve contextual information for different scales of objects. Therefore, instead of performing RoI pooling with one ﬁxed resolution, we choose three different pooling reso-lutions to capture richer context features. Then an attention mechanism is introduced to adaptively aggregate output fea-tures to present a more comprehensive representation.
The contributions of this paper can be summarized as follows: 1. We propose a dense relation distillation module for few-shot detection problem, which targets at fully ex-ploiting support information to assist the detection pro-cess for objects from novel classes. 2. We propose an adaptive context-aware feature aggre-gation module to better capture global and local fea-tures to alleviate scale variation problem, boosting the performance of few-shot detection. 3. Extensive experiments illustrate that our approach has achieved a consistent improvement on PASCAL VOC and MS COCO datasets. Specially, our approach achieves better performance than the state-of-the-art methods on the two datasets. 2.