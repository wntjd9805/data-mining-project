Abstract
Novel view synthesis is a challenging and ill-posed in-verse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering ap-proaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions.
We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit sur-face and a radiance ﬁeld of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline ac-celerates SOTA neural volume rendering by about two or-ders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface rep-resentations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achiev-ing unprecedented image quality compared to other surface methods. We assess the quality of our approach using exist-ing datasets as well as high-quality 3D face data captured with a custom multi-camera rig. 1.

Introduction
Novel view synthesis and 3D shape estimation from 2D images are inverse problems of fundamental importance in applications as diverse as photogrammetry, remote sensing, visualization, AR/VR, teleconferencing, visual effects, and games. While traditional 3D computer vision pipelines have been studied for decades, only emerging neural rendering techniques have been able to achieve photorealistic quality for novel view synthesis (e.g., [38, 56]).
State-of-the-art neural rendering approaches, such as neural radiance ﬁelds [38], however, do not offer real-time framerates, which severely limits their applicability to the aforementioned problems. This limitation is primarily im-posed by the choice of implicit neural scene representation and rendering algorithm, namely a volumetric representa-tion that requires a custom neural volume renderer. Neural
Figure 1. Overview of our framework. Given a set of multi-view images, we optimize representation networks modeling shape and appearance of a scene end to end using a differentiable sphere tracer. The resulting models can be exported to enable view-dependent real-time rendering using traditional graphics pipelines. surface representations, for example using signed distance functions (SDFs) [41, 18, 2, 60], occupancy ﬁelds [35], or feature-based representations [54], on the other hand im-plicitly model the surface of objects, which can be extracted using the marching cubes algorithm [32] and exported into traditional mesh-based representations for real-time render-ing. Although implicit neural surface representations have recently demonstrated impressive performance on shape re-construction [60], their performance on view interpolation and synthesis tasks is limited. Thus, SOTA neural render-ing approaches either perform well for view synthesis [38] or 3D shape estimation [60], but not both.
Here, we adopt an SDF-based sinusoidal representation 4287
network (SIREN) as the backbone of our neural rendering system. While these representations have recently demon-strated impressive performance on representing shapes via direct 3D supervision with point clouds [52], we are the ﬁrst to demonstrate how to leverage SIREN’s extreme capacity in the context of learning 3D shapes using 2D supervision with images via neural rendering. For this purpose, we devise a novel loss function that maintains SIREN’s high-capacity encoding for the supervised images while constraining it in the angular domain to prevent overﬁtting on these views.
This training procedure allows us to robustly ﬁt a SIREN-based SDF directly to a sparse set of multi-view images.
Our 2D-supervised implicit neural scene representation and rendering approach performs on par with NeRF on view in-terpolation tasks while providing a high-quality 3D surface that can be directly exported for real-time rendering at test time.
Speciﬁcally, we make the following contributions:
• We develop a neural rendering framework compris-ing an implicit neural 3D scene representation, a neu-ral renderer, and a custom loss function for training.
This approach achieves 10⇥ higher rendering rates than NeRF while providing comparable, SOTA image quality with the additional beneﬁt of optimizing an im-plicitly deﬁned surface.
• We demonstrate how both shape and view-dependent appearance of our neural scene representation can be exported and rendered in real time using traditional graphics pipelines.
• We also build a custom camera array and capture sev-eral datasets of faces and heads for evaluating our ap-proach and baselines. These data are available on the project website.1 2.