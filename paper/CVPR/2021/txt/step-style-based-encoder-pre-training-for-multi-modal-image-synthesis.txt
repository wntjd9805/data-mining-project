Abstract
We propose a novel approach for multi-modal Image-to-image (I2I) translation. To tackle the one-to-many rela-tionship between input and output domains, previous works use complex training objectives to learn a latent embedding, jointly with the generator, that models the variability of the output domain. In contrast, we directly model the style vari-ability of images, independent of the image synthesis task.
Speciﬁcally, we pre-train a generic style encoder using a novel proxy task to learn an embedding of images, from ar-bitrary domains, into a low-dimensional style latent space.
The learned latent space introduces several advantages over previous traditional approaches to multi-modal I2I trans-lation. First, it is not dependent on the target dataset, and generalizes well across multiple domains. Second, it learns a more powerful and expressive latent space, which improves the ﬁdelity of style capture and transfer. The proposed style pre-training also simpliﬁes the training objective and speeds up the training signiﬁcantly. Furthermore, we provide a detailed study of the contribution of different loss terms to the task of multi-modal I2I translation, and propose a simple alternative to VAEs to enable sampling from unconstrained latent spaces. Finally, we achieve state-of-the-art results on six challenging benchmarks with a simple training objective that includes only a GAN loss and a reconstruction loss. 1.

Introduction
Image-to-Image (I2I) translation is the task of transform-ing images from one domain to another (e.g., semantic maps
→ scenes, sketches → photo-realistic images, etc.). Many problems in computer vision and graphics can be cast as I2I translation, such as photo-realistic image synthesis [1–3], super-resolution [4], colorization [5, 6], and inpainting [7].
Therefore, I2I translation has recently received signiﬁcant attention in the literature. One main challenge in I2I trans-lation is the multi-modal nature for many such tasks – the relation between an input domain A and an output domain
B is often times one-to-many, where a single input image
I A i ∈ A can be mapped to different output images from domain B. For example, a sketch of a shoe or a handbag can be mapped to corresponding objects with different colors or styles, or a semantic map of a scene can be mapped to many scenes with different appearance, lighting and/or weather conditions. Since I2I translation networks typically learn one-to-one mappings due to their deterministic nature, an extra input is required to specify an output mode to which an input image will be translated. Simply injecting extra random noise as input proved to be ineffective as shown in [2, 8], where the generator network just learns to ignore the extra noise and collapses to a single or few modes (which is one form of the mode collapse problem). To overcome this problem, Zhu et al. [8] proposed BicycleGAN, which trains an encoder network E, jointly with the I2I translation network, to encode the distribution of different possible out-puts into a latent vector z, and then learns a deterministic mapping G : (A, z) → B. So, depending on the latent vector z, a single input I A i ∈ A can be mapped to multiple outputs in B. While BicycleGAN requires paired training data, several works , like MUNIT [9] and DRIT [10], ex-tended it to the unsupervised case, where images in domains
A and B are not in correspondence (‘unpaired’). One main component of unpaired I2I is a cross-cycle consistency con-straint, where the network generates an intermediate output by swapping the styles of a pair of images, then swaps the style between the intermediate output again to reconstruct the original images. This enforces that the latent vector z preserves the encoded style information when translated from an image i to another image j and back to image i again. This constraint can also be applied to paired training data, where it encourages style/attribute transfer between im-ages. However, training BicycleGAN [8] or its unsupervised counterparts [9, 10] is not trivial. For example, BicycleGAN combines the objectives of both conditional Variational Auto-Encoders (cVAEs) [11] and a conditional version of Latent
Regressor GANs (cLR-GANs) [12, 13] to train their net-work. The training objective of [9, 10] is even more involved to handle the unsupervised setup.
In this work, we propose a novel weakly-supervised pre-training strategy to learn an expressive latent space for the task of multi-modal I2I translation. While end-to-end train-ing of the encoder network E with the I2I translation network poses a convenience, we show that it can be advantageous to break down the training into proxy tasks. In speciﬁc, we 3712
show both quantitatively and qualitatively that the proposed pre-training yields the following advantages:
• It learns a more powerful and expressive latent space.
Speciﬁcally, we show that: (1) Our pre-trained latent space captures uncommon styles that are not well represented in the training set, while baselines like BicycleGAN [8] and MUNIT [9] fail to do so and instead tend to simplify such styles/appearances to the nearest common style in the train set. (2) Pre-training yields more faithful style capture and transfer. (3) Finally, the better expressiveness of the pre-trained latent space leads to more complex style interpolations compared to the baselines.
• The learned style embedding is not dependent on the target dataset and generalizes well across many domains, which can be useful especially when having limited training data.
• Style pre-training simpliﬁes the training objective by re-quiring fewer losses, which also speeds up the training.
• Our approach improves the training stability and the over-all output quality and diversity.
We note that our proposed style pre-training is weakly-supervised and doesn’t require any manual labeling.
In-stead, it relies on a pre-trained VGG network [14] to provide training supervision. Our approach is inspired by and ex-tends the work of Meshry et al. [15] which utilizes a staged training strategy to re-render scenes under different light-ing, time of day, and weather conditions. Our work is also inspired by the standard training paradigm in visual recog-nition of ﬁrst pre-training on a proxy task, either large su-pervised datasets (e.g., ImageNet) [16–18] or unsupervised tasks (e.g., [19, 20]), and then ﬁnetuning (transfer learning) on the desired task. Similarly, we propose to pre-train the encoder using a proxy task that encourages capturing style into a latent space. Our goal is to highlight the beneﬁts of encoder pre-training and demonstrate its effectiveness for multi-modal image synthesis. In particular, we make the following contributions:
• We propose to pre-train an encoder to learn a low-dimensional projection of Gram matrices (from Neural
Style Transfer) and show that the pre-trained embedding is effective for multi-modal I2I translation, and that it simpliﬁes and stabilizes the training.
• We show that the pre-trained latent embedding is not de-pendent on the target domain and generalizes well to other domains (transfer learning).
• We provide a study of the importance of different loss terms for multi-modal I2I translation network.
• We propose an alternative to enable sampling from a latent space instead of enforcing a prior as done in VAE training.
• We achieve state-of-the art results on six benchmarks in terms of style capture and transfer, and diversity of results. 2.