Abstract
Vector graphics are widely used to represent fonts, lo-gos, digital artworks, and graphic designs. But, while a vast body of work has focused on generative algorithms for raster images, only a handful of options exists for vec-tor graphics. One can always rasterize the input graphic and resort to image-based generative approaches, but this negates the advantages of the vector representation. The current alternative is to use specialized models that require explicit supervision on the vector graphics representation at training time. This is not ideal because large-scale high-quality vector-graphics datasets are difﬁcult to obtain. Fur-thermore, the vector representation for a given design is not unique, so models that supervise on the vector repre-sentation are unnecessarily constrained. Instead, we pro-pose a new neural network that can generate complex vec-tor graphics with varying topologies, and only requires in-direct supervision from readily-available raster training im-ages (i.e., with no vector counterparts). To enable this, we use a differentiable rasterization pipeline that renders the generated vector shapes and composites them together onto a raster canvas. We demonstrate our method on a range of datasets, and provide comparison with state-of-the-art
SVG-VAE and DeepSVG, both of which require explicit vec-tor graphics supervision. Finally, we also demonstrate our approach on the MNIST dataset, for which no groundtruth vector representation is available. Source code, datasets and more results are available at http://geometry. cs.ucl.ac.uk/projects/2021/Im2Vec/. 1.

Introduction
In vector graphics, images are represented as collections of parametrised shape primitives rather than a regular raster of pixel values. This makes for a compact, inﬁnitely scal-able representation with appearance that may be varied at need simply by modifying stroke or colour parameters. As a result, it is favoured by graphic artists and designers.
Unfortunately, creating vector graphics still remains a difﬁcult task largely limited to manual expert workﬂows,
Figure 1: We present Im2Vec that can be trained with only image supervision to produce a latent space for vector graphics output. The learned space supports reprojection, sampling (i.e., generation), and interpolation. because the same irregular structure makes it ill-suited for today’s convolution-based generative neural architectures.
There is demand for a generative approach suitable for this domain, but it is not yet well served by research because of the difﬁcult design requirements. Suitable approaches (i) produce output in vector format; (ii) estab-should: lish correspondence across elements of the same family; (iii) support reconstruction, sampling, and interpolation; (iv) give user control over accuracy versus compactness of the representation; and ﬁnally, (v) be trainable directly us-ing images without the need for vector supervision.
SVG-VAE [24] and DeepSVG [5], the two leading gen-erative algorithms for vector graphics, cast synthesis as a se-quence prediction problem, where the graphic is a sequence of drawing instructions, mimicking how common formats actually represent vector art. Training these methods there-fore requires supervision from ground truth vector graphics 17342
sequences, which are difﬁcult to collect in large volumes.
Furthermore, the mapping from sequences of parametrised drawing instruction to actual images is highly non-linear with respect to the parameters and also non-injective, al-lowing a variety of different sequences to produce the same visual result. This makes it difﬁcult to consider appearance as a criterion, and also causes the produced results to inherit any structural bias baked into the training sequences.
An approach aiming to do away with such vector su-pervision would need to overcome a number of challenges.
First, the relationship between the representation and its ap-pearance must be made explicit and differentiable. Sec-ond, it must operate on an internal representation that di-rectly maps to a vector graphics representation and is ﬂexi-ble enough to support a large range of topologies and shape complexities. Finally, it should extract correspondences be-tween related shapes, directly from unlabelled images.
In this paper, we propose such a method, called Im2Vec, based on a representation that mimics the compositing be-haviour of complex vector graphics.
It uses a variable-complexity closed B´ezier path as the fundamental primi-tive, with the capability to composite a variable number of these to create shapes of arbitrary complexity and topology (shown in Figure 2).
I
L1, d1
L2, d2
L3, d3 . . . LT , dT
Figure 2: Im2Vec encodes a shape as a layered set of ﬁlled curves (or shapes). Each shape is obtained by deformation of a topological disk, differentiably rasterized into images
Li, then differentiably composited back-to-front according to scalar depth variables di.
The key insight that allows the handling of arbitrary complexity is that we can treat any primitive closed shape as a deformation of a unit circle, which is modelled as 1D con-volution on samples from this circle conditioned on a com-mon latent vector. By recombining these primitive paths through a differentiable rasterizer [22] and differentiable compositing [28], we can natively represent vector art while learning to generate it purely based on appearance, obviat-ing the need for vector supervision.
We evaluate Im2Vec on a variety of examples with vary-ing complexity and topology including fonts, emojis, and icons. We demonstrate that Im2Vec, even without any vec-tor supervision, consistently performs better reconstruction compared to SVG-VAE and DeepSVG when trained on the same dataset. We also compare our approach to a purely raster-based autoencoder, which we dub ImageVAE. While
ImageVAE and Im2Vec produce comparable reconstruction quality, Im2Vec outputs vector graphics and hence enjoys the associated editability and compactness beneﬁts. Finally, we quantify the compactness versus approximation power of our method, and demonstrate Im2Vec can be used to vec-torize the MNIST dataset for which no groundtruth vector representation is available. 2.