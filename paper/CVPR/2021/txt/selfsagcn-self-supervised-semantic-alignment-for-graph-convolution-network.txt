Abstract
Graph convolution networks (GCNs) are a powerful deep learning approach and have been successfully applied to representation learning on graphs in a variety of real-world applications. Despite their success, two fundamental weaknesses of GCNs limit their ability to represent graph-structured data: poor performance when labeled data are severely scarce and indistinguishable features when more layers are stacked. In this paper, we propose a simple yet effective Self-Supervised Semantic Alignment Graph Con-volution Network (SelfSAGCN), which consists of two crux techniques: Identity Aggregation and Semantic Alignmen-t, to overcome these weaknesses. The behind basic idea is the node features in the same class but learned from seman-tic and graph structural aspects respectively, are expected to be mapped nearby. Speciﬁcally, the Identity Aggrega-tion is applied to extract semantic features from labeled n-odes, the Semantic Alignment is utilized to align node fea-tures obtained from different aspects using the class central similarity. In this way, the over-smoothing phenomenon is alleviated, while the similarities between the unlabeled fea-tures and labeled ones from the same class are enhanced.
Experimental results on ﬁve popular datasets show that the proposed SelfSAGCN outperforms state-of-the-art methods on various classiﬁcation tasks. 1.

Introduction
Graphs, representing entities and their relationships, have been successfully applied to a wide range of appli-cations [36, 4], such as social networks [26], knowledge graphs [34], and molecular structures [8]. In recent years, many studies focus on developing deep learning approaches for graph structure data [38], leading to rapid development in the ﬁeld of graph convolution networks (GCNs) [13, 7].
∗Corresponding author.
GCNs, generalizing deep convolutional neural network (C-NN) [30] to graph-structured data, apply the linear transfor-mation and graph aggregation to all the neighbors of a n-ode and adopt a nonlinear activation function to obtain low-dimensional features for graph nodes.
Normally, GCNs can be categorized into spatial and spectral convolution methods [39, 12]. The operation on node neighbor groups is utilized to deﬁne the graph convo-lution layer in the spatial methods. Diffusion-Convolutional
Neural Network (DCNN) [1] adopts a graph diffusion mod-ule to incorporate the contextual information of the node.
MoNet [24] integrates CNN and provides a uniﬁed gen-eralization of CNN architectures on graphs. Moreover,
Graph Attention Network (GAT) [29] is utilized to semi-supervised classiﬁcation task by designing an attention lay-er, which can learn the weight of each neighbor for feature aggregation. For spectral methods, the graph convolution operation is deﬁned by the spectral representation of graphs.
The Fourier domain based on eigen-decomposition of graph
Laplacian matrix [2] is proposed to deﬁne graph convolu-tion, and the spectral ﬁlters based on Chebyshev expansion of the Laplacian are provided to avoid the high computa-tional complexity of eigen-decomposition [6]. More recent-ly, Kipf et al. [13] proposed a simple Graph Convolution-al Network (GCN) for semi-supervised learning. Simplify-ing Graph Convolutional (SGC) [31] is proposed to capture higher-order information in the graph by applying the K-th power of the graph convolution matrix in a single neural network layer.
Despite their signiﬁcant success, the performance of cur-rent GCNs drops sharply with a diminishing number of la-beled nodes per class.
It is easy to cause the over-ﬁtting problem with severely scarce labeled nodes, which exhibits a large testing error even though its training error is small.
The reason for this phenomenon is the GCNs rely on graph structure to realize feature propagation, whereas it cannot effectively propagate the labels to the entire graph when 16775
only a few labeled nodes are provided. Moreover, most of the recent GCNs are shallow and achieve their best perfor-mance with 2-layer models. Stacking more layers with non-linearity activations tends to degrade their performances, which is called over-smoothing [17]. Such a phenomenon suggests that the receptive ﬁelds are extremely large with stacking more layers and thus the node features are inclined to converge to a certain value [20].
The propagation based on the graph structure can trans-fer the labeled node features to the unlabeled ones in the same class, making the node features in the same class sim-ilar. However, the graph propagation of GCNs is not e-nough [35], and the phenomenons of over-ﬁtting and over-smoothing restrict their performance in many cases. Re-cently, some methods are taking an interest in the semantic information of the nodes. DAGNN [20] is proposed to adap-tively incorporate semantic information from large recep-tive ﬁelds. GCNII [3] adopts identity mapping to preserve the inputs information directly. Geom-GCN [25] and non-local GNNs [21] are proposed to capture long-range depen-dencies from the node features according to non-local ag-gregators. But, few works discuss the relationship between semantic information and graph structure, and solve these problems effectively. In this paper, we present a simple yet effective Self-Supervised Semantic Alignment Graph Con-volution Network (SelfSAGCN), which integrates seman-tic extraction and alignment into traditional GCNs, to si-multaneously overcome the over-ﬁtting and over-smoothing problems. The behind basic idea is the node features in the same class but from semantic and graph structural aspect-s respectively, are expected to be mapped nearby. Mean-while, unlabeled node features should be central similar to the labeled ones.
The Self-Supervised Semantic Alignment Graph Con-volution Network consists of two crux techniques: Identi-ty Aggregation and Semantic Alignment. Speciﬁcally, we
ﬁrst apply the Identity Aggregation to extract semantic in-formation layer-by-layer from the labeled nodes, which is not plagued by the over-smoothing problem. Moreover, the Semantic Alignment transfers the learned semantics to unlabeled node features obtained from the graph aggrega-tion operations using the class central similarity optimiza-tion. In this way, the node features from semantic and graph structural aspects are mapped nearby, which has a dramat-ic effect on alleviating the over-smoothing. Particularly, we construct the class centroids of unlabeled nodes by assign-ing pseudo-labels in the similarity optimization, and update them gradually to suppress noise signals. The alignment of the labeled nodes and unlabeled ones can further enhance the performance of the model when the labeled nodes are severely scarce. Our experiments show that the SelfSAGC-N model outperforms state-of-the-art methods on various classiﬁcation tasks.
The main contributions of this paper are in three-folds:
• We propose a simple yet effective method called Self-Supervised Semantic Alignment Graph Convolution
Network (SelfSAGCN), which consists of Identity Ag-gregation and Semantic Alignment techniques in a synergetic fashion, to jointly mitigate the over-ﬁtting and over-smoothing problems.
• To improve the discriminative power of node features and boost the classiﬁcation performance, we explore the Identity Aggregation to extract discriminative se-mantic features from the labeled nodes, which has the consistent receptive ﬁelds in different layers. Then the unlabeled node features obtained from the graph ag-gregation operations are aligned with the semantic fea-tures by the Semantic Alignment technique for seeking extra supervised information.
• We evaluate the proposed SelfSAGCN on ﬁve popu-lar benchmarks, including some standard citation net-works and image datasets, and the experiments show that the proposed model outperforms the state-of-the-art methods on various classiﬁcation tasks. 2.