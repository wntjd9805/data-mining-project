Abstract
Aligning partial views of a scene into a single whole is essential to understanding one’s environment and is a key component of numerous robotics tasks such as SLAM and
SfM. Recent approaches have proposed end-to-end systems that can outperform traditional methods by leveraging pose supervision. However, with the rising prevalence of cam-eras with depth sensors, we can expect a new stream of raw RGB-D data without the annotations needed for su-pervision. We propose UnsupervisedR&R: an end-to-end unsupervised approach to learning point cloud registration from raw RGB-D video. The key idea is to leverage dif-ferentiable alignment and rendering to enforce photomet-ric and geometric consistency between frames. We evaluate our approach on indoor scene datasets and ﬁnd that we out-perform existing traditional approaches with classical and learned descriptors while being competitive with supervised geometric point cloud registration approaches. 1.

Introduction
Consider the two scenes depicted in Fig 1. How are they related? What is the layout of the room they depict? Align-ing partial views of a scene into a single whole is essential to understanding one’s environment and is a key component of numerous robotics tasks such as SLAM and SfM. Recent approaches have leveraged supervised learning to develop end-to-end systems that outperform traditional methods in both accuracy and speed [9, 22]. However, with the rising prevalence of cameras with depth sensors, we can expect a new stream of raw RGB-D data without the annotations needed for supervision. How can we leverage this data for unsupervised learning of point cloud registration?
The common approach to point cloud registration relies on correspondence extraction and geometric model ﬁtting.
Traditional approaches rely on hand-crafted features [30, 38] and robust estimators such as RANSAC [20]. While those approaches work well, their performance is limited
Figure 1. What 3D scene do the two views on the left portray?
Given 2 RGB-D images, we train a model to estimate the camera motion between them through enforcing photometric and geomet-ric consistency losses on point cloud renderings of the scene. by their inability to ﬂexibly adapt to different data distribu-tions. Recent work leverages supervised learning to address those limitations by learning to extract feature descrip-tors [10, 16, 71], ﬁnding better correspondences [9, 22, 52], and training more efﬁcient robust estimators [6, 7, 48].
However, accurate pose annotation can be challenging to attain automatically, due to sensor error or reliance on tradi-tional SfM pipelines with no convergence guarantees [53].
Meanwhile, self-supervised visual learning has made re-markable progress in learning semantic [15, 17, 21, 25, 60] and 3D [29, 35, 62, 64, 81] features. The key idea is to use natural transformations in the data as indirect supervision.
RGB-D video provides us with this supervision since suc-cessive frames capture different views of the same scene. In this case, aligning two point clouds from nearby frames is not only about achieving good geometric consistency, but also showing good photometric consistency between the two views. By achieving both photometric and geometric consistency, we can train our model using RGB-D image pairs without relying on additional supervision.
We propose using view synthesis between RGB-D im-7129
ages as a task for learning point cloud registration. Given two RGB-D video frames, we extract features from each frame to generate a feature point cloud, where each point is represented by both a 3D coordinate and a feature vector.
The extracted features serve as descriptors for correspon-dence estimation. The model is trained end-to-end using photometric and geometric consistency losses between the input and rendered frames. Through using differentiable components, we back-propagate the losses to the feature encoder to learn features that allow us to estimate unique correspondences and accurately register the two views.
We evaluate our model on ScanNet [12]; a large indoor scene dataset. We ﬁnd that our model outperforms the tradi-tional registration pipeline with visual or geometric descrip-tors (§ 4.1). Furthermore, it performs on par with super-vised geometric registration approaches despite being unsu-pervised; supporting our claim that RGB-D self-supervision can alleviate the need for pose annotation. Finally, we ana-lyze our model through several ablations (§ 4.2).
In summary, our contributions are as follows:
• We propose an unsupervised approach to point cloud reg-istration via differentiable alignment and rendering;
• We show how a differentiable variant of Lowe’s ratio test is sufﬁcient for correspondence matching;
• We empirically demonstrate our approach’s efﬁcacy against traditional & supervised registration approaches;
• We validate our design choices by evaluating our model with several ablations. 2.