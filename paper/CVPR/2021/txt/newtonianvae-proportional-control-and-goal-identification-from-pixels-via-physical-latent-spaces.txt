Abstract
Learning low-dimensional latent state space dynam-ics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simpliﬁes and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal dis-covery when applied to imitation learning of switching controllers from demonstration. Notably, such propor-tional controlability also allows for robust path following from visual demonstrations using Dynamic Movement
Primitives in the learned latent space. 1.

Introduction
Vision-based control is highly desirable across numer-ous industrial applications, both in robotics and process control. At present, much practical vision-based control relies on supervised learning to build bespoke percep-tion modules, prior to downstream dynamics modelling and controller design. This can be expensive and time consuming, and as a result there is growing interest in developing model-based approaches for direct vision-based control.
Model-based approaches for visual control tend to learn latent dynamics models that are subsequently used within suitable planning or model predictive con-trol (MPC) frameworks, or to train policies for later use.
We argue that this decoupling of dynamics and control is computationally expensive and often unnecessary. In-stead we learn a structured latent dynamical model that directly allows for simple proportional control to be ap-plied. Proportional-Integral-Derivative (PID) feedback control produces commands that are proportional to an error or cost term between current system state x and a (potentially dynamic) target state xgoal: ut = Kp (xgoal t − xt) + Ki (xgoal t′ − xt′ ) +
Xt′
Kd xt − xt−1
∆t (1)
Gain terms (Kp, Ki, Kd) shape the controller response to errors. PID control is ubiquitous in industry, and broadly applicable across numerous domains, providing a simple and reliable oﬀ-the-shelf mechanism for stabil-ising systems. PID control is also the basis of a wide range of more powerful control strategies, including the more ﬂexible dynamic movement primitives [24, 43] that augment PD control laws with a forcing function for trajectory following. Essentially we learn the state encoding x(I) from images I for which robots can be trivially controlled from pixels according to Eq 1.
We structure latent dynamics so that that PID con-trol can be applied to move between latent states, to remove the requirement for complex planning or rein-forcement learning strategies. Moreover, we show that imitation learning from demonstrations becomes a sim-ple goal inference problem under a proportional control model in this latent space, and can even be extended to sequential tasks comprising multiple sub-goals.
Imitation learning from high dimensional visual data is particularly challenging [2]. Behaviour cloning, which seeks to reproduce demonstrations, is particularly vul-nerable to generalisation failures for high dimensional visual inputs, while inverse reinforcement learning (IRL)
[38] strategies are hard to train and extremely sample ineﬃcient. By learning a structured dynamics model, we allow for more robust control in the presence of noise and simplify the inverse reward inference process. In summary, the primary contributions of this work are:
Embedding for proportional controllability We induce a latent space where taking an action in the direction between the current position and some target 4454
position, u ∝ xtarget − x, moves the system towards the target position. Uniquely, this enables simple pro-portional control from pixels.
Imitation learning using latent switching pro-portional control laws We leverage the properties of this embedding to frame imitation learning as a goal inference problem under a switching proportional control law model in the structured latent space for se-quential goal reaching problems. This enables one-shot interpretable imitation learning of switching controllers from high-dimensional pixel observations.
Imitation learning using dynamic movement primitives (DMPs) We also leverage the properties of our embedding to ﬁt dynamic movement primitives in the structured latent space for trajectory tracking problems. This enables one-shot imitation learning of trajectory following controllers from pixels.
Results show that embedding for proportional con-trollability produces more interpretable latent spaces, allows for the use of simple and eﬃcient controllers that cannot be applied with less structured latent dynamical models, and enables one-shot learning of control and interpretable goal identiﬁcation in sequential multi-task imitation learning settings. 2.