Abstract
Artiﬁcial neural networks perform state-of-the-art in an ever-growing number of tasks, and nowadays they are used to solve an incredibly large variety of tasks. There are prob-lems, like the presence of biases in the training data, which question the generalization capability of these models. In this work we propose EnD, a regularization strategy whose aim is to prevent deep models from learning unwanted bi-ases. In particular, we insert an “information bottleneck” at a certain point of the deep neural network, where we dis-entangle the information about the bias, still letting the use-ful information for the training task forward-propagating in the rest of the model. One big advantage of EnD is that it does not require additional training complexity (like de-coders or extra layers in the model), since it is a regular-izer directly applied on the trained model. Our experiments show that EnD effectively improves the generalization on unbiased test sets, and it can be effectively applied on real-case scenarios, like removing hidden biases in the COVID-19 detection from radiographic images. 1.

Introduction
In the last two decades artiﬁcial neural network models (ANNs) received huge interest from the research commu-nity. Nowadays, complex and even ill-posed problems can be tackled provided that one can train a deep enough ANN model with a large enough dataset. Furthermore, they aim to become a powerful tool helping us take a variety of de-cisions: for example, AI is currently used for scouting and hiring people [18]. These ANNs are trained to process a de-sired output from some inputs. We have no clear idea how the information is effectively processed inside. Recently,
AI trustworthiness has been recognized as major prereq-uisite for people and societies to use and accept such sys-tems [14, 43]. In April 2019, the High-Level Expert Group on AI of the European Commission deﬁned the three main aspects of trustworthy AI [14]: it should be lawful, ethical and robust. Providing a warranty on this topic is currently a matter of study and discussion [26, 30, 34, 38].
Focusing on the concept of robustness for AI, Atten-berg et al. discussed the problem of ﬁnding the so-called
“unknown unknowns” [3] in data. These unknown un-knowns relate to the case when the deep model elaborates information in an unintended way, but shows high conﬁ-dence on its predictions. Such behavior affected many re-cent works proposing AI-based solutions on the COVID de-tection from radiographic images. Unfortunately, the avail-able datasets at the beginning of the pandemic were heavily biased. This often resulted in models predicting COVID di-agnosis with a high conﬁdence, thanks to the presence of unwanted biases, for example by detecting the presence of catheters or medical devices for positive patients, their age (at the beginning of the pandemic, most ill patients were el-derly people), or even by recognizing the origin of the data itself (when negative cases were augmented borrowing sam-ples from other datasets) [2, 29, 31].
In this work we propose a regularization strategy which
Entangles the deep features extracted by patterns belong-ing to the same target class and Disentangles the biased fea-tures: we name it EnD, and with it we wish to put an end to the bias propagation in any deep model. We assume we know data might have some bias (like in the case of COVID, the origin of data) but we ignore what it translates into (we do not have a prior knowledge on whether the bias is the presence of some color, a speciﬁc feature in the image or anything else). EnD regularizes the output of some layer
Γ within the deep model in order to create an “information bottleneck” where the regularizer:
• entangles the feature vectors extracted from data be-longing to the same target class;
• disentangles the features extracted from data having the same “bias label”.
Since the deep model is trained minimizing both the loss and EnD, all the biased features are discouraged to be ex-tracted in favor of the unbiased ones. Compared to other de-biasing techniques, we have no training overhead: we do not train extra models to perform gradient inversion on the biased information or involve the use of GaNs, or even de-bias the input data. EnD works directly on the target model, and is minimized via standard back-propagation. 13508
Figure 1: Model overview. The features for EnD are ex-tracted at the output of Γ , after a normalization layer per-forming the operation as in (3).
In general, directly tackling the problem of mutual in-formation’s minimization is hard, given both its non-differentiability and the computational complexity in-volved. Nonetheless, previous works have already shown that adding further constraints to the learning problem could be effective [33] as, typically, the trained ANN models are over-sized and allows a large number of solutions to the same learning task [32]. Our experiments show that EnD effectively favors the choice of unbiased features over the biased ones at training time, yielding competitive general-ization capabilities compared to models trained with other un-biasing techniques.
The rest of the work is structured as follows. In Sec. 2 we review some works close to our problem. Then, in Sec. 3 we introduce EnD in detail providing intuitions on its effect.
Sec. 4 shows some empirical results and ﬁnally, in Sec. 5, the conclusions are drawn. 2.