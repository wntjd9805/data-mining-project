Abstract
The Sinkhorn divergence has become a very popu-lar metric to compare probability distributions in optimal transport. However, most works resort to the Sinkhorn di-vergence in Euclidean space, which greatly blocks their ap-plications in complex data with nonlinear structure. It is therefore of theoretical demand to empower the Sinkhorn divergence with the capability of capturing nonlinear struc-tures. We propose a theoretical and computational frame-work to bridge this gap.
In this paper, we extend the
Sinkhorn divergence in Euclidean space to the reproduc-ing kernel Hilbert space, which we term “Hilbert Sinkhorn divergence” (HSD). In particular, we can use kernel ma-trices to derive a closed form expression of the HSD that is proved to be a tractable convex optimization problem.
We also prove several attractive statistical properties of the proposed HSD, i.e., strong consistency, asymptotic behav-ior and sample complexity. Empirically, our method yields state-of-the-art performances on image classiﬁcation and topological data analysis. 1.

Introduction
As an important tool to compare probability distribu-tions, optimal transport theory [52] has found many suc-cessful applications in machine learning. Examples include generative modeling [56, 19], domain adaptation [17], dic-tionary learning [42], text mining [29], sampling [54, 55] and single-cell genomics [41]. Optimal transport aims at minimizing the cost of moving a source distribution to a target distribution. The minimal transportation cost de-ﬁnes a divergence between the two distributions, which is called the Wasserstein or Earth-Mover distance [51, 40].
Roughly speaking, the Wasserstein distance measures the
*Equal contribution
†Corresponding author minimal cost required to deform a distribution to another distribution. Different from other divergence, such as Kull-back–Leibler divergence and the L2 distance, the Wasser-stein distance could compare probability distributions in a geometrically faithful manner. This entails a rich geometric structure on the space of probability distributions.