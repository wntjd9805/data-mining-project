Abstract
Text-video retrieval is a challenging task that aims to search relevant video contents based on natural language descriptions. The key to this problem is to measure text-video similarities in a joint embedding space. However, most existing methods only consider the global cross-modal similarity and overlook the local details. Some works in-corporate the local comparisons through cross-modal local matching and reasoning. These complex operations intro-duce tremendous computation. In this paper, we design an efﬁcient global-local alignment method. The multi-modal video sequences and text features are adaptively aggregated with a set of shared semantic centers. The local cross-modal similarities are computed between the video feature and text feature within the same center. This design enables the meticulous local comparison and reduces the computa-tional cost of the interaction between each text-video pair.
Moreover, a global alignment method is proposed to pro-vide a global cross-modal measurement that is complemen-tary to the local perspective. The global aggregated visual features also provide additional supervision, which is indis-pensable to the optimization of the learnable semantic cen-ters. We achieve consistent improvements on three standard text-video retrieval benchmarks and outperform the state-of-the-art by a clear margin. 1.

Introduction
Video is one of the most informative media due to the abundant multi-modal content and temporal dynam-ics. Text-video retrieval systems enable humans to search videos with a simple and natural interaction approach. Re-cently, some efforts have been made in building retrieval systems with complex text inputs [2, 9], e.g., retrieving con-tents of “a group of men inspect and test a brand new yellow car”. This is more applicable as the users could search con-tent based on more detailed descriptions.
One of the promising directions to enable cross-modal
*Work done during an internship at Baidu Research.
Global Alignment a group …   yellow car
Local Alignment a  group  of  men  inspect  and  test  a  brand  new  yellow  car
Figure 1. Global alignment gives a comprehensive similarity mea-surement between texts and videos. Local alignment provides ﬁne-grained comparisons by computing the similarities between the lo-cal text-video features from the same semantic centers. video retrieval is to measure text-video similarities using metric learning [34, 7].
In this case, the common prac-tice is to embed both descriptions and videos into a joint embedding space. Most existing works [24] [6] [22] [9] encode the descriptions and video content to global repre-sentations and compare their similarities from a global per-spective. These methods focus on the learning of effective language and video representations but overlook the ﬁne-grained semantic alignment. For instance, Gabeur et al. [9] leveraged a multi-modal transformer to enhance the valu-able cross-modal interaction to generate more discrimina-tive video features. Some other works [2, 21, 32] lever-aged complex cross-modal matching operations to exploit the local details and align multiple semantic cues. Chen et al. [2] proposed a hierarchical graph reasoning model to capture both global events and local actions through local graph matching. They manually designed three levels of se-mantics, including events, actions, and entities. However, these methods require a high computational cost due to the expensive pairwise matching operation. 5079
In this paper, we propose an efﬁcient global-local se-quence alignment method for text-video retrieval.
In the local perspective, we aim to utilize a number of learn-able semantic topics to jointly summarize both texts and videos. Instead of parsing text descriptions to a hierarchi-cal semantic role graph [2], it is hoped that these semantic topics could be discovered and automatically learned dur-ing the end-to-end training. We further share the weights of text topics and video topics to offer a joint topic repre-sentation learning and to reduce the semantic gap between text and video data. To achieve local alignment, we mini-mize the distance between the grouped text feature and the corresponding grouped video features within the same top-ics. In the global perspective, the multi-modal video se-quences are aggregated temporally within each modality.
The global similarity is computed between the aggregated video features and global text features. The global align-ment not only serves as a complementary measurement to local alignment but also provides additional supervision for the learnable semantic topics.
We implement the idea of local semantic topic alignment with the help of a NetVLAD operation [1]. In NetVLAD, the learnable centers are regarded as “visual words” of the input data, which can be readily utilized as latent seman-tic topics on our cross-modal video retrieval task. For both text and video modalities, we use NetVLAD operations to obtain an aggregated feature for each topic, where the topic centers are shared between the two modalities. The text features and video features are softly assigned to topics based on their corresponded similarities. Without complex graph operations [2] and multi-layer transformers [9], we surprisingly ﬁnd that our collaborative encoding method, namely Text-to-Video VLAD (T2VLAD), could boost the retrieval performance on various datasets. The contribution of this paper can be summarized as below:
• First, we propose to automatically learn text-and-video semantic topics and re-emphasize the importance of local semantic alignment between texts and videos for better cross-modal retrieval.
• Second, we introduce an effective strategy to locally align text inputs and video inputs. Based on the success of NetVLAD encoding [1], we propose a
T2VLAD encoding for cross-modal retrieval, where we exploit shared centers to reduce the semantic gap between texts and videos instead of the complex pair-wise local matching operation.
• Third, we demonstrate signiﬁcant improvements of
T2VLAD on three standard text-video retrieval bench-i.e., MSRVTT [35], ActivityNet Captions marks,
[19], and LSMDC [28]. Notably, we outperform a
HowTo100M-pretrained [25] multi-modal transformer
[9] with 2.9% gain (Rank@1) on MSRVTT without any additional data. 2.