Abstract
We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adver-sarial ﬁltration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodiﬁed ex-amples transfer to various unseen models reliably, demon-strating that computer vision models have shared weak-nesses. The ﬁrst dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the ﬁrst out-of-distribution detection dataset created for
ImageNet models. On IMAGENET-A a DenseNet-121 ob-tains around 2% accuracy, an accuracy drop of approx-imately 90%, and its out-of-distribution detection perfor-mance on IMAGENET-O is near random chance levels.
We ﬁnd that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we ﬁnd that improvements to computer vision architectures provide a promising path towards robust models. 1.

Introduction
Research on the ImageNet [10] benchmark has led to numerous advances in classiﬁcation [36], object detection
[34], and segmentation [21].
ImageNet classiﬁcation improvements are broadly applicable and highly predictive
Improvements on of improvements on many tasks [35].
ImageNet classiﬁcation have been so great that some call ImageNet classiﬁers “superhuman” [23]. However, performance is decidedly subhuman when the test distri-bution does not match the training distribution [26]. The distribution seen at test-time can include inclement weather conditions and obscured objects, and it can also include objects that are anomalous.
Recht et al., 2019 [42] remind us that ImageNet test
*Equal Contribution.
Figure 1: Natural adversarial examples from IMAGENET-A and IMAGENET-O. The black text is the actual class, and the red text is a ResNet-50 prediction and its conﬁdence.
IMAGENET-A contains images that classiﬁers should be able to classify, while IMAGENET-O contains anomalies of unforeseen classes which should result in low-conﬁdence predictions.
ImageNet-1K models do not train on exam-ples from “Photosphere” nor “Verdigris” classes, so these images are anomalous. Most natural adversarial examples lead to wrong predictions despite occurring naturally. examples tend to be simple, clear, close-up images, so that the current test set may be too easy and may not represent harder images encountered in the real world. Geirhos et al., 2020 argue that image classiﬁcation datasets contain
“spurious cues” or “shortcuts” [16, 2]. For instance, models may use an image’s background to predict the foreground object’s class; a cow tends to co-occur with a green pasture, to the and even though the background is inessential object’s identity, models may predict “cow” primarily using the green pasture background cue. When datasets contain 115262
Figure 2: Various ImageNet classiﬁers of different architectures fail to generalize well to IMAGENET-A and IMAGENET-O.
Higher Accuracy and higher AUPR is better. See Section 4 for a description of the AUPR out-of-distribution detection measure. These speciﬁc models were not used in the creation of IMAGENET-A and IMAGENET-O, so our adversarially
ﬁltered image transfer across models. spurious cues, they can lead to performance estimates that are optimistic and inaccurate.
To counteract this, we curate two hard ImageNet test sets of natural adversarial examples with adversarial
ﬁltration. By using adversarial ﬁltration, we can test how well models perform when simple-to-classify examples are removed, which includes examples that are solved with simple spurious cues. Some examples are depicted in Figure 1, which are simple for humans but hard for models. Our examples demonstrate that it is possible to reliably fool many models with clean natural images, while previous attempts at exposing and measuring model fragility rely on synthetic distribution corruptions [18, 26], artistic renditions [24], and adversarial distortions.
We demonstrate that clean examples can reliably de-grade and transfer to other unseen classiﬁers using our ﬁrst dataset. We call this dataset IMAGENET-A, which contains images from a distribution unlike the ImageNet training distribution. IMAGENET-A examples belong to ImageNet classes, but the examples are harder and can cause mistakes across various models. They cause consistent classiﬁca-tion mistakes due to scene complications encountered in the long tail of scene conﬁgurations and by exploiting classiﬁer blind spots (see Section 3.2). Since examples transfer reli-ably, this dataset shows models have unappreciated shared weaknesses.
The second dataset allows us to test model uncertainty estimates when semantic factors of the data distribution shift. Our second dataset is IMAGENET-O, which contains image concepts from outside ImageNet-1K. These out-of-distribution images reliably cause models to mistake the ex-amples as high-conﬁdence in-distribution examples. To our knowledge this is the ﬁrst dataset of anomalies or out-of-distribution examples developed to test ImageNet models.
While IMAGENET-A enables us to test image classiﬁca-tion performance when the input data distribution shifts,
IMAGENET-O enables us to test out-of-distribution detec-tion performance when the label distribution shifts.
We examine methods to improve performance on adversarially ﬁltered examples. However, this is difﬁ-cult because Figure 2 shows that examples successfully transfer to unseen or black-box models.
To improve robustness, numerous techniques have been proposed.
We ﬁnd data augmentation techniques such as adversarial training decrease performance, while others can help by a few percent. We also ﬁnd that a 10× increase in training data corresponds to a less than a 10% increase improving model in accuracy. architectures is a promising avenue toward increasing robustness. Even so, current models have substantial room for improvement. Code and our two datasets are available at github.com/hendrycks/natural-adv-examples.
Finally, we show that 2.