Abstract
Recent self-supervised contrastive learning provides an effective approach for unsupervised person re-identiﬁcation (ReID) by learning invariance from different views (trans-formed versions) of an input. In this paper, we incorporate a Generative Adversarial Network (GAN) and a contrastive learning module into one joint training framework. While the GAN provides online data augmentation for contrastive learning, the contrastive module learns view-invariant fea-tures for generation. In this context, we propose a mesh-based view generator. Speciﬁcally, mesh projections serve as references towards generating novel views of a per-son. In addition, we propose a view-invariant loss to fa-cilitate contrastive learning between original and gener-ated views. Deviating from previous GAN-based unsuper-vised ReID methods involving domain adaptation, we do not rely on a labeled source dataset, which makes our method more ﬂexible. Extensive experimental results show that our method signiﬁcantly outperforms state-of-the-art methods under both, fully unsupervised and unsupervised domain adaptive settings on several large scale ReID dat-sets. Source code and models are available under https:
//github.com/chenhao2345/GCL. 1.

Introduction
A person re-identiﬁcation (ReID) system is targeted at identifying subjects across different camera views. In par-ticular, given an image containing a person of interest (as query) and a large set of images (gallery set), a ReID sys-tem ranks gallery-images based on visual similarity with the query. Towards this, ReID systems are streamlined to bring to the fore discriminative representations, which al-low for robust comparison of query and gallery images. In this context, supervised ReID methods [4, 33] learn rep-resentations guided by human-annotated labels, which is time-consuming and cumbersome. Towards omitting such human annotation, researchers increasingly place empha-sis on unsupervised person ReID algorithms [35, 24, 27], which learn directly from unlabeled images and thus allow for scalability in real world deployments.
*Equal contribution.
Figure 1: Left: Traditional self-supervised contrastive learning maximizes agreement between representations (f1 and f2) of aug-mented views from Data Augmentation (DA). Right: Joint gener-ative and contrastive learning maximizes agreement between orig-inal and generated views.
Recently, self-supervised contrastive methods [16, 6] have provided an effective retrieval-based approach for unsupervised representation learning. Given an image, such methods maximize agreement between two augmented views of one instance (see Fig. 1). Views refer to trans-formed versions of the same input. As shown in very re-cent works [6, 7], data augmentation enables a network to explore view-invariant features by providing augmented views of a person, which are instrumental in building ro-bust representations. Such and similar methods considered traditional data augmentation techniques, e.g., ‘random ﬂip-ping’, ‘cropping’, and ‘color jittering’. Generative Adver-sarial Networks (GANs) [15] constitute a novel approach for data augmentation. As opposed to traditional data aug-mentation, GANs are able to modify id-unrelated features substantially, while preserving id-related features, which is highly beneﬁcial in contrastive ReID.
Previous GAN-based methods [1, 9, 54, 25, 41, 49] considered unsupervised ReID as an unsupervised domain adaptation (UDA) problem. Under the UDA setting, re-searchers used both, a labeled source dataset, as well as an unlabeled target dataset to gradually adjust a model from a source domain into a target domain. GANs can be used in cross-domain style transfer, where labeled source do-main images are generated in the style of a target domain.
However, the UDA setting necessitates a large-scale labeled source dataset. Scale and quality of the source dataset 2004
strongly affect the performance of UDA methods. Recent research has considered fully unsupervised ReID [35, 24], where under the fully unsupervised setting, a model directly learns from unlabeled images without any identity labels.
Self-supervised contrastive methods [16, 6] belong to this category. In this work, we use a GAN as a novel view gen-erator for contrastive learning, which does not require a la-beled source dataset.
Here, we aim at enhancing view diversity for contrastive learning via generation under the fully unsupervised set-ting. Towards this, we introduce a mesh-based novel view generator. We explore the possibility of disentangling a person image into identity features (color distribution and body shape) and structure features (pose and view-point) under the fully unsupervised ReID setting. We estimate 3D meshes from unlabeled training images, then rotate these 3D meshes to simulate new structures. Compared to skeleton-guided pose transfer [14, 25], which neglects body shape, mesh recovery [21] jointly estimates pose and body shape. Estimated meshes preserve body shape during the training, which facilitates the generation and provides more visual clues for ﬁne-grained ReID. Novel views can be gen-erated by combining identity features with new structures.
Once we obtain the novel views, we design a pseudo la-bel based contrastive learning module. With the help of our proposed view-invariant loss, we maximize representation similarity between original and generated views of a same person, whereas representation similarity of other persons is minimized.
Our proposed method incorporates generative and con-trastive modules into one framework, which are trained jointly. Both modules share the same identity feature en-coder. The generative module disentangles identity and structure features, then generates diversiﬁed novel views.
The novel views are then used in the contrastive module to improve the capacity of the shared identity feature en-coder, which in turn improves the generation quality. Both modules work in a mutual promotion way, which signiﬁ-cantly enhances the performance of the shared identity fea-ture encoder in unsupervised ReID. Moreover, our method is compatible with both UDA and fully unsupervised set-tings. With a labeled source dataset, we obtain better per-formance by alleviating the pseudo label noise.
Our contributions can be summarized as follows. 1. We propose a joint generative and contrastive learn-ing framework for unsupervised person ReID. Gener-ative and contrastive modules mutually promote each other’s performance. 2. In the generative module, we introduce a 3D mesh based novel view generator, which is more effective in body shape preservation than skeleton-guided gen-erators. 3. In the contrastive module, a view-invariant loss is pro-posed to reduce intra-class variation between original and generated images, which is beneﬁcial in building view-invariant representations under a fully unsuper-vised ReID setting. 4. We overcome the limitation of previous GAN-based unsupervised ReID methods that strongly rely on a la-beled source dataset. Our method signiﬁcantly sur-passes the performance of state-of-the-art methods un-der both, fully unsupervised, as well as UDA settings. 2.