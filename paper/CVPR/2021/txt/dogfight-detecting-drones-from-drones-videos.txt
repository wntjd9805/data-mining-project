Abstract
As airborne vehicles are becoming more autonomous and ubiquitous, it has become vital to develop the capa-bility to detect the objects in their surroundings. This paper attempts to address the problem of drones detection from other ﬂying drones. The erratic movement of the source and target drones, small size, arbitrary shape, large inten-sity variations, and occlusion make this problem quite chal-lenging. In this scenario, region-proposal based methods are not able to capture sufﬁcient discriminative foreground-background information. Also, due to the extremely small size and complex motion of the source and target drones, feature aggregation based methods are unable to perform well. To handle this, instead of using region-proposal based methods, we propose to use a two-stage segmentation-based approach employing spatio-temporal attention cues. Dur-ing the ﬁrst stage, given the overlapping frame regions, de-tailed contextual information is captured over convolution feature maps using pyramid pooling. After that pixel and channel-wise attention is enforced on the feature maps to
In the second stage, ensure accurate drone localization.
ﬁrst stage detections are veriﬁed and new probable drone locations are explored. To discover new drone locations, motion boundaries are used. This is followed by track-ing candidate drone detections for a few frames, cuboid formation, extraction of the 3D convolution feature map, and drones detection within each cuboid. The proposed approach is evaluated on two publicly available drone de-tection datasets and outperforms several competitive base-lines. 1.

Introduction
Drones are actively being used in several daily life ap-plications such as agriculture [8], wildﬁre ﬁghting [34], in-ventory applications [19], cinematography [16] and surveil-lance [47, 28]. Due to a large-scale application of drones, recently computer vision researchers have put forward sev-eral new techniques for object detection [10], tracking [28],
Figure 1. A comparison of our approach (PR) with state-of-the-art object detectors: FCOS (FC) [36], Mask-RCNN (MR) [13],
MEGA (ME) [7], SLSA (SL) [39], and SCRDet (SC) [42].
In this frame (1080×1920), there are four drones of sizes: 10×15, 11×22, 12×20, 6×17. The green bounding box represents the ground truth and output of detectors are shown in black colors.
For the clarity, we have not shown all false positives. The pro-posed approach provides better drone localization, reduces false positives and improves recall. agriculture monitoring [8] and human action recognition
[1, 2] in the imagery obtained through drones. In addition to detecting different objects from a drone video, it is also im-portant to detect the drone itself from a video captured by another drone to avoid drone attacks [3], drone collisions
[33] and safe multi-drone ﬂights [24, 32].
Detection of ground objects and aerial drones from drone videos is a very challenging problem due to a large and abrupt camera motion, arbitrary drone shape and view changes, occlusion, and more importantly small object size.
Although a lot of recent research has been conducted to detect and track ground objects and to detect human ac-tion using drones [10, 47, 28, 1, 2, 35], limited work is being done to detect drones from drone videos [24, 33].
To tackle the problem of drone detection, Li et al., [24] proposed a new drone to drone detection dataset and em-ployed handcrafted features for background estimation and foreground moving objects detection. Similarly, Rozantsev et al., [33] introduced a new challenging dataset of drones 7067
and air-crafts. They employed regression-based approaches to achieve object-centric stabilization and perform cuboid classiﬁcation for detection purposes.
Usually ﬂying drones occupy a few pixels in the video frames. For instance, the average drone size respectively is 0.05% and 0.07% of the average frame size in drone detec-tion datasets proposed in [24] and [33]. Note that this is much smaller than that of PASCAL VOC (22.62%) and Im-ageNet (19.94%). Small objects including drones usually appear in the cluttered background and are oriented in dif-ferent directions which makes its detection quite difﬁcult.
This issue was also pointed out by Huang et al., in [17], where they demonstrated that the mean Average Precision (mAP) of small objects is much lower than that of larger objects. Furthermore, the object detection performance fur-ther worsens in the videos [7]. To address this, Noh et al., [29] proposed a feature-level super-resolution-based ap-proach that utilizes high-resolution target features for super-vising a low-resolution model. However, this would require the availability of both low and high-resolution drone im-ages which are difﬁcult to obtain in drone videos where the drone is already ﬂying at a far distance. Similarly, Yang et al., [42] employed a region proposal-based multi-level feature fusion approach to detect small objects and intro-duce a new loss function to handle rotated objects. How-ever, due to very small size objects, less salient, and clut-tered backgrounds i.e., clouds, buildings, etc., it is difﬁ-cult to obtain well-localized region proposals, speciﬁcally in drone detection datasets. Through adversarial learning,
Wu et al., [40] proposed to learn domain-speciﬁc features employing metadata (ﬂying altitudes, weather, and view an-gles). Given the recent low prices of drones, it is more use-ful to use an RGB camera for detection and collision avoid-ance purposes instead of relying on the expensive hardware for metadata collection. Authors in [48, 27, 39] proposed to use convolution feature aggregations across video frames to achieve improved video object detection. Our experimen-tal results and analysis reveal that although feature aggre-gations [48, 27, 39] techniques work well for large video object detection, for drone detection explicit motion infor-mation is more useful.
In this paper, we propose a two-stage segmentation-based approach to detect drones in cluttered backgrounds.
The ﬁrst stage uses only the appearance cues while the sec-ond stage exploits spatio-temporal cues. Given a video frame, we divide it into overlapping frame regions. Each frame region is passed through deep residual networks [14] to obtain the convolution feature maps which are then fol-lowed by pyramid pooling layers [44] to embed the con-textual information. After that pixel-wise and channel-wise attention is employed on the convolution feature maps to discriminate drone boundaries from the background and achieve improved drone localization. The purpose of the second stage is to discover missing detections, remove false detections, and conﬁrm the true positive detections by employing motion information. To discover the miss-ing drones, we employ motion boundaries to ﬁnd probable drone locations. Given the detections from the ﬁrst stage and motion boundaries locations, we track each location forward and backward for a few (eight) frames. After that, cuboids are extracted across those tracks and are fed to a 3D convolutional neural network [5] for spatio-temporal fea-ture extraction. This is followed by pyramid pooling layers.
Similar to the ﬁrst stage, we employ pixel and channel-wise attention in the second stage as well to get the improved localization. The proposed approach signiﬁcantly outper-In the experimental forms several competitive baselines. section, we validate the efﬁcacy of each step of the proposed approach. The rest of the paper is organized as follows. Sec-tion 2 provides a brief overview of the related developments in small object detection including drones in video and im-ages. Section 3 deals with our proposed methodology and
Section 4 covers experimental results. Finally, Section 5 concludes the paper. 2.