Abstract
Dense depth estimation plays a key role in multiple appli-cations such as robotics, 3D reconstruction, and augmented reality. While sparse signal, e.g., LiDAR and Radar, has been leveraged as guidance for enhancing dense depth es-timation, the improvement is limited due to its low density and imbalanced distribution. To maximize the utility from the sparse source, we propose Sparse Signal Superdensity (S3) technique, which expands the depth value from sparse cues while estimating the conﬁdence of expanded region.
The proposed S3 can be applied to various guided depth estimation approaches and trained end-to-end at different stages, including input, cost volume and output. Extensive experiments demonstrate the effectiveness, robustness, and
ﬂexibility of the S3 technique on LiDAR and Radar signal. 1.

Introduction
Dense depth estimation is crucial in the ﬁeld of 3D re-construction [14], 3D object detection [44, 47], and robotic vision [25, 28]. Many works have proposed to estimate depth from RGB images or stereo pairs. Yet, the stereo es-timation could be unreliable on homogeneous planes, large illumination changes, and repetitive textures [38, 43]; while monocular depth estimation is an ill-posed problem [11] and inherently ambiguous and unreliable [20, 24]. To at-tain a higher level of robustness and accuracy, modern so-lutions commonly leverage raw sparse signal, such as Li-DAR [2, 34, 24] and Radar [5, 29], to improve depth estima-tion results or object detection for the challenging outdoor scenes, termed guidance in this paper.
Despite the success of those sparse-guidance methods, however, we still ﬁnd two big problems with sparse sig-nal. First, raw sparse signal can be ignored by networks when it is largely different from depth predicted with RGB (shown in Figure 1a). This situation stems from the low density property of the sparse signal, which is a common problem in many large-scale dataset. For example, KITTI dataset [12] wraps up an average density of 4.0% and
RGB
Sparse	Signal
Ground	Truth
Depth	from	RGB
Guide	with	Sparse	Signal
Guide	with	S3 (a) Low Density Problem.
RGB 4-beam	Sparse Signal
Ground	Truth
Depth	from	RGB
Guide	with	4-beam
Guide	with	S3 (b) Imbalanced Signal Distribution.
Figure 1: Major problems of sparse depth signal. (a)
The network tends to ignore the low density hint if depth from RGB is hugely different from sparse signal. (b) Im-balanced signal distribution would make the guidance to be not equally distributed. We can observe the trace of the 4 scanning lines of LiDAR (yellow circle). Our proposed S3 method successfully overcome the two problems. The noise in the top example is removed, and the guided result in the bottom example is smoother and closer to the ground truth.
Best viewed in zoomed digital. nuScenes dataset [4] has an average of less than 50 Radar points over a 900 × 1600 image. Actually, the guidance module tends to ignore the accurate but sparse signals when they strongly disagree with the original prediction.
Furthermore, imbalance guidance is also the main prob-lem. As shown in Figure 1b, the algorithms only focus on the small region with high signal density while barely correct the low-density region between scanning lines and cause non-smoothing result. However, these low-density parts neither implicate less importance nor less conﬁdence. 16706
In fact, there could be important objects like cars at these parts, and the imbalanced guidance stems from the uneven signal distribution of sensing devices in space. For exam-ple, LiDAR signals are mostly localized on the scanning lines with the same polar angles in the spherical coordinate, and the azimuth resolution of Radar signals is poor [10, 37].
As a result, for previous methods that conduct experiments under the assumption of uniformly distributed signal can be unreliable for real-world imbalanced cases.
To tackle the critical low density and imbalanced distri-bution problems, we propose a novel framework, Sparse
Signal Superdensity (S3), to enhance the density and mit-igate imbalanced sparse signal for guided depth estimation.
S3 consists of two components: (1) sparse signal expan-sion (2) conﬁdence weighting. For sparse signal expan-sion, S3 ﬁrst estimates the expanded area for each sparse signal based on the RGB image, and then assigns appro-priate depth value to the expanded region. For conﬁdence weighting, S3 measures the conﬁdence of the assigned depth to control the amount of inﬂuence to the sparse-guidance methods. Our method effectively utilizes conﬁ-dence weighting to increase the density of the sparse signal.
S3 framework, implemented with a light-weight net-work, can be applied to existing sparse-guidance depth es-timation methods. For instance, embedding it in existing depth networks and trained in an end-to-end fashion. Losses are developed to allow S3 network to learn sparse signal ex-pansion and conﬁdence weighting from data either for pre-training purposes or training jointly with depth networks.
We conduct qualitative experiments to show the effective-ness of S3 network on LiDAR and Radar guidance meth-ods. The experimental results show that using our proposed
S3 can solve the low density and imbalanced distribution problems. Our method can highly increase the utility of the sparse signal and make substantial improvements on four typical sparse-guidance schemes on KITTI [13, 27, 41] and nuScenes [4] dataset.
To sum up, our contributions are highlighted as follows,
• The ﬁrst work to point out the defective properties of the sparse signal and the subsequent inﬂuence to the depth estimation results.
• The novel and general framework Sparse Signal Super-density (S3) enhances the density of sparse signal, mit-igates the imbalanced distribution problem, and pro-vides extra conﬁdence cues for depth estimation.
• S3 largely increases the robustness and accuracy on depth estimation tasks using sparse signals, e.g., Li-DAR and Radar. 2.