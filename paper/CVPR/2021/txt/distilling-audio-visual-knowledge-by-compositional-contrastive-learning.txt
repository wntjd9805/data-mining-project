Abstract
Having access to multi-modal cues (e.g. vision and au-dio) empowers some cognitive tasks to be done faster com-pared to learning from a single modality. In this work, we propose to transfer knowledge across heterogeneous modal-ities, even though these data modalities may not be seman-tically correlated. Rather than directly aligning the rep-resentations of different modalities, we compose audio, im-age, and video representations across modalities to uncover richer multi-modal knowledge. Our main idea is to learn a compositional embedding that closes the cross-modal se-mantic gap and captures the task-relevant semantics, which facilitates pulling together representations across modali-ties by compositional contrastive learning. We establish a new, comprehensive multi-modal distillation benchmark on three video datasets: UCF101, ActivityNet, and VG-GSound. Moreover, we demonstrate that our model signiﬁ-cantly outperforms a variety of existing knowledge distilla-tion methods in transferring audio-visual knowledge to im-prove video representation learning. Code is released here: https://github.com/yanbeic/CCL. 1.

Introduction
Videos often contain informative multi-modal cues, such as visual objects, motion, auditory events, and textual infor-mation encoded in captions or speech – all of which pro-vide rich, transferrable semantics for representation learn-ing. The majority of existing works in video understand-ing utilises visual-only content for representation learning
[63, 52, 57, 64, 10, 19]. Our objective, on the other hand, is to distill the rich multi-modal knowledge available in net-works pre-trained on spatial imagery data and temporal au-ditory data to learn expressive video representations.
In contrast to the standard knowledge distillation tech-niques [29, 9] which transfer unimodal knowledge learned from the same modality and dataset, our multi-modal distil-lation framework uniquely utilises knowledge learned from multiple data modalities. Although prior works have con-sidered cross-modal distillation [24, 6, 3, 34, 2], they gener-ally assume pairwise semantic correspondence between two
"
#
$
Image Teacher
Video Student
Audio Teacher
!"
!#
!!
!#"
+
!!#
+ negatives positives negatives multi-modal shared latent space
+
: A learnable compositional embedding
Figure 1. Our generic multi-modal distillation framework aligns audio, image, and video representations in the latent space by com-positional contrastive learning, where a compositional embedding is learned to bridge the cross-modal semantic gap and capture the task-relevant semantics for more informative knowledge transfer. modalities. However, in unconstrained scenarios, the cross-modal content may not always be semantically correlated or temporally aligned, e.g. a video of applying lipstick, may be accompanied by audio not directly related to the action, such as music or speech. On the other hand, similar audio cues, e.g. music, may accompany videos showing distinct visual content, e.g. applying lipstick and playing cello.
In this work, we tackle a realistic multi-modal distil-lation paradigm that can distill heterogeneous audio and visual knowledge for video representation learning. This requires to bridge the cross-modal semantic gap, domain gap, as well as dealing with inconsistent network architec-tures across modalities. To address these challenges in a uniﬁed formulation, we propose compositional contrastive learning – a novel, generic framework to distill the multi-modal knowledge by ﬂexibly plugging in the teacher net-works pre-trained on different data modalities. Speciﬁcally, a compositional embedding is learned to close the cross-modal gap between the teacher and student networks and capture the task-relevant semantics. By jointly pulling to-gether the teacher, student, and their compositional embed-dings through compositional contrastive learning, the multi-modal knowledge is transferred to the video student net-work to learn more powerful video representations. 7016
Our contributions are as follows. (1) We propose a novel
Compositional Contrastive Learning (CCL) model, featured by learnable compositional embeddings that close the cross-modal semantic gap, and a distillation objective which con-trasts different modalities jointly in the shared latent space, where class labels are introduced to distill the multi-modal knowledge in an informative way. (2) We establish a new benchmark on multi-modal distillation, comparing CCL ex-tensively to seven state-of-the-art distillation methods on three video datasets: UCF101, ActivityNet, VGGSound, in (3) We two tasks: video recognition and video retrieval. demonstrate the advantages of our model in comparison to the prior state-of-the-art methods, and provide an insightful quantitative and qualitative ablative analysis. 2.