Abstract
The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic
ﬁdelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via mul-tiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an at-tentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discrimina-tor, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN’s output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but– more importantly–people prefer XMC-GAN by 77.3% for image quality and 74.1% for image-text alignment, com-pared to three other recent models. XMC-GAN also gen-eralizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91. 1.

Introduction
Compared to other kinds of inputs (e.g., sketches and ob-ject masks), descriptive sentences are an intuitive and ﬂex-ible way to express visual concepts for generating images.
The main challenge for text-to-image synthesis lies in learn-ing from unstructured description and handling the different statistical properties between vision and language inputs.
*Equal contribution.
†Work done as a member of the Google AI Residency program.
‡Work performed at Google Research.
Figure 1: Inter-modal and intra-modal contrastive losses in our proposed XMC-GAN text-to-image synthesis model.
Generative Adversarial Networks (GANs) [12] have shown promising results on text-to-image generation [44, 61, 62], using a conditional GAN formulation [11]. At-tnGAN [58] proposes a multi-stage reﬁnement framework to generate ﬁne-grained details by attending to relevant words in the description. These models generate high ﬁ-delity images on single domain datasets (e.g., birds [56] and
ﬂowers [35]), but struggle on complex scenes with many objects—such as those in MS-COCO [30]. Recent meth-ods [18, 27, 16, 22] propose object-driven, hierarchical ap-proaches that explicitly model object instances within an image. Given the text description, they ﬁrst infer a semantic layout (e.g., object bounding boxes, segmentation masks, or a combination), and then generate an image from the layout.
These hierarchical methods are cumbersome to apply to real-world scenarios; generation becomes a multi-step pro-cess (box-to-mask-to-image), and the model requires much more ﬁne-grained object labels to train.
We study contrastive learning in the context of text-to-image synthesis and demonstrate that a simple one-stage
GAN without object-level annotation can outperform prior object-driven and multi-stage approaches. Besides gener-ating realistic images, we also hope (1) the image should holistically match the description; (2) generated images should match real images when they are conditioned on the 833
same description; (3) individual image regions should be recognizable and consistent with words in the sentence. To fulﬁll these desiderata and achieve strong language align-ment, we propose to maximize the mutual information be-tween the corresponding pairs through contrastive learning.
Our method, the Cross(X)-Modal Contrastive Generative
Adversarial Network (XMC-GAN), uses image to sentence, image region to word, and image to image contrastive losses to enforce alignment between generated images and their captions (Fig. 1). Our primary contributions include:
• We propose XMC-GAN, a simple one-stage GAN that employs several contrastive losses. XMC-GAN pro-duces dramatic improvements over previous models, reducing FID [15] from 24.70 to 9.33 on MS-e.g.
COCO and from 48.70 to 14.12 on LN-COCO (the
MS-COCO portion of Localized Narratives [40]).
• We conduct thorough human evaluations comparing
XMC-GAN to three recent models. These show that people prefer XMC-GAN 77.3% of the time for image realism, and 74.1% for image-text alignment.
• We establish a strong benchmark on the challenging
LN-OpenImages (Open Images subset of Localized
Narratives). To the best of our knowledge, this is the
ﬁrst text-to-image results training and testing on the diverse images and descriptions for Open Images.
• We conduct a thorough analysis of contrastive losses used in XMC-GAN to provide general modeling in-sights for contrastive learning in conditional GANs.
XMC-GAN consistently produces images that are more co-herent and detailed than previous models.
In addition to greater realism (with clearer, more delineated objects), they better capture the full image description, including the pres-ence of named objects and background compositions. 2.