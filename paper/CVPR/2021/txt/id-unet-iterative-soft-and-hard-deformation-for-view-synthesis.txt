Abstract
View synthesis is usually done by an autoencoder, in which the encoder maps a source view image into a latent content code, and the decoder transforms it into a target view image according to the condition. However, the source contents are often not well kept in this setting, which leads to unnecessary changes during the view translation. Al-though adding skipped connections, like Unet, alleviates the problem, but it often causes the failure on the view confor-mity. This paper proposes a new architecture by perform-ing the source-to-target deformation in an iterative way.
Instead of simply incorporating the features from multiple layers of the encoder, we design soft and hard deformation modules, which warp the encoder features to the target view at different resolutions, and give results to the decoder to complement the details. Particularly, the current warping
ﬂow is not only used to align the feature of the same res-olution, but also as an approximation to coarsely deform the high resolution feature. Then the residual ﬂow is esti-mated and applied in the high resolution, so that the de-formation is built up in the coarse-to-ﬁne fashion. To bet-ter constrain the model, we synthesize a rough target view image based on the intermediate ﬂows and their warped features. The extensive ablation studies and the ﬁnal re-sults on two different data sets show the effectiveness of the proposed model. https://github.com/MingyuY/
Iterative-view-synthesis 1.

Introduction
Novel view synthesis, also known as view translation, facilitates the computer to render the same object under ar-bitrary poses, given an input object image in a source pose.
This is a challenging task, since it requires the model to understand not only the image content, but also the relation
∗Corresponding author, supported by the the Science and Technology
Commission of Shanghai Municipality (No.19511120800).
Figure 1: (a) The ID-Unet realizes the translation from the source view to the target, either existing in the MultiPIE dataset (−30◦,−15◦,0◦), or under a new view (inside the yellow box) by the linear interpolation between two adja-cent view conditions. (b) Extra results on CelebA from the existing model training on MultiPIE. between the object poses and its appearances showing in the image. The model needs to ﬁgure out the intrinsic shape of the object and keep it stable during the translation. Mean-while, it should be able to synthesize the appearance of the object, conforming to the target view condition.
Recently, learning-based method has been employed broadly for this task. Particularly, view synthesis is com-monly regarded as a multi-domain image-to-image transla-tion task, which is often modeled by the autoencoder (AE)
[6, 42] or variational autoencoder (VAE) [4, 43]. Both con-sist of a pair of encoder and decoder, in which only the last layer of the encoder connects to the decoder, as shown in
Figure 2 (a). However, their limitation has already been re-alized [20, 41]. Basically, using the latent code from the last layer is not enough to represent the content. Since the decoder can only get one latent code, the source content cannot be kept well in the translated image. A simple but effective solution is the Unet [30] structure. It utilizes sev-eral skipped connections by making the shortcuts from the encoder to the decoder, therefore the output can take more features from the source, as shown in Figure 2 (b). Such as
V-Unet [9] is a VAE model with skipped connections and 7220
used for person synthesis. Unet indeed improves the image quality. But directly using the low-level encoder features makes it difﬁcult to satisfy the domain requirement, hence the image sometimes fails to be translated into the target domain.
Intuitively, in view translation, the encoder feature needs to be deformed before giving it to the decoder. A straight-forward way is to apply the the same optical ﬂow on the different resolutions of the feature map. The ﬂow can be either determined by the priory knowledge [31] or learned by the model [43], and the structure is shown Figure 2 (c).
However, we ﬁnd that using the same ﬂow on different reso-lutions limits the model’s ability for synthesis. On one hand, the ﬂow is often not accurate enough. It is estimated based on the feature of a certain resolution, therefore may be inap-propriate for other sizes. On the other hand, the model can already change the view even without any intentional defor-mations, which implies that we should give it the ﬂexibility to determine the deformation on different resolutions.
To properly exploit the encoder features in the view syn-thesis, this paper proposes an iterative way to deform them in the coarse-to-ﬁne fashion, so that they can be aligned with the corresponding part in the decoder. The deformed features skip several intermediate layers, and are directly given to the layers in the decoder to complement the con-tent details. Inspired by the idea of progressively estimating the optical ﬂow for the raw pixels [3, 22], our model speci-ﬁes the offset vectors for the encoder features from the low to the high resolution, and these displacements are accumu-lated across the multiple resolutions. Speciﬁcally, we ﬁrst use offsets from the low resolution as an approximation to coarsely deform the feature, then the residual offsets are es-timated by comparing the roughly deformed result to the decoder feature of the same size. The residuals reﬁne the coarse ﬂow and they are applied to give the additional de-formation. The reﬁned ﬂow is further employed by the next block in a larger size. In brief, the encoder feature is ﬁrst warped according to the coarse ﬂow, and then the remaining offsets is estimated and applied, so that the result is better consistent with the target view.
To compute the initial ﬂow and its following-up residu-als, we design the Soft and Hard Conditional Deformation
Modules (SCDM and HCDM) based on the features from the encoder and decoder. The view label is the extra con-ditional input to control the amount of displacement. The idea of the soft ﬂow is to compute the similarity scores (also known as the attention matrix) between the encoder and de-coder features like [37, 39]. Given the two of them, the spa-tial and channel similarities are measured, and then applied onto the encoder features to align them into the target view.
However, the soft ﬂow is not efﬁcient enough to compute on multiple resolutions. Furthermore, if the target view is far from the source, the similarity may no longer reﬂect the
Figure 2: An illustration of several comparing frameworks. (a) and (b) are cVAE and Unet, respectively. (c) is the combination of them, and T realizes the translation from source view a to target view b based on optical ﬂow. (d) improved from cVAE+Unet, the optical ﬂow is estimated it-eratively. The initial ﬂow T1 is calculated according to the low-resolution features. As the resolution increases layer by layer, the residual ∆Tn is calculated to progressively reﬁne the previous result. spatial deformation. Our solution is to estimate the opti-cal ﬂow to ”hard” warp the feature before the spatial and channel attention in SCDM. Moreover, we also design the
HCDM which gives the high resolution residuals onto the previous small optical ﬂow, and it ”hard” warps the current feature and further aligns it to the target view.
The contributions lie in following aspects: (1) We pro-pose an iterative view translation framework which deforms the encoder feature from different layers and gives them to the decoder to improve the synthesis quality. (2) We de-sign the SCDM and HCDM and use them to align the en-coder feature into the target view. (3) Extensive experiments on two different datasets show the effectiveness of the pro-posed framework and our designed modules. 2.