Abstract
In this paper, we tackle the problem of convolutional neu-ral network design.
Instead of focusing on the design of the overall architecture, we investigate a design space that is usually overlooked, i.e. adjusting the channel conﬁgura-tions of predeﬁned networks. We ﬁnd that this adjustment can be achieved by shrinking widened baseline networks and leads to superior performance. Based on that, we artic-ulate the “heterogeneity hypothesis”: with the same train-ing protocol, there exists a layer-wise differentiated net-work architecture (LW-DNA) that can outperform the origi-nal network with regular channel conﬁgurations but with a lower level of model complexity.
The LW-DNA models are identiﬁed without extra com-putational cost or training time compared with the orig-inal network. This constraint leads to controlled experi-ments which direct the focus to the importance of layer-wise speciﬁc channel conﬁgurations.
LW-DNA models come with advantages related to overﬁtting, i.e. the rel-ative relationship between model complexity and dataset size. Experiments are conducted on various networks and datasets for image classiﬁcation, visual tracking and image restoration. The resultant LW-DNA models consistently outperform the baseline models. Code is available at https://github.com/ofsoundof/
Heterogeneity_Hypothesis.git. 1.

Introduction
Since the advent of the deep learning era, convolu-tional neural network (CNN) [21] design has replaced the role of feature design in various computer vision tasks.
Recently, neural network design has also evolved from manual design [44, 14, 18] to neural architecture search (NAS) [31, 45] and semi-automation [49, 16, 38]. State-of-the-art network designs focus on discovering the overall network architecture with regularly repeated convolutional layers. This has been the golden standard of current CNN designs. For example, Ma et al. mentioned that a network should have equal channel width [33]. But their analysis is limited to minimizing the memory access cost given the
FLOPs for a single pointwise convolution.
The motivation of this paper kind of contradicts the pre-vious design heuristics. It investigates a design space that is usually overlooked and thus not fully explored, namely adjusting the layer-wise channel conﬁgurations. In this pa-per, the channel conﬁguration of a network is deﬁned as the vector that summarizes the output channels of the convolu-tional layers. We try to answer three questions: 1) whether there exists a layer-wise differentiated network architecture (LW-DNA) that can outperform the original one; 2) if so, how to identify it efﬁciently; and 3) why it can beat the reg-ular conﬁguration.
Question 1: The existence of LW-DNA. To answer the
ﬁrst question, we formally articulate the following hypoth-esis. The Heterogeneity Hypothesis: For a CNN, when trained with exactly the same training protocol (e.g. num-ber of epochs, batch size, learning rate schedule), there exists a layer-wise differentiated network architecture (LW-DNA) that can outperform the original network with regular layer-wise channel conﬁgurations but with a lower model complexity in term of FLOPs and parameters.
To be speciﬁc, we aim at adjusting the numbers of channels of the convolutional layers in predeﬁned CNNs.
The other layer conﬁgurations such as kernel size and stride are not changed. Formally, consider an L-layer
CNN f (X; Θ, c), where c = (c1, c2, · · · , cL) is the chan-nel conﬁguration of all of the convolutional layers, Θ de-notes the parameters in the network, and X is the in-put of the network. The heterogeneity hypothesis im-there should exist a new channel conﬁgura-plies that tion c′ = (c′
L) such that the new architecture f ′(X; Θ′, c′) performs no worse than the original one. Af-ter the adjustment, the channel conﬁgurations c′ l could be either larger or smaller than the original cl. We try to an-swer this question by empirical experiments. 2, · · · , c′ 1, c′ 2144
3. The accuracy gain of the LW-DNA models might be related to overﬁtting by the baseline models. We de-rive this conjecture from several observations. I. By comparing the training and testing curves of an LW-DNA model and its baseline in Fig. 4, we ﬁnd that towards the end of the training, the identiﬁed LW-DNA model shows a higher training error but a lower testing error, i.e. improved generalization. This phe-nomenon is consistent across different datasets. This also matches the observations from the pioneering un-structured pruning, like a brain surgeon trying to boost network generalization after brain damage [22, 13]. II.
The accuracy gain of an LW-DNA model is larger for smaller datasets (i.e. Tiny-ImageNet) that are easier to get overﬁtted to, compared with larger datasets (i.e.
ImageNet). III. On the same dataset (i.e. ImageNet), it is easier to identify an LW-DNA model version for larger networks (i.e. ResNet50) than for smaller net-works (i.e. MobileNetV3).
The contributions of this paper can be summarized as follows. First, it demonstrates the possibility of identify-ing a superior version of a network by only adjusting the channel conﬁguration of the network. This could be used as a post-searching mechanism complementary to semi- or fully automated neural architecture search. Secondly, a method that can identify LW-DNA models almost without additional computational cost and training time is proposed.
This method only needs the computation of one random batch. Thirdly, the possible reason for the improved per-formance of an LW-DNA is explained by observing the ex-perimental results. 2.