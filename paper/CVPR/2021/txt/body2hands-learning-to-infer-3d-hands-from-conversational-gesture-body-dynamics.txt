Abstract
We propose a novel learned deep prior of body motion for 3D hand shape synthesis and estimation in the domain of conversational gestures. Our model builds upon the insight that body motion and hand gestures are strongly correlated in non-verbal communication settings. We formulate the learning of this prior as a prediction task of 3D hand shape over time given body motion input alone. Trained with 3D pose estimations obtained from a large-scale dataset of internet videos, our hand prediction model produces con-vincing 3D hand gestures given only the 3D motion of the speaker’s arms as input. We demonstrate the efﬁcacy of our method on hand gesture synthesis from body motion input, and as a strong body prior for single-view image-based 3D hand pose estimation. We demonstrate that our method out-performs previous state-of-the-art approaches and can gen-eralize beyond the monologue-based training data to multi-person conversations. 1.

Introduction
When we communicate, we convey nonverbal signals with our body and hands [38].
In particular, subtle nu-ances can be conveyed by performing speciﬁc conversa-tional hand gestures, as the human hand is richly expres-sive with many degrees of joint freedom. This primordial form of communication is deeply ingrained in human na-ture. From early infancy, human babies pay extra attention to their own and others’ hands [46] and subsequently learn to convey their needs via hand and ﬁnger gestures long be-fore they speak. Endowing machines with the ability to per-ceive and use conversational hand gestures is therefore an essential step towards teaching them to effectively interact with humans.
However, learning the intricacies of conversational hand gestures requires vast amounts of data. While previous approaches attempted rule-based [4] and data-driven [23] methods, a learning based method from large swaths of data would allow for both modeling the ﬁne-grained details of hand motion as well as generalization beyond the training set. Unfortunately, there are many challenges in capturing conversational hand gestures in realistic settings. These in-clude the elaborate motions of ﬁngers, the relatively small size of the hand with respect to the body, and frequent self occlusions. Such challenges make capturing the motion of human hands difﬁcult, even for industry-level multi-camera, optical-marker-based motion capture systems [16, 29]. Em-bodied 3D hand motion capture datasets in realistic conver-sational scenarios are therefore extremely rare.
In this paper, we propose an approach for learning con-versational hand gestures on a large-scale dataset of in-the-111865
wild videos with noisy pseudo-ground truth. We build upon the insight that body motion and hand gestures are strongly coupled during speech [23]. By learning this correlation, we can build a reliable prior for hand gestures conditioned only on the observation of body motion. This approach allows us to take advantage of readily available body motion data from current motion capture systems as well as single-view image-based 3D body pose estimation approaches [24, 51].
Speciﬁcally, we formulate a 3D hand-gesture prediction problem from 3D arm motion input and demonstrate that body-hand correlations can be learned from a large-scale publicly-available monologue video dataset.
Leveraging the learned body-motion-to-hand correla-tion, we present two applications: First, we propose a learned approach for realistic conversational hand gesture synthesis from body-only input (See Fig. 1 for an example).
Second, we use the learned correlation as a body-motion prior for single-view 3D hand pose estimation. While body priors for pose estimation have been classically consid-ered in a non-learning, general setting [48], recent 3D hand trackers [59, 51, 58] have overlooked them.
Our novelty is in proposing a learned deep body prior for the domain-speciﬁc setting of conversational gestures.
Focusing on a single domain of motion, such as nonver-bal communication, allows us to learn a stronger prior than in the general setting. This prior is especially effective in scenarios where the captured appearance of the hands in video is degraded due to occlusions, motion blur, or low resolution. We demonstrate that our proposed model, trained without any clean mocap ground truth, generalizes beyond the training set of in-the-wild monologue data to other speakers as well as to multi-person settings. 2.