Abstract
Balanced CIFAR-10
Long-tailed CIFAR-10
Adversarial robustness has attracted extensive studies recently by revealing the vulnerability and intrinsic char-acteristics of deep networks. However, existing works on adversarial robustness mainly focus on balanced datasets, while real-world data usually exhibits a long-tailed distri-bution. To push adversarial robustness towards more real-istic scenarios, in this work we investigate the adversarial vulnerability as well as defense under long-tailed distribu-tions.
In particular, we ﬁrst reveal the negative impacts induced by imbalanced data on both recognition perfor-mance and adversarial robustness, uncovering the intrinsic challenges of this problem. We then perform a systematic study on existing long-tailed recognition methods in con-junction with the adversarial training framework. Several valuable observations are obtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of robust accuracy exists under unreliable evaluation, and 3) boundary error limits the promotion of robustness. Inspired by these obser-vations, we propose a clean yet effective framework, RoBal, which consists of two dedicated modules, a scale-invariant classiﬁer and data re-balancing via both margin engineer-ing at training stage and boundary adjustment during in-ference. Extensive experiments demonstrate the superiority of our approach over other state-of-the-art defense meth-ods. To our best knowledge, we are the ﬁrst to tackle adver-sarial robustness under long-tailed distributions, which we believe would be a signiﬁcant step towards real-world ro-bustness. Our code is available at: https://github. com/wutong16/Adversarial_Long-Tail. 1.

Introduction
Despite the great progress on a variety of computer vi-sion tasks, deep neural networks are found to be vulnerable to minor adversarial perturbations [38], i.e., easily misled to make incorrect predictions. The existence of adversarial examples reveals a non-negligible security risk to modern computer vision models, with extensive efforts devoted to r e b m u n e l p m a
S y c a r u c c
A
Plainly-trained A_nat
AT-trained A_nat
AT-trained A_rob
Classes
Robust 
Accuracy
Our n o i t i n g o c e
R d e l i a t
-g n o
L e s n e f e
D
Baseline
Natural 
Accurac y
Figure 1. Upper: A long-tailed data distribution induces decreas-ing natural and robust accuracy from head to tail and a magniﬁed
“sacriﬁce” of natural accuracy especially to tail classes when ad-versarial training is applied. Lower: Evaluation results on two metric dimensions, including a number of long-tailed recognition methods combined with adversarial training, several state-of-the-art defense methods, and our RoBal in a region with trade-off. improving adversarial robustness.
Existing adversarial robustness research mainly focuses on balanced datasets such as MNIST, CIFAR, and Ima-geNet [12]. Nevertheless, real-world data usually exhibit a long-tailed distribution [40, 11], which brings challenges not only to the recognition tasks themselves but also to ro-bustness against adversarial attacks. The former has been attracting increasing attention recently, with a number of al-gorithms [24, 16, 51, 7, 2, 44] proposed to tackle the issue;
On the other hand, the latter remains largely unexplored. 8659  
To cast light on the challenges of adversarial robustness in long-tailed recognition (LT), we ﬁrst perform an intu-itive comparison between networks trained on the balanced and long-tailed versions of CIFAR-10, respectively. Apart from normally trained models, we also adopt the adver-sarial training (AT) framework [25], which is one of the most effective and widely used defense methods, to provide the basic adversarial robustness for the networks. Per-class classiﬁcation recalls are evaluated on clean images and im-ages permuted by PGD attack [25], denoted by natural ac-curacy Anat and robust accuracy Arob, respectively. Anat is evaluated on both plain models and AT-trained models, while Arob is performed only on the latter. Results are vi-sualized in Fig. 1. There are three main observations from the comparison: 1) Anat on plain models drops from head to tail, which is exactly what traditional long-tailed recogni-tion aims to solve. 2) A similar decreasing tendency reason-ably occurs in Arob. 3) It is worth noting that Anat drops more signiﬁcantly at the tail when adversarial training is applied, indicating that the well-known “sacriﬁce” of the natural accuracy induced by adversarial training is further magniﬁed for tail classes under a long-tailed distribution.
To form a better understanding of the problem, the re-lationship between natural and robust accuracy can be con-nected by boundary error Rbdy [50] as:
Arob = Anat − Rbdy, where Rbdy represents how likely the features of clean and correctly predicted inputs are close to the ǫ-extension of the decision boundary. It represents the gap between the two forms of accuracy and indicates the vulnerability of samples against adversarial attacks. (1)
Hence, to achieve improvement on both recognition per-formance and adversarial robustness, a natural idea is to raise Anat while keeping a small value of Rbdy. Speciﬁ-cally, on the one hand, we are able to address the issue of imbalance in data distribution via re-balancing strategies, thus we conduct a systematic study of currently widely used long-tailed recognition approaches to explore the proper combinations of these methods and the adversarial train-ing framework. On the other hand, we would analyze why a normalized embedding space promotes model resistance against attacks, and then a scale-invariant classiﬁer is in-troduced to replace the ﬁnal linear layer. The idea of data re-balancing is then well aligned with the cosine classiﬁer by the cooperation of class-aware and pair-aware margins during training and boundary adjustment at inference.
Note that the imbalance in data distribution and the de-ﬁciency in sample numbers are two issues induced simulta-neously when we turn to long-tailed datasets instead of the artiﬁcially balanced ones. Although the importance of data scale in adversarial robustness has been widely studied [33], we mainly focus on the problem of imbalance in this paper.
We study the effect of them separately in Sec 5, verifying that eliminating prediction priors is crucial to reducing the vulnerability of tail classes under attack.
Our contributions are as follows: 1) To our best knowl-edge, we are the ﬁrst to tackle adversarial robustness under long-tailed distribution, which we believe would be a signif-icant step towards real-world robustness. 2) We conduct a systematic study on existing long-tailed recognition meth-ods and their adoption into the adversarial training proce-dure. Important insights are gained based on experimental observations. 3) We further develop a clean yet effective approach, RoBal, that achieves state-of-the-art performance on both natural and robust accuracy. 2.