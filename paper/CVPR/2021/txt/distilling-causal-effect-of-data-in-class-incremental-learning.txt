Abstract
We propose a causal framework to explain the catas-trophic forgetting in Class-Incremental Learning (CIL) and then derive a novel distillation method that is orthogonal to the existing anti-forgetting techniques, such as data re-play and feature/label distillation. We ﬁrst 1) place CIL into the framework, 2) answer why the forgetting happens: the causal effect of the old data is lost in new training, and then 3) explain how the existing techniques mitigate it: they bring the causal effect back. Based on the causal frame-work, we propose to distill the Colliding Effect between the old and the new data, which is fundamentally equiva-lent to the causal effect of data replay, but without any cost of replay storage. Thanks to the causal effect analysis, we can further capture the Incremental Momentum Effect of the data stream, removing which can help to retain the old ef-fect overwhelmed by the new data effect, and thus alleviate the forgetting of the old class in testing. Extensive exper-iments on three CIL benchmarks: CIFAR-100, ImageNet-Sub&Full, show that the proposed causal effect distillation can improve various state-of-the-art CIL methods by a large margin (0.72%–9.06%). 1 1.

Introduction
Any learning systems are expected to be adaptive to the ever-changing environment. In most practical scenarios, the training data are streamed and thus the systems cannot store all the learning history: for animate systems, the neurons are genetically designed to forget old stimuli to save en-ergy [6, 34]; for machines, old data and parameters are dis-carded due to limited storage, computational power, and bandwidth [28, 44, 50]. In particular, a practical learning task as Class-Incremental Learning1 (CIL) [1, 13, 35, 47] is studied, where each incremental data batch consists of new samples and their corresponding new class labels. The 1Code is available at https://github.com/JoyHuYY1412/
DDE_CIL 1There are also other settings like task-incremental [14, 26].
Figure 1: Forgetting and existing anti-forgetting solutions in
Class-Incremental Learning at step t and t + 1. (a): Forget-ting happens when the features are overridden by the t + 1 step training only on the new data. (b): The key to mitigate forgetting is to impose data, feature, and label effects from step t on step t + 1. key challenge of CIL is that discarding old data is espe-cially catastrophic for deep-learning models, as they are in the data-driven and end-to-end representation learning na-ture. The network parameters tuned by the old data will be overridden by the SGD optimizer using new data [11, 33], causing a drastic performance drop — catastrophic forget-ting [36, 10, 28] — on the old classes.
We illustrate how CIL forgets old classes in Figure 1 (a).
At step t, the model learns to classify “dog” and “zebra”; at the next step t+1, it updates with new classes “hare” and
“robin”. If we ﬁne-tune the model only on the new data, old features like “stripe” and “furry” will be inevitably overrid-den by new features like “long-ear” and “feather”, which are discriminative enough for the new classes. Therefore, the new model will lose the discriminative power — forget
— about the old classes (e.g., the red “zebra” at step t + 1).
To mitigate the forgetting, as illustrated in Figure 1 (b), existing efforts follow the three lines: 1) data re-play [35, 13, 25], 2) feature distillation [13, 9], and 3) la-3957
bel distillation [1, 47]. Their common nature is to impose the old training effect on the new training (the red arrows in
Figure 1 (b), which are formally addressed in Section 3.3).
To see this, for replay, the imposed effect can be viewed as including a small amount of old data (denoted as dashed borders) in the new data; for distillation, the effect is the features/logits extracted from the new data by using the old network, which is imposed on the new training by using the distillation loss, regulating that the new network behavior should not deviate too much from the old one.
However, the distillation is not always coherent with the new class learning — it will even play a negative role at chances [42, 43]. For example, if the new data are out-of-distribution compared to the old one, the features of the new data extracted from the old network will be elusive, and thus the corresponding distillation loss will mislead the new training [23, 20]. We believe that the underlying rea-son is that the feature/label distillation merely imposes its effect at the output (prediction) end, violating the end-to-end representation learning, which is however preserved by data replay.
In fact, even though that the ever-increasing extra storage in data replay contradicts with the require-ment of CIL, it is still the most reliable solution for anti-forgetting [35, 45, 5]. Therefore, we arrive at a dilemma: the end-to-end effects of data replay are better than the output-end effects of distillation, but the former requires ex-tra storage while the latter does not. We raise a question: is there a “replay-like” end-to-end effect distillation?
In this paper, we positively answer this question by fram-ing CIL into a causal inference framework [31, 29], which models the causalities among data, feature, and label at any consecutive t and t + 1 learning steps (Section 3.1). Thanks to it, we can explain the reasons 1) why the forgetting hap-pens: the causal effect from the old training is lost in the new training (Section 3.2), and 2) why data replay and fea-ture/label distillation are anti-forgetting: they win the effect back (Section 3.3). Therefore, the above question can be easily reduced to a more concrete one: besides replay, is there another way to introduce the causal effect of the old data? Fortunately, the answer is yes, and we propose an effective yet simple method called: Distilling Colliding Ef-fect (Section 4.1), where the desired end-to-end effect can be distilled without any replay storage. Beyond, we ﬁnd that the imbalance between the new data causal effect (e.g., hundreds of samples per new class) and the old one (e.g., 5–20 samples per old class) causes severe model bias on the new classes. To this end, we propose an Incremental Mo-mentum Effect Removal method to remove the biased data causal effect that causes forgetting (Section 4.2).
Through extensive experiments on CIFAR-100 [18] and
ImageNet [7], we observe consistent performance boost by using our causal effect distillation, which is agnostic to methods, datasets, and backbones. For example, we improve the two previously state-of-the-art methods: LU-CIR [13] and PODNet [9], by a large margin (0.72%– 9.06%) on both benchmarks. In particular, we ﬁnd that the improvement is especially larger when the replay of the old data is fewer. For example, the average performance gain is 16.81% without replay, 7.05% with only 5 samples per old class, and 1.31% with 20 samples per old class on CIFAR-100. The results demonstrate that our distillation indeed preserves the causal effect of data. 2.