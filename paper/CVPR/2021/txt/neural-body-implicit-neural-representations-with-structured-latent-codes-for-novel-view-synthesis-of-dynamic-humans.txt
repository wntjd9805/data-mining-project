Abstract 1.

Introduction
This paper addresses the challenge of novel view synthe-sis for a human performer from a very sparse set of cam-era views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves re-markable view synthesis quality given dense input views.
However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed prob-lem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new hu-man body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated.
The deformable mesh also provides geometric guidance for the network to learn 3D representations more efﬁciently.
To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our ap-proach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset.
The authors from Zhejiang University are afﬁliated with the State Key
Lab of CAD&CG. ∗Corresponding author: Xiaowei Zhou.
Free-viewpoint videos of human performers have a vari-ety of applications such as movie production, sports broad-casting, and telepresence. Previous free-viewpoint video systems either rely on a dense array of cameras for image-based novel view synthesis [20, 23] or require depth sen-sors for high-quality 3D reconstruction [8, 14] to produce realistic rendering. The complicated hardware makes free-viewpoint video systems expensive and only applicable in constrained environments.
This work focuses on the problem of novel view synthe-sis for a human performer from a sparse multi-view video captured by a very limited number of cameras, as illustrated in Figure 1. This setting signiﬁcantly decreases the cost of free-viewpoint systems and makes the systems more widely applicable. However, this problem is extremely challeng-ing. Traditional image-based rendering methods [20, 12] mostly require dense input views and cannot be applied here. For reconstruction-based methods [54, 22], the wide baselines between cameras make dense stereo matching in-tractable. Moreover, part of the human body may be invisi-ble due to self-occlusion in sparse views. As a result, these methods tend to give noisy and incomplete reconstructions, resulting in heavy rendering artifacts.
Recent works [58, 47, 44] have investigated the poten-tial of implicit neural representations on novel view synthe-sis. NeRF [44] shows that photorealistic view synthesis can 19054
process. This model is inspired by the latent variable model
[36] in statistics, which enables us to effectively integrate observations at different frames. Another advantage of the proposed method is that the deformable model provides a geometric prior (rough surface location) to enable more ef-ﬁcient learning of implicit ﬁelds.
To evaluate our approach, we create a multi-view dataset called ZJU-MoCap that captures dynamic humans in com-plex motions. Across all captured videos, our approach ex-hibits state-of-the-art performances on novel view synthe-sis. We also demonstrate the capability of our approach to capture moving humans from monocular RGB videos on the People-Snapshot dataset [2]. Furthermore, our approach can be used for 3D reconstruction of the performers.
In summary, this work has the following contributions:
• We present a new approach capable of synthesizing photorealistic novel views of a performer in complex motions from a sparse multi-view video.
• We propose Neural Body, a novel implicit neural rep-resentation for a dynamic human, which enables us to effectively incorporate observations over video frames.
• We demonstrate signiﬁcant performance improve-ments of our approach compared to prior work. 2.