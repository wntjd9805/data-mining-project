Abstract
Spatial-temporal, channel-wise, and motion patterns are three complementary and crucial types of information for video action recognition. Conventional 2D CNNs are com-putationally cheap but cannot catch temporal relationships; 3D CNNs can achieve good performance but are computa-tionally intensive. In this work, we tackle this dilemma by designing a generic and effective module that can be em-bedded into 2D CNNs. To this end, we propose a spAtio-temporal, Channel and moTion excitatION (ACTION) module consisting of three paths: Spatio-Temporal Excita-tion (STE) path, Channel Excitation (CE) path, and Motion
Excitation (ME) path. The STE path employs one chan-nel 3D convolution to characterize spatio-temporal repre-sentation. The CE path adaptively recalibrates channel-wise feature responses by explicitly modeling interdepen-dencies between channels in terms of the temporal aspect.
The ME path calculates feature-level temporal differences, which is then utilized to excite motion-sensitive channels.
We equip 2D CNNs with the proposed ACTION module to form a simple yet effective ACTION-Net with very limited extra computational cost. ACTION-Net is demonstrated by consistently outperforming 2D CNN counterparts on three backbones (i.e., ResNet-50, MobileNet V2 and BNIncep-tion) employing three datasets (i.e., Something-Something
V2, Jester, and EgoGesture). Code is provided at https:
//github.com/V-Sense/ACTION-Net. 1.

Introduction
Video understanding has attracted an increasing amount of interest, since it is a crucial step towards real-world applications, such as Virtual Reality/Augmented Reality (VR/AR) and video-sharing social networking services.
For instance, millions of videos are uploaded to Tik-Tok, Douyin, and Xigua Video to be processed everyday, wherein video understanding acts a pivotal part. However,
This work is ﬁnancially supported by Science Foundation Ireland (SFI) under the Grant Number 15/RP/2776. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the NVIDIA
DGX station used for this research. the explosive growth in this video streaming gives rise to challenges on performing video understanding at high ac-curacy and low computation cost.
Action recognition, a fundamental problem in video un-derstanding, has been a growing demand in video-related applications, such as content moderation (i.e., recognize content in videos that break terms of service) and content recommendations (i.e., videos are ranked by most liked and recommended to similar customers). The complex actions in videos are normally temporal-dependent, which do not only contain spatial information within each frame but also include temporal information over a duration. For example, symmetric action pairs (‘opening a box’, ‘closing a box’), (‘rotate ﬁsts clockwise’, ‘rotate ﬁsts counterclockwise’) contain similar features in spatial domains, but the tem-poral information is completely reversed. Traditional hu-man action recognition is more scene-related [36, 18, 15], wherein actions are not as temporal-dependent e.g., ‘apply eye makeup’, ‘walking’, ‘running’. With how rapid technol-ogy is developing, like VR, which requires employing ges-tures to interact with environments, temporal-related action recognition has recently become a focus for research.
The mainstreams of existing methods are 3D CNN-based frameworks and 2D CNN-based frameworks. 3D
CNNs have been shown to be effective in terms of spatio-temporal modeling [39, 4, 37], but spatio-temporal mod-eling is unable to capture adequate information contained in videos. The two-stream architecture was proposed to take spatio-temporal information and optical ﬂow into ac-count [35, 3, 33], which boosted performance signiﬁcantly compared to the one-stream architecture. However, compu-tation on optical ﬂow is very expensive, which poses chal-lenges on real-world applications. 3D CNNs suffer from problems including overﬁtting and slow convergence [8].
With more large-scale datasets being released, such as Ki-netics [3], Moments in Time [29] and ActivityNet [2], opti-mizing 3D CNNs becomes much easier and more popular.
However, heavy computations inherent in 3D CNN-based frameworks contribute to slow inferences, which would limit their deployment on real-world applications, such as
VR that relies on online video recognition. Current 2D 13214
Raw
TSN
TSM
Ours
Figure 1: Visualization for signiﬁcant features extracted by TSN, TSM and our ACTION-Net for the action ‘Rotate ﬁsts counterclockwise’.
Features extracted by each method are visualized by using CAM [48]. Compared to TSN and TSM, it can be noticed that ACTION-Net is able to extract features that are related to movements in an action especially for highlighted frames i.e., 4th and 5th columns. More examples can be referred to Supplementary Materials.
[14, 35, 41, 47, 22, 20] enjoy
CNN-based frameworks lightweight and fast inferences. These approaches oper-ated on a sequence of short snippets (known as segments) sparsely sampled from the entire video and were initially in-troduced in TSN [41]. Original 2D CNNs lack the ability of temporal modeling, which causes losing essential sequen-tial information in some actions e.g., ‘opening a box’ vs
‘closing a box’. TSM [22] introduced temporal information to 2D CNN-based frameworks by shifting a part of chan-nels on the temporal axis, which signiﬁcantly improved the baseline for 2D CNN-based frameworks. However, TSM still lacks explicit temporal modeling for an action, such as motion information. Recent works [19, 13, 21, 24, 25] introduced embedded modules into 2D CNNs in terms of
ResNet architecture [9], which possessed the capability for motion modeling. In order to capture multi-type informa-tion contained by videos, previous works normally oper-ated on input-level frames. For instance, SlowFast net-works sampled raw videos at multiple rates to character-ize slow and fast actions; two-stream networks utilized pre-computed optical ﬂow for reasoning motion information.
This kind of approaches commonly require multi-branch networks, which need expensive computations.
Inspired by the aforementioned observation, we pro-pose a new plug-and-play and lightweight spAtio-temporal,
Channel and moTion excitatION (ACTION) module to ef-fectively process the multi-type information on the feature level inside a single network by adopting multipath exci-tation. The combination of spatio-temporal features and motion features can be understood similarly as the two-stream architecture [35], but we model the motion inside the network based on the feature level rather than generat-ing another type of input (e.g., optical ﬂow [11]) for train-ing the network, which signiﬁcantly reduces computations.
Inspired by SENet, the channel-wise features are extracted based on the temporal domain to characterize the channel interdependencies for the network. Correspondingly, a neu-ral architecture equipped with such a module is dubbed
ACTION-Net. ACTION comprises three components for extracting aforementioned features (1) Spatio-Temporal Ex-citation (STE), (2) Channel Excitation (CE) and (3) Motion
Excitation (ME). Figure 1 visualizes features characterized by TSN, TSM, and ACTION-Net for the action ‘rotate ﬁsts counterclockwise’. It can be observed that both TSN and
TSM focus on recognizing objects (two ﬁsts) independently instead of reasoning an action. Compared to TSN and TSM, our proposed ACTION-Net better characterizes an action by representing feature maps that cover the two ﬁsts, espe-cially for the highlighted 4th and 5th columns. In a nutshell, our contributions are three-fold:
• We propose an ACTION module that works in a plug-and-play manner, which is able to extract appropriate spatio-temporal patterns, channel-wise features, and motion information to recognize actions. 13215
• A simple yet effective neural architecture referred to
ACTION-Net. We test it on three backbones (ResNet-50 [9], BNIception [12] and MobileNet V2 [31]).
• We conducted extensive experiments and shown our superior performances on three datasets Something-Something V2 [7], Jester [27] and EgoGesture [46]. 2.