Abstract 1.

Introduction
Most 3D face reconstruction methods rely on 3D mor-phable models, which disentangle the space of facial de-formations into identity and expression geometry, and skin reﬂectance. These models are typically learned from a lim-ited number of 3D scans and thus do not generalize well across different identities and expressions. We present the
ﬁrst approach to learn complete 3D models of face identity and expression geometry, and reﬂectance, just from images and videos. The virtually endless collection of such data, in combination with our self-supervised learning-based ap-proach allows for learning face models that generalize be-yond the span of existing approaches. Our network design and loss functions ensure a disentangled parameterization of not only identity and albedo, but also, for the ﬁrst time, an expression basis. Our method also allows for in-the-wild monocular reconstruction at test time. We show that our learned models better generalize and lead to higher quality image-based reconstructions than existing approaches. We show that the learned model can also be personalized to a video, for a better capture of the geometry and albedo.
Monocular 3D face reconstruction is deﬁned as recover-ing the dense 3D facial geometry and skin reﬂectance of a face from a monocular image. It has applications in several domains such as VR/AR, entertainment, medicine, and hu-man computer interaction [65, 16]. We are concerned with in-the-wild images which can include faces of many differ-ent identities with varied expressions and poses, in uncon-strained environments with widely different illumniation.
This problem has been well-studied, where a lot of success can be owed to the emergence of 3D Morphable Models [5].
These models deﬁne the space of deformations for faces as separate disentangled models such as facial identity, expres-sion and reﬂectance. They are widely used in the literature to limit the search space for reconstruction [65, 16]. How-ever, these models are often learned from a limited number of 3D scans, which constrains their generalizability to sub-jects and expressions outside the space of the scans.
Recent efforts have proposed to learn face models with better generalizability from internet images or videos [55, 56, 58, 59, 60]. However, learning from in-the-wild data is highly challenging, requiring solutions for handling the strong inherent ambiguities and for ensuring disentangle-ment between different components of the reconstruction. 3361
Some approaches deal with a slightly easier problem of re-ﬁning an initial morphable model pretrained on 3D data on in-the-wild imagery [56, 59, 61, 60, 58]. Our objective is to learn face models without using any pretrained models to start with. The closest approach to ours is Tewari et al. [55], which learns only the models of facial identity geometry and reﬂectance from in-the-wild videos. However, they still use a pretrained expression model to help disentangle the identity and expression variations in geometry. We present the ﬁrst approach that learns the the complete face model of identity geometry, albedo and expression just from in-the-wild videos. We start just from a template face mesh with-out using any priors about deformations of the face, other than smoothness. This also makes ours the ﬁrst approach to learn face expression models from 2D data.
We achieve this through several technical contributions.
We design a neural network architecture which, in combi-nation with specially tailored self-supervised loss functions, enables (1) learning of face identity, expression and skin re-ﬂectance models, as well as (2) joint 3D reconstruction of faces from monocular images at state-of-the-art accuracy.
We use a siamese network architecture which can process multiple frames of video during training, enabling consis-tent identity reconstructions along with per-frame expres-sions and scene parameters. We use a differentiable ren-derer to render synthetic images of the network’s recon-structions. To compare reconstructions to the input, we use a new combination of appearance-based and face segmen-tation losses that permit learning of the face geometry and appearance, as well as a high-quality expression basis of detailed mouth and lip motion. Our novel lip segmenta-tion consistency loss aligns the lip region in 3D with 2D segmentations. Our loss is robust to noisy outliers, lead-ing to qualitatively better lip segmentations than the ground truth used. We also introduce a disentanglement loss which ensures that the expression component of a reconstructed mesh is small when the input image contains a neutral face.
We show that the combination of these innovations is cru-cial to learn a full face model with proper component disen-tanglement from in-the-wild imagery. Our monocular re-construction outperforms the state-of-the-art image-based face reconstruction methods.
In summary we make the following contributions: 1) the
ﬁrst approach for learning all components - identity, albedo and expression bases - of a morphable face model, trained on in-the-wild 2D data, 2) the ﬁrst approach to learn 3D expression models of faces in a self-supervised manner, 3) a lip segmentation consistency loss to enforce accurate mouth modeling and reconstruction, 4) enforcing disentanglement of identity and expression geometry by utilizing a dataset of neutral images. 2.