Abstract
Deep neural networks suffer from the major limitation of catastrophic forgetting old tasks when learning new ones.
In this paper we focus on class incremental continual learn-ing in semantic segmentation, where new categories are made available over time while previous training data is not retained. The proposed continual learning scheme shapes the latent space to reduce forgetting whilst improving the recognition of novel classes. Our framework is driven by three novel components which we also combine on top of existing techniques effortlessly. First, prototypes matching enforces latent space consistency on old classes, constrain-ing the encoder to produce similar latent representation for previously seen classes in the subsequent steps. Second, features sparsiﬁcation allows to make room in the latent space to accommodate novel classes. Finally, contrastive learning is employed to cluster features according to their semantics while tearing apart those of different classes. Ex-tensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the effectiveness of our approach, signiﬁcantly outperforming state-of-the-art methods. 1.

Introduction
Semantic segmentation is a challenging computer vision problem with many real-world applications ranging from robot sensing, to autonomous driving, video surveillance, virtual reality, and many others. For most applications, con-tinuously improving the set of classes to be distinguished is a fundamental requirement. Current state-of-the-art seman-tic segmentation approaches are typically based on auto-encoder structures and on fully convolutional models [38] that are trained in a single-shot requiring all the dataset to be available at once. Indeed, existing architectures are not designed to incrementally update their inner classiﬁcation model to accommodate new categories. This issue is well-known for deep neural networks and it is called catastrophic forgetting [41, 18, 20], as deep architectures fail to update current	batch	prototype cumulative	prototype attractive	force	(clustering) attractive	force	(proto.	distillation) repulsive	force unconstrained	latent	space	span
Figure 1. Our continual learning scheme is driven by 3 main com-ponents: latent contrastive learning, prototypes matching and fea-tures sparsity. Latent representations of old classes are preserved via prototypes matching and clustering, whilst also making room for accommodating new classes via sparsity and repulsive force of contrastive learning. The decoder preserves previous knowledge via output-level distillation. In the ﬁgure, bike and cars represent old classes and leave more space to new classes (the dog) thanks to the novel constraints (green dotted ovals versus gray-ﬁlled ovals). their parameters for learning new categories while preserv-ing good performance on the old ones.
Continual learning has been widely studied in image classiﬁcation [32, 36] and object detection [56, 34], while has been tackled only recently in the semantic segmenta-tion ﬁeld [42, 58, 4, 33]. In this paper, we investigate class-incremental continual learning in semantic segmentation.
Differently from the majority of previous approaches both in image classiﬁcation [36, 51, 3] and semantic segmen-tation [58, 42, 4, 33], we do not mainly or solely rely on output-level knowledge distillation. In this work, we focus on latent space organization which has been only marginally investigated in the current literature, and we empirically prove it to be complementary to other existing techniques.
The main idea is depicted in Fig. 1, where some of the latent space constraints are introduced. First, a prototype match-ing is devised to enforce features extraction consistency on old classes between the cumulative prototype computed us-ing all previous samples and the current prototype (i.e., the
In other prototype computed on the current batch only). 1114
words, we force the encoder to produce similar latent rep-resentations for previously seen classes in the new steps.
Second, a features sparsiﬁcation constraint makes room in the latent space to accommodate novel classes. To fur-ther regularize the latent space, we introduce an attraction-repulsion rule similar in spirit to the recent advancements in contrastive learning. Finally, to enforce the decoder to pre-serve discriminability on previous categories during classi-ﬁcation, we employ a targeted output-level distillation.
Although continual semantic segmentation has only been faced recently, it already comes with different experimen-tal protocols depending on how the incremental data are considered (see Section 3.1): namely, sequential (new im-ages are labeled with both new and old classes), disjoint (new images are labeled with only new classes, old classes are assigned to the background) and overlapped (new im-ages are labeled with only new classes, images are repeated across training steps with different semantic maps associ-ated to them). In this paper we devise a common framework which allows to tackle all these scenarios and can be applied in combination with previous techniques, which has never been attempted before. We evaluate on standard seman-tic segmentation datasets, like Pascal VOC2012 [16] and
ADE20K [76], in many scenarios.
Summing up, the main contributions of this work are: 1)
We investigate class-incremental learning in semantic seg-mentation, providing a common framework for different ex-perimental protocols. 2) We explore the latent space organi-zation and we propose complementary techniques with re-spect to the existing ones. 3) We propose novel knowledge preservation techniques based on prototypes matching, con-trastive learning and features sparsity. 4) We benchmark our approach on standard semantic segmentation datasets out-performing state-of-the-art continual learning methods. 2.