Abstract
We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer, and can reconstruct geometry, materials, and illumination from scratch from a set of images. Our framework represents specular BRDFs and environmental illumination using mix-tures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer
Perceptron. The use of spherical Gaussians allows us to efﬁciently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reﬂectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our re-constructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination. 1.

Introduction
Vision as inverse graphics has long been an intriguing concept. Solving inverse rendering problems, i.e., recov-ering shape, material and lighting from images, has thus been a long-standing goal. Recently, neural rendering meth-ods [46, 54, 29, 31, 25, 55, 21, 32, 28, 44, 43, 35, 4, 47], have drawn signiﬁcant attention due to their remarkable suc-cess in a range of problems, including shape reconstruction, novel view synthesis, non-physically-based relighting, and surface reﬂectance map estimation. These neural rendering methods adopt scene representations that are either physical, neural, or a mixture of both, along with a neural-network-based renderer. Methods that reconstruct textures or radiance
ﬁelds [25, 54, 31] work well for the task of interpolating novel views, but do not factorize appearance into lighting and materials, precluding physically-based appearance ma-nipulation like material editing or relighting.
*Authors contributed equally to this work.
†Project page: https://kai- 46.github.io/PhySG-website/.
Figure 1: PhySG performs physics-based inverse rendering by taking as input multi-view images of a static glossy object under static natural illumination and jointly optimizes for geometry (rep-resented by an SDF), material BRDF and environment maps (both represented by a mixture of spherical Gaussians), which can then be used for novel view synthesis, relighting and material editing.
Prior multi-view inverse rendering methods assume
RGBD input [35, 4] or varying illumination across input images achieved either by co-locating an active ﬂashlight with moving cameras [7, 8, 40, 30] or capturing objects on a turntable with a ﬁxed camera [51, 10]. Learning-based single-view methods that recover shape, illumination, and material properties have also been proposed [20, 5].
In this work, we tackle the multi-view inverse render-ing problem under the challenging setting of normal RGB input images sharing the same static illumination, with-out assuming scanned geometry. To this end, we propose
PhySG, an end-to-end physically-based differentiable ren-dering pipeline to jointly estimate lighting, material, geom-etry and surface normals from posed multi-view images of specular objects. In our pipeline, we represent shape using signed distance functions (SDFs), building on their success-ful use in recent work [54, 15, 34, 24, 58]. Additionally, a key component of our framework is our use of spheri-cal Gaussians to approximate lighting and specular BRDFs allowing for efﬁcient approximate evaluation of light trans-port [53]. From 2D images alone, our method jointly re-constructs shape, illumination, and materials and allows for 5453
subsequent physics-based appearance manipulations such as material editing and relighting.
In summary, our contributions are as follows:
• PhySG, an end-to-end inverse rendering approach to this problem of jointly estimating lighting, material properties, and geometry from multi-view images of glossy objects under static illumination. Our pipeline utilizes spherical Gaussians to approximately and efﬁ-ciently evaluate the rendering equation in closed form.
• Compared to prior neural rendering approaches, we show that PhySG not only generalizes to novel view-points, but also enables physically-intuitive material editing and relighting. 2.