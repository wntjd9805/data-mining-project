Abstract
Effective regularization techniques are highly desired in deep learning for alleviating overﬁtting and improving gen-eralization. This work proposes a new regularization scheme, based on the understanding that the ﬂat local minima of the empirical risk cause the model to generalize better. This scheme is referred to as adversarial model perturbation (AMP), where instead of directly minimizing the empiri-cal risk, an alternative “AMP loss” is minimized via SGD.
Speciﬁcally, the AMP loss is obtained from the empirical risk by applying the “worst” norm-bounded perturbation on each point in the parameter space. Comparing with most existing regularization schemes, AMP has strong the-oretical justiﬁcations, in that minimizing the AMP loss can be shown theoretically to favour ﬂat local minima of the empirical risk. Extensive experiments on various modern deep architectures establish AMP as a new state of the art among regularization schemes. Our code is available at https://github.com/hiyouga/AMP-Regularizer. 1.

Introduction
To date, the generalization behaviour of deep neural net-works is still a mystery, despite some recent progress (see, e.g., [1, 3, 19, 24, 26, 46, 48]). A commonly accepted and empirically veriﬁed understanding in this regard is that the model parameter that corresponds to a ﬂat minimum of the empirical risk tends to generalize better. For example, the au-thors of [19, 26] argue that ﬂat minima correspond to simple models, which are less likely to overﬁt. This understanding has inspired great effort studying factors in the optimization process (such as learning rate and batch size) that impacting the ﬂatness of the found minima [14, 20, 25] so as to better understand the generalization behaviour of deep networks.
Meanwhile developing effective regularization techniques remains as the most important approach in practice to allevi-ate overﬁtting and force model towards better generalization
*Corresponding author
Figure 1: An example showing an empirical risk curve (left) and its corresponding AMP loss curve (right, blue). (e.g., [16, 28, 29, 39, 41, 49]). Some recent research in fact suggests that the effectiveness of certain regularization tech-niques is due to their ability to ﬁnd ﬂatter minima [22, 44].
Additionally, there have been signiﬁcant research ad-vances in recent years in developing more effective regu-larization schemes, which include, for example, MixUp,
Flooding [22, 49]. Despite their great success, these tech-niques usually fall short of strong principles or theoretical justiﬁcations. Thus one expects more principled and more powerful regularization schemes are yet to be discovered.
This work sets out to develop a powerful regularization scheme under the principle of ﬁnding ﬂat local minima of the empirical risk. To that end, we propose a novel regular-ization scheme which can be strongly justiﬁed in terms of its ability to ﬁnding ﬂat minima. This scheme is referred to as Adversarial Model Perturbation or AMP, where instead of minimizing the empirical risk LERM(θ) over model pa-rameter θ, it minimizes an alternative “AMP loss”. Brieﬂy, the AMP loss LAMP(θ) at a parameter setting θ is the worst (or highest) empirical risk of all perturbations of θ with the perturbation norm no greater than a small value ǫ, namely,
LAMP(θ) := max
∆:k∆k≤ǫ
LERM(θ + ∆) (1)
To see why minimizing the AMP loss provides opportu-nities to ﬁnd ﬂat local minima of the empirical risk, consider the example in Figure 1. Figure 1 (left) sketches an empirical risk curve LERM, which contains two local minima, a sharp one on the left and a ﬂat one on the right. The process of ob-taining the AMP loss from the empirical risk can be seen as 8156
Figure 2: Landscapes of the empirical risks obtained from the PreActResNet18 [18] models trained with ERM (red) and AMP (blue) on CIFAR-10. Left: on training set; right: on test set. These curves are computed using the technique presented in [31], where δ indicates a random direction and
α is a displacement in that direction. a “max-pooling” operation, which slides a window of width 2ǫ (in high dimension, more precisely, a sphere with radius
ǫ) across the parameter space and, at each location, returns the maximum value inside the window (resp. sphere). The resulting AMP loss is shown as the blue curve in Figure 1 (right). Since the right minimum in the AMP loss is lower than the left one, minimizing the AMP loss gives the right minimum as its solution.
In this paper, we formally analyze the AMP loss mini-mization problem and its preference of ﬂat local minima.
Speciﬁcally, we show that this minimization problem im-plicitly uses the “narrowest width” of a local minimum as a notion of ﬂatness, and tries to penalize the minima that are not ﬂat in this sense.
We derive a mini-batch SGD algorithm for solving this minimization problem, which gives rise to the proposed
AMP regularization scheme. Interestingly, we show that this algorithm can also be seen as the regular empirical risk minimization with an additional penalty term on the gradient norm. This provides an alternative justiﬁcation of the AMP scheme. Figure 2 contains an experimental result suggesting that AMP indeed selects ﬂatter minima than ERM does.
We conduct experiments on several benchmark image classiﬁcation datasets (SVHN, CIFAR-10, CIFAR-100) to validate the effectiveness of the proposed AMP scheme.
Compared with other popular regularization schemes, AMP demonstrates remarkable regularization performance, estab-lishing itself as a new state of the art.
Our contributions can be summarized as follows. 1) Motivated by the understanding that ﬂat minima help generalization, we propose adversarial model perturbation (AMP) as a novel and efﬁcient regularization scheme. 2) We theoretically justify that AMP is capable of ﬁnding
ﬂatter local minima, thereby improving generalization. 3) Extensive experiments on the benchmark datasets demonstrate that AMP achieves the best performance among the compared regularization schemes on various modern neural network architectures. 2.