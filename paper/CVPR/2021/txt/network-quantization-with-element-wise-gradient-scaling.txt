Abstract
Network quantization aims at reducing bit-widths of weights and/or activations, particularly important for im-plementing deep neural networks with limited hardware resources. Most methods use the straight-through esti-mator (STE) to train quantized networks, which avoids a zero-gradient problem by replacing a derivative of a dis-cretizer (i.e., a round function) with that of an identity func-tion. Although quantized networks exploiting the STE have shown decent performance, the STE is sub-optimal in that it simply propagates the same gradient without consider-ing discretization errors between inputs and outputs of the discretizer. In this paper, we propose an element-wise gra-dient scaling (EWGS), a simple yet effective alternative to the STE, training a quantized network better than the STE in terms of stability and accuracy. Given a gradient of the discretizer output, EWGS adaptively scales up or down each gradient element, and uses the scaled gradient as the one for the discretizer input to train quantized networks via backpropagation. The scaling is performed depending on both the sign of each gradient element and an error between the continuous input and discrete output of the discretizer.
We adjust a scaling factor adaptively using Hessian infor-mation of a network. We show extensive experimental re-sults on the image classiﬁcation datasets, including CIFAR-10 and ImageNet, with diverse network architectures under a wide range of bit-width settings, demonstrating the effec-tiveness of our method. 1.

Introduction
Convolutional neural networks (CNNs) have shown re-markable advances in many computer vision tasks, such as image classiﬁcation [16, 24, 36], semantic segmenta-tion [15, 28], object detection [13, 27], and image restora-tion [9], while at the cost of large amounts of weights and operations. Network quantization lowers bit-precision of weights and/or activations in a network. It is effective in particular to reduce the memory and computational cost of
∗Corresponding author. (a) Gradient propagation using STE [3]. (b) Gradient propagation using EWGS.
Figure 1: Comparison of STE [3] and EWGS. We visualize dis-crete levels and a loss landscape by straight lines and a contour plot, respectively. In a forward pass, a continuous latent point xn is mapped to a discrete point xq using a round function. Training a quantized network requires backpropagating a gradient from xq to xn. (a) The STE propagates the same gradient i.e., Gxn = Gxq without considering the value of xn, where we denote by Gxn and Gxq the gradients of xn and xq, respectively. (b) Our ap-proach, on the other hand, scales up or down each element of the gradient during backpropagation, while taking into account dis-cretization errors i.e., xn − xq. (Best viewed in color.)
CNNs, and thus network quantization could be a potential solution for implementing CNNs with limited hardware re-sources. For example, binarized neural networks [19, 34] use 32 less memory compared to the full-precision (32-bit) counterparts, and the binarization techniques allow to replace multiplication and addition with XNOR and bit-count operations, respectively.
×
Quantized networks involve weight and/or activation quantizers in convolutional or fully-connected layers. The quantizers take full-precision weights or activations, and typically perform normalization, discretization, and denor-6448
malization steps to convert them into low-precision ones.
The main difﬁculty of training a quantized network arises from the discretization step, where a discretizer (i.e., a round function) maps a normalized value to one of dis-crete levels. Since an exact derivative of the discretizer is either zero or inﬁnite, gradients become zero or ex-plode during backpropagation. Most quantization meth-ods [7, 12, 21, 31, 42, 44] overcome this issue by exploiting the straight-through estimator (STE) [3]. The STE propa-gates the same gradient from an output to an input of the discretizer, assuming that the derivative of the discretizer is equal to 1. This could bring a gradient mismatch prob-lem [39], since the discretizer used in a forward pass (i.e., the round function) does not match up with that in a back-ward pass (i.e., an identity or hard tanh functions). Nev-ertheless, recent methods exploiting the STE have shown reasonable performance [4, 12, 21, 31].
We take a different point of view on how the STE works.
We interpret that a full-precision input (which we call a “la-tent value”) of the discretizer moves in a continuous space, and a discretizer output (which we call a “discrete value”) is determined by projecting the latent value to the nearest discrete level in the space. This suggests that shifting the latent values in the continuous space inﬂuences the discrete values. The STE, in this sense, shifts (or updates) the la-tent values with coarse gradients [41], that is, the gradients obtained with the discrete values (Fig. 1a), which is sub-optimal. For example, both latent values of 0.51 and 1.49 produce the same discrete value of 1 using a round func-tion, and the STE forces to update the latent values equally with the same gradient from the discrete value of 1, regard-less of their discretization errors induced by the rounding.
Updating these latent values should be treated differently, because, for example, a small increment for the latent value of 1.49 leads to changing the discrete value from 1 to 2, whereas the increment for the latent value of 0.51 cannot.
Similarly, a small decrement for the latent value of 0.51 can convert the discrete value from 1 to 0, but the latent value of 1.49 requires a much larger decrement to do so.
In this paper, we present an element-wise gradient scal-ing (EWGS) that enables better training of a quantized net-work, compared with the STE, in terms of stability and ac-curacy. Given a gradient of discrete values, EWGS adap-tively scales up or down each element of the gradient con-sidering its sign and discretization errors between latent and discrete values. The scaled gradient is then used to update the latent value (Fig. 1b). Since optimal scaling factors, which control the extent of EWGS, may vary across weight or activation quantizers in different layers, we propose an approach to adjusting the factors adaptively during training.
Speciﬁcally, we relate the scaling factor with the second-order derivatives of a task loss w.r.t the discrete values, and propose to estimate the factor with the trace of a Hessian matrix, which can be computed efﬁciently with the Hutchin-son’s method [1, 40]. Without an extensive hyperparam-eter search, training schedules [39, 43, 47], or additional modules [6, 30, 46], various CNN architectures trained with our approach achieve state-of-the-art performance on Ima-geNet [8]. Note that the STE is a special case of EWGS, in-dicating that it can be exploited to other quantization meth-ods using the STE. The main contributions of our work can be summarized as follows:
•
•
•
We introduce EWGS that scales up or down each gradi-ent element of the discrete value adaptively for backprop-agation, while considering discretization errors between inputs and outputs of a discretizer.
We relate a scaling factor with the second-order deriva-tives of a loss function w.r.t discrete values, allowing to compute the parameter effectively and adaptively with the Hessian information of a quantized network.
We demonstrate the effectiveness of our method with var-ious CNN architectures under a wide range of bit-widths, outperforming the state of the art on ImageNet [8]. We also verify that our approach boosts the performance of other quantization methods, such as DoReFa-Net [44] and PROFIT [31].
Our code and models are available online: https:// cvlab.yonsei.ac.kr/projects/EWGS. 2.