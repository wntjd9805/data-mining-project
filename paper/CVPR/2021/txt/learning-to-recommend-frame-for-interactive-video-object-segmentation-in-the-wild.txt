Abstract
This paper proposes a framework for the interactive video object segmentation (VOS) in the wild where users can choose some frames for annotations iteratively. Then, based on the user annotations, a segmentation algorithm reﬁnes the masks. The previous interactive VOS paradigm selects the frame with some worst evaluation metric, and the ground truth is required for calculating the evaluation met-ric, which is impractical in the testing phase. In contrast, in this paper, we advocate that the frame with the worst eval-uation metric may not be exactly the most valuable frame that leads to the most performance improvement across the video. Thus, we formulate the frame selection problem in the interactive VOS as a Markov Decision Process, where an agent is learned to recommend the frame under a deep reinforcement learning framework. The learned agent can automatically determine the most valuable frame, making the interactive setting more practical in the wild. Experi-mental results on the public datasets show the effectiveness of our learned agent without any changes to the underlying
VOS algorithms. Our data, code, and models are available at https://github.com/svip-lab/IVOS-W . 1.

Introduction
Video object segmentation aims to segment the objects of interest in a video sequence. It has been widely applied to many downstream applications such as video editing and object tracking. Recently, DAVIS dataset [24, 25] and
YouTube-VOS dataset [36] are introduced and signiﬁcantly drive forward this task. However, collecting such densely-annotated datasets is expensive and time-consuming. For
∗Corresponding author.
The worst frame  (!&ℱ = 37.56%)
VOS
VOS
↑ !&ℱ = 18.31%
Video frames with  segmentation masks
The most valuable frame  (!&ℱ = 47.14%)
↑ !&ℱ = 20.57%
Figure 1. The frame with the worst segmentation quality vs. the most valuable frame in a single round. The frame with the worst segmentation quality only improves the performance of VOS by 18.31 %, while the most valuable one improves the performance by 20.57 %. example, labeling a single object in one frame of DAVIS dataset requires more than 100 seconds [2], ﬁnally resulting in either limited sizes [24, 25] or coarse annotations [36] in the existing VOS datasets.
To minimize the human efforts, Caelles et al. [2] intro-duces a human-in-the-loop VOS setting, i.e., the interactive
VOS with scribble supervision. Speciﬁcally, the interactive
VOS algorithm will predict an initial segmentation mask for each frame based on the initial scribbles provided by a user.
It will then gradually reﬁne the segmentation masks with additional scribbles of some frames selected by the user, who may evaluate the result by the segmentation quality be-tween the predictions and the ground truths. Whereas, the ground-truth segmentation masks are not available in prac-tice, so the user cannot select a potential frame based on the segmentation quality. Further, the frame with the worst segmentation quality may not be exactly the most valuable one contributing the most to the reﬁnement performance, as shown in Figure 1. In this paper, we claim that the most valuable frame is not necessarily the one with the worst seg-mentation quality for the interactive VOS task. 15445
This paper aims to develop a criterion for determining the worthiness of the frame. The worthiness of a frame re-ﬂects how much it can improve the segmentation perfor-mance across the video sequence if it is selected to provide additional scribbles. However, measuring the worthiness is difﬁcult due to the complexity and variety of videos and un-certainties in the reﬁnement process. To this end, we formu-late the frame recommendation problem as a Markov Deci-sion Process (MDP) and train the recommendation agent with Deep Reinforcement Learning (DRL). To narrow the state space, we deﬁne the state as the segmentation quality of each frame instead of the image frames and segmentation masks. We also include the recommendation history of each frame in the state. Inspired by Mask Scoring R-CNN [15], we use a segmentation quality assessment module to esti-mate the segmentation quality. Given the user scribbles on the recommended frame, we leverage the off-the-shelf in-teractive VOS algorithms [12, 19, 21] to reﬁne the segmen-tation masks. Without any ground-truth information, the learned agent can recommend the frame. To further eval-uate the ability of generalization of our agent, we follow the DAVIS dataset [2] to extend a subset of YouTube-VOS dataset [36] with initial scribbles. The experimental re-sults show that our learned agent outperforms all baseline frame selection strategies on DAVIS and YouTube-VOS dataset without any changes to underlying VOS algorithms, whether the ground truth is available or not.
In summary, our contributions are as follows: (i) We demonstrate that the frame used in the current interactive
VOS paradigm, i.e., the frame with the worst segmenta-tion quality, for user annotation is not the best one. (ii)
We propose a novel deep reinforcement recommendation agent for interactive VOS, where the agent recommends the most valuable frame for user annotation. The agent does not require any ground-truth information in the test-ing phase, therefore it is more practical. (iii) Following the interactive VOS setting [2], we extend a subset of YouTube-VOS dataset [36] with initial scribbles for performance evaluation. (iv) Extensive experiments on the challenging datasets, namely DAVIS dataset and YouTube-VOS dataset, validate the effectiveness of our proposed method. 2.