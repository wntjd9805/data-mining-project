Abstract
This paper deﬁnes a new visual reasoning paradigm by introducing an important factor, i.e. transformation. The motivation comes from the fact that most existing visual rea-soning tasks, such as CLEVR in VQA, are solely deﬁned to test how well the machine understands the concepts and relations within static settings, like one image. We argue that this kind of state driven visual reasoning approach has limitations in reﬂecting whether the machine has the ability to infer the dynamics between different states, which has been shown as important as state-level reasoning for human cognition in Piaget’s theory. To tackle this prob-lem, we propose a novel transformation driven visual rea-soning task. Given both the initial and ﬁnal states, the target is to infer the corresponding single-step or multi-step transformation, represented as a triplet (object, at-tribute, value) or a sequence of triplets, respectively. Fol-lowing this deﬁnition, a new dataset namely TRANCE is constructed on the basis of CLEVR, including three lev-els of settings, i.e. Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transfor-mation with variant views). Experimental results show that the state-of-the-art visual reasoning models perform well on Basic, but are still far from human-level intelligence on
Event and View. We believe the proposed new paradigm will boost the development of machine visual reasoning.
More advanced methods and real data need to be investi-gated in this direction. The resource of TVR is available at https://hongxin2019.github.io/TVR. 1.

Introduction
Visual reasoning is the process of solving problems on the basis of analyzing the visual information, which goes well beyond object recognition [11, 20, 30, 31]. Though
*Corresponding author.
Figure 1. Illustration of three settings in TRANCE. Basic: Find the single-step transformation between the initial and ﬁnal state.
Event: Find the multi-step transformation between two states.
View: Similar to Event but the view angle of the ﬁnal state is randomly chosen from Left, Center (Default), and Right. the task is easy for the human, it is tremendously difﬁcult for vision systems, because it usually requires higher-order cognition and reasoning about the world. In recent years, several visual reasoning tasks have been proposed and at-tract a lot of attention in the community of computer vi-sion, machine learning, and artiﬁcial intelligence. For ex-ample, the most representative visual question answering (VQA) tasks, such as CLEVR [18], deﬁne a question an-swering paradigm to test whether machines have spatial, relational, and other reasoning abilities for a given image.
Visual entailment tasks such as NLVR [32, 33] ask models to determine whether a sentence is true about the states of two images. Visual commonsense reasoning tasks, such as
VCR [42], further require the model to provide a rationale explaining why its answer is right.
We can see that these visual reasoning tasks are all de-ﬁned at state level. For example, the language descriptions in NLVR as well as the questions and answers in VQA and VCR are just related to the concepts or relations within states, i.e. an image, or two images. We argue that this kind of state driven visual reasoning fails to test the ability of reasoning dynamics between different states. Take two im-ages as an example. In the ﬁrst image, there is a cat on a 6903
tree, and in the second image, the same cat is under the tree.
It is natural for a human to reason that the cat jumps down the tree after analyzing the two images. Piaget’s cognitive development theory [29] describes the dynamics between states as transformation, and tells that human intelligence must have functions to represent both the transformational and static aspects of reality.
In addition, transformation is the key to tackle some more complicated tasks such as storytelling [13] and visual commonsense inference [28].
Though these tasks are closer to reality, they are too com-plicated to serve as a good testbed for transformation based reasoning. Because many other factors like representation and recognition accuracy may have some effects on the per-formance. Therefore, it is crucial to deﬁne a speciﬁc task to test the transformation reasoning ability.
In this paper, we deﬁne a novel transformation driven visual reasoning (TVR) task. Given the initial and ﬁnal states, like two images, the goal is to infer the correspond-ing single-step or multi-step transformation. Without loss of generality, in this paper, transformations indicate changes of object attributes, so a single-step and multi-step transforma-tion are represented as a triplet (object, attribute, value) and a sequence of triplets, respectively.
Following the deﬁnition of TVR, we construct a new dataset called TRANCE (Transformation on CLEVR), to test and analyze how well machines can understand the transformation. TRANCE is a synthetic dataset based on
CLEVR [18], since it is better to ﬁrst study TVR in a simple setting and then move to more complex real scenarios, just like people ﬁrst study VQA on CLEVR and then generalize to more complicated settings like GQA. CLEVR has de-ﬁned ﬁve types of attributes, i.e. color, shape, size, material, and position. Therefore, it is convenient to deﬁne the trans-formation for each attribute, e.g. the color of an object is changed from red to blue. Given the initial and ﬁnal states, i.e. two images, where the ﬁnal state is obtained by apply-ing a single-step or multi-step transformation on the initial state, a learner is required to well infer such transformation.
To facilitate the test for different reasoning levels, we de-sign three settings, i.e. Basic, Event, and View. Basic is designed for testing single-step transformation. Event and
View are designed for more complicated multi-step trans-formation reasoning, where the difference is that View fur-ther considers variant views in the ﬁnal state. Figure 1 gives an example of three different settings.
In the experiments, we would like to test how well existing reasoning techniques [14, 19] work on this new task. However, since these models are mainly designed for existing reasoning tasks, they cannot be directly ap-plied to TRANCE. To tackle this problem, we propose a new encoder-decoder framework named TranceNet, specif-ically for TVR. With TranceNet, existing techniques can be conveniently adapted to TVR. We test several differ-ent encoders, e.g. ResNet [12], Bilinear-CNN [22] and
DUDA [27]. While for the decoder, an adapted GRU [6] network is used to employ the image features and additional object attributes from TRANCE to predict the transforma-tion, which is a sequence of triplets. Experimental results show that deep models perform well on Basic, but are far from human’s level on Event and View, demonstrating high research potentials in this direction.
In summary, the contributions of our work include: 1) the deﬁnition of a new visual reasoning paradigm, to learn the dynamics between different states, i.e. transformation; 2) the proposal of a new dataset called TRANCE, to test three levels of transformation reasoning, i.e. Basic, Event, and View; 3) experimental studies of the existing SOTA rea-soning techniques on TRANCE show the challenges of the
TVR and some insights for future model designs. 2.