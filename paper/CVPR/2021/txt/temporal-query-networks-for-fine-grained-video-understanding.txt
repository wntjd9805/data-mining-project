Abstract
Our objective in this work is ﬁne-grained classiﬁcation of actions in untrimmed videos, where the actions may be tem-porally extended or may span only a few frames of the video.
We cast this into a query-response mechanism, where each query addresses a particular question, and has its own re-sponse label set. (i) We pro-We make the following four contributions: pose a new model—a Temporal Query Network—which enables the query-response functionality, and a structural understanding of ﬁne-grained actions.
It attends to rel-evant segments for each query with a temporal attention mechanism, and can be trained using only the labels for each query. (ii) We propose a new way—stochastic fea-ture bank update—to train a network on videos of vari-ous lengths with the dense sampling required to respond (iii) we compare the TQN to to ﬁne-grained queries. other architectures and text supervision methods, and an-alyze their pros and cons. Finally, (iv) we evaluate the method extensively on the FineGym and Diving48 bench-marks for ﬁne-grained action classiﬁcation and surpass the state-of-the-art using only RGB features. Project page: https://www.robots.ox.ac.uk/~vgg/research/tqn/. 1.

Introduction
Imagine that you wish to answer particular questions about a video. These questions could be quite general, e.g., “what instrument is being played?”, quite speciﬁc, e.g., “do people shake hands?”, or require a composite answer, e.g., “how many somersaults, if any, are performed in this video, and where?”. Answering these questions will in general re-quire attending to the entire video (to ensure that nothing is missed), and the response is query dependent. Further, the response may depend on only a very few frames where a subtle action occurs. With such video understanding ca-pability, it is possible to effortlessly carry out regular video metrology such as performance evaluation in sports train-ing, or issuing reports on video logs.
Figure 1. Coarse vs. ﬁne-grained action recognition. Top: Ob-ject and background cues from only a few frames can inform clas-sic coarse-grained action recognition in datasets like Kinetics [27], where visually distinct activities are to be distinguished. Bottom:
However, for ﬁner-grain classiﬁcation which depends on subtle differences in pose, the speciﬁc sequence, duration and number of certain sub-actions, as for the gymnastics sequence above, requires reasoning about events at varying temporal scales and attention to
ﬁne details. We develop a novel query-based video network and a training framework for such ﬁne-grained temporal reasoning.
The objective of this paper is a network and training frame-work that will enable questions of various granularity to be answered on a video. Speciﬁcally, we consider untrimmed videos and train with weak supervision, meaning that at training time we are not provided with the temporal local-ization information for the response. To this end, we intro-duce a new Transformer-based [56] video network architec-ture, the Temporal Query Network (TQN), for ﬁne-grained action classiﬁcation. The TQN ingests a video and a pre-deﬁned set of queries and outputs responses for each query, where the response is query dependent.
The queries act as ‘experts’ that are able to pick out from the video the temporal segments required for their response.
Since the temporal position of the response is unknown, they must examine the entire duration of the video and be able to ignore irrelevant content, in a similar manner to a
‘matched ﬁlter’ [54]. Furthermore, since the duration of re-sponse segments may only be a few frames, excessive tem-poral aggregation (for example, by average pooling the en-tire untrimmed video) may lose the signal in the noise. 4486
As the TQN must attend densely to the video frames for an-swering speciﬁc queries, and cannot sub-sample in time, we also introduce a stochastically updated feature bank so that the model can be trained beyond the constraints imposed by ﬁnite GPU memory. For this we use a temporal feature bank in which features from densely sampled contiguous temporal segments are cached over the course of training, and only a random subset of these features is computed on-line and backpropagated through in each training iteration.
We demonstrate the TQN on two ﬁne-grained action recog-nition datasets with untrimmed video sequences: Fine-Gym [46] and Diving48 [37]. Both of these datasets share the following challenges: (i) object and backgrounds can-not be used to inform classiﬁcation, as is possible for more coarse-grained action recognition datasets, e.g., Ki-netics [27] and UCF-101 [49] (see Figure 1). (ii) subtle differences in actions, relative spatial orientations and tem-poral ordering of objects/actors need to be distinguished. (iii) events have a short duration of approx. 0.3 seconds in video clips which are typically 6-10 seconds long, and can be as much as 30 seconds in length. (iv) Finally, the dura-tion and position of events vary and is unknown in training.
This lack of alignment between text-description (labels) and videos means that that supervision is weak.
Summary of contributions: (i) we introduce a new model—a Temporal Query Network (TQN)–which enables query-response functionality on untrimmed videos. It can be trained using only the labels for each query. We show how ﬁne-grained video classiﬁcation can be cast as a query-response task. (ii) We propose a new way—stochastic fea-ture bank update—to train a network on videos of various lengths with the dense sampling required to respond to ﬁne-grained queries. (iii) We compare the TQN to other architec-tures and text supervision methods, and analyze their pros and cons. Finally, (iv) we evaluate the method extensively on the FineGym [46] and Diving48 [37] benchmarks for
ﬁne-grained action classiﬁcation. We demonstrate the ben-eﬁts of the TQN and stochastic feature bank update over baselines and with ablations, and the importance of ex-tended and dense temporal context. The TQN with stochas-tic feature bank update training surpass the state-of-the-art on these two benchmarks using only RGB features. 2.