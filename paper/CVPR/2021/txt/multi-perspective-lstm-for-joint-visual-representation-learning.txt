Abstract
We present a novel LSTM cell architecture capable of learning both intra- and inter-perspective relationships available in visual sequences captured from multiple per-spectives. Our architecture adopts a novel recurrent joint learning strategy that uses additional gates and memories at the cell level. We demonstrate that by using the pro-posed cell to create a network, more effective and richer visual representations are learned for recognition tasks.
We validate the performance of our proposed architecture in the context of two multi-perspective visual recognition tasks namely lip reading and face recognition. Three rele-vant datasets are considered and the results are compared against fusion strategies, other existing multi-input LSTM architectures, and alternative recognition solutions. The ex-periments show the superior performance of our solution over the considered benchmarks, both in terms of recogni-tion accuracy and complexity. We make our code publicly available at https://github.com/arsm/MPLSTM. 1.

Introduction
Today, images and videos captured from multiple vi-sual perspectives (multi-perspective) or view-points are ex-tensively available thanks to the wide-spread adoption of consumer-level cameras, notably in smartphones, able to capture visual scenes simultaneously from multiple an-gles [30]. Multi-perspective sequences can be recorded by i) several video cameras positioned at different angles, si-multaneously acquiring the sequences, each of which in-cluding multiple samples/instances along time; and/or ii) multi-view cameras such as Light Field (LF) cameras [17], from which all samples/instances of all sequences are ac-quired at a single time instant, e.g., with changing hori-zontal and vertical perspectives. We call these two types of multi-perspective sequences multi-perspective sequences over time and multi-perspective sequences over space, re-spectively. When either of these sequences are used for vi-sual recognition tasks, it is possible to exploit both the intra-Cam 1
Cam 5
Cam 2
Cam 3
Cam 4
…
Seq. 1
Seq. 2
Seq. 3
Seq. 4
Seq. 5 e v i t c e p s r e p
-r e t n
I
Intra-perspective  relationships i s p h s n o i t a e r l
Figure 1. Sequences captured from multiple perspectives include intra- and inter-perspective relationships that need to be effectively learned for robust visual recognition. We propose a novel LSTM cell capable of jointly learning incoming visual representations from various perspectives. perspective relationships (within each input/view sequence) and the inter-perspective relationships (between the differ-ent input/view sequences), as illustrated in Figure 1.
Recurrent neural networks (RNN) [18] such as long short-term memory (LSTM) [13], have been widely used for learning sequential data. Nonetheless, conventional or vanilla LSTM networks, hereafter referred to only as LSTM networks [13, 7], learn from a single sequence, as each cell only accepts an instance of one particular sequence.
In this context, in order to learn from multiple sequences (e.g., multi-perspective sequences), a separate LSTM net-work needs to be learned for each input sequence. As a re-sult, inter-sequence relationships such as inter-perspective information are typically not learned. To aggregate the in-formation learned by individual LSTM networks, fusion strategies have often been adopted [8, 32, 6, 29]. Score-level fusion, also known as late fusion, can be employed to combine the classiﬁcation scores using different strategies such as a [weighted] sum rule or voting. This approach im-plies that the overall learning strategy is unable to learn the inter-sequence relationships and only relies on the aggrega-tion of the class probabilities for the ﬁnal decision. To avoid this problem, feature-level fusion, also known as early fu-sion, can be used by concatenating the input sequences and feeding them consecutively to a single network. Nonethe-16540  
less, in this approach, the input representation is treated as a whole and not as different simultaneous perspectives captured from the same event, whereas in reality, different parts of this representation convey overlapping or compli-mentary information about the scene. Hence the parameters of the network are learned irrespective of the relationships between the available sequences, which are located in dif-ferent parts of the concatenated representation.
In this paper, we propose a novel Multi-Perspective
LSTM (MP-LSTM) cell architecture to jointly learn the intra-perspective and inter-perspective relationships avail-able in multi-perspective sequences. To this end, we mod-ify the conventional LSTM cell architecture, by incorporat-ing additional gates and cell memories to adopt a novel re-current joint learning strategy. These modiﬁcations enable our novel LSTM architecture to jointly update the long-term shared cell memory with respect to the information associated to several input perspective sequences simulta-neously. This leads to more effective learning of the avail-able inter-perspective relationships by identifying the com-plimentary or contradicting information across the perspec-tive sequences when creating the output feature representa-tions. Our experiments show that the proposed MP-LSTM networks can learn richer representations to achieve better performance as exempliﬁed for our experiments on two dif-ferent visual recognition tasks.
The main contributions can be summarized as follows: (1) We propose the novel MP-LSTM cell architecture capa-ble of jointly learning the intra- and inter-perspective rela-tionships available in multi-perspective sequences; (2) we integrate our MP-LSTM network into two visual recogni-tion solutions, for lip reading and face recognition tasks, covering two different types of multi-perspective sequences over time and over space; (3) our solutions achieve supe-rior results over the state-of-the-art, with considerable per-formance gains of up to 5% when multi-perspective infor-mation is jointly learned using our proposed model com-pared to other joint-learning or fusion strategies; and (4) we make our implementation publicly available1 to enable re-producibility and future comparisons. 2.