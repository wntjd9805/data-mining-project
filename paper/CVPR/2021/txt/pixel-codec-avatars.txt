Abstract
Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving au-thentic face-to-face communication in 3D over remote phys-ical distances.
In this work, we present the Pixel Codec
Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction perfor-mance while being computationally efﬁcient and adaptive to the rendering conditions during execution. Our model com-bines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a rendering-adaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh track-ing over training images. We demonstrate that PiCA im-proves reconstruction over existing techniques across test-ing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a sin-gle Oculus Quest 2 mobile VR headset, 5 avatars are ren-dered in realtime in the same scene. 1.

Introduction
Photorealistic Telepresence in Virtual Reality (VR) as proposed in [10, 26], describes a technology for enabling authentic communication over remote distances that each communicating party feels the genuine co-location pres-ence of the others. At the core of this technology is the
Codec Avatar, which is a high ﬁdelity animatable human face model, implemented as the decoder network of a Vari-ational AutoEncoder (VAE). Imagine a two-way communi-cation setting. At the transmitter end, an encoding process is performed: cameras mounted on transmitter’s VR head-set capture partial facial images and an encoder model en-codes the captured images into latent code of the decoder in realtime. At the receiver end a decoding process is per-formed: upon receiving the latent code over the internet,
Figure 1. An multi-person conﬁguration for teleconference in VR.
At normal interpersonal distances [20], the head occupies only a subset of pixels in the display, where the amount of coverage largely depends on distance to the viewer. Roughly half of the head is not visible from any viewing angle due to self occlusion.
Our method avoids wasting computation on areas that do not di-rectly contribute to the ﬁnal image. In ﬁrst row we show the gen-erated and rasterized geometry, along with texture maps showing visible pixels from the corresponding views; in the second row we show the rendered avatars and the percentage of pixels they cover over the entire image. the decoder decodes the avatar’s geometry and appearance so that the transmitter’s realtime photorealistic face can be rendered onto the VR display.
Multi-person communication via Photorealistic VR
Telepresence will enable applications that are in great need in the modern society, such as family re-union over far phys-ical distances in which each member genuinely feels the co-location presences of the others, or collaboration in re-mote working where team members can effectively com-municate face-to-face in 3D. However, rendering with the decoder model proposed in [10] does not scale well with the number of communicating parties. Speciﬁcally, a full 64
texture of ﬁxed resolution 1K×1K is decoded at each frame despite the distance of the avatar to the viewer and visibility of different facial regions. This leads to signiﬁcant waste of computation when the avatar is far away, for which case the rendered avatar only consists a small number of pixels (Fig. 1), resulting in a large number of pixels in the decoded texture map unused. Also, most of the time half of the head is not visible due to self-occlusion, so the pixels in the de-coded texture map for the occluded part are also unused.
For a 2K display such as the one in Quest2, rendering more than 4 avatars amounts to computing more pixels than that of the display. This is obviously limiting, e.g. family re-union of more than 4 persons or team collaboration of more than 4 members are common place.
To solve this issue and scale the rendering to the number of persons in the VR telepresence, we should compute only the visible pixels, thus upper bounding the computation by the number of pixels of the display. Recent works in neural rendering such as the defferred neural rendering[24], the neural point-based graphics[2], the implicit differentiable rendering [27], use neural network to compute pixel values in the screen space instead of the texture space thus comput-ing only visible pixels. However, in all these works, either static scene is assumed, or the viewing distance and per-spective are not expected to be entirely free in the 3D space.
However, for telepresence, the ability to animate the face in realtime and render it from any possible viewing angle and distance is crucial.
In this paper, we present Pixel Codec Avatars (PiCA) that aims to achieve efﬁcient and yet high ﬁdelity dynamic hu-man face rendering that is suitable for multi-person telep-resence in VR on devices with limited compute. To avoid wasteful computation in areas of the face that do not con-tribute to the ﬁnal rendering, PiCA employs per-pixel de-coding only in areas of the image covered by a rasteriza-tion of the geometry. Similar to recent advances in im-plicit neural rendering [11, 17, 21], this decoder relies on a rich face-centric position encoding to produce highly de-tailed images. We employ two strategies to generate such encodings efﬁciently. First, we make use of the spatially-shared computation of convolutional networks in texture space to produce spatially varying expression- and view-speciﬁc codes at a reduced resolution (256×256). This is complemented by a pre-computed high resolution (1K×1K) learned non-parametric positional encoding, that is jointly rasterized into screen space similarly to [24]. To achieve an even higher resolution result, we further compliment the signal with 1D positional encodings at 10K resolution, in-dependently for the horizontal and vertical dimensions of the texture domain. Together, these maps enable the model-ing of sharp spatial details present in high resolution facial images. Because the best encoding values for the UV coor-dinates are directly learned from data, a low 8-dimensional encoding is sufﬁcient to recover high frequencies. This is in contrast to existing positional encoding schemes (e.g.
[11]) that achieve high details using sinusoidal functions, but require increasing the dimensionality by 20×, with cor-responding computational costs. Secondly, in contrast to other works such as [24, 2, 27], we do not employ convo-lutions in screen space, but instead apply a shallow MLP at each contributing pixel. This has the advantage of avoiding visual artifacts during motion and stereo inconsistencies, as well as challenges in generalizing to changes in scale, rota-tion and perspective, all of which are common in interactive immersive 3D media.
Our other main insight is that the complexity of view-dependent appearance in prior work stems mostly from in-adequate geometric models of the face. Recent work into implicit scene modeling (i.e. NeRF [11]) has demonstrated that complex view dependent effects such as specularity can be adequately modeled using a shallow network given good estimates of the scene’s geometry. Inspired by these results, our construction involves a variational geometry decoder that is learned in a self-supervised manner, using image and depth reconstruction as a supervisory signal. The resulting mesh acquired from this decoder contains more accurate geometry information, substantially simplifying the view-dependent texture generation task, allowing for the use of lightweight pixel-wise decoding.
Contributions: Our contributions are as follows:
• We propose Pixel Codec Avatar, a novel light weight representation that decodes only the visible pixels on the avatar’s face in the screen space towards enabling high ﬁdelity facial animation on compute-constrained platforms such as mobile VR headsets.
• We make the two major technical innovations to achieve high quality decoding with a small model: learned positional encoding functions and fully con-volutional dense mesh decoder trained in a weakly-supervised fashion. 2.