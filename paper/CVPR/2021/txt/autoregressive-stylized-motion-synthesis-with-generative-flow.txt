Abstract
Motion style transfer is an important problem in many computer graphics and computer vision applications, in-cluding human animation, games, and robotics. Most exist-ing deep learning methods for this problem are supervised and trained by registered motion pairs. In addition, these methods are often limited to yielding a deterministic output, given a pair of style and content motions. In this paper, we propose an unsupervised approach for motion style trans-fer by synthesizing stylized motions autoregressively using a generative ﬂow model M. M is trained to maximize the ex-act likelihood of a collection of unlabeled motions, based on an autoregressive context of poses in previous frames and a control signal representing the movement of a root joint.
Thanks to invertible ﬂow transformations, latent codes that encode deep properties of motion styles are efﬁciently in-ferred by M. By combining the latent codes (from an input style motion S) with the autoregressive context and control signal (from an input content motion C), M outputs a styl-ized motion which transfers style from S to C. Moreover, our model is probabilistic and is able to generate various plausible motions with a speciﬁc style. We evaluate the pro-posed model on motion capture datasets containing differ-ent human motion styles. Experiment results show that our model outperforms the state-of-the-art methods, despite not requiring manually labeled training data. 1.

Introduction
In computer graphics, there has been a long-standing in-terest in motion style transfer, since this task beneﬁts var-ious applications including human animation, games, and robotics, etc. Early methods rely on handcrafted features to
*Corresponding author
†Equal contribution
Figure 1. Our stylized motion synthesis that transfers the style from an input style motion to an input content motion. The gener-ative ﬂow model is trained via unsupervised learning on unlabeled motion data of different styles. The trained model extracts the style latent codes from the input style motion. Then, it outputs a high quality stylized motion with the style latent codes (from the input style motion), the autoregressive context and the control signal (from the input content motion). design different motion styles [38, 2]. To release the bur-den of handcrafted feature design, data-driven motion style transfer methods using deep learning models have been pro-posed. They automatically learn useful features from the input motion samples. However, most of the existing deep learning methods [19, 36, 43, 35] are supervised and require paired and registered data to perform style transfer. Such methods also need a large number of motion samples to ex-tract a speciﬁc style. Therefore, these methods are limited by a tedious preprocess to collect a large amount of motion 13612
data for training. For example, actors have to perform sev-eral motion cycles in different styles with almost identical steps, followed by a motion registration step.
Recently, Aberman et al. [1] propose a style transfer method, which does not require paired and registered train-ing data. However, the method still requires motion samples with manual labeling styles. It encodes input content and style motions into latent codes for content and style, which are then recombined and decoded to output a stylized mo-tion. Since they use a deterministic model to extract style and content latent codes, the output motion is also deter-ministic. Moreover, their model is built upon 1D temporal convolutional layers. Thus, its raw outputs have some arti-facts, which should be resolved with additional efforts. For example, the foot contact positions of the output motion are corrected in accordance with the input content motion to address the problem of foot skating during walking. Then, dynamic time warping is used to make the global velocity of the output motion to properly reﬂect the style of the input style motion (e.g., when transferring the style from “old” walking to “neutral” walking, the global velocity should de-crease).
In this paper, we propose an unsupervised method for motion style transfer by synthesizing stylized motions based on a generative ﬂow model M. M is trained to max-imize the exact log-likelihood (rather than a lower bound of it in other models such as variational autoencoders (VAEs)) over unlabeled motions of different styles captured in re-ality. Therefore, the synthesized motions from M are hu-manlike and have no artifacts such as foot skating. Com-pared to [1], the distinguishing feature of our method is to be trained via unsupervised learning and output a high qual-ity stylized motion in a probabilistic way. The probabilistic nature of the generative ﬂow model offers more ﬂexibility to remove the artifacts appearing in the deterministic results in [1]. Through invertible ﬂow transformations, the latent codes that encode deep properties of motion styles are ef-fectively inferred from the input style motion. Then, the inferred latent codes are combined with the input content motion for synthesizing various plausible stylized motions directly. To the best of our knowledge, we are the ﬁrst to introduce the generative ﬂow model for motion style trans-fer. We improve the efﬁciency of M by performing the ﬂow transformation on one half of the motion features and keep-ing the other half unchanged, based on the afﬁne coupling layers [6, 7, 22]. Furthermore, a Transformer is imposed into the invertible ﬂow transformation to extract autoregres-sive features. Experiment results show that the Transformer enables the generative ﬂow model to learn a ﬂexible latent distribution, in which the latent vectors encode deep proper-ties of the motion style that is even unseen during training.
Thus, M has a good scalability by transferring the unseen style from the style motion to the content motion.
In summary, we make three contributions in this paper: (1) We introduce a generative ﬂow model for motion style transfer by synthesizing stylized motions with input style and content motions. The probabilistic nature of the model offers more ﬂexibility to generate high quality stylized mo-tions. (2) Thanks to the invertible ﬂow transformations in the generative model, the latent codes are efﬁciently in-ferred from the input style motion to encode deep style fea-tures. (3) We impose a Transformer into each invertible ﬂow transformation in our generative model to learn a ﬂexible latent distribution for encoding deep properties of the mo-tion style that is even unseen during training. Thus, our proposed model is able to transfer the unseen style to the content motion. 2.