Abstract
To address the problem of long-tail distribution for the large vocabulary object detection task, existing methods usually divide the whole categories into several groups and treat each group with different strategies. These methods bring the following two problems. One is the training in-consistency between adjacent categories of similar sizes, and the other is that the learned model is lack of discrim-ination for tail categories which are semantically similar to some of the head categories.
In this paper, we devise a novel Adaptive Class Suppression Loss (ACSL) to effec-tively tackle the above problems and improve the detection performance of tail categories. Speciﬁcally, we introduce a statistic-free perspective to analyze the long-tail distribu-tion, breaking the limitation of manual grouping. According to this perspective, our ACSL adjusts the suppression gra-dients for each sample of each class adaptively, ensuring the training consistency and boosting the discrimination for rare categories. Extensive experiments on long-tail datasets
LVIS and Open Images show that the our ACSL achieves 5.18% and 5.2% improvements with ResNet50-FPN, and sets a new state of the art. Code and models are available at https://github.com/CASIA-IVA-Lab/ACSL. 1.

Introduction
With the advent of deep Convolutional Neural Network, researchers have achieved signiﬁcant progress on object de-tection task. Many efforts have been paid to refresh the
Figure 1: The label distribution of LVIS [11] dataset. The x-axis represents the sorted category index of LVIS. The y-axis is the image number of each category. record on classic benchmarks like PASCAL VOC [9] and
MS COCO [21]. However, these benchmarks usually have limited quantities of classes and exhibit relatively balanced category distribution. Whereas, in real-world scenarios, data usually comes with a long-tail distribution. A few head classes (frequent classes) contribute most of the training samples, while the huge number of tail classes (rare classes) are under-represented in data. Such extremely imbalanced class distribution proposes new challenges for researchers.
An intuitive solution is to re-balance the data distribution by re-sampling technique [12, 7, 28, 23]. By over-sampling 3103
the tail or under-sampling the head classes, a less imbal-anced distribution could be artiﬁcially generated. Neverthe-less, over-sampling usually brings undesirable over-ﬁtting issues on tail classes. And under-sampling may miss valu-able information of head classes. The effectiveness of such techniques is limited when the dataset is extremely imbal-anced. To improve the performance of tail classes while avoiding the over-ﬁtting issue, Tan et al. [29] devises Equal-ization Loss, which argues that the poor performance of tail classes originates from the over-suppression of samples from head classes. Since tail classes only contain few sam-ples, they receive much more negative gradients than posi-tive ones during training, thus they are consistently in a state of being suppressed in most of the training time. In order to prevent tail classiﬁers from being over-suppressed, Equal-ization Loss proposes to ignore all negative gradients from head classes. Balanced Group Softmax (BAGS) [18] puts categories with similar numbers of training instances into the same group and computes group-wise softmax cross-entropy loss respectively. BAGS achieves relative balance within each group, thus can effectively ameliorate the dom-ination of the head classes over tail classes.
The above methods can efﬁcaciously reduce the sup-pression on tail classiﬁers. However, they need to parti-tion the categories into several groups based on their cate-gory frequency prior. Such hard division between head and tail classes brings two problems, namely training inconsis-tency between adjacent categories and lack of discrimina-tive power for rare categories. As shown in Figure 1, when two categories with similar instance statistics are divided into two different groups, there exists a huge gap between their training strategies. Such training inconsistency may deteriorate the network’s performance. In general, it is sub-optimal to distinguish the head and tail classes by the abso-lute number of samples. In addition, it frequently happens that two categories with high appearance similarity hold totally different sample frequency for datasets with large category vocabulary. For instance, category “sunglasses” and “eye mask” belong to the head and tail classes, respec-tively. To prevent the tail classiﬁer “eye mask” from being over-suppressed, the negative gradients from category “sun-glasses” is ignored by the classiﬁer of “eye mask”. Never-theless, this can also reduce the differentiability between these semantically similar cases in feature space, the classi-ﬁer of “eye mask” has a high chance to misclassify sample
“sunglasses” as “eye mask”. For those categories which be-long to different groups but have high appearance similarity, it is hard for the network to learn a discriminative feature representation.
Therefore, in this paper, we present a novel Adaptive
Class Suppression loss (ACSL) to address the above two problems. The design philosophy is simple and straight-forward: We assume all categories are from “tail” group, and adaptively generate suppression gradients for each cat-egory according to their current learning status. To be spe-ciﬁc, we propose to treat all object categories as scarce cate-gories, regardless of the statistics of per-category instances, thus eliminating the dilemma of manually deﬁning the head and tail. Furthermore, in order to alleviate the insufﬁcient learning and representation, we introduce an Adaptive Class
Suppression Loss to adaptively balance the negative gradi-ents between different categories, which effectively boosts the discriminative ability for tail classiﬁers. On the one hand, the proposed method can exterminate several heuris-tics and hyper-parameters of data distribution. On the other hand, it can also avoid the problem caused by over-sampling and under-sampling, and ensure the training consistency for all classes and sufﬁcient learning of rare or similar cate-gories. Finally, it yields reliable and signiﬁcant improve-ments in detection performance on large-scale benchmarks like LVIS and Open Images.
To sum up, this work makes the following three contri-butions: 1. We propose a new statistic-free perspective to un-derstand the long-tail distribution, thus signiﬁcantly avoiding the dilemma of manual hard division. 2. We present a novel adaptive class suppression loss (ACSL) that can effectively prevent the training incon-sistency of adjacent categories and improve the dis-criminative power of rare categories. 3. We conduct comprehensive experiments on long-tail object detection datasets LVIS and Open Images.
ACSL achieves 5.18% and 5.2% improvements with
ResNet50-FPN on LVIS and OpenImages respectively, which validates its effectiveness. 2.