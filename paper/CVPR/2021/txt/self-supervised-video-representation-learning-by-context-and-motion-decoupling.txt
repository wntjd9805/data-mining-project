Abstract
A key challenge in self-supervised video representa-tion learning is how to effectively capture motion informa-tion besides context bias. While most existing works im-plicitly achieve this with video-speciﬁc pretext tasks (e.g., predicting clip orders, time arrows, and paces), we de-velop a method that explicitly decouples motion supervision from context bias through a carefully designed pretext task.
Speciﬁcally, we take the key frames and motion vectors in compressed videos (e.g., in H.264 format) as the supervi-sion sources for context and motion, respectively, which can be efﬁciently extracted at over 500 fps on CPU. Then we de-sign two pretext tasks that are jointly optimized: a context matching task where a pairwise contrastive loss is cast be-tween video clip and key frame features; and a motion pre-diction task where clip features, passed through an encoder-decoder network, are used to estimate motion features in a near future. These two tasks use a shared video backbone and separate MLP heads. Experiments show that our ap-proach improves the quality of the learned video represen-tation over previous works, where we obtain absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Moreover, we ﬁnd the motion prediction to be a strong regularization for video networks, where using it as an auxiliary task improves the accuracy of action recognition with a margin of 7.4% ∼ 13.8%. 1.

Introduction
Self-supervised representation learning from unlabeled videos has recently received considerable attention [18, 50].
Compared with static images, the redundancy, temporal consistency, and multi-modality of videos potentially pro-vide richer sources of “supervision”. Various methods have been proposed in this ﬁeld that learn video representation by designing video-speciﬁc pretext tasks [33, 54, 49], adapting contrastive learning to videos [17, 50, 38, 18], cross-modal learning [23, 43, 36, 32], and contrastive clustering [2]. (a) Decoded stream of a video.
RGB images
Motion vectors
Residuals
I-frame
P-frame
P-frame
P-frame
P-frame
… (b) Compressed stream of a video.
Figure 1: (a) and (b) show the decoded and compressed streams of a sample video, respectively. We notice that the context and motion information are roughly decoupled in
I-frames and motion vectors of the compressed stream. We exploit these modalities as the supervision sources for self-supervised video representation learning.
This paper focuses on visual-only video representation learning, and we distinguish two orthogonal but comple-mentary aspects of video representation: context and mo-tion. Context depicts coarse-grained and relatively static environments, while motion represents dynamic and ﬁne-grained movements or actions. Context information alone can be used to classify certain actions (e.g., swimming is most likely to take place at swimming pool), but it also leads to background bias [40, 28]. For actions that heavily depend on movement patterns (e.g., breaststroke and frontcrawl), motion information must be introduced. We aim to design a self-supervised video representation learning method that jointly learns these two complementary information. Our idea is to design a multi-task framework, where context and motion representation learning is decoupled in pretext tasks.
One problem here is the source of supervision. Consider-ing the scalability of our framework on larger datasets, we endeavor to avoid the use of computationally expensive fea-tures such as optical ﬂow [59, 6] and dense trajectories [47]. 113886
Context features
Motion features
Context supervision
I-Network
Pool + MLP
I-frame (C2, H2, W2)
Context matching
Contrastive learning
Pool + MLP (C, 1) (C, 1)
V-Network t=1 t=2
… t=T1
RGB video clip (C1, T1, H1, W1)
Transformer + MLP
Motion supervision t= T1+1 t= T1+2
… t= T1+T3
Motion vectors
M-Network
MLP (C, T3, H3, W3)
Pointwise contrastive learning (C3, T3, H3, W3) (C, T3, H3, W3)
Motion prediction
Figure 2: An overview of our framework. We decouple the context and motion supervision in two separate pretext tasks: context matching and motion prediction. The context matching task takes the relatively static I-frames in the compressed video as the source of supervision, and casts a contrastive loss between global features of I-frames and video clips. The motion prediction task takes the motion vector maps extracted from future frames of the compressed video as the source of supervision, and compares the predicted and “groundtruth” motion features in a pointwise way using the contrastive loss.
We notice that video in compressed format (such as
H.264 and MPEG-4) roughly decouples the context and mo-tion information in its I-frames and motion vectors. As shown in Figure 1, a compressed video stores only a few key frames (i.e., I-frames) completely, and it reconstructs other frames based on motion vectors (i.e., pixel offsets) and residual errors from the key frames. I-frames can rep-resent relatively static and coarse-grained context informa-tion, while motion vectors depict dynamic and ﬁne-grained movements. Moreover, both modalities can be efﬁciently extracted at over 500 fps on CPU [52].
Inspired by this, we present a self-supervised video rep-resentation learning method where two decoupled pretext tasks are jointly optimized: context matching and motion prediction. Figure 2 shows an overview of our framework.
The context matching task aims to give the video network a rough grasp of the environment in which actions take place.
It casts a noise contrastive estimation (NCE) loss [16, 35] between global features of video clips and I-frames, where clips and I-frames from the same videos are pulled together, while those from different videos are pushed away. The mo-tion prediction task requires the model to predict pointwise motion dynamics in a near future based on visual informa-tion of the current clip. The assumption is that, in order to predict future motion, the video network needs to extract low-level movements from visual data and reorganize them into high-level trajectories. In this way, the learned repre-sentation should contain semantic and long-term motion in-formation, helpful for downstream tasks. In our framework, instead of directly estimating the values of motion vectors, we use pointwise contrastive learning to compare predicted and real motion features at every spatial and temporal loca-tion (x, y, t). We ﬁnd that this leads to more stable pretrain-ing and better transferring performance.
We conduct extensive experiments on three network ar-chitectures, three datasets, and two downstream tasks (i.e., action recognition and video retrieval) to assess the quality of our learned video representation. We achieve state-of-the-art performance on all these experiments. For example, we achieve R@1 video retrieval scores of 41.7% and 16.8% respectively on UCF101 [41] and HMDB51 [24] datasets, obtaining 16.0% and 11.1% absolute gains compared to ex-isting works. We also validate several modeling options in our ablation studies, where we ﬁnd that the motion predic-tion can serve as a strong regularization for video networks, and using it as an auxiliary task clearly improves the perfor-mance of supervised action recognition.
We summarize our contributions in the following:
• Unlike existing works where the source of supervision usually comes from the decoded raw video frames, we present a self-supervised video representation learning method that explicitly decouples the context and mo-tion supervision in the pretext task. 213887
• We present a context matching task for learning coarse-grained and relatively static context representation, and a motion prediction task for learning ﬁne-grained and high-level motion representation.
• To the best of our knowledge, we present the ﬁrst approach that exploits the modalities in compressed videos as the efﬁcient supervision sources for visual representation learning.
• We achieve signiﬁcant improvements over existing works on downstream tasks of action recognition and video retrieval. Extensive ablation studies also validate the effectiveness of our several modeling options. 2.