Abstract
CEC
…
…
…
Few-shot class-incremental learning (FSCIL) aims to de-sign machine learning algorithms that can continually learn new concepts from a few data points, without forgetting knowledge of old classes. The difﬁculty lies in that limited data from new classes not only lead to signiﬁcant overﬁtting issues but also exacerbate the notorious catastrophic forget-ting problems. Moreover, as training data come in sequence in FSCIL, the learned classiﬁer can only provide discrimi-native information in individual sessions, while FSCIL re-quires all classes to be involved for evaluation. In this pa-per, we address the FSCIL problem from two aspects. First, we adopt a simple but effective decoupled learning strat-egy of representations and classiﬁers that only the classi-ﬁers are updated in each incremental session, which avoids knowledge forgetting in the representations. By doing so, we demonstrate that a pre-trained backbone plus a non-parametric class mean classiﬁer can beat state-of-the-art methods. Second, to make the classiﬁers learned on in-dividual sessions applicable to all classes, we propose a
Continually Evolved Classiﬁer (CEC) that employs a graph model to propagate context information between classiﬁers for adaptation. To enable the learning of CEC, we de-sign a pseudo incremental learning paradigm that episodi-cally constructs a pseudo incremental learning task to opti-mize the graph parameters by sampling data from the base dataset. Experiments on three popular benchmark datasets, including CIFAR100, miniImageNet, and Caltech-USCD
Birds-200-2011 (CUB200), show that our method signiﬁ-cantly outperforms the baselines and sets new state-of-the-art results with remarkable advantages. 1.

Introduction
Deep Convolutional Neural Networks have gained re-markable success in many computer vision tasks [10, 19,
* indicates equal contribution.
†Corresponding author: G. Lin (e-mail: gslin@ntu.edu.sg)
…
FC 0
…
…
FC 1
FC 2
FC i
Figure 1: Illustration of our proposed continually evolved classiﬁers for
FSCIL. We employ a graph model to adapt the classiﬁer weights learned on individual sessions for the prediction over all classes. 22, 38, 55], stemming from the availability of big curated datasets, along with unprecedented computing power. How-ever, a classiﬁcation model that is trained by supervised learning can only make predictions on a set of pre-deﬁned image categories.
If we want to extend a trained model on new classes, a large amount of labeled data for new classes as well as data from old classes are both neces-sary for network ﬁnetuning, which inevitably hinders its real-world applications. If the dataset of old classes is no longer available, directly ﬁnetuning a deployed model with new classes can lead to the notorious catastrophic forgetting problem that knowledge about old classes is quickly forgot-ten [11, 17, 34]. In contrast to machine learning systems, humans are readily able to learn a new concept with few examples without forgetting old knowledge. The gap be-tween humans and the machine learning algorithms fuels in-terest in few-shot class-incremental learning (FSCIL) [39], which aims to design machine learning algorithms that can be continually extended to new classes with only a few data points. The challenge of FSCIL lies in that the scarcity in the data of new classes will not only cause severe overﬁtting but also exacerbates the catastrophic forgetting problem of old classes. In this paper, we undertake the task of few shot incremental learning and consider to solve the aforemen-tioned problems from two aspects. 12455
First, as the data from base classes and new classes are severely unbalanced, we propose to decouple the learning of representations and classiﬁers for the FSCIL problem.
Speciﬁcally, the model only learns the representations in the ﬁrst session where abundant data from base classes are available, and in the new sessions, we ﬁx the network back-bone and only adapt the classiﬁer for new classes. Thus, we can avoid the overﬁtting problem as well as the catastrophic forgetting problem in the representations. By doing so, we demonstrate that a pre-trained network backbone based on data from base classes plus a class mean classiﬁer can beat state-of-the-art approaches.
Second, as the classiﬁers are always learned from the classes in individual incremental sessions, they can only provide discriminative information for classifying internal categories, while incremental learning aims to learn models that can apply to all classes. As a result, even if a clas-siﬁer can learn a well-separated decision boundary for the previous classes, it may lose the generalization ability when more novel classes are involved. For example, a vehicle-related representation wheel is chosen by the classiﬁer as a discriminative representation to distinguish the categories car, dog and cup in the current classiﬁcation task. However, when a new category trunk is involved in the new sessions, such representation may not be discriminative enough to classify all categories. Therefore, the incremental learn-ing algorithm should have the ﬂexibility to adjust the clas-siﬁers in previous sessions based on the overall task con-text to undertake the entire classiﬁcation task. To this end, we present a Continually Evolved Classiﬁer (CEC) that can progressively adapt the classiﬁer weights based on current and history tasks. At the core of our network is a classi-ﬁer adaptation module which uses a graph attention network (GAT) [41] to adapt the classiﬁer weights learned on each task. By contextualizing individual classiﬁer weights over the global task, the adapted classiﬁers highlight the discrim-inative representations in the backbone and generate better decision boundaries over all involved classes.
To enable the learning of the proposed continually evolved classiﬁer, it is important to optimize the graph model under an incremental learning scenario. However, in incremental learning, datasets from different training ses-sions can never be accessed simultaneously for training.
To overcome the issue, we propose a pseudo incremen-tal learning paradigm, where we episodically construct a pseudo incremental learning task from the dataset in the base session to simulate the incremental learning scenario for training. Our design takes inspirations from the meta-learning paradigm [42]. In each pseudo incremental learn-ing episode, we ﬁrst sample a set of classes from the base dataset to play the role of the base classes, then we sam-ple another group of classes to play the role of incremen-tal classes to learn the model. However, as the pre-trained backbone has already learned feature representations that can well classify the base classes, directly using the sam-pled classes from the base dataset for learning may by-pass the GAT and thus fail to impose context knowledge.
We solve this problem by randomly rotating the sampled pseudo incremental classes with a large angle to synthesize new classes. In this way, we intentionally synthesize unfa-miliar classes at training time to enforce context knowledge propagation in the graph model. Once the graph model is learned, we can use the graph model to update the classiﬁer weights learned in incremental sessions.
To validate the effectiveness of our proposed method, we conduct comprehensive experiments on multiple bench-mark datasets. The contribution of this work is summarized as follows:
• We adopt a decoupled training strategy for represen-tation learning and classiﬁer learning to avoid knowl-edge forgetting and overﬁtting in the backbone.
• We propose a continually evolved classiﬁer that em-ploys a graph model to combine classiﬁers learned on individual sessions for incremental learning.
• To enable the learning of the graph model in CEC, we design a pseudo incremental learning paradigm.
• Experiments on the CIFAR100, CUB200 and mini-Imagenet datasets show that our method signiﬁcantly outperforms the baselines and sets new state-of-the-art performance with remarkable advantages. 2.