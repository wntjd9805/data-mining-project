Abstract
Multi-frame human pose estimation in complicated sit-uations is challenging. Although state-of-the-art human joints detectors have demonstrated remarkable results for static images, their performances come short when we ap-ply these models to video sequences. Prevalent shortcom-ings include the failure to handle motion blur, video defo-cus, or pose occlusions, arising from the inability in cap-turing the temporal dependency among video frames. On the other hand, directly employing conventional recurrent neural networks incurs empirical difﬁculties in modeling spatial contexts, especially for dealing with pose occlu-sions. In this paper, we propose a novel multi-frame human pose estimation framework, leveraging abundant temporal cues between video frames to facilitate keypoint detection.
Three modular components are designed in our framework.
A Pose Temporal Merger encodes keypoint spatiotemporal context to generate effective searching scopes while a Pose
Residual Fusion module computes weighted pose residu-als in dual directions. These are then processed via our
Pose Correction Network for efﬁcient reﬁning of pose esti-mations. Our method ranks No.1 in the Multi-frame Person
Pose Estimation Challenge on the large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have re-leased our code, hoping to inspire future research. 1.

Introduction
Human pose estimation is a fundamental problem in computer vision, which aims at locating anatomical key-*Corresponding Authors points (e.g., wrist, ankle, etc.) or body parts. It has enor-mous applications in diverse domains such as security, vio-lence detection, crowd riot scene identiﬁcation, human be-havior understanding, and action recognition [22]. Earlier methods [40, 38, 48, 29] adopt the probabilistic graphical model or the pictorial structure model. Recent methods have built upon the success of deep convolutional neural networks (CNNs) [5, 36, 41, 12, 27, 9, 25, 23, 24], achiev-ing outstanding performance in this task. Unfortunately, most of the recent state-of-the-art methods are designed for static images, with greatly diminished performance when handling video input.
In this paper, we focus on the problem of multi-person pose estimation in video sequences. Conventional image-based approaches disregard the temporal dependency and geometric consistency across video frames. Dissevering these additional cues results in failure cases when dealing with challenging situations that inherently occurs in video sequences such as motion blur, video defocus, or pose oc-clusions. Effectively leveraging the temporal information in video sequences is of great signiﬁcance to facilitate pose es-timation and often plays an indispensable role for detecting heavily occluded or blurry joints.
A direct and intuitive approach to tackle this issue is to employ recurrent neural networks (RNNs) such as Long-Short Term Memory (LSTM), Gate Recurrent Unit (GRU) or 3DCNNs to model geometric consistency as well as tem-poral dependency across video frames. [25] uses convolu-tional LSTM to capture temporal and spatial cues, and di-rectly predicts the keypoint heatmap sequences for videos.
This RNN based approach is more effective when the hu-man subjects are spatially sparse such as single-person scenes with minimal occlusion. However, performance 525
point context into localized search scopes, computes pose residuals, and subsequently reﬁnes the keypoint heatmap estimations. Speciﬁcally, we design three task-speciﬁc modules within the DCPose pipeline. 1) As illustrated in Fig. 1, a Pose Temporal Merger (PTM) network per-forms keypoints aggregation over a continuous video seg-ment (e.g., three consecutive frames) with group convolu-tion, thereby localizing the search range for the keypoint. 2) A Pose Residual Fusion (PRF) network is introduced to efﬁciently obtain the pose residuals between the current frame and adjacent frames. PRF computes inter-frame key-point offsets by explicitly utilizing the temporal distance. 3) Finally, a Pose Correction Network (PCN) comprising
ﬁve parallel convolution layers with different dilation rates is proposed for resampling keypoint heatmaps in the local-ized search range.
It is worth mentioning that the architecture of our net-work extends the successful PoseWarper architecture [3] in three ways. (1) PoseWarper focuses on enabling effective label propagation between frames, while we aim to reﬁne the pose estimation of current frame using the motion con-text and temporal information from unlabeled neighboring frames. (2) Information from two directions are utilized and we explicitly consider weighted residuals between frames. (3) Instead of applying the learned warping operation to a heatmap from one adjacent frame, the new network fuses together heatmaps from the adjacent frames and the current frame.
To summarize, our key contributions are: 1) A novel dual consecutive pose estimation framework is proposed. DC-Pose effectively incorporates bidirectional temporal cues across frames to facilitate the multi-person pose estimation task in videos. 2) We design 3 modular networks within i) a
DCPose to effectively utilise the temporal context: novel Pose Temporal Merger network for effectively ag-gregating keypoint across frames and identifying a search scope, ii) a Pose Residual Fusion network to efﬁciently compute weighted pose residuals across frames, and iii) a Pose Correction Network that updates the pose estima-tion with the reﬁned search scope and pose residual in-formation. 3) Our method achieves state-of-the-art results on PoseTrack2017 and PoseTrack2018 Multi-frame Person
Pose Estimation Challenge. To facilitate future research, our source code is released at https://github.com/
Pose-Group/DCPose. 2.