Abstract
We present a novel method for multi-view depth estima-tion from a single video, which is a critical task in var-ious applications, such as perception, reconstruction and robot navigation. Although previous learning-based meth-ods have demonstrated compelling results, most works esti-mate depth maps of individual video frames independently, without taking into consideration the strong geometric and temporal coherence among the frames. Moreover, current state-of-the-art (SOTA) models mostly adopt a fully 3D con-volution network for cost regularization and therefore re-quire high computational cost, thus limiting their deploy-ment in real-world applications. Our method achieves tem-porally coherent depth estimation results by using a novel
Epipolar Spatio-Temporal (EST) transformer to explicitly associate geometric and temporal correlation with multiple estimated depth maps. Furthermore, to reduce the compu-tational cost, inspired by recent Mixture-of-Experts mod-els, we design a compact hybrid network consisting of a 2D context-aware network and a 3D matching network which learn 2D context information and 3D disparity cues sepa-rately. Extensive experiments demonstrate that our method achieves higher accuracy in depth estimation and signiﬁ-cant speedup than the SOTA methods. 1.

Introduction
Multi-view depth estimation aims to recover 3D geome-try of given images with known camera parameters, which is one of fundamental problems in computer vision. Many applications beneﬁt from the recovered depth, such as au-tonomous driving [39], augmented reality [18], 3D model-ing [13], and image-based rendering [10].
Recently, learning based depth estimation methods, whether designed for images [9, 23, 17, 42] or videos [22, 46, 37, 29, 27], have achieved great improvements against their traditional counterparts [45, 31, 1, 15]; however, these methods, especially for videos, have signiﬁcant room for improvement in terms of temporal coherence and computa-tional efﬁciency.
Figure 1. Independent estimation vs Joint estimation. Prior works estimate the depth maps of frames from a video independently de-spite their temporal coherence, thus the estimated depth maps may contain inconsistent values for the same region (red circles). Our model could estimate multiple depth maps jointly utilizing tempo-ral coherence, enabling the estimated depth maps to be consistent.
Temporal coherence. Most of multi-view depth estima-tion methods [17, 42, 21, 26, 35] are designed for individ-ual images so they are not suitable for temporally coherent videos. Directly extending the existing methods from im-ages to video sequences causes ﬂickering artifacts. i.e. in-consistent estimated depth maps across consecutive frames, because they do not take temporal coherence into consider-ation. As shown in Figure. 1, the independently estimated depth maps contain inconsistent values. Therefore, it is nec-essary to jointly estimate depth maps of a video sequence to produce temporally coherent results.
Existing works on video depth estimation can be divided into two categories: single-view methods [46, 6, 37, 29, 27] and multi-view methods [22], according to the input of the depth estimation network. Recurrent neural units are widely used in single-view video methods [46, 6, 37, 29] to encode temporal coherence implicitly. Although the inconsistency problem is alleviated by incorporating temporal coherence, these methods still suffer from the ambiguity of depth scale since depth estimation from a single image is an ill-posed problem.
Multi-view video methods are advantageous over single-view methods because epipolar geometry information pro-vided by multiple images could be used to avoid depth scale 8258
ambiguity problem. To the best of our knowledge, there has been only one multi-view method [22] for video depth es-timation based on the epipolar geometry information. This method produces compelling results but it is restricted by its speciﬁc network design that it can only make use of one preceding estimation to improve current estimation. We im-prove upon this multi-view method by proposing a novel
Epipolar Spatio-Temporal (EST) network that is capable of utilizing temporal coherence of multiple preceding estima-tions, thus producing more accurate depth maps with better temporal coherence.
Computational Efﬁciency. Top-performing multi-view depth estimation models [17, 21, 22] are slow with low computational efﬁciency, because they adopt a single fully 3D convolution networks for cost regularization to learn both local feature matching information and global context information, which have been shown to be critical for accu-rate depth estimation [2, 19]. While the local feature infor-mation is necessary for matching texture-rich regions, the global context information is crucial for scenes with texture-less regions. The existing networks tend to use deeper and deeper networks to improve the ability of learning global context information, leading to increased computation cost.
It should be pointed out that the global context informa-tion is essentially 2D, so learning it by deep 3D convolution layers will unnecessarily consume masses of computational resources.
Our insight is that for depth estimation the local feature information and global semantic information can be learned by two separate networks, as inspired by recent Mixture-of-Experts models [38, 47, 28]. Speciﬁcally, we propose a hybrid cost regularization network, consisting of two com-plementary expert sub-networks: a 2D ContextNet focusing on 2D global context information, and a shallow 3D Match-Net concentrating on 3D local matching information. By explicitly disentangles these two different types of informa-tion, our hybrid network consumes much less GPU compu-tational resources and achieves faster running speed.
Our main contributions are summarized as follows:
• We proposed an Epipolar Spatio-Temporal (EST) transformer that propagates temporal coherence to per-form joint depth estimation of multiple frames to make estimated depth maps more temporally coherent.
• We designed a hybrid network for cost regulariza-tion that consists of two expert networks to learn 3D local matching information and 2D global con-text information separately. This decoupling approach achieves faster speed and consumes less computational resources than using a single fully 3D network in prior works.
• Based on these two contributions we developed a new mutli-view method for generating temporally coherent depth maps from videos. We conducted extensive ex-periments on several datasets to demonstrate that our method outperforms the SOTAs by a large margin in terms of accuracy and speed. 2.