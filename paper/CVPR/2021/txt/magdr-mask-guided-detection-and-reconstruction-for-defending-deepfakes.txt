Abstract 1Deepfakes raised serious concerns on the authenticity of visual contents. Prior works revealed the possibility to disrupt deepfakes by adding adversarial perturbations to the source data, but we argue that the threat has not been eliminated yet. This paper presents MagDR, a mask-guided detection and reconstruction pipeline for defending deep-fakes from adversarial attacks. MagDR starts with a detec-tion module that deﬁnes a few criteria to judge the abnor-mality of the output of deepfakes, and then uses it to guide 1This work was supported by NSFC under Grant 61972312 and by the
Key Research and Development Program of Shaanxi under Grant 2020GY-002. a learnable reconstruction procedure. Adaptive masks are extracted to capture the change in local facial regions. In experiments, MagDR defends three main tasks of deepfakes, and the learned reconstruction pipeline transfers across in-put data, showing promising performance in defending both black-box and white-box attacks. 1.

Introduction
Deepfakes originally appeared as a neutral technology that can synthesize images with the human face replaced by another identity. While the technique beneﬁts the com-munity in the scenarios of e.g. creating new characters or decorate them with vivid facial expressions, it gradually be-9014      
comes infamous for the unethical applications (e.g. swap fake of celebrities into pornographic videos, or generate a fraud video that delivers fake and malicious messages).
To avoid negative impacts to the public, researchers started to develop algorithms to detect the images and videos that have been contaminated by deepfakes [36, 56]. However, the follow-up research [2, 10, 31] quickly realized that these detectors are easily fooled by adversarial perturbations. An-other way to confront deepfakes is to add adversarial per-turbations to the source image so that the output is severely damaged [38, 53]. This was believed to be more robust than the deepfakes detectors.
However, in this paper, we reveal the feasibility of de-fending the adversarial attacks to deepfakes. We propose a framework named mask-guided detection and recon-struction (MagDR). It starts with deﬁning a few criteria (e.g., SSIM, PSNR, etc.) that are sensitive to the abnormal-ity of the outputs. Then, a mask-guided detector is trained to judge, from the output image, whether the input image has been contaminated. If yes, a reconstruction algorithm follows to eliminate the damage of the adversarial perturba-tions and recover the desired output.
A highlight of our approach is that we maintain a num-ber of masks and use them to provide auxiliary informa-tion in the detection and reconstruction procedures. The masks can be learned from an individual training process, and each of them corresponds to a speciﬁc part of the hu-man face. Guided by the masks, the detector can be par-titioned into two components which detect distortion and inconsistency, both of which indicate the regions that are likely to be contaminated. To reconstruct the desired output, we design a pipeline containing several modules and equip them with a changeable execution order and adjustable pa-rameters. Then, we perform an adaptive optimization that suppresses all the pre-deﬁned criteria and produce the re-covered output.
We evaluate our approach on two popular datasets, namely, FaceForensics++ [37] and CelebA [23]. We cor-respond three image-to-image translation methods, Cycle-GAN [57], StarGAN [6], and GANimation [35], to the three main functions of deepfakes, face editing, facial reenact-ment, and face replacement, respectively. We investigate two settings, one is the oblivious attack in which the attack-ers transfer the perturbations computed on original deep-fakes models to the defender, and the other is the adaptive attack in which the parameters of the defenders are known to the attacker. Experiments show that deepfakes are vul-nerable in both scenarios, but MagDR is able to eliminate the impact of the attack in most cases. Typical examples are shown in Figure 1. MagDR also shows advantages in exten-sive experiments against state-of-the-art adversarial attack-ers [4, 25] and defenders [24, 27, 26, 38, 39, 30, 9, 48].
Interestingly, MagDR is able to transfer across different scenarios, demonstrating its ability in both black-box and white-box attacks, and implying that adversarial perturba-tions are detectable by some common rules.
The contributions of this paper are as follows:
• We reveal that the threats of deepfakes have not yet been eliminated by adding adversarial perturbations to the input image or videos.
• We ﬁnd that the corruption to image-to-image transla-tion can inﬂuence either a part of the image or the en-tire image. The proposed mask-guided design follows this property and achieves satisfying performance.
• We propose a heuristic, hierarchical reconstruction module for each conditional attribute patch. We adjust it through a progressive approach, which can largely reduce the computational costs. Therefore, we verify that different regions are complementary in recover-ing the detailed textures, and the layer-by-layer archi-tecture with a proper execution order can enhance the performance of defense. 2.