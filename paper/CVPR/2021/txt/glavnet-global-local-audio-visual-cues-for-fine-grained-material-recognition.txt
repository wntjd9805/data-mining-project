Abstract
In this paper, we aim to recognize materials with com-bined use of auditory and visual perception. To this end, we construct a new dataset named GLAudio that consists of both the geometry of the object being struck and the sound captured from either modal sound synthesis (for vir-tual objects) or real measurements (for real objects). Be-sides global geometries, our dataset also takes local ge-ometries around different hitpoints into consideration. This local information is less explored in existing datasets. We demonstrate that local geometry has a greater impact on the sound than the global geometry and offers more cues in ma-terial recognition. To extract features from different modal-ities and perform proper fusion, we propose a new deep neural network GLAVNet that comprises multiple branches and a well-designed fusion module. Once trained on GLAu-dio, our GLAVNet provides state-of-the-art performance on material identiﬁcation and supports ﬁne-grained material categorization. 1.

Introduction
A fundamental problem in computer vision is that of identifying material categories, e.g., metal, glass or wood, from RGB images. In many applications, such as 3D scene understanding [18] and robot control [17], materials of ob-jects provide useful hints for substantially improving the performance [16].
As an active area of research, there have been quite a few works on material recognition, ranging from tradi-tional solutions relying on hand-designed image features
[29, 23, 40] to convolutional neural networks (CNNs) [55, 53] combined with large-scale datasets, e.g., Flickr Material
Database (FMD) [44] and Materials in Context Database (MINC) [7]. Since these methods and most others in litera-ture use solely the visual modality, rich surface textures or geometrical variations are required to make the recognition
† Corresponding Authors. (a) Global geometry (b) Local geometry (c) Spectrogram
Figure 1. Impact of the hitpoint on the generation of sound. De-spite the same global geometry (a), different local geometries around the hitpoints (b) may lead to distinct auditory features (c). process robust.
However, there are many scenarios in which strong vi-sual ambiguities exist, especially for smooth and texture-less surfaces, as a given material can take on many differ-ent appearances depending on the viewpoint, illumination and shape [4]. Worse still, the visual appearance may be signiﬁcantly altered by painting or coating (e.g., a wooden sculpture covered with gold lacquer), making the visual per-ception unreliable. One way of tackling this problem is to involve additional sensory modalities to complement the vi-sual perception. Recent research conﬁrms that sound aug-ments visual inputs and is an important modality for identi-fying materials [2, 36, 57, 48].
In this paper, we aim to identify materials of rigid ob-jects that appear in our daily life. To this end, we construct a new dataset, called GLAudio, containing both auditory and visual perception of each object. In this dataset, every object (virtual or real) is encoded in a voxelized representa-tion while every impact sound is generated either by modal sound synthesis or by striking a real object. Compared with some existing sound datasets such as Sound-20K [57] and
RSAudio [48], the proposed GLAudio dataset offers two 14433
main improvements. Firstly, we incorporate the informa-tion of hitpoint producing the sound into our dataset. The hitpoint is encoded in the form of a local geometry around it and produces discriminative features for the sound, as demonstrated in Fig. 1. In some situations, this local ge-ometry is more important than the global geometry of the whole object in generating the sound, e.g., in knocking the handhold of a cup. Secondly, to our knowledge our dataset is the ﬁrst one that contains ﬁne-grained materials. For in-stance, our dataset has 10 different metal classes including aluminum, iron, gold, etc. This opens up new opportuni-ties for solving more complex problems like testing gold for discerning the real from the fake one.
Based on our GLAudio dataset, we design a deep neu-ral network, i.e., GLAVNet, to infer material information from both visual and auditory cues. We show that the fu-sion of global and local shapes of the object outperforms previous methods which rely solely on the use of global information. We also demonstrate that GLAVNet enables
ﬁne-grained material recognition which is extremely chal-lenging for these previous works.
In summary, we make the following contributions:
• We introduce the ﬁrst audio-visual dataset, GLAudio, that contains both global geometry of a 3D object and local geometry around the hitpoint.
• We propose a new deep neural network, GLAVNet, that supports ﬁne-grained material recognition with the help of GLAudio.
• Once GLAVNet is trained, our method achieves state-of-the-art performance on material recognition and en-ables fast inference. 2.