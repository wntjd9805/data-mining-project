Abstract
Synthetic datasets play a critical role in pre-training
CNN models for optical ﬂow, but they are painstaking to generate and hard to adapt to new applications. To au-tomate the process, we present AutoFlow, a simple and effective method to render training data for optical ﬂow that optimizes the performance of a model on a target dataset. AutoFlow takes a layered approach to render syn-thetic data, where the motion, shape, and appearance of each layer are controlled by learnable hyperparameters.
Experimental results show that AutoFlow achieves state-of-the-art accuracy in pre-training both PWC-Net and RAFT.
Our code and data are available at autoflow-google. github.io. 1.

Introduction
Datasets have been a driving force for the develop-ment of AI algorithms. Convolutional neural networks (CNNs) [26] were proposed in the 1990’s but were not widely adopted for vision tasks until the early 2010’s, with the advent of AlexNet [24]. One key ingredient for deep
CNN models was the large amount of manually labeled im-ages, e.g., from ImageNet [41]. The performance gain by
AlexNet over shallow models stimulated a paradigm shift in high-level vision tasks. Since then, new models have been invented in rapid succession, even achieving “superhuman” performance on image classiﬁcation tasks [14, 15].
Manual labeling, however, cannot provide reliable ground truth for a variety of low-level vision tasks like op-tical ﬂow and stereo. Since these labels are either difﬁcult or impossible to obtain, synthetic data play a key role in en-abling deep models to perform well on such tasks. For ex-ample, all top-performing CNN models for optical ﬂow are pre-trained on two large synthetic datasets, FlyingChairs [8] and FlyingThings3D [32], before being ﬁne-tuned on lim-ited target datasets, e.g., Sintel [3] and KITTI [12].
However, the success of FlyingChairs raises some in-teresting questions. For example, how realistic should the rendering be? Several new datasets have been devel-oped to be more realistic than FlyingChairs, such as vir-tual KITTI [11], VIPER [38], and REFRESH [30], but none of them have proven more effective than FlyingChairs and
FlyingThings3D at pre-training models. In fact, a compre-hensive study has revealed that “realism is overrated” [32].
There are some hypotheses for why FlyingChairs works, e.g., that it has been designed to match the motion statis-tics of Sintel, or that it has many thin structures and ﬁne motion details. However, it still remains unclear what set of 10093
principles makes an effective optical ﬂow dataset.
To address these questions, we argue that we should make explicit the objective function for rendering training data. We formulate the generation of training data as a joint optimization problem, which couples rendering the data with training the model. This generation process de-pends on a set of hyperparameters being optimized. The hyperparameters are evaluated by the performance of the trained model on a target dataset, as shown in Fig. 1.
To understand what matters, we ask: how simple can the rendering be? Thus, we start from an even simpler ren-dering pipeline than FlyingChairs, a 2D layered approach that requires neither manual labeling nor 3D models. The motion and shape of each layer are randomly generated ac-cording to hyperparameters, as shown in Fig. 2. We can then learn the rendering hyperparameters to optimize the performance of a model on a target dataset.
This simple rendering pipeline is surprisingly effective at generating training datasets for optical ﬂow. Trained on its rendered data from scratch, both the recent RAFT model and the widely-used PWC-Net model obtain consistent im-provements in accuracy on Sintel and KITTI over the same models trained on FlyingChairs (Fig. 1 and Table 1). Fur-ther, using 4 AutoFlow examples with augmentation results in lower errors on Sintel.ﬁnal for RAFT than using 22,872
FlyingChairs examples with augmentation. More interest-ingly, the gap between PWC-Net and RAFT becomes small when trained on enough AutoFlow examples.
An analysis of the rendered data also suggests some in-teresting properties. For example, the motion statistics of the AutoFlow dataset and its augmented version do not re-semble those of Sintel (Fig. 8) and underrepresent small motions. Though at ﬁrst glance this distribution may seem abnormal, there may be a simple, intuitive explanation: tiny motion matters little in the overall error.
To summarize, our contributions are the following.
• We have introduced, to our knowledge, the ﬁrst learn-ing approach to render training data for optical ﬂow.
• AutoFlow compares favorably against FlyingChairs and FlyingThings3D in pre-training RAFT.
• AutoFlow also leads to a signiﬁcant performance gain for PWC-Net, even competitive against RAFT.
• We present a detailed analysis of what features are im-portant to dataset generation for optical ﬂow. 2.