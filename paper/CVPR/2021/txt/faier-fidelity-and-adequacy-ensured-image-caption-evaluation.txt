Abstract
Image caption evaluation is a crucial task, which in-volves the semantic perception and matching of image and text. Good evaluation metrics aim to be fair, comprehen-sive, and consistent with human judge intentions. When hu-mans evaluate a caption, they usually consider multiple as-pects, such as whether it is related to the target image with-out distortion, how much image gist it conveys, as well as how ﬂuent and beautiful the language and wording is. The above three different evaluation orientations can be sum-marized as ﬁdelity, adequacy, and ﬂuency. The former two rely on the image content, while ﬂuency is purely related to linguistics and more subjective. Inspired by human judges, we propose a learning-based metric named FAIEr to ensure evaluating the ﬁdelity and adequacy of the captions. Since image captioning involves two different modalities, we em-ploy the scene graph as a bridge between them to represent both images and captions. FAIEr mainly regards the visual scene graph as the criterion to measure the ﬁdelity. Then for evaluating the adequacy of the candidate caption, it high-lights the image gist on the visual scene graph under the guidance of the reference captions. Comprehensive exper-imental results show that FAIEr has high consistency with human judgment as well as high stability, low reference de-pendency, and the capability of reference-free evaluation. 1.

Introduction
Good evaluations lead to continuous progress in many computer vision tasks. Different from other visual tasks, the evaluation of the image captioning [25, 39, 4, 40, 33, 38, 8, 7] is very difﬁcult because the outputs of the image captioning are in the form of natural language and need to mirror the content of the given image, which involves multi-modals. Since image captioning can be regarded as trans-lating the visual information into natural language, early image captioning methods follow the evaluation mode of the machine translation [30, 31, 12], which ignore the vi-sual modal information. That is, the candidate caption is scored only based on the similarity with the human-labeled reference captions of the target image. In this formulation, early popular metrics [26, 9, 22, 32] measure the similar-ity of two sentences by the n-gram overlap, which results in low robustness to text ambiguity. To address this de-ﬁciency, SPICE [3] breaks the shackles of sentence struc-ture by using the scene graph representations, which can measure the semantic similarity of sentences. However, “A picture paints a thousand words.” Limited numbers of ref-erence captions are hard to cover all contents in an image.
Therefore, the reference-based metrics usually lead to bi-ased evaluations. With recent breakthroughs, some studies
[15, 6, 14] introduce the image information into the cap-tion evaluation, which measures the similarity between the candidate caption and both the target image and reference captions simultaneously.
Developing automatic metrics aims to replace human judges, so the goal of a good metric is to reveal the hu-man’s evaluation intentions. From the human perspective, as shown in Fig.1(a), it is fundamental that the messages conveyed by a caption are related to the given image with no extra or distortion. Then, an adequate caption should describe the gist of the image concerned by humans [35].
Moreover, idiomatic wording and beautiful sentences will further get a higher score. We summarize them as three evaluating orientations: ﬁdelity, adequacy, and ﬂuency, which we think roughly form a multi-aspect criteria for im-age caption evaluation, to some extent similar to the ma-chine translation evaluation systems [37, 27, 36]. With no consideration of image information, previous reference-based metrics cannot assess ﬁdelity adequately, leading to biased evaluations. For example, if the candidate caption contains information not included in references but in the image, the reference-based metrics will fail to give a cor-rect evaluation. It is also hard for those metrics based on n-gram overlap to ensure the evaluation of adequacy. Recent 14050
Reference Captions
•
• two children are standing in a grassy field. two small children with umbrellas in a field  near the shore.
Candidate Caption
Fidelity
Adequacy
There are some trees near the water.
A couple of kids on top of a lush green field.
✓
✓
✓
Reference
Caption
Reference
Caption
Reference
Caption fidelity visual scene graph
F u s e
Match
Candidate
Caption union scene graph candidate scene graph reference scene graphs adequacy (a) (b)
Figure 1. (a) An example image with human-labeled reference captions (from MS COCO [23]) and two candidates, showing the criterion of ﬁdelity and adequacy. (b) The designing principle of FAIEr. learning-based metrics [15, 6, 14] take ﬁdelity into account by involving image information, yet fail to disassemble the complex human evaluation intentions. Beyond ﬁdelity and adequacy, ﬂuency is deemed to measure the quality of lan-guage expression more subjectively, which concerns little the image content and is purely related to linguistics.
In this paper, we focus on the ﬁrst two objective orienta-tions and propose a Fidelity and Adequacy ensured Image
Caption Evaluation metric named FAIEr. For fair evalua-tion, it gives the correct captions deserved scores, and ones containing more image gist will get more awards. FAIEr takes the image, reference captions, and candidate caption as input. The evaluation of ﬁdelity mainly depends on the matching between the image and the candidate caption. To reward the adequacy, we need to compare the candidate and the reference captions (as the references convey the humans captured gist of the image). Therefore, the problem can be formulated as a multi-instance image-text matching task.
To address such complex cross-modal matching task, FAIEr uses scene graphs as the intermediation to dissect and align the visual and textual information and then calculates the multi-modal similarity by scene graphs fusing and match-ing. As shown in Fig.1(b), FAIEr ﬁrstly represents the input image and captions as scene graphs. Taking the visual scene graph as the foundation, FAIEr further highlights the crucial contents that draw much human attention by fusing the ref-erence and visual scene graphs into a union scene graph.
The highlighted nodes will have larger weights in the eval-uation process, which incorporates human evaluation inten-tion intuitively. Finally, FAIEr scores the candidate cap-tion by calculating the similarity between the candidate and union scene graphs.
Comprehensive experiments on Composite Dataset [1],
Flickr8k [13], and PASCAL-50S [32] show the high con-sistency of FAIEr with human judgement, and verify the ad-vantages of scene graph representations. In practical appli-cation scenarios, it is quite common that no human-labeled reference caption is available. Beneﬁtting from the ﬂexible scene graphs fusion module, FAIEr has the reference-free evaluation capability, which can readily address this issue. 2.