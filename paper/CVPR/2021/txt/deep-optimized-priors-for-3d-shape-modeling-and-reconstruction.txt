Abstract   Learned Shape Prior
Many learning-based approaches have difﬁculty scaling to unseen data, as the generality of its learned prior is lim-ited to the scale and variations of the training samples.
This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that ﬁxes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers con-strained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our frame-work using the implicit surface representation and validate the efﬁcacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input.
Experimental results show that our approach compares fa-vorably with the state-of-the-art methods in terms of both generality and accuracy. 1.

Introduction
Deep generative models have brought impressive ad-vances to the state-of-the-art across a wide variety of gen-erative tasks, including 2D image synthesis and 3D shape reconstruction. At the moment, it is widely believed that these leaps in performance come primarily from the real-istic priors learned from a large amount of training data.
Based on this observation, most of the previous 3D learning approaches focus on learning stronger priors during training and strictly respecting the learned prior at test time. Specif-ically, there are two common ways to leverage the learned
*Equal contribution
†Correspondence to Kui Jia <kuijia@scut.edu.cn> (b) (a) (c)
Real Distribution 
Ground-truth Mesh
Figure 1: The shape prior learned from the limited train-ing data cannot capture the full landscape of the real data distribution. Common practice that uses a ﬁxed pre-trained generator is constrained within the prior (path a) and thus fails to model the unseen data lying outside the prior, even with latent code optimization at test time. Optimizing a ran-domly initialized generator, on the other hand, is prone to be trapped in a local minimum due to the complex energy land-scape (path b). Whereas the pre-train prior could provide a good initialization in a forward pass, we propose to fur-ther optimize the parameters of the prior and the latent code according to the task-speciﬁc constraints at test time. We show in this work that the proposed framework can effec-tively break the barriers of pre-trained prior and generalize to the unseen data that is out of the prior domain (path c).
Hence, our approach can generate results (ending point of path c) closest to the ground truth (star point on the real data manifold) compared to the other learning methods. shape priors. One is to train an encoder to retrieve the most likely prior by mapping the input into the latent code, which is a low-dimensional representation of the shape prior. The other is to optimize the latent code until its decoded out-put achieves a minimal loss. Note that both methods ﬁx the learned prior/generator once the training is completed as the prior is considered to be the most valuable asset in a learning-based approach. However, is this the best strategy of using the prior in a 3D learning task? 3269
The quality of the learned prior highly relies on the scale and diversities of the training examples. Yet, even with a large amount of data, the prior learned by the neural net-work may still be a crude approximation of the real data distribution (see Figure 1), making the network vulnera-ble to unseen data. This is particularly true with the 3D learning tasks, where the ground-truths are notoriously dif-ﬁcult to obtain, which greatly limits the scale of the train-ing samples. Optimization-based approaches that leverage constraints from data, e.g. multi-view consistency, do not require any training to be usable. However, they are strict with the inputs and tend to fail on the sparsity of the data (e.g. single/sparse-view reconstruction) or the physical mis-alignment (e.g. unregistered/mismatched images).
To alleviate the generality issue of the learning-based ap-proach while maintaining a friendly requirement for the in-puts, we advocate a new 3D learning paradigm that connects the good ends of both learning-based and optimization-based approaches. In particular, we propose that the pre-trained data prior could obtain a maximum generality if it is optimized, rather than ﬁxed, according to the data con-straints at test time. Our approach shares a similar incentive with deep image priors [30], where high-quality images can be synthesized simply by optimizing an untrained and ran-domly initialized deep generator. However, unlike image synthesis, we show that optimizing a randomly initialized neural network often fails to achieve satisfactory results in 3D learning, especially in highly ill-posed conﬁgurations, such as sparse-view based 3D reconstruction.
Instead of ﬁxing the priors or using random priors, we propose to jointly optimize the pre-trained shape prior and the latent code towards the input physical measurements at test time. Our observation is that though the learned prior cannot capture the full landscape of the real data distribu-tion, it does provide a fairly good initialization for search-ing for the optimal solution in the entire embedding space (Figure 1). Further, by introducing the physically based op-timization, the searching path could break the barrier of the pre-trained priors and converge at some point on the real prior which is more realistic but unreachable by only searching inside the learned priors (Figure 1). While it is possible that the optimization may lead to 3D shapes that do not look plausible, we propose that an l2 regularization works surprisingly well in regularizing the searching space.
We materialize our idea using the implicit surface rep-resentation, as it is ﬂexible to handle shapes with arbitrary topologies. We show that our proposed approach is a gen-eral 3D learning framework that supports a wide range of downstream applications, including shape modeling and re-construction, with various forms of inputs. We also demon-strate that our framework can signiﬁcantly improve the gen-erality of the learning-based approach, even in the pres-the ence of highly sparse or collapsed observations, e.g. sparse point clouds obtained from the 3D scanning, sin-gle or sparse views of the object of interest, etc. We ver-ify the effectiveness of our approach in a variety of chal-lenging tasks, including shape auto-encoding, sparse-view reconstruction and sparse point cloud reconstruction. Ex-perimental results show that our approach is superior to the state-of-the-arts both quantitatively and qualitatively. 2.