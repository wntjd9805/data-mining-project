Abstract
Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasi-ble for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the induc-tive bias of CNNs with the expressivity of transformers en-ables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efﬁciently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial in-formation, such as object classes, and spatial information, such as segmentations, can control the generated image.
In particular, we present the ﬁrst results on semantically-guided synthesis of megapixel images with transformers.
Project page at https://git.io/JLlvY. 1.

Introduction
Transformers are on the rise—they are now the de-facto standard architecture for language tasks [64, 50, 51, 5] and are increasingly adapted in other areas such as audio
[12] and vision [8, 15]. In contrast to the predominant vi-sion architecture, convolutional neural networks (CNNs), the transformer architecture contains no built-in inductive prior on the locality of interactions and is therefore free to learn complex relationships among its inputs. However, this generality also implies that it has to learn all relation-ships, whereas CNNs have been designed to exploit prior knowledge about strong local correlations within images.
Thus, the increased expressivity of transformers comes with quadratically increasing computational costs, because all pairwise interactions are taken into account. The result-ing energy and time requirements of state-of-the-art trans-former models thus pose fundamental problems for scaling them to high-resolution images with millions of pixels.
Observations that transformers tend to learn convolu-tional structures [15] thus beg the question: Do we have to re-learn everything we know about the local structure and regularity of images from scratch each time we train a vision model, or can we efﬁciently encode inductive im-age biases while still retaining the ﬂexibility of transform-ers? We hypothesize that low-level image structure is well described by a local connectivity, i.e. a convolutional ar-chitecture, whereas this structural assumption ceases to be effective on higher semantic levels. Moreover, CNNs not only exhibit a strong locality bias, but also a bias towards 12873
spatial invariance through the use of shared weights across all positions. This makes them ineffective if a more holistic understanding of the input is required.
Our key insight to obtain an effective and expressive model is that, taken together, convolutional and transformer architectures can model the compositional nature of our vi-sual world [44]: We use a convolutional approach to efﬁ-ciently learn a codebook of context-rich visual parts and, subsequently, learn a model of their global compositions.
The long-range interactions within these compositions re-quire an expressive transformer architecture to model distri-butions over their consituent visual parts. Furthermore, we utilize an adversarial approach to ensure that the dictionary of local parts captures perceptually important local struc-ture to alleviate the need for modeling low-level statistics with the transformer architecture. Allowing transformers to concentrate on their unique strength — modeling long-range relations — enables them to generate high-resolution images as in Fig. 1, a feat which previously has been out of reach. Our formulation directly gives control over the gen-erated images by means of conditioning information regard-ing desired object classes or spatial layouts. Finally, experi-ments demonstrate that our approach retains the advantages of transformers by outperforming previous codebook-based state-of-the-art approaches based on convolutional architec-tures. 2.