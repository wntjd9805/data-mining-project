Abstract
Weakly supervised video anomaly detection (WS-VAD) is to distinguish anomalies from normal events based on dis-criminative representations. Most existing works are lim-ited in insufﬁcient video representations. In this work, we develop a multiple instance self-training framework (MIST) to efﬁciently reﬁne task-speciﬁc discriminative representa-tions with only video-level annotations. In particular, MIST is composed of 1) a multiple instance pseudo label gener-ator, which adapts a sparse continuous sampling strategy to produce more reliable clip-level pseudo labels, and 2) a self-guided attention boosted feature encoder that aims to automatically focus on anomalous regions in frames while extracting task-speciﬁc representations. Moreover, we adopt a self-training scheme to optimize both compo-nents and ﬁnally obtain a task-speciﬁc feature encoder. Ex-tensive experiments on two public datasets demonstrate the efﬁcacy of our method, and our method performs compara-bly to or even better than existing supervised and weakly su-pervised methods, speciﬁcally obtaining a frame-level AUC 94.83% on ShanghaiTech. 1.

Introduction
Video anomaly detection (VAD) aims to temporally or spatially localize anomalous events in videos [33]. As in-creasingly more surveillance cameras are deployed, VAD is playing an increasingly important role in intelligent surveil-lance systems to reduce the manual work of live monitoring.
Although VAD has been researched for years, develop-ing a model to detect anomalies in videos remains challeng-ing, as it requires the model to understand the inherent dif-ferences between normal and abnormal events, especially anomalous events that are rare and vary substantially. Pre-vious works treat VAD as an unsupervised learning task
∗Corresponding author
Decision boundary
Refine the  discriminative  representation
Feature space
𝐸(cid:3020)(cid:3008)(cid:3002)
𝐺
𝑦(cid:3548)(cid:3036)(cid:2878)(cid:2870)
𝑦(cid:3548)(cid:3036)(cid:2878)(cid:2869) y(cid:3548)(cid:3036)
Clip-Level Labeled 
Normal Video  (cid:3041)
𝑉
Video-Level Labeled 
Abnormal Video  (cid:3028)
Input/Output of 
𝑉
Pseudo Clip-Level Labeled
Abnormal Video 
Input/Output of 
Normal (cid:3028)
Abnormal
𝑉
𝑮
𝑬𝑺𝑮𝑨
Figure 1: Our proposed MIST ﬁrst assign clip-level pseudo labels ˆY a = {ˆya i } to anomaly videos with the help of a pseudo label generator G. Then, MIST leverages informa-tion from all videos to reﬁne a self-guided attention boosted feature encoder ESGA.
[29, 14, 7, 15, 13, 5, 32] , which encodes the usual pat-tern with only normal training samples, and then detects the distinctive encoded patterns as anomalies. Here, we aim to address the weakly supervised video anomaly detection (WS-VAD) problem [20, 31, 28, 34, 24] because obtaining video-level labels is more realistic and can produce more re-liable results than unsupervised methods. More speciﬁcally, existing methods in WS-VAD can be categorized into two classes, i.e. encoder-agnostic and encoder-based methods.
The encoder-agnostic methods [20, 28, 24] utilize task-agnostic features of videos extracted from a vanilla feature encoder denoted as E (e.g. C3D [21] or I3D [2]) to esti-mate anomaly scores. The encoder-based methods [34, 31] 14009
train both the feature encoder and classiﬁer simultaneously.
The state-of-the-art encoder-based method is Zhong et al.
[31], which formulates WS-VAD as a label noise learn-ing problem and learns from the noisy labels ﬁltered by a label noise cleaner network. However, label noise results from assigning video-level labels to each clip. Even though the cleaner network corrects some of the noisy labels in the time-consuming iterative optimization, the reﬁnement of representations progresses slowly as these models are mistaught by seriously noisy pseudo labels at the beginning.
We ﬁnd that the existing methods have not considered training a task-speciﬁc feature encoder efﬁciently, which of-fers discriminative representations for events under surveil-lance cameras. To overcome this problem for WS-VAD, we develop a two-stage self-training procedure (Figure 1) that aims to train a task-speciﬁc feature encoder with only video-level weak labels. In particular, we propose a Multi-ple Instance Self-Training framework (MIST) that consists of a multiple instance pseudo label generator and a self-guided attention boosted feature encoder ESGA. 1) MIL-pseudo label generator. The MIL framework is well ver-iﬁed in weakly supervised learning. MIL-based methods can generate pseudo labels more accurately than those sim-ply assigning video-level labels to each clip [31]. Moreover, we adopt a sparse continuous sampling strategy that can force the network to pay more attention to context around the most anomalous part. 2) Self-guided attention boosted feature encoder. Anomalous events in surveillance videos may occur in any place and with any size [11], while in commonly used action recognition videos, the action usu-ally appears with large motion [3, 4]. Therefore, we utilize the proposed self-guided attention module in our proposed feature encoder to emphasize the anomalous regions with-out any external annotation [11] but clip-level annotations of normal videos and clip-level pseudo labels of anomalous videos. For our WS-VAD modelling, we introduce a deep
MIL ranking loss to effectively train the multiple instance pseudo label generator. In particular, for deep MIL rank-ing loss, we adopt a sparse-continuous sampling strategy to focus more on the context around the anomalous instance.
To obtain a task-speciﬁc feature encoder with smaller domain-gap, we introduce an efﬁcient two-stage self-training scheme to optimize the proposed framework. We use the features extracted from the original feature encoder to produce its corresponding clip-level pseudo labels for anomalous videos by the generator G. Then, we adopt these pseudo labels and their corresponding abnormal videos as well as normal videos to reﬁne our improved feature en-coder ESGA (as demonstrated in Figure 1). Therefore, we can acquire a task-speciﬁc feature encoder that provides dis-criminative representations for surveillance videos.
The extensive experiments based on two different feature encoders, i.e. C3D [21] and I3D [2] show that our frame-work MIST is able to produce a task-speciﬁc feature en-coder. We also compare the proposed framework with other encoder-agnostic methods on two large datasets i.e.
, UCF-Crime [20] and ShanghaiTech[15]. In addition, we run ablation studies to evaluate our proposed sparse contin-uous sampling strategy and self-guided attention module.
We also illustrate some visualized results to provide a more intuitive understanding of our approach. Our experiments demonstrate the effectiveness and efﬁciency of MIST. 2.