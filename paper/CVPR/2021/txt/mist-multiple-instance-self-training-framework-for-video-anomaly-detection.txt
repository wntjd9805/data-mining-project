Abstract
Weakly supervised video anomaly detection (WS-VAD) is to distinguish anomalies from normal events based on dis-criminative representations. Most existing works are lim-ited in insufï¬cient video representations. In this work, we develop a multiple instance self-training framework (MIST) to efï¬ciently reï¬ne task-speciï¬c discriminative representa-tions with only video-level annotations. In particular, MIST is composed of 1) a multiple instance pseudo label gener-ator, which adapts a sparse continuous sampling strategy to produce more reliable clip-level pseudo labels, and 2) a self-guided attention boosted feature encoder that aims to automatically focus on anomalous regions in frames while extracting task-speciï¬c representations. Moreover, we adopt a self-training scheme to optimize both compo-nents and ï¬nally obtain a task-speciï¬c feature encoder. Ex-tensive experiments on two public datasets demonstrate the efï¬cacy of our method, and our method performs compara-bly to or even better than existing supervised and weakly su-pervised methods, speciï¬cally obtaining a frame-level AUC 94.83% on ShanghaiTech. 1.

Introduction
Video anomaly detection (VAD) aims to temporally or spatially localize anomalous events in videos [33]. As in-creasingly more surveillance cameras are deployed, VAD is playing an increasingly important role in intelligent surveil-lance systems to reduce the manual work of live monitoring.
Although VAD has been researched for years, develop-ing a model to detect anomalies in videos remains challeng-ing, as it requires the model to understand the inherent dif-ferences between normal and abnormal events, especially anomalous events that are rare and vary substantially. Pre-vious works treat VAD as an unsupervised learning task
âˆ—Corresponding author
Decision boundary
Refine the  discriminative  representation
Feature space
ğ¸(cid:3020)(cid:3008)(cid:3002)
ğº
ğ‘¦(cid:3548)(cid:3036)(cid:2878)(cid:2870)
ğ‘¦(cid:3548)(cid:3036)(cid:2878)(cid:2869) y(cid:3548)(cid:3036)
Clip-Level Labeled 
Normal Video  (cid:3041)
ğ‘‰
Video-Level Labeled 
Abnormal Video  (cid:3028)
Input/Output of 
ğ‘‰
Pseudo Clip-Level Labeled
Abnormal Video 
Input/Output of 
Normal (cid:3028)
Abnormal
ğ‘‰
ğ‘®
ğ‘¬ğ‘ºğ‘®ğ‘¨
Figure 1: Our proposed MIST ï¬rst assign clip-level pseudo labels Ë†Y a = {Ë†ya i } to anomaly videos with the help of a pseudo label generator G. Then, MIST leverages informa-tion from all videos to reï¬ne a self-guided attention boosted feature encoder ESGA.
[29, 14, 7, 15, 13, 5, 32] , which encodes the usual pat-tern with only normal training samples, and then detects the distinctive encoded patterns as anomalies. Here, we aim to address the weakly supervised video anomaly detection (WS-VAD) problem [20, 31, 28, 34, 24] because obtaining video-level labels is more realistic and can produce more re-liable results than unsupervised methods. More speciï¬cally, existing methods in WS-VAD can be categorized into two classes, i.e. encoder-agnostic and encoder-based methods.
The encoder-agnostic methods [20, 28, 24] utilize task-agnostic features of videos extracted from a vanilla feature encoder denoted as E (e.g. C3D [21] or I3D [2]) to esti-mate anomaly scores. The encoder-based methods [34, 31] 14009
train both the feature encoder and classiï¬er simultaneously.
The state-of-the-art encoder-based method is Zhong et al.
[31], which formulates WS-VAD as a label noise learn-ing problem and learns from the noisy labels ï¬ltered by a label noise cleaner network. However, label noise results from assigning video-level labels to each clip. Even though the cleaner network corrects some of the noisy labels in the time-consuming iterative optimization, the reï¬nement of representations progresses slowly as these models are mistaught by seriously noisy pseudo labels at the beginning.
We ï¬nd that the existing methods have not considered training a task-speciï¬c feature encoder efï¬ciently, which of-fers discriminative representations for events under surveil-lance cameras. To overcome this problem for WS-VAD, we develop a two-stage self-training procedure (Figure 1) that aims to train a task-speciï¬c feature encoder with only video-level weak labels. In particular, we propose a Multi-ple Instance Self-Training framework (MIST) that consists of a multiple instance pseudo label generator and a self-guided attention boosted feature encoder ESGA. 1) MIL-pseudo label generator. The MIL framework is well ver-iï¬ed in weakly supervised learning. MIL-based methods can generate pseudo labels more accurately than those sim-ply assigning video-level labels to each clip [31]. Moreover, we adopt a sparse continuous sampling strategy that can force the network to pay more attention to context around the most anomalous part. 2) Self-guided attention boosted feature encoder. Anomalous events in surveillance videos may occur in any place and with any size [11], while in commonly used action recognition videos, the action usu-ally appears with large motion [3, 4]. Therefore, we utilize the proposed self-guided attention module in our proposed feature encoder to emphasize the anomalous regions with-out any external annotation [11] but clip-level annotations of normal videos and clip-level pseudo labels of anomalous videos. For our WS-VAD modelling, we introduce a deep
MIL ranking loss to effectively train the multiple instance pseudo label generator. In particular, for deep MIL rank-ing loss, we adopt a sparse-continuous sampling strategy to focus more on the context around the anomalous instance.
To obtain a task-speciï¬c feature encoder with smaller domain-gap, we introduce an efï¬cient two-stage self-training scheme to optimize the proposed framework. We use the features extracted from the original feature encoder to produce its corresponding clip-level pseudo labels for anomalous videos by the generator G. Then, we adopt these pseudo labels and their corresponding abnormal videos as well as normal videos to reï¬ne our improved feature en-coder ESGA (as demonstrated in Figure 1). Therefore, we can acquire a task-speciï¬c feature encoder that provides dis-criminative representations for surveillance videos.
The extensive experiments based on two different feature encoders, i.e. C3D [21] and I3D [2] show that our frame-work MIST is able to produce a task-speciï¬c feature en-coder. We also compare the proposed framework with other encoder-agnostic methods on two large datasets i.e.
, UCF-Crime [20] and ShanghaiTech[15]. In addition, we run ablation studies to evaluate our proposed sparse contin-uous sampling strategy and self-guided attention module.
We also illustrate some visualized results to provide a more intuitive understanding of our approach. Our experiments demonstrate the effectiveness and efï¬ciency of MIST. 2.