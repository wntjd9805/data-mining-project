Abstract
We address the problem of weakly-supervised seman-tic segmentation (WSSS) using bounding box annotations.
Although object bounding boxes are good indicators to they do not specify ob-segment corresponding objects, ject boundaries, making it hard to train convolutional neu-ral networks (CNNs) for semantic segmentation. We ﬁnd that background regions are perceptually consistent in part within an image, and this can be leveraged to discriminate foreground and background regions inside object bounding boxes. To implement this idea, we propose a novel pool-ing method, dubbed background-aware pooling (BAP), that focuses more on aggregating foreground features inside the bounding boxes using attention maps. This allows to ex-tract high-quality pseudo segmentation labels to train CNNs for semantic segmentation, but the labels still contain noise especially at object boundaries. To address this problem, we also introduce a noise-aware loss (NAL) that makes the networks less susceptible to incorrect labels. Experimen-tal results demonstrate that learning with our pseudo la-bels already outperforms state-of-the-art weakly- and semi-supervised methods on the PASCAL VOC 2012 dataset, and the NAL further boosts the performance. 1.

Introduction
Semantic segmentation is one of the fundamental tasks in computer vision, and has received a lot of attention over the last decades. It aims at assigning a semantic label to each pixel, which can be leveraged to various applications including scene understanding, autonomous driving, image editing, and robotics. Supervised methods based on convo-lutional neural networks (CNNs) [5, 40, 47] have achieved remarkable success in semantic segmentation, but they re-quire lots of training samples with pixel-level labels, which are extremely labor-intensive to annotate, to train networks.
∗Corresponding author. 1For MCG, we compute intersection-over-union (IoU) scores using pairs of segment proposals and bounding boxes, and choose the best one for each box.
Input image. Ground truth.
Ours.
Ours∗.
SDI [27].
GrabCut [48]. MCG1 [44]. WSSL [42].
Figure 1: Visual comparison of pseudo ground-truth labels. Our approach generates better segmentation labels than other WSSS methods using object bounding boxes (WSSL [42] and SDI [27]).
Hand-crafted methods (GrabCut [48] and MCG [44]) fail to seg-ment object boundaries. Ours∗: Ours with an indication of unreli-able regions. Best viewed in color.
Weakly-supervised semantic segmentation (WSSS) has re-cently been introduced to exploit a weak form of super-visory signals such as image-level labels [11, 14, 22, 28, 32, 56, 60], points [3], scribbles [35, 53, 54], and ob-ject bounding boxes [8, 27, 42, 52]. WSSS methods us-ing image-level labels typically leverage class activation maps (CAMs) [63], obtained from CNNs for image clas-siﬁcation using global average pooling (GAP), to localize objects. Since CAMs tend to highlight discriminative parts, these methods more or less resort to off-the-shelf saliency detectors [13, 21, 24, 33, 57]. This, however, requires ad-ditional pixel-level ground-truth annotations for salient ob-jects. Other approaches attempt to exploit object bounding boxes. They are easy to annotate compared to pixel-level labels and provide rich semantics to localize objects. The object bounding boxes, however, contain a mixture of fore-ground and background, and do not specify exquisite object boundaries. To overcome this, recent approaches [8, 27, 31] use off-the-shelf segmentation methods [44, 48].
We introduce a simple yet effective WSSS method us-ing bounding box annotations.
In particular, we investi-gate two aspects of this problem – How can we gener-ate high-quality but possibly noisy pixel-level labels (i.e., a pseudo ground truth) from object bounding boxes (Fig. 1)?
How can we train CNNs for semantic segmentation (e.g., 6913
DeepLab [5, 6]) with noisy segmentation labels? Motivated by the methods using image-level labels [1, 22, 32, 55, 56], for the ﬁrst aspect, we leverage a CNN for image classiﬁca-tion, instead of exploiting off-the-shelf segmentation meth-ods (e.g., [44, 48]). To this end, we propose a background-aware pooling (BAP) method using an attention map, en-abling discriminating foreground and background inside the bounding boxes. This allows to aggregate features within a foreground for image classiﬁcation, while dis-carding those for a background, resulting in more accu-rate CAMs, rather than mainly highlighting the most dis-criminative parts (e.g., faces in the person class) as in
GAP [36, 63]. Speciﬁcally, we retrieve background regions inside the bounding boxes, based on our ﬁnding that back-ground regions are perceptually consistent in part within an image. This provides attention maps for the background regions adaptively for individual images. We exploit the attention maps and CAMs, together with prototypical fea-tures, to generate pseudo ground-truth labels. For the sec-ond one, we introduce a noise-aware loss (NAL) to train
CNNs for semantic segmentation that makes the networks less susceptible to incorrect labels. Speciﬁcally, we exploit a conﬁdence map, using the distances between CNN fea-tures for prediction and classiﬁer weights for semantic seg-mentation, to compute a cross-entropy loss adaptively. Ex-perimental results demonstrate that our approach to using
BAP already outperforms the state of the art on the PAS-CAL VOC 2012 dataset [10], and the NAL further boosts the performance. We also demonstrate the effectiveness of our approach by extending it to the task of instance seg-mentation on the MS-COCO dataset [38]. We summarize the contributions of our work as follows:
• We introduce a novel pooling method for WSSS, dubbed
BAP, that uses bounding box annotations, allowing to generate high-quality pseudo ground-truth labels.
• We propose a NAL exploiting the distances between
CNN features for prediction and classiﬁer weights for se-mantic segmentation, lessening the inﬂuence of incorrect labels.
• We set a new state of the art on the PASCAL VOC 2012 dataset for weakly- and semi-supervised semantic seg-mentation. We also provide an extensive experimental analysis with ablation studies.
Our code and models are available online: https:// cvlab.yonsei.ac.kr/projects/BANA. 2.