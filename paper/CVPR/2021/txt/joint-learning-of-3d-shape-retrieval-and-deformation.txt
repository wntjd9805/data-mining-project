Abstract
Database with heterogeneous deformations
We propose a novel technique for producing high-quality 3D models that match a given target object image or scan.
Our method is based on retrieving an existing shape from a database of 3D models and then deforming its parts to match the target shape. Unlike previous approaches that in-dependently focus on either shape retrieval or deformation, we propose a joint learning procedure that simultaneously trains the neural deformation module along with the embed-ding space used by the retrieval module. This enables our network to learn a deformation-aware embedding space, so that retrieved models are more amenable to match the tar-get after an appropriate deformation. In fact, we use the embedding space to guide the shape pairs used to train the deformation module, so that it invests its capacity in learn-ing deformations between meaningful shape pairs. Further-more, our novel part-aware deformation module can work with inconsistent and diverse part-structures on the source shapes. We demonstrate the beneﬁts of our joint training not only on our novel framework, but also on other state-of-the-art neural deformation modules proposed in recent years. Lastly, we also show that our jointly-trained method outperforms various non-joint baselines. 1.

Introduction
Creating high-quality 3D models from a reference im-age or a scan is a laborious task, requiring signiﬁcant ex-pertise in 3D sculpting, meshing, and UV layout. While neural generative techniques for 3D shape synthesis hold promise for the future, they still lack the ability to create 3D models that rival the ﬁdelity, level of detail, and overall quality of artist-generated meshes [38]. Several recent tech-niques propose to directly retrieve a high-quality 3D model from a database and deform it to match a target image or point cloud, thereby approximating the target shape while preserving the quality of the original source model. These prior methods largely focus on one of two complementary subproblems: either retrieving an appropriate mesh from a database [26, 6], or training a neural network to deform a source to a target [14, 43, 49, 36]. In most cases, the static database mesh most closely matching the target is retrieved, (54 parameters, 24 constraints) (42 parameters, 18 constraints) (42 parameters, 24 constraints) (36 parameters, 27 constraints) (60 parameters, 33 constraints)
…
Our Joint Learning
Deform
Static Retrieval
Non-joint
Deformation-aware retrieval
Retrieval-aware deformation
Baselines
Target 2D image
Figure 1. Given an input target we use jointly-learned retrieval and deformation modules to ﬁnd a source model in a heterogeneous database and align it to the target. We demonstrate that our joint learning outperforms static retrieval and non-joint baselines.
Jointly trained and then deformed for a better ﬁt [19]. The retrieval step is not inﬂuenced by the subsequent deformation procedure, and thus ignores the possibility that a database shape with different global geometry nevertheless possess local details that will produce the best match after deformation.
Only a few works explicitly consider deformation-aware retrieval [34, 41]. However, in these works the deforma-tion module is a ﬁxed, non-trainable black box, which re-quires complete shapes (and not e.g., natural images or par-tial scans) as targets, does not handle varying shape struc-tures across the database, may necessitate time-consuming, manually-speciﬁed optimization of a ﬁtting energy, exhaus-tive enumeration of deformed variants, and does not support back-propagating gradients through it for directly translat-ing deformation error to retrieval error.
In this paper, we argue that retrieval and deformation should be equal citizens in a joint problem. Given a database of source models equipped with some parametric representation of deformations, our goal is to learn how to retrieve a shape from the database and predict the optimal deformation parameters so it best matches a given target. A key feature of our method is that both retrieval and defor-mation are learnable modules, each inﬂuencing the other and trained jointly. While the beneﬁt of deformation-aware retrieval has been explored previously, we contribute the no-tion of retrieval-aware deformation: our learnable deforma-11713
tion module is optimized for ﬁtting retrieved shapes to target shapes, and does not try to be a general-purpose algorithm for arbitrary source-target pairs. Thus, the retrieval module is optimized to retrieve sources that the deformation module can ﬁt well to the input target, and the deformation module is trained on sources the retrieval module predicts for the input target, thereby letting it optimize capacity and learn only meaningful deformations.
The robustness of the joint training enables us to devise a more elaborate deformation space. Speciﬁcally, we de-vise a differentiable, part-aware deformation function that deforms individual parts of a model while respecting the part-to-part connectivity of the original structure (Figure 1).
Importantly, it accommodates varying numbers of parts and structural relationships across the database, and does not re-quire part labels or consistent segmentations. It can work with automatically-segmented meshes and even multiple differently segmented instances of the same source shape.
We propose a way to encode each part in each source and to enable a general MLP to predict its deformation regardless of the part count. This holistic view of joint retrieval and de-formation is especially important when considering hetero-geneous collections of shapes “in the wild” that often vary in their part structure, topology, and geometry. These re-quire different deformation spaces for different source mod-els, which the retrieval module must be aware of.
We evaluate our method by matching 2D image and 3D point cloud targets. We demonstrate that it outper-forms various baselines, such as vanilla retrieval [26], or deformation-aware retrieval using direct optimization for deformation [41], or a ﬁxed, pre-trained, neural deforma-tion module (i.e. omitting joint training). We also show that our method can be used even with imperfect and incon-sistent segmentations produced automatically. Finally, we show that even with a different deformation module (e.g.,
Neural Cages [49]), our joint training leads to better results. 2.