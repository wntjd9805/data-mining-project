Abstract
How can neural networks be trained on large-volume temporal data efﬁciently? To compute the gradients required to update parameters, backpropagation blocks computations until the forward and backward passes are completed. For temporal signals, this introduces high latency and hinders real-time learning. It also creates a coupling between consec-utive layers, which limits model parallelism and increases memory consumption. In this paper, we build upon Side-ways, which avoids blocking by propagating approximate gradients forward in time, and we propose mechanisms for temporal integration of information based on different vari-ants of skip connections. We also show how to decouple computation and delegate individual neural modules to dif-ferent devices, allowing distributed and parallel training.
The proposed Skip-Sideways achieves low latency training, model parallelism, and, importantly, is capable of extracting temporal features, leading to more stable training and im-proved performance on real-world action recognition video datasets such as HMDB51, UCF101, and the large-scale
Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence they can better utilize motion cues. 1.

Introduction
Popular deep video models generally rely on spatio-temporal convolutional networks (3D CNNs) [12, 18, 52, 55, 63] or recurrent networks [16, 24, 34] that are trained with backpropagation (BP) using stochastic gradient descent (SGD) [7, 19, 29, 33, 46, 57, 58, 60, 64]. This is a powerful training paradigm but also an expensive one as it needs to store all the activations in memory to compute Jacobian ten-sors for the gradient calculations. First, all the activations are computed in the forward mode, from the beginning to the end of the sequence. Next, gradients are computed in the reverse direction, from the end to the beginning. All that
∗The corresponding author: mateuszm@google.com severely limits the scale at which we can train temporal mod-els on large-volume sequences such as videos. Therefore, typically these video models are trained on short video clips (about 2.5s at usual frame rate), in ofﬂine batch mode.
Sideways [38] is a recent training technique for video models that decouples the computation along the depth of the network and introduces computation steps. As a new frame is fed into the processing pipeline, each layer indepen-dently updates the internal state of the network by computing new activations and gradients. Next, these are passed to the layers above and below in the next computation step (see
Figure 1 left). Moreover, the information cannot be back-propagated to the same units that produced the activations as this happened in the past computation steps. One may say that “everything ﬂows forward in time”, including the backward pass. Due to these properties, Sideways is more biologically plausible than the regular backprop, as it does not block the computation and respects the arrow of time.
Although Sideways, as originally proposed, operates in a temporal forward fashion, it is not a temporal model per se, as it has access only to the present frame at each time step. This results in improved memory efﬁciency similar to single-frame models, making it suitable for real-time appli-cations. However, this comes with the cost of not integrating information temporally, which limits the expressive power of the resulting models.
In this work, we show that it is possible for models to process one frame at a time similar to Sideways, while still being able to extract temporal features. We do this by in-troducing shortcut (skip) connections in addition to direct connections between the layers of the model. We study the training dynamics of the resulting training procedure, which we call Skip-Sideways, and show that shortcut connections lead to more stable training and higher accuracy.
Regular skip connections [21] alter the information ﬂow along the data path of the network by allowing activations to ‘skip’ layers, creating data shortcuts. In the proposed
Skip-Sideways, activations and gradients along the shortcut connections are also sent forward in time, effectively cre-ating data paths across time, making it possible to extract 9249
temporal features. This change not only extends the mod-elling space, which subsumes spatio-temporal models, but also gives an interesting perspective on shortcut connections that are typically associated with vanishing or exploding gradients [20, 23], or ensembles [56].
To validate the proposed setting, we train a traditional image architecture, e.g., VGG [49], on action recognition datasets [31, 50], including a large-scale one – Kinetics-600 [12] – by encapsulating the neural modules in Skip-Sideways units. Since there is no data dependency between units at any one time step, they can process the data in a depth-asynchronous fashion, maximising parallelism. This results in a signiﬁcant speed-up and reduced latency. The efﬁciency of the whole network depends only on the worst-case efﬁciency of an individual Skip-Sideways unit.
To the best of our knowledge, Skip-Sideways is the ﬁrst alternative to backpropagation, more biologically plausible, that is successfully used to train video models at scale. 2.