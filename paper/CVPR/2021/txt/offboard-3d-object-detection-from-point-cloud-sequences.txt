Abstract
While current 3D object recognition research mostly fo-cuses on the real-time, onboard scenario, there are many offboard use cases of perception that are largely under-explored, such as using machines to automatically generate high-quality 3D labels. Existing 3D object detectors fail to satisfy the high-quality requirement for offboard uses due to the limited input and speed constraints.
In this paper, we propose a novel offboard 3D object detection pipeline using point cloud sequence data. Observing that different frames capture complementary views of objects, we design the offboard detector to make use of the temporal points through both multi-frame object detection and novel object-centric reﬁnement models. Evaluated on the Waymo Open
Dataset, our pipeline named 3D Auto Labeling shows sig-niﬁcant gains compared to the state-of-the-art onboard de-tectors and our offboard baselines. Its performance is even on par with human labels veriﬁed through a human label study. Further experiments demonstrate the application of auto labels for semi-supervised learning and provide exten-sive analysis to validate various design choices. 1.

Introduction
Recent years have seen a rapid progress of 3D object recognition with advances in 3D deep learning and strong application demands. However, most 3D perception re-search has been focusing on real-time, onboard use cases and only considers sensor input from the current frame or a few history frames. Those models are sub-optimal for many offboard use cases where the best perception quality is needed. Among them, one important direction is to have machines “auto label” the data to save the cost of human la-beling. High quality perception can also be used for simula-tion or to build datasets to supervise or evaluate downstream modules such as behavior prediction.
In this paper, we propose a novel pipeline for offboard 3D object detection with a modular design and a series of tailored deep network models. The offboard pipeline makes use of the whole sensor sequence input (such video data is common in applications of autonomous driving and aug-(cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:51)(cid:76)(cid:79)(cid:79)(cid:68)(cid:85)
PointPillar (cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:51)(cid:76)(cid:79)(cid:79)(cid:68)(cid:85) (cid:51)(cid:82)(cid:76)(cid:81)(cid:87)(cid:51)(cid:76)(cid:79)(cid:79)(cid:68)(cid:85) (cid:51)(cid:57)(cid:53)(cid:38)(cid:49)(cid:49) (cid:22)(cid:39)(cid:3)(cid:36)(cid:88)(cid:87)(cid:82)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:11)(cid:50)(cid:88)(cid:85)(cid:86)(cid:12) 3D Auto Labeling (Ours) (cid:22)(cid:39)(cid:3)(cid:36)(cid:88)(cid:87)(cid:82)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:11)(cid:50)(cid:88)(cid:85)(cid:86)(cid:12) (cid:22)(cid:39)(cid:3)(cid:36)(cid:88)(cid:87)(cid:82)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:11)(cid:50)(cid:88)(cid:85)(cid:86)(cid:12)
PVRCNN (cid:51)(cid:57)(cid:53)(cid:38)(cid:49)(cid:49) (cid:51)(cid:57)(cid:53)(cid:38)(cid:49)(cid:49)
+11.4% 
+20.7% 
+19.9% 
+40.2% 
+47.7% 
+108.9% 1.0 (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19) (cid:76) (cid:76) n o i s i c e r
P e g a r e v
A
D 3 (cid:81) (cid:82) (cid:86) (cid:70) (cid:72) (cid:85) (cid:51) (cid:3) (cid:72) (cid:74) (cid:68) (cid:85) (cid:72) (cid:89) (cid:36) (cid:39) (cid:22) (cid:3) 0.8 (cid:19)(cid:17)(cid:27) 0.6 (cid:19)(cid:17)(cid:25) 0.4 (cid:19)(cid:17)(cid:23) (cid:76) (cid:76) (cid:76) (cid:76) (cid:81) (cid:81) (cid:82) (cid:82) (cid:86) (cid:86) (cid:70) (cid:70) (cid:72) (cid:72) (cid:85) (cid:85) (cid:51) (cid:51) (cid:3) (cid:3) (cid:72) (cid:72) (cid:74) (cid:74) (cid:68) (cid:68) (cid:85) (cid:85) (cid:72) (cid:72) (cid:89) (cid:89) (cid:36) (cid:36) (cid:39) (cid:39) (cid:22) (cid:22) (cid:3) (cid:3) (cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:23) 0.2 (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:24) (cid:19)(cid:17)(cid:24) (cid:19)(cid:17)(cid:26) (cid:19)(cid:17)(cid:26) 0.8 (cid:19)(cid:17)(cid:27) 0.7 (cid:19)(cid:17)(cid:26) 0.5 (cid:19)(cid:17)(cid:24) (cid:22)(cid:39)(cid:3)(cid:44)(cid:82)(cid:56)(cid:3)(cid:55)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71) (cid:22)(cid:39)(cid:3)(cid:44)(cid:82)(cid:56)(cid:3)(cid:55)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71) (cid:22)(cid:39)(cid:3)(cid:44)(cid:82)(cid:56)(cid:3)(cid:55)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71) 0.6 (cid:19)(cid:17)(cid:25) 3D IoU Threshold
Figure 1. Our offboard 3D Auto Labeling achieved signiﬁcant gains over two representative onboard 3D detectors (the ef-ﬁcient PointPillar [20] and the top-performing PVRCNN [45]).
The relative gains (the percentage numbers) are higher under more strict standard (higher IoU thresholds). The metric is 3D AP (L1) for vehicles on the Waymo Open Dataset [52] val set. (cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27) mented reality). With no constraints on the model causality and little constraint on model inference speed, we are able to greatly expand the design space of 3D object detectors and achieve signiﬁcantly higher performance.
We design our offboard 3D detector based on a key ob-servation: different viewpoints of an object, within a point cloud sequence, contain complementary information about its geometry (Fig. 2). An immediate baseline design is to extend the current detectors to use multi-frame inputs.
However, as multi-frame detectors are effective they are still limited in the amount of context they can use and are not naively scalable to more frames – gains from adding more frames diminish quickly (Table 5).
In order to fully utilize temporal point clouds (e.g. 10 or more seconds), we step away from the common frame-based input structure where the entire frames of point clouds are merged. Instead, we turn to an object-centric design.
We ﬁrst leverage a top-performing multi-frame detector to give us initial object localization. Then, we link objects detected at different frames through multi-object tracking.
Based on the tracked boxes and the raw point cloud se-quences, we can extract the entire track data of an object, including all of its sensor data (point clouds) and detec-tor boxes, which is 4D: 3D spatial plus 1D temporal. We 6134    
then propose novel deep network models to process such 4D object track data and output temporally consistent and high-quality boxes of the object. As they are similar to how a human labels an object and because of their high-quality output, we call those models processing the 4D track data as “object-centric auto labeling models” and the entire pipeline “3D Auto Labeling” (Fig. 3).
We evaluate our proposed models on the Waymo Open
Dataset (WOD) [52] which is a large-scale autonomous driving benchmark containing 1,000+ Lidar scan sequences with 3D annotations for every frame. Our 3D Auto Labeling pipeline dramatically lifts the perception quality compared to existing 3D detectors designed for the real-time, onboard use cases (Fig. 1 and Sec. 5.1). The gains are even more sig-niﬁcant at higher standards. To understand how far we are from human performance in 3D object detection, we have conducted a human label study to compare auto labels with human labels (Sec. 5.2). To our delight, we found that auto labels are already on par or even slightly better compared to human labels on the selected test segments.
In Sec. 5.3, we demonstrate the application of our pipeline for semi-supervised learning and show signiﬁ-cantly improved student models trained with auto labels.
We also conduct extensive ablation and analysis experi-ments to validate our design choices in Sec. 5.4 and Sec. 5.5 and provide visualization results in Sec. 5.6.
In summary, the contributions of our work are:
• Formulation of the offboard 3D object detection prob-lem and proposal of a speciﬁc pipeline (3D Auto La-beling) that leverages our multi-frame detector and novel object-centric auto labeling models.
• State-of-the-art 3D object detection performance on the challenging Waymo Open Dataset.
• The human label study on 3D object detection with comparisons between human and auto labels.
• Demonstrated the effectiveness of auto labels for semi-supervised learning. 2.