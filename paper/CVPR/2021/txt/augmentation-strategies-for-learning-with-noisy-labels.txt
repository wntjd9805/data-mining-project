Abstract
Imperfect labels are ubiquitous in real-world datasets.
Several recent successful methods for training deep neu-ral networks (DNNs) robust to label noise have used two primary techniques: ﬁltering samples based on loss dur-ing a warm-up phase to curate an initial set of cleanly labeled samples, and using the output of a network as a pseudo-label for subsequent loss calculations. In this pa-per, we evaluate different augmentation strategies for al-gorithms tackling the "learning with noisy labels" prob-lem. We propose and examine multiple augmentation strate-gies and evaluate them using synthetic datasets based on
CIFAR-10 and CIFAR-100, as well as on the real-world dataset Clothing1M. Due to several commonalities in these algorithms, we ﬁnd that using one set of augmentations for loss modeling tasks and another set for learning is the most effective, improving results on the state-of-the-art and other previous methods. Furthermore, we ﬁnd that applying aug-mentation during the warm-up period can negatively im-pact the loss convergence behavior of correctly versus in-correctly labeled samples. We introduce this augmentation strategy to the state-of-the-art technique and demonstrate that we can improve performance across all evaluated noise levels. In particular, we improve accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the Clothing1M dataset. 1.

Introduction
Data augmentation is a common method used to expand datasets and has been applied successfully in many com-puter vision problems such as image classiﬁcation [32] and object detection [28], among many others.
In particular,
*Equal contribution
Source code is available at https://github.com/KentoNishi/
Augmentation-for-LNL. there has been much success using learned augmentations such as AutoAugment [6] and RandAugment [7] which do not require an expert who knows the dataset to curate aug-It has been shown that incorporating mentation policies. augmentation policies during training can improve gener-alization and robustness [12, 8]. However, few works have explored their efﬁcacy for the domain of learning with noisy labels (LNL) [21].
Many techniques which tackle the LNL problem make use of the network memorization effect, where correctly la-beled data ﬁt before incorrectly labeled data as discovered by Arpit et al. [2]. This phenomenon was successfully ex-plored in Deep Neural Networks (DNNs) through model-ing the loss function and the training process, leading to the development of approaches such as loss correction [29] and sample selection [10]. Recently, the incorporation of
MixUp augmentation [35] has dramatically improved the ability for algorithms to tolerate higher noise levels [1, 14].
While many existing works use the common random ﬂip and crop image augmentation which we refer to as weak augmentation, to the best of our knowledge, no work at the time of writing has explored using more aggressive aug-mentation from learned policies such as AutoAugment dur-ing training for LNL algorithms. These stronger augmenta-tion policies include transformations such as rotate, invert, sheer, etc. We propose to incorporate these stronger aug-mentation policies into existing architectures in a strategic way to improve performance. Our intuition is that for any augmentation technique to succeed, they must (1) improve the generalization of the dataset and (2) not negatively im-pact the loss modeling and loss convergence behavior that
LNL techniques rely on.
With this in mind, we propose an augmentation strategy we call Augmented Descent (AUGDESC) to beneﬁt from data augmentation without negatively impacting the net-work memorization effect. Our idea for AUGDESC is to use two different augmentations: a weak augmentation for any loss modeling and pseudo-labeling task, and a strong augmentation for the back-propagation step to improve gen-8022
eralization.
In this paper, we propose and examine how we can incor-porate stronger augmentation into existing LNL algorithms to yield improved results. We provide some answers to this problem through the following contributions:
• We propose an augmentation strategy, Augmented
Descent, which demonstrates state-of-the-art perfor-mance on synthetic and real-world datasets under noisy label scenarios. We show empirically that this can increase performance across all evaluated noise levels (Section 4.4). In particular, we improve accu-racy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the real-world dataset
Clothing1M (Section 4.5).
• We show that there is a large effect on performance de-pending on how augmentation is incorporated into the training process (Section 4.2). We empirically deter-mine that it is best to use weaker augmentation during earlier epochs followed by stronger augmentations to not adversely affect the memorization effect. We ana-lyze the behavior of loss distribution to yield insight to guide effective incorporation of augmentation in future work (Section 4.3).
• We evaluate the effectiveness of our augmentation methodology by performing generalization studies on existing techniques (Section 4.7). Without tuning any hyperparameters, we were able to improve existing techniques with only the addition of our proposed aug-mentation strategy by up to 5% in absolute accuracy. 2.