Abstract
Our objective is language-based search of large-scale image and video datasets. For this task, the approach that consists of independently mapping text and vision to a joint embedding space, a.k.a. dual encoders, is attrac-tive as retrieval scales and is efﬁcient for billions of im-ages using approximate nearest neighbour search. An al-ternative approach of using vision-text transformers with cross-attention gives considerable improvements in accu-racy over the joint embeddings, but is often inapplicable in practice for large-scale retrieval given the cost of the cross-attention mechanisms required for each sample at test time.
This work combines the best of both worlds. We make the following three contributions. First, we equip transformer-based models with a new ﬁne-grained cross-attention ar-chitecture, providing signiﬁcant improvements in retrieval accuracy whilst preserving scalability. Second, we intro-duce a generic approach for combining a Fast dual encoder model with our Slow but accurate transformer-based model via distillation and re-ranking. Finally, we validate our ap-proach on the Flickr30K image dataset where we show an increase in inference speed by several orders of magnitude while having results competitive to the state of the art. We also extend our method to the video domain, improving the state of the art on the VATEX dataset. 1.

Introduction
Imagine yourself looking for an image that best matches a given textual description among thousands of other im-ages. One effective way would be to ﬁrst isolate a few promising candidates by giving a quick glance at all the images with a fast process, e.g. by eliminating images that
*Equal contribution. 1D´epartement d’informatique de l’ENS, ´Ecole normale sup´erieure,
CNRS, PSL Research University, 75005 Paris, France. 3Czech Institute of Informatics, Robotics and Cybernetics at the Czech
Technical University in Prague.
A man rides a  bike with a cat  wearing  sunglasses.
A man rides a  bike with a cat  wearing  sunglasses.
Vision  network
Text  network
Distillation (at training)
Cross-Attention  network
Similarity
Fast
Dual encoder
Reranking (at query time)
Similarity
Slow
Cross-Attention
Figure 1: On the left, the Fast models, a.k.a dual encoders, inde-pendently process the input image and text to compute a similarity score via a single dot product, which can be efﬁciently indexed and is thus amenable to large-scale search. On the right, the Slow models, a.k.a cross-attention models, jointly process the input im-age and text with cross-modal attention to compute a similarity score. Fast and indexable models are improved by Slow models via distillation at training time (ofﬂine). Slow models are accelerated and improved with the distilled Fast approaches using a re-ranking strategy at query time. have clearly nothing in common with the description. In the second phase, you may start paying more attention to im-age details with a slow process, e.g. by grounding individ-ual words of a query sentence to make sure the scrutinized image is the best match.
Analogous to the fast process above, fast retrieval sys-tems can be implemented by separately encoding visual and textual inputs into a joint embedding vector space where similarities can be computed by dot product. Such methods are regarded as indexable, i.e. they allow application of fast approximate nearest neighbour search [11, 32, 53, 65] and enable efﬁcient billion-scale image retrieval. However, the accuracy of such methods is limited due to the simplicity of vision-text interaction model deﬁned by the dot product in the joint embedding space. We refer to these techniques as
Dual Encoders (DE) or Fast approaches. 4VGG, Dept. of Engineering Science, University of Oxford
Vision-text transformers compare each word to all loca-9826
tions in the image using cross-attention [12, 29, 46], allow-ing for grounding, and can be related to the slow process mentioned earlier. Such methods, referred to here as Cross-attention (CA) or Slow approaches, signiﬁcantly boost re-trieval performance. Modeling text-vision interactions with attention, however, makes these models slow and imprac-tical for large-scale image retrieval given the cost of the cross-attention mechanisms required for each sample at test time. Hence, the challenge we consider is the following:
How to beneﬁt from accurate cross-attention mechanisms while preserving the fast and scalable visual search?
Our short answer is: By thinking Fast and Slow [10].
As illustrated in Figure 1, we propose to combine dual en-coder approaches with cross-attention via two complemen-tary mechanisms. First, we improve Fast DE models with a novel distillation objective that transfers knowledge from accurate but Slow CA models to the Fast and indexable dual encoders. Second, we propose to combine DE and CA mod-els with re-ranking where a few most promising candidates obtained with the Fast model are re-ranked using the Slow model. Our resulting approach is both fast and accurate.
Since the speed of CA is not a bottleneck anymore, we further improve performance by enriching the vision-text cross-attention model with a novel feature map upsampling mechanism enabling ﬁne-grained attention. Note that our work can also be applied to vision-to-text retrieval. How-ever, we focus on text-to-vision retrieval due to its wider practical application.
Contributions. (i) We ﬁrst propose a gradual feature up-sampling architecture for improved and ﬁne-grained vision and text cross-attention. Our model is trained with a bi-directional captioning loss which is remarkably competi-tive for retrieval compared to standard cross-modal match-(ii) We introduce a generic approach for ing objectives. scaling-up transformer-based vision-text retrieval using two core ideas: a method to distill the knowledge of Slow cross-attention models into Fast dual-encoders, and re-ranking top results of the Fast models with the Slow ones. (iii) Fi-nally, we validate our approach on image retrieval with the
COCO [43] and Flickr30K [60] datasets and show we can reduce the inference time of powerful transformer-based models by 100× whilst also getting competitive results to the state of the art. We also successfully extend our ap-proach to text-to-video retrieval and improve state of the art on the challenging VATEX [73] dataset. 2.