Abstract
The recently introduced introspective variational au-toencoder (IntroVAE) exhibits outstanding image genera-tions, and allows for amortized inference using an image encoder. The main idea in IntroVAE is to train a VAE ad-versarially, using the VAE encoder to discriminate between generated and real data samples. However, the original In-troVAE loss function relied on a particular hinge-loss for-mulation that is very hard to stabilize in practice, and its theoretical convergence analysis ignored important terms in the loss. In this work, we take a step towards better under-standing of the IntroVAE model, its practical implementa-tion, and its applications. We propose the Soft-IntroVAE, a modiﬁed IntroVAE that replaces the hinge-loss terms with a smooth exponential loss on generated samples. This change signiﬁcantly improves training stability, and also enables theoretical analysis of the complete algorithm.
Interest-ingly, we show that the IntroVAE converges to a distribution that minimizes a sum of KL distance from the data distribu-tion and an entropy term. We discuss the implications of this result, and demonstrate that it induces competitive im-age generation and reconstruction. Finally, we describe an application of Soft-IntroVAE to unsupervised image trans-lation, and demonstrate compelling results. Code and ad-ditional information is available on the project website -taldatech.github.io/soft-intro-vae-web. 1.

Introduction
Two popular approaches for learning deep gener-ative models are generative adversarial training (e.g.,
GANs [14]), and variational inference (e.g., VAEs [33]).
VAEs are known to have a stable training procedure, display resilience to mode collapse, and enable amortized infer-ence. Moreover, the VAE’s inference module makes them prominent in many domains, such as learning disentangled representations [2] and reinforcement learning [17]. GANs, on the other hand, lack an inference module, but are capa-ble of generating images of higher quality and are popular in computer vision applications, but can suffer from training instability and low sampling diversity [41, 34].
Narrowing the gap between VAEs and GANs has been the aim of many works, in an attempt to combine the best of both worlds: building a stable and easy to train generative model that allows efﬁcient amortized inference and high-quality sampling [36, 11, 40, 58, 44]. While progress has been made, the search for better generative models is an active research ﬁeld.
Recently, Huang et al. [25] proposed IntroVAE – a VAE that is trained adversarially, and demonstrated outstanding image generation results. A key idea in IntroVAE is intro-spective discrimination – instead of training a separate dis-criminator network to discriminate between real and gener-ated samples, as in a GAN, the output of the encoder acts as the discriminatory signal, based on the Kullback-Leibler divergence between the approximate posterior of a sample and the prior latent distribution. Intuitively, this signal can be understood as making the generated samples less likely, as their posterior is more distant from the prior.
Importantly, IntroVAE uses a hard margin, m, as a threshold on the KL divergence for which above it a ‘fake’ sample no longer affects the loss function. This approach leads to difﬁcult training in practice – the optimization is very sensitive to the values of m, and extensive tuning is required to ﬁnd a good, stable, margin. Theoretically, [25] proved that IntroVAE converges to the data distribution, but their analysis ignored several terms in the loss function that are important for correct operation of the algorithm.
We aim to provide a better understanding of the intro-spective training paradigm and improve its stability. To that end, we introduce Soft-IntroVAE (S-IntroVAE) – an introspective VAE that utilizes the evidence lower bound (ELBO) as the discriminatory signal, and replaces the hard margin with a soft threshold function, making it more sta-ble to optimize. This new formulation allows us to analyze the convergence properties of the complete algorithm, and provide new insights into introspective VAEs. 4391
(a) FFHQ dataset – samples from S-IntroVAE (FID: 17.55). (b) FFHQ – reconstructions.
Figure 1: Generated samples (left) and reconstructions (right) of test data (left: real, right: reconstruction) from a style-based
S-IntroVAE trained on FFHQ at 256x256 resolution.
Interestingly, our theoretical analysis shows that, in con-trast to the original IntroVAE, the S-IntroVAE encoder con-verges to the true posterior, thus maintaining the inference capabilities of the VAE framework. Our analysis also re-veals that the S-IntroVAE model converges to a generative distribution that minimizes a sum of KL divergence from the data distribution and an entropy term, in contrast to
GANs, where the distribution of the generator converges to the data distribution. We further analyze the consequences of this result and its practical implications, and show that the model produces high-quality generations from a distribu-tion with a sharp support. In practice, S-IntroVAE is much more stable than IntroVAE, as it does not involve the sen-sitive threshold parameter, and we rigorously validate this claim in experiments, ranging from inference on 2D distri-butions to high-quality image generation.
Finally, we demonstrate a practical application of our model to the task of unsupervised image translation. We exploit the fact that S-IntroVAE has both good generation quality and strong inference capabilities, and combine it with an encoder-decoder architecture that induces disentan-glement. Inductive bias of this sort is required for unsuper-vised learning of disentangled representations [38], and our results show that using this architecture, S-IntroVAE is in-deed capable of successfully transferring content between two images without any supervision.
Our contribution is summed as follows: (1) We propose
Soft-IntroVAE, a modiﬁcation of the original IntroVAE that that utilizes the evidence lower bound (ELBO) as a discrim-inatory signal, and does not require a hard-margin thresh-old; (2) We provide a deeper theoretical understanding of introspective VAEs; (3) We validate that training our model is signiﬁcantly more robust than training the original In-troVAE; (4) We show that our method is capable of high-quality image synthesis; (5) We demonstrate a practical ap-plication of our model to unsupervised image translation. 2.