Abstract
In this paper, we propose a progressive unsupervised learning (PUL) framework, which entirely removes the need for annotated training videos in visual tracking. Specif-ically, we ﬁrst learn a background discrimination (BD) model that effectively distinguishes an object from back-ground in a contrastive learning way. We then employ the BD model to progressively mine temporal correspond-ing patches (i.e., patches connected by a track) in sequen-tial frames. As the BD model is imperfect and thus the mined patch pairs are noisy, we propose a noise-robust loss function to more effectively learn temporal correspon-dences from this noisy data. We use the proposed noise ro-bust loss to train backbone networks of Siamese trackers.
Without online ﬁne-tuning or adaptation, our unsupervised real-time Siamese trackers can outperform state-of-the-art unsupervised deep trackers and achieve competitive results to the supervised baselines. 1.

Introduction
Visual object tracking (VOT) is one of the fundamental tasks in computer vision, which has gained much attention in recent years due to its wide usage in many practical ap-plications, such as robotics [37], video surveillance [25] and eye-tracking applications [24]. Although much progress has been made on VOT, it is still far from being fully solved due to the numerous real-world challenges, e.g., background cluster, rotation, occlusion and illumination variation.
To handle with the above challenges, the existing end-to-end trainable deep trackers [62, 65] learn rich feature representations from large-scale annotated training videos based on supervised learning. The representative works are
Siamese trackers [29, 30], which learn feature representa-tions in an ofﬂine manner, and achieve favorable perfor-mance even without online ﬁne-tuning or adaptation. The success of Siamese-based trackers demonstrate that the nu-merous annotated data is the key to feature learning of deep trackers. For example, the pioneering deep tracker
SiamFC [4] is trained with the ILSVRC [43] dataset, which contains about 2 million labeled frames. Some extensions
VOT17/18 supervised unsupervised
SiamDW
Ours-ResPUL
DSiam  
SiamFC
DCFNet
S2Siam
Ours-AlexPUL 0.24 0.22 0.2
O
A
E 0.18 0.16 0.14
CycleSiam 0.12 0.36 0.38 0.4 0.42 0.44
LUDT
UDT 0.46
Accuracy 0.48 0.5 0.52 0.54
Figure 1: Tracking accuracy and EAO obtained by supervised (denoted by blue color) and unsupervised trackers (red color) on the VOT17/18 dataset.
Without online ﬁne-tuning or adaptation, our unsupervised AlexPUL and
ResPUL trackers outperform state-of-the-art online updating-based un-supervised trackers, meanwhile performing favorably against supervised deep trackers, which shows the potential of our unsupervised method. like SiamRPN++ [30] commonly need more labeled video datasets for training, e.g., using Got-10k [18], LaSOT [12],
Youtube-BB [42]. These supervised deep trackers assume that a robust tracking model can be learned by using large-scale annotated video datasets. However, they usually ig-nore the fact that annotating such large scale video datasets can be prohibitively expensive and time-consuming.
Unlabeled videos or images are a fertile source for unsu-pervised learning, since they are easily collected from on-line services. The main problem is how to construct a super-visory signal from these sources for learning a feature map representation that is useful for visual tracking. The super-vision in visual tracking generally comes from two aspects: spatial supervision in a static video frame, and sequential supervision across multiple frames. Spatial supervision [45] can be achieved by learning template correspondence in a single frame or image. The training cost is reduced by us-ing static images, but the learned features lack invariance to visual changes in sequential frames. On the other hand, se-quential supervision is commonly achieved by ﬁnding tem-poral correspondence in multiple frames. The recent cycle learning [2, 51] is based on sequential supervision, but the tracking performance is still limited without online adapta-tion [53]. To achieve favorable performance, an online up-dated correlation ﬁlter is required [8]. In this paper, we pro-pose to use both spatial and sequential supervision for un-2993
Figure 2: Overview of Progressive Unsupervised Learning (PUL) for learning feature representations for tracking. Given various instances sampled from unlabeled videos, we ﬁrst use contrastive learning to learn a background discrimination (BD) model, applying anchor-based hard negative mining. In order to learn temporal correspondence (TC), we then apply the BD model to mine temporal corresponding patches. Since the mined patch pairs are noisy (i.e., they lack exact spatial correspondence), we propose a noise-robust (NR) loss function for TC learning. In the temporally-mined patches, the estimated target center is the red “x”, while the true target center is the green circle. supervised representation learning in visual tracking. With-out applying online ﬁne-tuning or updating, our method can achieve favorable unsupervised performance.
In this paper, our main motivation is that a robust track-ing model should suppress background distractors and iden-tify temporal correspondences across multiple frames si-multaneously. To achieve this, we propose a progressive unsupervised learning (PUL) framework (see Fig. 2). The key idea is to progressively learn an unsupervised tracking model based on the two parts of learning: background dis-crimination (BD) and temporal correspondence (TC) learn-ing. For the former, we formulate BD learning as a con-trastive learning task [7, 16], which aims to separate dif-ferent identity instances in the deep embedding space. To learn more discriminative features for visual tracking, we extend the original contrastive learning framework [7] by proposing an anchor-based hard negative mining (AHM) strategy. Our AHM further boosts the feature discrimina-tion by learning to suppress hard negative distractors.
For temporal correspondence learning, we employ the
BD model to progressively mine corresponding-pairs of patches in short video clips. Following [4], a common prac-tice is to use pixel-wise binary-cross entropy (BCE) loss between the tracker and GT response maps for learning the temporal correspondence across multiple frames in a video.
However, the mined patch-pairs that are used as the pseudo ground-truth (GT) are not well matched, due to an imperfect
BD model (e.g., see right side of Fig. 2). That is, there exists non-negligible spatial noise in the GT locations, whereas
BCE assumes no spatial noise. This mismatch between the actual noise in the GT and the assumed noise by the loss will limit the generalization and localization ability of the trained model. To alleviate problem, we propose a new like-lihood function for tracking response maps, which is based on a generative model of the GT spatial noise. Notably, our likelihood function models correlations between pixels in the response map, whereas BCE does not consider these correlations. We use the negative log likelihood as our new noise-robust (NR) loss function, to more effectively learn temporal correspondence from noisy patch pairs.
We apply the proposed PUL framework to train back-bone networks used in two Siamese trackers [4, 65].
The ofﬂine-learned backbone networks (i.e., AlexNet [28] and CIResNet-22 [65]) can be directly applied for online
Siamese tracking. We show that without using online ﬁne-tuning or adaptation, our unsupervised trackers outperform state-of-the-art unsupervised trackers on ﬁve benchmarks.
In summary, the main contributions of our work are:
• We propose a progressive unsupervised learning (PUL) framework, which formulates visual tracking as a combination of background discrimination and tem-poral correspondence learning.
• We propose to learn a background discrimination model through contrastive learning and anchor-based hard negative mining.
• We propose a noise-robust loss to effectively learn temporal correspondences from noisy patch-pairs. 2.