Abstract
Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive ﬁelds. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive ﬁeld, through ei-ther dilated/atrous convolutions or inserting attention mod-ules. However, the encoder-decoder based FCN architec-ture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmenta-tion as a sequence-to-sequence prediction task. Speciﬁcally, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed
SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and com-petitive results on Cityscapes. Particularly, we achieve the
ﬁrst position in the highly competitive ADE20K test server leaderboard on the day of submission. 1.

Introduction
Since the seminal work of [35], existing semantic seg-mentation models have been dominated by those based on fully convolutional network (FCN). A standard FCN seg-*Work done while Sixiao Zheng was interning at Tencent Youtu Lab.
†Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with
School of Data Science, Fudan University. Yanwei Fu is with the School of Data Science, MOE Frontiers Center for Brain Science, and Shanghai
Key Lab of Intelligent Information Processing, Fudan University. Jianfeng
Feng is with the Institute of Science and Technology for Brain-Inspired
Intelligence, Fudan University. mentation model has an encoder-decoder architecture: the encoder is for feature representation learning, while the de-coder for pixel-level classiﬁcation of the feature representa-tions yielded by the encoder. Among the two, feature rep-resentation learning (i.e., the encoder) is arguably the most important model component [7, 27, 55, 58]. The encoder, like most other CNNs designed for image understanding, consists of stacked convolution layers. Due to concerns on computational cost, the resolution of feature maps is re-duced progressively, and the encoder is hence able to learn more abstract/semantic visual concepts with a gradually in-creased receptive ﬁeld. Such a design is popular due to two favorable merits, namely translation equivariance and local-ity. The former respects well the nature of imaging pro-cess [56] which underpins the model generalization ability to unseen image data. Whereas the latter controls the model complexity by sharing parameters across space. However, it also raises a fundamental limitation that learning long-range dependency information, critical for semantic segmentation in unconstrained scene images [1,48], becomes challenging due to still limited receptive ﬁelds.
To overcome this aforementioned limitation, a number of approaches have been introduced recently. One approach is to directly manipulate the convolution operation. This in-cludes large kernel sizes [39], atrous convolutions [7, 21], and image/feature pyramids [58]. The other approach is to integrate attention modules into the FCN architecture. Such a module aims to model global interactions of all pixels in the feature map [47]. When applied to semantic segmenta-tion [24, 28], a common design is to combine the attention module to the FCN architecture with attention layers sitting on the top. Taking either approach, the standard encoder-decoder FCN model architecture remains unchanged. More recently, attempts have been made to get rid of convolutions altogether and deploy attention-alone models [46] instead.
However, even without convolution, they do not change the nature of the FCN model structure: an encoder downsam-6881
ples the spatial resolution of the input, developing lower-resolution feature mappings useful for discriminating se-mantic classes, and the decoder upsamples the feature rep-resentations into a full-resolution segmentation map.
In this paper, we aim to provide a rethinking to the se-mantic segmentation model design and contribute an alter-native. In particular, we propose to replace the stacked con-volution layers based encoder with gradually reduced spa-tial resolution with a pure transformer [44], resulting in a new segmentation model termed SEgmentation TRans-former (SETR). This transformer-alone encoder treats an input image as a sequence of image patches represented by learned patch embedding, and transforms the sequence with global self-attention modeling for discriminative fea-ture representation learning. Concretely, we ﬁrst decom-pose an image into a grid of ﬁxed-sized patches, forming a sequence of patches. With a linear embedding layer applied to the ﬂattened pixel vectors of every patch, we then obtain a sequence of feature embedding vectors as the input to a transformer. Given the learned features from the encoder transformer, a decoder is then used to recover the original image resolution. Crucially there is no downsampling in spatial resolution but global context modeling at every layer of the encoder transformer, thus offering a completely new perspective to the semantic segmentation problem.
This pure transformer design is inspired by its tremen-dous success in natural language processing (NLP) [13,44].
More recently, a pure vision transformer or ViT [15] has shown to be effective for image classiﬁcation tasks. It thus provides direct evidence that the traditional stacked convo-lution layer (i.e., CNN) design can be challenged and image features do not necessarily need to be learned progressively from local to global context by reducing spatial resolution.
However, extending a pure transformer from image classi-ﬁcation to a spatial location sensitive task of semantic seg-mentation is non-trivial. We show empirically that SETR not only offers a new perspective in model design, but also achieves new state of the art on a number of benchmarks.
The following contributions are made in this paper: (1)
We reformulate the image semantic segmentation problem from a sequence-to-sequence learning perspective, offer-ing an alternative to the dominating encoder-decoder FCN model design. (2) As an instantiation, we exploit the trans-former framework to implement our fully attentive feature representation encoder by sequentializing images. (3) To extensively examine the self-attentive feature presentations, we further introduce three different decoder designs with varying complexities. Extensive experiments show that our SETR models can learn superior feature representa-tions as compared to different FCNs with and without at-tention modules, yielding new state of the art on ADE20K (50.28%), Pascal Context (55.83%) and competitive results on Cityscapes. Particularly, our entry is ranked the 1st place in the highly competitive ADE20K test server leaderboard. 2.