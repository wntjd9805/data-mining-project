Video object segmentation (VOS) is an important task in computer vision, with applications in object tracking, autonomous driving, and video surveillance. Existing techniques can be categorized into semi-supervised video object segmentation and unsupervised video object segmentation (UVOS). UVOS models, in particular, face the challenge of identifying primary objects in videos without prior knowledge. These primary objects can be salient objects, moving objects, or recurring objects. Previous approaches using optical flow to capture motion information struggle to distinguish foreground objects from background objects. To address this limitation, we propose a Reciprocal Transformation Network (RTNet) that integrates and evolves appearance and motion representations. We introduce a Reciprocal Transformation Module (RTM) to enable feature interactions and a Spatial Temporal Attentive Fusion Module (STAFM) to achieve spatio-temporal consistency in segmenting primary objects. Experimental results on public benchmarks show that our approach outperforms state-of-the-art methods, achieving significant improvements in region similarity and boundary accuracy. Our paper contributes by presenting a novel RTNet that effectively identifies and segments primary objects in videos, proposing a reciprocal transformation approach to handle moving outliers, and introducing a STAFM for integrating appearance and motion features. Despite using a smaller backbone and less training data, our lightweight model achieves comparable performance to the latest competitors.