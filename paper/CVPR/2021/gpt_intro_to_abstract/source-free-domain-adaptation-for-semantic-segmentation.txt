Semantic segmentation plays a crucial role in computer vision for scene understanding and has various applications such as autonomous driving and image editing. However, current segmentation techniques heavily rely on expensive, densely-labeled datasets. To address this issue, unsupervised domain adaptation (UDA) has been proposed to transfer knowledge from well-trained models on source datasets to unlabeled target domains. However, existing UDA methods assume access to both source models and labeled source datasets, which may not be available in source-free scenarios like autonomous driving. In this paper, we formulate the problem of source-free domain adaptation for semantic segmentation and propose a novel framework called SFDA. Our framework consists of two stages â€“ knowledge transfer and model adaptation. We leverage a generator to estimate the source domain and synthesize fake samples for domain knowledge transfer. We also introduce a dual attention distillation mechanism to retain contextual information and an intra-domain patch-level self-supervision module to exploit patch-level knowledge in the target domain. Experimental results demonstrate the effectiveness of our framework in synthetic-to-real and cross-city segmentation scenarios, even outperforming state-of-the-art source-driven UDA approaches under the source-free setting.