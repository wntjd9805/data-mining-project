Generative adversarial networks (GANs) have shown remarkable progress in synthesizing high-fidelity images for various vision applications. However, GAN models heavily rely on large and diverse training datasets, which are often difficult to collect. Several recent approaches have attempted to address the issue of limited training data, primarily through data augmentation methods. However, there is a lack of regularization techniques specifically designed to improve the generalization of GAN models trained on limited data.In this paper, we propose a novel regularization scheme to improve the robustness and generalization performance of GAN models trained on limited data. Our approach introduces an â„“2 norm regularization that modulates the discriminator's prediction by comparing it with a moving average variable tracking historical predictions of generated images. The theoretical analysis shows that this regularization transforms the GAN formulation towards minimizing the LeCam-divergence, which is more robust under limited data settings.Extensive experiments are conducted to evaluate the efficacy of our regularization scheme. The results demonstrate three key merits: improved generalization performance of popular GAN models such as BigGAN and StyleGAN2, stabilized training dynamics under limited training data, and empirical complementarity with data augmentation methods. Combining our regularization scheme with a data augmentation method achieves state-of-the-art performance on a limited ImageNet dataset.Overall, our work contributes to addressing the data insufficiency issue in GAN training by proposing a regularization scheme that enhances the generalization capabilities of GAN models on limited training data.