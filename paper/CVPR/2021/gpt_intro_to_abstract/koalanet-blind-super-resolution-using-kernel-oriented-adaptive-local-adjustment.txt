In single image super-resolution (SR), deep neural networks often struggle with generalization beyond a specific scenario, leading to performance deterioration when faced with different conditions. Most existing neural-network-based SR methods focus on upscaling low resolution (LR) images to high resolution (HR) images under the commonly used bicubic downsampling setting. However, their performance suffers greatly when the LR image is degraded by even a slightly different downsampling kernel, which is often the case in real images. To address this issue, recent SR methods aim for blind SR where the true degradation kernels are unknown. This paper proposes a blind SR framework called KOALAnet, which utilizes a kernel-oriented adaptive local adjustment approach to jointly learn the degradation and restoration kernels. KOALAnet consists of two networks: a downsampling network that estimates spatially-variant blur kernels and an upsampling network that maps the predicted degradation kernels to the feature kernel space and applies degradation-specific local feature adjustments on the SR feature maps. The proposed framework demonstrates accurate prediction of degradation kernels and effective utilization of this information for SR, with good generalization ability on historic images containing unknown degradations. In addition, experiments on real aesthetic depth-of-field images show that KOALAnet effectively handles images with intentional blur. The proposed framework outperforms state-of-the-art blind SR methods on synthesized LR images obtained under randomized degradation conditions and historic LR images with unknown degradations. Furthermore, the study analyzes SR results on images mixed with in-focus and out-of-focus regions, demonstrating KOALAnet's ability to differentiate intentionally blurry areas and maintain the photographer's intent after SR.