In this paper, the authors address the problem of domain shift in supervised learning and propose a solution based on semi-supervised domain adaptation (Semi-DA). They argue that while labeled target data is usually more difficult to obtain, it can lead to better accuracy. Thus, they focus on designing algorithms that can make use of a small amount of labeled data from the target domain in addition to the labeled source data. The authors derive a generalization bound for Semi-DA, showing that the accuracy discrepancy between domains depends on the distance between the marginal feature distributions and the optimal predictors. Based on this observation, they introduce a novel algorithm called LIRR (Learning Invariant Representations and Risks) that jointly learns invariant representations and risks to minimize the accuracy discrepancy across domains. They provide theoretical justifications for their approach and perform extensive experiments to demonstrate its effectiveness in both classification and regression tasks. The results show significant improvements in Semi-DA compared to methods that only learn invariant representations or risks. Additionally, they show that LIRR can successfully exploit the structure in source data to improve generalization on the target domain, even surpassing an oracle method trained only on labeled target data. Overall, this work contributes theoretical insights and practical algorithms for semi-supervised domain adaptation in the presence of domain shift.