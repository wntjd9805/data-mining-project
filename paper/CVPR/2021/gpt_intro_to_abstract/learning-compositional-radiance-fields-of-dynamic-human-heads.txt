Modeling, rendering, and animating dynamic human heads at high fidelity poses a significant research challenge due to the complexity of the human head's geometry and the variations in skin, hair, teeth, and eyes. Discrete and continuous neural scene representations have been explored to achieve controllable rendering, but they have limitations such as struggling to model thin structures like hair or lacking connectivity information. Volumetric representations based on voxel grids can model thin structures using semi-transparency but have scalability issues. Scene Representation Networks (SRNs) map world coordinates to local feature representations but are limited to diffuse objects. Neural Radiance Fields (NeRF) show impressive results for static scenes but require further exploration for dynamic sequences of real humans. This paper proposes a novel compositional 3D scene representation that combines a 3D-structure-aware grid of animation codes with a continuous learned scene function, addressing the limitations of existing approaches. The proposed representation allows for high-quality dynamic neural radiance fields of human heads in motion, includes an importance sampling strategy tailored to human heads for faster rendering, and achieves state-of-the-art results for synthesizing novel views of dynamic human heads.