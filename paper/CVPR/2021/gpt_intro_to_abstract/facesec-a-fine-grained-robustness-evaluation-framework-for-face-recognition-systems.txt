Face recognition has become a widely used technology in various applications, including security-sensitive areas such as airport check-in and online financial transactions. However, recent studies have found that deep face recognition models are vulnerable to adversarial attacks, both in the digital space, where imperceptible perturbations are added to input images, and in the physical space, where adversarial perturbations are applied to physical objects. This vulnerability poses risks in critical domains such as security and finance, highlighting the need for methods that can comprehensively evaluate the robustness of face recognition systems in adversarial settings. The main challenges in evaluating the robustness of face recognition systems include the diversity of these systems and the variety of adversarial environments. To address this, we propose FACESEC, a fine-grained robustness evaluation framework that incorporates four dimensions: the nature of adversarial perturbations, the attacker's knowledge about the target face recognition system, the goals of the attack, and the attacker's capability. We implement both digital and physically realizable attacks in FACESEC, utilizing state-of-the-art attack paradigms and proposing novel attacks involving pixel-level stickers and grid-level face masks. Our framework allows researchers to identify the vulnerability of each face recognition component and assess different levels of robustness in response to various attacks. Additionally, we propose a general approach for producing universal adversarial examples, which improves the efficiency of evaluation. We perform a comprehensive evaluation on five publicly available face recognition systems in various settings to demonstrate the effectiveness of FACESEC. Our contributions include the development of an evaluation framework, the introduction of novel physically realizable attacks, and the creation of a more efficient approach for producing universal adversarial examples.