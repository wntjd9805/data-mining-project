In recent years, deep neural network-based multi-view (MV) crowd counting has been proposed as a solution for counting people in wide-area scenes that cannot be covered by a single camera. These MV counting models typically fuse feature maps from multiple camera views to predict a scene-level crowd density map. However, the current MV paradigm has a major disadvantage in that the models are trained and tested on the same single scene and fixed camera layout, leading to poor generalization to other scenes or camera layouts.To address this limitation, this paper proposes a new paradigm of cross-view cross-scene (CVCS) multi-view counting, where MV counting models are trained and tested on different scenes and arbitrary camera layouts. This poses challenges because both the scenes and camera layouts change at test time. In CVCS counting, the selection of features from each camera and handling non-correspondence errors caused by camera calibration or erroneous features must be dynamically learned by the MV model.To overcome these challenges, the paper introduces a CVCS counting model that attentively selects and fuses features from multiple cameras using a camera layout-guided attention mechanism and max-pooling. Furthermore, a noise-injection regularization scheme is proposed to simulate non-correspondence errors and improve model generalization.The authors generate a synthetic CVCS multi-view dataset comprising 31 scenes with around 100 camera views each, enabling generalization across camera layouts. This dataset is used to train the proposed CVCS model, which is then applied to real-world multi-view counting datasets with promising results. Unsupervised domain adaptation is also applied to fine-tune the trained model on test images, further improving performance.The contributions of this paper are three-fold: (1) the proposal of a CVCS DNN model that adaptively selects and fuses multi-camera views and incorporates a noise view regularization method for improved generalization; (2) the introduction of a large synthetic multi-view crowd counting dataset, enabling research on cross-scene cross-view problems; and (3) the demonstration that the proposed CVCS model outperforms existing state-of-the-art MV models in the cross-view cross-scene paradigm. Furthermore, the CVCS model, trained on synthetic scenes and adapted with unsupervised domain adaptation, achieves promising performance compared to MV models trained on single scenes.