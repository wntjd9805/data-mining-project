This paper addresses the need for multilingual and multimodal representation learning in the field of computer science. While previous research has focused on English-trained models, maintaining language-specific models for every language in the world is impractical. Therefore, there is a growing interest in developing a single model that can handle multiple languages. Early works have focused on machine translation and image-text retrieval, but scalability and generalization to more languages remain challenges. The recent availability of large-scale multimodal datasets and multilingual corpora has paved the way for advancements in pre-training models. The proposed Universal Cross-lingual Cross-modal (UC2) pre-training framework primarily relies on images and complements with English data for multilingual multimodal representation learning. To overcome the lack of paired image-aligned multilingual data, English-only datasets are augmented with other languages through machine translation. The paper introduces two novel pre-training objectives, Masked Region-to-Token Language Modeling and Visual Translation Language Modeling, which enable fine-grained alignment between words and image regions and joint learning of cross-lingual cross-modal mapping. Extensive experiments demonstrate the effectiveness of the UC2 framework, achieving state-of-the-art performance on multilingual image-text retrieval and visual question answering tasks. The contributions of this research include the construction of a multilingual V+L corpus, the proposal of the UC2 framework, and the introduction of new pre-training tasks that outperform existing methods.