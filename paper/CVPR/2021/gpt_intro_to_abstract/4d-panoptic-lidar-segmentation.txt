The spatio-temporal interpretation of raw sensory data is crucial for autonomous vehicles to understand and interact with their environment. Previous research has tackled different aspects of dynamic scene understanding, such as semantic segmentation, object detection, instance segmentation, and multi-object tracking, independently. These developments were driven by advancements in deep learning and large-scale datasets. However, recent efforts in the computer vision community have aimed to converge these tasks, such as multi-object tracking and segmentation, and semantic and instance segmentation. This paper focuses on the challenging problem of sequence-level LiDAR panoptic segmentation, where state-of-the-art methods often face memory constraints. The proposed approach involves creating overlapping 4D volumes of scans and assigning semantic interpretation to points in 4D space-time. This allows for efficient processing and implicit temporal association via clustering, eliminating the need for explicit data association. For evaluation, a point-centric higher-order tracking metric is introduced to measure the semantic and spatio-temporal aspects of the task. The SemanticKITTI dataset is used for analysis and comparison with existing LiDAR semantic/instance segmentation approaches. The contributions of this paper include a unified space-time perspective to 4D LiDAR panoptic segmentation, a point-centric evaluation protocol, and a thorough analysis of the proposed model's performance. The code, experimental data, and benchmark are publicly available.