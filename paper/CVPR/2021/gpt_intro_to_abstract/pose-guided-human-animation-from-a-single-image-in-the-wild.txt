Being able to animate a human in everyday apparel with an arbitrary pose sequence from just a single still image opens the door to many creative applications. However, classical computer graphics techniques for this task are complex and time consuming. In this paper, we propose a pose transfer algorithm that synthesizes the appearance of a person at arbitrary pose by transforming the appearance from an input image without requiring a 3D animatable textured human model. Existing works on pose transfer have shown promising results, but they are limited in their ability to generalize to diverse poses, shapes, appearance, viewpoints, and backgrounds. To address this challenge, we decompose the pose transfer task into modular subtasks and employ explicit silhouette prediction for animation blending. In the inference phase, we introduce an efficient strategy for synthesizing temporally coherent human animations controlled by a sequence of body poses. Our experiments demonstrate that our method outperforms existing approaches in terms of synthesis quality, temporal consistency, and generalization ability. Our technical contributions include a novel approach for generating realistic animations of people, a compositional pose transfer framework, and an effective inference method for enforcing temporal consistency and preserving identity.