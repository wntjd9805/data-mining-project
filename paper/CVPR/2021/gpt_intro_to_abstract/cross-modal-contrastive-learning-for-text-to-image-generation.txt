This paper introduces a text-to-image synthesis model called XMC-GAN that utilizes contrastive losses to improve image realism and alignment with the corresponding text description. Previous approaches in this field have often relied on object-driven and multi-stage methods, but XMC-GAN demonstrates that a one-stage GAN without object-level annotation can outperform these models. The proposed XMC-GAN model achieves significant improvements in terms of image fidelity, reducing Fr√©chet Inception Distance (FID) on MS-COCO and LN-COCO datasets. Human evaluations also confirm the superiority of XMC-GAN in terms of image realism and alignment. Furthermore, this paper establishes a strong benchmark on the LN-OpenImages dataset, which has diverse images and descriptions. The authors also conduct an insightful analysis of contrastive losses in XMC-GAN and provide valuable modeling insights for contrastive learning in conditional GANs. Overall, XMC-GAN generates more coherent and detailed images compared to previous models, capturing the full image description and enhancing realism.