Efficiently understanding the appearance and structure of 3D scenes is crucial in various computer vision applications, such as autonomous vehicles, robotics, and augmented/mixed reality. Traditional stereo cameras with ordinary lenses have limited field of view, making it insufficient to capture scenes in all directions. Building a multi-camera setup like a light-field camera array can capture scenes in all directions, but it significantly increases manufacturing and computational costs. To reduce the number of cameras while covering all directions, a smaller number of fisheye lenses can be used. However, there is a tradeoff between performance and accuracy when computing a full 360째 panorama and distance using an omnidirectional camera configuration with multiple fisheye lenses. This is due to the optical characteristics of fisheye lenses, such as the invalidity of the conventional pinhole camera model for wide field of views and curved epipolar lines.This paper proposes a real-time sphere-sweeping stereo method that can directly process multiview fisheye images without requiring additional spherical rectification. The method addresses three key points: adaptive spherical matching to evaluate stereo matching directly on the fisheye image domain, fast inter-scale cost volume filtering for stable sphere sweeping volume, and fast inpainting using the dense distance map to combine colors at different distance maps into a complete 360째 panorama and distance map. The algorithm is implemented on a prototype consisting of an embedded computer with a mobile GPU and four fisheye cameras. The prototype captures complete 360째 RGB-D video at a resolution of two megapixels and 29 frames per second. Experimental results demonstrate that the real-time algorithm outperforms traditional omnidirectional stereo and learning-based 360째 stereo algorithms in terms of accuracy and performance.