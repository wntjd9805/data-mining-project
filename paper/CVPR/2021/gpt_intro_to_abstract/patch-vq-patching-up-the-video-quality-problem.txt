User-generated content (UGC) and video streaming have seen a massive increase in popularity on social media platforms. With billions of video views occurring daily, it becomes crucial to measure and control the quality of UGC videos. Full-reference (FR) video quality assessment models compare quality against pristine videos, while no-reference (NR) models do not rely on such comparisons. NR video quality monitoring has the potential to transform video processing in various domains such as social media, telemedicine, surveillance, and robotics. However, measuring video quality without a pristine reference poses significant challenges. UGC video distortions arise from diverse capture conditions and various factors that affect how they are perceived by viewers. Additionally, existing video quality resources are often small and unrepresentative of real-world distortions. Moreover, most existing NR algorithms fail to account for complex space-time distortions common in UGC videos. In this study, the authors address these challenges and make significant contributions. They build the largest video quality database comprising real-world UGC videos, conduct the largest subjective video quality study, and develop a deep blind video quality predictor that accurately predicts both global video quality and local space-time patch quality. They also create a prediction model that generates space-time maps of video quality, enabling the localization, visualization, and action on video distortions. Overall, this research aims to advance NR video quality prediction in the context of UGC content.