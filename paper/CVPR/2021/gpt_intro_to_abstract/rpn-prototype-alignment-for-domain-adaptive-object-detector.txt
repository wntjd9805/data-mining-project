Object detection is a crucial task in computer vision that involves identifying and localizing objects in images. Recent advancements in large-scale benchmarks and CNN-based detection frameworks have led to significant progress in object detection. However, state-of-the-art detectors typically require a large number of training samples with annotations, which can be labor-intensive and costly to obtain. To address this challenge, unsupervised domain adaptation (UDA) has been developed to adapt object detection models trained on annotated source samples to perform well on target samples without annotations.UDA aims to learn domain knowledge from the source domain data to make the model effective in the target domain. One common approach is to build invariant feature representations that align the feature distributions of the two domains. This can be achieved by minimizing the domain shift using correlation distances or through domain adversarial learning. In this paper, we focus on the problem of domain adaptation for two-stage object detectors.Previous works have attempted to incorporate adversarial learning into mainstream detection frameworks like Faster R-CNN. In two-stage object detectors, there are three main modules: the backbone network, the region proposal network (RPN), and the region proposal classifier (RPC). Existing methods typically minimize the domain disparity at multiple levels, such as image and instance alignment, to improve feature adaptation. However, most of these methods focus on aligning features in the backbone network or RPC, neglecting the transferability of the RPN across domains.We conduct an analysis experiment to investigate the importance of the RPN in domain adaptation. Our results indicate that the quality of RPN proposals plays a crucial role in cross-domain object detection. While previous methods that align features in the backbone network can improve proposal quality, they often fail to distinguish between foreground and background, resulting in suboptimal performance.To address these challenges, we propose a novel RPN prototype alignment method to align foreground and background RPN features separately. We introduce learnable RPN prototypes and enforce alignment between the RPN features and corresponding prototypes in both the source and target domains. We also propose a simple yet effective pseudo label generation method for RPN feature alignment. Additionally, we utilize Grad CAM to identify discriminative regions within a proposal, enabling us to adjust RPN features accordingly and improve alignment accuracy.Our contributions include the proposed RPN prototype alignment method, the effective pseudo label generation method, and the discriminability-aware prototype alignment module. We evaluate our approach on multiple benchmark scenarios, and the results demonstrate its effectiveness compared to previous state-of-the-art methods.