Collaborative learning systems, like federated learning systems, have shown promise in improving training speed, model performance, and generalization while protecting training data privacy. However, sharing gradients in collaborative learning can indirectly leak sensitive data, making participants vulnerable to reconstruction attacks. Existing privacy-aware solutions attempt to obfuscate gradients but often compromise model accuracy. This paper proposes a different approach by obfuscating training data using data augmentation techniques. The goal is to discover effective transformations that prevent reconstruction of transformed or original samples. To overcome challenges, the paper introduces a systematic approach to automatically discover effective transformation policies. Two novel metrics are designed to quantify the policies without training a complete model, enabling the identification of optimal policies within 2.5 GPU hours. The identified transformation policies effectively preserve privacy while maintaining model performance. They are general, lightweight, and highly transferable to different collaborative learning systems and datasets.