Monocular 3D reconstruction of the human face has long been a challenging problem in computer science with a wide range of potential applications. Existing approaches for 3D face reconstruction often rely on strong assumptions or priors, such as shape-from-shading or the use of 3D Morphable Models (3DMM). Recently, deep neural networks have been introduced to regress 3DMM parameters from 2D images, either through supervised learning or by improving the non-linearity of the 3DMM. However, these single-view guided methods may suffer from 2D ambiguity, leading to inaccurate or inconsistent reconstructions. Other works have leveraged multi-view consistency to improve 3D face modeling using 3DMM, but these models have limitations in capturing facial texture, teeth, skin details, and anatomic-grounded muscles. To address these limitations, alternative approaches have been proposed that learn to model 3D face directly from input images, regressing face normal or depth. While these approaches are efficient, they struggle to model facial texture or achieve a canonical view without occlusions. To overcome these challenges, a recent work called Unsup3D introduced a weakly symmetric constraint to disentangle the face into intrinsic factors and achieved unsupervised canonical reconstruction. However, without a reliable 3DMM prior, these non-parametric methods often suffer from ambiguity in image appearance and produce coarse or inconsistent shape reconstructions. In this paper, we propose a novel framework called Learning to Aggregate and Personalize (LAP) for unsupervised non-parametric 3D face modeling. LAP first aggregates consistent face factors of an identity from an in-the-wild photo collection, leveraging shape and appearance consistency assumptions. It then personalizes these factors to reconstruct a scene-specific face for a target image of the same identity. LAP achieves this disentanglement of identity-consistent facial structure and scene-specific local details without relying on the 3DMM shape assumption. Our framework is able to model 3D face from an arbitrary number of images, or even a single image, with superior quality and higher resolution compared to state-of-the-art methods. This paper contributes by introducing the LAP framework, which enables unsupervised 3D face modeling, disentangling ID-consistent factors, and modeling scene-specific attributes and details.