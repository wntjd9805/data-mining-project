Shadows in natural images have various applications in image understanding tasks, such as scene geometry extraction, light direction estimation, and object detection. While image shadow detection has been extensively studied, there has been limited research on shadow detection in dynamic scenes. In addition, video processing techniques, such as salient object detection and object segmentation, have seen significant advancements, yet video shadow detection lags behind. One reason for this is the lack of a standard benchmark dataset for video shadow detection, especially one that incorporates temporal information. To address this gap, we present the Video Shadow Detection (ViSha) dataset, which consists of 120 videos with annotated pixel-level shadow masks. This dataset covers various object classes, motion and lighting conditions, and different instance numbers. We also propose a triple-cooperative video shadow detection network (TVSD-Net) as a baseline model for this task. TVSD-Net utilizes both intra-video and inter-video correlations to capture temporal information effectively. We evaluate our dataset with 12 state-of-the-art models, and the results demonstrate the superiority of our approach. This work contributes to the development of a learning-oriented benchmark for video shadow detection and provides a new perspective on video object detection. Our dataset and code are publicly available for further research in this area.