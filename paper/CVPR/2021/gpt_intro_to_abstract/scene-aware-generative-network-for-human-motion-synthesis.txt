The synthesis of human motions is essential for various real-world applications, such as virtual reality, film-making, and action forecasting. However, existing methods often overlook the role of scene context, focusing solely on the movements of human bodies. This neglects the fact that human movements are influenced by their surroundings and the need to observe the environment to execute reasonable motions. This paper aims to explore the synthesis of human motions under the influence of scene context. The authors identify two major challenges: effectively reflecting the guidance provided by the scene context and modeling the complex physical relationship between scenes and action sequences. To address these challenges, the authors propose a scene-aware fully generative framework that incorporates the scene context in the synthesis of human motions. The framework utilizes convolutional sequence generation networks to model both the trajectory of humans in the scene and their corresponding body movements. To ensure physical compatibility, the framework also introduces the geometry structure of the scene as prior knowledge. This is achieved through the supervision of an encoder that extracts geometry context from depth maps. Additionally, the authors propose projection and context discriminators to encourage compatibility between synthesized human motions and the scene context. The proposed framework is evaluated on two challenging datasets, demonstrating its effectiveness in synthesizing human motions with fidelity, diversity, and consistency with the corresponding scenes. The paper also introduces new metrics for evaluating human motion synthesis with scene context. The contributions of this work include formulating human motion synthesis with scene context as a conditional generation problem and developing a novel geometry-aware fully generative framework that captures the diversity of human motions in a scene.