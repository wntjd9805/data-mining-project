Deep neural networks (DNNs) have achieved great success in computer vision tasks, but their large parameter size and computational requirements hinder their application on resource-constrained devices. Network compression methods, such as channel pruning, efficient architecture design, and network quantization, have been proposed to address this issue. Network quantization aims to replace floating-point operations with fixed-point or bitwise operations, reducing computational costs. While quantization methods have shown promise for image classification, using aggressively low-bit quantized networks for more complex tasks like object detection remains a challenge. Existing quantized object detection methods achieve promising results with low-bit quantization but suffer a significant performance drop with aggressive low-bit quantization. Additionally, some layers still require floating-point operations, which consume more energy and area compared to fixed-point operations. This paper proposes an Accurate Quantized object Detection (AQD) method to eliminate floating-point computation in all layers of the network while maintaining performance. The proposed method replaces floating-point operations with fixed-point operations, reducing computational overhead. To address the performance drop, a new variant of batch normalization called multi-level BN is proposed. Experimental results on the COCO detection benchmark demonstrate that AQD achieves comparable or better performance than its full-precision counterpart, improving on-device efficiency.