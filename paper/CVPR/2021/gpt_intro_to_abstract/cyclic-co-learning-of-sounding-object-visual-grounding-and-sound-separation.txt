Abstract:This paper addresses the problem of multi-modal sound separation and sounding object alignment, which are important tasks in audio-visual perception. While previous works have explored these tasks separately, this work proposes a co-learning framework that jointly models audio-visual sound separation (AVSS) and sound source visual grounding (SSVG) to mimic human perception. The proposed framework leverages the joint modeling of these tasks to discover objects that make sounds and separate their corresponding sounds without using annotations. Unlike previous methods that assume all objects make sounds or focus on simple scenarios, the proposed framework aims to separate sounds only for sounding objects in a cocktail-party environment. To enable co-learning, a sounding object-aware sound separation strategy is introduced, and individual sounding objects are discovered in visual scenes from visual object candidates. The co-learning framework involves using separation results to sample more reliable training examples for grounding, leading to improved performance in both tasks. Experimental results demonstrate that the proposed framework outperforms recent methods in sounding object visual grounding and audio-visual sound separation tasks, validating the effectiveness of the co-learning approach. The contributions of this paper include the proposal of the co-learning framework, the introduction of the sounding object-aware sound separation strategy, and the validation of the mutual benefits of the proposed co-learning approach.