Deep generative models have made significant advancements in various generative tasks, such as 2D image synthesis and 3D shape reconstruction. These improvements are attributed to realistic priors learned from large training datasets. Current 3D learning approaches focus on learning stronger priors during training and strictly adhering to them during testing. Two common methods to leverage learned shape priors are using an encoder to retrieve the most likely prior or optimizing the latent code until the decoded output achieves minimal loss. However, these approaches assume that the learned prior accurately represents the full landscape of the real data distribution, which may not always be the case. Particularly in 3D learning tasks where obtaining ground-truth data is challenging, the learned prior may be a crude approximation of the data distribution, making the network susceptible to unseen data. Optimization-based approaches that leverage data constraints do not require training but have constraints and limitations. To address these challenges, we propose a new 3D learning paradigm that combines the benefits of learning-based and optimization-based approaches. We advocate for optimizing the pre-trained shape prior and the latent code according to data constraints at test time. By introducing physically-based optimization, we can break the barriers of pre-trained priors and converge on a more realistic, but previously unseen, prior. We demonstrate the effectiveness of our approach using implicit surface representation and show its generality in various downstream applications, including shape modeling and reconstruction. Experimental results show that our approach outperforms existing methods both quantitatively and qualitatively, even in challenging tasks such as sparse-view reconstruction and point cloud reconstruction.