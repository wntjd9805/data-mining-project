Video prediction is a challenging task in computer vision that involves predicting future frames based on past frames. Existing methods often struggle to accurately predict complex actions involving interactions between multiple objects. This paper proposes a Human-Object Relational Network (HORN) that learns a layout sequence as an intermediate representation for video prediction of human-object interactions. The layout sequence combines pose and object sequences to capture the dynamic interactions between objects and poses. This approach improves the accuracy of frame predictions by capturing both the motion of the person and the locations of various objects throughout the action. The proposed model has several key contributions, including modeling full-body motion for humans, capturing pose and object locations in the intermediate representation, and generalizing to new interactions not seen in the training set.