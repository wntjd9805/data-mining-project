Deep learning has achieved significant success in various fields, but it still faces limitations in practical applications where labeled samples are scarce or expensive. Few-shot learning (FSL), which aims to generalize to new concepts with minimal supervision, has gained attention due to the difficulty of machines in quickly adapting to new tasks with limited labeled examples. Self-supervised learning (SSL) has emerged as a promising approach for learning representations without manual labels. It leverages pretext tasks to exploit the structural information present in the data, improving generalization in the presence of limited labeled data. Some recent few-shot auxiliary learning (FSAL) approaches apply SSL as an auxiliary task to enhance few-shot learning. However, optimizing the objectives of distinct tasks poses challenges, as they differ and can conflict with each other. Previous solutions rely on experience-based trade-offs, which may not achieve optimal performance or consistency. To address this, we propose Pareto self-supervised training (PSST), a novel approach that casts few-shot auxiliary learning as a multi-objective optimization problem. Instead of a linear combination, PSST adopts a preferred Pareto exploration strategy that allows for efficient and accurate exploration within the preference region. Experimental results demonstrate that PSST effectively models the trade-off between tasks, leading to state-of-the-art performance on benchmark datasets. This paper makes the following contributions: (1) highlighting the issue of conï¬‚icting objectives in few-shot auxiliary learning and proposing a multi-objective optimization solution, (2) introducing PSST as an approach for few-shot auxiliary learning, leveraging preferred Pareto exploration, and (3) demonstrating the effectiveness of PSST through extensive experiments on benchmark datasets.