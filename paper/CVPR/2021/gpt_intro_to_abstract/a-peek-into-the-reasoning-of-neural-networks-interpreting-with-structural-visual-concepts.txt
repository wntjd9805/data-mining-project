In recent years, there has been a significant increase in the use of machine learning, particularly in areas such as security and medicine. However, transparency and explainability of these neural network models are crucial for end-user trust in the algorithm. This has led to the emergence of explainable AI (xAI) as an important research topic. Current xAI approaches focus on visualizations that explain the correlation between input pixels or low-level features and the NN's decision process. Perturbation-based and gradient-based methods have received particular attention. However, these methods have limitations in terms of providing in-depth reasoning, verifying model explanations, and guiding error correction. To address these limitations, this paper proposes the visual reasoning explanation framework (VRX). The VRX utilizes high-level category-specific visual concepts and their relationships to build structural concept graphs (SCGs) that highlight spatial relationships. A graph reasoning network (GRN) is then introduced to explain the NN's reasoning process by optimizing the structural relationships between concepts. The GRN helps answer interpretability questions and provides systematic verification techniques. In addition, the VRX can also assist in diagnosing incorrect predictions and improving model performance. Qualitative and quantitative results demonstrate the efficacy and reliability of the proposed framework. Overall, the VRX framework is a valuable contribution towards building next-generation explainable and transparent AI systems.