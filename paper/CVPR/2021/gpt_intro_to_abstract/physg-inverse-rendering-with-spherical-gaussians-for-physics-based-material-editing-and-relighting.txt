Vision as inverse graphics has long been a fascinating concept in computer science. The goal of solving inverse rendering problems, which involve recovering shape, material, and lighting from images, has been a longstanding pursuit. Recently, there has been significant interest in neural rendering methods, which have shown remarkable success in various tasks such as shape reconstruction, novel view synthesis, non-physically-based relighting, and surface reflectance map estimation. These methods incorporate scene representations that can be physical, neural, or a combination of both, along with a neural-network-based renderer.While methods that reconstruct textures or radiance fields have been effective in interpolating novel views, they fail to separate appearance into lighting and materials. This limitation prevents physically-based appearance manipulation, such as material editing or relighting.In this work, we address the multi-view inverse rendering problem under the challenging conditions of using normal RGB input images with the same static illumination and without assuming scanned geometry. To overcome these challenges, we propose PhySG, an end-to-end physically-based differentiable rendering pipeline.PhySG jointly estimates lighting, material, geometry, and surface normals from posed multi-view images of specular objects. Our approach represents the shape using signed distance functions (SDFs), which have shown success in recent research. Additionally, a key component of our framework is the use of spherical Gaussians to approximate lighting and specular BRDFs, enabling efficient evaluation of light transport.By reconstructing shape, illumination, and materials from 2D images alone, our method allows for subsequent physics-based appearance manipulations, such as material editing and relighting. Furthermore, we demonstrate that PhySG not only generalizes to novel viewpoints but also enables physically-intuitive material editing and relighting, surpassing prior neural rendering approaches.In summary, our contributions include the development of PhySG, an end-to-end inverse rendering approach for jointly estimating lighting, material properties, and geometry from multi-view images of glossy objects under static illumination. Our pipeline leverages spherical Gaussians for efficient evaluation of the rendering equation. Compared to previous methods, PhySG enables generalization to novel viewpoints and allows for physically-intuitive material editing and relighting.