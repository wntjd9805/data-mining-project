Deep neural networks are difficult to train due to non-convexity and limitations of first-order methods. Architectural designs such as normalization layers and residual connections have helped in training. Adaptive gradient methods like AdaGrad and Adam have shown promise in training complex networks, but recent works have highlighted their shortcomings. SGD is still dominant in training simple architectures in computer vision tasks, while adaptive methods are more commonly used in natural language processing tasks. This paper focuses on finding conditions and properties for an optimizer to be dominant in multiple domains, which is referred to as domain-independent dominance. The paper analyzes the convergence of adaptive methods for stochastic non-convex problems and provides a sufficient condition for convergence. A new adaptive method called AvaGrad is proposed that decouples the learning rate and the adaptability parameter. Extensive experiments show that Adam can outperform SGD in certain tasks. AvaGrad performs as well as SGD and Adam, while requiring less hyperparameter tuning. The paper provides new insights on the role of the adaptability parameter in convergence and performance of adaptive methods. Contributions include provable convergence of Adam for non-convex problems, experiments showing the superiority of Adam over SGD, and the introduction of AvaGrad as a theoretically-motivated adaptive method.