This paper focuses on the audio-visual video parsing (AVVP) task, which aims to analyze auditory, visual, and audio-visual events in videos without assuming temporal alignment between audio and visual data. Existing research works assume a correlation between audio and visual data, but in practice, this alignment may not always hold. The authors propose addressing the modality uncertainty issue by exchanging audio and visual tracks with other unrelated videos, obtaining modality-aware event labels individually for each modality. They also introduce temporal heterogeneous constraint into the attention model using contrastive learning, which improves temporal localization performance. Experiments show that the proposed method outperforms state-of-the-art methods on all evaluation metrics. Overall, this paper presents a novel approach to AVVP that addresses modality uncertainty and improves temporal boundary detection.