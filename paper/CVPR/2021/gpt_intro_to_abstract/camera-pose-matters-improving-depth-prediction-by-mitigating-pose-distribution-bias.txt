Monocular depth prediction is a challenging task that aims to estimate the 3D geometry of a scene from a 2D image. Convolutional neural network (CNN) based depth predictors have achieved impressive performance on this task by leveraging large-scale datasets. However, an important factor that has been overlooked is the bias in the distribution of camera poses in these training datasets. This bias leads to unreliable depth predictions on images captured from uncommon camera poses.To address this issue, we propose two novel techniques. Firstly, we introduce perspective-aware data augmentation (PDA) which generates new training examples with more diverse viewpoints by perturbing the camera poses. This approach ensures that the resulting images and target depth are geometrically consistent. Secondly, we propose a conditional depth predictor (CPP) which incorporates the camera pose as a prior during depth estimation. We encode the camera pose as an additional channel alongside the RGB input, resulting in more accurate depth predictions that generalize well under diverse camera poses at test-time.Extensive experiments demonstrate the effectiveness of our proposed techniques. They significantly improve the depth prediction performance on images captured from uncommon and never-before-seen camera poses. Moreover, our techniques can be applied to any network architecture and have shown to enhance the performance of state-of-the-art models. Additionally, we highlight the importance of addressing the biased camera pose distribution for domain adaptation in 3D geometric prediction tasks.Overall, our contributions provide valuable insights and practical solutions to improve monocular depth prediction by addressing the bias in camera pose distribution during training and test time.