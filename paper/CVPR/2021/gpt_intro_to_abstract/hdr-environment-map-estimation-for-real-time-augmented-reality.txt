This paper discusses video see-through augmented reality (AR) applications, focusing on the visual coherence between virtual and real objects in the composited video. The specific challenge addressed in this work is creating reflections and lighting for virtual objects by estimating an omnidirectional HDR environment map. The environment map needs to be high dynamic range and have sufficient resolution to accurately represent objects and features in the scene. The paper proposes a method using the equirectangular projection and RGB color space for the environment maps. The mobile AR framework allows for obtaining camera frames, poses, and scene geometry, which enables real-time rendering of light probes at the 3D location of the virtual object. The method takes a partial environment map composed of low dynamic range camera frames as input and produces a completed environment map with higher dynamic range. The completed map retains the color and details from the input while filling in the unknown pixels with coherent content. The paper demonstrates through quantitative and qualitative comparisons that the proposed method outperforms existing techniques in estimating high-quality and visually plausible HDR environment maps. The directional error for lights is reduced by more than half, and the Frechet Inception Distance is significantly lower.