This paper introduces the concept of one-shot talking face generation, which involves synthesizing a realistic talking avatar video using a reference facial image and driving audio. This technology is important for various applications such as virtual assistants, mixed realities, and animation movies. While many efforts have been made to create realistic videos, the generation of high-resolution videos remains a challenge. Existing works only produce videos with low resolutions, and there is a lack of appropriate datasets for high-resolution talking face generation. Previous approaches also struggle to handle high-resolution videos due to the complexity of mappings from facial landmarks to images and the limitations of sparse landmarks. To address these challenges and promote the development of high-resolution talking face generation, the authors first create a large in-the-wild high-resolution audio-visual dataset called the High-definition Talking Face Dataset (HDTF). This dataset contains over 16 hours of 720P-1080P videos from YouTube, with more than 300 subjects and 10k different sentences. The authors then propose a novel flow-guided framework for synthesizing high-quality videos. The framework consists of an audio-to-animation module and an animation-to-video module. The audio-to-animation module uses 3DMM to decouple the face into facial animation parameters and introduces a style-specific animation generator to produce subject-dependent animations. In the animation-to-video module, a flow-guided approach is employed to transform animation parameters into dense flow, which provides richer facial details compared to sparse landmarks. A carefully-designed video generator is then used to synthesize talking face videos from the dense flow, resulting in more realistic visual quality. The contributions of this paper include the creation of the HDTF dataset, the development of a style-specific animation generator for specific style animation parameters, the utilization of one animation generator with multi-task learning for one-shot talking face generation, and the proposal of a novel flow-guided framework for higher visual quality video synthesis.