Reconstructing 3D shapes from single-view RGB images is a challenging problem in computer vision. Recent advancements in deep learning have led to the development of data-driven methods for single-view 3D reconstruction using neural implicit models. However, these methods struggle to recover fine-level geometric details, limiting the quality of the reconstructed shapes. In this paper, we propose a new approach called D2IM-Net that aims to recover both topological structures and surface details from single-view images. Our key idea is to train the network to learn a detail disentangled reconstruction consisting of a coarse shape and a detail function. We tackle the challenge of defining geometric details by training the network without direct supervision. D2IM-Net consists of an encoder that extracts global and local features from the input image, and two decoders that predict a coarse shape and displacement maps representing details, respectively. We define several loss functions to optimize the reconstruction, including a Laplacian loss that enforces the recovery of surface details. We train and evaluate our network on the ShapeNet Core dataset across various shape categories. Our results show that D2IM-Net outperforms other state-of-the-art methods in terms of reconstructing shape details. We also demonstrate a novel application of our network for detail transfer from an image onto a reconstructed 3D shape. Our work contributes to the advancement of single-view 3D reconstruction methods by focusing on recovering fine-level geometric details.