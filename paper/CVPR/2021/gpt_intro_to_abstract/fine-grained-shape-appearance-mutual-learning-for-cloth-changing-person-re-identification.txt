Person re-identification (Re-ID) is the task of matching the same person across different cameras. While advanced methods using deep learning have achieved high performance in Re-ID, most current approaches rely heavily on color appearance. This assumption of consistent clothing leads to a decrease in performance when people change their clothes. To address this limitation, this paper focuses on the cloth-changing Re-ID problem. Instead of relying on color appearance, the paper proposes to learn cloth-unrelated features, specifically the body shape. The body shape is a stable cue for identification even when clothing changes. The paper introduces a Fine-grained Shape-Appearance Mutual learning framework (FSAM) composed of a shape stream and an appearance stream. In the shape stream, fine-grained human masks are learned to capture detailed shape differences between pedestrians, while a pose-specific feature extractor handles mask deformation caused by pose variations. In the appearance stream, a dense interactive mutual learning method transfers fine-grained body shape knowledge from the shape stream, enhancing the learning of robust cloth-unrelated appearance features. Compared to existing methods that typically use off-the-shelf estimators for pose or contour estimation, FSAM learns fine-grained masks with discriminative shape details, saving computational costs in inference. Experimental results on benchmark cloth-changing Re-ID datasets demonstrate the effectiveness of FSAM in achieving state-of-the-art performance. The contributions of this paper include the learning of fine-grained body shape features for cloth-changing Re-ID, the introduction of a dense interactive mutual learning framework, and the achievement of state-of-the-art results on several benchmark datasets.