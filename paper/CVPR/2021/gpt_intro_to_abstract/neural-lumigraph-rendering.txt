Novel view synthesis and 3D shape estimation from 2D images are important problems in computer science with applications in various fields such as photogrammetry, remote sensing, visualization, AR/VR, teleconferencing, visual effects, and games. While traditional 3D computer vision pipelines have been extensively studied, recent neural rendering techniques have shown promising results in achieving realistic quality for novel view synthesis. However, state-of-the-art neural rendering approaches, like neural radiance fields, lack real-time framerates, limiting their usefulness in practical applications. This limitation is primarily due to the choice of implicit neural scene representation and rendering algorithm, which requires a custom neural volume renderer.In this paper, we propose a new framework that addresses the limitations of existing neural rendering approaches. Our approach adopts an SDF-based sinusoidal representation network (SIREN) as the backbone of our neural rendering system. SIREN has shown impressive performance in representing shapes through direct 3D supervision with point clouds. In our work, we demonstrate how to leverage SIREN's capacity in learning 3D shapes using 2D supervision with images via neural rendering. To prevent overfitting on supervised views, we devise a novel loss function that constrains SIREN's high-capacity encoding in the angular domain.By training our approach with a sparse set of multi-view images, we achieve robust fitting of a SIREN-based SDF. Our 2D-supervised implicit neural scene representation and rendering approach performs comparably to NeRF on view interpolation tasks, while also providing a high-quality 3D surface that can be directly exported for real-time rendering. We develop a neural rendering framework consisting of an implicit neural 3D scene representation, a neural renderer, and a custom loss function for training. Our approach achieves rendering rates 10 times higher than NeRF while maintaining comparable image quality and optimizing an implicitly defined surface.Additionally, we demonstrate how both the shape and view-dependent appearance of our neural scene representation can be exported and rendered in real-time using traditional graphics pipelines. We further contribute by constructing a custom camera array and capturing datasets of faces and heads for evaluating our approach and comparing it against baselines. These datasets are available on our project website. Overall, our framework offers a novel solution for real-time neural rendering, with improvements in rendering rates and image quality, making it suitable for various practical applications.