Modern object detectors in computer vision often generate a set of bounding boxes with classification scores and then remove duplicates using non-maximum suppression (NMS). However, using classification scores to rank the bounding boxes in NMS can harm detection performance, as these scores do not always accurately represent the localization accuracy of the bounding boxes. This can result in mistakenly removing accurately localized detections with low classification scores. To address this problem, existing dense object detectors predict additional scores, such as IoU or centerness, as estimates of localization accuracy, and multiply them by the classification scores in NMS. However, this approach is sub-optimal as it may worsen the ranking basis. In this paper, we propose a solution by merging the localization accuracy into the classification score. We introduce the concept of an IoU-aware classification score (IACS) that represents both the presence of an object class and the localization accuracy of a bounding box. Our contributions include showing that accurately ranking candidate detections is crucial for dense object detectors, proposing a new Varifocal Loss for training dense object detectors to predict IACS, designing a new star-shaped bounding box feature representation for IACS computation and bounding box refinement, and developing a new dense object detector called VarifocalNet (VFNet) based on FCOS+ATSS to exploit the advantages of IACS. Experimental results on the COCO benchmark demonstrate that VFNet consistently outperforms baseline methods by approximately 2.0 AP, achieving state-of-the-art results on single-model single-scale detection tasks.