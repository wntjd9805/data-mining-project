This paper focuses on the task of visual-audio fixation prediction in videos, which aims to predict human-eye fixations in the form of scattered coordinates spread across the scene. Existing state-of-the-art approaches for visual-audio fixation prediction heavily rely on fully supervised training with real visual fixations, but the data scarcity issue is a major limitation. To address this problem, the authors propose a weakly-supervised approach that generates pseudo fixations using video category tags. Inspired by class activation mapping (CAM) techniques, they introduce a novel selective class activation mapping (SCAM) method that selects the most discriminative regions from multiple sources, ensuring that less-discriminative scattered regions are filtered out. With the obtained pseudo fixations, a spatial-temporal-audio (STA) fixation prediction network is trained to accurately predict fixations in unlabeled videos. This work contributes to the exploration of deep learning-based weakly-supervised visual-audio fixation prediction and has the potential to enhance visual-audio information integration in computer vision applications.