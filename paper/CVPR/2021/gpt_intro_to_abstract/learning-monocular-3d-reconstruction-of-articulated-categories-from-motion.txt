Monocular 3D reconstruction of articulated object categories is a challenging task for computer vision systems, despite being routinely performed by humans. While breakthroughs have been made for humans using 3D joint locations as strong supervision, the problem remains in its infancy for general categories due to lack of strong supervision and the challenge of representing articulated deformations. Recent works have started tackling this problem using minimal 2D-based supervision, such as keypoint annotations or masks, and learning morphable model priors or hand-crafted mesh segmentations. In this paper, we leverage the rich information available in videos and use networks trained for 2D tasks to complement 2D keypoint-level supervision. We make three contributions to monocular 3D articulated reconstruction: (1) using motion information from videos to supervise monocular 3D category reconstruction, (2) introducing a model for regularized mesh deformations that allows for learnable, part-level mesh control, and (3) adopting an optimization-based approach inspired by bundle adjustment for refined 3D reconstruction. We evaluate our approach on a range of object categories and demonstrate that it outperforms recent approaches in 3D shape, pose, and texture reconstruction.