Video captioning, which involves describing videos with natural language sentences, has become an area of increasing research interest due to the widespread presence of videos in our lives. While previous work has focused on generating single-sentence captions for carefully trimmed videos with a single major event, the videos found in the wild are typically untrimmed with complex temporal event structures. Thus, generating a single sentence is insufficient to convey fine-grained information in such videos. Recent works have attempted to address this issue by generating story-oriented paragraphs with multiple sentences to comprehensively describe video contents.Existing approaches in this area generally adopt a two-stage framework, where event segments in the video are first detected, and then event descriptions are generated for each segment. However, this framework requires costly annotation of temporal segment coordinates, and detecting precise event segments in open-domain untrimmed videos is challenging. Poorly detected events can greatly harm the performance of paragraph captioning. Some works use ground-truth event segments to generate video paragraphs, but these approaches cannot generalize to videos without event annotations.In this paper, we challenge the necessity of event detection for video paragraph captioning. Taking inspiration from approaches in image paragraph captioning, where sentences are directly generated from images without predicting sequences of image coordinates, we aim to generate video paragraph descriptions in a single stage without the need for event segment detection. However, there are three main challenges in video paragraph captioning without event segments: the larger number of frames in untrimmed videos compared to images, the difficulty of learning an effective attention mechanism to form coherent descriptive logic with diverse events, and the tendency to generate redundant words and phrases in long paragraph generation.To overcome these challenges, we propose a one-stage framework that automatically selects keyframes during video encoding, improving computational efficiency. We also introduce dynamic video memory to guide the model in effective description logic learning, and token-level and sequence-level penalties to encourage more unique expressions and improve diversity in generated paragraphs. Experimental results demonstrate that our model outperforms two-stage methods even when utilizing ground-truth event segments and achieves state-of-the-art performance on both ActivityNet and Charades datasets without using any event boundary annotations.In summary, our contributions include being the first to generate paragraphs for untrimmed videos without relying on event detection and proposing an attention mechanism with dynamic video memories and diversity-driven training objectives. Our model achieves improved generation efficiency and state-of-the-art results on benchmark datasets.