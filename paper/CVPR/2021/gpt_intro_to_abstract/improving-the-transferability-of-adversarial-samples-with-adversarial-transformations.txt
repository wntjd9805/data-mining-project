Deep neural networks (DNNs) have achieved remarkable success in various vision tasks. However, they are highly susceptible to adversarial samples, which are artificially modified images that can mislead DNNs into making incorrect predictions. This vulnerability poses a significant threat to the security of DNN-based systems, particularly in safety-critical domains like self-driving. To address this issue, synthesizing adversarial samples can serve as a crucial tool for evaluating the robustness of DNNs and developing effective defenses. There are two main lines of adversarial attacks: white-box attacks, where attackers have perfect knowledge of the target model, and black-box attacks, where attackers have limited information about the target model. Black-box attacks, especially transfer-based attacks, are considered more realistic and challenging. However, existing transfer-based attacks often suffer from limited transferability, where adversarial samples crafted for one model fail to fool a different target model. To improve transferability, previous works have focused on training adversarial samples to be robust against common image transformations. However, these approaches often overfit to specific transformations and struggle against unknown distortions. To address this limitation, we propose an adversarial transformation network that automates the distortion tuning process. By training the network to capture and resist harmful deformations caused by adversarial noises, we can generate adversarial samples that are more robust and have improved transferability. Our approach outperforms state-of-the-art baselines in attacking both undefended and defended models, demonstrating its potential as a general strategy for boosting adversarial transferability.