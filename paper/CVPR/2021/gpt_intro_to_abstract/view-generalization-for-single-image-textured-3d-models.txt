In this paper, we address the challenge of recovering realistic 3D geometry and texture from a single 2D image. While current algorithms can infer 3D shape and texture from a single image, they often suffer from severe view generalization problems, resulting in unrealistic representations when viewed from new perspectives. To overcome this issue, we propose a controllable convolutional deformation approach for predicting a single UV deformation map over a spherical surface. By varying the predicted spatial map resolution, we can control the extent of deformation and achieve better view generalization. Additionally, we present two novel cycle consistency losses that improve both the appearance and shape of the inferred textured 3D models. Our geometry and texture models are controlled by rotation GAN cycle consistency and texture mesh alignment consistency losses, which ensure the realism and coherence of the synthesized models. Our experiments demonstrate that our framework significantly enhances the fidelity of the synthesized textured 3D models. We show that the controllable deformation approach is particularly effective for deforming non-rigid object categories, such as birds, which undergo varying degrees of deformation across different images. Furthermore, we find that leveraging multiple mesh templates improves the overall quality of the recovered 3D inferences without the need for finely curated multi-view datasets. In contrast to the standard evaluation metrics, which focus on the intersection over union (IoU) compared to the ground truth mask from the original view, we evaluate view generalization by comparing multiple renderings from novel viewpoints with real images of real objects. We use the Fr√©chet Inception Distance (FID) scores to quantify the similarity between the synthesized models and real images. Additionally, we conduct user studies to compare our inferred 3D models with baseline methods.In summary, our main contributions include the development of a controllable convolutional deformation approach for better recovery of 3D geometry, the introduction of two novel cycle consistency losses for improved textured 3D model inference, and the synthesis of high-fidelity textured 3D models from a single image.