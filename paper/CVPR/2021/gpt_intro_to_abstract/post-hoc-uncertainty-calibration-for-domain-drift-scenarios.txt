Deep neural networks are increasingly being used in decision making systems in real world applications due to their high predictive power. However, these systems require not only high accuracy, but also reliable and calibrated uncertainty estimates. A classifier is considered calibrated if the confidence of predictions matches the probability of being correct for all confidence levels. This is particularly important in safety critical applications in medicine and in dynamically changing environments in industry. Practitioners need access to reliable predictive uncertainty throughout the entire life-cycle of the model, including gradual domain drift scenarios where the distribution of input samples gradually changes from in-domain to out-of-distribution. Existing post-hoc uncertainty calibration methods have focused on in-domain predictions, but little attention has been paid to uncertainty calibration under domain drift. In this work, we address the task of post-hoc uncertainty calibration under domain drift and make the following contributions: (1) We demonstrate that neural networks yield overconfident predictions under domain shift even after re-calibration using existing post-hoc calibrators. (2) We generalize existing post-hoc calibration methods by transforming the validation set before performing the calibration step. (3) We show that our approach results in substantially better calibration under domain shift on a wide range of architectures and image datasets. The code for our approach is publicly available.