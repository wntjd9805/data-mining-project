Improving the performance of Convolutional Neural Networks (ConvNets) has been a significant research area in computer science. While advancements in architecture design, such as the Inception models, have demonstrated the enrichment of feature space and improved performance, they often come at the cost of slower inference due to complicated structures. Additionally, increased parameters and connections typically lead to better performance, but the size of ConvNets is limited by business requirements and hardware constraints. To address these issues, this paper proposes a method that inserts complicated structures into ConvNet architectures during training while maintaining the original inference-time costs. This is achieved by decoupling the training-time and inference-time network structures, where the model is complicated only during training and then converted back to the original structure for inference. The authors introduce a powerful building block called Diverse Branch Block (DBB), which enhances ConvNets by adopting a multi-branch topology with various convolution sizes, average pooling, and branch addition. The DBBs can be transformed into regular convolutions for deployment, ensuring no extra inference-time costs. Through experiments, it is demonstrated that DBBs improve the performance of ConvNets without sacrificing the macro architecture or incurring additional inference-time costs. The contributions of this paper include the incorporation of microstructures into ConvNet architectures to enhance performance while preserving the macro architecture, the introduction of the DBB as a universal building block, and the demonstration of improved performance on various datasets.