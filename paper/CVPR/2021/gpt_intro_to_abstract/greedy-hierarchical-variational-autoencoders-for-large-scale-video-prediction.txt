A core aspect of intelligence is the ability to predict the future. This paper introduces Greedy Hierarchical VAEs (GHVAEs), a set of local latent VAE modules that can be sequentially stacked and trained in a greedy, module-by-module fashion. GHVAEs address the challenges of learning accurate large-scale video prediction by both optimizing memory constraints and addressing the instability of deep hierarchical VAEs. By training sub-modules sequentially, GHVAEs enable larger models to be learned within the same amount of GPU or TPU memory. Additionally, optimizing hierarchical VAEs in a greedy and modular fashion breaks the bidirectional dependency among individual latent variables, improving stability throughout the training process. Experimental evaluations show that GHVAEs outperform state-of-the-art video prediction models in terms of accuracy and success rates on different datasets and real robotic manipulation tasks. This work demonstrates that using greedy machine learning can enhance the optimization stability and memory efficiency of hierarchical VAEs, leading to significant improvements in video prediction accuracy and robotic task success rates.