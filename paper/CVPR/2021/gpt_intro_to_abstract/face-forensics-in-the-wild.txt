The rise of synthetic audiovisual media, particularly deepfakes, has raised concerns about the reliability of video and audio recordings as representations of reality. Accurate face forgery detection is crucial in mitigating the potential harm caused by deepfakes, such as face recognition attacks and fake news. To support this effort, several benchmarks, including FaceForensics++, have been established. However, these benchmarks have limitations, particularly in representing real-world, multi-person scenarios. To address this, a new large-scale dataset called FFIW10K is introduced, which features manipulated videos with multiple individuals but only some manipulated faces. This presents a significant challenge to current techniques as real faces still outnumber fake faces in the videos. FFIW10K provides both video- and face-level annotations, allowing for benchmarking on forgery classification and localization tasks. Additionally, a discriminative attention model is proposed for face forgery classification and localization in multi-person scenarios. This model utilizes multiple instance learning and comprises three essential parts: a multi-temporal-scale instance feature aggregation module, an attention-based bag feature aggregation module, and a sparse regularization loss. The contributions of this work include the FFIW10K dataset, a model-agnostic quality assessment model, and a discriminative attention model for multi-person face forgery detection.