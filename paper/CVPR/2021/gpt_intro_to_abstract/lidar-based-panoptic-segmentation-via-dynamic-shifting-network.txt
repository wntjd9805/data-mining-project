Abstract:Autonomous driving has made significant progress in recent years, with perception systems playing a crucial role in the development of autonomous vehicles. While classic tasks like 3D object detection and semantic segmentation have mature solutions, there is still a gap in achieving holistic perception for challenging autonomous driving scenarios. This paper proposes a solution to this gap by exploring the task of LiDAR-based panoptic segmentation, which involves predicting point-level semantic labels for both background (stuff) classes and performing instance segmentation for foreground (things) classes in LiDAR point clouds. Existing point cloud instance segmentation methods struggle with the complex point distributions in LiDAR data, leading to suboptimal results. To address these challenges, the Dynamic Shifting Network (DS-Net) is proposed, which features a strong backbone design, a novel Dynamic Shifting Module for clustering regressed centers, and a Consensus-driven Fusion Module for unifying semantic and instance results. Extensive experiments on the SemanticKITTI benchmark demonstrate the effectiveness of DS-Net, outperforming state-of-the-art methods and achieving first place on the public leaderboard. The contributions of this work include presenting one of the first attempts at LiDAR-based panoptic segmentation, achieving state-of-the-art performance, and providing extensive experimental evaluations and observations.