This paper addresses the challenge of Class-Incremental Learning (CIL) in adaptive learning systems. CIL involves learning new samples and their corresponding class labels in incremental data batches. The problem of discarding old data in CIL is especially detrimental to deep learning models, as it leads to catastrophic forgetting of the previously learned classes. Existing approaches to mitigate forgetting include data replay, feature distillation, and label distillation, but they have limitations in coherence with new class learning and require significant storage. To address these limitations, this paper proposes a causal inference framework to model the causalities among data, feature, and label in consecutive learning steps. By framing CIL within this framework, the paper explains why forgetting happens and how data replay and feature/label distillation mitigate forgetting. The paper then introduces a new method called Distilling Colliding Effect, which effectively distills the desired end-to-end effect without the need for replay storage. Additionally, the paper addresses the issue of model bias caused by the imbalance between the causal effect of new and old data. A method called Incremental Momentum Effect Removal is proposed to remove biased data causal effects that cause forgetting. Extensive experiments on CIFAR-100 and ImageNet datasets show consistent performance improvements using the proposed causal effect distillation method. The approach outperforms state-of-the-art methods, particularly when the replay of old data is limited. These results demonstrate the effectiveness of the proposed distillation method in preserving the causal effect of data in CIL tasks.