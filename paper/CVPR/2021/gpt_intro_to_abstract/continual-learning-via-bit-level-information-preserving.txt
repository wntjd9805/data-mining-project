Continual learning is challenging in the field of artificial neural networks as it often results in severe performance degradation on previously learned tasks, known as catastrophic forgetting. Current methods to address this issue can be categorized into two types: model pruning-based methods and regularization-based methods. Model pruning-based methods identify important parameters for a task and store a task-specific mask to prevent them from changing in subsequent learning. On the other hand, regularization-based methods impose regularization terms to prevent model parameters from deviating away from previously learned ones. However, both types of methods face a trade-off between forgetting prevention and memory efficiency. In this paper, we propose a novel approach called Bit-Level Information Preserving (BLIP) to address the forgetting problem in continual learning. BLIP quantizes model parameters and preserves information gain on previous tasks through bit freezing. Our method does not rely on task-specific masks, resulting in constant memory overheads. Additionally, BLIP provides a stronger regularization technique compared to previous methods, effectively preventing forgetting. We validate the effectiveness of BLIP through extensive experiments in various settings. To the best of our knowledge, this work is the first to explore weight quantization for continual learning.