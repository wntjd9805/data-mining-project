This paper introduces the challenge of reasoning about causal and temporal relationships in video question answering (VideoQA) tasks. While neural network models have made significant progress in recognizing objects and describing independent actions in videos, understanding the causal and temporal relations between actions and answering related questions remains a challenge. To address this, the authors propose NExT-QA, a benchmark dataset containing videos with manually annotated question-answer pairs. The dataset includes causal, temporal, and descriptive questions, and offers two levels of difficulty: multi-choice QA and open-ended QA. Through evaluating various state-of-the-art video QA techniques on NExT-QA, the authors find that while these techniques perform well on descriptive questions, they struggle with causal and temporal questions. This suggests a lack of understanding of the underlying structure of actions. The authors' contributions include exploring causal and temporal action reasoning in VideoQA, providing the NExT-QA benchmark dataset, and conducting an extensive analysis of baselines and established video reasoning techniques. This paper aims to advance video understanding by promoting research on deeper explanations beyond shallow descriptions.