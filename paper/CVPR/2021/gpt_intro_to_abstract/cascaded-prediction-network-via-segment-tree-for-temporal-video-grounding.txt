In this paper, we introduce the task of temporal video grounding, which aims to automatically determine when an action or event corresponding to a given text query occurs in a video. We categorize previous approaches into proposal-based and proposal-free methods and identify limitations in both. Proposal-based methods rely heavily on hand-crafted pre-definitions and have fixed proposal boundaries, while proposal-free methods have a large decision space and lack supervision from segment boundaries. To address these issues, we propose a Cascaded Prediction Network (CPN) that performs multiple cascaded prediction subtasks in a coarse-to-fine manner. We utilize a segment-tree-based structure to store and represent segments of sequential data, enhancing computing speed and maintaining the framework. Our approach involves feature extraction, fusion representation, segment feature generation, and refinement through a message-passing graph neural network. We perform decision navigation and signal decomposition to fully leverage boundary annotations and response signals associated with the text query.Our main contributions include proposing CPN as a multi-step prediction solution, developing an effective representation learning method for discriminative segment features, and conducting extensive experiments on three public benchmarks to demonstrate the effectiveness of our approach. Our results show improved performance compared to existing methods, highlighting the potential of CPN for temporal video grounding.