This paper introduces a self-supervised learning framework for cross-modal representations in computer vision. The authors propose a contrastive learning approach, leveraging recent advances in contrastive learning. They introduce an Audio-Visual Instance Discrimination (AVID) task that learns a cross-modal similarity metric by grouping video and audio instances that co-occur. The authors demonstrate that the cross-modal discrimination task, predicting which audio matches a video, is more powerful than within-modal discrimination. They show that their technique improves action recognition benchmarks compared to prior self-supervised methods. The authors also propose improvements to the AVID task, including the use of Cross-Modal Agreement (CMA) to optimize for visual similarity and the grouping of semantically related videos. They demonstrate that optimizing visual similarity among related videos significantly improves learned visual representations. This approach outperforms AVID on action recognition tasks. The findings highlight the potential of self-supervised learning for cross-modal representation learning in computer vision.