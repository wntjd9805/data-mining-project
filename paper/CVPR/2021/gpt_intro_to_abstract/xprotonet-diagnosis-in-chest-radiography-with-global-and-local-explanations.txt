Chest radiography is a commonly used method for diagnosing heart and chest diseases. Automated diagnostic methods using deep neural networks (DNNs) have shown promising results in recent years. However, the black-box nature of DNNs raises concerns about their trustworthiness, particularly in medical applications where decisions can have life-or-death consequences. Many automated diagnostic methods provide localization as an explanation for predictions, but fail to explain the decision-making process of the network. Interpretable models, on the other hand, are designed to operate in a human-understandable manner and provide global and local explanations.In this paper, we propose XProtoNet, an interpretable automated diagnosis framework for chest radiography. XProtoNet predicts the occurrence area of a disease and learns disease-representative features within that adaptive area as prototypes. By comparing the features of an input image with the learned prototypes, XProtoNet can provide both global explanations, indicating discriminative features for a certain disease, as well as local explanations, rationalizing the classification of a specific image. We evaluate our method on the public NIH chest X-ray dataset and demonstrate its state-of-the-art diagnostic performance.The main contributions of this paper are as follows:1. We introduce the first interpretable model for chest radiography diagnosis that provides both global and local explanations.2. We propose a novel method for learning disease-representative features within a dynamic area, improving interpretability and diagnostic performance.3. We demonstrate the superior performance of our proposed framework compared to other state-of-the-art methods on the public NIH chest X-ray dataset.