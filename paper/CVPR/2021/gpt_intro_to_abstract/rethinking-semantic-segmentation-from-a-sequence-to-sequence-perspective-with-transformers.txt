Existing semantic segmentation models in computer vision have primarily relied on the fully convolutional network (FCN) architecture. This architecture consists of an encoder for feature representation learning and a decoder for pixel-level classification. The encoder, which is crucial for the model's performance, typically uses stacked convolution layers to progressively reduce the resolution of feature maps. While this design has proven effective for image understanding tasks, it has limitations in capturing long-range dependencies necessary for semantic segmentation in complex scenes.To address this limitation, recent approaches have explored alternative methods, such as using larger kernel sizes, atrous convolutions, image pyramids, or incorporating attention modules. However, these approaches still maintain the encoder-decoder FCN structure. Recently, attention-alone models without convolutions have also been proposed, but they still adhere to the encoder-decoder architecture.In this paper, we propose a novel semantic segmentation model called SEgmentation TRansformer (SETR) that departs from the conventional FCN design. Instead of using convolutional layers, our model employs a pure transformer, inspired by its success in natural language processing and image classification. In SETR, the input image is divided into fixed-sized patches, with each patch represented by a learned embedding. The transformer model is then applied to these patch embeddings, enabling global self-attention modeling for discriminative feature representation learning. Importantly, there is no downsampling in spatial resolution, and global context modeling occurs at every layer of the encoder transformer.We demonstrate the effectiveness of SETR through extensive experiments on benchmark datasets. Our models achieve state-of-the-art performance on ADE20K, Pascal Context, and competitive results on Cityscapes. Specifically, our model achieves the top ranking on the highly competitive ADE20K test server leaderboard.Overall, our contributions include proposing a new perspective on semantic segmentation model design, reformulating the problem as a sequence-to-sequence learning task, and introducing a pure transformer-based encoder architecture. Through empirical evaluation, we show that our SETR models outperform various FCN models with and without attention modules, highlighting the potential of transformer-based approaches for semantic segmentation tasks.