This paper presents a novel approach to unsupervised learning of point cloud registration using RGB-D data. Traditional methods rely on hand-crafted features and robust estimators, but their performance is limited by their inability to adapt to different data distributions. To address this limitation, recent work has leveraged supervised learning to learn feature descriptors and better correspondences. However, obtaining accurate pose annotations can be challenging. This paper proposes using view synthesis between RGB-D images as a task for learning point cloud registration. By enforcing photometric and geometric consistency losses on point cloud renderings of the scene, the model can estimate camera motion between two frames. The model is trained using differentiable components, allowing for end-to-end learning. The proposed approach is evaluated on the ScanNet dataset and outperforms traditional registration pipelines with visual or geometric descriptors. It also performs on par with supervised approaches, demonstrating the efficacy of RGB-D self-supervision. Several ablations are conducted to validate the design choices. Overall, the contributions of this work include the proposed unsupervised approach, the differentiable variant of Lowe's ratio test for correspondence matching, and the empirical demonstration of the approach's effectiveness against traditional and supervised methods.