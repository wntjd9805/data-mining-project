The field of computer vision has made significant advancements in face recognition using neural network classifiers. However, it has been found that these classifiers heavily rely on global image statistics, such as average brightness, for recognition tasks. In this study, we propose a method that removes the dependence on global image statistics, leading to improved classification results in modern neural networks. We achieve this by creating a deliberate mismatch between the activations of a layer and its accumulated statistics. By normalizing with unmatched statistics, the distribution of activation values becomes an unreliable source for label information. While previous work focused on changing global image statistics for image generation purposes, our approach demonstrates its usefulness in various discriminative settings. Moreover, we show that our method improves both category-based image recognition and texture recognition simultaneously, suggesting that invariant global image statistics enhance both shape and texture recognition. We validate the effectiveness of our method through several experiments. Firstly, we demonstrate improved classification performance on CIFAR100 and ImageNet datasets across multiple network architectures with the addition of our permutation-based regularization. Secondly, we train a linear classifier on top of a pre-trained image classification network, showing that texture classification accuracy peaks when image classification results are maximized without deteriorating the network's representation of shape. Additionally, our method effectively reduces the adverse effects of domain shift, achieving state-of-the-art results in domain adaptation and generalization tasks. Finally, we exhibit greater robustness in handling corrupted images compared to baseline methods, outperforming them on both ImageNet-C and CIFAR-100-C datasets for multiple architectures.