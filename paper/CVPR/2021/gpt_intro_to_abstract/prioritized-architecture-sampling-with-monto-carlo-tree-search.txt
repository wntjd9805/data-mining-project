Deep learning has achieved remarkable performance in tasks such as image recognition and object detection. In particular, it has shown promise in mobile edge devices. Neural architecture search (NAS) takes this a step further by developing more promising and cost-effective architectures without relying on expert knowledge. However, the search space of conventional NAS algorithms is extremely large, making exhaustive searching unfeasible. To overcome this, heuristic searching methods, such as reinforcement learning-based, evolution-based, Bayesian optimization-based, and gradient-based methods, are used. One-shot NAS methods, which train the entire search space simultaneously, have gained popularity due to their efficiency. However, existing one-shot NAS methods often overlook the dependencies between operation choices on different layers, leading to inaccurate evaluation of the neural architectures during the search. To address this, this paper establishes a Monte Carlo Tree (MCT) in the architecture search space and proposes an effective sampling strategy based on Monte Carlo Tree Search (MCTS) for NAS. A hierarchical node selection technique is introduced to accurately evaluate posterior nodes. A NAS benchmark called NAS-Bench-Macro is also proposed to enable better comparison of different NAS methods. Experimental results demonstrate the superiority of the proposed method in efficiently searching for optimal architectures. The method achieves significant improvements in search efficiency and performance compared to other one-shot NAS methods, reducing search costs and achieving high accuracy with limited computational resources.