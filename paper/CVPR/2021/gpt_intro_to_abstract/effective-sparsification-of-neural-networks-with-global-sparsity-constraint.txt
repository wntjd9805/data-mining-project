Weight pruning is a technique used to reduce redundancy in deep neural networks (DNNs), making inference more efficient and reducing computation demands. Typically, pruning algorithms identify and remove unimportant weights based on specific criteria. This allows for a reduction in network size and improved efficiency without significantly sacrificing performance. However, finding a suitable global comparator to measure weight importance and determine sparsity levels for different layers remains a challenge. Existing pruning methods often use a uniform threshold or pruning ratio for all layers, which can result in imperfect weight allocation and unsatisfactory results at high pruning rates.To address these limitations, this paper proposes a new method called probabilistic masking (ProbMask) for network sparsification. The key idea is to view weight pruning as a problem of finding a sparse binary mask and weights that minimize empirical loss. By reparameterizing the mask as independent Bernoulli random variables with probabilities, the discrete optimization problem is transformed into a continuous problem over the probability space. This allows for the control of model size through a global sparsity constraint. The Gumbel-Softmax trick is then used to solve the continuous problem and obtain a deterministic sparse mask. Importantly, the proposed method automatically identifies the amount of weight redundancy in each layer, eliminating the need for different pruning ratios for different layers.Experimental results on network pruning and supermask finding demonstrate the effectiveness of ProbMask compared to state-of-the-art methods. It outperforms existing methods on both small and large-scale datasets, achieving higher accuracy with significant pruning rates. The contributions of ProbMask include providing evidence for the use of probability as a global comparator for weight importance and sparsity level identification, presenting a natural formulation of the global sparsity constraint, and demonstrating its effectiveness across various models and datasets. Additionally, ProbMask is shown to be a powerful tool for identifying supermasks and achieving top-1 accuracy on CIFAR-100 under high pruning rates.