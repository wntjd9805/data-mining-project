Human motion transfer has gained significant attention in computer graphics and computer vision due to its potential applications in various fields such as virtual characters and movie making. Generative networks and image translation frameworks have enabled the generation of photo-realistic images for human motion transfer. In this process, the pose sequence of a target video is extracted, and the pose is used as input to generate videos of a new person mimicking the motion of the target video. However, there are challenges in accurately capturing the appearance of a new person with limited training data. This paper proposes a novel approach for few-shot human motion transfer that leverages a geometry generator and a texture generator. The appearance of the new person can be provided in two ways: either by training an individual model for a specific person using a large amount of training data, or by using a few images of the new person as input. The latter approach is more challenging as it requires the network to learn the complex relationship between human appearance and pose with limited information. Existing methods that directly condition the output image on the pose and appearance suffer from poor quality and artifacts. To overcome these limitations, the proposed method directly transfers pixels from the source pose to the target pose without generating new pixels. The DensePose framework is used to provide the UV map of a person, enabling the transfer of texture between different poses. However, the original DensePose model fails to generate realistic human images, especially in the few-shot scenario. Therefore, this paper introduces a new method that improves upon previous few-shot approaches. A geometry generator is used to generate a personalized UV map, while a texture generator merges incomplete texture maps and completes missing information. The texture map is then rendered to the UV map to generate high-quality human images.The contributions of this research include the proposal of a geometry generator and a texture generator that collaborate to produce high-quality human motion transfer. The method is trained on multiple videos of multiple persons and fine-tuned on a few examples of an unseen person, successfully transferring geometry and texture knowledge to new individuals. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively.