This paper addresses the challenge of depth completion for transparent objects using RGB-D cameras. Traditional depth sensors fail to accurately capture the depth of transparent objects due to the lack of light reflection. Previous works on transparent object geometry estimation are often studied in controlled settings and require additional information such as environment maps and refractive indices. ClearGrasp achieves impressive results but requires contact edges with non-transparent objects and is not suitable for real-time applications. In this paper, the authors propose a fast end-to-end framework called LIDF (Local Implicit Depth Function) for transparent object depth completion. LIDF leverages color and local depth information to infer the depth of transparent objects. The authors also introduce a depth refinement model to update the predictions iteratively. To train the framework, a large-scale synthetic dataset called Omiverse Object dataset is created. Experimental results show that the proposed approach outperforms existing methods in terms of accuracy and speed. The authors' contributions include the introduction of LIDF, a two-stage system for depth completion, the creation of a large-scale dataset, and the evaluation of the proposed framework.