This paper introduces a novel GAN inversion technique called Tedi-GAN that can map multi-modal information, such as texts, sketches, or labels, into a common latent space of a pretrained StyleGAN. The proposed method consists of three modules: StyleGAN inversion, visual-linguistic similarity, and instance-level optimization. The StyleGAN inversion module learns the inversion of real images into the W space, while the visual-linguistic similarity module ensures consistency between visual and linguistic representations by projecting them into the common W space. The instance-level optimization module preserves identity during editing and precisely manipulates desired attributes according to the input texts. The proposed method can generate diverse and high-quality images with resolutions up to 1024^2 and supports multi-modal inputs. Additionally, a new dataset called Multi-Modal CelebA-HQ is introduced to facilitate research in text-to-image synthesis for faces. This work contributes a unified framework for image generation and manipulation, a GAN inversion technique for multi-modal information, and a new dataset for face synthesis.