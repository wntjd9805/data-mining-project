With the rise of computer vision applications in fields like autonomous driving, robotics, and unmanned aerial vehicles, there is a growing need to capture high-speed motion scenes. Conventional cameras have limitations as they rely on a fixed exposure time window, which results in blurry artifacts for moving objects. To address this issue, a novel retina-inspired spike camera has been developed, which records visual information continuously and at a high temporal resolution. Various image reconstruction methods have been proposed for spike cameras, but they have not been able to handle challenges posed by noise and high-speed motion simultaneously, leading to unsatisfactory results. In this paper, we propose a deep convolutional neural network architecture called Spk2ImgNet for reconstructing dynamic scenes from spike camera data. Our hierarchical architecture exploits temporal correlation progressively, allowing the network to handle noise and high-speed motion effectively. We also formulate the mechanism of spike generation and develop a spike camera simulator to generate synthetic spike streams and corresponding ground-truth images for training. Experimental results on real and synthetic spike data demonstrate that our proposed network achieves state-of-the-art reconstruction performance for dynamic scenes. This is the first attempt to solve the spike camera image reconstruction problem using an end-to-end neural network.