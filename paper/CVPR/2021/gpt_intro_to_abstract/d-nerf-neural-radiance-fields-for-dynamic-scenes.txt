Rendering realistic views of a scene using a sparse set of input images is crucial in various applications such as augmented reality, virtual reality, 3D content production, games, and the movie industry. Traditional methods like Structure-from-Motion and image-based rendering have been surpassed by recent advances in neural rendering techniques that encode both geometry and appearance. One such technique, Neural Radiance Fields (NeRF), uses multilayer perceptron networks to map spatial locations and camera views to radiance values and volume density, enabling free-viewpoint rendering with high realism. However, existing approaches assume a static scene without moving objects. In this paper, we propose the first end-to-end neural rendering system for dynamic scenes consisting of still and moving/deforming objects. Our approach represents the scene as a continuous 6D function that considers the spatial location, time, and camera view. Naively extending NeRF to include the time component does not yield satisfactory results due to a lack of exploitation of temporal redundancy. Inspired by 3D scene flow, we propose Dynamic-NeRF (D-NeRF), which consists of two modules: one for learning the spatial mapping between each point in the scene and a canonical configuration, and another for regressing scene radiance and volume density given the augmented coordinates. Our method allows for the synthesis of novel images with control over camera view and time, and also produces complete 3D meshes capturing the time-varying geometry. We evaluate D-NeRF on scenes with different types of deformation and demonstrate its ability to render high-quality images while controlling camera view and time components. Additionally, D-NeRF enables the generation of 3D meshes from a single viewpoint, capturing the scene's dynamic state.