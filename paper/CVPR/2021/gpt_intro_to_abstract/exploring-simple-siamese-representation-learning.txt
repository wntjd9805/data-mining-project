Abstract:Recent advancements in un-/self-supervised representation learning have demonstrated promising results on various visual tasks. One common approach in these methods involves the use of Siamese networks, which are weight-sharing neural networks applied on multiple inputs. However, Siamese networks can suffer from the issue of outputs collapsing to a constant, resulting in undesired trivial solutions. To address this problem, previous strategies have focused on contrastive learning and clustering methods. In this paper, we introduce a simple Siamese network called SimSiam that maximizes the similarity between two views of the same image, without the use of negative pairs or a momentum encoder. Our method demonstrates competitive performance without collapsing solutions, highlighting the effectiveness of Siamese architectures for unsupervised representation learning. We provide empirical evidence for the importance of a stop-gradient operation, which suggests a different underlying optimization problem. Furthermore, we propose that Siamese networks can introduce useful inductive biases for modeling invariance, similar to the role of weight-sharing convolutions for translation-invariance. Our findings aim to inspire further exploration and re-evaluation of Siamese architectures in unsupervised representation learning.