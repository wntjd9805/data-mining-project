This paper introduces CLCC, a contrastive learning framework for the task of color constancy in computer vision. Color constancy is the ability of the human visual system to perceive the same color of an object under different illuminants. Computational color constancy aims to mimic this ability in camera pipelines for processing raw sensor signals to sRGB images. Traditional statistical methods often fail in complex scenes where their assumptions are violated. Deep learning methods have achieved significant improvements but suffer from the difficulty of collecting paired data for supervised training. Learning with insufficient data can lead to spurious correlations and biases that affect the accuracy of the model. To address this, the proposed CLCC framework aims to learn scene-invariant, illuminant-dependent representations through contrastive learning. By comparing similar and dissimilar samples, CLCC learns generalized and robust representations. However, conventional self-supervised contrastive learning generates trivial contrastive pairs, so data augmentation is crucial. Existing augmentations designed for high-level vision tasks may not be suitable for color constancy, so novel color augmentations based on color domain knowledge are proposed. CLCC improves the state-of-the-art deep color constancy model by generating diverse and harder contrastive pairs and learning illuminant-dependent features. The proposed method achieves state-of-the-art results without increasing model complexity and allows effective learning from small training datasets. Overall, CLCC contributes to the task of color constancy by providing a supervised contrastive learning framework that improves representation learning and enhances model robustness.