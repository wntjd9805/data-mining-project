This paper focuses on the task of human multimodal emotion recognition, which involves recognizing the sentiment attitude of humans from video clips using data from different modalities such as natural language, facial gestures, and acoustic behaviors. However, the collected multimodal streams are usually asynchronous, posing challenges for efficient multimodal fusion. Previous works have addressed this issue through manual word-level alignment, but this process is labor-intensive and requires domain knowledge. To overcome these limitations, the Multimodal Transformer (MulT) approach has been proposed, which uses a modality reinforcement unit to fuse crossmodal information from unaligned data sequences. However, the MulT approach only allows fusion between each directional modality pair and does not exchange information across all modalities involved. Furthermore, it fails to effectively utilize the high-level features of the source modality.This paper introduces the Progressive Modality Reinforcement (PMR) approach for multimodal fusion from unaligned multimodal sequences. The PMR approach utilizes a message hub to exchange information with each modality, enabling effective information flow and exploration of element-level dependencies across all three modalities. Additionally, a dynamic filter mechanism is introduced to determine the proportions of reinforced features. Compared to the MulT model, the PMR approach promotes three-way interactions and leverages high-level features for modality reinforcement, leading to improved performance. Experimental results on human multimodal emotion recognition benchmarks demonstrate the superiority of the PMR approach over existing state-of-the-art methods.