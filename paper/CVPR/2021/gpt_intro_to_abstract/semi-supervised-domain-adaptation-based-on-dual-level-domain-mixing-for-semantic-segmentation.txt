Semantic segmentation, or assigning semantic labels to pixels in an image, is a crucial task in computer vision with applications in autonomous driving and robotic navigation. Recent advancements in deep convolutional neural networks (CNNs) have significantly improved semantic segmentation performance. However, these models often struggle to generalize well to unseen test scenarios and require a large amount of manually labeled data. To address these challenges, this paper focuses on two approaches: the use of simulation data and unsupervised domain adaptation (UDA) strategies. Simulation data, obtained from game engines, can provide additional training samples without the need for pixel-wise annotation. UDA methods aim to transfer knowledge from a synthetic label-rich source domain to a label-scarce target domain to bridge the domain gap. Several UDA methods have been successful in extracting domain-invariant representations, but performance still lags behind supervised methods. Another approach to reducing the annotation demand is semi-supervised learning (SSL), which leverages limited labeled data and a large amount of unlabeled data. However, SSL models risk overfitting to the small labeled data. The recently introduced task of semi-supervised domain adaptation (SSDA) combines SSL with labeled source domain data and unlabeled target domain data. While naive approaches, such as equipping UDA methods with additional supervision or self-training, have been explored, they do not fully exploit the available labeled and unlabeled data in both domains. This paper proposes a framework that combines region-level and sample-level data mixing methods to reduce the domain gap. Additionally, the framework incorporates knowledge distillation from complementary domain-mixed teachers to improve the performance of the student model for the target domain. The proposed method achieves superior semantic segmentation results on common synthetic-to-real benchmarks with limited labeled data.