Event cameras have gained significant attention in the computer vision and robotics community due to their unique advantages, including high dynamic range and reduced motion blur. These cameras detect intensity changes asynchronously at each pixel and generate event streams encoding time, pixel location, and polarity. Deep neural network (DNN)-based methods using labeled image data have shown great success on various tasks. However, learning effective event-based DNNs has been challenging due to the lack of large-scale labeled event data. Existing works have relied on manually annotated datasets or pseudo labeling using active pixel sensor (APS) images, which suffer from low accuracy and domain gaps. Some approaches have explored unsupervised learning but have focused on pixel-level prediction tasks. Other research has reconstructed intensity images from events but still require labeled data and introduce extra latency during inference. In this paper, we propose a novel method called EvDistill to address these limitations and efficiently learn a student network on unpaired and unlabeled event data. Our approach leverages large labeled image data as the source modality and aims to transfer knowledge using cross-modal learning and knowledge distillation. Unlike existing cross-modal learning methods that rely on paired data or assume labels for both modalities, our approach utilizes a bidirectional modality reconstruction (BMR) module to bridge the gap between the image and event modalities. We simultaneously exploit both modalities to distill knowledge via crafted pairs, without introducing extra computation cost during inference. Additionally, we address distribution mismatch between the two modalities by leveraging structural similarities and adapting knowledge based on the BMR module. We propose a graph affinity knowledge distillation loss and other losses to improve the feature representation of the event data. We evaluate the performance of our framework on three datasets for semantic segmentation and one dataset for object recognition. The experimental results demonstrate that our approach outperforms previous methods for both end-tasks and the naive setting. We provide the validation code and trained models for reproducibility. Overall, our approach effectively learns a model on unpaired and unlabeled event data by distilling knowledge from a teacher network trained with labeled image data, showing promising results in event-based computer vision tasks.