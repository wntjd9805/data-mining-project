Facial attribute recognition (FAR) is a crucial task in computer vision, with applications in face verification, image generation, and image retrieval. However, FAR is challenging due to variations in facial appearance caused by factors such as pose, illumination, and occlusion. State-of-the-art deep learning-based FAR methods typically require a large amount of labeled data, which can be time-consuming and costly to obtain. This paper focuses on the problem of FAR with limited labeled data.To address this challenge, the paper proposes a novel Spatial-Semantic Patch Learning (SSPL) method. The SSPL method aims to learn the spatial-semantic relationship in facial images, which is crucial for accurately classifying attributes. The training of SSPL involves two stages. First, three auxiliary tasks, including Patch Rotation Task (PRT), Patch Segmentation Task (PST), and Patch Classification Task (PCT), are jointly learned to obtain a powerful pre-trained model. Second, the pre-trained model is fine-tuned on limited labeled data to perform FAR.The PRT task predicts the index of a rotated facial patch to exploit spatial information, while the PST task performs semantic segmentation to assign semantic labels to facial patches. The PCT task predicts facial component labels to capture image-level semantic information. By jointly training these tasks, SSPL effectively captures the spatial-semantic relationship and improves FAR performance with limited labeled data.The contributions of this work include the proposal of the SSPL method, which effectively utilizes spatial and semantic information from unlabeled data to obtain a powerful pre-trained model. Additionally, three auxiliary tasks (PRT, PST, and PCT) are designed specifically for FAR, enabling the extraction of semantic-aware feature representations. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods when labeled data is limited, highlighting the potential of learning the spatial-semantic relationship for FAR.