3D human pose estimation is a crucial task in computer vision with applications in action recognition, human-robot interaction, and human tracking. Although learning-based methods have achieved success in indoor scenarios, they struggle to generalize to cross-scenario datasets due to limited training data diversity. Recent works on data augmentation have proven effective in enhancing model generalization, but they often treat augmentation and model training as separate phases, leading to suboptimal results. In this paper, we propose PoseAug, a novel auto-augmentation framework for 3D human pose estimation. PoseAug jointly optimizes the augmentation process with network training in an online manner, leveraging the feedback from the training process to guide and adapt the augmentation. We introduce a differentiable augmentation module implemented by a neural network, which augments 2D-3D pose pairs in the training data by controlling joint angles, body size, and viewpoint. By integrating an error feedback strategy, the augmentor and pose estimator are trained together to enrich the input pose pairs and improve model performance. To ensure plausible augmented poses, we incorporate a pose discriminator module that enhances joint angle plausibility and guides body size, viewpoint, and position plausibility. Our experiments demonstrate the flexibility and effectiveness of the PoseAug framework, achieving significant improvements in model performance on both source and cross-scenario datasets compared to existing methods. Overall, our contributions include the investigation of differentiable data augmentation in 3D human pose estimation, the proposal of a differentiable pose augmentor along with an error feedback design, and the introduction of a part-aware 3D discriminator that enhances data plausibility and diversity.