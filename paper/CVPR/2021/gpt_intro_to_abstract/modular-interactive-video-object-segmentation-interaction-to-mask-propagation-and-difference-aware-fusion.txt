Video object segmentation (VOS) is a crucial task in video understanding and editing, aiming to accurately segment target objects in video sequences. Existing VOS methods can be categorized into two types: semi-supervised methods that require pixel-wise annotation of the first frame, and interactive VOS (iVOS) approaches that take user interactions as input to refine the segmentation results. This paper focuses on iVOS, which has applications in video editing as it allows users to provide simpler interactions such as scribbles or clicks. In this work, we introduce a decoupled modular framework for iVOS that addresses the challenges of joint interaction understanding and temporal propagation. Our framework leverages a novel difference-aware fusion module, which models the user's intent by capturing the difference between the mask before and after each interaction. This preserves the user's intent and improves the accuracy of iVOS while reducing the amount of user interaction needed. Additionally, we propose a lightweight top-k filtering scheme for memory read operation in mask generation and provide a large-scale synthetic VOS dataset to facilitate future research. Experimental results demonstrate the effectiveness and generalizability of our approach.