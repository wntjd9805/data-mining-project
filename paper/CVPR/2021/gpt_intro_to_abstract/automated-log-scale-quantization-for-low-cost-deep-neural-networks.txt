Quantization is crucial for energy-efficient hardware implementation of deep neural networks. While previous work has primarily focused on uniform quantization, non-uniform quantization schemes like logarithmic-scale quantization (log-scale quantization) offer potential advantages in hardware implementation. Log-scale quantization can replace multiplier hardware with a shifter or an adder, depending on whether it is applied to one or both operands. However, log-scale quantization can lead to disproportionately high errors at high-magnitude values, negatively impacting its accuracy compared to linear quantization. To address this issue, selective two-word logarithmic quantization schemes have been proposed. These schemes utilize the benefits of log-scale quantization while ensuring similar accuracy to linear quantization. Nevertheless, existing methods have limitations, such as the use of hyperparameters, suboptimal optimization goals, and lack of direct support for finding the best weight parameters for a given two-word quantization ratio. In this paper, we propose a novel differentiable training framework for selective two-word logarithmic quantization. Our method allows for direct optimization of weights based on the desired two-word quantization ratio. Surprisingly, our approach enables the separation of weight parameters requiring two-word quantization from the rest, allowing for different treatment using distinct loss functions. We also introduce per-tile quantization to determine the number of quantized words per weight tile. These optimizations result in significantly improved performance compared to the state-of-the-art training method for selective two-word logarithmic quantization. We demonstrate close-to-floating-point baseline performance for ResNet models using 3-bit selective two-word logarithmic quantization and achieve competitive results for challenging networks and tasks. Additionally, we compare our approach against state-of-the-art quantization techniques, with favorable results in terms of accuracy and hardware complexity. Overall, our proposed training method for selective two-word logarithmic quantization offers valuable contributions in terms of improved performance and flexibility.