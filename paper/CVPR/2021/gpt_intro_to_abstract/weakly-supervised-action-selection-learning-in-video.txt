Temporal action localization is a crucial task in computer vision, with applications in video understanding and modeling. The weakly supervised localization problem aims to solve this task using only video-level labels instead of frame-level annotations, reducing labeling efforts but increasing the problem's complexity. One common approach is multiple instance learning, where a classifier is trained on instances such as frames or short segments. However, existing methods may select instances that contain contextual information but not the actual action, leading to context error. Conversely, they may fail to activate on instances with actions due to occlusion or uncommon settings, resulting in actionness error. Previous work attempted to filter out background using attention models but risked removing important context. Our motivation is to design a learning framework that can use context information for class prediction while identifying action instances for localization. Inspired by leading object detection and temporal localization methods, we propose Action Selection Learning (ASL), where a class-agnostic actionness model predicts which frames will be selected in the top-k sets by the classifier. During inference, we combine predictions from the actionness model with the class activation sequence and demonstrate significant improvements in localization accuracy. ASL achieves new state-of-the-art results on popular benchmarks, THUMOS-14 and ActivityNet-1.2, outperforming leading baselines by 10.3% and 5.7% in mean Average Precision (mAP), respectively. We also conduct further analysis to highlight the advantages of the actionness approach.