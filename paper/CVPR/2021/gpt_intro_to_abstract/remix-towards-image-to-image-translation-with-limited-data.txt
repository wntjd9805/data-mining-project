Generative Adversarial Networks (GANs) have made significant progress in image-to-image translation tasks. However, these methods heavily rely on large amounts of training data, which can be expensive or impractical to collect. This paper addresses the issue of training GANs with limited data and the problem of model overfitting. The authors propose a data augmentation strategy called ReMix, which mixes source images in the feature space using convex combinations. This helps the generator learn to map mixed samples to the target space and prevents overfitting. The authors also introduce a novel content loss that maintains perceptual relations among samples, improving the discriminator's ability to distinguish augmented fake samples. The proposed method is evaluated on various tasks, including face translation and image synthesis, and achieves significant improvements even when trained on only 10% of the available data. The main contributions of this paper are the data augmentation strategy based on feature-level interpolation, the maintenance of perceptual relations to optimize interpolated translations, and the improvements in multiple image synthesis tasks with limited training data.