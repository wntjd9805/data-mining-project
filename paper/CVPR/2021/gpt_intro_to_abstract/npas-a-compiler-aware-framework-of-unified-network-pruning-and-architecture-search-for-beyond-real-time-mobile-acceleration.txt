The growing popularity of mobile AI applications and the demand for real-time Deep Neural Network (DNN) executions present challenges for DNN accelerations. DNN weight pruning has been proven effective in reducing storage and computation costs of DNN models. Previous work focuses on unstructured and structured pruning schemes, each with its limitations. Neural Architecture Search (NAS) aims to design more efficient DNN architectures using automatic searching algorithms. Recent work proposes fine-grained weight pruning applied to 3Ã—3 convolutional layers but has limited applicability. In this paper, we propose a general category of fine-grained structured pruning schemes applicable to various DNN layers. We develop a compiler-based code generation framework supporting these pruning schemes and other types of pruning for different layers. We demonstrate the advantages of our proposed pruning schemes in terms of accuracy and mobile acceleration. Additionally, we introduce a compiler-aware framework of joint network pruning and architecture search to maximize accuracy while satisfying latency constraints. To efficiently search the large optimization space, we propose a meta-modeling procedure based on reinforcement learning and Bayesian optimization. Our key contributions include bridging the gap between model compression and NAS, designing a systematic search acceleration strategy, and achieving superior mobile acceleration performance on an off-the-shelf mobile phone.Our NPAS framework achieves 6.7ms, 5.9ms, and 3.9ms ImageNet inference times with 78.2%, 75%, and 71% Top-1 accuracy, respectively, on a mobile phone device.