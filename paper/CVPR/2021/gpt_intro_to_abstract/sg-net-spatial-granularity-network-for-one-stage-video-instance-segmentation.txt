Video instance segmentation (VIS) is a challenging computer vision task that involves detecting, segmenting, and tracking instances in video frames. This task has valuable implications for applications such as augmented reality, video editing, and autonomous driving. Current dominant VIS frameworks follow a two-stage paradigm, which has limitations including difficulty in sharing features between sub-task heads, restricted resolution for instance masks, and redundant representations of candidate proposals. In this paper, we propose a new perspective for solving the VIS task by treating detection, segmentation, and tracking as interconnected problems. We introduce SG-Net, a compact one-stage method that achieves spatial granularity by dynamically dividing instances into sub-regions and performing segmentation on each region. Our method outperforms existing approaches in terms of segmentation accuracy and computational efficiency. We use only convolutional operations and leverage the state-of-the-art one-stage object detector, FCOS, allowing for effective feature sharing and joint optimization. Additionally, we introduce a tracking head that models the movement of object centerness, resulting in improved robustness to object variations.