The RectiÔ¨Åed Linear Unit (ReLU) is widely used as an activation function in neural networks and has been instrumental in state-of-the-art computer vision algorithms. The Swish activation, discovered through the Neural Architecture Search (NAS) technique, has achieved high accuracy on the ImageNet benchmark. However, the mechanism behind Swish is still poorly understood. In this paper, we propose a new activation function called ACON, which is an extension of Swish. ACON follows the framework of the ReLU-Swish conversion and approximates the Maxout family of activation functions. We show that ACON is smooth, differentiable, and has no computational overhead. By identifying fixed upper/lower bounds as the obstacle to improving accuracy, we introduce ACON with learnable upper/lower bounds. Additionally, we introduce meta-ACON, which optimizes the switching factor in ACON for fast learning and provides significant improvements on various tasks. ACON transfers well to different models and outperforms ReLU counterparts in terms of accuracy. Our contributions include the understanding of Swish as a smoothed ReLU, the connection between ReLU and Swish, and the introduction of meta-ACON for improved performance.