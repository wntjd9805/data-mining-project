This paper introduces the concept of adversarial examples in the context of two different threat models: the white-box setting, where the attacker has full knowledge of the target classifier, and the black-box setting, where the attacker has only query access to the unknown classifier. The black-box setting is considered more relevant for assessing the intrinsic robustness of classifiers in real-world applications. Black-box attacks are iterative procedures that refine adversarial examples based on input-output pairs. They can be score-based, where the attacker observes the top-k predicted probabilities, or decision-based, where the attacker only learns the top-k labels. The latter case, where the output is solely the top-1 label, is the most challenging as the attacker cannot rely on rich information for crafting adversarial examples. Black-box attacks typically use substitution to replace missing information. Recent score-based attacks resort to gradient estimation to compensate for the lack of back-propagation, while decision-based attacks often probe noisy versions of an image to derive a score-like function from the top-k labels. The paper argues for faster attacks with fewer queries and introduces SurFree, a fast black-box decision-based attack that does not use any substitution mechanism. It also presents a geometrical mechanism for maximizing distortion decrease under the assumption of a hyperplane boundary. Experimental results show that SurFree outperforms existing approaches in terms of distortion under a low query budget while remaining competitive with unlimited queries.