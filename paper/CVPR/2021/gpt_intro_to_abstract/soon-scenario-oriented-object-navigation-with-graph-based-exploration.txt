Recent research in embodied navigation tasks has made significant progress in enabling agents to reach a target by following various instructions. However, existing approaches still fall short of real-world navigation activities, as they rely on fixed trajectories and detailed step-by-step instructions. In real-world scenarios, people often provide complex instructions that only describe the target, without specifying a starting point, expecting the robot to explore and make autonomous decisions. To address these limitations, we propose a new task called Vision Situated Object Navigation (SOON), where an agent is instructed to find a thoroughly described target object in a house. The instructions in SOON guide the agent to find a unique object anywhere in the house, employing a coarse-to-fine navigation process that closely resembles real-world situations. Additionally, the SOON task is starting-independent, allowing the agent to freely explore the environment without being confined to a specific trajectory. We introduce the Graph-based Semantic Exploration (GBE) method to approach the SOON task, which models the navigation process as a graph and combines imitation learning and reinforcement learning to stabilize training. We present a large-scale benchmark, From Anywhere to Object (FAO), built on the Matterport3D simulator, comprising realistic housing environments and annotated instructions. Experimental analyses demonstrate the effectiveness of GBE and the quality of the FAO dataset. Our approach significantly outperforms previous navigation methods and we provide human performance to quantify the human-machine gap. Overall, this work contributes to advancing the field of embodied navigation and highlights the importance of enabling agents to navigate towards language-guided targets in real-world environments.