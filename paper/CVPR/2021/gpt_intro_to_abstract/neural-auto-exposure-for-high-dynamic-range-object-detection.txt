Computer vision systems often need to measure and analyze a wide range of luminances, ranging from complete darkness to bright sunlight. This luminance range can exceed 280 dB, far beyond the capabilities of conventional CMOS image sensors which can only capture around 60-70 dB in a single image. This limitation poses a challenge for vision tasks in uncontrolled scenarios and is particularly critical for applications such as outdoor robotics, drones, self-driving vehicles, and navigation.To overcome this limitation, existing vision pipelines rely on high dynamic range (HDR) sensors that capture multiple exposures of the same scene. However, this approach introduces motion artifacts and reduces sensor resolution and cost. In this paper, we propose an alternative approach using low dynamic range (LDR) sensors paired with learned exposure control.We introduce a neural auto-exposure network that predicts optimal exposure values for a downstream object detection task. This control network is trained in an end-to-end fashion with a differentiable image processing pipeline, mapping raw sensor measurements to RGB images used by the object detection model. We train this model using a synthetic LDR image formation model, simulating the image formation process of a low-dynamic range sensor from input HDR captures.We validate our method through simulation and experimentation with an automotive prototype. Our proposed neural auto-exposure control method outperforms conventional auto-exposure methods by 6.6 mAP points across diverse automotive scenarios. We make the following contributions: introducing a novel neural network architecture for real-time exposure prediction, proposing a synthetic training procedure based on a synthetic LDR image formation model, and demonstrating the superiority of our approach for automotive object detection.