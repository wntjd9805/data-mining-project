Generative Adversarial Networks (GANs) have made significant advancements in image synthesis, particularly in the generation of realistic face images. StyleGAN has emerged as a state-of-the-art method for generating high-resolution images with exceptional visual quality. However, controlling and editing the latent space of StyleGAN remains a challenge. Existing methods adopt an "invert first, edit later" approach, but inverting real images into StyleGAN's latent space does not achieve accurate reconstruction. To overcome this, recent works encode real images into an extended latent space, W+, consisting of the concatenation of multiple 512-dimensional vectors. However, the optimization process required for encoding in W+ is time-consuming. Some methods employ an encoder to infer an approximate vector in W+ as an initial point. In this paper, we propose a novel encoder architecture that directly encodes arbitrary images into W+, enabling accurate reconstruction without necessitating extensive optimization. Furthermore, we extend this framework to image-to-image translation tasks by combining the encoder with a pre-trained StyleGAN generator. This approach allows us to utilize StyleGAN for image translation even when the input and output images belong to different domains. Unlike previous approaches, our generic framework simplifies the training process and avoids the need for an adversary discriminator. By leveraging pre-trained StyleGAN's generator, our method offers advantages such as reduced locality bias and support for multi-modal synthesis. We refer to our method as pSp, which stands for pixel2style2pixel translation. The main contributions of this paper are the novel StyleGAN encoder for W+ latent domain encoding and the methodology for image-to-image translation using a pre-trained StyleGAN generator.