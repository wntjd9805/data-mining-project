Advances in hardware for training and running convolutional neural networks (CNNs) have allowed for the development of larger and more accurate models. In particular, it has become feasible to train a large number of small models and select the best one, in terms of both accuracy and speed. However, in the high-compute regime, where it may only be feasible to train a handful or even a single model, network scaling becomes essential. Existing work on model scaling focuses on accuracy, but this paper aims to explore scaling strategies that optimize both accuracy and model runtime.The paper introduces a framework for analyzing the complexity of different scaling strategies, considering not just the number of floating-point operations (FLOPs), but also the number of parameters and activations. It is shown that different scaling strategies scale activations at different asymptotic rates relative to FLOPs. Moreover, it is observed that on modern accelerators, the runtime of a scaled model is more strongly correlated with activations than FLOPs.Based on this analysis, the paper proposes a new family of scaling strategies, parameterized by a single parameter α, that control the relative scaling along model width and other dimensions. This allows for the careful control of the asymptotic rate at which model activations scale. The proposed scaling strategy, referred to as fast compound model scaling, yields models that are both fast and accurate.Experimental results demonstrate that fast scaling can achieve large models that are as accurate as state-of-the-art models but faster. For example, when applied to scale a RegNetY-4GF model to 16GF, it is found to be faster, more accurate, and use less memory compared to EfﬁcientNet-B4 with 4 fewer FLOPs.To facilitate further research, the paper will release the code and pretrained models introduced in this work.