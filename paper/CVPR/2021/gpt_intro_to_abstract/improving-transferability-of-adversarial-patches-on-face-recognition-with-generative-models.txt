Deep convolutional neural networks (CNNs) have significantly improved the performance of various computer vision tasks, including face recognition. However, these deep CNNs are vulnerable to adversarial examples, which are perturbed images designed to fool the models. Adversarial patches, which perturb only a small cluster of pixels, have been shown to be physically realizable and can deceive deep CNNs in real-world scenarios. In this paper, we focus on evaluating the robustness of face recognition models under the query-free black-box setting, where the attacker does not have access to the model parameters or the ability to make excessive queries.To address this challenge, we propose a method to improve the transferability of adversarial patches by optimizing them on a low-dimensional manifold represented by a generative model. The generative model, pre-trained on legitimate human face data, can generate diverse and unseen human face images by manipulating latent vectors. By optimizing the adversarial objective in this latent space, the resulting adversarial perturbations resemble human face features, thereby improving the transferability between the white-box substitute and the black-box target model. Our experiments demonstrate the effectiveness of this approach for black-box attacks on face recognition, both in digital and physical scenarios. We also extend this method to other computer vision tasks, such as image classification.