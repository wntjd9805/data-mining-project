In this paper, we address the photogrammetric problem of reconstructing 3D representations of an object or scene from 2D images taken from multiple viewpoints. This problem is well-studied in computer vision and has various applications, such as multi-view stereo, structure from motion, and simultaneous localization and mapping. The key requirement for these reconstructions is knowledge of the camera parameters, which are typically estimated using methods like structure from motion and simultaneous localization and mapping.Photogrammetry tools have been developed for both long-range and close-range applications. However, there has been comparatively less work done to push photogrammetry to mesoscopic and microscopic scales. Challenges at smaller scales include limited depths of field and increased impact of camera distortion. Existing approaches typically require careful camera distortion pre-calibration, expensive cameras, dedicated setups, or attachment of control points to the object.In this paper, we demonstrate that a smartphone can capture quantitative 3D mesoscopic images with high accuracy without camera distortion pre-calibration. We present a new photogrammetric reconstruction algorithm that stitches multi-perspective images, reconstructs a sample's 3D height profile, and estimates camera parameters in an end-to-end fashion. Our approach does not rely on feature point extraction and matching. We carefully model camera distortions, which is crucial for mesoscopic applications. Additionally, we reparameterize the camera-centric height maps using an untrained convolutional neural network, which reduces reconstruction artifacts and improves accuracy. Our method allows for high-accuracy height map estimation without camera precalibration or stabilization.