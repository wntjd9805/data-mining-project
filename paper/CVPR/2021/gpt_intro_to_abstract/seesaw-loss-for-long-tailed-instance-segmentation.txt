Deep learning-based object detection and instance segmentation approaches have achieved significant success on datasets with balanced category distribution. However, in the real world, the distribution of categories is long-tailed, with a few classes containing abundant instances and most other classes comprising relatively few instances. Existing instance segmentation frameworks struggle to perform accurately on long-tailed datasets, showing unsatisfactory performance on tail classes. This is primarily due to the overwhelming quantity of negative samples contributed by instances from head classes, resulting in heavily imbalanced gradients for tail classes in the learning process.To address this issue, we propose Seesaw Loss, a dynamic loss function that re-balances positive and negative gradients for each category using two complementary factors: a mitigation factor and a compensation factor. The mitigation factor reduces the penalties for relatively rare classes based on the ratios of cumulative training sample numbers, while the compensation factor increases the penalty to a specific category when a false positive sample is observed. This approach mitigates the overwhelming punishments on tail classes and compensates for the risk of misclassification caused by diminished penalties, leading to better overall performance.Seesaw Loss has three appealing properties: it is dynamic, self-calibrated, and distribution-agnostic. It dynamically adapts to the ratios of cumulative training sample numbers and instance-wise misclassification, unlike previous static group split or constant loss reweighting methods. It effectively relieves the overwhelming punishments on tail classes without increasing false positives, unlike previous strategies that blindly reduce penalties or decrease loss weights. It is also not reliant on pre-computed datasets' distribution and can operate well with any data sampler, gradually approximating the real data distribution during training.Extensive experiments demonstrate the consistent improvements achieved by Seesaw Loss in different instance segmentation frameworks and data samplers. On the challenging LVIS dataset, Seesaw Loss achieves significant AP improvements compared to Mask R-CNN and Cascade Mask R-CNN. It also significantly improves classification accuracy on the ImageNet-LT dataset in the long-tailed image classification task. Additionally, Seesaw Loss provides a simpler and more effective solution to long-tailed instance segmentation without relying on complex training pipelines.