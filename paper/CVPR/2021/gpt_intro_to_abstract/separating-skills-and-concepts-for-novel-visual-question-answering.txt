This paper introduces a new view of compositionality in Visual Question Answering (VQA) and proposes a novel evaluation setting to assess the ability of VQA models to generalize to novel compositions of skills and concepts. The study highlights the lack of built-in compositionality in existing state-of-the-art models and addresses the issue through a contrastive learning approach that separates skills and concepts within the internal representations of the model. The approach utilizes grounding as a proxy to identify and extract concepts from image-question pairs and learns to predict concepts based on relevant visual information. The advantage of this approach is that it learns grounding in a self-supervised manner using VQA data alone, eliminating the need for external annotations and reducing expenses. The method also does not rely on answer labels to learn skill-concept separation, allowing the use of unlabeled image-question pairs. The paper presents significant improvements in answering novel skill-concept compositions and generalizing to unlabeled image-question pairs with unseen concepts. The contributions of the paper include a novel view and evaluation setting for compositionality in VQA, a contrastive learning approach for skill-concept disentanglement, and improved performance over existing models.