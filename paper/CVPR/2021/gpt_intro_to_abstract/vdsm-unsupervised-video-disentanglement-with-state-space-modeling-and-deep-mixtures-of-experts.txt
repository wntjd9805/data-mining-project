Humans are capable of reasoning about object identity and motion independently, suggesting that these attributes are disentangled. This disentanglement is desirable in order to reason independently about the latent factors underlying identity and motion. However, achieving disentangled representations in machine learning has proven to be challenging without supervision or inductive bias. In this paper, we propose Video Disentanglement via State-Space-Modeling (VDSM), an unsupervised approach that leverages the generative structure of video sequences to achieve disentanglement. VDSM incorporates a novel structure designed to factorize appearance and motion, producing embeddings that achieve state-of-the-art classification performance and superior disentanglement compared to supervised approaches. Experimental results demonstrate the effectiveness of VDSM in terms of sequence generation, disentanglement, and image quality. The rest of the paper is organized as follows: related work is discussed in Section 2, the structure and training of VDSM are described in Section 3, experiments are presented in Section 4, and the paper concludes in Section 5.