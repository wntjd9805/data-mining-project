CLIPBERT: A Generic and Efficient Framework for End-to-End Video-and-Language LearningHumans communicate in a dynamic visual world using languages, signs, and gestures. The joint understanding of visual and textual cues is crucial for intelligent agents to interpret multimodal signals. Various real-life video-based tasks, such as text-to-video retrieval, video captioning, and video question answering, have been designed to evaluate this ability. The de facto paradigm for addressing these cross-modal tasks involves extracting dense video features from pre-trained vision models and text features from pre-trained language models and then fusing them in a shared embedding space.However, existing approaches following this paradigm suffer from two main drawbacks. First, there is a disconnection between the tasks/domains, as offline feature extractors are often trained on different tasks and domains than the target task. Second, there is a disconnection in multimodal features, as features from different modalities are learned independently of each other. End-to-end task-specific fine-tuning can mitigate these disconnections, but existing methods that extract features from the full sequence of video frames are memory and computationally intensive, making it challenging to directly apply them to video+language learning frameworks.Motivated by this, we propose CLIPBERT, a generic and efficient framework for end-to-end video-and-language learning. CLIPBERT sparsely samples single or a few short clips from full-length videos at each training step, as we hypothesize that these sparse clips capture sufficient visual and semantic information. During inference, predictions from multiple densely-sampled clips are aggregated to obtain the final video-level prediction, reducing computational demands significantly. Additionally, we leverage image-text pre-training using large-scale image datasets to initialize model weights. Empirical results demonstrate that the knowledge learned in image-text pre-training improves video-text tasks, achieving better or comparable performance to state-of-the-art methods in text-to-video retrieval and video question answering tasks.Our contributions in this work are three-fold. First, CLIPBERT achieves superior or comparable performance across diverse video-text tasks with varying video lengths. Second, the proposed sparse-training-then-dense-inference strategy with sparsely sampled clips outperforms traditional approaches that use densely extracted video features. Third, we show that image-text pre-training benefits video-text tasks and provide comprehensive ablation studies to identify the key factors contributing to the success of CLIPBERT.