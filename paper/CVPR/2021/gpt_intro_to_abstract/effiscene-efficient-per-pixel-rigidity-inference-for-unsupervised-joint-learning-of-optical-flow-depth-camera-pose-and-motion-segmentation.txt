Scene flow is a technique that describes the 3D motion of a dynamic scene by using 2D optical flow and scene depth. It is useful for applications such as self-driving and robotics navigation. However, obtaining ground truth data for scene flow is expensive and impractical. Unsupervised learning methods have been developed to overcome this problem by minimizing the photometric differences between original and synthesized pixel pairs. However, optimizing pixel-wise photometric error without supervision is challenging due to the ambiguity of pixel correspondence between consecutive frames, especially in unstructured or texture-less regions. To address this issue, additional constraints are needed to eliminate ambiguities for successful unsupervised scene flow estimation.Existing approaches use rigid constraints to separate the scene into static (rigid) and moving (non-rigid) areas. Current methods employ an auxiliary instance segmentation network to predict rigid pixels. However, the segmentation performance is often poor, leading to inaccurate estimation of rigid pixels and compromising the rigid constraint. This is because the independent rigidity inference in the existing pipeline limits the learning of pixel rigidity. To overcome this limitation, the authors propose a novel framework called EfﬁScene, which jointly considers optical flow, depth, and camera pose for rigidity learning. This new pipeline allows for more effective rigid constraint learning and more efficient scene flow framework optimization by eliminating the need for a deep instance segmentation network.EfﬁScene solves four unsupervised sub-tasks: optical flow estimation, stereo depth prediction, camera pose estimation for visual odometry, and motion segmentation. The authors propose a Rigidity From Motion (RfM) layer to estimate pixel rigidity by modeling the correlation between optical flow and rigid flow. The RfM layer includes three steps: correlation extraction, boundary learning, and outlier exclusion. The rigid map produced by the RfM layer can be interpreted as motion segmentation. In training, two new losses are introduced to optimize RfM and regularize flow boundary discontinuity. These losses improve the accuracy of scene flow estimation.Experimental results on KITTI benchmarks demonstrate that EfﬁScene outperforms existing state-of-the-art approaches in all four sub-tasks: optical flow, depth estimation, visual odometry, and motion segmentation. EfﬁScene achieves highly efficient rigidity inference with a significantly smaller model size compared to existing methods. The authors' proposed framework shows promise for improving unsupervised scene flow estimation by jointly considering optical flow, depth, and camera pose.