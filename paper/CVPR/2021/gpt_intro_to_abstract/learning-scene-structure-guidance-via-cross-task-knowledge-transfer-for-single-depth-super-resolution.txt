This paper introduces a novel scene structure guidance learning method for depth super-resolution (DSR), which aims to improve the quality of low-resolution depth maps. Existing DSR methods often rely on paired RGB-D data for training, which may be limited or expensive to obtain. In addition, processing high-resolution RGB data can be computationally expensive. Moreover, RGB discontinuities may not always align with depth map discontinuities, leading to artifacts in the DSR process. To address these limitations, this paper proposes a cross-modality knowledge distillation approach, where both RGB and depth modalities are used during training but only a single depth modality is available during testing. The approach includes an auxiliary depth estimation task and a cross-task interaction module to enable knowledge transfer between the DSR and depth estimation tasks. A structure prediction task is also introduced to regularize the learned structures. Experimental results demonstrate that the proposed method outperforms existing color-guided DSR approaches in terms of accuracy and runtime. The contributions of this paper include the introduction of a multi-modal training paradigm for DSR, a cross-task distillation scheme, and a structure prediction network for addressing structural inconsistency in the DSR process.