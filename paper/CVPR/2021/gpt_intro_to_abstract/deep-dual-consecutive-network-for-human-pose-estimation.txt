Human pose estimation, which involves locating anatomical key points or body parts, is a crucial problem in computer vision with applications in various domains. While earlier methods relied on probabilistic graphical models or pictorial structure models, recent approaches have leveraged deep convolutional neural networks (CNNs) to achieve remarkable performance. However, most state-of-the-art methods are designed for static images, resulting in diminished performance when handling video input. This paper focuses on the problem of multi-person pose estimation in video sequences and addresses the limitations of image-based approaches by considering temporal dependency and geometric consistency across video frames. The authors propose utilizing recurrent neural networks (RNNs) such as LSTMs, GRUs, or 3DCNNs to model these cues. A specific RNN-based approach is introduced, which utilizes convolutional LSTM to capture temporal and spatial cues and predict keypoint heatmap sequences for videos. The authors then propose a novel framework called DC-Pose, which incorporates bidirectional temporal cues across frames for efficient multi-person pose estimation in videos. The framework consists of three task-specific modules: a Pose Temporal Merger network for aggregating keypoints and localizing the search range, a Pose Residual Fusion network for efficiently computing pose residuals, and a Pose Correction network for updating the pose estimation with refined search scope and pose residual information. The authors compare their method to existing approaches and achieve state-of-the-art results on PoseTrack2017 and PoseTrack2018 Multi-frame Person Pose Estimation Challenge. The proposed method offers a significant contribution to the field of human pose estimation in videos and the source code is made publicly available for further research.