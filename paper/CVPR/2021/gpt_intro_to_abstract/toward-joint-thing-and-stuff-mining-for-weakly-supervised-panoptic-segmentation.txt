Panoptic segmentation aims to simultaneously segment all object instances and semantic content in an image, making it a critical task in computer vision. Recent progress in panoptic segmentation involves combining instance segmentation and semantic segmentation through a multi-branch scheme. However, these deep models require large amounts of training data with expensive instance-level and pixel-wise annotations, which hinders their application in real-world scenarios. Weakly supervised panoptic segmentation (WSPS) aims to address this limitation by using weak annotations for model training. Previous work in WSPS requires bounding boxes for thing categories and image-level tags for stuff, which still requires significant human effort for large-scale datasets and categories. In this paper, we focus on the extreme case of WSPS where only image-level labels are available. Existing approaches for weakly supervised instance segmentation and semantic segmentation independently, which fails to utilize rich contextual cues between things and stuff. To address this, we propose a Joint Thing-and-Stuff Mining (JTSM) framework for panoptic segmentation with only image-level labels. JTSM considers foreground things and background stuff as uniform object instances and models the correlations between objects and background at the instance level. We introduce novel techniques such as mask of interest pooling (MoIPool) to extract fixed-size pixel-accurate feature maps, panoptic mining to mine target categories in a unified manner, and self-training to refine segmentation masks with parallel instance and semantic segmentation branches. Experimental results demonstrate the effectiveness of JTSM compared to strong baselines on various datasets, and our approach also achieves competitive results for weakly supervised object detection and instance segmentation tasks. Overall, this work contributes to the development of a unified framework for weakly supervised panoptic segmentation and introduces novel techniques to improve segmentation accuracy and efficiency.