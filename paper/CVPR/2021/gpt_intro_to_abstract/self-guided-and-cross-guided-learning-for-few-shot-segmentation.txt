Semantic segmentation has advanced with the use of deep neural networks, particularly the Fully Convolutional Network (FCN). However, current state-of-the-art approaches heavily rely on large amounts of annotated data, making them less effective for unseen classes or with limited annotated data. Few-shot segmentation is a promising method that aims to segment new classes using only a few annotated samples. Most existing approaches encode both support and query images using a Siamese Convolutional Neural Network (SCNN), extracting features as a prototype for segmentation. However, this method does not carry sufficient information and can lead to inaccurate segmentation. In this paper, we propose a Self-Guided and Cross-Guided Learning approach (SCL) to address these limitations. The SCL consists of a Self-Guided Module (SGM) that extracts comprehensive support information and a Cross-Guided Module (CGM) that evaluates prediction quality from each support image for multiple-shot segmentation. Our approach achieves new state-of-the-art performances on the PASCAL-5i and COCO-20i datasets. The contributions of this paper include the observation of lost information in existing methods, the introduction of a self-guided mechanism to capture comprehensive support information, and the proposal of a cross-guided module to improve multiple-shot segmentation performance. Our approach can be directly applied to different baselines, demonstrating its versatility and effectiveness in improving performance for few-shot segmentation tasks.