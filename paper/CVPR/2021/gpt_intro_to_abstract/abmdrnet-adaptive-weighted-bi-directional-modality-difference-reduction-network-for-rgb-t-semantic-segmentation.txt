Semantic segmentation is a crucial task in computer vision, as it assigns category labels to each pixel in an image. This is applicable in various domains such as autonomous driving, pedestrian detection, and pathological analysis. While CNN-based RGB semantic segmentation methods have achieved impressive results in large-scale datasets, they often struggle under poor lighting conditions. To address this issue, recent research has focused on RGB-T semantic segmentation, utilizing thermal images to complement RGB images in low light. However, existing models for multi-modality pixel-level prediction tasks do not adequately capture the modality differences between RGB and thermal images, leading to suboptimal performance. In this paper, we propose a novel subnetwork called Modality Difference Reduction and Fusion (MDRF) to better exploit the complementary information from RGB and thermal images. The MDRF subnetwork employs a bridging-then-fusing strategy, where a bi-directional image-to-image translation method is used to reduce modality differences. Furthermore, we introduce a Channel Weighted Fusion (CWF) module to capture cross-modality information between corresponding channels of RGB and thermal features. To address the problem of diversity in objects within an image, we propose two additional novel modules: Multi-Scale Spatial Context (MSC) and Multi-Scale Channel Context (MCC). MSC and MCC modules exploit multi-scale contextual information and long-range dependencies in cross-modality features along spatial and channel dimensions, respectively. Experimental results demonstrate that our proposed method achieves state-of-the-art performance in RGB-T semantic segmentation. Overall, this paper contributes an end-to-end approach for RGB-T semantic segmentation that considers both multi-modality difference reduction and multi-scale contextual information, as well as novel modules to exploit cross-modality information and improve segmentation accuracy.