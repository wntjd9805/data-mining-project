Video salient object detection (VSOD) models are designed to segment salient objects in both the spatial and temporal domains. Current VSOD methods employ two main approaches: one encodes temporal information using a recurrent network, such as LSTM, while the other encodes geometric information using the optical flow constraint. However, the pixel-wise labeling required for VSOD makes it a more expensive task than RGB image-based saliency detection. To tackle this issue, existing methods combine RGB image saliency datasets with VSOD training datasets, but this approach still requires costly pixel-wise labeling. In this paper, we propose a weakly supervised video saliency detection network that learns from scribble annotations. We introduce fixation guided scribble annotation, which incorporates temporal information from existing VSOD training datasets. We design an appearance-motion fusion module and a temporal information enhanced module to effectively fuse appearance and motion features. We also introduce a foreground-background similarity loss and a weak annotation boosting strategy to fully exploit the weak annotations. Our approach achieves comparable results to state-of-the-art fully supervised methods and offers a more cost-effective configuration for video salient object detection.