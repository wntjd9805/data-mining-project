Holistic scene understanding in computer vision involves training a model to explain every pixel in an image, distinguishing between "stuff" (uncountable regions of similar texture) and "things" (countable objects). While early research focused on holistic scene understanding, modern deep learning approaches typically handle semantic segmentation (stuff) and instance segmentation (things) as separate tasks. However, recent work has introduced the concept of panoptic segmentation, which combines both tasks into a unified framework.In this paper, we propose a novel approach to panoptic segmentation by learning hierarchical pixel embeddings. By creating a unified embedding for the task, we leverage the hierarchical relationship between instances and semantics. Our method involves training a fully convolutional network to predict hierarchical embeddings, instance seeds, and variances for each pixel. We then use these embeddings for panoptic decoding and segmentation.Our main contribution is a representation learning approach that treats panoptic segmentation as a unified task. We utilize the Lovász hinge loss to learn a structured Hierarchical Lovász Embedding space, where object categories and stuff classes can be represented jointly. Compared to conventional panoptic segmentation models, our approach provides temporal stability between video frames, enabling smooth predictions for downstream applications like object tracking and prediction in autonomous driving. Experimental results on benchmark datasets like Cityscapes, COCO, and Vistas demonstrate that our method achieves state-of-the-art results and outperforms two-stage models.Overall, our work provides a novel framework for panoptic segmentation that combines instance and semantic segmentation into a unified task. By leveraging hierarchical embeddings, we generate accurate and temporally stable predictions, making significant advancements in visual understanding for applications such as autonomous driving and robotics.