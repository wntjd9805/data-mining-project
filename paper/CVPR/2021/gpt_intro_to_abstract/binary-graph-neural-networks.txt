Standard Convolutional Neural Networks (CNNs) are designed for data with a regular grid structure, such as images or videos. However, there is a growing need to analyze data that does not fit this framework. Graph theory provides a mathematical framework for modeling interactions and is commonly used in fields like network sciences, bioinformatics, and recommender systems. To address this, Graph Neural Networks (GNNs) have emerged as models that generalize the operations of CNNs to arbitrary topologies. As the complexity and scale of graph datasets increase, the need for faster and smaller models becomes crucial. Resource-efficient deep learning is important for applications on embedded devices with energy and storage constraints, as well as for large-scale data mining. While recent work has focused on scalability improvements for GNNs, this paper focuses on compressing existing architectures while maintaining model performance. The paper proposes a binarization strategy inspired by binary neural networks and knowledge distillation techniques. Additionally, an efficient dynamic graph neural network model is developed, which constructs the dynamic graph in Hamming space, leading to significant speed-ups at inference time. The paper includes a thorough study of hyperparameters and techniques used in the approach, and also demonstrates real-world acceleration on a budget ARM device.