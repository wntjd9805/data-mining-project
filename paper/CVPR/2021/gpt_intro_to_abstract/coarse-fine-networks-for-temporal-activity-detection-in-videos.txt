This paper introduces a new approach for learning video representations that dynamically selects temporal frames within the model. The proposed method addresses the challenge of capturing long-term motion in videos by allowing the model to selectively process informative frames. A two-stream architecture, called Coarse-Fine Networks, is used to process information at different temporal resolutions. The Coarse stream learns to downsample the temporal resolution by selecting the most informative frame locations, while the Fine stream processes the input at the original resolution to provide fine-grained context. The information from both streams is fused using a multi-stage fusion mechanism. To achieve dynamic frame selection, a learnable temporal downsampling operation called Grid Pool is introduced, which interpolates the representations based on a non-uniform grid with learnable grid locations. Experimental results show that the proposed approach achieves superior performance in video representation learning tasks compared to previous methods, while reducing computational complexity.