With the development of 3D sensors, semantic segmentation of 3D point clouds has become a popular topic in computer vision. However, segmenting large-scale point clouds is challenging due to their unstructured nature. Deep Neural Network (DNN)-based methods have been proposed for 3D point cloud segmentation, but they are computationally expensive or limited to small point clouds. This paper addresses the problem of learning effective features from large-scale point clouds for semantic segmentation. Inspired by the success of contextual information in visual tasks, the authors propose a learnable module called Spatial Contextual Feature (SCF) that consists of the local polar representation block, the dual-distance attentive pooling block, and the global contextual feature block. The SCF module can be utilized by various network architectures, and a new 3D semantic segmentation network called SCF-Net is presented. The main contributions of this work are the proposed Local Polar Representation (LPR) block, Dual-Distance Attentive Pooling (DDAP) block, Global Contextual Feature (GCF) block, and the SCF module itself. Experimental results demonstrate that SCF-Net achieves state-of-the-art performance in 3D point cloud segmentation.