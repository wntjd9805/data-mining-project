Image-to-image translation is a fundamental task in computer vision that involves mapping an input image from one domain to another. Generative adversarial networks (GANs) have been successful in creating realistic images and have been widely utilized in image translation studies such as facial attribute editing. However, obtaining paired datasets for all the desired attributes is impractical, leading to the development of unpaired image translation approaches. Existing models face challenges in preserving attribute-irrelevant regions while changing the desired attributes of an image. Efforts have been made to address this through the use of additional modules and explicit loss functions. However, these approaches are limited in their applicability and lack pixel-level preservation. To overcome these limitations, we propose a novel loss function called the CAM-consistency loss for image-to-image translation. Our proposed loss leverages Grad-CAM in adversarial training and can be applied to existing image translation approaches without modifications to their architectures. This loss effectively preserves attribute-irrelevant regions at a pixel level while ensuring the discriminator attends to attribute-relevant information at a feature level. The CAM-consistency loss overcomes limitations of existing methods and demonstrates the possibility of using Grad-CAM as a training objective. Overall, our contributions include the introduction of the CAM-consistency loss, which improves attribute preservation without requiring additional information or architectural modifications, and the demonstration of the direct use of Grad-CAM in training.