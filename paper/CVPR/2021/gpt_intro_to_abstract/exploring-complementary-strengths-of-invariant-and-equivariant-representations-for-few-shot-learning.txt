In recent years, deep learning methods have achieved significant success on challenging problems, largely due to the availability of large-scale labeled datasets. However, acquiring such datasets is often infeasible in real-world scenarios. Few-shot learning (FSL) addresses this issue by training a model on a set of base classes and examining its adaptability to novel classes with only a few samples. FSL has traditionally been solved using meta-learning approaches, but recent studies have shown that improving the base feature extractor could further enhance FSL performance. In this paper, we propose a novel feature learning approach that combines the strengths of both equivariant and invariant features through a multi-task objective. We argue that learning generic features with both equivariance and invariance improves the generalization capabilities of the base feature extractor. We validate our approach through extensive experimentation on multiple benchmark datasets, demonstrating its effectiveness in improving FSL performance. Our contributions include the enforcement of complimentary equivariance and invariance to model the data structure, the use of self-supervised tasks as auxiliary supervision, and the gains achieved through cross-task knowledge distillation.