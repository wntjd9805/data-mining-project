Self-supervised representation learning from unlabeled videos has gained significant attention in recent years. Videos provide rich sources of "supervision" compared to static images, due to their redundancy, temporal consistency, and multi-modality. Various methods have been proposed in this field, including video-specific pretext tasks, contrastive learning, cross-modal learning, and contrastive clustering. This paper focuses on visual-only video representation learning, particularly context and motion representation, which are orthogonal but complementary aspects of video representation. It is observed that context alone can be used to classify certain actions, but for actions that heavily depend on movement patterns, motion information is necessary. To address this, the authors propose a multi-task framework that decouples context and motion representation learning in pretext tasks. The challenge lies in the source of supervision, as computationally expensive features like optical flow and dense trajectories are to be avoided. The authors exploit the separation of context and motion information in compressed video formats, where I-frames represent static context information and motion vectors depict dynamic movements. Inspired by this, they propose a self-supervised video representation learning method with two decoupled pretext tasks: context matching and motion prediction. The context matching task compares global features of video clips and I-frames, while the motion prediction task requires the model to predict pointwise motion dynamics based on visual information. Extensive experiments on various architectures, datasets, and downstream tasks show the effectiveness of the proposed method, achieving state-of-the-art performance. The contributions of this paper include the decoupling of context and motion supervision, the use of compressed video modalities as supervision sources, and the significant improvements achieved in action recognition and video retrieval tasks.