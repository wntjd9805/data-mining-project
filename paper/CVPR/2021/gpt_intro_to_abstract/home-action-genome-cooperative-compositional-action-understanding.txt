Action understanding in videos is a challenging task in computer science, with various real-world applications such as robotics, human-computer interaction, healthcare, and elderly behavior monitoring. While deep learning methods have shown success in image classification, complex and holistic action understanding remains a difficult problem to solve.There are several challenges associated with action understanding. The variability in executing complex activities poses a critical difficulty in building action understanding models. Actions are composed of various parts spanned in space and time, making it different from simple object detection tasks. The hierarchical structure of actions, with multiple entities and atomic actions, further adds to the complexity. Capturing the variability in executing complex activities and understanding each part becomes crucial for a holistic understanding of actions in the 3D world.Previous research has separately investigated these challenges using different datasets and advanced methods. However, there is still a need for a benchmark that unifies all these challenges and tasks. In this paper, a new benchmark called Home Action Genome (HOMAGE) is introduced for hierarchical action recognition. The dataset includes multi-modal synchronized videos from multiple viewpoints, along with hierarchical action and atomic-action labels. This benchmark aims to tackle the complexities of actions in home environments, including long-term actions, object interactions, and occlusions.To tackle these challenges, a novel method called CCAU (Cooperative Co-training with All Views) is proposed. It involves simultaneous co-training with multiple modalities (RGB, audio, and scene composition annotations) and viewpoints to learn rich video representations. Through experiments, it is observed that training on the HOMAGE dataset improves action recognition performance even when only a single modality is used during inference. This suggests that CCAU effectively leverages information from all views and modalities.The contributions of this paper include the introduction of the HOMAGE dataset, which enables research in multi-modal and compositional perception for home actions. The dataset provides annotations of scene graphs and hierarchical activity labels. Additionally, a novel learning framework called CCAU is proposed, which leverages multiple modalities and hierarchical action labels to improve baseline performance. Experimental results show the benefits of the approach, with a significant improvement using only ego-view during inference.In conclusion, this paper addresses the challenges of action understanding in videos by introducing the HOMAGE dataset and proposing the CCAU learning framework. The research contributes to the advancement of multi-modal and compositional perception for home actions, and encourages further research in privacy-aware recognition and sensor-fusion.