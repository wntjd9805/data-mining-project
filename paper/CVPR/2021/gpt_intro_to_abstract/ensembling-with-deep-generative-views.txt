Image datasets are crucial for learning-based vision problems, but they only capture discrete snapshots of the continuous world. Generative adversarial networks (GANs) have shown promise in imitating the real-image manifold, mapping random samples from a latent distribution to realistic image outputs. GANs have a locally smooth latent space, meaning that nearby samples in this space will appear similar in image space. This "interpolating mechanism" enables GANs to blend and recombine images in a continuous manner, potentially generating unlimited variants of a given image.In this paper, we explore the use of GAN outputs as test-time augmentation for classification tasks. In the standard classification pipeline, passing an image through a trained classifier generates predictions for the image's class. However, using multiple views of an image can improve classification performance. Traditionally, additional views are generated by cropping the image at different locations. However, we propose a data-driven approach using GANs to generate alternative views by altering the image's pose, shape, or color based on the learned variations in the GAN.Using unconditional GANs has the added advantage of not requiring image labels during training, allowing them to learn from larger datasets compared to tasks involving manual annotation. Training on large datasets enables the generator to learn meaningful variations in the data without explicit training objectives.One challenge in using GANs for generating augmented samples is the potential domain gap between real images and GAN outputs. The quality of generated samples must be sufficient for use in classification tasks while preserving visual patterns relevant for accurate discrimination. To address this challenge, we propose a hybrid encoder and optimization approach that reconstructs image samples using the GAN. Additionally, we ensure that generated variations of an image do not cross classifier boundaries, meaning they should not modify visual appearance that affects its classification.We evaluate our method using the StyleGAN2 generator on various classification tasks, including facial attributes, cat faces, and cars. We find that test-time ensembling with GAN samples improves classification performance, even when the classifier is trained only on real images. However, training the classifier on generated samples offers further improvements, especially for more challenging domains like cars and cats.The code for our method is available on our website, enabling reproducibility and further exploration.