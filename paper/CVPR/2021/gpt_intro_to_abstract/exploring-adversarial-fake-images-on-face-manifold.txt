Abstract:With the rapid advancements in image generation techniques, it has become increasingly difficult for human eyes to differentiate between real and fake images. This poses significant concerns in terms of privacy and security, as manipulated or generated fake images can be used maliciously. Image forensic models have been developed to identify and distinguish real images from fake ones. While these models have demonstrated considerable performance in various benchmarks and datasets, they are susceptible to more sophisticated attacks. In order to combat the generation and spread of undetectable "deep fakes," it is crucial for image forensics researchers to develop anti-forensic techniques that enhance the robustness and generalization ability of existing forensic models.In this paper, we propose an efficient method for generating high-quality fake images that can deceive forensic detectors. By leveraging the manifold of the powerful generative model Style-GAN, we can generate adversarial fake face images that fool forensic models. Although the images generated by Style-GAN can be easily detected by models based on Xception or EfficientNet, we successfully identify and generate adversarial vectors in the latent space that produce fake images undetectable by forensic models.Our method offers several advantages over existing adversarial attack methods. Firstly, by modifying the manifold, we can achieve higher updating strength without being restricted by image pixel constraints. Secondly, our method generates images with no obvious artifacts, thus making it harder for human eyes or specially designed detectors to detect the fakes.Our contributions in this study are fourfold: 1. We propose a novel method for generating adversarial anti-forensic images by exploring the manifold of Style-GAN. These images successfully bypass two commonly used forensic models, Xception and EfficientNet, highlighting the need for more robust forensic models.2. We compare our method with norm-based adversarial attacks and demonstrate that our method achieves the same attack success rate while introducing less visible perturbation, making it more challenging for our adversarial images to be detected.3. We conduct our attack in different ensemble ways and show that our adversarial images can transfer between different forensic models, posing a threat even in situations where the architecture of the forensic models is unknown to the attacker.4. We analyze the effects of adversarial strength, input noise vector level, and attributes of the generated images, providing valuable insights for future investigations.Overall, our proposed method presents a promising approach for generating adversarial anti-forensic images that can effectively deceive forensic models while maintaining high visual quality.