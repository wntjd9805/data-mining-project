Recent advancements in convolutional neural networks (CNNs) and the availability of large-scale video benchmark datasets have led to the dominance of deep learning approaches in video action recognition. These approaches utilize 2D-CNNs, 3D-CNNs, or a combination of both to achieve state-of-the-art performance on benchmark datasets. However, there are several fundamental questions that remain unanswered in the field, such as understanding the factors contributing to improved temporal representations and the differences between 2D-CNN and 3D-CNN approaches in terms of accuracy and spatial-temporal modeling. The lack of fairness in performance evaluation related to datasets, backbones, and experimental practices further complicates understanding the progress in action recognition. This paper aims to provide a common ground for comparative analysis of 2D-CNN and 3D-CNN models by conducting comprehensive experiments and analysis on three benchmark datasets. The main contributions of this work include a unified framework for action recognition, spatio-temporal analysis of 2D-CNN and 3D-CNN models, and benchmarking of state-of-the-art approaches. The findings shed light on the temporal modeling, differences between CNN models, and the impact of input sampling strategy on model performance. This work aims to promote a better understanding of CNN-based action recognition models and stimulate discussions on fair comparisons in the field.