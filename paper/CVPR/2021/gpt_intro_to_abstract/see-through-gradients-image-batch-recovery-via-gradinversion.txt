Collaborative, distributed, and federated learning of deep networks involve sharing weight updates or gradients during training. This approach allows for the advantages of keeping user data private and avoiding the need to store, transfer, and manage large datasets. However, recent research has raised concerns about the potential for gradients to leak private information from the training data. This paper introduces GradInversion, a method for recovering hidden training image batches by inverting averaged gradients. The proposed method formulates an optimization process that transforms noise into input images, starting with label restoration from the gradient of the fully connected layer. The optimization process includes fidelity regularization and registration-based group consistency regularization to improve the reconstruction quality. The paper demonstrates the feasibility of recovering high-fidelity ImageNet samples from batch gradients using the proposed method. This paper also discusses the challenges of gradient inversion and compares the proposed method to existing techniques. The contributions of this paper include the introduction of GradInversion, the development of a label restoration method, the proposal of a group consistency regularization term, and the demonstration of feasible image recovery from batch-averaged gradients. The paper also introduces a new metric, Image Identifiability Precision, to measure the ease of inversion for different batch sizes and identify samples vulnerable to inversion.