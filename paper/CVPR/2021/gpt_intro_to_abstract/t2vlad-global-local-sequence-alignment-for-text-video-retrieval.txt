Abstract:Video is a highly informative medium with rich content and temporal dynamics. Text-video retrieval systems have been developed to enable users to search videos using text queries. While previous research has focused on retrieving content based on simple text inputs, there is a need for more complex search capabilities that allow users to provide detailed descriptions. This paper proposes an efficient global-local sequence alignment method for text-video retrieval. The approach combines global alignment, which measures similarities between text and video at a high level, with local alignment, which focuses on fine-grained semantic alignment. The proposed method utilizes learnable semantic topics to summarize text and video data, reducing the semantic gap between the two modalities. The method achieves local alignment by minimizing the distance between grouped text features and corresponding grouped video features within the same topics, and global alignment by computing similarity between aggregated video features and global text features. The proposed method outperforms existing approaches on various datasets, demonstrating the effectiveness of the local semantic alignment strategy. The contributions of this paper include the automatic learning of text-and-video semantic topics, the introduction of an effective strategy for local alignment, and significant improvements in text-video retrieval performance.