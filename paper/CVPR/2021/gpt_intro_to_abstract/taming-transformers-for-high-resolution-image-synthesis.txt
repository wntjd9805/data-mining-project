Transformers have become the standard architecture for language tasks and are being increasingly used in other areas such as audio and vision. Unlike convolutional neural networks (CNNs), transformers do not have a built-in prior on the locality of interactions, allowing them to learn complex relationships among inputs. However, this generality also means that transformers have to learn all relationships, while CNNs are designed to exploit local correlations within images. As a result, transformers have higher computational costs, making them challenging to scale for high-resolution images with millions of pixels.Recent observations suggest that transformers tend to learn convolutional structures, raising the question of whether we can efficiently encode prior knowledge about the local structure of images while retaining the flexibility of transformers. We hypothesize that low-level image structure can be effectively described by a convolutional architecture, while this assumption becomes less effective at higher semantic levels. Furthermore, CNNs exhibit a bias towards spatial invariance, which limits their effectiveness for holistic understanding of input.Our approach combines convolutional and transformer architectures to model the compositional nature of the visual world. We use a convolutional approach to learn a codebook of context-rich visual parts and then model their global compositions using a transformer architecture to capture long-range interactions. We also employ an adversarial approach to ensure that the codebook captures important local structures, reducing the need for the transformer architecture to model low-level statistics. This allows transformers to focus on their strength of modeling long-range relations, enabling the generation of high-resolution images. Our formulation provides control over the generated images through conditioning information on object classes and spatial layouts. Experimental results demonstrate that our approach outperforms previous codebook-based approaches based on convolutional architectures, while retaining the advantages of transformers.In conclusion, our hybrid approach combining convolutional and transformer architectures effectively models the compositional nature of the visual world and generates high-resolution images while incorporating prior knowledge about local structures. This approach represents an advancement over previous techniques and demonstrates the advantages of transformers in vision tasks.