Sketch-based 3D modeling has gained attention in recent years as a means to simplify and speed up the modeling process. The rise of portable touch screens and VR/AR technologies has increased the demand for 3D content creation by novice users. However, existing sketch-based 3D modeling techniques have limitations in terms of ease of use and customization. This paper focuses on the problem of generating a 3D mesh from a single free-hand sketch, which poses unique challenges due to the characteristics of such sketches. Free-hand sketches can be poorly drawn, lack important visual cues, and exhibit geometric distortions, making it difficult for algorithms to accurately understand them.One challenge stems from the "camera-shape ambiguity" in which the sketched object's orientation is unclear. Subjectivity in sketch creation can lead to different interpretations, further complicating the understanding of sketches. Existing single-view reconstruction approaches mainly rely on image recognition rather than geometry reconstruction, thus struggling to generalize to unseen data and perform well on hand-drawn sketches.To address these challenges, this paper proposes a view-aware sketch-based 3D reconstruction method that explicitly conditions the generation process on a given viewpoint. The traditional encoder-decoder architecture is extended to incorporate view information by decomposing image features into a latent view code and a latent shape code. A view auto-encoder transforms viewpoints to the latent view space, and a random view reinforced training strategy is introduced to encourage the network to learn how to match the input sketch from different viewpoints. Synthetic sketch data is used for training, with domain adaptation techniques applied to bridge the gap between synthetic and real sketches.Extensive experiments are conducted to evaluate the effectiveness of the proposed method. Case studies demonstrate the advantages of the view-aware architecture and the use of specified views in eliminating ambiguities. The newly created ShapeNet-Sketch dataset is used to evaluate the method on free-hand sketches, showing that it generates higher-quality shapes and better conveys user intentions compared to alternative baselines. Ablation studies confirm the necessity of various components of the method.In summary, this paper contributes by investigating the problem of generating 3D mesh from a single free-hand sketch, providing a fast and easy-to-use 3D content creation solution for novice users. The importance of viewpoint specification is addressed, and a view-aware generation architecture is designed to explicitly condition the generation process on viewpoints. The method is evaluated through quantitative and qualitative assessments, demonstrating promising results and improved generalization on free-hand sketches.