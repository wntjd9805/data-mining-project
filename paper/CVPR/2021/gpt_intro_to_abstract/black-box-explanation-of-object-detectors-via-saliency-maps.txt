Object detection has seen significant improvements with the use of deep neural networks (DNNs), but these models remain opaque and difficult to understand, debug, and improve. Explanation techniques, such as attribution techniques that produce saliency maps, have been proposed as solutions to these issues. However, existing attribution techniques are primarily focused on image classification tasks and have not been extensively explored for object detection. In this paper, we introduce D-RISE (Detector Randomized Input Sampling for Explanation), the first method capable of generating saliency maps for object detectors that explain both the localization and classification aspects of the detection. D-RISE is a black-box method that can be applied to any object detector. We demonstrate the generalizability of D-RISE by explaining two commonly used object detectors with different architectures. Additionally, we analyze potential errors and biases in object detectors trained on the MS-COCO dataset and propose an evaluation procedure to measure the effectiveness of saliency methods in uncovering biases. Our method outperforms classification baselines and provides valuable insights into the learned patterns in object detection models.