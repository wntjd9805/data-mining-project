This paper focuses on the localization of instances of signs in continuous sign language sequences. The goal is to achieve automatic sign localization in order to enable various practical applications such as constructing sign language dictionaries, indexing signing content for efficient search, and assisting linguistic analysis of signing corpora. While significant progress has been made in localizing human actions and spotting words in spoken languages through large-scale annotated datasets, sign language datasets are more limited in scale and diversity. To address this, the paper proposes using sign-interpreted TV broadcast footage with subtitles as a large-scale source for training a Transformer model to predict sign annotations. This approach is challenging due to the weak alignment between subtitles and signing content, differences in grammatical structures between sign and spoken languages, and the need to account for translator interpretations. However, the hypothesis is that the attention mechanism of the Transformer model will learn to localize signs in order to solve the sequence prediction task. Through experiments and evaluations, the paper demonstrates that the attention patterns of the Transformer model can indeed be used to localize signs. The contributions of the paper include showing the ability of the Transformer attention mechanism to localize signs, generating automatic annotations for a large sign vocabulary, collecting manually verified sign instances, and achieving improved sign language recognition performance.