The majority of visual classification work focuses on the multi-class setting, where each image is assigned to one class. However, in reality, the world is multi-label, with images containing multiple objects or scenes revealing multiple health conditions. Obtaining accurate multi-label annotations for training deep neural networks can be expensive. While heuristics can reduce annotation efforts, they increase label errors. False negatives are common due to rare classes being missed by human annotators, and detecting absence is often more difficult than detecting presence. This presents a challenge in multi-class datasets, where images can belong to multiple classes. This paper explores the problem of single positive multi-label learning, where only one positive label is observed for each training image. This problem is worthwhile as it reduces annotation costs, addresses the presence of multiple classes in datasets, and explores the performance limits of multi-label classifiers. The paper presents a unified framework for existing multi-label approaches, introduces a novel single positive multi-label loss, and evaluates multiple loss functions across four multi-label classification datasets. Experimental results show that training with a single positive label per image significantly reduces the supervision required for multi-label image classification without a substantial drop in performance.