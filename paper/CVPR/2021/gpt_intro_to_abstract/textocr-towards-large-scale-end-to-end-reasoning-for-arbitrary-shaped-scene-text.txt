This paper introduces the TextOCR dataset, which aims to improve the understanding and reasoning of scene text in images beyond optical character recognition (OCR) extraction. Existing OCR systems still struggle to accurately detect and extract text in real-life scenarios due to the lack of large annotated datasets. The TextOCR dataset addresses this gap by providing high-quality and large-quantity text annotations on TextVQA images, allowing for end-to-end training of downstream application models with OCR systems. Previous OCR datasets have limitations in terms of size and focus on specific scene types. In contrast, TextOCR contains over 28,000 images and 903,000 words, with an average of 32 words per image. It can be used as a benchmark dataset for OCR performance evaluation and as training data for tasks with high text density. The paper also presents a novel architecture called PixelM4C, which connects an OCR model (Mask TextSpotter) with a TextVQA model (M4C) in an end-to-end trainable fashion. The architecture enables text-based reasoning directly on the images, improving performance in tasks such as TextVQA and TextCaps. Experimental results show that TextOCR is effective both as training data to advance OCR state-of-the-art on multiple datasets and as testing data to present new challenges to the computer vision community. The PixelM4C architecture achieves state-of-the-art performance on TextVQA and provides insights for future research in the field.