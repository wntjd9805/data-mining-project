Machine learning models sometimes make mistakes by learning the "wrong" thing, using confounding factors within the data. These mistakes are not easily identified through standard performance measures. Instead, researchers have explored the use of explanations to understand what features the models are actually relying on. Explanatory Interactive Learning (XIL) depends on the quality of the explanations provided. Many explanation methods in deep learning only provide visual explanations, which are insufficient for tasks that require a concept-level understanding. This limitation hinders the ability to fix Clever-Hans behavior in machine learning models.To address this limitation, we introduce Neuro-Symbolic XIL (NeSy XIL), a novel approach that decomposes a visual scene into an object-based, symbolic representation and allows for computing and interacting with neuro-symbolic explanations. We evaluate the advantages of NeSy XIL using a newly compiled dataset called CLEVR-Hans, which includes scenes that are classified based on specific combinations of object attributes and relations. Importantly, CLEVR-Hans encodes confounding factors that are not separable in the original input space.The contributions of this work are as follows: (i) empirical confirmation on the CLEVR-Hans dataset that Neuro-Symbolic concept learners may also exhibit Clever-Hans behavior, (ii) the development of a novel Neuro-Symbolic concept learner that combines Slot Attention and Set Transformer in an end-to-end differentiable manner, (iii) the introduction of a new loss function to address Clever-Hans behavior, (iv) efficient optimization of the Neuro-Symbolic concept learner using symbolic annotations about incorrect explanations, and (v) the first XIL approach that operates on both the visual and conceptual levels. These contributions are significant for advancing conversational explanations between machines and human users, improving trust development, and enabling truly explanatory interactive learning.