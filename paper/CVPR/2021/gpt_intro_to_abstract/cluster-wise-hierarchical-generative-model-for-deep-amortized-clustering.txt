Clustering is a fundamental task in unsupervised machine learning that involves grouping similar data points into multiple clusters. It has various applications in computer vision, natural language processing, social network analysis, and medical informatics. Probabilistic clustering models, specifically finite mixture models and infinite mixture models, have gained significant attention due to their ability to introduce a discrete latent variable indicating cluster identity for each observation. In recent years, deep generative clustering models have been developed, incorporating deep neural networks to predict latent variable states in a probabilistic program or generative model. These models typically use a finite mixture prior, such as the Gaussian Mixture Model, to fit the data set. However, using a fixed number of clusters may not be optimal when the underlying distribution of the data changes. To address this issue, researchers have explored the application of infinite mixture models, which use nonparametric Bayesian techniques to automatically determine the appropriate number of mixture components. To approximate the infinite mixture posterior, deep amortized clustering methods have been proposed, utilizing neural networks for amortized inference of cluster assignments and parameters. However, these methods rely on constructing a nonparametric prior, which can be time-consuming and difficult to converge for large-scale datasets. In this paper, we introduce the Cluster-wise Hierarchical Generative model for deep amortized clustering (CHiGac), which aims to learn a nonparametric deep Bayesian posterior from a cluster-wise perspective. CHiGac focuses on generating clusters rather than individual data points, resulting in an efficient generation process that depends on the number of clusters rather than the number of data points. It also allows for the exploitation of inter-cluster and intra-cluster structures during the learning process. To approximate the nonparametric Bayesian posterior, we propose the Ergodic Amortized Inference (EAI) algorithm, which considers the average behavior over a sequence to reduce the amortization gap and provide flexible parameterization for the neural amortized inference model. We demonstrate the superiority of CHiGac through experiments on synthetic and real-world datasets, evaluating clustering performance and inference optimization performance.