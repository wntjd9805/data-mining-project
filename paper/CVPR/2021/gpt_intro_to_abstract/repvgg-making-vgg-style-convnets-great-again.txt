The introduction of this computer science paper discusses the evolution of Convolutional Neural Networks (ConvNets) for image recognition. The authors mention the success of simple architectures like VGG, but note that recent research has shifted towards more complex designs such as Inception, ResNet, and DenseNet. These architectures often involve multi-branch designs and specific components that can make implementation and customization difficult, slow down inference, and reduce memory utilization. The authors argue that the number of floating-point operations (FLOPs) does not accurately reflect the actual speed of these models. To address these drawbacks, the authors propose a new architecture called RepVGG. This architecture uses structural re-parameterization to decouple a training-time multi-branch topology with an inference-time plain architecture. The authors explain that the body of an inference-time RepVGG consists of a single type of operator (3x3 conv followed by ReLU), which makes it fast on generic computing devices like GPUs. They also suggest that RepVGG allows for specialized hardware to achieve even higher speeds by reducing the number of required operators and memory units.The authors summarize their contributions as follows: 1) proposing RepVGG as a simple architecture with a favorable speed-accuracy trade-off compared to state-of-the-art models, 2) introducing structural re-parameterization to decouple training and inference architectures, and 3) demonstrating the effectiveness and ease of implementation of RepVGG in image classification and semantic segmentation tasks.