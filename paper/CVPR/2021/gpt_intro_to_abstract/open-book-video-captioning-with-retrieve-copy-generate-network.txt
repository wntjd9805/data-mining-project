Video captioning is an important task in computer vision and natural language processing, aiming to automatically describe the contents of a video using textual descriptions. While various methods have been proposed for video captioning, they often suffer from limitations such as generic sentence generation and fixed knowledge domain. To address these issues, this paper proposes an Open-book Video Captioning paradigm, inspired by open-domain question answering. Instead of directly generating captions, the proposed paradigm involves two stages: Video-Text Retrieval (VTR) to search for relevant sentences from a text corpus, and caption generation guided by the retrieval sentences. During inference, the generator can either generate words based on the video content or copy expressions from the retrieved sentences. This flexible approach allows for the extension and revision of the model's knowledge domain. The proposed Retrieve-Copy-Generate (RCG) network incorporates a Video-to-Text Retriever and a Copy-Mechanism Generator to effectively retrieve relevant sentences and dynamically decide whether to copy or generate new words. Experimental results on VATEX and MSR-VTT datasets demonstrate the effectiveness of the proposed approach, achieving state-of-the-art performance in video captioning. The contributions of this work include the open-book paradigm for video captioning, the RCG network, and the improved performance in caption generation through cross-modal retrieval and copy mechanisms.