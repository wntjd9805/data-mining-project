Computing the multi-view representation of a 3D object/scene is a fundamental problem in 3D vision. The goal is to generate a small set of views that maximize a scoring function based on scene coverage and viewing constraints. Previous approaches densely sample views and solve a set cover problem, but this incurs significant computational overhead. In this paper, we propose a novel approach that avoids explicit sampling by training view prediction networks. These networks take a small set of views as input and predict views for other camera poses. We approximate the scoring function using a neural network that combines view prediction and a scoring module. The entire network is trained end-to-end, transforming the multi-view representation optimization problem into a simple continuous optimization problem. Our approach, which avoids explicit sampling of rendered images, is motivated by the fact that visibility patterns of a scene have low-dimensional structures. We represent the scene using a volumetric representation and design a network that optimizes camera configurations efficiently and effectively. Our approach makes two major contributions: converting the multiple-view selection problem into a continuous optimization problem and providing a solver with reduced computational cost. Evaluation results on a benchmark dataset demonstrate similar coverage results to greedy methods with 10Ã— speed and outperform other fast baseline methods.