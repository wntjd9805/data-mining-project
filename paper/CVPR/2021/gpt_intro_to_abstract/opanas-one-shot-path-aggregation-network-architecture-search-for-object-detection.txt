Recognizing objects at different scales is a major challenge in computer vision. In this paper, we propose a new NAS framework called OPANAS (One-Shot Path Aggregation Neural Architecture Search) to automatically search for an optimal Feature Pyramid Network (FPN) for object detection. We carefully design six information paths, including parameterized and parameter-free ones, to build our search space. We then propose a novel FPN search space representation using a densely-connected directed acyclic graph, allowing for richer aggregation topological structures. Our method consists of training a super-net and searching for the optimal sub-net using an evolutionary algorithm. Experimental results show that our method is efficient and achieves better detection accuracy with fewer parameters and operations. Furthermore, our searched FPN architecture can be easily integrated into mainstream detectors, improving their accuracy by 2.3% to 3.2% mAP. Our contributions include the investigation of aggregations of multiple information paths, the proposal of an efficient and effective one-shot search method, and the achievement of a new state-of-the-art accuracy-speed trade-off in object detection.