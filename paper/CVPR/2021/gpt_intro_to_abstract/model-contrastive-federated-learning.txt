Deep learning models typically require large and representative datasets for optimal training. However, in practical scenarios, data is often distributed among different parties, such as mobile devices and companies, due to privacy concerns and data protection regulations. Federated learning has emerged as a solution to enable multiple parties to collaborate in training a machine learning model without sharing their local data. One popular federated learning algorithm, FedAvg, aggregates updates from local models to update a global model without exchanging raw data. However, a challenge in federated learning is the heterogeneity of data distribution among parties, which can degrade performance. Existing approaches to address this issue have shown limited effectiveness, especially on image datasets with deep learning models. In this paper, we propose a novel approach called model-contrastive learning (MOON) to address the non-IID (non-identically distributed) issue in federated learning. MOON corrects local updates by maximizing the agreement between the representations learned by the current local model and the global model. Unlike traditional contrastive learning approaches that compare representations of different images, MOON conducts contrastive learning at the model-level. Experimental results show that MOON outperforms existing federated learning algorithms on various image classification datasets. It achieves at least 2% higher accuracy compared to other approaches with only lightweight modifications to FedAvg. Our study focuses on the local training phase and does not explore personalized federated learning or improving the aggregation phase. Overall, MOON provides a simple and effective framework for federated learning, addressing the non-IID data issue through model-based contrastive learning.