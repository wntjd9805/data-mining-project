This paper introduces SOHO, an end-to-end vision-language pre-training framework for cross-modal learning. Unlike existing models that rely on region-based image features, SOHO directly learns image and language embeddings and their semantic alignment from image-text pairs. The framework utilizes a visual dictionary to represent comprehensive semantics in the visual domain, which is dynamically updated through a trainable CNN backbone. Pre-training tasks include Masked Vision Modeling, Masked Language Modeling, and Image-Text Matching. Experimental results demonstrate that SOHO outperforms existing methods on various downstream tasks, including text retrieval, visual reasoning, and visual question answering. The proposed model achieves a significant improvement in performance and inference speed compared to models that rely on object bounding boxes. The paper concludes by emphasizing the contributions of SOHO and the release of the model and code to the research community.