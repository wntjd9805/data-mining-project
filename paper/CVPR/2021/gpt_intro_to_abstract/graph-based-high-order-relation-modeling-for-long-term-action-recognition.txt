This paper introduces a novel approach for recognizing long-term actions in computer vision. Previous studies have primarily focused on short-term actions, which are easily extracted from pre-trimmed clips of videos. However, the complexity of real-life videos makes it both time-consuming and labor-intensive to extract short-term actions from long videos. Additionally, the lack of consideration for internal relations between actions hinders a deeper understanding of human behaviors. In contrast, long-term actions are actions that occur in untrimmed videos over an extended period, and they often involve multiple sub-actions with intricate relations. The goal of long-term action recognition is to identify the long-term action or all of its sub-actions, reducing the annotation cost of datasets. This approach aims to capture the high-order relations between visual concepts, such as objects, motions, and sub-actions, within long-term actions. By modeling these high-order relations, the proposed Graph-based High-order Relation Modeling (GHRM) module enhances the recognition of long-term actions. The GHRM module utilizes graph convolutional modules to model each basic relation, where each segment in the video represents a graph node. Incorporating information from all other basic relations, the GHRM module effectively captures the high-order relations in the temporal evolution of long-term actions. The proposed model further employs a GHRM-layer, consisting of the Temporal-GHRM and Semantic-GHRM branches, to model both local temporal and global semantic high-order relations in the long-term actions. Experimental results using three popular long-term action recognition datasets demonstrate that the proposed model outperforms existing methods, highlighting its effectiveness in capturing and leveraging high-order relations.