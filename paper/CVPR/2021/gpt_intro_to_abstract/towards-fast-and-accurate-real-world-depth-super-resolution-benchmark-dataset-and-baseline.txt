The use of depth maps as a supplement to RGB images has been applied in various applications such as bokeh rendering, AR modeling, face recognition, and gesture recognition. The popularity of low-power depth sensors on mobile consumer electronics has made depth map resolution a limitation for practical applications. This paper focuses on depth map super-resolution (SR) as a solution to improve the resolution of depth maps. Existing depth map SR algorithms often use downsampling to construct training samples, but this fails to simulate real-world correspondences between low-resolution (LR) and high-resolution (HR) depth maps. To bridge this gap, the authors construct the "RGB-D-D" benchmark dataset that includes paired LR and HR depth maps captured from mobile phones and HR sensors. The dataset consists of indoor and outdoor scenes, offering both downsampled LR depth maps and raw LR depth maps with noise and depth holes.Although numerous algorithms have been proposed for depth map SR, there are still challenges in preserving details, reducing computation complexity, and meeting real-world application requirements. The authors introduce a high-frequency guided multi-scale dilated structure to recover sharp boundaries and elaborate details in depth map SR. They also design a high-frequency layer in the network to balance efficiency and accuracy, allowing for application on mobile devices and embedded systems. Moreover, they use the paired depth maps from the RGB-D-D dataset to improve the accuracy and visual effects of the depth map SR task.The contributions of this paper include the construction of the RGB-D-D benchmark dataset, which bridges the gap between theoretical research and real-world applications, as well as the design of a fast depth map SR baseline model that achieves superior performance in terms of speed and accuracy. The algorithm is evaluated on public datasets and the RGB-D-D benchmark dataset, demonstrating improved results in terms of accuracy and boundary clarity for real-world depth map SR tasks.