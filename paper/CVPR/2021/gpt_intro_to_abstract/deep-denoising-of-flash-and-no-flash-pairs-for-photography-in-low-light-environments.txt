Flash photography has long been a popular method for capturing high-quality images in low-light conditions. However, the use of flash can result in unattractive shading and harsh shadows, failing to capture the true mood and ambience of the scene. Previous approaches have attempted to enhance low-light images by combining pairs of flash and no-flash images, but these methods assume moderate noise levels and accurate alignment between the images.In this paper, we propose a novel approach that leverages the power of deep neural networks and the unique combination of appearance information in flash and no-flash image pairs. Our goal is to generate a high-quality image of the scene under ambient lighting, even in extremely low-light situations where the no-flash image is highly noisy and the appearance is dominated by flash illumination. We also consider the challenge of unknown geometric misalignment between the image pair.To address these challenges, we train a deep neural network to take noisy and misaligned flash/no-flash image pairs as input and output a denoised image of the scene under ambient illumination. Instead of directly predicting the denoised image, our network outputs a kernel field for filtering the no-flash image, and a scale map for incorporating high-frequency image details from the flash image. We combine a kernel basis prediction approach with efficient kernel upsampling to effectively filter out the noise in the no-flash input.We extensively evaluate our approach under different ambient light levels and spatial misalignments, and our results demonstrate state-of-the-art performance in low-light denoising. Our method outperforms denoising without flash, as well as other standard denoising approaches trained directly on flash/no-flash pairs. This highlights the effectiveness of our network architecture and formulation.In conclusion, our proposed approach effectively enhances low-light images by utilizing the information from both flash and no-flash images. Our method achieves superior results compared to existing techniques and demonstrates the importance of careful design and formulation. The code and pre-trained models for our method are publicly available for further exploration.