Point cloud videos provide a valuable source of visual information in understanding the dynamics of the 3D world. They offer flexibility for action recognition in poor visibility environments and capture more precise geometry dynamics. However, modeling the spatio-temporal structure in point cloud videos is challenging due to their irregular and unordered nature. Existing solutions involve converting point cloud videos into regular and ordered voxels or employing point tracking. These methods come with computational inefficiencies and limitations in handling colorless point clouds.In this paper, we propose a novel approach called Point 4D Transformer Network (P4Transformer) to model the spatio-temporal structure in raw point cloud videos. Instead of voxelization or point tracking, we introduce a point 4D convolution to encode the local structures in the videos directly on raw points. By merging local points along the spatial and temporal dimensions, we reduce the computational complexity. To capture global appearance and motion information, we utilize the transformer with self-attention to adaptively merge related local areas based on attention weights.Our proposed P4Transformer is evaluated on a video-level classification task (3D action recognition) and a point-level prediction task (4D semantic segmentation) using multiple datasets. Experimental results demonstrate the effectiveness of our approach in improving the accuracy of action recognition and semantic segmentation. The contributions of this paper include the introduction of the transformer for spatio-temporal modeling of raw point cloud videos, the development of the point 4D convolution to embed local structures and reduce computation, and the successful application of the P4Transformer in improving classification and segmentation performance.