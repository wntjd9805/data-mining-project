This paper addresses the fundamental challenge in computer vision of object identification and segmentation with limited examples. Inspired by a child's ability to identify object parts based on prior visual experience, we propose a few-shot setup where we aim to identify semantic parts in unseen images given only a few annotated examples. Unlike traditional few-shot learning setups, our approach focuses on a single object class with limited annotations. Existing methods leverage prior knowledge learned from non-target classes or image-level annotations, but still require expensive labels or part annotations. In contrast, we propose a novel approach that utilizes a generative adversarial network (GAN) to learn meaningful pixel-wise representations without any labels. We demonstrate that these representations can be directly used for semantic part segmentation and achieve high-quality segmentation results with only one example label. Additionally, we introduce an extension called auto-shot segmentation that bypasses the latent optimization process and allows for faster and more efficient predictions. By performing geometric data augmentation during training, we can segment multiple objects with different sizes and orientations simultaneously. Our main contribution is the innovative use of GANs for unsupervised pixel-wise representation learning, which achieves unprecedented performance in few-shot semantic part segmentation. We also address domain gap issues by extending our approach to real-world scenarios. Overall, our findings highlight the discriminative power of the learned representations and the potential of GANs in computer vision tasks.