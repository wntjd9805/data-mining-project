Deep convolutional neural networks (CNNs) have achieved impressive results in various computer vision tasks, such as image classification, object detection, and video analysis. While the focus has largely been on improving model performance, recent research has also emphasized the importance of model efficiency, particularly in resource-constrained hardware devices like mobile phones and wearable devices. To address this, several model compression algorithms have been developed to reduce the computational complexity of deep networks without sacrificing performance. These include techniques such as quantization, knowledge distillation, low-rank approximation, and channel pruning.Channel pruning, which involves discarding redundant filters to obtain a more compact network, is especially popular in industrial products due to its potential for significant speed-up on mainstream hardware. However, conventional channel pruning methods treat all input samples equally and do not fully exploit the diversity of instance complexity and the relationships between different instances. Recent approaches have proposed dynamic channel pruning methods that consider the importance of filters on a per-instance basis and achieve better performance. For example, some methods use auxiliary modules to predict the saliency of channels based on given input data and prune unimportant filters at runtime. Others induce different sub-networks for different input samples using instance-wise sparsity. In this paper, we propose a new paradigm for dynamic pruning that maximizes network redundancy, taking into account the arbitrary instance. We utilize the manifold information of all samples in the training set to derive corresponding sub-networks that preserve the relationship between different instances. Specifically, we identify the complexity of each instance and adaptively adjust the penalty weight on channel sparsity. We also preserve the similarity between samples in the pruned results by considering the features with abundant semantic information obtained by the network. By applying this approach, we achieve more reasonable allocation of resources, resulting in pruned networks with higher performance and lower costs. Our experiments on various benchmarks demonstrate the effectiveness of our method, outperforming the state-of-the-art pruning algorithms in terms of network accuracy and speed-up ratios.