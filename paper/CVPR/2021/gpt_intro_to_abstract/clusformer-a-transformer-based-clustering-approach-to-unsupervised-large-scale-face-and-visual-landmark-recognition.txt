Automatic unsupervised visual clustering has become a prominent research area due to the abundance of unlabeled data and the need for consistent visual recognition algorithms across challenging conditions. While supervised face recognition algorithms have seen significant advancements, the field of face clustering remains relatively limited. Visual clustering methods face challenges such as illumination, poses, and occlusions in real-world scenarios, with robust feature extraction being particularly crucial.Graph Convolutional Networks (GCNs) have emerged as popular methods for unsupervised visual clustering, but they still have limitations in terms of accuracy, algorithm complexity, and computational time. This work introduces Clusformer, a novel Transformer-based approach for automatically clustering visual samples in an unsupervised manner. Clusformer effectively deals with noisy and hard samples through its self-attention mechanism, making it one of the first works to utilize self-attention in Transformers for visual clustering.The contributions of this work are three-fold. Firstly, a new Transformer-based clustering architecture is introduced for top-down clustering of large-scale unsupervised visual databases. Secondly, the Clusformer framework incorporates new Visual Grammar and Cosine Distance Encoding (CDE) modeling mechanisms to efficiently solve visual clustering problems. Finally, the proposed approach consistently achieves state-of-the-art results compared to recent clustering methods on standard visual benchmarks such as Google Landmark and MS-Celeb-1M face database.