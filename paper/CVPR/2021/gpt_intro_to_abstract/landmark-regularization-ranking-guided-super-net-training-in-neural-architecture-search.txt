Recent advancements in neural architecture search (NAS) algorithms have enabled the discovery of architectures that surpass human-designed ones in computer vision tasks. One key factor behind this progress is parameter sharing, which significantly reduces search time. These NAS methods rely on a shared network, or super-net, that encompasses all potential architectures in the search space. To train the super-net, architectures are sampled and trained either explicitly or implicitly. However, previous studies have shown a poor correlation between the relative performance of architectures in the super-net and their performance when trained independently. This is mainly due to the fact that optimal architectures should have different parameter values, which is not possible with parameter sharing. In this paper, we propose a regularization term that encourages architectures in the super-net to have similar rankings as their stand-alone counterparts. We utilize a small set of landmark architectures with known performance to guide the super-net training. Our regularization term is agnostic to the sampling algorithm used for super-net training, making it compatible with various weight-sharing NAS algorithms. Experimental results on CIFAR-10, ImageNet, and monocular depth estimation tasks show that our approach significantly reduces ranking disorder, allowing for the discovery of better-performing architectures.