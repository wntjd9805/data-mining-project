Neural architecture search (NAS) has made significant advancements in automatically designing efficient models for image classification. However, existing NAS methods for dense prediction tasks, such as semantic segmentation and pose estimation, have not been able to match the performance of hand-crafted models. One crucial aspect of dense prediction tasks is the need for both global context and high-resolution representations, which have not been adequately addressed in existing NAS algorithms for classification. This paper proposes a NAS algorithm that incorporates in-network multi-scale features and transformers, allowing for adaptive changes based on task objectives and resource constraints. The authors introduce a lightweight and plug-and-play transformer architecture that can be combined with convolutional neural architectures. They also develop a multi-resolution search space that includes both convolutions and transformers to model in-network multi-scale information and global context for dense prediction tasks. Additionally, a resource-aware search strategy is introduced to customize efficient architectures for different tasks. Extensive experiments demonstrate that models produced by this NAS algorithm achieve state-of-the-art results on multiple dense prediction tasks and widely used benchmarks while maintaining lower computational costs. The main contributions of this work are the introduction of a novel lightweight transformer architecture, the integration of transformers in a resource-constrained NAS search space for computer vision, and the development of a resource-aware search strategy for efficient architecture customization.