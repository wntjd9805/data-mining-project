This paper introduces a method for improving image retrieval by combining fast and slow processes. In the fast process, images are quickly scanned to eliminate those that do not match a given textual description. In the slow process, more attention is paid to image details to ensure the best match. To implement fast retrieval systems, visual and textual inputs are encoded separately into a joint embedding vector space, allowing for efficient similarity computations. However, the accuracy of these methods is limited due to the simplicity of the vision-text interaction model. On the other hand, slow retrieval systems, which use cross-attention mechanisms to compare words to image locations, significantly improve retrieval performance but are slow and impractical for large-scale image retrieval.To address this challenge, the paper proposes combining dual encoder approaches with cross-attention using two complementary mechanisms. First, a novel distillation objective transfers knowledge from accurate slow models to fast dual encoders. Second, a re-ranking strategy combines the results of fast models with slow models to improve accuracy. The proposed approach is both fast and accurate, with the speed of cross-attention no longer being a bottleneck. The paper also introduces a feature map upsampling mechanism to enhance fine-grained attention. The approach is validated on image retrieval tasks using the COCO and Flickr30K datasets, showing significant improvements in inference time while achieving competitive results. The approach is also extended to text-to-video retrieval and achieves state-of-the-art performance on the VATEX dataset.