Photorealistic view synthesis has the potential to allow users to explore virtual landscapes with realistic images from different viewpoints. This paper presents a new method for achieving photorealistic view synthesis by utilizing a set of input images taken from a handheld video of the scene. The method involves constructing a 3D geometric scaffold using off-the-shelf techniques, encoding input images using a convolutional network, and mapping the feature vectors onto the scaffold. By synthesizing new views, each pixel in the target view is mapped onto the geometric scaffold to obtain input rays and an output ray. A differentiable module is used to aggregate the feature vectors, considering the geometry of the rays, and a feature tensor is created. The final image is rendered using a convolutional network. The entire pipeline is trainable end-to-end, supporting realism optimization. The method is evaluated on three datasets and is shown to outperform the state of the art in terms of LPIPS error, PSNR, and SSIM. The results demonstrate significant improvements in photorealism for novel views in real scenes and objects. Supplementary materials, such as images and video sequences, are also provided.