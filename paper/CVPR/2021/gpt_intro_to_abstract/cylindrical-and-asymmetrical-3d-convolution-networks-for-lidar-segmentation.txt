In this paper, we tackle the task of 3D LiDAR segmentation for autonomous driving. While existing methods have utilized 2D projection techniques, we propose a new framework that focuses on the inherent 3D structure of outdoor LiDAR point clouds. We identify the key challenges of sparsity and varying density in outdoor scenes, which differ from dense and uniform-density indoor scenes. To address these challenges, our framework consists of two components: 3D cylindrical partition and asymmetrical 3D convolution networks. The cylindrical partition divides the point cloud dynamically based on distance, resulting in a more balanced point distribution. The asymmetrical 3D convolution networks are designed to match the point distribution of objects in driving scenes and improve robustness to sparsity. Additionally, we introduce a point-wise module to refine features obtained from voxel-based networks and reduce information loss caused by label encoding. We evaluate our method on two large-scale outdoor datasets and achieve state-of-the-art performance in LiDAR segmentation, panoptic segmentation, and 3D detection tasks. Our contributions include focusing on the 3D structure of outdoor LiDAR point clouds, introducing a novel framework to address the challenges of sparsity and varying density, and achieving top performance on multiple benchmark datasets.