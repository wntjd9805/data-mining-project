Recent advancements in deep learning have led to the development of complex computer vision applications that can be run on edge devices. This trend has resulted in several benefits, including improved privacy, reduced power consumption, and personalized predictions. There is a growing interest in expanding the scope of machine learning to ultra-low power systems, known as TinyML. TinyML aims to perform always-on use-cases in the mW range and below, powered by microcontroller units (MCUs) or application-specific integrated circuits (ASICs). However, deploying convolutional neural networks (CNNs) on MCUs is challenging due to on-chip memory restrictions, power consumption issues, and latency problems. Typical microcontrollers have limited on-chip memory, and the peak memory of TinyML models should not exceed this limit. Fetching partial model weights from flash storage increases read access time and cache miss ratio. While previous works focused on reducing peak memory, our proposed method significantly reduces both peak and weight memory to below 60 KB. This ultra-low memory footprint is crucial for low-power platforms as it reduces memory accesses. We introduce RaScaNet, a novel architecture that processes non-overlapping sub-images sequentially instead of full-frame images. This aligns well with modern image sensors that read a small number of rows at a time. By exploiting the data stream of an image sensor, RaScaNet can operate with minimum peak memory. We propose a deep neural network architecture that fits in ultra-low power systems and achieves state-of-the-art efficiency in terms of accuracy versus memory. RaScaNet requires significantly smaller peak memory and weight memory compared to existing tiny models while maintaining competitive accuracy. We also design three important components in RaScaNet and introduce an early termination scheme for further acceleration. Overall, our contributions include the development of RaScaNet, which processes input images in a raster-scan manner, achieving superior memory efficiency, and the introduction of important components for practical computer vision tasks.