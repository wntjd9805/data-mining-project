Recent research has shown that combining multiple modalities, such as audio and visual, can improve the representation of deep features. However, creating large-scale multi-modality pre-training datasets typically requires extensive manual labor for cleaning and annotating raw video sets. To address this issue, recent studies have explored learning from noise supervision or automatically filtering out unpaired samples.In this paper, we focus on the task of Audio-Visual Event (AVE) localization. AVE refers to events that are both audible and visible in a video segment, where a sound source appears in an image and also exists in the audio portion. The goal of AVE localization is to identify video segments containing audio-visual events and classify them into specific categories.To tackle this task, we need to consider both intra-modal relations (temporal relations within a single modality) and cross-modal relations (relations between audio and visual modalities). Previous methods often concatenate features from synchronized audio-visual pairs but do not explicitly consider these relations, leading to unsatisfying accuracy. Some recent works incorporate temporal relations within intra-modality and encode cross-modal relations through self-attention mechanisms. However, they often overlook the interference caused by irrelevant audio-visual segment pairs during the fusion process.In this paper, we propose a Positive Sample Propagation (PSP) module to address these issues. PSP constructs an all-pair similarity map between audio and visual segments, filters out low similarity entries, and aggregates audio and visual features without considering negative and weak entries. This approach allows for more relevant features, even if they are not synchronized, to be aggregated in an online fashion.In addition to the PSP module, we also introduce improvements for both fully and weakly supervised settings. In the fully supervised setting, we introduce an audio-visual pair similarity loss that encourages the network to learn highly correlated features for audio and visual segments belonging to the same event. In the weakly supervised setting, we propose a weighting branch that assigns temporal weights to segment features.We evaluate our method on the standard AVE dataset and demonstrate that our techniques consistently improve performance, achieving state-of-the-art results under both fully and weakly supervised settings.