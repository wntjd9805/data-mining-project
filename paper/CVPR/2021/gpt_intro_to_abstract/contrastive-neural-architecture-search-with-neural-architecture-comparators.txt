Deep neural networks (DNNs) have shown significant progress in various challenging tasks, but designing effective architectures is a labor-intensive process that relies heavily on human expertise. Neural Architecture Search (NAS) aims to automate the design process and has shown promising results. However, existing NAS methods rely on the absolute performance of architectures as the training signal, which has limitations. Firstly, obtaining stable and accurate absolute performance for all candidate architectures is non-trivial and can greatly hamper the search performance. Secondly, obtaining the absolute performance from the supernet is time-consuming. To overcome these limitations, this paper proposes a Contrastive Neural Architecture Search (CTNAS) method that searches for architectures using architecture comparisons. A Neural Architecture Comparator (NAC) is introduced to perform pairwise architecture comparisons, using the comparison results as the reward instead of relying on absolute performance. A curriculum learning approach is employed to gradually improve the baseline architecture and accelerate the search process. Additionally, a data exploration method is proposed to improve the generalization ability of NAC. Experimental results on three search spaces demonstrate that CTNAS outperforms state-of-the-art methods in terms of searched architectures.