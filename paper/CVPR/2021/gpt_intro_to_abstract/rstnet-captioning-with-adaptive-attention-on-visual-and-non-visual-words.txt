This paper presents a novel approach to the image captioning task, which involves generating natural language sentences to describe the content of an image. The authors propose the use of a Grid-Augmented (GA) module to address the loss of spatial information in the visual features extracted from the image. By incorporating the spatial geometric relationships between relative locations into the grid features, the GA module allows for a more comprehensive use of the grid features. In addition, the authors introduce an Adaptive Attention (AA) module that dynamically measures the contribution of visual signals and language context in the caption generation process. This module helps to process visual and non-visual words differently, taking into account the semantic gap between vision and language. The proposed approach is integrated into a transformer-based image captioning model called Relationship-Sensitive Transformer (RSTNet). The RSTNet is evaluated on the MSCOCO benchmark dataset and achieves state-of-the-art performance both offline and online. The authors also introduce a cross-domain attribute called visualness, which quantitatively measures the visualizability of each word in the vocabulary. Overall, the proposed approach significantly improves the captioning performance and provides insights into the impact of the semantic gap in visual understanding.