Recent years have seen significant advancements in RGB-D salient object detection (SOD) due to its various applications in fields like image retrieval, video segmentation, person re-identification, and visual tracking. RGB-D SOD aims to identify and segment visually salient regions in a scene using both RGB and depth channels. This paper focuses on developing a depth-sensitive SOD model that can automatically learn the interaction architecture between RGB and depth features. Existing RGB-D SOD methods often treat depth as an auxiliary input channel, which limits their ability to effectively utilize depth information for capturing salient object layouts. To address this, the authors propose decomposing the raw depth map into multiple regions and extracting depth-sensitive RGB features in each region using a depth-sensitive attention module (DSAM). This helps enhance the RGB features with depth-wise geometric prior knowledge, leading to improved saliency analysis.Furthermore, designing an effective feature interaction architecture between RGB and depth branches is crucial for multi-modal feature fusion in RGB-D SOD. The authors leverage neural architecture search (NAS) to automatically explore an effective feature fusion module. They construct a new search space tailored for multi-modal feature fusion across multiple scales, resulting in an automatically-found feature fusion architecture that achieves state-of-the-art performance.The contributions of this paper include the proposal of a depth-sensitive attention module to enhance RGB features using depth prior knowledge, the design of a new search space for heterogeneous feature fusion in RGB-D SOD, and the introduction of NAS for RGB-D SOD. Extensive experiments conducted on seven benchmarks demonstrate that the proposed method outperforms other state-of-the-art approaches.