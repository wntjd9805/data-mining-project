Estimating the geographic location of an image is a key problem in computer vision, with applications in autonomous driving, robotics, and augmented reality. Initially, image retrieval techniques were used to determine the location of a query street view image by comparing it against a database of GPS-tagged street images. However, this approach has limitations due to varying coverage and sparsity in rural areas. To address these limitations, researchers turned to cross-view image-based geo-localization, which involves predicting the latitude and longitude of a street-level image by matching it against a GPS-tagged satellite database. However, the significant domain gap between street view and satellite images makes this approach challenging, as the two types of images differ in appearance, lighting conditions, and viewpoint. Previous methods have used polar coordinate transformations and generative adversarial networks (GANs) to bridge this gap, but these approaches have limitations in terms of retrieval accuracy and synthesis quality. In this work, we propose a novel method that combines cross-view synthesis and geo-localization in a single architecture, leveraging the mutual reinforcement between the two tasks. Our method achieves state-of-the-art performance in terms of retrieval accuracy and synthesis quality, outperforming existing approaches. Additionally, our network uses polar transformed satellite images as input, which simplifies the image generation process. Overall, our method offers a powerful and comprehensive solution for the challenging problem of geo-localization in computer vision.