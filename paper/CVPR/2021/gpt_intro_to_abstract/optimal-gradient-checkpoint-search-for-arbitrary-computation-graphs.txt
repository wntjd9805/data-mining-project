This paper introduces the concept of Gradient CheckPointing (GCP) in deep neural network (DNN) training to address the issue of high GPU memory requirements. GCP involves storing only a subset of intermediate tensors called Gradient Checkpoints (GCs) during the forward pass, and computing missing tensors via local re-forwards during the backward pass. This approach allows for significant memory reduction in DNN training. The paper proposes sophisticated theories and efficient algorithms to automatically find the optimal GCs in Arbitrary Computation Graphs (ACGs), enabling GCP training for a wide range of DNN architectures. The proposed GC searching algorithm provides the smallest memory cost in GCP training, allowing for training of much larger models within the same GPU memory constraints. The potential applications of GCP training extend to tasks such as image classification and Neural Architecture Search (NAS) networks.