Deep learning has made significant advancements in instance-level detection methods such as object detection, instance segmentation, and keypoint detection. However, the heavy computational requirements of deep-learning-based instance-level detection models hinder their adoption in real-world applications. While model compression techniques have been proposed to train compact models for accelerated inference, they primarily focus on reducing network depth or width or adopting efficient block structure designs. The reduction of input resolution to accelerate instance-level detection is generally not considered viable due to significant performance degradation. This paper aims to address this challenge by studying the fundamental problem of upgrading the performance of a low-resolution detection model to that of its high-resolution counterpart. Previous studies have explored knowledge distillation (KD) from a high-resolution teacher to a low-resolution student in the context of image classification, but applying KD to instance-level detection is non-trivial due to the output size mismatch between the teacher and student networks. To resolve this issue, the paper proposes an aligned multi-scale training method and a crossing feature-level fusion module to train a strong teacher. This methodology enables knowledge distillation from the teacher to the student by aligning the feature maps of models at different input resolutions. The paper's contributions include the alignment concept for feature map alignment, a framework for training a robust multi-scale and multi-resolution fusion teacher, and extensive experiments demonstrating the effectiveness of the proposed methods in various instance-level detection tasks.