Incremental learning (IL) is essential for artificial intelligence systems to continually acquire new knowledge while retaining previously learned information. However, deep neural networks (DNNs) tend to forget previous knowledge when learning new tasks. To address this issue, several approaches have been proposed, but most of them do not perform well in the challenging scenario of class-incremental learning (CIL). CIL involves learning a unified classifier for disjoint sets of classes in a sequential manner without task-identifiers at inference time. One of the main causes of catastrophic forgetting in IL is the overlapping or confusion between representations of new and old classes in the feature space. Storing old data or training deep generative models are possible solutions, but they have limitations and disadvantages. This paper proposes a novel approach called Prototype Augmentation and Self-Supervision (PASS) to address the catastrophic forgetting problem in CIL. PASS involves memorizing class-representative prototypes for old classes and augmenting them with Gaussian noise when learning new classes. This helps maintain the discrimination and balance between old and new classes. Additionally, PASS leverages self-supervised learning to learn task-agnostic and transferable representations, reducing task-level overfitting. Experimental results show that PASS outperforms existing non-exemplar based methods and performs comparably to exemplar based methods in the CIL scenario. Overall, this paper provides a simple and effective solution to address catastrophic forgetting in CIL, contributing to the field of incremental learning.