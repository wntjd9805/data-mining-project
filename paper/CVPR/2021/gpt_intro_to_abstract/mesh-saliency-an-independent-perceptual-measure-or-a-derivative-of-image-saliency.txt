This paper introduces the concept of mesh saliency, which measures the regional importance of 3D surfaces in accordance with human visual perception. Previous methods for mesh saliency have been found to be poor at predicting human fixations, as shown by recent eye-tracking work. This study explores the hypothesis that mesh saliency might be a derivative of image saliency rather than an independent perceptual measure. To investigate this, the authors propose learning mesh saliency from ground-truth saliency of general 2D images. Additionally, it is observed that 3D objects of the same category usually have similar saliency distributions, suggesting a link between object classification and saliency. To address these challenges, a weakly supervised deep neural network is presented, jointly trained with saliency maps of 2D images and category labels of 3D objects. This approach is especially valuable considering the difficulties in collecting eye-fixation data for 3D objects. The authors demonstrate that their weakly supervised method accurately predicts ground-truth fixations of various 3D objects, outperforming existing state-of-the-art mesh saliency methods. The proposed method utilizes a Multi-Input Multi-Output Generative Adversarial Network (MIMO-GAN), leveraging transfer learning from image saliency and 3D object classification to mesh saliency. The contribution of this work includes a novel method for mesh saliency, validation of the relationship between image saliency and mesh saliency, and the demonstration of improved performance on publicly available datasets.