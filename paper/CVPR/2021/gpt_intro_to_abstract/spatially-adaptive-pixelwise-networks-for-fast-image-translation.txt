Recent research in computer vision has focused on translating images from one domain to another. Most approaches employ conditional Generative Adversarial Networks (GANs) to learn a direct mapping between domains. While these methods have improved visual quality, they suffer from increased model size and inference time, especially for high-resolution images. In this paper, we propose a novel architecture for fast image-to-image translation. Our key contribution is a pixel-wise generator that processes each pixel independently using a lightweight Multi-Layer Perceptron (MLP). Despite the seemingly limited representation power, our network is fully expressive due to three components: spatially varying parameters, low-resolution prediction of these parameters, and the use of sinusoidal encoding for pixel position. Our model, called ASAP-Net (A Spatially-Adaptive Pixelwise Network), generates high-quality images at significantly reduced runtimes compared to existing methods. We demonstrate its superiority through visual comparisons and runtime measurements on various image domains and resolutions. Additionally, our model shows flexibility in handling different translation tasks, such as predicting depth maps from indoor images. Human perceptual studies and automatic metrics validate the visual quality of our output while confirming its runtime advantage over state-of-the-art methods.