Estimating outdoor lighting from a single image is a challenging task in computer vision with numerous applications in various fields. Existing solutions typically utilize low-dimensional parametric models to represent the sky illumination but struggle to capture the complexity of real-world lighting. Recent non-parametric approaches using autoencoders and large-scale sky panorama datasets show promising results. However, current methods only consider outdoor illumination as a single global map without spatially-varying information. In contrast, spatially-varying lighting estimation has been successful in indoor scenarios using spherical harmonics or panoramic environment maps. Extending this approach to outdoor lighting estimation is difficult due to the high-dynamic-range sunlight, complex sky light, non-parametric nature, and lack of available HDR and panoramic datasets. In this paper, we propose SOLID-Net, a neural network for spatially-varying outdoor lighting estimation. SOLID-Net tackles these challenges by decomposing the input image into intrinsic parts, including albedo, normal, plane distance, and shadow, which provide shading constraints based on global illumination. The estimated geometry is then used to warp the image and shadow map to a spherical projection centered at the target location, reducing the ill-posed nature of the problem. A synthetic dataset with ground truth labels is created using Blender SceneCity to facilitate network training. SOLID-Net outperforms existing methods by integrating shading constraints, producing high-frequency local lighting estimation, and providing a spatially-varying outdoor lighting dataset with ground truth labels.