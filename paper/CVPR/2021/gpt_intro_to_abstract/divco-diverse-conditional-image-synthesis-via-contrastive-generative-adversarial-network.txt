Generative adversarial networks (GANs) have shown great potential in capturing complex distributions and generating high-dimensional samples. The introduction of conditional generative adversarial networks (cGANs) further emphasizes the importance of preserving conditional information in image synthesis. However, existing applications of cGANs suffer from the problem of mode collapse, where the generated samples are not diverse enough. Previous approaches, such as latent regression loss and mode seeking loss, have tried to address this issue but have been unable to provide satisfactory results. In this paper, we propose a novel approach called latent-augmented contrastive learning to enhance generation diversity in cGANs. We adapt contrastive learning, commonly used in self-supervised representation learning tasks, to incorporate the relations between generated images and their input latent codes. By considering both "positive" and "negative" relations in a contrastive manner, we ensure that images with similar latent codes are more similar in the generated space, while images with distinct latent codes are pushed further apart. Our proposed approach alleviates the problem of mode collapse and improves the performance of diverse image synthesis.Our contributions can be summarized as follows: 1. We introduce contrastive learning in cGANs to promote diverse image synthesis. This approach, called Div-Co learning, can be easily integrated into existing cGAN frameworks.2. We propose a latent-augmented contrastive loss that discriminates the latent representations of generated samples. This effectively addresses the issue of mode collapse in cGANs.3. Extensive experiments in different conditional generation tasks demonstrate the effectiveness of our proposed method, showing improvements in generation diversity without sacrificing visual quality.Overall, our approach provides a solution to the mode collapse problem in cGANs and enhances the diversity of generated images in various conditional generation tasks.