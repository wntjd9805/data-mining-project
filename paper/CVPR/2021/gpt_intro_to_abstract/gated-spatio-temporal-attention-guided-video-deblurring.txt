Video deblurring is a challenging problem in computer vision and graphics, as it involves predicting sharp frames from blurry video sequences. Blurriness in videos can be caused by factors such as camera shake, object motion, and depth variations. Previous methods for video deblurring have focused on motion compensation and alignment techniques to aggregate information from neighboring frames. However, these methods have disadvantages such as introducing extra parameters and incorrect alignment, which can lead to artifacts.In this paper, we propose a new approach for video deblurring that addresses two critical aspects: how to effectively gather spatio-temporal information and where to gather this information. We introduce a non-local self-attention module that computes correlations between pixels within and across frames, without the need for alignment. This module allows for effective spatio-temporal fusion of information, enhancing the reconstruction performance.Additionally, we explore the problem of finding the most relevant frames to use for deblurring. Instead of relying on a fixed neighborhood of frames, we propose a reinforcement learning-based frame selection network that dynamically decides which frames to utilize on a per-frame basis. This approach significantly improves restoration performance by adaptively selecting key frames with the most relevant information.Our contributions include the development of a factorized spatio-temporal self-attention mechanism that is more efficient and memory-friendly than existing non-local blocks. We also present the first approach for finding key frames with relevant information for video deblurring, which significantly enhances restoration performance. Extensive experiments on video deblurring benchmarks demonstrate the state-of-the-art accuracy and interpretability achieved by our architecture.