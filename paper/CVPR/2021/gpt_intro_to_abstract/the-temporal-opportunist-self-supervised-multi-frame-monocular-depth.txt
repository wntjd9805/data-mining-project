This paper introduces a novel self-supervised multi-frame depth estimation model called ManyDepth. The model combines the strengths of monocular and multi-view depth estimation by utilizing multiple frames at test time, when they are available. The paper addresses the challenges caused by moving objects, scale ambiguity, and static cameras, proposing efficient losses and training solutions to overcome these issues. Furthermore, an adaptive cost volume is introduced to overcome the scale ambiguity arising from self-supervised training on monocular sequences. The ManyDepth model outperforms existing single and multi-frame approaches on the KITTI and Cityscapes datasets.