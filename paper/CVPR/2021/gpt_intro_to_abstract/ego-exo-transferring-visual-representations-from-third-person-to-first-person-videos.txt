Egocentric video captured by wearable cameras has gained significant research interest due to its unique perspective into human behavior. This type of video offers distinct characteristics, such as encoded egocentric motion patterns and a focus on hands, objects, and faces. However, these unique properties pose challenges for video understanding, particularly in terms of dataset scale and diversity. Current egocentric video datasets are small and lack diversity when compared to third-person datasets. On the other hand, using third-person video for pre-training video models ignores the unique properties of egocentric video and creates a domain mismatch. Previous attempts to bridge this domain gap require paired egocentric and third-person videos, limiting their scope. In this paper, we propose a feature learning approach that utilizes latent signals in third-person video to guide the learning of egocentric video models. We introduce ego-inspired tasks that capture egocentric-specific properties, and incorporate these tasks as knowledge-distillation losses during training. Our approach allows video models to benefit from large amounts of labeled third-person training data while embedding egocentric signals. Our experiments demonstrate that our framework achieves strong egocentric feature representations and outperforms existing methods in egocentric video tasks.