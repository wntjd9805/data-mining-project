Machine learning (ML) and deep learning require large amounts of training data, making the resulting ML models valuable intellectual property. These models are often deployed as a service via APIs or on end-user devices, making their predictions accessible to customers. However, this also creates a risk as adversaries can leverage the model's predictions to steal its knowledge. This threat, known as model extraction attacks, can allow adversaries to use the stolen model for monetary gains or mount further attacks.Model extraction attacks involve the adversary querying the victim model using samples from a surrogate dataset with similarity to the original training set. The victim's response may be limited to the most-likely label or include confidence values for different class labels. The number of queries, or query complexity, is an important consideration for the adversary.In this work, we demonstrate that the success of current model extraction techniques depends on the similarity between the surrogate distribution and the victim's proprietary training distribution. To address this, we propose data-free model extraction (DFME) techniques that do not require knowledge of the training data distribution. We leverage recent advances in data-free knowledge distillation to synthesize queries that maximize disagreement between the victim and extracted models. We introduce a new loss function to quantify this disagreement and propose an approach for training the generator without access to the victim model's gradients.Our main contributions include demonstrating the importance of the surrogate dataset's distribution, proposing DFME for extracting ML models without access to training data, presenting a method for recovering per-example logits from ML model outputs, and validating our DFME technique on the SVHN and CIFAR10 datasets with high extraction accuracy. We also conduct an ablation study to highlight the effectiveness of our approach.