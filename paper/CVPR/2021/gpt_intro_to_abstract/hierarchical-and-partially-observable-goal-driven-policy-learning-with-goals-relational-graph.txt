The goal of goal-driven visual navigation is for an intelligent agent to navigate to a specified goal in an unknown environment. This capability is essential for various tasks in computer science. In this paper, we present a novel approach called Hierarchical Reinforcement Learning with Goals Relational Graph (HRL-GRG) to tackle goal-driven visual navigation. We formulate the task as a partially observable goal-driven task and introduce a 10-tuple to represent it. Traditional reinforcement learning (RL) algorithms are limited to fully observed state spaces and fixed goal states, which are not applicable in real-world scenarios. In real-world applications like visual navigation, the agent's sensory inputs only capture local information and the goals may change, requiring a goal-adaptive system. Our HRL-GRG model incorporates a Goals Relational Graph (GRG) to learn goal relations from training data dynamically. It decomposes the task into a high-level sub-goal selection task and a low-level fully observable goal-driven task. We show the effectiveness of our method through experiments in a grid-world domain and robotic object search tasks. Our HRL-GRG model outperforms other baseline approaches and exhibits superior performance in terms of generalization ability.