Semantic segmentation, the task of assigning semantic labels to every pixel in an image, has seen significant advancements with the use of convolutional neural networks (CNNs). However, the success of these deep methods relies on densely annotated images, which are laborious and expensive to obtain. To address this issue, simulators have been used to generate large-scale datasets with dense annotations for semantic segmentation. However, there is still a significant domain gap between synthetic datasets and real scenes, leading to poor performance of networks trained on synthetic data in real target scenes. To mitigate this gap, various approaches for unsupervised domain adaptation (UDA) have been proposed, but most of them focus on a single source domain. In this paper, we propose a multi-source domain adaptation approach for semantic segmentation that exploits multiple source domains to improve adaptation. We propose a collaborative learning and image translation method to address the appearance discrepancy between source domains and the target domain. We also propose two collaborative learning strategies to explore essential and domain-invariant semantic contexts across different domains. Our method significantly outperforms state-of-the-art single-source and multi-source UDA methods, demonstrating the effectiveness of image translation and domain-invariant feature learning. Our method can be trained in both end-to-end and stage-wise manners.