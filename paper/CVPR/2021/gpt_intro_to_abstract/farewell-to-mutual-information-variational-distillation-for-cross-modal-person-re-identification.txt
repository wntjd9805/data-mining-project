The Information Bottleneck (IB) has been instrumental in the development of machine perception systems across various domains. It transforms raw observations into lower-dimensional representations, making it applicable to deep neural networks (DNNs). However, the estimation of mutual information, a crucial aspect of IB, remains a challenging problem. Traditional approaches rely on non-parametric estimators, limiting their application to known probability distributions or discrete variables. Some works have introduced trainable parametric neural estimators but with limited scalability. Additionally, conventional IB optimization struggles to strike a balance between a concise representation and accurate prediction. In this paper, we propose Variational Self-Distillation (VSD) as a new strategy for the information bottleneck. VSD effectively preserves task-relevant information while discarding task-irrelevant distractors without explicitly estimating mutual information. We employ variational inference to provide an analytical solution to VSD, enabling the network to capture intrinsic correlations between data and labels. We further extend VSD to multi-view learning, introducing Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML). These strategies enhance the robustness of the information bottleneck to view-changes by eliminating view-specific and task-irrelevant information without relying on strong prior assumptions. By implementing VSD, VCD, and VML as training losses, they mutually benefit from each other, leading to improved performance. Experimental evaluations conducted on a cross-modal person re-identification task demonstrate the effectiveness, robustness, and superior performance of our approaches compared to state-of-the-art methods. Our contributions include the design of VSD for representation learning, a scalable analytical solution for fitting mutual information, and the extension of our approach to multi-view representation learning, improving the robustness to view-changes.