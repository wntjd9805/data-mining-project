In this paper, we address the challenge of cross-modal video moment retrieval, which aims to retrieve a specific video moment that matches a given query sentence. Traditional video retrieval methods are not suitable for fine-grained retrieval, so we propose a multi-modal relational graph (MMRG) framework to comprehensively investigate this task. The MMRG framework involves constructing separate graphs for visual and textual objects, with relations among objects treated as nodes to address ambiguous relation definitions. We also introduce a customized multi-task pre-training strategy in visual relation understanding to enhance visual representation. Additionally, we utilize graph matching and boundary regression to regularize the cross-modal retrieval process. Our contributions include being the first to investigate the interactions among visual and textual objects for cross-modal video moment retrieval, proposing the MMRG framework for modeling semantic consistency and object interactions, and demonstrating the effectiveness of our method through extensive experiments on two datasets. We have also released our dataset and implementation to facilitate future research in this area.