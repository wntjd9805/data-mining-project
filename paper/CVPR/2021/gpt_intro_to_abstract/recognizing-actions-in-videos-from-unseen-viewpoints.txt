Activity recognition with convolutional neural networks (CNNs) has been successful in recognizing actions when provided diverse labeled data. However, a major limitation of these CNNs is their inability to recognize actions outside of the training data distribution, especially for unseen classes. This paper focuses on the problem of unseen viewpoint activity recognition, where the actions are the same but occur from different camera angles. The paper presents an example to illustrate this problem, using the Human3.6M dataset. It shows that a trained CNN fails to recognize actions when tested from different camera views. The paper highlights that humans are able to recognize these actions regardless of viewpoint, suggesting the need for invariant representations of actions. This problem is frequently encountered in real data, where existing datasets provide videos in multiple viewpoints. However, it is impractical to create datasets covering all possible viewpoints. The paper explores different approaches to address this problem, including training on large-scale video datasets and using 3D human pose information. It introduces a challenging new dataset for unseen viewpoint recognition in unseen environments, building on the MLB-YouTube dataset. The contributions of this paper are a computationally efficient geometric-based layer for learning view invariant representation, thorough evaluation of multiple approaches to unseen viewpoint action recognition, and the introduction of a new dataset for unseen viewpoint recognition.