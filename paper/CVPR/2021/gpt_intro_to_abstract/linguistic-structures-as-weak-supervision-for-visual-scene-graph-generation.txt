Recognizing visual entities and understanding their relationships are fundamental problems in computer vision. Object detection (OD) and visual relation detection (VRD) are tasks that aim to detect entities and classify their relationships, while scene graph generation (SGGen) requires both entity detection and relation classification. However, the types of supervision used in previous methods only capture contextual information in an impoverished way. These methods rely on subject-predicate-object triplets with bounding boxes or image-level triplets, limiting the capture of global context in scene graph generation.To address this limitation, we propose a new approach that leverages captions as supervision for scene graph generation. By parsing captions, we can utilize the available image-text data on the internet, which provides a rich source of linguistic structure and relational information. Captions capture global context, enabling the linking of multiple triplets and more accurate localization of entities. Additionally, captions are advantageous in terms of cost, as they are naturally provided by humans when uploading visual content. However, caption supervision also contains noise and presents challenges, such as incomplete coverage of objects and difficulty in localizing mentioned content.Our approach overcomes these challenges by modeling context in two ways. First, we extract information from captions beyond subject-predicate-object entities, such as attributes, which improves the accuracy of representations and localization. Second, we use the visuo-linguistic context to reason about common-sense relationships within each triplet, preventing the generation of nonsensical triplets. To mitigate the noise in captions, we employ an iterative detection method that prunes spurious relations between caption words and image regions.We validate our approach in two settings. Firstly, we construct a ground-truth triplet graph using certain overlap criteria, demonstrating significant improvements over previous methods. Secondly, we evaluate our method using actual captions, achieving strong results and competitive performance compared to methods using clean image-level supervision.In summary, our contributions include a new mechanism for scene graph generation using captions as weak supervision, contextualized embeddings for subject/object entities based on linguistic structures, joint classification and localization of subject, object, and predicate within a triplet, and the application of weakly-supervised object detection techniques to improve scene graph generation.