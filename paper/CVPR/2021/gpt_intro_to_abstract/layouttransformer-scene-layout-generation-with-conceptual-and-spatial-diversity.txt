Text-to-image generation, which converts text descriptions into realistic images, has garnered attention in the computer vision and deep learning communities due to its potential applications in fields such as computer-aided design, art generation, and image editing. The main challenge in text-to-image generation is producing high-quality and plausible images that maintain semantic relationships across different objects. Bridging the gap between semantic and perceptual information requires collaboration between researchers in computer vision and machine learning. This paper proposes a novel approach called the LayoutTransformer Network (LT-Net) to address these challenges. LT-Net takes scene-graph inputs and utilizes a masked language model based on BERT to exploit implicit objects and relationships. This enables the generation of conceptually related yet diverse outputs. The transformer decoder in LT-Net sequentially generates bounding boxes for each object/relation, and Gaussian mixture models are used to model the distribution of these components, allowing for spatially diverse layouts. Additionally, a visual-textual co-attention module refines the layout output by considering both semantic and spatial representations. The contributions of this research include the introduction of LT-Net, which produces conceptually and spatially diverse layouts, and the utilization of a masked language model and transformer decoder that enable the exploitation of implicit objects and relations for increased diversity and realism. The proposed framework shows promising results in the field of text-to-image generation.