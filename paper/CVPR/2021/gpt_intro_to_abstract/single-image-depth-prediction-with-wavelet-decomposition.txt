Single-image depth estimation is a crucial task in various real-time applications such as robotics, autonomous driving, and augmented reality. Efficiency in prediction time is particularly important in resource-constrained areas. Current neural network models for depth estimation predominantly utilize U-Net architectures with skip connections, aiming to improve depth accuracy without focusing on efficiency. Previous attempts to address efficiency borrowed techniques from "efficient network" approaches, utilizing standard convolutions throughout the network. In this work, inspired by the sparse representations achieved through wavelet decomposition, we propose a novel network representation called Wavelet-Monodepth for more efficient depth estimation. We exploit the observation that depth images in man-made environments often consist of piece-wise flat regions with occasional jumps in depth. This structure is well-suited for wavelets, where low-frequency components capture the overall scene structure, while high-frequency components represent the jumps in depth. By leveraging the sparsity of high-frequency components, we can significantly reduce computation while maintaining high-quality depth estimation. We introduce a single-image depth estimation network that predicts wavelet coefficients, trained with a self-supervised loss on the final depth signal. Our experiments on NYU and KITTI datasets demonstrate the ability of our approach to trade off depth accuracy and runtime computation effectively.