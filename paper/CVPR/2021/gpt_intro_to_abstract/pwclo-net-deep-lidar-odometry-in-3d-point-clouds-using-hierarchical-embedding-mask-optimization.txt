The visual/LiDAR odometry plays a crucial role in autonomous driving, providing the relative pose transformation between consecutive frames and serving as the foundation for decision-making in mobile robots. Learning-based odometry methods have recently demonstrated higher accuracy compared to traditional methods based on hand-crafted features. However, most of these methods focus on 2D visual odometry or utilize 2D projections of LiDAR, leaving the LiDAR odometry in 3D point clouds relatively unexplored. This paper aims to directly estimate LiDAR odometry using raw 3D point clouds. There are three main challenges: the lack of precise corresponding point pairs between frames, occlusion and limited LiDAR resolution affecting point visibility, and the uncertainty of dynamic object points for pose estimation. To address these challenges, the paper introduces a cost volume approach for obtaining a soft correspondence between consecutive frames, an embedding mask for weighting local motion patterns to regress the overall pose, and a Point Feature Pyramid, Pose Warping, and Cost Volume (PWC) structure for capturing large motion and refining pose estimation. The proposed PWCLO-Net framework is fully differentiable and enables end-to-end optimization. Experimental results on the KITTI odometry dataset demonstrate the superiority of the proposed method compared to recent learning-based LiDAR odometry approaches and even outperform the geometry-based LOAM method on most sequences.