Deep neural networks (DNNs) have shown great power in complex decision-making tasks, such as image classification. However, they are known for their sensitivity to changes in training conditions and lack of interpretability. This has led to the need for hyperparameter optimization (HPO), which can be time-consuming and costly. Linearizing DNN models has been explored as a way to improve robustness and interpretability, but previous attempts have not yielded significant improvements in practice. In this paper, we propose a method called Linear Quadratic Fine-Tuning (LQF) that achieves comparable performance to non-linear fine-tuning (NLFT) on real-world image classification tasks, while enjoying the benefits of linear-quadratic optimization. LQF eliminates the need for hyperparameter optimization and allows for the prediction of the effect of individual training samples on the trained classifier. It also offers the flexibility to incorporate linear constraints during training. Our experimental results show that LQF performs well, particularly in low-data scenarios which are common in real applications. The key techniques employed in LQF, such as using mean-squared error loss, Leaky-ReLU activation function, and Kronecker factorization for preconditioning, have individually shown limited improvements in non-linear models but have a significant impact when combined in linearized models.