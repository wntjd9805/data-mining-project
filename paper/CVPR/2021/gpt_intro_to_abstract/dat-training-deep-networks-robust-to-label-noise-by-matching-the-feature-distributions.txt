Deep networks have achieved impressive performance in various vision problems, thanks to large-scale annotation datasets like ImageNet. However, manual annotation is expensive and time-consuming, leading researchers to explore alternative methods such as crowdsourcing or crawling websites for faster and cheaper annotation. However, these methods introduce noisy labels, and as deep networks become more complex, they have a higher capacity to overfit these noisy labels. Thus, there is a need for robust learning methods against noisy labels.Existing methods either model noise probability or obtain clean labels through the memorization effect. However, these methods have limitations. Noise modeling methods only work for class-level noisy labels due to the conditional independent assumption, which states that noisy labels are only related to ground-truth labels. The memorization effect, on the other hand, relies on simple pattern learning and gradually correcting noisy samples but is not effective on real-world noisy datasets.To address these limitations, this paper proposes a novel method called Discrepant Adversarial Training (DAT), which focuses on processing noisy labels in the feature distribution instead of the label distribution. Unlike existing approaches, DAT can handle both class-level and instance-level noisy labels and captures differences in the feature space even if the noisy distribution is close to the original distribution. The paper theoretically proves that matching the feature distribution can tackle the label-noise problem and introduces DAT as an adversarial training method.Additionally, the paper proposes a new metric called h△H-divergence for calculating distribution divergence, which has a tighter upper bound for handling noisy labels compared to other metrics. DAT can also be used as a regularization method to prevent overfitting noisy labels even without clean data.The contributions of this paper are:1. The theoretical proof of the effectiveness of matching feature distributions in dealing with label-noise problems.2. The introduction of the DAT method, which performs adversarial training to enforce prominent feature extraction and achieve high-quality results.3. The proposal of the h△H-divergence metric for calculating distribution divergence.4. The extension of DAT to handle noisy labels without clean data using a trick to prevent overfitting.DAT achieves state-of-the-art accuracy on both synthetic noisy datasets with class-level noisy labels and real-world noisy datasets with instance-level noisy labels. Overall, this paper presents a promising methodology for processing noisy labels in the feature space, providing new insights and advancements in training deep networks with noisy data.