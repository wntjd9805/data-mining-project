Transfer learning is a valuable approach for improving performance on target tasks with limited labeled data by leveraging knowledge from related source tasks. Quantifying the transferability of this knowledge is crucial for effective transfer learning. Existing transferability metrics either rely on expensive retraining of the source model or make strict assumptions about the data. In this paper, we focus on the challenging cross-domain cross-task setting, where both domain and task differences exist between the source and target tasks. We propose a novel transferability metric called the Optimal Transport based Conditional Entropy (OTCE) score, which efficiently estimates transfer performance by learning the domain difference and task difference in a unified framework. The OTCE score utilizes the Optimal Transport problem to measure domain difference and estimate joint probabilities between source and target samples to capture task difference. We also develop a linear model that correlates transfer accuracy with domain and task differences, facilitating the decomposition of transferability into different factors. Extensive experiments on large cross-domain datasets demonstrate that the OTCE score outperforms state-of-the-art metrics, showing an average gain of 21% in predicting transfer performance. We also explore the applications of the OTCE score in source model selection and multi-source feature fusion. Our contributions include the analytical investigation of transferability estimation under the cross-domain cross-task setting, the proposal of the OTCE score, and its superior performance in predicting transfer performance.