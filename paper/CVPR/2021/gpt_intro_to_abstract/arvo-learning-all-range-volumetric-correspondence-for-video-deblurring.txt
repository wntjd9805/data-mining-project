Hand-held devices are commonly used to capture videos of dynamic scenes, but they often suffer from blurring due to high-speed object movements and camera shakes. Video deblurring aims to improve the video quality by restoring sharp frames from blurry sequences. Current video deblurring methods rely on spatially aligned neighboring frames to remove blurs, but constructing visual correspondence between frames is challenging due to motion. Existing approaches use homography or optical flows to address misalignment, but these methods struggle with fast object motions, occlusions, and depth variations. To address these limitations, we propose an implicit approach that estimates pairwise image correspondence in the feature space. By matching pixel-wise correlations of visual features, we can better capture visual dependencies and handle blurs caused by fast motions. Additionally, we introduce a pyramid of correlation volumes to achieve feature matching at different spatial scales, while maintaining fine-grained visual details. To optimize our model and improve temporal consistency, we apply a generative adversarial paradigm with a new temporal consistency loss. We train our model progressively, incorporating restored frames from previous stages to gradually restore details. Furthermore, we evaluate our method on the widely-adopted DVD dataset and introduce a new high-frame-rate dataset called HFR-DVD, which features sharper frames and more realistic motion blurs. Our experimental results demonstrate the effectiveness of the proposed feature correlation methods for video deblurring on both datasets. Overall, our contributions include a novel video deblurring method that constructs spatial correspondence in the feature space, a correlative aggregation module to enhance reference frame features, an adversarial loss for optimizing temporal consistency, benchmarking on HFR-DVD dataset, and establishing a new state-of-the-art for video deblurring.