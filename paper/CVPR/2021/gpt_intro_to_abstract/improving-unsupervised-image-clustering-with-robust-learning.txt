Unsupervised clustering is an important task in computer vision for identifying class memberships of images without using labels. Existing deep learning-based algorithms often rely on distance in feature space to assign data points to classes. However, training without guidance can lead to trivial solutions based on low-level visual traits. Alternative objectives have been proposed to guide training indirectly, but they can still result in overconfident predictions. This paper introduces a novel robust learning training method called RUC (Robust learning for Unsupervised Clustering) to address these challenges. RUC works alongside existing clustering models to filter out noisy data and update the model's misaligned knowledge. It employs label smoothing, co-training, and loss correction techniques to reduce wrong gradient signals from unclean labels and prevent overconfidence. RUC comprises two key components: extracting clean samples and retraining the model with the refined dataset. The paper proposes confidence-based, metric-based, and hybrid strategies to filter misclassified pseudo-labels. The retraining process utilizes the MixMatch semi-supervised learning technique and leverages label smoothing to denoise the labels. A co-training architecture with two networks is used to mitigate noise accumulation from unclean samples. Experimental evaluations on CIFAR-10, CIFAR-20, STL-10, and ImageNet-50 datasets demonstrate that incorporating RUC into existing clustering models significantly improves accuracy and robustness. The contributions of this paper include introducing the RUC algorithm, demonstrating its performance improvement, conducting an ablation study to highlight the importance of its components, and showcasing its robustness against adversarial noise. The implementation details and codes are available at the provided GitHub repository.