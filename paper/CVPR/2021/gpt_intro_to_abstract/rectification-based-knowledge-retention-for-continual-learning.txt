Deep learning models have achieved remarkable success in solving real-world problems, outperforming humans in many tasks. However, these models suffer from the issue of catastrophic forgetting, where their performance on previously seen classes or tasks deteriorates when trained on new data without access to the original training data. Humans, on the other hand, can continually learn new classes without losing their previously gained knowledge. Additionally, deep learning models struggle with the zero-shot learning problem, where their performance significantly degrades when tested on classes not seen during training. This paper presents a solution to the task incremental learning problem in both the zero-shot and non zero-shot settings. The task incremental learning problem involves training the model on one task at a time, with each task consisting of a set of non-overlapping classes. When a new task becomes available, the model must prevent forgetting the knowledge acquired from previous tasks. The proposed approach, Rectification-based Knowledge Retention (RKR), learns weight rectifications to adapt the network weights for a new task. This allows the network to quickly adapt to the new task by applying these weight rectifications. The authors also introduce affine transformations to improve network adaptation. Experimental results demonstrate the effectiveness of RKR in both the zero-shot and non zero-shot settings. The proposed approach significantly outperforms existing state-of-the-art methods while introducing minimal additional parameters during training, resulting in minimal model size growth.