Obtaining lighting information is a fundamental challenge in computer vision and graphics, with applications in various vision tasks such as photometric stereo, virtual object compositing, and scene understanding. Previous research has focused on estimating or calibrating lighting using a single image of an illuminated object or scene. However, these approaches often assume a distant light source and struggle to handle localized light sources in indoor scenes. Recent advances have addressed this issue by estimating spatially-varying lighting or predicting 3D positions of light sources. However, these methods still suffer from the problem of intensity-distance ambiguity, where the inferred light distance may not be unique due to changes in light source intensity.In this paper, we propose leveraging event cameras to address the intensity-distance ambiguity in indoor lighting estimation. Event cameras offer advantages such as high temporal resolution, high dynamic range, and sensitivity to small intensity changes. We introduce a novel setup using an event camera to capture intensity changes on a purely diffuse sphere in a dark room when the light is turned on. Through analysis, we demonstrate that the intensity-distance ambiguity is hardly present in event streams. We develop an optimization-based method and a learning-based method to estimate the distance of the light source using event streams as input. We evaluate our methods on two testing datasets and show that they not only achieve superior performance for lighting estimation but also alleviate the intensity-distance ambiguity. Additionally, we provide a byproduct application for classifying the types of light sources.Our contributions include the introduction of a novel setup that alleviates the intensity-distance ambiguity in indoor lighting estimation, the proposal of well-posed methods for estimating lighting distance based on our setup, and the demonstration of superior performance and reduced ambiguity in lighting estimation.