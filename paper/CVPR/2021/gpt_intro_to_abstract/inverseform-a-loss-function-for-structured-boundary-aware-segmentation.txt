Semantic segmentation is an important computer vision task with various practical applications. Recent advancements in deep learning models, such as fully convolutional networks (FCN), have significantly improved the performance of semantic segmentation. Researchers have explored different approaches to enhance semantic segmentation, including the integration of multi-resolution and hierarchical feature maps, as well as the utilization of boundary information. Multi-task learning (MTL) frameworks have also been investigated to optimize semantic segmentation together with other related tasks, such as boundary detection and depth estimation. This paper focuses on the exploration of boundary information for semantic segmentation. The authors introduce a boundary distance-based measure called InverseForm, which addresses the limitation of the commonly used weighted cross-entropy loss for boundary detection. They propose a boundary-aware segmentation scheme that integrates the InverseForm loss into existing pixel-based losses, allowing for the capture of boundary transformations. This scheme can be applied to any segmentation model and is flexible enough to fit into MTL frameworks. Comprehensive experiments are conducted on benchmark datasets, including Cityscapes and PASCAL-Context, to demonstrate the effectiveness of the proposed approach. The contributions of this work include the introduction of InverseForm as a more accurate measure for boundary transforms, a flexible scheme that can be integrated into various segmentation models without additional inference cost, and the consistent outperformance of the proposed method compared to baselines and state-of-the-art approaches in both single-task and multi-task settings.