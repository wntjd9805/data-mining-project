Humans have the capability to continuously acquire new information while retaining previously learned knowledge, but this ability is difficult for deep neural networks (DNNs) to achieve. Incremental learning (IL) or continual learning aims to design machine learning systems that can assimilate new information without forgetting past knowledge. In class-incremental learning (class-IL), models undergo rounds of training sessions to accumulate knowledge for a specific objective, such as classification. However, models trained on previously seen tasks often suffer from catastrophic forgetting, where there is a drastic drop in performance after training incrementally on different tasks.In recent years, several continual learning algorithms for DNNs have been proposed to mitigate the effect of catastrophic forgetting. These algorithms can be categorized into regularization-based methods involving knowledge distillation, exemplar-based methods that keep partial copies of data from previously learned tasks, and modified architectures utilizing task-specific network components. However, the performance of these algorithms varies across different datasets, and their ability to address catastrophic forgetting is insufficient due to factors such as domain shift and class imbalance.The main challenge in deep continual learning is the "black box" nature of DNNs, as their complexity makes it difficult for humans to understand the mapping from data input to prediction and control the parameters to fit new data without losing knowledge of old data. In this work, we propose a novel approach to incremental learning by utilizing a "white box" DNN architecture called ReduNet, which is derived from the principle of rate reduction. ReduNet allows for explicit computation of each layer in a forward-propagation manner, with precise statistical interpretations for each parameter, making it suitable for incremental learning.We introduce a new incremental learning algorithm using ReduNet to demonstrate the power and scalability of designing more interpretable networks for continual learning. We show that a ReduNet trained incrementally can be equivalent to a jointly trained network where all data, both new and old, is available at training time. Additionally, we observe that ReduNet outperforms current continual DNN approaches on MNIST and CIFAR-10 datasets.