This paper introduces M3P, a Multitask Multilingual Multimodal Pre-trained model, aimed at learning universal representations that can map objects from different modalities and texts expressed in different languages into a common semantic space. The paper addresses the challenge of extending pre-trained models to multilingual-multimodal scenarios by utilizing Multimodal Code-switched Training (MCT) to enforce explicit alignments between images and non-English languages. M3P achieves new state-of-the-art results in multi-lingual image-text retrieval tasks and provides insights on the effectiveness of MCT and each pre-training task. The proposed model combines multilingual pre-training and multimodal pre-training, offering a unified framework for improved performance in multilingual-multimodal scenarios.