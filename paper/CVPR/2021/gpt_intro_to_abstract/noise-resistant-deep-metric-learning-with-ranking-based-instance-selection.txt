Noisy labels in training data can significantly degrade the performance of machine learning models. The traditional approach of manual inspection and correction of labels is time-consuming and not scalable for large datasets. Thus, there is a need for training techniques that are robust to incorrect labels. While previous work has focused on noise-resistant neural networks for image classification, little research effort has been devoted to noise-resistant deep metric learning (DML). DML aims to learn a distance metric that maps similar data points together and dissimilar data points apart. This has applications in image retrieval, landmark identification, and self-supervised learning. Pair-based loss functions are commonly used in DML, and larger batch sizes have been shown to improve performance. However, indiscriminate use of all samples can lower performance in the presence of substantial noise. To address these challenges, we propose a noise-resistant DML algorithm called Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM computes the probability that a label is clean based on similarities between data points, using features from previous training iterations. Data points with high probability are inserted into a memory bank for subsequent model updates. Additionally, we introduce the Small Cluster noise model to mimic open-set noise and evaluate PRISM's performance on synthetic and real datasets. The experimental results demonstrate that PRISM outperforms existing DML and noise-resistant training techniques under various noise conditions. Furthermore, we introduce an acceleration trick that speeds up the algorithm significantly on the SOP dataset. The code and data for PRISM are available at the given GitHub link.