Abstract:Digital 3D scene representations of interiors are essential in various emerging applications. However, the availability of texture-mapped 3D scene models for built interiors is limited. In this paper, we propose a novel approach for 3D interior digitization using residential floorplans and photos as input. We introduce the Plan2Scene task, which involves converting a floorplan and a set of photos into a textured 3D scene model. We focus on texturing architectural surfaces, a challenging task due to the lack of camera poses for the photos, photometric calibration difficulties, and limited photo coverage. Our key idea is to model the architectural surfaces and infer appropriate textures for each surface using an encoder-decoder architecture. We handle unobserved surfaces with a graph neural network that learns inter/intra-room consistency. Additionally, we contribute by extending an existing floorplan and photo database and curating datasets of textures for training our texture synthesis method. Through evaluations and a user study, we demonstrate that our approach outperforms baseline methods. We release all our code, data, and pretrained models to the community.