This paper introduces a deep learning approach to synthesize 3D talking faces driven by an audio speech signal. The system uses speaker-specific videos to train the model in a data-efficient manner by employing 3D facial tracking. The resulting system has applications in video editing, dubbing, personalized avatars in gaming and CGI, as well as multimedia communication compression. The paper discusses existing methods for talking head synthesis and highlights the limitations of these approaches. The authors propose a personalized model approach and emphasize the importance of 3D pose and lighting normalization to achieve higher visual fidelity in the synthesized faces. The model predicts 3D talking faces instead of just 2D images, allowing for broader applicability. The paper also presents technical contributions, including a method to decouple 3D pose, geometry, texture, and lighting, a novel algorithm for normalizing facial lighting, and an auto-regressive texture prediction model for smooth video synthesis. The authors evaluate their method using human ratings and objective metrics and show that it outperforms contemporary baselines in terms of realism, lip-sync, and visual quality scores.