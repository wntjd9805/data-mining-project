Automated medical report generation is in demand to reduce the workload and diagnostic errors of radiologists. This research is closely related to the broader topic of image captioning in computer vision, where free-form text descriptions are generated to describe the content of images. However, directly applying image captioning methods to medical report generation often results in performance decline. Radiographic images have fine-grained visual differences that need to be narrated, requiring tighter visual and text representations.To address this, this paper proposes a self-boosting framework for radiographic report generation. It introduces an auxiliary task of image-text matching to learn strongly correlated visual and text features. These features improve the fine-grained recognition task in report generation. The framework tightly couples the image-text matching and report generation tasks, allowing them to mutually boost each other through cooperative interactions. The network is gradually self-boosted, resulting in improved report generation performance.The contributions of this work are as follows: 1. Utilizing an auxiliary task of image-text matching to learn text-correlated visual features without additional annotations or external datasets/knowledge.2. Introducing a self-boosting framework that leverages the cooperation between image-text matching and report generation to progressively improve both tasks.3. Learning an effective text feature extractor using image-text matching, which is used to evaluate feature similarity between generated reports and ground truth, further promoting report generation.4. Demonstrating promising performance on two benchmarks, generating reports from classic chest X-ray images and CT images with COVID-19, outperforming state-of-the-art methods in both image captioning and medical report generation.