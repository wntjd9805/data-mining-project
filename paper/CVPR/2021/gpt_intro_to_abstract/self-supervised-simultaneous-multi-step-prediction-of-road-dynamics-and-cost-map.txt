Conventional autonomous driving (AD) systems consist of multiple modules responsible for object detection, prediction, motion planning, and control. While this approach allows for interpretability, it lacks the ability to propagate errors and uncertainties between modules. Additionally, manual design and tuning of cost functions limit the system's capability to handle complex driving scenarios. As an alternative, some systems have been proposed that use raw sensory input to directly generate control commands, eliminating the need for a cost function. However, this monolithic approach lacks interpretability and is unsuitable for safety-critical real-world deployment.In this paper, the authors propose a new approach for autonomous driving that enables meaningful interpretation while avoiding manual data labeling and cost function design. The proposed approach utilizes a novel architecture for learning-based space-time Cost Map Estimation (CME). This architecture encodes high-dimensional Occupancy Grid Maps (OGMs) and predicts OGMs as well as estimates the cost map for multiple future steps. The resulting space-time cost maps can be used by a motion planner to rank possible future trajectories.To train the system, the authors decompose the CME objective into two parts. The first part incorporates prior knowledge about the environment, while the second part utilizes an auxiliary imitation task that forces the model to predict the expert's intention and trajectory based on the estimated cost maps. This combination of training objectives enables the model to make accurate predictions of the cost maps.The main contributions of this paper include the proposed architecture for estimating cost maps in a fully self-supervised manner, specific training objectives that incorporate environment constraints and expert behavior, and empirical demonstrations of the effectiveness of the approach through multiple experiments and generalization tests.