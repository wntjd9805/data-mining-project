In this paper, we address the challenge of class-incremental learning in scenarios where new class-specific data is obtained incrementally over time. We focus on few-shot class-incremental learning, where new tasks come with only a few examples per class, making the learning process even more challenging. Traditional approaches such as knowledge distillation struggle to handle the class imbalance in few-shot scenarios and the trade-off between novel and base classes. To overcome these challenges, we propose a semantically-guided knowledge distillation approach for few-shot class-incremental learning. We leverage semantic word vectors, obtained from models like word2vec or GloVe, to provide auxiliary knowledge representation for each class. Inspired by zero-shot learning techniques, we estimate semantic word vectors for input images and measure their similarity with the word vectors of possible class labels. This allows us to predict class scores without adding new trainable weights for novel classes.To further improve alignment between visual and semantic word vectors, we introduce a visual-semantic alignment strategy using automatically assigned superclass annotations. We train multiple embedding modules, each assigned to a superclass, to handle different subsets of classes. An attention module is employed to merge the outputs from multiple embeddings, and a loss function ensures appropriate alignment during training with few-shot instances.Our proposed approach outperforms the current state-of-the-art on datasets like MiniImageNet, CUB200, and CIFAR100, thanks to its combination of auxiliary semantic information from word vectors and knowledge distillation. The contributions of this paper include the semantically-guided knowledge distillation approach, the visual-semantic alignment strategy, and extensive experiments demonstrating the effectiveness of our approach.