Recent developments in supervised learning have focused on obtaining knowledge for known classes. However, problems arise when tested on instances from unseen categories, as the model treats them as known instances, leading to a decline in performance. Open-set recognition has been proposed as a solution to classify known instances while also detecting those from unknown classes. One approach to separating known and unknown instances is through thresholding the output probability. However, deep learning methods tend to produce overconfident predictions, making it difficult to tune the threshold. Additionally, diverse class compositions make it hard to find an optimal threshold that suits all open-set tasks. Several methods have attempted to address this issue by anticipating novel class distributions and calibrating the output. Our proposed method, PROSER, aims to calibrate open-set classifiers by augmenting the closed-set classifier with an extra classifier placeholder that represents the threshold between known and unknown classes. We also generate data placeholders that mimic open-set categories with limited complexity cost. This transformation allows us to adaptively predict the class-specific threshold during testing. Experimental results on various datasets validate the effectiveness of PROSER in detecting unknown instances and recognizing open-set classes. Visualization of decision boundaries demonstrates that PROSER learns adaptive thresholds for different class combinations. The paper concludes with a review of related work, a discussion of the proposed method, experimental results, and a summary.