Video text detection in videos has gained significant attention due to its wide range of applications in video analysis and multimedia information retrieval. Previous methods have made efforts in text detection and tracking, but the task remains challenging due to motion blur and illumination changes. Most existing methods treat text detection and tracking separately, ignoring temporal contexts and information interaction between the two tasks. To address this issue, an end-to-end trainable framework integrating text detection and tracking has been proposed. However, the proposed descriptor based on text appearance is easily influenced by perspective and illumination changes. In contrast, semantic features are robust cues for matching text instances, as the position and category of text characters remain similar despite perspective changes. However, character-level annotations of real datasets are costly. To overcome this, a semantic-aware video text detection framework is proposed, generating character-level annotations directly from word-level annotated real datasets. The framework leverages ConvLSTM to propagate frame-level information and Mask R-CNN with a character center segmentation task to encode position and category as semantic features. Appearance-semantic-geometry descriptors (ASGD) are introduced to robustly represent text instances, enabling text tracking. While character-level annotations are required, a sliding-window based text recognizer is adopted, making the framework easily applicable to multiple languages. The proposed framework is the first to introduce semantic features into video text detection and tracking, achieving state-of-the-art performance on various datasets. The contributions of this paper include a novel end-to-end video text detector, an appearance-semantic-geometry descriptor, and the generation of character-level annotations in a weakly-supervised manner, improving the practicality of the proposed method.