Future trajectory prediction is a crucial challenge in the safe operation of autonomous vehicles that interact with pedestrians, cars, and cyclists. To address this challenge, we propose a cross-modal embedding framework that utilizes multiple sensor data for motion prediction. Our framework leverages the multi-modal sensor data equipped in autonomous vehicles, such as LiDAR scanner, RGB cameras, and radar, to jointly optimize objective functions across different input modalities. At test time, our model takes a single input modality and predicts future motion in the same space as the input. This approach allows us to benefit from the use of multiple input modalities in model training while maintaining the same computational time for trajectory generation. We are the first to employ multi-modal sensor data from a single framework for trajectory prediction, distinguishing our work from existing studies on multi-modal pipelines for scene understanding. Our proposed framework also addresses issues related to computation time and sensor failure by generating alternative prediction solutions using different input modalities. We introduce a regularizer to mitigate the posterior collapse problem of variational autoencoders (VAEs) and predict diverse modes of future motion. We mathematically derive the objective of shared cross-modal embedding and implement it in our framework. Additionally, we propose an interaction graph with a graph-level target, a new evaluation metric for prediction success, and the use of absolute motions in frontal view. Throughout the paper, we use the term "multi-modality" to refer to multiple input sources and multiple variations in prediction outputs.