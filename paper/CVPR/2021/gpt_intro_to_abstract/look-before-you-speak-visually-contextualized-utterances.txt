This paper introduces the concept of developing an intelligent dialogue system that not only emulates human conversation but also predicts and suggests future actions. The goal is to enable humans to interact with systems in free-form natural language, similar to how they communicate with each other. While there has been extensive research in natural language processing for conversational agents, most of these works are limited to linguistic interactions only. In contrast, human interaction in the physical world involves multiple modalities, such as verbal, visual, and haptic cues, which complement each other seamlessly. Incorporating visual context into dialogue systems has been a challenge due to the lack of suitable data. Traditional conversational datasets are typically text-based and do not contain visual information about the surrounding physical environment.To address this challenge, the paper proposes leveraging online videos to learn from naturally co-occurring vision and dialogue in a scalable manner. Certain video domains, such as narrated instructional videos and lifestyle vlogs, are abundant and likely to contain narration explicitly linked to the visual content. The paper introduces a future prediction task, where the goal is to predict the next utterance in an instructional video given both visual and textual contexts. This task is made possible by the availability of high-quality automatic speech recognition (ASR) and freely available paired visual and textual data from online videos.The paper presents a two-stream co-attentional transformer based model that effectively attends to features within each modality and across modalities through lateral self-attention blocks. Experimental results show that incorporating both visual and textual information leads to a significant performance gain over using text alone, and the proposed two-stream co-attentional model outperforms single-stream multimodal models. Furthermore, the model trained on the future prediction task achieves state-of-the-art performance on various video question-answering benchmarks.In summary, the paper formulates a future utterance prediction task that incorporates both dialogue and vision. Freely available online instructional video datasets are repurposed to create training and testing benchmarks for this task. The proposed two-stream multimodal video transformer based architecture effectively attends to words in text and visual objects/scenes to learn visual-dialogue context. The model trained on unlabelled instructional videos also achieves state-of-the-art performance on several downstream vision-language question-answering datasets.