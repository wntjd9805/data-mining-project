Person ReID, the task of identifying a target person from a set of individuals captured by a distributed camera system, heavily relies on clothing appearance textures. However, existing methods face challenges when clothing textures are confusing due to changes in clothing or similar clothing worn by different people. In this paper, we propose modeling discriminative clues beyond clothing textures by incorporating human shape representations. We explore two approaches for learning shape-related features: 2D image space and 3D source data. While 2D-based methods only utilize structure and shape information in 2D space, 3D-based methods require additional devices, which may not be feasible in surveillance environments. To overcome this limitation, we propose a novel feature learning schema that combines 3D human reconstruction from a single image. Instead of extracting features from imprecise reconstructed 3D meshes, our approach extracts texture-insensitive 3D features directly from 2D images. We employ a multi-task framework supervised by identification and reconstruction losses to train a ReID model that decouples 3D shape information from visual texture, resulting in more reliable features for texture-confusing persons. Additionally, we introduce a purely unsupervised framework, Adversarial Self-Supervised Projection (ASSP), to train the 3D reconstruction model in the absence of 3D ground truth. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed model for person ReID, particularly in scenarios with texture-confusing situations.