Visual grounding, which establishes correspondence between visual regions and language entities, is crucial for bridging human and machine intelligence. Many vision-language tasks, such as image captioning and visual question answering, rely on this instance-level correspondence. While strong supervision methods have achieved progress in visual grounding, they require costly annotations. Weakly-supervised visual grounding, which learns from image-caption pairs, provides a more scalable alternative but faces challenges due to the ambiguity in object location and language-object correspondence. Existing approaches often rely on precomputed object proposals and implicit representations, resulting in visual and matching ambiguities. To address these limitations, we propose a flexible and context-aware object representation for weakly-supervised visual grounding. Our approach refines object proposals and enriches their representation with relation context cues, alleviating the impact of inaccurate object detection and cross-modal matching ambiguities. We develop a two-stage deep network with a backbone for proposal generation and refinement, and a visual object graph network for context modeling and matching prediction. To train the network, we introduce a multi-task loss function that leverages both model predictions and linguistic relation cues. We evaluate our method on the Flickr30K Entities and ReferItGame benchmarks, demonstrating superior performance compared to prior state-of-the-art approaches. Our contributions include the adoption of a coarse-to-fine strategy, the introduction of a self-taught regression loss for proposal refinement, and the use of a visual relation loss for context-aware object representation.