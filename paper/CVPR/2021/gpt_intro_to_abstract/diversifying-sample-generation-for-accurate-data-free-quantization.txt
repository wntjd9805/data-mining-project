Recently, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have achieved great success in various domains such as image classification, object detection, and semantic segmentation. However, deploying these state-of-the-art models on resource-constrained devices remains challenging due to their massive parameters and high computational complexity. Quantization has emerged as a promising approach to obtain efficient neural networks, especially with the availability of hardware that supports low-precision computations. While quantization-aware training methods are time-consuming and computationally intensive, post-training quantization methods have been proposed to directly quantize FP32 models without retraining or fine-tuning. However, these methods still require real training data to calibrate quantized models, which may not always be readily available due to privacy or security concerns.To address this issue, data-free quantization methods have been proposed to quantize models without any access to real data. These methods generate "optimal synthetic data" that aims to match the Batch Normalization statistics of the FP32 model. However, models calibrated with real data tend to perform better than those calibrated with synthetic data, despite the latter matching BN statistics more closely. This study reveals significant homogenization issues in the data generation process of typical data-free quantization methods at both the distribution and sample levels. The synthetic data distribution may overfit the BN statistics, leading to a lack of diversity compared to real data. Additionally, all samples of synthetic data are optimized using the same objective function, resulting in centralized feature distribution statistics, unlike the dispersed statistics observed in real data.To mitigate these adverse effects, a novel Diverse Sample Generation (DSG) scheme is proposed. DSG enhances the diversity of data in data-free quantization through two technical contributions: (1) Slack Distribution Alignment (SDA), which relaxes the constraint of distribution by slackening the alignment of feature statistics in BN layers, and (2) Layerwise Sample Enhancement (LSE), which applies layerwise enhancement to different data samples to reinforce specific layers.The DSG scheme is evaluated on a large-scale image classification task using various network architectures, including ResNet-18, ResNet-50, SqueezeNext, ShuffleNet, and InceptionV3. The results show that DSG outperforms previous methods by a wide margin and even surpasses models calibrated with real data. Furthermore, the synthetic data generated by the DSG scheme can be applied to advanced post-training quantization methods such as AdaRound.