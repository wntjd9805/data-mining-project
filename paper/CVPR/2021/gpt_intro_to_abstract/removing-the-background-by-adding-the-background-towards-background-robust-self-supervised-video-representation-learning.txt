Convolutional neural networks (CNNs) have achieved impressive results in various video understanding tasks. However, these models heavily rely on manually annotated datasets, which are time-consuming and expensive to obtain. On the other hand, there is a vast amount of unlabeled data available on the internet, which can be utilized to improve the performance of CNNs through self-supervised learning. Self-supervised learning methods have been effective in the image field, but there are significant differences between video datasets and image datasets. Current video datasets have implicit biases towards scene and object structure, leading to background cheating, where models make predictions based on background cues rather than motion patterns. In this work, we propose a method to reduce the impact of background cheating by adding a distracting background and promoting consistent feature learning. We randomly select a static frame and add it to every other frame in a video, creating a distracting video. By enforcing feature consistency between the original and distracting videos, our method reduces excessive dependence on the background. Experimental results show that our approach effectively reduces background cheating and improves the robustness and generalization ability of the learned representations. Furthermore, our method can be easily incorporated into existing self-supervised video learning methods, resulting in significant performance gains on popular video datasets. Overall, our contributions include a robust video representation learning method and its compatibility with existing self-supervised video learning approaches.