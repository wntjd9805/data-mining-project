Recent years have seen significant advancements in deep learning (DL) achieved through training on large datasets. However, many DL models deployed in real-world scenarios encounter non-stationary situations, where data is acquired sequentially and its distribution changes over time. This poses a challenge as deep neural networks (DNNs) trained by Stochastic Gradient Descent (SGD) easily forget previous knowledge when adapting to incoming tasks, leading to catastrophic forgetting. To address this, continual learning (CL) algorithms aim to learn consecutive tasks without severe degradation in performance on previous tasks. One popular approach is to use episodic memories, where representative data from old tasks is stored. The network parameters are optimized by replaying the recorded samples and incorporating samples from new tasks. Existing methods, such as GEM, A-GEM, and S-GEM, modify gradients to ensure the loss of each old task does not increase. However, in this work, we argue that the gradients of multiple tasks have mixed information and should be disentangled. Specifically, there are shared gradients and task-specific gradients mixed in the gradients. While optimization along the shared gradient helps memorize all old tasks, optimization along task-specific gradients creates a dilemma as optimizing one episodic memory can damage another. To address this, we impose different constraints on the two gradients, encouraging consistency between the gradient for update and the shared gradient, while also ensuring the update gradient is orthogonal to all task-specific gradients. We show that the second constraint can be relaxed by Principal Component Analysis (PCA), which captures the most important gradient constraints. Furthermore, we observe that gradients in different layers have varying magnitudes, but existing gradient modification methods ignore this variation. To tackle this, we propose a layer-wise gradient update strategy, where unique gradient constraints are imposed by each layer, resulting in optimizations specific to the parameters in that layer. Our analysis demonstrates that this layer-wise optimization strategy improves the efficiency of reducing losses from episodic memory. The two-fold contribution of this paper is: (1) leveraging gradient decomposition to specify shared and task-specific information in episodic memory, and imposing different constraints based on these gradients; (2) proposing a layer-wise gradient update strategy to address magnitude variations between gradients from different layers and efficiently reduce episodic memory losses. Extensive ablation studies validate the effectiveness of these improvements.