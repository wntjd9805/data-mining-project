Recent advancements in generative adversarial networks (GANs) have enabled the generation of photorealistic images of single objects, such as faces, cars, and cats. However, generating complex images with multiple objects of different classes against natural backgrounds remains a challenge. This is due to the large appearance variations and the complicated relationships between objects and their surroundings. To address this challenge, various conditional inputs have been introduced to provide constraints on the image generation process, including textual descriptions, scene graphs, and semantic maps. This paper focuses on the conditional image generation task using layouts, which define a set of bounding boxes with specified size, location, and categories. While recent layout to image (L2I) generation models have shown promising results, they still suffer from two major limitations. First, the relations between objects and objects-to-stuff are often broken, leading to poorly generated occluded regions. Second, the appearance of generated objects lacks class-defining characteristics and can exhibit distortions. These limitations are a result of design flaws in existing L2I models, specifically in their GAN generators and discriminators. To overcome these limitations, this paper proposes a context-aware feature transformation module in the generator to capture inter-object and object-to-stuff relations, and a location-sensitive object appearance representation in the discriminator using Gram matrix to preserve shape and texture characteristics. Experimental results on datasets such as COCO-Thing-Stuff and Visual Genome demonstrate the effectiveness of the proposed approach. The contribution of this work includes identifying the limitations of existing L2I models, introducing novel components to address these limitations, and achieving state-of-the-art performance in complex image generation.