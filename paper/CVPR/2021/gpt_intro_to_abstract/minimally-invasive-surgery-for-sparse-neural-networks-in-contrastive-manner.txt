Deep learning technologies have achieved remarkable performance in various applications, driven by the use of deeper and larger neural networks. However, these networks with a vast number of parameters suffer from several shortcomings, namely, high computational and memory requirements. In particular, the large size of the networks leads to increased inference time and energy consumption, making real-time interactions and deployment on mobile devices challenging.To address these issues, neural network compression techniques have been proposed. One common approach is network pruning, where the weights with small magnitudes are set to zero and the remaining weights are fine-tuned. Another method is quantization, which involves changing the data types to accelerate operations. However, preserving the accuracy of the original network during compression remains a challenge.In this work, we introduce a new compression method called Minimally Invasive Surgery (MIS), inspired by the principles of real minimally invasive surgery. MIS leverages knowledge extracted from a pair of dense and compressed models to achieve better performance compared to existing methods such as knowledge distillation and network compression. We provide a theoretical demonstration of MIS based on information entropy and Bayes perspectives. Moreover, our experiments demonstrate the generality of MIS in various networks and tasks, even in unsupervised learning scenarios without ground truth labels. Additionally, MIS offers end-to-end compression for neural networks to meet the hardware acceleration requirements.Overall, this research contributes to the field of neural network compression by proposing an effective and versatile method that improves performance while reducing computational and memory demands, enabling efficient deployment of deep learning models in real-world applications.