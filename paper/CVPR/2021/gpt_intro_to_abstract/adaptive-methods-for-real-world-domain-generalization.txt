Domain generalization is a challenging problem in machine learning, which involves learning a classifier that can generalize to new unseen domains. One of the most effective formalizations for this problem is domain-invariant learning, which learns feature representations that are invariant to the underlying domain. This provides a universal classifier that can generalize to new domains. However, it has been observed that while domain-invariant classifiers minimize the average risk across domains, they may not perform well for specific test domains, especially when the distribution of domains has high variance. In this paper, we propose an adaptive classifier that can be adapted to any new domain using very few unlabeled samples without requiring any further training. Our approach consists of two steps: embedding each domain into a vector space using a few unlabeled samples, and leveraging these domain embeddings as supplementary signals to train a domain-adaptive classifier. We adapt low-shot prototypical learning to construct domain embeddings and provide an algorithm to use these embeddings for training adaptive classifiers. We justify our design choices with novel theoretical results based on kernel mean embeddings and derive generalization bounds on the average risk for adaptive classifiers. We also introduce a large-scale domain generalization benchmark, Geo-YFCC, which contains 40 training domains, 7 validation domains, and 15 test domains, with over 1.1 million samples. This benchmark is constructed from the YFCC100M dataset and exhibits characteristics of domain shift, label shift, and long-tailed class distributions. We demonstrate the effectiveness of our approach on existing benchmarks and show that fine-tuning with adaptive classification outperforms existing approaches. Furthermore, we discuss the importance of rigorous model selection and large benchmarks in understanding domain generalization.