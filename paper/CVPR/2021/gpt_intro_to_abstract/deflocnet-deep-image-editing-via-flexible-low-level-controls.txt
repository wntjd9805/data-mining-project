The investigation on image editing is growing, as it reduces manual efforts during image content generation. Convolutional neural networks (CNNs) have improved image representations, allowing for the creation of meaningful and visually appealing content. Users can draw holes in images to indicate regions to be edited. CNNs can automatically fill these holes by producing coherent image content. Additional inputs from users, such as lines and colors, allow CNNs to create meaningful content while maintaining visual pleasantness. Deep image editing offers flexibility for generating diverse content, applicable to areas such as data enhancement, occlusion removal, and privacy protections.However, achieving both user control flexibility and high-quality content generation is challenging. Existing approaches use high-level inputs for semantic content generation, but this limits flexibility. This paper focuses on incorporating low-level controls, such as sketch lines and colors, which make editing more interactive and flexible. Two main challenges are observed: the guidance from low-level inputs diminishes in the CNN feature space, and propagating spatially sparse color signals to desired regions is difficult. To address these issues, DeFLOCNet is proposed, which retains the guidance of low-level controls for reinforcing user intentions. It utilizes a deep encoder-decoder for structure and texture generations, with a novel structure generation block and an additional decoder for texture enhancement. Experiments on benchmark datasets demonstrate the effectiveness of DeFLOCNet compared to state-of-the-art approaches.