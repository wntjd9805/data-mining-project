Video matting, which involves extracting high-quality alpha mattes of moving foreground objects from videos, is widely used in special effects and TV/film production. In this paper, we address the challenges of video matting, including the need for spatial and temporal coherence, unreliable optical flow estimation within complex matting regions, and the requirement of dense trimap inputs for each frame. To overcome these challenges, we propose an encoder-decoder network with a novel spatio-temporal feature aggregation module (ST-FAM) that utilizes spatial and temporal information across multiple frames. Our network can effectively address the video matting problem without relying on optical flow estimation and can generate spatially and temporally coherent alpha mattes. We also introduce a correlation layer to propagate trimaps across different frames, allowing for reliable frame-by-frame trimaps with minimal user inputs. To support video matting research, we contribute a high-quality video matting dataset with ground truth alpha mattes. Additionally, we provide 10 high-resolution real-world videos with dense and frame-by-frame human annotated trimaps for evaluation. Experimental results show that our deep video matting method outperforms image-based deep matting methods and conventional video matting approaches, particularly in handling complex scenarios such as rapidly moving objects with fuzzy boundaries or complex backgrounds.