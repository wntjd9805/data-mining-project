The study addresses the challenge of understanding human intent behind images on social media platforms. While visual recognition techniques can identify objects and scenes in images, understanding the intent behind them goes beyond standard visual recognition. The paper introduces a human intent dataset called Intentonomy, consisting of 14,455 images labeled with 28 intent categories. The study evaluates the performance of intent recognition based on the amount and properties of object/context information. It reveals that different intent categories rely on different sets of objects and scenes for recognition, and in some cases, visual content has negligible impact. The paper also proposes a multimodal framework for intent recognition, incorporating salient regions in images, weakly-supervised localization, and textual information from hashtags. The evaluation demonstrates that combining visual and textual information improves intent prediction significantly. The contributions of the work include the Intentonomy dataset, a systematic study on intent recognition, and a framework for narrowing the gap between human and machine understanding of images.