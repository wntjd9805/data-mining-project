Video data transmission is a significant component of internet traffic, driven by the widespread use of mobile devices and the popularity of video streaming for both productivity tools and entertainment platforms. The demand for high-quality video, such as 4k UHD and VR 360, necessitates efficient video compression techniques. While traditional video coding standards have achieved impressive performance, they may not be optimal for machine-related tasks, particularly deep learning-based video analytics. The application of deep neural networks (DNNs) for image compression has been explored in previous works, leveraging deep autoencoders (AEs) to optimize the rate-distortion trade-off and demonstrating the feasibility of using latent representations as compressed signals. Video compression, on the other hand, involves exploiting temporal correlation among consecutive frames through learned video prediction. Recent research addresses the uncertainties of real-world videos by utilizing stochastic video prediction networks with autoencoders and/or generative adversarial network (GAN) structures. Early approaches either interpolated key frames or emulated hand-crafted codecs with neural networks. More recent efforts focus on data-driven algorithms that leverage the end-to-end trainability of DNNs, often adopting autoencoder-style structures for encoding frame and residual representations in latent space.