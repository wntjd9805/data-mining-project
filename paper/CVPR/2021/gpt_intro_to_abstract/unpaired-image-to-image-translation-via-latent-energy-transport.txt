The task of unpaired image-to-image translation aims to learn mappings between different domains without relying on paired-image information. Existing models typically use generative adversarial networks (GANs) and cycle consistency regularizers to enforce style transfer while maintaining content. However, these methods can be restrictive and hinder model optimization efficiency. In this paper, we propose a plug-and-play energy-based model (EBM) in the latent space to overcome these limitations. Our model leverages a pre-trained autoencoder and the EBM to manipulate the latent code and achieve image translation. The EBM models a density distribution of latent variables, allowing for the transportation of latent codes between source and target domains. Additionally, the EBM's score function implicitly separates content and style codes, enabling the translation of style appearance while preserving content information. Our approach provides a computationally efficient alternative to GAN-based methods and addresses the challenges of EBM learning in the data space. Experimental results demonstrate the effectiveness of our latent EBM model for unpaired image-to-image translation.