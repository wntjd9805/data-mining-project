In this computer science paper, the authors address the laborious and expensive task of curating image datasets with pixel-wise labels. They propose a method called DatasetGAN that generates large, high-quality labeled datasets by labeling only a small number of examples. The approach leverages semi-supervised learning techniques, such as pseudo-labels and consistency regularization, as well as contrastive learning to train powerful image representations. The authors demonstrate that the latent representations learned by state-of-the-art generative models can be used for complex pixel-wise tasks, resulting in an infinite annotated dataset generator. By annotating images with extreme detail, the authors generate datasets for various image segmentation tasks, including human face parts and car parts. Their approach outperforms semi-supervised baselines and is on par with fully supervised methods, while requiring significantly less annotated data. The authors also showcase the application of their method in 3D reconstruction of animatable objects. Overall, their work highlights the potential of training successful computer vision models with minimal labeled examples and opens doors to exciting downstream applications.