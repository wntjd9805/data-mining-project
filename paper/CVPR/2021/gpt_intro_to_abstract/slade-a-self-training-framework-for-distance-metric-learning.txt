Existing distance metric learning methods in computer science primarily focus on learning sample similarities and image embeddings using labeled data. However, this approach often requires a large amount of data to achieve good performance. Previous studies have shown that most methods perform similarly when hyper-parameters are appropriately tuned, indicating that the choice of network architecture may play a significant role in performance gains. To address this, we investigate an alternative approach that utilizes unlabeled data to improve retrieval performance. Recent advances in self-supervised learning and self-training have demonstrated promising results using unlabeled data. Self-supervised learning leverages unlabeled data to learn general features in a task-agnostic manner, which can be transferred to downstream tasks through fine-tuning. Moreover, recent models have shown that features obtained from self-supervised learning are comparable to those from supervised learning for tasks such as detection or classification.Although self-training methods have been successful in improving the performance of fully-supervised approaches, existing methods mainly focus on classification rather than retrieval. Hence, we propose a self-training framework for distance metric learning, called SLADE, which leverages unlabeled data. The framework consists of training a teacher model on the labeled dataset and using it to generate pseudo labels for the unlabeled data. We then train a student model using both labeled and pseudo-labeled data to generate a final feature embedding. Our proposed SLADE framework aims to enhance retrieval performance by incorporating unlabeled data, providing an alternative approach to distance metric learning that reduces the reliance on large labeled datasets.