Video inpainting, which involves filling missing regions in video frames with plausible content, has various application domains such as video restoration, watermark/logo removal, and object removal. Existing state-of-the-art video inpainting methods rely on three main approaches: 3D temporal convolution, optical flow, and attention mechanisms. However, these methods have limitations in accurately predicting structural details and spatial alignment of features. This paper proposes a novel framework called Progressive Temporal Feature Alignment Network that combines the advantages of temporal convolution and optical flow-based warping approaches. The proposed method utilizes a temporal shift-and-aligned module to align features using optical flow and progressively applies it to feature maps in different scales. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on video inpainting benchmarks, producing visually appealing results in terms of spatial alignment, resolution, and fine-grained structures.