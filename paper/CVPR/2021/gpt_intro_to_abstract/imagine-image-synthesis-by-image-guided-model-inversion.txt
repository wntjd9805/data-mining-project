In this paper, we propose a new approach called IMAge-Guided model INvErsion (IMAGINE) to address the challenge of automatically synthesizing semantically meaningful variations of a reference image. Unlike existing methods based on generative adversarial networks (GANs) or GAN projection, IMAGINE leverages the knowledge of image semantics from a pre-trained classifier to generate variations of a target image. Specifically, we optimize for a new image that best explains the class prediction of the target image, while enforcing feature matching constraints at various network layers. This is complemented by adversarial training to improve the realism of the synthesized images. We show that IMAGINE outperforms existing methods, particularly in synthesizing non-repetitive images with high-level semantics. The resulting algorithm allows for multifaceted control over the synthesized image, enabling modifications to object shape, identity, and even object position. Additionally, we provide extensive evaluations, both quantitative and qualitative, to demonstrate the superiority and generalizability of IMAGINE across different image domains. Our paper presents the first attempt to utilize model inversion for the image-guided synthesis problem and introduces adversarial training to improve image quality. We also discuss various applications of IMAGINE, including object position control.