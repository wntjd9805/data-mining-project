This paper addresses the issue of regression in the update of image classification models, where a new model is replaced by an old model. Regression occurs when the new model introduces errors that were not present in the old model, leading to a negative flip rate (NFR) that measures the extent of regression. The authors argue that reducing the overall error rate (ER) is not sufficient to reduce the NFR, as models trained on the same data with different conditions and parameters tend to yield similar error rates but with errors occurring on different samples. The authors propose positive-congruent (PC) training as a solution to minimize both the error rate and the NFR. They introduce a method for PC training when the old model is given, which involves minimizing an additional loss term along with the standard classification loss. Among the variants of the additional loss, Focal Distillation (FD) is found to be the most effective. The authors also explore the problem of "future-proofing" the reference model by using ensembles to select the reference model, resulting in lower regression. The contributions of this paper include formalizing the problem of quality regression in classifier pairs, proposing a variant of model distillation for PC training, and demonstrating the adaptation of reference models for future PC training through ensembles. Experiments conducted on large-scale image classification benchmarks provide a baseline (Focal Distillation) and a paragon (Ensemble) for future evaluation.