Many safety-critical systems, such as aviation and medical diagnosis, now rely on deep convolutional neural networks (CNNs). However, these CNNs are vulnerable to adversarial attacks, where imperceptible perturbations to input images can drastically alter their output. As these systems become increasingly automated, it is crucial to develop methods to make CNNs robust against such attacks. Current strategies for generating adversarial inputs often assume a white-box attack, where the attacker has full knowledge of the model. Gradient-based methods that leverage the gradient of the output loss with respect to the input have been shown to significantly reduce the accuracy of CNNs when faced with adversarial examples. This has led to a "tug of war" between adversarial attacks and defense strategies in recent years. Attackers focus on finding perturbations that maximize the model's loss, while defenders aim to make the loss landscape smoother by training with adversarial examples.From a human perception perspective, shallow layers of CNNs extract simple local textures, while deep layers specialize in differentiating complex objects. It is intuitive to expect that incorrect extraction of shallow features will prevent correct formation of higher-level features, affecting subsequent layers. To investigate this, we introduce LPGD, a method that attacks an intermediate layer of the network and scrambles the features extracted. We observe increasing discrepancies between the features of natural images and their corresponding adversarial examples in deeper layers.However, current attack and defense strategies approach the challenge of evaluating or promoting model robustness in a model-holistic manner, ignoring the latent features extracted by intermediate layers. Some recent defense strategies have achieved high robustness against conventional attacks, but we hypothesize that this is due to the model-holistic nature of these attacks.To investigate the vulnerability of latent features and their cascading effect on the remaining layers, we propose a new strategy called LAFEAT. LAFEAT harnesses latent features in a generalized framework, drawing inspiration from effective attack techniques discovered in recent years. Our experimental results show that LAFEAT outperforms existing methods in terms of attack performance and computational efficiency.To the best of our knowledge, LAFEAT is currently the strongest method against a wide variety of defense mechanisms and matches the top performance on the CIFAR-10 white-box leaderboard. We argue that future evaluation of model robustness should consider the effective use of hidden components in defending models, moving away from a holistic perspective.