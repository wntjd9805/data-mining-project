Self-contact is a common aspect of human behavior, yet it has been largely overlooked in computer vision research. This paper aims to address this limitation by introducing three new datasets focusing on self-contact at different levels of detail. Additionally, two new optimization-based methods for fitting 3D bodies to images with contact information are proposed. These methods leverage the datasets to estimate pseudo ground-truth 3D poses with self-contact. The pose and shape of the human body are represented using the SMPL-X body model, which accurately captures body surface details, including the hands and face. Neural networks are then trained on the new datasets to regress 3D human pose and shape more accurately than state-of-the-art methods. The datasets include a 3D Contact Pose dataset, which consists of realistic self-contact poses obtained from 3D scans and an optimization process. A Mimic The Pose dataset is also created, where workers on Amazon Mechanical Turk mimic poses displayed on rendered 3D Contact Pose meshes. Additionally, a Discrete Self-Contact dataset is generated by labeling images from public datasets with discrete body-part contacts. The proposed methods, SMPLify-XMC and SMPLify-DC, optimize body shape using image features and contact labels. Fine-tuning the HPS regression network, SPIN, on the MTP and DSC datasets significantly improves the accuracy of regressed poses, even for non-self-contact poses. These results demonstrate the value of self-contact information in 3D pose estimation. The introduced regression method, TUCH, achieves state-of-the-art results on 3D pose estimation benchmarks. The datasets and code are made available for research purposes.