Video object segmentation (VOS) is a crucial task in computer vision with numerous applications in video editing, video composition, and autonomous driving. This paper focuses on the challenging task of semi-supervised video object segmentation, where given a video and the ground truth object mask of the first frame, the goal is to predict segmentation masks for the specified objects throughout the remaining frames. The task becomes extremely challenging due to the large appearance changes, occlusions, and similar instances of objects in video sequences.Memory-based approaches have recently shown significant performance improvements in popular VOS benchmarks. However, these methods primarily rely on pixel-level matching, which can lead to errors in prediction, especially when non-target regions share similar visual appearance with the target object. To address these limitations, this paper proposes a novel framework called Learn position and target Consistency for Memory-based video object segmentation (LCM). LCM leverages a memory mechanism, called the Global Retrieval Module (GRM), to perform pixel-level matching and store previous information in a memory pool.To improve the accuracy of segmentation, LCM introduces two key modules: the Position Guidance Module (PGM) and the Object Relation Module (ORM). PGM maintains position consistency by learning a location response that guides the segmentation process based on the trajectory of the objects. ORM integrates object-level feature information into the memory-based network to ensure target consistency throughout the video.Experimental results on benchmark datasets, including DAVIS and Youtube-VOS, demonstrate that LCM achieves state-of-the-art performance and ranks first in the DAVIS 2020 challenge for semi-supervised VOS task. The proposed PGM and ORM modules effectively address the limitations of memory-based methods, leading to more accurate and consistent video object segmentation.