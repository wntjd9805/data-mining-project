Transformers have been successful in various areas, including natural language processing and computer vision. However, current approaches for vision-language pre-training models (PTMs) focus on coarse matching and may not be suitable for fine-grained representation learning in specific domains like fashion. In this paper, we propose Kaleido-BERT, a novel framework for fashion-based tasks that aims to bridge the semantic gaps between text and image. We introduce an efficient "kaleido" strategy to extract multi-grained image patches and use attention mechanisms to build pre-alignments between these patches and text tokens. This approach allows Kaleido-BERT to learn fine-grained cross-modality information and outperform existing models in the fashion domain. We also present an alignment-guided masking strategy to further enhance the model's ability to learn semantic connections between vision and language. Experimental results demonstrate the effectiveness of our approach. Our contributions include the Kaleido Patch Generator, Attention-based Alignment Generator, and Alignment Guided Masking strategies.