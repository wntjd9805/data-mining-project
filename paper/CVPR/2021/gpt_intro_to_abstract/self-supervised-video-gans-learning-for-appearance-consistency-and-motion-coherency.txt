Abstract:Generative Adversarial Networks (GANs) have emerged as a powerful tool for unsupervised data distribution modeling, specifically in the image domain. Recent advancements in GAN objectives and architectures have addressed mode collapse and training instability issues, allowing for the synthesis of visually plausible and diverse images. However, extending GANs to the video domain presents additional challenges due to the high dimensionality of videos. Videos have an extra dimension of time, exponentially expanding the video space and requiring not only spatial realness but also temporal coherency in order to generate perceptually satisfying videos. In this paper, we propose Self-supervised Video GANs (SVGAN) to address these challenges. SVGAN incorporates explicit constraints on appearance and motion using two pretext self-supervision tasks: appearance contrastive learning and temporal structure puzzle. These constraints significantly reduce the video space, making the video generation problem less complex. Unlike previous approaches that mainly focus on the architecture of the generator, we emphasize the objectives of the discriminator in video GANs. Our experimental results on facial expressions and human actions datasets demonstrate that SVGAN enhances video generation performance compared to state-of-the-art techniques, regardless of the generator architecture. This work sheds light on the role of discriminator objectives in video GANs and introduces an effective method for generating realistic videos.