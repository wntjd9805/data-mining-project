Deep neural networks have achieved great success in supervised learning tasks, particularly in computer vision. However, this success heavily relies on a large amount of labeled data, which can be expensive and time-consuming to obtain. To address this issue, Semi-Supervised Learning (SSL) methods have been developed to effectively leverage both labeled and unlabeled data. Recent state-of-the-art SSL methods can be categorized into three main approaches: consistency based regularization, entropy minimization, and pseudo label. While most of these methods focus on training a randomly initialized model from scratch, this paper considers a more realistic setting that utilizes powerful pre-trained models. These pre-trained models have been trained on large-scale datasets for general purposes and have been shown to have excellent transferability to downstream tasks. The authors propose a semi-supervised transfer learning framework that extends the idea of consistency regularization to adapt to inductive transfer learning, where a pre-trained weight learned from a source task is available. The framework consists of two essential components: Adaptive Knowledge Consistency (AKC), which transfers knowledge from the pre-trained model to the target model, and Adaptive Representation Consistency (ARC), which adjusts the representation produced by supervised learning using unlabeled examples. The authors evaluate their method on various semi-supervised transfer learning settings using popular datasets in different domains. They demonstrate that their adaptive consistency regularization outperforms classic SSL algorithms and can even improve upon existing state-of-the-art SSL techniques. This work makes several contributions, including the development of an advanced end-to-end semi-supervised transfer learning framework for deep neural networks and the introduction of adaptive consistency regularization to leverage the characteristics of both SSL and transfer learning.