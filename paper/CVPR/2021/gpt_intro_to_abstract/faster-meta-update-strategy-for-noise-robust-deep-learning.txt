Deep neural networks (DNNs) have achieved remarkable success in computer vision applications such as image classification, object detection, and semantic segmentation. However, DNNs are susceptible to memorizing the training data, leading to biased classifiers and degraded generalization capabilities. Meta-learning has emerged as an effective framework to address this issue by using a meta-model to correct bias. Despite its promising results, meta-learning approaches suffer from slow training times, hindering their application in many scenarios.In this paper, we propose a Faster Meta Update Strategy (FaMUS) to improve the training efficiency of meta-learning while maintaining generalization capabilities. Our approach leverages the finding that the meta gradient can be reasonably approximated by the accumulation of gradients from only a few network layers. We design a learnable gradient sampler to determine whether or not to aggregate the meta gradient from each layer, effectively reducing computation when gradient samplers are turned off. Additionally, we demonstrate that our approach yields a meta gradient with lower variance, resulting in faster and more stable optimization.We conduct extensive experiments to validate the efficiency and efficacy of our method. Our experiments show that FaMUS speeds up recent meta-learning methods by at least three times without sacrificing generalization performance. Furthermore, our method achieves state-of-the-art performance on benchmarks with noisy labels, including the challenging CNWL benchmark. We also demonstrate competitive performance on the long-tailed recognition task.In summary, our contributions include the proposal of FaMUS, which significantly reduces training time for meta-learning methods, the empirical demonstration of reduced gradient variance and improved generalization performance, and the achievement of state-of-the-art results on benchmarks with noisy labels.