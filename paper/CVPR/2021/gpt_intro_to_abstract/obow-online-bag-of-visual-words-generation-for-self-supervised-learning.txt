Unsupervised learning of image representations using convolutional neural networks (convnets) has gained significant attention in recent years. Various convnet-based methods have been proposed, including pre-text tasks, generative models, and instance-discrimination training. However, existing methods primarily focus on invariant and discriminative embeddings, neglecting contextual reasoning. Reconstruction-based approaches, such as the prediction of image contents from a given region, offer richer representations but suffer from the optimization of low-level pixel details. To address this, we introduce a novel self-supervised approach called OBoW, which leverages the advantages of reconstruction while focusing on high-level visual concepts. OBoW overcomes limitations of existing BoWNet approaches by designing an online teacher-student learning scheme and a dynamic BoW prediction module. We also enhance contextual reasoning through aggressive cropping, spatial perturbations, and multi-scale BoW reconstruction targets. Our method simplifies and improves the BoW-guided reconstruction task, outperforming state-of-the-art unsupervised learning benchmarks.