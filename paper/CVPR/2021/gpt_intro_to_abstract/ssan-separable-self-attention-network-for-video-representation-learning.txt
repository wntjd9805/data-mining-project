Video representation learning is a critical task in computer vision, particularly for tasks such as detection, segmentation, and action recognition. While CNN-based approaches have been widely explored for capturing spatial-temporal correlations in videos, learning strong and generic video representations remains challenging. This may be due to the fact that videos not only contain rich semantic elements within individual frames but also involve temporal reasoning across time that reveals the semantic-level information for actions and events. Modeling long-range dependencies among pixels is essential for capturing this contextual information, which current CNN operations struggle to achieve. RNN-based methods have been used for this purpose but suffer from high computational costs and do not establish direct pairwise relationships between positions irrespective of their distance. Self-attention mechanisms have been recognized as effective approaches for building long-range dependencies. In the natural language processing domain, self-attention based transformers have been successfully used to capture contextual information from sequential data. Recent efforts have also introduced self-attention to computer vision tasks such as segmentation and classification. However, existing methods that incorporate self-attention for video action recognition often combine spatial and temporal correlations, which may capture irrelevant information and lead to ambiguity in action understanding, especially for videos with complex activities. Therefore, decoupling the spatial and temporal dimensions while considering short-term temporal dependencies is necessary for efficiently capturing correlations in videos. In this paper, we propose a separable self-attention (SSA) module that can effectively capture spatial and temporal contexts for temporal modeling. Our design first performs spatial self-attention independently for input frames, obtaining attention maps that convey spatial contextual information. These maps are then aggregated along the temporal dimension and sent to the temporal attention module. This approach ensures that the spatial contextual information contributes to capturing both short-term and long-term temporal correlations, enabling a comprehensive understanding of actions in videos. We evaluate our approach on the Something-Something and Kinetics datasets for video action recognition and demonstrate its superior performance compared to state-of-the-art 3D and 2D based methods. Furthermore, our models outperform counterparts with shallower network structures and fewer modalities, highlighting the effectiveness of SSA. Additionally, we showcase the efficiency of SSA in video-language tasks, such as video retrieval, by showing improved performance when searching candidate video clips using text queries. Overall, our work contributes to the advancement of video representation learning by proposing a separable self-attention module that efficiently captures spatial and temporal correlations while considering short-term temporal dependencies.