Self-supervised learning has made significant advancements in learning representations solely through self-supervision. There have been promising results showing that self-supervised methods, fine-tuned with a small percentage of training labels, can achieve comparable performance to fully supervised methods. However, in domains such as the study of the natural world, where labeling data is expensive and time-consuming, the benefits of self-supervised learning can be constrained. To address this, we introduce NeWT, a diverse benchmark of natural world visual understanding tasks, including animal health, behavior, and life stage. We also present iNat2021, a large-scale image dataset containing over 2.7 million images from 10,000 different species. Our evaluation shows that while self-supervised features have made progress, they still lag behind supervised variants. Through this work, we aim to expand the applicability and impact of computer vision models in domains with large media collections and limited labeling resources.