Self-supervised learning, a form of unsupervised learning, has shown promising results in capturing feature representations from unlabeled visual data. This type of learning does not rely on human annotation for target objectives, and it has been observed that these representations can outperform labeled representations in certain cases. Specifically, instance contrastive learning, where a single image is augmented and a network is trained to distinguish the augmented images, has been at the forefront of these advances.This paper focuses on the evaluation of self-supervised models and the selection of data augmentation policies when labeled data is not available. Traditional supervised evaluations may not be feasible or accurate in certain domains such as medical imaging or fashion categorization. The paper introduces a linear image rotation prediction task as an evaluation criterion and shows that it is highly correlated with downstream supervised performance on various recognition datasets and tasks.The authors adapt two automatic data augmentation algorithms for instance contrastive learning using self-supervised evaluation. These algorithms discover augmentation policies that match or outperform policies obtained through supervised feedback, while requiring significantly less computational resources.The paper also compares the effectiveness of evaluation tasks and finds that image rotation prediction has a stronger correlation with supervised performance than other tasks such as jigsaw or color prediction.Based on these contributions and experiments, the paper concludes that image rotation prediction is a robust and unsupervised evaluation criterion for selecting data augmentations in instance contrastive learning.