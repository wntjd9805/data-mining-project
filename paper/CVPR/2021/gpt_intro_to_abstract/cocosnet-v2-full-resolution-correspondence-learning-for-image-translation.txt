Image-to-image translation has been widely used in various applications, with exemplar-based methods allowing for more user control by conditioning the translation on a specific exemplar. However, producing high-quality and faithful translations while maintaining high resolution poses a challenge. Previous studies have used generative adversarial networks to learn the mapping, but they do not effectively utilize information from the exemplar image. Other methods have attempted to refer to the exemplar image during translation but only transfer the global style, resulting in washed-out textures. Recently, CoCosNet established dense semantic correspondence between cross-domain images, allowing for the incorporation of fine textures from the exemplar. However, the high memory footprint required for estimating high-resolution correspondence limits the effectiveness of this approach. In this paper, we propose cross-domain correspondence learning in full resolution, leveraging meticulous details from the exemplar to generate high-resolution translated images of photo-realistic quality. Inspired by PatchMatch, which iteratively propagates correspondence from the neighborhood, we introduce techniques to overcome the challenges of applying PatchMatch to high-resolution feature maps. These techniques include a hierarchical strategy, ConvGRU for refinement, and considering a larger context during correspondence propagation. Our proposed method, CoCosNet v2, achieves significantly higher quality image translations than state-of-the-art approaches due to the full-resolution cross-domain correspondence. We demonstrate visually appealing image translations at large resolutions and highlight the major contributions of our work, including the learning of full-resolution correspondence and the efficiency of our proposed method.