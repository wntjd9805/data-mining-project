This paper introduces a dual-generators framework for 3D point cloud generation, addressing the challenges posed by the surface-centric nature of point clouds, such as occlusion and distance issues during object scanning. The proposed framework consists of two generators accompanied by a shared discriminator, generating point clouds in a progressive manner. The generated point clouds can be used in various 3D tasks, such as segmentation, volumetric shape estimation, object detection, and scene understanding. The paper focuses on deep learning-based point cloud generation models under the generative adversarial learning framework. It aims to generate high-quality, dense, complete, and uniform point clouds from sparse, noisy, and non-uniform latent codes. The progressive generation/reÔ¨Ånement approach is used, where the dual-generators sequentially transform the input latent variables into a suitable 3D representation, fooling the discriminator. The main contributions include the proposal of the dual-generators architecture, considering both shape-wise and point-wise cues, and the incorporation of an oversampling conception for generating point clouds. The effectiveness of the proposed method is demonstrated through extensive qualitative and quantitative experiments on the ShapeNet dataset. This research provides valuable insights on designing effective point cloud generation models.