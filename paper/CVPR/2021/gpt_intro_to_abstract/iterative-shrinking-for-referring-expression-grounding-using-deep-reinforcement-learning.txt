Referring expression grounding (REG) is a task that involves recognizing and localizing a target object in an image based on a query sentence. This task requires the integration of visual and linguistic information. REG serves as the basis for various downstream tasks in computer vision and natural language processing, such as visual question answering, image captioning, and image-text matching. Existing methods formulate REG as a region-retrieval problem, relying on ground-truth bounding boxes or pre-trained object detectors. However, these methods have limitations in handling complex queries and lack interpretability. To address these issues, we propose a novel approach that formulates REG as a sequence of image-level shrinking processes. In each iteration, the image is shrunk in a certain direction, removing non-target image regions. The shrinking direction is predicted by a trainable network using reinforcement learning. Our method leverages contextual relations between target and reference objects, allowing for improved performance on complex queries. We demonstrate the effectiveness of our approach on the Ref-COCOg dataset, achieving a 4.32% accuracy improvement over the state-of-the-art method. Our method also achieves an average accuracy gain of 1.33% on Ref-COCOg, RefCOCO+, and RefCOCO datasets.