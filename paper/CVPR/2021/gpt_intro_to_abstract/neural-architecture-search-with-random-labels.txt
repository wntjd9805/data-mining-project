Recent years, Neural Architecture Search (NAS) has gained significant attention for its superior performance compared to human-designed architectures in various tasks such as image classification, object detection, and semantic segmentation. Most existing NAS frameworks can be summarized as a nested bilevel optimization. However, the mechanism behind the success of performance-based NAS is still not fully understood. This paper proposes an alternative NAS paradigm called convergence-based NAS, which focuses on architectures with fast convergence. The authors investigate the role of labels in both performance-based NAS and convergence-based NAS, and find that convergence-based NAS requires weaker label requirements. Motivated by this finding, the authors propose a convergence-based NAS framework called Random Label NAS (RLNAS) that only requires random labels for search. RLNAS achieves comparable or even better performances than many supervised/unsupervised methods on popular search spaces and is shown to transfer well to downstream tasks. The major contribution of this paper is the introduction of RLNAS, which provides a stronger baseline for future NAS algorithms and offers new insights into the effectiveness of existing NAS methods.