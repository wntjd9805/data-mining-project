Semantic segmentation is crucial in image understanding, and the advancement of deep learning has provided a powerful tool for this task. However, data annotation can be expensive, especially in scenarios like medical image analysis where there may be an abundance of unlabeled data. Semi-supervised learning, which combines labeled and unlabeled data, is important in these scenarios. One effective approach is self-learning, where an initial model is trained on labeled data and then fine-tuned on unlabeled data using pseudo labels generated by itself. However, we observe that in the self-learning process, the similarity between the teacher and student models tends to increase, leading to weak supervision from the pseudo labels and a plateau in the learning process. We refer to this phenomenon as lazy mimicking, where prediction errors persist and accuracy on the reference set stops growing. To overcome this optimization trap, we propose the asynchronous teacher-student optimization (ATSO) algorithm. ATSO introduces two modifications to the self-learning pipeline: starting each generation from the same initialized model and partitioning the reference set into two subsets to prevent a teacher model from supervising its direct student. Experimental evaluations on medical and autonomous driving image datasets demonstrate the effectiveness of ATSO in improving segmentation accuracy. Specifically, ATSO achieves promising results in pancreas segmentation from CT scans and transferring a model between different datasets. In the field of autonomous driving, ATSO produces competitive segmentation accuracy with basic-level augmentations and achieves satisfying results in model transfer. Overall, this paper contributes by revealing the lazy mimicking phenomenon and presenting the ATSO pipeline, which improves semi-supervised image segmentation. Additionally, the idea of super-class pseudo labels is introduced to enhance knowledge distillation in multi-class segmentation tasks.