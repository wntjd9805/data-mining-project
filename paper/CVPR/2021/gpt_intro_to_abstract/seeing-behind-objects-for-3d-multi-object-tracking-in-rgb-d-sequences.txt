Understanding how objects move over time is crucial for higher-level perception of real-world environments in various domains, including mixed reality and robotic perception. While significant progress has been made in RGB-D tracking and reconstruction for static scenes, the assumption of a static environment limits its applicability to dynamic real-world environments where objects are in motion. Tracking and maintaining robust correspondences of objects that are rigidly moving pose significant challenges due to changing views and occlusion patterns.Existing approaches for dynamic object tracking in RGB-D sequences typically focus on object detection and finding correspondences between frames. However, these methods only consider observed geometry, which can lead to insufficient overlap and unreliable correspondences, particularly under faster object or camera motion, resulting in tracking failure.To address these challenges, we propose leveraging prior knowledge of object geometry to aid object tracking, inspired by human perception. Our key idea is to "see behind objects" by hallucinating the complete object geometry. We develop an end-to-end approach that detects objects characterized by their 3D bounding boxes, predicts the complete geometry of each object, and establishes dense correspondence mappings to its canonical space. We use differentiable pose optimization based on the predicted correspondences to determine object poses per frame and their correspondences within frames.Experimental results demonstrate that our joint object completion and tracking approach outperforms state-of-the-art methods by 6.5% in MOTA (Multiple Object Tracking Accuracy) and exhibits good performance in scenarios with challenging occlusions. Our findings indicate that this approach has significant potential for object-based understanding of real-world environments.