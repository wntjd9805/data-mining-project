This paper introduces a new task in computer vision called part-aware panoptic segmentation (PPS), which aims to simultaneously understand a scene at multiple levels of abstraction. The existing tasks of scene parsing and part parsing focus on either recognizing and segmenting foreground objects or segmenting based on part-level semantics. PPS combines these two tasks to provide a holistic understanding of the scene. A new metric called part-aware panoptic quality (PartPQ) is introduced to evaluate the performance of PPS. To facilitate research on PPS, consistent part-aware panoptic annotations are provided for the Cityscapes and Pascal VOC datasets. Baseline results are generated for PPS using state-of-the-art techniques in panoptic segmentation and part segmentation. The paper also explores different design choices and strategies for combining predictions at multiple levels of abstraction. Overall, this work contributes to advancing the field of scene understanding by introducing the PPS task, providing evaluation metrics and annotations, and establishing baseline results for future research.