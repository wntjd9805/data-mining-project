Abstract: In recent years, image synthesis has experienced significant advancements, particularly with the help of generative adversarial networks (GANs). This paper focuses on semantic image synthesis, which involves generating photo-realistic images from semantic label masks. Previous methods in this domain have achieved good results using image-to-image translation frameworks and spatially-varying conditional normalization techniques. However, these approaches lack controllability over generation diversity. This paper proposes a novel framework that achieves controllable diversity in semantic image synthesis by treating each semantic class as a probability distribution. The proposed method consists of variational modulation models, instance-adaptive sampling, and prior noise remapping. Extensive experiments on multiple datasets demonstrate that the proposed method outperforms state-of-the-art approaches in terms of instance-level diversity while maintaining comparable generation quality.