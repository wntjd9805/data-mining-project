This paper introduces a novel approach to depth map prediction using multimodal audio and visual inputs. The authors propose an attention-based fusion mechanism that takes into account the material properties of objects in the scene to improve the accuracy of depth estimation. The proposed method is compared to existing approaches and evaluated on Replica and Matterport3D datasets, demonstrating superior performance in terms of RMSE. The authors also conduct ablation experiments to validate the design choices in the network and provide qualitative results. Overall, the paper contributes a deep neural network architecture for depth estimation from binaural audio and monocular images, achieving state-of-the-art performance in the field.