The effective representation of 3D objects in neural networks is crucial for various computer vision tasks, such as 3D model reconstruction and understanding. In previous studies, traditional geometry representations like voxel grids, point clouds, and meshes have been used. However, deep implicit functions (DIFs) have emerged as an alternate representation that offers expressive capacity and flexibility for representing complex shapes. Despite their advantages, DIFs lack the ability to establish relationships between different shapes, creating challenges for downstream applications like shape understanding and editing. To overcome this limitation, we propose a new approach called Deep Implicit Templates. This approach decomposes a conditional deep implicit function into two components: a template implicit function and a conditional spatial warping function. The template implicit function represents the average shape for a category of objects, while the spatial warping function deforms the template to generate specific object instances. This decomposition preserves the benefits of deep implicit representations, such as compactness and efficiency, while establishing dense correspondences across different objects.Our method differs from other techniques that use primitives to represent 3D shapes in two key aspects. Firstly, our decomposition provides a global template for training data, enabling applications like uv mapping and keypoint labeling. Additionally, our method builds accurate correspondences in the entire 3D space, whereas element-based methods rely on interpolation or feature matching. This gives our method more flexibility and scalability in controlling the template and its deformation.Training deep implicit templates poses challenges due to the absence of ground-truth mappings between templates and shape instances, and dense correspondence annotations. To address these challenges, we propose a Spatial Warping LSTM that decomposes the conditional spatial warping into multi-step point-wise transformations. We also introduce a progressive reconstruction loss and two-level regularization to obtain plausible templates with accurate correspondences in an unsupervised manner.Our experiments show that our method can learn plausible implicit templates for sets of shapes, accurately representing shapes while establishing dense correspondences without supervision. This expansion of DIF capabilities makes our Deep Implicit Templates a more effective implicit representation for 3D learning tasks. The code for our method is available on GitHub.