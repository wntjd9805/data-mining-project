This paper presents a framework for efficient human trajectory prediction in dynamic environments. Predicting human motions in such environments is challenging due to various factors that influence people's trajectories. Previous works have used energy functions to model human-human interactions, but they are limited in capturing complex interactions in crowded environments. To overcome these limitations, data-driven approaches based on recurrent neural networks (RNNs) have been proposed. However, these methods struggle to capture the influence of farther people and assign equal importance to nearby trajectories. Attention-based models have been integrated with RNNs and spatio-temporal graphs to address these issues, but they mostly rely on kinematics data and do not fully utilize the visual context of the scene. This paper introduces a conditional 3D visual attention mechanism that leverages the information from the entire video to capture human-human interactions and dynamic constraints. The proposed framework consists of two parallel encoding streams for gathering visual and kinematics information and a decoding stream for predicting future trajectories. The visual encoder uses the conditional 3D attention mechanism to extract relevant spatio-temporal primitives and attend to informative features. Experimental results on UCY and ETH datasets demonstrate the superiority of the proposed method compared to the state-of-the-art approaches, achieving a significant reduction in prediction error.