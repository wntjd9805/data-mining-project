Deep learning has seen significant advancements in various learning frameworks, particularly in the field of autoencoders (AE). AE aims to compactly represent and accurately reproduce input signals using an encoder and decoder. Variational autoencoders (VAE) extend this idea by learning a variational model for latent variables. The objective of generative autoencoders is to maintain both instance-level fidelity (faithfulness to individual input samples) and set-level fidelity (faithfulness to the entire dataset). While VAE/GAN algorithms combine reconstruction and adversarial losses to achieve these fidelity goals, sub-optimal results have been observed. One issue is that pixel-wise reconstruction losses lead to blurry images with degraded semantics. This can be addressed by measuring fidelity in a more semantically meaningful feature space and using a learned instance-level distance function. We propose a new generative autoencoder model, called dual contradistinctive VAE (DC-VAE), which combines discriminative losses for instance-level fidelity and adversarial losses for set-level fidelity. DC-VAE leverages metric learning and contrastive learning to formulate both losses in an induced feature space. Our contributions include demonstrating the significance of the two loss terms in the DC-VAE algorithm, showcasing its effectiveness in tasks such as image reconstruction, synthesis, interpolation, and representation learning across various resolutions, and achieving improved performance compared to competing methods in image synthesis without architectural changes. DC-VAE bridges the performance gap between baseline VAE and competitive GAN models, providing a potential solution for AE/VAE-based modeling and representation applications.