This paper introduces the vulnerability of deep neural networks (DNNs) to adversarial examples, which can cause misbehavior with imperceptible perturbations. This vulnerability raises concerns about the robustness and real-world deployment of DNNs in image retrieval and object detection tasks. The paper presents a taxonomy of adversarial attacks and proposes a query-based black-box attack approach that only requires query access to the target model. The paper explains the challenges in defining the objective function for query-based attacks and the difficulties in estimating gradients without complete knowledge of the target model. To address these challenges, the paper introduces a relevance-based loss to quantify the attack effects on target models and proposes a recursive model stealing method to acquire transfer-based priors and guide gradient estimation. The proposed method achieves a high attack success rate against image retrieval systems in both simulated and real-world scenarios, demonstrating its practicality. The contributions of the paper include the formulation of black-box attacks on image retrieval systems, the proposal of a new relevance-based loss, and the development of a recursive model stealing method. Experimental results validate the efficacy of the proposed attack method.