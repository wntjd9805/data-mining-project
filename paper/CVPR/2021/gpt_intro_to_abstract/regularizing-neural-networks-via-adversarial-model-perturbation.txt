To date, understanding the generalization behavior of deep neural networks remains a mystery. Previous research has shown that models with parameters corresponding to flat minima of the empirical risk tend to generalize better. This understanding has led to studying factors in the optimization process that affect the flatness of found minima, such as learning rate and batch size. Additionally, effective regularization techniques have been developed to alleviate overfitting and improve generalization. However, these techniques often lack strong principles or theoretical justifications.This paper aims to develop a powerful regularization scheme based on finding flat local minima of the empirical risk. The proposed scheme, called Adversarial Model Perturbation (AMP), minimizes an alternative "AMP loss" instead of the empirical risk. The AMP loss considers the worst empirical risk among all perturbations of the model parameters within a small norm constraint. Minimizing the AMP loss provides opportunities to find flat local minima, as illustrated in Figure 1.The paper formally analyzes the AMP loss minimization problem, showing that it implicitly considers the "narrowest width" of a local minimum as a measure of flatness. A mini-batch SGD algorithm is derived for solving this problem, resulting in the AMP regularization scheme. Interestingly, this algorithm can be viewed as regular empirical risk minimization with an additional penalty term on the gradient norm.Experimental results on benchmark image classification datasets validate the effectiveness of the proposed AMP scheme. Compared to other popular regularization techniques, AMP demonstrates remarkable regularization performance, establishing itself as a new state-of-the-art approach.In summary, this paper introduces the AMP regularization scheme, motivated by the understanding that flat minima aid generalization. Theoretical justifications and experimental results demonstrate the effectiveness of AMP in finding flatter local minima and improving generalization performance on various neural network architectures.