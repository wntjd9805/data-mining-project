The recent success of deep learning methods has led to efforts to apply them in real-world scenarios. However, deploying large deep learning models on computationally-restrictive platforms, such as smartphones and edge devices, can be challenging. Additionally, obtaining labeled datasets in specialized domains or with restricted access can be difficult. Cross-modal data from neighboring modalities also remains underutilized by conventional frameworks.Knowledge distillation (KD) offers a potential solution to these challenges. KD involves improving a student model's performance by supplementing it with feedback from a teacher model. The teacher, typically with larger capacity or access to additional data, transfers valuable knowledge to the student. Early KD methods used the Kullback-Leibler (KL) divergence to compare the output prediction distribution between the student and teacher.However, relying solely on prediction distributions limits the amount of information transferred, especially in cross-modal transfer scenarios. Recent works have explored using intermediate representations for more informative learning signals, but they often perform poorly compared to basic KD methods due to difficulties in defining a proper distance metric between teacher and student features. They also heavily rely on copying the teacher's behavior, which may not necessarily generalize well to the student.To address these limitations, this paper introduces Wasserstein Contrastive Representation Distillation (WCoRD), a novel KD framework that reduces the generalization gap between teacher and student for better knowledge transfer. WCoRD consists of distillation and generalization blocks. The distillation block maximizes the mutual information between student and teacher representation distributions using the dual form of the Wasserstein distance (WD). A 1-Lipschitz constraint is imposed to the critic via spectral normalization, improving stability. This approach is termed global contrastive knowledge transfer.The generalization block indirectly bounds generalization error by regularizing the Wasserstein distance between student and teacher feature distributions, allowing for coupling of features across multiple examples within each mini-batch. This approach, termed local contrastive knowledge transfer, directly matches the feature distributions of the student and teacher networks.The main contributions of this paper are: (i) the novel Wasserstein learning framework for representation distillation, utilizing the dual and primal forms of the Wasserstein distance for global contrastive learning and local feature distribution matching, respectively, and (ii) comprehensive experiments demonstrating the superiority of the proposed approach on benchmark datasets for model compression and cross-modal transfer, as well as real-world datasets for privileged information distillation.