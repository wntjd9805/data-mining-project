This paper introduces FS-Net, a fast shape-based network for estimating category-level 6D object size and pose in computer vision applications. Existing methods for category-level pose estimation often rely on mapping different objects in the same category to a uniform model using RGB features or RGB-D fusion features. However, the effectiveness of these features for pose estimation remains questionable. This paper addresses this issue by using RGB features for 2D detection and shape features extracted from point clouds for category-level pose estimation. To overcome the challenge of learning a representative uniform shape, the authors propose a 3D graph convolution (3DGC) autoencoder that learns the category-level pose features through observed points reconstruction rather than uniform shape mapping. An online box-cage based 3D data augmentation mechanism is also proposed to reduce the dependency on labeled data. FS-Net is divided into three parts: 2D detection, 3D segmentation & rotation estimation, and translation & size estimation. The YOLOv3 algorithm is used for 2D detection, a 3DGC autoencoder is designed for segmentation and observed points reconstruction, and a coordinate residual estimation network based on PointNet is used for translation and size estimation. To increase the generalization ability of FS-Net, an online 3D deformation mechanism is used for data augmentation. The main contributions of this paper include the proposal of a fast shape-based network that achieves efficient category-level pose feature extraction and runs at 20 FPS on a GTX 1080 Ti GPU. A 3DGC autoencoder is also introduced to learn latent orientation features and a decoupled rotation mechanism is used for orientation decoding. Additionally, a box-cage based 3D deformation mechanism is proposed for data augmentation, leading to improved pose accuracy by 7.7%.