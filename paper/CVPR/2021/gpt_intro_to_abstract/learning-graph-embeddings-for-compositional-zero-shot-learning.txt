Humans possess the remarkable ability to generalize their knowledge of known entities to novel concepts. This is evident in our capacity to recognize and understand new phenomena based on shared and discriminative properties of objects. In the field of computer vision, where visual concepts exhibit a long-tailed distribution, it is not feasible to gather supervision for all possible concepts. As a result, the recognition of unseen compositions of known objects without explicit supervision has become an important research area. While some modern vision systems have exhibited a certain degree of compositionality, such as feature sharing, most models do not possess compositional classifiers and treat each class as an independent entity requiring explicit training for new concepts.In this paper, we delve into the problem of Compositional Zero-Shot Learning (CZSL) or the study of state-object compositionality. CZSL aims to learn the compositional nature of observed objects and their states to enable generalization to novel compositions. Existing works in this field have explored various approaches, such as learning transformation networks on top of individual classifiers, treating states as linear transformations of object vectors, and learning modular networks conditioned on compositional classes. However, these approaches typically treat each state-object composition as independent, failing to capture the rich dependency structure among different states, objects, and their compositions. To address this limitation, we propose a novel graph formulation called Compositional Graph Embedding (CGE) to model the dependency relationship between visual primitives and compositional classes. This graph can be constructed independently of external knowledge bases like WordNet. Furthermore, we introduce a multimodal compatibility learning framework that embeds related states, objects, and compositions in close proximity, while pushing unrelated ones farther apart. To evaluate the effectiveness of our approach, we curate a new benchmark dataset called C-GQA, which is derived from the GQA dataset and offers diverse compositional classes with clean annotations. Our model outperforms state-of-the-art methods on numerous metrics across the MIT-States, UT-Zappos, and C-GQA datasets.In summary, our contributions include the introduction of CGE as a novel graph formulation for capturing the dependency relationship among visual primitives and compositions, the proposal of a multimodal compatibility learning framework, the creation of the C-GQA benchmark dataset, and the demonstration of significant improvements over existing methods.