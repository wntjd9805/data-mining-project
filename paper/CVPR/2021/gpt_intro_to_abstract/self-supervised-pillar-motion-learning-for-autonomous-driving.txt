Understanding the motion of various traffic agents is crucial for self-driving vehicles to safely operate in dynamic environments. Self-driving vehicles are typically equipped with multiple sensors, with LiDAR being the most commonly used. Representing and extracting temporal motion from point clouds captured by LiDAR is a fundamental challenge in autonomous driving. This challenge is further complicated by the fact that there are numerous agent categories, point clouds are sparse and lack exact correspondence between sweeps, and estimation processes must meet tight runtime constraints.In this paper, we propose a self-supervised learning framework for open-set motion estimation that leverages free supervisory signals from multiple sensors. Specifically, we adopt a bird's eye view (BEV) representation of point clouds, where a point cloud is discretized into grid cells and motion is described by encoding each cell with a 2D displacement vector on the ground plane. This representation simplifies scene motion and allows for efficient computation using 2D convolutions. To estimate motion, we introduce a self-supervision scheme based on pillar or object structure constancy between consecutive sweeps. However, due to the lack of exact point correspondence in sparse LiDAR scans, we utilize optical flow extracted from paired camera images to provide self-supervised and cross-sensory regularization. This approach enables us to learn pillar motion in point clouds in a fully self-supervised manner.Our proposed framework represents the first learning paradigm capable of performing pillar motion prediction in a fully self-supervised manner. We integrate point clouds and paired camera images to achieve self-supervision and cross-sensory signals. Experimental results demonstrate the superiority of our approach compared to existing supervised methods. The code and model implementation will be made publicly available.