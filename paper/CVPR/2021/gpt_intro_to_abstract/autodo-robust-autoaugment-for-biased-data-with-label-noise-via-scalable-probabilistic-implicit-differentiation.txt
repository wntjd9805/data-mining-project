Data augmentation (DA) is a crucial technique for improving the performance of deep neural networks (DNNs), as it enhances the size and diversity of the training dataset, leading to better generalization to test data. However, designing effective DA strategies often requires expert knowledge, dataset analysis, and costly experimentation. This becomes even more challenging in real-world scenarios with biased and noisy-label data. The automation of DA aims to overcome these difficulties by automatically estimating data transformation models. One popular approach is AutoAugment (AA), which uses reinforcement learning to learn a policy model for DA. Although AA achieves impressive results, its policy search process is computationally expensive. Recent works have attempted to address this issue while maintaining comparable accuracy. In this paper, we demonstrate that existing methods are not robust to distortions in the training dataset, such as distribution bias and noisy labels. We attribute this to the shared-parameter policy model used by these methods. To illustrate this, we conduct experiments on a toy classification task, where the distribution of digits "2" and "5" in the training dataset is uneven and less diverse compared to the test distribution. The estimated augmentation policies fail to properly increase the diversity of all digits, resulting in incorrect decision boundaries for the linear classifier. This issue becomes even more severe in a multi-class classification setting, particularly when the model overfits to examples with noisy labels. To address these limitations, we propose a novel approach called AutoDO, which estimates DA hyperparameters for each individual data point in the training dataset. Additionally, our model optimizes loss weights to account for data biases and incorporates soft-labels to handle noisy labels. This reformulation transforms the shared-policy search task of AA into a generalized automated dataset optimization problem. The objective of our AutoDO model is to align the distribution of a small clean validation dataset with the larger distorted training dataset using per-point hyperparameters. We optimize our model using large-scale hyperparameters and utilize implicit differentiation to analytically demonstrate its equivalence to maximizing the Fisher information between empirical datasets. Experimental results on class-imbalanced data with noisy labels show the superiority of our approach.