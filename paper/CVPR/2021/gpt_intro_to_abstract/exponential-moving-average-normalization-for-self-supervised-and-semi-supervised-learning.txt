Supervised learning in computer vision has achieved remarkable success with large annotated datasets. However, in domains like medical imaging, collecting annotations can be costly and time-consuming. This has led to the need for learning effective representations with limited or no manual annotation. One approach is the use of student-teacher models, where the teacher's outputs guide the learning of the student on unlabeled data. A common method is updating the teacher using exponential moving average (EMA) of the student's parameters. However, the use of standard batch normalization (BN) in both networks can lead to cross-sample dependency and model parameter mismatches. To address these issues, we propose exponential moving average normalization (EMAN) as a replacement for standard BN. EMAN removes cross-sample dependency and reduces potential mismatches by updating the normalization statistics and model parameters using EMA. We evaluate EMAN in various student-teacher frameworks and demonstrate its effectiveness in improving the performance of semi-supervised and self-supervised learning techniques. EMAN achieves consistent improvements across different methods, network architectures, training duration, and datasets without requiring additional communication or synchronization. We believe that EMAN can be beneficial for future student-teacher variants.