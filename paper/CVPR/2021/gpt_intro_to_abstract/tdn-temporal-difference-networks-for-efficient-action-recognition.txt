This paper introduces a new temporal modeling mechanism for action recognition in videos. Current deep learning approaches use either a two-stream network or 3D convolutions to capture motion information. However, these methods require high computational cost or lack consideration in the temporal dimension. To address this challenge, the authors propose a temporal difference based module (TDM) that leverages temporal derivative and captures both appearance and motion information in a unified framework. The authors argue that short-term and long-term temporal information are crucial and present a two-level temporal modeling framework called Temporal Difference Network (TDN). TDN includes efficient TDMs for local and long-range motion modeling. The authors demonstrate the effectiveness of TDN by implementing it with ResNets and achieving state-of-the-art performance on the Kinetics and Something-Something datasets. This paper contributes by presenting an efficient TDM for motion modeling, proposing a video-level motion modeling framework, and achieving improved recognition performance.