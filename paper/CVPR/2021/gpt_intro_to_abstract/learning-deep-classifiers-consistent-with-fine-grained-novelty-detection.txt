Deep convolutional neural networks (CNNs) have revolutionized image classification, but they are trained under the assumption that test examples belong to one of the classes on which the CNN was trained. This assumption is violated in many practical settings, such as medical diagnosis or autonomous driving, where CNNs may encounter images from unseen classes. Novelty detection aims to identify and reject examples from unseen classes. This paper focuses on a challenging setting where both seen and unseen classes are sub-classes of a common category. This setting is valuable in practical applications, such as wildlife surveillance or autonomous driving, where it is impossible to train the classifier for all possible sub-classes. The core of a novelty detection algorithm is a novelty score, which measures the degree to which an example does not belong to the seen classes. Two popular classes of novelty scores are probabilistic and metric-based. The novelty detection problem becomes more complicated when the training data is not sufficient to uniquely estimate the class-conditional distributions and the associated Bregman divergences. This paper proposes a regularization technique called Class-Conditional Gaussianity (CCG) that shapes the class-conditional distributions to be Gaussian. The combination of the CCG loss with the standard cross-entropy loss results in a classifier that is consistent with novelty detection. Experimental results on various fine-grained visual classification datasets demonstrate that the proposed method outperforms state-of-the-art approaches for novelty detection. In summary, this paper addresses the problem of novelty detection in the context of sub-class categorization. It proposes a regularization technique called CCG to shape the class-conditional distributions and improve novelty detection performance. Experimental results show significant advancements over existing approaches.