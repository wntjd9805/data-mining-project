Indoor scene parsing from images is a challenging task due to issues such as distorted object shapes, severe occlusions, viewpoint variations, and scale ambiguities. One approach to address these issues is to use auxiliary geometric information, typically in the form of depth maps. However, earlier methods require the availability of depth maps, limiting their applicability to general situations where depth data is not available. To overcome this constraint, some methods propose to predict depth maps from RGB input, but the accuracy of these predictions affects the overall performance. Additionally, prior works only explore depth maps as auxiliary cues, which provide only a partial view of the 3D scene and require paired RGB-depth data in training.In this paper, we present a flexible and lightweight framework called 3D-to-2D distillation, which leverages occlusion-free, viewpoint-invariant 3D representations derived from 3D point clouds to enhance 2D CNN features. Our approach utilizes large-scale 3D data repositories and recent advancements in 3D scene understanding for 3D feature extraction and allows the use of unpaired 3D data for training. We incorporate a two-stage dimension normalization module to align the statistical distributions of 2D and 3D features, reducing the numerical distribution gap between different data modalities. We also introduce a Semantic Aware Adversarial Loss to optimize the framework without paired 2D-3D data, enhancing its flexibility and applicability.Extensive experiments on indoor scene parsing datasets demonstrate that our approach consistently outperforms baselines, including state-of-the-art depth-assisted semantic parsing methods. Further experiments on a depth reconstruction task show that our framework effectively embeds 3D representations into 2D features, resulting in improved reconstruction results. Importantly, our model achieves significant performance gains even when evaluated on data from unseen domains, indicating that the 3D information embedded by our approach promotes the generalizability of CNNs. Overall, our framework offers a promising solution for indoor scene parsing without the constraints of depth availability or manual preparation of paired RGB-depth data.