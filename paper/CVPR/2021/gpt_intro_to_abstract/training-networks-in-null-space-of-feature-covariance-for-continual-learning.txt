Deep neural networks have shown promising performance in various tasks in natural language processing and machine intelligence. However, their ability for continual learning, which involves learning from sequential tasks, is limited. One major challenge in continual learning is catastrophic forgetting. This paper explores different strategies for continual learning, including regularization-based, distillation-based, architecture-based, replay-based, and algorithm-based strategies. The authors propose two theoretical conditions for stability and plasticity of neural networks in continual learning and introduce a novel network training algorithm called Adam-NSCL. This algorithm forces the network parameter updates to lie in the null space of the input features of previous tasks, addressing the plasticity-stability dilemma. The authors conduct experiments on continual learning benchmarks where datasets from previous tasks are unavailable, demonstrating the effectiveness of Adam-NSCL compared to state-of-the-art methods. The paper concludes with a discussion of related work and future directions.