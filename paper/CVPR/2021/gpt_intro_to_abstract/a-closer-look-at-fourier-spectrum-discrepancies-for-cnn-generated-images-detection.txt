This paper focuses on the detection of deepfake multimedia content, a growing concern due to its potential malicious use. The improvement of CNN-based generative modeling has made it increasingly difficult to assess the authenticity of synthetic content in the RGB pixel space. Recent research suggests that CNN-based generation methods fail to reproduce high-frequency distribution found in real images. Frequency artifacts and spectral discrepancies have been observed in CNN-generated images at the highest frequencies. Some techniques have been proposed to disguise or avoid these discrepancies. Previous studies have used the Fourier representation of the images to train detectors, showing better results compared to using the RGB counterpart. This work analyzes the high-frequency decay discrepancies in CNN-generated images, focusing on the last layer of the generators. It is hypothesized that inner generator layers are not directly responsible for these discrepancies. The study identifies the component related to the discrepancies across multiple GAN loss functions, architectures, datasets, and resolutions. Experiments show that modifying the feature map scaling of the last layer largely avoids the frequency discrepancies. The cause of these discrepancies is still debated among researchers, with suggestions including transposed convolution and linear dependencies in the spectrum of convolutional filters. This study provides counterexamples to argue that high-frequency discrepancies are not intrinsic to CNN-generated images and questions the reliability of using frequency-based features for detection.