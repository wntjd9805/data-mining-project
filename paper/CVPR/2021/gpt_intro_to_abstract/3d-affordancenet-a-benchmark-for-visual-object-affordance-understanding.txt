The concept of affordance, which refers to the interactions between humans and their environment, plays a crucial role in robot navigation and operation. Affordance understanding enables robots to anticipate future actions, recognize agent activities, and provide valid functionality of objects. One specific aspect of affordance understanding is semantic labeling, which involves localizing the position of possible affordances. Visual sensors are the most effective modality for affordance understanding, and recent research has focused on developing algorithms based on deep neural networks. However, existing datasets primarily capture 2D or 2.5D visual information and lack detailed geometric properties that are essential for affordance understanding. To address this limitation, we propose a benchmark dataset, called 3D AffordanceNet, which contains fine-grained 3D affordance annotations for a diverse range of shapes. The dataset is created by leveraging the PartNet dataset, which provides hierarchical information about 3D shapes. We define 18 types of affordances over 23 semantic objects and propagate sparse affordance labels using a label propagation method. The resulting dataset enables benchmarking of full-shape, partial-view, and rotation-invariant affordance estimations. The benchmark also includes evaluation of three state-of-the-art point cloud deep learning networks and proposes a semi-supervised affordance estimation method using unlabeled data. Our contributions include the introduction of 3D AffordanceNet, a large-scale dataset with well-defined probabilistic affordance score annotations, and the demonstration of affordance learning tasks and evaluation of baseline methods. Overall, this work provides a valuable resource for advancing research in visual affordance understanding in realistic 3D scenarios.