Deep convolutional neural networks (CNNs) have achieved remarkable success in computer vision tasks but are computationally and memory intensive. Knowledge distillation has emerged as an effective technique to address this challenge, by training a smaller network (student) under the supervision of a larger network (teacher). While previous methods focused on using same-level information to guide the student, we propose a novel approach that utilizes low-level features from the teacher to supervise deeper features of the student, resulting in improved performance. We introduce a review mechanism that extracts useful information from multi-level information of the teacher and transfers them to the student. To enhance this mechanism, we propose a residual learning framework, an attention-based fusion module, and a hierarchical context loss function. Experimental results demonstrate that our proposed knowledge review strategy achieves state-of-the-art performance in various computer vision tasks. The main contributions of this paper include the proposal of a review mechanism in knowledge distillation, a residual learning framework, and the introduction of an attention-based fusion module and a hierarchical context loss function.