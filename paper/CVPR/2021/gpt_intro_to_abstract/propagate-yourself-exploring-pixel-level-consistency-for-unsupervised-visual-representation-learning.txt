This paper introduces pixel-level pretext tasks for self-supervised visual representation learning. The current self-supervision methods focus on image-level comparisons, leading to representations that lack spatial sensitivity for pixel-level predictions. To address this, the authors propose two approaches: PixContrast, which uses contrastive learning at the pixel level, and PixPro, which incorporates pixel-to-propagation consistency. Empirical results show that PixPro outperforms PixContrast across various downstream tasks. The proposed pixel-level pretext tasks also demonstrate effectiveness in pre-training both backbone and head networks, making them beneficial for downstream tasks with limited annotated data. A combination of the pixel-level and instance-level methods further enhances performance. The proposed PixPro achieves state-of-the-art transfer performance on object detection and semantic segmentation benchmarks. The authors advocate for evaluating unsupervised representation learning based on downstream task performance rather than just linear classification on ImageNet.