Batch Normalization (BN) has been widely used in deep neural networks to address the issue of internal covariate shift, where network activations change during training due to updates in network parameters. However, BN assumes that the distribution statistics computed from each mini-batch reflect the statistics of the full training set. This assumption breaks down in the small batch size regime, limiting the application of BN to memory-consuming tasks such as object detection and semantic segmentation.In order to improve estimation of statistics in the small batch size regime, alternative normalizers such as Layer Normalization (LN), Instance Normalization (IN), and Group Normalization (GN) have been proposed. Each of these methods computes the mean and variance over different dimensions, but they may not be suitable for all tasks. Synchronized BN (SyncBN) processes larger batch sizes across multiple GPUs, but it introduces overhead due to synchronization.To address the limitations of existing approaches, this paper proposes a method called Cross-Iteration Batch Normalization (CBN). CBN computes statistics over examples from multiple recent training iterations, effectively enlarging the pool of data used for normalization. To account for network weight changes among iterations, CBN uses a low-order Taylor polynomial approximation to approximate the means and variances of recent iterations for the current network weights. These compensated statistics are then combined with the statistics of the current iteration. Experimental results show that CBN leads to performance improvements over original BN in the small batch size regime, as demonstrated in ImageNet classification and object detection on COCO. This approach introduces negligible overhead, as the statistics from previous iterations have already been computed. The success of CBN suggests that time-based cues extracted from training iterations can be effectively used for batch normalization, opening new directions for investigation.