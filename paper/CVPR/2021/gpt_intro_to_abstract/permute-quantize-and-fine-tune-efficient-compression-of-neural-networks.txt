State-of-the-art computer vision tasks rely heavily on deep neural networks, which often come with significant memory and computational requirements, limiting their usability on mobile and low-power computational platforms. However, recent studies have shown that these networks are often overparameterized and can be compressed without sacrificing accuracy. Scalar quantization is commonly used for network compression, but it has limitations in achieving high compression rates. To address this, researchers have explored vector quantization (VQ) approaches that compress multiple parameters into a single code. VQ has demonstrated state-of-the-art compression-to-accuracy ratios in computer vision and natural language processing tasks. However, deciding which network parameters should be compressed jointly can be challenging, especially for layers without spatial dimensions. Current approaches rely on suboptimal clustering or minimizing reconstruction error, which is hard to optimize. In this paper, we propose the Permute, Quantize, and Fine-tune (PQF) algorithm, which leverages concepts from rate-distortion theory to identify redundancies among parameter groups and find permutations of network weights that lead to easier quantization while maintaining functionality. PQF includes an efficient annealed quantization algorithm to reduce quantization error and uses gradient-based fine-tuning to recover network accuracy. Our approach achieves state-of-the-art results in terms of model size vs. accuracy, as demonstrated by compressing popular architectures for image classification, object detection, and segmentation. For example, on Imagenet object classification, compressing a ResNet-50 model down to about 3 MB results in a 40-60% relative error reduction compared to the current state-of-the-art (∼31× compression). Similarly, compressing a Mask-RCNN architecture down to about 6.6 MB leads to a 60% (resp. 70%) relative error reduction in object detection (resp. mask segmentation) on COCO compared to previous work (∼26× compression).