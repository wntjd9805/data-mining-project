Deep convolutional neural networks (CNNs) have made significant advancements in various computer vision tasks such as image classification, segmentation, and object detection. However, CNNs struggle to effectively capture global context information in an image due to the local context characteristics of convolutional kernels. To address this limitation, attention mechanisms, particularly channel attention mechanisms, have been widely adopted. The squeeze-and-excitation network (SENet) introduced adaptive channel-wise dependencies, leading to improved performance in various CNNs. Several channel attention blocks have since been proposed, but they often introduce a large number of parameters to learn the relationship between global contexts and attention activations, which may not be optimal.This paper introduces a novel channel attention block called the Gaussian Context Transformer (GCT) that directly maps global contexts to attention activations using a Gaussian function. The GCT block hypothesizes a negative correlation between global contexts and attention activations and utilizes the Gaussian function to excite the normalized global contexts. The GCT-B0 variant is a parameter-free attention block that doesn't require contextual feature transform learning. Experimental results show that GCT-B0 outperforms other state-of-the-art channel attention blocks without introducing any parameters. Additionally, a learnable variant, GCT-B1, which adaptively learns the standard deviation of the Gaussian function, is also proposed.The contributions of this work include providing insight into the fact that parameterized contextual feature transform learning is not essential for channel attention mechanisms, proposing a simple and efficient channel attention block (GCT) that significantly boosts the performance of deep CNNs and detectors, and demonstrating the superiority and generalization ability of the proposed GCTs through comprehensive experiments on ImageNet and MS COCO datasets. GCT-B0, in particular, achieves impressive results without introducing any parameters.