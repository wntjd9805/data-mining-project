The ability to segment the visual field into coherently moving regions is a fundamental trait shared among visual animals. Humans, in particular, spend significant amounts of time interacting with moving objects during early development, which aids in object modeling and learning invariances. The mature visual system, on the other hand, can learn objects from static examples without the need for motion. This suggests that motion can serve as a cue to bootstrap object models for detection in static images without explicit supervision. In this paper, we propose a method called Dynamic-Static Bootstrapping (DyStaB) to learn object segmentation using unlabeled videos. Our method utilizes a motion segmentation module to separate regions in a temporally consistent manner. The resulting motion segmentation then primes a detector for static images, creating a feedback loop that reinforces both modules. During training, the dynamic model minimizes the mutual information between partitions of the motion field, while enforcing temporal consistency, resulting in a state-of-the-art unsupervised motion segmentation method. The resulting putative regions, along with their uncertainty, are used to train the static model. Our approach achieves superior performance in motion segmentation and object segmentation in both video and static images, surpassing the state-of-the-art methods by 10% in average precision across six benchmark datasets. Remarkably, our method outperforms recent supervised methods by almost 5% on average, without requiring any manual annotation.