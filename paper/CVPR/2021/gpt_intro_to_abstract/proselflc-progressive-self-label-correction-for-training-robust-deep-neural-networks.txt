This paper introduces various target modification approaches in computer science. The approaches can be categorized into two groups: output regularization (OR) and label correction (LC). OR methods aim to penalize over-confident predictions in order to regularize deep neural networks, while LC methods add similarity structure information to label distributions to create structured and soft learning targets. LC can also correct noisy label distributions. LC can further be divided into non-self LC, which requires additional learners, and self LC, which relies on the model itself. Non-self LC approaches include knowledge distillation, where predictions from other models are used as auxiliary information. Self LC methods include Pseudo-Label, bootstrapping, Joint Optimization, and Tf-KDself. The paper identifies the limitations of existing approaches and proposes a novel method called Progressive Self Label Correction (ProSelfLC). ProSelfLC progressively and adaptively modifies the target labels during the training process. The paper also addresses the question of whether low entropy should be penalized or rewarded, and defends the concept of entropy minimization. The main contributions of the paper include a theoretical study on target modification methods, the proposal of ProSelfLC, and extensive experiments that demonstrate the effectiveness of ProSelfLC in clean and noisy settings.