Generative Adversarial Networks (GANs) have shown impressive results in image generation tasks. However, the training process for GANs is time-consuming. Existing GANs use a two-stage training process where the generator and discriminator are trained alternately. This process involves repetitive computations and is burdensome. Some approaches have been proposed to address this issue but are limited to specific types of GANs. In this paper, we propose a novel one-stage training scheme called One-Stage GANs (OSGANs) that can be applied to both Symmetric and Asymmetric GANs. The key idea is to integrate the optimization for the generator and discriminator during forward inference and decompose their gradients during back-propagation. For Symmetric GANs, we only need to compute the gradient of one term and adopt it for both losses. For Asymmetric GANs, we carefully analyze the composition of the discriminator's gradients and discover a feasible solution to decompose and update the gradients of different adversarial terms. We show that Symmetric GANs can be treated as a degenerate case of Asymmetric GANs. Computational analysis and experimental results demonstrate that OSGANs achieve a 1.5Ã— speedup compared to the vanilla adversarial training strategy.