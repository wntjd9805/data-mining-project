Motion pattern understanding is crucial for various video understanding tasks, such as action recognition, action localization, and action detection. Traditional methods have used hand-crafted features based on dense trajectories and optical flow to represent motions. However, with the rise of deep neural networks and large-scale image and video datasets, researchers have focused on developing architectures to extract meaningful motion features using deep learning techniques. One challenge in training 3D convolutional models is the need for a large amount of manually labeled videos. Recently, self-supervised learning has emerged as a powerful technique for training video models without labeled data. These methods learn visual representations by predicting spatial or temporal sequences or by predicting partial contents. However, none of them explicitly aim to model motion information of videos. In this work, we propose a learning framework called MoSI (Motion Synthesis from Images) that learns motions directly from static images. By generating pseudo motions from images and correctly classifying their direction and speed, MoSI enables models to encode motion patterns. Additionally, a static mask is applied to the pseudo motion sequences to guide the network's attention to inconsistent local motions. MoSI leverages large-scale image datasets, such as ImageNet, to pre-train video models, allowing for powerful motion understanding. Experimental results on HMDB51 and UCF101 datasets demonstrate the effectiveness of MoSI, achieving state-of-the-art results for learning video representations using RGB modality. This work represents the first time that static images are used as a data source for pre-training video models.