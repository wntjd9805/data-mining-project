Instance segmentation in computer vision is a challenging task that involves identifying each individual instance of a class of objects along with their visible extent. This information is crucial for scene understanding. With the increasing use of 3D sensors like LiDAR and laser scanners, point clouds have become an important modality for scene understanding. However, instance segmentation in 3D point clouds is more difficult due to the irregularity, sparsity, and diversity of the data. Existing approaches for point cloud instance segmentation often rely on bottom-up strategies, which involve heuristic grouping algorithms or complex post-processing steps. These methods have limitations in terms of performance sensitivity to hyperparameter values, the need for manual tuning, and the reliance on proposal quality. In this paper, we propose a novel pipeline called DyCo3D that tackles 3D point cloud instance segmentation using dynamic convolution. Our approach uses a few convolution layers with filters generated on the fly, conditioned on the category and position of the instance. We also propose a category-specific context encoding sub-network to enhance filter distinguishability and parallel decoding of instance masks. Additionally, we introduce a small transformer to capture long-range dependencies and build high-level interactions among different regions. Our method outperforms previous approaches in terms of efficiency, effectiveness, and robustness. Experimental results demonstrate state-of-the-art performance, improved robustness, and faster inference speed compared to existing methods.