In recent years, multi-modal dialog tasks have become popular in the field of computer science. These tasks, such as GuessWhat?!, VisDial, and ImageChat, require models to understand both images and language, as well as track and comprehend multi-turn dialogues. One challenging dataset for evaluating these tasks is GuessWhat?!, a two-player game where one player asks binary questions about objects in an image, and the other player provides yes/no answers. This paper focuses on three agents in the GuessWhat?! dataset: the Oracle model, Guesser model, and Questioner model.The Oracle model is essentially an object-aware Visual Question Answering (VQA) task, where the model answers yes/no questions about a target object in an image. To improve the Oracle model's performance on complex questions, we introduce VilBERT-Oracle, which leverages the VilBERT model's state-of-the-art performance on VQA tasks. We also propose a two-way fusion mechanism to predict correct binary answers related to a target object.The Guesser model can be viewed as a referring expression comprehension problem, where the model must make a final guess based on the image and the entire dialogue history. We introduce VilBERT-Guesser, which builds on the strength of VilBERT in single-turn referring expression comprehension and incorporates an object state tracking mechanism. This allows the model to dynamically update beliefs about object states throughout the dialogue, significantly improving performance.The Questioner model is a special case of the referring expression generation problem. Previous approaches typically encode image features and dialogue history to generate questions. However, these models may struggle with long-term history and generating repetitive questions. Inspired by recent work on state-tracking in the Questioner model, we introduce an object state estimation mechanism into VilBERT-Questioner. Additionally, we leverage the weights from the trained VilBERT-Guesser model to improve the Questioner's state estimation capabilities.Our contributions include novel Oracle, Guesser, and Questioner models, which outperform existing state-of-the-art models on the GuessWhat?! dataset. We also propose a unified framework for the Guesser and Questioner models, allowing the Questioner to benefit from the robust state-estimator learned from VilBERT-Guesser. Through thorough analysis and ablation studies, we find that a shared vision-linguistic representation across the three agents enhances mutual understanding and end-game success. Our code is publicly available.