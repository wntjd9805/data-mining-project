Online 3D dense reconstruction has seen significant advancements, enabling various applications such as augmented reality and robotic navigation. Previous approaches focused on globally consistent 3D reconstruction but struggled with high memory storage requirements and suboptimal geometric quality. Recent developments in deep geometry learning have demonstrated the efficacy of implicit geometric representations parameterized by neural networks, allowing for more flexible and efficient reconstruction. These representations also enable encoding geometric priors for shape interpolation and reconstruction. We seek to incorporate deep implicit representations into online 3D reconstruction systems but face challenges related to modeling geometric uncertainty, accurate camera tracking formulation, and efficient surface mapping strategy. In response, we propose DI-Fusion, the first online 3D reconstruction system with tracking and mapping modules supported by deep implicit representations. We introduce Probabilistic Local Implicit Voxel (PLIVox), an extension of local implicit grids that encodes both scene geometry and uncertainty using a deep neural network. We demonstrate the utility of this uncertainty encoding in online depth fusion. Our approach includes an approximate gradient solution for efficient camera tracking and a encoder-decoder network design for high-quality and efficient geometry integration. Through evaluation on public 3D RGB-D benchmark datasets, we show improved tracking and mapping quality compared to previous representations. Our implementation is publicly available.