Capturing and modeling real scenes from image inputs is a well-studied problem in the fields of computer vision and computer graphics. The goal is to create a 3D model that accurately represents the scene and can be used for realistic rendering in applications such as e-commerce, virtual reality, and augmented reality. Traditional 3D reconstruction methods typically represent objects as meshes, which are then combined with textures for appearance editing. However, mesh-based reconstruction struggles to produce highly realistic images for complex objects. In recent years, various neural scene representations have been developed to address this challenge. One such representation is NeuTex, which represents geometry as a 3D volume and appearance as a 2D neural texture in an automatically discovered texture UV space. This approach allows for the synthesis of highly realistic images and intuitive surface appearance editing. Unlike volume-based methods, NeuTex disentangles geometry and appearance, enabling easier editing and enhancing the practicality of neural rendering approaches. The main contribution of this paper is the introduction of NeuTex, which combines volumetric representation of geometry with texture mapping for appearance, allowing for both realistic image synthesis and flexible surface appearance editing. The authors describe the architecture and training process of NeuTex, including a novel cycle consistency loss to ensure a consistent mapping between the 2D texture space and the 3D surface. The paper demonstrates the effectiveness of NeuTex in generating photo-realistic images and outperforming traditional mesh-based reconstruction methods and previous neural rendering methods. The ability to recover a meaningful surface-aware texture parameterization and enable surface appearance editing applications makes NeuTex a valuable tool in 3D design workflows.