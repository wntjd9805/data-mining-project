Point cloud registration is a crucial problem in computer vision, essential for tasks such as odometry, mapping, re-localization, and SLAM. With the advent of new 3D sensors and multi-sensor setups, the expectations for point cloud registration have increased, requiring more precise feature correspondences. Most state-of-the-art methods for point cloud registration use locally describable features in a global optimization step, without utilizing modern machine learning algorithms. However, recent research in point cloud processing has shown the potential of neural networks in improving registration, mapping, and odometry. Traditional methods based on global rigid body operations assume static environments and proper viewpoints, limiting their performance in unstable and challenging situations. To address these limitations, we propose StickyPillars, a novel point cloud registration approach using graph neural networks based on pillar-shaped point descriptors. Our approach focuses on computing feature correspondences rather than end-to-end odometry estimations and utilizes a ground truth match matrix in the training process. This allows us to predict poses with low computation time and robustness against dynamic objects. We demonstrate the robust real-time registration capabilities of StickyPillars, showcasing its effectiveness under challenging conditions like dynamic environments, challenging viewpoints, and small overlapping areas. Through evaluation on the KITTI odometry benchmark, we significantly outperform state-of-the-art frame to frame matching approaches, enabling more precise odometry estimation for applied robotics.