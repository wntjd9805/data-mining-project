Retargeting human body motion, the process of transferring motion from one subject to another, has gained significant attention in recent years due to its practical and entertaining applications in content generation. These applications include creating special effects in cinema and television, generating entertaining performances, and creating deepfake videos. However, retaining the target subject's identity while rendering them in new poses remains a challenging task.Existing approaches often require a large number of training frames of a specific person and substantial training time. In contrast, this work focuses on the few-shot setting, where only a few reference images of the target subject are available, and fast video generation is desired without subject-specific training. To overcome the lack of data, other techniques leverage existing human body models to construct an approximate representation of the subject. However, these explicit representations have limited modeling power and fidelity.This work proposes a flexible and expressive modeling power by exploiting an implicit 3D representation called the Transformable Bottleneck Network (TBN). TBN encodes image content into an implicit volumetric representation, allowing for novel view synthesis of rigid objects. Building upon this approach, the authors tackle the challenge of performing motion retargeting for non-rigid humans. They address challenges such as aggregating volumetric features from images with changes in camera and body pose and learning this aggregation without explicit 3D or camera pose supervision.The authors introduce a multi-resolution scheme to retain fine-grained details while expressing large-scale motion. Their network pipeline is designed to extract and manipulate the foreground of the encoded images, with a separate network for handling the background. They employ specialized training techniques to produce plausible results without explicit 3D supervision or the use of explicit 3D models. This approach allows for learning directly from real 2D images and videos.Experimental results show that the proposed approach outperforms state-of-the-art methods in human motion transfer, even when using only a few example images or a single image of the target subject. The authors present novel neural network architectures for implicit volumetric human motion retargeting, a framework for training these networks with limited target-specific data, and evaluations demonstrating the superiority of their approach over existing alternatives.