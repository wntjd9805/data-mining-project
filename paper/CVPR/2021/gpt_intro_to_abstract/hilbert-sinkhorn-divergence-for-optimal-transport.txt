Optimal transport theory, a tool for comparing probability distributions, has found numerous successful applications in machine learning, including generative modeling, domain adaptation, dictionary learning, text mining, sampling, and single-cell genomics. An important concept in optimal transport is the Wasserstein or Earth-Mover distance, which measures the minimal cost required to deform one distribution into another. Unlike other divergences such as Kullback-Leibler and L2 distance, the Wasserstein distance allows for a geometrically faithful comparison of probability distributions, resulting in a rich geometric structure on the space of probability distributions.