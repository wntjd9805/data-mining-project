The goal of this work is to animate a target person using a single input image to mimic the motion of a driving person in a video sequence. The proposed method extends the capabilities of existing methods by allowing variations in body shape, age, ethnicity, gender, pose, and viewpoint of the target person, as well as unconstrained motion sequences and arbitrary backgrounds. Existing methods often struggle with maintaining the target person's appearance and avoiding mixing elements from the driving video, as well as requiring an input video of the target person and producing unnatural motion limited to specific backgrounds. To achieve these capabilities, the method utilizes pre-trained pose recognition networks, human parsing networks for image segmentation, face embedding networks, and inpainting networks. Additionally, specific representations are employed to capture clothing and face appearance realistically. The method separates the pose and frame generation parts, each performed by a different network. Extensive experiments show that the proposed method provides accurate and visually pleasing results, outperforming previous methods based on numerical metrics, user studies, and visual examples. The method emphasizes handling diversity in the target and generated individuals, promoting inclusion in this line of work.