Virtual reality (VR) and om-nidirectional images (ODIs) have become increasingly important in human life. ODIs provide an immersive and interactive experience through the use of head-mounted displays (HMDs). However, due to constraints in capture, storage, and transmission, the resolution of ODIs is often limited. Super-resolution (SR) techniques have been used to address this issue for two-dimensional (2D) planar images, but they are not suitable for ODIs that are projected onto 2D planes using equirectangular projection (ERP). This projection method results in non-uniform pixel density across latitudes and significant distortion in high-latitude areas. Existing ODI SR methods rely on assembling multiple low-resolution (LR) ODIs to create a high-resolution (HR) ODI, but their performance depends on the number of LR images and registration accuracy. A recent approach using generative adversarial networks (GANs) does not consider the varying pixel density across latitudes. In this paper, we propose a novel approach called the latitude adaptive upscale network (LAU-Net) that dynamically upscales different latitude bands of ODIs with various upscaling factors. We train evaluators for each latitude band using reinforcement learning (RL) to determine the optimal upscaling factors. Our LAU-Net achieves better SR performance with lower computational complexity by stopping training on "easy" patches at the first level and progressively going deeper for "hard" patches. Our contributions include establishing a large ODI SR database, proposing LAU-Net with distinct upscaling factors for different latitude bands, and developing an RL scheme for automatic upscaling factor selection.