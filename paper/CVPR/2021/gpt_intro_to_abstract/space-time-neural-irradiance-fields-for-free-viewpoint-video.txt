This paper focuses on the problem of rendering videos from novel viewpoints. Traditional methods require complex hardware setups with multiple cameras capturing scenes from different angles. However, this paper proposes a solution that utilizes single videos captured from everyday devices like smartphones, without the need for dedicated hardware. The authors build on recent advancements in monocular video depth estimation to create a globally consistent, spatiotemporal representation of dynamic scenes. This representation overcomes issues such as unnatural stretches and disoccluded holes that arise from per-frame depth-based warping. Instead of using frame-wise representations, the authors leverage neural implicit representations to achieve continuous representations without resolution loss. By learning neural irradiance fields as a function of space and time, they can render the video from novel viewpoints and time. The proposed method aggregates frame-wise 2.5D representations into a globally consistent spatiotemporal representation and addresses motion-appearance ambiguity using video depth supervision. Experimental results demonstrate the effectiveness of the approach in providing a compelling free-viewpoint video rendering experience using casual videos captured from smartphones.