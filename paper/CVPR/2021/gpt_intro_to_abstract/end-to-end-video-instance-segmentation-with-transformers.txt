Instance segmentation plays a crucial role in computer vision, with significant advancements made in image instance segmentation. However, video instance segmentation has received less attention, despite its increased complexity due to the need to perform instance segmentation for each frame and establish data association across consecutive frames. Existing methods employ complex pipelines, relying on image-level instance segmentation models and human-designed rules. In this paper, we propose a new video instance segmentation framework, VisTR, based on Transformers, which simultaneously classifies, segments, and tracks object instances in videos. By exploiting the richer information present in video frames, such as motion patterns and temporal consistency, we aim to develop a simple and effective end-to-end framework for video instance segmentation. We demonstrate that Transformers, with their ability to model long-range dependencies through self-attention, are well-suited for video processing tasks like instance segmentation. VisTR views video instance segmentation as a parallel sequence decoding/prediction problem and utilizes a bipartite graph matching strategy and a segmentation module to maintain the order of outputs and obtain the mask sequence for each instance. Our framework achieves impressive results on the YouTube-VIS dataset, with a mask mAP of 38.6% at a speed of 57.7 FPS, outperforming other single-model methods. Overall, VisTR simplifies the video instance segmentation pipeline and showcases the potential of Transformers in a wide range of video processing tasks in computer vision.