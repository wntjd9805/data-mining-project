Human motion transfer methods, also known as performance cloning or reenactment methods, have important applications in AR/VR and video editing. These methods generate realistic video animations of an actor following a target motion specified by a user. Current motion transfer methods utilize machine learning techniques to learn a direct mapping between an actor-independent motion space and the resulting target actor's appearance space. However, existing approaches often require a training video of an actor performing a rich set of motions, and struggle to accurately capture the fine-scale details and dynamics of actors wearing loose clothing.In this paper, we present a new human motion transfer framework that addresses these limitations. Our framework generates visually plausible video animations that are spatially and temporally coherent, and exhibit natural dynamics even for actors wearing loose garments. We achieve this by training a stack of deep generative networks to learn a mapping from 2D pose to a silhouette with semantic part labels, and per-pixel appearance of the actor. We also model the person's shape as a dense foreground silhouette mask with per-pixel labels encoding assignment to limbs and garments. To capture the structure of wrinkles and texture patterns of garments, we extract local image gradients using a bank of oriented filter kernels.By explicitly decoupling the actor's appearance into intermediate representations of silhouette and structure, our method enhances the temporal and spatial quality of synthesized videos. Additionally, our representation provides greater control over the image generation process, allowing manipulation of color, appearance, and garment style in a purely image-based manner. Our contributions include: (1) a new motion transfer framework that focuses on visually-plausible fine-scale deformations and dynamics in clothing, (2) a decomposition of the pose-to-image translation task into better conditioned cascaded processes, and (3) the use of intermediate representations to improve temporal coherence and enable control over individual aspects of the final rendering.Compared to current approaches, our method improves visual fidelity while using only a single RGB camera. Furthermore, our framework provides an additional level of control over the final image generation, offering opportunities for creative manipulation of garment style and appearance.