Dense prediction tasks, such as semantic segmentation and audio source separation, often require high-dimensional input data and produce predictions of the same dimensions. To handle this efficiently and model the context within a large field, various neural network architectures have been proposed. Convolutional neural networks (CNNs) have become a crucial component in these architectures, with advanced CNN architectures proposed to improve performance. One important component of these architectures is a skip connection that creates shortcuts from early layers to later layers. DenseNet is a proposed architecture that connects all preceding layers with a dense connectivity pattern, allowing maximum information flow and making CNNs deeper while reusing intermediate representations.Deeper CNNs have larger receptive fields, enabling the modeling of a large context. This is particularly important for tasks that require wide-area or long-term dependence, such as semantic segmentation and audio tasks. However, simply stacking convolution layers is not an optimal way to increase the receptive field, as it requires too many layers and makes training difficult. Alternative approaches include downsampling intermediate network outputs and incorporating low-resolution representations or using dilated convolution to cover a large receptive field with fewer layers. Previous CNN architectures have interchanged information in different resolutions only a few times, but a more frequent interchange could be beneficial due to the dependence between local and global structures.This paper proposes a novel CNN architecture that densely incorporates representations in multiple resolutions. It combines the advantages of dense skip connections and dilated convolution, introducing a new network architecture called the multidilated dense block (D2 block). To combine these elements effectively, the paper introduces a multidilated convolution layer with multiple dilation factors within a single layer. A nested architecture of multidilated dense blocks, called D3Net, is also proposed to effectively repeat dilation factors multiple times with dense connections. The importance of dense multiresolution representation learning is emphasized, and the proposed methods are evaluated on two dense prediction tasks: image semantic segmentation and audio source separation. The results demonstrate the effectiveness of the proposed architecture over state-of-the-art baselines in both tasks, highlighting its generality across different task types and data domains.