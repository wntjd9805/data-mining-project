The progress of deep neural networks in decision-critical applications has created a need for human-understandable explanations for model predictions. Model output attribution, which identifies the importance of input features, has become a popular approach for this purpose. However, the susceptibility of neural networks to false predictions for imperceptibly modified inputs has presented a challenge in generating reliable explanations. Current methods often fail to accurately identify essential features and can be manipulated by random or adversarial perturbations. In this paper, we propose RelEx, a novel method for generating robust and accurate saliency maps of pixel-level importance for a given image. Inspired by adversarial training, RelEx is built on the assumption of a locally smooth explanation for the input vicinity. We address the lack of existing methods in simultaneously achieving robustness and accuracy in explanation generation.Our analysis characterizes the behavior of RelEx based on the assumption of local explanations. We establish a quadratic approximation to identify the trade-off between accuracy and robustness in saliency maps. We find that the curvature of the loss function for learning a saliency map is inversely proportional to the â„“1-norm of saliency maps, which affects explanation accuracy. We propose an easy-to-implement objective function for learning a saliency map using backpropagation.In evaluations on naturally and adversarially trained models, RelEx outperforms existing methods in identifying input features relevant to decision-making. Our method exhibits remarkable robustness in retrieving target classes from strong white-box attacks. The advantage of RelEx lies in its ability to learn appropriate saliency maps even in the presence of severe perturbations.Overall, this paper introduces a reliable method for explaining predictions of neural network-based classifiers, addressing the need for robust and accurate explanations in decision-critical applications.