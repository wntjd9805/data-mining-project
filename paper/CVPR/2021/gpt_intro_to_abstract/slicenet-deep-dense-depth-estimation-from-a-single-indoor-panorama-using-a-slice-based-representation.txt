Understanding the 3D layout of indoor scenes from images is crucial for various applications, including augmented reality, indoor mapping, autonomous navigation, 3D reconstruction, and scene understanding. Prior methods for depth estimation from single images have relied on limited field-of-view cameras, but with the emergence of 360-degree capture, new solutions are needed. Existing approaches that adapt traditional depth estimation methods to 360-degree images have shown sub-optimal results. In this paper, we propose SliceNet, a deep neural network that predicts depth maps from indoor 360-degree images using a gravity-aligned equirectangular projection. We partition the input image into vertical slices, preserve global information using a convolutional LSTM network, and decode the slices to increase vertical resolution. Our contributions include a slice-based representation that exploits the characteristics of equirectangular projections without the need for distortion-aware convolution, and a refined feature flattening technique. We also introduce a LSTM multi-layer module for recovering spatial relationships between slices. Experimental results on synthetic and real datasets demonstrate that our method outperforms existing approaches in prediction accuracy, especially for real-world scenes. Our method leverages gravity alignment, which is common in indoor scene capture, and is robust to small variations in inclination.