Referring expression comprehension is a topic of great importance in computer vision, as it involves localizing regions described by natural language in images or videos. This capability is crucial for various research problems, including image/video captioning, visual question answering, and image/video retrieval. It also plays a significant role in machine intelligence applications such as human-computer interaction, robotics, and early education.While previous work has focused on referring expression comprehension for static images and achieved promising results, the comprehension of referring expressions in videos has been less explored. This task is challenging yet important, as it involves localizing spatio-temporal tubes that correspond to the entire sentence. Existing approaches treating referring expression comprehension in videos as a tracking problem suffer from template selection errors, and proposals to match spatio-temporal tube candidates with textual features have limited performance.To address these challenges, we propose a new approach called co-grounding for referring expression comprehension in videos. Our method integrates semantic attention learning in a one-stage framework based on YOLO, predicting bounding boxes and confidence simultaneously. We employ a semantic attention mechanism to obtain attribute-specific features for both vision and language, including a proposal-free subject attention scheme to parse subject-related words and an object-aware location attention scheme to parse location-related words from the expression. The interaction between attribute-specific textual and visual features determines the subject score and location score for each visual region. To improve cross-frame prediction consistency, we introduce co-grounding feature learning, which utilizes frame correlation to enhance visual features and stabilize the grounding process during training and testing. Additionally, a post-processing strategy is applied to enhance temporal consistency during inference.Our contributions include proposing co-grounding as an effective approach to referring expression comprehension in videos in a one-stage framework. We introduce semantic attention learning to parse referring cues, including proposal-free subject attention and object-aware location attention. Our networks achieve state-of-the-art performance on referring expression comprehension benchmarks for both video and image grounding.