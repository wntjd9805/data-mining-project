Detecting 3D objects and estimating their 6D poses is a crucial task in computer vision applications such as augmented reality, robotics, and machine vision. While deep learning approaches have shown impressive pose estimation results using RGB images, they often rely on manual labels that can be costly to obtain. Some works have attempted to train on synthetic images rendered from 3D object models to overcome this cost, but there is a significant discrepancy between synthetic and real images, known as domain shift.To address the domain shift, previous methods have applied data augmentation techniques or transfer learning using unlabeled target data. However, these methods still suffer from reduced performance and limitations in 6D pose estimation accuracy. In this paper, we propose a keypoint-graph-driven learning framework that combines domain transfer and task optimization for object pose estimation across domains.Our framework, called Domain Adaptive Keypoints Detection Network (DAKDN), predicts 2D keypoints of objects across domains. We train DAKDN using both synthetic images and unlabeled real images, leveraging an adaptation layer and a domain alignment loss based on maximum mean discrepancy. To improve keypoint detection on real images, we transfer a domain-invariant structure among keypoints from synthetic to real images as a constraint. We represent the geometry relations between keypoints as graphs and train a graph convolutional network (GCN) to model the structure of object keypoints from synthetic images. This structure is then transferred to real images, guiding DAKDN to accurately detect keypoints.By jointly optimizing for domain invariance and structure prediction, our method achieves better results compared to state-of-the-art approaches without the need for manual pose labels. It also competes with approaches that rely on real manual pose labels images. Our experiments demonstrate the effectiveness of our approach in addressing the lack-of-label problem for 6D pose estimation across domains.