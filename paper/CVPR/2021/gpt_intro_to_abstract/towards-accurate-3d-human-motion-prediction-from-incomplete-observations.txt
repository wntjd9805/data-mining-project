3D human motion prediction is a significant research topic in computer vision, with applications in human behavior understanding, machine intelligence, and autonomous driving. Existing approaches typically treat this as a sequence-to-sequence generation task and utilize recurrent neural networks (RNNs) or graph convolutional networks (GCNs) to predict future human motion. However, these approaches overlook the problem of incomplete historical observations, which are common in real-world scenarios due to occlusion or missing sensor measurements. This can lead to unexpected or distorted predictions and hinder the accuracy of human motion prediction. To address this issue, we propose a novel multi-task graph convolutional network (MT-GCN) that simultaneously considers predicting human actions and repairing incomplete observations. The MT-GCN consists of three modules: a shared context encoder (SCE), a sequence repairing module (SRM), and a human action predictor (HAP). The SCE utilizes GCNs and temporal convolutional networks (TCNs) to extract the context code of 3D skeleton sequences. The SRM incorporates GCNs and a temporal self-attention mechanism to repair corrupted poses by selecting relevant information from the sequence. The HAP uses a multi-head graph attention network (GAT) to aggregate information from neighboring nodes and a non-autoregressive pipeline to generate each predicted frame independently. Position embedding is also introduced to ensure continuity in the predicted sequence. The contributions of this work are threefold: (1) It explicitly addresses the problem of predicting human motion with missing values in observed poses; (2) It proposes a multi-task learning framework that considers both repairing corrupted observations and predicting future actions; (3) It achieves state-of-the-art performance on three large-scale benchmarks compared to existing approaches. This research opens up new possibilities for more accurate and robust human motion prediction in real-world scenarios.