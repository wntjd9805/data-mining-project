This paper focuses on the collection and analysis of language that explains emotions generated by observing visual artworks. The goal is to understand the link between the visual properties of an artwork, the subjective affective experience it produces, and how these emotions are explained through language. The study specifically focuses on visual art because it is often created to provoke emotional reactions from viewers and often defies simple explanations. The researchers introduce a large-scale dataset called ArtEmis, which associates human emotions with artworks and contains explanations in natural language. The dataset allows for subjective and personal emotional explanations, distinguishing it from other captioning datasets. The difficulty of producing emotional explanations in language is discussed, as well as the potential of ArtEmis for building affective neural speakers. The paper also discusses related works in computer vision and deep learning, and the contributions of the research include the introduction of the ArtEmis dataset and the development of machine learning models for emotion prediction and generating emotion explanations.