Recent advancements in face recognition methods have focused on using margin penalty to improve the discriminative feature embedding. However, these methods often represent each class as a single point in the latent space, which does not capture class-wise variations. This can lead to model degeneration, especially in cases where there is limited intra-class variation. In this paper, we propose a Variational Prototype Learning (VPL) method that represents each class as a distribution instead of a point. This allows for sample-to-sample comparisons within the classification framework, leading to more exploratory behavior by the SGD solver. We also introduce a computationally efficient and memory-saving approach for variational prototype sampling by injecting memorized features into the corresponding prototypes. Our VPL approach is a plug-and-play module that can be integrated with existing margin-based or mining-based softmax methods. Experimental results on popular face recognition benchmarks demonstrate the superiority of our VPL method over state-of-the-art competitors. This work addresses the limitations of prototype learning and provides a novel approach for improving deep face recognition performance.