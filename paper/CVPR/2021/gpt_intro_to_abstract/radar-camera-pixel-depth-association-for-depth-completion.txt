This paper aims to investigate the potential of incorporating automotive radar as a contributing sensor to 3D scene estimation. While previous research has focused on fusing radar with video for object detection, this study proposes the pixel-level fusion of radar and video for improved dense depth estimation. Despite LiDAR, stereo, and monocular techniques dominating outdoor depth estimation, the fusion of LiDAR and video has shown promising results for accurate dense depth completion. However, radar has been primarily used for object detection in Advanced Driver Assistance Systems (ADAS). This paper explores the suitability of using radar instead of LiDAR for dense depth estimation and proposes a two-stage algorithm to fuse radar returns with image data for depth completion. The first stage establishes an association between radar returns and image pixels, filtering occluded radar returns and densifying the projected radar depth map. The second stage utilizes a standard depth completion approach to combine radar and image data and estimate a dense depth map. The lack of public datasets with radar presents a practical challenge, but the nuScenes dataset is used for experiments along with its annotations. The key contributions of this work include upgrading the projection of radar onto images for improved depth estimation, enhancing radar depth for accurate depth completion, and leveraging optical flow to accumulate LiDAR ground truth for higher quality dense depth images.