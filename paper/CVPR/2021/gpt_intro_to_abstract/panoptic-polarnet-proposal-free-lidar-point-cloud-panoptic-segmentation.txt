This paper introduces the problem of panoptic segmentation in the context of processing and analyzing 3D scanning data, which is of great importance in applications such as autonomous driving and robotics. Panoptic segmentation aims to unify instance segmentation and semantic segmentation in a single training architecture. The authors propose a panoptic segmentation framework that leverages new LiDAR point cloud datasets to explore this problem for 3D scanning data. The paper discusses the two different categories of panoptic segmentation, proposal-free and proposal-based, and highlights the challenges faced in 2D image panoptic segmentation. The authors emphasize the need for a more suitable architecture for 3D panoptic segmentation considering the unique characteristics of LiDAR data and the requirements for real-time processing. They propose a proposal-free approach that integrates a backbone semantic prediction network with a network for class-agnostic instance clustering. The authors argue that the discretized BEV representation is highly suitable for instance clustering in LiDAR point clouds. They introduce a panoptic segmentation framework, named Panoptic-PolarNet, that simultaneously learns semantic and instance features on the discretized BEV map. The predictions from the semantic and instance heads are fused to create the final panoptic segmentation. The authors evaluate their approach on SemanticKITTI and nuScenes datasets and demonstrate that Panoptic-PolarNet achieves state-of-the-art performance. They highlight the advantages of their approach, including its efficiency in clustering instances on top of semantic segmentation, the shared decoding layers that reduce redundancy and improve computational efficiency, and the avoidance of conflicts in class prediction due to the proposal-free design. The paper also introduces two novel point cloud data augmentation methods. The experimental results show that their approach outperforms strong baselines with smaller and near real-time latency.