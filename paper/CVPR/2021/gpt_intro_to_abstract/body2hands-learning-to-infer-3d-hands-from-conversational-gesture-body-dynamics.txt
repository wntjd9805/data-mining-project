This paper introduces a novel approach for learning conversational hand gestures using a large-scale dataset of in-the-wild videos. Previous methods for capturing hand gestures have relied on rule-based or data-driven approaches, but learning-based methods offer the ability to capture fine-grained details and generalize beyond the training set. However, capturing conversational hand gestures in realistic settings is challenging due to factors such as finger motions, occlusions, and the relatively small size of the hand. To address these challenges, the proposed approach leverages the correlation between body motion and hand gestures during speech. By learning this correlation, the authors are able to build a reliable prior for hand gestures based on the observation of body motion. This allows them to synthesize realistic conversational hand gestures from body-only input and use the learned correlation as a body-motion prior for single-view 3D hand pose estimation. The approach is demonstrated on a publicly-available monologue video dataset and shows promising results in generalizing to other speakers and multi-person scenarios. Overall, this research contributes to the field of computer science by improving the understanding and modeling of conversational hand gestures for effective human-machine interaction.