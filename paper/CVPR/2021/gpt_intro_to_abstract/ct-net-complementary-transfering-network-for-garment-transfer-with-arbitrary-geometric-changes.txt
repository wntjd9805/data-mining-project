Existing virtual try-on methods in computer science are often limited by simplifying assumptions, such as the availability of pure clothing images or 3D information, and simple pose changes without misalignments or occlusions. To address these limitations, we propose a novel image-based garment transfer network called CT-Net. This network does not rely on pure clothing images or 3D information and is capable of dealing with different levels of geometric changes. CT-Net can transfer clothes across different people with arbitrary poses or shapes and synthesize photo-realistic images with well-preserved details of the desired clothing. While various methods have been proposed for virtual try-on, there is still a gap between these methods and realistic scenarios. Some methods use 3D information to handle occlusions, but they are limited by expensive devices and high computational costs. Others rely on stand-alone clothing images, which are not readily available online. Additionally, many methods attempt to model geometric changes using Thin Plate Spline (TPS) warping, which is limited by the number of parameters and can only handle simple deformations. Our proposed CT-Net aims to tackle these challenges by introducing three modules: the Complementary Warping Module, the Layout Prediction Module, and the Dynamic Fusion Module. The Complementary Warping Module uses two complementary warpings, DF-guided dense warping and TPS warping, to accurately align the desired clothes with the target pose and transfer them into the target region while preserving textures. The Layout Prediction Module predicts the target layout based on aligned warping results, providing more accurate guidance for preservation and generation of body parts. The Dynamic Fusion Module combines the information from previous modules using an attention mechanism to synthesize photo-realistic garment transfer results.Experiments conducted on the DeepFashion dataset demonstrate the superiority of our method compared to state-of-the-art methods. Our contributions include the proposal of CT-Net, an image-based garment transfer network that synthesizes photo-realistic garment transfer results with well-preserved clothing characteristics and human identities. We also introduce a Layout Prediction Module that adds spatial constraints to the training of warpings and enhances the accuracy of predictions. Evaluations show that CT-Net outperforms other methods both qualitatively and quantitatively.