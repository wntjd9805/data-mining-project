This paper introduces the problem of limited dynamic range in standard digital cameras and the algorithmic solution of High Dynamic Range (HDR) imaging. HDR imaging aims to create images with a wider dynamic range, similar to what human eyes perceive. To generate HDR images, multiple images with different exposure values are captured and merged. However, when the exposure stack contains camera and object motion, fusing the images can result in undesirable ghosting artifacts. Existing methods for HDR deghosting often rely on registration and exclusion of motion-affected regions or alignment using dense correspondence, resulting in limited quality or warping artifacts. Deep learning-based methods have shown promising results but suffer from the challenge of collecting large amounts of labeled training data for HDR deghosting. This is due to difficulties in capturing labeled data and the post-capture manual examination required to discard samples with unwanted motion. Additionally, existing datasets have limited diversity in camera parameters. To address these limitations, this paper proposes zero and few-shot learning strategies for HDR deghosting using a pool of unlabeled dynamic exposure stacks. The approach consists of two stages of training, including supervised and self-supervised losses for labeled and unlabeled samples. The trained model is used to predict HDRs for unlabeled samples, and artificial dynamic input images are generated from these predictions to further improve the model in the second stage. The proposed method achieves comparable or better results than existing methods trained on complete datasets using only a few labeled dynamic samples and unlabeled samples. Comprehensive experiments and ablation studies demonstrate the significance of various components of the proposed approach. Overall, this work explores data-efficient Deep HDR Deghosting methods and offers novel contributions to the field.