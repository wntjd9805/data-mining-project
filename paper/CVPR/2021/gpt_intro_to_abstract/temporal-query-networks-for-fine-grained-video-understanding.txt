This paper introduces a novel network and training framework called the Temporal Query Network (TQN) for fine-grained action classification in untrimmed videos. The TQN enables query-response functionality, where the network ingests a video and a set of queries and outputs responses for each query. The objective is to answer questions of various granularity on a video, without temporal localization information for training. The TQN uses transformer-based architecture and leverages queries as "experts" to identify relevant temporal segments in the video. To handle the dense temporal context required for fine-grained queries, a stochastic feature bank update is introduced to train the network on videos of different lengths. The TQN is evaluated on the Fine-Gym and Diving48 datasets, demonstrating its superiority over other architectures and text supervision methods. The results highlight the benefits of the TQN and stochastic feature bank update in achieving state-of-the-art performance on these fine-grained action classification benchmarks.