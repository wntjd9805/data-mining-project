Novel view synthesis aims to generate photo-realistic images of a scene from new viewpoints based on a set of posed images. Traditional methods, such as image-based rendering (IBR), typically rely on warping, resampling, and blending source views, requiring dense input views or explicit proxy geometry. Neural scene representations, particularly Neural Radiance Fields (NeRF), have emerged as promising alternatives that represent scenes using the weights of neural networks. NeRF leverages multi-layer perceptrons (MLPs) and positional encoding to model the continuous 5D radiance field of a scene, enabling realistic view synthesis on complex scenes. However, neural scene representations often necessitate a lengthy optimization process for each scene, limiting their practical applicability. This work introduces a new learning-based method that combines ideas from IBR and NeRF to generate a continuous scene radiance field on-the-fly without scene-specific optimization or precomputed proxy geometry. The key component is a lightweight MLP network called IBRNet, which performs density/occlusion/visibility reasoning and color blending to render a ray. By aggregating information from source views, IBRNet produces a final color value for each ray. The method is fully differentiable and can be trained end-to-end using multi-view images. Experimental results demonstrate that the proposed approach can generate high-resolution photo-realistic novel views for complex scenes. The method also outperforms existing one-shot view synthesis methods and can be fine-tuned on a per-scene basis to achieve performance comparable to state-of-the-art neural scene representation methods like NeRF. The contributions of this work include a new learning-based IBR approach, the IBRNet model architecture, and a per-scene fine-tuning procedure.