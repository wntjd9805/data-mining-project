The Common Task Framework has shown impressive results in tracking research progress in computer vision tasks, particularly in instance segmentation. However, not all error modes have seen equal progress due to the sensitivity of different evaluation metrics to different types of errors. While several methods have improved overall Average Precision (AP), few have specifically addressed mask boundary quality. This limitation suggests that current evaluation metrics may not adequately capture errors near object boundaries. In this paper, we propose a new evaluation metric called Boundary IoU that focuses on boundary segmentation quality across all scales. We compare Boundary IoU to Mask IoU and existing boundary-focused measures, demonstrating its effectiveness in measuring boundary quality for large objects and its ability to avoid over-penalizing errors on small objects. We also introduce task-specific evaluation metrics, including Boundary Average Precision (Boundary AP) for instance segmentation and Boundary Panoptic Quality (Boundary PQ) for panoptic segmentation. Through experiments on popular datasets, we show that the new metrics are more sensitive to boundary quality improvements and provide a more comprehensive assessment of segmentation performance. By adopting these boundary-sensitive evaluations, we hope to accelerate progress towards segmentation models with better boundary quality.