The aim of this research is to analyze the decision mechanism of deep Convolutional Neural Networks (CNNs) in order to provide explanations and improve interpretability. In this paper, we propose a new model called Recurrent Decision Tree (RDT) that integrates recent advances in multi-agent communication. The RDT formulates the decision process as an iterative decision tree embedded in the memory representation of a Recurrent Neural Network (RNN). The model uses message-passing with discrete symbols from a vocabulary, which can be learned from scratch or mapped to human-understandable attributes for better interpretability. By encoding the decision tree into the RNN's memory, our model retains the flexibility and performance of CNNs while being scalable. Unlike traditional decision trees, our model learns significantly fewer nodes with a constant number of parameters for an arbitrary tree depth. After training, the neural model can be converted into a standard decision tree for computational efficiency at test time. Our framework provides an explainable decision chain by breaking down the decision process into binary decisions and associating the answers with semantic attributes. Our contributions include proposing the RDTC model with hard node splits to overcome limitations of depth scalability and flexibility in decision trees, predicting attributes in an end-to-end manner for human-interpretable explanations, and demonstrating the efficiency of our model compared to related methods while maintaining the performance of CNNs. The code for our model is publicly available at the given GitHub repository.