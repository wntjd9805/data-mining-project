Automatic 3D human sensing from images and video has the potential to revolutionize various fields such as virtual apparel try-on, fitness, health, AR and VR, self-driving systems, special effects, human-computer interaction, and gaming. However, current methods heavily rely on complete supervision for training, which is impractical and inaccurate for complex 3D articulated structures. The success of these systems also depends on the interplay between 3D initialization using neural networks and non-linear optimization based on losses computed over image primitives. Non-linear optimization is challenging due to the high dimensionality of problems like 3D human pose and shape estimation. Additionally, integrating non-linear output state optimization into parameter learning is computationally expensive. In this paper, we propose a novel approach called HUND (Human Neural Descent) that replaces the non-linear gradient refinement stage with neural descent. HUND uses recurrent neural network stages to refine the state output based on previous estimates, loss values, and a context encoding of the input image. Unlike gradient-based models, HUND can be trained end-to-end using stochastic gradient descent and supports more complex step updates compared to non-linear optimization. By using HUND, we demonstrate that a 3D human pose and shape estimation system trained from monocular images can bootstrap itself without necessarily relying on completely synchronous supervision. Our experiments, ablation studies, and qualitative results validate the effectiveness of HUND in challenging imagery.