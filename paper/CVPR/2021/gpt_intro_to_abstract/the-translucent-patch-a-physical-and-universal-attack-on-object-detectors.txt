Deep neural networks (DNNs) and convolutional neural networks (CNNs) have emerged as powerful tools for computer vision tasks such as image classification, object detection, and image segmentation. However, the effectiveness of these models is compromised by their vulnerability to adversarial attacks. Adversarial attacks, particularly in the computer vision domain, have gained considerable attention in recent years. These attacks manipulate the input data to mislead the model's output. Physical attacks, where a patch is applied to the targeted object, have been studied extensively, but they require access to the object itself and can only hide one object at a time. In this paper, we propose a novel physical attack that fools object detection models by placing a carefully crafted patch on the camera lens. The patch is calculated via a gradient-based optimization process, aiming to hide a target class while minimizing the impact on other classes and creating a printable patch that is inconspicuous. We compare our attack with a previous camera-based attack that targeted image classification models and demonstrate the challenges posed by object detection models, which detect and classify multiple objects in an image. We successfully deceive Tesla's advanced driving assistance system (ADAS) using simple color patches on the camera lens, misclassifying a stop sign and misinterpreting a traffic light. We then evaluate our attack on CNN-based object detection models using real-world datasets for autonomous cars. The results show a significant decrease in the average precision for the targeted class while maintaining high detection rates for other classes. Our contributions include introducing the first camera-based physical adversarial attack on object detection models, crafting a universal perturbation for a specific object class, demonstrating the transferability of the attack, and considering real-world constraints in the design and optimization process.