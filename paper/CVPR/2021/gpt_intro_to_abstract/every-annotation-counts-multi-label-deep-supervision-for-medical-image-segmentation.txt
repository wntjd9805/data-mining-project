Medical imaging tools have become essential in modern healthcare, leading to the recording of millions of 2D and 3D images each year. However, the evaluation of these images has become increasingly challenging for medical professionals. Deep learning using artificial neural networks has shown promise in assisting with image interpretation, but requires a large amount of annotated data, particularly for semantic segmentation. The annotation process for this task is already difficult and time-consuming in real-world scenarios. This problem is further compounded in the medical domain, where domain experts are constrained by their clinical work. Therefore, there is a need to minimize annotation efforts while maximizing model accuracy.Existing approaches in this field focus on incorporating non-annotated data or using cheap weak labels of varying quality. These semi- and weakly-supervised methods have provided valuable insights and results. However, they often do not consider the simultaneous presence of multiple types of supervision. In many cases, there may be a small set of pixel-wise annotated data, image-level labels from medical reports, and a large amount of unlabeled data available, all from the same distribution. It is unclear how to integrate these diverse supervision types to train a semantic segmentation system effectively.To address these challenges, we propose a novel approach that integrates training signals deep into segmentation network layers using a new form of deep supervision. We enhance these signals by generating robust pseudo-labels from weakly or unlabeled images using a mean-teacher segmentation model. Additionally, we present experiments with a rigorous evaluation protocol, reporting test accuracies and standard deviations across multiple data splits.Our contributions include: (1) Thorough investigation of varying numbers of training samples and different supervision types for semantic segmentation. (2) Introduction of a new perspective on the deep supervision paradigm, adapted for segmentation through our Multi-label Deep Supervision technique. We also propose a flexible semi-weakly supervised pathway, the Self-Taught Deep Supervision approach, to integrate un- or weakly labeled images. (3) Our best-performing method, Mean-Taught Deep Supervision, incorporates invariance to perturbations and robust pseudo-label generation, achieving results close to fully supervised baselines while using only a small fraction of strong labels.