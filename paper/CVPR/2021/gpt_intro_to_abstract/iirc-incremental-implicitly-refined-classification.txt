Deep learning algorithms have made significant advancements in various fields, such as computer vision, natural language processing, speech processing, reinforcement learning, robotics, and recommender systems. These algorithms have achieved super-human performance in several tasks. However, their super-human performance is limited to narrow and well-defined setups. Additionally, existing learning algorithms face challenges when training over a sequence of tasks in lifelong learning scenarios.Lifelong learning involves models encountering new knowledge in the form of new tasks from the same or different domains while retaining useful knowledge from previous tasks. There are two popular paradigms in lifelong learning: task incremental learning and class incremental learning. In task incremental learning, models have access to task delimiters, while class incremental learning does not have access to task delimiters, making it a more challenging scenario.Existing benchmarks for evaluating lifelong learning models focus primarily on measuring catastrophic forgetting, overlooking other unique challenges in class incremental learning. One common assumption made by these benchmarks is that information about a given sample does not change across tasks. However, in real-life scenarios, we continually update and refine our knowledge about entities.To fill this gap and address the challenges posed by class incremental learning, we propose the Incremental Implicitly-Refined Classification (IIRC) setup. We adapt the CIFAR100 and ImageNet datasets to create a benchmark for the IIRC setup and evaluate several well-known algorithms on this benchmark.The main contributions of our work are:1. Proposal of the IIRC setup, where models start with coarse, high-level classes and observe new, fine-grained classes as they train over new tasks.2. Development of a standardized benchmark to evaluate lifelong learning models in the IIRC setup using the ImageNet and CIFAR datasets.3. Evaluation of well-known lifelong learning algorithms on the benchmark, highlighting their strengths and limitations in a fair and standardized setup.The IIRC setup uncovers challenges not adequately explored in existing benchmarks, including associations between high-level and low-level classes, the forgetting of associations, and handling multiple labels for the same sample across different tasks.By providing a comprehensive and standardized benchmark, our goal is to shed light on the challenges posed by the IIRC setup rather than developing a new state-of-the-art model. This work contributes to advancing research in lifelong learning and provides insights into the limitations and capabilities of existing lifelong learning algorithms.