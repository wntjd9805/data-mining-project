Multi-view depth estimation is a fundamental problem in computer vision, aiming to recover the 3D geometry of given images with known camera parameters. It has applications in autonomous driving, augmented reality, 3D modeling, and image-based rendering. Learning-based depth estimation methods have shown significant improvements compared to traditional methods, but there is still room for improvement in terms of temporal coherence and computational efficiency.Existing methods for multi-view depth estimation are designed for individual images and are not suitable for temporally coherent videos. Extending these methods to video sequences can result in flickering artifacts and inconsistent depth maps across consecutive frames. To address this, it is necessary to jointly estimate depth maps of a video sequence to produce temporally coherent results.Video depth estimation methods can be categorized into single-view methods and multi-view methods. Single-view methods use recurrent neural units to encode temporal coherence implicitly, but they still suffer from depth scale ambiguity. Multi-view methods, on the other hand, utilize epipolar geometry information from multiple images to avoid depth scale ambiguity. However, existing multi-view methods are limited in their use of preceding estimations to improve current estimation.In terms of computational efficiency, top-performing multi-view depth estimation models are slow due to the use of a single fully 3D convolution network for cost regularization. This network learns both local feature matching information and global context information, which are critical for accurate depth estimation. However, learning global context information using deep 3D convolution layers is computationally expensive.The authors propose an Epipolar Spatio-Temporal (EST) network that jointly estimates depth maps of multiple frames to improve temporal coherence. They also introduce a hybrid network for cost regularization, consisting of two expert sub-networks: a 2D ContextNet focusing on global context information and a shallow 3D Match-Net focusing on local matching information. By separating these two types of information, the hybrid network achieves faster speed and consumes fewer computational resources.The main contributions of the paper are the EST transformer for joint depth estimation, the hybrid network for cost regularization, and the development of a new multi-view method for generating temporally coherent depth maps from videos. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art approaches in terms of accuracy and speed.