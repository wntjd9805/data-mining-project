Representation learning is crucial in the field of deep learning, as it enables the extraction of meaningful features from input data. A good representation should be able to capture multiple configurations, organize factors hierarchically, and have strong transferability. Deep neural networks trained for image classification have resulted in expressive and discriminative visual features. However, these features are highly dependent on the training objective and may not be suitable for other tasks, limiting their transferability. It is also unclear how these discriminative features can be applied to generative applications like image editing. Generative Adversarial Networks (GANs) have made significant progress in synthesizing photo-realistic images by learning the underlying distribution of real data. However, existing GAN models lack the inference ability to extract visual features from input images. Although encoders have been introduced into GAN models, they often omit the pre-layer information learned by the generator. Furthermore, the transferability of representation learning from GAN models has not been fully explored, particularly in mid-level and low-level tasks. In this paper, we propose a novel hierarchical encoder, based on the StyleGAN model, that aligns with layer-wise representations from the generator. We demonstrate that training with the pre-trained GAN generator as a learned loss function can yield competitive and generalizable hierarchical visual features. We evaluate these features on various generative and discriminative tasks, showing their compelling hierarchical and transferable properties for downstream applications.