Deep neural networks have achieved remarkable successes in various fields but face challenges in practical deployment due to their high memory footprint and computational requirements on resource-constrained devices. Network quantization methods have emerged as a solution to compress the model size and reduce computational resources for inference. However, two main problems need to be addressed: how to quantize weights and activations with lower precision and higher accuracy, and how to allocate the optimal bit-width for quantization. This paper explores the use of multi-bit quantization (MBQ) as a solution and proposes a distribution-aware approach to optimize the quantization schemes. Inspired by the utilization of distribution in quantization, a lookup table based strategy is proposed to minimize the expected mean square error and optimize the quantization schemes with low computational cost during the training iterations. Additionally, the paper addresses the challenge of bit-width allocation. Existing methods require significant computation, hindering their practical application. To overcome this, a metric based on Taylor expansion is formulated to evaluate the loss-sensitivity of quantized weights and activations. By adaptively adjusting the quantization bit-width in the training process, the metric enables loss-guided bit-width allocation without excessive computational load. The contributions of this paper include the introduction of the distribution-aware multi-bit quantization (DMBQ) method for efficient and optimal MBQ quantization, the proposal of a loss-guided bit-width allocation (LBA) method based on first-order Taylor expansion, and extensive comparisons with state-of-the-art methods to demonstrate the effectiveness and efficiency of the proposed approach.