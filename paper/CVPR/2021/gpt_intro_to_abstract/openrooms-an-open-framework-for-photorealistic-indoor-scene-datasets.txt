This paper introduces OpenRooms, a framework for synthesizing photorealistic indoor scenes with applications in computer vision, graphics, and robotics. Traditional methods for creating ground truth data for indoor scenes are challenging and expensive. OpenRooms addresses this issue by assigning high-quality material and lighting to RGBD scans of real indoor scenes, providing all the necessary tools to create such datasets inexpensively. It also provides extensive high-quality ground truth for complex light transport, including spatially-varying microfacet bidirectional reflectance distribution function (SVBRDF) for materials and spatially-varying lighting effects. Images are rendered using a custom GPU-accelerated physically-based renderer. The resulting dataset contains over 100K HDR images with ground truth depths, normals, BRDF, light sources, and per-pixel lighting and visibility masks. Semantic labels are also provided. OpenRooms is publicly available and can be extended through community efforts. This framework has applications in various areas such as depth prediction, material classification, and lighting estimation. Additionally, OpenRooms can enhance robotics research by improving simulation environments and enabling sim-to-real transfer. The paper demonstrates the efficacy of the dataset through accurate inverse rendering results on real images and showcases various applications such as object insertion, material editing, and light source detection. The OpenRooms framework is illustrated, highlighting its ability to create large-scale synthetic indoor datasets from commodity RGBD sensor scans.