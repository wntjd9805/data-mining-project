This paper addresses the problem of material recognition in computer vision, specifically the identification of material categories from RGB images. The authors highlight the significance of material information in improving performance in various applications such as 3D scene understanding and robot control. Previous works in material recognition have utilized hand-designed image features or convolutional neural networks (CNNs) combined with large-scale datasets. However, these approaches rely solely on the visual modality and struggle with smooth and texture-less surfaces, where visual ambiguities exist. To overcome this limitation, the authors propose incorporating additional sensory modalities, specifically sound, to complement visual perception. They discuss how sound has been proven to significantly contribute to material identification. The main objective of the paper is to identify materials of rigid objects in daily life using a dataset called GLAudio, which contains both auditory and visual perceptions of each object. GLAudio includes the local geometry around the hitpoint, encoded as a discriminative feature for sound generation. Their dataset also offers fine-grained materials, allowing for the solving of more complex problems such as distinguishing real gold from fake gold. The authors design a deep neural network called GLAVNet, which utilizes both visual and auditory cues for material recognition and outperforms previous methods that rely solely on global information. They emphasize the contributions of introducing the GLAudio dataset, proposing the GLAVNet model, and achieving state-of-the-art performance in material recognition.