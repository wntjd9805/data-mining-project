Image-to-Image (I2I) translation is a task in computer vision and graphics that involves transforming images from one domain to another. This task has gained significant attention in recent years due to its applicability in various areas such as photo-realistic image synthesis, super-resolution, colorization, and inpainting. One of the main challenges in I2I translation is its multi-modal nature, where a single input image can be mapped to multiple output images. Existing I2I translation networks typically learn one-to-one mappings, leading to the problem of mode collapse when trying to generate diverse outputs. To address this challenge, previous works have proposed encoding the distribution of different possible outputs into a latent vector, allowing for more varied translations. However, training these networks, especially in the unsupervised case, is not trivial. In this paper, we propose a novel weakly-supervised pre-training strategy for learning an expressive latent space for multi-modal I2I translation. Our approach demonstrates several advantages, including capturing uncommon styles, generalizing well across domains, simplifying the training objective, improving stability, and producing high-quality and diverse outputs. We achieve state-of-the-art results on multiple benchmarks and provide insights into the importance of different loss terms for multi-modal I2I translation. Our proposed method utilizes pre-training on a proxy task using a VGG network and is inspired by the staged training strategy and transfer learning approaches commonly used in visual recognition tasks. Overall, our work highlights the benefits of encoder pre-training for multi-modal image synthesis.