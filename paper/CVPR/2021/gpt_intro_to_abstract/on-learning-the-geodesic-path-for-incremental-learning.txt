This paper addresses the challenges of incremental learning (IL) where a machine must gradually learn and remember various tasks or concepts. The gradual nature of IL, combined with constraints and limitations, can lead to a degradation in performance known as "Catastrophic forgetting". To overcome this, the paper proposes a novel distillation loss called GeoDL that considers the information geometry aspect in IL. The distillation loss is based on the geodesic flow between two tasks, which leads to improved performance compared to distillation losses using the Euclidean metric. The paper also highlights the limitations of prior methods and introduces the concept of gradual change inspired by human learning. The contributions of this work include the proposed distillation loss and demonstrating the effectiveness of using geodesic paths for knowledge distillation.