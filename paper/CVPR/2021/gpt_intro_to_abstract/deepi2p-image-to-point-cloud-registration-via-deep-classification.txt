Image-to-point cloud registration is a fundamental task in computer vision and robotics, where the goal is to find the rigid transformation that aligns the 3D point cloud with the corresponding image. While same-modality registration approaches exist, they come with various limitations. Point cloud-to-point cloud registration requires expensive lidars and high memory complexity, while image-to-image registration suffers from the need for meticulous feature matching and is sensitive to illumination and seasonal changes. Cross-modality image-to-point cloud registration offers a potential solution to these limitations by utilizing lidar-acquired point clouds and image data from cameras. However, there is limited research in this area. In this paper, we propose a novel approach called DeepI2P that addresses these challenges by using a two-stage classification and optimization framework. We design a two-branch neural network with attention modules to learn labels indicating whether a 3D point is within the camera frustum. We then formulate an unconstrained continuous optimization problem to find the optimal camera pose. Our method requires less storage memory compared to traditional feature-based approaches and does not rely on explicit feature descriptors. Experimental results on Oxford Robotcar and KITTI datasets demonstrate the feasibility of our approach. The contributions of this paper include circumventing the need for cross-modal feature descriptors, designing a neural network with attention modules, proposing an inverse camera projection optimization method, and demonstrating the potential of deep classification for cross-modal image-to-point cloud registration.