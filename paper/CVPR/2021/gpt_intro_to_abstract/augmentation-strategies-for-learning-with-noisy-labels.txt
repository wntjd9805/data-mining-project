Data augmentation is a widely used technique in computer vision problems to expand datasets. It has shown success in various tasks, such as image classification and object detection. While learned augmentations like AutoAugment and RandAugment have been successful in improving generalization and robustness, their efficacy in learning with noisy labels (LNL) has not been extensively explored. Existing approaches for LNL often rely on network memorization, where correctly labeled data is learned before incorrectly labeled data. Incorporating augmentation policies during training, such as MixUp augmentation, has shown promising results in tolerating higher noise levels. However, most existing works only use weak augmentation techniques like random flip and crop images. This paper proposes the use of more aggressive augmentations from learned policies like AutoAugment during training for LNL algorithms. The authors introduce an augmentation strategy called Augmented Descent (AUGDESC) that aims to improve performance without negatively impacting the network memorization effect. Weak augmentation is used for loss modeling and pseudo-labeling tasks, while strong augmentation is employed during back-propagation to improve generalization. The authors evaluate the effectiveness of AUGDESC on synthetic and real-world datasets with noisy labels, achieving state-of-the-art performance. They also analyze the impact of augmentation incorporation and provide insight for future work. Moreover, the authors perform generalization studies, demonstrating that combining existing techniques with AUGDESC can further enhance performance without hyperparameter tuning.