Object pose estimation, which involves determining the 6D rigid transformation from the object coordinate system to the camera reference frame, is crucial in various applications such as robotic manipulation and augmented reality. While previous methods have used RGB features learned with convolutional neural networks, they struggle with texture-less objects or unseen surface texture/appearance. In contrast, human perception relies on object geometry to infer object pose. To leverage geometry for pose inference, this paper proposes learning object pose inference based on 3D surface patches extracted from a single view depth image. Specifically, the focus is on planar and cylindrical patches, which are prevalent on household objects. By exploiting geometric stability analysis, a minimal set of stable patches can lock all six degrees of freedom and accurately reason about 6D object poses. A deep neural network called StablePose is designed to regress the 6D object pose based on geometrically stable patch groups. The network extracts both intra-patch geometric features and inter-patch contextual features and predicts a pose for each stable patch group. The method outperforms state-of-the-art learning-based methods for depth-only and RGBD inputs on benchmark datasets. Additionally, it generalizes well for category-level pose estimation and handles symmetric objects, objects with occlusion, and unseen objects. Overall, this paper introduces the concept of geometric stability into 6D object pose estimation and presents a robust deep learning approach for inferring object pose based on geometrically stable patch groups.