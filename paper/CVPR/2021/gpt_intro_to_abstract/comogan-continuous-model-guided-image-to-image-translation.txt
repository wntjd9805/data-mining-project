Image-to-image (i2i) translation networks have been widely used for various applications in computer vision and robotics. These networks learn translations between different domains, allowing the transformation of source images into a target appearance. This has enabled tasks such as neural photo editing, domain adaptation, and robotics-oriented tasks like time-of-day or weather selection. Despite the significant advancements in i2i translation networks, there are still limitations that need to be addressed.Existing works on i2i translation networks often rely on supervision of intermediate domain points and assume linearity of the domain manifold, which limits their ability to handle cyclic translations or continuous translations that are costly or impractical to label. To overcome these limitations, we propose CoMoGAN, which is the first i2i framework capable of learning non-linear continuous translations without the need for supervised target data. CoMoGAN leverages physics-inspired models for guidance and employs continuous disentanglement of domain features to reduce model dependency.One interesting aspect of CoMoGAN is its ability to discover the ordering of the target data manifold in an unsupervised manner. We evaluate CoMoGAN on new translation tasks, including cyclic and linear mappings from source to target domains. These tasks involve lighting translations, depth of field variations, and transforming synthetic clear images into realistic foggy ones.Our contributions include a novel model-guided setting for continuous i2i translation, the CoMoGAN framework that enables unsupervised disentanglement of continuously evolving features in generated images, the introduction of a novel Functional Instance Normalization (FIN) layer, and the evaluation of CoMoGAN against recent baselines and new tasks, where it outperforms the existing literature.Overall, CoMoGAN provides a promising approach for non-linear continuous i2i translation and opens up possibilities for various applications in computer vision and robotics.