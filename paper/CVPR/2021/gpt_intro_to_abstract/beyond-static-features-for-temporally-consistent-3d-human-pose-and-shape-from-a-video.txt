Various methods have been proposed to analyze humans from images, with the goal of estimating 3D human pose and shape. However, this task is still challenging, especially in the monocular case. Existing methods based on single images tend to produce temporally inconsistent and unsmooth 3D motion when applied to videos. To address this issue, some methods have extended the single image-based approaches to the video case by feeding a sequence of images to pretrained networks. Although these methods have improved per-frame accuracy and motion smoothness, their qualitative results still suffer from temporal inconsistency. This is due to a strong dependency on the static feature of the current frame and the limited utilization of temporal information from past and future frames.In response to these limitations, we propose a temporally consistent mesh recovery system (TCMR) that aims to produce smooth and consistent 3D human motion from video data. TCMR removes the residual connection between the static and temporal features in the encoding process. Additionally, we introduce a Pose-Forecast module that uses two temporal encoders to forecast the current pose without relying on the current frame. The temporal features from Pose-Forecast are then integrated with the current temporal feature to predict the current SMPL parameters. By reducing the dependency on the current static feature, our method allows for a better utilization of temporal information from past and future frames.Despite its simplicity, TCMR achieves high effectiveness in obtaining temporally consistent and smooth 3D human motion. It also improves the per-frame accuracy of 3D pose and shape estimation by utilizing better temporal information. Experimental results show that TCMR outperforms previous video-based methods in terms of temporal consistency and per-frame accuracy on various 3D video benchmarks. Overall, our contributions include the introduction of TCMR, which addresses the temporal inconsistency issue in 3D human motion estimation from videos, and the demonstration of its superior performance compared to existing methods.