With the increasing amount of multimedia data on the Internet, cross-modal retrieval, which involves retrieving data from one modality based on a query from another modality, has gained significant attention in the multimedia community. Deep learning methods have been proposed for cross-modal retrieval, focusing on learning modal-invariant representations in a common space. However, existing methods have limitations, such as relying on pre-trained models that may not be optimally representative and being designed for only two modalities. In this paper, we propose a new loss function called Cross-modal Center Loss to address these limitations. The proposed loss function minimizes intra-class variation across multiple modalities by learning a unique center for each class in the common feature space of all modalities. We present an end-to-end framework for cross-modal retrieval that incorporates the proposed loss function, cross-entropy and mean square error loss, and a weight sharing strategy. We evaluate our method on a novel 3D cross-modal retrieval task and demonstrate its superiority over state-of-the-art methods. This paper contributes a new loss function and a comprehensive framework for cross-modal retrieval, with promising results in both cross-modal and in-domain retrieval tasks.