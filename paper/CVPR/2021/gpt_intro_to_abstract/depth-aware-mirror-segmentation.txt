Mirrors are commonly found in human-made scenes and pose a challenge for vision systems in distinguishing real from reflected scenes. Automatic mirror segmentation is necessary for better scene understanding and practical applications. Previous mirror segmentation solutions relied on user interaction or specialized hardware. Learning-based methods, such as MirrorNet and PMD, have limitations when there are large variations in contextual contrast and correlations. In this paper, we propose leveraging depth information for mirror segmentation and introduce the first RGB-D mirror segmentation dataset. We also present a novel positioning and delineating network (PDNet) that detects and locates mirrors using global and local discontinuity and correlation cues in both RGB and depth. Our approach includes a dynamic weighting scheme to fuse RGB and depth correlations. Experimental results demonstrate the effectiveness of our approach, highlighting the power of depth as a cue for mirror segmentation. Our contributions include considering both RGB and depth for mirror segmentation, providing a new dataset, introducing a depth-aware mirror segmentation network, and proposing a dynamic weighting scheme for fusion of RGB and depth correlations.