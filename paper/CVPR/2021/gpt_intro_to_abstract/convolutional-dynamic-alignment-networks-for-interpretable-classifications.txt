Neural networks are powerful but lack interpretability, making it challenging to understand the reasoning behind their predictions. Linear models, on the other hand, are considered interpretable because they explicitly provide the contribution of each input dimension to the output. Surprisingly, many modern neural networks implicitly model the output as a linear transformation of the input. However, the contributions per input dimension in these networks do not accurately represent the learned model parameters. To address this issue, we present a novel network architecture called Convolutional Dynamic Alignment Networks (CoDA-Nets). CoDA-Nets are dynamic linear networks that compute their outputs through input-dependent linear transforms using our proposed Dynamic Alignment Units (DAUs). These DAUs are structurally biased to align with relevant patterns in the inputs, resulting in contribution maps that represent the most discriminative features as used by the model. Our contributions include the introduction of the DAUs, which improve the interpretability of neural networks by aligning their weights with discriminative input patterns, and the CoDA-Nets, which are built using multiple layers of DAUs and produce inherent contribution maps highlighting discriminative patterns in the input. We also demonstrate that temperature scaling can further promote the alignment of the DAUs. The resulting contribution maps perform well under quantitative criteria for attribution methods and exhibit a high level of detail when qualitatively inspected. Additionally, CoDA-Nets are highly performant classifiers, achieving competitive classification accuracies on the CIFAR-10 and TinyImagenet datasets. Overall, this work presents a new direction for developing more interpretable neural network architectures with high modeling capacity.