Deep Learning has shown remarkable success in computer vision tasks such as image classification and semantic segmentation. However, deep neural networks trained on specific datasets often struggle to generalize to new domains due to the domain shift problem. Unsupervised domain adaptation (UDA) aims to transfer predictive models from a labeled source domain to an unlabeled target domain.While many UDA methods achieve high accuracy on the target domain using explicit supervision in the source domain, it is often challenging to provide large-scale annotations even in the source domain. This is particularly true in applications such as medical imaging, where annotation costs are high.In this paper, we propose a few-shot unsupervised domain adaptation (FUDA) setting, where only a small fraction of source samples are labeled while the rest of the samples, both in the source and target domains, remain unlabeled. Most state-of-the-art UDA methods align source and target features by minimizing distribution distances and learn discriminative representations using fully-labeled source data. However, in FUDA, where labeled source samples are scarce, learning discriminative features becomes much more difficult.To address this challenge, we propose a novel framework called Prototypical Cross-domain Self-supervised learning (PCS) for FUDA. PCS consists of three major components: in-domain prototypical self-supervision, cross-domain instance-to-prototype matching, and unifying prototype learning with a cosine classifier. By implicitly encoding the semantic structure of the data, transferring knowledge from source to target in a robust manner, and adaptively updating the cosine classifier, PCS learns both discriminative and domain-invariant features.Our contributions include the introduction of PCS, a single-stage framework for few-shot unsupervised domain adaptation, and the use of prototypes to improve semantic structure learning, discriminative feature learning, and cross-domain alignment in an unsupervised and adaptive manner. In addition, PCS is easily trained in an end-to-end manner and outperforms all state-of-the-art methods by a large margin across multiple benchmark datasets.