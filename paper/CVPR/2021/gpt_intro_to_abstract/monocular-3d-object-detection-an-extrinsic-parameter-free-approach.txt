This paper addresses the problem of 3D object detection using a monocular camera in automated driving scenarios. While most existing methods rely on accurate depth-of-field information obtained from LiDAR or other sensors, monocular 3D detection offers advantages in terms of low cost, low power consumption, and ease of deployment. However, the lack of direct depth information makes this task challenging. The authors highlight the issue of camera extrinsic parameter perturbations due to uneven road surfaces, which can significantly impact the accuracy of 3D detection. They note that existing datasets and detectors do not consider this perturbation, leading to unreliable results. The paper proposes a novel MonoEF framework that implicitly leverages the extrinsic parameter change in the image. The framework estimates camera pose change with respect to the ground plane from image features and optimizes the predicted 3D locations of objects by incorporating the camera extrinsic geometry constraint. The proposed method outperforms state-of-the-art approaches on KITTI 3D benchmark and nuScenes dataset, particularly in perturbative scenarios. The contributions of the paper include introducing a perturbation-free Mono3D detector, designing a feature transformation network to recover non-perturbative information, and proposing an extrinsic module for improved 3D object detection. The codebase and experimental results will be made publicly available.