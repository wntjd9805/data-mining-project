This paper introduces a new approach for modeling clothed human bodies using a surface element representation. Most existing approaches only model "minimally-clothed" body shapes and do not represent clothing, resulting in a lack of statistical models for clothed humans. The challenges in modeling clothing shapes include variations in spatial scales, smooth and discontinuous surfaces, diverse topology, changing relationships between clothing and the body, and compatibility with existing body models and real-world applications. No existing 3D shape representations satisfy all these requirements. The standard approach of using draped 3D meshes with physics simulation is inappropriate for inference. Other methods rely on pre-defined garment templates or require manual clothing design. Neural implicit representations can reconstruct varying clothing types but are not compatible with existing graphics tools and are expensive to render. Point clouds support arbitrary topology but require many points for highly detailed geometry. The proposed solution is a surface element representation that smoothly conforms to the global shape of the clothed body. This representation allows for efficient shape inference and supports topologically varying surfaces. Local geometric details such as folds and wrinkles are modeled using a global patch descriptor and local non-rigid shape regression. The proposed model, called SCALE (Surface Codec of Articulated Local Elements), demonstrates state-of-the-art performance in modeling per-subject pose-dependent shape of clothed humans. It addresses the limitations of existing models by providing a robust, high-visual-fidelity, easily controllable, and fast inference approach for clothed human shape modeling. The paper also discusses the use of neural rendering to produce high-quality rendered results. The code is available for research purposes.