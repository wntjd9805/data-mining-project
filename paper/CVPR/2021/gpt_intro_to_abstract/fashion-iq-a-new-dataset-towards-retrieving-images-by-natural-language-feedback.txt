Fashion is a multi-billion-dollar industry with significant social, cultural, and economic implications. Computer vision has shown great success in various fashion applications, such as trend forecasting, modeling influence relations, and fashion design. However, interactive image retrieval for fashion product search remains a challenging research problem. The main challenges are empowering users to express their preferences effectively and developing a retrieval machine with high performance. Traditional systems have used relevance feedback and relative attribute feedback, but these approaches have limitations in capturing user intent. Recent work has explored the use of natural language feedback to address this issue, allowing users to describe their preferences in a more detailed and nuanced way. However, there are still questions regarding the interaction between side information, such as visual attributes, and natural language user inputs to improve dialog-based image retrieval systems. This paper introduces a new dataset, Fashion IQ, which enables joint modeling of natural language feedback and side information for effective image retrieval. The dataset supports both single-shot retrieval and dialog-based retrieval, with the latter allowing for progressive improvement of retrieval results. The paper also presents a transformer-based user simulator and interactive image retriever that leverages multimodal inputs during training and outperforms existing methods. The study explores the benefits of combining natural language user feedback and attributes for dialog-based image retrieval, demonstrating superior performance for user modeling and retrieval. This research contributes to the development of more effective interfaces for conversational fashion retrieval.