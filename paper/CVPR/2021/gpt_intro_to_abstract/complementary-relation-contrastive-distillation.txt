Knowledge distillation is a technique in deep learning that involves transferring knowledge from one model (the teacher) to another model (the student). This transfer of knowledge can be used to distill a larger network into a smaller one or to combine multiple models into a single model. Knowledge distillation has become particularly important in industries where neural networks need to be deployed on devices with limited computational resources.While significant progress has been made in the field of knowledge distillation, there is still ongoing debate regarding what type of knowledge should be preserved in the distillation process. One popular distillation method, Contrastive Representation Distillation (CRD), focuses on capturing the correlations and higher-order dependencies in the output of each sample, as opposed to treating all dimensions as independent information like traditional methods.However, existing distillation methods, including CRD, have limitations when it comes to preserving the relationships between samples. These relationships are often more valuable than the individual sample representations in practical tasks such as retrieval and classification. Existing contrastive distillation methods may push the student representation of a sample away from its neighboring samples, which can disrupt the latent structural geometry. Recent research has shown that transferring the mutual similarity between samples, rather than their actual representations, can be beneficial for student representation learning.To address these issues, this paper proposes a new distillation method called Complementary Relation Contrastive Distillation (CRCD). CRCD defines a new cross-space relation between samples and supervises this relation using the corresponding relation in the teacher representation space. This new approach simultaneously optimizes both the representation and relation by ensuring that the anchor-student relation is consistent with the anchor-teacher relation.CRCD offers several advantages over existing methods. Firstly, it effectively and robustly distills both sample representations and inter-sample relations. Secondly, it models the relation using two complementary elements: the feature and its gradients. This captures both the structural information of the feature and the optimization kinetics. Lastly, CRCD leverages contrastive learning to maximize the mutual information between the anchor-teacher relation and the anchor-student relation.Extensive experiments validate the effectiveness of CRCD and demonstrate its superior performance compared to existing methods on various benchmarks. The proposed method improves the current state-of-the-art in knowledge distillation, making it a valuable contribution to the field.