Cartoon videos in the animation industry are traditionally created through labor-intensive and time-consuming manual drawing processes. With the need for efficiency and cost reduction, animation producers often use the same drawing multiple times, resulting in low frame rates. Therefore, there is a demand for computational algorithms to automatically generate intermediate frames. While video interpolation methods for natural videos have made significant progress in recent years, existing techniques fail to produce satisfactory in-between frames in animation videos. The unique characteristics of animations, such as explicit sketches, segmented color areas, and exaggerated movements, pose challenges for accurate motion estimation and frame prediction. In this work, we propose AnimeInterp, a novel framework for video interpolation in animations. It consists of two modules: a Segment-Guided Matching (SGM) module and a Recurrent Flow Refinement (RFR) module. The SGM module uses global semantic matching among color segments to compute a coarse piece-wise optical flow, addressing the lack of texture issue. The RFR module further enhances the estimated flow using a Transformer-like network, specifically designed for animations with non-linear and large motions. We introduce a large-scale animation triplet dataset, ATD-12K, which contains frame triplets from 30 animation movies in various styles. ATD-12K enables comprehensive training and evaluation of video interpolation methods for cartoon videos. Our contributions include formalizing the animation video interpolation problem, proposing the AnimeInterp framework, which outperforms existing methods, and creating the ATD-12K dataset to facilitate future research in animation.