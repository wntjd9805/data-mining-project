This paper introduces the task of few-shot common action localization in time and space, aiming to localize an action in a video without the need for class labels or box annotations. The existing approaches rely on generating numerous proposals and a secondary supervised step, but this paper proposes a proposal-free method inspired by transformers. The authors propose a few-shot transformer with an encoder-decoder structure optimized for joint commonality learning and localization prediction. They also reorganize video datasets to allow for the evaluation of few-shot common action localization. Experimental results show the effectiveness of the proposed approach, even with noisy support videos, and demonstrate compatibility with few-shot and one-shot temporal action localization. The proposed few-shot transformer can also be extended to common action localization per pixel. The code is available on GitHub.