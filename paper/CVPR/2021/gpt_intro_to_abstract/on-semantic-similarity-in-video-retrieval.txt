Video understanding approaches incorporating language have achieved notable success in various tasks such as captioning, video question answering, and navigation. Video retrieval, which involves searching for videos using language, has also gained significant attention in research. These methods rely on an underlying multi-modal embedding space to relate videos and captions, often utilizing large-scale datasets and video retrieval benchmarks for evaluation. However, the assumption that only the video collected with that particular caption is relevant is a critical limitation in these benchmarks. This paper presents a critical analysis of this assumption and proposes the concept of semantic similarity relevance for both training and evaluation. Inspired by previous works questioning assumptions and biases in different research areas, we aim to address the inefficiencies and ad hoc nature of popular video retrieval datasets. We introduce continuous similarity, allowing for multiple videos to be considered equally relevant to a query. Our contributions include exposing the shortcomings of instance retrieval in current video retrieval benchmarks, proposing video retrieval with semantic similarity for evaluation and training, providing proxies for predicting semantic similarities, and analyzing the impact of semantic similarity on current baselines and evaluations using three benchmark datasets. By challenging traditional retrieval methods, this work advances the understanding of video retrieval and lays the foundation for more robust and comprehensive evaluation techniques.