Matching all pixels between images is a challenging task in computer vision, especially when it comes to semantic correspondence between visually similar images. This problem is important for various applications such as attribute transfer, image editing, and object discovery. Traditional approaches use hand-crafted image descriptors combined with regularization methods, but these methods struggle with capturing high-level semantics and handling variations in appearance, deformation, and clutter. Self-supervised deep learning models have been successful in learning semantic descriptors or regressing global transformation parameters, but they often rely on synthetic data which limits their ability to handle real-world variations. To address this limitation, we propose a probabilistic teacher-student framework for knowledge distillation. Our approach generates pseudo semantic flows on unlabeled real pairs using a teacher model trained on synthetic data, and then trains a student model using the pseudo-groundtruth flows. To overcome the noise and bias in the teacher model's predictions, we use a probabilistic teacher model to provide multiple matching hypotheses and a student model with a probabilistic supervision reweighting (PSR) module to distill correct supervision signals. We also introduce a confidence-aware loss (CAL) to reduce the impact of errors during knowledge distillation. Our approach, called Probabilistic Model Distillation (PMD), outperforms state-of-the-art self-supervised approaches even without manual annotations. We further extend our approach to incorporate stronger supervision using matched key points, achieving even better performance on multiple benchmarks. Our contributions include a novel probabilistic approach for model distillation, a probabilistic teacher-student network for semantic correspondence, state-of-the-art results on benchmarks, and potential generalizability to stronger supervision.