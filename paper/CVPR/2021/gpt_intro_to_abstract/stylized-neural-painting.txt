This paper introduces a novel method called "Stylized Neural Painter" for automatic image-to-painting translation. Unlike previous methods that manipulate pixels directly, our method simulates the behavior of human painting by generating vectorized strokes with physical significance. These strokes can be rendered at any resolution and in various painting styles. Our method can also be integrated into a neural style transfer framework to transfer visual styles based on different style reference images. Instead of using step-wise greedy search or reinforcement learning, we reformulate stroke prediction as a "parameter searching" process to maximize similarity between the input and output in a self-supervised manner. We make all components differentiable by utilizing neural rendering. We also address the issue of parameter coupling in complex rendering scenarios and propose a new neural renderer architecture with a rasterization network and a shading network. Additionally, we discuss the problem of zero-gradient optimization with pixel-wise similarity metrics and propose a solution based on optimal transportation. We demonstrate the effectiveness of our method through extensive testing on various real-world images and photos, showing high-quality painting results in terms of realism and artistic sense. The contributions of this paper include the development of a new stroke-based image-to-painting translation method, the exploration of the zero-gradient problem from an optimal transport perspective, and the design of a new neural renderer architecture.