The Capsule network (CapsNet) has been proposed as a potential improvement over convolutional networks (ConvNet) in terms of efficiency and robustness to affine transformations. However, there is a lack of comprehensive comparisons to support this assumption, and the effectiveness of individual components in CapsNets has not been thoroughly studied. This paper aims to examine the effects of different components in CapsNets on robustness, specifically in terms of affine transformations, recognizing overlapping digits, and semantic representation compactness. The investigations reveal that some widely believed benefits of Capsule networks may be incorrect, such as the assumption that dynamic routing improves robustness to affine transformations. Furthermore, it is found that certain components of CapsNets, such as conditional reconstruction and squashing function, are beneficial for learning semantic representations and can be applied beyond CapsNets. Additionally, common ConvNets can be enhanced by incorporating useful components from CapsNets to achieve greater robustness. The paper presents a detailed analysis of CapsNets and ConvNets' behavior in different scenarios and concludes with discussions on the findings and future directions.