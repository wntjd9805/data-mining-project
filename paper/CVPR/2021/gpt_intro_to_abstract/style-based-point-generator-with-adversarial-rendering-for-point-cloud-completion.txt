This paper introduces the SpareNet, a Style-based Point generator with Adversarial Rendering, for point cloud completion. With the increasing availability of 3D scanning devices, point cloud data has gained significant research interest in the vision and robotics community. However, raw point cloud data captured by these devices is often sparse and incomplete. To address this, researchers have proposed various methods for inferring the complete shape from partial observations.Existing methods either leverage intermediate representations or directly process the raw points using point-based networks. However, these methods often fail to preserve detailed structures, infer the global shape accurately, and ensure sharp and accurate local structures. Additionally, there is a lack of perceptual metrics for measuring the visual quality of completed point clouds.In response to these limitations, the paper presents the SpareNet, which improves upon existing methods by proposing three new modules: channel-attentive EdgeConv, Style-based Point Generator, and Adversarial Point Rendering. The channel-attentive EdgeConv considers both local and global context to better extract features. The Style-based Point Generator enhances the folding capability by leveraging style codes, improving the modeling of structural details. Lastly, Adversarial Point Rendering projects the generated point clouds into view images and utilizes adversarial discriminators to ensure high perceptual quality.Extensive experiments on ShapeNet and KITTI datasets demonstrate that the SpareNet outperforms state-of-the-art methods in terms of both quantitative and qualitative evaluations. The SpareNet addresses the limitations of existing methods by faithfully preserving detailed structures, accurately inferring global shapes, and achieving high visual quality.