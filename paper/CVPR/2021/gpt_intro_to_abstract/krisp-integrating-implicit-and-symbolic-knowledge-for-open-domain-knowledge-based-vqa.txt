The paper introduces the problem of answering complex questions in Visual Question Answering (VQA) by leveraging both implicit and symbolic knowledge. Previous approaches focused on learning knowledge solely from the VQA training set, which is limited and prone to biases. To address this limitation, the paper proposes KRISP, a multi-modal architecture that combines the implicit knowledge learned from large-scale BERT training and the explicit knowledge represented in knowledge graphs. KRISP utilizes a multi-modal BERT-pretrained transformer to process the question and image, while also incorporating a graph network to make use of the symbolic knowledge bases. The proposed method constructs a diverse knowledge graph by drawing on various knowledge sources, including DB-Pedia, ConceptNet, VisualGenome, and hasPart KB. By preserving the symbolic meaning of knowledge and employing a late-fusion strategy, the proposed method achieves improved performance in answering knowledge-based questions in VQA.