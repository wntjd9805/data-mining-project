Object detection has seen significant advancements thanks to the development of deep convolutional networks (CNNs) and the availability of large annotated datasets. However, collecting extensive annotated data across different domains for object detection is costly. An alternative approach is to apply a pre-trained detection model from a source domain to a target domain. However, deep object detectors suffer from performance degradation in the presence of domain shift. This problem has led to research on Unsupervised Domain Adaptation (UDA), aiming to bridge the distribution gap between source and target domains through knowledge transfer. While various approaches have been proposed for cross-domain image classification and segmentation, cross-domain object detection is more challenging due to the need for simultaneous adaptation of classification and regression. Current methods primarily rely on adversarial feature adaptation, but they heavily depend on region proposals and ROI-based instance-level features, making them less suitable for one-stage object detectors. In this paper, we propose an Implicit Instance-Invariant Network (I3Net) that implicitly learns instance-invariant features by aligning transferable regions and images while preserving inter-domain class relationships. We introduce a Dynamic and Class-Balanced Reweighting (DCBR) strategy to address adaptation difficulty and develop a Category-aware Object Pattern Matching (COPM) module and Regularized Joint Category Alignment (RJCA) module to enhance cross-domain foreground object matching and enable category alignment. Experimental results demonstrate that our I3Net significantly improves the state-of-the-art performance of one-stage cross-domain object detection on multiple benchmarks.