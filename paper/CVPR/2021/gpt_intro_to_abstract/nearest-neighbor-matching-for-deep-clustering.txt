Unsupervised learning has gained popularity due to the high cost of obtaining labeled data. Clustering methods, a crucial component of unsupervised learning, aim to group samples into clusters based on their similarities. Traditional clustering methods, such as K-Means, Spectral Clustering, and Nonnegative Matrix Factorization, have been widely used but are limited by their focus on low-level information. Deep clustering methods, which leverage deep learning techniques, have emerged to address this limitation by transforming samples into a latent embedded space for improved features and performance.While deep learning representations show potential for clustering on unlabeled data, there remains a question of how to enhance the semantic confidence of these clusters. Previous approaches have addressed this challenge by iteratively evaluating clustering assignments or simultaneously learning feature representations and clustering assignments. However, these methods often overlook the semantic relationships between samples at both local and global levels and may produce suboptimal results due to dynamically updated deep features.In this paper, we propose a novel method called Nearest Neighbors Matching (NNM) for deep clustering. NNM leverages the rich semantic relationships in both local and global features by matching nearest neighbors from both levels. We introduce consistent loss and class contrastive loss to ensure consistent clustering assignments between neighbors and classes. Importantly, NNM can be applied as a plug-in module to enhance the semantic feature representation in any network.The contributions of our research are as follows: 1. We introduce NNM, a deep clustering framework based on two-level nearest neighbors matching, that improves clustering performance by considering both local and global features. Furthermore, NNM can be easily incorporated into existing networks to learn more semantic feature representations.2. We provide confusion matrices and show the desired block-diagonal structure after optimizing the NNM loss. We also conduct additional ablation studies to determine the optimal techniques for deep clustering.3. We present extensive experimental results on three benchmark datasets, demonstrating the superiority of NNM over other state-of-the-art methods. Notably, our method achieves supervised performance on the STL-10 dataset and outperforms the latest comparison method by 3.7% on the CIFAR-100 dataset.Overall, our proposed NNM method offers a promising approach to deep clustering by leveraging semantic relationships at both local and global levels.