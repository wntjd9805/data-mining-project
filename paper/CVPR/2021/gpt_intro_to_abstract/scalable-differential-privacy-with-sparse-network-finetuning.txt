The increasing prevalence of computer vision technology has raised concerns about the robustness and privacy of vision models. Numerous attacks have successfully revealed sensitive information from training data, such as credit card and social security numbers. This is particularly worrisome in fields like medical imaging that handle legally-protected data. The responsibility of safeguarding private data falls on machine learning practitioners and may be enforced by regulations like the GDPR and the California Consumer Privacy Act. Differential privacy has emerged as a widely recognized approach to address privacy concerns in computer vision. It provides strict upper bounds on the information derived from resulting models and offers guarantees of robustness to adversarial examples and compliance with data privacy laws. However, building powerful differentially private machine learning systems involves trade-offs between model utility and privacy. Larger networks suffer greater disruption during training compared to their non-private counterparts, resulting in significant utility penalties. This is due to the implementation of differential privacy, which requires bounding the influence of each example on the mini-batch gradient. In this paper, we propose a novel solution to improve the trade-off between privacy and utility in deep neural networks with differential privacy. We leverage additional public datasets to instill strong representations in large models, which are then adapted to private datasets at minimal privacy cost. We minimize the negative effects of differential privacy by reducing the number of trainable parameters to only those necessary for effective transfer learning. We also identify and finetune specific parameters that carry domain-specific information, minimizing the domain gap between public and private datasets. Additionally, we incorporate insights from model pruning techniques and propose a novel approach for selecting and finetuning a small subset of parameters in convolutional layers. Our contributions include an effective method for scaling differential privacy to large neural networks, enabling differential private training on small private datasets at a reasonable privacy cost, and achieving state-of-the-art performance with a smaller privacy budget on the CIFAR-10 benchmark.