Recent advances in deep learning have enabled significant progress in vision-language tasks. Referring image segmentation, a task that aims to extract the most relevant visual region in an image based on a given language query, has gained attention due to its potential applications in language-based human-robot interaction. Traditional semantic and instance segmentation methods focus on segmenting specific semantic categories or objects, while referring image segmentation aims to find a certain part of an image based on the understanding of a language query, making it a foreground/background segmentation problem. This paper proposes an encoder fusion network with co-attention embedding (CEFNet) for referring image segmentation. Unlike previous methods that adopt decoder fusion strategies, CEFNet leverages the encoder to guide the progressive learning of multimodal features using language. The proposed co-attention mechanism ensures semantic alignment between different modalities, facilitating cross-modal matching and bridging the gap between referring expression and visual segmentation. Additionally, a boundary enhancement module is introduced to recover finer details in the targeted region. Experimental results on four large-scale datasets demonstrate that the proposed approach achieves state-of-the-art performance while maintaining real-time processing speed.