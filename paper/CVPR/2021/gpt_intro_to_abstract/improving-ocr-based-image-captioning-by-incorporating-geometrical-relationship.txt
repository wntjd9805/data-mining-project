This paper introduces a novel approach to OCR-based image captioning, which aims to generate descriptive sentences based on visual entities found in images. Despite progress in traditional image captioning and OCR technologies, OCR-based image captioning still faces challenges in understanding words, recognizing visual entities, and reasoning among them. The scene text, closely related to both visual content and language descriptions, plays a crucial role in this task. Existing methods have shown the importance of encoding visual, semantic, and spatial features of OCR tokens for improved image understanding. However, these methods may produce incorrect descriptions due to imprecise relationships between OCR tokens. This paper proposes the utilization of geometrical relationships between OCR tokens, such as size relations and position relations, to enhance image captioning. A relation-aware pointer network is introduced to select OCR tokens based on the learned geometrical relationships. The proposed LSTM-R architecture combines Faster R-CNN and OCR systems to extract features and encode relation vectors. Multi-label loss optimization is used to train the model, considering multiple OCR regions for a single OCR token in the image. Additionally, a word encoding method is designed to incorporate the rich OCR representations in the model training process. Experimental results on the TextCaps dataset demonstrate the effectiveness of the proposed approach, achieving state-of-the-art performance in OCR-based image captioning.