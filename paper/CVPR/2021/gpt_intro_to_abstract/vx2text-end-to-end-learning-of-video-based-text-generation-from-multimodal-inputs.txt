This paper introduces VX2TEXT, a video-based approach for developing conversational multimodal systems in AI. The goal is to extract information from different modalities, combine them effectively, and generate human-comprehensible text. VX2TEXT accomplishes this by using modality-specific classifiers to convert input signals into a semantic language space, enabling the application of powerful language models. A generative text decoder then transforms multimodal features into text, allowing for open-ended sentence generation. The architecture is trained end-to-end, resulting in significant performance gains compared to separate modules. VX2TEXT outperforms existing approaches in captioning, question answering, and dialogue tasks.