High-level scene understanding is a crucial task in computer vision, with applications in robotics and augmented or mixed reality. Recent advancements in depth sensors and 3D datasets have shifted research focus towards enhancing 3D maps with semantic information about scene components. While several methods have employed neural networks to process complete 3D scans, these methods typically rely on offline processing and require prior knowledge of 3D geometry. Real-time scene understanding that incrementally builds 3D scans presents challenges such as handling partial and ambiguous scene geometry. Additionally, fusing multiple network predictions to ensure consistency in the global map is challenging. Semantic scene graphs have been used in the image domain to derive relationships among scene entities, demonstrating their power in representation. Recent works have explored scene graph prediction from entire 3D scans offline, but building up semantic graph maps online remains a major challenge. This paper proposes a real-time method to incrementally build a globally consistent semantic scene graph parallel to 3D mapping. The approach utilizes a geometric segmentation method and an inductive graph network to handle missing edges and nodes in partial 3D point clouds. The method predicts scene semantics, identifies object instances, and learns relationships among over-segmented regions. The paper presents the first online 3D scene graph prediction and introduces a new relationship type for merging nodes into 3D instances. It also introduces a novel attention method to handle partial and incomplete 3D data and dynamic edges. Experimental results show that the proposed method outperforms 3D scene graph prediction and achieves comparable performance on 3D semantic and instance segmentation benchmarks, running at 35Hz.