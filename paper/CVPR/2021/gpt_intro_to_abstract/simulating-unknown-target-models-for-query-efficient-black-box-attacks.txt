Deep neural networks (DNNs) are susceptible to adversarial attacks, where imperceptible alterations are made to images to deceive the target model. Adversarial attacks can be categorized as white-box or black-box attacks, with the latter being more practical in real-world scenarios. Query-based attacks estimate gradients through queries, but they are costly and underutilize the feedback from the target model. Transfer-based attacks train a local substitute model to mimic the target model, but this requires querying the target model and can be defended against easily. To address these limitations, this paper proposes a meta-learning-based framework called "Simulator Attack" that trains a generalized substitute model to mimic the output of any target model. The Simulator is trained using a query-sequence level partition strategy and a mean square error-based knowledge-distillation loss. The proposed approach significantly reduces query complexity compared to baseline methods and achieves similar success rates with a low number of queries. The contributions of this work include the development of the Simulator Attack, the identification of a new security threat, and the demonstration of the proposed approach's effectiveness through extensive experiments on various datasets.