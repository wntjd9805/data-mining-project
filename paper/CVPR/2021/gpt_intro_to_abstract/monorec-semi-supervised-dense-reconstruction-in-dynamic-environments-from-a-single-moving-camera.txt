Obtaining a comprehensive understanding of the 3D environment is a significant challenge in robotics, AR/VR, and autonomous driving. Currently, this is achieved through the fusion of multiple sensor sources. However, maintaining and calibrating such a complex sensor suite is expensive and challenging, leading to a demand for reducing the number of sensors. While single monocular cameras have been successful in ego-motion estimation, reliable dense 3D mapping of the environment is still an open research topic. There are two parallel lines of research to address this problem: dense multi-view stereo (MVS) methods and monocular depth prediction methods. MVS methods assume a stationary environment and struggle with dynamic objects, while monocular depth prediction methods rely heavily on specific camera intrinsics and extrinsics and do not generalize well. To combine the advantages of both methods, we propose MonoRec, a novel monocular dense reconstruction architecture. It includes a MaskModule and a DepthModule that use cost volumes based on structural similarity index measure (SSIM) instead of absolute differences. The MaskModule identifies moving pixels, allowing MonoRec to estimate depth for both static and dynamic objects without artifacts. Through a multi-stage training scheme, MonoRec achieves state-of-the-art performance on the KITTI dataset and demonstrates strong generalization capabilities on the Oxford RobotCar and TUM-Mono datasets.