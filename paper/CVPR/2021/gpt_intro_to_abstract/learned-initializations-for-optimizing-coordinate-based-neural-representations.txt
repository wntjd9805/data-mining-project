In this paper, we propose using meta-learning to find initial network weights that enable faster convergence and better generalization in coordinate-based neural representations. These representations, implemented as deep fully-connected neural networks, map input coordinates to the corresponding signal values. Unlike discrete signal arrays, coordinate-based representations are continuous and not limited by fixed spatial resolution. Our approach aims to overcome the computational expense of optimizing network weights for reproducing a given signal by computing a specialized weight initialization. This approach has been successfully applied to reduce the number of gradient descent steps required for optimizing neural representations of signed distance fields. We extend this idea to learn weight initializations across various signal types, such as images, volumetric data, and 3D scenes. Compared to randomly initialized networks, our learned initial weights facilitate faster convergence and better generalization, particularly when only partial observations of the target signal are available. By using optimization-based meta-learning algorithms, we produce initial weights tailored to specific signal classes. Our approach is simple to implement and can significantly improve network behavior during optimization.