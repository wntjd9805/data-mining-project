Virtual try-on of fashion images aims to change the clothes of a person with other in-shop clothes, enabling applications such as costume matching, fashion image editing, and clothes retrieval for e-commerce. Existing methods primarily focus on direct try-on based on 2D images, as obtaining paired images of multiple models wearing different and pixel-wise aligned clothes is impractical. However, handling unpaired images poses a challenge. In response, previous approaches such as VITON, CP-VTON, CP-VTON+, and ACGPN have been developed, each with its own limitations. These methods either perform inpainting-based reconstruction from the corrupted input image to the original image using the same in-shop clothes, or employ cycle consistency by substituting the clothes of an input image with an arbitrary target in-shop image. However, both approaches suffer from issues such as inaccurate generation and artifacts. To address these limitations, we propose a disentangled cycle-consistency try-on network (DCTON) that disentangles the virtual try-on process into three sub-modules: clothes warping, skin synthesis, and image composition. DCTON disentangles these components from input images to form a try-on cycle for self-supervised learning. Extensive experiments conducted on benchmark datasets demonstrate that DCTON outperforms state-of-the-art virtual try-on approaches.