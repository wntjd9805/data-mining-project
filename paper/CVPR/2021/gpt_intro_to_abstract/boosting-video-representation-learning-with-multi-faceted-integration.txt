Deep Neural Networks have been proven to be highly effective for learning vision models on large-scale datasets. However, while there are various image datasets available for training deep models, video datasets pose additional challenges due to their spatio-temporal nature. To overcome these challenges, one approach is to acquire more video data. Previous work has focused on specific facets of video content, such as action, while neglecting other facets. This paper proposes a novel approach to video representation learning, which integrates six facets of video content from different datasets, including action, event, interaction, sport, object, and scene. The goal is to learn a more discriminative and generic representation by leveraging multiple facets of video information. The proposed framework, called MUFI, employs a visual-semantic embedding learning approach, in which a semantic space is generated by pre-training a model on unannotated text data. Video representations are projected into this semantic space and optimized using intra-facet and inter-facet supervision. The proposed framework is evaluated on a union of four large-scale video datasets and two image datasets, demonstrating its effectiveness in improving video representation learning. The main contribution of this work is the exploration of multifaceted video content and the development of a unified framework for learning representations that capture multiple facets of videos.