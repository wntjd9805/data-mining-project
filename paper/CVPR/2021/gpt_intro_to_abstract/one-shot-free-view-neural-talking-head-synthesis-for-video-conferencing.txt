We present a novel approach to generating realistic talking-head videos using a neural rendering technique. Our method allows for the synthesis of a talking-head video using only a single source image of the target person and a driving video. We avoid the use of 3D graphics models, which simplifies the process and eliminates the need for laborious and expensive 3D model acquisition. Moreover, our approach overcomes the limitations of existing 2D-based methods by enabling the rendering of the talking-head from various viewpoints. This is achieved through the use of a 3D keypoint representation, which allows for the simulation of head pose changes and local free-view control. We extensively validate our method through comparisons with state-of-the-art techniques and demonstrate its effectiveness in tasks such as video reconstruction, motion transfer, and face redirection. Additionally, we show that our approach can significantly reduce the bandwidth required for video streaming, making it a valuable tool for applications such as video conferencing and remote collaborations. Our contributions include the development of a superior one-shot neural talking-head synthesis approach, the achievement of local free-view control without 3D graphics models, and a substantial reduction in bandwidth for video streaming.