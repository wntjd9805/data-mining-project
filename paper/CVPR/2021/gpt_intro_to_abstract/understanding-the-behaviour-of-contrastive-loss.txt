Deep neural networks have made significant advancements in recent years, largely due to the availability of large human-annotated datasets. However, manual labeling of these datasets is time-consuming and expensive. Unsupervised learning methods offer a promising solution by allowing for the learning of transferable representations without the need for human supervision. Contrastive learning methods, which aim to learn feature functions that map raw pixels into features residing on a hypersphere space, have achieved great success in unsupervised learning. These methods use the contrastive loss, which encourages positive pairs to be attracted and negative pairs to be separated. By leveraging heavy augmentations and the abstraction ability of convolutional neural networks, contrastive models can learn semantic structures. However, the contrastive loss function itself has certain properties that can affect the quality of the learned representations. In this paper, we specifically analyze the properties of the contrastive loss using the temperature as a proxy. We find that the contrastive loss is a hardness-aware loss function, automatically concentrating on optimizing hard negative samples. The temperature parameter plays a role in controlling the strength of penalties on hard negatives. Additionally, the uniformity of the learned embedding distribution is crucial for learning separable features. We explore the relationship between temperature and embedding uniformity, discovering that while uniformity is important for contrastive models, excessive pursuit of uniformity can undermine the underlying semantic structure. We identify a uniformity-tolerance dilemma in contrastive learning and propose that a good choice of temperature can strike a balance between these two properties and significantly improve feature quality. Our contributions include an analysis of the contrastive loss, validation of the hardness-aware property, demonstration of the role of temperature in penalizing hard negatives, and exploration of the uniformity-tolerance dilemma in contrastive learning.