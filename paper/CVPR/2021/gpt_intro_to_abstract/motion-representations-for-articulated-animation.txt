Animated characters and objects in education and entertainment have become increasingly popular due to their ability to enhance creativity, appeal, and user experiences. Traditionally, achieving high-quality animation required specialized tools and skills. However, recent progress in vision and graphics communities has led to the development of data-driven methods that can overcome some of these limitations. These methods rely on training with ground truth pose and shape data, which can be challenging to obtain. To address this issue, unsupervised motion transfer techniques have been introduced, which aim to eliminate the need for ground truth data. Despite significant advancements, two key challenges remain. First, representing the parts of articulated or non-rigid moving objects, including their shapes and poses, is still a challenge. Second, animating these object parts using the sequence of motions in a driving video is also problematic. Existing approaches have utilized end-to-end frameworks and unsupervised keypoints for motion alignment and generation. However, these keypoints are often detected on object boundaries, making tracking and establishing correspondences between frames difficult. Additionally, they do not represent semantically meaningful object parts and are susceptible to background motion leakage. Furthermore, absolute motion transfer can lead to a decrease in source identity fidelity. To address these challenges, this paper introduces three contributions: redefining the motion representation using regions, explicitly modeling background or camera motion, and disentangling shape and pose. These contributions improve unsupervised motion transfer methods, particularly for articulated objects. The proposed framework does not require labeled data and is optimized using reconstruction losses. Experimental results demonstrate the effectiveness of the proposed approach on various datasets and benchmark evaluations.