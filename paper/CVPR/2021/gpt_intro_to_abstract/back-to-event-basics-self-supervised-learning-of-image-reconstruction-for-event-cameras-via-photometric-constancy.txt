Event cameras, which sample light based on scene dynamics, offer advantages such as low power consumption, high dynamic range, low latency, and high temporal resolution compared to conventional cameras. However, the novel output format of event cameras presents challenges in algorithm design. To bridge the gap between event cameras and existing frame-based computer vision literature, researchers have developed methods to reconstruct images from events. Recent work has focused on machine learning approaches, showing that learning-based methods outperform hand-crafted techniques in terms of image quality. However, these architectures are typically trained on large sets of synthetic data, which may not accurately represent real-world scenarios. To address this, the authors propose a self-supervised learning framework that does not rely on ground-truth or synthetic data. The framework consists of two neural networks, FlowNet for optical flow estimation and ReconNet for image reconstruction. Both networks are trained using proxy loss functions, and several existing networks are retrained using this method. Experimental results demonstrate that the reconstructed images are comparable to those generated by learning-based approaches, despite the lack of ground-truth data during training. Additionally, the authors introduce FireFlowNet, a lightweight architecture for fast optical flow estimation. The paper presents two main contributions: the self-supervised learning framework for event-based image reconstruction and FireFlowNet for efficient optical flow estimation. Extensive evaluations on multiple datasets validate the proposed methods.