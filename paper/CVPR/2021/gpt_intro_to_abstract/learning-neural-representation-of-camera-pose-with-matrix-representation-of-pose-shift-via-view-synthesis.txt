This paper introduces a novel approach for learning neural representations of camera poses and local camera movements. Inspired by the way grid cells in the mammalian brain enable mental self-navigation, the authors propose a vector representation for the underlying 3D scene and a distinct vector representation for each camera pose. When the camera undergoes a local displacement, the 3D scene vector remains unchanged while the camera pose vector is rotated by a matrix representation of the camera movement. The matrix representation is parametrized by a matrix Lie group and its corresponding matrix Lie algebra. These neural representations can be shared across multiple scenes, allowing for improved performance through shared learning. The paper demonstrates the effectiveness of this approach in various experiments on synthetic and real datasets for novel view synthesis and camera pose regression. The contributions of this work include the proposal of a new method for learning neural camera pose and movement representations, the association of these representations with visual input through a generative model, and the demonstration of their effectiveness in camera pose regression.