Single image super-resolution (SISR) is a fundamental computer vision task that aims to restore high-resolution details from low-resolution images. Deep convolutional neural networks (CNN) have been successful in addressing this problem, but the ill-posed nature of SISR makes it challenging to recover high-quality details. This paper explores the use of reference-based super-resolution (RefSR), which incorporates an external reference image to enhance the super-resolution process. Reference images typically have similar content and texture to the low-resolution image and can be acquired from web searches or different viewpoints. This approach has shown promising results in recent work, with methods focusing on addressing two critical issues: matching useful content between the reference and low-resolution images and transferring features to facilitate high-resolution reconstruction.Existing methods employ different strategies to tackle these issues, with some performing spatial alignment between the reference and low-resolution images using techniques like optical flow or deformable convolutions. However, these alignment-based methods face challenges in finding long-distance correspondences. Other methods rely on patch matching in the feature space, but this leads to high computational costs and large memory usage.To address these problems, this paper proposes a reference-based super-resolution method called MASA-SR, which improves patch matching and feature transfer. MASA-SR introduces the Match & Extraction Module (MEM), which performs correspondence matching in a coarse-to-fine manner, significantly reducing computational cost while maintaining matching quality. By exploiting the local coherence property of natural images, MEM narrows the search space for each patch in the low-resolution feature map to a specific block in the reference feature map.Additionally, MASA-SR employs a Spatial Adaptation Module to handle cases where there are significant disparities in color or luminance distribution between the low-resolution and reference images. This module learns to adapt the distribution of reference features to the low-resolution features in a spatially adaptive way, enabling more effective transfer and utilization of useful information.The proposed MASA-SR method achieves state-of-the-art performance in the RefSR task, with significant reductions in computational cost compared to previous approaches. The Match & Extraction Module reduces computational costs by two orders of magnitude in terms of FLOPS (floating-point operations per second). Furthermore, the Spatial Adaptation Module is robust to differences in color and luminance distributions, enhancing the network's ability to utilize information from the reference images effectively.In summary, this paper presents a reference-based super-resolution method, MASA-SR, which addresses the challenges of patch matching and feature transfer.