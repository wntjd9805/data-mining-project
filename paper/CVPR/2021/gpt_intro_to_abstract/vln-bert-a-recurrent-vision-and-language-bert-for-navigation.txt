Recent advancements in vision-and-language navigation (VLN) setups have paved the way for research on enabling robots to navigate complex environments based on human instructions. Various methods have been explored to leverage visual and language clues to assist navigation, such as enhancing visual-textual correspondence in the popular R2R navigation task. Additionally, pre-training models like V&L BERT have shown significant improvement in visiolinguistic problems by learning generic cross-modal representations. Inspired by these advancements, this paper proposes the use of V&L BERT for VLN, allowing navigation learning to benefit from pre-trained visual-textual knowledge.However, VLN differs from other vision-and-language tasks as it can be considered a partially observable Markov decision process, where future observations depend on the agent's current state and action. Furthermore, the visual observation at each navigational step only corresponds to a partial instruction, requiring the agent to track navigation progress and localize relevant sub-instructions for decision-making. Another challenge of applying V&L BERT for VLN is the high computational power demand, as self-attention on long visual and textual sequences during training can be memory-intensive.To address these challenges, the paper introduces a recurrent vision-and-language BERT for navigation, or VLN-BERT. Instead of using large-scale datasets for pre-training, this approach leverages the history-dependent state representations using a recurrent function within the original V&L BERT architecture. The paper also reduces memory consumption by controlling self-attention during navigation and enables the model to be trained on a single GPU without performance degradation. Additionally, the proposed model has the potential for multi-task learning, addressing other vision and language problems alongside navigation.The performance of VLN-BERT is evaluated on two datasets, R2R and REVERIE. The agent, initialized from a pre-trained V&L BERT and fine-tuned on these datasets, achieves state-of-the-art results. Furthermore, initializing the model with the PREVALENT model leads to absolute improvements in success rates on the R2R test split and satisfactory results on the remote referring expression task in REVERIE. These results demonstrate the strong generalization ability of VLN-BERT and its potential for merging the learning of VLN with other vision and language tasks.