This paper introduces a novel approach for visualizing the decision process of Transformer networks, which are state-of-the-art methods in natural language processing (NLP) and computer vision. The main focus is on self-attention layers, which assign attention values between tokens in NLP and image patches in computer vision. The traditional approach of visualizing single attention layers or averaging attentions across multiple layers can lead to blurring of the signal and overlook the different roles of the layers. The rollout method, which assigns attention scores by considering pairwise attentions, has shown improvement but highlights irrelevant tokens. The authors propose a relevancy propagation rule that maintains the sum of relevancy throughout layers and addresses challenges posed by skip connections and non-linearities in Transformer networks. Their approach provides class-specific visualization and focuses on improving performance on acceptable benchmarks in the field of explainability. The proposed method is evaluated on various computer vision benchmarks, including image segmentation and perturbations, as well as an NLP explainability benchmark.