Convolution is a key component in convolutional neural networks (CNNs) used for computer vision tasks. However, it has two main shortcomings: being content-agnostic and computation-heavy. Content-agnostic refers to the fact that convolution filters are shared across all pixels in an image, which may not be optimal for capturing features across different regions with varying contents. Several existing dynamic filters have been proposed to address this issue, but they are either computationally or memory intensive. On the other hand, the computation complexity of standard convolution increases with the size of the filters or number of channels, which is problematic for CNNs with a large number of channels. Although grouped or depth-wise convolutions can reduce computation complexity, they often lead to performance drops. To address these shortcomings, this work introduces the Decoupled Dynamic Filter (DDF) as a replacement for standard convolution layers. DDF decouples the dynamic filter into spatial and channel filters, similar to attention mechanisms in recent works. Specifically, separate attention-style branches are used to predict spatial and channel dynamic filters, which are then combined to form a filter at each pixel. This decoupling makes DDF lightweight and computationally efficient, while achieving better performance compared to existing dynamic filters. DDF can replace all k x k (k > 1) convolutions in a CNN and has a smaller memory footprint compared to dynamic filters. Experimental results show that applying DDF consistently improves performance while reducing computational costs in classification networks. Additionally, a variant of DDF called DDF-Up is introduced for specialized upsampling tasks and demonstrates superior performance in object detection and joint upsampling networks. In summary, DDF and DDF-Up provide content-adaptive filtering, reduce memory consumption, and consistently improve performance when replacing standard convolutions in various networks and tasks.