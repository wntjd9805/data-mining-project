This paper addresses the problem of unsupervised representation learning from label-free data in computer science. The goal is to bridge the performance gap between supervised and unsupervised learning algorithms. The existing methods in this area can be categorized into handcrafted pretext tasks and contrastive visual representation learning. Pretext tasks involve learning data-level invariant features through tasks such as jigsaw puzzle, image rotation, and image colorization. Contrastive visual representation learning focuses on learning similarity and dissimilarity between data pairs through image transformations. However, the quality of the learned representations in both approaches is highly dependent on the initial design of the pretext tasks or the configuration of the contrastive loss and image transformations. To address this issue, the authors propose a framework called Progressive Stage-wise Learning (PSL) for improving unsupervised/self-supervised tasks. PSL adopts a curriculum learning approach and introduces a progressive stage-wise training scheme. The framework partitions the unsupervised learning task into multiple levels of increasing complexity and assigns different learning stages to capture easy-to-hard concepts. Training is performed stage by stage, with each stage benefiting from the learning experience of previous stages.The authors present a specific implementation of PSL for the jigsaw puzzle task, but emphasize that PSL can be applied to any unsupervised learning scenario with appropriate multi-level task design. The effectiveness of PSL is validated through experiments on various unsupervised/self-supervised tasks, including jigsaw puzzle and image rotation pretext tasks, as well as contrastive learning. The authors evaluate the learned representations through linear classification, semi-supervised learning, and transfer learning tasks.The contributions of this paper can be summarized as follows: 1. PSL introduces new dimensions for unsupervised learning research, including task series, network partitions, and stage-wise training.2. PSL is designed as a general framework applicable to multiple unsupervised learning tasks, whether they involve pretext tasks or contrastive learning.3. Experimental results demonstrate that the feature representations learned by PSL consistently achieve better quality than the original unsupervised tasks in downstream applications, such as semi-supervised learning and transfer learning.