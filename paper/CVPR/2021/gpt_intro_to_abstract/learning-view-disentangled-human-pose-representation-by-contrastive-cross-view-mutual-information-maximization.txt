Understanding human poses and actions is a fundamental problem in computer vision, with widespread applications in areas such as video content analysis, intelligent photography, augmented and virtual reality, and human-computer interface. Deep learning approaches have shown remarkable improvements in this field, but they are often susceptible to changes in viewpoints, especially when dealing with unseen viewpoints at testing time. Previous methods for cross-view action recognition have relied on extensive supervision from multiple views, which can be costly and challenging to collect in real-world scenarios. In this paper, we propose a novel view-disentangled representation learning approach for human poses. We train a representation model using pairs of 2D poses from different viewpoints, without requiring additional task-relevant supervision. Our goal is to disentangle pose-dependent (view-invariant) and view-dependent representations. To achieve this, we maximize the cross-view mutual information (MI) between representations of the same pose from different views, and we introduce regularization terms to encourage disentanglement and smoothness of the learned representation. Our approach leverages recent advances in MI estimation and uses negative training pairs for stronger representative powers. We demonstrate the effectiveness of our approach in fully-supervised action recognition and introduce a novel task called single-shot cross-view action recognition, where models are trained with 2D poses from a single view but expected to generalize to unseen views at testing time. Our method consistently outperforms competing approaches in scenarios with limited data and in the single-shot cross-view setting, while performing comparably to state-of-the-art methods in the fully-supervised setting. Our contributions include a novel objective for learning view-disentangled representation, regularization techniques for disentanglement and smoothness, and the introduction of the single-shot cross-view action recognition task for evaluating view-invariant representation for human poses.