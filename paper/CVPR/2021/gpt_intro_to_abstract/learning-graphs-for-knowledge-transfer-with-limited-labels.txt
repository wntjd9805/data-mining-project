Graph Convolution Network (GCN) techniques have been widely used in transfer learning tasks with limited labeled data, such as semi-supervised learning and zero-shot/few-shot learning. These techniques rely on an input graph that captures the relationships between nodes. The goal is to transfer information from training nodes to test nodes. However, the existing GCN-based approaches have limitations, particularly regarding the fixed input graph structure and the inability to learn to add or drop connections in the graph. In this paper, we propose an adaptive learning approach to update the input adjacency matrix of the graph during the GCN training. We demonstrate that our learned graph yields better results for downstream tasks compared to the input graph from an external source. Our approach does not require adding new network weights, distinguishing it from other related graph learning works. To address the issues of arbitrary graph updates and sparse connections, we utilize a triplet loss formulation on intermediate output nodes to add constraints on the degree of the nodes. This formulation ensures that negative neighbors are farther than positive ones, avoiding degenerate solutions.Our contributions include a simple learning approach to update input graphs for the GCN-based transfer learning framework and a triplet loss formulation that allows flexibility of degree constraints and avoids degenerate solutions. We evaluate the effectiveness of our approach on semi-supervised, zero-shot, and few-shot learning setups, using various datasets and input graphs. Our results demonstrate improved performance compared to existing methods in these learning scenarios.Keywords: Graph Convolution Network, transfer learning, adaptive learning, input graph, triplet loss, degree constraints, semi-supervised learning, zero-shot learning, few-shot learning.