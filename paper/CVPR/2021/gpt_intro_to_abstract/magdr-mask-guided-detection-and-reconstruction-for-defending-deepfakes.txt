Deepfakes initially emerged as a technology with neutral applications, allowing for the synthesis of images in which human faces could be replaced with different identities. However, deepfakes have gained infamy due to unethical uses, such as creating fake pornographic videos featuring celebrities or spreading malicious messages. Efforts to detect and mitigate the impact of deepfakes have been made, but research has shown that existing detectors can be easily fooled by adversarial perturbations. Another approach involves adding adversarial perturbations to the source image to severely damage the output, which was believed to be more robust. In this paper, we propose a framework called mask-guided detection and reconstruction (MagDR) to defend against adversarial attacks on deepfakes. Our approach involves defining criteria sensitive to abnormalities in the output, training a mask-guided detector to determine if the input image has been contaminated, and employing a reconstruction algorithm to remove the damage caused by adversarial perturbations and restore the desired output. The novelty of our approach lies in the use of masks to provide auxiliary information in the detection and reconstruction processes. These masks, learned from individual training processes, correspond to specific parts of the human face. Guided by the masks, the detector can partition into two components: one detecting distortion and the other detecting inconsistency, both indicating regions likely to be contaminated. To reconstruct the desired output, we design a pipeline with adjustable parameters, allowing for a changeable execution order. Through adaptive optimization, we suppress all predefined criteria and generate the recovered output. We evaluate our approach on two popular datasets and three image-to-image translation methods commonly used in deepfakes. The experiments include both oblivious and adaptive attack scenarios, demonstrating the vulnerability of deepfakes and the efficacy of MagDR in mitigating their impact. Furthermore, our approach shows advantages over state-of-the-art adversarial attackers and defenders. MagDR also exhibits transferability across different scenarios, suggesting that adversarial perturbations can be detected based on common rules. The contributions of this paper include highlighting the persistence of deepfake threats despite the addition of adversarial perturbations, designing a mask-guided detection and reconstruction framework that effectively addresses deepfake corruption, and proposing a heuristic and hierarchical reconstruction module that significantly reduces computational costs while enhancing defense performance.