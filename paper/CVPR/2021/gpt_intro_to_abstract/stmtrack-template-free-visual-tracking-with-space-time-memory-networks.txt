Visual object tracking is a crucial task in computer vision, with applications in areas such as human-computer interactions, video surveillance, and autonomous driving. Despite significant efforts, practical applications of visual tracking still face challenges, such as occlusions, fast object motions, and non-rigid deformations. This necessitates the development of trackers that possess adaptiveness and robustness. In recent years, deep learning techniques have dominated the tracking field, with two widely studied methodologies. One addresses object tracking as a similarity matching problem between the target template and search frames, while the other focuses on template updating mechanisms. However, the latter approach requires computationally intensive resources and involves hyper-parameter tuning. In this paper, we propose a memory-based tracking model that avoids template updating by predicting the object's state from historical information stored in a memory network. We refer to our model as a template-free method. Additionally, our tracker computes pixel-level similarities, which make it more robust to occlusions and non-rigid deformations compared to methods that use feature-map-level cross correlation. Our proposed method achieves state-of-the-art performance on six benchmarks, including OTB-2015 and TrackingNet, while running in real-time at 37 FPS. The contributions of this work include a novel memory-based tracking framework that is simple, efficient, and adaptable, a new approach that deviates from the traditional template-based tracking methods, and a memory mechanism based on pixel-level similarity computation that enhances robustness and accuracy. Overall, our approach demonstrates superiority in real-time visual object tracking.