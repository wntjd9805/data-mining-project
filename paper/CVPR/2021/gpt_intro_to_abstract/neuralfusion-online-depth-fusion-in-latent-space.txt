Reconstructing scene geometry plays a vital role in various 3D computer vision applications such as robot navigation, augmented reality, and 3D scene understanding. One challenge in online surface reconstruction is dealing with noisy, incomplete, and outlier-ridden depth maps. Depth map fusion is a critical component in 3D reconstruction methods, but existing approaches struggle with outliers and thin geometry due to local integration of depth values. Current methods use heuristic-based filtering techniques that trade off accuracy for completeness. To address these limitations, we propose a new network architecture that separates scene representations for depth fusion and final output. Our approach fuses depth maps in a latent scene representation, implicitly learned to encode features such as confidence and local scene information. We also introduce end-to-end learnable outlier filtering in a translation step, significantly improving outlier handling. Despite being fully trainable, our method only performs localized updates to the global map, maintaining online capability. Our contributions include a novel network architecture, more accurate and complete fusion results, end-to-end learnable outlier filtering, and localized updates to the global map.