Video grounding is a challenging task in multimedia information retrieval that aims to locate the temporal boundaries of a target video span given a textual description. Existing methods for video grounding can be categorized into ranking methods, regression methods, and reinforcement learning approaches. However, these approaches may suffer from spurious correlations between textual and visual features due to dataset selection bias. This paper proposes a novel paradigm called interventional video grounding (IVG) based on causal inference to address this problem. IVG leverages Pear's structured causal model (SCM) to mitigate the spurious correlations and introduces a dual contrastive learning (DCL) method to learn more informative feature representations. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed IVG-DCL model. The contributions of this paper include the introduction of IVG, a novel model for video grounding, the development of an approximation method for latent confounders based on SCM, and the proposal of DCL, a dual contrastive learning method.