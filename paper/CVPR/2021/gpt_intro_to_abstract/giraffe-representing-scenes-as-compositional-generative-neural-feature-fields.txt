The goal of photorealistic image generation and manipulation has long been sought after in computer vision and graphics. While modern computer graphics techniques have achieved impressive results, they are expensive and labor-intensive. In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful class of generative models capable of synthesizing high-resolution photorealistic images. However, the generation process should not only produce realistic images but also be controllable. Many works have investigated the learning of disentangled representations without explicit supervision, allowing for control over specific attributes without affecting others. However, most approaches operate in the 2D domain, disregarding the three-dimensional nature of the world and leading to entangled representations. This paper introduces GIRAFFE, a novel method for generating controllable and photorealistic scenes by directly incorporating a compositional 3D scene representation into the generative model. This approach allows for more consistent image synthesis and enables complex operations such as object translations and additions. GIRAFFE outperforms existing methods by achieving faster inference and generating more realistic images at higher resolutions. The method is applicable to both single-object and multi-object scenes when trained on raw unstructured image collections. The code and data for GIRAFFE are available for further exploration.