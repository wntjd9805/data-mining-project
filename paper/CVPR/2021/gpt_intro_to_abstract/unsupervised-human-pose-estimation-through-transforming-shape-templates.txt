In today's digitized world, there is a wealth of unlabeled but structured data in the form of images and videos. Leveraging this vast reserve of information requires the ability to reason in an unsupervised manner, which is a significant challenge in machine learning and computer vision. Self-supervision techniques have shown promise in providing a good supervisory signal for video data, where objects maintain their intrinsic feature distribution but change their linear relationships between localized features. This paper focuses on the problem of human pose estimation, which has numerous applications in fields like motion capture, visual surveillance, and robot control. While manually annotated ground truth poses and key points are commonly used for supervision, the process of generating such labels is laborious and often limited to specific domains. In domains with limited demand or special requirements, extensive labeling efforts may not be justified, such as in medical applications where body shapes and key point definitions may vary. Additionally, applying models trained on benchmark datasets to new domains is often challenging. To address these issues, unsupervised and weakly supervised pose estimation methods have been proposed, which decompose images into appearance and pose components. Self-supervision tasks, such as image reconstruction or translation, have shown potential in estimating pose without strong manual supervision. This paper presents a method for unsupervised 2D pose estimation using a simple template and unannotated videos. The proposed approach utilizes structured information from videos to control inductive bias for object categories, thereby automatically learning a neural representation that predicts 2D pose from single input images. The accuracy and compliance of the 2D pose predictions with an expected shape prior enable extrapolation to 3D poses with lower error compared to existing methods. The method introduced in this paper does not require additional datasets or unpaired pose examples, instead relying solely on a simple 2D template. The template consists of connected body parts modeled as 2D Gaussians, and their update can be learned through affine transformations. Anchor points are introduced to ensure connectivity between body parts and regularize model training and prediction. The contributions of this paper include the introduction of a conceptually simple yet effective method for learning 2D human-interpretable keypoints, the capability to perform 2D human pose estimation without additional labeled data, and the demonstration of the approach's adaptability through evaluation on benchmark data and a challenging infant pose estimation dataset.