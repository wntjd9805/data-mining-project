Jointly learning vision and language is a significant task in the machine learning and pattern recognition community. This paper focuses on the challenging task of language-instructed object segmentation, which aims to generate a segmentation mask of an object in an image based on a natural language expression. This task has wide applications, such as interactive image editing and language-guided human-robot interaction.Language-referring image segmentation is more challenging than traditional semantic segmentation due to the semantic gap between image and language. The textual expression may contain descriptive words, object properties, actions, and positional relationships. The overall performance of a referring image segmentation model depends on two essential issues: highlighting the most discriminative candidate area in the image corresponding to the given language, and generating a fine segmentation result.Existing referring image segmentation methods utilize Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to represent image and language features, respectively. Cross-modal attention and recurrent ConvLSTM are used to fuse these features to obtain a coarse mask. Dense CRF is then used as post-processing to refine the segmentation.Previous works have focused on fusing image and language features but struggle to effectively model the alignment between the two modalities. Some methods propose cross-modal attention to focus on important regions in the image and informative keywords in the language. Others exploit linguistic structure or perceive all entities referred by the expression. However, these methods lack explicit object localization guided by the language expression and heavily rely on time-consuming post-processing.This paper proposes a decoupled approach for referring image segmentation, consisting of two sub-sequential tasks: referring object position prediction and object segmentation mask generation. The model fuses visual and linguistic features, uses a localization module to obtain the object prior as visual positional guidance, and combines it with cross-modal features to generate the final segmentation mask using a lightweight ConvNets.Experimental results on challenging benchmarks demonstrate that the proposed approach outperforms state-of-the-art methods by a significant margin. Extensive ablation studies validate the effectiveness of each component of the model. Overall, the proposed method provides a simple yet surprisingly effective solution to language-instructed object segmentation.