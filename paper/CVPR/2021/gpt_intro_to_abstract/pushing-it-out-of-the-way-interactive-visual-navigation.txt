Embodied AI has made significant advancements in recent years, particularly in the field of visual navigation. However, most existing navigation methods assume a static environment where agents cannot interact with objects. This paper addresses the challenge of planning for navigation when the agent needs to interact with objects to reach its target. The first challenge is to learn how agent actions affect the pose of objects. Navigation actions usually do not change object positions, while interaction actions, such as pushing objects, can alter their poses. The second challenge is to learn how objects move in response to agent movements or interactions. Finally, the paper proposes a novel model called the Neural Interaction Engine (NIE) that predicts changes in the scene and object poses based on the agent's actions. The model incorporates a navigation policy and predicts the affine transformations of objects from the agent's perspective. The model is evaluated on two tasks: Ob-sNav, where the agent needs to reach specific coordinates while objects block its path, and ObjPlace, where the agent needs to push objects to a target location while navigating. The experiments demonstrate the effectiveness of the NIE model in predicting the outcome of interactions and show significant improvements over baselines. The paper's contributions include the proposal of the NIE model, the creation of new datasets for navigation-based tasks with dynamic object interactions, and highlighting the importance of predicting action outcomes for embodied agents.