Realistic and articulated human simulation plays a crucial role in various applications such as video games, movies, AR/VR experiences, sports, social media, and robotic system testing. However, the traditional manual process of human reconstruction and animation is time-consuming and not cost-effective for large-scale implementation. Automating this process is challenging due to variations in pedestrian shape, pose, clothing, and accoutrement. Existing methods often rely on separate modules for joint estimation, shape reconstruction, texture reconstruction, and mesh association, requiring expensive equipment or controlled environments.In this paper, we propose a scalable solution for reconstructing and animating 3D humans in the wild using sensor data captured in urban cities. However, this setting presents challenges as in-the-wild data lacks ground-truth 3D shape and pose, and the captured data can be noisy, low-resolution, and under non-canonical views and poses. We address these challenges by developing a novel approach that jointly predicts 3D mesh, skeleton joints, and skinning weights using a single network. Inspired by implicit modeling and neural radiance fields, we represent a 3D human as a continuous multi-dimensional neural field, allowing for flexibility in capturing fine details, handling different surface topologies, and adapting to unseen human shapes. Our end-to-end architecture overcomes the issue of error propagation in conventional pipelines.We evaluate the effectiveness of our approach using photorealistic synthetic human 3D data and a large-scale real-world self-driving dataset. Our method outperforms state-of-the-art methods in terms of shape reconstruction quality, and we demonstrate reliable re-animation of the reconstructed 3D human in novel poses. Overall, our approach provides an efficient and effective solution for automating the reconstruction and animation of 3D humans at scale, opening up possibilities for various applications.