Pre-training and fine-tuning are widely used methods in deep learning-based object recognition in computer vision. Pre-training involves training neural network weights on a large dataset, such as ImageNet, and then transferring those weights to initialize models in downstream tasks. This pre-training can improve downstream tasks by providing well-learned low-level features and meaningful semantic representations.However, the empirical gain brought by pre-training for object detection diminishes with larger pre-training datasets. In addition, training from random initialization can achieve similar results with sufficient data and training time, raising questions about the effectiveness of pre-training.We argue that the diminishing gain is due to several mismatches between pre-training and fine-tuning tasks. Firstly, the objectives of classification and detection differ, leading to differences in loss functions and sensitivity to object locations and scales. Secondly, the data distributions for pre-training and fine-tuning are misaligned, as localization information required in detection is not explicitly made available in pre-training. Finally, the architectures used in pre-training and detection have different components, which are not pre-trained and are randomly initialized during fine-tuning.To address these mismatches, we propose a Detection-Aware Pre-training (DAP) procedure. DAP starts with pre-training a classifier on classification data and extracts localization information using Weakly Supervised Object Localization (WSOL) tools. This information is then used to pre-train a detection model, with the pre-trained weights used for model initialization in downstream detection tasks. This enables pre-training of the entire detector architecture and explicitly performs localization.Our experiments demonstrate that adding the DAP steps between classification pre-training and fine-tuning leads to consistent gains in various downstream detection tasks, especially in low-data scenarios. In the full-data setting, DAP also achieves faster convergence and improved detection accuracy. This work highlights the potential benefits of a detection-specific pre-training strategy with weak supervision, offering a first attempt towards detection-aware pre-training.