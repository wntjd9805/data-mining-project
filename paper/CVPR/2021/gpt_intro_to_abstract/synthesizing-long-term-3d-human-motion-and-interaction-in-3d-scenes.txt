Capturing and synthesizing realistic human motion in 3D scenes is crucial for applications such as virtual reality, video game animations, and human-robot interactions. However, existing works either focus on generating realistic motion or modeling human-scene interactions, but rarely address both simultaneously. In this paper, we propose a hierarchical learning framework that combines motion synthesis and affordance learning. We introduce a two-level approach where we generate sub-goals between the start and end positions in the scene, predict the human poses for each sub-goal using a Conditional Variational Autoencoder (CVAE), and then synthesize short-term human motion between every two sub-goals using a bi-directional LSTM. We model human-scene interaction using the differentiable SMPL-X model, which allows for flexible geometry constraints and realistic contact modeling. We also incorporate explicit geometry reasoning between the human mesh and 3D scene point clouds through optimization. Experimental results on PROX and MP3D 3D environments demonstrate the effectiveness of our approach in generating realistic and physically plausible human motion. Our contributions include a hierarchical learning framework, an optimization process to improve synthesized human poses, and state-of-the-art motion synthesis results.