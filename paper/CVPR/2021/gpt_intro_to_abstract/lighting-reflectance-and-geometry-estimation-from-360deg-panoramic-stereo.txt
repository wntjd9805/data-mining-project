In computer vision, the decomposition of scene properties, such as lighting, geometry, and reflectance, is crucial for various applications, including augmented reality. Traditional methods using perspective cameras have limitations in capturing the entire scene, making the problem of scene property estimation more challenging. To address this, we propose a method that utilizes a pair of 360-degree images under equirectangular projection as input. This approach offers advantages such as adequate observation for lighting estimation and the natural encoding of depth information for geometry estimation. By leveraging the physical constraints between lighting and geometry, we can reveal the reflectance of the scene. Our method achieves spatially-varying and 3D coherent lighting estimation, as well as high-definition and mirror-like reï¬‚ectance effects. In contrast to perspective methods, our approach overcomes the lack of consistency between different locations and the difficulties in inferring unseen regions of the scene. Through experiments, we demonstrate the effectiveness of our method in jointly estimating lighting, reflectance, and geometry using the 360-degree input.