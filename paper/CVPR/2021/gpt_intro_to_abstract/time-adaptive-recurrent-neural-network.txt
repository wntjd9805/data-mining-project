This paper focuses on the trainability of vanilla Recurrent Neural Networks (RNNs), which are widely used in IoT applications due to their light memory footprint. A challenge with vanilla RNNs is the gradient decay/explosion during training, leading to poor generalization for processes with long-term dependencies. Previous works have proposed various solutions, including matrix designs and novel architectures, as well as techniques inspired by ordinary differential equations (ODEs). This paper introduces a novel approach that draws upon ODEs to improve vanilla RNN trainability in two key ways. First, the time constant Î» is allowed to be time-varying and a function of the previous hidden state and input. This allows for adaptive "pondering" of each new input, selectively adapting the amount of processing required. Second, the current input is augmented with the hidden state, resulting in a composite input that improves partial gradient properties for the hidden state sequence.The proposed method, called time-adaptive RNN (TARNN), offers several advantages. It eliminates gradient decay and explosion, making it suitable for tasks with long-term dependencies. It also enhances the contribution from informative inputs while suppressing noisy segments. The paper includes experiments on benchmark datasets, demonstrating improved performance compared to standard LSTM and other recent works.The TARNN model is implemented as an RNN cell that can be easily deployed in any deep learning library. The paper provides a simple implementation code for reference. Further experiments show the robustness of TARNN to time-series distortions such as noise paddings.Overall, this paper introduces a novel approach to improve the trainability of vanilla RNNs, offering better performance in handling long-term dependency tasks with a lighter memory footprint. The proposed method has potential applications in IoT tasks.