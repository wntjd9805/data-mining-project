Deep neural networks have demonstrated exceptional performance in various tasks, such as image classification and object detection. However, these networks are susceptible to adversarial attacks. Even imperceptible perturbations can lead to incorrect predictions. Madry et al. developed the concept of adversarial training as a robust optimization problem, using projected gradient descent to find the worst-case adversarial example during training. While these networks achieve empirical robustness against many attacks, they lack verifiable robustness, making them vulnerable to stronger attacks. This has led to the development of robustness verification, which aims to provide a certificate that a neural network consistently predicts for all inputs in a given set, typically an lp ball around a clean image. Various methods, such as Satisfiability Modulo Theory and Mixed Integer Linear Programming, have been proposed to compute exact or relaxed bounds of the output logits for a given input range. However, these programming-based approaches are computationally expensive and not suitable for large networks. An alternative approach is to make linear relaxations of the nonlinear activation functions in a network. One such method, CROWN, can compute relatively tight bounds efficiently. Wong et al. proposed incorporating bounds computed by linear relaxation-based methods into the loss function to train verifiably robust networks. However, these methods introduce heavy computational overhead. Interval Bound Propagation (IBP) was introduced as a fast and scalable technique to compute bounds, despite being less accurate than linear relaxation methods. Zhang et al. improved this technique by combining IBP with CROWN, resulting in CROWN-IBP. However, CROWN-IBP trained networks still exhibit loose bounds. This paper presents a relaxed version of CROWN, called linear bound propagation (LBP), which offers better scalability and tighter bounds than IBP, whether applied to normally trained networks or IBP trained networks. The authors prove that CROWN and LBP are consistently tighter than IBP when adopting a specific tight strategy for choosing bounding lines. Additionally, the paper introduces a new activation function, parametrized ramp function (ParamRamp), which promotes the training of verifiably robust networks. Networks with ParamRamp activation achieve state-of-the-art verifiability against lâˆž robustness on various datasets such as MNIST, CIFAR-10, and Tiny-ImageNet. Overall, this paper contributes novel methods for improving the verifiable robustness of deep neural networks, offering tighter bounds and more diverse neuron status during training.