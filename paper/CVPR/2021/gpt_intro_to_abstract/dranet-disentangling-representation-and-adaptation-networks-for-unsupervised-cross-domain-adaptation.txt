The use of deep neural networks (DNN) has shown significant improvements in computer vision, machine learning, and natural language processing. However, domain gaps between data remain a problem, leading to degraded model performance. Unsupervised domain adaptation has been explored to overcome this challenge by aligning the distribution shift in labeled source data with unlabeled target data. Feature-level methods aim to learn features that combine task discrimination and domain invariance, while pixel-level approaches perform distribution alignment using Generative Adversarial Networks (GANs). Some studies incorporate both pixel-level and feature-level approaches. Disentangled representations have also been shown to improve domain adaptation performance, but they require multiple encoders and generators specialized in individual domains. In this paper, we propose DRANet, a single feed-forward network that does not require ground-truth task labels for cross-domain adaptation. Our approach focuses on extracting domain-specific features and disentangles them into content and style components using a non-linear separator. DRANet is the first approach to solely utilize individual domain characteristics for unsupervised cross-domain adaptation. We demonstrate the effectiveness of our approach on various tasks, achieving state-of-the-art performance.