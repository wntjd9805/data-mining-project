Deep video models commonly rely on spatio-temporal convolutional networks (3D CNNs) or recurrent networks trained with backpropagation (BP) using stochastic gradient descent (SGD). However, this training paradigm is expensive as it requires storing all activations in memory to compute Jacobian tensors for gradient calculations. As a result, video models are typically trained on short video clips. Sideways is a training technique that decouples computation along the depth of the network and introduces computation steps. It allows each layer to independently update the internal state of the network as new frames are fed in. This approach is more biologically plausible and respects the arrow of time. However, it lacks the ability to integrate information temporally, limiting the expressive power of the resulting models.In this work, we propose Skip-Sideways, a modification of Sideways that enables models to process one frame at a time while still extracting temporal features. We introduce shortcut connections in addition to direct connections between layers, allowing activations and gradients to flow forward in time. This enables the extraction of temporal features and leads to more stable training and higher accuracy.Regular skip connections allow activations to skip layers, creating data shortcuts. In Skip-Sideways, these shortcut connections also send activations and gradients forward in time, creating data paths across time. This expands the modeling space by incorporating spatio-temporal models and offers a new perspective on shortcut connections.To validate our approach, we train a traditional image architecture (e.g., VGG) on action recognition datasets, including the large-scale Kinetics-600 dataset, using Skip-Sideways units. These units can process data in a depth-asynchronous fashion, maximizing parallelism and reducing latency. Skip-Sideways is the first alternative to backpropagation that successfully trains video models at scale while being more biologically plausible.