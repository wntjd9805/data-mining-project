This paper introduces the concept of using cross-modal learning from images and sound to improve perception models in a self-supervised manner. The authors propose a novel Multi-Modal Distillation Network (MM-DistillNet) that combines multiple teacher networks trained on different modalities (RGB, depth, thermal) to predict bounding boxes. An audio student network is then trained to learn the mapping of sounds to the combined teachers' predictions. The proposed network shows promising results in detecting and tracking objects in the visual frame using only sound as input, without relying on metadata or manual annotations. A large-scale driving dataset is collected to evaluate the performance of the MM-DistillNet, which outperforms existing methods. The paper also presents ablation studies to highlight the contributions of the proposed framework. The dataset, code, and models are made publicly available.