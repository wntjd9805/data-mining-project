Large-scale data with sufficient annotations are crucial for machine learning, but real-world data is often unlabeled and manual annotations are costly. Transfer learning techniques have been proposed to address the lack of labeled data, particularly in domain adaptation scenarios. The most common assumption in Unsupervised Domain Adaptation (UDA) is that the labeled source domain and unlabeled target domain share the same feature spaces but have different marginal distributions. Many methods have been proposed to mitigate the discrepancy between the feature distributions of the two domains, such as explicit discrepancy minimization, domain invariant feature learning, Optimal Transport-based feature matching, and adversarial domain adaptation. However, these methods may overlook discriminant information in the label distributions. Recent advancements show that adaptation models can be more discriminative on the target domain by carefully exploring target label information. In addition to the marginal shift assumption, the conditional shift problem has been studied to build a conditional invariant model. Various methods, such as multi-layer feature approximation, conditional variants of Maximum Mean Discrepancy, conditional invariant learning with causal interpretations, and Optimal Transport-based joint distribution models, have been proposed for conditional/joint distribution matching in domain adaptation. In this paper, we propose a novel metric, called Conditional Kernel Bures (CKB), to estimate the transport cost in Reproducing Kernel Hilbert Space for continuous conditional distributions. Inspired by previous work on the conditional covariance operator, we show that the CKB metric reflects the discrepancy between conditional distributions directly. We provide an explicit empirical estimation of the CKB metric and present its consistency theory. We also propose a conditional distribution matching network based on the CKB metric for discriminative domain alignment and extend it to a joint distribution matching variant. Experimental results demonstrate the effectiveness of the CKB metric and the superiority of our proposed model. Our contributions include the introduction of the CKB metric, a computable measurement for conditional domain discrepancy, and the development of a conditional distribution matching network for discriminative domain alignment.