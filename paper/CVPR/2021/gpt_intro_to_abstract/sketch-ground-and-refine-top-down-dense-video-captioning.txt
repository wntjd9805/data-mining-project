Video understanding is a crucial research area in computer vision, with recent advancements in deep learning and large-scale datasets leading to promising results in action recognition and temporal action localization. However, existing video understanding approaches have limitations in providing detailed information about the video content. To address this, the task of Dense Video Captioning (DVC) aims to discover key events in a video and describe them using a coherent story. Most current DVC methods follow a "detect-then-describe" framework that produces redundant and incoherent captions. In this paper, we propose a top-down approach called Sketch, Ground, and Refine (SGR) for DVC. In the Sketch stage, a coherent video-level paragraph is generated to describe the video globally. The Grounding module localizes the sentences (events) in the video-level paragraph. The Refine stage focuses on fine-grained details of the event captions using a Dual-Path Cross Attention module and a refinement-enhanced training scheme. Our SGR approach outperforms existing DVC frameworks in terms of performance on benchmark datasets. This top-down approach provides more detailed and coherent video captions, benefiting applications such as content-based video retrieval and recommendation.