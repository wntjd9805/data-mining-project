Many intelligent technologies in computer science rely on computational capacity and deep neural networks for high reliability. However, training these models requires massive amounts of data to account for various practical scenarios. Domain shift, which occurs when the test data and training data follow different distributions, poses a challenge for effective domain adaptation techniques. While deep domain adaptation methods have been successful, they often do not consider the transferability of different target samples and may not be suitable for real-world situations with stringent computational resource constraints. To address this, the paper proposes a framework called Dynamic Domain Adaptation (DDA) that balances transferable performance and computational cost for target inference. The framework utilizes a multi-exist adaptive architecture and applies domain confusion constraints to reduce distribution discrepancy. It then leverages pseudo-labeled target data to retrain the network and improve target prediction performance. The proposed method aims to handle domain discrepancy without losing accuracy on resource-constrained devices. Experimental results show that the method saves time and computational resources while maintaining promising cross-domain recognition accuracy in different prediction settings. Overall, this paper introduces a novel framework and strategies that contribute to the efficient inference of domain adaptation in computer science.