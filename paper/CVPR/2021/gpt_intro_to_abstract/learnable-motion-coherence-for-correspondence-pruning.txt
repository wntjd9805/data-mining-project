In computer vision tasks like Structure-from-Motion, visual localization, image stitching, and visual SLAM, estimating correspondences between images is crucial. Most correspondence estimation pipelines rely on local feature matching to establish putative correspondences, but these often include numerous false correspondences. To address this, correspondence pruning algorithms like RANSAC and its variants are used to select a reliable subset of true correspondences. However, due to large outlier ratios, identifying the correct geometric model can be challenging. In addition to task-specific geometric models, true correspondences also adhere to a general motion model called motion coherence. Previous works have proposed handcrafted rules to identify motion coherence, but modeling it on sparse correspondences generated by local descriptors is difficult. Sparse correspondences are discrete and unevenly distributed, making it hard to estimate the underlying motion field and find a uniform coherence constraint. Moreover, complex structures and motion patterns can further complicate this task. To address these challenges, we propose a neural network called LMCNet to learn motion coherence for correspondence pruning. LMCNet captures global motion coherence using our novel formulation called Laplacian Motion Fitting (LMF) and captures local motion coherence using a Local Coherence Layer (LC-Layer). By integrating these layers, LMCNet can robustly predict the probability of each correspondence being an inlier. We conducted experiments on relative camera pose estimation and correspondence pruning of dynamic scenes, and LMCNet outperformed other baseline methods, demonstrating its effectiveness and potential for object tracking and video object recognition. Our contributions include the novel formulation of motion coherence, the development of differentiable layers to capture motion coherence, and the design of LMCNet for correspondence pruning.