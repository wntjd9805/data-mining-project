Human pose estimation from single images is a popular research topic in computer vision. Existing supervised deep learning solutions achieve high accuracy but rely on large amounts of annotated training data, which can be challenging to acquire for activities not present in common datasets. In this paper, we propose a novel self-supervised training procedure called CanonPose that does not require any 2D or 3D annotations in the multi-view training dataset. Our method works with uncalibrated cameras and only requires at least two temporally synchronized cameras observing a person from different angles. Unlike other approaches, our method does not rely on sparse 3D annotations, unpaired 3D data, or known camera positions. Instead, we mix the outputs of multiple weight-sharing neural networks to estimate a person's 3D pose. Our approach consists of two stages: predicting the 2D pose from an image using a pre-trained network and lifting these 2D detections to a 3D pose represented in a learned canonical coordinate system. We evaluate our method on benchmark datasets and achieve state-of-the-art results in self-supervised 3D pose estimation. Our approach does not assume camera calibration or static cameras, making it highly practical. Additionally, we showcase results on the challenging SkiPose dataset, which captures outdoor scenes with fast motions and pan-tilt-zoom cameras. The code for our method is available on GitHub. In summary, our contributions include the CanonPose approach, which enables self-supervised single image 3D pose estimation from unlabeled multi-view images without prior knowledge of the scene or camera calibration. We also demonstrate the integration of 2D joint estimator confidences into the training pipeline.