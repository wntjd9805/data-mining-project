Person re-identification (reID) is the task of identifying a specific person across multiple cameras. With the availability of large video datasets and increased computational resources, video person reID has gained attention. Videos contain rich spatial and temporal cues that can improve reID performance. Existing methods, however, do not fully utilize these clues. Spatially, most methods employ the same operation on each frame, resulting in redundant features that focus on the same local region. Temporally, current methods either model short-term or long-term temporal relations, but fail to capture both effectively. To address these limitations, we propose an efficient spatial-temporal representation for video reID. We introduce a Bilateral Complementary Network (BiCnet) that extracts complementary spatial features across consecutive frames. BiCnet includes two branches, Detail Branch and Context Branch, which operate at different resolutions to capture spatial details and long-range contexts, respectively. Multiple parallel spatial attention modules are appended to each branch to ensure diversity and focus on different regions. The complementary features from the two branches are aggregated to form a comprehensive spatial representation. Additionally, we develop a Temporal Kernel Selection (TKS) block that adaptively models short and long-term temporal relations using multiple parallel convolution paths with varying kernel sizes. TKS selects the dominant temporal scale based on global information and can vary the scale of temporal modeling depending on input video properties. Our approach, called BiCnet-TKS, outperforms state-of-the-art methods on multiple benchmarks and reduces computation cost by approximately 50% through downsampling.