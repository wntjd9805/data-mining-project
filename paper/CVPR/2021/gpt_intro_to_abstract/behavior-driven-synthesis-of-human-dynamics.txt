Understanding human appearance, posture, and behavior is crucial in the field of computer vision and has numerous applications such as autonomous driving, surveillance, and medical treatment. While there has been significant progress in the representation and synthesis of posture and appearance, the understanding and synthesis of behavior remain open problems.Human motor behavior is determined by the dynamics of our limbs and body. Certain behaviors can be performed independently of a particular initial body configuration, while others require specific starting postures. To synthesize complex behaviors from arbitrary starting poses, a transition to fitting initial body configurations may be required. Specific body features do not affect the ability to perform certain behaviors.Representation of behavior as a sequence of individual postures is suboptimal. We want the overall behavior to be represented as the same, regardless of the initial posture. The current work on human motion synthesis represents behavior directly through observed posture sequences, limiting the ability to control and synthesize behavior. Controlling such sequences, or re-enacting a novel behavior, requires a posture-independent representation capturing only the behavior dynamics to be transferred.In this paper, we propose a conditional variational generative model for controlled human behavior synthesis. Our model learns to understand the characteristic motor dynamics of behavior and transfers behavior between videos without requiring class labels. We learn a dedicated representation extracting behavior dynamics while disentangling posture information. To achieve this, we propose an explicit disentanglement framework based on an alternating optimization procedure.Our experiments show that our model effectively transfers behavior between sequences and is capable of sampling novel and diverse behavior. Quantitative comparison against existing approaches for human motion synthesis demonstrates the competitive performance of our approach.