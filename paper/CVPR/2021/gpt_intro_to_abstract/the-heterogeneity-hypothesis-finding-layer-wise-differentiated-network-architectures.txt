Abstract:With the rise of deep learning, convolutional neural networks (CNNs) have taken over feature design in various computer vision tasks. Neural network design has evolved from manual design to neural architecture search (NAS) and semi-automation. Current state-of-the-art network designs focus on discovering the overall network architecture with regularly repeated convolutional layers. However, the channel configuration of each layer is often overlooked and not fully explored. This paper aims to investigate this design space and answer three questions: 1) if there exists a layer-wise differentiated network architecture (LW-DNA) that can outperform the original one, 2) how to efficiently identify it, and 3) why it can beat the regular configuration. To answer the first question, the Heterogeneity Hypothesis is introduced, stating that for a CNN trained under the same protocol, there exists an LW-DNA that can outperform the original network with lower model complexity. Empirical experiments are conducted to validate this hypothesis. The paper also explores the relationship between LW-DNA models and overfitting, observing that LW-DNA models show improved generalization. Contributions of this paper include the possibility of identifying a superior network version by adjusting the channel configuration, a method for identifying LW-DNA models with minimal additional computational cost, and an explanation for the improved performance of LW-DNA models.