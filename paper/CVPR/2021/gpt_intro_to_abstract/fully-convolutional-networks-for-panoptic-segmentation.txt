Panoptic segmentation, which assigns semantic labels and unique identities to each pixel, is a challenging task in computer vision. One of the main difficulties in achieving a unified representation for countable instances (things) and uncountable instances (stuff) arises from the conflicting properties required by each. Countable instances rely on instance-aware features to distinguish among different identities, while uncountable instances prefer semantically consistent characteristics to ensure consistent predictions for pixels with the same semantic meaning. This paper proposes a fully convolutional framework called Panoptic FCN, which encodes each instance into a specific kernel and generates predictions through convolutions. This allows for the prediction of both things and stuff with the same resolution, satisfying the instance-aware and semantically consistent properties in a unified workflow. The proposed method utilizes a kernel generator and feature encoder to generate kernel weights and encode shared features. Experimental results demonstrate the superiority of Panoptic FCN in panoptic segmentation, outperforming previous methods in terms of efficiency and achieving leading performance on benchmark datasets.