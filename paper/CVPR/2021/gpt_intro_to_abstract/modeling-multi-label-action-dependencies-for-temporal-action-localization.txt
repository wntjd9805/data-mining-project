Action localization in complex video sequences is a well-researched problem in computer vision. Existing methods either propose top-down or bottom-up approaches to predict the action(s) present at each time-step of a video sequence. While bottom-up methods perform well for multi-label cases, they do not explicitly model the relationships between different action labels, hindering their ability to determine the presence or absence of classes within a video. This paper introduces a novel network architecture that leverages both co-occurrence and temporal action dependencies to improve feature representations for multi-label temporal action detection. The proposed approach incorporates attention-based layers to refine class-level features based on these dependencies, allowing for improved interpretability of the model. The paper also presents new metrics for evaluating temporal action localization methods that explicitly measure how well pair-wise class/action dependencies are modeled within and across time-steps. Experimental evaluations on two large-scale publicly available multi-label action datasets demonstrate that the proposed approach outperforms existing state-of-the-art methods.