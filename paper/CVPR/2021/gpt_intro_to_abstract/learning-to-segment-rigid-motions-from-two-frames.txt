Autonomous agents, such as self-driving cars, face the challenge of safely navigating in dynamic environments. While static environments can be processed easily using geometric constraints, dynamic environments require the ability to segment moving obstacles and estimate their depth and speed. Existing solutions, such as object detection or semantic segmentation, have limitations when it comes to accurately detecting moving objects. This poses safety implications for truly autonomous agents operating in open-world environments.In this paper, we address the problem of motion segmentation in dynamic environments. We focus on segmenting rigid bodies from two frames, as this is the minimal set of inputs needed to study the problem and respond immediately to dynamic scenes. Although dynamic scenes may contain nonrigid objects, we expect that they can be modeled as rigid bodies over short time scales.Geometric constraints, such as epipolar geometry, have been used in previous work on rigid motion segmentation. However, these constraints have limitations when camera motion is close to zero or when points moving along epipolar lines cannot be distinguished from the rigid background. Additionally, geometric criteria are often not robust to noisy motion correspondences and camera egomotion estimates, leading to failures in practice.To address these challenges, we propose a method that analyzes ambiguities in 3D rigid motion segmentation and resolves them using upgraded 2D motion observations to 3D with optical expansion and monocular depth cues. We also design a convolutional architecture that can segment the rigid background and multiple rigid bodies from a given motion field, while dealing with noisy motion correspondences and degenerate scene motion. The 3D motion of individual rigid bodies is parameterized using 3D rigid transformations.The contributions of this paper are threefold. First, we provide a geometric analysis of ambiguities in 3D rigid motion segmentation and propose solutions to address them. Second, we introduce a geometry-aware architecture for 3D rigid motion segmentation from two RGB frames, which is resilient to different motion types and robust to noisy motion observations. Finally, our method achieves state-of-the-art performance in rigid motion segmentation, improving downstream tasks such as depth and scene flow estimation.Overall, our approach aims to enhance the ability of autonomous agents to navigate safely in dynamic environments by accurately segmenting moving objects and estimating their motion in a 3D world.