Generating audio-driven photo-realistic portrait videos is crucial for multimedia applications such as film-making, telepresence, and digital human animation. Previous techniques have focused on animating talking heads or portraits with synchronized lip movements, but have not adequately addressed the modeling of emotion. This paper introduces Emotional Video Portraits (EVP), a novel algorithm that enables emotion control in video-based editing of talking faces. The algorithm incorporates two key components: Cross-Reconstructed Emotion Disentanglement, which extracts emotion information from audio and allows for emotion control in generated portraits, and Target-Adaptive Face Synthesis, which bridges the gap between the generated face and the target video in terms of head pose and movement. To evaluate the effectiveness of EVP, extensive experiments were conducted, and the results demonstrate superior performance and the effectiveness of the proposed techniques. The contributions of this work are the EVP system itself, the introduction of Cross-Reconstructed Emotion Disentanglement technique, and the introduction of Target-Adaptive Face Synthesis. Overall, this paper presents an innovative approach to achieving emotional control in video-based editing of talking face generation methods.