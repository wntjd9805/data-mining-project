Visual object tracking is a challenging task in computer vision, primarily due to challenges such as occlusion, deformation, and appearance changes. The rich temporal information in video flow plays a crucial role in visual tracking, but most existing tracking paradigms overlook the temporal relationships among successive frames. This paper introduces the transformer architecture to the visual tracking community to bridge isolated video frames and convey rich temporal cues across them. By leveraging the attention mechanism of the transformer, this work establishes pixel-wise correspondence across frames and freely conveys various signals in the temporal domain. The proposed transformer-assisted tracking framework consists of a transformer encoder and decoder that are integrated into a Siamese-like structure. The transformer encoder mutually reinforces individual template features, while the decoder propagates features and spatial masks, enhancing representation and smoothing appearance changes. The designed transformer is integrated into two popular tracking frameworks and achieves outstanding results on seven prevalent tracking benchmarks, setting new state-of-the-art records. This work presents a novel and neat approach to visual tracking using the transformer architecture, considering feature and attention transformations, and demonstrates the generalization of the approach on various tracking pipelines.