Camera localization using sparse feature correspondences plays a crucial role in augmented reality, virtual reality, and robotic systems. The Perspective-n-Point (PnP) methods, particularly PnP(L), have proven effective in estimating the pose of calibrated cameras based on sparse feature correspondences. In man-made environments lacking distinctive surface textures, line features can significantly enhance localization accuracy, thus motivating the usage of PnP(L) methods. However, existing PnP approaches typically assume absolute accuracy of the features, without considering uncertainties.In this paper, we propose globally convergent PnP(L) solvers that leverage a complete set of 2D and 3D uncertainties for camera pose estimation. Our contribution involves integrating feature uncertainty into the PnP(L) methods. We extend the classical DLS and EPnP methods and introduce a modification to the standard nonlinear refinement process to account for 3D uncertainties. To evaluate the effectiveness of our proposed methods, we conducted extensive experiments on synthetic data as well as two real indoor and outdoor datasets. The results demonstrate that our new PnP(L) methods offer significantly improved accuracy compared to state-of-the-art approaches, both independently and as part of a complete pipeline. For instance, our DLSU method reduces the mean translation error on the KITTI dataset by 18%. Furthermore, the uncertain pose refinement technique enhances pose accuracy by up to 16% with a marginal increase in computational time.In a synthetic setting where there is noise in 2D feature detections, our methods achieve accuracy comparable to the most accurate 2D uncertainty-aware methods available. The code for our proposed methods is publicly available at https://alexandervakhitov.github.io/uncertain-pnp/. Through our research, we aim to advance the field of camera localization by considering both 3D and 2D uncertainties in the estimation process.