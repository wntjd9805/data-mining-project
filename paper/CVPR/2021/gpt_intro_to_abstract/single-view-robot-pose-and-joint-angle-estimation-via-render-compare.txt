The goal of this work is to recover the state of a known articulated robot within a 3D scene using a single RGB image. This is important for enabling mobile and autonomous systems to interact with other robots using visual information in non-instrumented environments. The problem is challenging due to the high degrees of freedom and infinite space of configurations that result in self-occlusions and depth ambiguities. Current methods use deep neural networks to localize keypoints and solve 2D-to-3D optimization problems. However, recent render & compare methods have outperformed 2D keypoints methods for rigid objects. In this paper, we investigate extending the render & compare paradigm for articulated objects. We introduce a new render & compare approach that can be trained from synthetic data, generalizes to unseen robot configurations, and can be applied to a variety of robots. We also experiment with different robot pose parameterizations and demonstrate the importance of choosing an effective parametrization strategy. We apply the proposed method in two settings: with known joint angles, predicting only the camera-to-robot 6D pose, and with unknown joint angles, predicting both the joint angles and the camera-to-robot 6D pose. Experimental results on benchmark datasets show significant improvements compared to the state of the art.