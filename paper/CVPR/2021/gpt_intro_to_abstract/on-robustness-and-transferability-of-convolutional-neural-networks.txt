Deep convolutional networks have achieved impressive results in visual classification benchmarks, but the real-world deployment conditions often differ from the training conditions. Understanding the impact of dataset shifts on the performance of these models is crucial. This problem, known as out-of-distribution (OOD) generalization, is closely related to transfer learning. In transfer learning, models improve their performance on a target task by leveraging data from related problems. However, under dataset shift, the model cannot be adapted using data from the target environment. The interplay between in-distribution performance, OOD performance, and transfer learning performance in the presence of dataset shifts is not well-explored. In this paper, we systematically investigate this interplay, focusing on the accuracy of image classification models on the training distribution, their generalization to OOD data without adaptation, and their transfer learning performance with adaptation in the low-data regime. We analyze the effects of model and data scale, training set size, training regime, and testing resolution on OOD robustness. Additionally, we introduce a new dataset for fine-grained OOD analysis, quantifying robustness to object size, location, and orientation. Our findings show that increasing model and data scale disproportionately improves transfer and OOD performance, while only marginally improving performance on the validation set. Scale is found to have the strongest effect on OOD robustness. We also demonstrate that models become less sensitive to variations in object size, location, and orientation as dataset size and model size increase. This paper presents a systematic study of the relationship between in-distribution, OOD, and transfer performance in the presence of dataset shifts, providing insights into the factors affecting OOD generalization.