In this paper, we present a video prediction architecture that aims to improve the future prediction capabilities of autonomous agents. We propose decomposing the scene into independent entities with their own attributes, as we believe this approach can benefit prediction tasks such as navigation and manipulation. By modeling the different dynamics of semantically consistent regions in the scene, our architecture explicitly captures the distinctive temporal changes induced by different objects. We describe our model, called the semantic-aware dynamic model (SADM), which decomposes the video into regions corresponding to different semantic classes. These regions are represented by binary semantic masks, making their evolution simpler and easier to learn compared to the motion of the entire video frames. We predict each region and then fuse the predictions with their content to generate future semantic maps and flow fields. The prediction in co-visible regions of future frames is warped from the past, with dis-occlusion detection mediated by the predicted semantic maps. Additionally, dis-occluded regions are filled-in using a generative model or conditional renderer, which is trained with both the warped images and predicted semantic maps to enable structured and semantically-aware synthesis. By explicitly modeling dis-occlusions, our model avoids the complexity of learning this phenomenon. We incorporate semantic segmentation, optical flow, and synthesis into a complete generative model for videos, allowing for semantically and geometrically consistent prediction of complete video frames. SADM achieves state-of-the-art performance on video prediction benchmarks.