This paper addresses the problem of online appearance reconstruction from RGB-D images in the context of 3D model reconstruction. Traditional approaches to appearance mapping rely on batch-based optimization methods, which are not suitable for applications that require online processing and do not have access to the entire dataset at processing time. Existing online fusion methods have limitations in encoding high-frequency appearance details on the surface and handling topology changes in an online reconstruction setting. Learning-based approaches have achieved high-quality results but are not well-suited for local online updates and do not scale to large-scale scenes.To overcome these limitations, the authors propose a novel scene representation called DeepSurfels and an efficient learning-based online appearance fusion pipeline. DeepSurfels combine the advantages of implicit grids and surfel representation, encoding both topology and high-frequency geometry and appearance information. The proposed pipeline iteratively fuses RGB-D frames into estimated DeepSurfels geometry and is optimized using a differentiable renderer and the reprojection error.The authors demonstrate the effectiveness of the proposed method by comparing it to existing methods on single and multi-object datasets. The results show that the DeepSurfels representation better captures high-frequency textures and that the method generalizes well to new scenes without retraining. The contributions of this paper include the DeepSurfels scene representation, the online appearance fusion pipeline, and the ability to generalize to new scenes without retraining. Overall, this work is a step towards a fully end-to-end appearance fusion method that can be deployed in real-world applications.