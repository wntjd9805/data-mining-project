Fast and lightweight methods for animating 3D characters are highly desirable in computer games and film visual effects. Traditional skinning-based mesh deformation offers a fast geometric approach but lacks realistic dynamics. Physically-based simulation can add visually realistic and vivid effects to skinned animations but requires heavy computation. Recent research has explored deep learning methods to approximate physically-based simulation in a more time-efficient manner. However, these methods have limitations, such as assuming a fixed mesh topology and operating on reduced subspaces, leading to low accuracy. In this paper, we propose a deep learning approach to predict the secondary motion of skinned animations of 3D characters. Our method addresses the limitations of previous learning-based approaches by designing a network architecture that reflects the underlying physical process. Our network models the simulation using a volumetric mesh composed of uniform tetrahedra surrounding the character mesh. By operating on local patches of the volumetric mesh, our method avoids the computational complexity of encoding high-resolution character meshes and can be applied to any character mesh, regardless of its topology. Furthermore, our network encodes per-vertex material properties and constraints, providing the user with the ability to control the dynamic behavior of different parts of the mesh. We demonstrate that our model has generalization capability, as it does not require a massive training dataset of complex meshes and motions. Instead, we construct our training data from primitive geometries like a volumetric mesh of a sphere, allowing our network to generate visually plausible secondary motions on complex 3D characters during testing. We improve the network's online predictions in unseen scenarios by assigning randomized motions to the primitives during training, broadening the motion space covered by local patches. We evaluate our method on various character meshes and complex motion sequences. Our results demonstrate visually plausible and stable secondary motion while being more than 30 times faster than the commonly used implicit Euler method in physically-based simulation. We compare our method to faster approaches like the explicit central differences method and other graph convolutional network-based learning approaches, and show that our method outperforms them in terms of accuracy and robustness.