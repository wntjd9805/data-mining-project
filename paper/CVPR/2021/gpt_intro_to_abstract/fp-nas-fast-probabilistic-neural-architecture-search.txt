Designing efficient architectures for visual recognition is a complex task that requires extensive exploration in the search space. Manual exploration is time-consuming and computationally expensive. To automate the architecture design process, Neural Architecture Search (NAS) has been introduced. However, early approaches based on evolution or reinforcement learning require significant computational resources.Recently, differentiable neural architecture search (DNAS) has relaxed the discrete representation of architectures to a continuous space, enabling more efficient search with gradient descent. However, DNAS has limitations in terms of memory footprint and computational cost.In this paper, we propose two novel techniques to accelerate the probabilistic version of DNAS called PARSEC. First, we introduce dynamic sampling that adapts to the entropy of the architecture distribution, encouraging exploration in the early stage and focusing on promising architectures later. We also propose a coarse-to-fine search strategy in the multi-variate space, using a factorized distribution representation for faster search in the beginning stage and converting it into the joint distribution for fine-grained search.Experimental results in the FBNetV2 space show that our method, called FP-NAS, samples 64% fewer architectures and searches 2.1 times faster compared to PARSEC. When compared to FBNetV2, FP-NAS is 3.5 times faster and achieves better accuracy-to-complexity trade-offs. In the search for large models, FP-NAS is 132 times faster than EfÔ¨ÅcientNet-B0 while discovering a model with 0.7% higher accuracy. We also expand the FBNetV2 search space by replacing the Squeeze-Excite module with a searchable Split-Attention module, showing the superiority of searched SA modules.To summarize, our contributions include an adaptive sampling method for fast probabilistic NAS, a coarse-to-fine search method using factorized distribution representation, the discovery of superior small and large models with better accuracy-to-complexity trade-offs, and the expansion of the FBNetV2 search space with improved modules.