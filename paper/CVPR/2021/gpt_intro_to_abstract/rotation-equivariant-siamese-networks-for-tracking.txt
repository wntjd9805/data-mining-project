Siamese trackers have become popular in the field of visual object tracking due to their strong discriminative power obtained from similarity matching. However, these trackers are still susceptible to failure under challenges such as partial occlusion, scale change, or object rotation. Object rotation, in particular, is a difficult challenge with no effective solution to date. This paper focuses on addressing the adverse effects of in-plane rotation on Siamese trackers. It is commonly observed in real-life scenarios, such as drones recording from the top or ego-centric videos with large head rotations. The existing Siamese tracker architectures are not inherently equivariant to in-plane rotations, leading to failure in accurately matching the template and candidate images. A straightforward approach to handle object rotation is through data augmentation, but this has limitations in terms of separate representations for different rotated variants and increases the computational budget. This paper introduces rotation equivariance in Siamese trackers by incorporating group-equivariant CNNs and steerable filters. This allows the trackers to capture rotation variations without additional data augmentation. The proposed approach, called RE-SiamNet, increases the discriminative power of the trackers for differences in orientation and can also be used for 2D pose estimation of objects in videos. A Rotating Object Benchmark (ROB) dataset is presented for benchmarking the performance of models in the presence of in-plane rotations. The contributions of this paper include the introduction of RE-SiamNets, estimation of relative 2D pose in an unsupervised manner, the motion constraint to improve temporal correspondence, and the demonstration of significant improvements in tracking performance and orientation estimation.