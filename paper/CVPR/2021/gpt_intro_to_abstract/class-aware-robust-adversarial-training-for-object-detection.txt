Recent breakthroughs in deep learning have led to promising performance in computer vision tasks such as object recognition and detection. However, these deep learning models are susceptible to security issues, particularly adversarial attacks. Adversarial examples, which are imperceptible perturbations added to input images, are capable of fooling deep learning models and generating unexpected outputs. Many attack methods have been developed to produce various adversarial examples, exposing the vulnerability of deep learning models. Object detection, an important area in computer vision, also faces the threat of adversarial attacks. However, the development of defense algorithms to improve the robustness of object detection models is limited compared to the development of attack methods. Adversarial training is one of the most effective defense approaches, but current methods do not properly take all objects in an image into consideration. This paper focuses on the one-stage object detector, Single-Shot Object Detector (SSD), and presents a novel class-aware robust adversarial training approach. The proposed approach generates a universal adversarial perturbation to attack all objects in an image by simultaneously maximizing the losses for each object. The total loss is decomposed into class-wise losses and normalized, allowing for balanced influence and improved adversarial robustness. Fast adversarial training methods are also incorporated to accelerate training. Experimental results on PASCAL VOC and MS-COCO datasets demonstrate the effectiveness of the proposed defense methods in enhancing the robustness of object detection models. The contributions of this work include the design of efficient and effective adversarial training algorithms for object detection, particularly in scenarios with multiple objects from different classes, and the establishment of a connection between universal adversarial perturbations in image classification and object detection.