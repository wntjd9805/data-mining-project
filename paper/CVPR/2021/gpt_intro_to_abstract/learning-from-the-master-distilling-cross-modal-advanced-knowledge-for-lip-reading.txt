Lip reading, also known as visual speech recognition, is a task that aims to predict spoken words or sentences from muted lip videos. This technology has various practical applications, such as providing subtitles for silent films, assisting aphonia patients in communicating, and enhancing security systems. Early research on lip reading utilized Hand-Crafted Features and Hidden Markov Model (HMM) techniques, while recent works have utilized deep neural networks. However, lip reading models face challenges due to the inherent limitations of visual information. Certain lip shapes, such as those in the characters "p" and "b," are difficult to distinguish in video clips, but can be uniquely identified using audio information in speech recognition. In fact, the performance gap between lip reading and speech recognition can be as high as 40% in terms of word error rate (WER). One potential solution is to transfer knowledge from audio data to video data using knowledge distillation techniques. However, existing approaches using a pretrained audio teacher network only provide limited hidden knowledge to the video student. This limitation is further highlighted by experiments conducted on the LRS2-BBC dataset, where a video teacher provides better knowledge transfer to the student compared to an audio teacher. To bridge the cross-modal gap and facilitate knowledge transfer, this paper proposes a deep lip reading model that incorporates a trainable "master" network. The master network takes speech audio data and lip video data as inputs, producing probabilities based on audio, video, and their combination. To guide knowledge fusion, the framework also includes "tutor" networks pretrained on audio and video data. A dynamic fusion loss and curriculum learning strategy are employed to balance and fuse different types of knowledge, allowing the student to learn in a more effective manner. The proposed method outperforms state-of-the-art lip reading methods on three benchmarks, demonstrating its effectiveness.