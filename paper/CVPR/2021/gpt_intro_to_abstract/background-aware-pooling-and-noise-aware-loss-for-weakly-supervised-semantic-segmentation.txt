Semantic segmentation is a crucial task in computer vision with applications in scene understanding, autonomous driving, image editing, and robotics. Supervised methods based on convolutional neural networks (CNNs) have achieved significant success in semantic segmentation, but they require a large number of pixel-level labeled training samples, which are time-consuming to annotate. To address this limitation, weakly-supervised semantic segmentation (WSSS) methods have been introduced, leveraging weak supervisory signals such as image-level labels, points, scribbles, and object bounding boxes. While object bounding boxes are easier to annotate than pixel-level labels, they do not provide precise object boundaries. This paper proposes a simple yet effective WSSS method that uses bounding box annotations to generate high-quality but potentially noisy pixel-level labels (pseudo ground truth) and trains CNNs for semantic segmentation with these labels. The method leverages a CNN for image classification, using a background-aware pooling (BAP) method with attention maps to discriminate foreground and background within the bounding boxes. This allows for more accurate class activation maps (CAMs) compared to off-the-shelf saliency detectors. Additionally, a noise-aware loss (NAL) is introduced to make the CNNs less susceptible to incorrect labels. Experimental results show that the proposed approach outperforms the state of the art on the PASCAL VOC 2012 dataset and performs well on the MS-COCO dataset for instance segmentation. The contributions of this work are the introduction of BAP for WSSS, the proposal of NAL, and the achievement of a new state of the art on PASCAL VOC 2012 dataset. The code and models are made available online.