Deep learning has been highly successful in discovering complex structures in high-dimensional data, particularly in tasks such as image classification, semantic segmentation, and object detection. However, training deep neural networks (DNNs) requires reliable and clean large-scale datasets, which can be challenging and expensive to collect. To address this issue, alternative approaches like crowdsourcing and web-crawlers have been used to improve annotation efficiency, but they often introduce low-quality annotations and unreliable labels due to noise.Learning from noisy labels has become a significant task, and existing methods primarily focus on noise-cleaning-based and sample-selection-based approaches. Noise-cleaning-based methods remove samples with suspicious labels or correct their noisy labels, while sample-selection-based methods aim to identify true-labeled samples from noisy training data. However, these methods have limitations, such as removing too many instances or keeping mislabeled instances, and they lack a global perspective to explore patterns in the relationship between samples.In this paper, we propose a robust learning algorithm called DualGraph, which takes into account the structural relationships among labels at two different levels. We introduce an iterate optimization mechanism that consists of reasoning and classification phases. In the reasoning phase, we generate the similarity distribution for each sample by calculating the similarity between samples. In the classification phase, we use the distribution features to reconstruct instance graph nodes and generate reliable predictions. We train two graph neural networks with a joint loss to capture both example classifier loss and distribution loss.Our contributions include being the first to exploit graph neural networks to capture structural relations among labels at multiple levels. We propose an iterate optimization mechanism that trains two graph neural networks simultaneously to establish robust label relations. Experimental results show that our approach significantly outperforms existing methods on various benchmarks with different types and levels of label noise.Overall, our proposed approach improves learning from noisy labels by considering the structural relationships among labels and achieving state-of-the-art results on multiple datasets.