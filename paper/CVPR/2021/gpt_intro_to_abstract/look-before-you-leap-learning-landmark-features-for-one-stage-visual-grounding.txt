This paper addresses the problem of visual grounding, which involves localizing entities described by referential expressions in images. Previous methods have achieved promising results by leveraging contextual information and using a two-stage paradigm. However, these methods suffer from time complexity and may not fully exploit contextual cues in the scene. To overcome these limitations, the authors propose a one-stage visual grounding framework called LBYL-Net. This framework introduces the concepts of Landmark Features and Landmark Feature Convolution to capture rich contextual cues for better localization. The Landmark Features represent global contextual information from landmark locations in the image, while the Landmark Feature Convolution integrates this information into the grid features. The proposed framework also incorporates a feature pyramid network for object feature extraction and employs a landmark feature convolution to characterize relationships between objects. Experimental results demonstrate the effectiveness and efficiency of LBYL-Net, with state-of-the-art performance achieved on the ReferitGame dataset. The contributions of this paper include the proposed LBYL-Net framework, the landmark feature convolution method, and extensive experimental evaluations.