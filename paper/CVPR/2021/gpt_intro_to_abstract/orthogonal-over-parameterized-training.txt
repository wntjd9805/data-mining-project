The paper introduces the concept of utilizing orthogonal over-parameterization in neural network training to improve generalization performance. The authors propose a generic framework called OPT (Orthogonal Over-Parameterized Training) that combines layer-shared orthogonal transformations with fixed randomly-initialized weight vectors to train neural networks. By separating the orthogonal transformation from the weight vector, OPT provides strong modeling flexibility. The authors also explore the use of different orthogonalization algorithms and parameterizations to encode different inductive biases. Additionally, they propose a refinement strategy to reduce the hyperspherical energy of randomly initialized neuron weights before training. To improve scalability, the authors introduce Stochastic OPT, which randomly samples neuron dimensions for orthogonal transformation. The paper demonstrates the advantages of OPT, including its generic nature, provable minimization of hyperspherical energy, no extra computational cost during inference, and scalability with comparable performance. The experimental results show that OPT performs well on different neural architectures.