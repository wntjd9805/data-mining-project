Deep convolutional neural networks (CNNs) have achieved remarkable success in recent years, but their state-of-the-art performance often relies on substantial computational resources, which limits their deployment on low-compute platforms like mobile phones and Internet of Things (IoT) products. To address this issue, researchers have been designing efficient CNN architectures. One promising architecture, DenseNet, improves computational efficiency by reusing early features with dense connections. However, dense connectivity may introduce redundancies as the network deepens. To mitigate this problem, previous models like CondenseNet and ShuffleNetV2 prune less important connections or drop early features based on layer distance. In this paper, we propose a novel approach to reviving and reusing these "obsolete" early features, rather than discarding them. We introduce a new module for feature reactivation that updates shallow features, enabling them to be efficiently reused by deep layers. Unlike previous models, our module allows the outputs of a layer to be reactivated by later layers, reducing redundancy in dense connections. We also develop a sparse feature reactivation (SFR) module that selectively reactivates early features using learned increments, resulting in minimal extra computational cost. We implement SFR in CondenseNet, resulting in CondenseNetV2, and evaluate its performance on image classification benchmarks (ImageNet and CIFAR) and the COCO object detection task. The results demonstrate that SFR significantly improves performance by encouraging long-distance feature reusing and that CondenseNetV2 compares favorably with even state-of-the-art lightweight deep models. We also show that SFR can be incorporated into other CNN architectures to enhance their efficiency.