Learning pixel-level representation for visual correspondence is important for various applications. However, obtaining dense annotations for pixel-level correspondence can be expensive and challenging, especially in the presence of occlusions and non-rigid object deformations. The use of synthetically generated data can be an alternative, but it limits generalization to real scenes. To overcome these limitations, several methods have leveraged abundant unlabeled videos as a source of free supervision. These methods track points over time and learn from inconsistencies between original points and tracked ones. However, finding correspondences in unconstrained videos is challenging due to temporal discontinuities. Existing methods address this challenge by considering more matching candidates over additional adjacent frames, but this leads to an increase in ambiguous matches. Recently, a probabilistic inference approach that incorporates negative correspondences has shown promising results. However, this approach still struggles with occlusions and the composition of negative examples. In this work, we propose a novel contrastive learning approach that collects well-defined positive correspondences and well-defined negative ones during training. We measure matching confidence without ground-truth annotation using a bottom-up pipeline that incorporates forward-backward consistency, optimal transport optimization, and temporal coherence constraints. We also dynamically adjust the hardness of negative samples during training to avoid the use of samples that are either too hard or too easy. Our proposed method achieves state-of-the-art performance on video label propagation tasks.