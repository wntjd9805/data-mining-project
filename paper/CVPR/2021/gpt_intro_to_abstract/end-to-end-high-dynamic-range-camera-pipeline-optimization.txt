Abstract:This paper addresses the challenge of capturing high dynamic range (HDR) scenes in robotics and navigation applications. Existing sensors are unable to capture HDR in a single shot, resulting in the need for sequentially and spatially-multiplexed acquisition techniques. Image Signal Processors (ISPs) play a crucial role in converting raw sensor data into images suitable for human viewing or scene understanding tasks. However, selecting optimal ISP hyperparameter values is challenging, as they depend on various factors such as the camera context, lens and sensor specifications, and the downstream task. This paper introduces an end-to-end, loss-driven approach to jointly optimize the sensor, ISP, and Convolutional Neural Network (CNN) detector for HDR imaging pipelines. The proposed method is validated experimentally and in simulation for human viewing and object detection tasks, and it outperforms existing methods. The paper also proposes a dynamic HDR lab setup, a perceptual image difference metric, and a data augmentation methodology to target HDR stitching artifacts. However, the method has some limitations, such as considering only one image understanding task and the sparsity of sensor hyperparameter sampling. Future work could explore more fine-grained methodologies and incorporate RAW video sequences for temporal cues.