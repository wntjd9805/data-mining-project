Semi-supervised learning (SSL) combines labeled and unlabeled data to improve machine learning models. SSL methods have the potential to generate accurate predictions with limited labeled data and enhance the performance and robustness of supervised learning by leveraging additional unlabeled data. Data augmentation has emerged as a powerful technique for SSL, with methods like unsupervised data augmentation (UDA), FixMatch, MixMatch, ReMixMatch, and Π-model incorporating consistency between labels and augmented versions of the data. The effectiveness of these SSL algorithms depends on the choice of matching objectives and algorithms. UDA applies a uniform consistency measure to all data points, while FixMatch focuses on high-confidence data points using a hard threshold approach. Most methods use iterative regularization procedures, but these may suffer from instability and non-convergence issues. This paper proposes two algorithmic advancements for consistency matching in SSL: 1) using alpha-divergence to measure label consistency, allowing for more flexible and soft regularization focused on high-confidence instances, and 2) introducing an optimization-based framework for consistency matching that improves convergence compared to iterative procedures. The proposed algorithm, AlphaMatch, achieves better SSL performance and stability compared to recent SSL methods such as FixMatch, ReMixMatch, MixMatch, and UDA, across various benchmarks (CIFAR-10, SVHN, CIFAR-100, and STL-10). AlphaMatch particularly excels when label availability is highly limited, outperforming FixMatch in accuracy, e.g., improving from 88.71% ± 3.35% to 91.35% ± 3.38% for CIFAR-10 with only 4 labeled images per class.