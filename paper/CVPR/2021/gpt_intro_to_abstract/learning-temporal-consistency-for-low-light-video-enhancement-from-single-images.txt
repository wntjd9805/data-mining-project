In this paper, we address the challenge of low light image enhancement, which is important for improving the signal-to-noise ratio (SNR) in low light scenes. Existing data-driven methods have achieved satisfactory results on single static images by implicitly learning models from image data. However, these methods require pixel-wise pairs of low and high SNR images for training, which is difficult to obtain in large scale and diverse environments. As a result, existing methods either use synthetic data or temporally inconsistent single image data for training, leading to artifacts and flickering in low light videos.To tackle this problem, we propose a novel method that enforces temporal consistency even when training from static images. Our approach involves learning and inferring motion fields (optical flow) from a single image and synthesizing short-range video sequences. By imposing consistency on the network, we are able to achieve low light video enhancement with improved temporal stability. Specifically, we use optical flow to mimic the motions of dynamic scenes, representing both global and local movements. We predict optical flow from static images and then warp the images with the flow to create adjacent frames, ensuring consistency in the deep model.We validate the effectiveness of our method through rigorous experiments on both synthetic and real data. The experimental results demonstrate that our method outperforms state-of-the-art single image methods and achieves comparable results to video-based methods, effectively reducing flickering without the need for videos. Additionally, a user study with 26 volunteers shows that 78.9% prefer our method, indicating its superior temporal stability.Our main contributions are as follows: 1. We propose a novel solution to the temporal inconsistency problem in low light video enhancement using only single image data.2. We introduce the use of optical flow prior to indicate motion from a single image, enabling the modeling of temporal consistency.3. Through rigorous experiments and a user study, we demonstrate the state-of-the-art performance of our method in terms of both objective metrics and user preference.