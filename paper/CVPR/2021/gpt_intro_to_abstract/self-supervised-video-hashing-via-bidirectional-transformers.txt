Scalable video retrieval is a crucial task for automatically identifying similar videos from a large database based on a query video. As the volume of videos continues to grow across various social media platforms and search engines, there is a pressing need to develop efficient video retrieval technologies. Hashing methods have garnered significant interest in this field as they transform high-dimensional data into compact binary codes while preserving the underlying similarity structure. Learning-based hashing, in particular, has shown promising results in image retrieval by leveraging data properties or label supervision to learn reliable hash functions. However, compared to images, the study of structured data like videos in the hashing field remains inadequate. Video hashing poses challenges due to the need to exploit correlations between video frames and the lack of large-scale labeled datasets like ImageNet to capture similarity structures. Thus, there is an urgent need to develop effective unsupervised video hashing methods. Existing unsupervised video hashing approaches focus on utilizing visual information within each video through deep neural networks, such as Recurrent Neural Networks (RNNs). Some methods also incorporate manually constructed similarity guidance. However, most of these approaches underuse bidirectional correlations among frames and fail to adequately exploit the similarity structure in video data due to suboptimal similarity matrix construction. To address these limitations, this paper proposes a self-supervised video hashing method based on bidirectional transformers. This approach effectively captures correlations among frames within a video and similarity structure between videos. Unlike existing video hashing methods that primarily exploit unidirectional correlations, the proposed method builds upon masked language models to capture bidirectional correlations. To unravel the similarity structure among videos, a pairwise similarity reconstruction objective is designed. It establishes reliable and effective pairwise similarity connections in the video space. Furthermore, a pseudo center set is developed from the training dataset, and cluster assignment on the latent outputs of transformers is enforced to learn more discriminative hash functions. Extensive experiments conducted on three popular video datasets demonstrate the superiority of the proposed method over state-of-the-art unsupervised video hashing methods.