Tracking multiple objects in a video is essential for various computer vision applications. The current approach, known as tracking-by-detection, involves detecting objects in individual frames and associating these detections with trajectories. However, this approach fails to maintain identities during occlusions, resulting in frequent identity switches. In this paper, we propose a stochastic motion model that addresses this issue by helping the tracker maintain identities, even in the presence of long-term occlusions. We demonstrate the importance of considering motion as a critical cue for tracking, and show that our approach outperforms the state of the art on multiple benchmark datasets. While motion has been previously considered in tracking-by-detection, we argue that human motion is a stochastic multi-modal process and should be modeled accordingly. We introduce a stochastic autoregressive motion model that learns the multi-modal distribution of natural trajectories, allowing us to estimate the likelihood of a tracklet given a sequence of bounding box locations and surrounding tracklets. This model enables us to compute the likelihood of a tracklet after assigning it a new detection and inpaint tracklets in the presence of occlusions. Our method preserves identities over longer time horizons compared to recent approaches, particularly in challenging scenarios with occlusions. We validate our approach through comprehensive ablation studies and experiments on benchmark datasets, where our method outperforms the state of the art in metrics related to long-term identity preservation. We provide the code for our model, called ArTIST (Autoregressive Tracklet Inpainting and Scoring for Tracking), which is publicly available.