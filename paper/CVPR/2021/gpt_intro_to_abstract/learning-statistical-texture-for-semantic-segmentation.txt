Semantic segmentation is a fundamental problem in computer vision, with applications in various areas such as automatic driving and human-machine interaction. Recent methods have focused on using high-level features from deep layers in convolutional neural networks (CNNs) to exploit contextual information. However, this approach can result in coarse and inaccurate outputs, as it misses crucial low-level details like edges. To address this issue, skip connections have been employed to fuse low- and high-level features.In this paper, we introduce the concept of statistical texture information to semantic segmentation and propose a Statistical Texture Learning Network (STLNet) that leverages both low-level and high-level features. We design a Quantization and Counting Operator (QCO) to effectively describe texture intensities in a statistical manner within deep neural networks. The QCO quantizes the input features into multiple levels, enabling the representation of diverse and continuous textures for easier description. Each level represents a kind of texture statistic, and the intensities are counted for texture feature encoding.Additionally, we propose a Texture Enhancement Module (TEM) and a Pyramid Texture Feature Extraction Module (PTFEM) to enhance texture details of low-level features and extract texture-related information from multiple scales, respectively. The TEM utilizes a graph-based approach inspired by histogram equalization to propagate information of all original quantization levels for texture enhancement. The PTFEM employs a texture feature extraction unit and a pyramid structure to exploit texture information at different scales.Our contributions include the introduction of statistical texture information to semantic segmentation, the development of the STLNet framework, the novel QCO for effective texture description in deep neural networks, and the TEM and PTFEM modules for texture enhancement and feature extraction. Experimental results on Cityscapes, Pascal Context, and ADE20K datasets demonstrate that our method achieves state-of-the-art performance.