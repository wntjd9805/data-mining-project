Semantic segmentation is a crucial task in computer vision that involves assigning a categorical label to each pixel in an image to partition it into meaningful segments. Convolutional neural networks (CNNs) have emerged as a dominant solution for semantic segmentation in recent years. While existing approaches have explored the use of contextual information for improving segmentation performance, they largely ignore the explicit dependence relations among different semantic categories. In this paper, we propose a novel approach called DependencyNet for semantic segmentation that explicitly models visual dependency relations in a CNN. We distinguish between three levels of visual dependency: intra-class, inter-class, and global dependency. The DependencyNet consists of three corresponding reasoning modules that leverage these dependency relations to enhance the segmentation process. Specifically, the network decouples the representations of different object categories and updates them based on their respective internal structures (intra-class reasoning). It also performs spatial and semantic reasoning based on the dependency relations among different object categories (inter-class reasoning). Additionally, the network refines the representations of each category based on scene information (global dependency reasoning). We encode the dependency relations as a dependency graph and leverage group weighted convolutions to perform reasoning. Unlike attention mechanisms and graph convolution networks, our approach does not depend on the input image and instead acts as prior knowledge. We extensively evaluate our approach through ablation studies and demonstrate its effectiveness on two datasets, achieving significant improvements over the base network. The proposed DependencyNet provides a unified framework for integrating dependency modeling with CNNs for semantic segmentation, bridging the gap between these two commonly used techniques in computer vision.