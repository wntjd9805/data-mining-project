The advancement of autonomous driving has led to increased interest in 3D object detection. While significant progress has been made in this field, most existing works focus on constrained settings where training and testing data come from similar scenarios. In practical applications, however, we often encounter more challenging situations with distribution mismatch between training and testing data collected from different devices at different times or locations. This poses a significant performance drop when deploying the trained model in a new scenario, known as the domain adaptation problem. While many works have addressed the domain adaptation problem for 2D images, there is limited research on cross-dataset object detection for 3D point clouds. Unlike 2D images, 3D point clouds lack texture information, making it more challenging to handle distribution mismatch. In this paper, we propose a new cross-dataset 3D object detection method called Scale-aware and Range-aware Domain Adaptation Network (SRDAN). Our method incorporates object geometric characteristics, such as size and distance, into deep neural networks to learn domain-invariant representation. We introduce scale-aware and range-aware domain alignment strategies to guide distribution alignment between the two domains. Specifically, we design a 3D voxel-based feature pyramid network for scale-aware alignment and utilize range information in 3D data for range-aware alignment. Our experiments on four datasets under three autonomous driving scenarios demonstrate the effectiveness of our approach for cross-dataset 3D object detection.