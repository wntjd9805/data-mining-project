Deep convolutional neural networks (CNNs) have found extensive applications in computer vision tasks such as image recognition, object detection, and image segmentation. However, these networks often have a large number of parameters and require significant computational resources, making them unsuitable for edge devices like mobile phones and autonomous cars. Various techniques, including quantization, pruning, and distillation, have been proposed to accelerate and compress CNNs. While these techniques have been successful on benchmark datasets and models, they often require access to the original training data, which is not always available due to privacy or transmission constraints.In this paper, we propose a data-free technique called Data-Free Noisy Distillation (DFND) for compressing CNNs without access to the original training data. Instead of generating images from the teacher network, we identify relevant images from a large unlabeled dataset to conduct knowledge distillation. We analyze the distance between the outputs of the teacher and student networks and develop a data selection method to identify useful unlabeled data. We collect these data along with the noisy labels derived from the teacher network. To improve the performance of the student network, we use a noise adaptation matrix to refine the labels provided by the teacher network.Experimental results on benchmark datasets show that our DFND algorithm outperforms other data-free distillation methods and achieves state-of-the-art performance. The resulting student network's accuracy is comparable to that of a student network trained using original data. Our proposed method provides an efficient and effective approach to learning portable student networks without access to training data, addressing the challenges of model compression in practice.