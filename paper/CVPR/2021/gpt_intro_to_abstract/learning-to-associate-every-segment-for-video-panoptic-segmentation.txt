Video understanding plays a crucial role in various computer vision applications. One approach to achieve video understanding is through video semantic segmentation, which involves the pixel-level annotation of different elements in a video. However, the scarcity of video annotations poses a challenge in developing effective training objectives. Most video segmentation benchmarks only provide annotations for single frames, limiting the annotations to the image level. To address this limitation, recent work has introduced the problem of video panoptic segmentation, which aims to predict object classes, masks, instance id associations, and semantic segmentation for all pixels in a video. In order to achieve robust visual reasoning in space and time, it is important to identify temporal correspondence in a video. While methods for learning temporal correspondence have been developed, they often focus on a single level of correspondence and fail to solve different levels jointly. In this paper, we propose an efficient framework for learning segment-level and pixel-level temporal correspondences simultaneously. We introduce a deep siamese model and train it with pairs of frames, encoding strong temporal consistency without heavy feature aggregation or fusion operations. Our approach achieves state-of-the-art results on video panoptic segmentation benchmarks and provides a novel understanding of temporal correspondence learning in videos. The contributions of this paper include the generalization of temporal correspondence learning to every segment in a video, the proposal of a new supervised contrastive learning method, and the achievement of state-of-the-art results on benchmarks. Extensive experimental analysis and ablation studies are presented to support our approach.