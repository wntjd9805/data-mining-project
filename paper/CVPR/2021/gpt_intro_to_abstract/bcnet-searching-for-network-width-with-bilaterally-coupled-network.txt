This paper focuses on the practical deployment of convolutional neural networks (CNNs) and the consideration of different hardware budgets, such as floating point operations (FLOPs), latency, memory footprint, and energy consumption. To accommodate these budgets, the authors propose pruning redundant channels of a model to obtain a compact network width. This pruning is typically implemented either in an end-to-end or layer-by-layer manner, maintaining the structure of the pre-trained model. The pruned network can then be further improved using techniques like quantization and knowledge distillation.Previous research has shown that the core of channel pruning is learning a more compact network width instead of the retained weights. Therefore, recent work has employed neural architecture search (NAS) or other automated techniques to directly search for an optimal network width. These methods typically use a one-shot supernet for evaluating different widths.To evaluate the performance of different network widths, the authors propose a bilateral approach within the supernet. Instead of unilaterally evaluating network widths based on the leftmost channels, as done in current methods, the authors introduce the Bilaterally Coupled Network (BCNet). In BCNet, each channel is fairly trained and responsible for the same amount of widths, ensuring training and evaluation fairness. Each width is determined symmetrically by the average performance of bilateral channels.To enforce training fairness over channels, the authors adopt a complementary training strategy. They also propose a prior sampling method to generate a good and steady initial population for the subsequent evolutionary algorithm-based searching.Extensive experiments on benchmark datasets, including CIFAR-10 and ImageNet, demonstrate that BCNet outperforms state-of-the-art methods under various FLOPs budgets. For example, their searched EfﬁcientNet-B0 achieves 74.9% Top-1 accuracy on the ImageNet dataset with 192M FLOPs (2× acceleration).