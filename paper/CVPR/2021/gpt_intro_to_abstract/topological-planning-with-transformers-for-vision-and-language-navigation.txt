Enabling robots to understand natural language and perform instructed tasks is a desired goal in the field of mobile robotics. Vision-and-language navigation (VLN) is a critical step towards achieving this goal. Most current VLN systems utilize deep learning models and unstructured memory, such as LSTM, which work well for constrained movements but struggle with free navigation. Furthermore, fully end-to-end learning for navigation can be difficult and costly, requiring extensive experience and ground truth odometry. Recent research in visual robot navigation has explored the use of structured memory and modular approaches, but these have not been well studied in the context of VLN. In this paper, we propose a modular approach using topological maps for VLN. Topological maps represent environments as graphs, eliminating the need for meticulous map construction and enabling efficient planning and interpretable navigation plans. The symbolic nature of topological maps makes them suitable for navigation with language, as the discretization provided by the maps facilitates learning the relationship between instructions and spatial locations. We leverage the parallel between navigation planning and language sequence prediction to utilize attention mechanisms for navigation in topological maps. Our proposed approach uses a cross-modal attention-based transformer to compute navigation plans based on language instructions in topological maps. Unlike language models that predict one word at a time, our transformer predicts one topological map node in the navigation plan at a time, enabling the agent to attend to relevant portions of the instruction and spatial regions of the environment. We evaluate our approach using the VLN-CE dataset and demonstrate the effectiveness of cross-modal attention-based planning and the robustness of our modular approach in correcting navigation mistakes.