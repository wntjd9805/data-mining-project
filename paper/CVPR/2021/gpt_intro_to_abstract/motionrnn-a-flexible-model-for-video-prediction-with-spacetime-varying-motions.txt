Real-world motions are complex and constantly changing in space and time, making it challenging to accurately predict them. Existing deterministic video prediction models focus on simple state transitions across time and overlook the complex variations within motions. Optical flow-based methods capture short-term temporal dependency but lack the characterization of long-term motion trends. This paper proposes MotionRNN, a novel framework for space-time varying motion prediction. MotionRNN decomposes motions into transient variation and motion trend, capturing both using a MotionGRU unit. Inspired by ResNet, a Motion Highway structure is introduced to prevent captured motions from vanishing and provide contextual spatiotemporal information. MotionRNN achieves state-of-the-art performance on three benchmarks and can be easily adapted to existing predictive models. The contributions of this paper include the design of the MotionGRU unit, the MotionRNN framework, and the improved performance on challenging benchmarks.