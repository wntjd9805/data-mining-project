This paper introduces a data-driven method to address the limitation of body-garment interpenetrations in digitalized 3D garments. The existing methods, while offering generalizable and physically accurate results, are computationally expensive and not easily differentiable. The proposed method leverages neural networks to learn a function that mimics garment behavior and incorporates a loss term to penalize interpenetrations. However, existing works still suffer from interpenetrations in test sequences, which are usually addressed through postprocessing steps. The authors propose a novel approach that enhances human body models to smoothly expand surface parameters, allowing for deformation modeling at any 3D point. They also address the assumption that garment deformations closely follow body deformations and propose a garment model that represents deformations in a unposed and deshaped canonical space. By leveraging this canonical representation, the authors learn a generative subspace of deformations and use a regressor to output mesh deformations encoded in this subspace. The proposed method ensures that the final deformed 3D garments do not interpenetrate the body mesh, regardless of shape and pose parameters.