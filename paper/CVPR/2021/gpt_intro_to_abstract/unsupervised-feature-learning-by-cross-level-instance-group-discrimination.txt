Representation learning aims to extract latent or semantic information from raw data. Traditionally, models are trained on large annotated datasets and then fine-tuned on smaller datasets for specific tasks. However, as models become bigger and deeper, the need for more annotated data becomes impractical. To address this issue, self-supervised learning has emerged as an alternative approach, where a model is trained on pretext tasks that do not require annotations but still require semantic understanding. One drawback of self-supervised learning is that the pretext tasks are domain-specific and not directly applicable to downstream tasks.A promising alternative to self-supervised learning is unsupervised contrastive learning, which aligns the training objective and downstream classification on discrimination. This approach has been successful in learning representations that capture semantic similarity. However, existing datasets used for benchmarking are distinct and class-balanced, whereas natural data often have highly correlated instances within classes and a long-tail distribution across classes.To address the challenges of natural data, this paper proposes integrating between-instance similarity into contrastive learning. Instead of directly grouping instances, the paper introduces cross-level discrimination (CLD) between instances and local instance groups. The dual forces of attraction and repulsion are used to discover groupings based on the similarity of instances. By pulling instances towards their corresponding groups and pushing them away from other groups, similar instances are mapped closer in the feature space.The proposed CLD method is added to popular unsupervised feature learning approaches, such as NPID, MoCo, InfoMin, and BYOL. Experimental results show that CLD significantly improves performance on datasets with high correlation and long-tail distribution, as well as on various self-supervision, semi-supervision, and transfer learning benchmarks. The addition of CLD has negligible overhead and achieves state-of-the-art performance on all benchmarks, surpassing MoCo v2 and SimCLR with much less computational resources.In summary, this paper extends unsupervised feature learning to natural data and proposes a method to integrate between-instance similarity into contrastive learning. The experimental results demonstrate the effectiveness of the proposed method in improving performance on various benchmarks.