This paper introduces a novel task called semantic audio-visual navigation, which aims to broaden the scope of real-world scenarios in audio-visual navigation. The task requires an agent to navigate to an object located in an environment that emits sound for a limited period of time. Unlike previous work that focused on objects with steady repeating sounds, this task incorporates objects that emit sporadic and short-lasting sounds. The agent not only needs to associate sounds with visual objects but also leverage the semantic context of objects to infer their likely locations in the scene. The paper proposes a deep reinforcement learning model that learns the association between visual and auditory cues and utilizes a goal descriptor module to predict goal properties from acoustic cues. The model is evaluated on various real-world environments and outperforms state-of-the-art models in audio-visual navigation. It demonstrates the potential for embodied agents to learn about the appearance and sound of objects in a 3D environment.