This paper addresses the problem of predicting and synthesizing plausible future progressions in videos based on a given image. Anticipating and understanding what happens next in a scene is crucial for artificial visual understanding in various applications such as autonomous driving, medical treatment, and robotic planning. However, accurately predicting future scenes requires a deep understanding of scene dynamics and how objects interact and evolve over time.Many existing methods focus on predicting likely video continuations using simplifying assumptions or side information. However, truly understanding the synthesis problem requires not only inferring image continuations but also describing and representing the scene dynamics within a video sequence.To tackle this problem, this paper proposes framing image-to-video synthesis as an invertible domain transfer problem. The authors introduce a dedicated probabilistic residual representation to capture the missing information in videos that is not present in images. By learning this invertible mapping, it becomes possible to synthesize future video sequences given an initial image and the instantiation of the latent residual.To reduce the complexity of the learning task, a separate conditional variational encoder-decoder architecture is trained to compute a compact, information-preserving representation for the video domain. Additionally, the authors demonstrate the ability to incorporate extra conditioning information to exercise control over the synthesis process.The proposed framework is evaluated on four video datasets, covering human motion synthesis and dynamic textures. The results show strong performance and demonstrate the effectiveness of the approach. Overall, this paper presents a novel method for image-to-video synthesis, highlighting the importance of understanding scene dynamics and showcasing the potential applications of the proposed approach.