Semantic segmentation is a challenging problem in computer vision with various practical applications. Existing state-of-the-art approaches are typically based on auto-encoder structures and fully convolutional models, but they lack the ability to incrementally update their classification model to accommodate new categories. This issue, known as catastrophic forgetting, is well-known in the context of deep neural networks. Continual learning, which has been extensively studied in image classification and object detection, has only recently been explored in the field of semantic segmentation. This paper investigates class-incremental continual learning in semantic segmentation. Unlike previous approaches that heavily rely on output-level knowledge distillation, this work focuses on latent space organization. The main idea involves introducing constraints such as prototype matching, features sparsification, and attraction-repulsion rule. Additionally, targeted output-level distillation is employed to preserve discriminability on previous categories during classification. The paper presents a common framework that can handle sequential, disjoint, and overlapped scenarios, allowing it to be applied in combination with previous techniques. Evaluation is conducted on standard semantic segmentation datasets, and the proposed approach outperforms state-of-the-art continual learning methods. The main contributions of this work include providing a common framework for class-incremental learning in semantic segmentation, exploring the latent space organization, proposing novel knowledge preservation techniques, and benchmarking the approach on standard datasets.