Determining correspondence between images is essential in computer vision for various applications. This task is challenging due to differences in illumination, viewpoint, texture, and season. Current methods use a detect-and-describe approach, detecting distinctive regions and describing them using descriptors. However, these methods often fail to find matches in smooth regions and struggle with scene-shift invariance. In cases without prior knowledge, we propose a new approach using a neural network called CD-UNet. It incorporates a Co-Attention Module to generate descriptors conditioned on both images and a Distinctiveness score for selecting the best matches. We explore supervised and self-supervised training methods and evaluate the model's performance on tasks like local matching, camera localization, 3D reconstruction, and style transfer. CD-UNet outperforms state-of-the-art models and achieves comparable results on all tasks. We show that conditioning descriptors on both images improves correspondence matching under challenging conditions, without the need for high-dimensional descriptors or multiple scales.