Monocular 3D object detection is a challenging task in computer vision, as it requires accurate localization without explicit depth information. Many existing methods leverage geometrical priors and solve object pose using 2D-3D constraints. However, these approaches often rely on a limited number of visible keypoints and may struggle with occlusion or truncation. In this paper, we propose MonoRUn, a novel monocular 3D object detection method that uses self-supervised reconstruction with uncertainty propagation. MonoRUn extends off-the-shelf 2D detectors by adding a 3D branch to regress dense 3D object coordinates within each predicted 2D box. To overcome the need for supervised foreground segmentation, we estimate the uncertainty of the predicted coordinates and adopt an uncertainty-driven PnP algorithm that focuses on low-uncertainty foreground regions. By forward propagating the uncertainty, we can estimate the pose covariance matrix and use it to score the detection confidence. Self-supervision is achieved by projecting the predicted 3D coordinates back to the image using the ground truth object pose and camera intrinsic parameters. To minimize the re-projection error, we propose the Robust KL loss, which considers uncertainty awareness. This loss function contributes to the state-of-the-art performance of the MonoRUn network.Our main contributions include the proposal of a monocular 3D object detection network with uncertainty awareness, which can be trained without additional annotations. We introduce the Robust KL loss for deep regression with uncertainty awareness, demonstrating its superiority over previous methods. Extensive evaluation on the KITTI benchmark reveals significant improvement compared to current state-of-the-art methods. Overall, our approach presents a dense correspondence method for 3D detection in real driving scenes.