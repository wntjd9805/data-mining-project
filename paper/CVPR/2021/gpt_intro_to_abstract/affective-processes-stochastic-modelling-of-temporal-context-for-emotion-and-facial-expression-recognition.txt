This paper addresses the problem of facial behavior recognition from video, specifically the recognition of apparent emotions in terms of Valence and Arousal, and facial expressions in terms of Action Unit intensity. While this problem has been extensively studied in the computer vision community, recent methods still struggle to achieve high accuracy on challenging datasets. This paper proposes effective temporal context modeling as a key feature for significantly advancing the state-of-the-art.The paper introduces a novel approach based on Neural Processes for modeling temporal context in emotion recognition. Unlike previous work that primarily focused on modeling facial expression dynamics using CNNs and RNNs, this approach emphasizes the importance of temporal context over facial dynamics. The cues for inferring a person's apparent emotion are often sparsely and non-regularly distributed over a temporal window, and collecting such distributed contextual cues is critical for robust emotion inference.To address the challenges of subjective and noisy annotations, as well as the need for stochastic modeling of context, the proposed model incorporates three key components: stochastic contextual representation with a global latent variable model, task-aware temporal context modeling using features and task-specific predictions, and effective temporal context selection.The paper presents an overview of the proposed method compared to RNN-based methods and methods based on self-attention. It highlights that the proposed approach surpasses these baselines in terms of modeling temporal context effectively, demonstrating consistent and significant improvement over state-of-the-art methods on challenging emotion recognition datasets.Overall, the contributions of this paper include the introduction of Affective Processes, a model for emotion recognition with global stochastic contextual representation, task-aware temporal context modeling, and temporal context selection. The effectiveness of this model is demonstrated through extensive ablation studies and performance comparisons with CNNs+RNNs and self-attention-based baselines. The proposed approach is validated on challenging emotion recognition databases and also shows potential for Action Unit intensity estimation.