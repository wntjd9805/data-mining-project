Motion style transfer in computer graphics has been a topic of interest due to its applications in human animation, games, and robotics. Early methods relied on handcrafted features, but data-driven approaches using deep learning models have been proposed to automate feature learning. However, most existing deep learning methods require paired and registered data, as well as a large amount of motion samples. This poses a challenge in collecting and preprocessing training data. Recently, a style transfer method was proposed that does not require paired and registered training data but still relies on manually labeled style samples. This method produces deterministic outputs and may have artifacts that need to be resolved. In this paper, we propose an unsupervised method for motion style transfer using a generative flow model. Our model is trained to maximize the exact log-likelihood over unlabeled motion data, resulting in high-quality stylized motions without artifacts. Unlike previous methods, our approach is trained unsupervised and generates probabilistic outputs. The generative flow model extracts deep properties of motion styles from the input style motion and combines them with the input content motion to synthesize various stylized motions. We introduce affine coupling layers and a Transformer in the invertible flow transformation to improve the efficiency and flexibility of our generative flow model. Our experimental results show that the Transformer enables the model to learn a flexible latent distribution, allowing it to transfer unseen styles to content motions. Our contributions include the introduction of a generative flow model for motion style transfer, which generates high-quality stylized motions in a probabilistic manner. We efficiently infer latent codes from the input style motion using invertible flow transformations. Additionally, we impose a Transformer in the flow transformation to learn a flexible latent distribution for encoding deep properties of unseen motion styles. Overall, our proposed model offers more flexibility, scalability, and higher quality results compared to existing methods for motion style transfer.