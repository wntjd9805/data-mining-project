Recent years have witnessed the emergence of various scene representations that utilize multi-layer perceptrons (MLPs) to encode spatially-varying properties of scenes. These implicit representations, although optimized through stochastic gradient descent, do not embody traditional learning but instead leverage MLPs as a compressed representation of scene content. Notable examples include DeepSDF, Scene Representation Networks, and Neural Radiance Fields (NeRF).Among these representations, NeRF and its variants demonstrate great potential in reconstructing scenes photorealistically from a sparse set of images. However, these methods assume static scenes or disregard the dynamic elements as uninteresting. Consequently, when objects within a scene are in motion, these approaches fail to correctly render novel views. While it is feasible to represent time-varying scenes by assigning a NeRF volume per frame or including time as an extra dimension, these solutions are computationally expensive and lack object-level comprehension.This paper aims to learn an interpretable and editable representation of a dynamic scene by observing a moving object from multiple viewpoints. Initially, the work focuses on a simplified scenario where the scene includes a single moving object with fully rigid motion. To address this challenge, the authors propose rendering a compositional neural radiance field using a composition of a static and a dynamic neural radiance field. This model necessitates accurate segmentation of the scene into the static and dynamic volumes, along with precise object pose estimation in each frame for accurate predictions across the entire video.The paper presents two main technical contributions. Firstly, it introduces a self-supervised neural rendering-based representation capable of simultaneously reconstructing a rigid moving scene and its background solely from video observations. This approach facilitates photo-realistic spatial-temporal novel view rendering and novel scene animation. Secondly, the paper presents an optimization scheme that effectively deals with ambiguity and local optima during training.Experimental results demonstrate the ability to recover the segmentation of static and dynamic content, as well as motion trajectory, with only multi-view RGB observations as supervision. Compared to NeRF and its extensions, the proposed approach achieves more realistic reconstruction in complex synthetic and real-world scenes. Furthermore, the factorized representation allows for object positioning in novel locations unseen during training, a capability unmatched by existing methods without 3D ground truth or supervision.