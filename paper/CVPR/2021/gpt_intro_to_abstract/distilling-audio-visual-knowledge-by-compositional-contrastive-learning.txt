Videos contain a wealth of informative multi-modal cues, including visual objects, motion, auditory events, and textual information. Existing video understanding methods mainly focus on using visual content for representation learning. In this paper, we propose a multi-modal distillation framework that leverages the rich multi-modal knowledge encoded in networks pre-trained on spatial imagery data and temporal auditory data to learn expressive video representations. Unlike traditional knowledge distillation methods that transfer knowledge from the same modality and dataset, our framework utilizes knowledge from multiple data modalities. We address the challenge of bridging the cross-modal semantic and domain gaps, as well as dealing with inconsistent network architectures across modalities. To achieve this, we introduce compositional contrastive learning, a novel framework that uses a learnable compositional embedding to bridge the cross-modal gap and capture task-relevant semantics. By jointly aligning the teacher, student, and compositional embeddings, our framework transfers multi-modal knowledge to the video student network for enhanced video representation learning. We present a new benchmark dataset and extensively compare our Compositional Contrastive Learning (CCL) model with state-of-the-art distillation methods in video recognition and retrieval tasks. Our results demonstrate the superiority of CCL and provide quantitative and qualitative analysis.