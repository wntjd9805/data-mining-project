Scene text recognition has become increasingly important in a variety of real-world applications. Two main frameworks for scene text recognition have emerged: the CRNN framework, which uses CNNs and RNNs for encoding and CTC for decoding, and the attention-based encoder-decoder framework, which aligns output texts with feature maps. However, both frameworks have their limitations. CTC-based methods often contain redundant information, while attention-based methods suffer from misalignment issues. To address these limitations, this paper proposes a novel scene text recognition framework that learns primitive representations of scene text images. Inspired by graph representation learning methods, the paper models feature maps as nodes of an undirected graph and learns primitive representations by globally aggregating features over the coordinate space. These primitive representations are then projected into the visual text representation space. The paper introduces pooling and weighted aggregation methods for global feature aggregation and utilizes graph convolutional networks to generate visual text representations. The proposed framework, called PREN, achieves a balance between accuracy and speed. Additionally, the paper integrates PREN into a 2D-attention-based encoder-decoder model, called PREN2D, which achieves state-of-the-art performance. Experimental results on various English and Chinese scene text recognition datasets demonstrate the effectiveness and efficiency of the proposed methods. The contributions of this paper include the novel framework for scene text recognition, the pooling and weighted aggregators for learning primitive representations, and the integration of the proposed method into attention-based frameworks.