Modern computer vision approaches excel in specific tasks but struggle with adapting to new tasks or visual domains without forgetting their initial training. This phenomenon, known as catastrophic forgetting, has been observed for decades. Lifelong learning or continual learning approaches aim to continuously learn without forgetting previous knowledge. In this paper, we focus on the problem of continual and supervised adaptation to new visual domains. We propose methods for learning visual representations that are robust against catastrophic forgetting, without storing data or expanding the model. We introduce perturbations to the current domain distribution to increase the likelihood of future domains being similar, thereby requiring less adaptation. We use image transformations for randomization and show that models trained in this manner are more robust against catastrophic forgetting in continual, supervised domain adaptation. Our second contribution is tackling the problem of learning representations that are inherently robust against transfer to new domains. We employ meta-learning and develop a regularization strategy, forcing the model to train for the current task while being resilient to parameter updates on domains different from the current one. We introduce auxiliary meta-domains created by randomizing the original distribution to address the lack of access to different meta-domains. We demonstrate the effectiveness of our strategies in experiments on digit recognition, the PACS dataset, and semantic segmentation. Additionally, we show the benefits of combining our methods with a small memory of samples from previous domains.