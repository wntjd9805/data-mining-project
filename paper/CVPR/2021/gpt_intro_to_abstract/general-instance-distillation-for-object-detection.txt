Object detection has seen significant advancements in accuracy with the emergence of deep convolutional neural networks (CNN) in recent years. These deep learning network structures, including both one-stage and two-stage detection models, have become the mainstream approach in the field. However, while these deep learning models offer high precision, they are often cumbersome and lack efficiency for practical applications. To address this, knowledge distillation (KD) has been proposed as a solution to transfer knowledge from large models to smaller, more efficient models. In the context of object detection, most KD methods are designed for multi-class classification problems and are less effective due to the unbalanced ratio of positive and negative instances in detection tasks. In this paper, we propose a novel distillation method based on discriminative instances, called general instances (GIs), which captures response-based, feature-based, and relation-based knowledge. Our method does not require manually setting the proportion of positive and negative areas or selecting only GT-related areas for distillation. Instead, GIs are automatically selected between student and teacher models, allowing for more effective knowledge transfer. Our method demonstrates strong generalization across various detection frameworks and achieves state-of-the-art performance on the MSCOCO and PASCAL VOC datasets for one-stage, two-stage, and anchor-free methods.