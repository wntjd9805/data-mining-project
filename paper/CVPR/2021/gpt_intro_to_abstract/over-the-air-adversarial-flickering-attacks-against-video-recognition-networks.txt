Deep Neural Networks (DNNs) have demonstrated remarkable performance in various tasks, including image classification, object detection, and semantic segmentation. However, these networks are vulnerable to adversarial attacks, wherein minor perturbations added to an image can lead to false predictions by the DNN. This vulnerability raises concerns for critical systems such as face recognition, surveillance cameras, autonomous vehicles, and medical applications. While research on adversarial patterns in images has received significant attention, there has been relatively less focus on video action recognition. This paper aims to address adversarial attacks against video action recognition by introducing flickering perturbations that are imperceptible to human observers. Unlike previous works, the goal is to avoid sparsity in the perturbation pattern, as sparse patterns are easily detectable by humans and image-based adversarial perturbation detectors. The proposed perturbation does not contain spatial information on a single frame other than a constant offset, resembling natural video changes. The targeted model for the attack is the I3D model based on InceptionV1, and experiments are expanded to include additional models. To make the perturbation unnoticeable, the paper introduces two regularization terms during the optimization process and a modified adversarial loss function. The effectiveness of the flickering attack is demonstrated on various models, including inter-model transferability, and real-world scenarios with over-the-air attacks. The paper's contributions include a methodology for developing flickering adversarial attacks, a time-invariant adversarial perturbation, transferability between different networks, and implementability using temporal perturbations. The paper concludes with experimental results, real-world examples, and future work. The attack videos, over-the-air scene-based attack videos, and over-the-air universal attack videos can be viewed, and the code is available in a repository.