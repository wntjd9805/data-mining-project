Contrastive learning techniques have achieved significant advancements in self-supervised representation learning for computer vision applications such as image classification, object detection, and segmentation. However, these techniques have not been extensively explored for text recognition. In this paper, we propose a novel approach that extends existing contrastive learning methods to sequential prediction tasks, specifically text recognition. Our approach focuses on applying contrastive learning to individual elements of the sequence while maintaining their order. We introduce an instance-mapping function that generates instances from consecutive frames in a sequence feature map, which serve as the atomic elements for contrastive learning. By augmenting the sequences to ensure sequence-level alignment, we enhance the effectiveness of the learned representations. Through extensive experimental validation on handwritten and scene text datasets, we demonstrate the superiority of our method over non-sequential contrastive approaches. Our approach outperforms supervised training methods with limited labeled data and achieves state-of-the-art results on standard handwritten datasets. Overall, our contributions include a contrastive learning approach for visual sequence-to-sequence recognition, the viewing of feature maps as sequences of individual instances for sub-word level contrastive learning, the definition of sequence-preserving augmentation procedures, and custom projection heads.