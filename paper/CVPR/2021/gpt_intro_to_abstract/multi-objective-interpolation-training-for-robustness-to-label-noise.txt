Building large datasets for training deep neural networks (DNNs) is challenging because manual labeling is time-consuming. Automatic data annotation based on web search and user tags allows for larger datasets but introduces label noise, which degrades DNN performance. Label noise in image classification involves incorrect labels and out-of-distribution samples. Various methods have been proposed to handle label noise, such as reducing the contribution of noisy samples, correcting their labels, or abstaining from classification. However, these methods mainly focus on classification losses and do not consider similarity learning frameworks. Similarity learning frameworks, popular in computer vision, learn representations for samples of the same class that are closer in feature space. Traditional methods sample pairs or triplets to measure similarities, while recent approaches consider a high number of negatives. However, there is limited research on applying contrastive learning losses to address label noise.This paper proposes a framework called Multi-Objective Interpolation Training (MOIT) that combines contrastive and semi-supervised learning to robustly learn in the presence of label noise. MOIT introduces a regularization of the contrastive loss and performs robust image classification. An interpolated contrastive learning (ICL) loss is used to mitigate performance degradation when training with label noise. A novel label noise detection strategy is also proposed, relying on noise-robust feature representations provided by ICL. Finally, a fine-tuning strategy over detected clean data is applied to further boost performance.Experimental results on various datasets with both synthetic and web label noise demonstrate that MOIT achieves state-of-the-art performance. The contributions of this paper include the MOIT framework, the ICL loss, the label noise detection strategy, and the MOIT+ fine-tuning strategy. These contributions enable robust learning in the presence of label noise and improve performance in image classification tasks.