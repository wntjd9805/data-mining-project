Deep convolutional neural networks have achieved remarkable results in various recognition tasks. However, when encountering new classes in a continual data stream, it becomes challenging to add them into the recognition tasks without retraining the model on all the old and new data. Fine-tuning old models with new data is a commonly used solution, but it can lead to catastrophic forgetting. To address this problem, recent learning-based approaches aim to preserve the representation space for old classes while introducing distillation losses and reconstructing classifiers. However, these methods assume that new class samples are available in large quantities, which is not typically the case in practical applications where new classes may be scarce. In this paper, we focus on the ability to incrementally learn new classes from few samples, which we refer to as few-shot class-incremental learning (FSCIL). Existing incremental learning methods do not perform well in FSCIL tasks due to the insufficiency of incremental class samples and the lack of supervision. To overcome these challenges, we propose an incremental prototype learning scheme that enables the learning of an extensible feature representation. This scheme uses a random episode selection strategy to enhance the extensibility of the feature representation and introduces a self-promoted prototype refinement mechanism to update existing prototypes based on the relation between representations of new class samples and old class prototypes. We also propose a dynamic relation projection module to guide the dynamic change of the prototype, maintaining existing knowledge and enhancing the discriminability of the new class. Experimental results on CIFAR-100, MiniImageNet, and CUB200 datasets demonstrate the superiority of our method over existing few-shot class-incremental and typical class-incremental methods.