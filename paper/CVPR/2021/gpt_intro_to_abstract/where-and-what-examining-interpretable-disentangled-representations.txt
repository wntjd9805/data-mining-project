Learning disentangled representations in generative models has gained significant interest in recent years. Disentangled representations aim to capture independent factors of variations in data, aligning with natural concepts understood by humans. These representations have various applications, such as controllable image generation and manipulation, domain adaptation, abstract reasoning, and machine learning fairness. In previous works, disentanglement has been characterized by informativeness and independence, with methods based on Generative Adversarial Networks (GANs) maximizing mutual information and Variational Autoencoders (VAEs) enforcing statistical independence. However, interpretability, which corresponds to the correspondence between learned representations and human-defined concepts, has been largely neglected in unsupervised settings. The nonuniqueness of representations satisfying informativeness and independence goals remains a significant flaw in existing models. Approaching interpretability without ground-truth labels seems impossible. However, it is feasible to identify interpretable variations from noninterpretable ones based on general biases in humans' definition of concepts. In this paper, we propose two hypotheses for interpretability, namely Spatial Constriction and Perceptual Simplicity, and design corresponding modules and losses to enforce these properties in disentangled representations. We also introduce a perceptual distance and demonstrate its use in unsupervised model selection. Experimental evaluations on various datasets validate the effectiveness of our proposed modules and the importance of modeling interpretability in disentangled representation learning.