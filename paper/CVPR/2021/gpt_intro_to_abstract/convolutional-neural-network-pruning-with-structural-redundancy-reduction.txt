Convolutional neural networks (CNNs) have made significant advancements in recent years and are widely used in various applications. However, the over-parameterization problem of CNNs limits their application on resource-limited devices. To address this issue, many approaches have been proposed to reduce the computation and storage cost of CNNs, with network pruning being one of the most popular methods.Network pruning can be categorized into weight (unstructured) pruning and channel (structured) pruning. While weight pruning zeros out specific weights in filters, channel pruning removes entire convolutional filters, making it a more flexible method. Many existing channel pruning approaches focus on identifying and pruning the least important filters. However, our study challenges this belief by theoretically showing that pruning filters in the layer with the most redundancy, even randomly, outperforms pruning the least important filters across all layers.Based on this finding, we propose a layer-adaptive channel pruning approach called Structural Redundancy Reduction (SRR). SRR utilizes statistical modeling to measure redundancy in each convolutional layer. By establishing a graph for each layer and using the â„“-covering number and quotient space size, we identify the layer(s) with the most redundancy and prune unimportant filters in those layers.The contributions of our study are threefold. Firstly, we provide a theoretical analysis of network pruning from a redundancy reduction perspective. Secondly, we propose the SRR approach, which builds a graph for each layer to measure redundancy and prunes filters in the most redundant layer(s). Finally, we validate our approach on various network architectures and datasets, achieving state-of-the-art performance compared to recent channel pruning methods. For example, our pruned ResNet50 model on ImageNet reduces 44.1% FLOPs with only a 0.37% loss in top-1 accuracy.In conclusion, our study challenges the common belief of pruning the least important filters and proposes a layer-adaptive channel pruning approach based on structural redundancy reduction. Our results demonstrate the effectiveness of our approach in reducing computational cost without significant performance loss.