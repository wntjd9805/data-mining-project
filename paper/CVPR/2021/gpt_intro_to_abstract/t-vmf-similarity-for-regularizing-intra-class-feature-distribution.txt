Deep convolutional neural networks (CNNs) have demonstrated significant success in computer vision tasks, particularly visual recognition. However, the performance of CNNs heavily relies on large-scale datasets with detailed annotations. Existing benchmark datasets, such as ImageNet, provide a diverse range of labeled samples for training and evaluation. Unfortunately, real-world tasks often suffer from limited and noisy training datasets, leading to degraded performance.To address the limitations of CNNs trained on poor datasets, this paper explores two approaches: architectural optimization and data augmentation techniques. Architectural optimization aims to reduce the parameter size of CNNs, while data augmentation injects perturbations into real image samples to virtually expand the training data. Both techniques contribute to improving the performance of CNNs on deteriorated datasets.In addition to architectural optimization and data augmentation, regularization techniques can be applied to enhance the generalization performance of CNNs. Notably, the neuron activations in the penultimate layer play a crucial role in feature representation, which is further used for classification. Regularizing these features can improve the representation learning even on datasets with poor sample collection. Various regularization techniques, such as center loss and Dropout, have been proposed in the literature.In this work, the focus is on the cosine similarity, a fundamental metric in the classifier, to impose regularization on features and thereby improve performance on deteriorated training datasets. The cosine similarity measures the angle between two feature vectors and is commonly used in CNNs. To go beyond the limitations of cosine similarity, the paper introduces a novel similarity metric based on the von Mises-Fisher (vMF) distribution. This proposed similarity is a compact-support function over angles, enabling implicit regularization of intra-class feature distribution.Compared to prior works that also utilize cosine similarity for regularization, the proposed method has several advantages. Firstly, it regulates features without introducing additional regularization loss. Secondly, it applies equally to all classes, without focusing on the ground-truth class. Lastly, the proposed similarity can easily replace cosine similarity in a computationally efficient manner.Overall, this paper presents a novel approach to improve the performance of CNNs on deteriorated training datasets, focusing on the regularization of features using a novel similarity metric derived from the vMF distribution. The experimental results demonstrate the effectiveness of the proposed method in enhancing feature representation and mitigating the limitations imposed by poor datasets.