This paper introduces Action Genome Question Answering (AGQA), a new benchmark for testing compositional spatio-temporal reasoning in visual events. AGQA consists of a large dataset of 3.9 million balanced and 192 million unbalanced question-answer pairs associated with 9.6 thousand videos. The questions in AGQA are generated by handcrafted programs that operate over spatio-temporal scene graphs, providing granular control over the required reasoning steps. The paper presents new training/test splits that test for specific forms of compositional spatio-temporal reasoning, including generalization to novel compositions, indirect references, and more reasoning steps. The evaluation of modern visual reasoning systems on AGQA shows that they barely outperform models that rely solely on linguistic bias, indicating a lack of generalization to novel compositions and limited performance on questions with more reasoning steps. This benchmark will help advance research in compositional spatio-temporal reasoning in visual events.