The nature of improvements achieved by high-capacity deep networks trained on large-scale data has raised questions in terms of prediction performance. Visual Question Answering (VQA) has emerged as a testbed for evaluating the reasoning and generalization capabilities of trained models. VQA combines heterogeneous modalities, such as images and language, with open questions and diverse variations. Previous studies have demonstrated that current models in VQA tend to exploit biases in training data, leading to unwanted shortcuts in learning. In this paper, we focus on studying the reasoning capabilities of VQA models. We define reasoning as algebraically manipulating words and visual objects to answer a new question, in contrast to exploiting biases in training data. We argue that learning to manipulate words and objects algebraically is challenging when visual input is noisy and uncertain. To validate this claim, we employ an in-depth analysis of attention mechanisms in Transformer-based models. By visualizing different operating modes of attention, we compare oracle models, which have perfect sight and are trained on noiseless visual data, to standard models that process noisy and uncertain visual input. We highlight the presence of reasoning patterns in oracle models and their absence in standard models. Based on this analysis, we propose fine-tuning the oracle model on real noisy visual input. Through the same analysis and visualization techniques, we show that attention modes absent in noisy models can be successfully transferred from oracle models to deployable models, resulting in improvements in overall accuracy and generalization. Our contributions include an in-depth analysis of reasoning patterns in Transformer-based models, a comparison between oracle and deployable models, visualization of attention modes, and the transfer of reasoning capabilities from the oracle model to state-of-the-art VQA methods with noisy input. Additionally, we demonstrate that this transfer is complementary to self-supervised large-scale pre-training.