Goal-directed action is a fundamental aspect of human behavior, and recognizing the goals behind actions is crucial for understanding and interpreting human actions. While children are able to reason about goals at a young age, machine recognition of goals remains challenging. This paper proposes that examples of failure are essential for action recognition systems to effectively discriminate between goals and actions. By considering unintentional actions, which are actions that do not achieve their intended goal, we can decouple the visible action from the latent goals and improve the accuracy of goal recognition models.The authors propose a model that leverages natural video data to learn goal-oriented video representations. They represent video as a trajectory and encode goals as the path for the trajectory. The model is trained to discriminate between successful and unsuccessful actions using an attention-based transformer architecture that captures both motion and relational features. The model is trained end-to-end and is capable of predicting goals from video data.To evaluate the effectiveness of their approach, the authors perform experiments on three goal prediction tasks. First, they detect unintentional action in video and show strong performance compared to baselines. Second, they evaluate the model's ability to predict goals with minimal supervision, such as predicting structured categories consisting of subject, action, and object triplets. Finally, they use their goal-oriented video representation to automatically correct unintentional action in videos by retrieving information from other videos or generating categorical descriptions.The main contribution of this paper is the proposed approach that learns goal-directed video representations by training on data of unintentional action. The authors demonstrate that their model is able to capture latent goals behind observed actions and achieve comparable or better performance than supervised models trained on intentional action datasets. Additionally, they introduce a method for automatically correcting unintentional action in videos. The paper concludes by discussing the details of the approach and mentioning the availability of code, data, and models for further exploration.