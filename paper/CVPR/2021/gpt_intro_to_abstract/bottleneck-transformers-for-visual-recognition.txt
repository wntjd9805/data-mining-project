Deep convolutional backbone architectures have played a crucial role in advancing image classification, object detection, and instance segmentation tasks. However, these architectures primarily rely on convolutional layers, which are effective for capturing local information but struggle with modeling long-range dependencies necessary for complex visual tasks. In natural language processing (NLP, self-attention has emerged as a powerful tool for capturing long-range dependencies in sequences. This has led to the adoption of self-attention layers in vision tasks by replacing spatial convolutional layers. One approach is to use the multi-head self-attention (MHSA) layer proposed in the Transformer model. This approach has been applied to different architecture designs, including replacing spatial convolutions in ResNet bottleneck blocks and using Transformer blocks on linear projections of non-overlapping patches. This paper introduces a hybrid architecture called Bottleneck Transformer (BoT) blocks, which combine the benefits of convolutional layers and self-attention. BoTNet models, which incorporate MHSA layers in the final bottleneck blocks, demonstrate improved performance in instance segmentation tasks, particularly on small objects. The proposed design efficiently handles large image sizes by using convolutions for downsampling and self-attention for information aggregation. BoTNet achieves competitive results on the COCO instance segmentation benchmark, outperforming prior architectures without complicated modifications. Additionally, the authors scale BoTNets, achieving high accuracy on the ImageNet validation set while maintaining faster computation times compared to popular EfficientNet models. Overall, BoTNet serves as a valuable reference backbone architecture that highlights the potential of self-attention in future vision architectures.