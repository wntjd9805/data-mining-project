Deep convolutional neural networks (CNNs) have achieved remarkable success in recent years, demonstrating state-of-the-art performance on benchmarks such as ImageNet and MS COCO. However, real-world training data often suffers from imbalanced class distributions. This imbalance, where a few classes dominate the training set while others are under-represented, leads to degraded performance when using standard training strategies. To address this issue, one solution is to augment the minority classes to obtain more training samples. Traditional data augmentation techniques, however, are limited by the small amount of training data in the minority classes.In this paper, we propose a meta semantic augmentation (MetaSAug) approach to perform effective semantic data augmentation for long-tailed problems. Our approach aims to learn more meaningful class-wise covariance to improve the augmentation procedure. We use a balanced validation set to minimize the validation loss and update the class-wise covariance at every training iteration. By optimizing the validation loss, we obtain an updated class-wise covariance matrix that contains rich semantic directions. With this improved covariance, we train models on the augmentation set with sufficient semantically augmented samples. Our approach is flexible and can be integrated with previous methods, enhancing the classification ability of focal loss and LDAM loss.We conducted extensive experiments on multiple long-tailed datasets, including CIFAR-10/100, ImageNet, and inaturalist 2017 and 2018. The results demonstrate the effectiveness of our MetaSAug approach, improving the performance of deep CNNs on long-tailed datasets.