This paper introduces a new learning method called Kernel-wise Soft Mask (KSM) to address the problem of catastrophic forgetting in Deep Neural Networks (DNNs). The KSM method combines a binary mask and a partial real-value scaling coefficient tensor to enhance the representation capacity of the network without involving additional training cost. The mask is designed in a kernel-wise manner to reduce size and improve computation efficiency. The paper also proposes using the softmax trick to improve the binary mask during training. Experimental results show that the KSM method achieves similar or better accuracy compared to existing methods like CPG and Piggyback, while maintaining similar or better training speed. Overall, the KSM method provides a novel approach to continual learning in DNNs, reducing catastrophic forgetting and improving knowledge transfer efficiency.