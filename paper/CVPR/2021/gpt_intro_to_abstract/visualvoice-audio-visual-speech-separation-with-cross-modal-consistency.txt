The paper discusses the process of automating speech separation in noisy environments by leveraging visual information to reduce audio ambiguities and focus on the active speaker. The authors propose a multi-task learning framework called VISUALVOICE, which combines audio-visual speech separation with cross-modal speaker embeddings. The framework uses facial appearance, lip motion, and vocal audio to isolate the sound track of a specific speaker. The authors highlight the mutual benefit of these two tasks, as cleaner sound separation improves the accuracy of cross-modal embeddings, and better embeddings enhance speech separation. The proposed framework achieves state-of-the-art results in audio-visual speech separation and enhancement, and the learned embeddings also improve unsupervised cross-modal speaker verification.