Deep Neural Networks (DNNs) have been found to be susceptible to adversarial examples, which are small perturbations to legitimate inputs that result in incorrect model predictions. Crafting adversarial examples has become an area of interest, as it helps identify model vulnerabilities and improve model robustness. These adversarial examples also exhibit good transferability, meaning they can fool multiple models, enabling black-box attacks without knowledge of the target model.While existing white-box attacks, where the attacker has access to the target model's architecture and parameters, have been effective, they often lack transferability, especially against models equipped with defense mechanisms. To address this, recent works have focused on enhancing the transferability of adversarial examples through advanced gradient calculations, attacking multiple models, and utilizing various input transformations. However, there is still a significant performance gap between white-box attacks and transfer-based black-box attacks.In this paper, we propose a novel variance tuning iterative gradient-based method to improve the transferability of adversarial examples. Unlike existing methods that perturb the input or accumulate a velocity vector, our method tunes the gradient with the gradient variance of the previous data point's neighborhood. This reduces variance and stabilizes the update direction, helping to escape poor local optima during the search process. Experimental results on the standard ImageNet dataset demonstrate that our method achieves significantly higher success rates for black-box models while maintaining similar success rates for white-box models.We further show the effectiveness of our method by combining variance tuning with gradient-based attacks on ensemble models and integrating these attacks with various input transformations. Extensive experiments highlight the remarkable improvement in attack transferability. Additionally, we compare our method with state-of-the-art attack methods against advanced defense mechanisms, consistently outperforming the baselines in both single model and multi-model settings.Overall, our proposed method offers a promising approach to boosting the transferability of adversarial examples, bridging the gap between white-box and black-box attacks.