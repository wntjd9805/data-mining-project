The problem of estimating per-pixel motion in video frames, known as optical flow, is a challenging task in computer vision. Optical flow systems face challenges such as occlusions, motion blur, and lack of texture. Deep learning has played a crucial role in recent research on this topic, initially for learning a data term and then for directly inferring dense optical flow fields. This has been made possible by the availability of extensive training data labeled with ground-truth optical flow fields. However, training neural networks on synthetic data alone is not sufficient for deployment in real environments due to domain shift. Fine-tuning on real imagery is usually required to achieve accuracy comparable to that on synthetic data. Obtaining ground-truth optical flow labels for real images is challenging, as there is no sensor capable of acquiring ground-truth correspondences between points in real-world scenes. To address these issues, we propose a scheme to distill proxy labels from real images for training optical flow estimation networks. Using a monocular depth estimation network, we reverse the annotation process by estimating the depth of a single image and generating a dense optical flow field through virtual camera motion. We refer to this process as Depthstillation. Our experiments on synthetic and real datasets show that training optical flow networks on unrelated images and using real images through Depthstillation improves transferability to real data. Furthermore, networks trained using Depthstillation exhibit better transferability to new real datasets compared to state-of-the-art self-supervised strategies using real videos.