Recent advances in deep generative models, including Generative Adversarial Networks (GANs), have revolutionized the creation of ultra-realistic fake videos. However, the potential misuse of these technologies for face manipulation has raised significant concerns. Such manipulations can alter a person's identity, expression, or lip movements, leading to political propaganda, defamation, or damage to trust in journalism. While deep convolutional neural networks (CNNs) trained on large-scale face forgery datasets can detect fake videos with high accuracy, they often struggle with novel manipulations and common perturbations. Various methods have been proposed to improve generalization to novel forgeries, but they still tend to overfit to seen manipulations. In this paper, we propose LipForensics, a novel approach to face forgery detection that leverages semantically high-level, spatio-temporal representations learned through the task of lipreading. By pretraining the network on a large corpus of real videos for visual speech recognition, we enable it to perceive subtle anomalies in forged oral movements. Our experiments demonstrate that LipForensics outperforms state-of-the-art methods in generalization to unseen forgeries and exhibits remarkable robustness to common corruptions. Furthermore, LipForensics can effectively learn even on heavily compressed data, unlike other detectors. We validate our design choices through ablation studies and compare with other large-scale pretraining tasks, showcasing the superiority of lipreading for achieving generalizable and robust face forgery detection.