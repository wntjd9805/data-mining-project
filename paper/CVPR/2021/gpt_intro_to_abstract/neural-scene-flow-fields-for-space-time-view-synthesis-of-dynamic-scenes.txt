Novel view synthesis has made significant progress in recent years with the use of neural networks to learn representations suitable for view synthesis tasks. However, most existing approaches assume static scenes or multiple synchronized input views, which do not align with the diverse dynamic content seen in videos shared on the internet. In this paper, we propose a new approach for novel view and time synthesis of dynamic scenes from monocular video input. This problem is highly challenging, as there can be multiple scene configurations leading to the same observed image sequences, and modeling dense 3D motion for moving objects is difficult. To address these challenges, we represent dynamic scenes as a continuous function of space and time, incorporating reï¬‚ectance, density, and 3D motion. We parameterize this function using a deep neural network and optimize it using a scene flow fields warping loss to ensure temporal consistency. Our approach is the first to achieve novel view and time synthesis of dynamic scenes captured from a monocular camera. Additionally, we introduce components that improve rendering quality by addressing scene flow ambiguity, using data-driven priors to avoid local minima, and combining static and dynamic scene representations. Our contributions include a neural representation called Neural Scene Flow Fields and a method for optimizing these fields on monocular video by leveraging multiview constraints.