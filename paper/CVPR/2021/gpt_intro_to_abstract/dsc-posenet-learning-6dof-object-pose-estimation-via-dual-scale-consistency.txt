Object pose estimation, which involves estimating the 3D orientation and translation of an object from a single RGB image, is a challenging task due to lighting changes and occlusions. Recent advancements in deep neural networks have led to the development of deep learning-based pose estimation algorithms that have shown promising performance. However, these methods often require large amounts of real images with 3D pose annotations for training, which can be difficult and time-consuming to obtain. In this paper, we propose a two-step framework for object pose estimation that leverages easily obtained 2D bounding box annotations. In the first step, we use a weakly supervised segmentation method to distinguish object pixels from background pixels. We train a segmentation network on synthetic images and use it to predict pseudo masks for unlabeled real data. By exploiting the 2D bounding boxes, we remove outliers and improve the segmentation results. In the second step, we introduce a dual-scale pose estimation network called DSC-PoseNet. This network is trained on synthetic images and uses differential EPnP and a renderer to achieve rendered object masks. The intersection over union between the segmentation results and rendered masks is utilized to train DSC-PoseNet on real images. Additionally, we resize object regions and project the estimated keypoints to improve feature extraction and enforce pose estimation consistency across scales. Our method outperforms state-of-the-art RGB-based methods without requiring real pose annotations. Overall, our contributions include proposing a weakly and self-supervised learning framework for pose estimation, introducing DSC-PoseNet for cross-scale self-supervision, and demonstrating the effectiveness of our approach in estimating 6DoF object poses from RGB images without relying on 3D pose annotations or depth images.