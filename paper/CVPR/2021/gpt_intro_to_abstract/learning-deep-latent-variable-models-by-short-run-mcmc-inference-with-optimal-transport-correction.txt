Recent advancements in computer vision have seen the successful application of deep generative models in various tasks such as image generation, image recovery, image representation, image disentanglement, and anomaly detection. These models typically involve generator networks that assume each observed example is generated by a low-dimensional vector of latent variables. However, inferring these latent variables presents challenges, often requiring Markov chain Monte Carlo (MCMC) methods to sample from the analytically intractable posterior distribution. Inference via MCMC can suffer from non-convergence and inefficiency problems, affecting model parameter estimation accuracy.To address these issues, variational inference has emerged as an attractive alternative by approximating the posterior with a tractable network. However, the variational auto-encoder (VAE) has its drawbacks, including the need to estimate additional parameters for the inference model and dependence on the accuracy of the inference model. Designing inference models for complex generator structures with multiple layers or time sequences of latent variables is challenging and may not guarantee performance.In this paper, we propose a novel approach that abandons VAE's reparameterization idea and instead utilizes MCMC-based inference for training deep latent variable models. Specifically, we employ short-run MCMC, such as short-run Langevin dynamics, for inference during training. To address the convergence issues of short-run MCMC, we introduce the use of optimal transport (OT) to correct the bias. OT can transform any probability distribution to a desired distribution with minimum transport cost, allowing us to measure the difference between two distributions.Our algorithm follows three steps: (1) inference step using short-run Langevin dynamics, (2) correction step using OT to move inferred latent vectors to the prior distribution, and (3) learning step updating model parameters based on corrected latent vectors and observed examples. The proposed algorithm offers advantages in terms of efficiency, convenience (automatic inference model without separate design and training), and accuracy (correcting non-convergent MCMC inference with OT).The contributions of this paper include the proposal of training deep latent variable models using non-convergent short-run MCMC inference with OT correction, extension of the semi-discrete OT algorithm for one-to-one mapping between inferred latent vectors and samples from prior distribution, and empirical results demonstrating the effectiveness of this strategy.