Video question answering (VideoQA) is a challenging task that requires answering questions about a given video using natural language. In recent years, several methods have focused on using spatio-temporal visual representations combined with linguistic cues for VideoQA. However, due to the dynamic spatiotemporal dependencies of videos and the sophisticated compositional semantics of questions, VideoQA remains a challenging problem. Previous works have adopted an encoder-decoder structure with LSTM-based encoders to encode video frames and questions, and attention mechanisms to provide answers. While these methods have shown promising results in learning temporal and spatial relations, they still struggle with capturing sophisticated appearance-motion or visual-question relations. In contrast, there have been approaches proposed in vision-language interaction tasks that aim to understand cross-modal relationships. These approaches use global or local representations of visual and textual information to match images or videos with text. Some of these methods have also been adopted in VideoQA, but they often rely on learning positional relationships rather than capturing deeper semantic meanings. This leads to challenges in capturing fine-grained video-word relations and can result in incorrect associations between words and visual elements. To address these limitations, structured representations that consider grammatical dependencies between sentence words have been suggested. These structured representations have been extensively explored in image-text matching tasks, but their application in VideoQA remains underexplored. In this paper, a novel method called Bridge to Answer is proposed to model semantic relations between appearance, motion, and questions through structure-aware interaction. Unlike existing approaches, Bridge to Answer constructs appearance, motion, and question graphs to represent visual and semantic relations between nodes. It leverages question-to-video interactions to propagate question nodes to relevant visual nodes and learn question-conditioned visual representations. Additionally, visual-to-visual interactions are applied to model appearance-motion relations using the question graph as an intermediate bridge. The proposed method is evaluated extensively on three datasets and compared with state-of-the-art methods, demonstrating its capability in VideoQA.