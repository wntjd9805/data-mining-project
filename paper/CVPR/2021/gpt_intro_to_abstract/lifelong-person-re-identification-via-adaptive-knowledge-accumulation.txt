Person re-identification (ReID) is a challenging task that involves linking the same pedestrian across different camera views. While deep learning methods have shown promising results in ReID, their training process is hindered by fixed and stationary datasets, which limits their applicability to scenarios where data is continuously increasing from different domains. To address this limitation, we propose a new task called lifelong person re-identification (LReID), which requires models to incrementally accumulate knowledge from seen domains and adapt it to both seen and unseen domains. LReID presents two main challenges: improving generalization on unseen classes and handling fine-grained appearance variations. To overcome these challenges, we propose the Adaptive Knowledge Accumulation (AKA) framework, which leverages the representation and operation capabilities inspired by human cognitive processes. AKA represents transferable knowledge as a knowledge graph and updates it using graph convolution, enabling the transfer of previous knowledge to new samples. Additionally, AKA employs plasticity loss and stability loss to balance the improvement of learned representations while considering the forgetting problem. Our contributions include the introduction of the LReID task, the AKA framework, and a benchmark dataset and evaluation protocols for LReID. Experimental results demonstrate the effectiveness of AKA compared to state-of-the-art methods in lifelong person re-identification.