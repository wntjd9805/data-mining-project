View synthesis and scene reconstruction are key challenges in computer graphics and computer vision. Traditional methods for addressing these problems involve sequential reconstructions and rendering pipelines that recover a compact scene representation, such as textured meshes or point clouds, and use it to render new views. However, these traditional pipelines struggle with capturing view-dependent features at discontinuities and illumination-dependent reflectance of scene objects. In recent years, neural rendering methods have emerged as a solution to these challenges. These methods depart from explicit scene representations and instead learn fully implicit representations using neural networks. One popular method is Neural Radiance Fields (NeRF), which encodes scene representations within the weights of a neural network. Despite their success, existing neural rendering approaches assume that training images come from a static scene. This work introduces a novel method capable of learning representations for complex, dynamic, multi-object scenes. The proposed approach decomposes scenes into static and dynamic parts and learns their representations structured as a scene graph. The method is able to synthesize novel views of scenes and render views for unseen arrangements of dynamic objects. Furthermore, it can also be used for 3D object detection through inverse rendering. Experiments with automotive datasets demonstrate the method's capability to represent scenes with highly dynamic objects and generate novel views of unseen scene compositions. The contributions of this work include the proposal of a neural rendering method that decomposes dynamic scenes into a scene graph, the learning of object representations from video frames and tracking data, validation through rendering of unseen views and dynamic scene arrangements, and the facilitation of 3D object detection through inverse rendering.