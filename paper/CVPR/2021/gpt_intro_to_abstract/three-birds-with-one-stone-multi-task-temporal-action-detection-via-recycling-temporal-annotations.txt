This paper introduces a novel multi-task learning algorithm for temporal action detection, which aims to recognize and localize actions in continuous videos. The proposed algorithm addresses the problem of labeled training data shortages by leveraging limited supervision information and constructing two auxiliary tasks. These tasks generate their own supervision information by recycling the given temporal annotations. Unlike traditional multi-task learning approaches, the focus is solely on improving the performance of temporal action detection. The proposed algorithm is evaluated on benchmark datasets, and the experimental results confirm its effectiveness in improving temporal action detection. The contributions of this paper include the proposal of a multi-task temporal action detection algorithm that mitigates the label shortage problem, the construction of two auxiliary tasks to improve temporal action localization, and extensive experiments on benchmark datasets to demonstrate the performance improvement achieved by the proposed method.