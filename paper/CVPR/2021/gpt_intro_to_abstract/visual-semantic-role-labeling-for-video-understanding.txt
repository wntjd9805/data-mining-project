Videos capture a wide range of events and interactions involving humans, objects, and the environment. Extracting useful information from videos has numerous applications, including video description, query answering, content retrieval, knowledge graph construction, and training embodied agents. While there has been significant research in parsing video content for tasks such as action classification, localization, and spatio-temporal detection, these approaches often overlook crucial details such as the agent, object, tool, location, and temporal relationships involved in the actions. Expository tasks like video captioning and storytelling provide a more holistic understanding of video content but lack clear definitions for evaluating the extracted information. Inspired by recent work in visual semantic role labeling for images, this paper introduces VidSRL, a task aimed at recognizing spatio-temporal situations in video content. VidSRL involves identifying and temporally localizing salient events, identifying the actors, objects, and locations involved, co-referencing these entities across events, and understanding the temporal relationships between events. To support research on VidSRL, the paper presents the VidSitu dataset, consisting of over 29k videos annotated with verbs and roles. The dataset also includes event relation annotations to capture causation and contingency between events. Key highlights of VidSitu include its vocabulary of verbs and entities, complex situations with inter-related events, and rich annotations with co-referencing and event-relation labels. The paper provides a benchmark for evaluating various capabilities required for solving VidSRL, including baselines using state-of-the-art components. The authors also highlight the need for improvement in achieving competency on the VidSitu benchmark. The contributions of this paper include the formalism of the VidSRL task, the curated VidSitu dataset, and an evaluation methodology with baselines. The dataset and code are publicly available for further research.