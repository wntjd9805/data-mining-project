Face editing, the process of changing specific attributes or regions of a facial image while keeping other attributes unchanged, has seen significant progress with the development of Generative Adversarial Networks (GANs). These recent advancements in face editing leverage conditional GANs and use cycle consistency to preserve non-edited areas. However, even with cycle consistency, the generated images may still lose rich details and appear blurry. This paper focuses on addressing this problem by proposing a new face editing approach called HifaFace. The approach tackles the issue from two perspectives: by directly incorporating high-frequency information of the input image to the generator and by introducing an additional discriminator to encourage the synthesis of rich details. The HifaFace method utilizes wavelet transformation to extract high-frequency information and uses a novel Wavelet-based Skip-Connection module instead of the original Skip-Connection. Additionally, the paper introduces a new loss function called the attribute regression loss for fine-grained and wider-range control over facial attributes. The method also effectively utilizes large amounts of unlabeled face images for training, leading to high-fidelity and controllable face editing results. The contributions of this work include the proposal of the HifaFace method, the analysis and solution to the steganography problem in face editing, and the demonstration of the effectiveness of the proposed framework through qualitative and quantitative results.