Unsupervised domain adaptation (UDA) is a subfield of semi-supervised learning that deals with the challenge of using unlabelled data from a different distribution than the annotated dataset. This can be particularly useful in scenarios where synthetic data with readily available annotation is used instead of real-world images, which are expensive to label. Techniques for UDA in semantic segmentation have shown promise, but the increasing complexity of these methods, often involving style transfer networks, adversarial training, and network ensembles, limits reproducibility and hinders further progress.In this paper, we propose a UDA framework that achieves state-of-the-art segmentation accuracy without requiring substantial training efforts. Our approach adopts a simple semi-supervised technique called self-training, which has been previously used in conjunction with adversarial training or network ensembles. However, we demonstrate that self-training can achieve competitive results on its own. Unlike other self-training methods that require multiple training rounds and expert intervention, our approach trains the model using co-evolving pseudo labels without the need for multiple interventions.We also leverage ubiquitous data augmentation techniques from fully supervised learning, including photometric jitter, flipping, and multi-scale cropping, to enforce consistency in the semantic maps produced by the model across various image perturbations. We formalize the key premise of our approach as an assumption that the pixelwise mapping from images to semantic output is invariant under photometric and spatial transformations.To provide stable targets for model updates, we introduce a training framework that incorporates a momentum network, which is a slowly advancing copy of the original model. This allows for more recent supervision compared to fixed supervision in model distillation. Furthermore, we address the problem of long-tail recognition in the context of generating pseudo labels for self-supervision. We maintain an exponentially moving class prior that discounts confidence thresholds for classes with few samples and increases their contribution to the training loss.Our framework is simple to train and adds moderate computational overhead compared to fully supervised setups. However, experimental results demonstrate that it achieves state-of-the-art performance on established benchmarks for semantic segmentation, surpassing other methods that rely on adversarial training and multiple training rounds.