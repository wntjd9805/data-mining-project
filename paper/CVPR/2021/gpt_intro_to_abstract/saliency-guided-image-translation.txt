Conditional image generation using GAN-based and VAE-based generative methods has been successful in producing high-quality images from various conditioning information. Image-to-image translation, a sub-class of these methods, focuses on generating images that are similar in structure to the original inputs but differ in style or texture detail. However, current methods do not allow for abstract modifications that alter the perception of the original image. This paper introduces the task of saliency-guided image translation, which aims to perform image-to-image translation based on user-specified target saliency maps. The proposed method, SalG-GAN, uses a GAN-based model with a disentangled representation framework to address the challenges of saliency ambiguity. A saliency-based attention module is also introduced to facilitate the generation process. The paper includes the development of a synthetic dataset and a real-world dataset labeled with attention information. Experimental results demonstrate the effectiveness of the proposed method for saliency-guided image generation.