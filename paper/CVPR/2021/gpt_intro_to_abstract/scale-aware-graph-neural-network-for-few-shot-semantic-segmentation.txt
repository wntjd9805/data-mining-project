Deep convolutional neural networks (CNNs) have significantly improved the development of various vision tasks, including semantic segmentation. However, training effective segmentation models requires large amounts of pixel-wise image annotations, which are expensive and time-consuming to acquire. Weakly supervised learning has been proposed to alleviate annotation costs, but it often leads to a significant drop in model performance. Additionally, both fully and weakly supervised models struggle with generalization to unseen domains with limited training data. Few-shot semantic segmentation (FSS) aims to address the challenges of unseen object segmentation and data annotation by using only a few annotated support images to segment the foreground object of an unseen class in a query image. In FSS, a base training set with annotations from different object categories can be used to assist in the learning of the model. Most existing FSS models adopt a two-stream network with global or multiple local prototypes calculated from the support branch and applied to the query feature map. However, these models struggle to capture object variations and often suffer from mis-segmentation. In this paper, we propose an end-to-end scale-aware graph neural network (SAGNN) for FSS. SAGNN explicitly and comprehensively reasons the high-order appearance relationships across multi-scale support-query features, modeling object variations in a structure-to-structure manner. SAGNN builds a scale-aware graph and encourages information exchange among support-query pairs to capture cross-scale relationships. We also introduce a self-node collaboration mechanism for enriching node features during feature aggregation. Our SAGNN achieves state-of-the-art results on two FSS benchmarks and sheds light on future research in this field.