Large deep neural networks have revolutionized various domains such as computer vision, speech recognition, and reinforcement learning by efficiently learning complex patterns from data. However, the deployment of these algorithms in critical application areas like healthcare and face recognition raises concerns about privacy and fairness. There may be sensitive information in the learned representations that the user wants to keep private but may inadvertently encode. For instance, in a scenario where citizens consent to the use of face recognition for identifying criminals, a malicious adversary could intercept the feature representations to reconstruct the face image or extract personal attributes without the citizens' consent. The goal of this research is to explore methods of improving privacy of sensitive information while preserving utility.Previous research in privacy-aware machine learning has mainly focused on protecting training data from attacks such as membership inference and model inversion. However, these methods are not easily scalable for deployment during inference due to computational limitations, intellectual property concerns, and the need for secure environments. This work proposes a collaborative inference approach where the inference network is distributed between client devices and a server, communicating via split activations. To ensure privacy, explicit security measures need to be encoded in the intermediate activations.While previous works have explored attribute leakage and adversarial representation learning, this research aims to selectively remove redundant features in the latent space to protect sensitive information and achieve a better privacy-utility trade-off. The proposed method, called DISCO, learns a dynamic and data-driven pruning filter to obfuscate sensitive information in the feature space. DISCO is evaluated and compared with baseline methods using various attack schemes on inputs and attributes. The results demonstrate that DISCO consistently outperforms existing state-of-the-art methods in terms of privacy and utility.The contributions of this work include the introduction of DISCO as a dynamic scheme for obfuscating sensitive channels in collaborative inference, providing a steerable and transferable privacy-utility trade-off. Diverse attack schemes for sensitive inputs and attributes are also proposed, leading to significant performance gains. To encourage further research in private collaborative inference, a benchmark dataset of one million sensitive representations is released.