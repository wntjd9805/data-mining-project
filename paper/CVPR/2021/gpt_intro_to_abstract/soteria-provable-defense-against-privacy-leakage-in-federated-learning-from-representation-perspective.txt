Abstract:Federated learning (FL) is a distributed learning approach that enables multiple devices to train a shared model without transferring their local data. While FL is motivated by privacy preservation, recent research has shown that sharing model updates can make FL vulnerable to inference attacks. Existing defense strategies for FL incur computational overheads or accuracy loss and are not specifically designed to address privacy leakage from communicated local updates. This paper investigates the essential causes of privacy leakage in FL and proposes a defense strategy called Soteria. The key observation is that class-wise data representations are embedded in shared local model updates, which can be inferred to perform model inversion attacks. The paper analyzes the embedding of data representations in model updates and presents an algorithm for inferring class-wise data representations. Empirical studies demonstrate a high correlation between the inferred and real data representations and validate that representation leakage is the primary cause of existing attacks. Soteria utilizes a perturbation algorithm to generate data representations that are similar to the true representations for FL performance but dissimilar to the original data for improved privacy. The defense strategy is evaluated on MNIST and CIFAR10 datasets, showing significant improvement in privacy without sacrificing accuracy. The contributions of this work include the explicit identification of data representations in model updates as the cause of privacy leakage, the development of an algorithm for reconstructing data from local updates, and the proposed defense strategy with certified robustness and convergence guarantees.