Object detection with transformers (DETR) is a framework that approaches object detection as a prediction problem using a transformer encoder-decoder. Unlike traditional methods, DETR does not rely on hand-designed sample selection and non-maximum suppression, yet achieves competitive performance with Faster R-CNN. However, DETR faces challenges in training and optimization, requiring large-scale training data and long training schedules. This paper proposes Unsupervised Pre-training DETR (UP-DETR) to address these challenges. UP-DETR aims to pre-train the transformers of DETR on a large-scale dataset using a novel unsupervised pretext task called random query patch detection. During pre-training, two critical issues are addressed: multi-task learning and multi-query localization. Experimental results show that UP-DETR outperforms DETR in object detection tasks, demonstrating faster convergence and better average precision. UP-DETR also achieves state-of-the-art performance in one-shot detection and panoptic segmentation. Ablation studies reveal that freezing the pre-training CNN backbone is crucial for preserving feature discrimination during pre-training.