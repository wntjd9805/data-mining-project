Visual representation learning is crucial for various computer vision tasks, such as image classification, tagging, object detection, and semantic and instance segmentation. While supervised pre-training over large-scale datasets yields useful visual features, fine-grained class labeling efforts are heavy. Self-supervised learning methods do not require annotations but still require large training sets or longer training epochs. In addition to labels, image data often comes with additional information like tags and captions, which offer higher-level abstract concepts and the potential for drawing connections across modalities.The objective of this paper is to learn visual representation from multimodal data in a unified training framework. The framework aims to fully exploit the potential of each unlabeled modality in a self-supervised manner, bridge the heterogeneity gap by comparing different modalities in a common semantic space, and easily incorporate new modalities. The goal is to learn high-quality visual features that benefit from cross-modal correlation modeling and intrinsic data properties.Previous methods have focused on generating visual representations from multimodal data. However, they typically do not fully exploit the potential within individual modalities. In this paper, a new visual representation learning framework is proposed that takes into account both intra-modal and inter-modal similarity preservation. This framework uses contrastive losses in multiple training paths to adjust features in all modalities. The contributions of this paper include a unified multi-modal training framework that can exploit intrinsic data properties and extract semantic information, as well as broad transferability of pre-trained visual representations to downstream computer vision tasks.Experimental comparisons are performed between supervised, self-supervised, and learning from text methods on various computer vision tasks to demonstrate the advantages of the proposed framework. The results show excellent performance in classification, tagging, cross-modal retrieval, object detection, and instance segmentation.