Temporal action localization (TAL) is a task in video understanding that aims to localize temporal boundaries of specific action categories in untrimmed videos. This task has attracted increasing attention due to its broad applications in various high-level tasks. Deep learning based methods have made impressive progress in this area, but most of them rely on fully supervised approaches, which require extensive manual annotations. To address this limitation, weakly supervised methods that only require video-level category labels have been proposed. These methods treat videos as bags of multiple segments and use a video-level classifier to obtain a class activation sequence (CAS). However, there are two primary challenges in weakly supervised TAL: localization completeness and background interference. Previous works have attempted to solve these challenges through erasing strategies, multi-branch architectures, and attention-based per-class feature aggregation schemes. In this paper, we propose a novel end-to-end framework called Action Unit Memory Network (AUMN) to address these challenges. AUMN leverages the characteristics of sharing units, sparsity, and smoothness in action localization and incorporates diverse mechanisms for memory updating. We introduce action unit templates as a memory bank and use a multi-layer perceptron network to embed these templates into the action class space. A cross-attention module and a self-attention module are used to compute relationships between video segments and templates, as well as between different segments in a video for aggregating context information. Additionally, we design three effective mechanisms to guide the updating of the memory bank based on the properties of action units and sparsity. Experimental results on challenging benchmarks demonstrate that AUMN outperforms state-of-the-art weakly supervised TAL methods. This work contributes to the modeling of action units with a memory network and proposes attention modules and mechanisms for adaptive memory updating in weakly supervised TAL.