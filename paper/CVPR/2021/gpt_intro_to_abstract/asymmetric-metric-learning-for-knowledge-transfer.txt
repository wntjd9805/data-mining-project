Loss functions based on pairwise distances or similarities play a crucial role in representation learning, particularly in tasks such as fine-grained classification, few-shot learning, local descriptor learning, and instance-level retrieval. These loss functions have also been used without supervision and have formed the basis for unsupervised representation learning. However, powerful network models used to generate these representations are often computationally expensive. As a result, there has been a push towards designing lightweight networks for mobile devices and exploring knowledge transfer from larger networks to smaller ones. Most of the existing research focuses on knowledge transfer in classification tasks using standard cross-entropy.In this work, we concentrate on the task of instance-level image retrieval, which relies on pairwise distances or similarities. We address the scenario where a database of images is represented and indexed by a large model, while queries are captured from mobile devices using a smaller model. Our motivation is to enable knowledge transfer from the larger model to the smaller one, ensuring that the student model can accurately map inputs to the same representation space. We refer to this task as asymmetric testing.Furthermore, we introduce a novel paradigm called asymmetric metric learning, which combines metric learning and knowledge transfer. In this approach, anchors are represented by the student model, while positives and negatives are represented by the teacher model. By applying any metric learning loss function, we achieve both metric learning and knowledge transfer without the need for a linear combination of two loss functions.The contributions of this work include the exploration of knowledge transfer in pair-based metric learning for instance-level image retrieval, the study of the asymmetric testing task, and the systematic evaluation of different teacher and student models, metric learning loss functions, and knowledge transfer loss functions. We also serve as a benchmark for future research in this area.Overall, our work provides insights into the use of loss functions based on pairwise distances or similarities in representation learning and highlights the potential of knowledge transfer in improving the efficiency and effectiveness of smaller network models in instance-level image retrieval.