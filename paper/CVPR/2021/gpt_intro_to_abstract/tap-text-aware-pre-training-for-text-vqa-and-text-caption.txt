This paper introduces the challenges posed by vision-language tasks that involve scene text, such as Text-VQA and Text-Caption, which require reading and understanding scene text in image context. These tasks have various applications, including robotics and assisting visually-impaired individuals. The typical framework for Text-VQA/Text-Caption consists of a feature encoder for each modality, a multi-modal fusion module, and a decoding module. Previous models have focused on optimizing for the correct answer/caption but lack a good joint representation among text word, visual object, and scene text. Inspired by the success of Vision-Language Pre-training (VLP) in image-text joint representation learning, this paper proposes Text-Aware Pre-training (TAP) to incorporate scene text in pre-training and improve Text-VQA/Text-Caption. TAP leverages effective network designs and introduces text-aware pre-training tasks to fuse scene text with text words and visual objects. The paper also introduces a "relative position prediction" task to learn the spatial relationships between scene text and object regions. The experiments demonstrate that TAP significantly outperforms previous methods and achieves state-of-the-art results on Text-VQA and Text-Caption tasks. Additionally, a large-scale dataset named OCR-CC is built for pre-training, leading to further improvement on the tasks. The contributions of this paper include being the first to explore pre-training for Text-VQA and Text-Caption, the development of TAP to learn a better aligned representation, and the construction of OCR-CC dataset for scene text-related image-text pairs.