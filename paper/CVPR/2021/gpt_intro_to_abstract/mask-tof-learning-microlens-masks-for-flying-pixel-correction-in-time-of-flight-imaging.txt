Large-scale image datasets and advancements in computational resources have transformed the field of image processing. Similarly, the acquisition of high-quality depth maps has become crucial for 3D graphics and vision applications. However, depth datasets obtained from depth-capturing devices are prone to errors that can negatively impact performance and generalizability. Passive sensing approaches use parallax cues to infer distances from monocular or multiview images. While these methods can utilize standard RGB cameras, they struggle with textureless regions and complex geometries. Active sensing approaches, on the other hand, involve sending out known illumination into the scene to reconstruct depth. This includes structured light methods like active stereo, as well as time-of-flight (ToF) approaches such as LiDAR. Amplitude-modulated continuous wave (AMCW) ToF cameras flood-illuminate a scene with periodic amplitude-modulated light and estimate the phase shift of the returned light to infer depth. These cameras offer complete depth maps at a high framerate, making them compact and affordable. However, they still face limitations such as noise, phase wrapping, multipath interference, and flying pixels. Of particular concern are flying pixels, which occur when light from the object and its background reaches the same sensor pixel, resulting in a mixed depth measurement. Rectifying these flying pixels has proven to be challenging and often leads to image artifacts. In this paper, we propose a method called Mask-ToF. We learn a microlens amplitude mask that allows for per-pixel aperture configurations with varying susceptibility to noise and flying pixels. We train an encoder-decoder network to aggregate this spatial information and refine depth estimates. The loss of the network is backpropagated to learn high-level mask patterns. We validate Mask-ToF using real-world data by manufacturing the learned mask and placing it on the camera sensor. Our contributions include developing a differentiable AMCW ToF image formation model, incorporating sub-aperture masking and a refinement network, and learning an optimal mask structure through a patch-based gradient descent approach. We evaluate the masks in simulation and assess their effectiveness in reducing overall error and flying pixels.