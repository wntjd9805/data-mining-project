Semi-supervised learning (SSL) has been successful in improving model performance on image classification benchmarks. However, the assumption of balanced class distributions in SSL benchmark datasets often does not hold in realistic scenarios, leading to poor SSL performance. While supervised learning on imbalanced data has been extensively studied, SSL on imbalanced data remains understudied. This introduces further challenges in SSL as the missing label information precludes rebalancing the unlabeled set. Pseudo-labels generated by models trained on imbalanced data can intensify the bias and deteriorate the model quality. Existing SSL algorithms have not been thoroughly evaluated on imbalanced data distributions. In this work, we address SSL in the context of class-imbalanced data where both labeled and unlabeled sets have the same imbalanced class distributions. We observe that existing SSL algorithms perform poorly on imbalanced data due to low recall on minority classes, while maintaining high precision on minority classes. Motivated by this observation, we propose a class-rebalancing self-training scheme (CReST) that samples pseudo-labeled data from the unlabeled set to supplement the original labeled set. CReST uses a stochastic update strategy that selects samples with a higher probability if they are predicted as minority classes. Furthermore, we extend CReST to CReST+ by incorporating distribution alignment to adjust predicted data distributions and alleviate model bias. Experimental results on CIFAR-LT and ImageNet127 datasets show that CReST and CReST+ outperform baseline SSL methods by a large margin, improving accuracy and recall on minority classes. Our method also outperforms existing SSL algorithms designed for imbalanced data. Ablation studies further demonstrate the efficacy of our method. Overall, our proposed approach provides a viable solution for SSL on imbalanced data.