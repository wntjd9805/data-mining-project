This paper explores the connection between auditory and visual experiences in immersive environments. It focuses on generating stereophonic audios with visual guidance to enhance the user experience in multimedia products. Previous studies have used supervised learning methods for this purpose, but these methods face challenges in real-world applications due to the need for high-quality stereo data and limited generalization capability. To address these challenges, the authors propose a novel pipeline called PseudoBinaural that generates visually coherent binaural audios without recorded binaural data. The key insight is to construct pseudo visual-stereo pairs from mono data. The authors introduce two mappings: a Mono-Binaural-Mapping for reproducing binaural audios from mono audio sources, and a Visual-Coordinate-Mapping that associates visual cues with spatial locations.The Mono-Binaural-Mapping uses spherical harmonic decomposition and head-related impulse responses (HRIR) to render binaural audios from mono sources. The Visual-Coordinate-Mapping establishes a correspondence between pixel coordinates and spherical coordinates to manipulate visual content in relation to the source direction. Existing models for visually informed binaural audio generation can be trained on the generated pseudo visual-stereo pairs.The authors also propose leveraging audio-visual source separation tasks to further enhance the training process. The trained models are then applied to videos with mono audios to generate corresponding binaural audios. The proposed framework is evaluated on two datasets and in real-world scenarios, demonstrating stable performance.The contributions of this work include: (1) identifying the mapping between source directions and binaural audios through theoretical analysis, (2) generating pseudo visual-stereo pairs for model training without recorded binaural data, and (3) validating the effectiveness and stability of the proposed method through extensive experiments. Additionally, the pseudo visual-stereo data can be used as strong augmentation in the supervised setting by combining it with real stereophonic recordings.