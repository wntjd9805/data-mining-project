Generative adversarial networks (GANs) are widely used in computer vision tasks such as image generation and editing. However, deploying GANs on edge devices like mobile phones is challenging due to their large storage requirements, high computational cost, and memory utilization. While network compression techniques have been successful in reducing the size of classification models, they are not directly applicable to GANs. Specialized GAN compression mechanisms have been proposed, but they mainly target conditional GANs and not unconditional GANs. Additionally, existing methods do not take into account the spatial correlation and semantic content of GAN-generated images. In this paper, we propose novel approaches to effectively compress unconditional GANs. We introduce a pruning metric to remove redundant channels and explore different distillation losses. Unlike previous methods, we combine norm-based and perceptual losses for knowledge distillation, resulting in improved compression results. We also leverage the semantic contents in generated images to guide the compression process. Specifically, we use a content-parsing network to identify contents of interest (COI) and design a content-aware pruning metric and distillation scheme based on COI. Our approach achieves better quantitative measurements and visual quality compared to existing methods. Additionally, our compressed models show improved generation quality and computational acceleration. Our contributions include the development of a new framework for unconditional GAN compression, the proposal of a content-aware compression paradigm, and significant advancements in image generation, embedding, and editing on SN-GAN and Style-GAN2. Furthermore, our compressed models offer a better resource-performance tradeoff and a smoother latent space manifold, which benefits image editing tasks.