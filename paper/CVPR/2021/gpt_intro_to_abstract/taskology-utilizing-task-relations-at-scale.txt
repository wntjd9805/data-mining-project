Many computer vision tasks, such as depth and surface normal estimation, flow prediction, pose estimation, semantic segmentation, and classification, are inherently related as they describe different aspects of the same scene. While specialized methods are often developed for each individual task, there has been research aimed at uncovering the relationships between these tasks. However, only a few methods have exploited these relationships effectively. Some approaches utilize deep networks to learn explicit mappings between tasks, but this may lead to inconsistencies across multiple tasks. Alternatively, multi-task learning involves training multiple tasks jointly, which increases the coherency between them and enables self-supervision. However, joint training has its own challenges, such as designing and maintaining a single model for multiple tasks, merging different modalities into a single model, and processing the model efficiently. In this paper, we introduce a novel approach called "Taskology" for distributed collective training that leverages the connections between multiple tasks. We design consistency losses for related tasks to enforce their logical or geometric structure. By explicitly enforcing consistency between tasks, we improve their individual performance and establish the correspondence among them, leading to a more comprehensive understanding of the scene. Our framework allows for modular design, where each task is trained with a separate network better suited for that task. This offers advantages in terms of design, development, and maintainability. Additionally, we can leverage unsupervised or partially labeled data using consistency losses, enabling training with limited labeled data. Furthermore, our framework supports distributed training on multiple compute nodes, where each network is processed independently but tied together through consistency losses. Our experiments show that networks for different tasks can be trained with stale predictions from their peers without a decrease in performance. Unlike existing distributed training methods, our framework separates training at the task level, allowing for scalability in the number of tasks and dataset sizes.In summary, the contributions of this paper are: 1. Introducing a framework for modular design and collective training of neural networks for multiple tasks2. Proposing consistency losses for coherently training multiple tasks jointly, improving their overall performance3. Demonstrating distributed training of multiple tasks, allowing for scalability4. Showing that collectively trained tasks can self-supervise, reducing the need for labeled data and leveraging unsupervised or simulated data.