The task of generating novel-view images from a limited number of captured viewpoints, referred to as "NVS", is challenging when the underlying 3D geometry is not available and occlusion and visibility are hard to infer. Conventional image-based rendering methods rely on multi-view stereo algorithms to reconstruct geometry and generate new views, but this separation can lead to inaccuracies. To address these challenges, we propose an end-to-end framework that estimates target-view depth and source-view visibility directly from source view images. We construct a volume under the target view camera frustum and use a source-view visibility estimation module to determine voxel visibility. We then aggregate the estimated visibility to create a consensus volume representing surface existence probabilities. A soft ray-casting mechanism is used to find the most front surfaces (depth), and source view pixels are warped and aggregated based on the estimated depth and visibility to generate the novel view. A refinement network is employed to reduce artifacts and synthesize realistic images. Since explicit supervision on depth and visibility is not available, the training signals are provided implicitly by the final image synthesis error.