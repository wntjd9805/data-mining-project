Deep learning methods have been vulnerable to data perturbations, especially in safety and security, as these perturbations are imperceptible to humans but destructive to machine intelligence. While attacks on static data have been widely studied, attacks on time-series data, specifically skeletal motions, have only recently been explored. Skeletal motion is important in activity recognition as it increases robustness. Previous methods for attack on skeletal motion have been primarily white-box methods, but we propose the first black-box attack method called BASAR. Skeletal motion has unique features that make it different from other data. It has low dimensionality and dynamics that are well-recognized by human perceptual systems. To reconcile conflicts between the complexities of classification boundaries and the data manifold, we propose the guided manifold walk method. We extensively evaluate BASAR on various datasets and compare it with other methods, showing its effectiveness and the existence of on-manifold adversarial samples. Our research demonstrates that adversarial attack is a genuine threat to skeleton-based activity recognition, and we provide key insights on classifiers' resistance to on-manifold adversarial samples.