Creating realistic and articulated human avatars is a challenging task with applications in telepresence, gaming, augmented reality, and virtual reality. Existing models have successfully captured the shape of the naked body, including facial and hand deformations, using mesh geometry and datasets of body scans. However, modeling clothing and hair adds complexity and requires scarce and hard-to-obtain 3D data. In this paper, we propose a new approach called neural dressing for creating 3D realistic full-body avatars from videos or a few images. Our approach combines deformable mesh models with a multi-channel neural texture to model the appearance of avatars with clothing and hair. We also develop a generative model of full-body avatars using a generative network for neural body texture derived from StyleGANv2. The complete model incorporates neural texture synthesis, mesh rendering, and neural rendering in a joint generation process trained adversarially on a large-scale dataset. We address the need for consistent appearance across poses and camera positions by adding an additional discriminator network and modifying the training process. By using our generative model, we can sample new realistic 3D artificial humans and create avatars for existing people by fitting the model to a single image or a few images. We also investigate regularization techniques for the over-parameterized generative model and compare our approach with previous few-shot avatar modeling methods.