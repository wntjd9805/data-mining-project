Deep learning has achieved remarkable success in machine learning tasks, surpassing human performance in certain cases. However, vulnerabilities in these systems, such as susceptibility to imperceptible perturbations during testing, have been discovered. This has led to the emergence of adversarial machine learning, which aims to make deep neural networks robust to test-time perturbations. Numerous defenses and attack methods have been proposed in this field. One widely studied setting is white box attacks under â„“p norm perturbations, where the adversary has complete knowledge of the neural network and can perturb the input within a specified perturbation bound. Current defenses focus on optimizing a robust objective to minimize the adversarial loss. However, existing approaches have limitations in terms of the choice of representation and the perturbation model. They often restrict perturbations to the pixel representation of the input, ignoring other natural representations such as the DCT basis for images. In addition, current approaches are designed for specific attack models and may not be robust to other types of attacks. This paper addresses these limitations by proposing a study of adversarial robustness in multiple representation spaces and under multiple attack models. The authors present a min-max formulation and apply the multiplicative weights update method from online learning theory to design an algorithm with theoretical guarantees. They further extend their algorithm to a practical implementation that can scale to multiple representation spaces and attack models. The effectiveness of the proposed algorithm is demonstrated through experiments on image classification tasks using the MNIST and CIFAR-10 datasets.