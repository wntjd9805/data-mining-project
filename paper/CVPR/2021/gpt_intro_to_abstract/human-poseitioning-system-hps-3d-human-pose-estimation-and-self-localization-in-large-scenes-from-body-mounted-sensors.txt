Capturing the full 3D pose of a human, along with localizing and registering it within a 3D reconstruction of the environment, using only wearable sensors, has the potential to revolutionize various applications and research in computer science. This technology would enable users of Augmented / Mixed / Virtual Reality to freely move and interact with virtual objects without the need for external cameras. Additionally, it would allow for the training of digital humans that can plan and move realistically based on visual data received through their eyes. Traditional approaches in computer vision often analyze humans from an external third-person camera, neglecting scene context and imposing limits on recording volume and duration. In this paper, we introduce a method called Human POSEitioning System (HPS), which is the first approach to recover the full body 3D pose of a human registered with a large 3D scan of the environment using only wearable sensors. HPS combines information from body-mounted IMUs and a head-mounted camera to approximate the visual field of view of the human. By placing the camera towards the scene rather than the body, we can capture both the human's 3D pose and their visual observations, posing new challenges for pose estimation. To overcome the limitations of pure IMU-based tracking and camera localization, HPS integrates the two sources of information to remove drift and recover the human trajectory. Additionally, scene constraints are incorporated when foot contact is detected. The capabilities of HPS are demonstrated through the capture of a dataset consisting of real people moving in large scenes, showcasing various activities. This dataset can be used as a testbed for ego-centric tracking with scene constraints and to understand how humans interact and move within large scenes over extended periods. The contributions of this paper include the introduction of HPS as the first approach to estimate full 3D human pose while localizing within a pre-scanned large 3D scene using wearable sensors, the introduction of a joint optimization framework that integrates camera localization, IMU-based tracking, and scene constraints to achieve accurate and smooth motion estimates, and the provision of the HPS dataset for further research in 3D human motion and behavior modeling. Overall, HPS and the accompanying dataset represent a significant step towards developing future algorithms for understanding and modeling 3D human motion and behavior within a 3D environment from an egocentric or third-person perspective.