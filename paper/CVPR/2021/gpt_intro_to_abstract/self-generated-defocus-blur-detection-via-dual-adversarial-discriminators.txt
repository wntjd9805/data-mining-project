Defocus blur detection (DBD) is a crucial task in computer vision with applications in various fields. Deep convolutional neural networks (CNNs)-based DBD methods have achieved high performance but require extensive manual annotation. To address this limitation, we propose an unsupervised learning framework for DBD by leveraging unsupervised segmentation tasks such as saliency object detection. We introduce a principle that allows defocus blur regions to be moved relative to realistic blurred images without affecting the overall blur judgment. Similarly, focused clear regions can be pasted randomly onto realistic clear images. We build a generative network that outputs a DBD mask without ground truth supervision and use discriminative networks to distinguish realistic clear and blurred composite images. To ensure accurate DBD mask generation, we employ dual adversarial discriminative networks and propose a bilateral triplet-excavating constraint to balance the discriminators. Our contributions include training an effective deep defocus blur detector without manual annotation, developing dual adversarial discriminative networks, and proposing a constraint to address degenerate solutions. We validate our framework on two benchmark datasets and demonstrate its effectiveness.