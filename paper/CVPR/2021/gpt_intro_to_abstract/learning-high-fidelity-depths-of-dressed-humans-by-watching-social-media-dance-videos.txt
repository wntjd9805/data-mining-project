Clothes play a vital role in our daily lives, serving functions such as self-expression and protection. The rise of virtual reality (VR) and augmented reality (AR) has made it crucial to accurately model the complex geometry of dressed humans for authentic social telepresence. Traditional approaches, like photogrammetry, rely on massive camera setups to capture local geometry but are impractical due to hardware requirements and computational complexity. Single view reconstruction offers a solution by learning 3D representations of humans from scanned models, but the limited data available leads to performance degradation. In this paper, we present a method to reconstruct high-fidelity 3D geometry of dressed humans from a single view image by leveraging dance videos shared on social media platforms like TikTok. We exploit the fact that each video captures diverse poses of a single person, and we hypothesize that the geometry of dressed humans remains relatively constant across poses, allowing us to learn from the real dance videos. By estimating transformations for each body part, we can warp the 3D geometry from one image to another and achieve self-supervision without 3D supervision.While existing depth estimators fail to capture fine local geometry, our method leverages the relationship between depths and surface normals to jointly learn and capture such details. We introduce HDNet, a network that learns the spatial relationship between an RGB image, human foreground, and human UV coordinates to generate intermediate surface normals. These predicted surface normals are then used to estimate high-fidelity depths of dressed humans. Our semi-supervised approach combines 3D scanned models and real dance videos for training.We demonstrate the superiority of our method over state-of-the-art human depth estimation approaches on both real and rendered images. Our contributions include the creation of the TikTok dataset consisting of over 300 dance video sequences, a novel formulation for warping 3D geometry to measure self-consistency using real dance videos, the HDNet design for predicting fine depths reflective of surface normals, and strong qualitative and quantitative results on real-world imagery.