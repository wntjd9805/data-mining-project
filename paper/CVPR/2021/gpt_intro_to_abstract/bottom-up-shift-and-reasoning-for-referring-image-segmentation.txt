The paper introduces the intersection of vision and language in the field of computer science and addresses the problem of Referring Image Segmentation (RIS) from a visual reasoning perspective. While previous methods for RIS have focused on either multimodal fusion or representation learning, this paper proposes a novel approach that combines the advantages of both one-stage and two-stage methods.The authors propose a Bottom-Up Shift (BUS) module that enables one-stage RIS to conduct visual reasoning by capturing visual scenes' constituents and their relationships. The BUS module performs stepwise reasoning by aligning visual regions with linguistic structure and shifting regions based on their relationships. This approach allows for hierarchical reasoning and the identification of the referent in complex expressions.Additionally, the paper introduces a Bidirectional Attentive Refinement (BIAR) module to accurately segment the referent from a coarse localization. The BIAR module integrates low-level visual features and high-level semantic features using a two-way attentive message passing mechanism, improving segmentation accuracy.The proposed BUS and BIAR modules are integrated into a Bottom-Up Shift and Reasoning Network (BUSNet). Experimental results demonstrate that BUSNet outperforms existing methods and achieves significant performance gains in referring expression reasoning. It also generates interpretable visualizations for stepwise reasoning and segmentation.Overall, this paper contributes to the field of vision and language by introducing a novel approach for RIS that combines visual reasoning with one-stage methods. The proposed BUSNet model shows promising results and provides explainable decision-making processes.