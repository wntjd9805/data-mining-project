This paper introduces the concept of curating two hard ImageNet test sets, namely IMAGENET-A and IMAGENET-O, to evaluate the performance and robustness of image classification models. The IMAGENET-A dataset contains images that are more challenging for models to classify compared to the ImageNet training distribution, thus exposing their weaknesses and limitations. The IMAGENET-O dataset, on the other hand, consists of out-of-distribution images that cause models to mistake them as high-confidence in-distribution examples. This dataset aims to test the models' ability to detect and handle anomalies. The authors demonstrate that clean and natural examples can reliably transfer and deceive models, highlighting the need for improved robustness. Different techniques, such as data augmentation and architecture modifications, are explored to enhance model performance on adversarially filtered examples. Although some techniques show promising results, there is still substantial room for improvement in current models' performance. The paper provides code and access to the datasets for further exploration and evaluation.