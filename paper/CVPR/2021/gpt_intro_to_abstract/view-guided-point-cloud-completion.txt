Abstract:Point cloud data plays a crucial role in various applications such as auto-driving, robotics, geography, and archaeology. However, the quality of acquired point cloud data can be compromised by factors like occlusions and low scanning precision, resulting in incomplete, sparse, and noisy point clouds. Existing methods for point cloud enhancement include completion, denoising, and super-resolution, with recent data-driven approaches showing promising results. In this paper, we focus on the task of point cloud completion, where an incomplete point cloud is transformed into a complete one. We address the challenge of incomplete point clouds by proposing a view-guided point completion framework (ViPC) that leverages complementary information from a single-view image. Sensor fusion, combining point cloud and image modalities, is becoming increasingly prevalent, and our framework effectively fuses pose and local details from the point cloud with global structure information from the single-view image. We tackle this cross-modality and cross-level challenge through a three-stage framework, involving the reconstruction of a coarse point cloud from the single-view image and differential refinement using a network called the Dynamic Offset Predictor. We also introduce a large-scale dataset, ShapeNet-ViPC, for evaluating point cloud completion methods. Extensive evaluations on this dataset demonstrate the superiority of our approach over existing state-of-the-art methods. Overall, our contributions include a novel solution for point cloud completion using single-view image information, a deep network for differential point cloud refinement, and a benchmark dataset for future research in this field.