Neural architecture search (NAS) is a time-consuming and challenging task in computer science due to the manual design of architectures to meet specific hardware constraints. To address this, researchers have proposed one-shot NAS methods that encode the search space into an over-parameterized neural network called a supernet. However, these methods still have limitations in terms of efficiency and flexibility. In this paper, we introduce a novel one-shot NAS framework called Searching by Generating NAS (SGNAS). The framework includes an architecture generator and a unified supernet that significantly reduces GPU memory consumption. The architecture generator can generate architecture parameters efficiently within the time of one forward pass. Additionally, the unified supernet reduces the number of parameters compared to previous single-path methods. Experimental results demonstrate that SGNAS achieves comparable performance with state-of-the-art single-path methods while being more efficient and flexible. The framework offers a promising approach to automate the search for optimal neural architectures under different hardware constraints.