Estimating the relative pose between two cameras is a fundamental problem in computer vision. Geometric approaches and deep neural networks (DNNs) have both been used to address this problem, but their combined potential remains under-explored. This paper introduces an uncertainty-based probabilistic framework that fuses geometric and DNN predictions to overcome the limitations of each approach. The framework leverages the well-understood rationale of geometric solutions and the ability of DNNs to handle complex scenarios. Geometric uncertainty is obtained using error function Jacobians, while a network predicts the uncertainty associated with camera pose estimation. The fusion of these predictions is achieved using Bayes' rule. Importantly, the fusion step is integrated into an end-to-end trainable pipeline, enabling strong interaction between DNNs and geometric methods during training. This approach ensures that the network learns to improve the geometric solution and provides an account of the geometric solution's uncertainty. The proposed uncertainty learning is distinct from previous works and enables a geometrically calibrated fusion of predictions. The network architecture incorporates a self-attention graph neural network, which proves effective in learning from keypoint correspondences. The resulting uncertainty-aware fusion framework achieves state-of-the-art performance in relative pose estimation, particularly in challenging indoor datasets with unconstrained motions. Overall, this paper presents a principled fusion framework that combines the strengths of geometric solvers and DNNs and demonstrates superior results on benchmark datasets.