Recently, deep learning has achieved impressive results due to advancements in GPUs and neural network architectures. Alongside these developments, there has been a growing interest in interpretable deep learning, particularly in the field of medical image processing. Various methods exist for analyzing deep learning models in computer vision, such as Class Activation Map (CAM) based methods and decomposition-based methods. CAM-based methods visualize model decisions by generating heatmaps that indicate where the model focuses its attention. However, CAM is limited to specific model architectures, while Grad-CAM and Grad-CAM++ provide more flexibility in model architecture. Layer-wise Relevance Propagation (LRP) is a decomposition-based method that redistributes model class outputs into input images. In this paper, we propose a novel explanation method called Relevance-CAM, which can analyze models not only at the last convolutional layer but also at intermediate layers. Our method outperforms other visualization methods in localizing target objects and is faithful and robust to the shattered gradient problem. We demonstrate the effectiveness of Relevance-CAM through objective evaluations and show that even shallow layers can extract class-specific information.