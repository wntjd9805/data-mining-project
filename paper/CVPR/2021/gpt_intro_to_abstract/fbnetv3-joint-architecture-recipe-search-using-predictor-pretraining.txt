Designing efficient computer vision models is a challenging problem in the field of computer science. Many applications, such as autonomous vehicles and augmented reality, require compact models that are highly accurate, even under constraints on power, computation, memory, and latency. Manual design of such models is nearly impossible due to the large number of possible constraint and architecture combinations. To address this challenge, recent work has employed neural architecture search (NAS) techniques to design efficient deep neural networks. One category of NAS is differentiable neural architecture search (DNAS), which uses path-finding algorithms to efficiently search for optimal architectures. However, DNAS cannot search for non-architecture hyperparameters that are crucial to the model's performance. Other NAS methods, such as reinforcement learning (RL) and evolutionary algorithms (ENAS), have their own limitations, such as ignoring training hyperparameters and having a limited search space. To overcome these challenges, we propose Neural Architecture-Recipe Search (NARS), a new approach that addresses the above limitations. NARS trains an accuracy predictor that can find architecture-recipe pairs for new resource constraints quickly. It scores both training recipes and architectures simultaneously to avoid the pitfalls of architecture-only or recipe-only searches. To avoid high training times, the predictor is pre-trained on proxy datasets to predict architecture statistics. NARS produces generalizable training recipes and compact models that outperform all existing manually designed or automatically searched neural networks. Our contributions include the introduction of Neural Architecture-Recipe Search, a predictor pre-training technique to enhance search efficiency, a multi-use predictor for fast evolutionary searches, state-of-the-art accuracy per FLOP results for the searched models, and the achievement of significant accuracy gains across various neural networks.