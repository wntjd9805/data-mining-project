Image inpainting is an important task in computer vision that involves filling in missing regions of an incomplete image to create a visually plausible complete image. It has various applications, including object removal, photo restoration, and error concealment. Traditional image inpainting methods have focused on learning a deterministic mapping from incomplete to complete images. However, this approach does not account for the fact that inpainting solutions are not unique, especially when dealing with large and complex missing regions.Recent deep learning-based methods have explored the use of probabilistic models for inpainting, such as variational auto-encoders (VAEs). These models assume a Gaussian distribution over continuous latent variables, allowing for the generation of diverse inpainting solutions. However, these methods often suffer from limited quality and structural diversity in their generated results.In this paper, we propose a new multiple-solution image inpainting method based on the hierarchical vector quantized VAE (VQ-VAE). Unlike previous methods, we adopt autoregressive modeling of discrete variables instead of parametric distribution modeling. This decision enables the use of powerful decoders and helps avoid the problem of posterior collapse. The hierarchical structure of the VQ-VAE also allows for the separation of structural and textural features in the inpainting process.Our proposed method consists of two stages: a diverse structure generator and a texture generator. The diverse structure generator employs a conditional autoregressive network to generate multiple sets of structural features. The texture generator then utilizes an encoder-decoder architecture to generate complete images based on the guidance of the generated structural features. Each set of structural features leads to a distinct inpainted image.We make several contributions in this paper. First, our method combines autoregressive modeling of discrete variables and hierarchical VQ-VAE, which improves the quality and diversity of the inpainting results. Second, we propose a conditional autoregressive network for generating diverse and reasonable structural features. Third, we introduce a structural attention module and two new feature losses to enhance structure coherence and texture realism in the inpainted images.We evaluate our method on three benchmark datasets, including CelebA-HQ, Places2, and ImageNet, and demonstrate its superiority in terms of both quality and diversity compared to existing methods. Overall, our method provides a more advanced and effective solution for multiple-solution image inpainting.