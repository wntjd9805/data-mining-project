The development of computer vision and natural language processing models has traditionally been separate, but there is a growing interest in building multi-modal models that align visual and language stimuli. These models aim to mimic human abilities to compress and translate information across modalities. Several joint visual recognition and natural language understanding tasks have emerged as tests for these models. Image captioning requires identifying and describing scene elements in an image, while visual grounding involves identifying objects in an image based on a given language query. Controlled image captioning combines these tasks by allowing users to specify which parts of an image they want described. This work has applications in generating localized descriptions of images for visually impaired users. Image captioning and visual grounding models typically consist of image encoders and language model decoders, often using attention mechanisms. However, existing models struggle to generate long-form captions with dense visual groundings. The Localized Narratives dataset is introduced as a solution, providing rich, longform captions and dense visual groundings in the form of mouse traces. This paper goes beyond the dataset and introduces tasks for predicting and generating mouse traces, as well as joint caption and trace generation. The predicted traces can be used for representing eye gaze and providing word-level grounding. A novel evaluation metric and a transformer-based model architecture are proposed, and it is shown that pre-training on these tasks benefits downstream tasks. Overall, this paper contributes by introducing novel tasks, presenting a new model architecture, proposing an evaluation metric, and demonstrating the benefits of joint learning.