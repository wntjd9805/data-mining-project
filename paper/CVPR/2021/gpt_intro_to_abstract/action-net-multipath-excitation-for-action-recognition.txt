Video understanding is an essential component in real-world applications such as Virtual Reality/Augmented Reality (VR/AR) and video-sharing social networking services. However, the increasing number of videos being uploaded and processed presents challenges in achieving high accuracy and low computation cost in video understanding. Action recognition, a key aspect of video understanding, has become crucial in various video-related applications such as content moderation and recommendations. Traditional human action recognition has focused on scene-related actions, but with the development of technologies like VR, temporal-related action recognition has gained more attention.Existing methods for action recognition mainly rely on 3D CNN-based frameworks and 2D CNN-based frameworks. While 3D CNNs are effective in spatio-temporal modeling, they fail to capture sufficient information in videos and suffer from problems like overfitting and slow convergence. On the other hand, 2D CNNs lack temporal modeling capabilities, resulting in the loss of sequential information in certain actions. To address these limitations, lightweight and fast inference frameworks based on 2D CNNs have been introduced, operating on short snippets of the video. However, these approaches still require expensive computations and lack explicit temporal modeling.In this paper, we propose a new lightweight and plug-and-play module called ACTION (spAatio-temporal, Channel and moTion excitatION) to effectively process multi-type information in videos at the feature level within a single network. Inspired by the two-stream architecture, we incorporate spatio-temporal features and motion features into the network, reducing the need for separate input types like optical flow. The ACTION module comprises three components: Spatio-Temporal Excitation (STE), Channel Excitation (CE), and Motion Excitation (ME).We also introduce ACTION-Net, a neural architecture equipped with the ACTION module, which we test on three different backbone models: ResNet-50, BNIception, and MobileNet V2. Extensive experiments conducted on three datasets, Something-Something V2, Jester, and EgoGesture, demonstrate the superior performance of ACTION-Net in action recognition.In summary, our contributions include the proposal of the ACTION module, a lightweight neural architecture (ACTION-Net), and extensive experimental results showcasing its superior performance on multiple datasets. The ACTION-Net offers a promising solution for efficient and accurate action recognition in video understanding applications.