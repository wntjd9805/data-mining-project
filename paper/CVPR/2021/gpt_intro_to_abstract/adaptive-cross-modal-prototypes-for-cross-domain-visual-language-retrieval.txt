Large-scale datasets of visual content paired with corresponding text descriptions have led to significant advances in cross-modal retrieval tasks. However, the manual collection of such paired data is labor-intensive and time-consuming, creating a barrier for applying cross-modal retrieval methods to new domains. In this paper, we investigate the question of how to learn knowledge on a source domain with paired data in order to generalize to other target domains without the need for additional data collection. We study unsupervised domain adaptation for cross-modal tasks involving vision and natural language descriptions. To address the challenges of compositionality, reporting bias, and domain shifts, we propose the ADAPTIVE CROSS-MODAL PROTOTYPES (ACP) framework. The ACP framework incorporates clustering and prototypical networks to preserve semantic structure and minimize domain shifts. Experimental results demonstrate that our method outperforms alternative domain adaptation strategies on multiple image and video retrieval datasets. Overall, this paper presents a novel framework for cross-modal retrieval in the unsupervised domain adaptation setting, achieving improvements in performance and addressing the challenges inherent in cross-modal retrieval tasks.