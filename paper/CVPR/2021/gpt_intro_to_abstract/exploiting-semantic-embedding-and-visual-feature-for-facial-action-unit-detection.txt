Automatic action unit (AU) detection in facial analysis has numerous applications in psychological research, behavioral studies, mental health assessments, and human-robot interaction. Previous deep learning approaches have shown promising results but are limited by the lack of labeled data for AU annotation, as it is a labor-intensive process. To overcome this limitation, researchers have explored auxiliary information such as domain knowledge, facial landmarks and expression labels, and freely available web face images. However, none of these methods have explored the use of AU semantic descriptions provided by the Facial Action Coding System (FACS). The AU semantic descriptions contain rich semantic information about facial areas, actions, motion directions, motion intensities, and the relations among AUs. In this paper, we propose a novel framework called SEV-Net that combines AU semantic embeddings with visual features for AU detection. SEV-Net utilizes the self-attention mechanism from the transformer and inter/intra attention modules to capture the semantic relations among AUs. The attention maps generated by SEV-Net are used as weights for aggregated feature extraction and AU classification. Our contributions are twofold: Firstly, we introduce attention modules at different levels (intra-AU attention, inter-AU attention, and cross-modality attention) to capture various AU semantic relations, leading to more discriminative feature learning from meaningful facial areas. Secondly, we demonstrate the effectiveness of utilizing AU semantic descriptions as auxiliary information for AU detection, achieving significant improvements compared to the state-of-the-art methods on three widely used datasets.