This paper presents a study on the robustness of audio-visual event recognition models against multimodal adversarial attacks. The authors explore the vulnerability of computational perception models to corrupted sensory inputs, similar to the perceptual illusion observed in human perception known as the McGurk effect. Several attack methods are used to generate adversarial examples in both audio and visual modalities. The correlation between model robustness and multisensory integration is validated through different audio-visual fusion methods. A weakly-supervised sound source visual localization model is developed to visually interpret audio-visual interactions under attacks. To mitigate adversarial attacks, an audio-visual defense method is proposed, which uses external feature memory banks to denoise corrupted features and learns compact unimodal embeddings to enhance invulnerability. A relative improvement metric is introduced to compare different defense approaches. Experimental results demonstrate the susceptibility of audio-visual models to adversarial perturbations and the weakening effect of audio-visual integration under attacks. The proposed defense method improves network invulnerability without sacrificing clean model performance. The contributions of this work include a systematic investigation of model robustness, qualitative interpretation of robustness over attacks, a novel defense method using external feature memory banks, and the proposal of a new evaluation metric.