Multi-view reconstruction problems often face challenges when estimating dense depth for pixels in smooth regions. In order to address this issue, 2D diffusion-based techniques have been used to perform gradient-based densification using a sparse set of depth labels in image space. These techniques assume smoothness between points to densify the point set, which is a reasonable assumption in many indoor scenes with low texture walls. However, diffusion from noisy point samples can lead to less accurate results, and it can be challenging to identify and filter out noisy or erroneous points from a sparse set.In this paper, we propose a method to optimize point constraints for a set of linear equations representing the solution to the standard Poisson problem of depth diffusion, specifically targeting the handling of noisy points. To achieve this, we develop a differentiable and occlusion-aware image-space representation for a sparse set of scene points, which enables us to efficiently solve the inverse problem through gradient descent. Our approach treats each point as a Gaussian to be "splatted" into the camera and utilizes the concept of radiative energy transfer through participating media to model the occlusion interaction between Gaussian distributions. This allows us to optimize position, depth, and weight parameters per point, and also optimize the point set using reprojection error from multiple RGB images.We demonstrate the effectiveness of our method on both synthetic and real-world data, including narrow-baseline light field multi-view data and initial results on wider-baseline unstructured data. Our approach successfully reduces significant diffusion errors caused by noisy or spurious points. Additionally, we investigate the challenges of optimizing edges through reprojection from depth maps. Through comparisons with image processing and deep learning baselines, our method exhibits competitive performance, especially in reducing bad pixels. Overall, our results highlight the potential of direct point optimization for diffusion-based dense depth estimation.The data, code, and results discussed in this paper can be found at visual.cs.brown.edu/diffdiffdepth.