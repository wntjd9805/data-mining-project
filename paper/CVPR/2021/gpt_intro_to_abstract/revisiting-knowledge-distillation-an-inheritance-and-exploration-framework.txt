This paper introduces the concept of knowledge distillation, which is a popular method for transferring knowledge from a larger teacher network to a smaller student network. The existing distillation methods focus on the consistency between the output class probabilities or intermediate features of the teacher and student networks. However, directly mimicking the teacher model restricts the student model's ability to learn new knowledge. This limits the student network's ability to incorporate new relevant patterns and can lead to poor solutions. To address this limitation, the authors propose a novel inheritance and exploration knowledge distillation framework (IE-KD) that allows the student network to partially inherit knowledge from the teacher network and partially explore new features. The framework is motivated by the principles of heredity and variation in evolution, as well as insights from Q-learning and AlphaGo. The authors demonstrate through extensive experiments that their IE-KD framework can improve the student network's ability to learn diverse and effective representations, leading to state-of-the-art performance. The framework can also be easily combined with existing distillation or mutual learning methods. Overall, IE-KD shows promise as a general technique for improving the training of deep neural networks.