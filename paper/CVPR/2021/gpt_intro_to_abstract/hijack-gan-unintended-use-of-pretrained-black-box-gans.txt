Generative Adversarial Nets (GANs) have shown remarkable success in applications like image synthesis and translation. However, training modern GANs requires significant data and computation resources. To address this, reusing pre-trained GANs for other tasks becomes essential. Prior works have demonstrated semantic manipulation using vector arithmetic or constant attribute vectors in latent spaces. However, these methods rely on linear manifolds and ignore the highly non-linear nature of latent spaces. To overcome this, we propose an iterative framework that gains high-level control over image generation by traversing non-linear latent spaces. We train a proxy model to dynamically decide the moving direction in each step, resulting in smoother and more effective attribute transitions. We also introduce an orthogonal constraint to solely edit the attribute of interest without affecting others. Our experiments show that our framework facilitates unintended tasks, including facial attribute manipulation, head poses, and landmarks. However, these unintended tasks also raise concerns about potential misuse. Despite black-box access, our method can be applied potentially for malicious purposes, highlighting the need for caution when releasing pre-trained GAN models. Our contributions include a framework to leverage pre-trained GANs for unintended vision tasks, a constraint for attribute editing, and extensive experimental results showcasing smoother manipulation while preserving non-target attributes.