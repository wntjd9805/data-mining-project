Graph Neural Networks (GNNs) have shown excellent performances in various graph-based tasks. However, current GNNs assume that the entire attributed graph is input, which can cause degradation in performance when the graph size is too large for limited memory resources. Two possible solutions are sampling a subgraph or compressing the input data and GNN model. This paper proposes a binarized GCN called Bi-GCN, which reduces memory consumption and accelerates inference by binarizing parameters and node attribute representations. A new backpropagation method is also designed. Experimental results show that Bi-GCN achieves significant memory reductions and accelerations while maintaining comparable performance to the standard GCN. The contributions include the proposal of Bi-GCN, a new backpropagation method, and the demonstration of its performance on benchmark datasets.