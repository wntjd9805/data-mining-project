Cross-modal retrieval is the task of retrieving relevant items from a database based on a query across different modalities. Most existing research focuses on the image and text modalities, using embedding functions to map inputs into a common space for retrieval. However, representing multiple modalities in a common space is challenging due to the multiplicity of concepts and variations in visual scenes and language descriptions. Standard approaches that rely on one-to-one relationships fail to capture this multiplicity effectively. Attempts to introduce multiplicity, such as proposing multiple candidate representations or computing region embeddings, come with increased computational cost.In this paper, we propose a Probabilistic Cross-Modal Embedding (PCME) method that represents images and captions as probability distributions in a common embedding space. PCME avoids explicit many-to-many representations and offers several advantages. It provides uncertainty estimates, allowing for applications like estimating the difficulty or chance of failure for a query. The probabilistic representation also enables a richer embedding space where set algebras can be applied, unlike deterministic representations that can only capture similarity relations. PCME complements existing deterministic retrieval systems.The assumption of one-to-one correspondences has also caused confusion in evaluation benchmarks, such as MS-COCO, which suffers from non-exhaustive annotations for cross-modal matches. To address this, we propose a smaller yet cleaner cross-modal retrieval benchmark using CUB and more sensible evaluation metrics.Our contributions include the introduction of PCME for representing one-to-many relationships in cross-modal retrieval, identifying shortcomings in existing benchmarks, and proposing alternative solutions. We analyze the joint embedding space using uncertainty estimates provided by PCME and demonstrate how intuitive properties arise.