Deep Neural Networks (DNNs) have been widely adopted by companies for various computer-vision tasks due to their ability to achieve state-of-the-art performance. Protecting the confidentiality of these models is crucial to prevent theft and misuse by adversaries. Model stealing attacks aim to train clone models that mimic the predictions of the target model by only having black-box access to the target model. Existing attacks rely on the availability of in-distribution or similar surrogate data, which may not be feasible in real-world scenarios. This paper presents MAZE, the first data-free model stealing attack that uses synthetic data created by a generative model. MAZE achieves high-accuracy clone models across multiple image classification datasets and complex DNN target models. The key insight is to utilize data-free knowledge distillation and zeroth-order gradient estimation to train the generative model. In cases where partial datasets are available, an extension of MAZE called MAZE-PD leverages generative adversarial training to improve clone accuracy. The findings highlight the need for better protection of machine learning models against competitors and malicious actors in the image classification domain.