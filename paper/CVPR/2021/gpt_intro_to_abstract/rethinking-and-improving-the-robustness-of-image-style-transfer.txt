Image style transfer, which aims to map the style of a reference image onto a content image, has gained significant attention in recent years. Neural style transfer algorithms based on deep networks, particularly those utilizing the activations of pre-trained VGG networks, have shown remarkable ability in capturing the visual style of images. However, when the VGG network is replaced with architectures known to perform better in other tasks, such as ResNet, InceptionNet, or DenseNet, the stylization performance noticeably degrades. This discrepancy in performance between VGG and non-VGG architectures has prompted discussions but remains largely unexplained.In this paper, we investigate the hypothesis that architectural differences between VGG and other networks contribute to the performance differences observed in style transfer. We compare the stylizations produced by activations from different architectures and analyze the statistical properties of these activations, including the derived Gram matrices used to encode image style. Our analysis reveals that non-VGG architectures, like ResNet, exhibit large peaks and low entropy in their activations, indicating that they are dominated by a few feature channels and have deterministic correlation patterns. This explains why non-VGG architectures struggle to transfer high-level style patterns captured in deeper layers of the network. In contrast, VGG activations are more uniform across layers, capturing a wider diversity of style patterns.We further investigate whether the robustness problem observed in non-VGG architectures can be addressed without modifying the network architecture. Taking inspiration from knowledge distillation, we propose a method called Stylization With Activation smoothing (SWAG) to smooth the activations used in stylization loss functions. By adding a softmax transformation to existing losses, SWAG improves the performance of popular stylization algorithms on various architectures, including ResNet, Inception, and WideResNet. It also brings the performance of random networks to a comparable level with pre-trained ones and, in some cases, even surpasses the VGG in stylization performance.Overall, this work provides insights into the architectural properties that contribute to the performance differences in style transfer and proposes a solution to improve stylization performance in non-VGG architectures. The findings contribute to a better understanding of neural style transfer algorithms and offer practical implications for improving style transfer techniques across a range of network architectures.