Instance segmentation is a challenging task in computer vision that involves detecting and segmenting all object instances in an image. Recent advancements in this field have been driven by large datasets such as COCO. However, creating such datasets for new domains can be prohibitively expensive due to the need for manual annotation of segmentation boundaries. Alternatively, weak labels like classification labels can be used to train instance segmentation models, but this approach poses its own challenges. Weakly supervised models often suffer from two types of errors: partial instance segmentation and missing object instances.This paper proposes a novel approach that leverages temporal information in videos to improve weakly supervised instance segmentation. Unlike existing methods that focus solely on images, the proposed approach takes advantage of object motion as an additional signal. Optical flow, which captures the movement of pixels in a video, is found to be closely related to instance segmentation masks. By incorporating video signals into the training process, the paper addresses the issues of partial segmentation and missing instances.The proposed method consists of two steps. In the first step, a modified version of the weakly supervised instance segmentation model is introduced, called flowIRN, which assigns similar labels to pixels with similar motion. This helps address the problem of partial segmentation. In the second step, a new module called MaskConsist is introduced to tackle the problem of missing instances. MaskConsist leverages temporal consistency between objects across consecutive frames to match predictions and transfer stable predictions to obtain additional pseudo-labels missed by flowIRN during training. Importantly, MaskConsist is a generic module that can be applied in combination with any weakly supervised segmentation method.Experimental results demonstrate the effectiveness of the proposed approach. The paper shows more than 5% and 3% improvement in average precision compared to image-centric methods on video frames from two challenging video datasets: Youtube-VIS and Cityscapes. Similar gains are observed on the video instance segmentation task in Youtube-VIS. The paper concludes by highlighting that this work is the first to utilize temporal consistency between frames to train a weakly supervised instance segmentation model for videos.