Scene parsing, which assigns a semantic label to each pixel in an image, is a crucial research area in computer vision with numerous applications. Convolutional Neural Networks (CNNs) have greatly advanced this field, and image-based datasets have been collected to evaluate scene parsing approaches. However, the real-world consists of videos, making video scene parsing more practical and reasonable. Unfortunately, limited progress has been made in video scene parsing due to the lack of suitable benchmarks. To address this, we present the Video Scene Parsing in the Wild (VSPW) dataset, covering various real-world scenarios and categories. VSPW consists of well-trimmed long-temporal clips with dense pixel-level annotations at a competitive frame rate and high resolution. The dataset contains 3,536 annotated videos, making it challenging to label, but we employ a human-computer collaboration scheme for efficient and accurate annotation. Based on VSPW, we propose an end-to-end Temporal Context Blending (TCB) network that utilizes long-range contextual information from previous frames to improve segmentation performance and temporal stability. Our TCB network outperforms existing baselines in both image-based and video-based segmentation methods. We hope that VSPW will inspire more researchers to develop efficient and accurate algorithms for video scene parsing and contribute to future research in this area.