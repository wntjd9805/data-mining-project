Deep Convolutional Neural Networks (CNNs) have achieved remarkable success in computer vision tasks due to the availability of large curated datasets and powerful computing resources. However, these CNNs trained using supervised learning can only predict from a predetermined set of image categories. Extending these models to new classes requires a large amount of labeled data for both the new and old classes, hindering real-world applications. Additionally, directly fine-tuning a model with new classes can lead to catastrophic forgetting, where knowledge about old classes is quickly forgotten. To address these challenges, we focus on few-shot class-incremental learning (FSCIL), aiming to design algorithms that can extend machine learning models to new classes with only a few data points. Our approach tackles these problems from two aspects.First, we propose to decouple the learning of representations and classifiers in FSCIL. By learning representations only in the first session where abundant data from base classes are available and fixing the network backbone while adapting the classifier in new sessions, we avoid overfitting and catastrophic forgetting in representations. We demonstrate that a pre-trained network backbone combined with a class mean classifier outperforms state-of-the-art approaches.Second, we address the issue of classifiers learned from individual incremental sessions only providing discriminative information for classifying internal categories. To tackle this, we present a Continually Evolved Classifier (CEC) that progressively adapts the classifier weights based on current and historical tasks. Using a graph attention network (GAT), the CEC contextualizes individual classifier weights over the global task, resulting in better decision boundaries for all involved classes.To optimize the graph model under an incremental learning scenario, we propose a pseudo incremental learning paradigm. Through episodic construction of pseudo incremental learning tasks and synthesizing unfamiliar classes, we enforce context knowledge propagation in the graph model.We validate the effectiveness of our proposed method through comprehensive experiments on benchmark datasets. Our contributions include the adoption of a decoupled training strategy, the introduction of a continually evolved classifier with a graph model, the design of a pseudo incremental learning paradigm, and achieving state-of-the-art performance on CIFAR100, CUB200, and mini-Imagenet datasets.