3D scene reconstruction is a crucial task in computer vision with numerous applications, particularly in augmented reality (AR) where accurate and real-time reconstruction is vital for realistic and immersive interactions. While camera motion can be accurately tracked with visual-inertial SLAM systems, real-time image-based dense reconstruction remains challenging due to low quality and high computational demands. Most real-time 3D reconstruction pipelines adopt the depth map fusion approach, similar to KinectFusion. However, this approach has drawbacks, including inconsistent and redundant depth estimations.In this paper, we propose NeuralRecon, a novel framework for real-time monocular reconstruction that directly reconstructs and fuses 3D geometry in a volumetric Truncated Signed Distance Function (TSDF) representation. Instead of estimating depth maps individually, NeuralRecon reconstructs local geometry in a view-independent 3D volume. The framework utilizes sparse convolutions to process a 3D feature volume and output a sparse TSDF volume, gradually refining the predicted TSDF at each level. By directly reconstructing the implicit surface, NeuralRecon learns the local smoothness and global shape prior of natural 3D surfaces, resulting in locally coherent geometry estimation.To achieve global consistency and eliminate redundant computation, NeuralRecon incorporates a learning-based TSDF fusion module using the Gated Recurrent Unit (GRU). This fusion module conditions the current-fragment reconstruction on the previously reconstructed global volume, ensuring joint reconstruction and fusion. The reconstructed mesh is dense, accurate, and globally coherent in scale. Additionally, the volumetric representation allows for the use of a larger 3D CNN while maintaining real-time performance.We evaluate NeuralRecon on the ScanNet and 7-Scenes datasets, and the results demonstrate its superiority over state-of-the-art multi-view depth estimation methods and the volume-based reconstruction method Atlas. NeuralRecon achieves a real-time performance of 33 key frames per second, which is approximately 10 times faster than Atlas. The supplementary video showcases the ability of our method to reconstruct large-scale 3D scenes in real-time. To the best of our knowledge, this is the first learning-based system capable of real-time reconstruction of dense and coherent 3D scene geometry.