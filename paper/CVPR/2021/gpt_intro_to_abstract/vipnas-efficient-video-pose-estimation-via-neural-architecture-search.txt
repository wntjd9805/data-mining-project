Human pose estimation has experienced significant advancements in recent years with the development of stronger neural networks. While many state-of-the-art models have focused on improving accuracy, they often neglect the computational complexity and real-time performance aspects. However, both accuracy and efficiency are crucial for real-world applications of video pose estimation. In this paper, we aim to address these challenges by developing a lightweight pose estimator that achieves state-of-the-art performance while significantly reducing model complexity.For video pose estimation, temporal redundancy is commonly observed, as adjacent frames in a video share similar global context information. This temporal contextual information can be leveraged to improve pose estimation. Therefore, it is essential to fuse features from adjacent frames to the current frame to effectively utilize this temporal contextual information and strike a balance between accuracy and efficiency. However, several open questions remain: 1) Which stage of features, low-level local features or higher-level global features, should be fused? 2) How to choose the optimal fusion operation among addition, multiplication, or concatenation? 3) How to efficiently allocate computation across different video frames while optimizing total accuracy subject to computation complexity constraints?To address these questions, we propose a novel spatial-temporal neural architecture search (NAS) framework called ViPNAS for efficient video pose estimation. In the spatial-level search, we optimize the neural architecture by exploring dimensions such as depth, width, kernel size, group number, and attention. In the temporal-level search, we jointly optimize the stage of features to be fused, the fusion operation, and the allocation of computation across video frames. By considering the total computation complexity (Flops) constraints over the entire video, we efficiently allocate computation resources across frames to optimize performance. Experimental results demonstrate that ViPNAS outperforms state-of-the-art methods such as SBL and LightTrack when used with various well-known backbones.The main contributions of this paper are as follows: 1) The proposal of ViPNAS, a novel spatial-temporal NAS framework for efficient video pose estimation.2) The ability of ViPNAS to dynamically allocate computational resources for different frames while considering total computation complexity constraints.3) The automatic search for temporal connections, including the fusion module and positions.Through experiments, we demonstrate that ViPNAS achieves state-of-the-art accuracy in video pose estimation while operating in real-time on CPU (>25 FPS).