This paper explores the intersection of visual scene understanding and natural language processing, focusing on the task of dense captioning. While previous work in this area has been primarily focused on 2D visual data, this paper introduces the novel task of dense captioning in 3D scenes. By leveraging 3D information such as object size and location, more accurate descriptions can be generated. The paper addresses the challenge of object relations by proposing a graph-based attentive captioning architecture that learns object features and object relation features, ultimately generating descriptive tokens. The paper makes four key contributions: the introduction of the 3D dense captioning task, a novel message passing graph module, an end-to-end trained method that incorporates 3D object features and relations, and the demonstration of improved performance compared to 2D captioning baselines.