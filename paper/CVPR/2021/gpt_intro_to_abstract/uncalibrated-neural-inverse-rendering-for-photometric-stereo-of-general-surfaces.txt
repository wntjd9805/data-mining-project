This paper introduces an uncalibrated deep photometric stereo method that aims to estimate an object's surface normals without requiring ground-truth surface normals during training. The existing photometric stereo formulation assumes a Lambertian reflectance model and does not apply to objects with unknown reflectance properties. While multiple-view geometry methods exist for surface normal estimation, photometric stereo excels at recovering fine details on the surface. However, most deep learning-based photometric stereo methods assume a calibrated setting, which requires expert skill for light direction calibration. To address this limitation, the proposed method estimates light source directions and intensities using image data and uses a neural inverse rendering network to estimate the surface normals. The method models the effects of both the light source and interreflections in the rendering process. The effectiveness of the proposed method is evaluated on the DiLiGenT dataset, and a new dataset is proposed to study the impact of interreflections on surface normal estimation. The paper concludes that the proposed method offers a more general and applicable solution compared to classical and recent deep learning methods.