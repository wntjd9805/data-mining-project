This paper focuses on the challenges of understanding the road scene for autonomous robotic systems, particularly in the context of self-driving cars. While image semantic segmentation is commonly used for this task, vision sensors have limitations such as poor lighting conditions and lack of depth information. In contrast, LiDAR sensors can provide accurate depth information regardless of lighting conditions. This paper proposes an end-to-end 3D sparse CNN method called (AF)2-S3Net for semantic segmentation using LiDAR point cloud data. The method utilizes an encoder-decoder architecture with an attentive feature fusion module and an adaptive feature selection module. Experimental results demonstrate that the proposed method achieves state-of-the-art accuracy on benchmark datasets. The contributions of this paper include the proposed method itself, as well as a comprehensive analysis of its performance compared to existing methods on multiple benchmarks.