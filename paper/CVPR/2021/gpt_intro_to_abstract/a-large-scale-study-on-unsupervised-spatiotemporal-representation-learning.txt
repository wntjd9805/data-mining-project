Recent methods in unsupervised representation learning from images have focused on maximizing similarity between different views of the same image using data augmentations. However, while artificial augmentations on images have been explored, natural augmentations provided by videos, such as motion, deformation, occlusion, and illumination changes, have not been fully utilized. In this paper, we aim to generalize image-based representation learning methods into the space-time domain by incorporating videos as a source of natural augmentations.We propose a simple objective that can be easily incorporated into existing image-based methods. Our hypothesis is that visual content in videos often exhibits temporal persistence, whether it is an action, an object, or a scene. We encourage feature persistency over time by maximizing the similarity between different clips of the same video. This objective can be integrated into various unsupervised learning frameworks, such as MoCo, SimCLR, BYOL, and SwAV. Our empirical results show that this objective works well across these frameworks, with or without the use of dissimilar (negative) samples.Our objective extends the idea of crops in images to clips in videos. This allows us to leverage recent advancements in unsupervised learning frameworks with minimal modifications. We explore factors such as the effective timespan between positives and the number of temporal clips to find that longer timespans and multiple samples yield improved downstream performance.To evaluate the effectiveness of our approach, we conduct unsupervised training on large-scale datasets, including Kinetics and million-scale Instagram sets. We evaluate representation quality on multiple downstream tasks, such as classification and detection, using datasets like Charades, Something-Something, and AVA. Our results indicate that unsupervised pre-training can achieve competitive performance in videos, and in some cases outperform supervised pre-training approaches.In conclusion, our study comprehensively investigates unsupervised representation learning in videos from multiple aspects, including different frameworks, datasets, downstream tasks, and training factors. We present state-of-the-art results on established benchmarks, highlighting the potential for further improvement in this area.