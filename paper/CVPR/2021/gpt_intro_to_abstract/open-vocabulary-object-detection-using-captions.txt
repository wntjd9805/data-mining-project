Object detection is a widely used application of artificial intelligence, particularly through the use of deep neural networks. However, training these models requires significant human supervision, as thousands of bounding boxes need to be manually annotated for each object category of interest. While valuable datasets like Open Images and MSCOCO exist, they only cover a limited set of object categories. Expanding object detection to a larger number of categories is resource-intensive and challenging. Humans effortlessly learn to recognize and locate objects through natural supervision, such as exploring the visual world and listening to descriptions. They build a rich visual and semantic vocabulary, which can be utilized for various tasks, including object detection. Humans can quickly learn to draw bounding boxes and generalize this skill to different object classes without the need for specific examples for each class. In this paper, we introduce an approach called Open-Vocabulary Object Detection (OVD), which imitates this human ability. OVD is a two-stage framework that utilizes a corpus of image-caption pairs to acquire a large vocabulary of concepts. This acquired knowledge is then used to train object detection models, requiring annotation only for a subset of object categories. This reduces the need for costly annotation and enables the use of freely available captions from the web. We propose a model that can detect objects within a given target vocabulary. To train this model, we use an image-caption dataset covering a wide range of words and a smaller dataset with localized object annotations from a set of base classes. Unlike traditional object detection settings, the target classes in OVD are not known during training and can be any subset of the language vocabulary. Our proposed method is based on the Faster R-CNN architecture, which is pretrained on the image-caption dataset and then fine-tuned on the bounding box dataset in a way that preserves the learned vocabulary. Through extensive experiments, we evaluate our method called Open Vocabulary R-CNN (OVR-CNN) and demonstrate its superiority over existing approaches in zero-shot learning and weakly supervised object detection. OVR-CNN achieves significantly higher performance with a mean average precision (mAP) of 27% compared to 10% in zero-shot learning, and 40% mAP compared to 26% in generalized zero-shot settings. We provide open-source code for reproducibility and further research.