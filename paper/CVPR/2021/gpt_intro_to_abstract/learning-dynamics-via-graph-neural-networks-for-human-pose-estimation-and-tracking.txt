Multi-person pose estimation and tracking have become increasingly important in various scenarios such as behavior analysis and action recognition. While both tasks focus on different aspects, they are often coupled together. Existing methods for pose estimation and tracking can be categorized into bottom-up and top-down approaches. However, these methods still face challenges in highly-cluttered and fast-motion scenes, leading to missed detections. In this paper, we propose a novel approach that leverages the dynamics of human poses within image sequences. Instead of relying on detection cues in each frame, our approach first predicts poses based on a history track of poses. We use a graph neural network for this prediction step, allowing us to produce a legitimate pose estimation from the start. Our approach combines predicted poses with detected poses in the same frame, using both dynamical and visual information. This strategy reduces the mismatched rate and complements the visual cues, resulting in improved pose estimation and tracking. We evaluate our method on benchmark datasets and demonstrate its superior performance compared to state-of-the-art approaches. We also analyze the impact of each component in our method, highlighting the effectiveness of learning pose dynamics.