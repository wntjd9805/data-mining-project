This paper introduces MVDNet, a multimodal deep fusion model for vehicle detection in adverse foggy weather conditions. Full Driving Automation (Level 5) relies on robust all-weather object detection, which is challenging in foggy weather. While existing object detectors mainly fuse lidar and camera, these visual sensors are sensitive to weather conditions and are unreliable in harsh weather like fog. Radar, on the other hand, has the potential to overcome foggy weather due to its ability to easily penetrate or diffract around fog particles. However, radars in existing datasets are underexplored due to data sparsity. The Oxford Radar Robotcar (ORR) dataset provides an opportunity for object detection in foggy weather as it deploys a radar with a rotating horn antenna, generating dense intensity maps. MVDNet is designed to take advantage of both lidar and radar while overcoming their limitations. It consists of two stages: generating proposals from lidar and radar separately, and adaptive fusion and temporal fusion using attention and 3D convolutions. MVDNet can detect vehicles occluded by fog in lidar point clouds and reject false alarms in noisy radar intensity maps. The authors validate MVDNet using a procedurally generated training dataset based on raw lidar and radar signals from ORR, demonstrating its performance in foggy weather conditions. MVDNet achieves better performance than lidar-alone detectors or lidar and radar fusion models, while requiring less computing resources. This paper contributes to the field by proposing a deep late fusion detector that effectively exploits the complementary advantages of lidar and radar, and by providing a labeled dataset with fine-grained lidar and radar point cloud in foggy weather conditions.