Deep metric learning focuses on learning an embedding space where samples of the same class are grouped closely together. This embedding space has been crucial for various tasks such as image retrieval, few-shot learning, zero-shot learning, and self-supervised representation learning. Previous methods have proposed new metric learning losses, sampling strategies, regularization techniques, and ensemble models to obtain high-quality and compact embedding spaces. In this paper, we introduce a new method called embedding transfer, which aims to transfer knowledge from a source embedding model to a target model. This task is a variant of knowledge distillation but focuses on metric learning instead of classification. The knowledge captured by the source model goes beyond class labels and includes intra-class variations and degrees of semantic affinity between samples.Existing methods for embedding transfer extract knowledge from the source embedding space using probability distributions, geometric relationships, or similarity ranks of samples. However, these methods have limitations in utilizing detailed inter-sample relations or considering the importance of samples.To address these limitations, we propose a new method that defines knowledge as pairwise similarities between samples in the source embedding space. Pairwise similarities provide detailed characterization of the embedding space and capture inter-sample relations. We propose a new loss called relaxed contrastive loss that uses pairwise similarities as relaxed labels of inter-sample relations. This loss enables rich supervision beyond what binary labels offer.Our method demonstrates its effectiveness in improving the performance of target embedding models and compressing them. It outperforms existing embedding transfer techniques on public benchmark datasets for deep metric learning, achieving state-of-the-art results and showing improvement in image retrieval performance, self-supervised representation learning, and knowledge distillation settings.