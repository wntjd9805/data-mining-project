A key challenge in computer vision and machine learning is to construct or learn discriminative representations for the semantic content of images that are invariant to various spatial transformations and variations. While deep neural networks have shown promise in generating relatively invariant features under small spatial variations or transforms, they are still susceptible to larger geometric transformations and spatial variations. This limitation arises from the fixed geometric structures of deep neural modules, such as the convolution and pooling layers. Researchers have also found that deep neural networks rely on absolute spatial locations and image boundary conditions for object recognition and classification, challenging the assumption of translation invariance in modern convolutional neural networks (CNNs).Efforts have been made to improve the robustness of deep neural networks under spatial transforms by developing methods such as transform-aware data augmentations, geometry adversarial training, and transform-invariant network modules and structures. However, these methods mainly address affine or perspective transforms and do not effectively handle generic spatial variations, such as changes in object poses, part configurations, and scene structures.In this work, we propose a new approach called the spatial assembly network (SAN) to address the challenging problem of invariant feature representation learning under more generic spatial variations. The SAN examines the input image and performs a learned re-organization or optimized assembly of feature points from different spatial locations to generate invariant features. This assembly process is conditioned by feature maps of previous network layers and can be incorporated into existing network architectures. The proposed SAN module aims to improve the capabilities of existing networks in handling spatial variations and structural changes in the image scene, thereby maximizing the discriminative power of the final feature representation.We demonstrate the effectiveness of the proposed SAN module in various tasks, including metric/representation learning, image retrieval, and classification. Experimental results show that the SAN module enhances the performance of these tasks in both supervised and unsupervised learning settings. Overall, our work contributes to the field of invariant feature representation learning and offers a promising solution for handling generic spatial variations in computer vision and machine learning applications.