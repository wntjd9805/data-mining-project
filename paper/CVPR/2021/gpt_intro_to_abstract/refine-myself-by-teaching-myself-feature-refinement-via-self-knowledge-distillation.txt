The paper introduces a novel self-knowledge distillation method called Feature Refinement via Self-Knowledge Distillation (FRSKD) for deep neural networks (DNNs) in computer vision tasks. The limited computing resources on mobile devices require the distribution of successful vision tasks. Model compression and knowledge distillation have been researched as solutions to this problem. Knowledge distillation involves transferring knowledge from a pretrained teacher network to a smaller student network, enabling the student network to replace the teacher network at the deployment stage. Self-knowledge distillation is a recent approach that trains the student network to distill and regularize its own knowledge without a pretrained teacher network. Self-knowledge distillation can be achieved through data augmentation or auxiliary network-based approaches. However, existing self-knowledge distillation methods have limitations in generating refined knowledge and preserving local information between instances. To address these limitations, the paper proposes the FRSKD method, which introduces an auxiliary self-teacher network to transfer refined knowledge to the classifier network. FRSKD utilizes both soft label and feature-map distillations for self-knowledge distillation, making it applicable to classification and semantic segmentation tasks that emphasize the preservation of local information. Experimental results demonstrate that FRSKD outperforms existing self-knowledge distillation methods in image classification and semantic segmentation tasks and is compatible with other self-knowledge distillation methods and data augmentation techniques.