3D human avatar creation has become increasingly popular with the rise of augmented reality (AR) and virtual reality (VR) technologies. These avatars are represented by 3D surface meshes that capture the shape of the human body and texture maps that encode the appearance of the avatar. In order to create more immersive experiences with believable digital humans, realistic texture maps are crucial. However, the process of creating texture maps is currently time-consuming and requires manual work or specialized equipment. In this paper, we propose a novel method for synthesizing photorealistic texture maps for human 3D meshes. Our method is designed to have properties such as high resolution, high fidelity, large diversity, and editability. Previous techniques for generating textured 3D human models have limitations, such as being limited to garment styles in the image dataset or lacking control over specific styles like floral or checkered patterns. To address these limitations, we introduce a new architecture called Region-adaptive Adversarial Variational AutoEncoder (ReAVAE). This architecture learns the probability distributions of per-region styles from texture maps using a VAE in a semi-supervised setup, allowing for per-region style controllability of the output texture. Our architecture consists of a style encoder, a VAE bottleneck, and a generator. The style encoder encodes an input texture map and performs region-wise average pooling based on a semantic segmentation mask. The VAE bottleneck learns the features of each class and generates a transformed feature vector. The generator takes the transformed feature vectors, a segmentation mask, and random Gaussian noise as inputs to generate the desired texture map. During inference, we use the generator to enable independent layout controllability through the input mask and per-region style controllability through input random vectors, resulting in the generation of a wide variety of textures. We also introduce a training strategy that allows for both reconstruction of an input image and generation of an arbitrary image. Additionally, to overcome the limited availability of training data from 3D scans, we propose a method to generate training data by lifting textures from full-body clothed human images to the UV space. In summary, our contributions include: 1. A novel architecture for semi-supervised synthesis of diverse high-fidelity texture maps for 3D humans, with independent layout and style controllability.2. Utilization of a VAE to learn region-specific style distributions, enabling the generation of a variety of textures by sampling from these distributions during inference.3.