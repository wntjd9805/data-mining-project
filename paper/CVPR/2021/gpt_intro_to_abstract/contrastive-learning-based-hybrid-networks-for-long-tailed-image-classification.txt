In the real world, image classes are often distributed in a long-tailed distribution, where certain classes have an abundance of samples while others have limited representation. This data imbalance creates challenges in learning unbiased classifiers. Existing approaches to address this issue focus on mitigating the data shortage in underrepresented classes, such as through data resampling, loss re-weighting, margin modification, and data augmentation. Recently, a new line of work has emerged that decouples representation learning and classifier learning into two separate stages, using different data sampling strategies for each task. However, it is unclear whether the typical cross-entropy loss is ideal for learning features from imbalanced data.In this paper, we propose a novel hybrid network structure for long-tailed image classification. Our approach combines a contrastive loss for feature learning and a cross-entropy loss for classifier learning. We follow a curriculum-based training approach to gradually transition from feature learning to classifier learning, under the principle that better features lead to improved classifiers. We explore two variants of supervised contrastive learning strategies, which aim to learn less skewed features and consequently less biased classifiers.The first strategy we explore is supervised contrastive (SC) learning, which extends the unsupervised contrastive learning method by incorporating within-class samples as positives for each anchor. Unlike the two-stage learning approach of the original SC learning, we propose a hybrid framework that jointly learns features and classifiers. We argue that the two-stage learning may not be optimal in a fully supervised scenario, as it can compromise the compatibility between features and classifiers. To overcome the memory bottleneck caused by the SC loss, we further propose a prototypical supervised contrastive (PSC) learning strategy. PSC learning avoids the explicit sampling of positives and negatives by learning a prototype for each class and pulling each sample towards its class prototype while pushing it away from prototypes of other classes. This approach enables more flexible and efficient data sampling, especially for large-scale datasets under limited memory budget.Experimental results on three long-tailed image classification datasets demonstrate that our contrastive learning based hybrid networks outperform cross-entropy based approaches, achieving state-of-the-art performance. The contributions of this work include the introduction of a novel hybrid network structure, the exploration of effective supervised contrastive learning strategies, and the demonstration that supervised contrastive learning can outperform cross-entropy loss for feature learning in long-tailed classification. The code for our proposed method is publicly available.