This paper addresses the challenge of reliably separating a target speech signal in human-computer interaction (HCI) systems, such as speech recognition, speaker recognition, and emotion recognition. Deep learning technologies using high-dimensional embeddings have shown promise in analyzing unique acoustic characteristics of different speakers from mixed signals. However, these methods often suffer from label permutation errors. To overcome this problem, permutation invariant training utilizing a permutation loss criterion has been proposed, but label ambiguity still arises for unseen speakers. This paper proposes leveraging visual cues from target speech signals as an alternative solution. Psychological experiments have shown that looking at speakers' faces helps with auditory perception in noisy environments, such as lip reading. Audio-visual speech separation (AVSS) systems combine audio and visual features to derive unique characteristics. The performance of AVSS depends on the accuracy of alignment between audio and video streams. However, practical issues like recording from different devices, transmission through independent channels, and different codec protocols often lead to unaligned audio and video streams. This paper introduces a novel cross-modal affinity network called CaffNet, which uses visual cues and relative timing information to tackle alignment problems in AVSS. CaffNet compensates for abrupt discontinuities in audio-visual data without external information or additional supervision. An affinity regularization module is proposed to match audio-visual sequences at the utterance level, avoiding label permutation problems. Additionally, the paper extends CaffNet with a complex-valued convolution network architecture to improve speech reconstruction quality by restoring the mask of both magnitude and phase spectrum. The proposed networks are evaluated on benchmark datasets, demonstrating significant improvements in unconditioned scenarios.