Representation learning plays a crucial role in computer vision tasks, and recent advancements in this area have successfully extended from static images to videos. While hand-crafted local invariant features have their counterparts in video domain, state-of-the-art neural networks for video understanding often extend 2D convolutional neural networks for images along the temporal dimension. Unsupervised or self-supervised learning of representations from unlabeled visual data has gained momentum in the literature, but it tends to gravitate towards different dimensions in videos and images. In this paper, we propose a Contrastive Video Representation Learning (CVRL) framework to learn spatiotemporal representations from unlabeled videos. We use positive and negative video clip pairs to train the framework by contrasting their similarities using the InfoNCE contrastive loss. To promote spatial self-supervision in videos, we carefully design data augmentations involving both spatial and temporal cues. We evaluate the learned video representations on Kinetics-400 and Kinetics-600 datasets and compare them with prior arts, achieving significant improvements in performance. We also demonstrate that CVRL benefits from larger datasets and networks, showing potential for scaling to larger unlabeled datasets.