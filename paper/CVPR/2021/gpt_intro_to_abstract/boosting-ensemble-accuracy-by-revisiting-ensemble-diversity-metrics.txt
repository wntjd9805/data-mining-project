Ensemble learning is a technique that aims to create a powerful model by combining the knowledge of multiple base models. There are two main approaches to constructing high-quality ensemble teams: data-driven or model-driven training, and selecting teams from a pool of diverse base models. Boosting algorithms, bagging methods, and random forests fall into the data-driven category, while ensembles of base models trained with diverse neural network structures and hyperparameters represent the second category. This paper focuses on the problem of selecting high-quality ensemble teams from a pool of base models. With M diverse base models, there are M exponential possible ensemble teams, many of which may not offer high performance due to insufficient failure independence among their member models. Ensemble diversity metrics are widely used to capture failure independence among member models and are expected to correlate with ensemble accuracy.