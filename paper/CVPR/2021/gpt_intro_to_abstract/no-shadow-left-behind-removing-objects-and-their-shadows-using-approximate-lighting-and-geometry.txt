Mixed reality aims to seamlessly combine the virtual and the real, expanding the scope of augmented reality applications. Previous work on object removal in augmented reality has focused on inpainting, replacing the pixels previously occupied by the removed object. However, for realistic results, it is necessary to remove not only the object but also its shadows and reflections. This paper focuses specifically on the shadow removal problem.Existing inpainting-based approaches either ignore the shadows or include them for inpainting as well. However, when dealing with large shadows, there may be limited image content available for pixel copying. Additionally, segmenting out the object's shadow can be challenging due to varying lighting conditions and surface textures that can be mistaken for shadows.Inspired by Debevec's work on virtual insertion of objects in scenes, this paper proposes the use of a scene proxy to determine the visual effects of a scene manipulation. The scene edit is performed on the proxy model, and the pixelwise difference between the pre- and post-edit renderings is applied to the input image, resulting in the final output. However, directly applying this method to solve the shadow removal problem is not practical due to the rough estimates of shadows in the proxy model. To overcome this, a neural network-based system for differential rendering is proposed for more accurate object removal.The paper addresses the question of obtaining an editable scene proxy, suggesting various options such as depth cameras, monocular depth estimation, or global models obtained through localization. In this work, depth maps from an affordable depth sensor and a 360Â° panorama are used to generate a proxy mesh. Despite the limited accuracy and completeness of the proxy model, the results show its effectiveness in generating plausible object removal outcomes under diverse conditions.The paper presents a method for removing objects and their shadows from input images, given a rough scene model and object masks. The system outperforms general image-to-image translation methods and inpainting methods, even when supplied with shadow regions to replace.