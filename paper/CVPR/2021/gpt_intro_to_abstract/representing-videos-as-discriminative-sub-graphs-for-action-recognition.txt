This paper introduces the concept of representing video structure as a spatio-temporal graph for action recognition. The goal is to analyze a video and identify the actions taking place in it. By representing videos as graphs, the authors aim to capture the interactive motion, actors, objects, and functional interactions involved in an action. The paper addresses two challenges: whether reasoning on the entire graph is necessary and how to model the discriminative patterns for each action in a unified framework.To tackle these challenges, the paper proposes the MUlti-scale Sub-graph LEarning (MUSLE) framework. This framework divides an input video into fixed-length video clips and uses Tubelet Proposal Networks (TPN) to produce space-time actor/object tubelets. These tubelets are then used as graph nodes, while dense connectivity between them serves as graph edges. The MUSLE framework decomposes the graph into multiple scales of sub-graphs, with each scale corresponding to a specific number of nodes. Gaussian Mixture Layers are used to interpret the distribution of sub-graphs and learn discriminative sub-graphs as action prototypes. During inference, the similarity between sub-graphs extracted from test videos and action prototypes is computed, and the action prototype with the highest similarity is predicted as the class label.The main contribution of this work is the representation of video structure as a graph and the discovery of discriminative sub-graphs for action recognition. The paper also presents an end-to-end learning approach for these sub-graphs and addresses the complexity of different actions in the reasoning process. The effectiveness of the proposed MUSLE framework is demonstrated through experiments on Something-Something V1&V2 and Kinetics-400 datasets, where superior performances are reported.