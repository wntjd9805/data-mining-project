Numerical optimization plays a crucial role in various scientific and engineering problems. In computer science, many problems in computer vision and graphics require model fitting, which can be formulated as nonlinear least squares. Nonlinear least squares are typically solved using the Levenberg-Marquardt (LM) algorithm, known for its robustness and fast convergence. However, existing solvers like G2o and Ceres do not efficiently handle extremely large problems.To address this issue, deep learning frameworks like TensorFlow and PyTorch have opened up opportunities to efficiently solve large-scale problems. These frameworks leverage GPUs, provide automatic backpropagation, and implement effective solvers for stochastic optimization. However, they are not ideal for solving nonlinear least squares due to slower convergence rates compared to the LM algorithm.Therefore, this paper aims to develop a stochastic LM optimizer within deep learning frameworks. The goal is to enhance the efficiency and scalability of solving general and large nonlinear least squares problems. The paper addresses two key challenges in developing such an optimizer.First, backpropagation cannot directly compute sparse Jacobians required by the LM algorithm. Backpropagation provides partial derivatives, but Jacobian matrices require derivatives of all residuals. Calling backpropagation for each residual is impractical due to memory and time constraints. To overcome this, the paper proposes a novel backward Jacobian network that computes the required scalar function and recovers the Jacobian using a single backpropagation.Second, batch-based stochastic optimization, commonly used for large problems, tends to cause the LM solver to diverge. The LM steps are often large and require accurate Jacobian computation from all data. To address this, the paper introduces a stochastic domain decomposition approach. Instead of traditional domain decomposition, which segments variables into different domains, this approach employs stochastic clustering to derive different segmentation for each epoch. This improves convergence quality, especially for boundary variables between domains. Additionally, a global reinitialization step is provided at the beginning of each epoch for further improvement.Experiments are conducted primarily on a fundamental problem known as bundle adjustment. The proposed stochastic approach significantly outperforms existing solutions in terms of quality, efficiency, and memory consumption. Ablation studies confirm that stochastic decomposition is the key contribution to quality. Overall, this paper presents a general, efficient, and scalable stochastic LM solver for large-scale nonlinear least squares problems within deep learning frameworks.