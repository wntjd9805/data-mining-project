This paper introduces a novel attention mechanism called Causal Attention (CATT) to mitigate dataset bias in vision and language models. The attention mechanism is a critical component in these models, but its unsupervised nature can lead to biased weights. CATT is based on the front-door adjustment principle and does not require the assumption of observed confounders. It is fundamentally different from existing deconfounding methods and can be applied in any domain where attention resides. CATT is built on In-Sample attention (IS-ATT) and Cross-Sample attention (CS-ATT), which improve efficiency by sharing parameters. Experimental results show that CATT consistently outperforms conventional attention in various vision-language models. For example, LXMERT+CATT achieves higher accuracies in VQA2.0 and NLVR2 tasks compared to UNITER while requiring fewer pre-training burdens. This demonstrates the potential of CATT in vision-language pre-training tasks.