Many machine learning approaches rely on statistical correlations within the training data to make accurate predictions. However, in real-world scenarios, these correlations are often disrupted due to various factors such as data biases and confounding variables. This leads to distribution shifts between the training and testing data, causing machine learning models to fail in making trustworthy predictions. To address this issue, out-of-distribution (OOD) generalization techniques have been proposed. These techniques aim to improve model generalization by mitigating the negative effects of spurious correlations between irrelevant features and category labels. The recognition of the 'dog' category is used as an example to illustrate the impact of distribution shifts. When most training images contain dogs in water, the visual features of water become strongly correlated with the label 'dog'. Consequently, the model is prone to producing false predictions when encountering images of dogs without water or other objects with water. Domain generalization (DG) techniques have been extensively studied to tackle distribution shift problems. DG aims to learn invariant representations across different domains while allowing irrelevant features to vary. However, existing DG methods either require manual division and labeling of domains or assume balanced latent domains, which are not always feasible in real applications. To address these limitations, the authors propose a more realistic and challenging setting where the domains of training data are unknown and not necessarily balanced. They introduce the concept of stable learning, which aims to decorrelate relevant and irrelevant features to enable OOD generalization. They propose StableNet, a method that decorrelates features using a novel nonlinear approach based on Random Fourier Features with linear computational complexity. They also present an efficient optimization mechanism that globally removes correlations by iteratively saving and reloading features and weights of the model. StableNet effectively removes irrelevant features and leverages truly relevant features for prediction, resulting in more stable performances in non-stationary environments. The proposed method addresses the challenges of nonlinear feature dependencies and excessive storage and computational costs in deep models. Experimental results demonstrate the effectiveness of StableNet in improving generalization under distribution shifts.