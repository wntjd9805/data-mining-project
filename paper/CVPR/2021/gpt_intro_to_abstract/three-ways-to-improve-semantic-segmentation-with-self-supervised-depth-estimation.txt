Convolutional Neural Networks (CNNs) have been highly successful in various computer vision tasks, including semantic segmentation. However, the training of CNNs often requires large-scale annotated datasets, which can be costly and time-consuming to collect. Self-supervised learning has emerged as a promising alternative to manual labeling, as it aims to learn representations from unlabeled data. In this paper, we explore the use of self-supervised monocular depth estimation (SDE) to improve semantic segmentation performance and reduce the amount of annotation needed. We propose a threefold approach that leverages SDE in the entire learning process, including data selection, data augmentation, and cross-task representation learning. Our contributions include using SDE as an auxiliary task for semantic segmentation, introducing a strong data augmentation strategy called DepthMix that respects the geometry of the scene, and proposing an Automatic Data Selection for Annotation method that eliminates the need for human annotators. Experimental evaluation on the Cityscapes dataset demonstrates significant performance gains and state-of-the-art results for semi-supervised semantic segmentation. Our contributions include utilizing SDE as an auxiliary task, proposing DepthMix for data augmentation, and introducing a novel data selection method based on SDE. Overall, our method allows for learning from unlabeled data and improves the performance of semi-supervised semantic segmentation.