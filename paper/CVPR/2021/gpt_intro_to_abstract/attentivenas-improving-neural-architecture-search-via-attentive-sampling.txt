Deep neural networks (DNNs) have achieved remarkable success but bringing them to edge devices is challenging due to network size and computation cost. Neural architecture search (NAS) automates efficient DNN design but faces nested optimization problems. Recent NAS advancements decouple parameter training and architecture optimization into separate stages for improved efficiency. However, existing methods use a uniform sampling strategy during training, missing the opportunity to further improve the accuracy of networks on the Pareto front. In this work, we propose AttentiveNAS, which improves uniform sampling by paying more attention to models likely to produce a better Pareto front. We explore two sampling strategies: BestUp, which focuses on the best Pareto front, and WorstUp, which improves candidate networks with worst-case performance trade-offs. We propose two approaches to efficiently determine networks on the best and worst Pareto front using training loss and accuracy predicted by a pre-trained predictor. Our contributions include proposing AttentiveNAS, comparing different sampling strategies, and achieving state-of-the-art accuracy on ImageNet while meeting FLOPs constraints.