Histopathological image understanding has been transformed by recent advancements in machine learning, particularly deep learning (DL). DL has improved diagnostic throughput and predictive performance in digital pathology, but it lacks transparency in decision-making processes. This necessitates the development of explainability techniques to enable pathologists to understand DL decisions. In the field of digital pathology, various explainers have been implemented, including feature attribution, concept attribution, and attention-based learning. However, pixel-level explanations have several limitations, such as disregarding biological tissue entities and producing blurry explanations. To address these issues, this paper proposes explainability in entity space using an entity graph representation and a Graph Neural Network (GNN). The entity graph highlights responsible entities for the diagnosis and generates intuitive explanations for pathologists. To determine the most suitable explainer, the paper introduces a quantitative metric based on class separability statistics using pathologist-understandable concepts. Furthermore, the paper proposes novel user-independent quantitative metrics that express pathologically-understandable concepts. These metrics are based on class separability statistics and can be applied in other domains. The proposed metrics are used to evaluate three types of graph explainers (graph pruning, gradient-based saliency, and layer-wise relevance propagation) in the context of computational pathology. The paper's contributions include the development of novel quantitative metrics, the application of explainability techniques in computational pathology, and an extensive assessment of graph explainability techniques with validation by expert pathologists.