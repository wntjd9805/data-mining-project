Semantic information in videos often spans a long duration, posing a challenge for current convolutional methods that struggle with the computational complexity of modeling the entire video. While splitting videos into short clips is a common solution, it limits the access to temporal information, thus failing to capture long-term semantics. In this paper, we explore whether there is a training method tailored for video tasks that can model long semantics without compromising computational complexity. The main cause of the problem lies in the 3D convolutional models that treat videos as integrated information blocks, making it infeasible to process lengthy videos. To overcome this trade-off between complexity and semantic integrity, we propose the progressive training (PGT) method. Inspired by Truncated Back-Propagation through Time (TBPTT), PGT treats videos as serial fragments with high-order Markov property and disassembles forward and backward propagation into multiple serial portions. This disassembly maintains the Markov dependency of the calculation flow and reduces resource consumption. We design Markov convolutional operators, which replace common convolutional operators, to satisfy the Markov property and propagate temporal information among the progressive steps. The proposed PGT method is effective and easy to implement, requiring minimal changes to existing video models. Empirical experiments demonstrate consistent performance improvements across different models, datasets, and training settings. We hope this method will provide new insights into modeling long videos.