The stereo matching task involves finding corresponding pixels in a stereo pair and determining the distance between them, known as disparity. Stereo matching plays a crucial role in depth perception and supporting various applications such as scene understanding, object detection, odometry, and SLAM. Recently, stereo matching methods have utilized fully convolutional networks to directly regress disparity maps, achieving state-of-the-art performance on stereo benchmarks. However, these methods often struggle to adapt from synthetic data to real-world scenes, resulting in limited performance. This paper focuses on the less explored problem of domain adaptation in stereo matching, specifically addressing the domain gaps between synthetic and realistic domains. The main differences between these domains are analyzed, including color, brightness, distribution of internal cost volumes, and geometry of output disparity maps. To bridge these gaps, the authors propose AdaStereo, a domain adaptation pipeline for stereo matching, consisting of three modules. The first module aligns input image-level inconsistency by employing a non-adversarial progressive color transfer algorithm, capturing target-domain color styles during adaptation. The second module aligns internal feature levels by introducing a cost normalization layer that reduces scaling inconsistency and regulates the norm distribution of pixel-wise feature vectors. The third module conducts self-supervised learning through a novel auxiliary task called self-supervised occlusion-aware reconstruction, improving disparity predictions across domains by involving more informative geometries from target scenes. Experimental results demonstrate that the proposed domain-adaptive models outperform other domain-invariant methods and even finetuned disparity networks on multiple stereo matching benchmarks.