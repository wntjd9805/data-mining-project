The introduction of this paper discusses the importance of autonomous navigation in building intelligent embodied agents. The paper focuses on the field of vision-language navigation (VLN) and its advancements in enabling agents to execute navigation instructions in 3D environments. The current VLN methods have made progress in learning paradigms, supervision signals, multi-modal embedding schemes, and path planning. However, these methods typically use a sequence-to-sequence framework, limiting the agent's access to past observations and understanding of the environment layout. To address this, the paper proposes a Structured Scene Memory (SSM) architecture that collects and stores percepts during navigation, allowing for efficient planning and decision making. SSM is graph-structured, providing a topological representation of the environment and a global action space. The paper also introduces a frontier-exploration based decision making strategy to handle the scalability of SSM. Experimental results on Room-to-Room and Room-for-Room datasets demonstrate the effectiveness of the proposed approach.