Student-teacher learning is a supervised learning approach where a well-trained teacher network is used to train a student network for various vision tasks. This work is inspired by knowledge distillation and has been extensively studied in recent years. The student-teacher training loss measures the difference between the features extracted by the student and the teacher networks, allowing the student to learn from noisy inputs by mimicking the teacher's clean features. Despite its success in tasks such as classification with noisy input and image denoising, there is a lack of theoretical analysis on why and when the hidden features of the teacher network enhance the student's generalization power. In this paper, we provide insights into the mechanism of feature-based student-teacher learning and investigate its success and failure factors in classification and regression tasks. We found that training the student to zero training loss is not ideal and that a knowledgeable teacher is generally preferred but has limitations. Additionally, well-decomposed knowledge leads to better knowledge transfer. We validate these findings through theoretical analysis in deep linear networks and experimental evaluations in wide non-linear networks.