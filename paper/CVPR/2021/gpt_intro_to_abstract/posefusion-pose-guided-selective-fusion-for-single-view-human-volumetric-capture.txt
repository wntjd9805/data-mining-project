Human volumetric capture has been a popular topic in computer vision and graphics for its potential applications in holographic communication, online education, games, and the movie industry. Single-view human volumetric capture has gained attention for its convenient setup, and current methods can be categorized into tracking-based and tracking-free methods. Tracking-based methods utilize pre-scanned templates or continuously fused meshes, while tracking-free methods focus on geometric and texture recovery from a single RGB(D) image. Both approaches have advantages and drawbacks, with tracking-based methods preserving geometric details but lacking the ability to handle topological changes, and tracking-free methods being able to handle topological changes but struggling with challenging poses and self-occlusions. In this paper, we propose a novel pipeline called POSEFusion that combines tracking-based and tracking-free inference to overcome the limitations of both approaches. We introduce pose-guided keyframe selection and adaptive implicit surface fusion techniques to achieve high-fidelity and dynamic reconstruction in both visible and invisible regions. Our experimental results demonstrate that POSEFusion outperforms state-of-the-art methods in capturing complete, dynamic, temporally continuous, and high-fidelity human reconstructions.