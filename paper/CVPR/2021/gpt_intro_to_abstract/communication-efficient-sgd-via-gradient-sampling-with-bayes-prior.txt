Recently, deep learning has achieved significant success in various tasks, including image classification, object detection, and video understanding. This success has been facilitated by data-parallel distributed training with increased GPU resources, which has reduced the computational time required for training deep neural networks. However, as the number of computing nodes increases, the communication cost also rises. Even in All-Reduce architecture, the transmission delay for communication is evident. To address this issue, gradient compression techniques have been proposed to reduce the size of transferred gradient data by substituting partial gradients for complete ones.Although gradient compression has been studied extensively, maintaining high accuracy at a high compression ratio remains a challenge. Existing approaches either overlook the importance of large gradients or suffer accuracy loss due to limited low-bit representations. In this paper, we propose a new gradient compression method called Gradient Sampling with Bayes Prior to improve accuracy. Our method aims to achieve a high compression ratio without sacrificing accuracy or practical acceleration benefits.The contributions of our work are as follows: 1. We propose a novel gradient compression method called Gradient Sampling, which efficiently captures large gradients based on the global gradient distribution.2. We enhance the Gradient Sampling scheme with Bayes Prior to balance the exploration and exploitation of gradient information, leading to improved accuracy.3. We prove the convergence bound of our proposed methods and demonstrate that their convergence rate is the same as that of Stochastic Gradient Descent under common assumptions.4. Experimental results on various computer vision tasks and backbones demonstrate that our method outperforms state-of-the-art techniques in terms of both speed and accuracy.Overall, our proposed Gradient Sampling with Bayes Prior method offers a promising solution to gradient compression, enabling efficient communication in distributed deep learning while maintaining high accuracy.