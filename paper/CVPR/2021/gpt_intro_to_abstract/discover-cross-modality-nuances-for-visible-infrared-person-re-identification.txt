Person re-identification (Re-ID) is a challenging task in computer vision, aiming to match pedestrian images captured by different cameras. Existing methods focus on single-modality matching using visible cameras, but they are not effective under poor illumination conditions. With the advancement of surveillance systems that can switch between visible and infrared modes, cross-modality data has been accumulated, leading to the need for cross-modality person Re-ID. In this setting, modality discrepancy and the discovery of nuances between different modalities become significant challenges. While several methods have been proposed to address the modality discrepancy and align features or pixel distributions, they still have limited ability to learn discriminative features due to the information buried in infrared images that remains unexplored. Fine-grained person Re-ID approaches that consider additional labeled priors also fail to fully exploit nuanced information in the cross-modality setting. In this paper, we propose a novel framework called the Joint Modality and Pattern Alignment Network (MPANet) to address these challenges. MPANet consists of Modality Alleviation Modules (MAM) to alleviate modality discrepancy, a Pattern Alignment Module (PAM) to discover nuances in different patterns, and a mutual mean learning fashion to train the model with a center cluster loss and cross-entropy loss for identity recognition. MAM uses instance normalization to alleviate modality discrepancy, while PAM generates pattern maps to attend different patterns and discover nuances, enforced by a region separation constraint. MPANet extracts modality-invariant features that represent nuances in different patterns. Our contributions include addressing nuances discovery and modality discrepancy in a unified framework, proposing PAM and MAM modules, and demonstrating the effectiveness of MPANet in visible-infrared person Re-ID.