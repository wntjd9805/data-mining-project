Self-supervised learning (SSL) of visual feature representations has gained significant attention in computer vision due to its potential to learn feature representations without human annotations. SSL methods based on contrastive learning have shown promise in matching or surpassing supervised pretraining on various downstream tasks. However, these methods primarily perform well on iconic images with single objects, whereas their performance decreases on more complex scene images with multiple objects. In this paper, we analyze contrastive self-supervised models to identify the causes of these limitations and propose a solution called Contrastive Attention-Supervised Tuning (CAST). CAST improves the visual grounding ability of contrastive SSL methods by introducing an intelligent geometric transform for cropping views from an input image and a Grad-CAM-based attention loss to provide explicit grounding supervision. We evaluate CAST on the COCO dataset and observe robust improvements in image classification, object detection, and instance segmentation tasks. These findings highlight the significance of addressing the visual grounding issues in self-supervised learning to improve performance on complex scene images.