This paper introduces the inverse projection problem in computer vision, which involves the mapping of 2D images to their corresponding 3D environment. To enable machines to perceive the 3D world, the authors aim to develop a model that can tackle this problem. The paper proposes a simplified version of the problem, where the goal is to restore 3D point clouds with semantic understandings from image sequences. This is formulated as Depth-aware Video Panoptic Segmentation (DVPS), which consists of monocular depth estimation and video panoptic segmentation. The authors present two derived datasets for DVPS, Cityscapes-DVPS and SemKITTI-DVPS, along with a new evaluation metric called Depth-aware Video Panoptic Quality (DVPQ). They also introduce ViP-DeepLab, a unified model that performs video panoptic segmentation and monocular depth estimation. In experiments, ViP-DeepLab outperforms existing methods in various tasks, such as video panoptic segmentation, multi-object tracking and segmentation, and monocular depth estimation. The paper contributes by proposing the DVPS task, presenting new datasets and evaluation metric, and developing a state-of-the-art model for solving the inverse projection problem.