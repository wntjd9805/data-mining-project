This paper investigates the problem of single-source adversarial robustness in multimodal neural networks used for autonomous driving systems. The authors consider a scenario where one modality (e.g., RGB) receives a worst-case perturbation and examine whether the model can still accurately identify objects using the remaining unperturbed modalities (e.g., LI-DAR, audio, etc.). The vulnerability of standard multimodal fusion strategies to single-source adversarial perturbations is highlighted, emphasizing the need to develop robustness strategies.Previous empirical studies on multimodal robustness have primarily focused on single-source corruptions and have not specifically addressed the adversarial setting. While adversarial training has been effective for unimodal models, it presents challenges for large, multimodal systems due to resource-intensiveness and performance degradation on clean data.The contributions of this paper are twofold. First, the authors conduct an empirical study of single-source adversarial robustness on various benchmark tasks, demonstrating that standard multimodal fusion practices are vulnerable to such perturbations. Naive ensembling of features from a perturbed modality with features from clean modalities does not guarantee robust predictions. Surprisingly, a multimodal model under a single-source perturbation does not necessarily outperform a unimodal model.To address this vulnerability, the authors propose an adversarially robust fusion strategy that leverages the detection of correspondence between features from different modalities. This strategy aims to defend against the perturbed modality while maintaining clean performance. The proposed approach is evaluated through extensive experiments, outperforming state-of-the-art methods in single-source robustness across different tasks.Overall, this paper highlights the lack of inherent robustness in multimodal models, but demonstrates a practical and effective strategy for improving their robustness without the drawbacks associated with end-to-end adversarial training. The combination of robust fusion architectures and robust fusion training presents a promising direction for defending real-world systems against adversarial attacks.