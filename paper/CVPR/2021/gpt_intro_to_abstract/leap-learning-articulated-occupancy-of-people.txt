This paper introduces LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation for human bodies in 3D space. Traditional mesh-based representations lack flexibility and are not suitable for deep neural networks. Neural implicit representations have been proposed for rigid objects, but their generalization to deformable objects is limited. LEAP aims to learn articulated neural occupancy representations for various human body shapes and poses. It takes inspiration from parametric human body models and uses inverse linear blend skinning (LBS) functions to deform the body mesh from a canonical space to a posed space. The occupancy values are computed using an occupancy network. LEAP overcomes the challenges of learning occupancy functions in different posed spaces by modeling accurate pose-dependent occupancy in a canonical space. It introduces novel encoding schemes for bone transformations and uses locally aware bone transformation encodings to capture detailed pose and shape-dependent deformations. Experiments demonstrate that LEAP effectively prevents interpenetration in 3D scenes and outperforms baseline methods. The contributions of this paper include introducing LEAP, proposing a canonicalized occupancy estimation framework, learning LBS weights via deep neural networks, and implementing novel encoding schemes for accurate shape deformations.