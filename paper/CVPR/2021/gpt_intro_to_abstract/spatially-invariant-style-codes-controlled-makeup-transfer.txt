Makeup application can be time-consuming and finding suitable makeup for each individual can be a challenge. To address this issue, deep learning approaches, especially GAN-based models, have been utilized for makeup transfer tasks. However, these methods require well-aligned frontal faces as input conditions and lack flexibility in controlling makeup styles. To overcome these limitations, a recent work called PSGAN has been proposed, which computes dense correspondence attention between images for component-to-component transfer. However, PSGAN suffers from issues like color bleeding and difficulty in implementing local transfers. In this paper, we propose a new model called Style-based controllable GAN (SCGAN) that extracts a spatially invariant 1D style code from the reference image and re-assigns it to the source image. This two-step approach eliminates the need for pixel-wise matching and simplifies the makeup assignment process. Our model consists of a Part-specific Style Encoder (PSEnc) to extract makeup styles, a Face Identity Encoder (FIEnc) to extract face identity features, and a Makeup Fusion Decoder (MFDec) to generate the final makeup transfer results. Our proposed model offers flexibility in makeup transfer, with shade-controllable makeup transfer and the ability to perform partial transfers from multiple reference images. Additionally, our model is invariant to pose variations. Extensive experiments and comparisons demonstrate the superiority of our proposed model compared to existing approaches. Our contributions include a fully automatic makeup transfer model with flexibility, a two-step extraction-assignment process to eliminate spatial misalignment, and state-of-the-art performance even with large spatial misalignment.