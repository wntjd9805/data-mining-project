Computing pixel correspondence in computer vision tasks, such as 3D vision and image editing, is a fundamental step. Existing methods have extensively studied the variants where images depict the same scene, but we address the dense semantic correspondence task where the images depict common visual concepts. This task is challenging due to large intra-class appearance and shape variations. The top-performing methods rely on neighborhood consensus, but they sacrifice computational efficiency. We propose a PatchMatch Neighborhood Consensus (PMNC) method that uses a CNN feature backbone and computes 4D correlations. Unlike previous methods, PMNC does not filter the full 4D correlation map using multiple convolutional layers but uses a PatchMatch-based inference approach. To compare patches in the images, we introduce a learned scoring function that performs local 4D convolutions on the correlation map. PMNC is more efficient as it computes convolutions on a fraction of the full search space. To train PMNC, we devise a differentiable proxy model that embeds the scoring function and feature backbone. We optimize the parameters of these modules by minimizing the deviation between predicted and ground truth probability maps. Our method achieves state-of-the-art results on different datasets while being faster and requiring less memory. In summary, our contributions are PMNC, a PatchMatch-inspired method that avoids exhaustive 4D convolutions, a simple training approach using a proxy model and sparse keypoint supervision, and extensive experiments showing the best accuracy with less memory usage.