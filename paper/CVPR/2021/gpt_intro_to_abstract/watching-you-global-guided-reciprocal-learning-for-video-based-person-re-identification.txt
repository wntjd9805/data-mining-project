Person re-identification (Re-ID) is a crucial task in computer vision, aiming to retrieve specific individuals across different cameras at different times and places. With the growing importance of Re-ID in advanced applications such as intelligent surveillance and criminal investigation, video-based person Re-ID has become a hot research topic. However, video-based Re-ID poses several challenges, including illumination changes, complicated backgrounds, and person occlusions.Previous methods for video-based Re-ID can be summarized into two steps: spatial feature extraction and temporal feature aggregation. However, these methods have limitations in handling temporal misalignment, background noise, and capturing small but meaningful subjects in videos. To address these drawbacks, recent research has proposed rigid-partition-based or soft-attention-based methods. While these methods enhance the discriminative local features, they often ignore the role of global features in person recognition.In this paper, we propose a novel Global-guided Reciprocal Learning (GRL) framework for video-based person Re-ID. The framework consists of two key modules: the Global-guided Correlation Estimation (GCE) module and the Temporal Reciprocal Learning (TRL) module. The GCE module estimates the correlation values of frame-level local features under the guidance of global features, generating two sets of disentangled features with distinct correlation degrees. The TRL module utilizes semantic enhancement and temporal memory strategies to effectively capture both conspicuous and fine-grained cues in videos.Experimental results on public benchmarks demonstrate that our GRL framework outperforms state-of-the-art methods in video-based person Re-ID. Our contributions include the proposal of the GRL framework, the GCE module for correlation estimation, and the TRL module for capturing both conspicuous and fine-grained information in videos.Overall, our framework addresses the challenges in video-based person Re-ID and achieves superior performance compared to existing methods.