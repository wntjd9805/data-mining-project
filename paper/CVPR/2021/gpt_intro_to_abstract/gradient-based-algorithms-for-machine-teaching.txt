The introduction of this computer science paper discusses the challenges associated with obtaining specialized and domain-specific annotations for expert domains such as biology or medical imaging. While data collection may be easy, annotations require highly specialized knowledge and are typically too expensive for large-scale annotation efforts. This has led to the exploration of alternative learning approaches, including few-shot learning, transfer learning, semi-supervised learning, and self-supervised learning. However, these approaches often underperform compared to supervised learning from fully labeled datasets. Machine teaching algorithms have recently gained interest as a way to train crowdsource annotators to label data from specialized domains. The paper focuses on machine teaching for image classification and its application in crowdsourcing platforms. The goal is to leverage a small annotated dataset to train crowd workers, enabling large-scale supervised learning of image classifiers. The proposed machine teaching setup involves an iterative interaction between a teacher and a student, where the teacher selects new examples from a large dataset to complement a teaching set used by the student to learn the task. The key question is how to select the teaching set to maximize class discrimination information with the fewest examples.Previous works have attempted to design optimal teaching algorithms, but they often rely on assumptions that may not hold in the context of crowdsource annotation. This work assumes that the student is an optimal learner, defined as learning a predictor of minimum empirical risk in the teaching set. It is shown that if the optimal student is allowed unbounded effort, they will always learn the optimal predictor for the task, and the role of the teacher is to optimize learning speed by selecting teaching examples.The search for the optimal teacher is formulated as a problem of functional optimization, where the teacher aims to align the steepest descent direction of the teaching set risk with the empirical risk over the entire example population. The optimal solution is the MaxGrad teacher, which maximizes the gradient of the risk on the selected examples per iteration. The paper provides MaxGrad teaching algorithms for both binary and multiclass tasks and demonstrates their effectiveness in experimental evaluations, outperforming previous algorithms for both machine learning and human students from Amazon Mechanical Turk (MTurk).