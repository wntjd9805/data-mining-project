In computer vision, understanding human movement in semantic terms is a crucial objective. This involves tasks like action recognition, video description, temporal localization, and generating human movement based on semantics. However, existing datasets have limitations in capturing variations in human movement and providing accurate action labels. To address this, we introduce BABEL, a large dataset of diverse and densely annotated actions in motion capture (mocap) sequences. BABEL leverages the AMASS dataset, which unifies multiple mocap datasets. It contains dense action annotations for over 43.5 hours of mocap data, with 9421 unique language labels organized into 252 actions belonging to 8 semantic categories. We collect the action labels and alignments using a combination of clustering and manual categorization. We benchmark the performance of models on BABEL for 3D action recognition, demonstrating its potential as a challenging real-world benchmark. Additionally, BABEL can be utilized for tasks like pose estimation, motion synthesis, temporal localization, and few shot learning. Our contributions include providing the largest 3D dataset of dense action labels, categorizing language labels for action classes, analyzing actions in BABEL sequences, and providing a publicly available dataset, baseline models, and evaluation code for research purposes.