Generative classifiers (GCs) and discriminative classifiers (DCs) are two contrasting approaches to solving classification tasks. While DCs model the class probability given an input directly, GCs model the likelihood of the input image conditioned on each class. The classification is then performed by finding the class with the highest likelihood for the image. The application of GCs has been limited to simple datasets, with DCs being the preferred choice for practical image classification tasks due to their excellent discriminative performance. However, GCs are said to have advantages in terms of trustworthiness, which includes explainability and robustness. DCs are often considered "black boxes" with limited understanding of their internal processes, whereas GCs provide more informative outputs and interpretable latent spaces. In terms of robustness, GCs are believed to generalize better under dataset shifts, have accurately calibrated posteriors, and detect abnormal inputs. However, it is unclear if GCs can maintain these advantages in more complex tasks while remaining competitive with DCs in performance. In this work, we design and train a GC that performs well on the ImageNet dataset, demonstrate unique explainability techniques of GCs, and explore the model's robustness. Our findings indicate that our GC outperforms a comparable DC in terms of trustworthiness, but previous superior generalization and immunity to adversarial attacks do not hold for the ImageNet dataset. However, our GC shows benefits in terms of detecting out-of-distribution inputs and adversarial attacks.