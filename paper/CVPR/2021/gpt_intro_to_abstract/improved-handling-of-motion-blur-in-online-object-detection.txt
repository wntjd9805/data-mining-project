Motion blur is a common issue in hand-held photography, especially as images are increasingly captured while on the move. This prompts the exploration of how motion blur severity affects object detection and what can be done to address it, particularly in egomotion-induced blur. The severity of motion blur is found to correlate with detection failure, highlighting the need for improved algorithms that can handle blur more effectively. Instead of a single breakthrough, a combination of approaches is likely required to address this challenge. Similar to previous research, the specific task requirements and pipeline choices are expected to have an impact. The main contribution of this paper is an empirical examination of five different remedies for reduced detection accuracy in the context of motion blur. These remedies tackle the proposed causes for detection issues, including testing whether the entire image is too blurry for reliable detection, compensating for texture mismatches along blur axes, addressing the differences between training data and test-time blur, customizing labels to match the detection-in-blur task, and treating detection in blur as a multi-task problem. The paper proposes a new model that focuses on the remedies related to incorrect training labels and the diverse types of egomotion blur. This new model sets a higher standard for online object detection in egomotion-induced blur. Overall, the study aims to mitigate the impact of motion blur on object detection and provide insights into improving detection performance in challenging conditions.