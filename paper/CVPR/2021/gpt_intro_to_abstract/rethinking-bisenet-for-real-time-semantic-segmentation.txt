Semantic segmentation is a critical task in computer vision that involves assigning pixel-level labels to images. With the advancements in deep learning, the performance of semantic segmentation has significantly improved, leading to increased demand in various applications such as autonomous driving, video surveillance, and robot sensing. To address these demands, researchers have proposed real-time semantic segmentation methods that focus on designing low-latency, high-efficiency convolutional neural network (CNN) models with satisfactory accuracy. However, these lightweight models borrowed from image classification may not be ideal for segmentation tasks due to the lack of task-specific design. Additionally, reducing the input image size to promote inference speed can result in loss of detailed appearance around boundaries and small objects. To overcome these challenges, this paper proposes a novel hand-crafted network called Short-Term Dense Concatenate (STDC) network, which aims to achieve faster inference speed, explainable structure, and competitive performance compared to existing methods. The STDC network incorporates STDC modules, which enable the extraction of variant scalable receptive fields with minimal parameters, and seamlessly integrates them into a U-net architecture for improved segmentation performance. In the decoding phase, the network utilizes Detail Guidance modules to guide the learning of spatial details in low-level layers, without the need for an additional time-consuming path. Extensive experiments demonstrate the effectiveness of the proposed methods, with STDC networks achieving new state-of-the-art results on ImageNet, Cityscapes, and CamVid datasets. The performance of the proposed STDC1-Seg50 and STDC2-Seg75 models on the Cityscapes test set are 71.9% mIoU at 250.4 FPS and 76.8% mIoU at 97.0 FPS, respectively.