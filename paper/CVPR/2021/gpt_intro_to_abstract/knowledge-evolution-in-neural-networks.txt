This paper introduces a novel approach called knowledge evolution (KE) for improving the performance of deep networks on relatively small datasets. Inspired by the concept of gene transfer, the authors strive to replicate the process of knowledge transfer from ancestors to descendants in deep networks. They encapsulate a deep network's knowledge inside a subnetwork called the "fit-hypothesis" and pass this knowledge from a parent network to its offspring in iterative generations. The results show a significant performance improvement in the descendant networks. The authors compare their approach to the lottery ticket literature, which treats dense networks as sets of hypotheses. However, the lottery ticket approach is limited by random initialization. In contrast, the KE approach evolves knowledge inside a deep network by splitting the network into two hypotheses: the fit-hypothesis and the reset-hypothesis. The fit-hypothesis is retrained for multiple generations while perturbing the weights inside the reset-hypothesis to encourage independent representation learning.The main contribution of this paper is the evolution-inspired training approach that boosts performance on small datasets and promotes better learning curves for descendant networks. The authors also propose a kernel-level convolutional-aware splitting (KELS) technique to reduce inference cost. KELS is tailored for convolutional neural networks and produces a slim independent network with reduced inference cost. The KE approach is shown to support various network architectures, loss functions, and integrates seamlessly with other regularization techniques. While KE increases training time, the KELS technique significantly reduces inference cost. This is particularly beneficial for applications where data collection is expensive, such as autonomous navigation and medical imaging.In summary, this paper presents a training approach (KE) that improves the performance of deep networks on small datasets, along with a network splitting technique (KELS) that reduces inference cost. The results demonstrate the effectiveness of these techniques in achieving state-of-the-art results in both classification and metric learning tasks.