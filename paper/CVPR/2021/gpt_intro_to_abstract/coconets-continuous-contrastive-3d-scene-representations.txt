Understanding the three-dimensional structure of objects and scenes is essential for machine perception and control in various tasks such as object detection, tracking, manipulation, and navigation. Recent developments have focused on learning representations of objects and scenes from multiview imagery and capturing the three-dimensional scene structure implicitly or explicitly. However, most existing methods evaluate the accuracy of the inferred 3D scene occupancy and the fidelity of rendered image views, instead of assessing the suitability of these representations for downstream semantic tasks.This paper presents Continuous Contrastive 3D Networks (CoCoNets), a model that learns to map RGB-D images to infinite-resolution 3D scene feature representations by contrastively predicting views. CoCoNets is trained to predict views of static scenes from 2.5D video streams, and its ability to detect and recognize objects in 3D is evaluated. The model maps the input streams to 3D feature grids and uses a contrastive loss to measure the matching error between inferred and target views. Plausible 3D completions of the scene can be generated given a single RGB-D image as input, allowing for the inference of occluded information and the estimation of object extents.The advantages of combining 3D neural bottleneck, implicit functions, and contrastive learning for 3D representation learning are demonstrated through comparisons with state-of-the-art self-supervised models. CoCoNets outperforms these models in tasks such as 3D object tracking and re-identification, 3D object detection, and 3D cross-view and cross-scene object alignment. Additionally, the model achieves competitive performance in predicting image views and 3D occupancies.In conclusion, this paper presents CoCoNets, a model that learns infinite-resolution 3D scene representations from RGB-D posed images. The model offers advancements in tracking and corresponding objects in 3D, pre-training 3D object detectors, and predicting views and 3D occupancies. Furthermore, CoCoNets sets a new state-of-the-art in self-supervision of 3D feature representations.