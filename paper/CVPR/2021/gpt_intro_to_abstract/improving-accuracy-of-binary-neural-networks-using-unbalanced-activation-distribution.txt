Deep Neural Networks (DNNs) have achieved remarkable performance in computer vision tasks, but their increasing compute cost and memory requirement pose challenges for resource-constrained environments. To address this problem, various techniques, including network quantization, pruning, and efficient architecture design, have been introduced. Network quantization has been actively studied, with recent works demonstrating the possibility of quantizing DNN models to 1-bit. Binary Neural Networks (BNNs), which use 1-bit weights and binary activations, have gained attention as lightweight solutions for edge devices. However, BNNs still suffer from accuracy degradation due to aggressive quantization. While weight quantization is easier, the quantization of activations poses challenges, especially when quantized to 1-bit precision. Previous works have explained the accuracy drop from 2-bit to 1-bit activation quantization using gradient mismatch caused by non-differentiable binary activation functions. Although the straight-through-estimator (STE) is used to approximate gradients, the discrepancy between the actual and approximated functions leads to gradient mismatch. In addition to the gradient mismatch, we propose that the symmetry of the sign function used in BNNs contributes to their poor performance. While ReLU activation functions in most DNN models result in skewed output distributions, binary activations using the sign function become symmetric. We demonstrate that models with unbalanced activation distributions perform better and show that shifting the threshold of the sign function improves BNN performance. We also analyze the training of the threshold and propose a simple method to shift it, resulting in improved accuracy of BNN models. Overall, our contributions include the identification of the benefits of unbalanced binary activation distributions and the proposal of threshold shifting to enhance BNN performance. Experimental results validate the effectiveness of our approach with minimal additional cost.