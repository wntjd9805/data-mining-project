This paper introduces a study on the design of powerful tools to navigate the large amounts of digital recipes and companion content available online. The authors focus on the task of cross-modal recipe retrieval, which involves finding relevant cooking recipes given a user-submitted food image. They propose an end-to-end joint embedding learning framework that utilizes Transformers for recipe encoding and introduces a self-supervised triplet loss to improve retrieval results. The authors conduct extensive experimentation and ablation studies to validate the effectiveness of their design choices. The proposed approach achieves state-of-the-art performance on the Recipe1M dataset, surpassing previous models in terms of medR and Recall@1 metrics. The contributions of this work include the introduction of a recipe encoder based on Transformers, the incorporation of a self-supervised loss term, and the validation of the model's performance through extensive experimentation.