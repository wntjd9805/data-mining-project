Abstract:Few-shot methods have gained attention in the field of computer vision, aiming to learn new classes with limited labeled examples. These methods have shown success in image classification and object recognition tasks, leading to recent advancements in few-shot video action recognition. Fine-grained actions, in particular, pose a challenge in collecting sufficient labeled examples. Existing approaches in few-shot video recognition match the query video to the single best video or the average across all support set videos belonging to the same class. In this paper, we propose a novel approach called Temporal-Relational CrossTransformers (TRX) for few-shot action recognition. We construct query-specific class prototypes by using an attention mechanism to match sub-sequences of the query video to sub-sequences of all support set videos. By considering sub-sequences rather than individual frames, TRX can better match actions performed at different speeds and in different parts of videos, enabling fine-grained class distinction. We combine multiple TRXs operating over different numbers of frames to leverage higher-ordered temporal relations. Our approach achieves state-of-the-art results on few-shot benchmarks for various datasets. We also perform a detailed ablation study, demonstrating the effectiveness of TRX in utilizing multiple videos of different lengths and temporal shifts from the support set. The results show a significant improvement when using tuple representations compared to single frames, particularly in scenarios where temporal ordering is critical.