In this paper, we address the task of text-based image captioning (TextCap) which aims to generate captions that not only describe visual contents but also read the texts in images. While previous methods have attempted to extend existing image captioning methods to this task, they often fail to consider the importance of texts in images. We propose a new Anchor-Captioner architecture consisting of an anchor proposal module (AnPM) and an anchor captioning module (AnCM) to generate multiple diverse captions focusing on different parts of an image. The AnPM module ranks the importance of each token and selects informative parts of texts, while the AnCM module uses a recurrent neural network to model the relationships between tokens and construct anchor-centred graphs (ACGs). Based on these ACGs, the AnCM module generates diverse captions that cover various OCR tokens. Our method achieves state-of-the-art results on the TextCaps dataset, in terms of both accuracy and diversity. This paper contributes to the field by proposing a method to exploit fine-grained text information and generate multiple captions for different parts of images, as opposed to generating a single caption for the entire image.