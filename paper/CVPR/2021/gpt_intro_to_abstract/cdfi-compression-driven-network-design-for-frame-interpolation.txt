Video frame interpolation is a crucial task in computer vision, as it involves generating intermediate frames between actual frames in a sequence to increase temporal resolution. It has various applications such as frame rate up-conversion, slow-motion generation, and novel view synthesis. However, this task is challenging due to the complex motion, occlusion, and feature variation in real-world videos.Recent research in this area has focused on deep neural networks (DNN) for their promising results in motion estimation, occlusion reasoning, and image synthesis. However, integrating pre-trained flow models or relying solely on pixel-level information has limitations in handling complex occlusion and blur. Kernel-based methods offer another approach by synthesizing intermediate frames through convolution operations, but they struggle with large motions and high computational cost. Hybrid methods combining advantages from both flow-based and kernel-based methods exist but are often cumbersome and limit their applications.There has been a trend towards designing more complicated and heavy DNN-based models for video frame interpolation, resulting in models with over 20 million parameters. These large models are difficult to train, inefficient during inference, and unsuitable for deployment on mobile devices. On the other hand, simpler and lightweight algorithms lack competitive performance or lack transferability.To address these challenges, this paper proposes a compression-driven network design for video interpolation (CDFI) that leverages model compression techniques. The authors introduce fine-grained pruning based on sparsity-inducing optimization to compress a state-of-the-art model called AdaCoF. The compressed AdaCoF still maintains benchmark performance, indicating redundancy in the original model. This compression provides two benefits: a deeper understanding of the model architecture, leading to an efficient design, and creating space for further improvements.The authors improve upon the compressed model by introducing a multi-resolution warping module that utilizes a feature pyramid representation of input frames. This improved model outperforms AdaCoF on benchmark datasets while being only a quarter of its initial size. The proposed framework offers a compression-driven approach that can be easily transferred to other DNN-based frame interpolation algorithms, leading to superior performance compared to other state-of-the-art methods.In summary, this paper presents a compression-driven framework for video interpolation that addresses the issue of over-parameterization. The authors compress the AdaCoF model and achieve similar performance while improving upon it. This approach results in superior performance and can be easily applied to other DNN-based frame interpolation algorithms.