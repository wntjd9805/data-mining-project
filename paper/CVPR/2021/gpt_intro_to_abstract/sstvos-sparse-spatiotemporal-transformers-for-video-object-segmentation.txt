Video object segmentation (VOS) is a challenging task that involves simultaneously tracking and segmenting objects in a video clip. It requires algorithms to overcome various difficulties such as object appearance changes, occlusion, and distinguishing similar objects in motion over time. A highly performant VOS system is crucial in downstream tracking applications such as player tracking in sports analytics, person tracking in security footage, and car and road obstacle tracking in self-driving vehicle applications. VOS methods also have relevance in interactive annotation of video data, where automatic segmentation can save annotator time.In this paper, we propose a Transformer-based model for video object segmentation that leverages self-attention over both time and space. Our model learns to segment an output frame by looking up similar regions in the temporal history and searching for reference masks. To address the high computational complexity of the problem, we introduce a sparse Transformer formulation that allows for efficient interaction between feature cells. We propose two sparse spatiotemporal attention variants: grid attention and strided attention.Previous methods for VOS can be categorized into online finetuning, mask refinement, and temporal feature propagation. However, these methods have inherent drawbacks such as the inability to adapt to changes in object appearance, compounding error over time, and lack of parallelizability. Inspired by the success of Transformers in natural language processing, we present a novel method called Sparse Spatiotemporal Transformers (SST) that overcomes these drawbacks. SST processes videos in a single feedforward pass and achieves parallelizability across a single example.Applying spatiotemporal attention operators to VOS presents challenges in computational complexity and distinguishing foreground objects. We address these challenges by introducing sparse attention operators and comparing two promising candidates. Our experiments demonstrate that SST significantly reduces computational complexity while achieving a high overall score on the YouTube-VOS 2019 validation set.In summary, we contribute a Transformer-based model for VOS that leverages spatiotemporal attention. We evaluate the model empirically and demonstrate its superiority over recurrent models. We also address computational complexity using sparse attention operator variants and provide implementation details. Our proposed methods have the potential to be applied not only to VOS but also to other dense video prediction tasks.