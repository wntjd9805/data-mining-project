Detecting anomalies in data is crucial for various fields, and computational anomaly detection involves classifying new samples as normal or anomalous based on training examples. This paper focuses on anomaly detection in three settings: anomaly detection, anomaly segmentation, and outlier exposure. While deep learning methods have been used for anomaly detection, they often rely on limited normal training data, resulting in suboptimal feature learning. The authors propose using pre-trained features from external datasets to improve anomaly detection and segmentation. They also address the challenge of feature adaptation in anomaly detection, which has received limited attention in previous research. The paper introduces simple techniques, such as constant-duration early stopping and adaptive early stopping, to achieve top performance and overcome catastrophic collapse. Experimental results demonstrate significant improvements compared to state-of-the-art methods, with enhanced performance in anomaly detection and segmentation tasks. The authors also analyze the effectiveness of pre-trained features and feature adaptation approaches, providing insights into their relative merits. The contributions of this paper include demonstrating the superiority of a simple baseline, highlighting the limitations of current methods, proposing effective solutions for feature adaptation, and presenting extensive evaluation results that outperform existing approaches.