In this paper, we explore the field of style transfer in computer graphics. Style transfer involves combining the content of one image with the style of another image to create a new image. Although several approaches have been proposed, the initial method suggested by Gatys et al. remains one of the best in terms of image quality. However, these approaches primarily focus on pixel-level manipulation, which may not align with the natural process of artistic style transfer using brushstrokes.To address this issue, we propose a novel representation for style transfer based on brushstrokes instead of pixels. We parameterize each brushstroke using a Bézier curve, along with additional parameters for color, width, and location. We also introduce a lightweight, explicit, and differentiable renderer that maps these parameterized brushstrokes to the pixel domain. This allows for a more natural transformation of the image, as groups of pixels can be spatially relocated.We validate the effectiveness of this brushstroke representation by combining it with the model proposed by Gatys et al. Our experiments demonstrate that this shift in representation, along with our rendering mechanism, outperforms modern style transfer approaches in terms of stylization quality. We measure the similarity to the style of an artist and the ability of human subjects to distinguish between real artworks and our stylizations.Furthermore, we apply our rendering mechanism to the task of image reconstruction. By fitting colored quadratic Bézier curves to approximate a target image, we achieve lower mean squared error compared to existing methods.Overall, our approach offers a more natural and controllable representation for style transfer, enabling improved image quality and greater flexibility in artistic expression.