Designing lightweight network architectures is important in the field of computer science. Many popular networks designed for ImageNet classification follow a design convention where a low-dimensional input channel is expanded by a few channel expansion layers to surpass the number of classes. Focusing on computational efficiency, lightweight models further shrink some channels, striking a balance between computational cost and accuracy. However, the optimal channel dimensions under the restricted computational cost have not been thoroughly studied. Existing models, including network architecture search-based models, have designed networks based on the conventional channel configuration without much exploration. In this paper, we investigate an effective channel configuration for lightweight networks that improves accuracy. Inspired by previous works, we explore the expressiveness of a layer by estimating the matrix rank of the output feature. We study the averaged rank of randomly generated networks with random sizes to determine the proper range of expansion ratio at an expansion layer. Based on this principle, we search for network architectures that yield better accuracy compared to the conventionally designed models. We find that the best channel configuration can be parameterized as a linear function by the block index in a network. This parameterization proves to be effective in designing lightweight models and we propose a new model based on this channel configuration. Our experiments show that this simple modification outperforms state-of-the-art networks like EfficientNets on ImageNet classification. The improved performance of our models also extends to other tasks such as object detection, instance segmentation, and fine-grained classification. Our contributions include a study on designing a single layer, network architecture exploration for effective channel configuration, achieving remarkable results on ImageNet, and demonstrating the high applicability of our models in various tasks.