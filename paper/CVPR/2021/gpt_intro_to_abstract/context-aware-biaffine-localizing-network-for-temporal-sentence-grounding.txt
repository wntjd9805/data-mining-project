Video understanding is a crucial task in computer vision, with various applications such as video event detection, video summarization, video captioning, and temporal action localization. In recent years, temporal sentence grounding (TSG) has emerged as an important and challenging task. TSG involves automatically determining the start and end timestamps of a target segment in an untrimmed video that corresponds to a given sentence description. This task requires modeling multi-modal interactions between visual and language features, as well as capturing complex context information for semantic alignment.Previous methods for TSG have followed multi-modal matching architectures or direct regression of temporal locations. However, these methods either heavily rely on the quality of proposals or fail to consider the joint consideration of start and end features. To address these limitations, we propose a novel Context-aware Biaffine Localizing Network (CBLN) for TSG. Our approach utilizes a biaffine mechanism to simultaneously score all pairs of start and end frames, effectively capturing the temporal structure and global context of videos. Additionally, we introduce a multi-context biaffine localization module that combines local and global contexts to enrich frame representations.We evaluate our proposed CBLN on three datasets (ActivityNet Captions, TACoS, and Charades-STA) and demonstrate its superiority over state-of-the-art methods. Our contributions include the adoption of a biaffine-based architecture for TSG, the development of a multi-context biaffine localization module, and extensive experimental validation of the effectiveness of our proposed approach.