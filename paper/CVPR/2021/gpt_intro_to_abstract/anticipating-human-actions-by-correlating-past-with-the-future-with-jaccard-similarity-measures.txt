Action anticipation is a fundamental ability of humans that allows us to effectively and safely perform daily tasks. This ability, often referred to as mental time travel, remains a mystery in terms of its internal mechanisms within the human brain. With the rise of Artificial Intelligence, human action anticipation has become a significant problem in the field of Computer Vision. Various forms of action prediction problems, such as early action recognition, anticipation, and activity forecasting, have been extensively studied in the literature.Early Action Recognition (EAR), also known as action prediction, aims to classify a given video based on a partial observation of the action. In this case, both the observed video and the full video contain the same action, and methods typically observe only a portion of the full video to recognize the action. Action Anticipation (AA) methods, on the other hand, focus on predicting a future action before it starts, even when the observed video contains a different action. In both cases, models analyze the initial part of the video and make predictions about the ongoing or future actions.Humans naturally possess the ability to correlate past experiences with future events. For example, when we see someone walking towards a door in a corridor, we can confidently anticipate that the person will open the door. This correlation between the action of walking towards the door and the future action of opening the door is likely learned at a young age. In this paper, we propose a new framework and three novel loss functions to solve the problem of action anticipation and early prediction by correlating past features with the future.Our framework aims to maximize the correlation between observed and future video representations. During training, our model learns to encapsulate future action information given the observed video. At test time, our model leverages this correlation to infer future temporal information and make accurate action predictions. We argue that it is most effective to exploit this correlation at higher levels of the video representation and at the class level. However, it remains unclear at what level of abstraction one should maximize this correlation. We demonstrate that commonly used techniques, such as minimizing the L2 distance or maximizing vector correlation or cosine similarity between future and observed video representations, are not ideal for this task. Although cosine similarity conceptually makes sense, it is not bounded and therefore has limitations when used for deep representation learning. We introduce a novel similarity measure called Jaccard Vector Similarity (JVS), inspired by Jaccard Similarity over sets, which takes into account both the magnitude and angle between vectors and is bounded.