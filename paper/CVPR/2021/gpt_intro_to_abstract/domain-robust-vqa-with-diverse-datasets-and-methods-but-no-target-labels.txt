Visual question answering (VQA) is a challenging task that requires perception, cognition, and language understanding. While recent VQA methods have shown strong performance, they often struggle with generalization and true reasoning due to dataset bias, domain-specific languages, or the need for separate training on new datasets. In contrast to object recognition, where domain adaptation techniques have been explored, there is a lack of analysis on domain-robust VQA methods. This paper aims to address this gap by investigating how different modalities, processing steps, and answer spaces contribute to domain shifts in VQA. The authors propose steps to measure visual and textual domain shifts across datasets, analyze the robustness of different VQA methods, and explore mechanisms to bridge domain gaps. The study involves comparing image and question representations across nine datasets and applying style transfer and paraphrasing techniques to measure robustness to artificial shifts. The findings suggest that neuro-symbolic, compositional models are more robust due to the disentanglement of perception and reasoning. Additionally, a two-stage domain adaptation technique is proposed, showing promising results in recovering performance lost due to domain gaps. This work, which operates in an unsupervised setting, expands on prior research in supervised domain adaptation for VQA methods and provides a reality check for VQA techniques. The contribution of this research includes insights into dataset differences, identification of methods robust to visual shifts, understanding generalization across different datasets, successful domain adaptation techniques, and the challenges of performing domain adaptation in unsupervised VQA.