Finding the optimal tradeoff between model performance and complexity is a critical problem in the computer science community. Currently, there are several popular approaches to address this issue, including Neural Architecture Search (NAS), pruning, and Knowledge Distillation (KD). NAS aims to automatically search for promising model architectures, while pruning removes redundant parameters from a model without compromising performance. KD focuses on transferring knowledge from a larger teacher model to a more compact student model. However, each of these methods has its limitations. NAS is parameter-agnostic and does not consider the structural dependence between architecture and parameters. Pruning is structure-agnostic and only considers parameters, while KD ignores the structural knowledge in the teacher-student pair. To overcome these limitations, we propose Joint-DetNAS, a unified framework for detection that optimizes NAS, pruning, and KD simultaneously.Our approach introduces two integrated processes: student morphism and dynamic distillation. Student morphism optimizes the student's architecture and removes redundant parameters using a carefully designed action space and weight inheritance strategy. This allows the student to adjust its architecture flexibly while utilizing the weights from the predecessor. Dynamic distillation focuses on finding the optimal matching teacher for the student by incorporating a dynamic teacher sampling approach. To facilitate teacher search, we build an elastic teacher pool that provides powerful detectors without the need for repeated training. Our framework uses a hill climbing strategy to evolve the student-teacher pair and requires fewer training epochs.We also investigate the relationship between the architectures of the student-teacher pair and observe the existence of structural knowledge and architecture matching in KD for detection. Extensive experiments are conducted to verify the effectiveness of each component, and our Joint-DetNAS framework showcases clear performance enhancements compared to baselines and pipelining approaches. For instance, our framework boosts the Average Precision (AP) of a classic R101-FPN detector from 41.4 to 43.9 on MS COCO, while reducing latency by 47%. This improvement is comparable to state-of-the-art EfÔ¨ÅcientDet while requiring less search cost.In summary, our contributions include investigating KD and pruning for detection, proposing an elastic teacher pool for efficient teacher sampling, developing a unified framework that jointly optimizes NAS, pruning, and dynamic KD, and conducting extensive experiments to verify the effectiveness of our approach.