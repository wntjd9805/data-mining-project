Abstract:The rapid increase in the use of 3D sensors such as Li-DAR has led to a need for compressing large amounts of 3D point cloud data for various applications, including autonomous driving. However, compressing orderless 3D points from point clouds is a more challenging task compared to image and video compression. Recently, deep learning methods have been developed for point cloud compression, including transforming the data to voxel representation and using existing image compression methods or directly compressing the raw point cloud data using backbone networks. These methods either ignore the sparsity characteristic of point clouds or suffer from high computational complexity or inefficiency in processing large point cloud data. To address these issues, we propose a new learning-based method, VoxelContext-Net, which exploits the voxel context in an octree-based framework. Our method organizes the input point cloud using the octree structure and employs a learning-based entropy model for efficient compression. We use local binary voxel representation to generate context information for the entropy model and include co-located voxel representations to reduce temporal redundancy. Additionally, we propose a coordinate refinement method for accurate reconstruction. We evaluate the performance of our method on large-scale static and dynamic point cloud datasets and demonstrate its superiority over hand-crafted and learning-based compression methods. Our approach achieves state-of-the-art compression performance and can be applied to both static and dynamic point cloud geometry compression.