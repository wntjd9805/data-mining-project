Visual object tracking is a critical task in computer vision with various applications in fields such as robot vision, video surveillance, and unmanned driving. However, tracking faces challenges like occlusion, deformation, and interference from similar objects. Current popular trackers rely on linear matching processes like correlation, which limits their ability to capture non-linear interactions between templates and regions of interest (ROI). To address this issue, we propose TransT, a Transformer-based tracking algorithm that combines an ego-context augment module and a cross-feature augment module using attention-based feature fusion. Our approach produces more semantic feature maps and achieves better performance compared to other trackers. We present a novel Transformer tracking framework that includes feature extraction, Transformer-like fusion, and head prediction modules. Our attention-based method focuses on useful information and establishes associations between distant features, resulting in improved classification and regression. Experimental results on various benchmarks demonstrate that TransT outperforms state-of-the-art algorithms, especially on large-scale datasets. Additionally, TransT runs in real-time, meeting the processing speed requirement of 50 fps on a GPU.