Deep neural networks (DNNs) have demonstrated remarkable performance in computer vision tasks. However, they are vulnerable to adversarial attacks, where even imperceptible noise can cause misclassifications. Adversarial attacks pose significant security risks in real-world applications of DNNs. This paper focuses on black-box attacks, where the attacker only has access to hard-labels or output scores of the target model. Previous research on black-box attacks has involved making numerous queries to the target model, limiting their usability in real situations.To overcome these limitations, the idea of substitute training has been widely explored. In substitute training, a substitute model is trained to make similar predictions as the target model, using the same input data. Using this approach, attacks can be conducted on the substitute model and then transferred to the target model. However, obtaining real input data for substitute training can be challenging, especially in real-world vision tasks where data might be subject to privacy regulations. Furthermore, using real images for substitute training does not guarantee transferability of the attack to the target model.To address these challenges, the authors propose a novel task-driven unified framework that uses specially-designed generated data for substitute training. This framework includes a Diverse Data Generation module (DDG) that samples noise combined with label-embedded information to generate diverse training data. Additionally, an Adversarial Substitute Training strategy (AST) is introduced to incorporate adversarial examples as boundary data into the training process. The joint learning of DDG and AST ensures consistency between the substitute and target models, improving attack success rate in black-box attacks without requiring real data beforehand.The contributions of this work include:1. Introducing a generation-based substitute training paradigm that boosts data-free black-box attacking performance.2. Proposing a diverse data generation module and an adversarial substitute training strategy to broaden the distribution of synthetic training data and improve the consistency of decision boundaries.3. Conducting comprehensive experiments and visualizations on multiple datasets to demonstrate the effectiveness of the proposed method against state-of-the-art attacks.Overall, this paper presents a novel approach to improve the performance of black-box attacks using substitute training with generated data, thereby addressing the limitations of real data availability and enhancing the transferability of attacks to target models.