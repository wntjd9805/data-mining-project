In this paper, we address the challenge of optimizing non-continuous, non-differentiable, or non-decomposable evaluation metrics in machine learning models used in real-world vision applications. We propose a method called MetricOpt that leverages a differentiable value function to model black-box evaluation metrics. Traditionally, practitioners rely on surrogate losses for optimization, such as the widely used cross-entropy loss for classification problems. However, these surrogate losses may not align well with the target evaluation metric, leading to inferior performance. Existing approaches either introduce hand-designed metric-aligned surrogate losses or learn adaptive losses in a relaxed or interpolated surrogate space, both of which have limitations.Our approach directly adapts the gradient-based optimization process to optimize black-box metrics without knowing any details about the metric function. We meta-learn a differentiable value function that models metric observations along optimization trajectories. The value function provides metric supervision, including approximate metric gradients, to augment surrogate loss gradients. This allows us to finetune a model that has been pre-trained using any given surrogate loss.We parameterize the value function using a lightweight network for fast training and inference speeds. The input to the value function are a small set of adapter parameters that modulate a pre-trained model. We meta-learn the value function using an enhanced ordinal regression objective to avoid overconfident metric estimates. Our value function can be easily applied to off-the-shelf optimizers like SGD and Adam, as well as to a learned optimizer.Our proposed method, MetricOpt, consistently improves different evaluation metrics across tasks such as classification, image retrieval, and object detection. It outperforms existing methods based on surrogate losses and demonstrates speed advantages as a fast finetuning method. MetricOpt also generalizes well to new tasks and model architectures. Overall, our contributions are the introduction of a differentiable value function for modeling black-box evaluation metrics, the development of a fast finetuning approach, and the achievement of state-of-the-art performance across various tasks and metrics.