Self-supervised representation learning has gained significant attention in recent years due to its ability to learn feature representations without expensive human annotations. Various self-supervised methods have emerged, such as instance discrimination, which matches features from different views or augmentations of the same instance while distinguishing them from features of other instances. This is usually achieved through a contrastive loss function. However, these methods primarily focus on single modality data. In this paper, we introduce the concept of cross-modal instance discrimination (xID), which extends instance discrimination to multiple modalities such as video, audio, or text. The xID framework allows for the learning of representations that can transfer across different modalities, enabling better generalization to downstream tasks. This paper presents our proposed xID approach and evaluates its effectiveness through extensive experiments.