Visual Question Answering (VQA) is a crucial component of many interactive AI systems, but recent studies have highlighted the issue of language bias in VQA models. These models often rely on linguistic correlations rather than true multi-modal reasoning, leading to poor generalization. Previous solutions have focused on enhancing the training data or using separate question-only branches, but these approaches still struggle to disentangle the good and bad aspects of language bias. To address this challenge, we propose CF-VQA, a counterfactual inference framework that reduces language bias in VQA. We formulate language bias as the direct causal effect of questions on answers and mitigate this bias by subtracting the direct language effect from the total causal effect. We introduce two scenarios, conventional VQA and counterfactual VQA, to estimate the total causal effect and direct language effect, respectively. In conventional VQA, both the question and the image are available, allowing us to estimate the total causal effect of the image and question on the answer. However, this scenario cannot disentangle linguistic correlations and multi-modal reasoning. In counterfactual VQA, we imagine a scenario where the machine hears the question but the multi-modal knowledge is blocked. By blocking the multi-modal impact, we can identify language bias by estimating the direct causal effect of the question on the answer, i.e., the pure language effect. We train CF-VQA using language-prior based methods and during the test stage, we use the debiased causal effect for inference. Experimental results show that CF-VQA outperforms methods without data augmentation on the VQA-CP dataset while remaining stable on the balanced VQA v2 dataset. The contributions of this paper are threefold. First, we formulate language bias in VQA as causal effects, providing a novel approach to addressing this issue. Second, we provide a causal interpretation for recent debiasing VQA works. Third, our counterfactual inference framework is general and applicable to different baseline VQA architectures and fusion strategies.