This paper introduces the concept of multi-perspective sequences, which are visual sequences captured from different angles or viewpoints. The authors address the challenge of effectively learning both the intra-perspective relationships within each sequence and the inter-perspective relationships between different sequences. They propose a novel LSTM (long short-term memory) cell architecture called MP-LSTM, which is capable of jointly learning the intra-perspective and inter-perspective relationships. The MP-LSTM architecture incorporates additional gates and cell memories to facilitate this joint learning strategy. Experimental results demonstrate that the proposed MP-LSTM networks outperform existing methods in two visual recognition tasks, namely lip reading and face recognition. The contributions of this paper include the introduction of the MP-LSTM cell architecture, its integration into visual recognition solutions, the achieved performance gains compared to other strategies, and the availability of the implementation for reproducibility and future research.