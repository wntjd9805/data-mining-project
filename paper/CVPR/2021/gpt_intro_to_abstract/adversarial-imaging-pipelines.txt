Deep neural networks have become widely used in computer vision, with applications in various fields including self-driving vehicles, medical diagnosis, video security, medical imaging, and robotics. However, these networks have been shown to be vulnerable to adversarial attacks, where carefully designed patterns are added to the input image to deceive the model while remaining imperceptible to humans. Understanding and exploring these adversarial perturbations is crucial for identifying weaknesses in current models and developing defenses against such attacks.Existing adversarial attacks focus on post-capture adversaries, tampering with the image after it is captured. Recent research has shown that physical objects placed in real-world scenes can generate adversarial patterns by capturing images of these objects. These attacks rely on network gradients to compute adversarial examples for each input image, but they struggle to transfer to other networks or images. Other approaches rely solely on network predictions and use surrogate networks or gradient approximations. However, none of these methods take into account the influence of the camera image processing pipeline (ISP), which performs various transformations on raw measurements to generate RGB images.In this paper, we propose a novel method that bridges the gap between physical attacks and attacks on post-processed images. Our method allows us to specifically attack cameras with a particular ISP, while leaving detections from other cameras intact, even if they use the same classifier but a different ISP. This camera-specific attack is not limited to deep networks but also targets hardware ISPs that were traditionally considered immune to adversarial attacks. Additionally, we introduce an attack on the optical system of a camera. Our method can accommodate proprietary black-box ISPs and complex compound optics without requiring accurate models, relying instead on differentiable approximations as gradient oracles.We validate our method using recent automotive hardware ISPs and automotive optics, achieving a success rate of 92% on raw images in experimental captures. Our contributions include introducing the first method for finding adversarial attacks that deceive specific camera ISPs and optics while leaving cameras with other ISPs or optics intact. We demonstrate attacks on embedded hardware ISPs that are not differentiable and only available as black-box algorithms, learning differentiable approximations of the image processing and sensing pipeline to serve as gradient oracles. We also analyze and validate the attack on raw input measurements for state-of-the-art hardware ISPs and validate physical attacks on recent automotive camera ISPs and optics, achieving a success rate exceeding 90%.