This paper introduces a generative model for interactive image-to-video synthesis that aims to achieve a fine-grained understanding of object dynamics. The goal is to synthesize video sequences that exhibit natural responses to local user interactions with images on a pixel-level. The model is derived from intuitive principles of physics and utilizes a hierarchical recurrent structure to model complex object dynamics. Importantly, the model is trained solely on video sequences without any ground-truth interactions provided. The proposed approach is evaluated on four video datasets, focusing on highly-articulated object categories such as humans and plants. The experiments demonstrate the effectiveness of the model in enabling fine-grained user interaction. Additionally, the plausibility of the generated object dynamics is validated through comparisons with state-of-the-art video prediction methods in terms of visual and temporal quality.Figure 1 provides an overview of the capabilities of the model.