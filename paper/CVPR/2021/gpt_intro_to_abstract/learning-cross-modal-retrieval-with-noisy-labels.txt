This paper introduces the problem of cross-modal retrieval in the context of multimodal learning. Most existing methods for cross-modal retrieval rely on clean-annotated training data, which can be expensive and time-consuming to obtain. Unsupervised methods can mitigate the labeling pressure but often result in worse performance. Semi-supervised methods that utilize labeled and unlabeled data have been proposed, but they still require a certain amount of clean-annotated data. To address the high labeling cost, the paper explores the use of non-expert sources such as Mechanical Turk and surrounding tags for annotation, but this introduces noise in the labels. Recent studies have shown that deep neural networks easily overfit to noisy labels, leading to poor generalization performance. To tackle this challenge, various methods have been developed for learning with noisy labels in unimodal scenarios, but few have explored the multimodal scenario. In this paper, the authors propose a Multimodal Robust Learning (MRL) framework to simultaneously mitigate the influence of noisy samples and narrow the heterogeneous gap. The framework includes multiple modality-specific networks and two novel loss functions: Robust Clustering (RC) and Multimodal Contrastive (MC) losses. RC weakens the influence of minor losses produced by noisy samples, improving robustness. It also projects different modalities into a common clustering space to narrow the heterogeneous gap. MC is a multimodal loss function that maximizes the inter-instance and inter-pair variances in both intra- and inter-modalities, mitigating the interference of noisy samples and narrowing the heterogeneous gap. The paper presents empirical results on cross-modal learning methods under noisy labels, demonstrating that networks easily overfit to noisy training sets and exhibit poor performance on validation sets. The proposed MRL framework shows promising results in mitigating the influence of noisy samples and narrowing the heterogeneous gap. The paper concludes with the contributions of the work, including the novel framework, the RC and MC losses, and extensive experiments on several multimodal datasets to demonstrate the robustness of the proposed methods.