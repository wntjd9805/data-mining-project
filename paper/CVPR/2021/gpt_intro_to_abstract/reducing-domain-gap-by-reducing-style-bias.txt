Convolutional Neural Networks (CNNs) have been successful in various applications but suffer from domain shift, where their performance degrades when faced with unseen environments. To address this problem, the research community has explored techniques such as Domain Generalization, Unsupervised Domain Adaptation, and Semi-Supervised Domain Adaptation. In contrast, humans have a remarkable ability to generalize visual recognition across domains. Recent studies have shown that CNNs have a bias towards styles, while humans rely more on object contents. This style bias makes CNNs more sensitive to domain shift, as image styles are more likely to change across domains. Previous research has demonstrated that reducing style bias can improve CNNs' robustness to domain shift. In this paper, we propose Style-Agnostic Networks (SagNets), which effectively improve CNNs' domain transferability by controlling their inductive bias. The framework includes separate content-biased and style-biased networks on top of a feature extractor. The content-biased network focuses on object contents by randomizing styles, while the style-biased network focuses on styles against which the feature extractor makes the styles incapable of discriminating class categories. Experimental analysis shows that reducing style bias reduces domain discrepancy, and SagNets achieve significant improvements in domain shift scenarios across various benchmarks. The proposed method is orthogonal to existing domain adaptation techniques and does not rely on domain labels or multiple domains, making it scalable and applicable to practical scenarios with unknown or ambiguous domain boundaries.