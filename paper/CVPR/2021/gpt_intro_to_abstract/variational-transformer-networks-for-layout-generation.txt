Layouts play a crucial role in various tasks, such as neural scene synthesis, graphic design, and data synthesis pipelines. While there has been some progress in synthesizing novel layouts using deep learning, this area remains largely unexplored and poses unique challenges for generative models based on neural networks. These challenges include the non-sequential nature of layout data, which consists of discrete classes and continuous coordinates. Generative models based on neural networks, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have been successful in learning complex distributions in tasks like image translation and text generation. However, there is still room for adapting the architectures of these models to exploit the specific properties of layout data. The attention mechanism, which has shown strong performance in tasks like language translation and object detection, is particularly suitable for modeling relationships in layout distributions. Previous work has demonstrated the effectiveness of deep learning in generating documents, natural scenes, and user interface designs. However, relying on handcrafted rules to define layout distributions is limited and subjective. Instead, this paper proposes an attention-based architecture within the VAE framework to learn the relationships between elements in a layout distribution in an unsupervised manner. The paper explores various design choices and evaluates the proposed model on publicly available datasets, achieving state-of-the-art performance in layout generation. The main contributions of this work are a novel generative model specialized in layout generation and the exploration of strategies for creating a variational bottleneck on sequences with varying lengths.