Generative Adversarial Networks (GANs) have been successful in synthesizing diverse and realistic images. However, in real-world applications, users often want to edit natural images instead of generating random samples. To address this, one can project the image into the GAN's image manifold by finding a latent code that reconstructs the image and then modifying it. Despite their impressive results, modern deep generative models are computationally expensive, making edge deployment challenging. This paper proposes "Anycost" GANs for interactive image synthesis and editing, aiming to train a generator that can be executed at various computational costs while producing visually consistent outputs. By using smaller generators nested inside the full generator through weight-sharing, the proposed approach allows for a smooth tradeoff between visual quality and interactivity. The authors introduce stage-wise training and a weight-sharing discriminator to stabilize the training process. Anycost GAN supports multi-resolution outputs and adaptive-channel inference, with two types of channel configurations: uniform channel reduction ratio and flexible ratios per layer. To handle diverse hardware capacities, an evolutionary search is used to find the best sub-generator under different computational budgets, ensuring output consistency with the full-cost generator. Consistency-aware encoder training and iterative optimization techniques are also proposed to improve consistency during image projection and editing. The results show that the anycost generator provides visually consistent outputs at various computational budgets, outperforming small generators and existing compression methods. It offers a significant speed-up for faster preview and maintains consistency after editing operations, providing an efficient and interactive image editing experience.