I2I translation, the task of modifying an input image to match the style of a target domain while preserving the content, is important in computer vision. However, current image-conditional GANs struggle to preserve scene structure during translation. To address this issue, various loss functions have been proposed, but they still have limitations in decoupling structure and appearance and adapting to arbitrary domains. In this paper, we propose to use self-similarity to design a domain-invariant representation of scene structure. We assume that all regions within the same category exhibit self-similarity and propose a network that learns representations of self-similarities beyond visual ones. We generate spatially-correlative maps that encode the structure using this self-similarity information. Our proposed loss, called F/LSeSim, captures the domain-invariant structure representation, learns a task-specific spatially-correlative map, and is more efficient than existing cycle-consistency architectures. We conduct experiments with various I2I translation tasks and demonstrate that our model outperforms existing translation methods in terms of performance and practicality.