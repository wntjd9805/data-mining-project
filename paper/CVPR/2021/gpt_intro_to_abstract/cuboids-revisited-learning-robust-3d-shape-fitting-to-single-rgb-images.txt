Humans have a tendency to visually abstract their environments by breaking them down into simple geometric forms such as cuboids, cylinders, and ellipsoids. This type of abstraction is also valuable for machines with visual perception, and scene representation based on geometric shape primitives has been a key topic in Computer Vision. While there have been advancements in 3D reconstruction from single images, most approaches focus on recovering detailed 3D information such as depth and meshes, rather than more parsimonious shape descriptions like cuboids or superquadrics. These existing 3D shape parsers work well for isolated objects but struggle to generalize to complex real-world scenes.This research proposes a novel approach for robustly parsing real-world scenes using 3D shape primitives, specifically cuboids. Leveraging advancements in single image 3D reconstruction and robust multi-model fitting techniques, the method uses a trainable RANSAC (RANdom SAmple Consensus) estimator to fit cuboids to 3D features obtained from a single RGB image. The estimator builds upon previous work and extends it by predicting multiple sets of RANSAC sampling weights concurrently, allowing for easier differentiation between different structures within a scene.To optimize the CNN used to extract 3D features, an end-to-end training approach is employed, based on geometrical consistency with existing 3D sensory data. During the process of primitive fitting, a novel occlusion-aware distance metric is proposed to handle scenes with occlusion or self-occlusion. This metric ensures that points are not assigned to primitive surfaces that cannot be seen by the camera due to occlusion.In order to compute cuboid parameters, numerical optimization is used as no closed-form solution exists. However, backpropagation through this optimization process is numerically unstable and computationally expensive. To address this, the gradient of the primitive parameters with respect to the input features is analytically derived, allowing for efficient end-to-end training without backpropagation through the minimal solver itself.The efficacy of the proposed method is demonstrated on the challenging real-world NYU Depth v2 dataset. The contributions of this research include a 3D scene parser capable of processing more complex real-world scenes, an occlusion-aware distance metric, analytical derivation of cuboid gradients to enable efficient training, and the ability to train the method using readily available sensory data without labor-intensive labels or annotations. The source code for this research is available on GitHub.