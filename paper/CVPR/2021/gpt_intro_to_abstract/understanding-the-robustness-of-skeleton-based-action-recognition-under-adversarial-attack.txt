The research in adversarial attack has shown that deep learning is vulnerable to imperceptible perturbations in data, raising concerns for security and safety. Adversarial attacks have also been used to enhance the robustness of classifiers. While adversarial attacks on static data have been extensively studied, their effectiveness on time-series data, such as videos, remains relatively unexplored. This paper focuses on adversarial attacks on a specific type of time-series data: 3D skeletal motion for action recognition tasks.Skeletal motion is commonly used in action recognition as it can improve accuracy by mitigating issues like lighting, occlusion, and posture ambiguity. However, this paper demonstrates that 3D skeletal motions are vulnerable to adversarial attacks, but their vulnerability is different from other data types. Adversarial attacks on 3D skeletal motion face two unique challenges: low redundancy and perceptual sensitivity.Unlike images with thousands of Degrees of Freedom (DoFs), skeletal motion is typically parameterized by fewer than 100 DoFs representing the joints of the skeleton. This restricted space of possible attacks limits the imperceptibility of adversarial samples. Small perturbations on single joints can be easily noticed, and coordinated perturbations on multiple joints within a single frame can disrupt the dynamics, making them obvious to observers.To systematically investigate the robustness of action recognizers, this paper proposes the Skeletal Motion Action Recognition Attack (SMART) method. SMART is based on an optimization framework that considers motion dynamics and skeletal structures. It formulates the attack as a balance between classification goals and perceptual distortions, using classification loss and perceptual loss. Varying the classification loss leads to different attacking strategies.SMART proves effective in both white-box and black-box settings, across various datasets and state-of-the-art models. This work systematically investigates the vulnerabilities of different methods under adversarial attacks and identifies potential areas for improvement. It introduces a new adversarial attack method with a novel perceptual loss function that captures perceptual realism and leverages motion dynamics. Furthermore, this paper provides insights into the role of dynamics in the imperceptibility of adversarial attacks, emphasizing that solely constraining perturbation magnitude is insufficient, differing from widely accepted approaches.