Image classification datasets often have noisy labels, especially those with many classes and large training sets. This paper proposes a principled probabilistic approach to modeling label noise in image classification. The method assumes a generative process for noisy labels using a multivariate Normal distributed latent variable at the final hidden layer of a neural network classifier. The mean and covariance parameters of this distribution are input-dependent, allowing for modeling of inter-class noise correlations. The method is evaluated on four large-scale image classification datasets and shows improved accuracy and negative log-likelihood compared to standard neural network training and other methods from the noisy labels literature. The method also demonstrates the ability to learn more general representations that transfer well to downstream tasks. The contributions of the paper include a new method for modeling inter-class correlated label noise, evaluation on multiple datasets, and the demonstration of correlations between semantically similar or commonly co-occurring classes. The paper also introduces a parameter-efficient version of the method to reduce memory and computational requirements for datasets with a large number of classes.