In recent years, there has been a growing trend of reusing pre-trained deep neural networks (DNNs) to reduce training effort and enhance performance. Knowledge distillation, where a pre-trained teacher model generates soft labels for a lightweight student model, has emerged as a popular research topic. While existing approaches focus on convolutional neural networks (CNNs) for regular data, this paper explores knowledge transfer from pre-trained graph neural networks (GNNs) for irregular data represented as graphs. The authors propose a novel knowledge amalgamation task to train a multi-talented student GNN from heterogeneous teacher GNNs specializing in different tasks. The student model is expected to integrate the teachers' expertise while being compact for resource-constrained applications. Challenges include handling varying graph feature dimensions and aligning topological semantics. The paper introduces a slimmable graph convolutional operation to accommodate different feature dimensions and a topological attribution map (TAM) for learning the teachers' topological semantics. The proposed method is evaluated on four different tasks, showing competitive performance compared to teachers and significant reduction in computational cost. Overall, this paper presents a GNN-based approach for knowledge amalgamation without human annotations, enabling the training of a versatile student model from heterogeneous-task teachers.