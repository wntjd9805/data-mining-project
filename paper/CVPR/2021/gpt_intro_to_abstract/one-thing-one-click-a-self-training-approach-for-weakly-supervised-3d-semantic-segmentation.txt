The paper discusses the challenge of annotating large amounts of data for 3D semantic segmentation and proposes an efficient approach called "One Thing One Click" (1T1C). The authors address the limitations of existing annotation methods and aim to reduce the annotation burden by requiring only one label per object, randomly chosen. They also propose a self-training approach with label propagation to overcome sparse annotations and improve the network's performance. The paper introduces a relation network to measure similarity among 3D super-voxels and a memory bank for better similarity measurement. Experimental results on ScanNet-v2 and S3DIS datasets demonstrate the effectiveness of the proposed method, surpassing existing weakly supervised approaches and achieving comparable results to fully supervised segmentation. The 1T1C scheme significantly reduces annotation time and the self-training approach improves overall performance. Overall, the paper presents an efficient and effective solution for 3D point cloud annotation and weakly supervised semantic segmentation.