Convolutional neural classifiers have shown impressive performance in various scenarios, but they heavily rely on large quantities of labeled images for the specific classes of interest. However, in real-world scenarios, obtaining a large number of annotated images for rare categories can be challenging and expensive. This problem becomes more pronounced in settings like robotics, where models need to quickly adapt without waiting for offline data collection. To address this issue, metric learning techniques, which replace the standard linear classifier with a class-agnostic distance function, have emerged as a promising approach. These techniques determine class membership based on the distance in the latent space from known class points. Simple distance functions like cosine and Euclidean distance, as well as more complex and learned parametric options, have shown notable improvements in classification performance.However, a common problem in these techniques is that convolutional feature extractors produce feature maps that characterize appearance at spatial locations, while the distance functions require a vector representation of the entire image. Existing approaches like global average-pooling discard spatial details by averaging information from different parts of the image, and flattening the feature map loses the individual features' location information. To overcome these trade-offs, the authors propose Feature Map Reconstruction Networks (FRN), which preserve spatial detail while disentangling it from location. FRN accomplishes this by reconstructing feature maps as a weighted sum of support features from images belonging to the same class. The reconstruction error is used as the class score, where images from the same class are easier to reconstruct compared to images from different classes.Unlike prior methods, FRN solves the feature map reconstruction as a ridge regression problem, allowing for a rapid closed-form solution with a single learned soft constraint. The resulting reconstructions are discriminative and semantically rich, making FRN simpler and more powerful than other reconstruction-based approaches. The authors validate the effectiveness of FRN on four fine-grained few-shot classification datasets and two general few-shot recognition benchmarks, achieving superior performance compared to other methods. These results hold for both shallow and deep network architectures.