Identity swapping techniques, such as DeepFakes, FaceSwap, FaceShifter, and SimSwap, have made it easy to create fake faces, but there is still room for improvement in terms of performance and visual realism. Additionally, face manipulation can provide challenging samples for face forensics. Evaluating the face swapping task requires considering both the fidelity of the source and target face identities and attributes. Existing methods like FaceShifter and SimSwap have addressed attribute preservation challenges but still have limitations. In this paper, we focus on heterogeneous identity swapping, which involves fitting new identities to target faces in diverse domains. We propose an Identity-Guided Face Inpainting (IGFI) framework to improve the controllability and generalization of face swapping models. Our method preserves the facial attributes of the target, including pose, expression, lighting, and hairstyle, while also improving visual quality in low-resolution scenes. We extract identity features from the source face and use a 3D fitting model to extract target attributes. We then recombine these factors using a Styled Face Inpainting Network (SFI-Net) to achieve efficient and controllable IGFI. We also propose a Joint Refinement Network (JR-Net) that refines identity, attributes, and boundary fusion. Our approach achieves high-quality identity-guided face swaps in various heterogeneous domains, including photorealistic and non-photorealistic styles. Our contributions include introducing a solution for the heterogeneous identity swapping task, proposing an effective IGFI framework with SFI-Net and JR-Net, and achieving high fidelity and generalization in diverse domains.