The paper introduces a novel approach called Transitional Adaptation of Pretrained Model (TAPM) for vision-to-language generation tasks. Existing models in this field consist of a visual encoder, a language generator, and a mechanism to combine the two modules. However, these models do not account for the substantial differences between the visual encoder and the language generator, which are separately trained on large sets of visual and text data. This can lead to issues such as dissonance between visual specificity and language fluency, catastrophic forgetting of language generation capability, and insufficient conditioning on the visual context. To address these challenges, TAPM proposes an explicit visual adaptation step to harmonize the visual encoder with the pretrained language models. This adaptation step can be trained using only visual inputs, like images or videos without text labels. The paper presents extensive experiments demonstrating the effectiveness of TAPM in improving the captioning quality of various language models. It also introduces a sequential coherence loss and two critical recipes for TAPM's success. The approach is evaluated in sequential video captioning and sequential image captioning tasks, achieving state-of-the-art performance in both scenarios.