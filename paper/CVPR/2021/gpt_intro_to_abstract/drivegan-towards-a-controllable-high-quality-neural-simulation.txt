The ability to simulate is essential for intelligence in both animals and robots. Simulation plays a crucial role in decision-making and planning, allowing animals to successfully navigate their environment and autonomous vehicles to be tested in safety-critical scenarios before real-world deployment. Simulators also enable fair comparisons between different autonomous driving systems by providing control over the repeatability of scenarios. However, existing simulators often require significant effort in content creation and complex behavior modeling. In this paper, we propose a data-driven approach to simulation, specifically focusing on the development of DriveGAN, a controllable neural simulator for generating high-fidelity real-world scenes. DriveGAN learns from sequences of video footage and associated actions to create a disentangled latent space for image transitions. It allows users to control various aspects of the environment, such as weather and object placement, and can re-simulate observed scenarios. By training DriveGAN on 160 hours of real driving data, we demonstrate its ability to surpass existing neural simulators in terms of fidelity and control over the environment, making it a pioneering driving simulator of its kind.