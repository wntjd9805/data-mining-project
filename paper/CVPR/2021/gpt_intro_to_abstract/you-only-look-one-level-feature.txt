This paper introduces the concept of feature pyramids in object detection, specifically focusing on the divide-and-conquer approach used by feature pyramid networks (FPN). Traditional FPNs rely on the fusion of multiple level features, leading to complex fusion methods or neural architecture search. However, the divide-and-conquer functionality of FPNs has been largely overlooked. To explore the influence of both multi-scale feature fusion and divide-and-conquer functionalities, the authors conduct experiments using the RetinaNet model. They compare the performance of Multiple-in-Multiple-out (MiMo), Single-in-Multiple-out (SiMo), Multiple-in-Single-out (MiSo), and Single-in-Single-out (SiSo) encoders. Surprisingly, the SiMo encoder, which does not perform feature fusion, achieves comparable results to the MiMo encoder. This suggests that the C5 feature contains sufficient context for object detection on various scales, highlighting the importance of the divide-and-conquer functionality over multi-scale feature fusion.Based on these findings, the authors propose a new approach called You Only Look One-level Feature (YOLOF) that utilizes only a single C5 feature for detection. YOLOF addresses the optimization problem in object detection by properly structuring the encoder to extract multi-scale contexts and applying a uniform matching mechanism to handle the imbalance problem with positive anchors.In experiments, YOLOF achieves comparable results to RetinaNet while being 2.5 times faster. In single feature detection, YOLOF matches the performance of DETR but converges much faster. The paper concludes by highlighting the importance of FPN's divide-and-conquer solution to the optimization problem in dense object detection and presenting YOLOF as a simple and efficient baseline without relying on FPN. Extensive experiments and comparisons with RetinaNet and DETR demonstrate the effectiveness and speed improvements of YOLOF.