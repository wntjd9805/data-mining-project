Automated document understanding is a crucial research area in computer science, with the potential to reduce manual document workflows and improve efficiency. However, document parsing remains challenging, particularly in terms of understanding contextual information and incorporating cues from multiple data modalities. In this paper, we propose a task-agnostic representation learning framework for document images that incorporates textual, visual, and positional information. We utilize a contextualized attention mechanism to model relationships among document components and introduce separate branches for textual and visual representation learning. Additionally, we propose a modality-adaptive attention mechanism for effective fusion of language and vision features. Furthermore, our framework leverages self-supervised learning to obtain a generic representation from unlabeled documents, which can then be fine-tuned for downstream applications. We compare our approach, SelfDoc, with a similar model called LayoutLM and evaluate its performance on document entity recognition, classification, and clustering tasks. Our results demonstrate that SelfDoc outperforms other pre-training and task-specific models, highlighting its potential for enhancing document analysis and intelligence. Overall, our work contributes to the advancement of automated document understanding by introducing a novel self-supervised learning framework that incorporates multimodal information and achieves superior performance with fewer training samples.