This paper introduces a two-stage framework for human pose transfer, with the aim of generating photo-realistic images of a person in a given target pose. The first stage involves predicting the target semantic parsing maps, which provide pose information and specify each body region. This helps the model handle complex poses and generate more robust and accurate results. The second stage incorporates semantic region adaptive normalization to generate the target image, utilizing per-region styles extracted from the source appearance. This approach addresses the challenges of inconsistent poses and self-occlusion. Experimental results on DeepFashion and Market-1501 datasets demonstrate that the proposed framework outperforms state-of-the-art methods, producing more consistent and photo-realistic person images. The contributions of this work include the use of predicted target semantic maps for improved pose transfer and reconstruction quality, as well as the superiority of the proposed framework in generating favorable and realistic person images.