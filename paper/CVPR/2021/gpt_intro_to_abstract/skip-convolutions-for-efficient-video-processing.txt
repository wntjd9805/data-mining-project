In the field of computer science, there has been a growing interest in developing neuromorphic sensors and processing algorithms that mimic the way our brains perceive and process information. One such example is event-based cameras and spiking neural networks, which have shown promise in efficiently processing video data. However, the lack of efficient training algorithms has hampered the success of spiking neural networks compared to conventional models. While there have been attempts to map spiking networks to conventional networks, these have mostly been limited to shallow architectures and simple problems.In this paper, we propose Skip-Convolutions (Skip-Convs) as a method to enhance the efficiency of convolutional networks for video stream processing. Instead of treating a video as a sequence of still images, we represent it as a series of changes across frames and network activations. We refer to these changes as residual frames. By reformulating the standard convolution operation, we are able to efficiently compute features only in regions with significant changes, while skipping the others. Each convolutional layer is coupled with a gating function, which is learned to distinguish important residual regions from background regions that can be safely ignored.Skip-Convs offer the advantage of adaptability, allowing for efficiency adjustments based on the input. In contrast to previous studies on conditional computation in images, we argue that distinguishing important and non-important regions is more challenging in still images. However, residual frames provide a strong prior on relevant regions, making the design of effective gating functions easier. As a result, Skip-Convs achieve a much higher cost reduction in videos compared to what has been previously reported for images.Our contributions in this work include: a simple reformulation of convolution that allows for computation on sparse residual frames; the introduction of two gating functions, Norm gate and Gumbel gate, which effectively decide whether to process or skip each location; the extension of these gating functions to generate structured sparsity for efficient hardware implementations; a general formulation of Skip-Conv that extends the idea to a broader range of transformations and operations; and extensive experiments on different tasks and state-of-the-art network architectures, demonstrating a consistent reduction in cost without sacrificing accuracy.Overall, our proposed Skip-Convolutions provide a novel approach to speeding up convolutional networks for video stream inference, offering significant efficiency gains without compromising accuracy.