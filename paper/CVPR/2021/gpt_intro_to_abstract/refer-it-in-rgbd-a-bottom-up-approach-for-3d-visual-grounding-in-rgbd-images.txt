This paper introduces a novel task of 3D visual grounding in single-view RGBD images using referring expressions. The goal is to estimate the bounding box of the referred object, even if it is only partially observed. Previous works have focused on visual grounding in images and videos, but they are not readily applicable to single-view RGBD images. To address this, the authors propose a bottom-up neural approach that matches the query expression to the input RGBD image and generates a content-aware heatmap. An adaptive search-and-match strategy is then employed to generate and refine the 3D object proposals to the final bounding box. The authors also contribute a large-scale dataset, SUNRefer, to facilitate future studies in visual grounding in single-view RGBD images. Experimental results show that the proposed method outperforms state-of-the-art methods on both SUNRefer and the ScanRefer dataset. This work makes three key contributions: introducing the task of 3D visual grounding in single-view RGBD images, proposing a content-aware, bottom-up approach that outperforms existing methods, and providing a large-scale dataset of referring phrases and ground-truth bounding boxes.