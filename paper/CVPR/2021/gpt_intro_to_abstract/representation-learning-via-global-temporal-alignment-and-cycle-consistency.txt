Temporal sequences, such as videos, provide valuable information and constraints for learning tasks. Most existing research on temporal sequence analysis focuses on global signal-level distinctions, such as action classification, which requires large amounts of labeled data. In this paper, we propose a weakly-supervised training approach for representation learning that can make fine-grained temporal distinctions. Previous approaches to fine-grained understanding of sequential signals rely on fully-supervised training methods, which are expensive and subjective. Our approach involves aligning pairs of temporal sequences depicting the same process and learning an embedding function to support this alignment. Our learned embeddings capture human pose and fine-grained temporal distinctions while being invariant to appearance, camera viewpoint, and background. We introduce a novel dynamic time warping formulation for scoring global alignments, which considers the temporal ordering of matches in sequences. The formulation includes a differentiable smoothMin operator that selects each path extension and has a contrastive effect across paths. Pairwise embedding similarities are defined as probabilities using the softmax operator, allowing us to optimize the loss by finding the maximum probability of any feasible alignment. Additionally, we propose a global cycle-consistency loss to further enforce temporal alignment. Our extensive evaluations and comparisons with previous methods demonstrate significant performance improvements on tasks requiring fine-grained temporal distinctions. We also present two downstream applications, 3D pose reconstruction, and audio-visual retrieval. Our code and trained models are available at the provided GitHub repository.