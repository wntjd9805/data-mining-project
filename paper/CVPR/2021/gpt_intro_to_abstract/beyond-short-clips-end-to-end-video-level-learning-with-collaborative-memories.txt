In recent years, end-to-end learning of 3D convolutional networks (3D CNNs) has become the dominant approach for video classification. However, current video models primarily optimize over short, fixed-length clips rather than the entire video, due to computational constraints. This clip-based learning framework has limitations in capturing long-range temporal structure and may not accurately represent video-level labels. Previous attempts to overcome these limitations involve separate networks or ad-hoc backbones.This paper proposes an end-to-end learning framework that optimizes the classification model using video-level information collected from multiple temporal locations of the video. A collaborative memory mechanism is introduced, where contextual information from multiple clips is shared back to enhance individual clip representations. This allows the model to capture long-range temporal dependencies and encode the relation between local clips and the global video-level context.Experimental results demonstrate the effectiveness and generality of the proposed framework. Significant accuracy gains are achieved across different state-of-the-art architectures, with negligible computational overhead and no increase in memory requirements. The framework is also extended to action detection, producing significant improvements without the need for additional information. The major contributions of this paper include a new framework for end-to-end learning of video-level dependencies, a collaborative memory mechanism for information exchange across clips, and state-of-the-art results in action recognition and detection tasks.