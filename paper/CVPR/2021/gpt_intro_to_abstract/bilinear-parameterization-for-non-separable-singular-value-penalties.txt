Low rank approximation and factorization methods are commonly used in various computer vision problems. Traditional approaches enforce a particular rank by restricting the number of columns of the factors. However, this approach has been shown to be prone to convergence issues. An alternative approach is to optimize directly over the elements of the matrix while applying penalties to the singular values. This approach has been popularized by the work on nuclear norms and their generalizations. However, these convex regularizers do not always yield low enough rank solutions for certain computer vision problems and need to be combined with thresholding schemes. Non-convex penalty functions are also used but can lead to slow convergence. In this paper, we propose second-order methods for a general class of objectives with non-convex and discontinuous penalty functions. We reformulate these objectives into bilinear objectives that can be accurately approximated with quadratic functions, allowing for rapid convergence with second-order methods. We introduce a relaxation method that guarantees global optimizers and propose a modified VarPro algorithm that offers superior performance for challenging computer vision problems.