In this paper, we address the problem of few-shot segmentation in computer vision, where the task is to segment objects in a query image with only a few support images available. Current few-shot segmentation networks rely on either prototypical feature learning or affinity learning to match features and transfer object masks from support to query images. However, prototypical learning loses spatial information and affinity learning is prone to overfitting. To overcome these shortcomings, we propose the Adaptive Superpixel-guided Network (ASGNet), which adaptively changes the number and spatial extent of prototypes based on the image content. We introduce two modules, Superpixel-guided Clustering (SGC) and Guided Prototype Allocation (GPA), to extract adaptive prototypes and allocate them to query features. ASGNet achieves top-performing results on Pascal-5i/COCO-20i datasets, outperforming the state-of-the-art methods in the 5-shot setting. Our contributions include a flexible and adaptive approach to few-shot segmentation, novel modules for prototype extraction and allocation, and improved performance with fewer parameters and less computation.