Hand gesture-to-gesture translation is a crucial task with a wide range of applications in sign language production, data augmentation, and human-computer interactions. However, the highly articulated joints and fine-grained texture of the hand pose a challenge in accurately converting source gestures to target gestures while preserving identity information. Previous methods have utilized 2D sparse joint representation, but they have limited representation capability and suffer from ambiguity. In this paper, we propose a model-aware gesture-to-gesture translation framework that incorporates hand prior and extracts hand representations in a more informative way. We utilize a fully differentiable statistical hand model that maps latent pose and shape embeddings to high-dimensional hand mesh representations. This model allows for more detailed hand reconstruction while filtering out irrational hand poses. Additionally, we build dense topology maps and transformation flows between the source and target hand meshes to preserve the structure information. Our method achieves state-of-the-art performance on two benchmark datasets, demonstrating improved spatial structure and finer details in the generated gesture images. Overall, our contributions include proposing the first model-aware gesture-to-gesture translation framework and introducing hand prior with hand meshes as an intermediate representation, along with alternative extraction methods and attention mechanisms for enhancing the translated hand gestures.