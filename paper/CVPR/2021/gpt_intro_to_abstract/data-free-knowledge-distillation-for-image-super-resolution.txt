Deep convolutional neural networks (CNNs) have made significant advancements in computer vision tasks. However, the heavy computational requirements of these networks limit their deployment on resource-constrained devices. To address this issue, various techniques such as quantization, pruning, and knowledge distillation have been proposed to compress and accelerate pre-trained CNN models. However, these methods typically require access to the original training data, which may not be available due to privacy or transmission constraints. To overcome this limitation, recent research has focused on data-free model compression and acceleration. This approach aims to compress and accelerate pre-trained models without relying on the original training dataset. Existing methods for data-free compression, such as using metadata, synthesizing training data, and generative adversarial networks, have been explored for image recognition tasks. However, these methods are not directly applicable to tasks like super-resolution, where semantic information is not involved and ground-truth images are the original high-resolution images.In this paper, we propose a new data-free knowledge distillation framework for super-resolution. Our approach utilizes a generator network to approximate the original training data from a pre-trained teacher network. We develop a reconstruction loss function that takes into account the similarity between the synthetic images and their super-resolution results. Additionally, an adversarial loss is incorporated to prevent model collapse, and progressive distillation is employed to alleviate training difficulties. Experimental results on benchmark datasets demonstrate the effectiveness of our framework in learning a portable network from a pre-trained model without any training data.The rest of the paper is organized as follows. Section 2 presents a review of related work on model compression in super-resolution and data-free knowledge distillation methods. Section 3 explains our data-free distillation method for image super-resolution. Section 4 provides experimental results on several benchmark datasets and models, and Section 5 concludes the paper.