Deep neural networks (DNNs) have been successful in capturing the relationship between features and labels when provided with a large dataset. However, the process of accurately annotating labels can be costly and time-consuming, leading to noisy datasets and potentially misleading correlations. Moreover, label noise can vary on an instance-dependent basis, further impacting the training outcome. Previous approaches to addressing instance-dependent label noise have primarily focused on loss correction and estimating noise rates. This paper proposes a novel second-order approach that utilizes additional second-order statistics to improve the robustness of learning with instance-dependent label noise. The main contributions of this work are as follows:1. Departing from previous methods that rely on first-order statistics, such as the expectation of model predictions, we highlight the importance of using second-order statistics, including covariance terms, when dealing with instance-dependent label noise.2. With perfect knowledge of the covariance terms, a new loss function is identified, which transforms the expected risk under instance-dependent label noise to a risk with only class-dependent label noise. This simplification enables the application of existing solutions.3. Efficient estimation techniques are proposed for estimating the second-order statistics, and the worst-case performance guarantee of the approach is proven for cases where the covariance terms cannot be perfectly estimated.4. The proposed second-order approach is evaluated on the CIFAR10 and CIFAR100 datasets with synthetic instance-dependent label noise and the Clothing1M dataset with real-world human label noise.Overall, this paper introduces a novel second-order approach that leverages additional statistics to improve the robustness of learning with instance-dependent label noise. The theoretical guarantees and experimental results demonstrate the effectiveness of the proposed method.