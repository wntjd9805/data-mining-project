This paper addresses the problem of inferring the physical geometry and material properties from observed images in computer vision. The ability to create realistic 3D models from standard photos has implications for recognition, robotics, computer graphics, and democratizing 3D content creation. The authors present an approach that estimates a volumetric 3D representation from images of a scene under arbitrary known lighting conditions. This representation allows for the rendering of high-quality novel images from unseen viewpoints and under novel lighting conditions. The authors discuss the progress made by the vision and graphics research communities towards the goal of novel view synthesis using Neural Radiance Fields (NeRF). NeRF synthesizes photorealistic images by mapping 3D locations in a scene to a continuous field of volume density and color. However, NeRF does not provide a solution for relighting, as it ignores the interactions between incoming light and the material properties of surfaces.To address the challenge of simulating the attenuation and reflection of light by particles in a neural volumetric representation, the authors propose a method called Neural Reflectance and Visibility Fields (NeRV). NeRV incorporates surface normals, material properties, and simulates the transport of light sources according to physically based rendering rules.The authors highlight the challenge of efficient computation of visibility in NeRV's neural volumetric representation. Existing approaches in graphics literature rely on precomputed data structures, which are not suitable for the dynamic optimization process in NeRV. To overcome this, the authors train a multi-layer perceptron (MLP) to act as a lookup table into a visibility field during rendering, effectively reducing the computational burden of estimating volume rendering integrals.The resulting NeRV model supports relighting and view synthesis, enabling the recovery of a NeRF-like model that can handle arbitrary environment lighting and one-bounce indirect illumination. This is a significant improvement over previous solutions limited to controlled settings with single point light illumination.