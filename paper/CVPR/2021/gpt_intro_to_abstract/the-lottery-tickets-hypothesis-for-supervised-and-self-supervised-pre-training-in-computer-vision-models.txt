Deep neural networks pretrained on large-scale datasets have become popular as general-purpose feature extractors. Traditional unsupervised pre-training methods have been surpassed by supervised pre-training on ImageNet, which allows the network to learn a hierarchy of generalizable features. Self-supervised pre-training methods, such as simCLR and MoCo, have also shown promising results by using unlabeled data in pre-training.Scaling up pre-training in computer vision (CV) has been achieved through the use of big networks and task-agnostic pre-training with unlabeled data. However, the size of pre-trained models can make fine-tuning computationally demanding. This paper aims to explore the possibility of aggressively reducing the complexity of pre-trained models without compromising their downstream transferability.To address this, the paper turns to the lottery ticket hypothesis (LTH), which investigates the sparse trainable subnetworks within dense networks. The study seeks to find universal matching subnetworks that can transfer to multiple downstream tasks without performance degradation. The paper presents a comprehensive experimental study on supervised and self-supervised pre-trained CV models to identify these matching subnetworks.The study finds matching subnetworks at different levels of sparsity in pre-trained weights from various pre-training methods. These subnetworks are shown to be transferable across different downstream tasks with nearly the same accuracies. The results also show that matching subnetworks found at pre-trained initialization are more sensitive to structure perturbations, and different pre-training methods yield diverse mask structures and perturbation sensitivities.Overall, this work demonstrates the potential of replacing large pre-trained models with smaller subnetworks, enabling more efficient downstream tuning without sacrificing transfer performance. It sheds light on the relevance of the lottery ticket hypothesis in the context of pre-training in computer vision.