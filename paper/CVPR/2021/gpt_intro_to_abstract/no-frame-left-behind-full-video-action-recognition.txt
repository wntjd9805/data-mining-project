Videos are often of arbitrary length with actions occurring at arbitrary moments, making video recognition challenging. Current methods for video recognition use convolutional neural networks (CNNs) on coarsely sub-sampled frames to deal with the computational feasibility of processing all frames. However, sub-sampling can miss crucial frames for accurate action recognition. In this paper, we propose a method called Full Video Action Recognition that leverages all video frames without sub-sampling. We analyze the computational infeasibility of training CNNs on full videos in terms of memory and calculations. While the calculations can be done in parallel, the memory required for storing activations and gradients becomes prohibitively large for even a few minutes of video. Existing approaches that trade off memory for compute are not applicable to videos as they still need to store each frame. To address these challenges, we introduce an efficient approach that clusters frame activations along the temporal dimension, aggregating each cluster to a single representation. We utilize Hamming distances over frame activations for computational speed and assume that similar activations have similar gradients. The aggregated representations approximate the individual frame activations, allowing us to effectively train using all frames without losing important information. We also propose an end-to-end trainable approach for grouping video frames based on temporally localized clustering and Hamming distances. Our experiments demonstrate that our method performs comparably to state-of-the-art methods on benchmark datasets such as UCF101, HMDB51, Breakfast, and Something-Something V1 and V2. Overall, our approach enables efficient and accurate action recognition by leveraging all video frames for training.