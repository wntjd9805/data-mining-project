Semantic segmentation is a crucial task in computer vision, with applications in autonomous driving, medical imaging, and human parsing. While significant progress has been made in semantic segmentation for daytime images, challenges arise when dealing with nighttime images due to indiscernible regions and visual hazards. Building high-quality pixel-level annotations for nighttime images is challenging, making it difficult to train deep neural networks for segmentation. To address this problem, domain adaptation methods have been proposed to transfer segmentation models from daytime to nighttime without using label information in the nighttime domain. However, these methods often rely on pre-processing stages and struggle to generate transferred images with consistent semantic information. In this paper, we present a novel one-stage domain adaptation network (DANNet) for nighttime semantic segmentation. Our approach uses adversarial learning and is based on the newly released Dark Zurich dataset, which contains unlabeled day-night image pairs roughly aligned using GPS recordings. DANNet performs a multi-target adaptation from Cityscapes data to Dark Zurich daytime and nighttime data. We incorporate an image relighting subnetwork to align intensity distributions across different domains, and use a weight-sharing semantic segmentation network to make predictions and perform adversarial learning for consistent layout across domains. We also design a re-weighting strategy to handle misalignment and boost prediction accuracy for small objects. Experimental results on the Dark Zurich and Nighttime Driving datasets show that DANNet achieves state-of-the-art performance in nighttime semantic segmentation. Our contributions include proposing a one-stage adaptation framework, demonstrating the use of pseudo supervision from daytime images for nighttime segmentation, and showcasing the effectiveness of our method through extensive experiments and ablation studies.