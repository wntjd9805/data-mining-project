In this paper, we propose a learning algorithm and model that can leverage both active sensors and RGB cameras for dense scene geometry measurement. Our goal is to develop a robust perception system that can still function effectively when only one modality is available. We specifically focus on utilizing a single monocular RGB camera combined with a low-cost active depth sensor that provides few 3D points per scene. Monocular depth prediction has become an important capability for various robotic applications where RGB cameras are commonly used. Recent studies have shown that supervised methods trained on raw videos can enable robots with a single camera to learn and predict dense depth information. However, in practical scenarios, an active range sensor is often available and can be used to provide additional supervision during training or inference, known as depth completion. Sparse depth information, even from a few pixels, has been shown to improve performance, and should not be disregarded.Our main contribution is a novel architecture called Sparse Auxiliary Networks (SANs), which allows a monocular depth prediction network to also perform depth completion when sparse 3D measurements are available at inference time. The SANs architecture utilizes a sparse depth convolutional encoder to incorporate depth information into the skip connections of state-of-the-art encoder-decoder networks. We conduct a thorough experimental evaluation on challenging outdoor and indoor datasets, namely KITTI, DDAD, and NYUv2, to demonstrate the effectiveness of our SAN architecture. Our results show improved monocular depth prediction performance, setting a new state of the art in this task.