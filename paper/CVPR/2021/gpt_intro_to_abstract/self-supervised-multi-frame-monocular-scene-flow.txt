Scene flow estimation is the task of estimating the 3D structure and motion of a dynamic scene, which is important for autonomous navigation systems. Various approaches have been proposed using different input data such as stereo images, RGB-D sequences, or 3D point clouds. Recently, monocular scene flow approaches have shown the possibility of estimating 3D scene flow using just a pair of consecutive monocular frames. These approaches eliminate the need for expensive sensor setups and can be performed using a simple monocular camera. However, the accuracy of such approaches is limited by the use of only two frames as input and the challenges of optimizing Convolutional Neural Networks (CNNs) for multiple tasks. Semi-supervised methods have achieved promising accuracy by combining CNNs with energy minimization or sequentially estimating optical flow and depth. However, these methods are not real-time efficient. In this paper, we propose a self-supervised monocular scene flow approach that surpasses the accuracy of previous real-time methods while maintaining their advantages. We introduce an improved two-frame backbone network and a multi-frame formulation that propagates the estimate from the previous time step for more accurate results. Our approach utilizes multiple consecutive frames, which are commonly available in real-world scenarios. Our contributions include an advanced two-frame backbone network with a split-decoder design, a multi-frame network based on overlapping triplets of frames and temporal propagation using a convolutional LSTM, an occlusion-aware census transform, and a gradient detaching strategy to improve accuracy and training stability. We validate our design choices through an ablation study and demonstrate the superiority of our approach in terms of accuracy and computational efficiency compared to previous methods.