Panoramic novel view synthesis is a crucial problem in the field of computer vision, particularly in emerging domains such as Virtual Reality (VR). While 360◦ cameras have gained popularity, capturing panoramas at a limited set of locations can restrict the degree-of-freedom (DoF) of scene viewing. This paper focuses on the indoor scenario, which is common in many applications, and proposes a layout-guided method for synthesizing panoramic novel views from a single indoor panorama.Previous work on novel view synthesis from a single perspective image has shown promising results, but the performance degrades when larger camera rotations and translations are involved. Panoramas, with their 360◦ field-of-view (FoV), inherently support rotational viewpoint change and only require consideration of camera translations. Additionally, panoramas provide omnidirectional information, making it possible to accommodate larger camera translations.The main challenge in panoramic novel view synthesis is recovering the missing areas caused by viewpoint change, particularly with large camera translations. To address this challenge, the proposed method exploits the structural information present in panoramas, particularly the room layout, which includes ceiling-wall boundaries, floor-wall boundaries, and wall-wall boundaries. Structural information has proven effective in guiding content generation processes, as demonstrated by previous work on image inpainting.The proposed method consists of three stages: feature extraction using convolutional neural networks (CNNs), transformation of the extracted features and room layout through spherical geometric transformations to synthesize the target panorama, and estimation and enforcement of the room layout consistency in the synthesized panorama.To facilitate research in this novel task, the paper introduces a large-scale photo-realistic dataset that considers both small and large camera translations. The dataset is built upon the Structured3D dataset and includes high-fidelity rendered images, which closely resemble realistic application scenarios.The main contributions of this paper include being the first to tackle the synthesis of panoramic novel views from a single indoor panorama, proposing a layout-guided method that can handle large camera translations, providing a new high-quality and challenging dataset for this task, and achieving state-of-the-art performance on this novel task through experimental results. The proposed method also demonstrates generalizability to real datasets.