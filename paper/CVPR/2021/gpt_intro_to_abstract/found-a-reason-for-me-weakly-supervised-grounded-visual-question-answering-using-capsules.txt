The use of Visual Question Answering (VQA) systems in real-life applications has increased, including answering questions based on radiology images, assisting visually impaired individuals, and human-robot interactions. However, as these systems mature, it becomes important to understand how answers are generated and assess if they are based on the correct cues. Evaluating the accuracy of grounding, which refers to how well the answer is based on relevant objects in the image, is crucial for assessing overall correctness. Current approaches rely on pre-trained object detection models or require object annotations, limiting the scope of known object classes. This paper focuses on weakly supervised visual grounding based on VQA supervision without using object-level information. The authors propose extending VQA frameworks with capsule networks, which have shown promise in image interpretability. A soft-masking procedure is introduced to select relevant capsules based on the input question, allowing for accurate grounding. The proposed method is evaluated on the GQA and CLEVR datasets, showcasing improved grounding abilities compared to existing methods.