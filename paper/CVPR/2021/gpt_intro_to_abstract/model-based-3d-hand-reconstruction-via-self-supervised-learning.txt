The reconstruction of 3D human hands from a single image is a crucial task in computer vision, with applications including action recognition, augmented reality, sign language translation, and human-computer interaction. However, due to the diversity of hands and the challenges of monocular 3D reconstruction, image-based 3D hand reconstruction remains a difficult problem. Recent advancements have made progress in recovering 3D representations of human hands from images, primarily focusing on predicting 3D hand pose from depth images or RGB images. However, some applications require surface information of the hand, which is not provided by sparse joint representations. Previous methods have attempted to predict the triangle mesh through per-vertex coordinate regression or parametric hand model deformation, but these approaches require extensive 3D annotations for training. Moreover, hand texture, which is important in certain applications such as virtual reality, has not been extensively explored in previous works.In this paper, we propose a self-supervised approach for 3D hand reconstruction that eliminates the need for manual annotation. Our key insight is that the 2D cues in the image space and the texture information contained in the image can provide important structural and texture representations for learning without relying on 3D annotations. To address the labor-intensive task of annotating 2D hand keypoints, we propose to extract geometric representations from unlabeled images to facilitate shape reconstruction and utilize the texture information in the input image for texture modeling.Our main contributions lie in the development of a self-supervised 3D hand reconstruction network, which accurately predicts 3D joints, mesh, and texture from a single image without the need for annotated training data. We introduce a trainable 2D keypoint estimator that enhances the 3D reconstruction through a mutual improvement mechanism, using a novel 2D-3D consistency loss. Additionally, we incorporate a hand texture estimation module for self-supervised learning of vivid hand texture. We evaluate our self-supervised method on challenging datasets and demonstrate comparable performance to previous fully-supervised methods.Overall, our approach enables the training of a neural network that can accurately predict 3D pose, shape, texture, and camera viewpoint from a single hand image, without the reliance on ground truth annotations.