This paper addresses the problem of few-shot object detection (FSOD), which aims to develop algorithms that can detect objects with a few labeled samples per class. While deep neural networks excel in tasks with a large amount of labeled data, they struggle with tasks that have limited labeled data. In contrast, humans are able to learn new concepts with only a few examples. This discrepancy motivates the development of few-shot learning techniques. The focus of this paper is on FSOD, which is a more challenging and practical case than few-shot classification. The authors propose a novel approach called the Transformation Invariant Principle (TIP) to improve the generalization ability of deep neural networks on transformed images. The TIP utilizes consistency regularization to guide the learning of representative features and introduces proposal consistency regularization for detecting objects in transformed images. The proposed TIP method achieves state-of-the-art results on benchmark datasets and can be extended to the semi-supervised FSOD scenario. Overall, this paper makes three contributions: (1) addressing the challenging FSOD problem from the perspective of sample expansion, (2) proposing the TIP approach for improved generalization ability on transformed images, and (3) demonstrating the effectiveness of the approach in both fully-supervised and semi-supervised FSOD settings.