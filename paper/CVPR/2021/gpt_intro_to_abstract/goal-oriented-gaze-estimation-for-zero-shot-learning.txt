Zero-shot learning (ZSL) is a challenging image classification setting that aims to classify test images of unseen classes based on semantic descriptions of both seen and unseen classes. In the generalized ZSL (GZSL) setting, the test images may belong to both seen and unseen classes. Early works on ZSL build embedding between seen classes and their attributes, but they have a bias towards seen classes under the GZSL setting. To address this issue, feature generation approaches have been proposed to generate unseen classes and convert ZSL into a conventional classification problem. Most existing methods in ZSL extract global features from pre-trained or end-to-end trainable models, but they may not effectively represent the fine-grained information between seen and unseen images. Recent attention-based end-to-end models have attempted to exploit semantic vectors as guidance to learn more discriminative part features, but they overlook the importance of discriminative attribute localization. Inspired by human gaze behavior, which focuses on parts of an object with discriminative attributes, this paper proposes a novel goal-oriented Gaze Estimation Method for Zero-Shot Learning (GEM-ZSL). The GEM consists of an attention module, attention transition module, and attribute localization module, which learn different attribute regions. The proposed method predicts human gaze based on attribute descriptions and transforms it into attribute attention for zero-shot recognition. It effectively localizes discriminative attributes, enhancing the discrimination of global features for ZSL. The contribution of this paper includes the proposal of the GEM-ZSL method, which mimics the human cognitive process for recognizing unseen classes. It effectively localizes discriminative attributes and improves the discrimination of global features for ZSL. Experimental results on three ZSL benchmarks demonstrate that the proposed method achieves superior or competitive performance compared to state-of-the-art methods. Additionally, quantitative and qualitative results on gaze estimation experiments validate the effectiveness of the GEM.