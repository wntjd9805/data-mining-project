In deep learning applications, the design of neural architectures is crucial for improving performance. Traditional methods rely on manual design, which is time-consuming and may not yield highly-performant architectures. Automated neural architecture search has gained attention as it can find optimal architectures that achieve the best performance on validation datasets. However, existing methods are susceptible to adversarial attacks, where small perturbations in input data significantly change prediction outcomes. While many defense methods focus on training the weights of a deep neural network (DNN) to be robust, the robustness is also influenced by the architecture itself. This paper introduces a novel approach for robust neural architecture search (NAS) that maximizes robustness metrics. Two differentiable metrics are defined: one based on certified lower bounds and the other based on the Jacobian norm bound. These metrics measure how resilient the architecture is to input perturbations. The proposed method is applicable to differentiable NAS methods and is robust against various norm-bound attacks. Experimental results on CIFAR-10, ImageNet, and MNIST datasets demonstrate that the proposed approach is more robust to attacks, more accurate in the absence of attacks, and achieves higher certified lower bounds compared to baseline methods. The paper concludes with a summary of related works, the proposed method and experiments, and the overall conclusions.