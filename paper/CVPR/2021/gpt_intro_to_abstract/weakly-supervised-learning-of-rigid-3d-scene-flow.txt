Understanding dynamic 3D environments is a fundamental challenge in computer vision and robotics, especially for tasks like self-driving and robot navigation. To address this challenge, there has been growing interest in 3D scene flow as a low-level representation of dynamic scenes. Scene flow, which is the 3D motion field of points in a scene, can provide valuable information for tasks like semantic segmentation and motion perception. However, representing dynamics using a free-form velocity field has limitations, as most applications involve rigid object motion. Additionally, acquiring dense supervision for accurate flow estimation is expensive and prone to annotation errors. Many methods have used simulated data or unsupervised approaches to overcome these challenges, but their performance is often suboptimal. In this paper, we propose a scene abstraction approach that leverages the use of rigid objects as basic components to improve flow estimation performance. By dividing the scene into foreground (movable objects) and background (static objects), we can explain background flow as the sensor ego-motion and foreground flow as clusters of rigidly moving entities. This approach allows us to enforce a rigidity constraint and reduces the need for dense flow supervision. Our weakly supervised method outperforms the state of the art, achieving significant gains in accuracy on various benchmarks while providing an interpretable object-level representation of the scene. We contribute a network that incorporates an inductive bias based on the geometry of rigid scene flow, a data-driven method for decomposing the scene into moving agents, and a novel test-time optimization for refining flow predictions. Our method demonstrates superior performance on benchmarks such as FT3D, stereoKITTI, lidarKITTI, and semanticKITTI, as well as generalization capabilities to the waymo-open dataset.