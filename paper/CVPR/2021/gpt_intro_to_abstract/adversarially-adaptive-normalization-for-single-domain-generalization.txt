Deep learning has shown significant success in various domains where training and test data are drawn from the same domain. However, when deep learning models are applied to new domains unseen in the training dataset, they often struggle to maintain consistent performance due to cross-domain distributional shift. To address this issue, previous works have proposed learning domain-invariant features using training data from multiple source domains. However, collecting multi-domain training data is difficult and costly. An alternative, yet less explored solution, is to train the model on a single source domain and enhance its ability to generalize to unseen domains. This emerging learning paradigm is referred to as single domain generalization. Existing works on single domain generalization focus on improving generalization capability through adversarial domain augmentation (ADA), which synthesizes new training images to simulate challenging virtual domains. In this paper, we present a novel approach to single domain generalization called adaptive standardization and rescaling normalization (ASR-Norm). ASR-Norm enhances the model's domain generalization by learning adaptive normalization statistics for each input sample. When used with ADA, ASR-Norm improves the model's ability to generalize across domains compared to traditional normalization approaches. Additionally, we show that ASR-Norm can be viewed as a generic form of other normalization techniques such as batch normalization (BN), instance normalization (IN), layer normalization (LN), group normalization (GN), and switchable normalization (SN). Our contributions include proposing ASR-Norm as a novel adaptive normalization technique, which is the first to incorporate both standardization and rescaling statistics in normalization with neural networks. We also demonstrate that ASR-Norm consistently improves the state-of-the-art ADA approaches on three commonly used single domain generalization benchmarks, with the performance gain becoming more significant as the domain discrepancy increases.