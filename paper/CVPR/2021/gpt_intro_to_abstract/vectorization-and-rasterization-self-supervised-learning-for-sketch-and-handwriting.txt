Deep learning architectures have become popular in computer vision applications, but their success heavily relies on access to large labeled datasets, which are expensive and time-consuming to collect. To overcome this annotation bottleneck, many unsupervised methods propose pre-training feature representations from large unlabeled data. One approach is to define a pretext task that provides free labels, such as colorization or jigsaw solving. This motivates the idea that a network trained on such tasks should encode high-level semantic understanding that can be used for other downstream tasks. This concept has been extended to subdomains like human pose estimation and depth estimation.In this paper, we propose a self-supervised method for a distinct class of visual data: sketches and handwriting images. Although these have been studied separately, there are similarities in how they are captured and represented. Both are recorded as a pen tip follows a trajectory on the canvas and rendered as sparse black and white lines. Both also pose challenges such as abstractness and variable levels of detail.We propose a novel self-supervised task that leverages the dual image/vector space representation of sketches and handwriting. We utilize cross-modal translation between image and vector space as a self-supervised task to enhance downstream performance using either representation. Existing self-supervised methods for images or videos are not suitable for sparse black and white handwritten images. Similarly, existing methods for vector sequences do not handle the stroke-by-stroke nature of handwriting. Our framework addresses these limitations and can learn powerful representations for both image and vector domain analysis tasks.We compare our approach with existing self-supervised methods and demonstrate that our cross-view rasterization/vectorization synthesis approach outperforms them. Our framework is simple to implement and achieves superior results compared to fully supervised alternatives.