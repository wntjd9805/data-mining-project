Despite the progress made in computer vision tasks, deep neural networks have been found to be vulnerable to minor adversarial perturbations, leading to incorrect predictions. This vulnerability poses a significant security risk to computer vision models. Efforts have been made to improve adversarial robustness, focusing mainly on balanced datasets. However, real-world data often exhibit long-tailed distributions, which present challenges not only to recognition tasks but also to robustness against adversarial attacks. While long-tailed recognition has gained attention, the study of adversarial robustness in long-tailed datasets remains largely unexplored.To shed light on the challenges of adversarial robustness in long-tailed recognition, this paper compares networks trained on balanced and long-tailed versions of CIFAR-10. Adversarial training (AT) is adopted as a defense method to provide basic adversarial robustness. The evaluation includes per-class classification recalls on clean and perturbed images, measuring natural accuracy and robust accuracy, respectively. The results show a drop in natural accuracy from head to tail, as expected in traditional long-tailed recognition. A similar decreasing trend occurs in robust accuracy, and the drop in natural accuracy is more significant for tail classes when adversarial training is applied.To improve both recognition performance and adversarial robustness, the paper proposes a combination of re-balancing strategies for long-tailed recognition and the adversarial training framework. Additionally, the paper introduces a scale-invariant classifier to replace the final linear layer, analyzing the benefits of a normalized embedding space in enhancing resistance against attacks. The approach aligns the idea of data re-balancing with the cosine classifier, utilizing class-aware and pair-aware margins during training and boundary adjustment at inference.The paper focuses on addressing the issue of imbalance in long-tailed datasets and explores the effect of imbalance separately from the issue of sample numbers. It is found that eliminating prediction priors is crucial to reducing the vulnerability of tail classes to adversarial attacks.The contributions of this paper are: 1) The first exploration of adversarial robustness in long-tailed distribution, which significantly contributes to real-world robustness. 2) A systematic study on existing long-tailed recognition methods and their combination with adversarial training, providing valuable insights. 3) The development of an effective approach, RoBal, which achieves state-of-the-art performance in both natural and robust accuracy.