Image-only and LiDAR-only methods have shown promising results for end-to-end driving. However, these approaches typically assume near-ideal behavior from other agents and lack the 3D information of the scene required in adversarial scenarios. To address this, we propose TransFuser, a Multi-Modal Fusion Transformer that integrates representations from both image and LiDAR modalities to capture the global context of the 3D scene. Our approach incorporates the attention mechanism of transformers into the feature extraction layers of different modalities, allowing for better handling of complex urban scenarios. We evaluate TransFuser in CARLA with adversarial scenarios and achieve state-of-the-art performance. Our code and trained models are available for further experimentation.