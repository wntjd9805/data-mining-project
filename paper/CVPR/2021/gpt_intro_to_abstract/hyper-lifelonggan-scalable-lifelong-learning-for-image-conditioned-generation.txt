This paper introduces Hyper-LifelongGAN, a generic continual learning framework that addresses the issue of catastrophic forgetting in generative tasks. While previous approaches have been limited in their ability to maintain the generation quality of previous tasks while learning new ones, Hyper-LifelongGAN employs hypernetworks and knowledge distillation techniques to preserve the quality of generated outputs. The framework factorizes conventional convolutional filters into dynamic task-specific base filters and a deterministic weight matrix shared across tasks, reducing the memory requirements. Experimental results demonstrate the scalability and effectiveness of Hyper-LifelongGAN in learning new generation tasks without forgetting previous ones.