Image captioning is a task in computer vision that aims to generate descriptive and meaningful captions for images. While current captioning models have achieved impressive performance in terms of accuracy-based evaluation metrics, they often produce generic descriptions and lack the same level of control as humans in the caption generation process. To address this issue, recent research has focused on introducing extra control signals, known as Controllable Image Captioning (CIC), to guide the caption generation process. These control signals can be subjective, such as sentiments and emotions, or objective, such as content and structure. However, existing objective control signals overlook two essential characteristics: event-compatibility and sample-suitability. In this paper, we propose a new objective control signal called Verb-specific Semantic Roles (VSR) that considers both event-compatibility and sample-suitability. VSR consists of a verb that captures the activity in the image and user-defined semantic roles that categorize how objects participate in the activity. We train a grounded semantic role labeling model to identify and ground entities for each role, and then use a semantic structure planner to rank the verb and semantic roles and generate descriptive structures. Finally, we use an RNN-based role-shift captioning model to generate captions by focusing on different roles sequentially. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach in achieving better controllability and generating diverse captions. Overall, our contributions include the proposal of VSR as a new control signal for CIC, the automatic learning of human-like semantic structures, and the achievement of state-of-the-art controllability and diversity in image captioning.