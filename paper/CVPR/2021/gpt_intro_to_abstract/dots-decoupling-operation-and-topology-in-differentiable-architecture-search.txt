Neural Architecture Search (NAS) has gained significant attention for its potential to automatically find optimal architectures in large search spaces. However, previous NAS approaches have been computationally expensive and time-consuming. To address this issue, one-shot methods and differentiable architecture search (DARTS) have been proposed, which reduce search costs by adopting weight sharing strategies. However, a question arises regarding the accuracy of the edge importance rankings in DARTS. This paper introduces Decoupling Operation and Topology Search (DOTS) to address these problems. DOTS decouples the topology representation from the operation weights and introduces a continuously relaxed topology search space. The operation and topology search processes are also decoupled, allowing for the incorporation of existing gradient-based NAS methods. Furthermore, the topology search is performed in a shrunk supernet, making it more efficient and accurate. Experimental results show that DOTS achieves high accuracy on CIFAR10 and ImageNet datasets with significantly reduced search costs. The proposed approach outperforms DARTS and has the potential to search for more complex structures by supporting a flexible number of edges in the searched cell.