Abstract:Place recognition and scene localization in large-scale and complex environments pose significant challenges in computer vision applications such as autonomous driving, robot navigation, and augmented reality. Traditional image-based methods for place recognition often suffer from performance degradation in the face of illumination and appearance variations caused by weather and seasonal changes. To address this issue, point cloud data acquired from LiDAR offers accurate and invariant 3D information. In this paper, we propose a novel network called SOE-Net (Self-attention and Orientation Encoding Network) for point cloud-based place recognition. SOE-Net incorporates a point orientation encoding module to capture neighborhood information and a self-attention unit to incorporate spatial relationships among local descriptors. We also introduce a new loss function, HPHN quadruplet loss, which improves the performance of large-scale point cloud-based retrieval. Experimental results on benchmark datasets demonstrate the superiority of SOE-Net over state-of-the-art methods, achieving a recall of 89.37% at top 1 retrieval on the Oxford RobotCar dataset.