Visual phrase grounding, which involves associating regions with phrases in an image's description, is a crucial problem in computer vision and natural language processing. Current approaches rely on fully supervised paradigms, requiring expensive annotations of bounding boxes for each phrase. Weakly supervised grounding, which only utilizes images and their sentence descriptions during training, has garnered attention as a more scalable alternative. However, weakly supervised grounding faces the challenge of distinguishing among concurrent visual concepts. To address this, existing methods leverage object detectors during training and/or inference to provide high-quality object regions and category labels. It remains unclear, though, how best to utilize external object detectors for weakly supervised grounding. To address this gap, this paper presents a principled approach to distilling knowledge from a generic object detector for weakly supervised phrase grounding. The proposed method uses contrastive learning and learns a score function between region-phrase pairs guided by two levels of similarity constraints. The first level of similarity is distilled from object detection outputs, aligning predicted region-phrase scores with soft targets computed based on object names and candidate phrases. The second level of similarity is computed through greedy matching and supervised by ground-truth image-sentence pairs. During inference, the method compares each image region to candidate phrases using the learned score function, without relying on object detection.The proposed method is evaluated on the Flickr30K Entities and ReferItGame datasets, demonstrating its superiority compared to state-of-the-art weakly supervised phrase grounding methods. It outperforms previous methods, including those using strong object detectors at test time or similar contrastive loss. The experiments also highlight the various components and best practices of the proposed model. Overall, this work aims to provide new ideas and practices for weakly supervised image-text grounding.