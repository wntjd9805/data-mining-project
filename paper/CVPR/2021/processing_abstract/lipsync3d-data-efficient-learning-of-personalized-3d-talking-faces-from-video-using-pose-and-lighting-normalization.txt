In this paper, we present a video-based learning frame-work for animating personalized 3D talking faces from au-dio. We introduce two training-time data normalizations that signiﬁcantly improve data sample efﬁciency. First, we isolate and represent faces in a normalized space that de-couples 3D geometry, head pose, and texture. This decom-poses the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo con-stancy of skin to isolate and remove spatio-temporal light-ing variations. Together, these normalizations allow sim-ple networks to generate high ﬁdelity lip-sync videos under novel ambient illumination while training with just a single speaker-speciﬁc video. Further, to stabilize temporal dy-namics, we introduce an auto-regressive approach that con-ditions the model on its previous visual state. Human rat-ings and objective metrics demonstrate that our method out-performs contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications en-abled by our framework. 