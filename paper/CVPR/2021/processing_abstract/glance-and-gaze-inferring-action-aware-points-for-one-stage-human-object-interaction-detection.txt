Modern human-object interaction (HOI) detection ap-proaches can be divided into one-stage methods and two-stage ones. One-stage models are more efﬁcient due to their straightforward architectures, but the two-stage models are still advantageous in accuracy. Existing one-stage model-s usually begin by detecting predeﬁned interaction areas or points, and then attend to these areas only for interac-tion prediction; therefore, they lack reasoning steps that dynamically search for discriminative cues. In this paper, we propose a novel one-stage method, namely Glance andGaze Network (GGNet), which adaptively models a set of action-aware points (ActPoints) via glance and gaze step-s. The glance step quickly determines whether each pix-el in the feature maps is an interaction point. The gaze step leverages feature maps produced by the glance step to adaptively infer ActPoints around each pixel in a pro-gressive manner. Features of the reﬁned ActPoints are ag-gregated for interaction prediction. Moreover, we design an action-aware approach that effectively matches each detect-ed interaction with its associated human-object pair, along with a novel hard negative attentive loss to improve the opti-mization of GGNet. All the above operations are conducted simultaneously and efﬁciently for all pixels in the feature maps. Finally, GGNet outperforms state-of-the-art meth-ods by signiﬁcant margins on both V-COCO and HICO-DET benchmarks. Code of GGNet is available at https://github.com/SherlockHolmes221/GGNet. 