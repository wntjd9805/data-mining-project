We investigate the weakly-supervised audio-visual video parsing task, which aims to parse a video into temporal event segments and predict the audible or visible event categories. The task is challenging since there only exist video-level event labels for training, without indicating the temporal boundaries and modalities. Previous works take the overall event labels to supervise both audio and visual model predictions. However, we argue that such overall la-bels harm the model training due to the audio-visual asyn-chrony. For example, commentators speak in a basketball video, but we cannot visually ﬁnd the speakers. In this pa-per, we tackle this issue by leveraging the cross-modal cor-respondence of audio and visual signals. We generate re-liable event labels individually for each modality by swap-ping audio and visual tracks with other unrelated videos. If the original visual/audio data contain event clues, the event prediction from the newly assembled data would still be highly conﬁdent. In this way, we could protect our models from being misled by ambiguous event labels. In addition, we propose the cross-modal audio-visual contrastive learn-ing to induce temporal difference on attention models within videos, i.e., urging the model to pick the current temporal segment from all context candidates. Experiments show we outperform state-of-the-art methods by a large margin. 