Autonomous driving can beneﬁt from motion behavior comprehension when interacting with diverse trafﬁc partic-ipants in highly dynamic environments. Recently, there has been a growing interest in estimating class-agnostic mo-tion directly from point clouds. Current motion estimation methods usually require vast amount of annotated train-ing data from self-driving scenes. However, manually la-beling point clouds is notoriously difﬁcult, error-prone and time-consuming. In this paper, we seek to answer the re-search question of whether the abundant unlabeled data collections can be utilized for accurate and efﬁcient motion learning. To this end, we propose a learning framework that leverages free supervisory signals from point clouds and paired camera images to estimate motion purely via self-supervision. Our model involves a point cloud based structural consistency augmented with probabilistic motion masking as well as a cross-sensor motion regularization to realize the desired self-supervision. Experiments reveal that our approach performs competitively to supervised meth-ods, and achieves the state-of-the-art result when combin-ing our self-supervised model with supervised ﬁne-tuning. 