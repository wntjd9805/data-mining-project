SiamRPN++DiMPLTMUAdversarial attack arises due to the vulnerability of deep neural networks to perceive input samples injected with im-perceptible perturbations. Recently, adversarial attack has been applied to visual object tracking to evaluate the robust-ness of deep trackers. Assuming that the model structures of deep trackers are known, a variety of white-box attack ap-proaches to visual tracking have demonstrated promising results. However, the model knowledge about deep trackers is usually unavailable in real applications. In this paper, we propose a decision-based black-box attack method for visual object tracking. In contrast to existing black-box ad-versarial attack methods that deal with static images for im-age classiﬁcation, we propose IoU attack that sequentially generates perturbations based on the predicted IoU scores from both current and historical frames. By decreasing theIoU scores, the proposed attack method degrades the accu-racy of temporal coherent bounding boxes (i.e., object mo-tions) accordingly. In addition, we transfer the learned per-turbations to the next few frames to initialize temporal mo-tion attack. We validate the proposed IoU attack on state-of-the-art deep trackers (i.e., detection based, correlationﬁlter based, and long-term trackers). Extensive experiments on the benchmark datasets indicate the effectiveness of the proposed IoU attack method. The source code is available at https://github.com/VISION-SJTU/IoUattack. 