We propose a new generative model for 3D garment de-formations that enables us to learn, for the ﬁrst time, a data-driven method for virtual try-on that effectively ad-dresses garment-body collisions.In contrast to existing methods that require an undesirable postprocessing step to ﬁx garment-body interpenetrations at test time, our ap-proach directly outputs 3D garment conﬁgurations that do not collide with the underlying body. Key to our success is a new canonical space for garments that removes pose-and-shape deformations already captured by a new diffused human body model, which extrapolates body surface prop-erties such as skinning weights and blendshapes to any 3D point. We leverage this representation to train a genera-tive model with a novel self-supervised collision term that learns to reliably solve garment-body interpenetrations. We extensively evaluate and compare our results with recently proposed data-driven methods, and show that our method is the ﬁrst to successfully address garment-body contact in unseen body shapes and motions, without compromising re-alism and detail. 