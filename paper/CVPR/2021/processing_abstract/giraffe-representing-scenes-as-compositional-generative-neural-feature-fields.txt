Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be con-trollable. While several recent works investigate how to dis-entangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional genera-tive neural feature ﬁelds allows us to disentangle one or multiple objects from the background as well as individual objects’ shapes and appearances while learning from un-structured and unposed image collections without any ad-ditional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and al-lows for translating and rotating them in the scene as well as changing the camera pose. 