Estimating 3D hand and object pose from a single im-age is an extremely challenging problem: hands and objects are often self-occluded during interactions, and the 3D an-notations are scarce as even humans cannot directly label the ground-truths from a single image perfectly. To tackle these challenges, we propose a uniÔ¨Åed framework for esti-mating the 3D hand and object poses with semi-supervised learning. We build a joint learning framework where we per-form explicit contextual reasoning between hand and object representations. Going beyond limited 3D annotations in a single image, we leverage the spatial-temporal consistency in large-scale hand-object videos as a constraint for generat-ing pseudo labels in semi-supervised learning. Our method not only improves hand pose estimation in challenging real-world dataset, but also substantially improve the object pose which has fewer ground-truths per instance. By training with large-scale diverse videos, our model also generalizes better across multiple out-of-domain datasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object 