Self-attention has been successfully applied to video rep-resentation learning due to the effectiveness of modeling long range dependencies. Existing approaches build the dependencies merely by computing the pairwise correla-tions along spatial and temporal dimensions simultane-ously. However, spatial correlations and temporal corre-lations represent different contextual information of scenes and temporal reasoning. Intuitively, learning spatial con-textual information ﬁrst will beneﬁt temporal modeling. In this paper, we propose a separable self-attention (SSA) module, which models spatial and temporal correlations se-quentially, so that spatial contexts can be efﬁciently used in temporal modeling. By adding SSA module into 2D CNN, we build a SSA network (SSAN) for video representation learning. On the task of video action recognition, our ap-proach outperforms state-of-the-art methods on Something-Something and Kinetics-400 datasets. Our models often outperform counterparts with shallower network and fewer modalities. We further verify the semantic learning abil-ity of our method in visual-language task of video retrieval, which showcases the homogeneity of video representations and text embeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by SSA signiﬁcantly improve the state-of-the-art performance. 