We propose Skip-Convolutions to leverage the large amount of redundancies in video streams and save compu-tations. Each video is represented as a series of changes across frames and network activations, denoted as residu-als. We reformulate standard convolution to be efﬁciently computed on residual frames: each layer is coupled with a binary gate deciding whether a residual is important to the model prediction, e.g. foreground regions, or it can be safely skipped, e.g. background regions. These gates can ei-ther be implemented as an efﬁcient network trained jointly with convolution kernels, or can simply skip the residuals based on their magnitude. Gating functions can also in-corporate block-wise sparsity structures, as required for ef-ﬁcient implementation on hardware platforms. By replac-ing all convolutions with Skip-Convolutions in two state-of-the-art architectures, namely EfﬁcientDet and HRNet, we reduce their computational cost consistently by a factor of 3 ∼ 4× for two different tasks, without any accuracy drop.Extensive comparisons with existing model compression, as well as image and video efﬁciency methods demonstrate that Skip-Convolutions set a new state-of-the-art by effec-tively exploiting the temporal redundancies in videos. 