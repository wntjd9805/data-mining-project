We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to ﬁrst establish pixel-wise dense matches at a coarse level and later reﬁne the good matches at a ﬁne level.In contrast to dense methods that use a cost volume to search corre-spondences, we use self and cross attention layers in Trans-former to obtain feature descriptors that are conditioned on both images. The global receptive ﬁeld provided by Trans-former enables our method to produce dense matches in low-texture areas, where feature detectors usually strug-gle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outper-forms state-of-the-art methods by a large margin. LoFTR also ranks ﬁrst on two public benchmarks of visual local-ization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/. 