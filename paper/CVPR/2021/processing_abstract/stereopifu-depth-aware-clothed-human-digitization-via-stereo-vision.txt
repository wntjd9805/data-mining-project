In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit func-tion representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectiﬁed images.First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware recon-struction. Moreover, the novel relative z-offset is employed to associate predicted high-ﬁdelity human depth and occu-pancy inference, which helps restore ﬁne-level surface de-tails. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Con-sequently, our StereoPIFu can naturally infer the human body’s spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human perfor-mance. Compared with previous works, our StereoPIFu sig-niﬁcantly improves the robustness, completeness, and accu-racy of the clothed human reconstruction, which is demon-strated by extensive experimental results. 