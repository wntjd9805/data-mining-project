Continual learning usually assumes the incoming data are fully labeled, which might not be applicable in real ap-plications. In this work, we consider semi-supervised con-tinual learning (SSCL) that incrementally learns from par-tially labeled data. Observing that existing continual learn-ing methods lack the ability to continually exploit the unla-beled data, we propose deep Online Replay with Discrim-inator Consistency (ORDisCo) to interdependently learn a classiﬁer with a conditional generative adversarial network (GAN), which continually passes the learned data distribu-tion to the classiﬁer. In particular, ORDisCo replays data sampled from the conditional generator to the classiﬁer in an online manner, exploiting unlabeled data in a time- and storage-efﬁcient way. Further, to explicitly overcome the catastrophic forgetting of unlabeled data, we selectively stabilize parameters of the discriminator that are impor-tant for discriminating the pairs of old unlabeled data and their pseudo-labels predicted by the classiﬁer. We exten-sively evaluate ORDisCo on various semi-supervised learn-ing benchmark datasets for SSCL, and show that ORDisCo achieves signiﬁcant performance improvement on SVHN,CIFAR10 and Tiny-ImageNet, compared to strong base-lines. 