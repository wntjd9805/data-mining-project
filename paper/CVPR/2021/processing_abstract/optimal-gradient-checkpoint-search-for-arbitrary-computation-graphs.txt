Deep Neural Networks(DNNs) require huge GPU mem-ory when training on modern image/video databases. Un-fortunately, the GPU memory is physically ﬁnite, which lim-its the image resolutions and batch sizes that could be used in training for better DNN performance. Unlike solutions that require physically upgrade GPUs, the Gradient Check-Pointing(GCP) training trades computation for more mem-ory beyond existing GPU hardware. GCP only stores a subset of intermediate tensors, called Gradient Checkpoints (GCs), during forward. Then during backward, extra local forwards are conducted to compute the missing tensors. The total training memory cost becomes the sum of (1) the mem-ory cost of the gradient checkpoints and (2) the maximum memory cost of local forwards. To achieve maximal mem-ory cut-offs, one needs optimal algorithms to select GCs.Existing GCP approaches rely on either manual input ofGCs or heuristics-based GC search on Linear ComputationGraphs (LCGs), and cannot apply to Arbitrary Computa-tion Graphs(ACGs). In this paper, we present theories and optimal algorithms on GC selection that, for the ﬁrst time, are applicable to ACGs and achieve the maximal memory cut-offs. Extensive experiments show that our approach not only outperforms existing approaches (only applicable onLCGs), and is applicable to a vast family of LCG and ACG networks, such as Alexnet, VGG, ResNet, Densenet, Incep-tion Net and highly complicated DNNs by Network Archi-tecture Search. Our work enables GCP training on ACGs, and cuts off up-to 80% of training memory1 with a moderate time overhead ( 30%-50%). Codes are available2.∼ 