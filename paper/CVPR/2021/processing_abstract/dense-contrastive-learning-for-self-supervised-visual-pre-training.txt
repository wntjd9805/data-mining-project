To date, most existing self-supervised learning methods are designed and optimized for image classiﬁcation. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level predic-tion and pixel-level prediction. To ﬁll this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning (DenseCL), which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images.Compared to the baseline method MoCo-v2, our method introduces negligible computation overhead (only <1% slower), but demonstrates consistently superior perfor-mance when transferring to downstream dense prediction tasks including object detection, semantic segmentation and instance segmentation; and outperforms the state-of-the-art methods by a large margin. Speciﬁcally, over the strongMoCo-v2 baseline, our method achieves signiﬁcant im-provements of 2.0% AP on PASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO in-stance segmentation, 3.0% mIoU on PASCAL VOC seman-tic segmentation and 1.8% mIoU on Cityscapes semantic segmentation.Code and models are available at: https://git.io/DenseCL 