Nested networks or slimmable networks are neural net-works whose architectures can be adjusted instantly dur-ing testing time, e.g., based on computational constraints.Recent studies have focused on a “nested dropout” layer, which is able to order the nodes of a layer by impor-tance during training, thus generating a nested set of sub-networks that are optimal for different conﬁgurations of re-sources. However, the dropout rate is ﬁxed as a hyper-parameter over different layers during the whole training process. Therefore, when nodes are removed, the perfor-mance decays in a human-speciﬁed trajectory rather than in a trajectory learned from data. Another drawback is the generated sub-networks are deterministic networks without well-calibrated uncertainty. To address these two prob-lems, we develop a Bayesian approach to nested neural net-works. We propose a variational ordering unit that draws samples for nested dropout at a low cost, from a proposedDownhill distribution, which provides useful gradients to the parameters of nested dropout. Based on this approach, we design a Bayesian nested neural network that learns the order knowledge of the node distributions.In exper-iments, we show that the proposed approach outperforms the nested network in terms of accuracy, calibration, and out-of-domain detection in classiﬁcation tasks. It also out-performs the related approach on uncertainty-critical tasks in computer vision. 