The Information Bottleneck (IB) provides an information theoretic principle for representation learning, by retaining all information relevant for predicting label while minimiz-ing the redundancy. Though IB principle has been applied to a wide range of applications, its optimization remains a challenging problem which heavily relies on the accurate estimation of mutual information. In this paper, we present a new strategy, Variational Self-Distillation (VSD), which provides a scalable, ﬂexible and analytic solution to essen-tially ﬁtting the mutual information but without explicitly estimating it. Under rigorously theoretical guarantee, VSD enables the IB to grasp the intrinsic correlation between representation and label for supervised training. Further-more, by extending VSD to multi-view learning, we in-troduce two other strategies, Variational Cross-Distillation (VCD) and Variational Mutual-Learning (VML), which sig-niﬁcantly improve the robustness of representation to view-changes by eliminating view-speciﬁc and task-irrelevant in-formation. To verify our theoretically grounded strategies, we apply our approaches to cross-modal person Re-ID, and conduct extensive experiments, where the superior perfor-mance against state-of-the-art methods are demonstrated.Our intriguing ﬁndings highlight the need to rethink the way to estimate mutual information. 