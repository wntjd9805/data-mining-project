Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassiﬁcations on model inputs containing a speciﬁc“trigger.” Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that apply digitally generated patterns as triggers. A critical question remains unanswered: “can backdoor attacks succeed using physi-cal objects as triggers, thus making them a credible threat against deep learning systems in the real world?”We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task.Using 7 physical objects as triggers, we collect a custom dataset of 3205 images of 10 volunteers and use it to study the feasibility of “physical” backdoor attacks under a va-riety of real-world conditions. Our study reveals two keyﬁndings. First, physical backdoor attacks can be highly successful if they are carefully conﬁgured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model’s dependence on key facial features. Sec-ond, four of today’s state-of-the-art defenses against (dig-ital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses.Our study conﬁrms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classiﬁcation tasks. We need new and more robust defenses against backdoors in the physical world. 