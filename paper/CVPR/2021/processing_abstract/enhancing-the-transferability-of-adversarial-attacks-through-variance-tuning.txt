Deep neural networks are vulnerable to adversarial ex-amples that mislead the models with imperceptible pertur-bations. Though adversarial attacks have achieved incredi-ble success rates in the white-box setting, most existing ad-versaries often exhibit weak transferability in the black-box setting, especially under the scenario of attacking models with defense mechanisms. In this work, we propose a new method called variance tuning to enhance the class of iter-ative gradient based attack methods and improve their at-tack transferability. Speciﬁcally, at each iteration for the gradient calculation, instead of directly using the current gradient for the momentum accumulation, we further con-sider the gradient variance of the previous iteration to tune the current gradient so as to stabilize the update direction and escape from poor local optima. Empirical results on the standard ImageNet dataset demonstrate that our method could signiﬁcantly improve the transferability of gradient-based adversarial attacks. Besides, our method could be used to attack ensemble models or be integrated with var-ious input transformations.Incorporating variance tun-ing with input transformations on iterative gradient-based attacks in the multi-model setting, the integrated method could achieve an average success rate of 90.1% against nine advanced defense methods, improving the current best attack performance signiﬁcantly by 85.1% . Code is avail-able at https://github.com/JHL-HUST/VT. 