Appearance-based detectors achieve remarkable perfor-mance on common scenes, beneﬁting from high-capacity models and massive annotated data, but tend to fail for scenarios that lack training data. Geometric motion segmen-tation algorithms, however, generalize to novel scenes, but have yet to achieve comparable performance to appearance-based ones, due to noisy motion estimations and degenerate motion conﬁgurations. To combine the best of both worlds, we propose a modular network, whose architecture is mo-tivated by a geometric analysis of what independent object motions can be recovered from an egomotion ﬁeld. It takes two consecutive frames as input and predicts segmentation masks for the background and multiple rigidly moving ob-jects, which are then parameterized by 3D rigid transforma-⇤Code is available at github.com/gengshan-y/rigidmask. tions. Our method achieves state-of-the-art performance for rigid motion segmentation on KITTI and Sintel. The inferred rigid motions lead to a signiﬁcant improvement for depth and scene ﬂow estimation. 