Recently, a large number of channel attention blocks are proposed to boost the representational power of deep convolutional neural networks (CNNs). These approaches commonly learn the relationship between global contexts and attention activations by fully-connected layers or linear transformations. However, we empirically ﬁnd that though many parameters are introduced, these attention blocks may not learn the relationship well. In this paper, we hypothesize that the relationship is predetermined. Based on this hy-pothesis, we propose a simple yet extremely efﬁcient chan-nel attention block, called Gaussian Context Transformer (GCT), which achieves contextual feature excitation using a Gaussian function that satisﬁes the presupposed relation-ship. According to whether the standard deviation of theGaussian function is learnable, we develop two versions ofGCT: GCT-B0 and GCT-B1. GCT-B0 is a parameter-free channel attention block by ﬁxing the standard deviation. It directly maps global contexts to attention activations with-out learning. In contrast, GCT-B1 is a parameterized ver-sion, which adaptively learns the standard deviation to en-hance the mapping ability. Extensive experiments on Im-ageNet and MS COCO benchmarks demonstrate that ourGCTs lead to consistent improvements across various deepCNNs and detectors. Compared with a bank of state-of-the-art channel attention blocks, such as SE [17] and ECA [42], our GCTs are superior in effectiveness and efﬁciency. 