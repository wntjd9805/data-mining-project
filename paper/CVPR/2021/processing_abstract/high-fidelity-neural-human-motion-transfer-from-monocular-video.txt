Video-based human motion transfer creates video ani-mations of humans following a source motion. Current methods show remarkable results for tightly-clad subjects.However, the lack of temporally consistent handling of plau-sible clothing dynamics, including ﬁne and high-frequency details, signiﬁcantly limits the attainable visual quality. We address these limitations for the ﬁrst time in the literature and present a new framework which performs high-ﬁdelity and temporally-consistent human motion transfer with nat-ural pose-dependent non-rigid deformations, for several types of loose garments. In contrast to the previous tech-niques, we perform image generation in three subsequent stages: synthesizing human shape, structure, and appear-ance. Given a monocular RGB video of an actor, we train a stack of recurrent deep neural networks that generate these intermediate representations from 2D poses and their temporal derivatives. Splitting the difﬁcult motion trans-fer problem into subtasks that are aware of the temporal motion context helps us to synthesize results with plausible dynamics and pose-dependent detail. It also allows artistic control of results by manipulation of individual framework stages.In the experimental results, we signiﬁcantly out-perform the state-of-the-art in terms of video realism. The source code is available at https://graphics.tu-bs. de/publications/kappel2020high-fidelity. 