Non-visual imaging sensors are widely used in the indus-try for different purposes. Those sensors are more expen-sive than visual (RGB) sensors, and usually produce images with lower resolution. To this end, Cross-Modality Super-Resolution methods were introduced, where an RGB image of a high-resolution assists in increasing the resolution of a low-resolution modality. However, fusing images from different modalities is not a trivial task, since each multi-modal pair varies greatly in its internal correlations. For this reason, traditional state-of-the-arts which are trained on external datasets often struggle with yielding an artifact-free result that is still loyal to the target modality character-istics.We present CMSR, a single-pair approach for Cross-Modality Super-Resolution.The network is internally trained on the two input images only, in a self-supervised manner, learns their internal statistics and correlations, and applies them to up-sample the target modality. CMSR con-tains an internal transformer which is trained on-the-ï¬‚y to-gether with the up-sampling process itself and without su-pervision, to allow dealing with pairs that are only weakly aligned. We show that CMSR produces state-of-the-art su-per resolved images, yet without introducing artifacts or ir-relevant details that originate from the RGB image only. 