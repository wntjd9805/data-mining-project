Perceptual metrics based on features of deep Convolu-tional Neural Networks (CNNs) have shown remarkable success when used as loss functions in a range of com-puter vision problems and signiﬁcantly outperform classi-cal losses such as L1 or L2 in pixel space. The source of this success remains somewhat mysterious, especially since a good loss does not require a particular CNN architecture nor a particular training method. In this paper we show that similar success can be achieved even with losses based on features of a deep CNN with random ﬁlters. We use the tool of inﬁnite CNNs to derive an analytical form for percep-tual similarity in such CNNs, and prove that the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distribu-tions of small patches in the two images. We use this equiva-lence to propose a simple metric for comparing two images which directly computes the MMD between local distribu-tions of patches in the two images. Our proposed metric is simple to understand, requires no deep networks, and gives comparable performance to perceptual metrics in a range of computer vision tasks. 