Style transfer aims to render the content of a given image in the graphical/artistic style of another image. The funda-mental concept underlying Neural Style Transfer (NST) is to interpret style as a distribution in the feature space of aConvolutional Neural Network, such that a desired style can be achieved by matching its feature distribution. We show that most current implementations of that concept have im-portant theoretical and practical limitations, as they only partially align the feature distributions. We propose a novel approach that matches the distributions more precisely, thus reproducing the desired style more faithfully, while still be-ing computationally efﬁcient. Speciﬁcally, we adapt the dual form of Central Moment Discrepancy (CMD), as re-cently proposed for domain adaptation, to minimize the dif-ference between the target style and the feature distribution of the output image. The dual interpretation of this met-ric explicitly matches all higher-order centralized moments and is therefore a natural extension of existing NST methods that only take into account the ﬁrst and second moments.Our experiments conﬁrm that the strong theoretical proper-ties also translate to visually better style transfer, and better disentangle style from semantic image content. 