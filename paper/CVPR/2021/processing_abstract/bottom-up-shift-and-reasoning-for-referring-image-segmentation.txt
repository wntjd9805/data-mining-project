Referring image segmentation aims to segment the ref-erent that is the corresponding object or stuff referred by a natural language expression in an image. Its main chal-lenge lies in how to effectively and efﬁciently differentiate between the referent and other objects of the same category as the referent. In this paper, we tackle the challenge by jointly performing compositional visual reasoning and ac-curate segmentation in a single stage via the proposed novelBottom-Up Shift (BUS) and Bidirectional Attentive Reﬁne-ment (BIAR) modules. Speciﬁcally, BUS progressively lo-cates the referent along hierarchical reasoning steps im-plied by the expression. At each step, it locates the corre-sponding visual region by disambiguating between similar regions, where the disambiguation bases on the relation-ships between regions. By the explainable visual reasoning,BUS explicitly aligns linguistic components with visual re-gions so that it can identify all the mentioned entities in the expression. BIAR fuses multi-level features via a two-way attentive message passing, which captures the visual details relevant to the referent to reﬁne segmentation re-sults. Experimental results demonstrate that the proposed method consisting of BUS and BIAR modules, can not only consistently surpass all existing state-of-the-art algorithms across common benchmark datasets but also visualize inter-pretable reasoning steps for stepwise segmentation. Code is available at https://github.com/incredibleXM/BUSNet. 