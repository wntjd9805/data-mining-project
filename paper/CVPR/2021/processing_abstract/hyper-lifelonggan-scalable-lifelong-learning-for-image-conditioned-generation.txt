Deep neural networks are susceptible to catastrophic forgetting: when encountering a new task, they can only remember the new task and fail to preserve its ability to ac-complish previously learned tasks. In this paper, we study the problem of lifelong learning for generative models and propose a novel and generic continual learning frameworkHyper-LifelongGAN which is more scalable compared with state-of-the-art approaches. Given a sequence of tasks, the conventional convolutional ﬁlters are factorized into the dy-namic base ﬁlters which are generated using task speciﬁcﬁlter generators, and deterministic weight matrix which lin-early combines the base ﬁlters and is shared across different tasks. Moreover, the shared weight matrix is multiplied by task speciﬁc coefﬁcients to introduce more ﬂexibility in com-bining task speciﬁc base ﬁlters differently for different tasks.Attributed to the novel architecture, the proposed method can preserve or even improve the generation quality at a low cost of parameters. We validate Hyper-LifelongGAN on diverse image-conditioned generation tasks, extensive ab-lation studies and comparisons with state-of-the-art models are carried out to show that the proposed approach can ad-dress catastrophic forgetting effectively. 