3D convolutional networks are prevalent for video recognition. While achieving excellent recognition perfor-mance on standard benchmarks, they operate on a sequence of frames with 3D convolutions and thus are computation-ally demanding. Exploiting large variations among differ-ent videos, we introduce Ada3D, a conditional computa-tion framework that learns instance-speciﬁc 3D usage poli-cies to determine frames and convolution layers to be used in a 3D network. These policies are derived with a two-head lightweight selection network conditioned on each in-put video clip. Then, only frames and convolutions that are selected by the selection network are used in the 3D model to generate predictions. The selection network is optimized with policy gradient methods to maximize a re-ward that encourages making correct predictions with lim-ited computation. We conduct experiments on three video recognition benchmarks and demonstrate that our method achieves similar accuracies to state-of-the-art 3D models while requiring 20% − 50% less computation across differ-ent datasets. We also show that learned policies are trans-ferable and Ada3D is compatible to different backbones and modern clip selection approaches. Our qualitative analysis indicates that our method allocates fewer 3D convolutions and frames for “static” inputs, yet uses more for motion-intensive clips. 