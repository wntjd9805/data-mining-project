(cid:52)(cid:88)(cid:72)(cid:85)(cid:92)(cid:29)(cid:3)(cid:51)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:81)(cid:3)(cid:87)(cid:75)(cid:85)(cid:82)(cid:90)(cid:76)(cid:81)(cid:74)(cid:3)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:83)(cid:82)(cid:81)(cid:74)(cid:3)(cid:69)(cid:68)(cid:79)(cid:79)(cid:86)(cid:3)(cid:76)(cid:81)(cid:87)(cid:82)(cid:3)(cid:69)(cid:72)(cid:72)(cid:85)(cid:3)(cid:73)(cid:76)(cid:79)(cid:79)(cid:72)(cid:71)(cid:3)(cid:70)(cid:88)(cid:83)(cid:86)(cid:17)Video grounding aims to localize a moment from an untrimmed video for a given textual query. Existing ap-proaches focus more on the alignment of visual and lan-guage stimuli with various likelihood-based matching or regression strategies, i.e., P (Y |X). Consequently, these models may suffer from spurious correlations between the language and video features due to the selection bias of the dataset. 1) To uncover the causality behind the model and data, we Ô¨Årst propose a novel paradigm from the per-spective of the causal inference, i.e., interventional video grounding (IVG) that leverages backdoor adjustment to deconfound the selection bias based on structured causal model (SCM) and do-calculus P (Y |do(X)). Then, we present a simple yet effective method to approximate the unobserved confounder as it cannot be directly sampled from the dataset. 2) Meanwhile, we introduce a dual con-trastive learning approach (DCL) to better align the text and video by maximizing the mutual information (MI) be-tween query and video clips, and the MI between start/end frames of a target moment and the others within a video to learn more informative visual representations. Experiments on three standard benchmarks show the effectiveness of our approaches. 