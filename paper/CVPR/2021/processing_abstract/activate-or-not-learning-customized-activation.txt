We present a simple, eﬀective, and general activa-tion function we term ACON which learns to activate the neurons or not. Interestingly, we ﬁnd Swish, the recent popular NAS-searched activation, can be interpreted as a smooth approximation to ReLU. Intuitively, in the same way, we approximate the more general Maxout family to our novel ACON family, which remarkably improves the performance and makes Swish a special case of ACON.Next, we present meta-ACON, which explicitly learns to optimize the parameter switching between non-linear (activate) and linear (inactivate) and provides a new de-sign space. By simply changing the activation function, we show its eﬀectiveness on both small models and highly optimized large models (e.g. it improves the ImageNet top-1 accuracy rate by 6.7% and 1.8% on MobileNet-0.25 and ResNet-152, respectively). Moreover, our novelACON can be naturally transferred to object detection and semantic segmentation, showing that ACON is an eﬀective alternative in a variety of tasks. Code is available at https: // github. com/ nmaac/ acon . 