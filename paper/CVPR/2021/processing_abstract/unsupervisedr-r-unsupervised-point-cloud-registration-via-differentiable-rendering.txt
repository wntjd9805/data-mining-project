Aligning partial views of a scene into a single whole is essential to understanding one’s environment and is a key component of numerous robotics tasks such as SLAM andSfM. Recent approaches have proposed end-to-end systems that can outperform traditional methods by leveraging pose supervision. However, with the rising prevalence of cam-eras with depth sensors, we can expect a new stream of raw RGB-D data without the annotations needed for su-pervision. We propose UnsupervisedR&R: an end-to-end unsupervised approach to learning point cloud registration from raw RGB-D video. The key idea is to leverage dif-ferentiable alignment and rendering to enforce photomet-ric and geometric consistency between frames. We evaluate our approach on indoor scene datasets and ﬁnd that we out-perform existing traditional approaches with classical and learned descriptors while being competitive with supervised geometric point cloud registration approaches. 