We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and cor-responding feature vectors that encode the appearance of this point in the input images. The core of SVS is view-dependent on-surface feature aggregation, in which direc-tional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features syn-thesized in this way for all pixels. The method is composed of differentiable modules and is trained end-to-end. It sup-ports spatially-varying view-dependent importance weight-ing and feature transformation of source images at each point; spatial and temporal stability due to the smooth dependence of on-surface feature aggregation on the tar-get view; and synthesis of view-dependent effects such as specular reï¬‚ection. Experimental results demonstrate thatSVS outperforms state-of-the-art view synthesis methods both quantitatively and qualitatively on three diverse real-world datasets, achieving unprecedented levels of realism in free-viewpoint video of challenging large-scale scenes.Code is available at https://github.com/intel-isl/StableViewSynthesis 12216