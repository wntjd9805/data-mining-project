As a vital problem in classiﬁcation-oriented trans-fer, unsupervised domain adaptation (UDA) has attracted widespread attention in recent years. Previous UDA meth-ods assume the marginal distributions of different domains are shifted while ignoring the discriminant information in the label distributions. This leads to classiﬁcation perfor-mance degeneration in real applications. In this work, we focus on the conditional distribution shift problem which is of great concern to current conditional invariant model-s. We aim to seek a kernel covariance embedding for con-ditional distribution which remains yet unexplored. Theo-retically, we propose the Conditional Kernel Bures (CKB) metric for characterizing conditional distribution discrep-ancy, and derive an empirical estimation for the CKB metric without introducing the implicit kernel feature map. It pro-vides an interpretable approach to understand the knowl-edge transfer mechanism. The established consistency the-ory of the empirical estimation provides a theoretical guar-antee for convergence. A conditional distribution matching network is proposed to learn the conditional invariant and discriminative features for UDA. Extensive experiments and analysis show the superiority of our proposed model. 