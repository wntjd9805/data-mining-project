Contrastive learning relies on constructing a collection of negative examples that are sufﬁciently hard to discrim-inate against positive queries when their representations are self-trained. Existing contrastive learning methods ei-ther maintain a queue of negative samples over minibatch-es while only a small portion of them are updated in an iteration, or only use the other examples from the curren-t minibatch as negatives. They could not closely track the change of the learned representation over iterations by up-dating the entire queue as a whole, or discard the useful information from the past minibatches. Alternatively, we present to directly learn a set of negative adversaries play-ing against the self-trained representation. Two players, the representation network and negative adversaries, are alter-nately updated to obtain the most challenging negative ex-amples against which the representation of positive queries will be trained to discriminate. We further show that the negative adversaries are updated towards a weighted com-bination of positive queries by maximizing the adversari-al contrastive loss, thereby allowing them to closely track the change of representations over time. Experiment results demonstrate the proposed Adversarial Contrastive (AdCo) model not only achieves superior performances (a top-1 ac-curacy of 73.2% over 200 epochs and 75.7% over 800 e-pochs with linear evaluation on ImageNet), but also can be pre-trained more efﬁciently with much shorter GPU time and fewer epochs. The source code is available at https://github.com/maple-research-lab/AdCo. 