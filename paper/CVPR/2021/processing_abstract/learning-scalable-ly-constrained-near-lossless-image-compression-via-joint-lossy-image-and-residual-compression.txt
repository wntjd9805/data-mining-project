We propose a novel lossy image and residual joint compression framework for learning ℓ∞-constrained near-lossless image compression. Speciﬁcally, we obtain a lossy reconstruction of the raw image through lossy image com-pression and uniformly quantize the corresponding resid-ual to satisfy a given tight ℓ∞ error bound. Suppose that the error bound is zero, i.e., lossless image compression, we formulate the joint optimization problem of compress-ing both the lossy image and the original residual in terms of variational auto-encoders and solve it with end-to-end training. To achieve scalable compression with the error bound larger than zero, we derive the probability model of the quantized residual by quantizing the learned prob-ability model of the original residual, instead of training multiple networks. We further correct the bias of the de-rived probability model caused by the context mismatch be-tween training and inference. Finally, the quantized resid-ual is encoded according to the bias-corrected probability model and is concatenated with the bitstream of the com-pressed lossy image. Experimental results demonstrate that our near-lossless codec achieves the state-of-the-art per-formance for lossless and near-lossless image compression, and achieves competitive PSNR while much smaller ℓ∞ er-ror compared with lossy image codecs at high bit rates. 