In this paper, we tackle the problem of convolutional neu-ral network design.Instead of focusing on the design of the overall architecture, we investigate a design space that is usually overlooked, i.e. adjusting the channel conﬁgura-tions of predeﬁned networks. We ﬁnd that this adjustment can be achieved by shrinking widened baseline networks and leads to superior performance. Based on that, we artic-ulate the “heterogeneity hypothesis”: with the same train-ing protocol, there exists a layer-wise differentiated net-work architecture (LW-DNA) that can outperform the origi-nal network with regular channel conﬁgurations but with a lower level of model complexity.The LW-DNA models are identiﬁed without extra com-putational cost or training time compared with the orig-inal network. This constraint leads to controlled experi-ments which direct the focus to the importance of layer-wise speciﬁc channel conﬁgurations.LW-DNA models come with advantages related to overﬁtting, i.e. the rel-ative relationship between model complexity and dataset size. Experiments are conducted on various networks and datasets for image classiﬁcation, visual tracking and image restoration. The resultant LW-DNA models consistently outperform the baseline models. Code is available at https://github.com/ofsoundof/Heterogeneity_Hypothesis.git. 