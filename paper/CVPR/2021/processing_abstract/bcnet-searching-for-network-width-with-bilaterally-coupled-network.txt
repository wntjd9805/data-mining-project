Searching for a more compact network width recently serves as an effective way of channel pruning for the de-ployment of convolutional neural networks (CNNs) under hardware constraints. To fulﬁll the searching, a one-shot supernet is usually leveraged to efﬁciently evaluate the per-formance w.r.t. different network widths. However, current methods mainly follow a unilaterally augmented (UA) prin-ciple for the evaluation of each width, which induces the training unfairness of channels in supernet.In this pa-per, we introduce a new supernet called Bilaterally Cou-In BCNet, pled Network (BCNet) to address this issue. each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we leverage a stochas-tic complementary strategy for training the BCNet, and pro-pose a prior initial population sampling method to boost the performance of the evolutionary search. Extensive experi-ments on benchmark CIFAR-10 and ImageNet datasets in-dicate that our method can achieve state-of-the-art or com-peting performance over other baseline methods. Moreover, our method turns out to further boost the performance ofNAS models by reﬁning their network widths. For example, with the same FLOPs budget, our obtained EfﬁcientNet-B0 achieves 77.36% Top-1 accuracy on ImageNet dataset, sur-passing the performance of original setting by 0.48%. 