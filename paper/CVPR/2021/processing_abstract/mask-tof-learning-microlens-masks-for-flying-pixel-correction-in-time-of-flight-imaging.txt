We introduce Mask-ToF, a method to reduce ﬂying pixels (FP) in time-of-ﬂight (ToF) depth captures. FPs are perva-sive artifacts which occur around depth edges, where light paths from both an object and its background are integrated over the aperture. This light mixes at a sensor pixel to pro-duce erroneous depth estimates, which can adversely affect downstream 3D vision tasks. Mask-ToF starts at the source of these FPs, learning a microlens-level occlusion mask which effectively creates a custom-shaped sub-aperture for each sensor pixel. This modulates the selection of fore-ground and background light mixtures on a per-pixel basis and thereby encodes scene geometric information directly into the ToF measurements. We develop a differentiableToF simulator to jointly train a convolutional neural net-work to decode this information and produce high-ﬁdelity, low-FP depth reconstructions. We test the effectiveness ofMask-ToF on a simulated light ﬁeld dataset and validate the method with an experimental prototype. To this end, we manufacture the learned amplitude mask and design an op-tical relay system to virtually place it on a high-resolutionToF sensor. We ﬁnd that Mask-ToF generalizes well to real data without retraining, cutting FP counts in half. 