We propose a new framework for understanding and rep-resenting related salient events in a video using visual se-mantic role labeling. We represent videos as a set of re-lated events, wherein each event consists of a verb and mul-tiple entities that fulﬁll various roles relevant to that event.To study the challenging task of semantic role labeling in videos or VidSRL, we introduce the VidSitu benchmark, a large scale video understanding data source with 29K 10-second movie clips richly annotated with a verb and†Part of the work was done during Arka’s internship at PRIOR@AI2 semantic-roles every 2 seconds. Entities are co-referenced across events within a movie clip and events are connected to each other via event-event relations. Clips in VidSitu are drawn from a large collection of movies (∼3K) and have been chosen to be both complex (∼4.2 unique verbs within a video) as well as diverse (∼200 verbs have more than 100 annotations each). We provide a comprehensive analysis of the dataset in comparison to other publicly available video understanding benchmarks, several illustrative baselines and evaluate a range of standard video recognition models.Our code and dataset is available at vidsitu.org. 5589