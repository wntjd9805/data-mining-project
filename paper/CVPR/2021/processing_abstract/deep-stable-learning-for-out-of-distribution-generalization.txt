Approaches based on deep neural networks have achieved striking performance when testing data and train-ing data share similar distribution, but can signiÔ¨Åcantly fail otherwise. Therefore, eliminating the impact of distri-bution shifts between training and testing data is crucial for building performance-promising deep models. Conven-tional methods assume either the known heterogeneity of training data (e.g. domain labels) or the approximately equal capacities of different domains. In this paper, we con-sider a more challenging case where neither of the above assumptions holds. We propose to address this problem by removing the dependencies between features via learning weights for training samples, which helps deep models get rid of spurious correlations and, in turn, concentrate more on the true connection between discriminative features and labels. Extensive experiments clearly demonstrate the ef-fectiveness of our method on multiple distribution general-ization benchmarks compared with state-of-the-art counter-parts. Through extensive experiments on distribution gen-eralization benchmarks including PACS, VLCS, MNIST-M, and NICO, we show the effectiveness of our method com-pared with state-of-the-art counterparts. 