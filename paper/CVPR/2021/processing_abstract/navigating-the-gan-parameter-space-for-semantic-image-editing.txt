Generative Adversarial Networks (GANs) are currently an indispensable tool for visual editing, being a stan-dard component of image-to-image translation and image restoration pipelines. Furthermore, GANs are especially advantageous for controllable generation since their latent spaces contain a wide range of interpretable directions, well suited for semantic editing operations. By gradually changing latent codes along these directions, one can pro-duce impressive visual effects, unattainable without GANs.In this paper, we signiÔ¨Åcantly expand the range of vi-sual effects achievable with the state-of-the-art models, likeStyleGAN2. In contrast to existing works, which mostly op-erate by latent codes, we discover interpretable directions in the space of the generator parameters. By several sim-ple methods, we explore this space and demonstrate that it also contains a plethora of interpretable directions, which-  Windows  + are an excellent source of non-trivial semantic manipula-tions. The discovered manipulations cannot be achieved by transforming the latent codes and can be used to edit both synthetic and real images. We release our code and models and hope they will serve as a handy tool for further efforts on GAN-based image editing. 