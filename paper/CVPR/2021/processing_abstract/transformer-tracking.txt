Self-attentionScreen ShotsCross-attentionCorrelation acts as a critical role in the tracking ﬁeld, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to con-sider the similarity between the template and the search re-gion. However, the correlation operation itself is a local linear matching process, leading to lose semantic informa-tion and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms.Is there any better feature fusion method than correlation?To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Speciﬁcally, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer track-ing (named TransT) method based on the Siamese-like fea-ture extraction backbone, the designed attention-based fu-sion mechanism, and the classiﬁcation and regression head.Experiments show that our TransT achieves very promis-ing results on six challenging datasets, especially on large-scale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 f ps on GPU. Code and models are available at https://github.com/chenxin-dlut/TransT. 