We present a novel boundary-aware loss term for seman-tic segmentation using an inverse-transformation network, which efﬁciently learns the degree of parametric transfor-mations between estimated and target boundaries. This plug-in loss term complements the cross-entropy loss in capturing boundary transformations and allows consistent and signiﬁcant performance improvement on segmentation backbone models without increasing their size and com-putational complexity. We analyze the quantitative and qualitative effects of our loss function on three indoor and outdoor segmentation benchmarks, including Cityscapes,NYU-Depth-v2, and PASCAL, integrating it into the train-ing phase of several backbone networks in both single-task and multi-task settings. Our extensive experiments show that the proposed method consistently outperforms base-lines, and even sets the new state-of-the-art on two datasets. 