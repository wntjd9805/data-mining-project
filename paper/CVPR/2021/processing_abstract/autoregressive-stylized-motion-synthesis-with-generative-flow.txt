Motion style transfer is an important problem in many computer graphics and computer vision applications, in-cluding human animation, games, and robotics. Most exist-ing deep learning methods for this problem are supervised and trained by registered motion pairs. In addition, these methods are often limited to yielding a deterministic output, given a pair of style and content motions. In this paper, we propose an unsupervised approach for motion style trans-fer by synthesizing stylized motions autoregressively using a generative ﬂow model M. M is trained to maximize the ex-act likelihood of a collection of unlabeled motions, based on an autoregressive context of poses in previous frames and a control signal representing the movement of a root joint.Thanks to invertible ﬂow transformations, latent codes that encode deep properties of motion styles are efﬁciently in-ferred by M. By combining the latent codes (from an input style motion S) with the autoregressive context and control signal (from an input content motion C), M outputs a styl-ized motion which transfers style from S to C. Moreover, our model is probabilistic and is able to generate various plausible motions with a speciﬁc style. We evaluate the pro-posed model on motion capture datasets containing differ-ent human motion styles. Experiment results show that our model outperforms the state-of-the-art methods, despite not requiring manually labeled training data. 