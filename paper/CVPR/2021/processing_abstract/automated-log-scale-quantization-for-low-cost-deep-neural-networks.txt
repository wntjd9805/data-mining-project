Quantization plays an important role in deep neural net-work (DNN) hardware. In particular, logarithmic quanti-zation has multiple advantages for DNN hardware imple-mentations, and its weakness in terms of lower performance at high precision compared with linear quantization has been recently remedied by what we call selective two-word logarithmic quantization (STLQ). However, there is a lack of training methods designed for STLQ or even logarith-mic quantization in general. In this paper we propose a novel STLQ-aware training method, which signiÔ¨Åcantly out-performs the previous state-of-the-art training method forSTLQ. Moreover, our training results demonstrate that with our new training method, STLQ applied to weight parame-ters of ResNet-18 can achieve the same level of performance as state-of-the-art quantization method, APoT, at 3-bit pre-cision. We also apply our method to various DNNs in image enhancement and semantic segmentation, showing compet-itive results. 