Compact video super-resolution (VSR) networks can be easily deployed on resource-limited devices, e.g., smart-phones and wearable devices, but have considerable per-formance gaps compared with complicated VSR networks that require a large amount of computing resources.In this paper, we aim to improve the performance of compactVSR networks without changing their original architec-tures, through a knowledge distillation approach that trans-fers knowledge from a complicated VSR network to a com-pact one. SpeciÔ¨Åcally, we propose a space-time distillation (STD) scheme to exploit both spatial and temporal knowl-edge in the VSR task. For space distillation, we extract spa-tial attention maps that hint the high-frequency video con-tent from both networks, which are further used for transfer-ring spatial modeling capabilities. For time distillation, we narrow the performance gap between compact models and complicated models by distilling the feature similarity of the temporal memory cells, which are encoded from the se-quence of feature maps generated in the training clips usingConvLSTM. During the training process, STD can be easily incorporated into any network without changing the origi-nal network architecture. Experimental results on standard benchmarks demonstrate that, in resource-constrained sit-uations, the proposed method notably improves the perfor-mance of existing VSR networks without increasing the in-ference time. 