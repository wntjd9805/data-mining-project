OCR-based image captioning aims to automatically de-scribe images based on all the visual entities (both visual objects and scene text) in images. Compared with conven-tional image captioning, the reasoning of scene text is re-quired for OCR-based image captioning since the gener-ated descriptions often contain multiple OCR tokens. Ex-isting methods attempt to achieve this goal via encoding the OCR tokens with rich visual and semantic representa-tions. However, strong correlations between OCR tokens may not be established with such limited representations. In this paper, we propose to enhance the connections betweenOCR tokens from the viewpoint of exploiting the geometri-cal relationship. We comprehensively consider the height, width, distance, IoU and orientation relations between theOCR tokens for constructing the geometrical relationship.To integrate the learned relation as well as the visual and semantic representations into a uniÔ¨Åed framework, a LongShort-Term Memory plus Relation-aware pointer network (LSTM-R) architecture is presented in this paper. Under the guidance of the geometrical relationship between OCR to-kens, our LSTM-R capitalizes on a newly-devised relation-aware pointer network to select OCR tokens from the scene text for OCR-based image captioning. Extensive experi-ments demonstrate the effectiveness of our LSTM-R. More remarkably, LSTM-R achieves state-of-the-art performance on TextCaps, with the CIDEr-D score being increased from 98.0% to 109.3%. 