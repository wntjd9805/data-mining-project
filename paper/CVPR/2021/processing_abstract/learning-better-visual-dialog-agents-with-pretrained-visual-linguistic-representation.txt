GuessWhat?! is a visual dialog guessing game which in-corporates a Questioner agent that generates a sequence of questions, while an Oracle agent answers the respec-tive questions about a target object in an image. Based on this dialog history between the Questioner and the Ora-cle, a Guesser agent makes a ﬁnal guess of the target ob-ject. While previous work has focused on dialogue pol-icy optimization and visual-linguistic information fusion, most work learns the vision-linguistic encoding for the three agents solely on the GuessWhat?! dataset without shared and prior knowledge of vision-linguistic representation. To bridge these gaps, this paper proposes new Oracle, Guesser and Questioner models that take advantage of a pretrained vision-linguistic model, VilBERT. For Oracle model, we introduce a two-way background/target fusion mechanism to understand both intra and inter-object questions. ForGuesser model, we introduce a state-estimator that best uti-lizes VilBERT’s strength in single-turn referring expression comprehension. For the Questioner, we share the state-estimator from pretrained Guesser with Questioner to guide the question generator. Experimental results show that our proposed models outperform state-of-the-art models signif-icantly by 7%, 10%, 12% for Oracle, Guesser and End-to-End Questioner respectively. 