Cross-modal retrieval aims to learn discriminative and modal-invariant features for data from different modalities.Unlike the existing methods which usually learn from the features extracted by ofﬂine networks, in this paper, we pro-pose an approach to jointly train the components of cross-modal retrieval framework with metadata, and enable the network to ﬁnd optimal features. The proposed end-to-end framework is updated with three loss functions: 1) a novel cross-modal center loss to eliminate cross-modal discrep-ancy, 2) cross-entropy loss to maximize inter-class varia-tions, and 3) mean-square-error loss to reduce modalityIn particular, our proposed cross-modal cen-variations. ter loss minimizes the distances of features from objects belonging to the same class across all modalities. Exten-sive experiments have been conducted on the retrieval tasks across multi-modalities including 2D image, 3D point cloud and mesh data. The proposed framework signiﬁcantly out-performs the state-of-the-art methods for both cross-modal and in-domain retrieval for 3D objects on the ModelNet10 and ModelNet40 datasets. 