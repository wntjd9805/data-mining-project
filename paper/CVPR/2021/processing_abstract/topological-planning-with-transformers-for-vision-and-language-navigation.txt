Conventional approaches to vision-and-language navi-gation (VLN) are trained end-to-end but struggle to per-form well in freely traversable environments. Inspired by the robotics community, we propose a modular approach to VLN using topological maps. Given a natural language instruction and topological map, our approach leverages attention mechanisms to predict a navigation plan in the map. The plan is then executed with low-level actions (e.g.FORWARD, ROTATE) using a robust controller. Experiments show that our method outperforms previous end-to-end ap-proaches, generates interpretable navigation plans, and ex-hibits intelligent behaviors such as backtracking. 