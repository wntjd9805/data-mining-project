In compositional zero-shot learning, the goal is to recog-nize unseen compositions (e.g. old dog) of observed visual primitives states (e.g. old, cute) and objects (e.g. car, dog) in the training set. This is challenging because the same state can for example alter the visual appearance of a dog drastically differently from a car. As a solution, we propose a novel graph formulation called Compositional Graph Em-bedding (CGE) that learns image features, compositional classiﬁers and latent representations of visual primitives in an end-to-end manner. The key to our approach is exploit-ing the dependency between states, objects and their com-positions within a graph structure to enforce the relevant knowledge transfer from seen to unseen compositions. By learning a joint compatibility that encodes semantics be-tween concepts, our model allows for generalization to un-seen compositions without relying on an external knowledge base like WordNet. We show that in the challenging gen-eralized compositional zero-shot setting our CGE signiﬁ-cantly outperforms the state of the art on MIT-States andUT-Zappos. We also propose a new benchmark for this task based on the recent GQA dataset. Code is available at: https://github.com/ExplainableML/czsl 