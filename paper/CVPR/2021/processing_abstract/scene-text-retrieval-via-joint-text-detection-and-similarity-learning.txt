Scene text retrieval aims to localize and search all text instances from an image gallery, which are the same or similar with a given query text. Such a task is usually re-alized by matching a query text to the recognized words, outputted by an end-to-end scene text spotter. In this pa-per, we address this problem by directly learning a cross-modal similarity between a query text and each text in-stance from natural images. Speciﬁcally, we establish an end-to-end trainable network, jointly optimizing the pro-cedures of scene text detection and cross-modal similar-ity learning. In this way, scene text retrieval can be sim-ply performed by ranking the detected text instances with the learned similarity. Experiments on three benchmark datasets demonstrate our method consistently outperforms the state-of-the-art scene text spotting/retrieval approaches.In particular, the proposed framework of joint detection and similarity learning achieves signiﬁcantly better per-formance than separated methods. Code is available at: https://github.com/lanfeng4659/STR-TDSL. 