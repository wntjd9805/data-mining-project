In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the ob-jective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predom-inantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the signiﬁcance of pow-erful feature representations with a simple embedding net-work that can outperform existing sophisticated FSL algo-rithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been em-ployed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Si-multaneous optimization for both of these contrasting ob-jectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transfor-mations. These complementary sets of features help gener-alize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive exper-imentation shows that even without knowledge distillation our proposed method can outperform current state-of-the-art FSL methods on ﬁve popular benchmark datasets. 