Generation of stroke-based non-photorealistic imagery, is an important problem in the computer vision commu-nity. As an endeavor in this direction, substantial recent research efforts have been focused on teaching machines“how to paint”, in a manner similar to a human painter.However, the applicability of previous methods has been limited to datasets with little variation in position, scale and saliency of the foreground object. As a consequence, we ﬁnd that these methods struggle to cover the granular-ity and diversity possessed by real world images. To this end, we propose a Semantic Guidance pipeline with 1) a bi-level painting procedure for learning the distinction be-tween foreground and background brush strokes at train-ing time. 2) We also introduce invariance to the position and scale of the foreground object through a neural align-ment model, which combines object localization and spa-tial transformer networks in an end to end manner, to zoom into a particular semantic instance. 3) The distinguishing features of the in-focus object are then ampliﬁed by max-imizing a novel guided backpropagation based focus re-ward. The proposed agent does not require any supervi-sion on human stroke-data and successfully handles varia-tions in foreground object attributes, thus, producing much higher quality canvases for the CUB-200 Birds [29] andStanford Cars-196 [17] datasets. Finally, we demonstrate the further efﬁcacy of our method on complex datasets with multiple foreground object instances by evaluating an ex-tension of our method on the challenging Virtual-KITTI [2] dataset. Source code and models are available at https://github.com/1jsingh/semantic-guidance. 