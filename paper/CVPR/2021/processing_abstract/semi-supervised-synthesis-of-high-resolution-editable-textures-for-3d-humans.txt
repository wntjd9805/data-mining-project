We introduce a novel approach to generate diverse highﬁdelity texture maps for 3D human meshes in a semi-supervised setup. Given a segmentation mask deﬁning the layout of the semantic regions in the texture map, our net-work generates high-resolution textures with a variety of styles, that are then used for rendering purposes. To accom-plish this task, we propose a Region-adaptive AdversarialVariational AutoEncoder (ReAVAE) that learns the proba-bility distribution of the style of each region individually so that the style of the generated texture can be controlled by sampling from the region-speciﬁc distributions. In addition, we introduce a data generation technique to augment our training set with data lifted from single-view RGB inputs.Our training strategy allows the mixing of reference image styles with arbitrary styles for different regions, a property which can be valuable for virtual try-on AR/VR applica-tions. Experimental results show that our method synthe-sizes better texture maps compared to prior work while en-abling independent layout and style controllability.∗This work was conducted during an internship at FRL Research. 7991