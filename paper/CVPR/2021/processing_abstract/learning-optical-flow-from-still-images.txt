This paper deals with the scarcity of data for training op-tical ﬂow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Speciﬁcally, we introduce a framework to generate accurate ground-truth optical ﬂow annotations quickly and in large amounts from any readily available single real pic-ture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vec-tors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical ﬂow ﬁeld con-necting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art opti-cal ﬂow networks achieve superior generalization to unseen real data compared to the same models trained either on an-notated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images. 