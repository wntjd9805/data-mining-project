Gradient compression has been widely adopted in data-parallel distributed training of deep neural networks to reduce communication overhead. Some literatures have demonstrated that large gradients are more important than small ones because they contain more information, such as Top-k compressor. Other mainstream methods, like random-k compressor and gradient quantization, usually treat all gradients equally. Different from all of them, we regard large and small gradients selection as the exploita-tion and exploration of gradient information, respectively.And we ﬁnd taking both of them into consideration is the key to boost the ﬁnal accuracy. So, we propose a novel gra-dient compressor: Gradient Sampling with Bayes Prior in this paper. Speciﬁcally, we sample important/large gradi-ents based on the global gradient distribution, which is pe-riodically updated across multiple workers. Then we intro-duce Bayes Prior into distribution model to further explore the gradients. We prove the convergence of our method for smooth non-convex problems in the distributed system.Compared with methods that running after high compres-sion ratio at the expense of accuracy, we pursue no loss of accuracy and the actual acceleration beneﬁt in prac-tice. Experimental comparisons on a variety of computer vision tasks (e.g. image classiﬁcation and object detec-tion) and backbones (ResNet, MobileNetV2, InceptionV3 and AlexNet) show that our approach outperforms the state-of-the-art techniques in terms of both speed and accuracy, with the limitation of 100× compression ratio. 