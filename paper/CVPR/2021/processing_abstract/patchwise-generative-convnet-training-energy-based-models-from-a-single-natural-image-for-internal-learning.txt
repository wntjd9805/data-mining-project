raising the following fundamental question:Exploiting internal statistics of a single natural im-age has long been recognized as a signiﬁcant research paradigm where the goal is to learn the internal distribu-tion of patches within the image without relying on exter-nal training data. Different from prior works that model such a distribution implicitly with a top-down latent vari-able model (e.g., generator), this paper proposes to explic-itly represent the statistical distribution within a single nat-ural image by using an energy-based generative framework, where a pyramid of energy functions, each parameterized by a bottom-up deep neural network, are used to capture the distributions of patches at different resolutions. Meanwhile, a coarse-to-ﬁne sequential training and sampling strategy is presented to train the model efﬁciently. Besides learning to generate random samples from white noise, the model can learn in parallel with a self-supervised task (e.g., recover the input image from its corrupted version), which can fur-ther improve the descriptive power of the learned model.The proposed model is simple and natural in that it does not require an auxiliary model (e.g., discriminator) to assist the training. Besides, it also uniﬁes internal statistics learning and image generation in a single framework. Experimen-tal results presented on various image generation and ma-nipulation tasks, including super-resolution, image editing, harmonization, style transfer, etc, have demonstrated the ef-fectiveness of our model for internal learning. 