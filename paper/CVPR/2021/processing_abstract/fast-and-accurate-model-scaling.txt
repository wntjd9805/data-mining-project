In this work we analyze strategies for convolutional neu-ral network scaling; that is, the process of scaling a base convolutional network to endow it with greater computa-tional complexity and consequently representational power.Example scaling strategies may include increasing model width, depth, resolution, etc. While various scaling strate-gies exist, their tradeoffs are not fully understood. Existing analysis typically focuses on the interplay of accuracy andﬂops (ﬂoating point operations). Yet, as we demonstrate, various scaling strategies affect model parameters, activa-tions, and consequently actual runtime quite differently. In our experiments we show the surprising result that numer-ous scaling strategies yield networks with similar accuracy but with widely varying properties. This leads us to pro-pose a simple fast compound scaling strategy that encour-ages primarily scaling model width, while scaling depth and resolution to a lesser extent. Unlike currently popular scal-ing strategies, which result in about O(s) increase in model activation w.r.t. scaling ﬂops by a factor of s, the proposed fast compound scaling results in close to O(√s) increase in activations, while achieving excellent accuracy. Fewer ac-tivations leads to speedups on modern memory-bandwidth limited hardware (e.g., GPUs). More generally, we hope this work provides a framework for analyzing scaling strate-gies under various computational constraints. 