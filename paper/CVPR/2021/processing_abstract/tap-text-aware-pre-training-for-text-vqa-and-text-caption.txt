In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks. These two tasks aim at reading and understanding scene text in images for question answering and image caption generation, respec-tively.In contrast to conventional vision-language pre-training that fails to capture scene text and its relationship with the visual and text modalities, TAP explicitly incorpo-rates scene text (generated from OCR engines) during pre-training. With three pre-training tasks, including masked language modeling (MLM), image-text (contrastive) match-ing (ITM), and relative (spatial) position prediction (RPP), pre-training with scene text effectively helps the model learn a better aligned representation among the three modali-ties: text word, visual object, and scene text. Due to this aligned representation learning, even pre-trained on the same downstream task dataset, TAP already boosts the ab-solute accuracy on the TextVQA dataset by +5.4%, com-pared with a non-TAP baseline. To further improve the per-formance, we build a large-scale scene text-related image-text dataset based on the Conceptual Caption dataset, named OCR-CC, which contains 1.4 million images with scene text. Pre-trained on this OCR-CC dataset, our ap-proach outperforms the state of the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA, +8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps. 