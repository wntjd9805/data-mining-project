The observation that computer vision methods overﬁt to dataset speciﬁcs has inspired diverse attempts to make ob-ject recognition models robust to domain shifts. However, similar work on domain-robust visual question answering methods is very limited. Domain adaptation for VQA dif-fers from adaptation for object recognition due to addi-tional complexity: VQA models handle multimodal inputs, methods contain multiple steps with diverse modules result-ing in complex optimization, and answer spaces in different datasets are vastly different. To tackle these challenges, weﬁrst quantify domain shifts between popular VQA datasets, in both visual and textual space. To disentangle shifts be-tween datasets arising from different modalities, we also construct synthetic shifts in the image and question domains separately. Second, we test the robustness of different fam-ilies of VQA methods (classic two-stream, transformer, and neuro-symbolic methods) to these shifts. Third, we test the applicability of existing domain adaptation methods and de-vise a new one to bridge VQA domain gaps, adjusted to speciﬁc VQA models. To emulate the setting of real-world generalization, we focus on unsupervised domain adapta-tion and the open-ended classiﬁcation task formulation. 