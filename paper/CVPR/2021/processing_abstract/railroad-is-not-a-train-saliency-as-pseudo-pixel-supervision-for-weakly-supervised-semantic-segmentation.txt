Existing studies in weakly-supervised semantic segmen-tation (WSSS) using image-level weak supervision have sev-eral limitations: sparse object coverage, inaccurate ob-ject boundaries, and co-occurring pixels from non-target objects.To overcome these challenges, we propose a novel framework, namely Explicit Pseudo-pixel Supervision (EPS), which learns from pixel-level feedback by combin-ing two weak supervisions; the image-level label provides the object identity via the localization map and the saliency map from the off-the-shelf saliency detection model offers rich boundaries. We devise a joint training strategy to fully utilize the complementary relationship between both infor-mation. Our method can obtain accurate object boundaries and discard co-occurring pixels, thereby signiﬁcantly im-proving the quality of pseudo-masks. Experimental results show that the proposed method remarkably outperforms ex-isting methods by resolving key challenges of WSSS and achieves the new state-of-the-art performance on both PAS-CAL VOC 2012 and MS COCO 2014 datasets. The code is available at https://github.com/halbielee/EPS.Figure 1. Motivating example of utilizing both the saliency map and the localization map for WSSS. (a) Groundtruth, (b) saliency map via PFAN [51], (c) localization map via CAM [52] and (d) our EPS utilizing both the saliency map and the localization map for training a classiﬁer. Note that the saliency map cannot capture person and car while our result can correctly restore them, and the localization map overly captures two objects. 