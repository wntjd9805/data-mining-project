Federated learning enables multiple parties to collab-oratively train a machine learning model without commu-nicating their local data. A key challenge in federated learning is to handle the heterogeneity of local data dis-tribution across parties. Although many studies have been proposed to address this challenge, we ﬁnd that they fail to achieve high performance in image datasets with deep learning models. In this paper, we propose MOON: model-contrastive federated learning. MOON is a simple and effective federated learning framework. The key idea ofMOON is to utilize the similarity between model represen-tations to correct the local training of individual parties, i.e., conducting contrastive learning in model-level. Our extensive experiments show that MOON signiﬁcantly out-performs the other state-of-the-art federated learning algo-rithms on various image classiﬁcation tasks. 