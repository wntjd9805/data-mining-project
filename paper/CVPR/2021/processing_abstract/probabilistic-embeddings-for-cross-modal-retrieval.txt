Cross-modal retrieval methods build a common repre-sentation space for samples from multiple modalities, typ-ically from the vision and the language domains. For im-ages and their captions, the multiplicity of the correspon-dences makes the task particularly challenging. Given an image (respectively a caption), there are multiple captions (respectively images) that equally make sense. In this paper, we argue that deterministic functions are not sufÔ¨Åciently powerful to capture such one-to-many correspondences. In-stead, we propose to use Probabilistic Cross-Modal Embed-ding (PCME), where samples from the different modalities are represented as probabilistic distributions in the com-mon embedding space. Since common benchmarks such asCOCO suffer from non-exhaustive annotations for cross-modal matches, we propose to additionally evaluate re-trieval on the CUB dataset, a smaller yet clean database where all possible image-caption pairs are annotated. We extensively ablate PCME and demonstrate that it not only improves the retrieval performance over its deterministic counterpart but also provides uncertainty estimates that render the embeddings more interpretable. Code is avail-able at https://github.com/naver-ai/pcme. 