Imperfect labels are ubiquitous in real-world datasets.Several recent successful methods for training deep neu-ral networks (DNNs) robust to label noise have used two primary techniques: ﬁltering samples based on loss dur-ing a warm-up phase to curate an initial set of cleanly labeled samples, and using the output of a network as a pseudo-label for subsequent loss calculations. In this pa-per, we evaluate different augmentation strategies for al-gorithms tackling the "learning with noisy labels" prob-lem. We propose and examine multiple augmentation strate-gies and evaluate them using synthetic datasets based onCIFAR-10 and CIFAR-100, as well as on the real-world dataset Clothing1M. Due to several commonalities in these algorithms, we ﬁnd that using one set of augmentations for loss modeling tasks and another set for learning is the most effective, improving results on the state-of-the-art and other previous methods. Furthermore, we ﬁnd that applying aug-mentation during the warm-up period can negatively im-pact the loss convergence behavior of correctly versus in-correctly labeled samples. We introduce this augmentation strategy to the state-of-the-art technique and demonstrate that we can improve performance across all evaluated noise levels. In particular, we improve accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the Clothing1M dataset. 