Most standard learning approaches lead to fragile mod-els which are prone to drift when sequentially trained on samples of a different nature—the well-known catastrophic forgetting issue.In particular, when a model consecu-tively learns from different visual domains, it tends to for-get the past domains in favor of the most recent ones.In this context, we show that one way to learn models that are inherently more robust against forgetting is do-main randomization—for vision tasks, randomizing the cur-rent domain’s distribution with heavy image manipulations.Building on this result, we devise a meta-learning strat-egy where a regularizer explicitly penalizes any loss associ-ated with transferring the model from the current domain to different “auxiliary” meta-domains, while also easing adaptation to them. Such meta-domains are also gener-ated through randomized image manipulations. We empir-ically demonstrate in a variety of experiments—spanning from classiﬁcation to semantic segmentation—that our ap-proach results in models that are less prone to catastrophic forgetting when transferred to new domains. 