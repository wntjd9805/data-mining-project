Generative Adversarial Networks (GANs) have achieved huge success in generating high-ﬁdelity images, however, they suffer from low efﬁciency due to tremendous compu-tational cost and bulky memory usage. Recent efforts on compression GANs show noticeable progress in obtaining smaller generators by sacriﬁcing image quality or involving a time-consuming searching process. In this work, we aim to address these issues by introducing a teacher network that provides a search space in which efﬁcient network ar-chitectures can be found, in addition to performing knowl-edge distillation. First, we revisit the search space of gener-ative models, introducing an inception-based residual block into generators. Second, to achieve target computation cost, we propose a one-step pruning algorithm that searches a student architecture from the teacher model and substan-tially reduces searching cost. It requires no ℓ1 sparsity regu-larization and its associated hyper-parameters, simplifying the training procedure. Finally, we propose to distill knowl-edge through maximizing feature similarity between teacher and student via an index named Global Kernel Alignment (GKA). Our compressed networks achieve similar or even better image ﬁdelity (FID, mIoU) than the original models with much-reduced computational cost, e.g., MACs. Code will be released at https://github.com/snap-research/CAT. 