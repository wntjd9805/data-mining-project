We propose a novel approach to few-shot action recog-nition, Ô¨Ånding temporally-corresponding frame tuples be-tween the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes us-ing the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video repre-sentations are formed from ordered tuples of varying num-bers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared.1Our proposed Temporal-Relational CrossTransform-ers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 andUCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers. 