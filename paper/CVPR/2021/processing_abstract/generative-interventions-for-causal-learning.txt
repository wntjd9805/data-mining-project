We introduce a framework for learning robust visual representations that generalize to new viewpoints, back-grounds, and scene contexts. Discriminative models of-ten learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distri-bution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and the-oretical results show this method learns robust representa-tions more consistent with the underlying causal relation-ships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset. 