Transductive zero-shot learning (T-ZSL) which could al-leviate the domain shift problem in existing ZSL works, has received much attention recently. However, an open prob-lem in T-ZSL: how to effectively make use of unseen-class samples for training, still remains. Addressing this prob-lem, we ﬁrst empirically analyze the roles of unseen-class samples with different degrees of hardness in the training process based on the uneven prediction phenomenon found in many ZSL methods, resulting in three observations. Then, we propose two hardness sampling approaches for selecting a subset of diverse and hard samples from a given unseen-class dataset according to these observations. The ﬁrst one identiﬁes the samples based on the class-level frequency of the model predictions while the second enhances the for-mer by normalizing the class frequency via an approximate class prior estimated by an explored prior estimation algo-rithm. Finally, we design a new Self-Training framework with Hardness Sampling for T-ZSL, called STHS, where an arbitrary inductive ZSL method could be seamlessly em-bedded and it is iteratively trained with unseen-class sam-ples selected by the hardness sampling approach. We in-troduce two typical ZSL methods into the STHS framework and extensive experiments demonstrate that the derived T-ZSL methods outperform many state-of-the-art methods on three public benchmarks. Besides, we note that the unseen-class dataset is separately used for training in some existing transductive generalized ZSL (T-GZSL) methods, which is not strict for a GZSL task. Hence, we suggest a more strictT-GZSL data setting and establish a competitive baseline on this setting by introducing the proposed STHS framework toT-GZSL. 