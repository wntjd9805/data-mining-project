Previous convolutional neural network (CNN) based blind super-resolution (SR) methods usually adopt an it-erative optimization way to approximate the ground-truth (GT) step-by-step. This solution always involves more com-putational costs to bring about time-consuming inference.At present, most blind SR algorithms are dedicated to ob-taining high-ﬁdelity results; their loss function generally employs L1 loss. To further improve the visual quality ofSR results, perceptual metric, such as NIQE, is necessary to guide the network optimization. However, due to the non-differentiable property of NIQE, it cannot be as the loss function. Towards these issues, we propose an adap-tive modulation network (AMNet) for multiple degradationsSR, which is composed of the pivotal adaptive modulation layer (AMLayer).It is an efﬁcient yet lightweight fusion layer between blur kernel and image features. Equipped with the blur kernel predictor, we naturally upgrade theInstead of considering it-AMNet to the blind SR model. erative strategy, we make the blur kernel predictor train-able in the whole blind SR model, in which AMNet is well-trained. Also, we ﬁt deep reinforcement learning into the blind SR model (AMNet-RL) to tackle the non-differentiable optimization problem. Speciﬁcally, the blur kernel predic-tor will be the actor to estimate the blur kernel from the input low-resolution (LR) image. The reward is designed by the pre-deﬁned differentiable or non-differentiable metric.Extensive experiments show that our model can outperform state-of-the-art methods in both ﬁdelity and perceptual met-rics. 