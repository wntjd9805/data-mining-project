Having access to multi-modal cues (e.g. vision and au-dio) empowers some cognitive tasks to be done faster com-pared to learning from a single modality. In this work, we propose to transfer knowledge across heterogeneous modal-ities, even though these data modalities may not be seman-tically correlated. Rather than directly aligning the rep-resentations of different modalities, we compose audio, im-age, and video representations across modalities to uncover richer multi-modal knowledge. Our main idea is to learn a compositional embedding that closes the cross-modal se-mantic gap and captures the task-relevant semantics, which facilitates pulling together representations across modali-ties by compositional contrastive learning. We establish a new, comprehensive multi-modal distillation benchmark on three video datasets: UCF101, ActivityNet, and VG-GSound. Moreover, we demonstrate that our model signiÔ¨Å-cantly outperforms a variety of existing knowledge distilla-tion methods in transferring audio-visual knowledge to im-prove video representation learning. Code is released here: https://github.com/yanbeic/CCL. 