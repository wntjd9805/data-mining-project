Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world ap-plications, yet it remains challenging. The difﬁculties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classiﬁcation-based representation learning with deep neu-ral networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of iden-tities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems.Speciﬁcally, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-ﬂy which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model paral-lel, we empirically verify in large-scale datasets that 10% of classes are sufﬁcient to achieve similar performance as using all classes. Moreover, the class weights are dynam-ically generated in a few-shot manner and therefore suit-able for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest pub-lic dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 in-stances. Code is available at https://github.com/ bilylee/DCQ 