With the rapid advances of autonomous driving, it be-comes critical to equip its sensing system with more holistic 3D perception. However, existing works focus on parsing either the objects (e.g. cars and pedestrians) or scenes (e.g. trees and buildings) from the LiDAR sensor. In this work, we address the task of LiDAR-based panoptic segmenta-tion, which aims to parse both objects and scenes in a uni-ﬁed manner. As one of the ﬁrst endeavors towards this new challenging task, we propose the Dynamic Shifting Network (DS-Net), which serves as an effective panoptic segmenta-tion framework in the point cloud realm. In particular, DS-Net has three appealing properties: 1) strong backbone de-sign. DS-Net adopts the cylinder convolution that is specif-ically designed for LiDAR point clouds. The extracted fea-tures are shared by the semantic branch and the instance branch which operates in a bottom-up clustering style. 2)Dynamic Shifting for complex point distributions. We ob-serve that commonly-used clustering algorithms like BFS orDBSCAN are incapable of handling complex autonomous driving scenes with non-uniform point cloud distributions and varying instance sizes.Thus, we present an efﬁ-cient learnable clustering module, dynamic shifting, which adapts kernel functions on-the-ﬂy for different instances. 3)Consensus-driven Fusion. Finally, consensus-driven fu-sion is used to deal with the disagreement between seman-tic and instance predictions. To comprehensively evalu-ate the performance of LiDAR-based panoptic segmenta-tion, we construct and curate benchmarks from two large-scale autonomous driving LiDAR datasets, SemanticKITTI and nuScenes. Extensive experiments demonstrate that our proposed DS-Net achieves superior accuracies over current state-of-the-art methods. Notably, we achieve 1st place on the public leaderboard of SemanticKITTI, outperforming 2nd place by 2.6% in terms of the PQ metric 1. 