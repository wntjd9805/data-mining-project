The success of supervised learning hinges on the as-sumption that the training and test data come from the same underlying distribution, which is often not valid in practice due to potential distribution shift. In light of this, most ex-isting methods for unsupervised domain adaptation focus on achieving domain-invariant representations and small source domain error. However, recent works have shown that this is not sufﬁcient to guarantee good generalization on the target domain, and in fact, is provably detrimen-tal under label distribution shift. Furthermore, in many real-world applications it is often feasible to obtain a small amount of labeled data from the target domain and use them to facilitate model training with source data.In-spired by the above observations, in this paper we pro-pose the ﬁrst method that aims to simultaneously learn in-variant representations and risks under the setting of semi-supervised domain adaptation (Semi-DA). First, we provide a ﬁnite sample bound for both classiﬁcation and regres-sion problems under Semi-DA. The bound suggests a prin-cipled way to obtain target generalization, i.e., by align-ing both the marginal and conditional distributions across domains in feature space. Motivated by this, we then in-troduce the LIRR algorithm for jointly Learning InvariantRepresentations and Risks. Finally, extensive experiments are conducted on both classiﬁcation and regression tasks, which demonstrate that LIRR consistently achieves state-of-the-art performance and signiﬁcant improvements com-pared with the methods that only learn invariant represen-tations or invariant risks. Our code will be released atLIRR@github 