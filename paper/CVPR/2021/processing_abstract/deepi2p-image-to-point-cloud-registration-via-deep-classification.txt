This paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point from a rgb-camera) and a cloud. Given an image (e.g. from a 3D Lidar scanner) cap-general point cloud (e.g. tured at different locations in the same scene, our method estimates the relative rigid transformation between the co-ordinate frames of the camera and Lidar. Learning com-mon feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difﬁculty by converting the registration problem into a classiﬁcation and inverse cam-era projection optimization problem. A classiﬁcation neu-ral network is designed to label whether the projection of each point in the point cloud is within or beyond the cam-era frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results on Ox-ford Robotcar and KITTI datasets demonstrate the feasi-bility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P. 