We propose a novel framework for creating large-scale photorealistic datasets of indoor scenes, with ground truth geometry, material, lighting and semantics. Our goal is to make the dataset creation process widely accessible, trans-forming scans into photorealistic datasets with high-quality ground truth for appearance, layout, semantic labels, high quality spatially-varying BRDF and complex lighting, in-cluding direct, indirect and visibility components. This en-ables important applications in inverse rendering, scene understanding and robotics. We show that deep networks trained on the proposed dataset achieve competitive perfor-mance for shape, material and lighting estimation on real images, enabling photorealistic augmented reality applica-tions, such as object insertion and material editing. We also show our semantic labels may be used for segmenta-tion and multi-task learning. Finally, we demonstrate that our framework may also be integrated with physics engines, to create virtual robotics environments with unique ground truth such as friction coefÔ¨Åcients and correspondence to real scenes. The dataset and all the tools to create such datasets will be made publicly available.1 