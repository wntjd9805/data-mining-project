Deep learning models suffer from catastrophic forget-ting when trained in an incremental learning setting.In this work, we propose a novel approach to address the task incremental learning problem, which involves training a model on new tasks that arrive in an incremental man-ner. The task incremental learning problem becomes even more challenging when the test set contains classes that are not part of the train set, i.e., a task incremental general-ized zero-shot learning problem. Our approach can be used in both the zero-shot and non zero-shot task incremental learning settings. Our proposed method uses weight rec-tiﬁcations and afﬁne transformations in order to adapt the model to different tasks that arrive sequentially. Speciﬁ-cally, we adapt the network weights to work for new tasks by“rectifying” the weights learned from the previous task. We learn these weight rectiﬁcations using very few parameters.We additionally learn afﬁne transformations on the outputs generated by the network in order to better adapt them for the new task. We perform experiments on several datasets in both zero-shot and non zero-shot task incremental learning settings and empirically show that our approach achieves state-of-the-art results. Speciﬁcally, our approach outper-forms the state-of-the-art non zero-shot task incremental learning method by over 5% on the CIFAR-100 dataset. Our approach also signiﬁcantly outperforms the state-of-the-art task incremental generalized zero-shot learning method by absolute margins of 6.91% and 6.33% for the AWA1 andCUB datasets, respectively. We validate our approach us-ing various ablation studies. 