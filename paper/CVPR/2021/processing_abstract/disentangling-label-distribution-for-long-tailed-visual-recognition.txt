The current evaluation protocol of long-tailed visual recognition trains the classiﬁcation model on the long-tailed source label distribution and evaluates its perfor-mance on the uniform target label distribution. Such pro-tocol has questionable practicality since the target may also be long-tailed. Therefore, we formulate long-tailed visual recognition as a label shift problem where the tar-get and source label distributions are different. One of the signiﬁcant hurdles in dealing with the label shift prob-lem is the entanglement between the source label distri-bution and the model prediction.In this paper, we fo-cus on disentangling the source label distribution from the model prediction. We ﬁrst introduce a simple but over-looked baseline method that matches the target label dis-tribution by post-processing the model prediction trained by the cross-entropy loss and the Softmax function. Al-though this method surpasses state-of-the-art methods on benchmark datasets, it can be further improved by di-rectly disentangling the source label distribution from the model prediction in the training phase. Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan rep-resentation. LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LT, Places-LT,ImageNet-LT, and iNaturalist 2018. Moreover, LADE out-performs existing methods on various shifted target label distributions, showing the general adaptability of our pro-posed method. 