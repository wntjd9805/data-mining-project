Few-shot semantic segmentation (FSS) aims to segment unseen class objects given very few densely-annotated sup-port images from the same class. Existing FSS methods ﬁnd the query object by using support prototypes or by directly relying on heuristic multi-scale feature fusion. However, they fail to fully leverage the high-order appearance rela-tionships between multi-scale features among the support-query image pairs, thus leading to an inaccurate localiza-tion of the query objects. To tackle the above challenge, we propose an end-to-end scale-aware graph neural network (SAGNN) by reasoning the cross-scale relations among the support-query images for FSS. Speciﬁcally, a scale-aware graph is ﬁrst built by taking support-induced multi-scale query features as nodes and, meanwhile, each edge is mod-eled as the pairwise interaction of its connected nodes. By progressive message passing over this graph, SAGNN is capable of capturing cross-scale relations and overcom-ing object variations (e.g., appearance, scale and location), and can thus learn more precise node embeddings. This in turn enables it to predict more accurate foreground ob-jects. Moreover, to make full use of the location relations across scales for the query image, a novel self-node col-laboration mechanism is proposed to enrich the current node, which endows SAGNN the ability of perceiving differ-ent resolutions of the same objects. Extensive experiments on PASCAL-5i and COCO-20i show that SAGNN achieves state-of-the-art results. 