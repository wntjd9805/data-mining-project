Self-supervised learning has gained prominence due to its efﬁcacy at learning powerful representations from un-labelled data that achieve excellent performance on many challenging downstream tasks. However, supervision-free pre-text tasks are challenging to design and usually modal-ity speciﬁc. Although there is a rich literature of self-supervised methods for either spatial (such as images) or temporal data (sound or text) modalities, a common pre-text task that beneﬁts both modalities is largely missing. In this paper, we are interested in deﬁning a self-supervised pre-text task for sketches and handwriting data. This data is uniquely characterised by its existence in dual modalities of rasterized images and vector coordinate sequences. We address and exploit this dual representation by proposing two novel cross-modal translation pre-text tasks for self-supervised feature learning: Vectorization and Rasteriza-tion. Vectorization learns to map image space to vector coordinates and rasterization maps vector coordinates to image space. We show that our learned encoder modules beneﬁt both raster-based and vector-based downstream ap-proaches to analysing hand-drawn data. Empirical evi-dence shows that our novel pre-text tasks surpass existing single and multi-modal self-supervision methods. 