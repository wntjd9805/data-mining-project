The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by ﬁtting the item to the desired body part and fusing the warped item with the person. While an increasing number of studies have been conducted, the resolution of synthesized images is still lim-ited to low (e.g., 256×192), which acts as the critical lim-itation against satisfying online consumers. We argue that the limitation stems from several challenges: as the resolu-tion increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the ﬁnal results; the architectures used in ex-* These authors contributed equally. isting methods have low performance in generating high-quality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024×768 virtual try-on images. Speciﬁcally, we ﬁrst prepare the segmentation map to guide our virtual try-on synthesis, and then roughly ﬁt the target clothing item to a given person’s body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS genera-tor to handle the misaligned areas and preserve the details of 1024×768 inputs. Through rigorous comparison with ex-isting methods, we demonstrate that VITON-HD highly sur-passes the baselines in terms of synthesized image quality both qualitatively and quantitatively. 14131