Convolutional Networks (ConvNets) excel at seman-tic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cam-eras present themselves as a perfect ﬁt in such systems.Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images.Transferring these models from the domain they were de-signed for to 360◦ perception, their performance drops dra-matically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Ef-ﬁcient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omni-directional imagery. In addition to the learned attention-based contextual priors that can stretch across 360◦ im-ages, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both:Densely labeled and unlabeled data originating from mul-tiple datasets. To foster progress in panoramic image seg-mentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks. 1 