Unsupervised domain adaptation (UDA) aims to trans-fer the knowledge from the labeled source domain to the unlabeled target domain. Existing self-training based UDA approaches assign pseudo labels for target data and treat them as ground truth labels to fully leverage unlabeled tar-get data for model adaptation. However, the generated pseudo labels from the model optimized on the source do-main inevitably contain noise due to the domain gap. To tackle this issue, we advance a MetaCorrection framework, where a Domain-aware Meta-learning strategy is devised to beneÔ¨Åt Loss Correction (DMLC) for UDA semantic seg-mentation. In particular, we model the noise distribution of pseudo labels in target domain by introducing a noise transition matrix (NTM) and construct meta data set with domain-invariant source data to guide the estimation ofNTM. Through the risk minimization on the meta data set, the optimized NTM thus can correct the noisy issues in pseudo labels and enhance the generalization ability of the model on the target data. Considering the capacity gap between shallow and deep features, we further employ the proposed DMLC strategy to provide matched and compat-ible supervision signals for different level features, thereby ensuring deep adaptation. Extensive experimental results highlight the effectiveness of our methoda against existing state-of-the-art methods on three benchmarks. 