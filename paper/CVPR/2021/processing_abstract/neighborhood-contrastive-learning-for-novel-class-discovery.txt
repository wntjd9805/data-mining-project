In this paper, we address Novel Class Discovery (NCD), the task of unveiling new classes in a set of unlabeled sam-ples given a labeled dataset with known classes. We exploit the peculiarities of NCD to build a new framework, namedNeighborhood Contrastive Learning (NCL), to learn dis-criminative representations that are important to clustering performance. Our contribution is twofold. First, we ﬁnd that a feature extractor trained on the labeled set gener-ates representations in which a generic query sample and its neighbors are likely to share the same class. We exploit this observation to retrieve and aggregate pseudo-positive pairs with contrastive learning, thus encouraging the model to learn more discriminative representations. Second, we notice that most of the instances are easily discriminated by the network, contributing less to the contrastive loss.To overcome this issue, we propose to generate hard nega-tives by mixing labeled and unlabeled samples in the feature space. We experimentally demonstrate that these two in-gredients signiﬁcantly contribute to clustering performance and lead our model to outperform state-of-the-art meth-ods by a large margin (e.g., clustering accuracy +13% onCIFAR-100 and +8% on ImageNet). 