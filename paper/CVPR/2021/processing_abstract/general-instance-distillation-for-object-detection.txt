Teacher BackboneStudent BackboneIn recent years, knowledge distillation has been proved to be an effective solution for model compression. This approach can make lightweight student models acquire the knowledge extracted from cumbersome teacher mod-els. However, previous distillation methods of detection have weak generalization for different detection frameworks and rely heavily on ground truth (GT), ignoring the valu-able relation information between instances. Thus, we pro-pose a novel distillation method for detection tasks based on discriminative instances without considering the posi-tive or negative distinguished by GT, which is called gen-eral instance distillation (GID). Our approach contains a general instance selection module (GISM) to make full use of feature-based, relation-based and response-based knowl-edge for distillation. Extensive results demonstrate that the student model achieves signiﬁcant AP improvement and even outperforms the teacher in various detection frame-works. Speciﬁcally, RetinaNet with ResNet-50 achieves 39.1% in mAP with GID on COCO dataset, which sur-passes the baseline 36.2% by 2.9%, and even better than the ResNet-101 based teacher model with 38.1% AP. 