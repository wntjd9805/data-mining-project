Adversarial attacks play a critical role in understand-ing deep neural network predictions and improving their robustness. Existing attack methods aim to deceive convo-lutional neural network (CNN)-based classiﬁers by manip-ulating RGB images that are fed directly to the classiﬁers.However, these approaches typically neglect the inﬂuence of the camera optics and image processing pipeline (ISP) that produce the network inputs. ISPs transform RAW mea-surements to RGB images and traditionally are assumed to preserve adversarial patterns.In fact, these low-level pipelines can destroy, introduce or amplify adversarial pat-terns that can deceive a downstream detector. As a result, optimized patterns can become adversarial for the classiﬁer after being transformed by a certain camera ISP or optical lens system but not for others.In this work, we examine and develop such an attack that deceives a speciﬁc cam-era ISP while leaving others intact, using the same down-stream classiﬁer. We frame this camera-speciﬁc attack as a multi-task optimization problem, relying on a differentiable approximation for the ISP itself. We validate the proposed method using recent state-of-the-art automotive hardwareISPs, achieving 92% fooling rate when attacking a speciﬁcISP. We demonstrate physical optics attacks with 90% fool-ing rate for a speciﬁc camera lens. 