Appearance SpaceA video can be represented by the composition of ap-pearance and motion. Appearance (or content) expresses the information invariant throughout time, and motion de-scribes the time-variant movement. Here, we propose self-supervised approaches for video Generative AdversarialNetworks (GANs) to achieve the appearance consistency and motion coherency in videos. Speciﬁcally, the dual dis-criminators for image and video individually learn to solve their own pretext tasks; appearance contrastive learning and temporal structure puzzle. The proposed tasks enable the discriminators to learn representations of appearance and temporal context, and force the generator to synthesize videos with consistent appearance and natural ﬂow of mo-tions. Extensive experiments in facial expression and hu-man action public benchmarks show that our method out-performs the state-of-the-art video GANs. Moreover, con-sistent improvements regardless of the architecture of videoGANs conﬁrm that our framework is generic. 