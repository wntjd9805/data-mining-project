When translating text inputs into layouts or images, ex-isting works typically require explicit descriptions of each object in a scene, including their spatial information or the associated relationships. To better exploit the text in-put, so that implicit objects or relationships can be prop-erly inferred during layout generation, we propose a Lay-outTransformer Network (LT-Net) in this paper. Given a scene-graph input, our LT-Net uniquely encodes the seman-tic features for exploiting their co-occurrences and implicit relationships. This allows one to manipulate conceptually diverse yet plausible layout outputs. Moreover, the decoder of our LT-Net translates the encoded contextual features into bounding boxes with self-supervised relation consis-tency preserved. By ﬁtting their distributions to Gaussian mixture models, spatially-diverse layouts can be addition-ally produced by LT-Net. We conduct extensive experiments on the datasets of MS-COCO and Visual Genome, and con-ﬁrm the effectiveness and plausibility of our LT-Net over recent layout generation models. Codes will be released atLayoutTransformer. 