Transfer learning with pre-training on large-scale datasets has played an increasingly signiﬁcant role in computer vision and natural language processing recently.However, as there exist numerous application scenarios that have distinctive demands such as certain latency con-straints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. In this paper, we focus on the area of object detection and present a transfer learning system named GAIA, which could automatically and efﬁciently give birth to customized solutions according to heterogeneous downstream needs. GAIA is capable of providing power-ful pre-trained weights, selecting models that conform to downstream demands such as latency constraints and spec-iﬁed data domains, and collecting relevant data for practi-tioners who have very few datapoints for their tasks. WithGAIA, we achieve promising results on COCO, Objects365,Open Images, Caltech, CityPersons, and UODB which is a collection of datasets including KITTI, VOC, WiderFace,DOTA, Clipart, Comic, and more. Taking COCO as an ex-ample, GAIA is able to efﬁciently produce models covering a wide range of latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and bells. To beneﬁt ev-ery practitioner in the community of object detection, GAIA is released at https://github.com/GAIA-vision. 