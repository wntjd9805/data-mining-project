In deep learning applications, the architectures of deep neural networks are crucial in achieving high accuracy.Many methods have been proposed to search for high-performance neural architectures automatically. However, these searched architectures are prone to adversarial at-tacks. A small perturbation of the input data can render the architecture to change prediction outcomes signiﬁcantly. To address this problem, we propose methods to perform dif-ferentiable search of robust neural architectures.In our methods, two differentiable metrics are deﬁned to measure architectures’ robustness, based on certiﬁed lower bound and Jacobian norm bound. Then we search for robust ar-chitectures by maximizing the robustness metrics. Different from previous approaches which aim to improve architec-tures’ robustness in an implicit way: performing adversar-ial training and injecting random noise, our methods ex-plicitly and directly maximize robustness metrics to harvest robust architectures. On CIFAR-10, ImageNet, and MNIST, we perform game-based evaluation and veriﬁcation-based evaluation on the robustness of our methods. The experi-mental results show that our methods 1) are more robust to various norm-bound attacks than several robust NAS base-lines; 2) are more accurate than baselines when there are no attacks; 3) have signiﬁcantly higher certiﬁed lower bounds than baselines. 