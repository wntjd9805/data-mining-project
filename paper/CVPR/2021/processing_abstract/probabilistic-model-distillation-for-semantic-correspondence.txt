Semantic correspondence is a fundamental problem in computer vision, which aims at establishing dense corre-spondences across images depicting different instances un-der the same category. This task is challenging due to large intra-class variations and a severe lack of ground truth. A popular solution is to learn correspondences from synthetic data. However, because of the limited intra-class appearance and background variations within syn-thetically generated training data, the model’s capability for handling “real” image pairs using such strategy is in-trinsically constrained. We address this problem with the use of a novel Probabilistic Model Distillation (PMD) ap-proach which transfers knowledge learned by a probabilis-tic teacher model on synthetic data to a static student model with the use of unlabeled real image pairs. A probabilis-tic supervision reweighting (PSR) module together with a conﬁdence-aware loss (CAL) is used to mine the useful knowledge and alleviate the impact of errors. Experimen-tal results on a variety of benchmarks show that our PMD achieves state-of-the-art performance. To demonstrate the generalizability of our approach, we extend PMD to incor-porate stronger supervision for better accuracy – the prob-abilistic teacher is trained with stronger key-point super-vision. Again, we observe the superiority of our PMD.The extensive experiments verify that PMD is able to in-fer more reliable supervision signals from the probabilistic teacher for representation learning and largely alleviate the inﬂuence of errors in pseudo labels. Code is available at https://github.com/fanyang587/PMD. 