In the setting of continual learning, a network is trained on a sequence of tasks, and suffers from catastrophic for-getting. To balance plasticity and stability of network in continual learning, in this paper, we propose a novel net-work training algorithm Adam-NSCL which sequentially optimizes network parameters in the null space of all pre-vious tasks. We ﬁrst propose two mathematical conditions respectively for achieving network stability and plasticity in continual learning. Based on them, the network training for sequential tasks without forgetting can be simply achieved by projecting the candidate parameter update into the ap-proximate null space of all previous tasks in the network training process, where the candidate parameter update can be generated by Adam. The approximate null space can be derived by applying singular value decomposition to the un-centered covariance matrix of all input features of previous tasks for each linear layer. For efﬁciency, the uncentered co-variance matrix can be incrementally computed after learn-ing each task. We also empirically verify the rationality of the approximate null space at each linear layer. We apply our approach to training networks for continual learning on benchmark datasets of CIFAR-100 and TinyImageNet, and the results suggest that the proposed approach outper-forms or matches the state-ot-the-art continual learning ap-proaches. 