Human pose estimation has achieved signiﬁcant progress in recent years. However, most of the recent meth-ods focus on improving accuracy using complicated mod-els and ignoring real-time efﬁciency. To achieve a better trade-off between accuracy and efﬁciency, we propose a novel neural architecture search (NAS) method, termed ViP-NAS, to search networks in both spatial and temporal levels for fast online video pose estimation. In the spatial level, we carefully design the search space with ﬁve different di-mensions including network depth, width, kernel size, group number, and attentions.In the temporal level, we search from a series of temporal feature fusions to optimize the to-tal accuracy and speed across multiple video frames. To the best of our knowledge, we are the ﬁrst to search for the temporal feature fusion and automatic computation alloca-tion in videos. Extensive experiments demonstrate the ef-fectiveness of our approach on the challenging COCO2017 and PoseTrack2018 datasets. Our discovered model fam-ily, S-ViPNAS and T-ViPNAS, achieve signiﬁcantly higher inference speed (CPU real-time) without sacriﬁcing the ac-curacy compared to the previous state-of-the-art methods. 