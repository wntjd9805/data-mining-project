Despite the impressive performance in many individual tasks, deep neural networks suffer from catastrophic for-getting when learning new tasks incrementally. Recently, various incremental learning methods have been proposed, and some approaches achieved acceptable performance rely-ing on stored data or complex generative models. However, storing data from previous tasks is limited by memory or pri-vacy issues, and generative models are usually unstable and inefﬁcient in training. In this paper, we propose a simple non-exemplar based method named PASS, to address the catas-trophic forgetting problem in incremental learning. On the one hand, we propose to memorize one class-representative prototype for each old class and adopt prototype augmen-tation (protoAug) in the deep feature space to maintain the decision boundary of previous tasks. On the other hand, we employ self-supervised learning (SSL) to learn more gen-eralizable and transferable features for other tasks, which demonstrates the effectiveness of SSL in incremental learn-ing. Experimental results on benchmark datasets show that our approach signiﬁcantly outperforms non-exemplar based methods, and achieves comparable performance compared to exemplar based approaches. 