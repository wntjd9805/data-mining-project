Cross-modal recipe retrieval has recently gained sub-stantial attention due to the importance of food in people’s lives, as well as the availability of vast amounts of digital cooking recipes and food images to train machine learn-ing models.In this work, we revisit existing approaches for cross-modal recipe retrieval and propose a simpliﬁed end-to-end model based on well established and high per-forming encoders for text and images. We introduce a hier-archical recipe Transformer which attentively encodes in-dividual recipe components (titles, ingredients and instruc-tions). Further, we propose a self-supervised loss function computed on top of pairs of individual recipe components, which is able to leverage semantic relationships within recipes, and enables training using both image-recipe and recipe-only samples. We conduct a thorough analysis and ablation studies to validate our design choices. As a result, our proposed method achieves state-of-the-art performance in the cross-modal recipe retrieval task on the Recipe1M dataset. We make code and models publicly available1. 