loss loss out_v1 out_v2 out_v1 out_v2We present a plug-in replacement for batch normaliza-tion (BN) called exponential moving average normaliza-tion (EMAN), which improves the performance of exist-ing student-teacher based self- and semi-supervised learn-ing techniques. Unlike the standard BN, where the statis-tics are computed within each batch, EMAN, used in the teacher, updates its statistics by exponential moving aver-age from the BN statistics of the student. This design re-duces the intrinsic cross-sample dependency of BN and en-hances the generalization of the teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2 points and semi-supervised learning by about 7/2 points, when 1%/10% supervised labels are available on ImageNet.These improvements are consistent across methods, network architectures, training duration, and datasets, demonstrat-ing the general effectiveness of this technique. The code will be made available online. 