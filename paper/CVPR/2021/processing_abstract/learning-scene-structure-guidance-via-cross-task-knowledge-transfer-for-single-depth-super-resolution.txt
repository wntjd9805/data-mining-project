Existing color-guided depth super-resolution (DSR) ap-proaches require paired RGB-D data as training samples where the RGB image is used as structural guidance to re-cover the degraded depth map due to their geometrical simi-larity. However, the paired data may be limited or expensive to be collected in actual testing environment. Therefore, we explore for the ﬁrst time to learn the cross-modality knowl-edge at training stage, where both RGB and depth modali-ties are available, but test on the target dataset, where only single depth modality exists. Our key idea is to distill the knowledge of scene structural guidance from RGB modality to the single DSR task without changing its network archi-tecture. Speciﬁcally, we construct an auxiliary depth esti-mation (DE) task that takes an RGB image as input to es-timate a depth map, and train both DSR task and DE task collaboratively to boost the performance of DSR. Upon this, a cross-task interaction module is proposed to realize bi-lateral cross-task knowledge transfer. First, we design a cross-task distillation scheme that encourages DSR and DE networks to learn from each other in a teacher-student role-exchanging fashion. Then, we advance a structure predic-tion (SP) task that provides extra structure regularization to help both DSR and DE networks learn more informative structure representations for depth recovery. Extensive ex-periments demonstrate that our scheme achieves superior performance in comparison with other DSR methods. 