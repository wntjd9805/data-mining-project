Traditional evaluation metrics for learned models that report aggregate scores over a test set are insufﬁcient for surfacing important and informative patterns of failure over features and instances. We introduce and study a method aimed at characterizing and explaining failures by identi-fying visual attributes whose presence or absence results in poor performance. In distinction to previous work that relies upon crowdsourced labels for visual attributes, we leverage the representation of a separate robust model to ex-tract interpretable features and then harness these features to identify failure modes. We further propose a visualization method aimed at enabling humans to understand the mean-ing encoded in such features and we test the comprehensi-bility of the features. An evaluation of the methods on theImageNet dataset demonstrates that: (i) the proposed work-ﬂow is effective for discovering important failure modes, (ii) the visualization techniques help humans to understand the extracted features, and (iii) the extracted insights can assist engineers with error analysis and debugging. 