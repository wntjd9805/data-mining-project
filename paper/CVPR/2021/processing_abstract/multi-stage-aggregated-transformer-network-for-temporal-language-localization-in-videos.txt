We address the problem of localizing a speciﬁc momen-t from an untrimmed video by a language sentence query.Generally, previous methods mainly exist two problems that are not fully solved: 1) How to effectively model the ﬁne-grained visual-language alignment between video and lan-guage query? 2) How to accurately localize the moment in the original video length? In this paper, we streamline the temporal language localization as a novel multi-stage ag-gregated transformer network. Speciﬁcally, we ﬁrst intro-duce a new visual-language transformer backbone, which enables iterations and alignments among all elements in visual and language sequences. Different from previous multi-modal transformers, our backbone keeps both struc-ture uniﬁed and modality speciﬁc. Moreover, we also pro-pose a multi-stage aggregation module topped on the trans-former backbone. In this module, we compute three stage-speciﬁc representations corresponding to different moment stages respectively, i.e. starting, middle and ending stages, for each video element. Then for a moment candidate, we concatenate the starting/middle/ending representations of its starting/middle/ending elements respectively to form theﬁnal moment representation. Because the obtained momen-t representation captures the stage speciﬁc information, it is very discriminative for accurate localization. Extensive experiments on ActivityNet Captions and TACoS datasets demonstrate our proposed method achieves signiﬁcant im-provements compared with all other methods. 