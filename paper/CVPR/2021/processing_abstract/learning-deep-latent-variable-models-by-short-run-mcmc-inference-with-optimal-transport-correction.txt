Learning latent variable models with deep top-down ar-chitectures typically requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference step typically relies on either time-consuming long-run Markov chain MonteCarlo (MCMC) sampling or a separate inference model for variational learning. In this paper, we propose to use a short-run MCMC, such as a short-run Langevin dynamics, as an approximate ﬂow-based inference engine. The bias existing in the output distribution of the non-convergent short-runLangevin dynamics is corrected by the optimal transport (OT), which aims at transforming the biased distribution produced by the ﬁnite-step MCMC to the prior distribution with a minimum transport cost. Our experiments not only verify the effectiveness of the OT correction for the short-runMCMC, but also demonstrate that the latent variable model trained by the proposed strategy performs better than the variational auto-encoder (VAE) in terms of image recon-struction/generation and anomaly detection. 