Video understanding calls for a model to learn the char-acteristic interplay between static scene content and its dy-namics: Given an image, the model must be able to pre-dict a future progression of the portrayed scene and, con-versely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bi-jective mapping between the video domain and the static content as well as residual information. In contrast to com-mon stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is nat-urally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus lay-ing the basis for controlled video synthesis. Experiments on diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results. Our project page is available at https://bit.ly/3dg90fV . 