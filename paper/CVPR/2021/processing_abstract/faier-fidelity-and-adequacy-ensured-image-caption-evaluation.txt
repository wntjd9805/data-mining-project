Image caption evaluation is a crucial task, which in-volves the semantic perception and matching of image and text. Good evaluation metrics aim to be fair, comprehen-sive, and consistent with human judge intentions. When hu-mans evaluate a caption, they usually consider multiple as-pects, such as whether it is related to the target image with-out distortion, how much image gist it conveys, as well as how ﬂuent and beautiful the language and wording is. The above three different evaluation orientations can be sum-marized as ﬁdelity, adequacy, and ﬂuency. The former two rely on the image content, while ﬂuency is purely related to linguistics and more subjective. Inspired by human judges, we propose a learning-based metric named FAIEr to ensure evaluating the ﬁdelity and adequacy of the captions. Since image captioning involves two different modalities, we em-ploy the scene graph as a bridge between them to represent both images and captions. FAIEr mainly regards the visual scene graph as the criterion to measure the ﬁdelity. Then for evaluating the adequacy of the candidate caption, it high-lights the image gist on the visual scene graph under the guidance of the reference captions. Comprehensive exper-imental results show that FAIEr has high consistency with human judgment as well as high stability, low reference de-pendency, and the capability of reference-free evaluation. 