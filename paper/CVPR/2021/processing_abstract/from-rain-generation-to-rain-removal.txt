For the single image rain removal (SIRR) task, the per-formance of deep learning (DL)-based methods is main-ly affected by the designed deraining models and training datasets. Most of current state-of-the-art focus on con-structing powerful deep models to obtain better derain-ing results.In this paper, to further improve the derain-ing performance, we novelly attempt to handle the SIRR task from the perspective of training datasets by exploring a more efﬁcient way to synthesize rainy images. Speciﬁ-cally, we build a full Bayesian generative model for rainy image where the rain layer is parameterized as a gen-erator with the input as some latent variables represent-ing the physical structural rain factors, e.g., direction, s-cale, and thickness. To solve this model, we employ the variational inference framework to approximate the expect-ed statistical distribution of rainy image in a data-driven manner. With the learned generator, we can automatically and sufﬁciently generate diverse and non-repetitive training pairs so as to efﬁciently enrich and augment the existing benchmark datasets. User study qualitatively and quan-titatively evaluates the realism of generated rainy images.Comprehensive experiments substantiate that the proposed model can faithfully extract the complex rain distribution that not only helps signiﬁcantly improve the deraining per-formance of current deep single image derainers, but al-so largely loosens the requirement of large training sam-ple pre-collection for the SIRR task. Code is available in https://github.com/hongwang01/VRGNet. 