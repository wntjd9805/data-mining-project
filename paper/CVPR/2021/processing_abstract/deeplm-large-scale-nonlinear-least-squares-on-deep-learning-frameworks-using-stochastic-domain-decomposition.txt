We propose a novel approach for large-scale nonlin-ear least squares problems based on deep learning frame-works. Nonlinear least squares are commonly solved with the Levenberg-Marquardt (LM) algorithm for fast conver-gence. We implement a general and efﬁcient LM solver on a deep learning framework by designing a new backward jacobian network to enable automatic sparse jacobian ma-trix computation. Furthermore, we introduce a stochastic domain decomposition approach that enables batched op-timization and preserves convergence for large problems.We evaluate our method by solving bundle adjustment as a fundamental problem. Experiments show that our op-timizer signiﬁcantly outperforms the state-of-the-art solu-tions and existing deep learning solvers considering quality, efﬁciency, and memory. Our stochastic domain decomposi-tion enables distributed optimization, consumes little mem-ory and time, and achieves similar quality compared to a global solver. As a result, our solver effectively solves non-linear least squares on an extremely large scale. Our code will be available based on Pytorch1 and Mindspore2. 