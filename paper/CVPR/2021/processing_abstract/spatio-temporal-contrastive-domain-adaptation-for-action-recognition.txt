Compared with image-based UDA, video-based UDA is comprehensive to bridge the domain shift on both spa-tial representation and temporal dynamics. Most previ-ous works focus on short-term modeling and alignment with frame-level or clip-level features, which is not dis-criminative sufﬁciently for video-based UDA tasks. To ad-dress these problems, in this paper we propose to estab-lish the cross-modal domain alignment via self-supervised contrastive framework, i.e., spatio-temporal contrastive do-main adaptation (STCDA), to learn the joint clip-level and video-level representation alignment. Since the ef-fective representation is modeled from unlabeled data by self-supervised learning (SSL), spatio-temporal contrastive learning (STCL) is proposed to explore the useful long-term feature representation for classiﬁcation, using self-supervision setting trained from the contrastive clip/video pairs with positive or negative properties. Besides, we in-volve a novel domain metric scheme, i.e., video-based con-trastive alignment (VCA), to optimize the category-aware video-level alignment and generalization between source and target. The proposed STCDA achieves stat-of-the-art results on several UDA benchmarks for action recognition. 