We address the problem of class incremental learning, which is a core step towards achieving adaptive vision in-telligence. In particular, we consider the task setting of in-cremental learning with limited memory and aim to achieve better stability-plasticity trade-off. To this end, we propose a novel two-stage learning approach that utilizes a dynam-ically expandable representation for more effective incre-mental concept modeling. SpeciÔ¨Åcally, at each incremental step, we freeze the previously learned representation and augment it with additional feature dimensions from a new learnable feature extractor. This enables us to integrate new visual concepts with retaining learned knowledge. We dy-namically expand the representation according to the com-plexity of novel concepts by introducing a channel-level mask-based pruning strategy. Moreover, we introduce an auxiliary loss to encourage the model to learn diverse and discriminate features for novel concepts. We conduct ex-tensive experiments on the three class incremental learning benchmarks and our method consistently outperforms other methods with a large margin.1 