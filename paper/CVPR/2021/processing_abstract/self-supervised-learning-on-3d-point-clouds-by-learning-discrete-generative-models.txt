While recent pre-training tasks on 2D images have proven very successful for transfer learning, pre-training for 3D data remains challenging. In this work, we intro-duce a general method for 3D self-supervised representa-tion learning that 1) remains agnostic to the underlying neural network architecture, and 2) speciﬁcally leverages the geometric nature of 3D point cloud data. The pro-posed task softly segments 3D points into a discrete number of geometric partitions. A self-supervised loss is formed under the interpretation that these soft partitions implic-itly parameterize a latent Gaussian Mixture Model (GMM), and that this generative model establishes a data likelihood function. Our pretext task can therefore be viewed in terms of an encoder-decoder paradigm that squeezes learned rep-resentations through an implicitly deﬁned parametric dis-crete generative model bottleneck. We show that any ex-isting neural network architecture designed for supervised point cloud segmentation can be repurposed for the pro-posed unsupervised pretext task. By maximizing data like-lihood with respect to the soft partitions formed by the un-supervised point-wise segmentation network, learned repre-sentations are encouraged to contain compositionally rich geometric information. In tests, we show that our method naturally induces semantic separation in feature space, re-sulting in state-of-the-art performance on downstream ap-plications like model classiﬁcation and semantic segmenta-tion. 