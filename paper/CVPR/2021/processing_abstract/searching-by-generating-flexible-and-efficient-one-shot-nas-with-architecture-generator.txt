In one-shot NAS, sub-networks need to be searched from the supernet to meet different hardware constraints. How-ever, the search cost is high and N times of searches are needed for N different constraints. In this work, we pro-pose a novel search strategy called architecture genera-tor to search sub-networks by generating them, so that the search process can be much more efﬁcient and ﬂexi-ble. With the trained architecture generator, given target hardware constraints as the input, N good architectures can be generated for N constraints by just one forward pass without re-searching and supernet retraining. More-over, we propose a novel single-path supernet, called uni-ﬁed supernet, to further improve search efﬁciency and re-duce GPU memory consumption of the architecture gener-ator. With the architecture generator and the uniﬁed super-net, we propose a ﬂexible and efﬁcient one-shot NAS frame-work, called Searching by Generating NAS (SGNAS). With the pre-trained supernt, the search time of SGNAS for N dif-ferent hardware constraints is only 5 GPU hours, which is 4N times faster than previous SOTA single-path methods.After training from scratch, the top1-accuracy of SGNAS on ImageNet is 77.1%, which is comparable with the SO-TAs. The code is available at: https://github.com/ eric8607242/SGNAS. 