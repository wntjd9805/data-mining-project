Image-text matching is an important multi-modal task with massive applications. It tries to match the image and the text with similar semantic information. Existing ap-proaches do not explicitly transform the different modali-ties into a common space. Meanwhile, the attention mech-anism which is widely used in image-text matching models does not have supervision. We propose a novel attention scheme which projects the image and text embedding into a common space and optimises the attention weights di-rectly towards the evaluation metrics. The proposed atten-tion scheme can be considered as a kind of supervised atten-tion and requiring no additional annotations. It is trained via a novel Discrete-continuous action space policy gradi-ent algorithm, which is more effective in modelling complex action space than previous continuous action space policy gradient. We evaluate the proposed methods on two widely-used benchmark datasets: Flickr30k and MS-COCO, out-performing the previous approaches by a large margin. 