One-shot object detection tackles a challenging task that aims at identifying within a target image all object instances of the same class, implied by a query image patch. The main difﬁculty lies in the situation that the class label of the query patch and its respective examples are not avail-able in the training data. Our main idea leverages the con-cept of language translation to boost metric-learning-based detection methods. Speciﬁcally, we emulate the language translation process to adaptively translate the feature of each object proposal to better correlate the given query feature for discriminating the class-similarity among the proposal-query pairs. To this end, we propose the AdaptiveImage Transformer (AIT) module that deploys an attention-based encoder-decoder architecture to simultaneously ex-plore intra-coder and inter-coder (i.e., each proposal-query pair) attention. The adaptive nature of our design turns out to be ﬂexible and effective in addressing the one-shot learning scenario. With the informative attention cues, the proposed model excels in predicting the class-similarity be-tween the target image proposals and the query image patch.Though conceptually simple, our model signiﬁcantly outper-forms a state-of-the-art technique, improving the unseen-class object classiﬁcation from 63.8 mAP and 22.0 AP50 to 72.2 mAP and 24.3 AP50 on the PASCAL-VOC and MS-COCO benchmark datasets, respectively. 