In this paper, we address the problem of rain streaks and rain accumulation removal in video, by developing a self-alignment network with transmission-depth consis-tency. Existing video based deraining methods focus only on rain streak removal, and commonly use optical ﬂow to align the rain video frames. However, besides rain streaks, rain accummulation can considerably degrade visibility; and, optical ﬂow estimation in a rain video is still erro-neous, making the deraining performance tend to be inac-curate. Our method employs deformable convolution lay-ers in our encoder to achieve feature-level frame alignment, and hence avoids using optical ﬂow. For rain streaks, our method predicts the current frame from its adjacent frames, such that rain streaks that appear randomly in the tempo-ral domain can be removed. For rain accumulation, our method employs a transmission-depth consistency loss to resolve the ambiguity between the depth and water-droplet density. Our network estimates the depth from consecu-tive rain-accumulation-removal outputs, and calculates the transmission map using a commonly used physics model.To ensure photometric-temporal and depth-temporal con-sistencies, our method estimates the camera poses, so that it can warp one frame to its adjacent frames. Experimental results show that our method is effective in removing both rain streaks and rain accumulation, outperforming those of state-of-the-art methods quantitatively and qualitatively. 