We propose SelfDoc, a task-agnostic pre-training frame-work for document image understanding. Because docu-ments are multimodal and are intended for sequential read-ing, our framework exploits the positional, textual, and vi-sual information of every semantically meaningful compo-nent in a document, and it models the contextualization between each block of content. Unlike existing document pre-training models, our model is coarse-grained instead of treating individual words as input, therefore avoiding an overly ﬁne-grained with excessive contextualization. Be-yond that, we introduce cross-modal learning in the model pre-training phase to fully leverage multimodal informa-tion from unlabeled documents. For downstream usage, we propose a novel modality-adaptive attention mecha-nism for multimodal feature fusion by adaptively empha-sizing language and vision signals. Our framework beneﬁts from self-supervised pre-training on documents without re-quiring annotations by a feature masking training strategy.It achieves superior performance on multiple downstream tasks with signiﬁcantly fewer document images used in the pre-training stage compared to previous works. 