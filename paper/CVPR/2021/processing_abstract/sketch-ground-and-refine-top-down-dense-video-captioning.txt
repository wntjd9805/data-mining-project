The dense video captioning task aims to detect and de-scribe a sequence of events in a video for detailed and co-herent storytelling. Previous works mainly adopt a “detect-then-describe” framework, which ﬁrstly detects event pro-posals in the video and then generates descriptions for the detected events. However, the deﬁnitions of events are di-verse which could be as simple as a single action or as com-plex as a set of events, depending on different semantic con-texts. Therefore, directly detecting events based on video information is ill-deﬁned and hurts the coherency and accu-racy of generated dense captions. In this work, we reverse the predominant “detect-then-describe” fashion, proposing a top-down way to ﬁrst generate paragraphs from a global view and then ground each event description to a video seg-ment for detailed reﬁnement. It is formulated as a Sketch,Ground, and Reﬁne process (SGR). The sketch stage ﬁrst generates a coarse-grained multi-sentence paragraph to de-scribe the whole video, where each sentence is treated as an event and gets localised in the grounding stage. In the re-ﬁning stage, we improve captioning quality via reﬁnement-enhanced training and dual-path cross attention on both coarse-grained event captions and aligned event segments.The updated event caption can further adjust its segment boundaries. Our SGR model outperforms state-of-the-art methods on ActivityNet Captioning benchmark under tradi-tional and story-oriented dense caption evaluations. Code will be released at github.com/bearcatt/SGR. 