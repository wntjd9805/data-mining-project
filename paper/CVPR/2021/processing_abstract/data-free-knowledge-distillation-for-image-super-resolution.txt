Convolutional network compression methods require training data for achieving acceptable results, but train-ing data is routinely unavailable due to some privacy and transmission limitations. Therefore, recent works focus on learning efﬁcient networks without original training data, i.e., data-free model compression. Wherein, most of ex-isting algorithms are developed for image recognition or segmentation tasks.In this paper, we study the data-free compression approach for single image super-resolution (SISR) task which is widely used in mobile phones and smart cameras. Speciﬁcally, we analyze the relationship between the outputs and inputs from the pre-trained net-work and explore a generator with a series of loss func-tions for maximally capturing useful information. The generator is then trained for synthesizing training sam-ples which have similar distribution to that of the origi-nal data. To further alleviate the training difﬁculty of the student network using only the synthetic data, we intro-duce a progressive distillation scheme. Experiments on var-ious datasets and architectures demonstrate that the pro-posed method is able to be utilized for effectively learn-ing portable student networks without the original data, e.g., with 0.16dB PSNR drop on Set5 for 2 super resolu-tion. Code will be available at https://github.com/huawei-noah/Data-Efﬁcient-Model-Compression.× 