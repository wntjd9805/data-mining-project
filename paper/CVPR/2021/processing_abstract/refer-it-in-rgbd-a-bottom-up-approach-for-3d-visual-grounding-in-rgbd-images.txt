Grounding referring expressions in RGBD image has been an emerging ﬁeld. We present a novel task of 3D visual grounding in single-view RGBD image where the referred objects are often only partially scanned due to occlusion.In contrast to previous works that directly generate object proposals for grounding in the 3D scenes, we propose a bottom-up approach to gradually aggregate content-aware information, effectively addressing the challenge posed by the partial geometry. Our approach ﬁrst fuses the lan-guage and the visual features at the bottom level to gen-erate a heatmap that coarsely localizes the relevant regions in the RGBD image. Then our approach conducts an adap-tive feature learning based on the heatmap and performs the object-level matching with another visio-linguistic fu-sion to ﬁnally ground the referred object. We evaluate the proposed method by comparing to the state-of-the-art meth-ods on both the RGBD images extracted from the ScanRefer dataset and our newly collected SUNRefer dataset. Experi-ments show that our method outperforms the previous meth-ods by a large margin (by 11.2% and 15.6% Acc@0.5) on both datasets. 