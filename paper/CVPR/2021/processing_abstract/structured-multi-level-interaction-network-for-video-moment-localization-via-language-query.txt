We address the problem of localizing a speciﬁc moment described by a natural language query. Existing works in-teract the query with either video frame or moment pro-posal, and neglect the inherent structure of moment con-struction for both cross-modal understanding and video content comprehension, which are the two crucial chal-lenges for this task. In this paper, we disentangle the ac-tivity moment into boundary and content. Based on the explored moment structure, we propose a novel StructuredMulti-level Interaction Network (SMIN) to tackle this prob-lem through multi-levels of cross-modal interaction coupled with content-boundary-moment interaction. In particular, for cross-modal interaction, we interact the sentence-level query with the whole moment while interacting the word-level query with content and boundary, as in a coarse-to-ﬁne manner. For content-boundary-moment interaction, we capture the insightful relations between boundary, con-tent, and the whole moment proposal. Through multi-level interactions, the model obtains robust cross-modal repre-sentation for accurate moment localization. Extensive ex-periments conducted on three benchmarks (i.e., Charades-STA, ActivityNet-Captions, and TACoS) demonstrate the proposed approach outperforms the state-of-the-art meth-ods. 