Predicting all applicable labels for a given image is known as multi-label classiﬁcation. Compared to the stan-dard multi-class case (where each image has only one la-bel), it is considerably more challenging to annotate train-ing data for multi-label classiﬁcation. When the number of potential labels is large, human annotators ﬁnd it difﬁ-cult to mention all applicable labels for each training im-age. Furthermore, in some settings detection is intrinsically difﬁcult e.g. ﬁnding small object instances in high resolu-tion images. As a result, multi-label training data is often plagued by false negatives. We consider the hardest version of this problem, where annotators provide only one relevant label for each image. As a result, training sets will have only one positive label per image and no conﬁrmed nega-tives. We explore this special case of learning from miss-ing labels across four different multi-label image classiﬁca-tion datasets for both linear classiﬁers and end-to-end ﬁne-tuned deep networks. We extend existing multi-label losses to this setting and propose novel variants that constrain the number of expected positive labels during training. Surpris-ingly, we show that in some cases it is possible to approach the performance of fully labeled classiﬁers despite training with signiﬁcantly fewer conﬁrmed labels. 