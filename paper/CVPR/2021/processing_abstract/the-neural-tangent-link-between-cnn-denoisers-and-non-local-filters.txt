Convolutional Neural Networks (CNNs) are now a well-established tool for solving computational imaging problems.Modern CNN-based algorithms obtain state-of-the-art per-formance in diverse image restoration problems. Further-more, it has been recently shown that, despite being highly overparameterized, networks trained with a single corrupted image can still perform as well as fully trained networks.We introduce a formal link between such networks through their neural tangent kernel (NTK), and well-known non-localﬁltering techniques, such as non-local means or BM3D. Theﬁltering function associated with a given network architec-ture can be obtained in closed form without need to train the network, being fully characterized by the random initial-ization of the network weights. While the NTK theory ac-curately predicts the ﬁlter associated with networks trained using standard gradient descent, our analysis shows that it falls short to explain the behaviour of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-localﬁltering function during training. We evaluate our ﬁndings via extensive image denoising experiments1. 