With increasing ﬁelds of application for neural networks and the development of neural networks, the ability to ex-plain deep learning models is also becoming increasingly important. Especially, prior to practical applications, it is crucial to analyze a model’s inference and the process of generating the results. A common explanation method is Class Activation Mapping(CAM) based method where it is often used to understand the last layer of the convolu-tional neural networks popular in the ﬁeld of Computer Vi-sion. In this paper, we propose a novel CAM method namedRelevance-weighted Class Activation Mapping(Relevance-CAM) that utilizes Layer-wise Relevance Propagation to obtain the weighting components. This allows the expla-nation map to be faithful and robust to the shattered gradi-ent problem, a shared problem of the gradient based CAM methods that causes noisy saliency maps for intermediate layers. Therefore, our proposed method can better explain a model by correctly analyzing the intermediate layers as well as the last convolutional layer. In this paper, we visu-alize how each layer of the popular image processing mod-els extracts class speciﬁc features using Relevance-CAM, evaluate the localization ability, and show why the gradi-ent based CAM cannot be used to explain the intermedi-ate layers, proven by experimenting the weighting compo-nent. Relevance-CAM outperforms other CAM-based meth-ods in recognition and localization evaluation in layers of any depth. The source code is available at: https://github.com/mongeoroo/Relevance-CAM 