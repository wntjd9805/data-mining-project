Recent advances in neuroscience have highlighted the effectiveness of multi-modal medical data for investigat-ing certain pathologies and understanding human cogni-tion. However, obtaining full sets of different modali-ties is limited by various factors, such as long acquisition times, high examination costs and artifact suppression. In addition, the complexity, high dimensionality and hetero-geneity of neuroimaging data remains another key chal-lenge in leveraging existing randomized scans effectively, as data of the same modality is often measured differently by different machines. There is a clear need to go beyond the traditional imaging-dependent process and synthesize anatomically speciﬁc target-modality data from a source in-put. In this paper, we propose to learn dedicated features that cross both intre- and intra-modal variations using a novel CSCℓ4Net. Through an initial uniﬁcation of intra-modal data in the feature maps and multivariate canon-ical adaptation, CSCℓ4Net facilitates feature-level mutual transformation. The positive deﬁnite Riemannian manifold-penalized data ﬁdelity term further enables CSCℓ4Net to re-construct missing measurements according to transformed features. Finally, the maximization ℓ4-norm boils down to a computationally efﬁcient optimization problem. Exten-sive experiments validate the ability and robustness of ourCSCℓ4Net compared to the state-of-the-art methods on mul-tiple datasets. 