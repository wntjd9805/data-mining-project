Self-supervised learning has been widely used to obtain transferrable representations from unlabeled images. Espe-cially, recent contrastive learning methods have shown im-pressive performances on downstream image classiﬁcation tasks. While these contrastive methods mainly focus on gen-erating invariant global representations at the image-level under semantic-preserving transformations, they are prone to overlook spatial consistency of local representations and therefore have a limitation in pretraining for localization tasks such as object detection and instance segmentation.Moreover, aggressively cropped views used in existing con-trastive methods can minimize representation distances be-tween the semantically different regions of a single image.In this paper, we propose a spatially consistent repre-sentation learning algorithm (SCRL) for multi-object andIn particular, we devise a novel location-speciﬁc tasks. self-supervised objective that tries to produce coherent spa-tial representations of a randomly cropped local region ac-cording to geometric translations and zooming operations.On various downstream localization tasks with benchmark datasets, the proposed SCRL shows signiﬁcant performance improvements over the image-level supervised pretrain-ing as well as the state-of-the-art self-supervised learning methods. Code is available at https://github.com/ kakaobrain/scrl. (a) (b)Figure 1. (a) AP on downstream of COCO detection task w.r.t. the upstream epochs on ImageNet. We use a ResNet-50-FPN back-bone with Faster R-CNN, using default training conﬁguration used in [42]. Only with 200 epochs of upstream, SCRL outperforms the ImageNet pre-trained counterpart as well as the state-of-the-art self-supervised learning methods. (b) AP on COCO detection task under varied downstream schedules from 0.5× (45k iterations) to 7× (630k iterations). SCRL consistently outperforms random ini-tialization, supervised pretraining, and BYOL in all the training schedules. 