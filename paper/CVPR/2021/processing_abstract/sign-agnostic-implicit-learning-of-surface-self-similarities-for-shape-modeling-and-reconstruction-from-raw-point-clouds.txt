Shape modeling and reconstruction from raw point clouds of objects stand as a fundamental challenge in vi-sion and graphics research. Classical methods consider analytic shape priors; however, their performance is de-graded when the scanned points deviate from the ideal con-ditions of cleanness and completeness. Important progress has been recently made by data-driven approaches, which learn global and/or local models of implicit surface repre-sentations from auxiliary sets of training shapes. Motivated from a universal phenomenon that self-similar shape pat-terns of local surface patches repeat across the entire sur-face of an object, we aim to push forward the data-driven strategies and propose to learn a local implicit surface net-work for a shared, adaptive modeling of the entire surface for a direct surface reconstruction from raw point cloud; we also enhance the leveraging of surface self-similarities by improving correlations among the optimized latent codes of individual surface patches. Given that orientations of raw points could be unavailable or noisy, we extend sign-agnostic learning into our local implicit model, which en-ables our recovery of signed implicit ﬁelds of local sur-faces from the unsigned inputs. We term our framework asSign-Agnostic Implicit Learning of Surface Self-Similarities (SAIL-S3). With a global post-optimization of local signﬂipping, SAIL-S3 is able to directly model raw, un-oriented point clouds and reconstruct high-quality object surfaces.Experiments show its superiority over existing methods. 