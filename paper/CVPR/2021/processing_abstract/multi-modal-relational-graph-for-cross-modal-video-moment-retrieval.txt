Untrimmed(cid:2)VideoGiven an untrimmed video and a query sentence, cross-modal video moment retrieval aims to rank a video moment from pre-segmented video moment candidates that best matches the query sentence. Pioneering work typically learns the representations of the textual and visual content separately and then obtains the interactions or alignments between different modalities. However, the task of cross-modal video moment retrieval is not yet thoroughly ad-dressed as it needs to further identify the ﬁne-grained differences of video moment candidates with high repeata-bility and similarity. Moveover, the relation among objects in both video and sentence is intuitive and efﬁcient for understanding semantics but is rarely considered.Toward this end, we contribute a multi-modal relational graph to capture the interactions among objects from the visual and textual content to identify the differences among similar video moment candidates.Speciﬁcally, we ﬁrst introduce a visual relational graph and a textual relational graph to form relation-aware representations via message propagation. Thereafter, a multi-task pre-training is designed to capture domain-speciﬁc knowledge about objects and relations, enhancing the structured visual representation after explicitly deﬁned relation. Finally, the graph matching and boundary regression are employed to perform the cross-modal retrieval. We conduct extensive experiments on two datasets about daily activities and cooking activities, demonstrating signiﬁcant improvements over state-of-the-art solutions. 