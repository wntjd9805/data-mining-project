Current dynamic networks and dynamic pruning methods have shown their promising capability in reducing theo-retical computation complexity. However, dynamic sparse patterns on convolutional ﬁlters fail to achieve actual ac-celeration in real-world implementation, due to the extra burden of indexing, weight-copying, or zero-masking. Here, we explore a dynamic network slimming regime, named Dy-namic Slimmable Network (DS-Net), which aims to achieve good hardware-efﬁciency via dynamically adjusting ﬁlter numbers of networks at test time with respect to different inputs, while keeping ﬁlters stored statically and contigu-ously in hardware to prevent the extra burden. Our DS-Net is empowered with the ability of dynamic inference by the proposed double-headed dynamic gate that comprises an attention head and a slimming head to predictively adjust network width with negligible extra computation cost. To ensure generality of each candidate architecture and the fairness of gate, we propose a disentangled two-stage train-ing scheme inspired by one-shot NAS. In the ﬁrst stage, a novel training technique for weight-sharing networks namedIn-place Ensemble Bootstrapping is proposed to improve the supernet training efﬁcacy. In the second stage, SandwichGate Sparsiﬁcation is proposed to assist the gate training by identifying easy and hard samples in an online way. Ex-tensive experiments demonstrate our DS-Net consistently outperforms its static counterparts as well as state-of-the-art static and dynamic model compression methods by a large margin (up to 5.9%). Typically, DS-Net achieves 2-4× com-putation reduction and 1.62× real-world acceleration overResNet-50 and MobileNet with minimal accuracy drops onImageNet.1 