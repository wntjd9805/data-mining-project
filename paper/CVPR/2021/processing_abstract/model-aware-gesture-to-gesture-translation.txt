Hand gesture-to-gesture translation is a signiﬁcant and interesting problem, which serves as a key role in many ap-plications, such as sign language production. This task in-volves ﬁne-grained structure understanding of the mapping between the source and target gestures. Current works fol-low a data-driven paradigm based on sparse 2D joint rep-resentation. However, given the insufﬁcient representation capability of 2D joints, this paradigm easily leads to blurry generation results with incorrect structure. In this paper, we propose a novel model-aware gesture-to-gesture translation framework, which introduces hand prior with hand meshes as the intermediate representation. To take full advantage of the structured hand model, we ﬁrst build a dense topology map aligning the image plane with the encoded embedding of the visible hand mesh. Then, a transformation ﬂow is calculated based on the correspondence of the source and target topology map. During the generation stage, we inject the topology information into generation streams by modu-lating the activations in a spatially-adaptive manner. Fur-ther, we incorporate the source local characteristic to en-hance the translated gesture image according to the trans-formation ﬂow. Extensive experiments on two benchmark datasets have demonstrated that our method achieves new state-of-the-art performance. 