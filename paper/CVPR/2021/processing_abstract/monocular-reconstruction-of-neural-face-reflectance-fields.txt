The reﬂectance ﬁeld of a face describes the reﬂectance properties responsible for complex lighting effects includ-ing diffuse, specular, inter-reﬂection and self shadowing.Most existing methods for estimating the face reﬂectance from a monocular image assume faces to be diffuse with very few approaches adding a specular component. This still leaves out important perceptual aspects of reﬂectance such as higher-order global illumination effects and self-shadowing. We present a new neural representation for face reﬂectance where we can estimate all components of the reﬂectance responsible for the ﬁnal appearance from a monocular image. Instead of modeling each component of the reﬂectance separately using parametric models, our neural representation allows us to generate a basis set of faces in a geometric deformation-invariant space, parame-terized by the input light direction, viewpoint and face ge-ometry. We learn to reconstruct this reﬂectance ﬁeld of a face just from a monocular image, which can be used to ren-der the face from any viewpoint in any light condition. Our method is trained on a light-stage dataset, which captures 300 people illuminated with 150 light conditions from 8 viewpoints. We show that our method outperforms existing monocular reﬂectance reconstruction methods due to bet-ter capturing of physical effects, such as sub-surface scat-tering, specularities, self-shadows and other higher-order effects. 