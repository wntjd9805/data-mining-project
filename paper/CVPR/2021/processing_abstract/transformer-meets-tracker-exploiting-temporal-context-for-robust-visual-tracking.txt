In video object tracking, there exist rich temporal con-texts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The trans-former encoder promotes the target templates via attention-based feature reinforcement, which beneÔ¨Åts the high-quality tracking model generation. The transformer decoder prop-agates the tracking cues from previous templates to the cur-rent frame, which facilitates the object searching process.Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed trans-former, a simple Siamese matching approach is able to out-perform the current top-performing trackers. By combin-ing our transformer with the recent discriminative track-ing pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks. 