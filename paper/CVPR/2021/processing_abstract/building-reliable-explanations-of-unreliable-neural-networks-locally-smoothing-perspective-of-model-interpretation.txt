We present a novel method for reliably explaining the predictions of neural networks. We consider an explanation reliable if it identiﬁes input features relevant to the model output by considering the input and the neighboring data points. Our method is built on top of the assumption of smooth landscape in a loss function of the model predic-tion: locally consistent loss and gradient proﬁle. A theoreti-cal analysis established in this study suggests that those lo-cally smooth model explanations are learned using a batch of noisy copies of the input with the L1 regularization for a saliency map. Extensive experiments support the analy-sis results, revealing that the proposed saliency maps re-trieve the original classes of adversarial examples crafted against both naturally and adversarially trained models, signiﬁcantly outperforming previous methods. We further demonstrated that such good performance results from the learning capability of this method to identify input features that are truly relevant to the model output of the input and the neighboring data points, fulﬁlling the requirements of a reliable explanation. 