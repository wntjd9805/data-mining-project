Deep neural networks for video classiﬁcation, just like image classiﬁcation networks, may be subjected to adver-sarial manipulation. The main difference between image classiﬁers and video classiﬁers is that the latter usually use temporal information contained within the video.In this work we present a manipulation scheme for fooling video classiﬁers by introducing a ﬂickering temporal per-turbation that in some cases may be unnoticeable by hu-man observers and is implementable in the real world. Af-ter demonstrating the manipulation of action classiﬁcation of single videos, we generalize the procedure to make uni-versal adversarial perturbation, achieving high fooling ra-tio. In addition, we generalize the universal perturbation and produce a temporal-invariant perturbation, which can be applied to the video without synchronizing the pertur-bation to the input. The attack was implemented on sev-eral target models and the transferability of the attack was demonstrated. These properties allow us to bridge the gap between simulated environment and real-world application, as will be demonstrated in this paper for the ﬁrst time for an over-the-air ﬂickering attack. (a) Diagram of a Flickering Adversarial Attack in a simulated en-vironment (digital). (b) Diagram of an Over-the-Air Flickering Adversarial Attack in the real-world (physical).Figure 1: Top ﬁgure shows the attack diagram in the digital domain performed by adding a uniform RGB perturbation to the attacked video. Bottom ﬁgure shows the modeling of the digitally-developed attack into the real-world by trans-mitting the perturbation in the scene using a smart RGB led bulb. 