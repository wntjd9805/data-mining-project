Unsupervised contrastive learning has achieved out-standing success, while the mechanism of contrastive lossIn this paper, we concentrate on has been less studied. the understanding of the behaviours of unsupervised con-trastive loss. We will show that the contrastive loss is a hardness-aware loss function, and the temperature τ con-trols the strength of penalties on hard negative samples. The previous study has shown that uniformity is a key property of contrastive learning. We build relations between the uni-formity and the temperature τ . We will show that uniformity helps the contrastive learning to learn separable features, however excessive pursuit to the uniformity makes the con-trastive loss not tolerant to semantically similar samples, which may break the underlying semantic structure and be harmful to the formation of features useful for downstream tasks. This is caused by the inherent defect of the instance discrimination objective. Speciﬁcally, instance discrimina-tion objective tries to push all different instances apart, ig-noring the underlying relations between samples. Pushing semantically consistent samples apart has no positive effect for acquiring a prior informative to general downstream tasks. A well-designed contrastive loss should have some extents of tolerance to the closeness of semantically sim-ilar samples. Therefore, we ﬁnd that the contrastive loss meets a uniformity-tolerance dilemma, and a good choice of temperature can compromise these two properties prop-erly to both learn separable features and tolerant to seman-tically similar samples, improving the feature qualities and the downstream performances. 