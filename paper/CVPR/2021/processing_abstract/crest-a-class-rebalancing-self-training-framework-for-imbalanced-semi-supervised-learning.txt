Semi-supervised learning on class-imbalanced data, al-though a realistic problem, has been under studied. While existing semi-supervised learning (SSL) methods are known to perform poorly on minority classes, we Ô¨Ånd that they still generate high precision pseudo-labels on minority classes.By exploiting this property, in this work, we propose Class-Rebalancing Self-Training (CReST), a simple yet effec-tive framework to improve existing SSL methods on class-imbalanced data. CReST iteratively retrains a baselineSSL model with a labeled set expanded by adding pseudo-labeled samples from an unlabeled set, where pseudo-labeled samples from minority classes are selected more frequently according to an estimated class distribution. We also propose a progressive distribution alignment to adap-tively adjust the rebalancing strength dubbed CReST+. We show that CReST and CReST+ improve state-of-the-art SSL algorithms on various class-imbalanced datasets and con-sistently outperform other popular rebalancing methods.Code has been made available at https://github. com/google-research/crest. 