Neural rendering techniques combining machine learn-ing with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance ﬁelds (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achiev-ing an unprecedented level of photorealism on the gener-ated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different im-ages.In this paper we introduce D-NeRF, a method that extends neural radiance ﬁelds to a dynamic domain, allow-ing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canon-ical space and another that maps this canonical represen-tation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can ren-der novel images, controlling both the camera view and the time variable, and thus, the object movement. We demon-strate the effectiveness of our approach on scenes with ob-jects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be avail-able at [1]. 