Despite remarkable progress achieved, most neural ar-chitecture search (NAS) methods focus on searching for one single accurate and robust architecture. To further build models with better generalization capability and perfor-mance, model ensemble is usually adopted and performs better than stand-alone models. Inspired by the merits of model ensemble, we propose to search for multiple diverse models simultaneously as an alternative way to ﬁnd pow-erful models. Searching for ensembles is non-trivial and has two key challenges: enlarged search space and poten-tially more complexity for the searched model. In this paper, we propose a one-shot neural ensemble architecture search (NEAS) solution that addresses the two challenges. For theﬁrst challenge, we introduce a novel diversity-based met-ric to guide search space shrinking, considering both the potentiality and diversity of candidate operators. For the second challenge, we enable a new search dimension to learn layer sharing among different models for efﬁciency purposes. The experiments on ImageNet clearly demon-strate that our solution can improve the supernet’s capacity of ranking ensemble architectures, and further lead to better search results. The discovered architectures achieve supe-rior performance compared with state-of-the-arts such asMobileNetV3 and EfﬁcientNet families under aligned set-tings. Moreover, we evaluate the generalization ability and robustness of our searched architecture on the COCO de-tection benchmark and achieve a 3.1% improvement on AP compared with MobileNetV3. Codes and models are avail-able here. 