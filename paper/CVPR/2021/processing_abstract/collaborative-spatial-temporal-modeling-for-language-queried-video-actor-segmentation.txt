Language-queried video actor segmentation aims to pre-dict the pixel-level mask of the actor which performs the actions described by a natural language query in the tar-get frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolu-tions are amenable to recognizing which actor is perform-ing the queried actions, it also inevitably introduces mis-aligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccu-rate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which con-tains a 3D temporal encoder over the video clip to recog-nize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to ï¬‚exibly integrate spatial and tem-poral features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial- and temporal-relevant lin-guistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new state-of-the-art performance on two popular benchmarks with less computational overhead than previous approaches. 