We explore and analyze the latent style space of Style-GAN2, a state-of-the-art architecture for image genera-tion, using models pretrained on several different datasets.We ﬁrst show that StyleSpace, the space of channel-wise style parameters, is signiﬁcantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to con-trol a distinct visual attribute in a highly localized and dis-entangled manner. Third, we propose a simple method for identifying style channels that control a speciﬁc attribute, using a pretrained classiﬁer or a small number of exam-ple images. Manipulation of visual attributes via theseStyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric.Finally, we demonstrate the applicability of StyleSpace con-trols to the manipulation of real images. Our ﬁndings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces. 