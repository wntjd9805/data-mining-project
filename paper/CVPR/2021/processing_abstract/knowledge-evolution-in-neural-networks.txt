Deep learning relies on the availability of a large cor-pus of data (labeled or unlabeled). Thus, one challenging unsettled question is: how to train a deep network on a rela-tively small dataset? To tackle this question, we propose an evolution-inspired training approach to boost performance on relatively small datasets. The knowledge evolution (KE) approach splits a deep network into two hypotheses: the ﬁt-hypothesis and the reset-hypothesis. We iteratively evolve the knowledge inside the ﬁt-hypothesis by perturbing the reset-hypothesis for multiple generations. This approach not only boosts performance, but also learns a slim network with a smaller inference cost. KE integrates seamlessly with both vanilla and residual convolutional networks. KE re-duces both overﬁtting and the burden for data collection.We evaluate KE on various network architectures and loss functions. We evaluate KE using relatively small datasets (e.g., CUB-200) and randomly initialized deep net-works. KE achieves an absolute 21% improvement margin on a state-of-the-art baseline. This performance improve-ment is accompanied by a relative 73% reduction in infer-ence cost. KE achieves state-of-the-art results on classiﬁ-cation and metric learning benchmarks. Code available at http://bit.ly/3uLgwYb 