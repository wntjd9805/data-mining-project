We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adver-sarial ﬁltration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodiﬁed ex-amples transfer to various unseen models reliably, demon-strating that computer vision models have shared weak-nesses. The ﬁrst dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the ﬁrst out-of-distribution detection dataset created forImageNet models. On IMAGENET-A a DenseNet-121 ob-tains around 2% accuracy, an accuracy drop of approx-imately 90%, and its out-of-distribution detection perfor-mance on IMAGENET-O is near random chance levels.We ﬁnd that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we ﬁnd that improvements to computer vision architectures provide a promising path towards robust models. 