Previous methodUnsupervised Domain Adaptation (UDA) transfers pre-dictive models from a fully-labeled source domain to an unlabeled target domain.In some applications, however, it is expensive even to collect labels in the source do-main, making most previous works impractical. To cope with this problem, recent work performed instance-wise cross-domain self-supervised learning, followed by an ad-ditional ﬁne-tuning stage. However, the instance-wise self-supervised learning only learns and aligns low-level dis-criminative features.In this paper, we propose an end-to-end Prototypical Cross-domain Self-Supervised Learn-ing (PCS) framework for Few-shot Unsupervised DomainAdaptation (FUDA)1. PCS not only performs cross-domain low-level feature alignment, but it also encodes and aligns semantic structures in the shared embedding space across domains. Our framework captures category-wise seman-tic structures of the data by in-domain prototypical con-trastive learning; and performs feature alignment through cross-domain prototypical self-supervision. Compared with state-of-the-art methods, PCS improves the mean classiﬁ-cation accuracy over different domain pairs on FUDA by 10.5%, 3.5%, 9.0%, and 13.2% on Ofﬁce, Ofﬁce-Home,VisDA-2017, and DomainNet, respectively. 