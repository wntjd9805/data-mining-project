Convolutional neural network (CNN) pruning has be-come one of the most successful network compression ap-proaches in recent years. Existing works on network prun-ing usually focus on removing the least important ﬁlters in the network to achieve compact architectures.In this study, we claim that identifying structural redundancy plays a more essential role than ﬁnding unimportant ﬁlters, the-oretically and empirically. We ﬁrst statistically model the network pruning problem in a redundancy reduction per-spective and ﬁnd that pruning in the layer(s) with the most structural redundancy outperforms pruning the least impor-tant ﬁlters across all layers. Based on this ﬁnding, we then propose a network pruning approach that identiﬁes struc-tural redundancy of a CNN and prunes ﬁlters in the selected layer(s) with the most redundancy. Experiments on various benchmark network architectures and datasets show that our proposed approach signiﬁcantly outperforms the pre-vious state-of-the-art. 