Semantic segmentation has made tremendous progress in recent years. However, satisfying performance highly de-pends on a large number of pixel-level annotations. There-fore, in this paper, we focus on the semi-supervised seg-mentation problem where only a small set of labeled data is provided with a much larger collection of totally unlabeled images. Nevertheless, due to the limited annotations, mod-els may overly rely on the contexts available in the training data, which causes poor generalization to the scenes un-seen before. A preferred high-level representation should capture the contextual information while not losing self-awareness. Therefore, we propose to maintain the context-aware consistency between features of the same identity but with different contexts, making the representations robust to the varying environments. Moreover, we present the Direc-tional Contrastive Loss (DC Loss) to accomplish the consis-tency in a pixel-to-pixel manner, only requiring the feature with lower quality to be aligned towards its counterpart.In addition, to avoid the false-negative samples and Ô¨Ålter the uncertain positive samples, we put forward two sam-pling strategies. Extensive experiments show that our sim-ple yet effective method surpasses current state-of-the-art methods by a large margin and also generalizes well with extra image-level annotations. 