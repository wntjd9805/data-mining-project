In the animation industry, cartoon videos are usually produced at low frame rate since hand drawing of such frames is costly and time-consuming. Therefore, it is desir-able to develop computational models that can automatically interpolate the in-between animation frames. However, ex-isting video interpolation methods fail to produce satisfying results on animation data. Compared to natural videos, animation videos possess two unique characteristics that make frame interpolation difﬁcult: 1) cartoons comprise lines and smooth color pieces. The smooth areas lack tex-tures and make it difﬁcult to estimate accurate motions on animation videos. 2) cartoons express stories via exagger-ation. Some of the motions are non-linear and extremely large. In this work, we formally deﬁne and study the an-imation video interpolation problem for the ﬁrst time. To address the aforementioned challenges, we propose an effec-tive framework, AnimeInterp, with two dedicated modules in a coarse-to-ﬁne manner. Speciﬁcally, 1) Segment-GuidedMatching resolves the “lack of textures” challenge by ex-ploiting global matching among color pieces that are piece-wise coherent. 2) Recurrent Flow Reﬁnement resolves the“non-linear and extremely large motion” challenge by recur-rent predictions using a transformer-like architecture. To facilitate comprehensive training and evaluations, we build a large-scale animation triplet dataset, ATD-12K, which comprises 12,000 triplets with rich annotations. Extensive experiments demonstrate that our approach outperforms ex-∗Equal contributions; BCorresponding author. isting state-of-the-art interpolation methods for animation videos. Notably, AnimeInterp shows favorable perceptual quality and robustness for animation scenarios in the wild.The proposed dataset and code are available at https://github.com/lisiyao21/AnimeInterp/. 