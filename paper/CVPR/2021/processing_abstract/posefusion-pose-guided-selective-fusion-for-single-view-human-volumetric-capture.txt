We propose POse-guided SElective Fusion (POSEFu-sion), a single-view human volumetric capture method that leverages tracking-based methods and tracking-free infer-ence to achieve high-ﬁdelity and dynamic 3D reconstruc-tion. By contributing a novel reconstruction framework which contains pose-guided keyframe selection and robust implicit surface fusion, our method fully utilizes the advan-tages of both tracking-based methods and tracking-free in-ference methods, and ﬁnally enables the high-ﬁdelity recon-struction of dynamic surface details even in the invisible re-gions. We formulate the keyframe selection as a dynamic programming problem to guarantee the temporal continuity of the reconstructed sequence. Moreover, the novel robust implicit surface fusion involves an adaptive blending weight to preserve high-ﬁdelity surface details and an automatic collision handling method to deal with the potential self-collisions. Overall, our method enables high-ﬁdelity and dynamic capture in both visible and invisible regions from a single RGBD camera, and the results and experiments show that our method outperforms state-of-the-art methods. 