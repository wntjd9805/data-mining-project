CEC………Few-shot class-incremental learning (FSCIL) aims to de-sign machine learning algorithms that can continually learn new concepts from a few data points, without forgetting knowledge of old classes. The difﬁculty lies in that limited data from new classes not only lead to signiﬁcant overﬁtting issues but also exacerbate the notorious catastrophic forget-ting problems. Moreover, as training data come in sequence in FSCIL, the learned classiﬁer can only provide discrimi-native information in individual sessions, while FSCIL re-quires all classes to be involved for evaluation. In this pa-per, we address the FSCIL problem from two aspects. First, we adopt a simple but effective decoupled learning strat-egy of representations and classiﬁers that only the classi-ﬁers are updated in each incremental session, which avoids knowledge forgetting in the representations. By doing so, we demonstrate that a pre-trained backbone plus a non-parametric class mean classiﬁer can beat state-of-the-art methods. Second, to make the classiﬁers learned on in-dividual sessions applicable to all classes, we propose aContinually Evolved Classiﬁer (CEC) that employs a graph model to propagate context information between classiﬁers for adaptation. To enable the learning of CEC, we de-sign a pseudo incremental learning paradigm that episodi-cally constructs a pseudo incremental learning task to opti-mize the graph parameters by sampling data from the base dataset. Experiments on three popular benchmark datasets, including CIFAR100, miniImageNet, and Caltech-USCDBirds-200-2011 (CUB200), show that our method signiﬁ-cantly outperforms the baselines and sets new state-of-the-art results with remarkable advantages. 