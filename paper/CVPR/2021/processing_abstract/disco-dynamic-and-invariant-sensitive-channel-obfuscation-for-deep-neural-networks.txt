Recent deep learning models have shown remarkable performance in image classiﬁcation. While these deep learning systems are getting closer to practical deploy-ment, the common assumption made about data is that it does not carry any sensitive information. This assump-tion may not hold for many practical cases, especially in the domain where an individual’s personal information is involved, like healthcare and facial recognition systems.We posit that selectively removing features in this latent space can protect the sensitive information and provide better privacy-utility trade-off. Consequently, we proposeDISCO which learns a dynamic and data driven pruningﬁlter to selectively obfuscate sensitive information in the feature space. We propose diverse attack schemes for sensi-tive inputs & attributes and demonstrate the effectiveness ofDISCO against state-of-the-art methods through quantita-tive and qualitative evaluation. Finally, we also release an evaluation benchmark dataset of 1 million sensitive repre-sentations to encourage rigorous exploration of novel at-tack and defense schemes at https://github.com/ splitlearning/InferenceBenchmark. 