We present a self-supervised learning approach to learn audio-visual representations from video and audio. Our method uses contrastive learning for cross-modal discrimi-nation of video from audio and vice-versa. We show that op-timizing for cross-modal discrimination, rather than within-modal discrimination, is important to learn good represen-tations from video and audio. With this simple but powerful insight, our method achieves highly competitive performance when ﬁnetuned on action recognition tasks. Furthermore, while recent work in contrastive learning deﬁnes positive and negative samples as individual instances, we general-ize this deﬁnition by exploring cross-modal agreement. We group together multiple instances as positives by measuring their similarity in both the video and audio feature spaces.Cross-modal agreement creates better positive and negative sets, which allows us to calibrate visual similarities by seek-ing within-modal discrimination of positive instances, and achieve signiﬁcant gains on downstream tasks. 