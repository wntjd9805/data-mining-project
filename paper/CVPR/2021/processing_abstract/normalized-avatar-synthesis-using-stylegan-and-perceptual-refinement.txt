We introduce a highly robust GAN-based framework for digitizing a normalized 3D avatar of a person from a sin-gle unconstrained photo. While the input image can be of a smiling person or taken in extreme lighting conditions, our method can reliably produce a high-quality textured model of a person’s face in neutral expression and skin textures under diffuse lighting condition. Cutting-edge 3D face re-construction methods use non-linear morphable face mod-els combined with GAN-based decoders to capture the like-ness and details of a person but fail to produce neutral head models with unshaded albedo textures which is critical for creating relightable and animation-friendly avatars for in-tegration in virtual environments. The key challenges for existing methods to work is the lack of training and ground truth data containing normalized 3D faces. We propose aHao Li is afﬁliated with Pinscreen and UC Berkeley; Koki Nagano is currently at NVIDIA. This work was fully conducted at Pinscreen. two-stage approach to address this problem. First, we adopt a highly robust normalized 3D face generator by embed-ding a non-linear morphable face model into a StyleGAN2 network. This allows us to generate detailed but normalized facial assets. This inference is then followed by a perceptual reﬁnement step that uses the generated assets as regulariza-tion to cope with the limited available training samples of normalized faces. We further introduce a Normalized FaceDataset, which consists of a combination photogrammetry scans, carefully selected photographs, and generated fake people with neutral expressions in diffuse lighting condi-tions. While our prepared dataset contains two orders of magnitude less subjects than cutting edge GAN-based 3D facial reconstruction methods, we show that it is possible to produce high-quality normalized face models for very chal-lenging unconstrained input images, and demonstrate supe-rior performance to the current state-of-the-art. 11662