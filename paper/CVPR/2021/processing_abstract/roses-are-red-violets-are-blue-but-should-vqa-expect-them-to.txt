Models for Visual Question Answering (VQA) are notori-ous for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to “rea-son”, leading them to perform “educated guesses” instead.In this paper, we claim that the standard evaluation met-ric, which consists in measuring the overall in-domain ac-curacy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit sub-tle training set statistics. Alternatively, naively introducing artiﬁcial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not re-ﬂect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are speciﬁcally designed for this particular setting, and do not generalize to other conﬁgurations. We propose the GQA-OOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we ex-perimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA mod-els and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions in-volving infrequent concepts and provide recommendations for future directions of research. 