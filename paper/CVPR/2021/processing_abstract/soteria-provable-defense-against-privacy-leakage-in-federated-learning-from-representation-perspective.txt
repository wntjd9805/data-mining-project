Federated learning (FL) is a popular distributed learn-ing framework that can reduce privacy risks by not ex-plicitly sharing private data. However, recent works have demonstrated that sharing model updates makes FL vulner-able to inference attack. In this work, we show our key ob-servation that the data representation leakage from gradi-ents is the essential cause of privacy leakage in FL. We also provide an analysis of this observation to explain how the data presentation is leaked. Based on this observation, we propose a defense called Soteria against model inversion attack in FL. The key idea of our defense is learning to per-turb data representation such that the quality of the recon-structed data is severely degraded, while FL performance is maintained. In addition, we derive a certiﬁed robustness guarantee to FL and a convergence guarantee to FedAvg, after applying our defense. To evaluate our defense, we conduct experiments on MNIST and CIFAR10 for defend-ing against the DLG attack and GS attack. Without sacri-ﬁcing accuracy, the results demonstrate that our proposed defense can increase the mean squared error between the reconstructed data and the raw data by as much as 160× for both DLG attack and GS attack, compared with base-line defense methods. Therefore, the privacy of the FL sys-tem is signiﬁcantly improved. Our code can be found at https://github.com/jeremy313/Soteria. 