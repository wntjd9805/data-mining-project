Most existing unsupervised video hashing methods are built on unidirectional models with less reliable training objectives, which underuse the correlations among frames and the similarity structure between videos. To enable efÔ¨Å-cient scalable video retrieval, we propose a self-supervised video Hashing method based on Bidirectional Transform-ers (BTH). Based on the encoder-decoder structure of trans-formers, we design a visual cloze task to fully exploit the bidirectional correlations between frames. To unveil the similarity structure between unlabeled video data, we fur-ther develop a similarity reconstruction task by establish-ing reliable and effective similarity connections in the video space. Furthermore, we develop a cluster assignment task to exploit the structural statistics of the whole dataset such that more discriminative binary codes can be learned. Ex-tensive experiments implemented on three public bench-mark datasets, FCVID, ActivityNet and YFCC, demonstrate the superiority of our proposed approach. 