We present GATSBI, a generative model that can trans-form a sequence of raw observations into a structured latent representation that fully captures the spatio-temporal con-text of the agent’s actions. In vision-based decision making scenarios, an agent faces complex high-dimensional obser-vations where multiple entities interact with each other. The agent requires a good scene representation of the visual observation that discerns essential components and con-sistently propagates along the time horizon. Our method,GATSBI, utilizes unsupervised object-centric scene repre-sentation learning to separate an active agent, static back-ground, and passive objects. GATSBI then models the in-teractions reﬂecting the causal relationships among decom-posed entities and predicts physically plausible future states.Our model generalizes to a variety of environments where dif-ferent types of robots and objects dynamically interact with each other. We show GATSBI achieves superior performance on scene decomposition and video prediction compared to its state-of-the-art counterparts. 