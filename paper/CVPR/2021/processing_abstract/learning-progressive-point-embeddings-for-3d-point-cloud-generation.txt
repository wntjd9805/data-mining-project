Generative models for 3D point clouds are extremely im-portant for scene/object reconstruction applications in au-tonomous driving and robotics. Despite recent success of deep learning-based representation learning, it remains a great challenge for deep neural networks to synthesize or reconstruct high-ﬁdelity point clouds, because of the dif-ﬁculties in 1) learning effective pointwise representations; and 2) generating realistic point clouds from complex dis-tributions. In this paper, we devise a dual-generators frame-work for point cloud generation, which generalizes vanilla generative adversarial learning framework in a progressive manner. Speciﬁcally, the ﬁrst generator aims to learn effec-tive point embeddings in a breadth-ﬁrst manner, while the second generator is used to reﬁne the generated point cloud based on a depth-ﬁrst point embedding to generate a robust and uniform point cloud. The proposed dual-generators framework thus is able to progressively learn effective point embeddings for accurate point cloud generation. Exper-imental results on a variety of object categories from the most popular point cloud generation dataset, ShapeNet, demonstrate the state-of-the-art performance of the pro-posed method for accurate point cloud generation. 