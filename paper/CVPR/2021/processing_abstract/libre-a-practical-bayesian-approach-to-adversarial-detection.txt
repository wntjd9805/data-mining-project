Despite their appealing ﬂexibility, deep neural networks (DNNs) are vulnerable against adversarial examples. Vari-ous adversarial defense strategies have been proposed to re-solve this problem, but they typically demonstrate restricted practicability owing to unsurmountable compromise on uni-versality, effectiveness, or efﬁciency. In this work, we pro-pose a more practical approach, Lightweight Bayesian Re-ﬁnement (LiBRe), in the spirit of leveraging Bayesian neu-ral networks (BNNs) for adversarial detection. Empow-ered by the task and attack agnostic modeling under Bayes principle, LiBRe can endow a variety of pre-trained task-dependent DNNs with the ability of defending heteroge-neous adversarial attacks at a low cost. We develop and integrate advanced learning techniques to make LiBRe ap-propriate for adversarial detection. Concretely, we build the few-layer deep ensemble variational and adopt the pre-training & ﬁne-tuning workﬂow to boost the effectiveness and efﬁciency of LiBRe. We further provide a novel in-sight to realise adversarial detection-oriented uncertainty quantiﬁcation without inefﬁciently crafting adversarial ex-amples during training. Extensive empirical studies cover-ing a wide range of scenarios verify the practicability of Li-BRe. We also conduct thorough ablation studies to evidence the superiority of our modeling and learning strategies.1 