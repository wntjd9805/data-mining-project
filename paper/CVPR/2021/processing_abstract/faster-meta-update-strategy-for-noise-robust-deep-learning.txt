It has been shown that deep neural networks are prone to overﬁtting on biased training data. Towards address-ing this issue, meta-learning employs a meta model for correcting the training bias. Despite the promising per-formances, super slow training is currently the bottleneck in the meta learning approaches. In this paper, we intro-duce a novel Faster Meta Update Strategy (FaMUS) to re-place the most expensive step in the meta gradient compu-tation with a faster layer-wise approximation. We empir-ically ﬁnd that FaMUS yields not only a reasonably accu-rate but also a low-variance approximation of the meta gra-dient. We conduct extensive experiments to verify the pro-posed method on two tasks. We show our method is able to save two-thirds of the training time while still maintain-ing the comparable or achieving even better generalization performance. In particular, our method achieves the state-of-the-art performance on both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recognition on standard benchmarks. Code are released at https://github.com/youjiangxu/FaMUS. 