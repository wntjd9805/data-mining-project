We present a generic image-to-image translation frame-work, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a se-ries of style vectors which are fed into a pretrained Style-GAN generator, forming the extended W+ latent space. Weﬁrst show that our encoder can directly embed real images into W+, with no additional optimization. Next, we pro-pose utilizing our encoder to directly solve image-to-image translation tasks, deﬁning them as encoding problems from some input domain into the latent domain. By deviating from the standard “invert ﬁrst, edit later” methodology used with previous StyleGAN encoders, our approach can han-dle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solv-ing translation tasks through StyleGAN signiﬁcantly sim-pliﬁes the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal syn-thesis via the resampling of styles. Finally, we demon-strate the potential of our framework on a variety of fa-cial image-to-image translation tasks, even when compared to state-of-the-art solutions designed speciﬁcally for a sin-gle task, and further show that it can be extended beyond the human facial domain. Code is available at https://github.com/eladrich/pixel2style2pixel. 