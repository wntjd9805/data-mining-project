We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural net-works are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via vol-ume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved.In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance ﬁelds. We show that without any additional human speciﬁed supervi-sion, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neu-*Work done while the author was an intern at FRL Research. ral representation. We achieve this by jointly optimizing the parameters of two neural radiance ﬁelds and a set of rigid poses which align the two ﬁelds at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where nov-elty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion. 