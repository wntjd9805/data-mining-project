Skeletal motion plays a vital role in human activity recognition as either an independent data source or a com-plement [33]. The robustness of skeleton-based activity recognizers has been questioned recently [29, 50], which shows that they are vulnerable to adversarial attacks when the full-knowledge of the recognizer is accessible to the at-tacker. However, this white-box requirement is overly re-strictive in most scenarios and the attack is not truly threat-ening.In this paper, we show that such threats do exist under black-box settings too. To this end, we propose theﬁrst black-box adversarial attack method BASAR. ThroughBASAR, we show that adversarial attack is not only truly a threat but also can be extremely deceitful, because on-manifold adversarial samples are rather common in skele-tal motions, in contrast to the common belief that adver-sarial samples only exist off-manifold [18]. Through ex-haustive evaluation and comparison, we show that BASAR can deliver successful attacks across models, data, and at-tack modes. Through harsh perceptual studies, we show that it achieves effective yet imperceptible attacks. By an-alyzing the attack on different activity recognizers, BASAR helps identify the potential causes of their vulnerability and provides insights on what classiﬁers are likely to be more robust against attack. 