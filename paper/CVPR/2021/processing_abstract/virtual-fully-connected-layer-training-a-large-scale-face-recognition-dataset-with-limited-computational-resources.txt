Recently, deep face recognition has achieved signiﬁ-cant progress because of Convolutional Neural Networks (CNNs) and large-scale datasets. However, training CNNs on a large-scale face recognition dataset with limited com-putational resources is still a challenge. This is because the classiﬁcation paradigm needs to train a fully-connected layer as the category classiﬁer, and its parameters will be in the hundreds of millions if the training dataset con-tains millions of identities. This requires many computa-tional resources, such as GPU memory. The metric learn-ing paradigm is an economical computation method, but its performance is greatly inferior to that of the classiﬁcation paradigm. To address this challenge, we propose a simple but effective CNN layer called the Virtual fully-connected (Virtual FC) layer to reduce the computational consump-tion of the classiﬁcation paradigm. Without bells and whis-tles, the proposed Virtual FC reduces the parameters by more than 100 times with respect to the fully-connected layer and achieves competitive performance on mainstream face recognition evaluation datasets. Moreover, the perfor-mance of our Virtual FC layer on the evaluation datasets is superior to that of the metric learning paradigm by a signiﬁcant margin. Our code will be released in hopes of disseminating our idea to other domains1. 