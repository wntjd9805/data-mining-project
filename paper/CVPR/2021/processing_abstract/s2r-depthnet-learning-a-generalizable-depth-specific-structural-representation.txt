Human can infer the 3D geometry of a scene from a sketch instead of a realistic image, which indicates that the spatial structure plays a fundamental role in understand-ing the depth of scenes. We are the ﬁrst to explore the learning of a depth-speciﬁc structural representation, which captures the essential feature for depth estimation and ig-nores irrelevant style information. Our S2R-DepthNet (Syn-thetic to Real DepthNet) can be well generalized to un-seen real-world data directly even though it is only trained on synthetic data. S2R-DepthNet consists of: a) a Struc-ture Extraction (STE) module which extracts a domain-invariant structural representation from an image by dis-entangling the image into domain-invariant structure and domain-speciﬁc style components, b) a Depth-speciﬁc At-tention (DSA) module, which learns task-speciﬁc knowledge to suppress depth-irrelevant structures for better depth esti-mation and generalization, and c) a depth prediction mod-ule (DP) to predict depth from the depth-speciﬁc representa-tion. Without access of any real-world images, our method even outperforms the state-of-the-art unsupervised domain adaptation methods which use real-world images of the tar-get domain for training. In addition, when using a small amount of labeled real-world data, we achieve the state-of-the-art performance under the semi-supervised setting. 