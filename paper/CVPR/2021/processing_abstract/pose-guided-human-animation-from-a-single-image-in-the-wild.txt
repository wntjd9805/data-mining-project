We present a new pose transfer method for synthesizing a human animation from a single image of a person con-trolled by a sequence of body poses. Existing pose transfer methods exhibit signiﬁcant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and fail-ures in preserving the identity and textures of the person. To address these limitations, we design a compositional neu-ral network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a uniﬁed representation of appearance and its labels in UV coordinates, which remains constant across poses. The uni-ﬁed representation provides an incomplete yet strong guid-ance to generating the appearance in response to the pose change. We use the trained network to complete the appear-ance and render it with the background. With these strate-gies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a tem-porally coherent way without any ﬁne-tuning of the network on the testing scene. Experiments show that our method out-performs the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability. 