In this paper, we present a new dataset with the target of advancing the scene parsing task from images to videos.Our dataset aims to perform Video Scene Parsing in theWild (VSPW), which covers a wide range of real-world sce-narios and categories. To be speciﬁc, our VSPW is fea-tured from the following aspects: 1) Well-trimmed long-temporal clips. Each video contains a complete shot, last-ing around 5 seconds on average. 2) Dense annotation.The pixel-level annotations are provided at a high frame rate of 15 f/s. 3) High resolution. Over 96% of the cap-tured videos are with high spatial resolutions from 720P to 4K. We totally annotate 3,536 videos, including 251,633 frames from 124 categories. To the best of our knowl-edge, our VSPW is the ﬁrst attempt to tackle the challenging video scene parsing task in the wild by considering diverse scenarios. Based on VSPW, we design a generic Tempo-ral Context Blending (TCB) network, which can effectively harness long-range contextual information from the past frames to help segment the current one. Extensive experi-ments show that our TCB network improves both the seg-mentation performance and temporal stability comparing with image-/video-based state-of-the-art methods. We hope that the scale, diversity, long-temporal, and high frame rate of our VSPW can signiﬁcantly advance the research of video scene parsing and beyond. The dataset is available at https://www.vspwdataset.com/. 