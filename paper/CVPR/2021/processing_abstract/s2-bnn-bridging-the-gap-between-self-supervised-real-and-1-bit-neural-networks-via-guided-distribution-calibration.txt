Real-valuedPrevious studies dominantly target at self-supervised learning on real-valued networks and have achieved many promising results. However, on the more challenging bi-nary neural networks (BNNs), this task has not yet been fully explored in the community.In this paper, we focus on this more difﬁcult scenario: learning networks where both weights and activations are binary, meanwhile, with-out any human annotated labels. We observe that the commonly used contrastive objective is not satisfying onBNNs for competitive accuracy, since the backbone net-work contains relatively limited capacity and representa-tion ability. Hence instead of directly applying existing self-supervised methods, which cause a severe decline in performance, we present a novel guided learning paradigm from real-valued to distill binary networks on the ﬁnal pre-diction distribution, to minimize the loss and obtain desir-able accuracy. Our proposed method can boost the sim-ple contrastive learning baseline by an absolute gain of 5.5∼15% on BNNs. We further reveal that it is difﬁcult for BNNs to recover the similar predictive distributions as real-valued models when training without labels. Thus, how to calibrate them is key to address the degradation in performance. Extensive experiments are conducted on the large-scale ImageNet and downstream datasets. Our method achieves substantial improvement over the simple contrastive learning baseline, and is even comparable to many mainstream supervised BNN methods. Code is avail-able at https://github.com/szq0214/S2-BNN . 