Temporal action detection on unconstrained videos has seen signiﬁcant research progress in recent years. Deep learning has achieved enormous success in this direction.However, collecting large-scale temporal detection datasets to ensuring promising performance in the real-world is a la-borious, impractical and time consuming process. Accord-ingly, we present a novel improved temporal action local-ization model that is better able to take advantage of limited labeled data available. Speciﬁcally, we design two auxil-iary tasks by reconstructing the available label information and then facilitate the learning of the temporal action de-tection model. Each task generates their supervision sig-nal by recycling the original annotations, and are jointly trained with the temporal action detection model in a multi-task learning fashion. Note that the proposed approach can be pluggable to any region proposal based temporal ac-tion detection models. We conduct extensive experiments on three benchmark datasets, namely THUMOS’14 [15], Cha-rades [35] and ActivityNet [14]. Our experimental results conﬁrm the effectiveness of the proposed model. 