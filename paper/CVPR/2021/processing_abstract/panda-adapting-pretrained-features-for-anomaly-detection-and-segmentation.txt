Anomaly detection methods require high-quality fea-tures.In recent years, the anomaly detection community has attempted to obtain better features using advances in deep self-supervised feature learning. Surprisingly, a very promising direction, using pre-trained deep features, has been mostly overlooked. In this paper, we ﬁrst empirically establish the perhaps expected, but unreported result, that combining pre-trained features with simple anomaly detec-tion and segmentation methods convincingly outperforms, much more complex, state-of-the-art methods.In order to obtain further performance gains in anomaly detection, we adapt pre-trained features to the target distri-bution. Although transfer learning methods are well estab-lished in multi-class classiﬁcation problems, the one-class classiﬁcation (OCC) setting is not as well explored. It turns out that naive adaptation methods, which typically work well in supervised learning, often result in catastrophic col-lapse (feature deterioration) and reduce performance inOCC settings. A popular OCC method, DeepSVDD, ad-vocates using specialized architectures, but this limits the adaptation performance gain. We propose two methods for combating collapse: i) a variant of early stopping that dynamically learns the stopping iteration ii) elastic reg-ularization inspired by continual learning. Our method,PANDA, outperforms the state-of-the-art in the OCC, out-lier exposure and anomaly segmentation settings by large margins1. 