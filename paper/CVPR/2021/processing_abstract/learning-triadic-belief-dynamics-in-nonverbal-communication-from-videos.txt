Humans possess a unique social cognition capabil-ity [43, 20]; nonverbal communication can convey rich so-cial information among agents. In contrast, such crucial so-cial characteristics are mostly missing in the existing scene understanding literature. In this paper, we incorporate dif-ferent nonverbal communication cues (e.g., gaze, human poses, and gestures) to represent, model, learn, and infer agents’ mental states from pure visual inputs. Crucially, such a mental representation takes the agent’s belief into account so that it represents what the true world state is and infers the beliefs in each agent’s mental state, which may differ from the true world states. By aggregating different beliefs and true world states, our model essentially forms“ﬁve minds” during the interactions between two agents.This “ﬁve minds” model differs from prior works that in-fer beliefs in an inﬁnite recursion; instead, agents’ beliefs are converged into a “common mind” [31, 47]. Based on this representation, we further devise a hierarchical energy-based model that jointly tracks and predicts all ﬁve minds.From this new perspective, a social event is interpreted by a series of nonverbal communication and belief dynam-ics, which transcends the classic keyframe video summary.In the experiments, we demonstrate that using such a so-cial account provides a better video summary on videos with rich social interactions compared with state-of-the-art keyframe video summary methods. 