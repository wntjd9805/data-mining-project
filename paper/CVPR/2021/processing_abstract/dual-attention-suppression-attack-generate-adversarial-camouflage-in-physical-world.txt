Deep learning models are vulnerable to adversarial ex-amples. As a more threatening type for practical deep learning systems, physical adversarial examples have re-ceived extensive research attention in recent years. How-ever, without exploiting the intrinsic characteristics such as model-agnostic and human-speciﬁc patterns, existing works generate weak adversarial perturbations in the physical world, which fall short of attacking across different models and show visually suspicious appearance. Motivated by the viewpoint that attention reﬂects the intrinsic characteristics of the recognition process, this paper proposes the DualAttention Suppression (DAS) attack to generate visually-natural physical adversarial camouﬂages with strong trans-ferability by suppressing both model and human attention.As for attacking, we generate transferable adversarial cam-ouﬂages by distracting the model-shared similar attention patterns from the target to non-target regions. Meanwhile, based on the fact that human visual attention always focuses on salient items (e.g., suspicious distortions), we evade the human-speciﬁc bottom-up attention to generate visually-natural camouﬂages which are correlated to the scenario context. We conduct extensive experiments in both the digi-tal and physical world for classiﬁcation and detection tasks on up-to-date models (e.g., Yolo-V5) and demonstrate that our method outperforms state-of-the-art methods.1 