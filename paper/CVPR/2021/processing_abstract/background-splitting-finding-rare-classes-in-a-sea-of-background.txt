We focus on the problem of training deep image clas-siﬁcation models for a small number of extremely rare categories.In this common, real-world scenario, al-most all images belong to the background category in the dataset. We ﬁnd that state-of-the-art approaches for training on imbalanced datasets do not produce ac-curate deep models in this regime. Our solution is to split the large, visually diverse background into many smaller, visually similar categories during training. We implement this idea by extending an image classiﬁcation model with an additional auxiliary loss that learns to mimic the predictions of a pre-existing classiﬁcation model on the training set. The auxiliary loss requires no additional human labels and regularizes feature learning in the shared network trunk by forcing the model to dis-criminate between auxiliary categories for all training set examples, including those belonging to the monolithic background of the main rare category classiﬁcation task.To evaluate our method we contribute modiﬁed versions of the iNaturalist and Places365 datasets where only a small subset of rare category labels are available during training (all other images are labeled as background).By jointly learning to recognize both the selected rare categories and auxiliary categories, our approach yields models that perform 8.3 mAP points higher than state-of-the-art imbalanced learning baselines when 98.30% of the data is background, and up to 42.3 mAP points higher than ﬁne-tuning baselines when 99.98% of the data is background. 