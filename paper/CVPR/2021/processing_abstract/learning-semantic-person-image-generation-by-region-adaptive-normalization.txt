Human pose transfer has received great attention due to its wide applications, yet is still a challenging task that is not well solved. Recent works have achieved great suc-cess to transfer the person image from the source to the target pose. However, most of them cannot well capture the semantic appearance, resulting in inconsistent and less realistic textures on the reconstructed results. To address this issue, we propose a new two-stage framework to han-dle the pose and appearance translation. In the ﬁrst stage, we predict the target semantic parsing maps to eliminate the difﬁculties of pose transfer and further beneﬁt the lat-ter translation of per-region appearance style. In the sec-ond one, with the predicted target semantic maps, we sug-gest a new person image generation method by incorpo-rating the region-adaptive normalization, in which it takes the per-region styles to guide the target appearance gen-eration. Extensive experiments show that our proposedSPGNet can generate more semantic, consistent, and photo-realistic results and perform favorably against the state of the art methods in terms of quantitative and qualitative evaluation. The source code and model are available at https://github.com/cszy98/SPGNet.git. 