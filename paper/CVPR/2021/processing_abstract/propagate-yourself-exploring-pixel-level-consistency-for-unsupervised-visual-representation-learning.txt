Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of con-trastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The ﬁrst task directly ap-plies contrastive learning at the pixel level. We addition-ally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Speciﬁcally, it achieves 60.2AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred toPascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using aResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pre-training not only regular backbone networks but also head networks used for dense downstream tasks, and are com-plementary to instance-level contrastive methods. These results demonstrate the strong potential of deﬁning pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning. Code is avail-able at https://github.com/zdaxie/PixPro. 