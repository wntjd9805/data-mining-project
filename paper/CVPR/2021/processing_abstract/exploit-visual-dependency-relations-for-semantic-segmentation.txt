Dependency relations among visual entities are ubiq-uity because both objects and scenes are highly structured.They provide prior knowledge about the real world that can help improve the generalization ability of deep learning ap-proaches. Different from contextual reasoning which fo-cuses on feature aggregation in the spatial domain, visual dependency reasoning explicitly models the dependency re-lations among visual entities. In this paper, we introduce a novel network architecture, termed the dependency network or DependencyNet, for semantic segmentation.It uniﬁes dependency reasoning at three semantic levels. Intra-class reasoning decouples the representations of different object categories and updates them separately based on the inter-nal object structures. Inter-class reasoning then performs spatial and semantic reasoning based on the dependency re-lations among different object categories. We will have an in-depth investigation on how to discover the dependency graph from the training annotations. Global dependency reasoning further reﬁnes the representations of each object category based on the global scene information. Extensive ablative studies with a controlled model size and the same network depth show that each individual dependency rea-soning component beneﬁts semantic segmentation and they together signiﬁcantly improve the base network. Experi-mental results on two benchmark datasets show the Depen-dencyNet achieves comparable performance to the recent states of the art. 