We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT , which introduces a novel kaleido strategy for fashion cross-modality representations from transformers.In contrast to random masking strat-egy of recent VL models, we design alignment guided mask-ing to jointly focus more on image-text semantic relations.To this end, we carry out ﬁve novel tasks, i.e., rotation, jigsaw, camouﬂage, grey-to-color, and blank-to-color for self-supervised VL pre-training at patches of different scale.Kaleido-BERT is conceptually simple and easy to extend to the existing BERT framework, it attains state-of-the-art re-sults by large margins on four downstream tasks, includ-ing text retrieval (R@1: 4.03% absolute improvement), im-age retrieval (R@1: 7.13% abs imv.), category recognition† Equal; * Corresponding author: Deng-Ping Fan (dengpfan@gmail.com). (ACC: 3.28% abs imv.), and fashion captioning (Bleu4: 1.2 abs imv.). We validate the efﬁciency of Kaleido-BERT on a wide range of e-commerical websites, demonstrating its broader potential in real-world applications. 