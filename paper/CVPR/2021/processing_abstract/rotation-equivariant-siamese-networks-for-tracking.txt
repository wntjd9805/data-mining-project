Rotation is among the long prevailing, yet still unre-solved, hard challenges encountered in visual object track-ing. The existing deep learning-based tracking algorithms use regular CNNs that are inherently translation equivari-ant, but not designed to tackle rotations.In this paper, we ﬁrst demonstrate that in the presence of rotation in-stances in videos, the performance of existing trackers is severely affected. To circumvent the adverse effect of ro-tations, we present rotation-equivariant Siamese networks (RE-SiamNets), built through the use of group-equivariant convolutional layers comprising steerable ﬁlters. SiamNets allow estimating the change in orientation of the object in an unsupervised manner, thereby facilitating its use in rel-ative 2D pose estimation as well. We further show that this change in orientation can be used to impose an addi-tional motion constraint in Siamese tracking through im-posing restriction on the change in orientation between two consecutive frames. For benchmarking, we present Rota-tion Tracking Benchmark (RTB), a dataset comprising a set of videos with rotation instances. Through experiments on two popular Siamese architectures, we show that RE-SiamNets handle the problem of rotation very well and out-perform their regular counterparts. Further, RE-SiamNets can accurately estimate the relative change in pose of the target in an unsupervised fashion, namely the in-plane ro-tation the target has sustained with respect to the refer-ence frame. Code and data can be accessed at https://github.com/dkgupta90/re-siamnet. 