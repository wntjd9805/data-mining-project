A key challenge in self-supervised video representa-tion learning is how to effectively capture motion informa-tion besides context bias. While most existing works im-plicitly achieve this with video-speciﬁc pretext tasks (e.g., predicting clip orders, time arrows, and paces), we de-velop a method that explicitly decouples motion supervision from context bias through a carefully designed pretext task.Speciﬁcally, we take the key frames and motion vectors in compressed videos (e.g., in H.264 format) as the supervi-sion sources for context and motion, respectively, which can be efﬁciently extracted at over 500 fps on CPU. Then we de-sign two pretext tasks that are jointly optimized: a context matching task where a pairwise contrastive loss is cast be-tween video clip and key frame features; and a motion pre-diction task where clip features, passed through an encoder-decoder network, are used to estimate motion features in a near future. These two tasks use a shared video backbone and separate MLP heads. Experiments show that our ap-proach improves the quality of the learned video represen-tation over previous works, where we obtain absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Moreover, we ﬁnd the motion prediction to be a strong regularization for video networks, where using it as an auxiliary task improves the accuracy of action recognition with a margin of 7.4% ∼ 13.8%. 