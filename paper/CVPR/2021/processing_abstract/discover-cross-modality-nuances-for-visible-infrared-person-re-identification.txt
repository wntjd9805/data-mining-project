Visible-infrared person re-identiﬁcation (Re-ID) aims to match the pedestrian images of the same identity from dif-ferent modalities. Existing works mainly focus on alleviat-ing the modality discrepancy by aligning the distributions of features from different modalities. However, nuanced but discriminative information, such as glasses, shoes, and the length of clothes, has not been fully explored, especially in the infrared modality. Without discovering nuances, it is challenging to match pedestrians across modalities using modality alignment solely, which inevitably reduces feature distinctiveness.In this paper, we propose a joint Modal-ity and Pattern Alignment Network (MPANet) to discover cross-modality nuances in different patterns for visible-infrared person Re-ID, which introduces a modality alle-viation module and a pattern alignment module to jointly extract discriminative features. Speciﬁcally, we ﬁrst pro-pose a modality alleviation module to dislodge the modality information from the extracted feature maps. Then, We de-vise a pattern alignment module, which generates multiple pattern maps for the diverse patterns of a person, to dis-cover nuances. Finally, we introduce a mutual mean learn-ing fashion to alleviate the modality discrepancy and pro-pose a center cluster loss to guide both identity learning and nuances discovering. Extensive experiments on the publicSYSU-MM01 and RegDB datasets demonstrate the superi-ority of MPANet over state-of-the-arts. 