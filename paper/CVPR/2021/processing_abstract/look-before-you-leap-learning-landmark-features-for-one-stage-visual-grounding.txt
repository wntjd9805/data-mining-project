An LBYL (‘Look Before You Leap’) Network is proposed for end-to-end trainable one-stage visual grounding. The idea behind LBYL-Net is intuitive and straightforward: we follow a language’s description to localize the target ob-ject based on its relative spatial relation to ‘Landmarks’, which is characterized by some spatial positional words and some descriptive words about the object. The core of our LBYL-Net is a landmark feature convolution mod-ule that transmits the visual features with the guidance of linguistic description along with different directions. Con-sequently, such a module encodes the relative spatial po-sitional relations between the current object and its con-text. Then we combine the contextual information from the landmark feature convolution module with the target’s vi-sual features for grounding. To make this landmark feature convolution light-weight, we introduce a dynamic program-ming algorithm (termed dynamic max pooling) with low complexity to extract the landmark feature. Thanks to the landmark feature convolution module, we mimic the human behavior of ‘Look Before You Leap’ to design an LBYL-Net, which takes full consideration of contextual information.Extensive experiments show our method’s effectiveness in four grounding datasets. Speciﬁcally, our LBYL-Net out-performs all state-of-the-art two-stage and one-stage meth-ods on ReferitGame. On RefCOCO and RefCOCO+, OurLBYL-Net also achieves comparable results or even better results than existing one-stage methods. Code is available at https://github.com/svip-lab/LBYLNet. 