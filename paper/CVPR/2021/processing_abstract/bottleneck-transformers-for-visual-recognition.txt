We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classiﬁca-tion, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the ﬁnal three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines signiﬁ-cantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in la-tency. Through the design of BoTNet, we also point out howResNet bottleneck blocks with self-attention can be viewed asTransformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCOInstance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classiﬁcation, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in “compute”1 time than the popu-lar EfﬁcientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2 