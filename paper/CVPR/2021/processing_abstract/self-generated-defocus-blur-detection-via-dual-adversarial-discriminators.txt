Although existing fully-supervised defocus blur detection (DBD) models signiﬁcantly improve performance, training such deep models requires abundant pixel-level manual an-notation, which is highly time-consuming and error-prone.Addressing this issue, this paper makes an effort to train a deep DBD model without using any pixel-level annota-tion. The core insight is that a defocus blur region/focused clear area can be arbitrarily pasted to a given realistic full blurred image/full clear image without affecting the judg-ment of the full blurred image/full clear image. Speciﬁ-cally, we train a generator G in an adversarial manner against dual discriminators Dc and Db. G learns to pro-duce a DBD mask that generates a composite clear image and a composite blurred image through copying the focused area and unfocused region from corresponding source im-age to another full clear image and full blurred image.Then, Dc and Db can not distinguish them from realis-tic full clear image and full blurred image simultaneous-ly, achieving a self-generated DBD by an implicit manner to deﬁne what a defocus blur area is. Besides, we pro-pose a bilateral triplet-excavating constraint to avoid the degenerate problem caused by the case one discriminator defeats the other one. Comprehensive experiments on t-wo widely-used DBD datasets demonstrate the superiority of the proposed approach. Source codes are available at: https://github.com/shangcai1/SG. 