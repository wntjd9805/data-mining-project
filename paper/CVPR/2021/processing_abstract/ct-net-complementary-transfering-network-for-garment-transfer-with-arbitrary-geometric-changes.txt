Garment transfer shows great potential in realistic ap-plications with the goal of transfering outﬁts across dif-ferent people images. However, garment transfer between images with heavy misalignments or severe occlusions still remains as a challenge. In this work, we propose Comple-mentary Transfering Network (CT-Net) to adaptively model different levels of geometric changes and transfer outﬁts between different people.In speciﬁc, CT-Net consists of three modules: i) A complementary warping module ﬁrst estimates two complementary warpings to transfer the de-sired clothes in different granularities. ii) A layout predic-tion module is proposed to predict the target layout, which guides the preservation or generation of the body parts in the synthesized images. iii) A dynamic fusion module adap-tively combines the advantages of the complementary warp-ings to render the garment transfer results. Extensive ex-periments conducted on DeepFashion dataset demonstrate that our network synthesizes high-quality garment transfer images and signiﬁcantly outperforms the state-of-art meth-ods both qualitatively and quantitatively. Our source code will be available online. 