(cid:40)(cid:74)(cid:82)(cid:70)(cid:72)(cid:81)(cid:87)(cid:85)(cid:76)(cid:70)(cid:3)(cid:79)(cid:76)(cid:78)(cid:72)(cid:79)(cid:76)(cid:75)(cid:82)(cid:82)(cid:71)(cid:3) (cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:85)(cid:76)(cid:69)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81) (cid:43)(cid:68)(cid:81)(cid:71)(cid:16)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:85)(cid:72)(cid:74)(cid:76)(cid:82)(cid:81)(cid:86)We introduce an approach for pre-training egocentric video models using large-scale third-person video datasets.Learning from purely egocentric data is limited by low dataset scale and diversity, while using purely exocentric (third-person) data introduces a large domain mismatch.Our idea is to discover latent signals in third-person video that are predictive of key egocentric-speciﬁc properties. In-corporating these signals as knowledge distillation losses during pre-training results in models that beneﬁt from both the scale and diversity of third-person video data, as well as representations that capture salient egocentric proper-ties. Our experiments show that our “Ego-Exo” framework can be seamlessly integrated into standard video models; it outperforms all baselines when ﬁne-tuned for egocen-tric activity recognition, achieving state-of-the-art results on Charades-Ego and EPIC-Kitchens-100. 