Both WrongPositive FlipsReducing inconsistencies in the behavior of different ver-sions of an AI system can be as important in practice as re-ducing its overall error. In image classiﬁcation, sample-wise inconsistencies appear as “negative ﬂips”: A new model incorrectly predicts the output for a test sample that was correctly classiﬁed by the old (reference) model. Positive-congruent (PC) training aims at reducing error rate while at the same time reducing negative ﬂips, thus maximizing congruency with the reference model only on positive pre-dictions, unlike model distillation. We propose a simple approach for PC training, Focal Distillation, which enforces congruence with the reference model by giving more weights to samples that were correctly classiﬁed. We also found that, if the reference model itself can be chosen as an ensemble of multiple deep neural networks, negative ﬂips can be further reduced without affecting the new model’s accuracy. 