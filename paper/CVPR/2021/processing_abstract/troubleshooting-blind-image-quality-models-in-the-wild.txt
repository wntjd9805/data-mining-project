Recently, the group maximum differentiation competition (gMAD) has been used to improve blind image quality assess-ment (BIQA) models, with the help of full-reference metrics.When applying this type of approach to troubleshoot “best-performing” BIQA models in the wild, we are faced with a practical challenge: it is highly nontrivial to obtain stronger competing models for efﬁcient failure-spotting. Inspired by recent ﬁndings that difﬁcult samples of deep models may be exposed through network pruning, we construct a set of“self-competitors,” as random ensembles of pruned versions of the target model to be improved. Diverse failures can then be efﬁciently identiﬁed via self-gMAD competition. Next, we ﬁne-tune both the target and its pruned variants on the human-rated gMAD set. This allows all models to learn from their respective failures, preparing themselves for the next round of self-gMAD competition. Experimental results demonstrate that our method efﬁciently troubleshoots BIQA models in the wild with improved generalizability. 