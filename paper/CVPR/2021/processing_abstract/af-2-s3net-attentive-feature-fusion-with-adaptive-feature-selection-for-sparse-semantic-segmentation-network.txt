dicted from input images, giving a clear perspective of the scene.Autonomous robotic systems and self driving cars rely on accurate perception of their surroundings as the safety of the passengers and pedestrians is the top priority. Se-mantic segmentation is one of the essential components of road scene perception that provides semantic information of the surrounding environment. Recently, several methods have been introduced for 3D LiDAR semantic segmentation.While they can lead to improved performance, they are ei-ther afﬂicted by high computational complexity, therefore are inefﬁcient, or they lack ﬁne details of smaller object in-stances. To alleviate these problems, we propose (AF)2-S3Net, an end-to-end encoder-decoder CNN network for 3DLiDAR semantic segmentation. We present a novel multi-branch attentive feature fusion module in the encoder and a unique adaptive feature selection module with feature map re-weighting in the decoder. Our (AF)2-S3Net fuses the voxel-based learning and point-based learning meth-ods into a uniﬁed framework to effectively process the po-tentially large 3D scene. Our experimental results show that the proposed method outperforms the state-of-the-art approaches on the large-scale nuScenes-lidarseg and Se-manticKITTI benchmark, ranking 1st on both competitive public leaderboard competitions upon publication. 