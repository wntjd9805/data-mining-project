Unsupervised Domain Adaptation (UDA) aims to gen-eralize the knowledge learned from a well-labeled source domain to an unlabled target domain. Recently, adver-sarial domain adaptation with two distinct classiﬁers (bi-classiﬁer) has been introduced into UDA which is effec-tive to align distributions between different domains. Pre-vious bi-classiﬁer adversarial learning methods only focus on the similarity between the outputs of two distinct classi-ﬁers. However, the similarity of the outputs cannot guaran-tee the accuracy of target samples, i.e., traget samples may match to wrong categories even if the discrepancy between two classiﬁers is small. To challenge this issue, in this pa-per, we propose a cross-domain gradient discrepancy min-imization (CGDM) method which explicitly minimizes the discrepancy of gradients generated by source samples and target samples. Speciﬁcally, the gradient gives a cue for the semantic information of target samples so it can be used as a good supervision to improve the accuracy of targetIn order to compute the gradient signal of tar-samples. get smaples, we further obtain target pseudo labels through a clustering-based self-supervised learning. Extensive ex-periments on three widely used UDA datasets show that our method surpasses many previous state-of-the-arts. 