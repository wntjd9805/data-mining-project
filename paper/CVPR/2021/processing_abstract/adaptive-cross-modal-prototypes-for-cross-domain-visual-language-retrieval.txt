Visual Content (Composition of objects)Cross-Modal EmbeddingSpace for retrievalText Description (Composition of words)In this paper, we study the task of visual-text retrieval in the highly practical setting in which labelled visual data with paired text descriptions are available in one domain (the “source”), but only unlabelled visual data (without text descriptions) are available in the domain of interest (the “target”). We propose the ADAPTIVE CROSS-MODALPROTOTYPES framework which seeks to enable target do-main retrieval by learning cross-modal visual-text represen-tations while minimising both uni-modal and cross-modal distribution shift across the source and target domains.Our approach is built upon two key ideas: ﬁrst, we en-code the inductive bias that the learned cross-modal rep-resentations should be compositional with respect to con-cepts in each modality—this is achieved through clustering pretrained uni-modal features across each domain and de-signing a careful regularisation scheme to preserve the re-sulting structure. Second, we employ mutual information maximisation between cross-modal representations in the source and target domains during learning—this provides a mechanism that preserves commonalities between the do-mains while discarding signal in each that cannot be in-ferred from the other. We showcase our approach for the task of cross-domain visual-text retrieval, outperforming ex-isting approaches for both images and videos. 