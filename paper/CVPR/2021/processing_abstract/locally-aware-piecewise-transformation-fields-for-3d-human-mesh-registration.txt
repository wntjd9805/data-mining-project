Registering point clouds of dressed humans to paramet-ric human models is a challenging task in computer vi-sion. Traditional approaches often rely on heavily engi-neered pipelines that require accurate manual initializa-tion of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major ob-stacle is that, despite recent effort on rotation representa-tion learning in neural networks, regressing joint rotations from point clouds or images of humans is still very chal-lenging. To this end, we propose novel piecewise trans-formation ﬁelds (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We com-bine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultane-ously predict shape and per-point correspondences between the posed space and the canonical space for clothed hu-man. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone trans-formations and joint rotations can be obtained efﬁciently via a least-square ﬁtting given the estimated point corre-spondences, circumventing the challenging task of directly 7639regressing joint rotations from neural networks. Further-more, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capabil-ity and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that ﬁt-ting parametric models with poses initialized by our net-work results in much better registration quality, especially for extreme poses. 