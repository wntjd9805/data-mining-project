While deep learning (DL)-based video deraining meth-ods have achieved signiﬁcant successes in recent years, they still have two major drawbacks. Firstly, most of them are insufﬁcient to model the characteristics of rain layers con-tained in rainy videos. In fact, the rain layers exhibit strong visual properties (e.g., direction, scale, and thickness) in spatial dimension and causal properties (e.g., velocity and acceleration) in temporal dimension, and thus can be mod-eled by the spatial-temporal process in statistics. Secondly, current DL-based methods rely heavily on the labeled train-ing data, whose rain layers are synthetic, thus leading to a deviation from real data. Such a gap between synthetic and real data sets results in poor performance when applying them to real scenarios. To address these issues, this paper proposes a new semi-supervised video deraining method, in which a dynamical rain generator is employed to ﬁt the rain layer for the sake of better depicting its intrinsic character-istics. Speciﬁcally, the dynamical generator consists of one emission model and one transition model to simultaneously encode the spatial appearance and temporal dynamics of rain streaks, respectively, both of which are parameterized by deep neural networks (DNNs). Furthermore, different prior formats are designed for the labeled synthetic and unlabeled real data so as to fully exploit their underlying common knowledge. Last but not least, we design a MonteCarlo-based EM algorithm to learn the model. Extensive experiments are conducted to verify the superiority of the proposed semi-supervised deraining model. 