Out-of-distribution (OOD) detection is essential to pre-vent anomalous inputs from causing a model to fail during deployment. While improved OOD detection methods have emerged, they often rely on the ﬁnal layer outputs and re-quire a full feedforward pass for any given input. In this paper, we propose a novel framework, multi-level out-of-distribution detection (MOOD), which exploits intermedi-ate classiﬁer outputs for dynamic and efﬁcient OOD infer-ence. We explore and establish a direct relationship be-tween the OOD data complexity and optimal exit level, and show that easy OOD examples can be effectively detected early without propagating to deeper layers. At each exit, theOOD examples can be distinguished through our proposed adjusted energy score, which is both empirically and theo-retically suitable for networks with multiple classiﬁers. We extensively evaluate MOOD across 10 OOD datasets span-ning a wide range of complexities. Experiments demon-strate that MOOD achieves up to 71.05% computational reduction in inference, while maintaining competitive OOD detection performance. 