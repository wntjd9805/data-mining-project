We introduce RoboPose, a method to estimate the joint angles and the 6D camera-to-robot pose of a known articu-lated robot from a single RGB image. This is an important problem to grant mobile and itinerant autonomous systems the ability to interact with other robots using only visual information in non-instrumented environments, especially in the context of collaborative robotics. It is also challeng-ing because robots have many degrees of freedom and an inﬁnite space of possible conﬁgurations that often result in self-occlusions and depth ambiguities when imaged by a single camera. The contributions of this work are three-fold.First, we introduce a new render & compare approach for es-timating the 6D pose and joint angles of an articulated robot that can be trained from synthetic data, generalizes to new unseen robot conﬁgurations at test time, and can be applied to a variety of robots. Second, we experimentally demon-strate the importance of the robot parametrization for the iterative pose updates and design a parametrization strategy that is independent of the robot structure. Finally, we show experimental results on existing benchmark datasets for four different robots and demonstrate that our method signiﬁ-cantly outperforms the state of the art. Code and pre-trained models are available on the project webpage [1]. 