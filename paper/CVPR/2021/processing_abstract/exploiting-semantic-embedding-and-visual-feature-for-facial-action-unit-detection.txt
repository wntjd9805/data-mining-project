Recent study on detecting facial action units (AU) has utilized auxiliary information (i.e., facial landmarks, rela-tionship among AUs and expressions, web facial images, etc.), in order to improve the AU detection performance. As of now, no semantic information of AUs has yet been ex-plored for such a task. As a matter of fact, AU semantic de-scriptions provide much more information than the binaryAU labels alone, thus we propose to exploit the SemanticEmbedding and Visual feature (SEV-Net) for AU detec-tion. More speciﬁcally, AU semantic embeddings are ob-tained through both Intra-AU and Inter-AU attention mod-ules, where the Intra-AU attention module captures the rela-tion among words within each sentence that describes indi-vidual AU, and the Inter-AU attention module focuses on the relation among those sentences. The learned AU semantic embeddings are then used as guidance for the generation of attention maps through a cross-modality attention network.The generated cross-modality attention maps are further used as weights for the aggregated feature. Our proposed method is unique in that the semantic features are exploited as the ﬁrst of this kind. The approach has been evaluated on three public AU-coded facial expression databases, and has achieved a superior performance than the state-of-the-art peer methods. 