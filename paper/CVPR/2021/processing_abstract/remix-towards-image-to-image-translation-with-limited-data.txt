Image-to-image (I2I) translation methods based on gen-erative adversarial networks (GANs) typically suffer from overﬁtting when limited training data is available. In this work, we propose a data augmentation method (ReMix) to tackle this issue. We interpolate training samples at the fea-ture level and propose a novel content loss based on the perceptual relations among samples. The generator learns to translate the in-between samples rather than memoriz-ing the training set, and thereby forces the discriminator to generalize. The proposed approach effectively reduces the ambiguity of generation and renders content-preserving results. The ReMix method can be easily incorporated into existing GAN models with minor modiﬁcations. Experimen-tal results on numerous tasks demonstrate that GAN mod-els equipped with the ReMix method achieve signiﬁcant im-provements. 