Lip reading aims to predict the spoken sentences from silent lip videos. Due to the fact that such a vision task usually performs worse than its counterpart speech recog-nition, one potential scheme is to distill knowledge from a teacher pretrained by audio signals. However, the latent domain gap between the cross-modal data could lead to a learning ambiguity and thus limits the performance of lip reading. In this paper, we propose a novel collabora-tive framework for lip reading, and two aspects of issues are considered: 1) the teacher should understand bi-modal knowledge to possibly bridge the inherent cross-modal gap; 2) the teacher should adjust teaching contents adaptively with the evolution of the student. To these ends, we in-troduce a trainable “master” network which ingests both audio signals and silent lip videos instead of a pretrained teacher. The master produces logits from three modalities of features: audio modality, video modality, and their com-bination. To further provide an interactive strategy to fuse these knowledge organically, we regularize the master with the task-speciﬁc feedback from the student, in which the requirement of the student is implicitly embedded. Mean-while, we involve a couple of “tutor” networks into our system as guidance for emphasizing the fruitful knowledgeﬂexibly. In addition, we incorporate a curriculum learning design to ensure a better convergence. Extensive experi-ments demonstrate that the proposed network outperforms the state-of-the-art methods on several benchmarks, includ-ing in both word-level and sentence-level scenarios. 