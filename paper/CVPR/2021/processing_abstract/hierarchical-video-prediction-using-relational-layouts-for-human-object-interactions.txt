Learning to model and predict how humans interact with objects while performing an action is challenging, and most of the existing video prediction models are in-effective in modeling complicated human-object interac-tions. Our work builds on hierarchical video prediction models, which disentangle the video generation process into two stages: predicting a high-level representation, such as pose sequence, and then learning a pose-to-pixels transla-tion model for pixel generation. An action sequence for a human-object interaction task is typically very complicated, involving the evolution of pose, person’s appearance, object locations, and object appearances over time. To this end, we propose a Hierarchical Video Prediction model usingRelational Layouts. In the ﬁrst stage, we learn to predict a sequence of layouts. A layout is a high-level representation of the video containing both pose and objects’ information for every frame. The layout sequence is learned by mod-eling the relationships between the pose and objects using relational reasoning and recurrent neural networks. The layout sequence acts as a strong structure prior to the sec-ond stage that learns to map the layouts into pixel space.Experimental evaluation of our method on two datasets,UMD-HOI and Bimanual, shows signiﬁcant improvements in standard video evaluation metrics such as LPIPS, PSNR, and SSIM. We also perform a detailed qualitative analysis of our model to demonstrate various generalizations. 