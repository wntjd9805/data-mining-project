In this paper, we propose to make a systematic study on machines’ multisensory perception under attacks. We use the audio-visual event recognition task against multi-modal adversarial attacks as a proxy to investigate the ro-bustness of audio-visual learning. We attack audio, visual, and both modalities to explore whether audio-visual inte-gration still strengthens perception and how different fusion mechanisms affect the robustness of audio-visual models.For interpreting the multimodal interactions under attacks, we learn a weakly-supervised sound source visual localiza-tion model to localize sounding regions in videos. To mit-igate multimodal attacks, we propose an audio-visual de-fense approach based on an audio-visual dissimilarity con-straint and external feature memory banks. Extensive ex-periments demonstrate that audio-visual models are sus-ceptible to multimodal adversarial attacks; audio-visual integration could decrease the model robustness rather than strengthen under multimodal attacks; even a weakly-supervised sound source visual localization model can be successfully fooled; our defense method can improve the in-vulnerability of audio-visual networks without signiﬁcantly sacriﬁcing clean model performance. The source code and pre-trained models are released in https://github. com/YapengTian/AV-Robustness-CVPR21. 