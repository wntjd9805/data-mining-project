Video stabilization is an essential component of visual quality enhancement. Early methods rely on feature track-ing to recover either 2D or 3D frame motion, which suffer from the robustness of local feature extraction and track-ing in shaky videos. Recently, learning-based methods seek to ﬁnd frame transformations with high-level information via deep neural networks to overcome the robustness issue of feature tracking. Nevertheless, to our best knowledge, no learning-based methods leverage 3D cues for the trans-formation inference yet; hence they would lead to artifacts on complex scene-depth scenarios. In this paper, we pro-pose Deep3D Stabilizer, a novel 3D depth-based learning method for video stabilization. We take advantage of the recent self-supervised framework on jointly learning depth and camera ego-motion estimation on raw videos. Our ap-proach requires no data for pre-training but stabilizes the input video via 3D reconstruction directly. The rectiﬁca-tion stage incorporates the 3D scene depth and camera mo-tion to smooth the camera trajectory and synthesize the sta-bilized video. Unlike most one-size-ﬁts-all learning-based methods, our smoothing algorithm allows users to manipu-late the stability of a video efﬁciently. Experimental results on challenging benchmarks show that the proposed solution consistently outperforms the state-of-the-art methods on al-most all motion categories. 