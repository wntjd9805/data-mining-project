Despite substantial progress in applying neural networks (NN) to a wide variety of areas, they still largely suffer from a lack of transparency and interpretability. While re-cent developments in explainable artiﬁcial intelligence at-tempt to bridge this gap (e.g., by visualizing the correlation between input pixels and ﬁnal outputs), these approaches are limited to explaining low-level relationships, and cru-cially, do not provide insights on error correction. In this work, we propose a framework (VRX) to interpret classiﬁ-cation NNs with intuitive structural visual concepts. Given a trained classiﬁcation model, the proposed VRX extracts relevant class-speciﬁc visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the rea-soning process of NNs and provide logical, concept-level explanations for ﬁnal model decisions. With extensive ex-periments, we empirically show VRX can meaningfully an-swer “why” and “why not” questions about the prediction, providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN’s performance. 