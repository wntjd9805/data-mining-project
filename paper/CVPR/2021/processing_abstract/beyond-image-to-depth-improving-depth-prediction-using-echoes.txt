We address the problem of estimating depth with multi modal audio visual data. Inspired by the ability of animals, such as bats and dolphins, to infer distance of objects with echolocation, some recent methods have utilized echoes for depth estimation. We propose an end-to-end deep learn-ing based pipeline utilizing RGB images, binaural echoes and estimated material properties of various objects within a scene. We argue that the relation between image, echoes and depth, for different scene elements, is greatly inﬂuenced by the properties of those elements, and a method designed to leverage this information can lead to signiﬁcantly im-proved depth estimation from audio visual inputs. We pro-pose a novel multi modal fusion technique, which incor-porates the material properties explicitly while combining audio (echoes) and visual modalities to predict the scene depth. We show empirically, with experiments on Replica dataset, that the proposed method obtains 28% improve-ment in RMSE compared to the state-of-the-art audio-visual depth prediction method. To demonstrate the effectiveness of our method on larger dataset, we report competitive per-formance on Matterport3D, proposing to use it as a multi-modal depth prediction benchmark with echoes for the ﬁrst time. We also analyse the proposed method with exhaus-tive ablation experiments and qualitative results. The code and models are available at https://krantiparida. github.io/projects/bimgdepth.html 