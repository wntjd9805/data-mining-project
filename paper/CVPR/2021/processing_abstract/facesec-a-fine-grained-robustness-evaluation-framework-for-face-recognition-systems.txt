We present FACESEC, a framework for ﬁne-grained ro-bustness evaluation of face recognition systems. FACESEC evaluation is performed along four dimensions of adversar-ial modeling: the nature of perturbation (e.g., pixel-level or face accessories), the attacker’s system knowledge (about training data and learning architecture), goals (dodging or impersonation), and capability (tailored to individual in-puts or across sets of these). We use FACESEC to studyﬁve face recognition systems in both closed-set and open-set settings, and to evaluate the state-of-the-art approach for defending against physically realizable attacks on these. Weﬁnd that accurate knowledge of neural architecture is signiﬁ-cantly more important than knowledge of the training data in black-box attacks. Moreover, we observe that open-set face recognition systems are more vulnerable than closed-set sys-tems under different types of attacks. The efﬁcacy of attacks for other threat model variations, however, appears highly dependent on both the nature of perturbation and the neural network architecture. For example, attacks that involve ad-versarial face masks are usually more potent, even against adversarially trained models, and the ArcFace architecture tends to be more robust than the others. 