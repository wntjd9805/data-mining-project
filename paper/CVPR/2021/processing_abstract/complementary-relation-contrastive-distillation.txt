Knowledge distillation aims to transfer representation ability from a teacher model to a student model. Previous approaches focus on either individual representation dis-tillation or inter-sample similarity preservation. While we argue that the inter-sample relation conveys abundant in-formation and needs to be distilled in a more effective way.In this paper, we propose a novel knowledge distillation method, namely Complementary Relation Contrastive Dis-tillation (CRCD), to transfer the structural knowledge from the teacher to the student. SpeciÔ¨Åcally, we estimate the mu-tual relation in an anchor-based way and distill the anchor-student relation under the supervision of its corresponding anchor-teacher relation. To make it more robust, mutual relations are modeled by two complementary elements: the feature and its gradient. Furthermore, the low bound of mu-tual information between the anchor-teacher relation distri-bution and the anchor-student relation distribution is max-imized via relation contrastive loss, which can distill both the sample representation and the inter-sample relations.Experiments on different benchmarks demonstrate the ef-fectiveness of our proposed CRCD. 