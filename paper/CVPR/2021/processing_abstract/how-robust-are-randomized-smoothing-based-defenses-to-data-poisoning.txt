Predictions of certiﬁably robust classiﬁers remain con-stant in a neighborhood of a point, making them resilient to test-time attacks with a guarantee. In this work, we present a previously unrecognized threat to robust machine learning models that highlights the importance of training-data qual-ity in achieving high certiﬁed adversarial robustness. Speciﬁ-cally, we propose a novel bilevel optimization based data poi-soning attack that degrades the robustness guarantees of cer-tiﬁably robust classiﬁers. Unlike other poisoning attacks that reduce the accuracy of the poisoned models on a small set of target points, our attack reduces the average certiﬁed radius (ACR) of an entire target class in the dataset. Moreover, our attack is effective even when the victim trains the models from scratch using state-of-the-art robust training methods such as Gaussian data augmentation[8], MACER[36], andSmoothAdv[29] that achieve high certiﬁed adversarial ro-bustness. To make the attack harder to detect, we use clean-label poisoning points with imperceptible distortions. The effectiveness of the proposed method is evaluated by poison-ing MNIST and CIFAR10 datasets and training deep neural networks using previously mentioned training methods and certifying the robustness with randomized smoothing. TheACR of the target class, for models trained on generated poi-son data, can be reduced by more than 30%. Moreover, the poisoned data is transferable to models trained with different training methods and models with different architectures. 