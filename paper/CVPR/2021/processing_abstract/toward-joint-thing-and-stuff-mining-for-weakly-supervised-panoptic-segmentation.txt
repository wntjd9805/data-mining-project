Panoptic segmentation aims to partition an image to ob-ject instances and semantic content for thing and stuff cat-egories, respectively. To date, learning weakly supervised panoptic segmentation (WSPS) with only image-level la-bels remains unexplored. In this paper, we propose an ef-ﬁcient jointly thing-and-stuff mining (JTSM) framework forWSPS. To this end, we design a novel mask of interest pool-ing (MoIPool) to extract ﬁxed-size pixel-accurate feature maps of arbitrary-shape segmentations. MoIPool enables a panoptic mining branch to leverage multiple instance learning (MIL) to recognize things and stuff segmentation in a uniﬁed manner. We further reﬁne segmentation masks with parallel instance and semantic segmentation branches via self-training, which collaborates the mined masks from panoptic mining with bottom-up object evidence as pseudo-ground-truth labels to improve spatial coherence and con-tour localization. Experimental results demonstrate the ef-fectiveness of JTSM on PASCAL VOC and MS COCO. As a by-product, we achieve competitive results for weakly su-pervised object detection and instance segmentation. This work is a ﬁrst step towards tackling challenge panoptic seg-mentation task with only image-level labels. 