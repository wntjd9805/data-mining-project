TrainingOut-of-Domain Test ImagesTraining deep networks with limited labeled data while achieving a strong generalization ability is key in the quest to reduce human annotation efforts. This is the goal of semi-supervised learning, which exploits more widely avail-able unlabeled data to complement small labeled data sets.In this paper, we propose a novel framework for discrim-inative pixel-level tasks using a generative model of both images and labels. Concretely, we learn a generative ad-versarial network that captures the joint image-label dis-tribution and is trained efﬁciently using a large set of un-labeled images supplemented with only few labeled ones.We build our architecture on top of StyleGAN2 [45], aug-mented with a label synthesis branch.Image labeling at test time is achieved by ﬁrst embedding the target image into the joint latent space via an encoder network and test-time optimization, and then generating the label from the in-ferred embedding. We evaluate our approach in two impor-tant domains: medical image segmentation and part-based face segmentation. We demonstrate strong in-domain per-formance compared to several baselines, and are the ﬁrst to showcase extreme out-of-domain generalization, such as transferring from CT to MRI in medical imaging, and pho-tographs of real faces to paintings, sculptures, and even cartoons and animal faces. Project Page: https://nv-tlabs.github.io/semanticGAN/ 