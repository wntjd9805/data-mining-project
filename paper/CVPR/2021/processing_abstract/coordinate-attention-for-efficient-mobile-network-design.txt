Recent studies on mobile network design have demon-strated the remarkable effectiveness of channel atten-tion (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the posi-tional information, which is important for generating spa-tially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embed-ding positional information into channel attention, which we call “coordinate attention”. Unlike channel attention that transforms a feature tensor to a single feature vec-tor via 2D global pooling, the coordinate attention factor-izes channel attention into two 1D feature encoding pro-cesses that aggregate features along the two spatial di-rections, respectively.In this way, long-range dependen-cies can be captured along one spatial direction and mean-while precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complemen-tarily applied to the input feature map to augment the rep-resentations of the objects of interest. Our coordinate at-tention is simple and can be ﬂexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, andEfﬁcientNet with nearly no computational overhead. Exten-sive experiments demonstrate that our coordinate attention is not only beneﬁcial to ImageNet classiﬁcation but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is avail-able at https://github.com/Andrew- Qibin/CoordAttention. 