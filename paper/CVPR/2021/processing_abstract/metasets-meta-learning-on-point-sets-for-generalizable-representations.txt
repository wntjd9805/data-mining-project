Deep learning techniques for point clouds have achieved strong performance on a range of 3D vision tasks. However, it is costly to annotate large-scale point sets, making it criti-cal to learn generalizable representations that can transfer well across different point sets. In this paper, we study a new problem of 3D Domain Generalization (3DDG) with the goal to generalize the model to other unseen domains of point clouds without any access to them in the training process.It is a challenging problem due to the substantial geometry shift from simulated to real data, such that most existing 3D models underperform due to overﬁtting the complete geome-tries in the source domain. We propose to tackle this problem via MetaSets, which meta-learns point cloud representations from a group of classiﬁcation tasks on carefully-designed transformed point sets containing speciﬁc geometry priors.The learned representations are more generalizable to vari-ous unseen domains of different geometries. We design two benchmarks for Sim-to-Real transfer of 3D point clouds. Ex-perimental results show that MetaSets outperforms existing 3D deep learning methods by large margins. 