Unsupervised representation learning with contrastive learning achieved great success. This line of methods du-plicate each training batch to construct contrastive pairs, making each training batch and its augmented version for-warded simultaneously and leading to additional computa-tion. We propose a new jigsaw clustering pretext task in this paper, which only needs to forward each training batch it-self, and reduces the training cost. Our method makes use of information from both intra- and inter-images, and outper-forms previous single-batch based ones by a large margin.It is even comparable to the contrastive learning methods when only half of training batches are used.Our method indicates that multiple batches during train-ing are not necessary, and opens the door for future re-search of single-batch unsupervised methods. Our mod-els trained on ImageNet datasets achieve state-of-the-art results with linear classiÔ¨Åcation, outperforming previous single-batch methods by 2.6%. Models transferred toCOCO datasets outperforms MoCo v2 by 0.4% with only half of the training batches. Our pretrained models outper-form supervised ImageNet pretrained models on CIFAR-10 and CIFAR-100 datasets by 0.9% and 4.1% respectively. 