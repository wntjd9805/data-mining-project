Video inpainting aims to ﬁll spatio-temporal “cor-rupted” regions with plausible content. To achieve this goal, it is necessary to ﬁnd correspondences from neigh-bouring frames to faithfully hallucinate the unknown con-tent. Current methods achieve this goal through attention,ﬂow-based warping, or 3D temporal convolution. However,ﬂow-based warping can create artifacts when optical ﬂow is not accurate, while temporal convolution may suffer from spatial misalignment. We propose ‘Progressive TemporalFeature Alignment Network’, which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical ﬂow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual qual-ity and temporal consistency of the inpainted videos. Us-ing the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github.com/MaureenZOU/TSAM . 