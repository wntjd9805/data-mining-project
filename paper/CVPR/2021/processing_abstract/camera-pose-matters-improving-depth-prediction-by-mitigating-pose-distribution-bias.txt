Monocular depth predictors are typically trained on large-scale training sets which are naturally biased w.r.t the distribution of camera poses. As a result, trained predic-tors fail to make reliable depth predictions for testing exam-ples captured under uncommon camera poses. To address this issue, we propose two novel techniques that exploit the camera pose during training and prediction. First, we in-troduce a simple perspective-aware data augmentation that synthesizes new training examples with more diverse views by perturbing the existing ones in a geometrically consis-tent manner. Second, we propose a conditional model that exploits the per-image camera pose as prior knowledge by encoding it as a part of the input. We show that jointly ap-plying the two methods improves depth prediction on im-ages captured under uncommon and even never-before-seen camera poses. We show that our methods improve perfor-mance when applied to a range of different predictor ar-chitectures. Lastly, we show that explicitly encoding the camera pose distribution improves the generalization per-formance of a synthetically trained depth predictor when evaluated on real images. 