Font generation is a challenging problem especially for some writing systems that consist of a large number of characters and has attracted a lot of attention in recent years. However, existing methods for font generation are often in supervised learning. They require a large num-ber of paired data, which is labor-intensive and expensive to collect. Besides, common image-to-image translation models often deﬁne style as the set of textures and col-ors, which cannot be directly applied to font generation.To address these problems, we propose novel deformable generative networks for unsupervised font generation (DG-Font). We introduce a feature deformation skip connection (FDSC) which predicts pairs of displacement maps and em-ploys the predicted maps to apply deformable convolution to the low-level feature maps from the content encoder. The outputs of FDSC are fed into a mixer to generate the ﬁ-nal results. Taking advantage of FDSC, the mixer outputs a high-quality character with a complete structure. To fur-ther improve the quality of generated images, we use three deformable convolution layers in the content encoder to learn style-invariant feature representations. Experiments demonstrate that our model generates characters in higher quality than state-of-art methods. The source code is avail-able at https://github.com/ecnuycxie/DG-Font. 