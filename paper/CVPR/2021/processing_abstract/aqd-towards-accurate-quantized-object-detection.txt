Table 1 – Energy and Area cost for different precision opera-tions (on 45nm CMOS technology) [9, 14, 25].Network quantization allows inference to be conducted using low-precision arithmetic for improved inference efﬁ-ciency of deep neural networks on edge devices. However, designing aggressively low-bit (e.g., 2-bit) quantization schemes on complex tasks, such as object detection, still remains challenging in terms of severe performance degra-dation and unveriﬁable efﬁciency on common hardware. In this paper, we propose an Accurate Quantized object Detec-tion solution, termed AQD, to fully get rid of ﬂoating-point computation. To this end, we target using ﬁxed-point op-erations in all kinds of layers, including the convolutional layers, normalization layers, and skip connections, allow-ing the inference to be executed using integer-only arith-metic. To demonstrate the improved latency-vs-accuracy trade-off, we apply the proposed methods on RetinaNet andFCOS. In particular, experimental results on MS-COCO dataset show that our AQD achieves comparable or even better performance compared with the full-precision coun-terpart under extremely low-bit schemes, which is of great practical value. Source code and models are available at: https://github.com/aim-uofa/model-quantization 