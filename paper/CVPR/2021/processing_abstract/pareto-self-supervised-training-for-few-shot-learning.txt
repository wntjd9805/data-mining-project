networks fail to address such a problem.While few-shot learning (FSL) aims for rapid generaliza-tion to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly com-puted from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has re-cently drawn much attention to deal with few labeled data.Previous works beneﬁt from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challeng-ing to select a proper weight to balance tasks and reduce task conﬂict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identiﬁed. Then, an effective preferredPareto exploration is proposed to ﬁnd a set of optimal solu-tions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance. 