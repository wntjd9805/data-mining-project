Disentangled representations support a range of down-stream tasks including causal reasoning, generative model-ing, and fair machine learning. Unfortunately, disentangle-ment has been shown to be impossible without the incorpo-ration of supervision or inductive bias. Given that supervi-sion is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an un-supervised, deep State-Space-Model for Video Disentangle-ment (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts de-coder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including iden-tity and dynamics transfer, sequence generation, Fr´echet In-ception Distance, and factor classiﬁcation. VDSM achieves state-of-the-art performance and exceeds adversarial meth-ods, even when the methods use additional supervision. 