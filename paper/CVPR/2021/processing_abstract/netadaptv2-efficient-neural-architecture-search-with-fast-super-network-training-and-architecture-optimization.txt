Neural architecture search (NAS) typically consists of three main steps: training a super-network, training and evaluating sampled deep neural networks (DNNs), and train-ing the discovered DNN. Most of the existing efforts speed up some steps at the cost of a signiﬁcant slowdown of other steps or sacriﬁcing the support of non-differentiable search metrics. The unbalanced reduction in the time spent per step limits the total search time reduction, and the inabil-ity to support non-differentiable search metrics limits the performance of discovered DNNs.In this paper, we present NetAdaptV2 with three innova-tions to better balance the time spent for each step while supporting non-differentiable search metrics. First, we pro-pose channel-level bypass connections that merge network depth and layer width into a single search dimension to re-duce the time for training and evaluating sampled DNNs.Second, ordered dropout is proposed to train multiple DNNs in a single forward-backward pass to decrease the time for training a super-network. Third, we propose the multi-layer coordinate descent optimizer that considers the interplay of multiple layers in each iteration of optimization to im-prove the performance of discovered DNNs while supporting non-differentiable search metrics. With these innovations,NetAdaptV2 reduces the total search time by up to 5.8× onImageNet and 2.4× on NYU Depth V2, respectively, and dis-covers DNNs with better accuracy-latency/accuracy-MAC trade-offs than state-of-the-art NAS works. Moreover, the discovered DNN outperforms NAS-discovered MobileNetV3 by 1.8% higher top-1 accuracy with the same latency.1 