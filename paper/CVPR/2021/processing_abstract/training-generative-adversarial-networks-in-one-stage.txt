Generative Adversarial Networks (GANs) have demon-strated unprecedented success in various image generation tasks. The encouraging results, however, come at the price of a cumbersome training process, during which the gen-erator and discriminator are alternately updated in twoIn this paper, we investigate a general training stages. scheme that enables training GANs efﬁciently in only one stage. Based on the adversarial losses of the generator and discriminator, we categorize GANs into two classes, Sym-metric GANs and Asymmetric GANs, and introduce a novel gradient decomposition method to unify the two, allowing us to train both classes in one stage and hence alleviate the training effort. We also computationally analyze the ef-ﬁciency of the proposed method, and empirically demon-strate that, the proposed method yields a solid 1.5× accel-eration across various datasets and network architectures.Furthermore, we show that the proposed method is readily applicable to other adversarial-training scenarios, such as data-free knowledge distillation. The code is available at https://github.com/zju-vipa/OSGAN . 