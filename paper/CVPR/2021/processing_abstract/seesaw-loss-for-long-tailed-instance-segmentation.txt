Cross-Entropy LossSeesaw LossInstance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail.Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative sam-ples on tail classes lead to a biased learning process for classiﬁers. Consequently, objects of tail categories are more likely to be misclassiﬁed as backgrounds or head categories.To tackle this problem, we propose Seesaw Loss to dynam-ically re-balance gradients of positive and negative sam-ples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The miti-gation factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassiﬁed instances to avoid false posi-tives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains signiﬁcant gains over Cross-Entropy Loss, and achieves state-of-the-art performance onLVIS dataset without bells and whistles. Code is available at https://github.com/open-mmlab/mmdetection. 