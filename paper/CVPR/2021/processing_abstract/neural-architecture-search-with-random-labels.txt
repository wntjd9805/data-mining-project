In this paper, we investigate a new variant of neural architecture search (NAS) paradigm – searching with ran-dom labels (RLNAS). The task sounds counter-intuitive for most existing NAS algorithms since random label provides few information on the performance of each candidate ar-chitecture.Instead, we propose a novel NAS framework based on ease-of-convergence hypothesis, which requires only random labels during searching. The algorithm in-volves two steps: ﬁrst, we train a SuperNet using ran-dom labels; second, from the SuperNet we extract the sub-network whose weights change most signiﬁcantly during the training. Extensive experiments are evaluated on multiple datasets (e.g. NAS-Bench-201 and ImageNet) and multiple search spaces (e.g. DARTS-like and MobileNet-like). Very surprisingly, RLNAS achieves comparable or even better re-sults compared with state-of-the-art NAS methods such asPC-DARTS, Single Path One-Shot, even though the coun-terparts utilize full ground truth labels for searching. We hope our ﬁnding could inspire new understandings on the essential of NAS. 