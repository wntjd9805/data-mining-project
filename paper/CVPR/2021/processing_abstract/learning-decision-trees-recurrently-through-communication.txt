Integrated interpretability without sacriﬁcing the predic-tion accuracy of decision making algorithms has the poten-tial of greatly improving their value to the user. Instead of assigning a label to an image directly, we propose to learn iterative binary sub-decisions, inducing sparsity and trans-parency in the decision making process. The key aspect of our model is its ability to build a decision tree whose struc-ture is encoded into the memory representation of a Recur-rent Neural Network jointly learned by two models commu-nicating through message passing. In addition, our model assigns a semantic meaning to each decision in the form of binary attributes, providing concise, semantic and rele-vant rationalizations to the user. On three benchmark image classiﬁcation datasets, including the large-scale ImageNet, our model generates human interpretable binary decision sequences explaining the predictions of the network while maintaining state-of-the-art accuracy. 