Vision-and-language pre-training has achieved impres-sive success in learning multimodal representations be-tween vision and language. To generalize this success to non-English languages, we introduce UC2, the ﬁrst ma-chine translation-augmented framework for cross-lingual cross-modal representation learning. To tackle the scarcity problem of multilingual captions for image datasets, weﬁrst augment existing English-only datasets with other lan-guages via machine translation (MT). Then we extend the standard Masked Language Modeling and Image-TextMatching training objectives to multilingual setting, where alignment between different languages is captured through shared visual context (i.e., using image as pivot). To fa-cilitate the learning of a joint embedding space of images and all languages of interest, we further propose two novel pre-training tasks, namely Masked Region-to-Token Mod-eling (MRTM) and Visual Translation Language Modeling (VTLM), leveraging MT-enhanced translated data. Evalua-tion on multilingual image-text retrieval and multilingual visual question answering benchmarks demonstrates that our proposed framework achieves new state of the art on diverse non-English benchmarks while maintaining compa-rable performance to monolingual pre-trained models onEnglish tasks. 