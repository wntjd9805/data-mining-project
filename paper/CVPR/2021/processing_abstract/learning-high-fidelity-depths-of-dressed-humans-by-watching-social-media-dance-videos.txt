A key challenge of learning the geometry of dressed hu-mans lies in the limited availability of the ground truth data (e.g., 3D scanned models), which results in the perfor-mance degradation of 3D human reconstruction when ap-plying to real-world imagery. We address this challenge by leveraging a new data resource: a number of social me-dia dance videos that span diverse appearance, clothing styles, performances, and identities. Each video depicts dy-namic movements of the body and clothes of a single person while lacking the 3D ground truth geometry. To utilize these videos, we present a new method to use the local transfor-mation that warps the predicted local geometry of the per-son from an image to that of another image at a different time instant. This allows self-supervision as enforcing a temporal coherence over the predictions. In addition, we jointly learn the depth along with the surface normals that are highly responsive to local texture, wrinkle, and shade by maximizing their geometric consistency. Our method is end-to-end trainable, resulting in high ﬁdelity depth esti-mation that predicts ﬁne geometry faithful to the input real image. We demonstrate that our method outperforms the state-of-the-art human depth estimation and human shape recovery approaches on both real and rendered images. 