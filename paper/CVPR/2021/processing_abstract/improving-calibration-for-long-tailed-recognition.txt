Org. CIFAR-100CIFAR-100-LT, IF100Deep neural networks may perform poorly when train-ing datasets are heavily class-imbalanced. Recently, two-stage methods decouple representation learning and classi-ﬁer learning to improve performance. But there is still the vital issue of miscalibration. To address it, we design two methods to improve calibration and performance in such scenarios. Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-conﬁdence for classes and im-prove classiﬁer learning. For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework.Our proposed methods set new records on multiple popu-lar long-tailed recognition benchmark datasets, includingCIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018. 