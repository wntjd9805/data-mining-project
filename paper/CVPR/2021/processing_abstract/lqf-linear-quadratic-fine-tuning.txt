Classiﬁers that are linear in their parameters, and trained by optimizing a convex loss function, have predictable be-havior with respect to changes in the training data, initial conditions, and optimization. Such desirable properties are absent in deep neural networks (DNNs), typically trained by non-linear ﬁne-tuning of a pre-trained model. Previous at-tempts to linearize DNNs have led to interesting theoretical insights, but have not impacted the practice due to the sub-stantial performance gap compared to standard non-linear optimization. We present the ﬁrst method for linearizing a pre-trained model that achieves comparable performance to non-linear ﬁne-tuning on most of real-world image classiﬁca-tion tasks tested, thus enjoying the interpretability of linear models without incurring punishing losses in performance.LQF consists of simple modiﬁcations to the architecture, loss function and optimization typically used for classiﬁcation:Leaky-ReLU instead of ReLU, mean squared loss instead of cross-entropy, and pre-conditioning using Kronecker factor-ization. None of these changes in isolation is sufﬁcient to approach the performance of non-linear ﬁne-tuning. When used in combination, they allow us to reach comparable per-formance, and even superior in the low-data regime, while enjoying the simplicity, robustness and interpretability of linear-quadratic optimization. 