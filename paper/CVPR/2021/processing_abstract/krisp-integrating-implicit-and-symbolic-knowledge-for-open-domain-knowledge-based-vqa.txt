One of the most challenging question types in VQA is when answering the question requires outside knowledge not present in the image. In this work we study open-domain knowledge, the setting when the knowledge required to an-swer a question is not given/annotated, neither at training nor test time. We tap into two types of knowledge represen-tations and reasoning. First, implicit knowledge which can be learned effectively from unsupervised language pretrain-ing and supervised training data with transformer-based models. Second, explicit, symbolic knowledge encoded in knowledge bases. Our approach combines both—exploiting the powerful implicit reasoning of transformer models for answer prediction, and integrating symbolic representa-tions from a knowledge graph, while never losing their ex-plicit semantics to an implicit embedding. We combine di-verse sources of knowledge to cover the wide variety of knowledge needed to solve knowledge-based questions. We show our approach, KRISP (Knowledge Reasoning withImplicit and Symbolic rePresentations), signiﬁcantly out-performs state-of-the-art on OK-VQA, the largest available dataset for open-domain knowledge-based VQA. We show with extensive ablations that while our model successfully exploits implicit knowledge reasoning, the symbolic answer module which explicitly connects the knowledge graph to the answer vocabulary is critical to the performance of our method and generalizes to rare answers. 1 