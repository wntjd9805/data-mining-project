Deep clustering gradually becomes an important branch in unsupervised learning methods. However, current ap-proaches hardly take into consideration the semantic sam-ple relationships that existed in both local and global fea-tures. In addition, since the deep features are updated on-the-ﬂy, relying on these sample relationships may construct more semantically conﬁdent sample pairs, leading to infe-rior performance. To tackle this issue, we propose a method called Nearest Neighbor Matching (NNM) to match samples with their nearest neighbors from both local (batch) and global (overall) levels. Speciﬁcally, for the local level, we match the nearest neighbors based on batch embedded fea-tures, as for the global one, we match neighbors from over-all embedded features. To keep the clustering assignment consistent in both neighbors and classes, we frame consis-tent loss and class contrastive loss for both local and global levels. Experimental results on three benchmark datasets demonstrate the superiority of our new model against state-of-the-art methods. Particularly on the STL-10 dataset, our method can achieve supervised performance. As for the CIFAR-100 dataset, our NNM leads 3.7% against the latest comparison method. Our code will be available at https://github.com/ZhiyuanDang/NNM . 