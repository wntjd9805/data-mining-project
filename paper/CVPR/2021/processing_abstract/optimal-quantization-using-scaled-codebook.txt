We study the problem of quantizing N sorted, scalar dat-apoints with a ﬁxed codebook containing K entries that are allowed to be rescaled. The problem is deﬁned as ﬁnding the optimal scaling factor α and the datapoint assignments into the α-scaled codebook to minimize the squared er-ror between original and quantized points. Previously, the globally optimal algorithms for this problem were derived only for certain codebooks (binary and ternary) or under the assumption of certain distributions (Gaussian, Lapla-cian). By studying the properties of the optimal quantizer, we derive an O(N K log K) algorithm that is guaranteed to ﬁnd the optimal quantization parameters for any ﬁxed codebook regardless of data distribution. We apply our al-gorithm to synthetic and real-world neural network quan-tization problems and demonstrate the effectiveness of our approach. 