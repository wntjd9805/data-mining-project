ImageNet has been the most popular image classiﬁca-tion benchmark, but it is also the one with a signiﬁcant level of label noise. Recent studies have shown that many samples contain multiple classes, despite being assumed to be a single-label benchmark. They have thus proposed to turn ImageNet evaluation into a multi-label task, with ex-haustive multi-label annotations per image. However, they have not ﬁxed the training set, presumably because of a formidable annotation cost. We argue that the mismatch be-tween single-label annotations and effectively multi-label images is equally, if not more, problematic in the training setup, where random crops are applied. With the single-label annotations, a random crop of an image may contain an entirely different object from the ground truth, introduc-ing noisy or even incorrect supervision during training. We thus re-label the ImageNet training set with multi-labels. We address the annotation cost barrier by letting a strong im-age classiﬁer, trained on an extra source of data, generate the multi-labels. We utilize the pixel-wise multi-label pre-dictions before the ﬁnal pooling layer, in order to exploit the additional location-speciﬁc supervision signals. Training on the re-labeled samples results in improved model perfor-mances across the board. ResNet-50 attains the top-1 accu-racy of 78.9% on ImageNet with our localized multi-labels, which can be further boosted to 80.2% with the CutMix reg-ularization. We show that the models trained with local-ized multi-labels also outperforms the baselines on trans-fer learning to object detection and instance segmentation tasks, and various robustness benchmarks. The re-labeledImageNet training set, pre-trained weights, and the source code are available at https://github.com/naver-ai/relabel_imagenet. 