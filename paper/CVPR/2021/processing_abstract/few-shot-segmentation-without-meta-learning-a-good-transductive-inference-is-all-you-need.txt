We show that the way inference is performed in few-shot segmentation tasks has a substantial effect on performances—an aspect often overlooked in the literature in favor of the meta-learning paradigm. We introduce a transductive inference for a given query image, leveraging the statistics of its unlabeled pixels, by optimizing a new loss containing three complementary terms: i) the cross-entropy on the labeled support pixels; ii) the Shannon en-tropy of the posteriors on the unlabeled query-image pix-els; and iii) a global KL-divergence regularizer based on the proportion of the predicted foreground. As our infer-ence uses a simple linear classiﬁer of the extracted fea-tures, its computational load is comparable to inductive in-ference and can be used on top of any base training. Forego-ing episodic training and using only standard cross-entropy training on the base classes, our inference yields compet-itive performances on standard benchmarks in the 1-shot scenarios. As the number of available shots increases, the gap in performances widens: on PASCAL-5i, our method brings about 5% and 6% improvements over the state-of-the-art, in the 5- and 10-shot scenarios, respectively. Fur-thermore, we introduce a new setting that includes domain shifts, where the base and novel classes are drawn from dif-ferent datasets. Our method achieves the best performances in this more realistic setting. Our code is freely avail-https://github.com/mboudiaf/ able online:RePRI-for-Few-Shot-Segmentation. 