Designing an efﬁcient model within the limited compu-tational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise conﬁguration of the channel di-mensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension conﬁguration towards better performance than the convention. To this end, we empirically study how to de-sign a single layer properly by analyzing the rank of the out-put feature. We then investigate the channel conﬁguration of a model by searching network architectures concerning the channel conﬁguration under the computational cost re-striction. Based on the investigation, we propose a simple yet effective channel conﬁguration that can be parameter-ized by the layer index. As a result, our proposed model following the channel parameterization achieves remark-able performance on ImageNet classiﬁcation and transfer learning tasks including COCO object detection, COCO in-stance segmentation, and ﬁne-grained classiﬁcations. Code and ImageNet pretrained models are available at https://github.com/clovaai/rexnet. 