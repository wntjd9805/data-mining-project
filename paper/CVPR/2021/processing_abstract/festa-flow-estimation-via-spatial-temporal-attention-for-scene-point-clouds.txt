Scene ﬂow depicts the dynamics of a 3D scene, which is critical for various applications such as autonomous driving, robot navigation, AR/VR, etc. Conventionally, scene ﬂow is estimated from dense/regular RGB video frames. With the development of depth-sensing technologies, precise 3D measurements are available via point clouds which have sparked new research in 3D scene ﬂow. Nevertheless, it remains challenging to extract scene ﬂow from point clouds due to the sparsity and irregularity in typical point cloud sampling patterns. One major issue related to irregular sampling is identiﬁed as the randomness during point set ab-straction/feature extraction—an elementary process in manyﬂow estimation scenarios. A novel Spatial Abstraction withAttention (SA2) layer is accordingly proposed to alleviate the unstable abstraction problem. Moreover, a TemporalAbstraction with Attention (TA2) layer is proposed to rec-tify attention in temporal domain, leading to beneﬁts with motions scaled in a larger range. Extensive analysis and experiments veriﬁed the motivation and signiﬁcant perfor-mance gains of our method, dubbed as Flow Estimation via Spatial-Temporal Attention (FESTA), when compared to several state-of-the-art benchmarks of scene ﬂow estimation. 