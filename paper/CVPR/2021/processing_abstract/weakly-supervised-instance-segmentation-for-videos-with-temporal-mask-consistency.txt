Weakly supervised instance segmentation reduces the cost of annotations required to train models. However, existing approaches which rely only on image-level class labels predominantly suffer from errors due to (a) par-tial segmentation of objects and (b) missing object predic-tions. We show that these issues can be better addressed by training with weakly labeled videos instead of images.In videos, motion and temporal consistency of predictions across frames provide complementary signals which can help segmentation. We are the Ô¨Årst to explore the use of these video signals to tackle weakly supervised instance segmentation. We propose two ways to leverage this in-formation in our model. First, we adapt inter-pixel rela-tion network (IRN) [1] to effectively incorporate motion information during training. Second, we introduce a newMaskConsist module, which addresses the problem of miss-ing object instances by transferring stable predictions be-tween neighboring frames during training. We demonstrate that both approaches together improve the instance seg-mentation metric AP50 on video frames of two datasets:Youtube-VIS and Cityscapes by 5% and 3% respectively. 