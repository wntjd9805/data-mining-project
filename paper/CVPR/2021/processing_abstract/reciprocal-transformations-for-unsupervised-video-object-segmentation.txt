Unsupervised video object segmentation (UVOS) aims at segmenting the primary objects in videos without any hu-man intervention. Due to the lack of prior knowledge about the primary objects, identifying them from videos is the ma-jor challenge of UVOS. Previous methods often regard the moving objects as primary ones and rely on optical ﬂow to capture the motion cues in videos, but the ﬂow infor-mation alone is insufﬁcient to distinguish the primary ob-jects from the background objects that move together. This is because, when the noisy motion features are combined with the appearance features, the localization of the pri-mary objects is misguided. To address this problem, we propose a novel reciprocal transformation network to dis-cover primary objects by correlating three key factors: the intra-frame contrast, the motion cues, and temporal coher-ence of recurring objects. Each corresponds to a repre-sentative type of primary object, and our reciprocal mech-anism enables an organic coordination of them to effec-tively remove ambiguous distractions from videos. Addi-tionally, to exclude the information of the moving back-ground objects from motion features, our transformation module enables to reciprocally transform the appearance features to enhance the motion features, so as to focus on the moving objects with salient appearance while re-moving the co-moving outliers. Experiments on the public benchmarks demonstrate that our model signiﬁcantly out-performs the state-of-the-art methods. Code is available at https://github.com/OliverRensu/RTNet. 