We introduce an inversion based method, denoted asIMAge-Guided model INvErsion (IMAGINE), to generate high-quality and diverse images from only a single train-ing sample. We leverage the knowledge of image seman-tics from a pre-trained classiﬁer to achieve plausible gen-erations via matching multi-level feature representations in the classiﬁer, associated with adversarial training with an external discriminator. IMAGINE enables the synthesis procedure to simultaneously 1) enforce semantic speciﬁcity constraints during the synthesis, 2) produce realistic images without generator training, and 3) give users intuitive con-trol over the generation process. With extensive experimen-tal results, we demonstrate qualitatively and quantitatively*Work done during internship at Adobe Research that IMAGINE performs favorably against state-of-the-artGAN-based and inversion-based methods, across three dif-ferent image domains (i.e., objects, scenes, and textures). 