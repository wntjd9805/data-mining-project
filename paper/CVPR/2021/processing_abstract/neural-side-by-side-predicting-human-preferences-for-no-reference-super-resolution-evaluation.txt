Super-resolution based on deep convolutional net-works is currently gaining much attention from both academia and industry. However, lack of proper eval-uation measures makes it diﬃcult to compare ap-proaches, hampering progress in the ﬁeld. Traditional measures, such as PSNR or SSIM, are known to poorly correlate with the human perception of image qual-ity. Therefore, in existing works common practice is also to report Mean-Opinion-Score (MOS) — the re-sults of human evaluation of super-resolved images.Unfortunately, the MOS values from diﬀerent papers are not directly comparable, due to the varying num-ber of raters, their subjectivity, etc. By this paper, we introduce Neural Side-By-Side — a new measure that allows super-resolution models to be compared au-tomatically, eﬀectively approximating human prefer-ences. Namely, we collect a large dataset of aligned image pairs, which were produced by diﬀerent super-resolution models. Then each pair is annotated by sev-eral raters, who were instructed to choose a more visu-ally appealing image. Given the dataset and the labels, we trained a CNN model that obtains a pair of images and for each image predicts a probability of being more preferable than its counterpart. In this work, we show that Neural Side-By-Side generalizes across both new models and new data. Hence, it can serve as a natu-ral approximation of human preferences, which can be used to compare models or tune hyperparameters with-out raters’ assistance. We open-source the dataset and the pretrained model1 and expect that it will become a handy tool for researchers and practitioners. 