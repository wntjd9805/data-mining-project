Deep neural networks achieve state-of-the-art and some-times super-human performance across various domains.However, when learning tasks sequentially, the networks easily forget the knowledge of previous tasks, known as“catastrophic forgetting”. To achieve the consistencies be-tween the old tasks and the new task, one effective solution is to modify the gradient for update. Previous methods en-force independent gradient constraints for different tasks, while we consider these gradients contain complex informa-tion, and propose to leverage inter-task information by gra-dient decomposition. In particular, the gradient of an old task is decomposed into a part shared by all old tasks and a part speciﬁc to that task. The gradient for update should be close to the gradient of the new task, consistent with the gra-dients shared by all old tasks, and orthogonal to the space spanned by the gradients speciﬁc to the old tasks. In this way, our approach encourages common knowledge consol-idation without impairing the task-speciﬁc knowledge. Fur-thermore, the optimization is performed for the gradients of each layer separately rather than the concatenation of all gradients as in previous works. This effectively avoids the inﬂuence of the magnitude variation of the gradients in dif-ferent layers. Extensive experiments validate the effective-ness of both gradient-decomposed optimization and layer-wise updates. Our proposed method achieves state-of-the-art results on various benchmarks of continual learning. 