There are rich synchronized audio and visual events in our daily life. Inside the events, audio scenes are associated with the corresponding visual objects; meanwhile, sounding objects can indicate and help to separate their individual sounds in the audio track. Based on this observation, in this paper, we propose a cyclic co-learning (CCoL) paradigm that can jointly learn sounding object visual grounding and audio-visual sound separation in a uniﬁed framework. Con-cretely, we can leverage grounded object-sound relations to improve the results of sound separation. Meanwhile, beneﬁting from discriminative information from separated sounds, we improve training example sampling for sound-ing object grounding, which builds a co-learning cycle for the two tasks and makes them mutually beneﬁcial. Exten-sive experiments show that the proposed framework outper-forms the compared recent approaches on both tasks, and they can beneﬁt from each other with our cyclic co-learning.The source code and pre-trained models are released in https://github.com/YapengTian/CCOL-CVPR21. 