We propose Joint-DetNAS, a uniﬁed NAS framework for object detection, which integrates 3 key components: Neu-ral Architecture Search, pruning, and Knowledge Distilla-Instead of naively pipelining these techniques, our tion.Joint-DetNAS optimizes them jointly. The algorithm con-sists of two core processes: student morphism optimizes the student’s architecture and removes the redundant pa-rameters, while dynamic distillation aims to ﬁnd the opti-mal matching teacher. For student morphism, weight in-heritance strategy is adopted, allowing the student to ﬂex-ibly update its architecture while fully utilize the predeces-sor’s weights, which considerably accelerates the search;To facilitate dynamic distillation, an elastic teacher pool is trained via integrated progressive shrinking strategy, from which teacher detectors can be sampled without additional cost in subsequent searches. Given a base detector as the input, our algorithm directly outputs the derived student detector with high performance without additional train-ing. Experiments demonstrate that our Joint-DetNAS out-performs the naive pipelining approach by a great mar-gin. Given a classic R101-FPN as the base detector, Joint-DetNAS is able to boost its mAP from 41.4 to 43.9 on MSCOCO and reduce the latency by 47%, which is on par with the SOTA EfﬁcientDet while requiring less search cost. We hope our proposed method can provide the community with a new way of jointly optimizing NAS, KD and pruning. 