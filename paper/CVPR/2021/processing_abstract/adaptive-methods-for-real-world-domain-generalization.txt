Invariant approaches have been remarkably successful in tackling the problem of domain generalization, where the objective is to perform inference on data distributions different from those used in training. In our work, we in-vestigate whether it is possible to leverage domain infor-mation from the unseen test samples themselves. We pro-pose a domain-adaptive approach consisting of two steps: a) we ﬁrst learn a discriminative domain embedding from unsupervised training examples, and b) use this domain em-bedding as supplementary information to build a domain-adaptive model, that takes both the input as well as its domain into account while making predictions. For un-seen domains, our method simply uses few unlabelled test examples to construct the domain embedding. This en-ables adaptive classiﬁcation on any unseen domain. Our approach achieves state-of-the-art performance on various domain generalization benchmarks. In addition, we intro-duce the ﬁrst real-world, large-scale domain generalization benchmark, Geo-YFCC, containing 1.1M samples over 40 training, 7 validation and 15 test domains, orders of mag-nitude larger than prior work. We show that the existing approaches either do not scale to this dataset or underper-form compared to the simple baseline of training a model on the union of data from all training domains. In contrast, our approach achieves a signiﬁcant 1% improvement. 