Collaborative learning has gained great popularity due to its beneﬁt of data privacy protection: participants can jointly train a Deep Learning model without sharing their training sets. However, recent works discovered that an ad-versary can fully recover the sensitive training samples from the shared gradients. Such reconstruction attacks pose se-vere threats to collaborative learning. Hence, effective mit-igation solutions are urgently desired.In this paper, we propose to leverage data augmentation to defeat reconstruction attacks: by preprocessing sensi-tive images with carefully-selected transformation policies, it becomes infeasible for the adversary to extract any use-ful information from the corresponding gradients. We de-sign a novel search method to automatically discover qual-iﬁed policies. We adopt two new metrics to quantify the impacts of transformations on data privacy and model us-ability, which can signiﬁcantly accelerate the search speed.Comprehensive evaluations demonstrate that the policies discovered by our method can defeat existing reconstruc-tion attacks in collaborative learning, with high efﬁciency and negligible impact on the model performance. 