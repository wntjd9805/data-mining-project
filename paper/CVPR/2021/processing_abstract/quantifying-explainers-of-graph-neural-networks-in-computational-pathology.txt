Explainability of deep learning methods is imperative to facilitate their clinical adoption in digital pathology.However, popular deep learning methods and explainabil-ity techniques (explainers) based on pixel-wise process-ing disregard biological entities’ notion, thus complicating comprehension by pathologists. In this work, we address this by adopting biological entity-based graph processing and graph explainers enabling explanations accessible toIn this context, a major challenge becomes pathologists. to discern meaningful explainers, particularly in a stan-dardized and quantiﬁable fashion. To this end, we propose herein a set of novel quantitative metrics based on statis-tics of class separability using pathologically measurable concepts to characterize graph explainers. We employ the proposed metrics to evaluate three types of graph explain-ers, namely the layer-wise relevance propagation, gradient-based saliency, and graph pruning approaches, to explainCell-Graph representations for Breast Cancer Subtyping.The proposed metrics are also applicable in other domains by using domain-speciﬁc intuitive concepts. We validate the qualitative and quantitative ﬁndings on the BRACS dataset, a large cohort of breast cancer RoIs, by expert pathologists.The code, data, and models can be accessed here1. 