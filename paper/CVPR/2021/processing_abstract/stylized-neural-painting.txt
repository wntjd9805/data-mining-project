This paper proposes an image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles. Different from previous image-to-image translation methods that formulate the translation as pixel-wise prediction, we deal with such an artistic cre-ation process in a vectorized environment and produce a se-quence of physically meaningful stroke parameters that can be further used for rendering. Since a typical vector ren-der is not differentiable, we design a novel neural renderer which imitates the behavior of the vector renderer and then frame the stroke prediction as a parameter searching pro-cess that maximizes the similarity between the input and the rendering output. We explored the zero-gradient problem on parameter searching and propose to solve this problem from an optimal transportation perspective. We also show that previous neural renderers have a parameter coupling problem and we re-design the rendering network with a ras-terization network and a shading network that better han-dles the disentanglement of shape and color. Experiments show that the paintings generated by our method have a high degree of Ô¨Ådelity in both global appearance and lo-cal textures. Our method can be also jointly optimized with neural style transfer that further transfers visual style from other images. Our code and animated results are available at https:// jiupinjia.github.io/ neuralpainter/ . 