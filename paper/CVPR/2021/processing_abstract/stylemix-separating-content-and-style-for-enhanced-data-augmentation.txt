In spite of the great success of deep neural networks for many challenging classiﬁcation tasks, the learned networks are vulnerable to overﬁtting and adversarial attacks. Re-cently, mixup based augmentation methods have been ac-tively studied as one practical remedy for these drawbacks.However, these approaches do not distinguish between the content and style features of the image, but mix or cut-and-paste the images. We propose StyleMix and StyleCutMix as the ﬁrst mixup method that separately manipulates the con-tent and style information of input image pairs. By care-fully mixing up the content and style of images, we can cre-ate more abundant and robust samples, which eventually enhance the generalization of model training. We also de-velop an automatic scheme to decide the degree of style mix-ing according to the pair’s class distance, to prevent messy mixed images from too differently styled pairs. Our exper-iments on CIFAR-10, CIFAR-100 and ImageNet datasets show that StyleMix achieves better or comparable perfor-mance to state of the art mixup methods and learns more robust classiﬁers to adversarial attacks. 