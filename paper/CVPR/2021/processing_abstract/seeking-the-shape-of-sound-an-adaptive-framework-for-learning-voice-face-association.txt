Nowadays, we have witnessed the early progress on learning the association between voice and face automati-cally, which brings a new wave of studies to the computer vi-sion community. However, most of the prior arts along this line (a) merely adopt local information to perform modality alignment and (b) ignore the diversity of learning difﬁculty across different subjects. In this paper, we propose a novel framework to jointly address the above-mentioned issues.Targeting at (a), we propose a two-level modality alignment loss where both global and local information are consid-ered. Compared with the existing methods, we introduce a global loss into the modality alignment process. The global component of the loss is driven by the identity classiﬁca-tion. Theoretically, we show that minimizing the loss could maximize the distance between embeddings across differ-ent identities while minimizing the distance between em-beddings belonging to the same identity, in a global sense (instead of a mini-batch). Targeting at (b), we propose a dynamic reweighting scheme to better explore the hard but valuable identities while ﬁltering out the unlearnable iden-tities. Experiments show that the proposed method outper-forms the previous methods in multiple settings, including voice-face matching, veriﬁcation and retrieval. 