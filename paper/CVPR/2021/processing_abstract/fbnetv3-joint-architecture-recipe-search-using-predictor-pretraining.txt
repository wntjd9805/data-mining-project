Neural Architecture Search (NAS) yields state-of-the-art neural networks that outperform their best manually-designed counterparts. However, previous NAS methods search for architectures under one set of training hyper-parameters (i.e., a training recipe), overlooking superior architecture-recipe combinations. To address this, we present Neural Architecture-Recipe Search (NARS) to search both (a) architectures and (b) their corresponding training recipes, simultaneously. NARS utilizes an accuracy pre-dictor that scores architecture and training recipes jointly, guiding both sample selection and ranking. Furthermore, to compensate for the enlarged search space, we leverage“free” architecture statistics (e.g., FLOP count) to pretrain the predictor, signiﬁcantly improving its sample efﬁciency and prediction reliability. After training the predictor via constrained iterative optimization, we run fast evolution-ary searches in just CPU minutes to generate architecture-recipe pairs for a variety of resource constraints, calledFBNetV3. FBNetV3 makes up a family of state-of-the-art compact neural networks that outperform both automati-cally and manually-designed competitors. For example, FB-NetV3 matches both EfﬁcientNet and ResNeSt accuracy onImageNet with up to 2.0× and 7.1× fewer FLOPs, respec-tively. Furthermore, FBNetV3 yields signiﬁcant performance gains for downstream object detection tasks, improving mAP despite 18% fewer FLOPs and 34% fewer parameters thanEfﬁcientNet-based equivalents. 