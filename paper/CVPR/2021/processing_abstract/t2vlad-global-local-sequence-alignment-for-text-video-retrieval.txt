Text-video retrieval is a challenging task that aims to search relevant video contents based on natural language descriptions. The key to this problem is to measure text-video similarities in a joint embedding space. However, most existing methods only consider the global cross-modal similarity and overlook the local details. Some works in-corporate the local comparisons through cross-modal local matching and reasoning. These complex operations intro-duce tremendous computation. In this paper, we design an efÔ¨Åcient global-local alignment method. The multi-modal video sequences and text features are adaptively aggregated with a set of shared semantic centers. The local cross-modal similarities are computed between the video feature and text feature within the same center. This design enables the meticulous local comparison and reduces the computa-tional cost of the interaction between each text-video pair.Moreover, a global alignment method is proposed to pro-vide a global cross-modal measurement that is complemen-tary to the local perspective. The global aggregated visual features also provide additional supervision, which is indis-pensable to the optimization of the learnable semantic cen-ters. We achieve consistent improvements on three standard text-video retrieval benchmarks and outperform the state-of-the-art by a clear margin. 