Beyond achieving high performance across many vi-sion tasks, multimodal models are expected to be robust to single-source faults due to the availability of redun-dant information between modalities.In this paper, we investigate the robustness of multimodal neural networks against worst-case (i.e., adversarial) perturbations on a single modality. We ﬁrst show that standard multimodal fusion models are vulnerable to single-source adversaries: an attack on any single modality can overcome the cor-rect information from multiple unperturbed modalities and cause the model to fail. This surprising vulnerability holds across diverse multimodal tasks and necessitates a solu-tion. Motivated by this ﬁnding, we propose an adversar-ially robust fusion strategy that trains the model to com-pare information coming from all the input sources, detect inconsistencies in the perturbed modality compared to the other modalities, and only allow information from the un-perturbed modalities to pass through. Our approach sig-niﬁcantly improves on state-of-the-art methods in single-source robustness, achieving gains of 7.8-25.2% on action recognition, 19.7-48.2% on object detection, and 1.6-6.7% on sentiment analysis, without degrading performance on unperturbed (i.e., clean) data. 