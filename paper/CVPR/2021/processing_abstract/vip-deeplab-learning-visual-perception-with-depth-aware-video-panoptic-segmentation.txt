In this paper, we present ViP-DeepLab, a uniÔ¨Åed model attempting to tackle the long-standing and challenging in-verse projection problem in vision, which we model as restoring the point clouds from perspective image se-quences while providing each point with instance-level se-mantic interpretations. Solving this problem requires the vi-sion models to predict the spatial location, semantic class, and temporally consistent instance label for each 3D point.ViP-DeepLab approaches it by jointly performing monocu-lar depth estimation and video panoptic segmentation. We name this joint task as Depth-aware Video Panoptic Seg-mentation, and propose a new evaluation metric along with two derived datasets for it, which will be made available to the public. On the individual sub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming previ-ous methods by 5.1% VPQ on Cityscapes-VPS, ranking 1st on the KITTI monocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The datasets and the eval-uation codes are made publicly available1. 