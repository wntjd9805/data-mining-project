Transfer learning across heterogeneous data distribu-tions (a.k.a. domains) and distinct tasks is a more gen-eral and challenging problem than conventional transfer learning, where either domains or tasks are assumed to be the same. While neural network based feature transfer is widely used in transfer learning applications, ﬁnding the optimal transfer strategy still requires time-consuming ex-periments and domain knowledge. We propose a transfer-ability metric called Optimal Transport based ConditionalEntropy (OTCE), to analytically predict the transfer per-formance for supervised classiﬁcation tasks in such cross-domain and cross-task feature transfer settings. Our OTCE score characterizes transferability as a combination of do-main difference and task difference, and explicitly evalu-ates them from data in a uniﬁed framework. Speciﬁcally, we use optimal transport to estimate domain difference and the optimal coupling between source and target distribu-tions, which is then used to derive the conditional entropy of the target task (task difference). Experiments on the largest cross-domain dataset DomainNet and Ofﬁce31 demonstrate that OTCE shows an average of 21% gain in the correlation with the ground truth transfer accuracy compared to state-of-the-art methods. We also investigate two applications of the OTCE score including source model selection and multi-source feature fusion. 