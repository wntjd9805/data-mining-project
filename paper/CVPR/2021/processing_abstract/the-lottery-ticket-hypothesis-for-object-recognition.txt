ResNet-18 on COCORecognition tasks, such as object recognition and key-point estimation, have seen widespread adoption in recent years. Most state-of-the-art methods for these tasks use deep networks that are computationally expensive and have huge memory footprints. This makes it exceedingly difﬁcult to deploy these systems on low power embedded devices.Hence, the importance of decreasing the storage require-ments and the amount of computation in such models is paramount. The recently proposed Lottery Ticket Hypothe-sis (LTH) states that deep neural networks trained on large datasets contain smaller subnetworks that achieve on par performance as the dense networks. In this work, we per-form the ﬁrst empirical study investigating LTH for model pruning in the context of object detection, instance segmen-tation, and keypoint estimation. Our studies reveal that lottery tickets obtained from Imagenet pretraining do not transfer well to the downstream tasks. We provide guidance on how to ﬁnd lottery tickets with up to 80% overall spar-sity on different sub-tasks without incurring any drop in the performance. Finally, we analyse the behavior of trained tickets with respect to various task attributes such as object size, frequency, and difﬁculty of detection.ResNet-50 on COCOFigure 1: Performance of lottery tickets discovered using direct pruning for various object recognition tasks. Here we have used a Mask R-CNN model with ResNet-18 backbone (top) and ResNet-50 backbone (bottom) to train models for object detection, segmentation and human keypoint es-timation on the COCO dataset. We show the performance of the baseline dense network, the sparse subnetwork obtained by transferring ImageNet pre-trained “universal” lottery tickets, as well as the subnetwork obtained by task-speciﬁc pruning. Task-speciﬁc pruning outperforms the universal tickets by a wide margin. For each of the tasks, we can obtain the same performance as the original dense networks with only 20% of the weights. 