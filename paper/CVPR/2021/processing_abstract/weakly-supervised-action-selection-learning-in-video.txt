Localizing actions in video is a core task in computer vi-sion. The weakly supervised temporal localization problem investigates whether this task can be adequately solved with only video-level labels, signiﬁcantly reducing the amount of expensive and error-prone annotation that is required.A common approach is to train a frame-level classiﬁer where frames with the highest class probability are se-lected to make a video-level prediction. Frame-level acti-vations are then used for localization. However, the ab-sence of frame-level annotations cause the classiﬁer to im-part class bias on every frame. To address this, we pro-pose the Action Selection Learning (ASL) approach to cap-ture the general concept of action, a property we refer to as “actionness”. Under ASL, the model is trained with a novel class-agnostic task to predict which frames will be selected by the classiﬁer. Empirically, we show thatASL outperforms leading baselines on two popular bench-marks THUMOS-14 and ActivityNet-1.2, with 10.3% and 5.7% relative improvement respectively. We further ana-lyze the properties of ASL and demonstrate the importance of actionness. Full code for this work is available here: https://github.com/layer6ai-labs/ASL. 