We present a novel large-scale dataset and accompa-nying machine learning models aimed at providing a de-tailed understanding of the interplay between visual con-tent, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, we focus on the affective experience triggered by visual artworks and ask the annotators to in-dicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explana-tion for their emotion choice. As we demonstrate below, this leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., “freedom” or “love”), or ref-erences that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. We focus on visual art (e.g., paint-ings, artistic photographs) as it is a prime example of im-agery created to elicit emotional responses from its viewers.Our dataset, termed ArtEmis, contains 455K emotion attri-butions and explanations from humans, on 80K artworks from WikiArt. Building on this data, we train and demon-strate a series of captioning systems capable of expressing and explaining emotions from visual stimuli. Remarkably, the captions produced by these systems often succeed in re-ﬂecting the semantic and abstract content of the image, go-ing well beyond systems trained on existing datasets. The collected dataset and developed methods are available at https://artemisdataset.org. 