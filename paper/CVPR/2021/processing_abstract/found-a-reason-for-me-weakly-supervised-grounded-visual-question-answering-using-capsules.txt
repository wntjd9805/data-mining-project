The problem of grounding VQA tasks has seen an in-creased attention in the research community recently, with most attempts usually focusing on solving this task by using pretrained object detectors. However, pre-trained object detectors require bounding box annotations for detecting relevant objects in the vocabulary, which may not always be feasible for real-life large-scale applications. In this pa-per, we focus on a more relaxed setting: the grounding of relevant visual entities in a weakly supervised manner by training on the VQA task alone. To address this problem, we propose a visual capsule module with a query-based selec-tion mechanism of capsule features, that allows the model to focus on relevant regions based on the textual cues about visual information in the question. We show that integrat-ing the proposed capsule module in existing VQA systems signiÔ¨Åcantly improves their performance on the weakly su-pervised grounding task. Overall, we demonstrate the ef-fectiveness of our approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the CLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with groundtruth bounding boxes for objects that are rele-vant for the correct answer, as well as on GQA, a real worldVQA dataset with compositional questions. We show that the systems with the proposed capsule module consistently outperform the respective baseline systems in terms of an-swer grounding, while achieving comparable performance on VQA task.1 