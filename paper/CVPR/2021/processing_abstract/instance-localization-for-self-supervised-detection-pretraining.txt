Prior research on self-supervised learning has led to considerable progress on image classiﬁcation, but often with degraded transfer performance on object detection.The objective of this paper is to advance self-supervised pretrained models speciﬁcally for object detection. Based on the inherent difference between classiﬁcation and detec-tion, we propose a new self-supervised pretext task, called instance localization. Image instances are pasted at various locations and scales onto background images. The pretext task is to predict the instance category given the compos-ited images as well as the foreground bounding boxes. We show that integration of bounding boxes into pretraining promotes better task alignment and architecture alignment for transfer learning. In addition, we propose an augmen-tation method on the bounding boxes to further enhance the feature alignment. As a result, our model becomes weaker at Imagenet semantic classiﬁcation but stronger at image patch localization, with an overall stronger pre-trained model for object detection. Experimental results demonstrate that our approach yields state-of-the-art trans-fer learning results for object detection on PASCAL VOC and MSCOCO1. 