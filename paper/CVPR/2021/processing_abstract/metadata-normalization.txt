Batch Normalization (BN) and its variants have deliv-ered tremendous success in combating the covariate shift in-duced by the training step of deep learning methods. While these techniques normalize the feature distribution by stan-dardizing with batch statistics, they do not correct the in-ﬂuence on features from extraneous variables or multiple distributions. Such extra variables, referred to as meta-data here, may create bias or confounding effects (e.g., race when classifying gender from face images). We in-troduce the Metadata Normalization (MDN) layer, a new batch-level operation which can be used end-to-end within the training framework, to correct the inﬂuence of meta-data on the feature distribution. MDN adopts a regres-sion analysis technique traditionally used for preprocessing to remove (regress out) the metadata effects on model fea-tures during training. We utilize a metric based on distance correlation to quantify the distribution bias from the meta-data and demonstrate that our method successfully removes metadata effects on four diverse settings: one synthetic, one 2D image, one video, and one 3D medical image dataset. 