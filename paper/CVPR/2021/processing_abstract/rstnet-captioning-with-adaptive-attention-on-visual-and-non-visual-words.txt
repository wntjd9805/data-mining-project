Recent progress on visual question answering has ex-plored the merits of grid features for vision language tasks.Meanwhile, transformer-based models have shown remark-able performance in various sequence prediction prob-lems. However, the spatial information loss of grid fea-tures caused by ﬂattening operation, as well as the defect of the transformer model in distinguishing visual words andIn this paper, non visual words, are still left unexplored. we ﬁrst propose Grid-Augmented (GA) module, in which relative geometry features between grids are incorporated to enhance visual representations. Then, we build a BERT-based language model to extract language context and pro-pose Adaptive-Attention (AA) module on top of a trans-former decoder to adaptively measure the contribution of visual and language cues before making decisions for word prediction. To prove the generality of our proposals, we apply the two modules to the vanilla transformer model to build our Relationship-Sensitive Transformer (RSTNet) for image captioning task. The proposed model is tested on the MSCOCO benchmark, where it achieves new state-of-art results on both the Karpathy test split and the online test server. Source code is available at GitHub 1. 