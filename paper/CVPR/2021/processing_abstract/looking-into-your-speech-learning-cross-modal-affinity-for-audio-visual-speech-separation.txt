In this paper, we address the problem of separating indi-vidual speech signals from videos using audio-visual neural processing. Most conventional approaches utilize frame-wise matching criteria to extract shared information be-tween co-occurring audio and video. Thus, their perfor-mance heavily depends on the accuracy of audio-visual syn-chronization and the effectiveness of their representations.To overcome the frame discontinuity problem between two modalities due to transmission delay mismatch or jitter, we propose a cross-modal afﬁnity network (CaffNet) that learns global correspondence as well as locally-varying afﬁnities between audio and visual streams. Given that the global term provides stability over a temporal sequence at the∗ Both authors contributed equally to this work† Corresponding authorsThis work was supported by the National Research Foundation ofKorea (NRF) grant funded by the Korea government (MSIT). (NRF-2021R1A2C2006703). utterance-level, this resolves the label permutation problem characterized by inconsistent assignments. By extending the proposed cross-modal afﬁnity on the complex network, we further improve the separation performance in the com-plex spectral domain. Experimental results verify that the proposed methods outperform conventional ones on vari-ous datasets, demonstrating their advantages in real-world scenarios. 