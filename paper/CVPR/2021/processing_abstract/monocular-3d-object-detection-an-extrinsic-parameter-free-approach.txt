Monocular 3D object detection is an important task in autonomous driving.It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight ﬂuctuation of road smoothness and slope. Due to the lack of insight in industrial appli-cation, existing methods on open datasets neglect the cam-era pose information, which inevitably results in the detec-tor being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we pro-pose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Speciﬁcally, the proposed framework predicts camera extrinsic param-eters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works indepen-dent of the extrinsic parameter variations and produces ac-curate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets. 