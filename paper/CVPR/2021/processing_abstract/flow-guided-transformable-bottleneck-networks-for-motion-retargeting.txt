Human motion retargeting aims to transfer the motion of one person in a “driving” video or set of images to another person. Existing efforts leverage a long training video from each target person to train a subject-speciﬁc motion transfer model. However, the scalability of such methods is limited, as each model can only generate videos for the given target subject, and such training videos are labor-intensive to acquire and process. Few-shot motion transfer techniques, which only require one or a few im-ages from a target, have recently drawn considerable at-tention. Methods addressing this task generally use either 2D or explicit 3D representations to transfer motion, and in doing so, sacriﬁce either accurate geometric modeling or the ﬂexibility of an end-to-end learned representation.Inspired by the Transformable Bottleneck Network, which renders novel views and manipulations of rigid objects, we propose an approach based on an implicit volumetric rep-resentation of the image content, which can then be spa-tially manipulated using volumetric ﬂow ﬁelds. We address the challenging question of how to aggregate information across different body poses, learning ﬂow ﬁelds that allow for combining content from the appropriate regions of in-put images of highly non-rigid human subjects performing complex motions into a single implicit volumetric represen-tation. This allows us to learn our 3D representation solely from videos of moving people. Armed with both 3D object understanding and end-to-end learned rendering, this cat-egorically novel representation delivers state-of-the-art im-age generation quality, as shown by our quantitative and qualitative evaluations. 