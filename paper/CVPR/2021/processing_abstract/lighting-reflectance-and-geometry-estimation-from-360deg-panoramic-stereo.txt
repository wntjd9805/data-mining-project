We propose a method for estimating high-deﬁnition spatially-varying lighting, reﬂectance, and geometry of a scene from 360◦ stereo images. Our model takes advantage of the 360◦ input to observe the entire scene with geomet-ric detail, then jointly estimates the scene’s properties with physical constraints. We ﬁrst reconstruct a near-ﬁeld envi-ronment light for predicting the lighting at any 3D location within the scene. Then we present a deep learning model that leverages the stereo information to infer the reﬂectance and surface normal. Lastly, we incorporate the physical constraints between lighting and geometry to reﬁne the re-ﬂectance of the scene. Both quantitative and qualitative ex-periments show that our method, beneﬁting from the 360◦ observation of the scene, outperforms prior state-of-the-art methods and enables more augmented reality applications such as mirror-objects insertion. 