We propose a novel spatially-correlative loss that is sim-ple, efﬁcient and yet effective for preserving scene structure consistency while supporting large appearance changes during unpaired image-to-image (I2I) translation. Previous methods attempt this by using pixel-level cycle-consistency or feature-level matching losses, but the domain-speciﬁc nature of these losses hinder translation across large do-main gaps. To address this, we exploit the spatial patterns of self-similarity as a means of deﬁning scene structure. Our spatially-correlative loss is geared towards only capturing spatial relationships within an image rather than domain appearance. We also introduce a new self-supervised learn-ing method to explicitly learn spatially-correlative maps for each speciﬁc translation task. We show distinct im-provement over baseline models in all three modes of un-paired I2I translation: single-modal, multi-modal, and even single-image translation. This new loss can eas-ily be integrated into existing network architectures and thus allows wide applicability. The code is available at https://github.com/lyndonzheng/F-LSeSim. 