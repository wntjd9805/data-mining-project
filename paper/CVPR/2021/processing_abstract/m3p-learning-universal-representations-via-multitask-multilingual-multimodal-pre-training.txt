We present M3P, a Multitask Multilingual MultimodalPre-trained model that combines multilingual pre-training and multimodal pre-training into a uniﬁed framework via multitask pre-training. Our goal is to learn universal repre-sentations that can map objects occurred in different modal-ities or texts expressed in different languages into a com-mon semantic space. In addition, to explicitly encourageﬁne-grained alignment between images and non-English lan-guages, we also propose Multimodal Code-switched Train-ing (MCT) to combine monolingual pre-training and multi-modal pre-training via a code-switch strategy. Experiments are performed on the multilingual image retrieval task across two benchmark datasets, including MSCOCO and Multi30K.M3P can achieve comparable results for English and new state-of-the-art results for non-English languages. 