Graph Neural Networks (GNNs) have emerged as a pow-erful and ﬂexible framework for representation learning on irregular data. As they generalize the operations of clas-sical CNNs on grids to arbitrary topologies, GNNs also bring much of the implementation challenges of their Eu-clidean counterparts. Model size, memory footprint, and energy consumption are common concerns for many real-world applications. Network binarization allocates a sin-gle bit to parameters and activations, thus dramatically re-ducing the memory requirements (up to 32x compared to single-precision ﬂoating-point numbers) and maximizing the beneﬁts of fast SIMD instructions on modern hardware for measurable speedups. However, in spite of the large body of work on binarization for classical CNNs, this area re-mains largely unexplored in geometric deep learning. In this paper, we present and evaluate different strategies for the binarization of graph neural networks. We show that through careful design of the models, and control of the training pro-cess, binary graph neural networks can be trained at only a moderate cost in accuracy on challenging benchmarks. In particular, we present the ﬁrst dynamic graph neural net-work in Hamming space, able to leverage efﬁcient k-NN search on binary vectors to speed-up the construction of the dynamic graph. We further verify that the binary models offer signiﬁcant savings on embedded devices. Our code is publicly available on Github1. 