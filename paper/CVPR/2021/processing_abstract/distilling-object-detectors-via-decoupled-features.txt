without KDCOCO mAP = 37.7%Bed 6.9% 5.4%KD: object regionsCOCO mAP = 39.9% 5.0% 5.9% 6.4% 4.0% 8.8% 5.7% 3.2% 10.0% 68.5% 70.2% 70.9%KD: background regionsCOCO mAP = 39.9% 6.9% 4.2% 4.8% 3.2% 10.0%Knowledge distillation is a widely used paradigm for in-heriting information from a complicated teacher network to a compact student network and maintaining the strong per-formance. Different from image classiﬁcation, object detec-tors are much more sophisticated with multiple loss func-tions in which features that semantic information rely on are tangled. In this paper, we point out that the information of features derived from regions excluding objects are also essential for distilling the student detector, which is usu-ally ignored in existing approaches. In addition, we eluci-date that features from different regions should be assigned with different importance during distillation. To this end, we present a novel distillation algorithm via decoupled fea-tures (DeFeat) for learning a better student detector. Specif-ically, two levels of decoupled features will be processed for embedding useful information into the student, i.e., decou-pled features from neck and decoupled proposals from clas-siﬁcation head. Extensive experiments on various detectors with different backbones show that the proposed DeFeat is able to surpass the state-of-the-art distillation methods for object detection. For example, DeFeat improves ResNet50 based Faster R-CNN from 37.4% to 40.9% mAP, and im-proves ResNet50 based RetinaNet from 36.5% to 39.7% mAP on COCO benchmark. Code will be released1,2. 