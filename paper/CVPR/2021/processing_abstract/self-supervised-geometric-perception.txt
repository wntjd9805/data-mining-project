We present self-supervised geometric perception (SGP), the ﬁrst general framework to learn a feature descriptor for correspondence matching without any ground-truth ge-ometric model labels (e.g., camera poses, rigid transforma-tions). Our ﬁrst contribution is to formulate geometric per-ception as an optimization problem that jointly optimizes the feature descriptor and the geometric models given a large corpus of visual measurements (e.g., images, point clouds). Under this optimization formulation, we show that two important streams of research in vision, namely robust model ﬁtting and deep feature learning, correspond to opti-mizing one block of the unknown variables while ﬁxing the other block. This analysis naturally leads to our second contribution – the SGP algorithm that performs alternating minimization to solve the joint optimization. SGP iteratively executes two meta-algorithms: a teacher that performs ro-bust model ﬁtting given learned features to generate geo-metric pseudo-labels, and a student that performs deep fea-ture learning under noisy supervision of the pseudo-labels.As a third contribution, we apply SGP to two perception problems on large-scale real datasets, namely relative cam-era pose estimation on MegaDepth and point cloud registra-tion on 3DMatch. We demonstrate that SGP achieves state-of-the-art performance that is on-par or superior to the su-pervised oracles trained using ground-truth labels.1 