Knowledge distillation transfers knowledge from the teacher network to the student one, with the goal of greatly improving the performance of the student network. Previ-ous methods mostly focus on proposing feature transforma-tion and loss functions between the same level’s features to improve the effectiveness. We differently study the factor of connection path cross levels between teacher and stu-dent networks, and reveal its great importance. For the ﬁrst time in knowledge distillation, cross-stage connection paths are proposed. Our new review mechanism is effective and structurally simple. Our ﬁnally designed nested and com-pact framework requires negligible computation overhead, and outperforms other methods on a variety of tasks. We apply our method to classiﬁcation, object detection, and in-stance segmentation tasks. All of them witness signiﬁcant student network performance improvement. 