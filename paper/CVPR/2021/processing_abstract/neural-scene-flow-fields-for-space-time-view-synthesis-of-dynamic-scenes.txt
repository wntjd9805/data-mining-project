We present a method to perform novel view and time syn-thesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduceNeural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our repre-sentation is optimized through a neural network to ﬁt the observed input views. We show that our representation can be used for varieties of in-the-wild scenes, including thin structures, view-dependent effects, and complex degrees of motion. We conduct a number of experiments that demon-strate our approach signiﬁcantly outperforms recent monoc-ular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos. 