Training (coarse) (few-shot) Testing (fine) g o d t a c h s i fFew-shot learning methods offer pre-training techniques optimized for easier later adaptation of the model to new classes (unseen during training) using one or a few ex-amples. This adaptivity to unseen classes is especially important for many practical applications where the pre-trained label space cannot remain ﬁxed for effective use and the model needs to be ”specialized” to support new categories on the ﬂy. One particularly interesting scenario, essentially overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where the training classes (e.g. animals) are of much ‘coarser granularity’ than the tar-get (test) classes (e.g. breeds). A very practical example of C2FS is when the target classes are sub-classes of the training classes. Intuitively, it is especially challenging as (both regular and few-shot) supervised pre-training tends to learn to ignore intra-class variability which is essentialIn this paper, we introduce a for separating sub-classes. novel ’Angular normalization’ module that allows to effec-tively combine supervised and self-supervised contrastive pre-training to approach the proposed C2FS task, demon-strating signiﬁcant gains in a broad study over multiple baselines and datasets. We hope that this work will help to pave the way for future research on this new, challenging, and very practical topic of C2FS classiﬁcation. 