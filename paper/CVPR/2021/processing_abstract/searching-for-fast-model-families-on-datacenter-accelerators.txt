Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. How-ever, as neither NAS nor model scaling considers sufﬁcient hardware architecture details, they do not take full advan-tage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efﬁcient inference on DC accelerators. We ﬁrst analyzeDC accelerators and ﬁnd that existing CNNs suffer from insufﬁcient operational intensity, parallelism, and execu-tion efﬁciency and exhibit FLOPs-latency nonproportional-ity. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise con-volutions, and block-wise activation functions. We further propose a latency-aware compound scaling (LACS), the ﬁrst multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and net-work width, which is quite different from the observations from previous compound scaling. With the new search space and LACS, our search and scaling on datacenter acceler-ators results in a new model series named EfﬁcientNet-X.EfﬁcientNet-X is up to more than 2X faster than Efﬁcient-Net (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfﬁcientNet-X is also up to 7X faster than re-cent RegNet and ResNeSt on TPUv3 and GPUv100. Source code is at https://github.com/tensorflow/tpu/tree/ master/models/official/efficientnet/tpu 