Deep neural networks trained with standard cross-entropy loss memorize noisy labels, which degrades their performance. Most research to mitigate this memorization proposes new robust classiﬁcation loss functions. Conversely, we propose a Multi-Objective Interpolation Training (MOIT) approach that jointly exploits contrastive learning and clas-siﬁcation to mutually help each other and boost performance against label noise. We show that standard supervised con-trastive learning degrades in the presence of label noise and propose an interpolation training strategy to mitigate this behavior. We further propose a novel label noise de-tection method that exploits the robust feature representa-tions learned via contrastive learning to estimate per-sample soft-labels whose disagreements with the original labels accurately identify noisy samples. This detection allows treating noisy samples as unlabeled and training a classi-ﬁer in a semi-supervised manner to prevent noise memo-rization and improve representation learning. We further propose MOIT+, a reﬁnement of MOIT by ﬁne-tuning on detected clean samples. Hyperparameter and ablation stud-ies verify the key components of our method. Experiments on synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves state-of-the-art results. Code is available at https://git.io/JI40X. 