Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive ﬁelds. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive ﬁeld, through ei-ther dilated/atrous convolutions or inserting attention mod-ules. However, the encoder-decoder based FCN architec-ture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmenta-tion as a sequence-to-sequence prediction task. Speciﬁcally, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termedSEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and com-petitive results on Cityscapes. Particularly, we achieve theﬁrst position in the highly competitive ADE20K test server leaderboard on the day of submission. 