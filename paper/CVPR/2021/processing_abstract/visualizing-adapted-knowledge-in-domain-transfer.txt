A source model trained on source data and a target model learned through unsupervised domain adaptation (UDA) usually encode different knowledge. To understand the adaptation process, we portray their knowledge dif-ference with image translation.Speciﬁcally, we feed a translated image and its original version to the two mod-els respectively, formulating two branches. Through up-dating the translated image, we force similar outputs from the two branches. When such requirements are met, dif-ferences between the two images can compensate for and hence represent the knowledge difference between models.To enforce similar outputs from the two branches and de-pict the adapted knowledge, we propose a source-free im-age translation method that generates source-style images using only target images and the two models. We visual-ize the adapted knowledge on several datasets with differ-ent UDA methods and ﬁnd that generated images success-fully capture the style difference between the two domains.For application, we show that generated images enable fur-ther tuning of the target model without accessing source data. Code available at https://github.com/hou-yz/DA_visualization. 