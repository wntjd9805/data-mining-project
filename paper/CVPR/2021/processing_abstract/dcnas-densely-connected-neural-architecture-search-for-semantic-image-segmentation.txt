Existing NAS methods for dense image prediction tasks usually compromise on restricted search space or search on proxy task to meet the achievable computational de-mands. To allow as wide as possible network architectures and avoid the gap between realistic and proxy setting, we propose a novel Densely Connected NAS (DCNAS) frame-work, which directly searches the optimal network struc-tures for the multi-scale representations of visual informa-tion, over a large-scale target dataset without proxy. Specif-ically, by connecting cells with each other using learnable weights, we introduce a densely connected search space to cover an abundance of mainstream network designs. More-over, by combining both path-level and channel-level sam-pling strategies, we design a fusion module and mixture layer to reduce the memory consumption of ample search space, hence favoring the proxyless searching. Compared with contemporary works, experiments reveal that the prox-yless searching scheme is capable of bridging the gap be-tween searching and training environments. Further, DC-NAS achieves new state-of-the-art performances on public semantic image segmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC 2012. We also retain leading performances when evaluating the ar-chitecture on the more challenging ADE20K and PASCAL-Context dataset. 