Referring image segmentation aims to segment the ob-jects referred by a natural language expression. Previ-ous methods usually focus on designing an implicit and re-current feature interaction mechanism to fuse the visual-linguistic features to directly generate the ﬁnal segmenta-tion mask without explicitly modeling the localization infor-mation of the referent instances. To tackle these problems, we view this task from another perspective by decoupling it into a “Locate-Then-Segment” (LTS) scheme. Given a lan-guage expression, people generally ﬁrst perform attention to the corresponding target image regions, then generate aﬁne segmentation mask about the object based on its con-text. The LTS ﬁrst extracts and fuses both visual and textual features to get a cross-modal representation, then applies a cross-model interaction on the visual-textual features to lo-cate the referred object with position prior, and ﬁnally gen-erates the segmentation result with a light-weight segmen-tation network. Our LTS is simple but surprisingly effective.On three popular benchmark datasets, the LTS outperforms all the previous state-of-the-arts methods by a large margin (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg).In addition, our model is more interpretable with explicitly locating the object, which is also proved by visualization ex-periments. We believe this framework is promising to serve as a strong baseline for referring image segmentation. 