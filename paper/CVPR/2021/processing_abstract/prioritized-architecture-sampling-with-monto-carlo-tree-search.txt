One-shot neural architecture search (NAS) methods sig-niﬁcantly reduce the search cost by considering the whole search space as one network, which only needs to be trained once. However, current methods select each operation in-dependently without considering previous layers. Besides, the historical information obtained with huge computation costs is usually used only once and then discarded. In this paper, we introduce a sampling strategy based on MonteCarlo tree search (MCTS) with the search space modeled as a Monte Carlo tree (MCT), which captures the depen-dency among layers. Furthermore, intermediate results are stored in the MCT for future decisions and a better exploration-exploitation balance. Concretely, MCT is up-dated using the training loss as a reward to the architec-ture performance; for accurately evaluating the numerous nodes, we propose node communication and hierarchical node selection methods in the training and search stages, respectively, making better uses of the operation rewards and hierarchical information. Moreover, for a fair com-parison of different NAS methods, we construct an open-source NAS benchmark of a macro search space evaluated on CIFAR-10, namely NAS-Bench-Macro. Extensive ex-periments on NAS-Bench-Macro and ImageNet demonstrate that our method signiﬁcantly improves search efﬁciency and performance. For example, by only searching 20 architec-tures, our obtained architecture achieves 78.0% top-1 ac-curacy with 442M FLOPs on ImageNet. Code (Benchmark) is available at: https://github.com/xiusu/NAS-Bench-Macro.*Equal contributions.†Corresponding authors.Independent	SamplingArchitecture Sampling 1 1 1 2 2 2 3 3 3 1 3 2ParentEvolutionChildren 1 3 1 1 3 1 2 1 2 3 2 3 1 3 2 2 3 1.........Prioritized	Sampling 1 1 3 1 3 root 2 root 2 3 root 2 3 1 3 3 1 3 3 1 2 3 1Operation candidates 1 32 1 2Figure 1. Comparison between existing methods (left) and our method (right). The existing method treats each layer indepen-dently in training (top-left) and search (bottom-left) stages, while our method models the search space with dependencies to a uniﬁed tree structure. 