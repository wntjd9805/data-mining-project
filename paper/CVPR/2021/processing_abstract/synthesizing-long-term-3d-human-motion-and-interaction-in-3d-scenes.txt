Synthesizing 3D human motion plays an important role in many graphics applications as well as understanding hu-man activity. While many efforts have been made on gener-ating realistic and natural human motion, most approaches neglect the importance of modeling human-scene interac-tions and affordance. On the other hand, affordance rea-soning (e.g., standing on the ﬂoor or sitting on the chair) has mainly been studied with static human pose and ges-tures, and it has rarely been addressed with human motion.In this paper, we propose to bridge human motion synthesis and scene affordance reasoning. We present a hierarchi-cal generative framework to synthesize long-term 3D hu-man motion conditioning on the 3D scene structure. Build-ing on this framework, we further enforce multiple geom-etry constraints between the human mesh and scene point clouds via optimization to improve realistic synthesis. Our experiments show signiﬁcant improvements over previous approaches on generating natural and physically plausible human motion in a scene.1 