High quality Machine Learning (ML) models are of-ten considered valuable intellectual property by companies.Model Stealing (MS) attacks allow an adversary with black-box access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existingMS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset nec-essary to query the target model. In this paper, we show that preventing access to the target dataset is not an ade-quate defense to protect a model. We propose MAZE – a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created us-ing a generative model to perform MS.Our evaluation with four image classiﬁcation models shows that MAZE provides a normalized clone accuracy in the range of 0.90⇥ to 0.99⇥, and outperforms even the recent attacks that rely on partial data (JBDA, clone accu-racy 0.13⇥ to 0.69⇥) and on surrogate data (KnockoffNets, clone accuracy 0.52⇥ to 0.97⇥). We also study an ex-tension of MAZE in the partial-data setting, and developMAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97⇥ to 1.0⇥) and reduces the query budget re-quired for the attack by 2⇥-24⇥. 