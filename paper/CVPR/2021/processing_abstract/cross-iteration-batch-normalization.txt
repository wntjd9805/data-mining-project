A well-known issue of Batch Normalization is its signiﬁ-cantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statis-tics upon which the normalization is deﬁned cannot be re-liably estimated from it during a training iteration. To ad-dress this problem, we present Cross-Iteration Batch Nor-malization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation qual-ity. A challenge of computing statistics over multiple itera-tions is that the network activations from different iterations are not comparable to each other due to changes in net-work weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polyno-mials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classiﬁcation with small mini-batch sizes, CBN is found to outperform the original batch nor-malization and a direct calculation of statistics over pre-vious iterations without the proposed compensation tech-nique. Code is available at https://aka.ms/cbn. 