Humans live within a 3D space and constantly interact with it to perform tasks. Such interactions involve physi-cal contact between surfaces that is semantically meaning-ful. Our goal is to learn how humans interact with scenes and leverage this to enable virtual characters to do the same. To that end, we introduce a novel Human-Scene In-teraction (HSI) model that encodes proximal relationships, called POSA for “Pose with prOximitieS and contActs”.The representation of interaction is body-centric, which en-ables it to generalize to new scenes. Speciﬁcally, POSA augments the SMPL-X parametric human body model such that, for every mesh vertex, it encodes (a) the contact prob-ability with the scene surface and (b) the corresponding semantic scene label. We learn POSA with a VAE con-ditioned on the SMPL-X vertices, and train on the PROX dataset, which contains SMPL-X meshes of people interact-ing with 3D scenes, and the corresponding scene seman-tics from the PROX-E dataset. We demonstrate the value of POSA with two applications. First, we automatically place 3D scans of people in scenes. We use a SMPL-X model ﬁt to the scan as a proxy and then ﬁnd its most likely placement in 3D. POSA provides an effective representa-tion to search for “affordances” in the scene that match the likely contact relationships for that pose. We perform a perceptual study that shows signiﬁcant improvement over the state of the art on this task. Second, we show thatPOSA’s learned representation of body-scene interaction supports monocular human pose estimation that is consis-tent with a 3D scene, improving on the state of the art.Our model and code are available for research purposes at https://posa.is.tue.mpg.de. 