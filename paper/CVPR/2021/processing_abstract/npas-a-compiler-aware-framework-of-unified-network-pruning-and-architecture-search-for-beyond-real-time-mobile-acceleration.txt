With the increasing demand to efﬁciently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the exe-cution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently, and do not fully con-sider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we ﬁrst propose (i) a gen-eral category of ﬁne-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting differentDNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware uniﬁed network pruning and architec-ture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learn-ing with fast evaluation and Bayesian optimization, ensur-ing the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, and 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mo-bile phone, consistently outperforming prior work. 