The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quan-tities of unlabeled images.In contrast, we aim to learn high-quality visual representations from fewer images. To this end we revisit supervised pretraining, and seek data-efﬁcient alternatives to classiﬁcation-based pretraining. We propose VirTex – a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks includ-ing image classiﬁcation, object detection, and instance seg-mentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet – supervised or unsu-pervised – despite using up to ten times fewer images. 