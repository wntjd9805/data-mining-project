Knowledge distillation is a method of transferring the knowledge from a pretrained complex teacher model to a student model, so a smaller network can replace a large teacher network at the deployment stage. To reduce the ne-cessity of training a large teacher model, the recent litera-tures introduced a self-knowledge distillation, which trains a student network progressively to distill its own knowl-edge without a pretrained teacher network. While Self-knowledge distillation is largely divided into a data aug-mentation based approach and an auxiliary network based approach, the data augmentation approach looses its lo-cal information in the augmentation process, which hin-ders its applicability to diverse vision tasks, such as se-mantic segmentation. Moreover, these knowledge distilla-tion approaches do not receive the reﬁned feature maps, which are prevalent in the object detection and seman-tic segmentation community. This paper proposes a novel self-knowledge distillation method, Feature Reﬁnement viaSelf-Knowledge Distillation (FRSKD), which utilizes an auxiliary self-teacher network to transfer a reﬁned knowl-edge for the classiﬁer network. Our proposed method,FRSKD, can utilize both soft label and feature-map distilla-tions for the self-knowledge distillation. Therefore, FRSKD can be applied to classiﬁcation, and semantic segmenta-tion, which emphasize preserving the local information.We demonstrate the effectiveness of FRSKD by enumer-ating its performance improvements in diverse tasks and benchmark datasets. The implemented code is available at https://github.com/MingiJi/FRSKD. 