Batch Normalization (BatchNorm) has become the de-fault component in modern neural networks to stabilize training. In BatchNorm, centering and scaling operations, along with mean and variance statistics, are utilized for feature standardization over the batch dimension. The batch dependency of BatchNorm enables stable training and better representation of the network, while inevitably ignores the representation differences among instances. We propose to add a simple yet effective feature calibration scheme into the centering and scaling operations of Batch-Norm, enhancing the instance-speciﬁc representations with the negligible computational cost. The centering calibra-tion strengthens informative features and reduces noisy fea-tures. The scaling calibration restricts the feature inten-sity to form a more stable feature distribution. Our pro-posed variant of BatchNorm, namely Representative Batch-Norm, can be plugged into existing methods to boost the performance of various tasks such as classiﬁcation, detec-tion, and segmentation. The source code is available in http://mmcheng.net/rbn. 