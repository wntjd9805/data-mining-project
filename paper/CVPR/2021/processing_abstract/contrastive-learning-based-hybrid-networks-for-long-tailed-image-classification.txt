Learning discriminative image representations plays a vital role in long-tailed image classiﬁcation because it can ease the classiﬁer learning in imbalanced cases. Given the promising performance contrastive learning has shown re-cently in representation learning, in this work, we explore effective supervised contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost the classiﬁcation accuracy thereon.Speciﬁcally, we propose a novel hybrid network structure be-ing composed of a supervised contrastive loss to learn image representations and a cross-entropy loss to learn classiﬁers, where the learning is progressively transited from feature learning to the classiﬁer learning to embody the idea that bet-ter features make better classiﬁers. We explore two variants of contrastive loss for feature learning, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. One of them is the recently proposed supervised contrastive (SC) loss, which is designed on top of the state-of-the-art unsu-pervised contrastive loss by incorporating positive samples from the same class. The other is a prototypical supervised contrastive (PSC) learning strategy which addresses the in-tensive memory consumption in standard SC loss and thus shows more promise under limited memory budget. Exten-sive experiments on three long-tailed classiﬁcation datasets demonstrate the advantage of the proposed contrastive learn-ing based hybrid networks in long-tailed classiﬁcation. 