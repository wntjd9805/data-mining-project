Quantization has emerged as one of the most prevalent approaches to compress and accelerate neural networks. Re-cently, data-free quantization has been widely studied as a practical and promising solution. It synthesizes data for calibrating the quantized model according to the batch nor-malization (BN) statistics of FP32 ones and signiﬁcantly relieves the heavy dependency on real training data in tra-ditional quantization methods. Unfortunately, we ﬁnd that in practice, the synthetic data identically constrained by BN statistics suffers serious homogenization at both distribution level and sample level and further causes a signiﬁcant per-formance drop of the quantized model. We propose DiverseSample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization. Speciﬁcally, we slack the alignment of feature statistics in the BN layer to relax the constraint at the distribution level and design a layerwise enhancement to reinforce speciﬁc layers for different data samples. Our DSG scheme is versatile and even able to be applied to the state-of-the-art post-training quantization method like AdaRound. We evaluate the DSG scheme on the large-scale image classiﬁcation task and consistently obtain signiﬁcant improvements over various network archi-tectures and quantization methods, especially when quan-tized to lower bits (e.g., up to 22% improvement on W4A4).Moreover, beneﬁting from the enhanced diversity, models calibrated with synthetic data perform close to those cali-brated with real data and even outperform them on W4A4. 