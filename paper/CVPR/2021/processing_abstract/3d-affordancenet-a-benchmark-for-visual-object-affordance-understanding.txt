The ability to understand the ways to interact with ob-jects from visual cues, a.k.a. visual affordance, is essential to vision-guided robotic research. This involves categoriz-ing, segmenting and reasoning of visual affordance. Rele-vant studies in 2D and 2.5D image domains have been made previously, however, a truly functional understanding of ob-ject affordance requires learning and prediction in the 3D physical domain, which is still absent in the community. In this work, we present a 3D AffordanceNet dataset, a bench-mark of 23k shapes from 23 semantic object categories, an-notated with 18 visual affordance categories. Based on this dataset, we provide three benchmarking tasks for evaluat-ing visual affordance understanding, including full-shape, partial-view and rotation-invariant affordance estimations.Three state-of-the-art point cloud deep learning networks are evaluated on all tasks. In addition we also investigate a semi-supervised learning setup to explore the possibility to beneÔ¨Åt from unlabeled data. Comprehensive results on our contributed dataset show the promise of visual affordance understanding as a valuable yet challenging benchmark. 