Panoptic segmentation is a challenging task aiming to simultaneously segment objects (things) at instance level and background contents (stuff) at semantic level. Exist-ing methods mostly utilize a two-stage detection network to attain instance segmentation results, and a fully convolu-tional network to produce a semantic segmentation predic-tion. Post-processing or additional modules are required to handle the conﬂicts between the outputs from these two nets, which makes such methods suffer from low efﬁciency, heavy memory consumption and complicated implementation. To simplify the pipeline and decrease computation/memory cost, we propose an one-stage approach called LightweightPanoptic Segmentation Network (LPSNet), which does not involve a proposal, anchor or mask head. Instead, we pre-dict a bounding box and semantic category at each pixel upon the feature map produced by an augmented feature pyramid, and design a parameter-free head to merge the per-pixel bounding box and semantic prediction into panop-tic segmentation output. Our LPSNet is not only efﬁcient in computation and memory, but also accurate in panop-tic segmentation. Comprehensive experiments on COCO,Cityscapes and Mapillary Vistas datasets demonstrate the promising effectiveness and efﬁciency of the proposed LP-SNet. 