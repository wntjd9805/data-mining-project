A deep facial attribute editing model strives to meet two requirements: (1) attribute correctness – the target attribute should correctly appear on the edited face image; (2) ir-relevance preservation – any irrelevant information (e.g., identity) should not be changed after editing. Meeting both requirements challenges the state-of-the-art works which resort to either spatial attention or latent space factoriza-tion. Speciﬁcally, the former assume that each attribute has well-deﬁned local support regions; they are often more ef-fective for editing a local attribute than a global one. The latter factorize the latent space of a ﬁxed pretrained GAN into different attribute-relevant parts, but they cannot be trained end-to-end with the GAN, leading to sub-optimal so-lutions. To overcome these limitations, we propose a novel latent space factorization model, called L2M-GAN, which is learned end-to-end and effective for editing both local and global attributes. The key novel components are: (1) A la-tent space vector of the GAN is factorized into an attribute-relevant and irrelevant codes with an orthogonality con-straint imposed to ensure disentanglement. (2) An attribute-relevant code transformer is learned to manipulate the at-tribute value; crucially, the transformed code are subject to the same orthogonality constraint. By forcing both the original attribute-relevant latent code and the edited code to be disentangled from any attribute-irrelevant code, our model strikes the perfect balance between attribute correct-ness and irrelevance preservation. Extensive experiments on CelebA-HQ show that our L2M-GAN achieves signiﬁ-cant improvements over the state-of-the-arts. 