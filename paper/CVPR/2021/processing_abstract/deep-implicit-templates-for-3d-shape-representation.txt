Deep implicit functions (DIFs), as a kind of 3D shape representation, are becoming more and more popular in the 3D vision community due to their compactness and strong representation power. However, unlike polygon mesh-based templates, it remains a challenge to reason dense corre-spondences or other semantic relationships across shapes represented by DIFs, which limits its applications in texture transfer, shape analysis and so on. To overcome this limi-tation and also make DIFs more interpretable, we proposeDeep Implicit Templates, a new 3D shape representation that supports explicit correspondence reasoning in deep im-plicit representations. Our key idea is to formulate DIFs as conditional deformations of a template implicit function.To this end, we propose Spatial Warping LSTM, which de-composes the conditional spatial transformation into mul-tiple point-wise transformations and guarantees general-ization capability. Moreover, the training loss is carefully designed in order to achieve high reconstruction accuracy while learning a plausible template with accurate corre-spondences in an unsupervised manner. Experiments show that our method can not only learn a common implicit tem-plate for a collection of shapes, but also establish dense correspondences across all the shapes simultaneously with-out any supervision. 