Image-to-image translation aims to preserve source con-tents while translating to discriminative target styles be-tween two visual domains. Most works apply adversarial learning in the ambient image space, which could be com-putationally expensive and challenging to train. In this pa-per, we propose to deploy an energy-based model (EBM) in the latent space of a pretrained autoencoder for this task. The pretrained autoencoder serves as both a latent code extractor and an image reconstruction worker. Our model, LETIT1, is based on the assumption that two do-mains share the same latent space, where latent represen-tation is implicitly decomposed as a content code and a domain-speciﬁc style code. Instead of explicitly extracting the two codes and applying adaptive instance normaliza-tion to combine them, our latent EBM can implicitly learn to transport the source style code to the target style code while preserving the content code, an advantage over exist-ing image translation methods. This simpliﬁed solution is also more efﬁcient in the one-sided unpaired image trans-lation setting. Qualitative and quantitative comparisons demonstrate superior translation quality and faithfulness for content preservation. Our model is the ﬁrst to be ap-plicable to 1024×1024-resolution unpaired image trans-lation to the best of our knowledge. Code is available at https://github.com/YangNaruto/latent-energy-transport. 