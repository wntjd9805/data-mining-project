In this paper, we explore the compression of deep neu-ral networks by quantizing the weights and activations into multi-bit binary networks (MBNs). A distribution-aware multi-bit quantization (DMBQ) method that incorporates the distribution prior into the optimization of quantization is proposed.Instead of solving the optimization in each iteration, DMBQ search the optimal quantization scheme over the distribution space beforehand, and select the quan-tization scheme during training using a fast lookup table based strategy. Based upon DMBQ, we further propose loss-guided bit-width allocation (LBA) to adaptively quan-tize and even prune the neural network. The ﬁrst-order Tay-lor expansion is applied to build a metric for evaluating the loss sensitivity of the quantization of each channel, and au-tomatically adjust the bit-width of weights and activations channel-wisely. We extend our method to image classiﬁca-tion tasks and experimental results show that our method not only outperforms state-of-the-art quantized networks in terms of accuracy but also is more efﬁcient in terms of train-ing time compared with state-of-the-art MBNs, even for the extremely low bit width (below 1-bit) quantization cases. 