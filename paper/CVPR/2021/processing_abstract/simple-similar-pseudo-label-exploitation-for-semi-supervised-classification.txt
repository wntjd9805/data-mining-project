A common classiﬁcation task situation is where one has a large amount of data available for training, but only a small portion is annotated with class labels. The goal of semi-supervised training, in this context, is to improve clas-siﬁcation accuracy by leverage information not only from labeled data but also from a large amount of unlabeled data.Recent works [2, 1, 26] have developed signiﬁcant improve-ments by exploring the consistency constrain between dif-ferently augmented labeled and unlabeled data. Following this path, we propose a novel unsupervised objective that focuses on the less studied relationship between the high conﬁdence unlabeled data that are similar to each other.The new proposed Pair Loss minimizes the statistical dis-tance between high conﬁdence pseudo labels with similar-ity above a certain threshold. Combining the Pair Loss with the techniques developed by the MixMatch family [2, 1, 26], our proposed SimPLE algorithm shows signiﬁcant perfor-mance gains over previous algorithms on CIFAR-100 andMini-ImageNet [31], and is on par with the state-of-the-art methods on CIFAR-10 and SVHN. Furthermore, SimPLE also outperforms the state-of-the-art methods in the transfer learning setting, where models are initialized by the weights pre-trained on ImageNet[15] or DomainNet-Real[23]. The code is available at github.com/zijian-hu/SimPLE. 