Human pose estimation from single images is a challeng-ing problem in computer vision that requires large amounts of labeled training data to be solved accurately. Unfortu-nately, for many human activities (e.g. outdoor sports) such training data does not exist and is hard or even impossible to acquire with traditional motion capture systems. We pro-pose a self-supervised approach that learns a single image 3D pose estimator from unlabeled multi-view data. To this end, we exploit multi-view consistency constraints to disen-tangle the observed 2D pose into the underlying 3D pose and camera rotation.In contrast to most existing meth-ods, we do not require calibrated cameras and can there-fore learn from moving cameras. Nevertheless, in the case of a static camera setup, we present an optional extension to include constant relative camera rotations over multiple views into our framework. Key to the success are new, unbi-ased reconstruction objectives that mix information across views and training samples. The proposed approach is eval-uated on two benchmark datasets (Human3.6M and MPII-INF-3DHP) and on the in-the-wild SkiPose dataset. 