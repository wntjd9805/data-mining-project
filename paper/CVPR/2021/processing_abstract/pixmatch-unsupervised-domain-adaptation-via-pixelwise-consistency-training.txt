Unsupervised domain adaptation is a promising tech-nique for semantic segmentation and other computer vi-sion tasks for which large-scale data annotation is costly and time-consuming. In semantic segmentation, it is attrac-tive to train models on annotated images from a simulated (source) domain and deploy them on real (target) domains.In this work, we present a novel framework for unsupervised domain adaptation based on the notion of target-domain consistency training. Intuitively, our work is based on the idea that in order to perform well on the target domain, a model’s output should be consistent with respect to small perturbations of inputs in the target domain. Speciﬁcally, we introduce a new loss term to enforce pixelwise consis-tency between the model’s predictions on a target image and a perturbed version of the same image. In comparison to popular adversarial adaptation methods, our approach is simpler, easier to implement, and more memory-efﬁcient during training. Experiments and extensive ablation studies demonstrate that our simple approach achieves remarkably strong results on two challenging synthetic-to-real bench-marks, GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes. 