Video instance segmentation (VIS) is the task that re-quires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typ-ically develop sophisticated pipelines to tackle this task.Here, we propose a new video instance segmentation frame-work built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence de-coding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly.At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspec-tive of similarity learning, thus considerably simplifying the overall pipeline and is signiﬁcantly different from existing approaches.Without bells and whistles, VisTR achieves the highest*Corresponding author. speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the ﬁrst time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy.We hope that VisTR can motivate future research for more video understanding tasks.Code is available at: https://git.io/VisTR 