We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions.Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D lo-cation and whose outputs are the following scene properties at that input location: volume density, surface normal, ma-terial parameters, distance to the ﬁrst surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indi-rect illumination effects. The predicted visibility and surface intersection ﬁelds are critical to our model’s ability to simu-late direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a sin-gle light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a signiﬁcant challenge to prior work. 