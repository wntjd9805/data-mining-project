This paper deﬁnes a new visual reasoning paradigm by introducing an important factor, i.e. transformation. The motivation comes from the fact that most existing visual rea-soning tasks, such as CLEVR in VQA, are solely deﬁned to test how well the machine understands the concepts and relations within static settings, like one image. We argue that this kind of state driven visual reasoning approach has limitations in reﬂecting whether the machine has the ability to infer the dynamics between different states, which has been shown as important as state-level reasoning for human cognition in Piaget’s theory. To tackle this prob-lem, we propose a novel transformation driven visual rea-soning task. Given both the initial and ﬁnal states, the target is to infer the corresponding single-step or multi-step transformation, represented as a triplet (object, at-tribute, value) or a sequence of triplets, respectively. Fol-lowing this deﬁnition, a new dataset namely TRANCE is constructed on the basis of CLEVR, including three lev-els of settings, i.e. Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transfor-mation with variant views). Experimental results show that the state-of-the-art visual reasoning models perform well on Basic, but are still far from human-level intelligence onEvent and View. We believe the proposed new paradigm will boost the development of machine visual reasoning.More advanced methods and real data need to be investi-gated in this direction. The resource of TVR is available at https://hongxin2019.github.io/TVR. 