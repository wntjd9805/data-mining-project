Modern deep learning techniques that regress the relative camera pose between two images have difﬁculty dealing with challenging scenarios, such as large camera motions result-ing in occlusions and signiﬁcant changes in perspective that leave little overlap between images. These models continue to struggle even with the beneﬁt of large supervised training datasets. To address the limitations of these models, we take inspiration from techniques that show regressing keypoint locations in 2D and 3D can be improved by estimating a discrete distribution over keypoint locations. Analogously, in this paper we explore improving camera pose regression by instead predicting a discrete distribution over camera poses. To realize this idea, we introduce DirectionNet, which estimates discrete distributions over the 5D relative pose space using a novel parameterization to make the estima-tion problem tractable. Speciﬁcally, DirectionNet factorizes relative camera pose, speciﬁed by a 3D rotation and a trans-lation direction, into a set of 3D direction vectors. Since 3D directions can be identiﬁed with points on the sphere,DirectionNet estimates discrete distributions on the sphere as its output. We evaluate our model on challenging syn-thetic and real pose estimation datasets constructed fromMatterport3D and InteriorNet. Promising results show a near 50% reduction in error over direct regression methods. 