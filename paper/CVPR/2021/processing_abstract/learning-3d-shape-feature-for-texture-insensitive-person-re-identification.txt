It is well acknowledged that person re-identiﬁcation (person ReID) highly relies on visual texture information like clothing. Despite signiﬁcant progress has been made in recent years, texture-confusing situations like clothing changing and persons wearing the same clothes receive lit-tle attention from most existing ReID methods. In this pa-per, rather than relying on texture based information, we propose to improve the robustness of person ReID against clothing texture by exploiting the information of a person’s 3D shape. Existing shape learning schemas for personReID either ignore the 3D information of a person, or re-quire extra physical devices to collect 3D source data. Dif-ferently, we propose a novel ReID learning framework that directly extracts a texture-insensitive 3D shape embedding from a 2D image by adding 3D body reconstruction as an auxiliary task and regularization, called 3D Shape Learn-ing (3DSL). The 3D reconstruction based regularization forces the ReID model to decouple the 3D shape informa-tion from the visual texture, and acquire discriminative 3D shape ReID features. To solve the problem of lacking 3D ground truth, we design an adversarial self-supervised pro-jection (ASSP) model, performing 3D reconstruction with-out ground truth. Extensive experiments on common ReID datasets and texture-confusing datasets validate the effec-tiveness of our model. 