Photorealistic rendering of dynamic humans is an im-portant capability for telepresence systems, virtual shop-ping, special effects in movies, and interactive experiences such as games. Recently, neural rendering methods have been developed to create high-ﬁdelity models of humans and objects. Some of these methods do not produce re-sults with high-enough ﬁdelity for driveable human models (Neural Volumes) whereas others have extremely long ren-dering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and con-tinuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a contin-uous learned scene function that maps every position and its corresponding local animation code to a view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dy-namic radiance ﬁeld can be used to synthesize novel un-seen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.See our project page1 for more results. 