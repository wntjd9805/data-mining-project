Training of Convolutional Neural Networks (CNNs) with data with noisy labels is known to be a challenge. Based on the fact that directly providing the label to the data (PositiveLearning; PL) has a risk of allowing CNNs to memorize the contaminated labels for the case of noisy data, the indirect learning approach that uses complementary labels (Nega-tive Learning for Noisy Labels; NLNL) has proven to be highly effective in preventing overﬁtting to noisy data as it reduces the risk of providing faulty target. NLNL further employs a three-stage pipeline to improve convergence. As a result, ﬁltering noisy data through the NLNL pipeline is cumbersome, increasing the training cost. In this study, we propose a novel improvement of NLNL, named Joint Neg-ative and Positive Learning (JNPL), that uniﬁes the ﬁlter-ing pipeline into a single stage. JNPL trains CNN via two losses, NL+ and PL+, which are improved upon NL andPL loss functions, respectively. We analyze the fundamental issue of NL loss function and develop new NL+ loss func-tion producing gradient that enhances the convergence of noisy data. Furthermore, PL+ loss function is designed to enable faster convergence to expected-to-be-clean data.We show that the NL+ and PL+ train CNN simultaneously, signiﬁcantly simplifying the pipeline, allowing greater ease of practical use compared to NLNL. With a simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classiﬁcation based on the superior ﬁltering ability. 