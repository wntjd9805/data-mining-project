The Sinkhorn divergence has become a very popu-lar metric to compare probability distributions in optimal transport. However, most works resort to the Sinkhorn di-vergence in Euclidean space, which greatly blocks their ap-plications in complex data with nonlinear structure. It is therefore of theoretical demand to empower the Sinkhorn divergence with the capability of capturing nonlinear struc-tures. We propose a theoretical and computational frame-work to bridge this gap.In this paper, we extend theSinkhorn divergence in Euclidean space to the reproduc-ing kernel Hilbert space, which we term “Hilbert Sinkhorn divergence” (HSD). In particular, we can use kernel ma-trices to derive a closed form expression of the HSD that is proved to be a tractable convex optimization problem.We also prove several attractive statistical properties of the proposed HSD, i.e., strong consistency, asymptotic behav-ior and sample complexity. Empirically, our method yields state-of-the-art performances on image classiﬁcation and topological data analysis. 