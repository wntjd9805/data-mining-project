Generative Adversarial Networks (GANs) are able to gen-erate high-quality images, but it remains difﬁcult to explicitly specify the semantics of synthesized images. In this work, we aim to better understand the semantic representation ofGANs, and thereby enable semantic control in GAN’s gener-ation process. Interestingly, we ﬁnd that a well-trained GAN encodes image semantics in its internal feature maps in a surprisingly simple way: a linear transformation of feature maps sufﬁces to extract the generated image semantics. To verify this simplicity, we conduct extensive experiments on various GANs and datasets; and thanks to this simplicity, we are able to learn a semantic segmentation model for a trained GAN from a small number (e.g., 8) of labeled im-ages. Last but not least, leveraging our ﬁnding, we propose two few-shot image editing approaches, namely Semantic-Conditional Sampling and Semantic Image Editing. Given a trained GAN and as few as eight semantic annotations, the user is able to generate diverse images subject to a user-provided semantic layout, and control the synthesized image semantics. We have made the code publicly available1. 