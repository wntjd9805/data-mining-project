In this paper, we present DRANet, a network architecture that disentangles image representations and transfers the visual attributes in a latent space for unsupervised cross-domain adaptation. Unlike the existing domain adaptation methods that learn associated features sharing a domain,DRANet preserves the distinctiveness of each domain’s characteristics. Our model encodes individual representa-tions of content (scene structure) and style (artistic appear-ance) from both source and target images. Then, it adapts the domain by incorporating the transferred style factor into the content factor along with learnable weights spec-iﬁed for each domain. This learning framework allows bi-/multi-directional domain adaptation with a single encoder-decoder network and aligns their domain shift. Addition-ally, we propose a content-adaptive domain transfer mod-ule that helps retain scene structure while transferring style.Extensive experiments show our model successfully sepa-rates content-style factors and synthesizes visually pleasing domain-transferred images. The proposed method demon-strates state-of-the-art performance on standard digit clas-siﬁcation tasks as well as semantic segmentation tasks. 