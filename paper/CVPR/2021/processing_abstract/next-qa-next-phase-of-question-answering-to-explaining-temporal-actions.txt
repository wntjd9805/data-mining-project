We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the tempo-ral actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reason-ing, temporal action reasoning, and common scene com-prehension. Through extensive analysis of baselines and es-tablished VideoQA techniques, we ﬁnd that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these mod-els to reason and highlights possibilities for improvement.With detailed results for different question types and heuris-tic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superﬁcial description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git) 