MeanShift is a popular mode-seeking clustering algo-rithm used in a wide range of applications in machine learn-ing. However, it is known to be prohibitively slow, with quadratic runtime per iteration. We propose MeanShift++, an extremely fast mode-seeking algorithm based on Mean-Shift that uses a grid-based approach to speed up the mean shift step, replacing the computationally expensive neigh-bors search with a density-weighted mean of adjacent grid cells. In addition, we show that this grid-based technique for density estimation comes with theoretical guarantees.The runtime is linear in the number of points and exponen-tial in dimension, which makes MeanShift++ ideal on low-dimensional applications such as image segmentation and object tracking. We provide extensive experimental analy-sis showing that MeanShift++ can be more than 10,000x faster than MeanShift with competitive clustering results on benchmark datasets and nearly identical image segmenta-tions as MeanShift. Finally, we show promising results for object tracking. tions, where in each iteration, each point is moved to the average of the points within a neighborhood ball centered at that point. The radius of the ball is a hyperparameter, often referred to as the bandwidth or window size. All ini-tial examples that converge to the same point are clustered together and the points of convergence are estimates of the modes or local maximas of the probability density function.It has been shown that MeanShift implicitly performs a gra-dient ascent on the kernel density estimate of the examples[3]. MeanShift thus serves two purposes: mode-seeking and clustering.MeanShift is often an attractive choice because it is non-parametric: unlike popular objective-based clustering al-gorithms such as k-means [5, 42] and spectral clustering[52, 78], it does not need to make many assumptions on the data, and the number of clusters is found automatically by the algorithm rather than a hyperparameter that needs to be set. In other words, MeanShift can adapt to general prob-ability distributions. However, one of the main drawbacks of this procedure is its computational complexity: each it-eration requires O computations. This is because for each example, calculating the window around the example is linear time in the worst case. n2 p q 