Point cloud videos exhibit irregularities and lack of or-der along the spatial dimension where points emerge incon-sistently across different frames. To capture the dynamics in point cloud videos, point tracking is usually employed.However, as points may ﬂow in and out across frames, computing accurate point trajectories is extremely difﬁcult.Moreover, tracking usually relies on point colors and thus may fail to handle colorless point clouds. In this paper, to avoid point tracking, we propose a novel Point 4D Trans-former (P4Transformer) network to model raw point cloud videos. Speciﬁcally, P4Transformer consists of (i) a point 4D convolution to embed the spatio-temporal local struc-tures presented in a point cloud video and (ii) a transformer to capture the appearance and motion information across the entire video by performing self-attention on the embed-ded local features. In this fashion, related or similar local areas are merged with attention weight rather than by ex-plicit tracking. Extensive experiments, including 3D action recognition and 4D semantic segmentation, on four bench-marks demonstrate the effectiveness of our P4Transformer for point cloud video modeling.Figure 1. Illustration of point cloud video modeling by our Point 4D Transformer (P4Transformer) network. Color encodes depth.A point cloud video is a sequence of irregular and unordered 3D coordinate sets. Points in different frames are not consistent. OurP4Transformer consists of a point 4D convolution and a trans-N ), former. The convolution encodes a point cloud video (3× where L and N denote the number of frames and the number ofN ′) and points in each frame, to a coordinate tensor (3N ′). The transformer performs self-L′ a feature tensor (C attention on the embedded tensors to capture the global spatio-temporal structure across the entire point cloud video.L′×××××L 