In this paper, we are interested in the bottom-up paradigm of estimating human poses from an image. We study the dense keypoint regression framework that is previ-ously inferior to the keypoint detection and grouping frame-work. Our motivation is that regressing keypoint positions accurately needs to learn representations that focus on the keypoint regions.We present a simple yet effective approach, named dis-entangled keypoint regression (DEKR). We adopt adaptive convolutions through pixel-wise spatial transformer to ac-tivate the pixels in the keypoint regions and accordingly learn representations from them. We use a multi-branch structure for separate regression: each branch learns a rep-resentation with dedicated adaptive convolutions and re-gresses one keypoint. The resulting disentangled represen-tations are able to attend to the keypoint regions, respec-tively, and thus the keypoint regression is spatially more ac-curate. We empirically show that the proposed direct re-gression method outperforms keypoint detection and group-ing methods and achieves superior bottom-up pose estima-tion results on two benchmark datasets, COCO and Crowd-Pose. The code and models are available at https://github.com/HRNet/DEKR. 