Channel pruning is a class of powerful methods for model compression. When pruning a neural network, it’s ideal to obtain a sub-network with higher accuracy. How-ever, a sub-network does not necessarily have high accu-racy with low classiﬁcation loss (loss-metric mismatch).In the paper, we ﬁrst consider the loss-metric mismatch problem for pruning and propose a novel channel pruning method for Convolutional Neural Networks (CNNs) by di-rectly maximizing the performance (i.e., accuracy) of sub-networks. Speciﬁcally, we train a stand-alone neural net-work to predict sub-networks’ performance and then max-imize the output of the network as a proxy of accuracy to guide pruning. Training such a performance prediction net-work efﬁciently is not an easy task, and it may potentially suffer from the problem of catastrophic forgetting and the imbalance distribution of sub-networks. To deal with this challenge, we introduce a corresponding episodic memory to update and collect sub-networks during the pruning pro-cess.In the experiment section, we further demonstrate that the gradients from the performance prediction network and the classiﬁcation loss have different directions. Exten-sive experimental results show that the proposed method can achieve state-of-the-art performance with ResNet, Mo-bileNetV2, and ShufﬂeNetV2+ on ImageNet and CIFAR-10. 