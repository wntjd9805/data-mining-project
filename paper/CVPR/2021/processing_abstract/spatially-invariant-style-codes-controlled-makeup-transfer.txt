Transferring makeup from the misaligned reference im-age is challenging. Previous methods overcome this barrier by computing pixel-wise correspondences between two im-ages, which is inaccurate and computational-expensive. In this paper, we take a different perspective to break down the makeup transfer problem into a two-step extraction-assignment process. To this end, we propose a Style-basedControllable GAN model that consists of three components, each of which corresponds to target style-code encoding, face identity features extraction, and makeup fusion, respec-tively. In particular, a Part-speciﬁc Style Encoder encodes the component-wise makeup style of the reference image into a style-code in an intermediate latent space W . The style-code discards spatial information and therefore is in-variant to spatial misalignment. On the other hand, the style-code embeds component-wise information, enablingﬂexible partial makeup editing from multiple references.This style-code, together with source identity features, is in-tegrated into a Makeup Fusion Decoder equipped with mul-tiple AdaIN layers to generate the ﬁnal result. Our proposed method demonstrates great ﬂexibility on makeup transfer by supporting makeup removal, shade-controllable makeup transfer, and part-speciﬁc makeup transfer, even with large spatial misalignment. Extensive experiments demonstrate∗The ﬁrst two authors contributed equally.†Corresponding author (hesfe@scut.edu.cn). the superiority of our approach over state-of-the-art meth-ods. Code is available at https://github.com/ makeuptransfer/SCGAN . 