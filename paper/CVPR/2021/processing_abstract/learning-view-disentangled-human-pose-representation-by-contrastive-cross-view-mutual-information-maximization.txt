We introduce a novel representation learning method to disentangle pose-dependent as well as view-dependent fac-tors from 2D human poses. The method trains a network us-ing cross-view mutual information maximization (CV-MIM) which maximizes mutual information of the same pose per-formed from different viewpoints in a contrastive learning manner. We further propose two regularization terms to en-sure disentanglement and smoothness of the learned repre-sentations. The resulting pose representations can be used for cross-view action recognition.To evaluate the power of the learned representations, in addition to the conventional fully-supervised action recog-nition settings, we introduce a novel task called single-shot cross-view action recognition. This task trains mod-els with actions from only one single viewpoint while models are evaluated on poses captured from all pos-sible viewpoints. We evaluate the learned representa-tions on standard benchmarks for action recognition, and show that (i) CV-MIM performs competitively compared with the state-of-the-art models in the fully-supervised sce-narios; (ii) CV-MIM outperforms other competing meth-ods by a large margin in the single-shot cross-view set-ting; (iii) and the learned representations can signiÔ¨Åcantly boost the performance when reducing the amount of super-vised training data. Our code is made publicly available at https : / / github . com / google - research / google-research/tree/master/poem. 