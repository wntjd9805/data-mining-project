Most explanation methods in deep learning map impor-tance estimates for a model’s prediction back to the original input space. These “visual” explanations are often insufﬁ-cient, as the model’s actual concept remains elusive. More-over, without insights into the model’s semantic concept, it is difﬁcult —if not impossible— to intervene on the model’s behavior via its explanations, called Explanatory Interac-tive Learning. Consequently, we propose to intervene on aNeuro-Symbolic scene representation, which allows one to revise the model on the semantic level, e.g. “never focus on the color to make your decision”. We compiled a novel con-founded visual scene data set, the CLEVR-Hans data set, capturing complex compositions of different objects. The results of our experiments on CLEVR-Hans demonstrate that our semantic explanations, i.e. compositional expla-nations at a per-object level, can identify confounders that are not identiﬁable using “visual” explanations only. More importantly, feedback on this semantic level makes it possi-ble to revise the model from focusing on these factors. 