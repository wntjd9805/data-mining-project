We evaluate the effectiveness of semi-supervised learning (SSL) on a realistic benchmark where data exhibits con-siderable class imbalance and contains images from novel classes. Our benchmark consists of two ﬁne-grained classiﬁ-cation datasets obtained by sampling classes from the Aves and Fungi taxonomy. We ﬁnd that recently proposed SSL methods provide signiﬁcant beneﬁts, and can effectively use out-of-class data to improve performance when deep net-works are trained from scratch. Yet their performance pales in comparison to a transfer learning baseline, an alternative approach for learning from a few examples. Furthermore, in the transfer setting, while existing SSL methods provide improvements, the presence of out-of-class is often detri-mental. In this setting, standard ﬁne-tuning followed by distillation-based self-training is the most robust. Our work suggests that semi-supervised learning with experts on re-alistic datasets may require different strategies than those currently prevalent in the literature.Figure 1. Accuracy of semi-supervised learning (SSL) algorithms on the Semi-Aves and Semi-Fungi datasets (see Fig. 2) using (i) different pre-trained models, and (ii) in-class (Uin) and out-of-class (Uin + Uout) unlabeled data. The performances of the supervised baseline and supervised oracle are also shown. Transfer learning from experts is far more effective than SSL from scratch, while in the transfer setting SSL provides modest gains. Though out-of-class data (Uout) is valuable when training from scratch, it is not the case when training from experts (details in Tab. 2 and 3). 