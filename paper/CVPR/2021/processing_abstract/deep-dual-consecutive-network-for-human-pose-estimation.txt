Multi-frame human pose estimation in complicated sit-uations is challenging. Although state-of-the-art human joints detectors have demonstrated remarkable results for static images, their performances come short when we ap-ply these models to video sequences. Prevalent shortcom-ings include the failure to handle motion blur, video defo-cus, or pose occlusions, arising from the inability in cap-turing the temporal dependency among video frames. On the other hand, directly employing conventional recurrent neural networks incurs empirical difﬁculties in modeling spatial contexts, especially for dealing with pose occlu-sions. In this paper, we propose a novel multi-frame human pose estimation framework, leveraging abundant temporal cues between video frames to facilitate keypoint detection.Three modular components are designed in our framework.A Pose Temporal Merger encodes keypoint spatiotemporal context to generate effective searching scopes while a PoseResidual Fusion module computes weighted pose residu-als in dual directions. These are then processed via ourPose Correction Network for efﬁcient reﬁning of pose esti-mations. Our method ranks No.1 in the Multi-frame PersonPose Estimation Challenge on the large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have re-leased our code, hoping to inspire future research. 