(a) Previous WS-DEC methodDense Event Captioning (DEC) aims to jointly local-ize and describe multiple events of interest in untrimmed videos, which is an advancement of the conventional video captioning task (generating a single sentence description for a trimmed video). Weakly Supervised Dense Event Cap-tioning (WS-DEC) goes one step further by not relying on human-annotated temporal event boundaries. However, there are few methods trying to tackle this task, and how to connect localization and description remains an open prob-lem. In this paper, we demonstrate that under weak supervi-sion, the event captioning module and localization module should be more closely bridged in order to improve descrip-tion performance. Different from previous approaches, in our method, the event captioner generates a sentence from a video segment and feeds it to the sentence localizer to re-construct the segment, and the localizer produces word im-portance weights as a guidance for the captioner to improve event description. To further bridge the sentence localizer and event captioner, a concept learner is adopted as the basis of the sentence localizer, which can be utilized to con-struct an induced set of concept features to enhance video features and improve the event captioner. Finally, our pro-posed method outperforms state-of-the-art WS-DEC meth-ods on the ActivityNet Captions dataset. 