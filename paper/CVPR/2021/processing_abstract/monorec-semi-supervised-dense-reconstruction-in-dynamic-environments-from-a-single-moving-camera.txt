In this paper, we propose MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic en-vironments. MonoRec is based on a multi-view stereo set-ting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts mov-ing object masks by leveraging the photometric inconsisten-cies encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is able to reconstruct both static and moving objects by leveraging the predicted masks. Fur-thermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not re-quire LiDAR depth values. We carefully evaluate MonoRec on the KITTI dataset and show that it achieves state-of-the-art performance compared to both multi-view and single-view methods. With the model trained on KITTI, we further-more demonstrate that MonoRec is able to generalize well to both the Oxford RobotCar dataset and the more chal-lenging TUM-Mono dataset recorded by a handheld cam-era. Code and related materials are available at https://vision.in.tum.de/research/monorec. 