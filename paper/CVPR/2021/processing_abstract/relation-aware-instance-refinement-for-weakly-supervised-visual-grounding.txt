Visual grounding, which aims to build a correspon-dence between visual objects and their language entities, plays a key role in cross-modal scene understanding. One promising and scalable strategy for learning visual ground-ing is to utilize weak supervision from only image-caption pairs. Previous methods typically rely on matching query phrases directly to a precomputed, ﬁxed object candidate pool, which leads to inaccurate localization and ambigu-ous matching due to lack of semantic relation constraints.In our paper, we propose a novel context-aware weakly-supervised learning method that incorporates coarse-to-ﬁne object reﬁnement and entity relation modeling into a two-stage deep network, capable of producing more ac-curate object representation and matching. To effectively train our network, we introduce a self-taught regression loss for the proposal locations and a classiﬁcation loss based on parsed entity relations. Extensive experiments on two public benchmarks Flickr30K Entities and Refer-ItGame demonstrate the efﬁcacy of our weakly grounding framework. The results show that we outperform the previ-ous methods by a considerable margin, achieving 59.27% top-1 accuracy in Flickr30K Entities and 37.68% in theReferItGame dataset respectively1. 