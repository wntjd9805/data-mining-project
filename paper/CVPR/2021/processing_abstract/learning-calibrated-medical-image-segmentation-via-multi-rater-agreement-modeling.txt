In medical image analysis, it is typical to collect multiple annotations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. Meanwhile, from the computer vision practi-tioner viewpoint, it has been a common practice to adopt the ground-truth labels obtained via either the majority-vote or simply one annotation from a preferred rater. This process, however, tends to overlook the rich information of agreement or disagreement ingrained in the raw multi-rater annotations. To address this issue, we propose to ex-plicitly model the multi-rater (dis-)agreement, dubbed MR-Net, which has two main contributions. First, an expertise-aware inferring module or EIM is devised to embed the expertise level of individual raters as prior knowledge, to form high-level semantic features. Second, our approach is capable of reconstructing multi-rater gradings from coarse predictions, with the multi-rater (dis-)agreement cues being further exploited to improve the segmentation performance.To our knowledge, our work is the ﬁrst in producing cali-brated predictions under different expertise levels for med-ical image segmentation. Extensive empirical experiments are conducted across ﬁve medical segmentation tasks of di-verse imaging modalities.In these experiments, superior performance of our MRNet is observed comparing to the state-of-the-arts, indicating the effectiveness and applica-bility of our MRNet toward a wide range of medical seg-mentation tasks. Source code is publicly available. 