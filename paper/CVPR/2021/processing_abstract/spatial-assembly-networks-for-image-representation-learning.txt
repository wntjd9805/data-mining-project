It has been long recognized that deep neural networks are sensitive to changes in spatial conﬁgurations or scene structures. Image augmentations, such as random transla-tion, cropping, and resizing, can be used to improve the ro-bustness of deep neural networks under spatial transforms.However, changes in object part conﬁgurations, spatial lay-out of object, and scene structures of the images may still result in major changes in the their feature representations generated by the network, creating signiﬁcant challenges for various visual learning tasks, including representation or metric learning, image classiﬁcation and retrieval.In this work, we introduce a new learnable module, called spa-tial assembly network (SAN), to address this important is-sue. This SAN module examines the input image and per-forms a learned re-organization and assembly of feature points from different spatial locations conditioned by fea-ture maps from previous network layers so as to maximize the discriminative power of the ﬁnal feature representation.This differentiable module can be ﬂexibly incorporated into existing network architectures, improving their capabilities in handling spatial variations and structural changes of the image scene. We demonstrate that the proposed SAN mod-ule is able to signiﬁcantly improve the performance of var-ious metric / representation learning, image retrieval and classiﬁcation tasks, in both supervised and unsupervised learning scenarios. 