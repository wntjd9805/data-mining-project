Self-attention has the promise of improving computer vi-sion systems due to parameter-independent scaling of recep-tive ﬁelds and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent inter-actions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efﬁcient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classiﬁcation benchmark. In preliminary transfer learning experiments, we ﬁnd that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efﬁcacy of self-attention models on settings traditionally dominated by convolutions. 1 