Learning to model and reconstruct humans in clothing is challenging due to articulation, non-rigid deformation, and varying clothing types and topologies. To enable learn-ing, the choice of representation is the key. Recent work uses neural networks to parameterize local surface ele-ments. This approach captures locally coherent geome-try and non-planar details, can deal with varying topol-ogy, and does not require registered training data. How-ever, naively using such methods to model 3D clothed hu-mans fails to capture ﬁne-grained local deformations and generalizes poorly. To address this, we present three key innovations: First, we deform surface elements based on a human body model such that large-scale deformations caused by articulation are explicitly separated from topo-logical changes and local clothing deformations. Second, we address the limitations of existing neural surface ele-ments by regressing local geometry from local features, sig-niﬁcantly improving the expressiveness. Third, we learn a pose embedding on a 2D parameterization space that en-codes posed body geometry, improving generalization to unseen poses by reducing non-local spurious correlations.We demonstrate the efﬁcacy of our surface representation by learning models of complex clothing from point clouds. The clothing can change topology and deviate from the topol-ogy of the body. Once learned, we can animate previously unseen motions, producing high-quality point clouds, from which we generate realistic images with neural rendering.We assess the importance of each technical contribution and show that our approach outperforms the state-of-the-art methods in terms of reconstruction accuracy and infer-ence time. The code is available for research purposes at https://qianlim.github.io/SCALE. 