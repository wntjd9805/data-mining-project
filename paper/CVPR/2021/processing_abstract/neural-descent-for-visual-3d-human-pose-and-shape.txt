We present deep neural network methodology to recon-struct the 3d pose and shape of people, including hand ges-tures and facial expression, given an input RGB image. We rely on a recently introduced, expressive full body statisti-cal 3d human model, GHUM, trained end-to-end, and learn to reconstruct its pose and shape state in a self-supervised regime. Central to our methodology, is a learning to learn and optimize approach, referred to as HUman Neural De-scent (HUND), which avoids both second-order differentia-tion when training the model parameters, and expensive state gradient descent in order to accurately minimize a semantic differentiable rendering loss at test time. Instead, we rely on novel recurrent stages to update the pose and shape pa-rameters such that not only losses are minimized effectively, but the process is meta-regularized in order to ensure end-progress. HUND’s symmetry between training and testing makes it the ﬁrst 3d human sensing architecture to natively support different operating regimes including self-supervised ones. In diverse tests, we show that HUND achieves very competitive results in datasets like H3.6M and 3DPW, as well as good quality 3d reconstructions for complex imagery collected in-the-wild. 