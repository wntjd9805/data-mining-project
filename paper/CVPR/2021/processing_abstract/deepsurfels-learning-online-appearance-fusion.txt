We present DeepSurfels, a novel hybrid scene represen-tation for geometry and appearance information. DeepSur-fels combines explicit and neural building blocks to jointly encode geometry and appearance information. In contrast to established representations, DeepSurfels better repre-sents high-frequency textures, is well-suited for online up-dates of appearance information, and can be easily com-bined with machine learning methods. We further present an end-to-end trainable online appearance fusion pipeline that fuses information from RGB images into the proposed scene representation and is trained using self-supervision imposed by the reprojection error with respect to the input images. Our method compares favorably to classical tex-ture mapping approaches as well as recent learning-based techniques. Moreover, we demonstrate lower runtime, im-proved generalization capabilities, and better scalability to larger scenes compared to existing methods. 