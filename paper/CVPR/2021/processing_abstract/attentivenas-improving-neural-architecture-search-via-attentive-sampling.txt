Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efﬁcient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efﬁciency and ac-curacy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the ﬁnal searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose At-tentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose al-gorithms to efﬁciently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large num-ber of networks across a wide range of FLOPs. Our dis-covered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outper-forms SOTA models, including BigNAS, Once-for-All net-works and FBNetV3. We also achieve ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models are available at https://github. com/facebookresearch/AttentiveNAS. 