We present a novel framework for contrastive learning of pixel-level representation using only unlabeled video. With-out the need of ground-truth annotation, our method is ca-pable of collecting well-deﬁned positive correspondences by measuring their conﬁdences and well-deﬁned negative ones by appropriately adjusting their hardness during train-ing. This allows us to suppress the adverse impact of am-biguous matches and prevent a trivial solution from being yielded by too hard or too easy negative samples. To ac-complish this, we incorporate three different criteria that ranges from a pixel-level matching conﬁdence to a video-level one into a bottom-up pipeline, and plan a curriculum that is aware of current representation power for the adap-tive hardness of negative samples during training. With the proposed method, state-of-the-art performance is attained over the latest approaches on several video label propaga-tion tasks. 