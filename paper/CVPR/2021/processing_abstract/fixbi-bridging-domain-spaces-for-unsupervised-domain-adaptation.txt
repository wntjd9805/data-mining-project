Unsupervised domain adaptation (UDA) methods for learning domain invariant representations have achieved remarkable progress. However, most of the studies were based on direct adaptation from the source domain to the target domain and have suffered from large domain dis-crepancies.In this paper, we propose a UDA method that effectively handles such large domain discrepancies.We introduce a ﬁxed ratio-based mixup to augment mul-tiple intermediate domains between the source and tar-get domain. From the augmented-domains, we train the source-dominant model and the target-dominant model that have complementary characteristics. Using our conﬁdence-based learning methodologies, e.g., bidirectional matching with high-conﬁdence predictions and self-penalization us-ing low-conﬁdence predictions, the models can learn from each other or from its own results. Through our proposed methods, the models gradually transfer domain knowledge from the source to the target domain. Extensive experi-ments demonstrate the superiority of our proposed method on three public benchmarks: Ofﬁce-31, Ofﬁce-Home, andVisDA-2017. 1 