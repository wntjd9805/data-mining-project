We introduce HoHoNet, a flexible and efficient framework for comprehensively understanding indoor 360-degree panoramas. Our approach utilizes a Latent Horizontal Feature (LHFeat) to flatten features vertically, which has proven successful for room layout reconstruction. HoHoNet improves upon existing methods in two key ways. Firstly, we have redesigned the deep architecture to enhance both speed and accuracy. Secondly, we propose a novel horizon-to-dense module that allows for per-pixel dense prediction from LHFeat, relaxing previous constraints. HoHoNet is highly efficient, achieving speeds of 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, even when modeling dense modalities from high-resolution panoramas. Furthermore, HoHoNet demonstrates strong accuracy in layout estimation, semantic segmentation, and dense depth estimation, outperforming previous state-of-the-art methods by a significant margin. The code for HoHoNet is available at https://github.com/sunset1995/HoHoNet.