This study focuses on the challenge of multimodal emotion recognition, which involves analyzing time-series data from different modalities such as language, motion, and sound. These modalities often have different sampling rates, resulting in unaligned streams of data. This asynchrony makes it difficult to efficiently combine the information from multiple modalities. To address this issue, the researchers propose a method called Progressive Modality Reinforcement (PMR) that leverages crossmodal transformer techniques. The PMR approach introduces a message hub that exchanges information with each modality. This hub sends common messages to each modality and reinforces their features through crossmodal attention. It also collects the reinforced features from each modality to generate a reinforced common message. This cycle process allows the common message and modalities' features to progressively complement each other. The reinforced features are then used to predict human emotions. The researchers conducted comprehensive experiments on various multimodal emotion recognition benchmarks, which clearly demonstrated the superior performance of their approach.