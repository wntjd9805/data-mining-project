To efficiently deploy deep neural networks (DNNs) on mobile edge devices, it is crucial to reduce unnecessary computation and improve execution speed. Previous methods such as model compression and network architecture search have been performed independently and have not fully considered compiler-level optimizations for mobile acceleration. In this study, we propose a fine-grained structured pruning technique applicable to various layers of DNNs, along with a compiler automatic code generation framework that supports different DNNs and pruning schemes. This framework bridges the gap between model compression and network architecture search. We also introduce NPAS, a compiler-aware unified approach for network pruning and architecture search. To address the large search space, we utilize a meta-modeling procedure based on reinforcement learning and Bayesian optimization, ensuring comparable training epochs to representative network architecture search frameworks. Our framework achieves impressive ImageNet inference times of 6.7ms, 5.9ms, and 3.9ms with Top-1 accuracies of 78.2%, 75% (comparable to MobileNet-V3), and 71% (comparable to MobileNet-V2) respectively on a standard mobile phone, consistently outperforming previous methods.