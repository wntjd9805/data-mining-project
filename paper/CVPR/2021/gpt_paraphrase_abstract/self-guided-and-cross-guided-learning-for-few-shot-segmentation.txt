The effectiveness of few-shot segmentation in segmenting unseen object classes with limited annotated samples has attracted attention. However, existing approaches using masked Global Average Pooling (GAP) for encoding support images suffer from the loss of discriminative information. In this study, we propose a self-guided learning approach that mines the lost critical information. This approach involves encoding the covered and uncovered foreground regions of the support image into primary and auxiliary support vectors through masked GAP. By aggregating both primary and auxiliary support vectors, we achieve better segmentation performance on query images. Furthermore, we introduce a cross-guided module for multiple-shot segmentation, which fuses predictions from multiple annotated samples to improve the final mask without re-training. Our approach outperforms existing methods on the PASCAL-5i and COCO-20i datasets, as demonstrated by extensive experiments. The source code is available at https://github.com/zbf1991/SCL.