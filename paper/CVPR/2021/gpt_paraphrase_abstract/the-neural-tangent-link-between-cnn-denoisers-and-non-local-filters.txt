Convolutional Neural Networks (CNNs) have become a widely used tool for solving computational imaging problems. These algorithms have achieved impressive performance in various image restoration tasks. Recent research has shown that even when trained with just one corrupted image, CNNs can still perform as well as fully trained networks, despite being highly overparameterized. We establish a formal connection between these networks and well-known non-local filtering techniques like non-local means or BM3D. By analyzing the neural tangent kernel (NTK) of these networks, we can derive the filtering function associated with a given network architecture without the need for training. This function is fully determined by the random initialization of the network weights. While the NTK theory accurately predicts the filter for networks trained using standard gradient descent, our analysis reveals that it fails to explain the behavior of networks trained with the popular Adam optimizer. The Adam optimizer leads to larger weight changes in hidden layers, allowing the non-local filtering function to adapt during training. We validate our findings through extensive image denoising experiments.