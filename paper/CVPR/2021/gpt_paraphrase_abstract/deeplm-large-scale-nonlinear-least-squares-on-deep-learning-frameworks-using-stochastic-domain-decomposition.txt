We present a new approach using deep learning frameworks to tackle large-scale nonlinear least squares problems. Traditionally, these problems are solved using the Levenberg-Marquardt algorithm, but we propose a general and efficient solver based on a deep learning framework. Our method incorporates a novel backward jacobian network that automates the computation of sparse jacobian matrices, leading to faster convergence. Additionally, we introduce a stochastic domain decomposition technique that allows for batched optimization and ensures convergence even for large problem sizes. To validate our approach, we apply it to the bundle adjustment problem, which serves as a fundamental test case. Experimental results demonstrate that our optimizer outperforms both state-of-the-art solutions and existing deep learning solvers in terms of quality, efficiency, and memory usage. Our stochastic domain decomposition method enables distributed optimization, requiring minimal memory and time, while achieving comparable quality to a global solver. Consequently, our solver effectively addresses nonlinear least squares problems on an extremely large scale. We will provide the code for our solver based on Pytorch and Mindspore frameworks.