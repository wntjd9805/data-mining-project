Adversarial robustness refers to the vulnerability of deep neural networks to imperceptible changes made during testing. Many algorithms have been developed to make neural networks resilient to adversarial changes in input images, which are typically measured using an ℓp norm. However, these methods often only work against specific attack types used during training. In this study, we expand on this concept by considering the training of deep neural networks that are robust to perturbations in multiple natural representation spaces. For image data, this includes both the standard pixel representation and the discrete cosine transform (DCT) basis representation. We propose a theoretically sound algorithm that provides formal guarantees for this problem. Additionally, our guarantees also hold when aiming for robustness against multiple ℓp norm-based attacks. We also provide an efficient practical implementation and demonstrate the effectiveness of our approach on standard image classification datasets.