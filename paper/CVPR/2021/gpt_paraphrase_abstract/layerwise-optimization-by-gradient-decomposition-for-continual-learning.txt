Deep neural networks excel in various domains, often surpassing human performance. However, when learning tasks sequentially, they often suffer from "catastrophic forgetting" where they forget the knowledge of previous tasks. To address this issue, previous methods enforced independent gradient constraints for different tasks. In contrast, we believe that these gradients contain valuable information and propose leveraging inter-task information through gradient decomposition.Specifically, we decompose the gradient of an old task into a part shared by all old tasks and a part specific to that task. The gradient for update should be similar to the gradient of the new task, consistent with the shared gradients of all old tasks, and orthogonal to the space spanned by the specific gradients of the old tasks. This approach promotes the consolidation of common knowledge without compromising task-specific knowledge.Furthermore, instead of optimizing the concatenation of all gradients as in previous works, we perform optimization separately for the gradients of each layer. This prevents the influence of varying gradient magnitudes across different layers.We conducted extensive experiments to validate the effectiveness of our gradient-decomposed optimization and layer-wise updates. Our proposed method outperformed previous approaches and achieved state-of-the-art results on various benchmarks of continual learning.