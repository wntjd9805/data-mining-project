Most neural architecture search methods involve constructing a super-net for the search process and deriving a target-net as a sub-graph for evaluation. However, this approach leads to a significant gap between the architectures used in search and evaluation, resulting in inconsistencies, inefficiencies, and inflexibility in the search process. To address this problem, we propose EnTranNAS, which consists of Engine-cells and Transit-cells. The Engine-cell is differentiable and used for architecture search, while the Transit-cell simply transits a sub-graph through architecture derivation. This significantly reduces the gap between the architectures in search and evaluation. Additionally, our method reduces memory and computation costs, leading to faster search processes.To optimize the search and make it more efficient, we introduce a feature sharing strategy. This strategy ensures a more balanced optimization and improves the efficiency of the search process. Furthermore, we develop an architecture derivation method that replaces the traditional hand-crafted rule. Our method enables differentiable sparsity, ensuring that the derived architecture remains equivalent to that of the Engine-cell. This improves the consistency between the search and evaluation processes. Importantly, our method also allows for more flexibility in the search for topology, where a node can be connected to prior nodes with any number of connections.We conducted experiments on CIFAR-10 and achieved an error rate of 2.22% with only 0.07 GPU-day. Moreover, we performed the search directly on ImageNet with learnable topology and achieved a top-1 error rate of 23.8% in 2.1 GPU-day.