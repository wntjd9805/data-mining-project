Current methods of video retrieval rely on the assumption that only a single caption is relevant to a query video, and vice versa. However, we argue that this assumption leads to inaccurate evaluations of retrieval models. To address this issue, we suggest shifting towards semantic similarity video retrieval. In this approach, multiple videos or captions can be considered equally relevant, and the ranking of retrieved videos or captions does not impact the reported performance of a method. We propose several ways to estimate semantic similarities in large-scale retrieval datasets without the need for additional annotations. We conducted our analysis on three widely used video retrieval datasets: MSR-VTT, YouCook2, and EPIC-KITCHENS.