Current evaluation metrics for learned models that provide overall scores on a test set are inadequate in revealing significant failure patterns across features and instances. We present a novel approach to identify and explain failures by detecting visual attributes that contribute to poor performance. Unlike previous methods that rely on crowdsourced labels for visual attributes, we utilize the representation of a separate robust model to extract interpretable features, which are then employed to identify failure modes. Additionally, we propose a visualization technique to enable humans to comprehend the meaning conveyed by these features and assess their comprehensibility. Through evaluation on the ImageNet dataset, we demonstrate that: (i) our proposed workflow effectively uncovers important failure modes, (ii) the visualization techniques aid human understanding of the extracted features, and (iii) the insights gained can aid engineers in error analysis and debugging.