This study addresses the difficulties in monocular 3D reconstruction of articulated objects, such as the lack of training data and the ill-posed nature of the problem. To overcome these challenges, the researchers propose using video self-supervision, which enforces consistency between consecutive 3D reconstructions through a motion-based cycle loss. This approach improves both optimization-based and learning-based 3D mesh reconstruction.Additionally, the researchers introduce an interpretable model of 3D template deformations that controls the surface of a 3D object by displacing a small number of local, learnable handles. They formulate this operation as a structured layer, incorporating mesh-laplacian regularization, and demonstrate that it can be trained end-to-end.To further enhance accuracy, the researchers introduce a per-sample numerical optimization approach that jointly optimizes mesh displacements and cameras within a video. This approach not only improves accuracy during training but also serves as a post-processing step during testing.Despite using only a small set of videos per object category for supervision, the proposed method achieves state-of-the-art reconstructions with diverse shapes, viewpoints, and textures for multiple articulated object categories. Supplementary materials, code, and videos related to this study can be found on the project page: https://fkokkinos.github.io/video_3d_reconstruction/.