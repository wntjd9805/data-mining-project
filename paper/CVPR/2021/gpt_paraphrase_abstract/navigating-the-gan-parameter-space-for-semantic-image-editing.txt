Generative Adversarial Networks (GANs) are essential for visual editing and are commonly used in image-to-image translation and restoration processes. They are particularly advantageous for controllable generation as their latent spaces offer various interpretable directions that are suitable for semantic editing. This allows for the creation of impressive visual effects that would not be possible without GANs.This research significantly expands the range of visual effects achievable with state-of-the-art models like StyleGAN2. Unlike previous works that primarily focus on manipulating latent codes, we explore interpretable directions in the space of the generator parameters. Through simple methods, we uncover a multitude of interpretable directions in this space that can be used for non-trivial semantic manipulations. These manipulations cannot be achieved by transforming latent codes alone and can be applied to both synthetic and real images.To facilitate further advancements in GAN-based image editing, we provide our code and models as a useful resource.