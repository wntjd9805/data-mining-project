The potential to enhance the value of decision making algorithms lies in integrating interpretability without compromising their prediction accuracy. Instead of directly labeling an image, we propose a method that learns iterative binary sub-decisions, resulting in a decision making process that is sparse and transparent. Our model's key feature is its ability to construct a decision tree, with its structure embedded in the memory representation of a Recurrent Neural Network. This network is jointly learned by two models that communicate through message passing. Additionally, our model assigns a semantic meaning to each decision by representing them as binary attributes. This provides concise, semantic, and relevant explanations to the user. We evaluated our model on three image classification datasets, including the extensive ImageNet dataset, and found that it produces human interpretable binary decision sequences that explain the network's predictions while maintaining state-of-the-art accuracy.