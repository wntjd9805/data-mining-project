Data-free model compression methods have been developed to address the lack of training data in convolutional network compression. However, most existing algorithms are designed for image recognition or segmentation tasks, and not for single image super-resolution (SISR) tasks commonly used in mobile phones and smart cameras. In this study, we propose a data-free compression approach for SISR tasks. We analyze the relationship between the outputs and inputs from a pre-trained network and develop a generator with loss functions to capture useful information. The generator is trained to synthesize training samples that have a similar distribution to the original data. To overcome the training difficulty of the student network using only synthetic data, we introduce a progressive distillation scheme. Experimental results on different datasets and architectures demonstrate the effectiveness of our method in learning portable student networks without the original data, with a 0.16dB PSNR drop on Set5 for 2 super resolution. The code for our method will be available at https://github.com/huawei-noah/Data-Efficient-Model-Compression.