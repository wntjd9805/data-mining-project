The task of 3D object detection in point clouds is challenging and has various applications in understanding the 3D visual world. Current research focuses on using end-to-end trainable Hough voting for generating object proposals. However, this strategy only receives partial votes from potential object surfaces and outlier votes from cluttered backgrounds, limiting the utilization of information from input point clouds. To address this, we propose a new method called Back-tracing Representative Points Network (BRNet). BRNet uses a back-tracing strategy to generatively trace representative points from vote centers and revisit complementary seed points around these generated points. This approach helps capture fine local structural features around potential objects in raw point clouds. By enforcing mutual consistency between predicted vote centers and raw surface points, BRNet achieves more reliable and flexible object localization and class prediction results. Our BRNet outperforms state-of-the-art methods on two large-scale point cloud datasets, ScanNet V2 (+7.5% mAP@0.50) and SUN RGB-D (+4.7% mAP@0.50), while remaining lightweight and efficient.