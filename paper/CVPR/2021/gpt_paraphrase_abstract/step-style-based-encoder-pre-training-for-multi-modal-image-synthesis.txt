We present a new method for multi-modal image-to-image translation that addresses the challenge of mapping between input and output domains. Existing approaches use complex training objectives to learn a latent embedding that captures the variability of the output domain. In contrast, we directly model the style variability of images, independent of the image synthesis task. Our approach involves pre-training a generic style encoder using a novel proxy task, which learns an embedding of images into a low-dimensional style latent space. This learned latent space offers several advantages compared to traditional approaches. It is not dependent on the target dataset and can generalize well across multiple domains. It also produces a more powerful and expressive latent space, resulting in improved style capture and transfer. Additionally, our style pre-training simplifies the training objective and significantly speeds up the training process. We conduct a detailed study of the different loss terms contributing to multi-modal image-to-image translation and propose a simple alternative to variational autoencoders (VAEs) for sampling from unconstrained latent spaces. Finally, our approach achieves state-of-the-art results on six challenging benchmarks using a training objective that only includes a GAN loss and a reconstruction loss.