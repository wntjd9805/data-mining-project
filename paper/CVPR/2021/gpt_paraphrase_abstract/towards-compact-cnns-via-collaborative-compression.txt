Channel pruning and tensor decomposition are popular techniques for compressing convolutional neural networks. However, when used independently, they often result in a significant decrease in accuracy at high compression rates. To address this issue, this paper proposes a Collaborative Compression (CC) scheme that combines channel pruning and tensor decomposition. This approach simultaneously learns the sparsity and low-rankness of the model. The paper first examines the compression sensitivity of each layer in the network and then introduces a Global Compression Rate Optimization method. This method transforms the compression rate decision into an optimization problem. Additionally, the paper proposes a multi-step heuristic compression technique that removes redundant compression units in a step-by-step manner. This technique takes into account the impact of the remaining compression space. The proposed CC scheme outperforms previous methods on various datasets and backbone architectures. For instance, on ImageNet 2012, the scheme achieves a 52.9% reduction in FLOPs (floating point operations) by removing 48.4% of parameters from ResNet-50, with only a 0.56% drop in Top-1 accuracy.