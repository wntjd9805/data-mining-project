Few-shot learning is a difficult task where the goal is to train a classifier for new classes with only a few examples. Previous methods have used pre-training and fine-tuning to address this problem. However, we have identified that the fine-tuning step is not very effective because the pre-trained feature space already has compact clusters for base classes, while novel classes have larger variances. In this paper, we propose a new approach that focuses on estimating more representative prototypes during meta-learning, instead of fine-tuning the feature extractor. Our method involves introducing primitive knowledge and using it to extract representative attribute features. We then use a prototype completion network to learn to complete prototypes based on these features. To address errors caused by primitive knowledge noises or class differences, we develop a Gaussian-based prototype fusion strategy that combines mean-based and completed prototypes using unlabeled samples. Experimental results show that our method achieves more accurate prototypes and outperforms state-of-the-art techniques in terms of classification accuracy by 2% to 9%. The code for our method is available online.