In recent years, there has been a growing interest in learning-based video compression. Previous methods for hybrid coding have relied on pixel space operations to reduce spatial and temporal redundancy. However, these approaches often suffer from inaccurate motion estimation and less effective motion compensation. To address this, we propose a feature-space video coding network (FVC) that performs all major operations in the feature space.Our approach involves several key steps. First, in the deformable compensation module, we use motion estimation in the feature space to generate motion information, which is then compressed using an auto-encoder style network. Next, we employ deformable convolution for motion compensation and generate a predicted feature. We then compress the residual feature between the current frame's feature and the predicted feature from our deformable compensation module.To improve frame reconstruction, we utilize the non-local attention mechanism in the multi-frame feature fusion module to fuse reference features from multiple previously reconstructed frames. This helps to enhance the overall quality of the reconstructed frames.We conducted comprehensive experiments and evaluated our proposed framework on four benchmark datasets, including HEVC, UVG, VTL, and MCL-JCV. The results demonstrate that our approach achieves state-of-the-art performance in terms of video compression.Overall, our feature-space video coding network offers a more accurate and effective approach to reduce redundancy and improve video compression compared to previous methods that rely on pixel space operations.