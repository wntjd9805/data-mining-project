The use of perceptual loss in image synthesis tasks, such as image super-resolution and style transfer, has been widely adopted. It has been believed that the success of perceptual loss lies in the high-level features extracted from pretrained CNNs. However, our research reveals that the network structure itself, rather than the trained weights, is what matters. We demonstrate that a randomly-weighted deep CNN can capture the dependencies between different levels of variable statistics, eliminating the need for pre-training and a specific network structure like VGG. This insight expands the range of applications for perceptual loss. We present improved results in tasks like semantic segmentation, depth estimation, and instance segmentation by using the extended randomized perceptual loss instead of pixel-wise loss alone. Our goal is to offer a simple and versatile structured-output loss that can be applied to various structured output learning tasks.