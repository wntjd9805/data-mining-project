Deep learning models often experience catastrophic forgetting when trained in an incremental learning setting. This work proposes a novel solution to address the problem of incremental learning, which involves training a model on new tasks that arrive sequentially. The challenge becomes even greater when the test set contains classes that were not part of the training set, resulting in a task incremental generalized zero-shot learning problem. The proposed approach can be applied to both zero-shot and non-zero shot task incremental learning settings. It utilizes weight rectifications and affine transformations to adapt the model to different tasks that arrive in sequence. Specifically, the network weights are rectified to accommodate new tasks by correcting the weights learned from the previous task. These weight rectifications are achieved with a small number of parameters. Additionally, affine transformations are learned for the network outputs to better adapt them to the new task. The approach is evaluated on various datasets in both zero-shot and non-zero shot task incremental learning settings, demonstrating state-of-the-art performance. It outperforms the current state-of-the-art non-zero shot task incremental learning method by more than 5% on the CIFAR-100 dataset. Furthermore, it significantly outperforms the state-of-the-art task incremental generalized zero-shot learning method, achieving absolute margins of 6.91% and 6.33% on the AWA1 and CUB datasets respectively. The effectiveness of the proposed approach is verified through various ablation studies.