This study examines different strategies for scaling convolutional neural networks, which involves increasing their computational complexity and representational power. These strategies may involve increasing the model's width, depth, resolution, and other factors. While there are various scaling strategies available, their tradeoffs are not fully understood. Existing analysis mainly focuses on the relationship between accuracy and floating point operations (flops). However, our experiments reveal that different scaling strategies have varying effects on model parameters, activations, and runtime. Surprisingly, numerous scaling strategies can result in networks with similar accuracy but with significantly different properties. Based on our findings, we propose a fast compound scaling strategy that primarily scales the model's width, with depth and resolution being scaled to a lesser extent. Unlike popular scaling strategies that lead to a linear increase in model activation in relation to scaling flops, our proposed approach results in a square root increase in activations while maintaining excellent accuracy. This reduction in activations can lead to faster performance on memory-bandwidth limited hardware such as GPUs. Overall, we aim to provide a framework for analyzing scaling strategies under different computational constraints.