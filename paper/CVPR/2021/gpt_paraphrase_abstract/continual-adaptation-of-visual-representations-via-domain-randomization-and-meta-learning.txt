Conventional learning methods often produce fragile models that struggle to adapt when trained on different types of data, leading to the problem of catastrophic forgetting. This occurs when a model trained on multiple visual domains tends to forget the knowledge acquired from previous domains in favor of the most recent ones. To address this issue, we propose the use of domain randomization, where the distribution of the current domain is randomized through image manipulations. We demonstrate that this approach leads to more robust models that are less susceptible to catastrophic forgetting. Additionally, we introduce a meta-learning strategy that incorporates a regularizer to penalize any loss associated with transferring the model from the current domain to different "auxiliary" meta-domains. These meta-domains are also generated through randomized image manipulations, facilitating adaptation to new domains. Through various experiments, including classification and semantic segmentation tasks, we empirically validate that our approach effectively reduces catastrophic forgetting when models are transferred to new domains.