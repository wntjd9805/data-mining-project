Recent research in audio-visual navigation has focused on scenarios where the target produces a continuous sound, with audio serving mainly to indicate the target's position. In this study, we present a new approach called semantic audio-visual navigation, where objects in the environment emit sounds that are consistent with their semantic meaning (e.g., a toilet flushing or a door creaking), and these acoustic events are sporadic and brief. To address this novel task, we propose a transformer-based model that incorporates an inferred goal descriptor, which captures both the spatial and semantic properties of the target. Our model's persistent multimodal memory allows it to successfully reach the goal even after the sound event has ceased. To support this task, we expand the SoundSpaces audio simulations to include semantically grounded sounds for various objects in Matterport3D. Through our method, we significantly outperform existing audio-visual navigation methods by learning to associate semantic, acoustic, and visual cues.