Adverse weather conditions, such as snow, rain, and fog, present significant challenges for both human and computer vision. These conditions are particularly problematic for autonomous vehicles, robotics, and drones, as they rely on accurate perception for safe decision making. However, most existing supervised imaging and vision approaches are trained on data collected in good weather conditions, making them ill-equipped to handle adverse conditions. As a result, autonomous vehicles often struggle in dense fog or snow and may even come to a halt. To address the lack of supervised training data for adverse weather conditions, we propose a novel approach called ZeroScatter. This method combines synthetic and indirect supervision to convert RGB-only captures taken in adverse weather into clear daytime scenes. ZeroScatter leverages various cues, including model-based, temporal, multi-view, multi-modal, and adversarial cues, in a joint manner. By training on unpaired and biased data, we overcome the limitations of existing approaches. We evaluate the effectiveness of ZeroScatter on real-world captures and compare it to existing monocular descattering methods. Our results show that ZeroScatter outperforms these methods by 2.8 dB PSNR (Peak Signal-to-Noise Ratio) in controlled fog chamber measurements. This demonstrates the superiority of our approach in improving visibility and reducing scattering effects caused by adverse weather conditions.