This study introduces a new approach called architecture generator for efficient and flexible sub-network searching in one-shot NAS. Traditional methods require multiple searches for different hardware constraints, resulting in high search costs. However, the proposed architecture generator allows for the generation of multiple sub-networks with just one forward pass, eliminating the need for repeated searches and supernet retraining. Additionally, a unified supernet called SGNAS is introduced to further enhance search efficiency and reduce GPU memory consumption. With the trained architecture generator and SGNAS, the proposed framework achieves impressive results, with a search time of only 5 GPU hours for N different hardware constraints, which is 4N times faster than previous methods. After training from scratch, SGNAS achieves a top1-accuracy of 77.1% on ImageNet, comparable to state-of-the-art approaches. The code for this framework is available at the provided GitHub link.