This paper focuses on unsupervised action segmentation. Previous studies have used a feature embedding to capture the temporal structure of videos at the frame level. In this paper, we propose a new self-supervised learning approach that considers both frame-level and action-level structures in videos. Our method trains a recurrent neural network (RNN) to recognize positive and negative action sequences, and uses the RNN's hidden layer as our new action-level feature embedding. The positive and negative sequences are created by sampling action segments from videos, where the positive sequences maintain the original time ordering and the negative sequences are shuffled. Since supervision of actions is not available, we introduce a hidden Markov model (HMM) that explicitly models action lengths and infer a maximum a posteriori (MAP) action segmentation using the Viterbi algorithm. This action segmentation is then used as pseudo-ground truth to estimate our action-level feature embedding and update the HMM. We repeat these steps within the Generalized EM framework to ensure convergence. Our evaluation on the Breakfast, YouTube Instructions, and 50Salads datasets demonstrates that our method outperforms the state-of-the-art approaches.