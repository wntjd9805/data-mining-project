Creating realistic renderings of moving humans is crucial for various applications such as telepresence systems, virtual shopping, movie special effects, and interactive experiences like games. Recent advancements in neural rendering techniques have allowed for the development of highly detailed models of humans and objects. However, some of these methods fail to generate realistic results for animated human models, while others have long rendering times. To address these limitations, we propose a new 3D representation that combines the strengths of previous methods to produce faster and higher-quality results. Our representation combines a coarse grid of animation codes with a learned scene function, resulting in a seamless combination of discrete and continuous volumetric representations. By employing differentiable volume rendering, we are able to generate photorealistic views of the human head and upper body, as well as train our representation using only 2D supervision. Furthermore, our approach allows for the synthesis of new facial expressions based on a global animation code. Our method achieves state-of-the-art results for generating novel views of dynamic human heads and upper bodies. For more detailed results, please refer to our project page.