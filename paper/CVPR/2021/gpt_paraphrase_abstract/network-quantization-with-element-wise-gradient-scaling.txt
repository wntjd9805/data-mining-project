Network quantization is an important technique for implementing deep neural networks with limited hardware resources. Most existing methods use the straight-through estimator (STE) to train quantized networks, which replaces the derivative of a discretizer with that of an identity function to avoid a zero-gradient problem. However, the STE does not consider the errors caused by discretization between inputs and outputs of the discretizer. In this paper, we propose an alternative method called element-wise gradient scaling (EWGS) that improves the stability and accuracy of training quantized networks compared to the STE. EWGS scales each gradient element based on the sign of the element and the error between the continuous input and discrete output of the discretizer. We adaptively adjust the scaling factor using Hessian information of the network. Extensive experiments on image classification datasets, including CIFAR-10 and ImageNet, using various network architectures and bit-width settings, demonstrate the effectiveness of our method.