The aim of this study is to explore the transfer of knowledge across different modalities, such as vision and audio, even when these modalities are not semantically correlated. Instead of directly aligning the representations of different modalities, the researchers propose to compose audio, image, and video representations to uncover richer multi-modal knowledge. The main idea is to learn a compositional embedding that bridges the cross-modal semantic gap and captures task-relevant semantics. This is achieved through compositional contrastive learning, which facilitates the integration of representations across modalities. The researchers introduce a new multi-modal distillation benchmark using three video datasets: UCF101, ActivityNet, and VG-GSound. They also demonstrate that their model outperforms existing knowledge distillation methods in transferring audio-visual knowledge to improve video representation learning. The code for this study is available at https://github.com/yanbeic/CCL.