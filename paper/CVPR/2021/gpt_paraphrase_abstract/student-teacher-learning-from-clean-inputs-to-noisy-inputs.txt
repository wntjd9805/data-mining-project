The successful transfer of knowledge from a pre-trained teacher network to a student network can be achieved through feature-based student-teacher learning. This method involves the student network mimicking the hidden features of the teacher network. Recent empirical evidence has shown that the teacher's features can enhance the student network's ability to generalize, even when the input data is corrupted by noise. However, there is a lack of theoretical understanding regarding the conditions under which this knowledge transfer method can be successful for tasks that are dissimilar. To address this gap, we conducted both theoretical analyses using deep linear networks and experimental studies using nonlinear networks. Our findings highlight three crucial factors that determine the success of this method: (1) whether the student network is trained to achieve zero training loss, (2) the level of expertise possessed by the teacher network in solving clean-input problems, and (3) how the teacher network decomposes its knowledge into hidden features. Failure to properly control any of these factors will result in the failure of the student-teacher learning method.