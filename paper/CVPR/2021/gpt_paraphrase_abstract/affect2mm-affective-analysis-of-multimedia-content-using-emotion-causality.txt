We introduce Affect2MM, a technique for predicting emotions in multimedia content over time. Our aim is to automatically capture the changing emotions portrayed by characters in real-life human-centered situations and behaviors. By incorporating concepts from emotion causation theories, we computationally model and determine the emotional state evoked in movie clips. Affect2MM employs attention-based methods and Granger causality to explicitly model temporal causality. To gain a comprehensive understanding of the scene, we utilize various elements such as facial features of actors, scene interpretation, visual aesthetics, action/situation description, and movie script to acquire a representation rich in emotions. For emotion perception, we employ an LSTM-based learning model. To assess our approach, we evaluate and compare its performance on three datasets: SENDv1, MovieGraphs, and the LIRIS-ACCEDE dataset. Our analysis reveals an average performance improvement of 10-15% across all three datasets compared to state-of-the-art methods.