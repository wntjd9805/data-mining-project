Most current self-supervised learning methods focus on image classification and may not perform well on dense prediction tasks that require pixel-level accuracy. In order to address this issue, we propose a dense self-supervised learning method called DenseCL. This method operates at the pixel level and takes into consideration the correspondence between local features. DenseCL utilizes a pairwise contrastive loss to optimize the (dis)similarity between two views of input images. Compared to the baseline method MoCo-v2, DenseCL introduces minimal computation overhead and consistently outperforms existing methods in various dense prediction tasks such as object detection, semantic segmentation, and instance segmentation. In comparison to MoCo-v2, DenseCL achieves significant improvements in performance, including 2.0% AP on PASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO instance segmentation, 3.0% mIoU on PASCAL VOC semantic segmentation, and 1.8% mIoU on Cityscapes semantic segmentation. The code and models for DenseCL are available at: https://git.io/DenseCL.