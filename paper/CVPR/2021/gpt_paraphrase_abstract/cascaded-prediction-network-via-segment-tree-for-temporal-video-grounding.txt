The goal of temporal video grounding is to identify the specific segment in an untrimmed video that corresponds to a given sentence. Existing methods for this task can be categorized into proposal-based and proposal-free approaches. However, proposal-based methods are burdened by the need to generate proposals and lack flexibility in determining precise boundaries, while proposal-free methods struggle with accurately determining the start and end timestamps. In this study, we propose a new approach called Cascaded Prediction Network (CPN) that addresses these limitations by treating the task as a multi-step decision problem. The CPN first encodes both the video and the query into a common latent space and combines them into integrated representations. Then, a segment-tree-based structure is constructed, and predictions are made through decision navigation and signal decomposition in a cascaded manner. We evaluate the performance of our proposed method on three widely used benchmarks (ActivityNet Caption, Charades-STA, and TACoS) and demonstrate that CPN outperforms existing state-of-the-art methods.