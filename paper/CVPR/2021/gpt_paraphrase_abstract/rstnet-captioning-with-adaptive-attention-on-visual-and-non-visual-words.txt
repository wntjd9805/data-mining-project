Recent advancements in visual question answering have focused on the effectiveness of grid features for tasks involving vision and language. At the same time, transformer-based models have demonstrated exceptional performance in various sequence prediction problems. However, the flattening operation used with grid features leads to a loss of spatial information, and the transformer model lacks the ability to distinguish between visual and non-visual words. These issues remain largely unaddressed.To address these shortcomings, this paper introduces the Grid-Augmented (GA) module, which incorporates relative geometry features between grids to improve visual representations. Additionally, a BERT-based language model is employed to extract language context, and an Adaptive-Attention (AA) module is introduced on top of a transformer decoder. This AA module adaptively measures the contribution of visual and language cues to inform word prediction decisions.To demonstrate the versatility of these proposals, the two modules are applied to the vanilla transformer model to create the Relationship-Sensitive Transformer (RSTNet) for the image captioning task. The proposed model is evaluated on the MSCOCO benchmark, where it achieves new state-of-the-art results on both the Karpathy test split and the online test server.The abstract concludes by mentioning that the source code for the proposed model is available on GitHub. Furthermore, it states that the answer format only provides the abstraction of the paper.