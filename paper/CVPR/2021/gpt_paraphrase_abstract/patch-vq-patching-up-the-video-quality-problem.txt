No-reference (NR) perceptual video quality assessment (VQA) is a challenging and significant issue in social and streaming media applications. Accurate and efficient video quality predictors are necessary to monitor and guide the processing of user-generated content (UGC), which is often imperfect and shared in large quantities. However, current NR models have limitations in predicting video quality in real-world UGC data. To address this problem, we have created the largest subjective video quality dataset, consisting of 38,811 real-world distorted videos, 116,433 space-time localized video patches (v-patches), and 5.5 million human perceptual quality annotations. Based on this dataset, we have developed two unique NR-VQA models: (a) PVQ, a local-to-global region-based architecture that predicts global video quality and achieves state-of-the-art performance on three UGC datasets, and (b) PVQ Mapper, a novel space-time video quality mapping engine that helps identify and visualize perceptual distortions in space and time. The entire dataset and prediction models are freely accessible at https://live.ece.utexas.edu/research.php.