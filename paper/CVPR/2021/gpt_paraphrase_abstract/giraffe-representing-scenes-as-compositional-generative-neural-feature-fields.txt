Deep generative models are capable of creating realistic images, but they lack control over the content. Most existing works focus on disentangling factors of variation in two-dimensional (2D) data, ignoring the three-dimensional (3D) nature of the world. Additionally, few studies consider the compositional aspects of scenes. We propose that incorporating a compositional 3D scene representation into the generative model can enhance controllable image synthesis. By representing scenes as compositional generative neural feature fields, we can disentangle objects from the background, as well as the shapes and appearances of individual objects. This can be achieved by learning from unstructured and unposed image collections without additional supervision. Combining this scene representation with a neural rendering pipeline allows for fast and realistic image synthesis. Our experiments demonstrate that our model successfully disentangles individual objects and enables translation, rotation, and changes in camera pose within the scene.