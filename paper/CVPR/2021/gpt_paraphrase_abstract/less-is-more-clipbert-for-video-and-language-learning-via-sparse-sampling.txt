The traditional method of video-and-language learning involves using neural models to learn from pre-extracted dense video features and text features. However, these features are trained independently and on different tasks, making them less effective for downstream tasks. Additionally, the computational burden of dense video features makes it difficult to integrate them into existing approaches. To address this issue, we propose CLIPBERT, a framework that enables affordable end-to-end learning for video-and-language tasks. CLIPBERT uses sparse sampling, where only a few short clips from a video are used during training. Our experiments on text-to-video retrieval and video question answering show that CLIPBERT outperforms existing methods that use full-length videos. This suggests that learning with sparsely sampled clips is often more accurate than using densely extracted features from full-length videos. We conduct comprehensive ablation studies and analyses to identify the factors contributing to this success. Our approach shows generalization ability across different domains and video lengths. The code for CLIPBERT is publicly available.