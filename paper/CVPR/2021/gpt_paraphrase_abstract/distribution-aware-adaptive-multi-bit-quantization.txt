This paper examines the compression of deep neural networks by quantizing the weights and activations into multi-bit binary networks (MBNs). The authors propose a distribution-aware multi-bit quantization (DMBQ) method that incorporates the distribution prior into the optimization process. Instead of solving the optimization in each iteration, DMBQ searches for the optimal quantization scheme over the distribution space beforehand and selects the quantization scheme during training using a fast lookup table-based strategy. Building upon DMBQ, the authors introduce loss-guided bit-width allocation (LBA) to adaptively quantize and potentially prune the neural network. They utilize the first-order Taylor expansion to create a metric for evaluating the loss sensitivity of each channel's quantization and automatically adjust the bit-width of weights and activations on a channel-wise basis. The authors extend their method to image classification tasks and experimental results demonstrate that their approach surpasses state-of-the-art quantized networks in terms of accuracy. Additionally, their method is more efficient in terms of training time compared to state-of-the-art MBNs, even for cases of extremely low bit width (below 1-bit) quantization.