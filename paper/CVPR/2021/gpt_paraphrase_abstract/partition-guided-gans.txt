Despite the success of Generative Adversarial Networks (GANs), their training is plagued by well-known issues such as mode collapse and difficulties in learning disconnected sets of manifolds. This paper aims to address the challenge of learning complex high-dimensional distributions by breaking it down into simpler sub-tasks. The proposed solution involves designing a partitioner that divides the space into smaller regions, each with a simpler distribution, and training a different generator for each partition. This partitioning is done in an unsupervised manner, without the need for labels. Two desired criteria for the partitioner are formulated: producing connected partitions and providing a measure of distance between partitions and data samples, along with a direction for reducing that distance. These criteria are developed to avoid generating samples from regions with low data density and to provide additional guidance for the generators during training. The paper establishes theoretical constraints for a space partitioner to satisfy these criteria. Based on the theoretical analysis, an effective neural architecture for the partitioner is designed, which empirically meets these conditions. Experimental results on standard benchmarks demonstrate that the proposed unsupervised model outperforms several recent methods.