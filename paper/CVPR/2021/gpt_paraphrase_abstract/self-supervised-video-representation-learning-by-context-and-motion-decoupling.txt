A major challenge in self-supervised video representation learning is effectively capturing motion information while avoiding context bias. Existing methods address this issue through video-specific pretext tasks, such as predicting clip orders, time arrows, and paces. In contrast, we propose a method that explicitly separates motion supervision from context bias by utilizing key frames and motion vectors in compressed videos as supervision sources. These sources can be efficiently extracted at high frame rates on a CPU. We introduce two jointly optimized pretext tasks: a context matching task that compares video clip and key frame features using a pairwise contrastive loss, and a motion prediction task that estimates motion features in the near future by passing clip features through an encoder-decoder network. These tasks share a video backbone but have separate MLP heads. Experimental results demonstrate that our approach improves the quality of learned video representation compared to previous methods, achieving absolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and HMDB51, respectively. Additionally, we find that motion prediction serves as a strong regularization for video networks, as using it as an auxiliary task enhances action recognition accuracy by a margin of 7.4% to 13.8%.