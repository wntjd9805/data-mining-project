Self-attention has been successfully utilized in video representation learning to capture long-range dependencies. However, existing methods focus on computing pairwise correlations simultaneously across spatial and temporal dimensions, disregarding the distinct contextual information conveyed by spatial and temporal correlations. It is intuitive that learning spatial contextual information first would enhance temporal modeling. To address this, we propose a separable self-attention (SSA) module that sequentially models spatial and temporal correlations, allowing for efficient utilization of spatial contexts in temporal modeling. By incorporating the SSA module into a 2D CNN, we introduce the SSA network (SSAN) for video representation learning. Our approach outperforms state-of-the-art methods on the Something-Something and Kinetics-400 datasets for video action recognition. Remarkably, our models achieve superior performance compared to shallower networks with fewer modalities. Additionally, we demonstrate the semantic learning capability of our method in the visual-language task of video retrieval, where SSA significantly improves the state-of-the-art performance on the MSR-VTT and Youcook2 datasets. This highlights the effectiveness of our approach in generating homogeneous video representations and text embeddings.