Batch Normalization (BN) and its variations have been highly effective in addressing the issue of covariate shift in deep learning. These techniques normalize the distribution of features by standardizing them using batch statistics. However, they fail to correct for the impact of extraneous variables or multiple distributions on features. These additional variables, referred to as metadata, can introduce bias or confounding effects in the classification process (e.g., the influence of race when classifying gender from face images).  To overcome this limitation, we propose a new batch-level operation called the Metadata Normalization (MDN) layer. This layer can be seamlessly integrated into the training framework to mitigate the influence of metadata on the feature distribution. MDN employs a regression analysis technique commonly used for preprocessing to eliminate the effects of metadata on model features during training.  To evaluate the effectiveness of our approach, we employ a distance correlation metric to quantify the bias in the feature distribution caused by metadata. We demonstrate the success of our method in removing metadata effects in four diverse scenarios: a synthetic dataset, a 2D image dataset, a video dataset, and a 3D medical image dataset.  In summary, our research introduces the Metadata Normalization (MDN) layer, a novel batch-level operation that effectively addresses the influence of extraneous variables or multiple distributions on feature distribution. Our method successfully removes metadata effects in various settings, highlighting its potential for improving the performance and robustness of deep learning models.