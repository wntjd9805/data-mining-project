Recent advancements in self-supervised contrastive learning have proven to be effective in unsupervised person re-identification (ReID) tasks. By learning invariance from diverse views of an input, these methods have shown promising results. In this study, we propose a novel approach by integrating a Generative Adversarial Network (GAN) and a contrastive learning module into a single joint training framework. The GAN serves as an online data augmentation tool for contrastive learning, while the contrastive module focuses on learning view-invariant features for generation. To achieve this, we introduce a mesh-based view generator that utilizes mesh projections as references to generate new views of a person. Furthermore, we introduce a view-invariant loss to facilitate the contrastive learning process between the original and generated views. Unlike previous GAN-based unsupervised ReID methods that rely on labeled source datasets for domain adaptation, our approach is more flexible as it does not require such labeled data. Extensive experiments conducted on several large-scale ReID datasets demonstrate the superiority of our method over state-of-the-art techniques in both fully unsupervised and unsupervised domain adaptive settings. The source code and models for our method are publicly available at https://github.com/chenhao2345/GCL.