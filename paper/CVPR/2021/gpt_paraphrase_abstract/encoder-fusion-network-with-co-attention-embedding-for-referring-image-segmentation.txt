Decoder fusion for referring image segmentation has gained significant attention recently. Existing methods focus on fusing language and vision during the decoding process, with linguistic and visual features interacting separately at each scale. However, this approach fails to consider the continuous guidance of language on multi-scale visual features. To address this limitation, we introduce an encoder fusion network (EFN) that transforms the visual encoder into a multi-modal feature learning network. The EFN progressively refines the multi-modal features using language. Additionally, we incorporate a co-attention mechanism into the EFN to enable parallel updates of the multi-modal features. This mechanism enhances the consistency of cross-modal information representation in the semantic space. To further improve the network's performance, we propose a boundary enhancement module (BEM) that focuses on the fine structure. Experimental results on four benchmark datasets demonstrate that our proposed approach achieves state-of-the-art performance across various evaluation metrics without requiring any post-processing.