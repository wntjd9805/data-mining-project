Realistic simulators play a crucial role in training and verifying robotics systems. Traditionally, these simulators have been manually created, but a more scalable approach is to utilize machine learning to learn how the environment behaves based on data. This research aims to develop a dynamic environment simulator in pixel-space by observing unannotated sequences of frames and associated actions. The proposed neural simulator, called DriveGAN, is unique in its ability to control different aspects of the environment without supervision. It not only includes steering controls but also features for sampling scene attributes like weather and the location of non-player objects. DriveGAN is fully differentiable, allowing for the re-simulation of recorded video sequences, enabling agents to drive through the scene again and make different decisions. The training of DriveGAN involves multiple datasets, including 160 hours of real-world driving data. The results demonstrate that DriveGAN outperforms previous data-driven simulators and introduces new features that were previously unexplored.