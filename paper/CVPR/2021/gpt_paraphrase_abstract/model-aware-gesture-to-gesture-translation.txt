Hand gesture-to-gesture translation is an important problem with various applications, like sign language production. Existing approaches use a data-driven approach based on sparse 2D joint representation, but this often leads to blurry results with incorrect structure. In this study, we propose a new model-aware framework for gesture-to-gesture translation that utilizes hand meshes as an intermediate representation. We create a dense topology map aligning the image plane with the encoded embedding of the hand mesh, and calculate a transformation flow based on the correspondence of the source and target topology map. During the generation stage, we incorporate topology information by modulating activations in a spatially-adaptive way. Additionally, we enhance the translated gesture image using the source's local characteristics according to the transformation flow. Our method achieves state-of-the-art performance on two benchmark datasets, as demonstrated by extensive experiments.