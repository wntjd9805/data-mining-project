The understanding of human movement and its semantics is a significant challenge that requires datasets with semantic labels. Current datasets either have numerous action labels but lack accurate 3D human motion data, or have precise body motions but only cover a limited number of actions. To address this, we introduce BABEL, a large dataset that includes language labels describing actions in motion-capture (mocap) sequences. BABEL consists of over 43 hours of mocap sequences with language labels for more than 250 unique actions. Each action label is precisely aligned with the corresponding action's duration in the mocap sequence. BABEL also allows for the overlap of multiple actions, resulting in over 66,000 action segments. These dense annotations can be used for action recognition, temporal localization, motion synthesis, and other tasks. We evaluate the performance of models on 3D action recognition using BABEL as a benchmark and demonstrate that it presents interesting challenges applicable to real-world scenarios. BABEL can serve as a valuable benchmark for progress in 3D action recognition and is available for academic research purposes at https://babel.is.tue.mpg.de/.