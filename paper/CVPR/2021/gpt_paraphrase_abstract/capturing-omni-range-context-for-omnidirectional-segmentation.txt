Convolutional Networks (ConvNets) have proven to be highly effective in semantic segmentation tasks, particularly in autonomous driving applications. However, when it comes to processing omnidirectional images from cameras that provide a comprehensive view of street scenes, the performance of standard segmentation models designed for narrow Field of View (FoV) images drops significantly, by approximately 30.0% in terms of mean Intersection over Union (mIoU) on established test datasets.To address this performance gap between narrow FoV and omnidirectional perception, we propose a novel approach called Efficient Concurrent Attention Networks (ECANets). These networks directly capture the long-range dependencies present in omnidirectional imagery, enabling them to better handle the unique characteristics of 360◦ images. Our approach incorporates learned attention-based contextual priors that can encompass the entire 360◦ field of view. In addition, we enhance the training process of our models by leveraging both densely labeled and unlabeled data from multiple datasets. This multi-source and omni-supervised learning strategy allows us to take advantage of a wider range of data sources, further improving the segmentation performance on panoramic images.To evaluate the effectiveness of our approach, we introduce and extensively evaluate our models on the Wild PAnoramic Semantic Segmentation (WildPASS) dataset, which is designed to capture diverse scenes from around the world. Our novel model architecture, training regimen, and multi-source prediction fusion techniques achieve new state-of-the-art results on both the public PASS benchmark (with a mIoU of 60.2%) and the recently introduced WildPASS benchmark (with a mIoU of 69.0%).Overall, our ECANets approach addresses the limitations of existing segmentation models when applied to omnidirectional imagery, and achieves significant performance improvements in panoramic image segmentation tasks.