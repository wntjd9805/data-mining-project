Supervised learning relies on the assumption that the training and test data come from the same distribution, but this assumption is often not valid due to distribution shift. Existing methods for unsupervised domain adaptation focus on achieving domain-invariant representations and minimizing source domain error. However, recent research has shown that this is not enough to ensure good generalization on the target domain, especially when there is a shift in label distribution. In real-world applications, it is possible to obtain a small amount of labeled data from the target domain, which can be used to improve model training with the source data.Based on these observations, this paper proposes a novel method for semi-supervised domain adaptation (Semi-DA) that simultaneously learns invariant representations and risks. The authors provide a finite sample bound for both classification and regression problems under Semi-DA, which suggests a principled approach to achieve target generalization by aligning the marginal and conditional distributions across domains in the feature space. Building on this, they introduce the LIRR algorithm that jointly learns invariant representations and risks.Extensive experiments are conducted on classification and regression tasks to evaluate the performance of LIRR. The results consistently show that LIRR outperforms existing methods that only focus on learning invariant representations or invariant risks. The code for LIRR is available on Github.