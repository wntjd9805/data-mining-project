Knowledge distillation is a technique used to transfer knowledge from a teacher network to a student network in order to improve the performance of the student network. Previous methods have focused on transforming features and defining loss functions at the same level to enhance effectiveness. However, our study takes a different approach by examining the importance of connection paths between different levels of the teacher and student networks. We propose cross-stage connection paths, which are a novel mechanism in knowledge distillation. Our approach is effective and simple in structure, requiring minimal computational overhead. Furthermore, our nested and compact framework outperforms other methods across various tasks such as classification, object detection, and instance segmentation. These tasks demonstrate significant improvements in student network performance when our method is applied.