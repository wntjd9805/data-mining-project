Event cameras are a new type of vision sensors that capture brightness changes in a non-linear manner, allowing for low-latency and high temporal resolution. These changes, known as events, have proven to be valuable for high-speed motion estimation. However, there is also interest in reconstructing intensity frames from these events to connect with existing research in appearance- and frame-based computer vision. Previous approaches have used neural networks trained on synthetic data with ground-truth information. In this study, we present a novel approach to intensity reconstruction using self-supervised learning. By leveraging our understanding of event cameras, we combine estimated optical flow and event-based photometric constancy to train neural networks without the need for ground-truth or synthetic data. Our method achieves comparable performance to state-of-the-art approaches across multiple datasets. Additionally, we introduce a lightweight neural network for optical flow estimation that maintains high inference speed with only a slight decrease in performance.