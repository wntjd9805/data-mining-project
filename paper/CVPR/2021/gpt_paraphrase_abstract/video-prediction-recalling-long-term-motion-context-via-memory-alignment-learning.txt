Our research tackles the issue of predicting future frames by addressing long-term motion context problems. To achieve accurate predictions, it is necessary to determine the specific long-term motion context (e.g., walking or running) to which the input motion (e.g., leg movement) belongs. The challenges that arise when dealing with long-term motion context include: (i) predicting the context that naturally matches input sequences with limited dynamics, and (ii) predicting the context with high-dimensional complexity. To overcome these challenges, we propose a novel motion context-aware video prediction approach. To address bottleneck (i), we introduce a long-term motion context memory (LMC-Memory) with memory alignment learning. This memory alignment learning allows for the storage of long-term motion contexts in the memory and their matching with sequences that have limited dynamics. Consequently, the long-term context can be retrieved from the limited input sequence. Furthermore, to resolve bottleneck (ii), we propose memory query decomposition, which involves storing local motion contexts (i.e., low-dimensional dynamics) and retrieving the appropriate local context for each individual part of the input. This approach enhances the alignment effects of the memory. Experimental results demonstrate that our proposed method outperforms other sophisticated RNN-based methods, particularly in long-term scenarios. Additionally, we validate the effectiveness of our network designs through ablation studies and memory feature analysis. The source code for our work is availableâ€ .