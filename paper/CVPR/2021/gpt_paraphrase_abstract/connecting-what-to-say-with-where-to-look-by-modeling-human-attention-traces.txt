We present a unified framework that combines images, text, and human attention traces. Based on the Localized Narratives annotation framework, our approach pairs each word of a caption with a mouse trace segment. We introduce two new tasks: predicting a trace given an image and caption, and predicting a caption and trace given only an image. However, accurately grounding each word is difficult due to noisy human-provided traces and words that cannot be visually grounded. To address this challenge, we propose a novel model architecture that is jointly trained on controlled trace generation and controlled caption generation tasks. We also propose a local bipartite matching distance metric to evaluate the quality of generated traces, allowing comparison of traces of different lengths. Through extensive experiments, we demonstrate that our model is robust to imperfect training data and outperforms existing methods. Furthermore, we show that our model, when pre-trained on our proposed tasks, can also improve the performance of COCO's guided image captioning task. Our code and project page are publicly available.