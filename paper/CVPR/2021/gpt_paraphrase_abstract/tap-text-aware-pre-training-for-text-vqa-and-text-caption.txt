This paper introduces Text-Aware Pre-training (TAP) as a method for improving performance on Text-VQA and Text-Caption tasks. These tasks involve understanding and interpreting text in images for question answering and caption generation. Unlike previous pre-training methods that do not adequately consider scene text and its relationship with visual and text elements, TAP incorporates scene text obtained from OCR engines during pre-training. TAP utilizes three pre-training tasks, including masked language modeling, image-text matching, and relative position prediction, to facilitate better alignment among text, visual objects, and scene text. Even when trained on the same downstream task dataset, TAP already achieves a significant increase in absolute accuracy on the TextVQA dataset compared to a baseline model without TAP. To further enhance performance, the authors create a large-scale dataset called OCR-CC, which focuses on scene text-related image-text pairs and consists of 1.4 million images. Pre-training on this dataset leads to substantial improvements over the state-of-the-art on multiple tasks, including TextVQA, ST-VQA, and TextCaps. Overall, TAP proves to be an effective approach for incorporating scene text information and improving performance on text-related visual tasks.