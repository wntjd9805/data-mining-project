We propose a new method for generating new views and synthesizing time in dynamic scenes using a single monocular video with known camera poses. Our approach utilizes Neural Scene Flow Fields, a novel representation that captures appearance, geometry, and 3D scene motion as a time-varying continuous function. By optimizing this representation through a neural network, we are able to accurately fit the observed input views. Our method is capable of handling various challenging scenes, such as thin structures, view-dependent effects, and complex motion. Through a series of experiments, we demonstrate that our approach outperforms recent monocular view synthesis methods and provide qualitative results showcasing the effectiveness of our technique on real-world videos.