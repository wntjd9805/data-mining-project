This abstract discusses the limitations of current video-based human motion transfer methods, particularly in handling clothing dynamics and achieving high visual quality. The authors propose a new framework that addresses these limitations by generating human shape, structure, and appearance in three stages. They use recurrent deep neural networks to generate intermediate representations based on 2D poses and their temporal derivatives. This approach allows for more realistic and temporally consistent motion transfer, as well as artistic control over the results. Experimental results show significant improvement over the state-of-the-art in terms of video realism. The source code for the framework is available at the provided website.