Event cameras have several advantages over conventional cameras, such as high dynamic range and less motion blur. However, training event-based models is challenging due to the lack of large labeled data. Previous approaches have relied on labeled or pseudo-labeled datasets from active pixel sensor (APS) frames, but the quality of such datasets is not comparable to those based on canonical images.   To address this issue, this paper proposes a new approach called EvDistill. It involves training a student network on unlabeled and unpaired event data using knowledge distillation (KD) from a teacher network trained with large-scale labeled image data. To enable KD across the unpaired modalities, the paper introduces a bidirectional modality reconstruction (BMR) module that bridges both modalities and exploits them simultaneously for knowledge distillation. This module does not require additional computation during inference.   The BMR module is enhanced by incorporating end-tasks and KD losses in an end-to-end manner. Additionally, the paper leverages the structural similarities between the two modalities and adapts the knowledge by matching their distributions. To further improve the distillation, the paper introduces an affinity graph KD loss, which is not commonly used in prior feature KD methods and is specifically designed for this problem.   Extensive experiments on semantic segmentation and object recognition demonstrate that EvDistill outperforms previous approaches and KD using only events and APS frames.