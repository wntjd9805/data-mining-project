Recently, there have been many algorithms developed to address the issue of vision-language navigation (VLN), which involves guiding an agent to navigate 3D environments based on linguistic instructions. However, current VLN agents store their past experiences in recurrent networks without considering the layout of the environment or making long-term plans. To overcome these limitations, we propose a crucial architecture called Structured Scene Memory (SSM). SSM effectively memorizes the agent's observations during navigation and also serves as a representation of the environment, capturing both visual and geometric cues. It includes a collect-read controller that gathers information to support decision making and mimics iterative algorithms for long-range reasoning. Additionally, SSM provides a complete action space, encompassing all navigable places on the map, and introduces a navigation decision-making strategy based on frontier exploration to enable efficient and global planning. Our method achieves state-of-the-art performance on multiple metrics, as demonstrated by experimentation on two VLN datasets (R2R and R4R).