Convolutional neural network (CNN) pruning has emerged as a highly effective method for compressing networks in recent years. Most existing approaches to network pruning focus on removing the least important filters to achieve a more compact architecture. However, this study argues that identifying and eliminating structural redundancy is actually more crucial than targeting unimportant filters, both theoretically and empirically. To support this claim, the researchers statistically model the network pruning problem from a redundancy reduction perspective. They discover that pruning the layer or layers with the most structural redundancy consistently produces better results than pruning the least important filters across all layers. Building on this insight, they propose a new network pruning approach that identifies the structural redundancy of a CNN and prunes filters in the selected layer or layers with the highest redundancy.The researchers conduct experiments on various benchmark network architectures and datasets to evaluate their proposed approach. The results demonstrate that their method significantly outperforms the previous state-of-the-art techniques. This finding highlights the importance of considering structural redundancy when pruning CNNs and suggests a promising direction for future network compression research.