We propose a novel approach called TesseTrack1 for 3D pose estimation and tracking of multiple people in various camera feeds. Our method simultaneously considers the 3D body joint reconstructions and associations of individuals in space and time within a single learnable framework. Our approach utilizes a spatio-temporal formulation that operates in a voxelized feature space derived from single or multiple camera views. After detecting individuals, a 4D convolutional neural network (CNN) generates person-specific representations, which are then linked across time using a differentiable matcher. These linked descriptions are merged and deconvolved to obtain 3D poses. Unlike previous strategies that treat different stages of pose estimation and tracking as independent sub-problems, our joint spatio-temporal formulation improves performance by considering them together. Additionally, TesseTrack is robust to changes in the number of camera views and achieves good results even with only a single view during inference. Our method outperforms state-of-the-art techniques in terms of 3D pose reconstruction accuracy on standard benchmarks. Furthermore, in our novel evaluation framework for multi-person articulated 3D pose tracking, TesseTrack demonstrates superior performance compared to strong baselines.