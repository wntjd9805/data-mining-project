A new model is proposed for generating complex images with multiple objects against a natural background, based on a given layout. Previous models in this area have made significant progress using generative adversarial networks (GANs), but they have two major limitations. Firstly, the relationships between objects and the background are often broken in the generated images. Secondly, the appearance of each object is distorted and lacks the defining characteristics associated with its class. These limitations are attributed to the lack of context-aware object and background feature encoding in the generators, as well as location-sensitive appearance representation in the discriminators. To overcome these limitations, two new modules are introduced. The first module is a context-aware feature transformation module in the generator, which ensures that the generated feature encoding of objects or the background is aware of the other objects or background elements in the scene. The second module replaces the use of location-insensitive image features in the discriminator with the Gram matrix computed from the feature maps of the generated object images. This helps preserve location-sensitive information and greatly enhances the appearance of objects. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks. The code for this method is available at https://github.com/wtliao/layout2img.