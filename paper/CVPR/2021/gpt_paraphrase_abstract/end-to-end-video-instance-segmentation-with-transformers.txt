We present VisTR, a novel video instance segmentation framework based on Transformers. Unlike previous methods that use complex pipelines, VisTR tackles the task by treating it as an end-to-end parallel sequence decoding/prediction problem. It takes a video clip as input and directly outputs a sequence of masks for each instance in the video. Our approach incorporates a new instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level. This perspective simplifies the overall pipeline and distinguishes VisTR from existing approaches. Despite its simplicity, VisTR achieves the highest speed among existing video instance segmentation models and delivers the best results on the YouTube-VIS dataset using a single model. We demonstrate that a simpler and faster video instance segmentation framework built upon Transformers can achieve competitive accuracy. We hope that VisTR will inspire future research in video understanding tasks. The code is available at https://git.io/VisTR.