Current models struggle to automatically detect or segment objects that blend in with their surroundings. The similarity between these foreground objects and the background makes it difficult for deep models to distinguish their features. To address this challenge, an ideal model should be able to extract additional clues from the scene and integrate them into a joint learning framework to enhance representation. Inspired by this idea, we propose a novel Mutual Graph Learning (MGL) model that extends the concept of mutual learning from regular grids to graphs. The MGL model separates an image into two feature maps: one for roughly locating the target and another for accurately capturing its boundary details. It leverages high-order relations through graphs to fully exploit the mutual benefits between these two tasks. Unlike other mutual learning approaches that use a shared function for all between-task interactions, MGL employs different functions to handle different complementary relations, maximizing information interactions. Experimental results on challenging datasets, including CHAMELEON, CAMO, and COD10K, demonstrate the effectiveness of our MGL model, surpassing existing state-of-the-art methods. The code for MGL is available at https://github.com/fanyang587/MGL.