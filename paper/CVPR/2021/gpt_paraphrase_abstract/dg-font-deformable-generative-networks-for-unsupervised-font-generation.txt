Font generation is a challenging problem, particularly for writing systems with a large number of characters. Current methods for font generation rely on supervised learning, which requires a significant amount of paired data that is time-consuming and costly to obtain. Additionally, existing image-to-image translation models define style as textures and colors, which is not directly applicable to font generation.   To overcome these issues, we propose a new approach called Deformable Generative Networks for unsupervised font generation (DG-Font). Our method introduces a feature deformation skip connection (FDSC) that predicts displacement maps and applies deformable convolution to low-level feature maps from the content encoder. The outputs of FDSC are then fed into a mixer to generate the final results, producing high-quality characters with a complete structure.   Furthermore, we enhance the quality of generated images by incorporating three deformable convolution layers in the content encoder to learn style-invariant feature representations. Experimental results show that our model outperforms state-of-the-art methods in generating high-quality characters. The source code for our approach is available at https://github.com/ecnuycxie/DG-Font.