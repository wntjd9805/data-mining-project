We propose using tracking as a proxy task for a computer vision system to learn visual representations, inspired by the development of tracking ability in human eyes during childhood. We introduce a Catch-the-Patch (CtP) game, modeled after the game played by children, for a 3D-CNN model to learn visual representations that aid in video-related tasks. In this pretraining framework, we extract an image patch from a given video and allow it to scale and move based on a predetermined trajectory. The proxy task involves estimating the position and size of the image patch in a sequence of video frames, using only the target bounding box in the first frame. We find that using multiple image patches simultaneously provides clear advantages, and we increase the difficulty of the game by randomly making patches invisible. Extensive experiments on popular benchmarks demonstrate the superior performance of CtP compared to other video pretraining methods. Additionally, CtP-pretrained features exhibit less sensitivity to domain gaps compared to features trained by a supervised action recognition task. Surprisingly, when both are trained on Kinetics-400, CtP-pretrained representations achieve higher action classification accuracy than their fully supervised counterparts on the Something-Something dataset.