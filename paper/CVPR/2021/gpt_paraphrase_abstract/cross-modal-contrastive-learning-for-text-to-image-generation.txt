Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) aims to improve the output quality of text-to-image synthesis systems. These systems should generate coherent, clear, and photo-realistic scenes that closely align with their given text descriptions. To achieve this, XMC-GAN focuses on maximizing the mutual information between images and text.XMC-GAN employs multiple contrastive losses to capture correspondences between different modalities and within the same modality. It utilizes an attentional self-modulation generator, which ensures strong text-image correspondence, and a contrastive discriminator that serves as both a critic and a feature encoder for contrastive learning.Compared to previous models, XMC-GAN demonstrates significant advancements in output quality, as demonstrated across three challenging datasets. On MS-COCO, XMC-GAN not only surpasses the state-of-the-art FID score of 24.70 with an impressive score of 9.33 but also receives higher preference ratings from people for image quality (77.3%) and image-text alignment (74.1%) compared to three other recent models.Furthermore, XMC-GAN successfully generalizes its performance to the Localized Narratives dataset, which features longer and more detailed descriptions. It improves the state-of-the-art FID score from 48.70 to 14.12 on this dataset.Lastly, XMC-GAN is trained and evaluated on the challenging Open Images dataset, achieving a benchmark FID score of 26.91, further establishing its effectiveness.In summary, our XMC-GAN model significantly enhances the output quality of text-to-image synthesis systems on various datasets, outperforming previous models in terms of FID scores and receiving higher preference ratings for image quality and image-text alignment.