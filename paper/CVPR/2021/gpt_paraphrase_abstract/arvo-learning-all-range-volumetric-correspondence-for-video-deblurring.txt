We present a new approach for video deblurring that addresses the limitations of existing methods in handling fast motions with large pixel displacements. Instead of relying on homography or optical flows for aligning blurry frames, we propose an implicit method that learns spatial correspondence in the feature space. Our model constructs a correlation volume pyramid to establish distant pixel correspondences between neighboring frames. We also introduce a correlative aggregation module to enhance the features of the reference frame by maximizing pixel-pair correlations with its neighbors based on the volume pyramid. The aggregated features are then fed into a reconstruction module to restore the frame. We optimize our model using a generative adversarial paradigm. We evaluate our method on the widely-adopted DVD dataset and a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). Both quantitative and qualitative experiments demonstrate that our model outperforms previous state-of-the-art methods on these datasets, highlighting the advantages of modeling all-range spatial correspondence for video deblurring.