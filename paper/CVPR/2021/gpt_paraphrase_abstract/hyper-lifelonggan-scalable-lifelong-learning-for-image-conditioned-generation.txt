We investigate the problem of catastrophic forgetting in deep neural networks, where they struggle to retain knowledge of previously learned tasks when encountering new ones. To address this, we propose a novel continual learning framework called Hyper-LifelongGAN, which offers improved scalability compared to existing approaches. Our method involves factorizing conventional convolutional filters into dynamic base filters generated by task-specific filter generators, along with a deterministic weight matrix that is shared across tasks. Additionally, task-specific coefficients are introduced to the shared weight matrix to allow for different combinations of task-specific base filters for each task. This architecture enables our method to maintain or even enhance generation quality with minimal parameter cost. We evaluate the effectiveness of Hyper-LifelongGAN on various image-conditioned generation tasks through extensive ablation studies and comparisons with state-of-the-art models, demonstrating its ability to mitigate catastrophic forgetting.