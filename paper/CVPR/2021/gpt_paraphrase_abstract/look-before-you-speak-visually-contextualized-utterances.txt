Most conversational AI systems primarily focus on text-based dialogue, but incorporating visual context can enhance the realism of conversations. However, the lack of large-scale labeled datasets poses a significant challenge. To address this, we propose a visually conditioned Future Utterance Prediction task. This task involves predicting the next utterance in a video by utilizing both visual frames and transcribed speech as context. To train our model without manual annotations, we leverage the abundance of instructional videos available online. Our model, a co-attentional multimodal video transformer, surpasses baseline models that solely rely on textual inputs when trained on both textual and visual context. Moreover, our model, trained on unlabeled videos for this task, achieves state-of-the-art performance on various downstream VideoQA benchmarks. These benchmarks include MSRVTT-QA, MSVD-QA, ActivityNet-QA, and How2QA.