Previous research on scene graph generation has focused on categorically supervised triplets, consisting of subjects, objects, and predicates. This supervision can include bounding box information or not. However, since scene graph generation is a holistic task, it can benefit from holistic contextual supervision. Therefore, in this study, we investigate how linguistic structures in captions can enhance scene graph generation. Our approach captures information from captions regarding relations between individual triplets, as well as context for subjects and objects, such as visual properties mentioned in the captions. Although captions provide weaker supervision compared to triplets due to the imperfect alignment between human-annotated triplets and the nouns in captions, linguistic supervision is more scalable due to the availability of diverse multimodal data on the web. We conduct extensive experimental comparisons with previous methods that use instance- and image-level supervision. Additionally, we analyze the impact of leveraging phrasal and sequential context, as well as techniques to improve the localization of subjects and objects.