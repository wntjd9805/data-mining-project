We introduce self-supervised geometric perception (SGP), a novel framework for learning a feature descriptor for correspondence matching without the need for ground-truth geometric model labels. Our approach formulates geometric perception as an optimization problem that simultaneously optimizes the feature descriptor and the geometric models using a large collection of visual measurements. We demonstrate that existing research on robust model fitting and deep feature learning can be viewed as optimizing one set of variables while keeping the other fixed. Building on this insight, we propose the SGP algorithm, which employs alternating minimization to solve the joint optimization problem. SGP consists of two meta-algorithms: a teacher algorithm that uses learned features to perform robust model fitting and generate geometric pseudo-labels, and a student algorithm that learns deep features while being supervised by the noisy pseudo-labels. Additionally, we apply SGP to two perception problems on real-world datasets, specifically relative camera pose estimation on MegaDepth and point cloud registration on 3DMatch. Our experiments demonstrate that SGP achieves state-of-the-art performance comparable to or better than supervised oracles trained with ground-truth labels.