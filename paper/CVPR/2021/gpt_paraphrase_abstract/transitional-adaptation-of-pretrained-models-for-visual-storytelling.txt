Previous models for vision-to-language generation tasks typically pretrain a visual encoder and a language generator in separate domains and then jointly fine-tune them for the target task. However, this direct transfer approach may encounter difficulties due to the mismatch between the specificity of visual information and the fluency of language generation, as these components are often trained separately using different datasets. In this study, we argue that a transitional adaptation task is necessary to bridge the gap between pretraining and fine-tuning, enabling better alignment between the visual encoder and the language model for challenging downstream tasks such as visual storytelling. To address this, we propose a novel method called Transitional Adaptation of Pre-trained Model (TAPM), which aligns the multi-modal modules by performing a simpler alignment task using only visual inputs without requiring text labels. Extensive experiments demonstrate that this adaptation step significantly enhances the performance of multiple language models for sequential video and image captioning tasks. We achieve state-of-the-art results in terms of both language evaluation metrics and human assessment in the multi-sentence description task of LSMDC 2019 and the image storytelling task of VIST. Importantly, our findings indicate that this improvement in caption quality is not dependent on the specific choice of language models.