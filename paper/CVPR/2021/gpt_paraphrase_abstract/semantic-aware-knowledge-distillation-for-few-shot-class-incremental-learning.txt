This study focuses on the problem of few-shot class incremental learning (FSCIL), where the learner gradually learns new concepts with only a limited number of examples per concept. Traditional incremental learning techniques are not directly applicable to FSCIL due to the scarcity of training examples. Therefore, this research proposes a distillation algorithm to address the FSCIL problem. Additionally, the study suggests utilizing semantic information, specifically word embeddings, during training. These embeddings are easily obtainable and aid in the distillation process. Moreover, the research introduces a method that uses an attention mechanism to align visual and semantic vectors, reducing issues related to catastrophic forgetting. Experimental results on MiniImageNet, CUB200, and CI-FAR100 datasets demonstrate that the proposed approach surpasses existing methods, establishing new state-of-the-art performance.