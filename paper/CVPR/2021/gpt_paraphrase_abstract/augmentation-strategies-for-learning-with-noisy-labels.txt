Various methods have been developed to train deep neural networks (DNNs) that are robust to label noise in real-world datasets. These methods typically involve filtering samples based on loss during a warm-up phase to obtain a set of clean labeled samples, and using the network's output as a pseudo-label for subsequent loss calculations. This study focuses on evaluating different augmentation strategies for algorithms addressing the "learning with noisy labels" problem. Synthetic datasets based on CIFAR-10 and CIFAR-100, as well as the real-world dataset Clothing1M, are used for evaluation. The authors propose and examine multiple augmentation strategies, finding that using one set of augmentations for loss modeling tasks and another set for learning yields the best results compared to existing methods. Additionally, it is discovered that applying augmentation during the warm-up period can negatively impact the convergence of correctly and incorrectly labeled samples. To address this issue, the authors introduce an augmentation strategy to the state-of-the-art technique and demonstrate improved performance across all evaluated noise levels. For instance, the accuracy on the CIFAR-10 benchmark at 90% symmetric noise is increased by more than 15% in absolute accuracy. Similar performance enhancements are observed on the Clothing1M dataset.