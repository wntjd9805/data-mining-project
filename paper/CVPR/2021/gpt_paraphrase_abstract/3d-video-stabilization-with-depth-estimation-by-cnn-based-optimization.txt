Video stabilization is crucial for improving visual quality in videos. Traditional methods rely on feature tracking to recover motion in frames, but they are not robust in shaky videos. Recently, learning-based methods have emerged, using deep neural networks to overcome the robustness issue. However, none of these methods utilize 3D cues, leading to artifacts in complex scenes. In this paper, we propose Deep3D Stabilizer, a novel method that leverages 3D depth information for video stabilization. We use a self-supervised framework to jointly learn depth and camera ego-motion estimation. Our approach does not require pre-training data and directly stabilizes the input video through 3D reconstruction. The rectification stage uses 3D scene depth and camera motion to smooth the camera trajectory and create a stabilized video. Unlike other methods, our smoothing algorithm allows users to efficiently manipulate the stability of the video. Experimental results demonstrate that our method outperforms state-of-the-art methods in various motion categories.