This paper presents a new approach to the challenging problem of referring expression comprehension in videos. Previous methods have used multiple stages to solve the problem, such as tracking and proposal-based matching. In contrast, our approach, called co-grounding, tackles the problem in a single stage. We improve the accuracy of grounding expressions in individual frames by using semantic attention learning to parse referring cues and reduce ambiguity. Furthermore, we enhance the consistency of grounding across frames by incorporating co-grounding feature learning, which integrates temporal correlation to handle scene dynamics. Experimental results on the VID and LiOTB video grounding datasets demonstrate the superior performance of our framework in generating accurate and stable results across frames. Additionally, our approach also shows improved performance on the RefCOCO dataset for referring expression comprehension in images. The details of our project can be found at https://sijiesong.github.io/co-grounding.