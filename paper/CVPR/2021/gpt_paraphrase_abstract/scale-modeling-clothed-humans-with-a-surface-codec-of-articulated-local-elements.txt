Creating accurate models of humans wearing clothing is a difficult task due to factors such as body movement, flexible fabric, and different types of clothing. The choice of representation plays a crucial role in enabling learning in this area. Recent research has utilized neural networks to parameterize local surface elements, which allows for capturing coherent geometry and non-planar details, accommodating various clothing topologies, and eliminating the need for registered training data. However, directly applying these methods to model 3D clothed humans fails to capture precise local deformations and performs poorly in generalization.To address these limitations, we propose three key innovations. First, we deform surface elements based on a human body model, separating large-scale deformations caused by body movement from topological changes and local clothing deformations. This approach allows for more explicit control over different types of deformations. Second, we overcome the shortcomings of existing neural surface elements by regressing local geometry from local features, significantly enhancing the expressiveness of the model. By leveraging local information, we can better capture fine-grained details. Third, we introduce a pose embedding technique on a 2D parameterization space that encodes the geometric information of the posed body. This embedding improves generalization to unseen poses by reducing unnecessary correlations between different body positions.To validate our approach, we demonstrate its effectiveness by learning models of complex clothing from point clouds. Our method can handle clothing that changes topology and deviates from the body's topology. Once trained, we can animate previously unseen motions, generating high-quality point clouds that can be used to generate realistic images using neural rendering techniques. We evaluate the significance of each technical contribution and show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy and inference time.For the convenience of researchers, we have made our code available at https://qianlim.github.io/SCALE for research purposes.