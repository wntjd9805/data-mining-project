Training deep neural networks requires estimating gradients from data batches in order to update parameters. These gradients are typically averaged over a set of data, which has been assumed to be safe for privacy-preserving training in collaborative and federated learning applications. Previous research has shown the possibility of recovering input data from gradients, but only under very restrictive conditions such as a single input point or a network without non-linearities. It was believed that averaging gradients over larger batches would be safe. However, in this study, we introduce GradInversion, a method that allows for the recovery of input images from larger batches (consisting of 8 to 48 images) and for complex networks like ResNets (with 50 layers) on datasets like ImageNet (with 1000 classes and images of size 224x224 pixels). We formulate an optimization task that converts random noise into natural images by matching gradients and preserving image fidelity. Additionally, we propose an algorithm to recover the target class label based on gradients. To further improve the reconstruction of the original data batch, we suggest a group consistency regularization framework where multiple agents, starting from different random seeds, collaborate. Our results demonstrate that gradients contain a significant amount of information, allowing us to recover individual images with high fidelity using GradInversion, even for complex datasets, deep networks, and large batch sizes.