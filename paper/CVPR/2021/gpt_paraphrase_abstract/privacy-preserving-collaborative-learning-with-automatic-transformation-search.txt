Collaborative learning has become popular as it protects data privacy by allowing participants to jointly train a Deep Learning model without sharing their training sets. However, recent studies have shown that an adversary can recover sensitive training samples from shared gradients, posing serious threats to collaborative learning. Therefore, effective solutions are needed to mitigate these reconstruction attacks. To address this issue, we propose using data augmentation to counter reconstruction attacks. By applying carefully-selected transformation policies to sensitive images before training, it becomes impossible for the adversary to extract useful information from the gradients. We have developed a novel search method to automatically discover suitable policies. Additionally, we have introduced two new metrics to measure the impact of transformations on data privacy and model usability, which significantly speeds up the search process.Comprehensive evaluations demonstrate that the policies identified by our method can successfully defend against existing reconstruction attacks in collaborative learning. Moreover, these policies are highly efficient and have negligible impact on the model's performance.