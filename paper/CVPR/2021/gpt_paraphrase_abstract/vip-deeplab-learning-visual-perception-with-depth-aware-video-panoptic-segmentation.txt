This paper introduces ViP-DeepLab, a unified model designed to address the challenging problem of inverse projection in computer vision. The objective is to reconstruct point clouds from sequences of perspective images while assigning instance-level semantic interpretations to each point. To achieve this, the model predicts the spatial location, semantic class, and temporally consistent instance label for each 3D point. ViP-DeepLab combines monocu-lar depth estimation and video panoptic segmentation to solve this problem, which is referred to as Depth-aware Video Panoptic Segmentation. The authors propose a new evaluation metric and provide two datasets for this task, which will be publicly available. In addition, ViP-DeepLab achieves state-of-the-art results on individual sub-tasks, surpassing previous methods in terms of performance. It outperforms previous approaches by 5.1% VPQ on Cityscapes-VPS, ranks first in the KITTI monocular depth estimation benchmark, and achieves the top position in KITTI MOTS pedestrian. The datasets and evaluation codes are also made publicly accessible.