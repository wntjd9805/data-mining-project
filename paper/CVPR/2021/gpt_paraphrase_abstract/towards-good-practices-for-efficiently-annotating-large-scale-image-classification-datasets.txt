The focus of this paper is on efficient annotation strategies for collecting multi-class classification labels for a large collection of images. The traditional approach of querying humans for a fixed number of labels per image and aggregating them is expensive. The paper proposes modifications and best practices based on online joint probabilistic modeling of human annotations and machine-generated beliefs to minimize human labeling effort. The authors make use of advances in self-supervised learning and view annotation as a semi-supervised learning problem. They identify and address pitfalls and ablate key design choices to propose effective guidelines for labeling. The analysis is conducted in a realistic simulation that involves querying human labelers, which uncovers issues with existing worker simulation methods. Simulated experiments on a subset of the ImageNet100 dataset show that the proposed approach can achieve 80% top-1 accuracy with an average of 0.35 annotations per image, which is a significant improvement over prior work and manual annotation.