Recently, there has been a focus on using neural architecture search (NAS) to design feature pyramid networks (FPNs) for visual object detection. In this study, we propose a new algorithm called One-Shot PathAggregation Network Architecture Search (OPANAS) that improves both the efficiency of the search process and the accuracy of object detection.To achieve this, we introduce six different information paths in our search space: top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect, and none. These paths represent different ways in which the FPNs can be constructed. We then represent each FPN candidate as a directed acyclic graph, with each node representing a feature pyramid and each edge representing one of the six information paths.To efficiently find the optimal path aggregation architecture, we develop a one-shot search method. This involves training a super-net and using an evolutionary algorithm to identify the best candidate. Experimental results demonstrate the effectiveness of OPANAS for object detection. Firstly, OPANAS is more efficient than existing methods, such as NAS-FPN and Auto-FPN, while requiring significantly less computational resources (only 4 GPU days on MS-COCO). Secondly, the optimal architecture found by OPANAS significantly improves mainstream detectors (e.g., RetinaNet, Faster R-CNN, and Cascade R-CNN) by 2.3-3.2% in mean average precision (mAP) compared to their FPN counterparts. Lastly, OPANAS achieves a new state-of-the-art accuracy-speed trade-off (52.2% mAP at 7.6 FPS) at a lower training cost compared to other recent approaches.The code for OPANAS will be made available at https://github.com/VDIGPKU/OPANAS.