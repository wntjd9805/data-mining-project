This study focuses on visible-infrared person re-identification (Re-ID), which involves matching pedestrian images of the same person from different modalities. Current approaches address the modality discrepancy by aligning the feature distributions across modalities but fail to fully explore nuanced yet discriminative information such as glasses, shoes, and clothing length, particularly in the infrared modality. The lack of attention to these nuances makes it challenging to match pedestrians across modalities using modality alignment alone, resulting in reduced feature distinctiveness. To address this issue, the authors propose a joint Modality and Pattern Alignment Network (MPANet) for visible-infrared person Re-ID. MPANet incorporates a modality alleviation module and a pattern alignment module to extract discriminative features. The modality alleviation module removes modality information from the feature maps, and the pattern alignment module generates multiple pattern maps to discover nuances in diverse patterns of a person. The study also introduces a mutual mean learning fashion to alleviate the modality discrepancy and a center cluster loss to guide identity learning and nuances discovery. Experimental results on the public SYSU-MM01 and RegDB datasets demonstrate the superior performance of MPANet compared to existing state-of-the-art methods.