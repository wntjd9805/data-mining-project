We present a new method for learning representations of 2D human poses that can separate factors related to pose and viewpoint. Our approach, called cross-view mutual information maximization (CV-MIM), trains a network to maximize the mutual information between the same pose observed from different viewpoints. We also introduce two regularization terms to ensure that the learned representations are disentangled and smooth. These pose representations can be used for cross-view action recognition. To evaluate the effectiveness of our learned representations, we introduce a novel task called single-shot cross-view action recognition. In this task, models are trained using actions from a single viewpoint but evaluated on poses captured from all possible viewpoints. We compare our method to state-of-the-art models in fully-supervised action recognition scenarios and show that CV-MIM performs competitively. Additionally, CV-MIM significantly outperforms other methods in the single-shot cross-view setting. Furthermore, we demonstrate that the learned representations can greatly improve performance when reducing the amount of supervised training data. We have publicly released the code for our method at the following GitHub repository: https://github.com/google-research/google-research/tree/master/poem.