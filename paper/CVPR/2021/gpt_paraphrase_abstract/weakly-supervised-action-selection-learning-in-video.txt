The weakly supervised temporal localization problem aims to localize actions in videos using only video-level labels, reducing the need for expensive and error-prone frame-level annotations. One common approach is to train a frame-level classifier and select frames with the highest class probability to make video-level predictions. However, the absence of frame-level annotations introduces class bias on every frame. To address this, the Action Selection Learning (ASL) approach is proposed to capture the general concept of action, referred to as "actionness." ASL trains the model with a novel class-agnostic task to predict which frames will be selected by the classifier. Experimental results on THUMOS-14 and ActivityNet-1.2 benchmarks demonstrate that ASL outperforms leading baselines, with relative improvements of 10.3% and 5.7% respectively. The properties of ASL are further analyzed, highlighting the importance of actionness. The full code for this work is available at https://github.com/layer6ai-labs/ASL.