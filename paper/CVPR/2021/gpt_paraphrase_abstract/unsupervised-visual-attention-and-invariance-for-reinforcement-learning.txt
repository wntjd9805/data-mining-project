Current approaches in vision-based reinforcement learning (RL) face challenges when it comes to generalizing to unknown test environments. While existing methods concentrate on training a universal RL policy for changing visual domains, our focus lies in extracting a universal visual foreground that remains invariant, thereby providing clean visual input to the RL policy learner. Our method, which is entirely unsupervised and does not require manual annotations or access to environment internals, involves learning how to extract foregrounds through unsupervised keypoint detection and subsequently employing unsupervised visual attention to generate a foreground mask for each video frame. By introducing artificial distractors and training a model to reconstruct the clean foreground mask from noisy observations, we only need this learned model during testing to supply distraction-free visual input to the RL policy learner. Our Visual Attention and Invariance (VAI) method significantly outperforms the current state-of-the-art in terms of visual domain generalization, achieving 15∼49% (61∼229%) more cumulative rewards per episode on DeepMind Control (Drawer-World Manipulation) benchmarks. Our results indicate that it is not only feasible to learn domain-invariant vision without any supervision, but also that freeing RL from visual distractions leads to a more focused policy and thus significantly improved performance.