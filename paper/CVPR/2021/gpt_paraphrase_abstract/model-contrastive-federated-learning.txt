Federated learning allows multiple parties to collaborate on training a machine learning model without sharing their local data. However, existing approaches struggle to handle the heterogeneity of data distribution in image datasets with deep learning models. To address this challenge, we introduce MOON: model-contrastive federated learning. MOON is a straightforward and efficient federated learning framework that leverages model representations' similarity to improve local training. By conducting contrastive learning at the model level, MOON surpasses other state-of-the-art federated learning algorithms in numerous image classification tasks.