Person re-identification (person ReID) relies heavily on visual texture information such as clothing. However, existing ReID methods often overlook situations where clothing changes or multiple individuals wear the same clothes. In this paper, we propose a new approach to enhance the robustness of person ReID against clothing texture by utilizing a person's 3D shape information. Current shape learning methods for person ReID either disregard the 3D information or require additional devices to capture 3D data. In contrast, our novel ReID learning framework, called 3D Shape Learning (3DSL), directly extracts a texture-insensitive 3D shape embedding from a 2D image by incorporating 3D body reconstruction as an auxiliary task and regularization. The regularization enforces the ReID model to separate the 3D shape information from the visual texture, resulting in discriminative 3D shape ReID features. To address the lack of 3D ground truth, we introduce an adversarial self-supervised projection (ASSP) model that performs 3D reconstruction without relying on ground truth. Extensive experiments on commonly used ReID datasets and datasets with texture-confusing scenarios demonstrate the effectiveness of our model.