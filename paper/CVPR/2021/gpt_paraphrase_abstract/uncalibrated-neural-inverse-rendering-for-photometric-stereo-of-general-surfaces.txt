This paper introduces a novel deep neural network framework for addressing the photometric stereo problem without requiring precise information about light directions or ground-truth surface normals. Existing methods for training neural networks in this domain often rely on exact light directions or ground-truth surface normals, which can be difficult to obtain in practice and limit the widespread use of photometric stereo algorithms in vision applications. To overcome this challenge, we propose an uncalibrated neural inverse rendering approach. Our method estimates the light directions from input images and then utilizes an image reconstruction loss to calculate surface normals, bidirectional reflectance distribution function values, and depth. Moreover, our approach explicitly models the concave and convex parts of complex surfaces to account for interreflections in the image formation process. Through extensive evaluation, our method demonstrates comparable or superior performance compared to supervised and classical approaches on challenging subjects.