Autonomous driving can benefit from understanding the motion behavior of different traffic participants in dynamic environments. There is increasing interest in estimating motion directly from point clouds, but current methods require a large amount of annotated training data, which is difficult and time-consuming to label. This study aims to determine if unlabeled data can be used for accurate and efficient motion learning. A learning framework is proposed that utilizes free supervisory signals from point clouds and paired camera images to estimate motion through self-supervision. The model includes a point cloud-based structural consistency, probabilistic motion masking, and cross-sensor motion regularization. Experimental results show that our approach performs competitively compared to supervised methods, and achieves state-of-the-art results when combined with supervised fine-tuning.