We propose a novel approach for training egocentric video models using third-person video datasets. The limitation of learning solely from egocentric data is the lack of dataset scale and diversity, while using only third-person data introduces a significant domain mismatch. To overcome this, we suggest identifying latent signals in third-person videos that can predict egocentric-specific properties. By incorporating these signals as knowledge distillation losses during pre-training, our models benefit from the scale and diversity of third-person data and capture important egocentric characteristics. Our experiments demonstrate that our "Ego-Exo" framework can seamlessly integrate into standard video models and outperform all baseline methods when fine-tuned for egocentric activity recognition. This approach achieves state-of-the-art results on Charades-Ego and EPIC-Kitchens-100 datasets.