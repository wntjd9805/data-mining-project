Non-visual imaging sensors are commonly used in various industries but tend to be more expensive and produce lower resolution images compared to visual (RGB) sensors. To address this issue, Cross-Modality Super-Resolution (CMSR) methods have been developed, which involve using an RGB image of higher resolution to enhance the resolution of a low-resolution modality. However, merging images from different modalities is challenging due to their significant internal variations. Existing state-of-the-art techniques trained on external datasets often struggle to produce artifact-free results that accurately reflect the characteristics of the target modality.In this study, we propose a novel approach called CMSR that focuses on a single-pair scenario for Cross-Modality Super-Resolution. Our network is internally trained solely on the two input images in a self-supervised manner. This allows the network to learn the internal statistics and correlations between the images and apply them to up-sample the target modality. CMSR incorporates an internal transformer that is trained on-the-fly along with the up-sampling process itself, without the need for supervision. This enables the network to handle pairs that are only weakly aligned.We demonstrate that CMSR achieves state-of-the-art results in super-resolving images, without introducing any artifacts or irrelevant details that may originate solely from the RGB image.