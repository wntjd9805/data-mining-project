Monocular 3D object detection in autonomous driving is challenging when there are changes in the ego-car pose relative to the ground plane. Existing methods on open datasets do not consider camera pose information, making the detector sensitive to camera extrinsic parameters. To address this, we propose a novel method that captures camera pose to make the detector immune to extrinsic perturbations. Our framework predicts camera extrinsic parameters by detecting vanishing points and changes in the horizon. We also design a converter to rectify perturbed features in the latent space. Our 3D detector performs well in realistic scenarios with potholed and uneven roads, where other monocular detectors fail. Experimental results show that our method outperforms state-of-the-art approaches by a significant margin on both KITTI 3D and nuScenes datasets.