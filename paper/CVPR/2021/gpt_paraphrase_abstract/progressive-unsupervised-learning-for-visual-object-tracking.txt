This paper presents a novel framework called progressive unsupervised learning (PUL) for visual tracking without the need for annotated training videos. The framework begins by learning a background discrimination (BD) model that effectively distinguishes objects from the background using contrastive learning. The BD model is then used to progressively mine temporal corresponding patches in sequential frames. However, since the BD model is not perfect, the mined patch pairs may contain noise. To address this, a noise-robust loss function is proposed to enhance the learning of temporal correspondences from the noisy data. The backbone networks of Siamese trackers are trained using this noise-robust loss function. Remarkably, the proposed unsupervised real-time Siamese trackers outperform existing unsupervised deep trackers and achieve results comparable to supervised baselines without online fine-tuning or adaptation.