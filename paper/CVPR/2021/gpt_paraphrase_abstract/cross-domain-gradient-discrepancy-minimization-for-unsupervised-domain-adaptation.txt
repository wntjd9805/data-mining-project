Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Adversarial domain adaptation using bi-classifiers has been effective in aligning distributions between domains. However, existing methods focus only on the similarity of outputs between classifiers, which may lead to inaccurate categorization of target samples. To address this issue, this paper proposes a method called cross-domain gradient discrepancy minimization (CGDM). CGDM explicitly minimizes the discrepancy of gradients generated by source and target samples. The gradients provide semantic information about the target samples and can be used to improve their accuracy. To compute the gradient signal of target samples, target pseudo labels are obtained through clustering-based self-supervised learning. Experimental results on three UDA datasets demonstrate that CGDM outperforms previous state-of-the-art methods.