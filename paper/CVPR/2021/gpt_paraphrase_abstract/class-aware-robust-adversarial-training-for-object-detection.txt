Object detection is a critical task in computer vision, but it is vulnerable to adversarial attacks. Previous defense methods have mainly focused on classification tasks and have not thoroughly examined object detection. To address this gap, we propose a new approach called class-aware robust adversarial training for object detection. Our approach generates a universal adversarial perturbation for each object in an image by maximizing the respective loss for each object. Instead of normalizing the total loss with the number of objects, we decompose the total loss into class-wise losses and normalize each class loss using the number of objects in that class. This approach balances the influence of each class and improves the adversarial robustness of trained models for all object classes compared to previous defense methods. We also introduce a fast version of our algorithm that can be trained more quickly without sacrificing performance. Through extensive experiments on challenging datasets, we demonstrate that our defense methods effectively enhance the robustness of object detection models.