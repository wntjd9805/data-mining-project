Differential Neural Architecture Search (NAS) faces limitations in search space and final architecture size due to the requirement of holding all layer choices in memory simultaneously. On the other hand, Probabilistic NAS methods, like PARSEC, learn a distribution of high-performing architectures and use only the necessary memory for training a single model. However, sampling multiple architectures in extensive spaces makes it computationally expensive. To address these challenges, we propose a sampling method that adjusts to the distribution entropy. It increases the number of samples in the initial stages to encourage exploration and decreases them as learning progresses. Additionally, we introduce a coarse-to-fine strategy called Fast Probabilistic NAS (FP-NAS) to efficiently search in the multi-variate space. This strategy involves using a factorized distribution initially, which significantly reduces the number of architecture parameters. FP-NAS offers advantages over PARSEC, sampling 64% fewer architectures and searching 2.1 times faster. Compared to FBNetV2, FP-NAS is 1.9-3.5 times faster, and the models it discovers outperform FBNetV2 models on ImageNet. FP-NAS enables the expansion of the FBNetV2 space, allowing for wider (more channel choices) and deeper (more blocks) architectures. It also facilitates the search for the number of splits and the inclusion of a Split-Attention block. When searching for a 0.4G FLOPS model, FP-NAS is 132 times faster than EfficientNet, and the searched FP-NAS-L0 model achieves a 0.7% higher accuracy than EfficientNet-B0. We conduct these searches without using any architecture surrogate or scaling tricks, directly searching for large models up to 1.0GFLOPS. Our FP-NAS-L2 model, with simple distillation, outperforms BigNAS-XL by 0.7% accuracy using similar FLOPS.