We present a new framework that allows for the creation of realistic datasets of indoor scenes, including accurate geometry, materials, lighting, and semantic labels. Our aim is to make this dataset creation process accessible to a wide range of users, transforming scans into photorealistic datasets with high-quality ground truth information. This includes detailed information on appearance, layout, semantic labels, spatially-varying BRDF (Bidirectional Reflectance Distribution Function), and complex lighting components such as direct, indirect, and visibility factors. By achieving this, we enable various applications in inverse rendering, scene understanding, and robotics.We demonstrate that deep neural networks trained on our proposed dataset achieve competitive performance in estimating shape, material properties, and lighting in real images. This leads to the potential for photorealistic augmented reality applications, such as object insertion and material editing. Additionally, our semantic labels can be utilized for segmentation and multi-task learning purposes.Furthermore, we show that our framework can be integrated with physics engines to create virtual robotics environments that closely resemble real scenes. This integration allows for the inclusion of unique ground truth information, such as friction coefficients and correspondence to real-world scenes.We will make the dataset and all the necessary tools for creating such datasets publicly available.