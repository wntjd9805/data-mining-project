Existing deep learning-based tracking algorithms are not designed to handle rotations, which severely affects their performance when rotation instances are present in videos. To address this issue, this paper introduces rotation-equivariant Siamese networks (RE-SiamNets) that utilize group-equivariant convolutional layers with steerable filters. RE-SiamNets can estimate the change in orientation of the object in an unsupervised manner, enabling relative 2D pose estimation. Additionally, the change in orientation can be used to impose a motion constraint in Siamese tracking by restricting the change in orientation between consecutive frames. To evaluate the performance of RE-SiamNets, the Rotation Tracking Benchmark (RTB) dataset is introduced, consisting of videos with rotation instances. Experimental results demonstrate that RE-SiamNets effectively handle rotation challenges and outperform regular Siamese networks. Moreover, RE-SiamNets can accurately estimate the relative change in pose of the target without supervision. The code and data for this study are available at https://github.com/dkgupta90/re-siamnet.