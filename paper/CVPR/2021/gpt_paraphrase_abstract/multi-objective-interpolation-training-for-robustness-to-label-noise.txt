We present a new approach called Multi-Objective Interpolation Training (MOIT) to address the issue of deep neural networks memorizing noisy labels during training, which leads to a decrease in performance. Unlike existing methods that propose new loss functions, our approach combines contrastive learning and classification to mutually enhance each other and improve performance against label noise. We observe that supervised contrastive learning is negatively affected by label noise, and to mitigate this, we introduce an interpolation training strategy. Additionally, we propose a novel method for detecting label noise by leveraging the robust feature representations learned through contrastive learning. This method estimates soft-labels for each sample and identifies noisy samples based on the disagreement between these soft-labels and the original labels. By treating noisy samples as unlabeled, we can train a classifier in a semi-supervised manner, preventing the memorization of noise and improving representation learning. We further enhance MOIT with a refinement step called MOIT+, where we fine-tune the model on clean samples detected through our label noise detection method. Through hyperparameter tuning and ablation studies, we validate the effectiveness of our approach. Our experiments on synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves state-of-the-art results. The code for our approach is available at https://git.io/JI40X.