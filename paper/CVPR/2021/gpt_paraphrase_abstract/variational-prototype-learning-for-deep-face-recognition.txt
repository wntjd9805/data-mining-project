The margin-based soft-max loss has greatly improved deep face recognition by using prototypes as the center of each class. However, these methods only consider sample-to-prototype comparisons and neglect sample-to-sample comparisons during training. This limits the further exploration of SGD. To address this, we propose Variational Prototype Learning (VPL), which represents each class as a distribution rather than a single point. By incorporating memorized features into prototypes, we approximate variational prototype sampling and enable sample-to-sample comparisons. VPL encourages the SGD solver to be more exploratory, resulting in improved performance. Additionally, VPL is straightforward to understand, easy to implement, computationally efficient, and memory-saving. Extensive experiments on popular benchmarks demonstrate that VPL outperforms state-of-the-art methods.