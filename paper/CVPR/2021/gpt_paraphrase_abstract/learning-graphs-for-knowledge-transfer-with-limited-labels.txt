Graph Convolution Networks (GCNs) commonly use fixed input graphs for knowledge transfer. These graphs are utilized to transfer information from training to testing nodes in various learning setups such as semi-supervised, zero-shot, and few-shot learning. In this study, we propose a generalized framework that improves the input graph within the standard GCN-based learning setup. To achieve this, we introduce additional constraints based on the relationships between similar and dissimilar neighbors for each node in the graph. These constraints are applied using triplet loss on the output of the intermediate layer. We evaluate our approach on benchmarking datasets including Citeseer, Cora, and Pubmed for semi-supervised learning, as well as UCF101 and HMDB51 datasets for zero/few-shot action recognition. Our results demonstrate significant performance improvements compared to existing approaches. Additionally, we provide qualitative results that visualize the graph connections learned by our approach.