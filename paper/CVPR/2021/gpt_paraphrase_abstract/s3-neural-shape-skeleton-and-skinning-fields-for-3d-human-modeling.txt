Creating and animating human figures is crucial in the development of virtual worlds, particularly in applications like virtual reality and robotics testing. To achieve this, it is essential to have methods that can automatically reconstruct and animate humans from real-world data on a large scale. In this study, we propose a novel approach where the shape, pose, and skinning weights of pedestrians are represented as neural implicit functions, which are learned directly from data. This representation allows us to handle a wide range of pedestrian variations without the need for explicitly fitting a parametric body model. Our experiments demonstrate the effectiveness of our approach, surpassing existing state-of-the-art methods in reconstructing humans. Additionally, we show that we can generate 3D human animations at scale using just a single RGB image as input, along with an optional LiDAR sweep.