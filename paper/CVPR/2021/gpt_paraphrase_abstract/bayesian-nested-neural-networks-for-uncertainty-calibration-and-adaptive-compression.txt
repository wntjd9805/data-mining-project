Nested networks, also known as slimmable networks, are neural networks that can be adjusted during testing based on computational constraints. Previous research has focused on a "nested dropout" layer that ranks the importance of nodes during training, creating a nested set of sub-networks optimized for different resource configurations. However, this method relies on a fixed dropout rate as a hyper-parameter, leading to performance decay based on human-defined trajectories rather than data-driven trajectories. Additionally, the generated sub-networks lack well-calibrated uncertainty. To address these issues, we propose a Bayesian approach to nested neural networks. Our method introduces a variational ordering unit that efficiently samples from a proposed Downhill distribution, providing useful gradients for the parameters of nested dropout. Through this approach, we develop a Bayesian nested neural network that learns the order knowledge of node distributions. Experimental results demonstrate that our proposed approach surpasses the nested network in terms of accuracy, calibration, and out-of-domain detection in classification tasks. Furthermore, it outperforms related methods in uncertainty-critical tasks in computer vision.