Transformers have become popular for learning long-range interactions on sequential data and achieving impressive results on various tasks. Unlike CNNs, transformers do not prioritize local interactions, making them more expressive but computationally challenging for long sequences like high-resolution images. However, we propose a method that combines the strengths of both CNNs and transformers to effectively model and synthesize high-resolution images. Firstly, we utilize CNNs to learn a context-rich vocabulary of image components. Then, we employ transformers to efficiently model the composition of these components within high-resolution images. Our approach is versatile and applicable to conditional synthesis tasks where both non-spatial information (e.g., object classes) and spatial information (e.g., segmentations) can control the generated image. Notably, we present pioneering results on semantically-guided synthesis of megapixel images using transformers. For more details, please refer to our project page at https://git.io/JLlvY.