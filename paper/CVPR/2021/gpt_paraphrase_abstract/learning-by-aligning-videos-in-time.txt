We present a self-supervised approach for learning video representations without the need for labeled data. Our method utilizes temporal video alignment as a pre-text task and takes advantage of both frame-level and video-level information. To train an encoder network, we combine a novel temporal alignment loss (Soft-DTW) with temporal regularization terms. The temporal alignment loss aims to minimize the cost of temporally aligning videos in the embedding space. However, optimizing solely for this term can lead to trivial solutions where all frames are mapped to a small cluster. To address this, we introduce a temporal regularization term (Contrastive-IDM) that encourages diverse mapping of frames in the embedding space. We evaluate our approach on various tasks, such as action phase classification, action phase progression, and fine-grained frame retrieval, using three datasets (Pouring, Penn Action, and IKEAASM). Our method outperforms state-of-the-art methods for self-supervised representation learning from videos, demonstrating its effectiveness. Additionally, our approach offers significant performance improvements when labeled data is lacking.