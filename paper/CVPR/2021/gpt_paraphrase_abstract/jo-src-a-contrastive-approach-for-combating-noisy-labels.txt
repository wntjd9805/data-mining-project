Training Deep Neural Networks (DNNs) with noisy labels often leads to subpar model performance due to the memorization effect. Existing methods typically use a sample selection strategy, choosing small-loss samples for further training. However, these methods overlook the imbalance of noise ratios in different mini-batches and fail to utilize valuable knowledge within high-loss samples. To address these issues, we propose Jo-SRC, a noise-robust approach that combines sample selection and model regularization based on consistency. Our method trains the network using contrastive learning, estimating the likelihood of each sample being clean or out-of-distribution based on predictions from two different views. Additionally, we introduce a joint loss that incorporates consistency regularization to improve model generalization performance. Our extensive experiments demonstrate the superiority of Jo-SRC compared to existing state-of-the-art methods. The source code and models are available at https://github.com/NUST-Machine-Intelligence-Laboratory/Jo-SRC.