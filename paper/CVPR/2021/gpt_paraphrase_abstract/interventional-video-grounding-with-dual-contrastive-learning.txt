Video grounding, which involves localizing a specific moment in a video based on a given textual query, is typically approached by aligning visual and language stimuli using likelihood-based matching or regression strategies. However, these methods often suffer from spurious correlations between language and video features due to dataset selection bias. To address this issue, we propose a new paradigm called interventional video grounding (IVG) that leverages backdoor adjustment and structured causal models (SCMs) to deconfound the selection bias. Additionally, we introduce a dual contrastive learning approach (DCL) to better align text and video by maximizing the mutual information (MI) between query and video clips, as well as between start/end frames of a target moment and other frames within the video. Experimental results on three benchmarks demonstrate the effectiveness of our proposed methods.