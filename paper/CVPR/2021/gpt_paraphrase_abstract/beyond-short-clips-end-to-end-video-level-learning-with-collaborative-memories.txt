Traditional methods of training video models involve selecting one clip from a video during each iteration and optimizing the prediction of that clip based on the overall video label. However, we argue that a single clip may not provide sufficient temporal coverage to accurately recognize the label, especially considering that video datasets often lack detailed temporal annotations. Additionally, optimizing the model using brief clips hinders its ability to learn long-term temporal dependencies. To address these limitations, we propose a collaborative memory mechanism that encodes information from multiple sampled clips of a video in each training iteration. This allows the model to learn long-range dependencies beyond just a single clip. We explore different design choices for the collaborative memory to make the optimization process more manageable. Our proposed framework is fully trainable from end to end and significantly enhances the accuracy of video classification without imposing a significant computational burden. Through extensive experiments, we demonstrate that our framework performs well across various video architectures and tasks, surpassing the current state-of-the-art in both action recognition (e.g., Kinetics-400 & 700, Charades, Something-Something-V1) and action detection (e.g., AVA v2.1 & v2.2).