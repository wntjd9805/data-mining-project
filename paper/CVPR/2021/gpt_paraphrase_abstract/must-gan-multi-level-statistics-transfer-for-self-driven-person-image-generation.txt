We propose a novel approach for pose-guided person image generation that eliminates the need for paired source-target images during training. Our model disentangles and transfers multi-level appearance features from person images and combines them with pose features to reconstruct the source person images. This allows the source images to be used as self-supervision for person image generation. Our model extracts multi-level features using an appearance encoder and learns optimal appearance representation through attention mechanism and attribute statistics. These features are then transferred to a pose-guided generator for re-fusion of appearance and pose. Our approach enables flexible manipulation of person appearance and pose properties for pose transfer and clothes style transfer tasks. Experimental results on the DeepFashion dataset show that our method outperforms state-of-the-art supervised and unsupervised methods. Furthermore, our approach also performs well in real-world scenarios.