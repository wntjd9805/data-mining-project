Self-supervised learning has shown great potential in leveraging unlabeled data for various video tasks. In this study, we focus on enhancing semi-supervised action proposal generation by utilizing self-supervised methods. We introduce a novel framework called Self-supervised Semi-supervised Temporal Action Proposal (SSTAP), which consists of two important branches: the temporal-aware semi-supervised branch and the relation-aware self-supervised branch. The semi-supervised branch improves the proposal model by incorporating two temporal perturbations, namely temporal feature shift and temporal feature flip, within the mean teacher framework. On the other hand, the self-supervised branch defines two pretext tasks, which are masked feature reconstruction and clip-order prediction, to learn the relationship between temporal clues. This allows SSTAP to effectively explore unlabeled videos and enhance the discriminative abilities of learned action features.To evaluate the effectiveness of SSTAP, we conducted extensive experiments on the THUMOS14 and ActivityNet v1.3 datasets. The results demonstrate that SSTAP outperforms state-of-the-art semi-supervised methods and even performs comparably to fully-supervised methods. We have made the code for SSTAP available at https://github.com/wangxiang1230/SSTAP.