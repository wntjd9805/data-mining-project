Generalization to unfamiliar data poses a challenge for Visual Question Answering (VQA) models. In order to assess the ability to handle new questions, we propose categorizing them into two parts: "skills" and "concepts". "Skills" refer to visual tasks like counting or recognizing attributes, while "concepts" pertain to objects and people mentioned in the question. Effective VQA models should be capable of combining skills and concepts in novel ways, even if those specific combinations were not encountered during training. However, our study reveals that existing models have room for improvement in handling new combinations. To address this, we introduce a novel approach that learns to combine skills and concepts by implicitly separating them within a model. We achieve this by learning grounded concept representations and disentangling the encoding of skills from concepts. To enforce these properties, we employ a contrastive learning procedure that does not rely on external annotations and can be learned from unlabeled image-question pairs. Our experiments demonstrate that our approach effectively enhances compositional and grounding performance. The answer format, however, only provides abstract outputs.