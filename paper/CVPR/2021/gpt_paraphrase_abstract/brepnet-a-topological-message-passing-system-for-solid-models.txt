This study introduces a neural network architecture called BRepNet, which is designed to directly process Boundary representation (B-rep) models in Computer-Aided Design (CAD) applications. B-rep models combine lightweight parametric curves and surfaces with topological information to describe 3D shapes. Unlike existing methods that approximate B-rep models as meshes or point clouds, BRepNet operates directly on the B-rep data structures. It defines convolutional kernels based on oriented coedges in the data structure. In the vicinity of each coedge, a small set of faces, edges, and coedges can be identified, and patterns in their feature vectors can be detected using learnable parameters. To facilitate further research on deep learning with B-reps, the Fusion 360 Gallery segmentation dataset is published, which consists of over 35,000 annotated B-rep models with information about the modeling operations that created each face. The study demonstrates that BRepNet can segment these models more accurately than methods that work on meshes or point clouds. The convolutional kernels of BRepNet are defined with respect to topological entities called coedges, and feature vectors from adjacent faces, edges, and coedges are multiplied by learnable parameters to perform face segmentation.