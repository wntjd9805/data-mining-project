This paper proposes a Graph-based High-order Relation Modeling (GHRM) module to analyze the complex relationships among visual concepts in long-term actions for recognition purposes. The authors highlight the importance of basic relations, such as objects, motions, and sub-actions, and how they influence each other during the temporal evolution of long-term actions. These interactions form high-order relations, which are crucial for accurate recognition. The GHRM module represents each basic relation as a graph, with each node representing a segment in a long video. Importantly, when modeling each basic relation, the module incorporates information from all other basic relations, enabling effective exploitation of high-order relations. To capture high-order relations in the temporal dimension, the module includes a Temporal-GHRM branch and a Semantic-GHRM branch. The former focuses on modeling local temporal high-order relations, while the latter captures global semantic high-order relations. The effectiveness of the proposed model is demonstrated through experiments conducted on three long-term action recognition datasets: Breakfast, Charades, and MultiThumos. The results validate the model's ability to exploit high-order relations and its potential for accurate recognition.