We propose a novel method for generating human animations from a single image and a sequence of body poses. Existing methods often produce visual artifacts and fail to preserve the person's identity and textures when applied to new scenes. To overcome these limitations, we develop a compositional neural network that predicts the silhouette, garment labels, and textures. Each modular network focuses on a specific subtask that can be learned from synthetic data. During inference, we use the trained network to create a unified representation of appearance and labels in UV coordinates, which remains consistent across poses. This representation serves as a strong guide for generating appearance based on pose changes. We then use the network to complete the appearance and render it with the background. By employing these strategies, we can synthesize human animations that maintain the person's identity and appearance in a temporally coherent manner without requiring fine-tuning on the testing scene. Our experiments demonstrate that our method outperforms existing approaches in terms of synthesis quality, temporal coherence, and generalization ability.