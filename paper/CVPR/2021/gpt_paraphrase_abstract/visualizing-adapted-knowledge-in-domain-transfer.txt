A common issue in unsupervised domain adaptation (UDA) is that the source and target models encode different knowledge. To better understand this adaptation process, we use image translation to highlight the differences in knowledge between the two models. We achieve this by feeding a translated image and its original version to both models separately, creating two branches. By updating the translated image, we aim to produce similar outputs from both branches. This allows us to use the differences between the two images to represent the knowledge disparity between the models. To enforce similar outputs from the two branches and visualize the adapted knowledge, we propose a source-free image translation method. This method generates source-style images using only target images and the two models. We apply this approach to various datasets with different UDA methods and observe that the generated images successfully capture the style differences between the two domains. Furthermore, we demonstrate the practical application of these generated images by showing that they can be used to fine-tune the target model without needing access to the source data. This ability to tune the target model without relying on the source data can be valuable in real-world scenarios. The code for our method is available at https://github.com/hou-yz/DA_visualization.