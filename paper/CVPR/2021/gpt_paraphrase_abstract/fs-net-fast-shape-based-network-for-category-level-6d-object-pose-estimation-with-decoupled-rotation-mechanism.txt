This paper introduces a fast shape-based network (FS-Net) for estimating 6D pose and size at the category level using a monocular RGB-D image. Existing methods in this area suffer from inefficient category-level pose feature extraction, resulting in low accuracy and inference speed. To address this issue, the authors propose FS-Net, which utilizes an orientation aware autoencoder with 3D graph convolution for efficient feature extraction. The 3D graph convolution ensures that the learned latent feature is insensitive to shifts in point positions and object sizes. Additionally, a novel decoupled rotation mechanism is introduced to efficiently decode category-level rotation information from the latent feature using two decoders. The translation and size are estimated using residuals based on the differences between the mean object points and the ground truth translation, as well as the mean size of the category and the ground truth size, respectively. To enhance the generalization ability of FS-Net, an online box-cage based 3D deformation mechanism is proposed for data augmentation during training. Experimental results on benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance in both category- and instance-level 6D object pose estimation. Notably, in category-level pose estimation, the method outperforms existing approaches by 6.3% on the NOCS-REAL dataset without the need for additional synthetic data.