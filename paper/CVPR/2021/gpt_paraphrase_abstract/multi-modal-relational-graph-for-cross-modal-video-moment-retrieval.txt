The aim of cross-modal video moment retrieval is to rank video moments based on a query sentence. Previous work focused on learning textual and visual representations separately and then finding interactions between them. However, this approach does not effectively address the need to identify fine-grained differences among video moment candidates. Additionally, the relationship between objects in both the video and the sentence is often overlooked. To address these issues, we propose a multi-modal relational graph that captures interactions between objects in the visual and textual content to identify differences among similar video moments. We first introduce visual and textual relational graphs to form relation-aware representations through message propagation. We then use multi-task pre-training to enhance the structured visual representation by explicitly defining relations. Finally, we employ graph matching and boundary regression for cross-modal retrieval. Our experiments on daily activities and cooking activities datasets demonstrate significant improvements over existing solutions.