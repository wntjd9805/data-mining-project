This study addresses the challenge of pre-training for 3D data, which has been successfully achieved for 2D images. The authors propose a general method for self-supervised representation learning in 3D data that is independent of the neural network architecture used and takes advantage of the geometric nature of 3D point cloud data. The proposed task involves segmenting 3D points into different geometric partitions. A self-supervised loss is formulated based on the assumption that these partitions implicitly represent a latent Gaussian Mixture Model (GMM), which establishes a data likelihood function. The pretext task can be seen as an encoder-decoder paradigm that utilizes an implicitly defined parametric discrete generative model bottleneck. The authors demonstrate that existing neural network architectures designed for supervised point cloud segmentation can be repurposed for this unsupervised pretext task. By maximizing data likelihood with respect to the soft partitions formed by the unsupervised point-wise segmentation network, the learned representations are encouraged to contain rich geometric information. Experimental results show that this method induces semantic separation in feature space, leading to state-of-the-art performance on downstream tasks such as model classification and semantic segmentation.