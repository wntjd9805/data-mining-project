This paper introduces a new way of visual reasoning that focuses on the concept of transformation. The authors argue that current visual reasoning tasks only test the machine's understanding of static settings, such as single images, and fail to capture its ability to infer dynamics between different states. To address this limitation, the authors propose a transformation-driven visual reasoning task that requires inferring single-step or multi-step transformations between initial and final states. They create a new dataset called TRANCE based on the CLEVR dataset, consisting of three levels of settings: Basic, Event, and View. Experimental results show that current visual reasoning models perform well on Basic but struggle with Event and View, indicating a need for more advanced methods and real data. The authors believe that this new paradigm will contribute to the development of machine visual reasoning and call for further research in this direction. The TVR resource can be accessed at https://hongxin2019.github.io/TVR.