We introduce a high-performance approach to achieve mask-level instance segmentation using only bounding-box annotations for training. Previous studies have explored this scenario, but our method demonstrates significantly improved performance (e.g., increasing the best reported mask average precision from 21.1% to 31.6% on the COCO dataset). Our approach involves redesigning the loss function for learning instance masks without modifying the segmentation network itself. This is accomplished through two loss terms: 1) a surrogate term that minimizes the discrepancy between the projections of the ground-truth box and the predicted mask, and 2) a pairwise loss that leverages the knowledge that neighboring pixels with similar colors are likely to have the same category label. Experimental results show that our redesigned mask loss produces high-quality instance masks using only box annotations. For instance, without any mask annotations, we achieve a mask average precision of 33.2% on the COCO test-dev split, compared to 39.1% for the fully supervised approach. Our method significantly reduces the performance gap between weakly and fully supervised instance segmentation. The code for our method is available at: https://git.io/AdelaiDet.