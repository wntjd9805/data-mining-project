The abstract discusses the need for a model that can understand videos by learning the interaction between the static content of a scene and its dynamics. The model should be able to predict the future progression of a scene based on an initial image and explain a video in terms of its static image content and additional characteristics not present in the initial frame. This suggests a bijective mapping between the video domain and the static content with residual information. Unlike existing image-to-video synthesis models, this approach provides a one-to-one mapping between residual vectors and videos with stochastic outcomes when sampling. The model is implemented using a conditional invertible neural network (cINN) that independently models static and video characteristics, enabling controlled video synthesis. Experimental results on various video datasets demonstrate the effectiveness of the approach in terms of both the quality and diversity of the synthesized results.