This paper introduces an agent that mimics human behavior in understanding and carrying out high-level instructions in visual environments. The agent focuses on the task of correctly localizing a remote target object specified by a natural language instruction. The agent undergoes a two-stage training process, starting with cross-modal alignment sub-tasks to learn scene and object grounding. Then, an attentive action decoder is used to generate action sequences by fusing vision, language, and memory experiences. Experimental results demonstrate that the proposed method outperforms previous state-of-the-art approaches, highlighting its effectiveness.