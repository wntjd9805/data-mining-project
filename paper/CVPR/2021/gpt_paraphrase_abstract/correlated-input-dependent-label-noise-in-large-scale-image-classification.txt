We propose a probabilistic approach to modeling label noise in large-scale image classification datasets. By placing a multivariate Normal distributed latent variable on the final hidden layer of a neural network classifier, we capture the input-dependent label noise. The covariance matrix of this latent variable represents the aleatoric uncertainty caused by label noise. Our method effectively captures known sources of label noise between semantically similar and co-occurring classes. In comparison to standard neural network training and other baselines, our approach achieves significantly improved accuracy on various datasets including Imagenet ILSVRC 2012 (79.3% + 2.6%), Imagenet-21k (47.0% + 1.1%), JFT (64.7% + 1.6%), and WebVision 1.0 (76.6% top-1 accuracy), setting a new state-of-the-art result. These datasets vary in size from over 1 million to over 300 million training examples and consist of 1,000 to more than 21,000 classes. Our method is simple to use and can be easily implemented as a drop-in replacement for the final fully-connected layer in a deep classifier.