The computational cost of generative adversarial networks (GANs), such as Style-GAN2, limits their efficient deployment on edge devices. Previous compression approaches have not been effective for GANs, especially for unconditional GANs. This paper proposes new methods for compressing unconditional GANs, including channel pruning, knowledge distillation, and a content-aware approach. By pruning unimportant channels and focusing on specific regions, the proposed methods achieve significant improvements in compression quality compared to existing methods. For example, the FLOPs of StyleGAN2 are reduced by 11Ã— without noticeable image quality loss. Additionally, the compressed model improves the disentangled latent manifold, making it more effective for image editing tasks.