Human motion retargeting is a technique that involves transferring the movement of one person in a video or set of images to another person. Current methods require a long training video specific to each target person to train a motion transfer model. However, this approach is not scalable as it requires labor-intensive acquisition and processing of training videos. Recently, few-shot motion transfer techniques that only require a small number of images have gained attention. These techniques typically use either 2D or explicit 3D representations, sacrificing either accurate geometric modeling or the flexibility of an end-to-end learned representation.   In this study, we propose a new approach inspired by the Transformable Bottleneck Network, which can render novel views and manipulations of rigid objects. Our approach is based on an implicit volumetric representation of the image content, which can be spatially manipulated using volumetric flow fields. We tackle the challenge of aggregating information across different body poses by learning flow fields that combine content from the appropriate regions of input images. This allows us to learn a 3D representation solely from videos of moving people. By combining 3D object understanding and end-to-end learned rendering, our representation achieves state-of-the-art image generation quality, as demonstrated by both quantitative and qualitative evaluations.