The current focus in temporal event or action localization is on actions captured by a single camera. However, in real-world scenarios, events or actions may be captured as a sequence of shots by multiple cameras at different positions. This paper introduces a new task called multi-shot temporal event localization and presents a large-scale dataset called MUSES, which contains 31,477 event instances across 716 hours of video. The key characteristic of MUSES is the frequent shot cuts, with an average of 19 shots per instance and 176 shots per video, resulting in significant variations within instances. Evaluations demonstrate that the current state-of-the-art method achieves only a 13.1% mean average precision (mAP) at an intersection over union (IoU) of 0.5. As a minor contribution, a simple baseline approach is proposed to handle intra-instance variations, achieving an mAP of 18.9% on MUSES and 56.9% on the THUMOS14 dataset at an IoU of 0.5. To support further research in this area, the dataset and project code are made available at https://songbai.site/muses/.