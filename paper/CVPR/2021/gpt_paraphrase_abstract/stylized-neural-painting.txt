This study presents a novel approach for translating images into realistic and visually appealing paintings with adjustable styles. Unlike previous methods that focused on pixel-level predictions, we tackle the artistic creation process in a vectorized environment. By generating a sequence of stroke parameters that hold physical meaning, we enable further rendering of the artwork. However, as traditional vector rendering is not differentiable, we introduce a neural renderer that mimics its behavior. We frame stroke prediction as a parameter search process that maximizes similarity between the input and rendering output. To address the zero-gradient problem in parameter search, we propose an optimal transportation-based solution. Additionally, we identify a parameter coupling issue in existing neural renderers and redesign the rendering network by introducing a rasterization network and a shading network. This redesign enhances the separation of shape and color. Experimental results demonstrate that our method produces paintings with high fidelity in terms of overall appearance and local textures. Furthermore, our approach can be combined with neural style transfer to incorporate visual styles from other images. The code and animated results are available at the following link: https://jiupinjia.github.io/neuralpainter/.