Visual question answering (VQA) is a task that involves providing a natural-language answer based on an image and a question. Current VQA models are primarily evaluated based on their accuracy in answering high-level reasoning questions. However, it is important to understand the extent to which low-level perception, such as object recognition, poses a problem in the overall compositional process required for high-level reasoning. In this study, we introduce MetaVQA, a framework that allows for the benchmarking of the perception capabilities of VQA models. MetaVQA employs principles from software metamorphic testing to assess the perception consistency of VQA models. Given an image, MetaVQA synthesizes a low-level perception question and transforms it along with the image into sub-questions and sub-images. By checking the answers to these transformed questions and images against a set of metamorphic relationships, MetaVQA can identify perception failures in VQA models. Through metamorphic testing, MetaVQA successfully detects over 4.9 million perception failures in popular VQA models. Notably, even the state-of-the-art VQA models, including the champion of the VQA 2020 Challenge, demonstrate perception consistency problems. In contrast, the Oscar VQA models, which utilize anchor points to align questions and images, exhibit better consistency in perception tasks. The introduction of MetaVQA aims to rekindle interest in improving the low-level perceptual abilities of VQA models, as they serve as the foundation for high-level reasoning. The answer format provided focuses solely on the abstract.