Registering dressed humans' point clouds to parametric human models is a difficult task in computer vision. Traditional methods require manual pose initialization and extensive post-processing, while newer learning-based approaches aim to automate the process. However, existing methods struggle to provide accurate pose initialization, particularly in regressing joint rotations from point clouds or images of humans. To address this, we propose a novel approach called piecewise transformation fields (PTF), which uses functions to learn 3D translation vectors for mapping query points in posed space to their corresponding positions in rest-pose space. By combining PTF with multi-class occupancy networks, we develop a learning-based framework that predicts shape and per-point correspondences between posed space and the canonical space for clothed humans. Our key insight is that translation vectors can be accurately estimated using point-aligned local features, enabling efficient calculation of rigid per bone transformations and joint rotations through least-square fitting. This avoids the challenging task of directly regressing joint rotations from neural networks. Additionally, the proposed PTF facilitates canonicalized occupancy estimation, improving generalization capability and resulting in more accurate surface reconstruction with fewer parameters compared to the state-of-the-art methods. Both qualitative and quantitative evaluations demonstrate that fitting parametric models with poses initialized by our network leads to superior registration quality, particularly for extreme poses.