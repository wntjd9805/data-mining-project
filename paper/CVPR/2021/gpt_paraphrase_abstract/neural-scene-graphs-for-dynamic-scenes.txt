Recent advancements in implicit neural rendering have demonstrated the ability to generate accurate view synthesis for complex scenes by training models on RGB images. However, these methods have limitations in representing dynamic scenes and decomposing them into individual objects. In this study, we introduce a novel neural rendering approach that represents multi-object dynamic scenes as scene graphs. We propose a learned scene graph representation that encodes object transformations and radiance, enabling efficient rendering of new scene arrangements and views. Our method utilizes implicitly encoded scenes and a jointly learned latent representation to describe similar objects with a single implicit function. We evaluate the proposed method using both synthetic and real automotive data, demonstrating its capability to learn dynamic scenes solely from observing a video of the scene. Furthermore, our approach allows for rendering realistic views of novel scene compositions, including unseen sets of objects in unseen poses.