We propose a new framework that uses visual semantic role labeling to understand and represent related events in videos. We represent videos as a collection of events, each consisting of a verb and multiple entities fulfilling different roles in that event. To address the challenging task of semantic role labeling in videos, we introduce the VidSitu benchmark. This benchmark is a large-scale dataset containing 29,000 10-second movie clips that have been extensively annotated with verbs and semantic roles every 2 seconds. Within each movie clip, entities are co-referenced across events, and events are connected through event-event relations. The clips in VidSitu are drawn from a diverse collection of approximately 3,000 movies and exhibit complexity with an average of 4.2 unique verbs per video. Furthermore, approximately 200 verbs have more than 100 annotations each, ensuring diversity in the dataset. We conduct a comprehensive analysis of the dataset, comparing it to other publicly available video understanding benchmarks, and evaluate various standard video recognition models using several illustrative baselines. The code and dataset for VidSitu can be accessed at vidsitu.org.