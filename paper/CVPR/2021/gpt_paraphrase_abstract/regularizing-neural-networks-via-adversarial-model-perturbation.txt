This study introduces a novel regularization technique called adversarial model perturbation (AMP) in deep learning. The authors argue that flat local minima of the empirical risk lead to better generalization, and therefore propose minimizing an alternative "AMP loss" instead of directly minimizing the empirical risk. The AMP loss is obtained by applying the worst norm-bounded perturbation on each point in the parameter space. Unlike other regularization schemes, AMP has strong theoretical justifications as it can be shown to favor flat local minima. Extensive experiments on different deep architectures demonstrate that AMP outperforms other regularization schemes, establishing it as a new state-of-the-art technique. The code for AMP is available at https://github.com/hiyouga/AMP-Regularizer.