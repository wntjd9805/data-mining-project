We introduce a new task of 3D visual grounding in single-view RGBD images, where the objects being referred to are often only partially visible due to occlusion. Previous approaches generated object proposals for grounding in 3D scenes, but we propose a bottom-up approach that gradually aggregates content-aware information to overcome the challenge of partial geometry. Our method combines language and visual features to generate a heatmap that roughly localizes the relevant regions in the RGBD image. We then use adaptive feature learning and visio-linguistic fusion to perform object-level matching and ground the referred object. We evaluate our approach on both the ScanRefer and SUNRefer datasets, and it outperforms state-of-the-art methods by a significant margin (11.2% and 15.6% Acc@0.5) on both datasets.