In our daily lives, we encounter a multitude of audio and visual events that are synchronized. These events consist of audio scenes that are associated with specific visual objects. Additionally, sounding objects can help to indicate and separate individual sounds in the audio track. Building upon this observation, we introduce a novel approach called cyclic co-learning (CCoL) that simultaneously learns visual grounding of sounding objects and audio-visual sound separation within a unified framework.The CCoL paradigm exploits the grounded relationships between objects and sounds to enhance the performance of sound separation. By leveraging discriminative information obtained from separated sounds, we are able to improve the training example sampling for sounding object grounding. This establishes a co-learning cycle between the two tasks, fostering mutual benefits and synergy.Extensive experiments demonstrate that our proposed framework surpasses the performance of recent approaches in both sounding object visual grounding and audio-visual sound separation. Furthermore, our cyclic co-learning approach enables the tasks to mutually enhance each other. The source code and pre-trained models for our framework can be accessed at https://github.com/YapengTian/CCOL-CVPR21.