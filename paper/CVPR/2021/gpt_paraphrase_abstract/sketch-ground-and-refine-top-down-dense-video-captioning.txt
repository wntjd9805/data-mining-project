The task of dense video captioning is to identify and describe a series of events in a video in a detailed and coherent manner. Previous approaches have followed a "detect-then-describe" framework, where events are first detected in the video and then descriptions are generated for those events. However, the definition of events can vary, ranging from simple actions to complex sets of events, depending on the semantic context. This makes directly detecting events based on video information ambiguous and negatively impacts the coherence and accuracy of the generated captions. In this study, we propose a different approach that reverses the "detect-then-describe" fashion. Our method takes a top-down approach, starting with generating paragraphs from a global perspective and then refining each event description by associating it with a specific video segment. This approach is called the Sketch, Ground, and Refine process (SGR). In the first stage, a coarse-grained multi-sentence paragraph is generated to describe the entire video. Each sentence in the paragraph represents an event, which is then localized in the grounding stage. In the refining stage, we enhance the quality of the captions through refinement-enhanced training and dual-path cross attention, which focuses on both the coarse-grained event captions and the aligned event segments. This allows the updated event captions to adjust their segment boundaries as needed.Our SGR model surpasses state-of-the-art methods in the evaluation of traditional and story-oriented dense captions on the ActivityNet Captioning benchmark. The code for our model will be made available on github.com/bearcatt/SGR.