Deepfake technology, which involves creating manipulated videos that appear to be real, has become a major concern in society. As a result, there has been a surge of research efforts focused on developing methods to detect these fake videos. Most existing approaches treat deepfake detection as a binary classification problem, where a global feature is extracted using a backbone network and then fed into a binary classifier to determine whether the video is real or fake. However, we argue that this approach is not optimal because the differences between real and fake images are often subtle and localized.   In this paper, we propose a new approach to deepfake detection that treats it as a fine-grained classification problem. Our method incorporates three key components:   1) Multiple spatial attention heads: This allows the network to focus on different local parts of the image, enabling a more detailed analysis of the subtle differences between real and fake images.  2) Textural feature enhancement block: This component enhances the shallow features of the image to better capture the subtle artifacts that are indicative of deepfakes.  3) Aggregation of low-level textural features and high-level semantic features: The attention maps guide the combination of these features, enabling a comprehensive analysis of the image.  To address the challenges in training this network, we introduce a new regional independence loss and an attention guided data augmentation strategy. These techniques help improve the learning process and enhance the network's ability to detect deepfakes.  We conducted extensive experiments on various datasets and compared our method to traditional binary classifiers. The results show that our approach outperforms these counterparts and achieves state-of-the-art performance in deepfake detection. Our models are available for public use at https://github.com/yoctta/multiple-attention.