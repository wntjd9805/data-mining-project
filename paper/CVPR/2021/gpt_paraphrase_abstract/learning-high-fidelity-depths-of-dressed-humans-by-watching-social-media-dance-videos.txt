Limited availability of ground truth data, such as 3D scanned models, poses a challenge in accurately reconstructing the geometry of dressed humans from real-world imagery. To overcome this challenge, we propose utilizing social media dance videos as a new data resource. These videos showcase diverse appearances, clothing styles, performances, and identities, but lack 3D ground truth geometry. Our method involves using local transformations to warp the predicted local geometry of a person from one image to another at a different time, enabling self-supervision and enforcing temporal coherence in the predictions. We also simultaneously learn the depth and surface normals, which are sensitive to local texture, wrinkles, and shading, by maximizing their geometric consistency. The proposed method is trainable end-to-end, resulting in high fidelity depth estimation that accurately predicts fine geometry based on the input real image. Experimental results demonstrate that our method surpasses existing approaches for human depth estimation and shape recovery on both real and rendered images.