Efficiently driving head pose in audio-driven talking face generation remains a challenge, despite achieving accurate lip synchronization for arbitrary subjects. Previous methods rely on pre-estimated structural information like landmarks and 3D parameters to generate personalized rhythmic movements. However, the accuracy of such estimates degrades under extreme conditions. This paper proposes a clean and effective framework for generating pose-controllable talking faces using non-aligned raw face images and a single photo as an identity reference. The key idea is to modularize audio-visual representations by creating an implicit low-dimension pose code. Speech content and head pose information are embedded together in a non-identity space. The speech content is defined by learning the intrinsic synchronization between audio and visual modalities, while the pose code is learned through a modulated convolution-based reconstruction framework. Extensive experiments demonstrate that our method generates lip-synced talking faces with accurate poses that can be controlled by other videos. Additionally, our model exhibits advanced capabilities such as robustness to extreme views and frontalization of talking faces.