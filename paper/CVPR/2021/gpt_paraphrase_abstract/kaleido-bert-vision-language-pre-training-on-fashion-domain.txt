We introduce a novel vision-language pre-training model called Kaleido-BERT, which utilizes a unique kaleido strategy to enhance cross-modality representations in the fashion domain using transformers. Unlike previous models that use random masking, we employ alignment guided masking to prioritize image-text semantic relations. Our approach involves five novel tasks - rotation, jigsaw, camouflage, grey-to-color, and blank-to-color - for self-supervised pre-training with patches of varying scales. Kaleido-BERT is straightforward to implement and can be easily integrated into the existing BERT framework. It achieves state-of-the-art performance on four downstream tasks: text retrieval (4.03% absolute improvement in R@1), image retrieval (7.13% absolute improvement in R@1), category recognition (3.28% absolute improvement in ACC), and fashion captioning (1.2 absolute improvement in Bleu4). We validate the effectiveness of Kaleido-BERT on various e-commerce websites, highlighting its potential for real-world applications.