Traditional methods for vision-and-language navigation (VLN) train models as a whole, but struggle to perform well in environments with unrestricted movement. Taking inspiration from robotics, we propose a modular approach to VLN using topological maps. By combining natural language instructions and topological maps, our method uses attention mechanisms to predict a navigation plan within the map. This plan is then executed using low-level actions controlled by a robust controller. Experimental results demonstrate that our approach surpasses previous end-to-end methods in terms of performance, generates navigation plans that are easy to understand, and exhibits intelligent behaviors like backtracking.