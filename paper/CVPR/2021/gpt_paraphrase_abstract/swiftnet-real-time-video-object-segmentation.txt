We introduce SwiftNet, a novel approach for real-time semi-supervised video object segmentation (one-shot VOS). Our method achieves impressive results, with 77.8% J & F and 70 FPS on the DAVIS 2017 validation dataset, outperforming all existing solutions in terms of overall accuracy and speed. We accomplish this by effectively reducing spatiotemporal redundancy in matching-based VOS using Pixel-Adaptive Memory (PAM). Temporally, PAM intelligently updates memory only on frames where objects exhibit significant inter-frame variations. Spatially, PAM selectively updates and matches memory on dynamic pixels while ignoring static ones, greatly reducing unnecessary computations on segmentation-irrelevant pixels. Additionally, we introduce a light-aggregation encoder with reversed sub-pixel to enhance efficient reference encoding in SwiftNet. Our aim is for SwiftNet to serve as a strong and efficient baseline for real-time VOS and promote its application in mobile vision. The source code of SwiftNet can be accessed at https://github.com/haochenheheda/SwiftNet.