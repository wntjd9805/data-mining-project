Automated radiographic report generation is a challenging task that involves generating descriptions of fine-grained visual differences between diseased and healthy cases. Existing methods for image captioning are not suitable for this task as they do not address this specific requirement. To address this gap, we propose a self-boosting framework that enhances radiographic report generation by combining the main task of report generation with an auxiliary task of image-text matching. These two tasks are integrated as branches in a network model and mutually influence each other. The image-text matching branch helps the report generation branch learn visual features that are highly correlated with text, resulting in high-quality reports. In turn, the improved reports generated by the report generation branch provide more challenging samples for the image-text matching branch, pushing it to improve its visual and text feature representations. This iterative and progressive training process allows the two branches to mutually enhance each other without the need for external resources. Experimental results demonstrate the effectiveness of our method on two public datasets, outperforming several state-of-the-art image captioning and medical report generation methods.