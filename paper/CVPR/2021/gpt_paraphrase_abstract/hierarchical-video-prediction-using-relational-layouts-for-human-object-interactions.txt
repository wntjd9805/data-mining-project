We address the challenge of modeling and predicting human-object interactions during actions, which is difficult due to the complexity involved. Current video prediction models are not effective in capturing these intricate interactions. Our approach is based on hierarchical video prediction models, which break down the video generation process into two stages. First, we predict a high-level representation, such as a sequence of poses, and then learn a model to translate these poses into pixel-level generation. Human-object interaction tasks typically involve the evolution of pose, appearance of the person, location of objects, and appearance of objects over time. To tackle this complexity, we propose a Hierarchical Video Prediction model that utilizes Relational Layouts. In the first stage, we learn to predict a sequence of layouts, which combine pose and object information for each frame. We achieve this by employing relational reasoning and recurrent neural networks to model the relationships between poses and objects. The layout sequence serves as a strong structural prior for the second stage, where we learn to map the layouts into pixel space. Our method is evaluated on two datasets, UMD-HOI and Bimanual, and demonstrates significant improvements in standard video evaluation metrics such as LPIPS, PSNR, and SSIM. Additionally, we perform a detailed qualitative analysis to showcase the various generalizations achieved by our model.