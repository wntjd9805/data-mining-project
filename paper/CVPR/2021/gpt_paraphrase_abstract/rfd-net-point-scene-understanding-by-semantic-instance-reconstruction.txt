Semantic scene understanding from point clouds is a difficult task due to the sparse nature of the data. Previous methods have relied on converting point clouds into regular grids and using grid-based convolutions for scene understanding. However, in this research, we propose a new approach called RfD-Net that directly detects and reconstructs dense object surfaces from raw point clouds.Instead of using regular grids, our method takes advantage of the sparsity of point cloud data and focuses on predicting shapes that have a high level of objectness. This approach separates the instance reconstruction into two steps: global object localization and local shape prediction. By doing so, we alleviate the challenge of learning 2-D manifold surfaces from sparse 3D space. Additionally, the point clouds within each object proposal contain shape details that help in learning implicit functions for reconstructing high-resolution surfaces.Our experiments demonstrate that instance detection and reconstruction have complementary effects. The shape prediction head consistently improves object detection when combined with modern 3D proposal network backbones. Furthermore, qualitative and quantitative evaluations show that our approach consistently outperforms existing methods and achieves an improvement of over 11 in mesh IoU for object reconstruction.