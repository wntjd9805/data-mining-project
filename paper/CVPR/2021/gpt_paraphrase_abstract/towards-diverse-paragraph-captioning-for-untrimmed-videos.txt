This paper presents a novel approach to video paragraph captioning, which aims to describe multiple events in untrimmed videos with descriptive paragraphs. Existing methods typically solve this problem in two steps: event detection and event captioning. However, this two-step approach heavily relies on the accuracy of event proposal detection, which is a challenging task. To overcome this limitation, the proposed model directly generates paragraphs for untrimmed videos without the need for event detection. To ensure coherent and diverse event descriptions, the model enhances the conventional temporal attention mechanism with dynamic video memories. This enhancement allows the model to progressively expose new video features and suppress over-accessed video contents, effectively controlling its visual focuses. Furthermore, a diversity-driven training strategy is introduced to improve the diversity of the generated paragraphs from a language perspective. Considering that untrimmed videos often contain redundant frames, the video encoder is augmented with keyframe awareness to improve efficiency. Experimental results on the ActivityNet and Charades datasets demonstrate that the proposed model outperforms state-of-the-art approaches in terms of both accuracy and diversity metrics, even without using event boundary annotations. The code for the proposed model will be made available at https://github.com/syuqings/video-paragraph.