Compressing large neural networks is crucial for deploying them on resource-constrained computational platforms. Vector quantization is an effective framework that represents multiple parameters with a single code, and has recently achieved impressive network compression results in various vision and natural language processing tasks. The key to successful vector quantization lies in determining which parameter groups should be compressed together. Previous approaches have relied on heuristics that group the spatial dimension of individual convolutional filters, but a comprehensive solution has yet to be addressed. This is particularly important for pointwise convolutions, linear layers, and convolutions with multiple filters compressed to the same codeword. To tackle this issue, we propose permuting the weights of adjacent layers, which allows for the same function to be expressed. By establishing a connection to rate-distortion theory, we search for permutations that make networks easier to compress. Additionally, we utilize an annealed quantization algorithm to improve network compression and achieve higher final accuracy. Our experiments on image classification, object detection, and segmentation demonstrate a reduction in the gap between the compressed and uncompressed models by 40 to 70% compared to the current state-of-the-art. The code used for all our experiments can be found at https://github.com/uber-research/permute-quantize-finetune.