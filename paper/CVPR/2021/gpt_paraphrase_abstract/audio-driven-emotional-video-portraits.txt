Previous studies on audio-driven talking heads have focused on the correlation between speech content and mouth shape, neglecting facial emotion as an important feature. In this research, we introduce Emotional Video Portraits (EVP), a system that synthesizes high-quality video portraits with dynamic emotional expressions driven by audio. We propose the Cross-Reconstructed Emotion Disentanglement technique to separate speech into two independent spaces: an emotion space unaffected by duration and a content space that varies with duration. Using these disentangled features, we can deduce dynamic 2D emotional facial landmarks. To generate the final high-quality video portraits, we propose the Target-Adaptive Face Synthesis technique, which bridges the gap between the deduced landmarks and the natural head poses of target videos. Our method is proven effective through extensive qualitative and quantitative experiments.