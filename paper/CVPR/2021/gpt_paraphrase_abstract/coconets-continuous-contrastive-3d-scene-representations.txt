This paper investigates the use of self-supervised learning to create amodal 3D feature representations from RGB and RGB-D images and videos. These representations are not dependent on the specific objects or scenes depicted and are then evaluated in tasks such as visual correspondence, object tracking, and object detection. The model generates a latent 3D representation of the scene by mapping continuous world 3D points to corresponding feature vectors. Training involves rendering 3D feature clouds from different viewpoints and matching them against the predicted 3D feature point cloud from the query view. Importantly, the representation can be queried for any 3D location, even if it is not visible in the input view. Our model combines three recent research ideas: 3D feature grids for view prediction, implicit functions to handle resolution limitations, and contrastive learning for unsupervised feature representation training. The resulting 3D visual feature representations effectively scale across objects and scenes, account for occluded or missing information, track objects over time, align semantically related objects in 3D, and improve 3D object detection. Our approach outperforms other state-of-the-art methods in 3D feature learning and view prediction, as they are either limited by spatial resolution, do not build amodal 3D representations, or cannot handle scene variability due to non-convolutional bottlenecks.