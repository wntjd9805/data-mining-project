Traditional methods for Unsupervised Domain Adaptation (UDA) rely on having access to labeled source data during training, which may not always be practical due to privacy and storage concerns. However, a recent approach has been proposed that transfers knowledge from a single source model to an unlabeled target domain without requiring access to the source data. However, when multiple trained source models are available, this method requires adapting each model individually to find the best source. This raises the question of whether it is possible to find the optimal combination of source models, without source data or target labels, that performs as well as the best individual source model. To address this, we propose a novel and efficient algorithm that automatically combines source models with suitable weights to achieve performance at least equal to the best source model. We provide theoretical insights to support our claim and conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our algorithm. In most cases, our method not only achieves the same accuracy as the best source model but also outperforms it.