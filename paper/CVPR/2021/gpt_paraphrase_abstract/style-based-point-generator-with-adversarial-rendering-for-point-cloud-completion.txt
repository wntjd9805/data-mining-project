This paper introduces a new approach called SpareNet for completing point clouds. The proposed method includes three key contributions. Firstly, a channel-attentiveEdgeConv is introduced to effectively capture both local structures and global shape information in point features. Secondly, inspired by the success of StyleGAN, the shape feature is treated as a style code that modulates normalization layers during the folding process, leading to improved performance in generating complex and accurate shapes. Thirdly, existing point supervisions are found to be inadequate in reflecting the perceptual quality of reconstructed points, so the completed points are projected onto depth maps using a differentiable renderer and adversarial training is applied to enhance the perceptual realism from different viewpoints. Extensive experiments on ShapeNet and KITTI datasets demonstrate the effectiveness of the proposed method, achieving state-of-the-art quantitative performance and superior visual quality.